<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nikkin.dev/blog/llm-entropy.html">Original</a>
    <h1>Entropy of a Large Language Model output</h1>
    
    <div id="readability-page-1" class="page">
<!-- Navigation bar for blog posts 
	darkmode.js should be included here instead of in header to work -->





<nav id="TOC" role="doc-toc">

</nav>
<p>Large language models like ChatGPT &amp; Claude have become
ubiquitous in today’s world. I visited a friend for the New years eve
and was impressed by how ChatGPT was being extensively used within the
family. The friend’s little kid is going to grow up in a world, where
turning to large language models (LLM) for quick answers is going to be
the norm rather than the exception.</p>
<p>LLMs do have their fair share of pitfalls. The primary, being the
problem of hallucination, meaning the LLM spitting out factually
incorrect answers. There has been (and will be) a lot of work into
understanding when and how these models hallucinate, but I wanted to
understand the output of the LLMs from an information theoretic
perspective.</p>
<p>Now, how does an LLM work? Language Models are probability
distributions over sequences of tokens/words<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> (In
Natural Language Processing, tokens are building blocks of text. But
they can be interchangeably used with “word” in our context for now).
Most of the times, we are dealing with something called Autoregressive
language models. The easiest way to think about it is that, it is a
black box to which, if you give a sequence of tokens (or words), it
spits out the next token (or word). Now you add this generated token to
the previous sequence of tokens and send it to the black box again. It
will spit the next one and so on. You get the idea!</p>
<p>LLMs have been possible, thanks to the concept of attention <a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> and a specific architecture of
neural networks called the transformers. For the interested readers, I
would divert your attention to the wonderful talk by Grant Sanderson <a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> on the same topic</p>
<p>The important thing to remember is that the output token of the LLM
(black box) is not deterministic. Rather, it is a probability
distribution over all the available tokens in the vocabulary. And this
might seem like a really large set. But in reality, most of the
probability is distributed over only a few tokens, while the rest have
close to negligible probability of appearing as the next token.</p>
<p>For the rest of the post, let us use ChatGPT as an example to
understand the concepts. When I type in the following text and look for
the next 3 possible tokens</p>
<blockquote>
<p>The protagonist went around the corner and …</p>
</blockquote>
<p><img src="https://nikkin.dev/blog/assets/2025-llm-entropy/image-20250108215806253.png"/></p>
<p>We can already see that the model finds certain tokens/words highly
probable than the rest.</p>
<p>When a probability distribution enters the picture, there are lots of
different ways to think about reasoning with it. Here, I wanted to
explore it from the perspective of entropy.</p>
<p>Entropy is a tricky concept and it can mean different things in
different contexts. Quanta Magazine has a nice post on what entropy
means <a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>. I would particularly recommend
playing around with the animation by Jonas Parnow &amp; Mark Belan in
that post to get an intuitive understanding of what the measure of
disorder actually means. For the sake of this post, we will stick to an
information theoretic understanding and borrow Shannon’s definition of
entropy for the probability distribution. It is defined as</p>
<p><span>\[\text{Shannon&#39;s
entropy}=-\sum\limits_xp(x)\times\log{p(x)}\]</span></p>
<p>where <span>\(x\)</span> are the available tokens
and <span>\(p(x)\)</span> refers to the probability
of that token appearing next. Now, what does the value of entropy say
about the underlying probability distribution. If all the states/tokens
are equally probable, the entropy would be the high. If the probability
is concentrated on a very few tokens, the entropy would be low. In other
words, a higher entropy can be interpreted as the model not being sure
of the next token relative to the case where the entropy is low, meaning
that the model is more confident of what token should come next</p>
<p>Let’s put this into practice. Let’s ask ChatGPT the following</p>
<blockquote>
<p>write a haiku about ai …</p>
</blockquote>
<p>Now, for each token in the output sequence, we obtain a probability
distribution for which the entropy can be calculated.</p>
<p>Note: The ChatGPT’s API only allows to look at the probability values
of the top 20 tokens. This could mean that we might be missing out on
non-zero probable next tokens beyond this range. And the definition of
entropy over a probability distribution assumes that it is normalized
(meaning all the different probability values for a single distribution
sums to 1) So, we normalize the probability values for the top 20 tokens
before calculating the entropy. Now, looking at the result…</p>
<p><img src="https://nikkin.dev/blog/assets/2025-llm-entropy/image-20250108221527524.png"/></p>
<p>This is interesting! The entropy is low (meaning the model is
relatively sure of the next token) for the cases where the sentence ends
or if the next token is the broken part of a single word (Wis-dom
above).</p>
<p>What if we did it for a longer text. Asking ChatGPT to</p>
<blockquote>
<p>Write me an essay about Claude Shannon …</p>
</blockquote>
<p><img src="https://nikkin.dev/blog/assets/2025-llm-entropy/image-20250108221759885.png"/></p>
<p>Too many tokens to visualize. Let’s just look at a histogram of the
entropy values</p>
<p><img src="https://nikkin.dev/blog/assets/2025-llm-entropy/image-20250108221856519.png"/></p>
<p>In most of the cases, the model seems to be pretty sure of the next
token. Since the plot above seems a bit erratic to make sense of it, I
tried taking the average over a moving window to smoothen the plot.
Below are different plots for different window sizes</p>
<p><img src="https://nikkin.dev/blog/assets/2025-llm-entropy/image-20250108222105604.png"/></p>
<p>The red lines that you see are the tokens which are or contain a full
stop (interpreted as the end of a sentence). The interpretation from the
first simple example still holds that the entropy drops down for the
cases where each sentence ends. I wanted to see what kind of tokens were
selected when there was high and low entropy respectively. Looking at
the top 10 tokens with the lowest entropies …</p>
<blockquote>
<p>194 | Institute | of | Michigan | th | 191 | Technology | | of |
,</p>
</blockquote>
<p>and comparing it with the top 10 tokens with the highest entropy
cases …</p>
<blockquote>
<p>led | wrote | considered | established | revolution | technology | ,
| is | regarded | conducted</p>
</blockquote>
<p>it can be crudely interpreted that factual statements (involving
proper nouns) come with low entropy, where the model has to be sure of
what comes next while descriptive statements have multiple options as
synonyms that could increase the pool of possible and probable next
tokens.</p>
<p>Next, I went and asked ChatGPT what kind of analysis can I build on
this and it told me to look at the clusters surrounding high entropy
tokens (words before and after)</p>
<blockquote>
<p>… and innovative thinking <u>led</u> to groundbreaking contributions
…</p>
<p>… time that Shannon <u>wrote</u> his master’s thesis …</p>
<p>… This work, <u>considered</u> one of the …</p>
<p>… paper, Shannon <u>established</u> the principles of …</p>
<p>… research that would <u>revolution</u> ize the field …</p>
<p>… have profoundly influenced <u>technology</u>, telecommunications,
…</p>
<p>… This work <u>,</u> considered one of …</p>
<p>… circuit design and <u>is regarded</u> as a cornerstone …</p>
<p>… where he <u>conducted</u> research that would …</p>
</blockquote>
<p>Yeah. I honestly didn’t know what to make of it. But was interesting
to give a voice to ChatGPT itself to contribute to this post.</p>
<p>I wanted to try a text completion in Tamil <a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> to
see how ChatGPT performs. I asked</p>
<blockquote>
<p>Finish the rest… எப்பொருள் யார்யார்வாய்க் கேட்பினும்</p>
</blockquote>
<p>And it gave me</p>
<blockquote>
<p>எப்பொருள் யார்யார்வாய்க் கேட்பினும் அப்பொருள்</p>
<p>மெய்ப்பொருள் காண்பது அறிவு.</p>
<p>This is a couplet from the ancient Tamil text “Thirukkural,” written
by the poet Thiruvalluvar. It emphasizes the importance of discerning
the truth regardless of who speaks it.</p>
</blockquote>
<p>And the probability distribution of tokens look like …</p>
<p><img src="https://nikkin.dev/blog/assets/2025-llm-entropy/image-20250108223723804.png"/></p>
<p>Here, you can clearly see that the tokens are individual letters.
While this is not ideal for autoregressive language models leading to
more compute time, it can actually be helpful in fine-grained model
performance evaluation.</p>
<p>Why did I do the things above? I found it interesting that the
OpenAI’s API for text generation using ChatGPT models gave back
information of the next token probabilities. I wanted to play around
with it from the point of view of entropy. While no substantial
conclusions can be made from this little experiment, it is of paramount
importance that we pay heed to the Thirukkural couplet above and discern
the truth regardless of who (or what) speaks it!</p>
<p>(You can find the code used to create all the plots <a href="https://github.com/nikkindev/scratchpad/blob/master/Entropy%20of%20a%20Large%20Language%20Model%20output.ipynb">here</a>)</p>

<!-- Footer for blog post -->



</div>
  </body>
</html>
