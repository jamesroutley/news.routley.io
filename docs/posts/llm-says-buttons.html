<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sygi.xyz/posts/2025-01-16-llm-says-buttons.html">Original</a>
    <h1>LLM says buttons</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
            <p>
    Posted on January 16, 2025
    
</p>

<p>The default interaction method used with contemporary Large Language Models (LLMs) is a text chat: user asks a question, LLM gives an answer.
Inspired by <a href="https://willwhitney.com/computing-inside-ai.html">Will Whitney’s post</a> and conversations with <a href="https://www.varungodbole.com/">Varun Godbole</a>, I propose an alternative
interface that could be implemented with current technology.</p>
<p>The commonly-used text interface is very expressive: the user can discuss topics from the latest car industry to finding the best place for a honeymoon.
It also feels natural: it resembles the way humans talk to each other and every user has years of experience in using this form of communication.</p>
<p>While humans communicate effectively this way, there are many cases when talking to a machine is done more efficiently, e.g.
- Drawing pictures in Photoshop
- Creating spreadsheets for financial data
- Writing code / this document: despite also using the text interface, the text editors have lots of useful features that makes editing drafts easier than prompting LLMs.</p>
<p>Two key UX challenges when using LLMs are:</p>
<ol type="1">
<li>Discoverability: getting the user to understand what features/options exist and what they can use the tool for,</li>
<li>Accessibility: allowing different users to adjust their experience based on their level of familiarity with a tool.</li>
</ol>
<h2 id="user-driven-research">User-driven research</h2>
<p>Researching a given idea involves scoping the problem right.
For example, when trying to meet with others in a restaurant, one needs to decide:</p>
<ol type="1">
<li>Who to meet with,</li>
<li>When,</li>
<li>What type of food,</li>
<li>What type of budget should be involved.</li>
</ol>
<p>When asked to help organize a restaurant visit, an LLM typically either makes assumptions about the user’s preferences (e.g., assuming they want lunch) or asks for clarification, leaving the decision to the user.</p>
<figure>
<a href="https://claude.ai/chat/047691a1-c977-403f-98ef-27efd5628219"><img src="https://sygi.xyz/images/llm-buttons/claude-chat.png" alt="A chat with an LLM trying to find a place to eat."/></a>
<figcaption>
A chat with an LLM trying to find a place to eat.
</figcaption>
</figure>
<p>One can visualize the research process as a decision tree with nodes as the questions and edges as the potential answers:</p>
<figure>
<figure>
<img src="https://sygi.xyz/images/llm-buttons/binary_background.png" alt="A decision tree showing the research process that can be"/>
<figcaption aria-hidden="true">A decision tree showing the research process that can be</figcaption>
</figure>
</figure>
<h2 id="interface">Interface</h2>
<p>Traversing this decision tree through lengthy responses interspersed with follow-up questions<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> is both inefficient and unnatural.</p>
<p>Instead, the LLM can analyze whether it has enough information to answer the question. Until it does, it can iterate:</p>
<ol type="1">
<li>asking itself about the next piece of information needed and the best widget to gather it (eg. a map for the location, slider for the price, button for cuisine type)</li>
<li>show the widget to the user to get the answer</li>
</ol><p>
Together with <a href="https://github.com/sherrybai">Sherry Bai</a> and <a href="https://github.com/l-armstrong">Lamone Armstrong</a>,
we prototyped a verion of this interface, where we only allow the model to display buttons:
</p><figure>
<figure>
<img src="https://sygi.xyz/images/llm-buttons/prototype.png" alt="Prototype of the idea implemented in an afternoon in Recurse Center."/>
<figcaption aria-hidden="true">Prototype of the idea implemented in an afternoon in <a href="https://www.recurse.com/scout/click?t=2b09e4dea03e120cda347cd253158d00">Recurse Center</a>.</figcaption>
</figure>
</figure>
<h2 id="benefits">Benefits</h2>
<p>The balance between making assumptions and asking users questions is a difficult one to strike.
On one hand, we want the LLM to provide as useful and accurate information as it can; on the other, repeatedly asking the user to clarify the request leads to fatigue and discourages users.</p>
<p>Making it easier for users to express preferences shifts the balance away from guessing, as pressing buttons three times is less cognitively demanding than reading multiple responses and composing text replies.</p>
<p>Furthermore, expressing the first question/prompt in natural language is not, paradoxically, the most natural way of starting a conversation for humans<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>.
Users’ behavior with Google’s search engine demonstrates this remark: even though it can handle natural language well these days, most of
us<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> only put keywords, like <code>python type self</code>, as they express the intention clearly enough in most of the cases.</p>
<p>A structured conversation makes it easier for users to backtrack on their answers and investigate a different continuation: now, if the user wants to provide a different answer to some of the clarifying questions,
she only needs to press a different button.</p>
<h3 id="storing-memories">Storing memories</h3>
<p>Driving the conversations in a more structured way will likely lead to the very same widget being displayed multiple times to the same user
over the course of their lifes: we go to get food many times over and decide how to do so every time.</p>
<p>Being able to retrieve the history of user’s answers to the particular question allows the LLM to skip asking it altogether (like it does now),
while taking the user’s preferences into account.</p>
<p>Even better, one can envision an entropy-based model, where the uncertainty of the user’s answer is modeled and the question is
re-asked in intervals that are propotional to the confidence level of the model, letting user to change their preferences every
now and then.</p>
<h2 id="concerns">Concerns</h2>
<p>This idea faces two challenges:</p>
<p>First, “LLM says buttons” interface requires multiple model calls, what is more expensive than the traditional chat interaction. The cost of sampling is proportional to the number of input and output tokens<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>.
Given that the clarifying discussion happens at the beginning of the conversation, the extra input to handle would be relatively small: apart from the KV-cached user prompt, it would be just a couple of sentences to the LLM asking for clarification question and widget.</p>
<p>Second, button interface increase latency in two ways:</p>
<ol type="1">
<li>Users must complete all widget interactions before receiving results</li>
<li>Each generation of a button requires a new model inference.</li>
</ol>
<p>While the first one is hard to eliminate, the second one could be handled by having a smaller (and thus: faster) network, potentially served closer to the user (as it’s cheaper to run), whose only goal would be generation of the UI elements.</p>
<h2 id="outro">Outro</h2>
<p>While text-based chat remains the foundation of LLM interfaces, introducing structured elements like buttons could bridge the gap between natural conversation and efficient tool usage. This hybrid approach might just be the key to making AI assistants both more powerful and more accessible to everyone.</p>

 



            
        </div></div>
  </body>
</html>
