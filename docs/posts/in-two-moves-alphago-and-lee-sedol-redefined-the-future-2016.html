<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/">Original</a>
    <h1>In Two Moves, AlphaGo and Lee Sedol Redefined the Future (2016)</h1>
    
    <div id="readability-page-1" class="page"><div><div data-journey-hook="client-content" data-testid="BodyWrapper"><div><p><span>SEOUL, SOUTH KOREA</span> --- In Game Two, the Google machine made a move that no human ever would. And it was beautiful. As the world looked on, the move so perfectly demonstrated the enormously powerful and rather mysterious talents of modern artificial intelligence.</p><p>But in Game Four, the human made a move that no machine would ever expect. And it was beautiful too. Indeed, it was just as beautiful as the move from the Google machine---no less and no more. It showed that although machines are now capable of moments of genius, humans have hardly lost the ability to generate their own transcendent moments. And it seems that in the years to come, as we humans work with these machines, our genius will only grow in tandem with our creations.</p><p>This week saw <a href="https://www.wired.com/2016/03/googles-ai-wins-fifth-final-game-go-genius-lee-sedol/">the end of the historic match</a> between Lee Sedol, one of the world&#39;s best Go players, and AlphaGo, an artificially intelligent system designed by a team of researchers at DeepMind, a London AI lab now owned by Google. The machine claimed victory in the best-of-five series, winning four games and losing only one. It marked <a href="https://www.wired.com/2016/03/googles-ai-taking-one-worlds-top-go-players/">the first time</a> a machine had beaten the very best at this ancient and enormously complex game---a feat that, until recently, <a href="https://www.wired.com/2014/05/the-world-of-computer-go/">experts didn&#39;t expect would happen for another ten years</a>.</p><p>The victory is notable because the technologies at the heart of AlphaGo are the future. They&#39;re already changing Google and Facebook and Microsoft and Twitter, and <a href="https://www.wired.com/2016/03/googles-ai-taking-one-worlds-top-go-players/">they&#39;re poised to reinvent everything from robotics to scientific research</a>. This is scary for some. The worry is that artificially intelligent machines will take our jobs and maybe even break free from our control---and on some level, those worries are healthy. We won&#39;t be caught by surprise.</p><p>But there&#39;s another way to think about all this---a way that gets us beyond the trope of human versus machine, guided by the lessons of those two glorious moves.</p><p>Move 37</p><p>With the 37th move in the match&#39;s <a href="https://www.wired.com/2016/03/googles-ai-wins-pivotal-game-two-match-go-grandmaster/j">second game</a>, AlphaGo landed a surprise on the right-hand side of the 19-by-19 board that flummoxed even the world&#39;s best Go players, including Lee Sedol. &#34;That&#39;s a very strange move,&#34; said one commentator, himself a nine dan Go player, the highest rank there is. &#34;I thought it was a mistake,&#34; said the other. Lee Sedol, after leaving the match room, took nearly fifteen minutes to formulate a response. Fan Gui---the three-time European Go champion who played AlphaGo during a closed-door match in October, <a href="https://www.wired.com/2016/01/in-a-huge-breakthrough-googles-ai-beats-a-top-player-at-the-game-of-go/">losing five games to none</a>---reacted with incredulity. But then, drawing on his experience with AlphaGo---he has played the machine time and again in the five months since October---Fan Hui <a href="https://www.wired.com/2016/03/sadness-beauty-watching-googles-ai-play-go/">saw the beauty in this rather unusual move</a>.</p><p>Indeed, the move turned the course of the game. AlphaGo went on to win Game Two, and at the post-game press conference, Lee Sedol was in shock. &#34;Yesterday, I was surprised,&#34; he said through an interpreter, referring to his loss in <a href="https://www.wired.com/2016/03/googles-ai-wins-first-game-historic-match-go-champion/">Game One</a>. &#34;But today I am speechless. If you look at the way the game was played, I admit, it was a very clear loss on my part. From the very beginning of the game, there was not a moment in time when I felt that I was leading.&#34;</p><p>It was a heartbreaking moment. But at the same time, those of us who watched the match inside Seoul&#39;s Four Seasons hotel could feel the beauty of that one move, especially after talking to the infectiously philosophical Fan Hui. &#34;So beautiful,&#34; he kept saying. &#34;So beautiful.&#34; Then, the following morning, David Silver, the lead researcher on the AlphaGo project, <a href="https://www.wired.com/2016/03/googles-ai-viewed-move-no-human-understand/">told me</a> how <em>the machine</em> had viewed the move. And that was beautiful too.</p><p>One in Ten Thousand</p><p>Originally, Silver and his team taught AlphaGo to play the ancient game using a deep neural network---a network of hardware and software that mimics the web of neurons in the human brain. This technology already underpins online services inside places like Google and Facebook and Twitter, helping to identify faces in photos, recognize commands spoken into smartphones, drive search engines, and more. If you feed enough photos of a lobster into a neural network, it can learn to recognize a lobster. If you feed it enough human dialogue, it can learn to <a href="https://www.wired.com/2015/06/google-made-chatbot-debates-meaning-life/">carry on a halfway decent conversation</a>. And if you feed it 30 million moves from expert players, it can learn to play Go.</p><p>But then the team went further. Using a second AI technology called reinforcement learning, they set up countless matches in which (slightly) different versions of AlphaGo played each other. And as AlphaGo played itself, the system tracked which moves brought the most territory on the board. &#34;AlphaGo learned to discover new strategies for itself, by playing millions of games between its neural networks, against themselves, and gradually improving,&#34; Silver said when Google unveiled AlphaGo early this year.</p></div></div></div></div>
  </body>
</html>
