<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.stylewarning.com/posts/brute-force-rubiks-cube/">Original</a>
    <h1>Can a Rubik&#39;s Cube be brute-forced?</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
<article>
<p><em>By Robert Smith</em></p>

<h2 id="introduction">Introduction</h2>
<p>When I was about 13, while still a middle-schooler, I became
fascinated with the Rubik’s Cube<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. I never got terribly good
at solving it, maybe eventually getting into the 30 to 40 seconds
range. While I didn’t have a penchant for memorizing move sequences, I
was drawn into how we <em>find</em> these move sequences.</p>
<p>The story about my interest and exploration in the Rubik’s Cube is for
another post. Long story short, I got interested in “computer
puzzling”—using computers to manipulate combinatorial puzzles, like
the Rubik’s Cube, either to solve them quickly, to discover patterns,
or to find novel move sequences for use in speedcubing—and ever
since, I’ve been working on different programs for solving Rubik-like
puzzles.</p>
<p>Purely in principle, it shouldn’t be hard to solve a Rubik’s Cube with
a computer, right? Our program would have three parts:</p>
<ol>
<li>A model of the Rubik’s Cube, that is, some data structure that
represents a cube state.</li>
<li>Some functions which can simulate turns of each side.</li>
<li>A solving procedure which takes a scrambled cube, tries every
possible turn sequence, and stops when solved.</li>
</ol>
<p>Truth be known, and details aside, this is a provably correct method
for solving a Rubik’s Cube. If you leave your computer on long enough,
it will return a solution.</p>
<p>The problem is that it takes a long time. Probably longer than your
lifetime.</p>
<h2 id="computer-puzzling-without-brute-force">Computer puzzling without brute-force</h2>
<p>“Brute-force” generally means to try every possibility of something
without much of any strategy. Our method above is a brute-force
algorithm. Brute-force algorithms generally aren’t practical, because
if you have $N$ of something to explore, a brute-force algorithm will
take $O(N)$ time. For a Rubik’s Cube, $N$ is 43 quintillion—a very
large number.</p>
<p>It has been known, practically since the Rubik’s Cube’s inception,
that something else is needed to solve a Rubik’s Cube. Rubik’s Cube
solutions, obviously, take into account the specific structure and
properties of the cube so as to implicitly or explicitly avoid
mindless search. These methods have turned out to be:</p>
<ol>
<li>Solving methods for humans: memorize some sequences which let you
move only a few pieces around in isolation, and apply these
sequences mechanically until all pieces are in place. The more
sequences you memorize, the faster you’ll be.</li>
<li>Heuristic tree search: do a tree search (with e.g.,
iterative-deepening depth-first search<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>), but aggressively
prune off branches by way of clever heuristics<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</li>
<li>Phase-based solvers: a deeply mathematical way which involves
characterizing the Rubik’s Cube as a sequence of nested
(mathematical) subgroups so that each successive coset space small
enough that it can be solved by computer.</li>
</ol>
<p>Computer puzzling mostly deals with the latter two approaches, usually
in some combination. Both approaches lead to extraordinarily
high-performing solvers. For example:</p>
<ul>
<li>Korf’s algorithm (approach #2) finds optimal solutions—solutions
of shortest length—but can take hours to find one.</li>
<li>Thistlethwaite’s algorithm (approach #3) solves a cube in four
phases almost instantaneously. The solutions are guaranteed to be no
longer than triple the optimal length.</li>
</ul>
<p>The story may as well end here. We have slow but optimal ways of
solving the Rubik’s Cube, and fast but sub-optimal ways. Pick your
poison (sub-optimal or slow), depending on what you’re trying to
achieve.</p>
<h2 id="taking-a-step-back-puzzles-as-permutations">Taking a step back: puzzles as permutations</h2>
<p>It seems that any Rubik’s Cube solver <em>has</em> to know <em>something</em> about
the structure of the cube. It might be worth asking how little
structure we can get away with, so as to make whatever solving
algorithm we write generic over a broad class of puzzles.</p>
<p>For a brute-force algorithm with tree search, we would need something
like the following:</p>
<pre tabindex="0"><code>interface GenericPuzzle:
  type State
  type Move

  function isSolved(State) -&gt; Boolean
  function allMoves() -&gt; List(Move)
  function performMove(State, Move) -&gt; State
</code></pre><p>With this, we could write the following solver based off of
iterative-deepening depth-first search, which is totally generic on
the above interface.</p>
<pre tabindex="0"><code>function solve(State) -&gt; List(Move)
function solve(p):
  if isSolved(p):
    return []

  for maxDepth from 1 to infinity:
    solved?, solution = dfs(0, maxDepth, p)
    
    if solved?:
      return solution

function dfs(Integer, Integer, State, List(Move)) -&gt; (Boolean, List(Move))
function dfs(depth, maxDepth, p, s):
  if isSolved(p):
    return (True, s)

  if depth == maxDepth:
    return (False, [])

  for m in allMoves():
    p&#39; = performMove(p, m)
    (solved?, solution) = dfs(depth+1, maxDepth, p&#39;, append(s, [m])

    if solved?:
      return (solved?, solution)
</code></pre><p>As discussed before, while this strategy is effective for problems
with small search spaces, it’s no help when the space is
large. Unfortunately, the <code>GenericPuzzle</code> interface doesn’t give us
much room for improvement. Can we still remain generic, while giving
us at least a little more room for exploring other algorithms?</p>
<p>The answer is yes, if we restrict ourselves to <em>permutation
puzzles</em>. Roughly speaking, a permutation puzzle is one where pieces
shift around according to a fixed and always available set of shifting
moves. The Rubik’s Cube is a phenomenal and non-trivial example: We
can label each mobile<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> sticker with a number 1 to 48, and
these stickers can always be shifted around with a twist of any of the
six sides. Since we can twist any of the six sides at any time, the
puzzle is a permutation puzzle. (Not all similar puzzles are
permutation puzzles. There are some puzzles which are “bandaged”, that
is, pieces of the puzzle are fused together, restricting some
available moves depending on the configuration.)</p>
<p>In this view, we looked at a solved configuration as a list of
numbers. For example, the solved Rubik’s Cube as a permutation would
be</p>
<p>$$
(1, 2, \ldots, 47, 48).
$$</p>
<p>When we turn a side, these numbers get permuted. For instance,
assuming a particular labeling of stickers with numbers, turning the
top face of a Rubik’s Cube might permute the first sticker in the list
to the third, the second sticker to the fifth, the third sticker to
the eighth, etc. We can use the same notation</p>
<p>$$
(3, 5, 8, 2, 7, 1, \ldots)
$$</p>
<p>This notation has two interpretations:</p>
<ol>
<li>The literal position of numbered stickers on a physical cube (with
an agreed upon labeling).</li>
<li>An instruction for how to relabel the stickers of a given cube.</li>
</ol>
<p>If we look at the notation under the second interpretation, a
permutation actually represents a <em>function</em> that’s applied to
<em>individual stickers</em>. For instance, if</p>
<p>$$
F := (3, 5, 8, 2, 7, 1, \ldots)
$$</p>
<p>then $F(1) = 3$, $F(2) = 5$, etc. All of the clockwise face
turns—Front, Right, Up, Back, Left, Down—of a Rubik’s Cube can be
described like so:</p>
<p>$$
\begin{align*}
F &amp;:= (1, 2, 3, 4, 5, 25, \ldots)\\
R &amp;:= (1, 2, 38, 4, 36, 6, \ldots)\\
U &amp;:= (3, 5, 8, 2, 7, 1, \ldots)\\
B &amp;:= (14, 12, 9, 4, 5, 6, \ldots)\\
L &amp;:= (17, 2, 3, 20, 5, 22, \ldots)\\
D &amp;:= (1, 2, 3, 4, 5, 6, \ldots, 48, 42, 47, 41, 44, 46)
\end{align*}
$$</p>
<p>We wrote some of the last elements of $D$ because a “down” move doesn’t
change the first six stickers in this labeling scheme.</p>
<p>This gives is a whole new interpretation of what it means to “solve” a
cube. Given a scrambled cube, we first write down the permutation that
describes how the stickers moved from a solved state to the scrambled
state. Let’s call it $s$. This is easy, because we can just read the
labeled stickers off of a cube one-by-one, in order. For example, $s$
might be:</p>
<p>$$
s := (27, 42, 30, 15, 39, 6, \ldots).
$$</p>
<p><em>This is a description of a function!</em> The value of $s(1)$ describes
how the first sticker of a cube will be shifted to its scrambled
position, in this case $27$. Next, solving a cube is finding a
sequence of $k$ moves $m_1, m_2, \ldots, m_k$ such that, for all $1\leq
i\leq 48$,</p>
<p>$$
i = m_k(m_{k-1}(\cdots(m_2(m_1(s(i)))))).
$$</p>
<p>Stated another way in function composition notation, the function</p>
<p>$$
m_k \circ m_{k-1} \circ \cdots \circ m_2 \circ m_1\circ s
$$</p>
<p>must be the identity function—a permutation that doesn’t move
anything.</p>
<p>In the permutation puzzle way of thinking, we can still implement our
<code>GenericPuzzle</code> interface:</p>
<ul>
<li><code>State</code> would be a permutation;</li>
<li><code>Move</code> would also be a permutation;</li>
<li><code>isSolved</code> would check if a permutation is $(1, 2, 3, \ldots)$;</li>
<li><code>allMoves</code> would be a hard-coded list of the possible moves, like
$F$, $R$, $U$, $B$, $L$, and $D$ for the Rubik’s cube; and</li>
<li><code>performMove</code> would take the input move permutation, and apply it as
a function to each element of the state permutation.</li>
</ul>
<p>This might even be <em>more</em> efficient than another choice of
representation, since permutations can be represented very efficiently
on a computer as packed arrays of bytes!</p>
<p>But we didn’t do all this mathematical groundwork just to goof around;
there’s something amazing lurking in these permutations.</p>
<h2 id="brute-force-still-ignorant-but-kinda-smart">Brute-force, still ignorant, but kinda smart?</h2>
<p>In the late 1980s, Adi Shamir<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> and his students made a
brilliant series of observations that came together to make for a
beautiful result. Unfortunately, to my knowledge, only two writings
exist on the topic.</p>
<ol>
<li>
<p>Shamir and his colleagues wrote a paper about it [1], sort of in
the style of a brief conference proceeding, but it’s very light on
details and skips implementation considerations. It’s the kind of
paper where you follow it, but you have to fill in a great number
of blanks to make anything from it work.</p>
</li>
<li>
<p>Shamir gave a talk sometime in the 80’s about his result, and
somebody (none other than Alan Bawden) wrote a brief email [2] to a
mailing list about his recollection of it.</p>
</li>
</ol>
<p>An amazing result, buried in history, without any good exposition that
I could find.</p>
<p>What’s the result? The essence of the result is this. Reminiscent of a
“meet in the middle” algorithm, if we want to brute-force a problem
that ordinarily requires visiting $N$ states to find an answer, we can
instead cleverly split the work into two searches that requires visits
to around $\sqrt{N}$ states. For a Rubik’s Cube, that cuts work
associated with 43 quintillion states, down to work associated with 6
billion states. The best part is, this is <em>still brute-force</em>;
virtually no knowledge of the structure of the problem is required to
make it work.</p>
<p>Let’s walk through the requisite steps and build up to the
result. I’ll attempt to write in a general framework (since it’s a
general algorithm), but make frequent appeals to the Rubik’s Cube
specifically.</p>
<h3 id="observation-1-decomposition-as-intersection">Observation #1: decomposition as intersection</h3>
<p>Suppose the following:</p>
<ul>
<li>We have a mysterious permutation $s$, say, a scrambled puzzle;</li>
<li>We have two sets of permutations $X$ and $Y$; and</li>
<li>We assume there’s an $\hat x\in X$ and $\hat y\in Y$ such that $s =
\hat y\circ \hat x$.</li>
</ul>
<p>The goal is to find precisely $\hat x$ and $\hat y$ are. The simplest
way to do this is to check every combination of elements in $X$ and
$Y$.</p>
<pre tabindex="0"><code>for x in X:
  for y in Y:
    when s = compose(y, x):
      return (x, y)
</code></pre><p>This will take time proportional to the product of the set sizes:
$O(\vert X\vert\cdot\vert Y\vert)$. Shamir noticed the following: If
$s=\hat y\circ\hat x$, then $\hat y^{-1}\circ s = \hat x$. With this, we
preprocess our $Y$ set to be instead</p>
<p>$$
Y&#39; := \{y^{-1}\circ s : y\in Y\}.
$$</p>
<p>By doing this, there must be an element in common between $X$ and
$Y&#39;$, since $\hat x\in X$ and $\hat y^{-1}\circ s\in Y&#39;$ and those are
equal. So we’ve reduced the problem to determining what the
intersection between $X$ and $Y&#39;$ is.</p>
<p>Once we find our $z$ which is in common with $X$ and $Y&#39;$, then our
recovered permutation will be $\hat x = z$ and $\hat y = (z\circ
s^{-1})^{-1}$.</p>
<p>We’ve just established that the problem of decomposing an element like
$s$ is identical to the problem of calculating a set
intersection. Still, if we want to do the intersection, our intuition
tells us we still need a quadratic algorithm, which brings us to the
second observation.</p>
<h3 id="observation-2-sorting-really-helps">Observation #2: sorting really helps!</h3>
<p>Permutations have a natural ordering, called <em>lexicographic
ordering</em>. If you have two permutations, and you read their elements
left-to-right, you can compare them like ordinary numbers. Just
as $123 &lt; 213$, we can say that</p>
<p>$$
(1,2,3) &lt; (2,1,3).
$$</p>
<p>A nice property of this is that the identity permutation $(1, 2, 3,
\ldots)$ is the smallest permutation of a given size.</p>
<p>How does this help us? Well, suppose we sort our sets $X$ and $Y&#39;$
into lists $L_X$ and $L_{Y&#39;}$, so the permutations are in order. If
$L_X$ and $L_{Y&#39;}$ have an element in common, we can find it in linear
time: $O(\min\{\vert X\vert, \vert Y&#39;\vert\})$. How? Something like
the following:</p>
<pre tabindex="0"><code>function findCommon(Lx, Ly):
  x = pop(Lx)
  y = pop(Ly)
  loop:
    if x == y:
      return x
    
    if empty(Lx) or empty(Ly):
      error(&#34;No common elements found.&#34;)

    if x &lt; y:
      x = pop(Lx)
    else if x &gt; y:
      y = pop(Ly)
</code></pre><p>This works because we are essentially looking at all of the elements
of $L_X$ and $L_{Y&#39;}$ together in sorted order. It’s like a merge
sort, without the merge part.</p>
<p>Before continuing, we should take a little scenic tour on a more
formal meaning of “moves” and “move sequences”, since ultimately any
permutation puzzle solving algorithm must produce them as output.</p>
<h3 id="what-is-a-move">What is a move?</h3>
<p>A quick bit about notation. If we have a permutation $f$, then its
inverse is written $f^{-1}$, and it’s $k$-fold repetition $f\circ
f\circ\cdots\circ f$ is written $f^k$. If we have a collection of
permutations $S := \{f_1, f_2, \ldots\}$, then we write the
following shorthands:</p>
<p>$$
\begin{align*}
S^{-1} &amp;:= \{f^{-1} : f \in S\}\\
S^{\times k} &amp;:= \{f^k : f \in S\}.
\end{align*}
$$</p>
<p>If $g$ is some permutation, we also write these shorthands:</p>
<p>$$
\begin{align*}
g\circ S &amp;:= \{g\circ f : f \in S\}\\
S\circ g &amp;:= \{f\circ g : f \in S\}.
\end{align*}
$$</p>
<p>Similarly, if $T := \{g_1, g_2, \ldots\}$, then we can write</p>
<p>$$
\begin{align*}
S\circ T &amp;:= \{f\circ g : f\in S, g\in T\}\\
&amp;= \{f_1\circ g_1, f_2\circ g_1, \ldots, f_1\circ g_2, \ldots\}.
\end{align*}
$$</p>
<p>With that out of the way, let’s talk about the concept of a single
“move”. What counts as a “move” in a permutation puzzle?</p>
<p>Really, we can choose any set of moves we please, so long as every
state of the puzzle is reachable through some combination of the
moves. For example, let</p>
<p>$$
C := \{F, R, U, B, L, D\},
$$</p>
<p>the basic and well understood ninety-degree clockwise moves of the
Rubik’s Cube. Indeed, $C$ itself is a fine definition of available
moves. All of the following are also valid definitions of moves:</p>
<p>$$
C\cup C^{-1},\quad C\cup C^{\times 2},\quad C^{-1},\quad C\cup C^{\times 2}\cup C^{-1},
$$</p>
<p>and so on. Perhaps surprisingly, we can take any element of $C$ and
remove it, and it would still be a valid set of moves for the Rubik’s
Cube<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>!</p>
<p>Which set of moves we select usually has little relevance
mathematically (they are all expressible as one another), but has
great relevance when we are synthesizing efficient move sequences, or when
we want to talk about “optimality”. For instance, consider a
counterclockwise move: $F^{-1}$. It’s natural to consider this a
single move, but if we consider our set to be $C$, then we’d have to
count it as three moves, since $F^{-1} = F\circ F\circ F = F^3$. What
about $F^2$? Is that one move or two? Speedcubers generally consider
$F^2$ to be one motion, so counting that as one move is natural, but
many computer puzzlers like the simplicity of $C\cup C^{-1}$, i.e.,
only ninety-degree turns<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>.</p>
<p>For the rest of this note, we’ll be in the former camp, where half-turns count as one, and we’ll denote this set of moves as:</p>
<p>$$
\bar C := C \cup C^{-1} \cup C^{\times 2}.
$$</p>
<h3 id="what-is-a-word">What is a word?</h3>
<p>After we agree on what we consider a move, we can be more specific as
to what we mean about move sequences. A <em>move sequence</em> is a possibly
empty list of moves. A move sequence can be <em>composed</em> to form the
permutation it represents. This composition operator is called
$\kappa$, and is easily defined. Let $M$ be a move set, and let $s =
[s_1, s_2, \ldots, s_n]$ be a sequence of $n$ moves with each
$s_{\bullet}$ a move from $M$. The <em>length</em> is $s$ is naturally $n$,
and its composition is defined as:</p>
<p>$$
\begin{align*}
\kappa([\,]) &amp;:= (1, 2, 3, \ldots)\\
\kappa([s_1, s_2, \ldots, s_{n-1}, s_n]) &amp;:= \kappa([s_1, s_2, \ldots, s_{n-1}])\circ s_n.
\end{align*}
$$</p>
<p>If $M$ is a move set, then the set of all move sequences (including
the empty sequence) is denoted $M^{*}$, a notation kindly borrowed
from formal language theory.</p>
<p>If we identify the elements of $M$ with symbols, then a move sequence
is called a <em>word</em>. We’ll always type symbols in $\texttt{typewriter}$
font. The moves $\{F, R, U, B, L, D\}$ have the symbols
$\{\texttt{F}, \texttt{R}, \texttt{U}, \texttt{B}, \texttt{L},
\texttt{D}\}$, an inverse $F^{-1}$ has the symbol $\texttt{F&#39;}$, and
a square $F^2$ has the symbol $\texttt{F2}$. And we type words as
symbols joined together in <em>reverse</em> order<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>, so $[R^{-1},
U^2, L]$ can be represented by the word $\texttt{L U2 R&#39;}$.</p>
<p>The distinction is subtle but important. In a computer program, a move
sequence is a list of permutations, while a word is a list of
symbols. A Rubik’s Cube solving program should take as input a
permutation, and output a word which when composed as a move sequence,
brings that permutation to identity.</p>
<p>When doing math, we often mix up all of these concepts since they have
little bearing on the correctness of an argument. Whether it’s the
permutation $F\circ R^{-1}$ or the move sequence $[F, R^{-1}]$ or the
word $\texttt{R&#39; F}$ or otherwise, they all represent roughly
the same thing, but computers need to be explicit about which
representation is being manipulated.</p>
<p>So, in summary:</p>
<ul>
<li>A <strong>move set</strong> is a set of permutations that “count” as one move.</li>
<li>A <strong>move sequence</strong> is a list of moves from a move set.</li>
<li>The <strong>composition</strong> of a move sequence is the permutation that move sequence represents.</li>
<li>A <strong>symbol</strong> is a designator for a move in a move set.</li>
<li>A <strong>word</strong> is a sequence of symbols.</li>
</ul>
<p>Back to this brute-force thing…</p>
<h3 id="observation-3-sorting-as-solving">Observation #3: sorting as solving</h3>
<p>As silly as the example is, let’s suppose we know, for a fact, that a
Rubik’s Cube was mixed up using only six moves from $\bar C$. Since
$\bar C$ has 18 elements, without any optimization, we might have to
try $18^6$ move sequences to find a solution.</p>
<p>Instead of brute-forcing in that way, we can do another trick. Let <code>s</code>
be our scrambled permutation.</p>
<ol>
<li>
<p>Write out every combination of 3 moves into a table. The key would
be the permutation, and the value would be the word associated with
that permutation. Call this table <code>A</code>.</p>
</li>
<li>
<p>Sort <code>A</code> in ascending lexicographic order on the permutation.</p>
</li>
<li>
<p>Make a copy of <code>A</code>, call it <code>B</code>. For all <code>(perm, word)</code> in <code>B</code>,
reassign <code>perm := compose(invert(perm), s)</code>. We do this because of
Observation #1.</p>
</li>
<li>
<p>Sort <code>B</code>.</p>
</li>
<li>
<p>Call <code>x := findCommon(A, B)</code>. We do this via Observation #2.</p>
</li>
<li>
<p>Reconstruct a word equal to <code>s</code> by <code>A[x].word ++ reverse(B[x].word)</code>. We do this to recover a final result via Observation
#1.</p>
</li>
</ol>
<p>Since we have a word that brings us <em>from solved to <code>s</code></em>, we can
invert the word to bring us <em>from <code>s</code> to solved</em>.</p>
<p>By this method, we avoided visiting all $16^6$ move sequences by
instead pre-calculating two groups of $16^3$ sequences and exploring
them for an intersection. We have cut the amount of work down to its
square root.</p>
<p>If we generalize to length $n+m$ (for some splitting of $n$ and $m$),
then we can replace the work of visiting $16^{n+m}$ states with
$16^m + 16^n$ states, which is much better.</p>
<p>So we’re done? We now know that the Rubik’s Cube requires no more than
20 moves, so if we make two tables enumerating 10 moves, we should be
good?</p>
<p>Well, err, $16^{10} = 1,099,511,627,776$. Unless we have trillions of
resources to space, be it time or space, it’s still not going to work.</p>
<h3 id="more-splitting">More splitting?</h3>
<p>An enterprising computer science student, at this point, might smell
recursion. If we split once, can we split again? If we know a Rubik’s
Cube can be solved in 20 moves, can we split it into two 10 move
problems, and each of those into two 5 move problems?</p>
<p>The problem with this is that at the top layer of recursion, it’s
clear what we are solving. At lower layers, it’s no longer clear. What
<em>actually</em> is the recursive structure at play? And if we could do this
trick, couldn’t we decimate any brute-force problem of exponential
complexity (e.g., in number of moves) into one of linear?</p>
<p>That isn’t going to work, but we can be inspired by it. Let $L := \bar
C^5$ be the set of 5-move combinations from $\bar C$. The size of $L$
is going to be $621,649$ if we don’t store redundant
permutations. This is definitely possible to compute. Then our goal is
to find a decomposition of $s$ in terms of an element in $L\circ
L\circ L\circ L$. Using the same trick from Observation #1, suppose
there is a decomposition $$s = l_4\circ l_3\circ l_2\circ l_1.$$ Then
$$l_3^{-1}\circ l_4^{-1} \circ s = l_2\circ l_1.$$ So we create four
tables:</p>
<ul>
<li>$L_1 = L$,</li>
<li>$L_2 = L_1$,</li>
<li>$L_4 = L_1^{-1}$, and</li>
<li>$L_3 = L_4\circ s$.</li>
</ul>
<p>No, the $4$ before $3$ is not a typo! We put this in order to save on
computation and avoid redundant work. Now our goal is to find an
element in common between the two sets</p>
<p>$$
\begin{align*}
X &amp;= L_2 \circ L_1\\
Y &amp;= L_4 \circ L_3.
\end{align*}
$$</p>
<p>Somehow, we must do this without actually calculating all elements of
$L_i\circ L_j$. And, to add insult to injury, for <code>findCommon</code> to
work, we need to be able to go through the set in sorted order.</p>
<h3 id="iterating-through-products-with-schroeppel--shamir">Iterating through products with Schroeppel–Shamir</h3>
<p>Suppose we have two lists of positive numbers $A$ and $B$. How can we
print the elements of $\{a+b : a\in A, b\in B\}$ in numerical order
without explicitly constructing and sorting this set? Shamir and his
collaborator Schroeppel did so with the following algorithm.</p>
<ol>
<li>
<p>Sort $A$ in ascending order. Pop off the first (and therefore
smallest) element $a_1$.</p>
</li>
<li>
<p>Create a priority queue $Q$ and initialize it with $(a,b)$ with
priority $a_1 + b$ for all $b\in B$.</p>
</li>
<li>
<p>Repeat the following until $Q$ is empty:</p>
<ol>
<li>Pop $(a,b)$ off $Q$. This will form the next smallest sum, so print $a+b$.</li>
<li>Find $a&#39;$ which immediately succeeds $a$ in our sorted list $A$.</li>
<li>Push $(a&#39;,b)$ with priority $a+b$ onto $Q$.</li>
</ol>
</li>
</ol>
<p>This algorithm will terminate, having printed each sum successively
with at most $O(\vert A\vert + \vert B\vert)$ space and almost linear
time. (The sorting and priority queue maintenance require some
logarithmic factors.)</p>
<p>With a little work, one can see why this works. In a sense it’s a
two-dimensional sorting problem, that depends on one crucial fact: If
$x \le y$ then $x+z \le y+z$. (This is to say that addition is
<em>monotonic</em>.) Given how the priority queue is constructed, it will
<em>always</em> contain the smallest sum.</p>
<p>Could we do this with permutations? If we have two lists of
permutations $A$ and $B$, and $a_1$ is the “smallest” (i.e.,
lexicographically least) permutation of $A$, and $b_1$ is the
“smallest” permutation of $B$, then it is <strong>patently not true</strong> that
$a_1\circ b_1$ is the smallest element of $A\circ B$. In symbols,</p>
<p>$$
(\min A) \circ (\min B) \neq \min (A\circ B).
$$</p>
<p>Similarly, if two permutations satisfy $a &lt; b$, then it is <strong>patently
not true</strong> that</p>
<p>$$
a\circ z &lt; b\circ z
$$</p>
<p>for a permutation $z$.</p>
<p>The monotonicity of addition is what allows us to do steps 3.2 and 3.3
so easily. If we did the same with permutations, we would no longer
have the guarantee that the minimum composition exists within the
queue.</p>
<p>This was the next hurdle Shamir cleared. Constant in the size of $A$
or $B$, Shamir found a way to solve the following problem: Given a
permutation $a\in A$ and $b\in B$, find the element $b&#39;\in B$ such
that $a\circ b&#39;$ immediately succeeds $a\circ b$. In other words, we
can generate, one-by-one, a sequence of $b$’s needed for step 3.2 and
3.3. With this algorithm (which we’ll describe in the next section),
our Shamir–Schroeppel algorithm for permutations becomes the
following:</p>
<p><strong>Algorithm (Walk Products)</strong>:</p>
<ol>
<li>Initialize an empty priority queue $Q$ whose elements are pairs of
permutations with priority determined by another permutation in
lexicographic ordering.</li>
<li>For each permutation $b\in B$:
<ol>
<li>With Shamir’s trick, find the $a\in B$ such that $a\circ b = \min (A\circ b)$.</li>
<li>Push $(a, b)$ onto $Q$ with priority $a\circ b$.</li>
</ol>
</li>
</ol>
<ul>
<li>(Invariant: At this point, we will certainly have $\min (A\circ B)$ in the queue.)</li>
</ul>
<ol start="3">
<li>Repeat the following until $Q$ is empty:
<ol>
<li>Pop $(a,b)$ off $Q$. This will form the next smallest $a\circ b$, so print it<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>.</li>
<li>With Shamir’s trick, find $a&#39;$ such that $a&#39;\circ b$ immediately succeeds $a\circ b$.</li>
<li>Push $(a&#39;,b)$ with priority $a&#39;\circ b$ onto $Q$.</li>
</ol>
</li>
</ol>
<p>This algorithm will produce the elements of $A\circ B$, one-by-one in
lexicographic order.</p>
<p>What is Shamir’s trick? We need a data structure and a clever observation.</p>
<h3 id="permutation-tries">Permutation tries</h3>
<p>In order to handle sets of ordered permutations better, Shamir created
a data structure. I call it a permutation trie. A <em>permutation trie</em>
of size-$k$ permutations is a $k$-deep, $k$-ary tree, such that a
path from root-to-leaf follows the elements of a permutation. The leaf
contains data which we want to associate with the permutation.</p>
<p>For example, consider permutations of size $5$. Suppose we wanted to
associate the symbol $\texttt{p6}$ with the permutation
$(2,4,1,3,5)$. Then we would have a $5$-layer tree with a root node
$R$, such that $R[2][4][1][3][5] = \texttt{p6}$.</p>
<p>More generally, let’s associate the following symbols with the
following permutations in a permutation trie $R$:</p>
<p>$$
\begin{align*}
\texttt{p1} &amp;\leftarrow (1,2,3,4,5) &amp; \texttt{p2} &amp;\leftarrow (1,2,3,5,4) &amp; \texttt{p3} &amp;\leftarrow (1,2,4,3,5)\\
\texttt{p4} &amp;\leftarrow (1,2,5,3,4) &amp; \texttt{p5} &amp;\leftarrow (1,3,4,5,2) &amp; \texttt{p6} &amp;\leftarrow (2,4,1,3,5)\\
\texttt{p7} &amp;\leftarrow (4,1,3,2,5) &amp; \texttt{p8} &amp;\leftarrow (4,1,3,5,2) &amp; \texttt{p9} &amp;\leftarrow (5,1,2,3,4)\\
\end{align*}
$$</p>
<p>The trie would be a data structure that looks like this:</p>
<p><img src="https://www.stylewarning.com/posts/brute-force-rubiks-cube/images/perm-trie.svg" alt="An example permutatioen trie." decoding="async"/>
</p>
<p>Even though we don’t show them, conceptually, each node in the trie
has a full length-$5$ array, with some elements empty (i.e., there are
no children).</p>
<p>What’s good about this data structure? First and foremost, pre-order
traversal will visit the permutations in lexicographic order. We can
use this data structure to store two things at the leaves (i.e.,
$\texttt{p}n$):</p>
<ol>
<li>The actual permutation data structure representing that path, and</li>
<li>The word we used to construct that permutation.</li>
</ol>
<p>This is the data structure, and now we get to Shamir’s
insight. Suppose we have a permutation $s$ and a permutation trie $R$
(which represents a set of permutations), and we want to traverse
$s\circ R$ in lexicographic order. The naive way is to construct a new
trie, but we wish to avoid that. To explain the idea, we’ll choose a
concrete example.</p>
<p>Let’s use $R$ from above. Let $s := (3,1,4,2,5)$. (Note that $s\not\in
R$, but that’s not important.) We wish to find an $r&#39;\in R$ such that
$s\circ r&#39; = \min (s\circ R)$. Well, the smallest permutation would be
one such that $r&#39;(1) = 2$, because then $s(r&#39;(1)) = s(2) = 1$. Looking
at our trie $R$, we can see the only candidate is that associated with
$\texttt{p6}$: $(2,4,1,3,5)$, which is the minimum.</p>
<p>What about the next smallest $s\circ r&#39;&#39;$? For ease, let’s call this
product $m$. We would want a permutatation such that $r&#39;&#39;(1) = 4$,
because $m(1) = s(r&#39;&#39;(1)) = s(1) = 2$. This time, there are two
candidates:</p>
<p>$$
(4,1,3,2,5)\qquad (4,1,3,5,2)
$$</p>
<p>So at least we know $m = (2, \ldots)$. To disambiguate, we need to
look at $r&#39;&#39;(2)$. These are the same, likewise $r&#39;&#39;(3)$, so we have no
degree of freedom at $2$ or $3$ to minimize the product. Thus $m = (2,
3, 4, \ldots)$. We have a choice at $r&#39;&#39;(4)$, however. The best choice
is $r&#39;&#39;(4) = 2$, because $m(4) = s(r&#39;&#39;(4)) = s(2) = 1$, the smallest
possible choice. This disambiguates our choice of $r&#39;&#39;$ to be
$(4,1,3,2,5)$ so that $m = (2,3,4,1,5)$.</p>
<p>We could repeat the procedure to find the next smallest product
$s\circ r&#39;&#39;&#39;$. What exactly is the procedure here? Well, we walked
down the tree $R$, but instead of walking down it straight, we instead
did so in a permuted order based on $s$—specifically
$s^{-1}$. Consider our normal algorithm for walking the tree<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup> in
lexicographic order:</p>
<pre tabindex="0"><code>function walkLex(R):
  if notTree(R):
    print R
  else:
    for i from 1 to length(R):
      if R[i] exists:
        walkLex(R[i])
</code></pre><p>We can instead walk in <em>permuted</em> order, so that we produce a sequence
$[r, r&#39;&#39;, r&#39;&#39;&#39;, \ldots]$ such that</p>
<p>$$
s\circ r &lt; s \circ r&#39; &lt; s \circ r&#39;&#39;&#39; &lt; \cdots,
$$</p>
<p>we modify our walking algorithm as so:</p>
<pre tabindex="0"><code>function walkProductLex(R, s):
  walk&#39;(R, inverse(s))

function walk&#39;(R, s):
  if notTree(R):
    print R
  else:
    for i from 1 to length(R):
      j = s(i)
      if R[j] exists:
        walk&#39;(R[j], s)
</code></pre><p>Note that $s$ was inverted before the recursion to make quick permuting of each node.</p>
<p>With this, we have the remarkable ability to iterate through products
in lexicographic order, without having to enumerate them all and sort
them. This was the last and critical ingredient.</p>
<h3 id="the-4-list-algorithm-and-solving-the-rubiks-cube">The 4-List Algorithm and solving the Rubik’s Cube</h3>
<p>Now we want to put this all together to create the <em>4-List
Algorithm</em>. Let’s restate the problem in clear terms.</p>
<p><strong>Problem (4-List)</strong>: Let $s$ be a permutation. Let $L_1$, $L_2$,
$L_3$, and $L_4$ be sets of permutations such that we know $s\in
L_4\circ L_3\circ L_2\circ L_1$. Find $l_1\in L_1$, $l_2\in L_2$,
$l_3\in L_3$, and $l_4\in L_4$ such that $s = l_4\circ l_3\circ
l_2\circ l_1$.</p>
<p>Piecing together the elements above, we arrive at the 4-List Algorithm.</p>
<p><strong>Algorithm (4-List)</strong>:</p>
<ol>
<li>Construct $L&#39;_3 := L_3^{-1}\circ s$ and $L&#39;_4 := L_4^{-1}$.</li>
<li>Create two generators<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>: $X_1$ that walks $L_2\circ L_1$ in
lexicographic order, and $X_2$ that walks $L&#39;_3\circ L&#39;_4$ in
lexicographic order. Do this by using the <strong>Walk Products</strong>
algorithm, which itself is implemented by constructing permutation
tries and using <code>walkProductLex</code>.</li>
<li>Call <code>findCommon</code> on $X_2$ and $X_1$. This is guaranteed to find a
solution $(l_3^{-1},l_4^{-1}\circ s,l_2,l_1)$. Process the solution
to return $(l_4, l_3, l_2, l_1)$.</li>
</ol>
<p>The main difficulty of this algorithm, aside from implementing each
subroutine correctly, is plumbing the right data around.</p>
<p>Now, we can use this to solve a scrambled Rubik’s Cube $s$.</p>
<p><strong>Algorithm (Solve Cube)</strong>:</p>
<ol>
<li>Let $L = \bar C^5$, keeping a record of the words used to construct
each element of $L$. (We recommend immediately making a permutation
trie, where the leaves store the words.)</li>
<li>Apply the <strong>4-List Algorithm</strong> to the problem $s \in L\circ L\circ
L\circ L$ to produce $(l_4, l_3, l_2, l_1)$.</li>
<li>Return words $(w_4, w_3, w_2, w_1)$ associated with the
permutations $(l_4, l_3, l_2, l_1)$.</li>
</ol>
<p>Amazingly, this algorithm really works, and answers our blog post
question in the affirmative: <em>yes, the Rubik’s Cube can be
brute-forced</em>.</p>
<h2 id="example-and-source-code">Example and source code</h2>
<p>This algorithm is implemented in Common Lisp, in my computational
group theory package
<a href="https://github.com/stylewarning/cl-permutation">CL-PERMUTATION</a>. CL-PERMUTATION
already has built in support for Rubik’s Cubes as permutation
groups. Starting a new Common Lisp session, we have the following:</p>
<pre tabindex="0"><code>&gt; (ql:quickload &#39;(:cl-permutation :cl-permutation-examples))
&gt; (in-package :cl-permutation)
&gt; (group-order (perm-examples:make-rubik-3x3))
43252003274489856000
&gt; (format t &#34;~R&#34; *)
forty-three quintillion two hundred fifty-two quadrillion three trillion
two hundred seventy-four billion four hundred eighty-nine million
eight hundred fifty-six thousand
NIL
</code></pre><p>The built-in Rubik’s Cube model only uses $\{F, R, U, B, L, D\}$, so
we make new generators corresponding to $\bar C$.</p>
<pre tabindex="0"><code>&gt; (defvar *c (loop :with cube := (perm-examples:make-rubik-3x3)
                   :for g :in (perm-group.generators cube)
                   :collect (perm-expt g 1)
                   :collect (perm-expt g 2)
                   :collect (perm-expt g 3)))
*C
&gt; (length *c)
18
</code></pre><p>Now we construct $\bar C^5$.</p>
<pre tabindex="0"><code>&gt; (defvar *c5 (generate-words-of-bounded-length *c 5))
*C5
&gt; (perm-tree-num-elements *c5)
621649
</code></pre><p>Note that this constructs a <code>perm-tree</code> object, which automatically
stores the words associated with each permutation generated.</p>
<p>Now let’s generate a random element of the cube group.</p>
<pre tabindex="0"><code>&gt; (defvar *s (random-group-element (perm-examples:make-rubik-3x3)))
*S
&gt; *s
#&lt;PERM 43 44 41 20 47 11 28 9 24 13 17 42 36 40 37 25 6 21 1 29 7 19 10 3 35 39 22 18 34 33 31 48 16 15 30 2 23 32 26 46 8 4 27 12 45 14 5 38&gt;
</code></pre><p>Lastly, we run the 4-list algorithm and wait.</p>
<pre tabindex="0"><code>&gt; (decompose-by-4-list *s *c5 *c5 *c5 *c5 :verbose t)
10,000,000: 52 sec @ 192,553 perms/sec; .0013% complete, eta 1114 hours 58 minutes
20,000,000: 48 sec @ 206,858 perms/sec; .0026% complete, eta 1037 hours 51 minutes
Evaluation took:
  145.094 seconds of real time
  145.097120 seconds of total run time (144.961382 user, 0.135738 system)
  [ Run times consist of 2.405 seconds GC time, and 142.693 seconds non-GC time. ]
  100.00% CPU
  421,375,385,955 processor cycles
  11,681,934,352 bytes consed

((8 11 14 2 4)
 (1 16 9 15 1)
 (7 6 18 8 15)
 (9 13 16 15 8))
</code></pre><p>We are pretty lucky this one ended in a mere 2 minutes 25 seconds! It
usually isn’t so prompt with an answer.</p>
<p>The results are printed as four words: our $l_4$, $l_3$, $l_2$, and
$l_1$. Each integer $n$ represents the 1-indexed $n$th permutation of
$\bar C$ (ordered by how it was constructed). We can create a more
traditional notation:</p>
<pre tabindex="0"><code>&gt; (defvar *solution (reduce #&#39;append *))
*SOLUTION
&gt; (defun notation (ws)
    (dolist (w (reverse ws))
      (multiple-value-bind (move order)
          (floor (1- w) 3)
        (format t &#34;~C~[~;2~;&#39;~] &#34;
                (aref &#34;FRUBLD&#34; move)
                order))))
NOTATION
&gt; (notation *solution)
U2 L&#39; D L U&#39; L&#39; U2 D&#39; R&#39; U F L&#39; U&#39; D F R F2 L2 B2 U2
</code></pre><p>How do we know if this is correct? We need to check that the
composition of this word equals our random element, which we do by
composing the word (using something CL-PERMUTATION calls a “free-group
homomorphism”), inverting the permutation, and composing it with our
scramble to see that it brings us to an identity permutation.</p>
<pre tabindex="0"><code>&gt; (defvar *hom (free-group-&gt;perm-group-homomorphism
                (make-free-group 18)
                (generate-perm-group *c)))
*HOM
&gt; (perm-compose (perm-inverse (funcall *hom *solution)) *s)
#&lt;PERM 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48&gt;
</code></pre><p>Indeed, we found a reconstruction of our cube.</p>
<h2 id="tips-for-optimizing-the-4-list-algorithm">Tips for optimizing the 4-List Algorithm</h2>
<p>One of the most troubling aspects of implementing this algorithm is
making it fast enough. My initial implementation worked at a whopping
200 permutations per second. That’s incredibly slow, and meant that it
would take well over a century (in the worst case) for my program to
finish. Now, it works at about 190,000 permutations per second, with
an estimated worst-case search time of 2 months. (I haven’t
encountered a scrambled cube position which has taken more than 10
hours.)</p>
<p>Here are some ways I sped things up.</p>
<ol>
<li>Be economical with memory. When doing exploratory programming, it’s
desirable to tag and store everything, but each of those storages
and accesses take time.</li>
<li><em>Don’t</em> use actual arrays in the permutation trie. When I did that,
I ran out of memory. I instead opted for a sparse representation
using an “a-list” (that is, a linked list of <code>(index, value)</code>
pairs).</li>
<li>Make the permutation handling fast, like composition, equality
testing, and lexicographic ordering. I was originally using generic
arithmetic and 64-bits to represent each permutation element, and
it degraded speed.</li>
<li>Use a good priority queue implementation. You’ll be pushing and
popping hundreds of millions of elements.</li>
<li>Do some analysis and compress the permutation trie
representation. Most nodes of the trie will only contain one
value. If that’s the case, just store instead the permutation (and
whatever value associated with it) at the shallowest depth. This
will save a lot of time by avoiding a lot of needless (permuted)
recursion.</li>
</ol>
<p>If you have other tips for speeding up the algorithm, please email me!</p>
<h2 id="sample-benchmarks">Sample benchmarks</h2>
<p>In the following, we only consider the problem of solving the Rubik’s
Cube using the 4-list algorithm, assuming a solution length of 20
moves.</p>
<p>My computer is a ThinkPad 25th Anniversary Edition. It has an Intel
Core i7-7500U processor at 2.70 GHz, but boosting to 3.50 GHz. It has
32 GiB of RAM, but comfortably runs the solver with around 3–4 GiB.</p>
<p>The algorithm as implemented is able to check around 190,000 elements
per second.</p>
<p>Generating the move lists and pre-processing is a relatively fixed
cost. The lists can be generated once, but the preprocessing (i.e.,
composing the scramble with one of the lists) needs to happen each
solve. In my implementation, the initialization cost is consistently 9
seconds.</p>
<p>After initialization, the search is conducted. The run time varies
wildly, anywhere from seconds to hours.</p>
<ul>
<li>64 s, 188 billion CPU cycles, 4 GiB of allocation</li>
<li>165 s, 480 billion CPU cycles, 12 GiB of allocation</li>
<li>2210 s, 6 trillion CPU cycles, 162 GiB of allocation</li>
<li>4613 s, 13 trillion CPU cycles, 356 GiB of allocation</li>
<li>24010 s, 70 trillion CPU cycles, 2 TiB of allocation</li>
</ul>
<p>These are randomly sampled Rubik’s Cube scrambles, sorted by time.</p>
<p>In principle, with the current level of optimization, the algorithm
can take as much as 2 months to finish. I’m confident that my
implementation can be brought down a factor of 2, less confident it
can be easily brought down a factor of 50—but it wouldn’t surprise
me either way.</p>
<p>One interesting thing about this algorithm is that it seems to return
very, very quickly if the solution is 10 or fewer moves. Why? I
haven’t done a careful analysis, but I believe it is essentially
because the solution will be in $L_2\circ L_1$. The permutations $l_3$
and $l_4$ will be identity, which reduces to the problem of just
finding $s\in L_2\circ L_1$.</p>
<h2 id="conclusion">Conclusion</h2>
<p>“Meet in the middle” algorithms are old and well understood. When we
can’t brute-force an entire space, we can try splitting it in two and
try to combine them. That’s of course the spirit of the 4-List
Algorithm, but the devil is always in the details, and I hope this
blog post showed a lot of disparate facts needed to come together to
realize the algorithm.</p>
<p>I think the algorithm communicated by Shamir and his colleagues has
been remarkable but forgotten. While better algorithms exist for the
specific task of solving the Rubik’s Cube, the generality of the
4-List Algorithm ought not be understated.</p>
<h2 id="references">References</h2>
<ol>
<li>A. Fiat, S. Moses, A. Shamir, I. Shimshoni and G. Tardos, “Planning and learning in permutation groups,” 30th Annual Symposium on Foundations of Computer Science, Research Triangle Park, NC, USA, 1989, pp. 274–279, doi: 10.1109/SFCS.1989.63490. (<a href="https://ieeexplore.ieee.org/document/63490">Link</a>)</li>
<li>A. Bawden. “Shamir’s talk really was about how to solve the cube!”. Alan Bawden. From the <em>Cube Lovers</em> mailing list. 27 May 1987. (<a href="http://www.math.rwth-aachen.de/~Martin.Schoenert/Cube-Lovers/Alan_Bawden__Shamir%27s_talk_really_was_about_how_to_solve_the_cube!.html">Link</a>)</li>
</ol>

</article>
</div></div>
  </body>
</html>
