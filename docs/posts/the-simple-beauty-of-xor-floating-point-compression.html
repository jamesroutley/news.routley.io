<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://clemenswinter.com/2024/04/07/the-simple-beauty-of-xor-floating-point-compression/">Original</a>
    <h1>The simple beauty of XOR floating point compression</h1>
    
    <div id="readability-page-1" class="page"><div>
			
<p>I recently implemented a small program to visualize the inner workings of a scheme that compresses floating point timeseries by XORing subsequent values. The resulting visualizations are quite neat and made it much easier for me to understand this beautiful algorithm than any of the explanations that I had previously encountered.</p>







<p>The algorithm is simple. We start by writing out the first floating point number in full. All subsequent numbers are XORed with the previous number and then encoded in one of three ways:</p>



<ol>
<li>We write a single 0<sub>2</sub> bit. This indicates that the number is identical to the previous number.</li>



<li>We write the two bit sequence 11<sub>2</sub>. We write a 5 bit integer that indicates the number of leading zeros in the XORed value. We write a 6 bit integer that indicates the number of bits in the subsequence that starts at the first 1 bit and includes and ends at the last 1 bit. We then write out this subsequence, omitting all leading and trailing zeros.</li>



<li>We write the two bit sequence 10<sub>2</sub>. We then immediately write out the subsequence of the XORed value that has the same starting index and length as the last subsequence we wrote.</li>
</ol>



<p>How do we choose between (2) and (3)? The advantage of (3) is that we omit the 5+6=11 bits we would otherwise require for storing the number of leading zeros and length of the sequence, but we can only use (3) if our nonzero subsequence fits within the window defined by the previous offset and length values. The <a href="https://www.vldb.org/pvldb/vol8/p1816-teller.pdf">Gorilla paper</a> proposes a simple heuristic: if the previously used subsequence of bits includes all nonzero bits, we go with (3) and omit writing out the offset and length bits. If not, we go with (2) and write out a new offset and length. Note that this heuristic minimizes the amount times we have to write out the 11 bits of offset/length values, but doesn’t necessarily give optimal compression since we might be writing much longer subsequences on subsequent values than necessary (more on that later).</p>



<p>While the algorithm is simple, its not immediately obvious from its description why it should lead to good compression. Case (1) is easy, repeated values are compressed to a single bit. But why does slicing off leading and trailing zeros on the XOR of subsequent floating point values work well? Let’s apply the algorithm to some real data and see what it looks like!</p>







<p>The first column shows the decimal representation of the timeseries we are compressing. This particular timeseries happens to measure the time spent in a particular code section of <a href="https://github.com/entity-neural-network/enn-trainer">enn-trainer</a> on each iteration of a training run.</p>



<p>The second column is the <a href="https://en.wikipedia.org/wiki/Double-precision_floating-point_format">64-bit floating point representation</a> of the same numbers with <mark>sign bit in red</mark>, <mark>exponent in green</mark>, and <mark>mantissa in blue</mark>. Note that the exponent bits are identical for almost all of the numbers in the series, which is expected when values vary by a lot less than 2x. Note also that the numbers have a lot of trailing zero bits, which is not something that’s obvious from the decimal representation. It looks like these timings were recorded at microsecond resolution, which means that values smaller than 0.02 all fit into just 15bits.</p>



<p>The third column shows the XOR of each value which the previous value, with the region of non-zero bits <mark>highlighted in yellow</mark>. The XORed values have many leading zeros (because the sign bit and exponent don’t change on subsequent values) and many trailing zeros (because the original values already shared a lot of trailing zeros). These regular long runs of leading and trailing zero bits are what makes the compression work!</p>



<p>The fourth column shows the bits output by the algorithm. On the first row, we simply have the full bit representation of the first number without any compression. On the second row, we first write the <mark>11 control bits</mark>, <mark>the number of leading zeros (8<sub>10</sub> = 01000<sub>2</sub>), </mark><mark>the length of the nonzero region – 1 (17<sub>10</sub> = 010001<sub>2</sub>)</mark>, and the <mark>nonzero bits 111111110110110011</mark>. On the third row, the region of nonzero bits falls into the same window, so we instead write the <mark>10 control bits</mark> and the nonzero subsequence <mark>111111100100111000</mark>. Note that this subsequence has three trailing 0s, but these 3 bits still take up less space than the 11 bits we would need to write out a new offset and length. We continue to omit the offset and length until the ninth row, where we reset to a new offset and length since the region of nonzero bits in the XOR does not fit within the previously used window.</p>



<p>This entire series (including more values not shown above) is compressed by a factor of 3.03. Even if we compare to just using 32-bit single precision floating point values to represent these numbers, we’re still compressing by a factor of 1.51.</p>



<p>See <a href="https://github.com/cswinter/LocustDB/blob/e6044bf2af810af7a2932c3a51fbe301843f8d6a/locustdb-compression-utils/src/xor_float/double.rs#L1-L95">here</a> for a simple implementation of both the encoding and decoding algorithm in less than 100 lines of Rust (not including the <code>BitReader</code>/<code>BitWriter</code> structs). It is adapted from <a href="https://github.com/jeromefroe/tsz-rs ">https://github.com/jeromefroe/tsz-rs</a> which has somewhat more complicated code but much better comments. My implementation contains two additional options: lossy compression that masks out some portion of the least significant bits in the mantissa, and a simple tweak described in the next section that can significantly improve compression for some time series.</p>







<p>As we noted in the previous section, the Gorilla compression algorithm uses a particular heuristic for resetting the window size. It greedily minimizes the number of window resets, which works well in practice most of the time, but is not guaranteed to yield maximum compression. In particular, there is one pathological failure mode: If a series contains an outlier value that requires a very large window to encode, and all subsequent values have nonzero values only in a much smaller subwindow, the inefficient larger window will get locked in because we never hit the condition that would trigger a reset of the window size. Here’s an example of this happening:</p>







<p>As a simple fix, I added a “regret” counter that keeps track of how many leading and trailing zeros we have encountered in the subsequences the algorithm outputs. If this this sum exceeds some threshold, we force a reset of the window. A “max regret” threshold of 100 seems to work quite well in practice, substantially improving compression for some series while only slightly worsening compression on others. Here is what compression of the same timeseries looks like with “max regret” of 100.</p>







<p>I haven’t given this too much thought so there’s a good chance there exist better and more principled heuristics. It would also be interesting to figure out what the most efficient algorithm is for finding the optimal choices for when to reset the window and to what range when shown the entire timeseries. This would be useful in cases where you care much more about minimizing compressed size rather than compression throughput (decompression could still use the same algorithm and would have similar throughput as with the heuristic compression).</p>







<p>My naive implementation achieves a throughput of more than 100MiB/s for encoding and 200MiB/s for decoding random floats on an i7-10875H (more compressible data achieves more than 200MiB/s encode, 350MiB/s decode). Not amazing, but pretty decent for such little code, and still a lot faster than most network connections (my main use case for this was reducing the size of network transfers). There’s a <a href="https://github.com/velvia/compressed-vec">SIMD Rust implementation that achieves multiple GB/s</a>, though it seems to use unstable features that need some updating to make it compatible the most recent version of Rust.</p>



<p>Profiling the naive implementation reveals that it spends most if its time in the <code>BitWriter</code>/<code>BitReader</code> methods.</p>



<figure><a href="https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?ssl=1"><img loading="lazy" decoding="async" width="625" height="353" data-attachment-id="9972" data-permalink="https://clemenswinter.com/2024/04/07/the-simple-beauty-of-xor-floating-point-compression/image-25-2/" data-orig-file="https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?fit=1386%2C782&amp;ssl=1" data-orig-size="1386,782" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="image-25" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?fit=625%2C353&amp;ssl=1" src="https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?resize=625%2C353&amp;ssl=1" alt="" srcset="https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?resize=1024%2C578&amp;ssl=1 1024w, https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?resize=768%2C433&amp;ssl=1 768w, https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?resize=624%2C352&amp;ssl=1 624w, https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?w=1386&amp;ssl=1 1386w, https://i0.wp.com/clemenswinter.com/wp-content/uploads/2024/03/image-25.png?w=1250&amp;ssl=1 1250w" sizes="(max-width: 625px) 100vw, 625px" data-recalc-dims="1"/></a></figure>



<p>Swapping these out for <a href="https://github.com/icewind1991/bitbuffer">bitbuffer</a> and <a href="https://github.com/cswinter/LocustDB/pull/171">adding some other small optimizations</a> gets us to 370MiB/s encode, 1GiB/s decode. I tried <a href="https://github.com/nickbabcock/bitter">bitter</a> as well, an early (broken) prototype that didn’t correctly handle reads with more than 56 bits got to &gt; 2GiB/s decode even in auto mode and a (probably correct) version was still something like 30% faster than my initial bitbuffer implementation at &gt; 1GiB/s.</p>







<p>Finally, here’s a collection of different timeseries for your viewing pleasure. Compression for all of these uses the default “max regret” threshold of 100. You can reproduce these by running <code>cargo run --example gorilla_time -- --verbose</code> at <a href="https://github.com/cswinter/LocustDB/tree/4cfbda2100abbe7b1bc6aaad541298c5c9fbdd27/locustdb-compression-utils.">4cfbda210</a>. All the timeseries here contain 246 values in total, only the first few are shown.</p>



<h2 id="low-bit-floating-point">Low-bit floating point</h2>



<p>Values in this time series are mean values calculated by dividing a small integer with a power of two, which results in very few significant bits (5.68x compression).</p>







<h2 id="highly-redundant-integers">Highly redundant integers</h2>



<p>Values in this series report the average throughput since program start rounded to an integer, which causes the timeseries to mostly converge to a single value as time goes on (13.5x compression).</p>











<h2 id="incrementing-integers">Incrementing integers</h2>



<p>Integer that increments by 1 at every timestep. Treating this as an integer rather than float and using double delta compression would be much more effective, but the XOR algorithm still manages to reduce this to just over one byte per data point (6.98x compression).</p>







<h2 id="unix-timestamps">Unix timestamps</h2>



<p>This series is a sequence of unix timestamps with ~microsecond precision. Our choice of using just 5 bits to store the number of leading 0s fails us here, since this limits us to at most 2^5 – 1 = 31 leading zeros. This series would also benefit from being stored as an integer and double delta compression (1.80x compression).</p>







<h2 id="noisy-single-precision-float">Noisy single precision float</h2>



<p>This is a fairly typical floating point time series which records the gradient norm at each iteration of a machine learning run. Most of the significant bits are incompressible noise. We still get some compression because the exponent bits don’t change and the number only uses single precision and does not use the least significant 29 bits in the mantissa (2.37x compression).</p>







<p>We can save a tiny amount of extra space if we treat it as a single precision floating point number to begin with since in that case we need one less bit to store the length of the nonzero subsequence. This makes very little difference in this case because the length field is stored rarely (2.41x compression).</p>







<p>If we’re willing to lossily compress, we can get significantly better compression by reducing the size of the mantissa even further to 4bits (8.48x compression).</p>










					</div></div>
  </body>
</html>
