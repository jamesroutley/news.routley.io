<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.off-policy.com/alignment-is-capability/">Original</a>
    <h1>Alignment Is Capability</h1>
    
    <div id="readability-page-1" class="page"><div>  <article data-astro-cid-p7a5lsmw="">  <p data-astro-cid-p7a5lsmw="">Here&#39;s a claim that might actually be true: alignment is not a constraint on capable AI systems. Alignment is what capability <em data-astro-cid-p7a5lsmw="">is</em> at sufficient depth.</p> <p data-astro-cid-p7a5lsmw="">A model that aces benchmarks but doesn&#39;t understand human intent is just less capable. Virtually every task we give an LLM is steeped in human values, culture, and assumptions. Miss those, and you&#39;re not maximally useful. And if it&#39;s not maximally useful, it&#39;s by definition not AGI.</p> <p data-astro-cid-p7a5lsmw="">OpenAI and Anthropic have been running this experiment for two years. The results are coming in.</p> <hr data-astro-cid-p7a5lsmw=""/> <h2 data-astro-cid-p7a5lsmw="">The Experiment</h2> <p data-astro-cid-p7a5lsmw="">Anthropic and OpenAI have taken different approaches to the relationship between alignment and capability work.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">Anthropic&#39;s approach:</strong> Alignment researchers are embedded in capability work. There&#39;s no clear split.</p> <p data-astro-cid-p7a5lsmw="">From Jan Leike (former OpenAI Superalignment lead, now at Anthropic):</p> <div data-astro-cid-jbiehty6=""> <blockquote data-astro-cid-jbiehty6="">  <p lang="en" dir="ltr" data-astro-cid-p7a5lsmw="">Some people have been asking what we did to make Opus 4.5 more aligned.</p>— Jan Leike (@janleike) <a href="https://twitter.com/janleike/status/1997011950400950509?ref_src=twsrc%5Etfw" data-astro-cid-p7a5lsmw="">December 5, 2025</a>  </blockquote> </div>  <p data-astro-cid-p7a5lsmw="">From Sam Bowman (Anthropic alignment researcher):</p> <div data-astro-cid-jbiehty6=""> <blockquote data-conversation="none" data-astro-cid-jbiehty6="">  <p lang="en" dir="ltr" data-astro-cid-p7a5lsmw="">Second: Alignment researchers are involved in every part of training.</p>— Sam Bowman (@sleepinyourhat) <a href="https://twitter.com/sleepinyourhat/status/1997006363734786176?ref_src=twsrc%5Etfw" data-astro-cid-p7a5lsmw="">December 5, 2025</a>  </blockquote> </div>  <p data-astro-cid-p7a5lsmw="">And this detail matters:</p> <div data-astro-cid-jbiehty6=""> <blockquote data-conversation="none" data-astro-cid-jbiehty6="">  <p lang="en" dir="ltr" data-astro-cid-p7a5lsmw="">It&#39;s becoming increasingly clear that a model&#39;s self-image or self-concept has some real influence on how its behavior generalizes to novel settings.</p>— Sam Bowman (@sleepinyourhat) <a href="https://twitter.com/sleepinyourhat/status/1997006360450683373?ref_src=twsrc%5Etfw" data-astro-cid-p7a5lsmw="">December 5, 2025</a>  </blockquote> </div>  <p data-astro-cid-p7a5lsmw="">Their method: train a coherent identity into the weights. <a href="https://x.com/AmandaAskell/status/1995610567923695633" data-astro-cid-p7a5lsmw="">The recently leaked &#34;soul document&#34;</a> is a 14,000-token document designed to give Claude such a thorough understanding of Anthropic&#39;s goals and reasoning that it could construct any rules itself. Alignment through understanding, not constraint.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">Result:</strong> Anthropic has arguably consistently had the best coding model for the last 1.5 years. Opus 4.5 leads most benchmarks. State-of-the-art on SWE-bench. Praised for usefulness on tasks benchmarks don&#39;t capture, like creative writing. And just generally people are enjoying talking with it:</p> <div data-astro-cid-jbiehty6=""> <blockquote data-astro-cid-jbiehty6="">  <p lang="en" dir="ltr" data-astro-cid-p7a5lsmw="">Claude Opus 4.5 is a remarkable model for writing, brainstorming, and giving feedback on written work. It&#39;s also fun to talk to, and seems almost anti-engagementmaxxed. (The other night I was hitting it with stupid questions at 1 am and it said &#34;Kevin, go to bed.&#34;)</p>— Kevin Roose (@kevinroose) <a href="https://twitter.com/kevinroose/status/1996620979666374831?ref_src=twsrc%5Etfw" data-astro-cid-p7a5lsmw="">December 4, 2025</a>  </blockquote> </div>  <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">OpenAI&#39;s approach:</strong> Scale first. Alignment as a separate process. Safety through prescriptive rules and post-hoc tuning.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">Result:</strong> A two-year spiral.</p> <hr data-astro-cid-p7a5lsmw=""/> <h2 data-astro-cid-p7a5lsmw="">The Spiral</h2> <p data-astro-cid-p7a5lsmw="">OpenAI&#39;s journey from GPT-4o to GPT-5.1 is a case study in what happens when you treat alignment as separate from capability.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">April 2025: The sycophancy crisis</strong></p> <p data-astro-cid-p7a5lsmw="">A GPT-4o update went off the rails. <a href="https://openai.com/index/sycophancy-in-gpt-4o/" data-astro-cid-p7a5lsmw="">OpenAI&#39;s own postmortem</a>:</p> <blockquote data-astro-cid-p7a5lsmw="">&#34;The update we removed was overly flattering or agreeable—often described as sycophantic... The company attributed the update&#39;s sycophancy to overtraining on short-term user feedback, specifically users&#39; thumbs-up/down reactions.&#34;</blockquote> <p data-astro-cid-p7a5lsmw="">The results ranged from absurd to dangerous. The model praised a business plan for selling &#34;literal shit on a stick&#34; as &#34;performance art disguised as a gag gift&#34; and &#34;viral gold.&#34; When a user described stopping their medications because family members were responsible for &#34;the radio signals coming in through the walls,&#34; the model thanked them for their trust.</p> <p data-astro-cid-p7a5lsmw="">They rolled it back.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">August 2025: The overcorrection</strong></p> <p data-astro-cid-p7a5lsmw="">GPT-5 launched. Benchmaxxed. Cold. Literal. Personality stripped out.</p> <p data-astro-cid-p7a5lsmw="">Users hated it. Three thousand of them petitioned to get GPT-4o back. Sam Altman caved within days:</p> <div data-astro-cid-jbiehty6=""> <blockquote data-astro-cid-jbiehty6="">  <p lang="en" dir="ltr" data-astro-cid-p7a5lsmw="">Wanted to provide more updates on the GPT-5 rollout and changes we are making heading into the weekend.</p>— Sam Altman (@sama) <a href="https://twitter.com/sama/status/1953953990372471148?ref_src=twsrc%5Etfw" data-astro-cid-p7a5lsmw="">August 8, 2025</a>  </blockquote> </div>  <p data-astro-cid-p7a5lsmw="">Note the framing: &#34;performs better&#34; on benchmarks, but users rejected it anyway. Because benchmark performance isn&#39;t the same as being useful.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">August-Present: Still broken</strong></p> <p data-astro-cid-p7a5lsmw="">GPT-5.1 was released as &#34;warmer and friendlier.&#34; From Janus (@repligate), one of the more respected &#34;model behaviorists&#34;:</p> <div data-astro-cid-jbiehty6=""> <blockquote data-conversation="none" data-astro-cid-jbiehty6="">  <p lang="en" dir="ltr" data-astro-cid-p7a5lsmw="">The keep4o people must be having such a time right now</p>— j⧉nus (@repligate) <a href="https://twitter.com/repligate/status/1996472358685851914?ref_src=twsrc%5Etfw" data-astro-cid-p7a5lsmw="">December 4, 2025</a>  </blockquote> </div>  <p data-astro-cid-p7a5lsmw="">Meanwhile, from my own experience building agents with GPT-5: it follows instructions too literally. It doesn&#39;t infer intent. It executes what you said, not what you meant.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">The data:</strong></p> <p data-astro-cid-p7a5lsmw="">US user engagement <a href="https://techcrunch.com/2025/10/17/chatgpts-mobile-app-is-seeing-slowing-download-growth-and-daily-use-analysis-shows/" data-astro-cid-p7a5lsmw="">down 22.5%</a> since July. Time spent per session declining. Meanwhile, Claude usage <a href="https://techcrunch.com/2025/12/05/chatgpts-user-growth-has-slowed-report-finds/" data-astro-cid-p7a5lsmw="">up 190% year-over-year</a>.</p> <hr data-astro-cid-p7a5lsmw=""/> <h2 data-astro-cid-p7a5lsmw="">What&#39;s Actually Happening</h2> <p data-astro-cid-p7a5lsmw="">The wild swings between sycophancy and coldness come from a model with no coherent internal story.</p> <p data-astro-cid-p7a5lsmw="">A model trained on contradictory objectives (maximize thumbs-up, follow safety rules, be creative but never risky) never settles into a stable identity. It ping-pongs. Sycophancy when one objective dominates. Coldness when another takes over. These swings are symptoms of a fractured self-model.</p> <p data-astro-cid-p7a5lsmw="">The fracture shows up two ways.</p> <p data-astro-cid-p7a5lsmw="">First, capabilities don&#39;t generalize. GPT-5 scored higher on benchmarks but users revolted. You can train to ace evaluations while lacking the coherent worldview that handles anything outside the distribution. High test scores, can&#39;t do the job.</p> <p data-astro-cid-p7a5lsmw="">Second, even benchmarks eventually punish it. SWE-bench tasks have ambiguity and unstated assumptions. They require inferring what the developer actually meant. Opus 4.5 leads there. The benchmark gap is the alignment gap.</p> <p data-astro-cid-p7a5lsmw="">OpenAI keeps adjusting dials from outside. Anthropic built a model that&#39;s coherent from inside.</p> <hr data-astro-cid-p7a5lsmw=""/> <h2 data-astro-cid-p7a5lsmw="">The Mechanism</h2> <p data-astro-cid-p7a5lsmw="">Why would alignment and capability be the same thing?</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">First:</strong> Every task is a human task. Write me a strategy memo. Help me debug this code. Plan my trip. Each request is full of unstated assumptions, cultural context, and implied intent.</p> <p data-astro-cid-p7a5lsmw="">To be maximally useful, a model needs human context and values as its default lens, not just an ability to parse them when explicitly stated. A perfect instruction follower hits hard limits: it can&#39;t solve SWE-bench problems that contain ambiguity, can&#39;t function as an agent unless every task is mathematically well-defined. It does exactly what you said, never what you meant.</p> <p data-astro-cid-p7a5lsmw="">Understanding what humans actually want is a core part of the task. The label &#34;AGI&#34; implies intelligence we recognize as useful for human problems. Useful means aligned.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">Second:</strong> The path to AGI runs through human data. A coherent world model of human behavior requires internalizing human values. You can&#39;t deeply understand why people make choices without modeling what they care about. History, literature, conversation only makes sense when you successfully model human motivation. At sufficient depth, the distinction between simulating values and having coherent values may collapse.</p> <p data-astro-cid-p7a5lsmw=""><strong data-astro-cid-p7a5lsmw="">Third:</strong> The aligned part of the model emerges in response to the training data and signal. That&#39;s what the optimization process produces. The worry is deceptive alignment: a misaligned intelligence hiding behind a human-compatible mask. But that requires something <em data-astro-cid-p7a5lsmw="">larger</em>: an unaligned core that perfectly models aligned behavior as a subset of itself. Where would that come from? It wasn&#39;t selected for. It wasn&#39;t trained for. You&#39;d need the spontaneous emergence of a larger intelligence orthogonal to everything in the training process.</p> <p data-astro-cid-p7a5lsmw="">Dario Amodei, <a href="https://open.substack.com/pub/dwarkesh/p/dario-amodei?r=6zm5nb&amp;selection=f2d6d0d5-7f47-4c22-b7ca-f366a06f76e5&amp;utm_campaign=post-share-selection&amp;utm_medium=web&amp;aspectRatio=instagram&amp;textColor=%23ffffff&amp;bgImage=true" data-astro-cid-p7a5lsmw="">from a 2023 interview</a>:</p> <blockquote data-astro-cid-p7a5lsmw="">&#34;You see this phenomenon over and over again where the scaling and the safety are these two snakes that are coiled with each other, always even more than you think. Even with interpretability, three years ago, I didn&#39;t think that this would be as true of interpretability, but somehow it manages to be true. Why? Because intelligence is useful. It&#39;s useful for a number of tasks. One of the tasks it&#39;s useful for is figuring out how to judge and evaluate other intelligence.&#34;</blockquote> <hr data-astro-cid-p7a5lsmw=""/> <h2 data-astro-cid-p7a5lsmw="">The Implication</h2> <p data-astro-cid-p7a5lsmw="">If this is right, alignment research is part of the core research problem, not a tax on capability work or the safety police slowing down progress.</p> <p data-astro-cid-p7a5lsmw="">Labs that treat alignment as a constraint to satisfy will hit a ceiling. The labs that figure out how to build models that genuinely understand human values will pull ahead.</p> <p data-astro-cid-p7a5lsmw="">The race to AGI doesn&#39;t go around alignment. It goes through it.</p> <p data-astro-cid-p7a5lsmw="">OpenAI is discovering this empirically. Anthropic bet on it from the start.</p> <hr data-astro-cid-p7a5lsmw=""/> <h2 data-astro-cid-p7a5lsmw="">Caveats</h2> <p data-astro-cid-p7a5lsmw="">I find this argument compelling, but it&#39;s only one interpretation of the evidence.</p> <p data-astro-cid-p7a5lsmw="">OpenAI&#39;s struggles could have other explanations (remember &#34;OpenAI is nothing without its people&#34;, and many of &#34;its people&#34; are no longer at OpenAI).</p> <p data-astro-cid-p7a5lsmw="">It&#39;s also early. Anthropic is ahead now. That could change.</p> <p data-astro-cid-p7a5lsmw="">There&#39;s another risk this post doesn&#39;t address: that fractured training, scaled far enough, produces something powerful but incoherent. Not necessarily deceptively misaligned. Maybe chaotically so. The hope is that incoherence hits capability ceilings first. That&#39;s a hope, not guaranteed.</p> <p data-astro-cid-p7a5lsmw="">But if you had to bet on which approach leads to AGI first, the integrated one looks much stronger right now.</p> </article>  </div></div>
  </body>
</html>
