<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://pepijndevos.nl/2023/07/15/chatlmza.html">Original</a>
    <h1>ChatLZMA</h1>
    
    <div id="readability-page-1" class="page"><div>
			
<article>
	
	<section>
	<p>I came across a random <a href="https://twitter.com/goodside/status/1679358632431853568">tweet</a>, found a <a href="https://en.wikipedia.org/wiki/Hutter_Prize">Wikipedia page</a> and bumped into some smart people and long story short, apparently <strong>compression is equivalent to general intelligence</strong>.</p>

<p>So, how do you build ChatGPT with data compression? What if you compress a large corpus of text to build up the encoding table, then you compress your prompt and append some random data and decompress the random data, and hope it decompresses to something sensible.</p>

<p>It feels vaguely similar to <a href="https://en.wikipedia.org/wiki/Diffusion_model">diffusion</a>, but what do I know. Look, this is just a dumb idea, let’s just see what happens ok? Well, here is my progress so far. It’s kind of whack but it’s hilarious to me that it produces something resembling words.</p>

<div><div><pre><code><span>import</span> <span>nltk</span>
<span>import</span> <span>lzma</span>
<span>import</span> <span>random</span>

<span>my_filters</span> <span>=</span> <span>[</span>
    <span>{</span><span>&#34;id&#34;</span><span>:</span> <span>lzma</span><span>.</span><span>FILTER_LZMA2</span><span>,</span> <span>&#34;preset&#34;</span><span>:</span> <span>9</span> <span>|</span> <span>lzma</span><span>.</span><span>PRESET_EXTREME</span><span>},</span>
<span>]</span>
<span>lzc</span> <span>=</span> <span>lzma</span><span>.</span><span>LZMACompressor</span><span>(</span><span>lzma</span><span>.</span><span>FORMAT_RAW</span><span>,</span> <span>filters</span><span>=</span><span>my_filters</span><span>)</span>

<span>corp</span> <span>=</span> <span>nltk</span><span>.</span><span>corpus</span><span>.</span><span>reuters</span><span>.</span><span>raw</span><span>().</span><span>encode</span><span>()</span>
<span>out1</span> <span>=</span> <span>lzc</span><span>.</span><span>compress</span><span>(</span><span>corp</span><span>)</span>

<span>corp</span> <span>=</span> <span>&#39; &#39;</span><span>.</span><span>join</span><span>(</span><span>nltk</span><span>.</span><span>corpus</span><span>.</span><span>brown</span><span>.</span><span>words</span><span>()).</span><span>encode</span><span>()</span>
<span>out2</span> <span>=</span> <span>lzc</span><span>.</span><span>compress</span><span>(</span><span>corp</span><span>)</span>

<span>corp</span> <span>=</span> <span>nltk</span><span>.</span><span>corpus</span><span>.</span><span>gutenberg</span><span>.</span><span>raw</span><span>().</span><span>encode</span><span>()</span>
<span>out3</span> <span>=</span> <span>lzc</span><span>.</span><span>compress</span><span>(</span><span>corp</span><span>)</span>

<span>out_end</span> <span>=</span> <span>lzc</span><span>.</span><span>flush</span><span>()</span>

<span>lzd</span> <span>=</span> <span>lzma</span><span>.</span><span>LZMADecompressor</span><span>(</span><span>lzma</span><span>.</span><span>FORMAT_RAW</span><span>,</span> <span>filters</span><span>=</span><span>my_filters</span><span>)</span>

<span>lzd</span><span>.</span><span>decompress</span><span>(</span><span>out1</span><span>)</span>
<span>lzd</span><span>.</span><span>decompress</span><span>(</span><span>out2</span><span>)</span>
<span>lzd</span><span>.</span><span>decompress</span><span>(</span><span>out3</span><span>)</span>
<span># mess around to avoid LZMAError: Corrupt input data
</span><span>lzd</span><span>.</span><span>decompress</span><span>(</span><span>out_end</span><span>[:</span><span>-</span><span>344</span><span>])</span>
<span># insert prompt????
</span><span>print</span><span>(</span><span>lzd</span><span>.</span><span>decompress</span><span>(</span><span>random</span><span>.</span><span>randbytes</span><span>(</span><span>50</span><span>)).</span><span>decode</span><span>(</span><span>errors</span><span>=</span><span>&#34;ignore&#34;</span><span>))</span>
</code></pre></div></div>

<p>Here are a few runs. Note how the start is always <code>, and tri</code>, usually completing it into some word. Are we doing some primitive accidental “prompting” or just flushing the buffer? Either way, not bad for mere seconds of “training”!</p>

<div><div><pre><code>$ python train.py 
, and triof billioerse,
But
ht and see th,
Thy smile, in to be happy,
Wmson,
Over tout as aThy smile;t as aThyrged in
 
ent, foldehe snoion since how long,
my roomr? Is ic books 

$ python train.py 
, and triompact, sca,
Take deepcky fouy vitaliz  bodiehow there i,
Nor drummiwisibly wile of the-ations, dutway?
Yet ld woman&#39;okesmanall whoy slow bekesmanalle me 

$ python train.py 
, and tri billions of the boftier, faie no acqutory&#39;s dazzd haOr that thpages:
(Sometimeseathe ihern, Sounte, fld Turkey n one,
Worlseathe Border Minstrelsy,ine, New-Ene Queen.

&#39;Thelicate l

$ python train.py 
, and tri, sleepinlke babes  bent,
Abird;
Forhis fair n!
By thea mystic strangehe gifts ofhe body aering
 t: I haue a lugs whipageantuperb-fnz

$ python train.py 
, and triions of b--n the gra, with
e open the countless buAnd bid theng;
Billi  toward you.i ally undya Songs

To f Death, istas of was mar to be UY 9,30
</code></pre></div></div>

	</section>
	
</article>


		</div></div>
  </body>
</html>
