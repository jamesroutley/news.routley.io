<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cinematography.com/index.php?/forums/topic/71346-why-23976-and-not-24-fps/&amp;tab=comments#comment-455454">Original</a>
    <h1>Why 23.976 and not 24 FPS?</h1>
    
    <div id="readability-page-1" class="page"><div data-role="commentContent" data-controller="core.front.core.lightboxedImages">
			
<p>Truth is that most digital projects shot for either broadcast or cinema use 23.976 instead of 24 in the U.S.</p>

<p>It&#39;s nothing to do with timecode.</p>

<p>From Wikipedia on NTSC:</p>
<p><em>In January 1950, the Committee was reconstituted to standardize color television. In December 1953, it unanimously approved what is now called the NTSC color television standard (later defined as RS-170a). The &#34;compatible color&#34; standard retained full backward compatibility with existing black-and-white television sets. Color information was added to the black-and-white image by introducing a color subcarrier of precisely 3.579545 MHz (nominally 3.58 MHz). The precise frequency was chosen so that horizontal line-rate modulation components of the chrominance signal would fall exactly in between the horizontal line-rate modulation components of the luminance signal, thereby enabling the chrominance signal to be filtered out of the luminance signal with minor degradation of the luminance signal. Due to limitations of frequency divider circuits at the time the color standard was promulgated, the color subcarrier frequency was constructed as composite frequency assembled from small integers, in this case 5×7×9/(8×11) MHz.[7] The horizontal line rate was reduced to approximately 15,734 lines per second (3.579545×2/455 MHz) from 15,750 lines per second, and the frame rate was reduced to approximately 29.970 frames per second (the horizontal line rate divided by 525 lines/frame) from 30 frames per second. These changes amounted to 0.1 percent and were readily tolerated by existing television receivers.</em></p>

<p>So analog color NTSC runs at 29.97 fps / 59.94i.  This frame rate has been carried over into digital HD broadcasting in the U.S.</p>

<p>So throughout the 2000&#39;s, the main reason you shot at 23.976 fps instead of 24P was <em>audio</em> post in the U.S.  Even movies shot at true 24 fps had to deal with this because a telecine transfer to NTSC for dailies and NLE post changed the frame rate to 23.976 fps, so the frame rate didn&#39;t get restored to 24 fps until the movie was finished to film and projected at 24 fps.  So after a movie was edited offline, tape copies were sent to sound post for cutting and mixing sound, so they were working with material running at 23.976 fps whether or not it was shot at 24 fps or 23.976 fps.  If the final product was for broadcast, it didn&#39;t matter because it was going to get shown at 59.94i with a 3:2 pulldown anyway.  But for material destined for theatrical projection, at some point before or after the final mix (depending probably on whether they were mixing to a video copy or a print), the speed had to be corrected back to 24 fps.</p>

<p>So it because easier for digital projects to just shoot at 23.976 fps instead of 24 fps so that the sound post could stay at 23.976 fps all the way to the mix, after which one could decide if one needed a true 24 fps version.</p>

<p>Now that most sound people get digital files instead of tape copies of the offline cut to work from, it would be possible to eliminate dealing with 23.976 and stay at true 24 all the way through post, but it&#39;s been hard to change the industry, especially since so much post work is still done for television broadcast compared to for theatrical release.</p>

<p>My own experience shooting one of the first 24P movies in 2000 was that since it was for theatrical, I chose 24 instead of 23.976 in the camera menu.  Later when I asked the editor how the sound mix was going, she said &#34;fine, except that the entire reels are drifting slightly out of sync and I&#39;m having to manually adjust them.&#34;  Live and learn... this was probably the first 24P HD feature ever posted in Los Angeles at the time.  Ever since then, I&#39;ve stuck with 23.976 for digital features and haven&#39;t had a problem.  But I think one could choose true 24 fps today and make it work.</p>

<p>As Landon says, DCI requires true 24 so digital movies shot and finished at 23.976 get converted to 24 for the theatrical DCP.</p>


			
		</div></div>
  </body>
</html>
