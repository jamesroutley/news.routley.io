<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thomasbroadley.com/blog/metr-publishes-re-bench/">Original</a>
    <h1>METR publishes RE-Bench</h1>
    
    <div id="readability-page-1" class="page"><article>
      <header>
        
        <p>
          <span data-pagefind-meta="created">2024-12-30</span>
        </p>

        <p>
          <a href="https://thomasbroadley.com/blog/vivaria-metr-s-platform-for-evaluating-ai-agents">←</a>

          <a href="https://thomasbroadley.com/blog/tags/ai-x-risk" data-pagefind-filter="tag">
            AI x-risk
          </a>

          <span>→</span>
        </p>

        <hr/>
      </header>

      <section>
        <p>
          About a month ago, METR published a
          <a href="https://arxiv.org/abs/2411.15114">paper</a> called &#34;RE-Bench:
          Evaluating frontier AI R&amp;D capabilities of language model agents
          against human experts&#34;. RE-Bench evaluates human experts and AI agents
          on machine learning research engineering tasks. Research engineers at
          Anthropic, Google DeepMind, and OpenAI perform these kinds of tasks in
          the course of their jobs.
        </p>
        <p>
          The benchmark&#39;s goal is to measure AI&#39;s ability to improve itself
          without human help. If AI can improve itself, its capabilities could
          increase rapidly, from human-level at some tasks to superhuman at all
          tasks. That scares me! What will happen if humanity builds AI that&#39;s
          smarter than us? It&#39;s hard to predict.
        </p>
        <p>
          RE-Bench includes seven difficult, realistic tasks. To confirm the
          tasks&#39; realism, METR consulted with machine learning professionals in
          academia and industry. To assess the tasks&#39; difficulty, we paid human
          experts to attempt them, allowing up to eight hours per attempt. The
          experts did well on the tasks. However, their solutions left room for
          improvement, even after eight hours.
        </p>
        <p>
          <img src="https://thomasbroadley.com/blog/metr-publishes-re-bench/results.png" alt="Graph comparing human and AI performance on RE-Bench tasks. AI agents perform better than humans on these tasks when we allow two hours (across multiple attempts) to complete the task. At eight or more hours, humans outperform AI"/>
        </p>
        <p>
          From the paper: &#34;We find that agents initially make faster progress
          than humans, but that human experts improve more rapidly with
          additional time.&#34;
        </p>
        <p>
          I&#39;m proud to say that the paper lists me as a contributor. I
          contributed by maintaining
          <a href="https://vivaria.metr.org">Vivaria</a>, the open-source
          software that METR uses to check how well AI agents (and humans)
          perform on the benchmark.
        </p>
        <p>
          I encourage you to read our
          <a href="https://metr.org/blog/2024-11-22-evaluating-r-d-capabilities-of-llms/">blog post</a>
          discussing the paper, or the
          <a href="https://arxiv.org/abs/2411.15114">paper</a> itself.
        </p>
      </section>
    </article></div>
  </body>
</html>
