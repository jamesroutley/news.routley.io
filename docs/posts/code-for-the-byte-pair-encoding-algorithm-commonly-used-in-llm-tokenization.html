<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/karpathy/minbpe">Original</a>
    <h1>Code for the Byte Pair Encoding algorithm, commonly used in LLM tokenization</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is &#34;byte-level&#34; because it runs on UTF-8 encoded strings.</p>
<p dir="auto">This algorithm was popularized for LLMs by the <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">GPT-2 paper</a> and the associated GPT-2 <a href="https://github.com/openai/gpt-2">code release</a> from OpenAI. <a href="https://arxiv.org/abs/1508.07909" rel="nofollow">Sennrich et al. 2015</a> is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.</p>
<p dir="auto">There are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:</p>
<ol dir="auto">
<li><a href="https://github.com/karpathy/minbpe/blob/master/minbpe/base.py">minbpe/base.py</a>: Implements the <code>Tokenizer</code> class, which is the base class. It contains the <code>train</code>, <code>encode</code>, and <code>decode</code> stubs, save/load functionality, and there are also a few common utility functions. This class is not meant to be used directly, but rather to be inherited from.</li>
<li><a href="https://github.com/karpathy/minbpe/blob/master/minbpe/basic.py">minbpe/basic.py</a>: Implements the <code>BasicTokenizer</code>, the simplest implementation of the BPE algorithm that runs directly on text.</li>
<li><a href="https://github.com/karpathy/minbpe/blob/master/minbpe/regex.py">minbpe/regex.py</a>: Implements the <code>RegexTokenizer</code> that further splits the input text by a regex pattern, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. This was introduced in the GPT-2 paper and continues to be in use as of GPT-4.</li>
<li><a href="https://github.com/karpathy/minbpe/blob/master/minbpe/gpt4.py">minbpe/gpt4.py</a>: Implements the <code>GPT4Tokenizer</code>. This class is a light wrapper around the <code>RegexTokenizer</code> (2, above) that exactly reproduces the tokenization of GPT-4 in the <a href="https://github.com/openai/tiktoken">tiktoken</a> library. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations. Note that the parity is not fully complete yet because we do not handle special tokens.</li>
</ol>
<p dir="auto">Finally, the script <a href="https://github.com/karpathy/minbpe/blob/master/train.py">train.py</a> trains the two major tokenizers on the input text <a href="https://github.com/karpathy/minbpe/blob/master/tests/taylorswift.txt">tests/taylorswift.txt</a> (this is the Wikipedia entry for her kek) and saves the vocab to disk for visualization. This script runs in about 25 seconds on my (M1) MacBook.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" tabindex="-1" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>usage</h2>
<p dir="auto">All of the files above are very short and thoroughly commented, and also contain a usage example on the bottom of the file. As a quick example, following along the <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="nofollow">Wikipedia article on BPE</a>, we can reproduce it as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
text = &#34;aaabdaaabac&#34;
tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges
print(tokenizer.encode(text))
# [258, 100, 258, 97, 99]
print(tokenizer.decode([258, 100, 258, 97, 99]))
# aaabdaaabac
tokenizer.save(&#34;toy&#34;)
# writes two files: toy.model (for loading) and toy.vocab (for viewing)"><pre><span>from</span> <span>minbpe</span> <span>import</span> <span>BasicTokenizer</span>
<span>tokenizer</span> <span>=</span> <span>BasicTokenizer</span>()
<span>text</span> <span>=</span> <span>&#34;aaabdaaabac&#34;</span>
<span>tokenizer</span>.<span>train</span>(<span>text</span>, <span>256</span> <span>+</span> <span>3</span>) <span># 256 are the byte tokens, then do 3 merges</span>
<span>print</span>(<span>tokenizer</span>.<span>encode</span>(<span>text</span>))
<span># [258, 100, 258, 97, 99]</span>
<span>print</span>(<span>tokenizer</span>.<span>decode</span>([<span>258</span>, <span>100</span>, <span>258</span>, <span>97</span>, <span>99</span>]))
<span># aaabdaaabac</span>
<span>tokenizer</span>.<span>save</span>(<span>&#34;toy&#34;</span>)
<span># writes two files: toy.model (for loading) and toy.vocab (for viewing)</span></pre></div>
<p dir="auto">The result above is exactly as expected, please see bottom of <a href="https://github.com/karpathy/minbpe/blob/master/minbpe/basic.py">minbpe/basic.py</a> for more details. To use the <code>GPT4Tokenizer</code>, simple example and comparison to <a href="https://github.com/openai/tiktoken">tiktoken</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="text = &#34;hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ðŸ˜‰&#34;

# tiktoken
import tiktoken
enc = tiktoken.get_encoding(&#34;cl100k_base&#34;)
print(enc.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

# ours
from minbpe import GPT4Tokenizer
tokenizer = GPT4Tokenizer()
print(tokenizer.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]"><pre><span>text</span> <span>=</span> <span>&#34;hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ðŸ˜‰&#34;</span>

<span># tiktoken</span>
<span>import</span> <span>tiktoken</span>
<span>enc</span> <span>=</span> <span>tiktoken</span>.<span>get_encoding</span>(<span>&#34;cl100k_base&#34;</span>)
<span>print</span>(<span>enc</span>.<span>encode</span>(<span>text</span>))
<span># [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]</span>

<span># ours</span>
<span>from</span> <span>minbpe</span> <span>import</span> <span>GPT4Tokenizer</span>
<span>tokenizer</span> <span>=</span> <span>GPT4Tokenizer</span>()
<span>print</span>(<span>tokenizer</span>.<span>encode</span>(<span>text</span>))
<span># [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]</span></pre></div>
<p dir="auto">(you&#39;ll have to <code>pip install tiktoken</code> to run).</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-tests" aria-hidden="true" tabindex="-1" href="#tests"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>tests</h2>
<p dir="auto">We use the pytest library for tests. All of them are located in the <code>tests/</code> directory. First <code>pip install pytest</code> if you haven&#39;t already, then:</p>

<p dir="auto">to run the tests.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-todos" aria-hidden="true" tabindex="-1" href="#todos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>todos</h2>
<ul dir="auto">
<li>write more optimized versions, both in Python and/or C/Rust?</li>
<li>handle special tokens? think through...</li>
<li>video coming soon ;)</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-license" aria-hidden="true" tabindex="-1" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">MIT</p>
</article></div></div>
  </body>
</html>
