<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/NullPxl/banrays">Original</a>
    <h1>Show HN: Glasses to detect smart-glasses that have cameras</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><strong>Glasses to detect smart-glasses that have cameras</strong></p>
<p dir="auto">I&#39;m experimenting with 2 main approaches:</p>
<ul dir="auto">
<li><a href="#optics">Optics</a>: classify the camera using light reflections.</li>
<li><a href="#networking">Networking</a>: bluetooth and wi-fi analysis.</li>
</ul>
<p dir="auto">So far fingerprinting specific devices based on bluetooth (BLE) is looking like easiest and most reliable approach. The picture below is the first version, which plays the legend of zelda &#39;secret found&#39; jingle when it detects a BLE advertisement from Meta Raybans.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://blog.jacobvosmaer.nl/NullPxl/banrays/blob/main/banrays_physical_v1.png"><img src="https://blog.jacobvosmaer.nl/NullPxl/banrays/raw/main/banrays_physical_v1.png" alt=""/></a></p>
<p dir="auto">I&#39;m essentially treating this README like a logbook, so it will have my current approaches/ideas.</p>

<p dir="auto">By sending IR at camera lenses, we can take advantage of the fact that the CMOS sensor in a camera reflects light directly back at the source (called &#39;retro-reflectivity&#39; / &#39;cat-eye effect&#39;) to identify cameras.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://blog.jacobvosmaer.nl/NullPxl/banrays/blob/main/irrb.jpg"><img src="https://blog.jacobvosmaer.nl/NullPxl/banrays/raw/main/irrb.jpg" alt=""/></a></p>
<p dir="auto">This isn&#39;t exactly a new idea. Some researchers in 2005 used this property to create &#39;capture-resistant environments&#39; when smartphones with cameras were gaining popularity.</p>
<ul dir="auto">
<li><a href="https://homes.cs.washington.edu/~shwetak/papers/cre.pdf" rel="nofollow">https://homes.cs.washington.edu/~shwetak/papers/cre.pdf</a></li>
</ul>
<p dir="auto">There&#39;s even some recent research (2024) that figured out how to classify individual cameras based on their retro-reflections.</p>
<ul dir="auto">
<li><a href="https://opg.optica.org/oe/fulltext.cfm?uri=oe-32-8-13836" rel="nofollow">https://opg.optica.org/oe/fulltext.cfm?uri=oe-32-8-13836</a></li>
</ul>
<p dir="auto">Now we have a similar situation to those 2005 researchers on our hands, where smart glasses with hidden cameras seem to be getting more popular. So I want to create a pair of glasses to identify these. Unfortunately, from what I can tell most of the existing research in this space records data with a camera and then uses ML, a ton of controlled angles, etc. to differentiate between normal reflective surfaces and cameras.</p>
<p dir="auto">I would feel pretty silly if my solution uses its own camera. So I&#39;ll be avoiding that. Instead I think it&#39;s likely I&#39;ll have to rely on being consistent with my &#39;sweeps&#39;, and creating a good classifier based on signal data. For example you can see here that the back camera on my smartphone seems to produce quick and large spikes, while the glossy screen creates a more prolonged wave.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://blog.jacobvosmaer.nl/NullPxl/banrays/blob/main/ts_plot_labeled.png"><img src="https://blog.jacobvosmaer.nl/NullPxl/banrays/raw/main/ts_plot_labeled.png" alt=""/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://blog.jacobvosmaer.nl/NullPxl/banrays/blob/main/ts_plot_spikes.png"><img src="https://blog.jacobvosmaer.nl/NullPxl/banrays/raw/main/ts_plot_spikes.png" alt=""/></a></p>
<p dir="auto">After getting to test some Meta Raybans, I found that this setup is not going to be sufficient. Here&#39;s a test of some sweeps of the camera-area + the same area when the lens is covered. You can see the waveform is similar to what I saw in the earlier test (short spike for camera, wider otherwise), but it&#39;s wildly inconsistent and the strength of the signal is very weak. This was from about 4 inches away from the LEDs. I didn&#39;t notice much difference when swapping between 940nm and 850nm LEDs.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://blog.jacobvosmaer.nl/NullPxl/banrays/blob/main/ir_rayban_first_sweeps.png"><img src="https://blog.jacobvosmaer.nl/NullPxl/banrays/raw/main/ir_rayban_first_sweeps.png" alt=""/></a></p>
<p dir="auto">So at least with current hardware that&#39;s easy for me to access, this probably isn&#39;t enough to differentiate accurately.</p>
<p dir="auto">Another idea I had is to create a designated sweep &#39;pattern&#39;. The user (wearing the detector glasses) would perform a specific scan pattern of the target. Using the waveforms captured from this data, maybe we can more accurately fingerprint the raybans. For example, sweeping across the targets glasses in a &#39;left, right, up, down&#39; approach. I tested this by comparing the results of the Meta raybans vs some aviators I had lying around. I think the idea behind this approach is sound (actually it&#39;s light), but it might need more workshopping.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://blog.jacobvosmaer.nl/NullPxl/banrays/blob/main/ir_rayban_sweeping_pattern.png"><img src="https://blog.jacobvosmaer.nl/NullPxl/banrays/raw/main/ir_rayban_sweeping_pattern.png" alt=""/></a></p>

<p dir="auto">For prototyping, I&#39;m using:</p>
<ul dir="auto">
<li>Arduino uno</li>
<li>a bunch of 940nm and 850nm IR LEDs</li>
<li>a photodiode as a receiver</li>
<li>a 2222A transistor</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://blog.jacobvosmaer.nl/NullPxl/banrays/blob/main/basicsetup.jpg"><img src="https://blog.jacobvosmaer.nl/NullPxl/banrays/raw/main/basicsetup.jpg" alt=""/></a></p>
<p dir="auto">TODO:</p>
<ul dir="auto">
<li>experiment with sweeping patterns</li>
<li>experiment with combining data from different wavelengths</li>
<li>collimation?</li>
</ul>

<p dir="auto">This has been more tricky than I first thought! My current approach here is to fingerprint the Meta Raybans over Bluetooth low-energy (BLE) advertisements. But, <strong>I have only been able to detect BLE traffic during 1) pairing 2) powering-on</strong>. I sometimes also see the advertisement as they are taken out of the case (while already powered on), but not consistently.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://blog.jacobvosmaer.nl/NullPxl/banrays/blob/main/ble_detect.jpg"><img src="https://blog.jacobvosmaer.nl/NullPxl/banrays/raw/main/ble_detect.jpg" alt=""/></a></p>
<p dir="auto">The goal is to detect them during usage when they&#39;re communicating with the paired phone, but to see this type of directed BLE traffic it seems like I would first need to see the <code>CONNECT_REQ</code> packet which has information as to what which of the communication channels to hop between in sync. I don&#39;t think what I currently have (ESP32) is set up to do this kind of following.</p>
<ul dir="auto">
<li>potentially can use an <a href="https://www.nordicsemi.com/Products/Development-tools/nRF-Sniffer-for-Bluetooth-LE" rel="nofollow">nRF module</a> for this</li>
</ul>
<p dir="auto">For any of the bluetooth classic (BTC) traffic, unfortunately the hardware seems a bit more involved (read: expensive). So if I want to do down this route, I&#39;ll likely need a more clever solution here.</p>
<p dir="auto">When turned on or put into pairing mode (or sometimes when taken out of the case), I can detect the device through advertised manufacturer data and service UUIDs. <code>0x01AB</code> is a Meta-specific SIG-assigned ID (assigned by the Bluetooth standards body), and <code>0xFD5F</code> in the Service UUID is assigned to Meta as well.</p>
<p dir="auto">capture when the glasses are powered on:</p>
<div data-snippet-clipboard-copy-content="[01:07:06] RSSI: -59 dBm
Address: XX:XX:XX:XX:XX:XX
Name: Unknown

META/LUXOTTICA DEVICE DETECTED!
  Manufacturer: Meta (0x01AB)
  Service UUID: Meta (0xFD5F) (0000fd5f-0000-1000-8000-00805f9b34fb)

Manufacturer Data:
  Company ID: Meta (0x01AB)
  Data: 020102102716e4

Service UUIDs: [&#39;0000fd5f-0000-1000-8000-00805f9b34fb&#39;]"><pre><code>[01:07:06] RSSI: -59 dBm
Address: XX:XX:XX:XX:XX:XX
Name: Unknown

META/LUXOTTICA DEVICE DETECTED!
  Manufacturer: Meta (0x01AB)
  Service UUID: Meta (0xFD5F) (0000fd5f-0000-1000-8000-00805f9b34fb)

Manufacturer Data:
  Company ID: Meta (0x01AB)
  Data: 020102102716e4

Service UUIDs: [&#39;0000fd5f-0000-1000-8000-00805f9b34fb&#39;]
</code></pre></div>
<p dir="auto">IEEE assigns certain MAC address prefixes (OUI, &#39;Organizationally Unique Identifier&#39;), but these addresses get randomized so I don&#39;t expect them to be super useful for BLE.</p>
<p dir="auto">Here&#39;s some links to more data if you&#39;re curious:</p>
<ul dir="auto">
<li><a href="https://www.bluetooth.com/wp-content/uploads/Files/Specification/HTML/Assigned_Numbers/out/en/Assigned_Numbers.pdf" rel="nofollow">https://www.bluetooth.com/wp-content/uploads/Files/Specification/HTML/Assigned_Numbers/out/en/Assigned_Numbers.pdf</a></li>
<li><a href="https://gitlab.com/wireshark/wireshark/-/blob/99df5f588b38cc0964f998a6a292e81c7dcf0800/epan/dissectors/packet-bluetooth.c" rel="nofollow">https://gitlab.com/wireshark/wireshark/-/blob/99df5f588b38cc0964f998a6a292e81c7dcf0800/epan/dissectors/packet-bluetooth.c</a></li>
<li><a href="https://www.netify.ai/resources/macs/brands/meta" rel="nofollow">https://www.netify.ai/resources/macs/brands/meta</a></li>
</ul>
<p dir="auto">TODO:</p>
<ul dir="auto">
<li>Read: <a href="https://dl.acm.org/doi/10.1145/3548606.3559372" rel="nofollow">https://dl.acm.org/doi/10.1145/3548606.3559372</a></li>
<li>try active probing/interrogating</li>
</ul>
<hr/>
<p dir="auto">Thanks to Trevor Seets and Junming Chen for their advice in optics and BLE (respectively). Also to Sohail for lending me meta raybans to test with.</p>
</article></div></div>
  </body>
</html>
