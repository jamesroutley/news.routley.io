<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://spectrum.ieee.org/chip-design-controversy">Original</a>
    <h1>Ending an Ugly Chapter in Chip Design</h1>
    
    <div id="readability-page-1" class="page"><div data-elid="2659708634" data-post-url="https://spectrum.ieee.org/chip-design-controversy" data-authors="Samuel K. Moore" data-headline="Ending an Ugly Chapter in Chip Design"><div><p>
	Discussions at chip design conferences rarely get heated. But a year ago at the <a href="https://ispd.cc/ispd2023/index.php" target="_blank">International Symposium on Physical Design</a> (ISPD), things got out of hand. It was described by observers as a “trainwreck” and an “ambush.” The crux of the clash was whether Google’s AI solution to one of chip design’s thornier problems was really better than those of humans or state-of-the-art algorithms. It pitted established male electronic design automation (EDA) experts against two young female <a href="https://spectrum.ieee.org/tag/google">Google</a> computer scientists, and the underlying argument had already led to the firing of one Google researcher.<br/></p><p>
	This year at that same conference, a leader in the field, <a href="https://cse.ucsd.edu/people/faculty-profiles/andrew-b-kahng" target="_blank">IEEE Fellow Andrew Kahng</a>, hoped to put an end to the acrimony once and for all. He and colleagues at the University of California, San Diego, delivered what he called <a href="https://github.com/TILOS-AI-Institute/MacroPlacement#faqs" rel="noopener noreferrer" target="_blank">“an open and transparent assessment” of Google’s reinforcement learning</a> approach. Using Google’s open-source version of its process, called Circuit Training, and reverse-engineering some parts that were not clear enough for Kahng’s team, they set reinforcement learning against a human designer, commercial software, and state-of-the-art academic algorithms. Kahng declined to speak with <em><a href="https://spectrum.ieee.org/">IEEE Spectrum</a></em> for this article, but he spoke to engineers last week at ISPD, which was held virtually.
</p><p>
	In most cases, Circuit Training was not the winner, but it was competitive. That’s especially notable given that the experiments did not allow Circuit Training to use its signature ability—to improve its performance by learning from other chip designs.
</p><p>
	 “Our goal has been clarity of understanding that will allow the community to move on,” he told engineers. Only time will tell whether it worked.
</p><h2>The Hows and the Whens</h2><p>
	The problem in question is called placement. Basically, it is the process of determining where chunks of logic or memory should be placed on a chip in order to maximize the chip’s operating frequency while minimizing its power consumption and the area it takes up. Finding an optimal solution to this puzzle is among the most difficult problems around, with more possible permutations than the game Go.
</p><p>
	But Go was ultimately defeated by a type of AI called deep reinforcement learning, and that’s just what former Google Brain researchers Azalia Mirhoseini and Anna Goldie applied to the placement problem. The scheme, then called Morpheus, treats placing large pieces of circuitry, called macros, as a game, learning to find an optimal solution. (The locations of macros have an outsize impact on the chip’s characteristics. In Circuit Training and Morpheus, a separate algorithm fills in the gaps with the smaller parts, called standard cells. Other methods use the same process for both macros and standard cells.)
</p><p>
	Briefly, this is<strong> how it works</strong>: The chip’s design file starts as what’s called a netlist—which macros and cells are connected to which others according to what constraints. The standard cells are then collected into clusters to help speed up the training process. Circuit Training then starts placing the macros on the chip “canvas” one at a time. When the last one is down, a separate algorithm fills in the gaps with the standard cells, and the system spits out a quick evaluation of the attempt, encompassing the length of the wiring (longer is worse), how densely packed it is (more dense is worse), and how congested the wiring is (you guessed it, worse). Called proxy cost, this acts like the score would in a reinforcement-learning system that was figuring out how to play a video game. The score is used as feedback to adjust the neural network, and it tries again. Wash, rinse, repeat. When the system has finally learned its task, commercial software does a full evaluation of the complete placement, generating the kind of metrics that chip designers care about, such as area, power consumption, and constraints on frequency.
</p><p><img alt="A chart showing how Google&#39;s reinforcement leaning system works." data-rm-shortcode-id="b85ad387ddef89aee43898599e8aaa23" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-chart-showing-how-google-s-reinforcement-leaning-system-works.png?id=33406238&amp;width=980" height="2121" id="d6a41" lazy-loadable="true" src="data:image/svg+xml,%3Csvg%20xmlns=&#39;http://www.w3.org/2000/svg&#39;%20viewBox=&#39;0%200%205000%202121&#39;%3E%3C/svg%3E" width="5000"/><small placeholder="Add Photo Caption...">Google’s reinforcement learning system treats placing large circuit blocks called macros as a game. The agent places one block at a time on the chip canvas. Then a separate algorithm fills in smaller parts called standard cells. The placement is scored according to several metrics, and that score is used as feedback to improve the agent.</small><small placeholder="Add Photo Credit...">IEEE Spectrum</small></p><p>
	Mirhoseini and Goldie published the results and method of Morpheus in <a href="https://www.nature.com/articles/s41586-021-03544-w" target="_blank"><em>Nature</em> in June 2021</a>, following a seven-month review process. (<a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https:/static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03544-w/MediaObjects/41586_2021_3544_MOESM1_ESM.pdf" rel="noopener noreferrer" target="_blank">Kahng was reviewer No. 3</a>.) And the technique was used to design more than one generation of <a href="https://spectrum.ieee.org/heres-how-googles-tpu-v4-ai-chip-stacked-up-in-training-tests" target="_self">Google’s TPU AI accelerator chips</a>. (So yes, data you used today may have been processed by an AI running on a chip partly designed by an AI. But that’s increasingly the case as EDA vendors such as Cadence and Synopsys go <a href="https://news.synopsys.com/2023-03-29-Synopsys-ai-Unveiled-as-Industrys-First-Full-Stack,-AI-Driven-EDA-Suite-for-Chipmakers" rel="noopener noreferrer" target="_blank">all in on AI-assisted chip design</a>.) In January 2022, they released an open-source version, <a href="https://github.com/google-research/circuit_training" rel="noopener noreferrer" target="_blank">Circuit Training, on GitHub</a>. But Kahng and others claim that even this version was not complete enough to reproduce the research.
</p><p>
	In response to the <em>Nature</em> publication, a separate group of engineers, mostly within Google, began research aimed at what they believed to be a better way of comparing reinforcement learning to established algorithms. But this was no friendly rivalry. According to <a href="https://www.wired.com/story/google-brain-ai-researcher-fired-tension/" rel="noopener noreferrer" target="_blank">press reports</a>, its leader Satrajit Chatterjee, repeatedly undermined Mirhoseini and Goldie personally and was fired for it in 2022.
</p><p>
	While Chatterjee was still at Google, his team produced a paper titled “<a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https:/statmodeling.stat.columbia.edu/wp-content/uploads/2022/05/MLcontra.pdf" rel="noopener noreferrer" target="_blank">Stronger Baselines</a>,” critical of the research published in <em>Nature</em>. He sought to have it presented at a conference, but after review by an independent resolution committee, Google refused. After his termination, an early version of the paper was leaked via an <a href="https://twitter.com/CADEXPERT4" rel="noopener noreferrer" target="_blank">anonymous Twitter account</a> just ahead of ISPD in 2022, leading to the public confrontation.
</p><h2>Benchmarks, Baselines, and Reproducibility</h2><p>
	When <em>IEEE Spectrum</em> spoke with EDA experts following ISPD 2022, detractors had three interrelated concerns—benchmarks, baselines, and reproducibility.
</p><p><strong>Benchmarks</strong> are openly available blocks of circuitry that researchers test their new algorithms on. The benchmarks when Google began its work were already about two decades old, and their relevance to modern chips is debated. University of Calgary professor Laleh Behjat compares it to planning a modern city versus planning a 17th-century one. The infrastructure needed for each is different, she says. However, others point out that there is no way for the research community to progress without everyone testing on the same set of benchmarks.
</p><p>
	Instead of the benchmarks available at the time, the <em>Nature</em> paper focused on doing the placement for Google’s TPU, a complex and cutting-edge chip whose design is not available to researchers outside of Google. The leaked “Stronger Baselines” work placed TPU blocks but also used the old benchmarks. While Kahng’s new work also did placements for the old benchmarks, the main focus centered on three more-modern designs, two of which are newly available, including a multicore RISC-V processor.
</p><p><strong>Baselines</strong> are the state-of-the art algorithms your new system competes against. <em>Nature</em> compared a human expert using a commercial tool to reinforcement learning and to the leading academic algorithm of the time, RePlAce. Stronger Baselines contended that the <em>Nature</em> work didn’t properly execute RePlAce and that another algorithm, simulated annealing, needed to be compared as well. (To be fair, simulated annealing results appeared in the addendum to the <em>Nature</em> paper.)
</p><p>
	But it’s the <strong>reproducibility</strong> bit that Kahng was really focused on. He claims that Circuit Training, as it was posted to GitHub, fell short of allowing an independent group to fully reproduce the procedure. So they took it upon themselves to reverse engineer what they saw as missing elements and parameters.
</p><p>
	Importantly, Kahng’s group publicly <a href="https://github.com/TILOS-AI-Institute/MacroPlacement#readme" rel="noopener noreferrer" target="_blank">documented the progress, code, data sets, and procedure</a> as an example of how such work can enhance reproducibility. In a first, they even managed to persuade EDA software companies Cadence and Synopsys to allow the publication of the high-level scripts used in the experiments. “This was an absolute watershed moment for our field,” said Kahng.
</p><p>
	The UCSD effort, which is referred to simply as <a href="https://github.com/TILOS-AI-Institute/MacroPlacement#readme" rel="noopener noreferrer" target="_blank">MacroPlacement</a>, was not meant to be a one-to-one redo of either the <em>Nature</em> paper or the leaked Stronger Baselines work. Besides using modern public benchmarks unavailable in 2020 and 2021, MacroPlacement compares Circuit Training (though not the most recent version) to a commercial tool, <a href="https://community.cadence.com/cadence_blogs_8/b/breakfast-bytes/posts/innovus-mixed-placer" rel="noopener noreferrer" target="_blank">Cadence’s Innovus concurrent macro placer (CMP)</a>, and to a method developed at Nvidia called <a href="https://developer.nvidia.com/blog/autodmp-optimizes-macro-placement-for-chip-design-with-ai-and-gpus/" rel="noopener noreferrer" target="_blank">AutoDMP</a> that is so new it was only publicly introduced at ISPD 2023 minutes before Kahng spoke.
</p><h2>Reinforcement Learning vs. Everybody</h2><p>
	Kahng’s paper reports results on the three modern benchmark designs implemented using two technologies—NanGate45, which is open source, and <a href="https://gf.com/gf-press-release/globalfoundries-introduces-new-12nm-finfet-technology-high-performance-applications/" rel="noopener noreferrer" target="_blank">GF12, which is a commercial GlobalFoundries FinFET process</a>. (The TPU results reported in <em>Nature</em> used even more advanced process technologies.) Kahng’s team measured the same six metrics Mirhoseini and Goldie did in their <em>Nature</em> paper: area, routed wire length, power, two timing metrics, and the previously mentioned proxy cost. (Proxy cost is not an actual metric used in production, but it was included to mirror the <em>Nature</em> paper.) The results were mixed.
</p><p>
	As it did in the original <em>Nature</em> paper, reinforcement learning beat RePlAce on most metrics for which there was a head-to-head comparison. (RePlAce did not produce an answer for the largest of the three designs.) Against a human expert, Circuit Training frequently lost. Versus simulated annealing, the contest was a bit more even. </p><p>
	For these experiments, the big winners were the newest entrants CMP and AutoDMP, which delivered the best metrics in more cases than any other method.
</p><p>
	In the tests meant to match Stronger Baselines, using older benchmarks, both RePlAce and simulated annealing almost always beat reinforcement learning. But these results report only one production metric, wire length, so they don’t present a complete picture, argue Mirhoseini and Goldie.
</p><h2>A Lack of Learning</h2><p>
	Understandably, Mirhoseini and Goldie have their own criticisms of the MacroPlacement work, but perhaps the most important is that it did not use neural networks that had been pretrained on other chip designs, robbing their method of its main advantage. Circuit Training “unlike any of the other methods presented, can learn from experience, producing better placements more quickly with every problem it sees,” they wrote in an email.
</p><p>
	But in the MacroPlacement experiments each Circuit Training result came from a neural network that had never seen a design before. “This is analogous to resetting AlphaGo before each match…and then forcing it to learn how to play Go from scratch every time it faced a new opponent!”
</p><p>
	The results from the <em>Nature</em> paper bear this out, showing that the more blocks of TPU circuitry the system learned from, the better it placed macros for a block of circuitry it had not yet seen. It also showed that a reinforcement-learning system that had been pretrained could produce a placement in 6 hours of the same quality as an untrained one after 40 hours.
</p><div id="rebelltitem2" data-id="2" data-reload-ads="false" data-is-image="False" data-href="https://spectrum.ieee.org/chip-design-controversy/particle-2" data-basename="particle-2" data-post-id="2659708634" data-published-at="1680566200" data-use-pagination="False"><h2>New Controversy?</h2><p>Kahng’s ISPD presentation emphasized a particular discrepancy between the methods described in <em>Nature</em> and those of the open-source version, Circuit Training. Recall that, as a preprocessing step, the reinforcement-learning method gathers up the standard cells into clusters. In Circuit Training, that step is enabled by commercial EDA software that outputs the netlist—what cells and macros are connected to each other—and an initial placement of the components.</p><p>According to Kahng, the existence of an initial placement in the <em>Nature</em> work was unknown to him even as a reviewer of the paper. According to Goldie, generating the initial placement, called physical synthesis, is <a href="https://www.synopsys.com/glossary/what-is-physical-synthesis.html" rel="noopener noreferrer" target="_blank">standard industry practice</a> because it guides the creation of the netlist, the input for macro placers. All placement methods in both <em>Nature</em> and MacroPlacement were given the same input netlists.</p><p>Does the initial placement somehow give reinforcement learning an advantage? Yes, according to Kahng. His group did <a href="https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/Docs/OurProgress#Question1" rel="noopener noreferrer" target="_blank">experiments</a> that fed three different impossible initial placements into Circuit Training and compared them to a real placement. Routed wire lengths for the impossible versions were between 7 and 10 percent worse.</p><p>Mirhoseini and Goldie counter that the initial placement information is used only for clustering standard cells, which reinforcement learning does not place. The macro-placing reinforcement learning portion has no knowledge of the initial placement, they say. What’s more, providing impossible initial placements may be like taking a sledgehammer to the standard cell-clustering step and therefore giving the reinforcement-learning system a false reward signal. “Kahng has introduced a disadvantage, not removed an advantage,” they write.</p><p>Kahng suggests that more carefully designed experiments are forthcoming.</p><h2>Moving On</h2><p>This dispute has certainly had consequences, most of them negative. Chatterjee is locked in a wrongful-termination lawsuit with <a href="https://spectrum.ieee.org/tag/google">Google</a>. Kahng and his team have spent a great deal of time and effort reconstructing work done—perhaps several times—years ago. After spending years fending off criticism from unpublished and unrefereed research, Goldie and Mirhoseini, whose aim was to help improve chip design, have left a field of engineering that has historically struggled to attract female talent. Since August 2022 they’ve been at <a href="https://www.anthropic.com/" rel="noopener noreferrer" target="_blank">Anthropic</a> working on <a href="https://arxiv.org/abs/2212.08073" rel="noopener noreferrer" target="_blank">reinforcement learning for large language models</a>.</p><p>If there’s a bright side, it’s that Kahng’s effort offers a model for open and reproducible research and added to the store of openly available tools to push this part of chip design forward. That said, Mirhoseini and Goldie’s group at Google had already made an <a href="https://github.com/google-research/circuit_training" rel="noopener noreferrer" target="_blank">open-source version of their research</a>, which is not common for industry research and required some nontrivial engineering work.</p><p>Despite all the drama, the use of machine learning generally, and reinforcement learning specifically, in chip design, has only spread. More than one group was able to <a href="https://dl.acm.org/doi/pdf/10.1145/3489517.3530617" rel="noopener noreferrer" target="_blank">build on Morpheus</a> even before it was made open source. And machine learning is assisting in ever-growing aspects of commercial EDA tools, such as those from <a href="https://www.synopsys.com/ai.html" rel="noopener noreferrer" target="_blank">Synopsys</a> and <a href="https://www.cadence.com/en_US/home/tools/digital-design-and-signoff/soc-implementation-and-floorplanning/cerebrus-intelligent-chip-explorer.html?utm_campaign=Cerebrus_GoogleSearch_Adtxt_03_22&amp;utm_source=Google&amp;utm_medium=Adtxt&amp;s_kwcid=AL!14272!3!587746534731!p!!g!!cadence%20machine%20learning&amp;gclid=CjwKCAjw5pShBhB_EiwAvmnNVwIETAtqyH7Ys_TFDfTGeY0R3xjb2bkJMhulfFCX8hqTxo6pPMy7sBoC578QAvD_BwE" rel="noopener noreferrer" target="_blank">Cadence</a>.</p><p>But all that good could have happened without the unpleasantness.</p><p><em>This post was corrected on 4 April. CMP was originally incorrectly characterized as being a new tool. On 5 April context and correction was added about how CT faired against a human and against simulated annealing. A statement regarding the clarity of experiments surrounding the initial placement issue was removed.</em></p><h2>To Probe Further:</h2><p>The <a href="https://github.com/TILOS-AI-Institute/MacroPlacement/tree/main/Docs/OurProgress#readme" rel="noopener noreferrer" target="_blank">MacroPlacement</a> project is extensively documented on GitHub.</p><p>Google’s Circuit Training entry on GitHub is <a href="https://github.com/google-research/circuit_training" target="_blank">here</a>.</p><p>Andrew Kahng documents his involvement with the <em>Nature</em> paper <a href="https://docs.google.com/document/d/1vkPRgJEiLIyT22AkQNAxO8JtIKiL95diVdJ_O4AFtJ8/edit" rel="noopener noreferrer" target="_blank">here</a>. <em>Nature</em> published the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03544-w/MediaObjects/41586_2021_3544_MOESM1_ESM.pdf" rel="noopener noreferrer" target="_blank">peer-review file</a> in 2022.</p><p>Mirhoseini and Goldie’s response to MacroPlacement can be found <a href="https://www.annagoldie.com/home/statement" target="_blank">here</a>.</p></div></div></div></div>
  </body>
</html>
