<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ask-fini/paramount">Original</a>
    <h1>Show HN: Paramount – Human Evals of AI Customer Support</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Paramount lets your expert agents evaluate AI chats, enabling:</p>
<ul dir="auto">
<li>quality assurance</li>
<li>ground truth capturing</li>
<li>automated regression testing</li>
</ul>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://decomposition.al/ask-fini/paramount/blob/main/usage.gif"><img src="https://decomposition.al/ask-fini/paramount/raw/main/usage.gif" alt="Example usage" data-animated-image=""/></a></p>

<ol dir="auto">
<li>Install the package:</li>
</ol>

<ol start="2" dir="auto">
<li>Decorate your AI function:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="@paramount.record()
def my_ai_function(message_history, new_question): # Inputs
    # &lt;LLM invocations happen here&gt;
    new_message = {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: new_question}
    updated_history = message_history + [new_message]
    return updated_history  # Outputs."><pre><span>@<span>paramount</span>.<span>record</span>()</span>
<span>def</span> <span>my_ai_function</span>(<span>message_history</span>, <span>new_question</span>): <span># Inputs</span>
    <span># &lt;LLM invocations happen here&gt;</span>
    <span>new_message</span> <span>=</span> {<span>&#39;role&#39;</span>: <span>&#39;user&#39;</span>, <span>&#39;content&#39;</span>: <span>new_question</span>}
    <span>updated_history</span> <span>=</span> <span>message_history</span> <span>+</span> [<span>new_message</span>]
    <span>return</span> <span>updated_history</span>  <span># Outputs.</span></pre></div>
<ol start="3" dir="auto">
<li>After <code>my_ai_function(...)</code> has run several times, launch the Paramount UI to evaluate results:</li>
</ol>

<p dir="auto">Your SMEs can now evaluate recordings and track accuracy improvements over time.</p>
<p dir="auto">Paramount runs completely offline in your private environment.</p>

<p dir="auto">After installation, run <code>python example.py</code> for a minimal working example.</p>

<p dir="auto">In order to set up successfully, define which input and output parameters represent the chat list used in the LLM.</p>
<p dir="auto">This is done via the <code>paramount.toml</code> configuration file that you add in your project root dir.</p>
<p dir="auto">It will be autogenerated for you with defaults if it doesn&#39;t already exist on first run.</p>
<div dir="auto" data-snippet-clipboard-copy-content="[record]
enabled = true
function_url = &#34;http://localhost:9000&#34;  # The url to your LLM API flask app, for replay

[db]
type = &#34;csv&#34; # postgres also available
	[db.postgres]
	connection_string = &#34;&#34;

[api]
endpoint = &#34;http://localhost&#34; # url and port for paramount UI/API
port = 9001
split_by_id = false # In case you have several bots and want to split them by ID
identifier_colname = &#34;&#34;

[ui]  # These are display elements for the UI

# For the table display - define which columns should be shown
meta_cols = [&#39;recorded_at&#39;]
input_cols = [&#39;args__message_history&#39;, &#39;args__new_question&#39;]  # Matches my_ai_function() example
output_cols = [&#39;1&#39;, &#39;2&#39;]  # 1 and 2 are indexes for llm_answer and llm_references in example above

# For the chat display - describe how your chat structure is set up. This example uses OpenAI format.
chat_list = &#34;output__1&#34;  # Matches output updated_history. Must be a list of dicts to display chat format
chat_list_role_param = &#34;role&#34;  # Key in list of dicts describing the role in the chat
chat_list_content_param = &#34;content&#34;  # Key in list of dicts describing the content"><pre>[<span>record</span>]
<span>enabled</span> = <span>true</span>
<span>function_url</span> = <span><span>&#34;</span>http://localhost:9000<span>&#34;</span></span>  <span><span>#</span> The url to your LLM API flask app, for replay</span>

[<span>db</span>]
<span>type</span> = <span><span>&#34;</span>csv<span>&#34;</span></span> <span><span>#</span> postgres also available</span>
	[<span>db</span>.<span>postgres</span>]
	<span>connection_string</span> = <span><span>&#34;</span><span>&#34;</span></span>

[<span>api</span>]
<span>endpoint</span> = <span><span>&#34;</span>http://localhost<span>&#34;</span></span> <span><span>#</span> url and port for paramount UI/API</span>
<span>port</span> = <span>9001</span>
<span>split_by_id</span> = <span>false</span> <span><span>#</span> In case you have several bots and want to split them by ID</span>
<span>identifier_colname</span> = <span><span>&#34;</span><span>&#34;</span></span>

[<span>ui</span>]  <span><span>#</span> These are display elements for the UI</span>

<span><span>#</span> For the table display - define which columns should be shown</span>
<span>meta_cols</span> = [<span><span>&#39;</span>recorded_at<span>&#39;</span></span>]
<span>input_cols</span> = [<span><span>&#39;</span>args__message_history<span>&#39;</span></span>, <span><span>&#39;</span>args__new_question<span>&#39;</span></span>]  <span><span>#</span> Matches my_ai_function() example</span>
<span>output_cols</span> = [<span><span>&#39;</span>1<span>&#39;</span></span>, <span><span>&#39;</span>2<span>&#39;</span></span>]  <span><span>#</span> 1 and 2 are indexes for llm_answer and llm_references in example above</span>

<span><span>#</span> For the chat display - describe how your chat structure is set up. This example uses OpenAI format.</span>
<span>chat_list</span> = <span><span>&#34;</span>output__1<span>&#34;</span></span>  <span><span>#</span> Matches output updated_history. Must be a list of dicts to display chat format</span>
<span>chat_list_role_param</span> = <span><span>&#34;</span>role<span>&#34;</span></span>  <span><span>#</span> Key in list of dicts describing the role in the chat</span>
<span>chat_list_content_param</span> = <span><span>&#34;</span>content<span>&#34;</span></span>  <span><span>#</span> Key in list of dicts describing the content</span></pre></div>
<p dir="auto">It is also possible to describe references via config but is not shown here for simplicity.</p>
<p dir="auto">See <code>paramount.toml.example</code> for more info.</p>

<p dir="auto">The deeper configuration instructions about the <code>client</code> &amp; <code>server</code> can be seen <a href="https://github.com/ask-fini/paramount/blob/main/paramount/README.md">here</a>.</p>

<p dir="auto">By using <code>Dockerfile.server</code>, you can containerize and deploy the whole package (including the client).</p>
<p dir="auto">With Docker, you will need to mount the <code>paramount.toml</code> file dynamically into the container for it to work.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker build -t paramount-server -f Dockerfile.server . # or make docker-build-server
docker run -dp 9001:9001 paramount-server # or make docker-run-server"><pre>docker build -t paramount-server -f Dockerfile.server <span>.</span> <span><span>#</span> or make docker-build-server</span>
docker run -dp 9001:9001 paramount-server <span><span>#</span> or make docker-run-server</span></pre></div>

<p dir="auto">This project is under <a href="https://github.com/ask-fini/paramount/blob/main/LICENSE">GPL License</a>.</p>
</article></div></div>
  </body>
</html>
