<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.sturdystatistics.com/posts/hnet_part_I/">Original</a>
    <h1>Hypernetworks: Neural Networks for Hierarchical Data</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">





<p>Neural nets assume the world is flat. Hierarchical data reminds us that it isn‚Äôt.</p>
<p>Neural networks are predicated on the assumption that a single function maps inputs to outputs. But in the real world, data rarely fits that mold.</p>
<p>Think about a clinical trial run across multiple hospitals: the drug is the same, but patient demographics, procedures, and record-keeping vary from one hospital to the next. In such cases, observations are grouped into distinct <em>datasets</em>, each governed by hidden parameters. The function mapping inputs to outputs isn‚Äôt universal ‚Äî it changes depending on which dataset you‚Äôre in.</p>
<p>Standard neural nets fail badly in this setting. Train a single model across all datasets and it will blur across differences, averaging functions that shouldn‚Äôt be averaged. Train one model per dataset and you‚Äôll overfit, especially when datasets are small. Workarounds like static embeddings or ever-larger networks don‚Äôt really solve the core issue: they memorize quirks without modeling the dataset-level structure that actually drives outcomes.</p>
<p>This post analyzes a different approach: <strong>hypernetworks</strong> ‚Äî a way to make neural nets <em>dataset-adaptive</em>. Instead of learning one fixed mapping, a hypernetwork learns to generate the parameters of another network based on a dataset embedding. The result is a model that can:</p>
<ul>
<li>infer dataset-level properties from only a handful of points,</li>
<li>adapt to entirely new datasets without retraining, and</li>
<li>pool information across datasets to improve stability and reduce overfitting.</li>
</ul>
<p><img src="https://blog.sturdystatistics.com/posts/hnet_part_I/title-image.png" alt="Title plot for the series, showing results from Parts I and II.  This image compares the performance of isolated neural networks, hypernetworks, and Bayesian networks on different types of datasets: big, noisy, and small.  While any technology will work on a dataset that is big enough, the Bayesian network handles difficult cases much better.  The hypernetwork shows performance in between the isolated neural network and the Bayesian network."/></p>
<p>We‚Äôll build the model step by step, with code you can run, and test it on synthetic data generated from Planck‚Äôs law. Along the way, we‚Äôll compare hypernetworks to conventional neural nets ‚Äî and preview why Bayesian models (covered in <a href="https://blog.sturdystatistics.com/posts/hnet_part_II/index.html">Part II</a>) can sometimes do even better.</p>
<section id="introduction">
<h2 data-anchor-id="introduction">1. Introduction</h2>
<p>In many real-world problems, the data is <em>hierarchical</em> in nature: observations are grouped into related but distinct datasets, each governed by its own hidden properties. For example, consider a clinical trial testing a new drug. The trial spans multiple hospitals and records the dosage administered to each patient along with the patient‚Äôs outcome. The drug‚Äôs effectiveness is, of course, a primary factor determining outcomes‚Äîbut hospital-specific conditions also play a role. Patient demographics, procedural differences, and even how results are recorded can all shift the recorded outcomes. If these differences are significant, treating the data as if it came from a single population will lead to flawed conclusions about the drug‚Äôs effectiveness.</p>
<p>From a machine learning perspective, this setting presents a challenge. The dataset-level properties‚Äîhow outcomes vary from one hospital to another‚Äîare <em>latent</em>: they exist, but they are not directly observed. A standard neural network learns a single, constant map from inputs to outputs, but that mapping is ambiguous here. Two different hospitals, with different latent conditions, would produce different outcomes, even for identical patient profiles. The function becomes well-defined (i.e.¬†single-valued) only once we condition on the dataset-level factors.</p>
<p>To make this concrete, we can construct a toy example which quantifies the essential features we wish to study. Each dataset consists of observations (ùùÇ, y) drawn from a simple function with a hierarchical structure, generated using <em>Planck‚Äôs law:</em></p>
<p><span>\[
y(\nu) = f(\nu; T, \sigma) = \frac{\nu^3}{e^{\nu/T} - 1} + \epsilon(\sigma)
\]</span></p>
<p>where:</p>
<ul>
<li><strong>ùùÇ</strong> is the covariate (frequency),</li>
<li><strong>y</strong> is the response (brightness or flux),</li>
<li><strong>T</strong> is a dataset-specific parameter (temperature), constant within a dataset but varying across datasets, and</li>
<li><strong>Œµ</strong> is Gaussian noise with scale <strong>œÉ</strong>, which is unknown but remains the same across datasets.</li>
</ul>
<p>This could represent pixels in a thermal image. Each pixel in the image has a distinct surface temperature <em>T</em> determining its spectrum, while the noise scale <em>œÉ</em> would be a property of the spectrograph or amplifier, which is consistent across observations.</p>
<p>The key point is that while (ùùÇ, y) pairs are observed and known to the modeler, the dataset-level parameter <em>T</em> is part of the data-generating function, but is unknown to the observer. While the function f(ùùÇ; T) is constant (each dataset follows the same general equation), since each dataset has a different <em>T</em>, the mapping y(ùùÇ) varies from one dataset to the next. This fundamental structure ‚Äî with hidden dataset-specific variables influencing the observed mapping ‚Äî is ubiquitous in real-world problems.</p>
<p>Naively training a single model across all datasets would force the network to ignore these latent differences and approximate an ‚Äúaverage‚Äù function. This approach is fundamentally flawed when the data is heterogeneous. Fitting a separate model for each dataset also fails: small datasets lack enough points for robust learning, and the shared functional structure, along with shared parameters such as the noise scale œÉ, cannot be estimated reliably without pooling information.</p>
<p>What we need instead is a <em>hierarchical model</em> ‚Äî one that accounts for dataset-specific variation while still sharing information across datasets. In the neural network setting, this naturally leads us to <em>meta-learning:</em> models that don‚Äôt just learn a function, but <em>learn how to learn</em> functions.</p>
<section id="takeaway">
<h3 data-anchor-id="takeaway">Takeaway</h3>
<p>Standard neural nets assume one function fits all the data; hierarchical data (which is very common) violates that assumption at a fundamental level. We need models that adapt per-dataset when required, while still sharing the information which is consistent across datasets.</p>
</section>
</section>
<section id="why-standard-neural-networks-fail-with-hierarchical-data">
<h2 data-anchor-id="why-standard-neural-networks-fail-with-hierarchical-data">2. Why Standard Neural Networks Fail with Hierarchical Data</h2>
<p>A standard neural network trained directly on (ùùÇ, y) pairs struggles in this setting because it assumes that <em>one universal function</em> maps inputs to outputs across all datasets. In our problem, however, each dataset follows a different function y(ùùÇ), determined by the hidden dataset-specific parameter <em>T</em>. The single-valued function f(ùùÇ; T) is not available to us because we cannot observe the parameter <em>T</em>. Without explicit access to <em>T</em>, the network cannot know which mapping to use.</p>
<section id="ambiguous-mappings">
<h3 data-anchor-id="ambiguous-mappings">Ambiguous Mappings</h3>
<p>To see why this is a problem, imagine trying to predict a person‚Äôs height without any other information. In a homogeneous population ‚Äî say, all adults ‚Äî simply imputing the mean might do reasonably well. (For example, if our population in question is adult females in the Netherlands, simply guessing the mean would be accurate to within ¬±2.5 inches 68% of the time.) But suppose the data includes both adults and children. A single distribution would be forced to learn an ‚Äúaverage‚Äù height that fits neither group accurately. Predictions using this mean would <em>virtually never</em> be correct.</p>
<p>The same problem arises in our hierarchical setting: since a single function cannot capture all datasets simultaneously, predictions made with an ‚Äúaverage‚Äù function will not work well.</p>
</section>
<section id="common-workarounds-and-why-they-fall-short">
<h3 data-anchor-id="common-workarounds-and-why-they-fall-short">Common Workarounds, and Why They Fall Short</h3>
<ol type="1">
<li><p><strong>Static dataset embeddings:</strong> A frequent workaround is to assign each dataset a unique embedding vector, retrieved from a lookup table. This allows the network to memorize dataset-specific adjustments. However, this strategy does not generalize: when a new dataset arrives, the network has no embedding for it and cannot adapt to the new dataset.</p></li>
<li><p><strong>Shortcut learning:</strong> Another possibility is to simply enlarge the network and provide more data. In principle, the model might detect subtle statistical cues ‚Äî differences in noise patterns or input distributions ‚Äî that indirectly encode the dataset index. But such ‚Äúshortcut learning‚Äù is both inefficient and unreliable. The network memorizes dataset-specific quirks rather than explicitly modeling dataset-level differences. In applied domains, this also introduces bias: for instance, a network might learn to inappropriately use a proxy variable (like zip code or demographic information), producing unfair and unstable predictions.</p></li>
</ol>
</section>
<section id="what-we-actually-need">
<h3 data-anchor-id="what-we-actually-need">What We Actually Need</h3>
<p>These limitations highlight the real requirements for a model of hierarchical data. Rather than forcing a single network to approximate every dataset simultaneously, we need a model that can:</p>
<ol type="1">
<li>Infer dataset-wide properties from only a handful of examples,</li>
<li>Adapt to entirely new datasets without retraining from scratch, and</li>
<li>Pool knowledge efficiently across datasets, so that shared structure (such as the functional form, or the noise scale œÉ) is estimated more robustly.</li>
</ol>
<p>Standard neural networks with a fixed structure simply cannot meet these requirements. To go further, we need a model that adapts dynamically to dataset-specific structure while still learning from the pooled data. Hypernetworks are one interesting approach to this problem.</p>
</section>
<section id="takeaway-1">
<h3 data-anchor-id="takeaway-1">Takeaway</h3>
<p>Workarounds like static embeddings or bigger models don‚Äôt fix the core issue: hidden dataset factors cause the observed mapping from inputs to outputs to be multiply-valued. A neural network, which is inherently single-valued, cannot fit such data. We need a model that (1) infers dataset-wide properties, (2) adapts to new datasets, and (3) pools knowledge across datasets.</p>
</section>
</section>
<section id="dataset-adaptive-neural-networks">
<h2 data-anchor-id="dataset-adaptive-neural-networks">3. Dataset-Adaptive Neural Networks</h2>
<section id="dataset-embeddings">
<h3 data-anchor-id="dataset-embeddings">3.1 Dataset Embeddings</h3>
<p>The first step toward a dataset-adaptive network is to give the model a way to represent dataset-level variation. We do this by introducing a <em>dataset embedding:</em> a latent vector <em>E</em> that summarizes the properties of a dataset as a whole.</p>
<p>We assign each training dataset a learnable embedding vector:</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span># dataset embeddings (initialized randomly &amp; updated during training)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>dataset_embeddings <span>=</span> tf.Variable(</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    tf.random.normal([num_datasets, embed_dim]),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    trainable<span>=</span><span>True</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span># Assign dataset indices for each sample</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>dataset_indices <span>=</span> np.hstack([np.repeat(i, <span>len</span>(v)) <span>for</span> i, v <span>in</span> <span>enumerate</span>(vs)])</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>x_train <span>=</span> np.hstack(vs).reshape((<span>-</span><span>1</span>, <span>1</span>))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y_train <span>=</span> np.hstack(ys).reshape((<span>-</span><span>1</span>, <span>1</span>))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span># Retrieve dataset-specific embeddings</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>E_train <span>=</span> tf.gather(dataset_embeddings, dataset_indices)</span></code></pre></div>
<p>At first glance, this might look like a common embedding lookup, where each dataset is assigned a static vector retrieved from a table ‚Äî but we have already discussed at length why that approach won‚Äôt work here!</p>
<p>During training, these embeddings do in fact act like standard embeddings (and are implemented as such). Like standard embeddings, ours serve to encode dataset-specific properties. The key distinction comes from how we handle the embeddings at <em>inference time:</em> the embeddings remain trainable, even during prediction. When a previously-unseen dataset appears at inference time, we initialize a new embedding for the dataset and optimize it on the fly. This turns the embedding into a function of the dataset itself, not a hard-coded (and constant) identifier. During training, the model learns embeddings that capture hidden factors in the data-generating process (such as the parameter <em>T</em> in our problem). At prediction time, the embedding continues to adapt, allowing the model to represent new datasets that it has never seen before. Such flexibility is crucial for generalization.</p>
</section>
<section id="introducing-the-hypernetwork">
<h3 data-anchor-id="introducing-the-hypernetwork">3.2 Introducing the Hypernetwork</h3>
<p>With dataset embeddings in place, we now need a mechanism to translate those embeddings into meaningful changes in the network‚Äôs behavior. A natural way to do this is with a <em>hypernetwork:</em> a secondary neural network that generates parameters for the main network.</p>
<p>The idea is simple but powerful. Instead of learning a single function f(ùùÇ), we want to learn a <em>family of functions</em> f(ùùÇ; E), parameterized by the dataset embedding <em>E</em>. The hypernetwork takes <em>E</em> as input and produces weights and biases for the first layer of the main network. In this way, dataset-specific information directly shapes how the main network processes its inputs. After the first layer, the remainder of the network is independent of the dataset; in effect, we have factored the family of functions f(ùùÇ; E) into the composition g(ùùÇ; h(E)), and the task is now to learn functions <em>g</em> and <em>h</em> which approximate our data-generating process.</p>
<p>Here is a minimal implementation in Keras:</p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span>def</span> build_hypernetwork(embed_dim):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;Generates parameters for the first layer of the main network&#34;&#34;&#34;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    emb <span>=</span> K.Input(shape<span>=</span>(embed_dim,), name<span>=</span><span>&#39;dataset_embedding_input&#39;</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    l <span>=</span> K.layers.Dense(<span>16</span>, activation<span>=</span><span>&#39;mish&#39;</span>, name<span>=</span><span>&#39;Hyper_L1&#39;</span>)(emb)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    l <span>=</span> K.layers.Dense(<span>32</span>, activation<span>=</span><span>&#39;mish&#39;</span>, name<span>=</span><span>&#39;Hyper_L2&#39;</span>)(l)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span># Generate layer weights (32 hidden units, 1 input feature)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    W <span>=</span> K.layers.Dense(<span>32</span>, activation<span>=</span><span>None</span>, name<span>=</span><span>&#39;Hyper_W&#39;</span>)(l)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    W <span>=</span> K.layers.Reshape((<span>32</span>, <span>1</span>))(W)  <span># Reshape to (32, 1)</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span># Generate biases (32 hidden units)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    b <span>=</span> K.layers.Dense(<span>32</span>, activation<span>=</span><span>None</span>, name<span>=</span><span>&#39;Hyper_b&#39;</span>)(l)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span>return</span> K.Model(inputs<span>=</span>emb, outputs<span>=</span>[W, b], name<span>=</span><span>&#34;HyperNetwork&#34;</span>)</span></code></pre></div>
<p>The hypernetwork transforms the dataset embedding into a set of layer parameters (<strong>W</strong>, <strong>b</strong>). These parameters will replace the fixed weights of the first layer in the main network, giving us a learnable, dataset-specific transformation of the input.</p>
<p>A hypernetwork maps dataset embeddings to neural network parameters. This lets us capture dataset-level variation explicitly, so that each dataset is modeled by its own effective function without training separate networks from scratch. Remarkably, despite this flexibility, <em>all</em> the parameters in the hypernetwork are constant with respect to the dataset. The only dataset-specific information needed to achieve this flexibility is the embedding (4 floats per dataset, in our example).</p>
</section>
<section id="main-network-integration">
<h3 data-anchor-id="main-network-integration">3.3 Main Network Integration</h3>
<p>Now that we have a hypernetwork to generate dataset-specific parameters, we need to integrate them into a main network which models the data. The main network can have any architecture we like; all we need to do is to replace the first fixed linear transformation with a <em>dataset-specific transformation</em> derived from the embedding.</p>
<p>We can do this by defining a custom layer that applies the hypernetwork-generated weights and biases:</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span>class</span> DatasetSpecificLayer(K.layers.Layer):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span>def</span> <span>__init__</span>(<span>self</span>, <span>**</span>kwargs):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span>super</span>(DatasetSpecificLayer, <span>self</span>).<span>__init__</span>(<span>**</span>kwargs)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span>def</span> call(<span>self</span>, inputs):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span>&#34;&#34;&#34; Applies the dataset-specific transformation using generated weights &#34;&#34;&#34;</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        x, W, b <span>=</span> inputs  <span># unpack inputs</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        x <span>=</span> tf.expand_dims(x, axis<span>=-</span><span>1</span>)       <span># Shape: (batch_size, 1, 1)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        W <span>=</span> tf.transpose(W, perm<span>=</span>[<span>0</span>, <span>2</span>, <span>1</span>])  <span># Transpose W to (batch_size, 1, 32)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        out <span>=</span> tf.matmul(x, W)          <span># Shape: (batch_size, 1, 32)</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        out <span>=</span> tf.squeeze(out, axis<span>=</span><span>1</span>)  <span># Shape: (batch_size, 32)</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span>return</span> out <span>+</span> b  <span># Add bias, final shape: (batch_size, 32)</span></span></code></pre></div>
<p>This layer serves as the bridge between the hypernetwork and the main network. Instead of relying on a single, fixed set of weights, the transformation applied to each input is customized for the dataset via its embedding.</p>
<p>With this building block in place, we can define the main network:</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>embed_dim <span>=</span> <span>4</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>hypernet <span>=</span> build_hypernetwork(embed_dim)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span>def</span> build_base_network(hypernet, embed_dim):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34; Main network that takes x and dataset embedding as input &#34;&#34;&#34;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    inp_x <span>=</span> K.Input(shape<span>=</span>(<span>1</span>,), name<span>=</span><span>&#39;input_x&#39;</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    inp_E <span>=</span> K.Input(shape<span>=</span>(embed_dim,), name<span>=</span><span>&#39;dataset_embedding&#39;</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span># Get dataset-specific weights and biases from the hypernetwork</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    W, b <span>=</span> hypernet(inp_E)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span># Define a custom layer using the generated weights</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    l <span>=</span> DatasetSpecificLayer(name<span>=</span><span>&#39;DatasetSpecific&#39;</span>)([inp_x, W, b])</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span># Proceed with the normal dense network</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    l <span>=</span> K.layers.Activation(K.activations.mish, name<span>=</span><span>&#39;L1&#39;</span>)(l)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    l <span>=</span> K.layers.Dense(<span>32</span>, activation<span>=</span><span>&#39;mish&#39;</span>, name<span>=</span><span>&#39;L2&#39;</span>)(l)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    l <span>=</span> K.layers.Dense(<span>32</span>, activation<span>=</span><span>&#39;mish&#39;</span>, name<span>=</span><span>&#39;L3&#39;</span>)(l)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    out <span>=</span> K.layers.Dense(<span>1</span>, activation<span>=</span><span>&#39;exponential&#39;</span>, name<span>=</span><span>&#39;output&#39;</span>)(l)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span>return</span> K.Model(inputs<span>=</span>[inp_x, inp_E], outputs<span>=</span>out, name<span>=</span><span>&#34;BaseNetwork&#34;</span>)</span></code></pre></div>
<p><em>Why exponential activation on the last layer? Outputs from Planck‚Äôs law are strictly positive, and they fall like exp(-x) for large x. This choice therefore mirrors our anticipated solution, and it allows the approximately linear outputs from the Mish activation in the L3 layer to naturally translate into an exponential tail. We have found this choice, motivated by the physics of the dataset, to lead to faster convergence and better generalization in the model. Exponential activations can have convergence issues with large-y values, but our dataset does not contain such values.</em></p>
<p>To recap, the overall process is as follows:</p>
<ol type="1">
<li>The <strong>hypernetwork</strong> generates dataset-specific parameters (<strong>W</strong>, <strong>b</strong>).</li>
<li>The <strong>DatasetSpecificLayer</strong> applies this transformation to the input ùùÇ, producing a transformed representation ùùÇ‚Äô. If the transformation works correctly, all the various datasets should be directly comparable in the transformed space.</li>
<li>The <strong>main network</strong> learns a single universal mapping from transformed inputs ùùÇ‚Äô to the outputs <em>y</em>.</li>
</ol>
<p>By integrating hypernetwork-generated parameters into the first layer, we transform the main network into a system that adapts automatically to each dataset. This allows us to capture dataset-specific structure while still training a single model across all datasets.</p>
</section>
<section id="takeaway-2">
<h3 data-anchor-id="takeaway-2">Takeaway</h3>
<p>Combining dataset embeddings with a hypernetwork allows one single-valued neural network to express a family of functions f(ùùÇ; E) by decomposing it into f(ùùÇ; E) = g(ùùÇ, h(E)) in which <em>g</em> and <em>h</em> are ordinary, single-valued neural networks. The first layer of the main network becomes dataset-specific; the rest behaves like an ordinary feed-forward network and learns a universal mapping on transformed inputs.</p>
</section>
</section>
<section id="training-results">
<h2 data-anchor-id="training-results">4. Training Results</h2>
<p>With the embeddings, base network, and hypernetwork now stitched together, we can now evaluate the model on our test problem. To do this, we train on a collection of 20 synthetic datasets generated from Planck‚Äôs law as described in Section 1. Each dataset has its own temperature parameter <em>T</em>, while the noise scale œÉ is shared across all datasets.</p>
<p>The figure below shows the training results. Each panel shows a distinct dataset from the test. In each panel,</p>
<ul>
<li>the <strong>Blue solid curve</strong> shows the true function derived from Planck‚Äôs law,</li>
<li>the <strong>Black points</strong> shows observed training data,</li>
<li>the <strong>Red dashed curve</strong> shows predictions from the hypernetwork-based model, and</li>
<li>the <strong>Gold dotted curve</strong> shows predictions from a conventional neural network trained separately on each dataset.</li>
</ul>
<div>
<figure>
<p><img src="https://blog.sturdystatistics.com/posts/hnet_part_I/hnet.png"/></p>
<figcaption>Figure 1: Training results for 20 synthetic datasets using the hypernetwork model. Black points are observations, blue curves are the true functions, red dashed curves are hypernetwork predictions, and gold dotted curves are isolated neural networks trained per dataset. The hypernetwork matches isolated networks on large datasets while avoiding overfitting on smaller ones.</figcaption>
</figure>
</div>
<p>Several key patterns are evident:</p>
<ol type="1">
<li><p><strong>Comparable overall accuracy:</strong> In many cases (such as the 1<sup>st</sup> column of the 1<sup>st</sup> row), the hypernetwork‚Äôs predictions (red) are very similar to those of an isolated neural network (gold). This shows that, despite strongly restricting the model and removing ~95% of its parameters, sharing parameters across datasets does not sacrifice accuracy when sufficient data are available.</p></li>
<li><p><strong>Improved stability:</strong> When training data are sparse (such as in the 1<sup>st</sup> column of the 2<sup>nd</sup> row, or the last column of the 3<sup>rd</sup> row), the hypernetwork over-fits considerably less than the isolated neural network. Its predictions remain smoother and closer to the true functional form, while the isolated neural network sometimes strains to fit individual points.</p></li>
<li><p><strong>Pooling across datasets:</strong> By training on all datasets simultaneously, the hypernetwork learns to separate the shared structure [such as the noise scale œÉ, or the underlying functional form f(ùùÇ; T)] from dataset-specific variation (the embedding <em>E</em>). This shared learning stabilizes predictions across the board, but it is especially visible in the panels with particularly noisy data (such as the last column of the 2<sup>nd</sup> row, or the 2<sup>nd</sup> column of the 3<sup>rd</sup> row).</p></li>
</ol>
<section id="takeaway-3">
<h3 data-anchor-id="takeaway-3">Takeaway</h3>
<p>The hypernetwork achieves comparable accuracy to isolated networks when data are plentiful, and superior stability when data are scarce. Its advantages result from pooling information across datasets, allowing the network to capture both shared and dataset-specific structure in a single model.</p>
</section>
</section>
<section id="predictions-for-new-datasets">
<h2 data-anchor-id="predictions-for-new-datasets">5. Predictions for New Datasets</h2>
<p>The training performance of our hypernetwork is encouraging, but the real test is how the model adapts to <em>new datasets it has never seen before</em>. Unlike a conventional neural network ‚Äî which simply applies its learned weights to any new input ‚Äî our model is structured to recognize that each dataset follows a distinct intrinsic function. To make predictions, it must first infer the dataset‚Äôs embedding.</p>
<section id="two-stage-process">
<h3 data-anchor-id="two-stage-process">5.1 Two-Stage Process</h3>
<p>Adapting to a new dataset proceeds in two steps:</p>
<ol type="1">
<li><strong>Optimize the dataset embedding</strong> <em>E‚Äô</em> so that it best explains the observed points.</li>
<li><strong>Use the optimized embedding</strong> <em>E‚Äô</em> to generate predictions for new inputs via the main network.</li>
</ol>
<p>This two-stage pipeline allows the model to capture dataset-specific properties with only a handful of observations, without retraining the entire network.</p>
</section>
<section id="embedding-optimization">
<h3 data-anchor-id="embedding-optimization">5.2 Embedding Optimization</h3>
<p>To infer a dataset embedding, <em>we treat E‚Äô as a</em> <em>trainable parameter</em>. Instead of training all of the network‚Äôs weights from scratch, we optimize only the embedding vector until the network fits the new dataset. Because the embedding is low-dimensional (in this case, just 4 floats), this optimization is efficient and requires little data to converge.</p>
<p>A convenient way to implement this is with a wrapper model that holds a single embedding vector and exposes it as a learnable variable:</p>
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span>class</span> DatasetEmbeddingModel(K.Model):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>   <span>def</span> <span>__init__</span>(<span>self</span>, base_net, embed_dim):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>       <span>super</span>(DatasetEmbeddingModel, <span>self</span>).<span>__init__</span>()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>       <span>self</span>.base_net <span>=</span> base_net</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>       <span>self</span>.E_new <span>=</span> tf.Variable(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>           tf.random.normal([<span>1</span>, embed_dim]), trainable<span>=</span><span>True</span>, dtype<span>=</span>tf.float32</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>       )</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>       <span># for better performance on small datasets, use tensorflow_probability:</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>       <span># self.E_new = tfp.distributions.Normal(loc=0, scale=1).sample((1, embed_dim))</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>   <span>def</span> call(<span>self</span>, x):</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>       <span># Tile E_new across batch dimension so it matches x&#39;s batch size</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>       E_tiled <span>=</span> tf.tile(<span>self</span>.E_new, (tf.shape(x)[<span>0</span>], <span>1</span>))</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>       <span>return</span> <span>self</span>.base_net([x, E_tiled])</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>   <span>def</span> loss(<span>self</span>, y_true, y_pred):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>       mse_loss <span>=</span> K.losses.MSE(y_true, y_pred)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>       reg_loss <span>=</span> <span>0.05</span> <span>*</span> tf.reduce_mean(tf.square(<span>self</span>.E_new))  <span># L2 regularization on E</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>       <span>return</span> mse_loss <span>+</span> reg_loss</span></code></pre></div>
<p>Here, the dataset embedding <em>E‚Äô</em> is initialized randomly, then updated via gradient descent to minimize prediction error on the observed points. Because we are only optimizing a handful of parameters, the process is lightweight and well-suited to small datasets.</p>
<p>By framing the embedding as a trainable parameter, the model can adapt to new datasets efficiently. This strategy avoids retraining the full network while still capturing the dataset-specific variation needed for accurate predictions.</p>
</section>
<section id="generalization">
<h3 data-anchor-id="generalization">5.3 Generalization</h3>
<p>One of the most compelling advantages of this approach is its ability to generalize to entirely new datasets with very little data. In practice, the model can often adapt with as few as ten observed points. This <em>few-shot adaptation</em> works because the hypernetwork has already learned a structured mapping from dataset embeddings to function parameters. When a new dataset arrives, we only need to learn its embedding, rather than fine-tune all of the network‚Äôs weights.</p>
<p>Compared to conventional neural networks ‚Äî which require hundreds or thousands of examples to fine-tune effectively ‚Äî this embedding-based adaptation is far more data-efficient. It allows the model to handle real-world scenarios where collecting large amounts of data for every new dataset is impractical.</p>
</section>
<section id="limitations">
<h3 data-anchor-id="limitations">5.4 Limitations</h3>
<p>Despite these strengths, the hypernetwork approach is not perfect. When we evaluate predictions on <em>out-of-sample datasets</em> ‚Äî datasets generated by the same process, but not included in the training data ‚Äî we observe a noticeable degradation in quality, especially on noisy data, censored data, or on very small datasets, as the following examples show:</p>
<div>
<figure>
<p><img src="https://blog.sturdystatistics.com/posts/hnet_part_I/hnet-preds.png"/></p>
<figcaption>Figure 2: Out-of-sample predictions from the hypernetwork model on challenging datasets. Black points are observations, blue curves are the true functions, and red dashed curves are hypernetwork predictions. While the model adapts reasonably well, performance degrades on censored, noisy, or very small datasets ‚Äî showing its limits beyond the training regime.</figcaption>
</figure>
</div>
<p>On the one hand, this is expected: these out-of-sample datasets are extremely challenging, and no machine learning model can guarantee perfect generalization to entirely unseen functions. On the other hand, the results are a little disappointing after seeing such promising performance on the training data. While they make dataset-specific adaptation possible, the functional forms the hypernetwork learned are not always stable when faced with data from outside the training regime.</p>
<p>Hypernetworks enable few-shot generalization by adapting embeddings instead of retraining networks. However, their predictions degrade out-of-sample, showing that while adaptation works, we may need alternative approaches to achieve greater robustness.</p>
<p>The degradation we see here looks a bit like over-fitting, and it is may be that it is caused by the optimization step we run at inference time: it is possible that some other combination of step size, stopping criteria, regularization, etc. might have produced better results. However, we were not able to find one. Instead, we hypothesize that this degradation is fundamentally caused by maximum-likelihood estimation (<em>optimization</em>, in neural-network terms). Optimization is not only problematic at inference time, but also as a training algorithm: in a future post, we‚Äôll explore why we believe optimization is the wrong paradigm to use in machine learning, and why maximum-likelihood estimates can cause degradation like this at inference time. In the next post we will explore an alternative technique based on Bayesian learning, which avoids optimization altogether. In the Bayesian setting, inference is not optimization but a probabilistic update, which has much better statistical behavior, and better geometric properties in high dimensions.</p>
</section>
<section id="takeaway-4">
<h3 data-anchor-id="takeaway-4">Takeaway</h3>
<p>For new datasets, we only optimize a small embedding <em>E‚Äô</em> (few-shot) instead of fine-tuning the whole network. It adapts quickly ‚Äî but out-of-sample stability can still degrade relative to the training performance.</p>
</section>
</section>
<section id="discussion-next-steps">
<h2 data-anchor-id="discussion-next-steps">6. Discussion &amp; Next Steps</h2>
<p>The hypernetwork approach shows how neural networks can go beyond brute-force memorization and move toward <em>structured adaptation</em>. By introducing dataset embeddings and using a hypernetwork to translate them into model parameters, we designed a network which is able to infer dataset-specific structure, rather than simply averaging across all datasets. This allows the model to generalize from limited data‚Äîa hallmark of intelligent systems.</p>
<p>The results highlight both the strengths and limitations of this strategy:</p>
<section id="strengths">
<h4 data-anchor-id="strengths"><strong>Strengths</strong></h4>
<ul>
<li>Few-shot adaptation: the model adapts well to new datasets with only a handful of observations.</li>
<li>Shared learning: pooling across datasets improves stability and reduces overfitting.</li>
<li>Flexible architecture: the hypernetwork framework can, by applying the same technique, be extended to richer hierarchical structures.</li>
</ul>
</section>
<section id="limitations-1">
<h4 data-anchor-id="limitations-1"><strong>Limitations</strong></h4>
<ul>
<li>Out-of-sample degradation: predictions become unstable for datasets outside the training distribution, especially small, censored, or noisy ones.</li>
<li>Implicit structure: embeddings capture dataset variation, but without explicit priors the model has no way to incorporate explicit knowledge, and it also struggles to maintain consistent functional forms.</li>
</ul>
<p>These tradeoffs suggest that, while hypernetworks are a promising step, they are not perfect, and we can improve upon them. In particular, they lack the <em>explicit probabilistic structure</em> needed to reason about uncertainty and to constrain extrapolation. This motivates a different family of models: <em>Bayesian hierarchical networks</em>.</p>
<p>Bayesian approaches address hierarchical data directly by modeling dataset-specific parameters as latent variables drawn from prior distributions. This explicit treatment of uncertainty often leads to more stable predictions, especially for small or out-of-sample datasets.</p>
<p>The next post in this series will explore Bayesian hierarchical models in detail, comparing their performance to the hypernetwork approach. As a teaser, the figure below shows Bayesian predictions on the same out-of-sample datasets we tested earlier:</p>
<div>
<figure>
<p><img src="https://blog.sturdystatistics.com/posts/hnet_part_I/bayes-pred.png"/></p>
<figcaption>Figure 3: Out-of-sample predictions from a Bayesian hierarchical model on the same datasets as Figure 2. Black points are observations, blue curves are the true functions, red dashed curves show posterior mean predictions, and shaded regions indicate 50% and 80% credible intervals. Unlike the hypernetwork, the Bayesian model extrapolates more stably and provides uncertainty intervals that correctly widen when data are scarce or noisy.</figcaption>
</figure>
</div>
<p>If you compare these out-of-sample predictions to the ones made by the hyper-network, the Bayesian results seem almost magical!</p>
<p>Hypernetworks bring meta-learning into hierarchical modeling, enabling flexible and data-efficient adaptation. But for robustness ‚Äî especially out-of-sample ‚Äî Bayesian models offer distinct advantages. Together, these approaches provide complementary perspectives on how to make machine learning more dependable in the face of hierarchical data.</p>
</section>
<section id="takeaway-5">
<h3 data-anchor-id="takeaway-5">Takeaway</h3>
<p>Hypernetworks bring structured adaptation and few-shot learning, but lack explicit priors and calibrated uncertainty. Next up: <a href="https://blog.sturdystatistics.com/posts/hnet_part_II/index.html">Bayesian hierarchical models to address robustness and uncertainty head-on.</a></p>
<!-- Local Variables: -->
<!-- fill-column: 1000000 -->
<!-- End: -->


</section>
</section>

</main> <!-- /main -->

</div></div>
  </body>
</html>
