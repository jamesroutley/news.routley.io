<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ROCm/ROCm/discussions/4276">Original</a>
    <h1>ROCm Device Support Wishlist</h1>
    
    <div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="11895837" data-target-translation-type="comment">
        <tr>
    <td>
        <p dir="auto">Biggest requests for continued, or official and long term support:</p>
<p dir="auto">MI60 (32GB) - 32GB VRAM, $500 on ebay usually. Know some folks with two of them for Llama 70B + 123B models and mixtral MOE (faster inference/token gen, but big actual model). Needs to be faster/still supported incase other optimizations come down and more platforms are supported by ROCM, and not requiring extremely specific or outdated versions to have support. (Please, also, let us flash MI60s to Pro VII 32GB or some display out+Windows compatible variants to make these less &#39;e-waste&#39;, thanks)</p>
<p dir="auto">People can still run CUDA and cutorch/pytorch llms on both Windows and Linux on their 1080ti/Titan XPs from 2017, why is a 2019, high bandwidth GPU not still given that care and ease of use? For scientific compute, the FP64 perf on the VII/MI60 is very good, and people won&#39;t buy newer higher-FP64 $2000-4000 Radeon Pros because these consumers can&#39;t afford them anyways, this increase in adoption by mindshare and word of mouth is extremely important to prevent non-gaming radeon/instinct owner regret lol</p>
<p dir="auto">Keep supporting the MI100 as well, 2X MI100 can be fast especially vs dual 3090s/4090s w/ larger models, but no point in people advocating for them if they&#39;re functional with all current software and llms, but dropped from ROCM like the MI60/MI50 did for unclear reasons.</p>
<p dir="auto">For consumer GPUs,</p>
<p dir="auto">For APUs/iGPUs, make Strix Halo in Windows a priority if/when it&#39;s given Linux support. Apple people can use EXOLabs software to run large models across thunderbolt 4/5 networked macs with fast, integrated RAM. People should be able to download pytorch, or their own fav llama inferencing client. Telling people who are used to Windows on their laptops, especially for the driver support or other misc. applications to swap OSes, when their competitor laptops don&#39;t need to for the same task is a frustration and will make it less likely people show off how good it is. 64GB to 128GB of 256GB/s or so RAM in the HX Max is great, especially if they are buyers who do not care about the pure CPU performance, but the iGPU intead</p>
    </td>
  </tr>

    </tbody>
  </div></div>
  </body>
</html>
