<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mkone.ai/blog/introducing-mk1">Original</a>
    <h1>MK-1</h1>
    
    <div id="readability-page-1" class="page"><div id="page" role="main">
        
          
<article id="sections" data-page-sections="64b8303ac41a0b266f5cf399">
  
  
    
    


  


<section data-test="page-section" data-section-theme="white-bold" data-section-id="64b8303ac41a0b266f5cf39b" data-controller="SectionWrapperController" data-current-styles="{
&#34;imageOverlayOpacity&#34;: 0.15,
&#34;backgroundWidth&#34;: &#34;background-width--full-bleed&#34;,
&#34;sectionHeight&#34;: &#34;section-height--medium&#34;,
&#34;customSectionHeight&#34;: 10,
&#34;horizontalAlignment&#34;: &#34;horizontal-alignment--center&#34;,
&#34;verticalAlignment&#34;: &#34;vertical-alignment--middle&#34;,
&#34;contentWidth&#34;: &#34;content-width--wide&#34;,
&#34;customContentWidth&#34;: 50,
&#34;sectionTheme&#34;: &#34;white-bold&#34;,
&#34;sectionAnimation&#34;: &#34;none&#34;,
&#34;backgroundMode&#34;: &#34;image&#34;
}" data-current-context="{
&#34;video&#34;: {
&#34;playbackSpeed&#34;: 0.5,
&#34;filter&#34;: 1,
&#34;filterStrength&#34;: 0,
&#34;zoom&#34;: 0,
&#34;videoSourceProvider&#34;: &#34;none&#34;
},
&#34;backgroundImageId&#34;: null,
&#34;backgroundMediaEffect&#34;: null,
&#34;divider&#34;: null,
&#34;typeName&#34;: &#34;blog-side-by-side&#34;
}" data-animation="none">
  
  <div>
    <div>
      
      
      
      
      
      
      <div data-content-field="main-content" data-item-id="">
  <article id="article-">
  
    <div>
      

      <div>
        <div><div data-layout-label="Post Body" data-type="item" id="item-64c9458b5048b2430141ed79"><div><div><div><div><div><div><div data-block-type="2" id="block-21ffb1b5f5c3265ff31b"><div>

<div>
  <p>Do you ever wonder how companies like OpenAI, Anthropic, and Google serve their large language models economically? For instance, they are able to generate dozens of tokens per second for each user while keeping the costs to a fraction of a penny per request. While these feats of engineering are proprietary, you can be sure that they employ every technique available to optimize their inference stack.</p><p><strong>Enter MK-1</strong>. Our mission is to give every company running AI models similar (or better) capabilities as these elite AI powerhouses. We&#39;re obsessed with performance and efficiency, and have developed our own tools that rival anything out there. Today, weâ€™re announcing our first product, MKML. MKML is a software package that can reduce LLM inference costs on GPUs by 2x with just a few lines of Python code. And it is plug and play with popular ecosystems like Hugging Face and PyTorch.</p><p>For a quick demo, hereâ€™s a Llama-2 7B running over twice as fast with MKML compared to the baseline model (FP16) on an RTX 4090 GPU.</p>
</div>



</div></div></div></div><div data-block-type="2" id="block-209fc631e097892a5554"><div>
  


<div>
  <p>Currently, MKML is in closed beta release. If you are interested in becoming an early partner and getting access to new features first, please contact us below. </p><h2>How can MKML help?</h2><p>Suppose you want to run a chatbot on the cloud using a Llama-2 13B model. Despite being one of the smaller Llama models, it requires 26GB (FP16) of memory â€“ just for the parameters! This has two implications:</p><ol data-rte-list="default"><li><p>Loading the model requires a GPU instance with enough memory, such as a pricey A100 40GB.</p></li><li><p>Running the model requires reading all 26GB from GPU memory for each forward pass, and this can impact the speed of token generation.</p></li></ol><p>The key observation is that the modelâ€™s large memory footprint is the critical bottleneck. MKML solves this: we have a one-time procedure that shrinks its size by ~60% while keeping a very high <span data-text-attribute-id="ffd0f9c0-aaa8-4ff6-bd9a-73e8b2dc1ce3"><strong>fidelity</strong></span> to the original model, which we will explain later in this post. So the 13B model shrinks from 26GB all the way down to 10.5GB. And crucially, MKML reduces the inference time for the forward pass by up to <span data-text-attribute-id="dc88a0a1-5047-4992-bc89-810fb4e45aae"><strong>2.3x</strong></span> compared to the base model on the same GPU, and these gains are multiplicative with system-level optimizations like continuous batching.</p><p>Letâ€™s explore two scenarios of how you might leverage MKML to optimize a Llama-2 13B chatbot.</p><h3>Case 1: Cost optimized</h3><p>With our compression, the Llama-2 13B model now fits on a single A10 24GB instance, which is ~45% less expensive than the A100. And incredibly, despite the A10 being less powerful than the A100 in terms of compute and memory bandwidth, MKML token generation on the A10 is still faster than the baseline model on the A100.</p>
</div>



</div></div></div></div><div><div><div data-block-type="5" id="block-2e146a5a32fbfbefb318"><div>









































  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png" data-image="https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png" data-image-dimensions="1948x1219" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png" width="1948" height="1219" alt="" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/87af060b-ed98-41dd-9590-4ba7e852ac92/Blog+Plot+-+Page+1.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"/>
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div></div><div data-block-type="2" id="block-ddc3a1f7201b5947ba84"><div>
  


<div>
  <h3>Case 2: Speed optimized</h3><p>If the budget allows for the A100 instance, MKMLâ€™s performance really starts to shine. In this case, MKML is ~2.0x faster than the baseline, which translates to serving more users. In addition, the memory saved on the A100 from the smaller model can be used to support larger context windows across users.</p>
</div>



</div></div><div><div><div data-block-type="5" id="block-0b0d657ed8b23369224a"><div>









































  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png" data-image="https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png" data-image-dimensions="1954x1369" data-image-focal-point="0.5,0.5" alt="" data-load="false" src="https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png" width="1954" height="1369" alt="" sizes="100vw" srcset="https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/64b82ec2cac59465c6dd7813/194a806d-1ee8-4f24-9eac-7e64399a4a98/Blog+Plot+-+Page+2.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs"/>
                
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div></div><div data-block-type="2" id="block-eae6266d85d12b927053"><div>
  


<div>
  <p>In summary, there are multiple ways MKML can help an engineer optimize their inference stack depending on their use case. In this example we picked a Llama-2 model, but MKML will work out of the box with other popular language models such as Falcon, MPT and GPT-J.</p><h2>MKML is Easy to Integrate</h2><p>Our approach with MKML is to bring production-level performance to the most flexible and widely used ecosystems out there. Here is an example workflow using Hugging Face.</p><p><strong><em>Model Compression</em></strong>: First, we load in an original Hugging Face model, and compress it using one of our MKML model codecs. Here we chose our MK600 codec, which shrinks the modelâ€™s size by ~60%. The compressed model is then saved to disk so it can be later loaded for inference. It&#39;s important to note that model compression is a one-time process and only needs to be repeated if the weights change, say from fine-tuning. It usually takes under a minute for a 7B parameter model.</p>
</div>



</div></div><div data-block-type="44" id="block-3de11b0aa4b3cd6a5e8d"><div><pre><code>from transformers import AutoModelForCausalLM
import mkml

model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)
model = mkml.compress(model, codec=&#34;mk600&#34;)
mkml.save(model, output_path)</code></pre>

</div></div><div data-block-type="2" id="block-559a670b6db215d117b7"><div>

<p><strong><em>Model Inference:</em></strong> Next, we load our model using MKML and use it immediately for inference. You can interact with it the same way you would the Hugging Face model, so it is plug and play with all your normal workflows. The main difference is that it now takes less memory and is more performant ðŸ¤—ðŸ”¥.</p>



</div></div><div data-block-type="44" id="block-48b5e5e812e8d7d7626e"><div><pre><code># Only change required: Load model with mkml instead of Hugging Face

# from transformers import AutoModelForCausalLM
# model = AutoModelForCausalLM.from_pretrained(args.model_path, torch_dtype=torch.float16)

import mkml
model = mkml.load(model_path, device=device)

# ------------------------------------------------------
# Code from this point on is the same as usual 
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

prompt = &#34;What are some differences between a llama and an alpaca?&#34;
input_ids = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids.cuda()
tokens = model.generate(input_ids)
response = tokenizer.decode(tokens[0, input_ids.shape[1]:])</code></pre>

</div></div><div data-block-type="2" id="block-7e2bd3f4ea02ac5926ad"><div>

<div>
  <h2>Benchmarks</h2><p>MKML supports a wide range of model sizes and system configurations. We benchmarked the speed for Llama-2 7B and 13B for different batch sizes and GPUs, and MKML is consistently faster than baseline on a coldstart speed test. Specifically, coldstart measures the rate of token generation during auto-regression, and is a key metric for estimating model latency and throughput. In this test, we generate 128 tokens starting from a single token prompt.</p>
</div>



</div></div><div data-block-type="2" id="block-f4adb299149dec1526de"><div>

<p>Note that for the Llama-2 13B, some baseline FP16 models <strong>do not</strong> even fit on certain GPUs at all due to their large memory footprint. However, MKML models <strong>do</strong> run on these GPUs and we report these numbers accordingly.</p>



</div></div><div data-block-type="2" id="block-40e857dfbec6a2247445"><div>

<p>We also benchmarked model fidelity using a standard perplexity measure for all the different Llama-2 models. The takeaway is that our compressed model is around 0.01 difference from the baseline model, which for all intents and purposes is a negligible difference. Stay tuned for a more detailed analysis on model fidelity where we will introduce our other model codecs with different compression ratios.</p>



</div></div><div data-block-type="23" id="block-e5112e52a1196bb375d3"><div><center>
<table>
<thead>
<tr>
<th>Perplexity (wikitext2)</th>
<th>Llama-2-7B</th>
<th>Llama-2-13B</th>
<th>Llama-2-70B</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16</td>
<td>5.472</td>
<td>4.884</td>
<td>3.319</td>
</tr>
<tr>
<td>mk600</td>
<td>5.485</td>
<td>4.887</td>
<td>3.324</td>
</tr>
</tbody>
</table>
</center></div></div><div data-block-type="2" id="block-8ac6043ec8797b6543e5"><div>

<div>
  <h2>Whatâ€™s next for MK-1?</h2><p>Currently, we believe that MKML offers the best balance between model fidelity, memory footprint and speed with the added benefit of being easy to use. We are just getting started.</p><p>Our long-term vision is to push the performance of AI to the very limits of whatâ€™s physically possible across the entire inference stack. We have an ambitious roadmap that we are excited to share as we make progress. If you are interested in keeping up to date with our journey please sign up below.</p>
</div>



</div></div></div></div></div></div>

        

        
        
          
        
      </div>

      
    </div>
  
</article>

</div>
    </div>
  
  </div>
  
</section>

  
</article>


          

          
            
              

            
          
        
      </div></div>
  </body>
</html>
