<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://yutongbai.com/lvm.html">Original</a>
    <h1>Predict the next frame. Results of training a transformer on &#34;visual sentences&#34;</h1>
    
    <div id="readability-page-1" class="page"><section>
  <div>
    <!-- Abstract. -->
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            We introduce a novel sequential modeling approach which enables learning a Large Vision Model (<b>LVM</b>) without making use of any linguistic data. </p>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div>
      <div>
        <h2>Visual Sentences Enable Unified Visual Data Format.</h2>
        <div>
          <p><img src="https://yutongbai.com/static/images/figure1_final.jpg" alt="Visual Sentences"/></p><p><b>Figure 1. Visual Sentence</b> allow us to format diverse vision data into the unified structure of image sequences.</p>
        </div>
        </div>
    </div>

    <div>
      <div>
        <h2>LVM Shows Scalability Across Model and Data Size.</h2>
        <div>
          <div>
            <p><img src="https://yutongbai.com/static/images/training_loss_plot.jpg" alt="Scalability Part 1" width="400" height="400"/></p><!-- <p class="caption"><b>Figure 2. Training loss for the 300M, 600M, 1B, and 3B models.</b> All models are trained on 420B tokens, which correspond to 1.64B images. The training scales well with model sizes.</p> -->
            <p><b>Figure 2. Training loss for the 300M, 600M, 1B, and 3B models.</b> All models are trained on 420B tokens, which correspond to 1.64B images. The training scales well with model sizes.</p>

            <!-- <p class="total-caption" style="width: 100%; text-align: center;"><b>Total Caption:</b> ThisThisThisThisThisThisThisThisThisThis is the total caption for all four images.</p> -->

          </div>


          
      

        </div>

        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3"></h2>
            <div class="content has-text-justified"> -->
              
              <p><b>Figure 3. Larger LVMs perform better on downstream tasks.</b> We evaluate LVMs of varying sizes on 4 different downstream tasks, following the 5 shot setting on the ImageNet validation set and report the perplexity. We find that perplexity decreases with larger models across all tasks, indicating the strong scalability.</p>
              <!-- </div>

          </div> -->
        <!-- </div> -->

        
      </div>
    </div>
    <div>
      <div>
        <h2>Results, everything in prompts.</h2>
        <div>
          <p><img src="https://yutongbai.com/static/images/videos.jpg" alt="Visual Sentences"/></p><p><b>Frame predictions.</b> LVM predicts the next frame (marked in red) given previous video frames as prompt. The results reveal the LVM can predict the video frames while considering dynamic objects and camera motion.</p>

          <p><img src="https://yutongbai.com/static/images/complex_task_2.jpg" alt="Visual Sentences"/></p><p><b>In and out of distribution prompting examples.</b> Every row is a prompt that contains a sequence of images interleaved with annotations, followed by a query. The last image is predicted by the model (marked in red). The last 5 rows show examples where the query image is out of distribution (painting, sketch, etc) for the task it was trained for. </p>

          <p><img src="https://yutongbai.com/static/images/complex_task_3.jpg" alt="Visual Sentences"/></p><p><b>Compositing &amp; novel tasks.</b> compositing
            several tasks together within a single prompt. Here, we
            demonstrate the rotation task together with the novel key-
            point correspondence task and request the model to continue
            the pattern. </p>



        </div>
        </div>

  </div>
</div></section></div>
  </body>
</html>
