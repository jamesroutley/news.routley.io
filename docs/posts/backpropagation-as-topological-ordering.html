<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eugeneha.ca/articles/backpropagation-as-topological-ordering/">Original</a>
    <h1>Backpropagation as topological ordering</h1>
    
    <div id="readability-page-1" class="page"><article>
<p>2024 April 27</p>

<p>
\(
\def\∂#1/∂#2{\mathchoice
  {\frac{\partial #1}{\partial #2}}
  {{\partial #1}/{\partial #2}}
  {{\partial #1}/{\partial #2}}
  {{\partial #1}/{\partial #2}}}
\newcommand\optensor\otimes
\newcommand\tensor{{\mskip2mu}{\optensor}{\mskip2mu}}
\newcommand\ophadamard\odot
\newcommand\hadamard{{\mskip2mu}{\ophadamard}{\mskip2mu}}
\newcommand\vec[1]{\mathbf{#1}}     % vectorization
\newcommand\upsigma{\unicode{963}}  % upright &#39;σ&#39;
\newcommand\act{\vec\upsigma}       % activation function
\newcommand\sact\sigma              % scalar activation
\newcommand\Dact{\vec{D\upsigma}}
\newcommand\obj\varphi              % objective function
\newcommand\error\varepsilon        % error (derivative)
\newcommand{\activate}{A\rlap{\strut_\lrcorner}}
\newcommand{\backprop}{\llap{\strut_\llcorner}B}
\newcommand{\bw}[1]{(b_{#1},w_{#1})}
\newcommand{\awz}[2]{(a_{#1},w_{#2},z_{#2})}
\newcommand{\ade}[2]{(a_{#1},\delta_{#2},\error_{#2})}
\newcommand{\dF}[1]{\left(\∂F/∂{b_{#1}},\∂F/∂{w_{#1}}\right)}
\)
</p>

<p>
In <i><a href="https://www.bbc.com/news/coroutines-and-backpropagation/">Coroutines and backpropagation</a></i>, we wrote the backpropagation equations (for a feedforward neural network) as a coroutine, and got the backpropagation algorithm as a result. In this article, we revisit the backpropagation equations from the conventional perspective of <i>computational graphs</i>. We again show that the backpropagation algorithm comes for free—in the guise of a canonical <i>topological graph ordering</i>. The resulting implementation is a nice exercise in functional programming.
</p>

<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#recap-of-the-backpropagation-equations">1. Recap of the backpropagation equations</a></li>
<li><a href="#the-backpropagation-graph">2. The backpropagation graph</a></li>
<li><a href="#backpropagation-as-topological-graph-ordering">3. Backpropagation as topological graph ordering</a></li>
<li><a href="#a-functional-programming-implementation-in-python">4. A functional-programming implementation (in Python)</a></li>
<li><a href="#code">Code</a></li>
</ul>
</div>
</div>

<div id="outline-container-recap-of-the-backpropagation-equations">
<h2 id="recap-of-the-backpropagation-equations"><span>1.</span> <a href="#recap-of-the-backpropagation-equations">Recap of the backpropagation equations</a></h2>
<div id="text-recap-of-the-backpropagation-equations">
<p>
We begin with a quick recap of the backpropagation equations, in the notation and terminology of the <a href="https://www.bbc.com/news/coroutines-and-backpropagation/#preliminary-definitions">previous article</a>. Functions are assumed to be differentiable, vectors and matrices are assumed to be real and finite-dimensional.
</p>

<p>
Given a <a id="org15d603c"></a>​<i>training pair</i> \((x,\obj)\), consisting of an <i>input vector</i> \(x\) and an <i>objective function</i> \(\obj\), consider the function
</p>

<p>
\begin{equation*}
F(b_1,w_1,\dots,b_n,w_n)
=\obj\bigl(\act(b_n+w_n\act(\dots\act(b_1+w_1x)\dots))\bigr)
\end{equation*}

</p><p>
of <i>bias</i> vectors \(b_i\) and <i>weight</i> matrices \(w_i\). The <i>activation function</i> \(\act\) is a vectorization of a scalar-valued function \(\sact\). The main problem is to <b>compute the gradient of \(\boldsymbol{F}\)</b>, that is, the partial derivatives of \(F\) with respect to the biases and weights.
</p>

<p>
For \(i=1,\dots,n\), let
</p><p>

\begin{equation}
\left\{
\begin{aligned}
a_0&amp;=x,             \\
z_i&amp;=b_i+w_ia_{i-1},\\
a_i&amp;=\act(z_i),     \\
\end{aligned}
\right.
\label{activation}\tag{A}
\end{equation}

</p><p>
and let
</p><p>

\begin{equation*}
\delta_i=\∂{z_i}/∂{a_{i-1}},\quad
\error_i=\∂F/∂{z_i}\quad\text{(the so-called “error”).}
\end{equation*}

</p><p>
In terms of these quantities, we have the following <a id="org3a6c61d"></a>​<b>backpropagation equations</b> for the partial derivatives of \(F\):
</p>

<div id="org7db3082">
<ol>
<li><p>
A recurrence relation describing the “backward” propagation of the error:
</p>

\begin{equation}
\delta_i=w_i,\quad
\error_i=
\begin{cases}
D\obj(\act(z_n))\hadamard\Dact(z_n)      &amp;(i=n),\\
(\error_{i+1}\delta_{i+1})\hadamard\Dact(z_i)&amp;(i\lt n).
\end{cases}
\label{error-propagation}\tag{B$_1$}
\end{equation}</li>

<li><p>
An expression of the partial derivatives of \(F\) in terms of the error:
</p>

\begin{equation}
\∂F/∂{b_i}=\error_i,\quad
\∂F/∂{w_i}=\error_i\tensor a_{i-1}.
\label{bias-weight-derivative}\tag{B$_2$}
\end{equation}</li>
</ol>

</div>

<p>
Here \(\ophadamard\) denotes the Hadamard product, \(\optensor\) the tensor product, and \(\Dact\) the vectorization of the (scalar) derivative \(D\sact\). (These are standard operations; see the <a href="https://www.bbc.com/news/coroutines-and-backpropagation/#backpropagation-in-a-nutshell">previous article</a> for a refresher.)
</p>

<p>
The <a href="https://www.bbc.com/news/coroutines-and-backpropagation/#backpropagation-in-a-nutshell">proof</a> of the backpropagation equations is straightforward.<sup><a id="fnr.1" href="#fn.1" role="doc-backlink">1</a></sup>
</p>
</div>
</div>

<div id="outline-container-the-backpropagation-graph">
<h2 id="the-backpropagation-graph"><span>2.</span> <a href="#the-backpropagation-graph">The backpropagation graph</a></h2>
<div id="text-the-backpropagation-graph">
<p>
We now come to the main focus of this article: the backpropagation computational graph.
</p>

<p>
A <i>computational graph</i> is a (labelled) directed graph in which each node signifies a quantity that is computed as (the value of) a function of the incoming nodes. A typical node in such a graph, say \(q\), is depicted as follows:
</p><p>

\begin{equation*}
\newcommand{\sw}{\swarrow}
\newcommand{\se}{\searrow}
\begin{array}{c@{\hskip-1em}c@{\hskip-1em}c}
q_1&amp;   &amp;\dots&amp;   &amp;q_m\\
   &amp;\se&amp;     &amp;\sw&amp;   \\[-1em]
   &amp;   &amp;q    &amp;   &amp;   \\[-1em]
   &amp;\sw&amp;     &amp;\se&amp;   \\
\end{array}
\quad
\text{($q=Q(q_1,\dots,q_n)$ for some function $Q$).}
\end{equation*}

</p><p>
The computational graph representing equation \eqref{activation} and the backpropagation equations \eqref{error-propagation}, \eqref{bias-weight-derivative} is as follows:
</p><p>

\begin{equation*}
\begin{CD}
@.                               (b_{i+1},w_{i+1})              \\
@.   \activate                   @VVV                           \\
(a_{i-1},w_i,z_i)           @&gt;&gt;&gt; (a_i,w_{i+1},z_{i+1})          \\
@VVV \backprop                   @.                             \\
(a_{i-1},\delta_i,\error_i) @&lt;&lt;&lt; (a_i,\delta_{i+1},\error_{i+1})\\
@VVV                                                            \\
\left(\∂F/∂{b_i},\∂F/∂{w_i}\right)                              \\
\end{CD}
\end{equation*}

</p><p>
Here \(A\) denotes the (<a href="https://en.wikipedia.org/wiki/Generic_programming">generic</a>) function that performs equation \eqref{activation} to compute \((a_i,w_{i+1},z_{i+1})\) as a function of \((a_{i-1},w_i,z_i)\) and \((b_{i+1},w_{i+1})\), namely
</p>

<p>
\begin{equation*}
A\bigl((a,w,z),(b&#39;,w&#39;)\bigr)
=\bigl(\act(z),w&#39;,b&#39;+w&#39;\act(z)\bigr).
\end{equation*}

</p><p>
Similarly, \(B\) denotes the (generic) function that performs equation \eqref{error-propagation}, namely
</p>

<p>
\begin{equation*}
B\bigl((a&#39;,w&#39;,z&#39;),(a,\delta,\error)\bigr)
=\bigl(a&#39;,w&#39;,(\error\delta)\hadamard\Dact(z&#39;)\bigr).
\end{equation*}

</p><p>
By amalgamating the computational graphs for all biases and weights, we get the following <a id="orgfb55c2a"></a>​<b>backpropagation (computational) graph</b>:
</p><p>

\begin{equation}
\begin{CD}
\bw1   @.      \bw2   @.            @.        \bw n     \\
@VVV \activate @VVV      @.         \activate @VVV      \\
\awz01 @&gt;&gt;&gt;    \awz12 @. {\cdots\ } @&gt;&gt;&gt;      \awz{n-1}n\\
@VVV \backprop @VVV      @.         \backprop @VVV      \\
\ade01 @&lt;&lt;&lt;    \ade12 @. {\cdots\ } @&lt;&lt;&lt;      \ade{n-1}n\\
@VVV           @VVV      @.                   @VVV      \\
\dF1   @.      \dF2   @.            @.        \dF n     \\
\end{CD}
\label{backpropagation-graph}\tag{C}
\end{equation}
</p></div>
</div>

<div id="outline-container-backpropagation-as-topological-graph-ordering">
<h2 id="backpropagation-as-topological-graph-ordering"><span>3.</span> <a href="#backpropagation-as-topological-graph-ordering">Backpropagation as topological graph ordering</a></h2>
<div id="text-backpropagation-as-topological-graph-ordering">
<p>
It is evident that the directed graph \eqref{backpropagation-graph} has a canonical <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological ordering</a>, namely the ordering that advances in step with the <i>unique</i> ordering of the middle rows. This ordering begins with
</p><p>

\begin{equation}
\bw1,\quad\awz01,\quad\dots,\quad\bw n,\quad\awz{n-1}n,
\label{forward-pass}\tag{FP}
\end{equation}

</p><p>
then continues with
</p><p>

\begin{equation}
\ade{n-1}n,\quad\dF n,\quad\dots,\quad\ade01,\quad\dF1.
\label{backward-pass}\tag{BP}
\end{equation}

</p><p>
Here is the main observation: 
</p>

<p>
The canonical topological ordering of the backpropagation graph \eqref{backpropagation-graph}—the ordering \eqref{forward-pass} followed by \eqref{backward-pass}—is tantamount to the backpropagation algorithm.
</p>

<p>
Indeed, the computations performed by traversing the canonical topological ordering coincide with the two passes of the <a href="https://www.bbc.com/news/coroutines-and-backpropagation/#backpropagation-in-a-nutshell">backpropagation algorithm</a>:
</p>

<dl>
<dt><a id="org97a3608"></a>1.  <i>Forward pass</i>​—traverse the sequence \eqref{forward-pass}</dt><dd>Compute \(\awz01\) as a function of \(\bw1\) according to equation \eqref{activation}. Then compute \(\awz{i-1}i\) in succession by applying the function <a href="#orgd0700f8">\(A\)</a>.</dd>

<dt><a id="orgcc4ceb5"></a>2.  <i>Backward pass</i>​—traverse the sequence \eqref{backward-pass}</dt><dd>Compute \(\ade{n-1}n\) as a function of \(\awz{n-1}n\) according to equation \eqref{error-propagation}, and output \(\dF n\) according to equation \eqref{bias-weight-derivative}. Then compute \(\ade{i-1}i\) in (descending) succession by applying the function <a href="#orgc18dc4a">\(B\)</a>, and output \(\dF i\) in turn, according, again, to equation \eqref{bias-weight-derivative}.</dd>
</dl>
</div>
</div>

<div id="outline-container-a-functional-programming-implementation-in-python">
<h2 id="a-functional-programming-implementation-in-python"><span>4.</span> <a href="#a-functional-programming-implementation-in-python">A functional-programming implementation (in Python)</a></h2>
<p>
Each pass described above corresponds to an operation known in functional programming as a <i>scan</i>. We describe this operation in general, then apply it to the two passes of the backpropagation algorithm.
</p>

<div id="outline-container-action-and-scan">
<h3 id="action-and-scan"><a href="#action-and-scan">Action and scan</a></h3>
<div id="text-action-and-scan">
<p>
Underlying the operation of a scan is the notion of an abstract “action.” A <i>right action</i> of a type \(A\) on a type \(X\) is simply a function of type \(X\times A\to X\).<sup><a id="fnr.2" href="#fn.2" role="doc-backlink">2</a></sup> We denote its application by a dot to suggest a kind of generalized product:
</p><p>

\begin{equation*}
r\colon X\times A\to X,\quad
(x,a)\mapsto x\cdot a.
\end{equation*}

</p><p>
Of particular interest is the cumulative right action of a finite sequence \(a_0,a_1,\dots,a_{-1}\):<sup><a id="fnr.3" href="#fn.3" role="doc-backlink">3</a></sup>
</p><p>

\begin{equation*}
x,\quad
x\cdot a_0,\quad
(x\cdot a_0)\cdot a_1,\quad
\dots,\quad
(\dots((x\cdot a_0)\cdot a_1)\cdot{\dots})\cdot a_{-1}.
\end{equation*}

</p><p>
The procedure that produces such a sequence is called <i>left scan</i>, and written “scanl.”<sup><a id="fnr.4" href="#fn.4" role="doc-backlink">4</a></sup> In Python, scanl corresponds to the function <a href="https://docs.python.org/3/library/itertools.html#itertools.accumulate"><var>itertools.accumulate</var></a>.
</p>

<div>
<pre><span>from</span> itertools <span>import</span> accumulate

<span>def</span> <span>scanl</span>(<span>r</span>, <span>x</span>, <span>a_</span>):
    <span>&#34;&#34;&#34;Iterator of x, r(x, a₀), r(r(x, a₀), a₁), ….&#34;&#34;&#34;</span>
    <span>return</span> accumulate(a_, r, initial=x)
</pre>
</div>

<p>
Similarly, a <i>left action</i> of a type \(A\) on a type \(X\) is a function of type \(A\times X\to X\), and the procedure that produces the cumulative left-action sequence is called <i>right scan</i>, and written “scanr.” A minor variation of <var>scanl</var> yields <var>scanr</var>.
</p>

<div>
<pre><span>def</span> <span>scanr</span>(<span>l</span>, <span>x</span>, <span>a_</span>):
    <span>&#34;&#34;&#34;Iterator of x, l(a₋₁, x), l(a₋₂, l(a₋₁, x)), ….&#34;&#34;&#34;</span>
    <span>return</span> accumulate(reverse(a_), flip(l), initial=x)

<span>def</span> <span>reverse</span>(<span>a_</span>):
    <span>&#34;&#34;&#34;Reverse an iterable.&#34;&#34;&#34;</span>
    <span>return</span> <span>reversed</span>(<span>list</span>(a_))

<span>def</span> <span>flip</span>(<span>l</span>):
    <span>&#34;&#34;&#34;Turn a left action into a right action.&#34;&#34;&#34;</span>
    <span>return</span> <span>lambda</span> x, a: l(a, x)
</pre>
</div>
</div>
</div>

<div id="outline-container-forward-and-backward-passes-as-scans">
<h3 id="forward-and-backward-passes-as-scans"><a href="#forward-and-backward-passes-as-scans">Forward and backward passes as scans</a></h3>
<div id="text-forward-and-backward-passes-as-scans">
<p>
The function <a href="#orgd0700f8">\(A\)</a> is a <i>right</i> action of bias-weight pairs \((b&#39;,w&#39;)\) on triples \((a,w,z)\). Therefore, we can dispatch the <a href="#org97a3608">forward pass</a> of the backpropagation algorithm with <var>scanl</var>.
</p>

<div>
<pre><span>def</span> <span>A</span>(<span>awz</span>, <span>bw</span>):
    <span>&#34;&#34;&#34;Equation (A) as a node in graph (C).&#34;&#34;&#34;</span>
    (<span>_</span>, <span>_</span>, <span>z</span>), (<span>b</span>, <span>w</span>) = awz, bw
    <span>return</span> (<span>a</span> := σ(z)), w, b + w@a  <span># @: matrix product</span>

<span>def</span> <span>forward</span>(<span>bw_</span>, <span>x</span>):
    <span>&#34;&#34;&#34;Iterator of the forward pass.&#34;&#34;&#34;</span>
    (<span>b</span>, <span>w</span>), <span>*bw_</span> = bw_
    <span>awz</span> = x, w, b + w@x  <span># Eq. (A)</span>
    <span>return</span> scanl(A, awz, bw_)
</pre>
</div>

<p>
On the other hand, the function <a href="#orgc18dc4a">\(B\)</a> is a <i>left</i> action of triples \((a&#39;,w&#39;,z&#39;)\) on triples \((a,\delta,\error)\). Therefore, we can dispatch the <a href="#orgcc4ceb5">backward pass</a> of the backpropagation algorithm with <var>scanr</var>.
</p>

<div>
<pre><span>def</span> <span>B</span>(<span>awz</span>, <span>aδε</span>):
    <span>&#34;&#34;&#34;Equation (B₁) as a node in graph (C).&#34;&#34;&#34;</span>
    (<span>a</span>, <span>w</span>, <span>z</span>), (<span>_</span>, <span>δ</span>, <span>ε</span>) = awz, aδε
    <span>return</span> a, w, (ε@δ)*Dσ(z)  <span># @/*: matrix/Hadamard product</span>

<span>def</span> <span>backward</span>(<span>awz_</span>, <span>Dφ</span>):
    <span>&#34;&#34;&#34;Iterator of the backward pass.&#34;&#34;&#34;</span>
    <span>*awz_</span>, (<span>a</span>, <span>w</span>, <span>z</span>) = awz_
    <span>aδε</span> = a, w, Dφ(σ(z))*Dσ(z)  <span># Eq. (B₁)</span>
    <span>return</span> scanr(B, aδε, awz_)
</pre>
</div>
</div>
</div>

<div id="outline-container-composing-the-forward-and-backward-passes">
<h3 id="composing-the-forward-and-backward-passes"><a href="#composing-the-forward-and-backward-passes">Composing the forward and backward passes</a></h3>
<div id="text-composing-the-forward-and-backward-passes">
<p>
Finally, the backpropagation algorithm for the partial derivatives of <a href="#org4d4df13">\(F\)</a> is implemented as a composition of the two passes.
</p>


<div>
<pre><span>def</span> <span>gradient_F</span>(<span>bw_</span>, <span>x</span>, <span>Dφ</span>):
    <span>&#34;&#34;&#34;Iterator of (∂F/∂bₙ,∂F/∂wₙ), …, (∂F/∂b₁,∂F/∂w₁).&#34;&#34;&#34;</span>
    <span>aδε_</span> = backward(forward(bw_, x), Dφ)
    <span>return</span> ((ε, tensor(ε, a)) <span>for</span> <span>a</span>, <span>_</span>, <span>ε</span> <span>in</span> aδε_)  <span># Eq. (B₂)</span>
</pre>
</div>

<p>
The argument <var>bw_</var> is an iterable of bias-weight pairs, while the pair of arguments <var>x</var>, <var>Dφ</var> is derived from a <a href="#org15d603c">training pair</a>. The function <var>tensor</var> implements tensor product.<sup><a id="fnr.5" href="#fn.5" role="doc-backlink">5</a></sup> The output of <var>gradient_F</var> is an iterator of pairs of partial derivatives of \(F\).
</p>

<p>
Since the function <var>gradient_F</var> is <i>operationally identical</i> to its namesake in the <a href="https://www.bbc.com/news/coroutines-and-backpropagation/#backpropagation-for-free">previous article</a>, the <a href="https://www.bbc.com/news/coroutines-and-backpropagation/#backpropagation-in-action">tests</a> in that article apply here as well.
</p>
</div>
</div>
</div>

<div id="outline-container-code">
<h2 id="code"><a href="#code">Code</a></h2>
<p>
The code in this article is available as a <a href="https://www.bbc.com/news/articles/topological_backprop.py" type="text/plain;charset=utf-8">Python module</a>. It includes a demonstration and test of the function <a href="#org88a217a"><var>gradient_F</var></a>.
</p>
</div>

<div id="footnotes">
<h2>Notes</h2>
<div id="text-footnotes">

<div><p><sup><a id="fn.1" href="#fnr.1" role="doc-backlink">1</a></sup></p><p>
It is evident from the compositional form of <a href="#org4d4df13">\(F\)</a> that the partial derivatives obey a recurrence relation. The particular form of the recurrence is trivially derived from the chain rule, because, apart from the activation function, every constituent function of <a href="#org4d4df13">\(F\)</a> is linear.
</p></div>

<div><p><sup><a id="fn.2" href="#fnr.2" role="doc-backlink">2</a></sup></p><p>
The notion of an action comes from algebra, where it is ubiquitous. (Important examples include group actions, modules, and linear representations.)
</p></div>

<div><p><sup><a id="fn.3" href="#fnr.3" role="doc-backlink">3</a></sup></p><p>
By convention, indices of a finite sequence are interpreted <i>modulo</i> sequence length. Thus index \(-1\) signifies the last element, index \(-2\) signifies the second-last element, and so on.
</p></div>

<div><p><sup><a id="fn.4" href="#fnr.4" role="doc-backlink">4</a></sup></p><p>
This naming convention is mildly confusing. “Left” refers to the association of terms.
</p></div>

<div><p><sup><a id="fn.5" href="#fnr.5" role="doc-backlink">5</a></sup></p><p>
See the <a href="https://www.bbc.com/news/coroutines-and-backpropagation/#vectorising-activation-and-tensor-product">previous article</a> for an implementation based on a representation of vectors and matrices by <a href="https://numpy.org/doc/stable/reference/arrays.html">NumPy arrays</a>.
</p></div>


</div>
</div>

</article></div>
  </body>
</html>
