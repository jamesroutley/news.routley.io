<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eugeneha.ca/articles/backpropagation-as-topological-ordering/">Original</a>
    <h1>Backpropagation as topological ordering</h1>
    
    <div id="readability-page-1" class="page"><div id="text-the-backpropagation-graph">
<p>
We now come to the main focus of this article: the backpropagation computational graph.
</p>

<p>
A <i>computational graph</i> is a (labelled) directed graph in which each node signifies a quantity that is computed as (the value of) a function of the incoming nodes. A typical node in such a graph, say \(q\), is depicted as follows:
</p><p>

\begin{equation*}
\newcommand{\sw}{\swarrow}
\newcommand{\se}{\searrow}
\begin{array}{c@{\hskip-1em}c@{\hskip-1em}c}
q_1&amp;   &amp;\dots&amp;   &amp;q_m\\
   &amp;\se&amp;     &amp;\sw&amp;   \\[-1em]
   &amp;   &amp;q    &amp;   &amp;   \\[-1em]
   &amp;\sw&amp;     &amp;\se&amp;   \\
\end{array}
\quad
\text{($q=Q(q_1,\dots,q_n)$ for some function $Q$).}
\end{equation*}

</p><p>
The computational graph representing equation \eqref{activation} and the backpropagation equations \eqref{error-propagation}, \eqref{bias-weight-derivative} is as follows:
</p><p>

\begin{equation*}
\begin{CD}
@.                               (b_{i+1},w_{i+1})              \\
@.   \activate                   @VVV                           \\
(a_{i-1},w_i,z_i)           @&gt;&gt;&gt; (a_i,w_{i+1},z_{i+1})          \\
@VVV \backprop                   @.                             \\
(a_{i-1},\delta_i,\error_i) @&lt;&lt;&lt; (a_i,\delta_{i+1},\error_{i+1})\\
@VVV                                                            \\
\left(\∂F/∂{b_i},\∂F/∂{w_i}\right)                              \\
\end{CD}
\end{equation*}

</p><p>
Here \(A\) denotes the (<a href="https://en.wikipedia.org/wiki/Generic_programming">generic</a>) function that performs equation \eqref{activation} to compute \((a_i,w_{i+1},z_{i+1})\) as a function of \((a_{i-1},w_i,z_i)\) and \((b_{i+1},w_{i+1})\), namely
</p>

<p>
\begin{equation*}
A\bigl((a,w,z),(b&#39;,w&#39;)\bigr)
=\bigl(\act(z),w&#39;,b&#39;+w&#39;\act(z)\bigr).
\end{equation*}

</p><p>
Similarly, \(B\) denotes the (generic) function that performs equation \eqref{error-propagation}, namely
</p>

<p>
\begin{equation*}
B\bigl((a&#39;,w&#39;,z&#39;),(a,\delta,\error)\bigr)
=\bigl(a&#39;,w&#39;,(\error\delta)\hadamard\Dact(z&#39;)\bigr).
\end{equation*}

</p><p>
By amalgamating the computational graphs for all biases and weights, we get the following <a id="orgfb55c2a"></a>​<b>backpropagation (computational) graph</b>:
</p><p>

\begin{equation}
\begin{CD}
\bw1   @.      \bw2   @.            @.        \bw n     \\
@VVV \activate @VVV      @.         \activate @VVV      \\
\awz01 @&gt;&gt;&gt;    \awz12 @. {\cdots\ } @&gt;&gt;&gt;      \awz{n-1}n\\
@VVV \backprop @VVV      @.         \backprop @VVV      \\
\ade01 @&lt;&lt;&lt;    \ade12 @. {\cdots\ } @&lt;&lt;&lt;      \ade{n-1}n\\
@VVV           @VVV      @.                   @VVV      \\
\dF1   @.      \dF2   @.            @.        \dF n     \\
\end{CD}
\label{backpropagation-graph}\tag{C}
\end{equation}
</p></div></div>
  </body>
</html>
