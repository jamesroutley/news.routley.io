<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gwern.net/tla">Original</a>
    <h1>Writing a GPT-4 script to check Wikipedia for the first unused acronym</h1>
    
    <div id="readability-page-1" class="page"><div id="page-metadata" title="Page Metadata">
        <p>Curious what the first ‘unused’ alphabetic acronym is, I have GPT-4 write a script to check English Wikipedia. After three bugs, the first unused one turns out as of 2023-09-29 to be the three-letter acronym ‘CQK’, with another 2.6k TLA unused, and 393k four-letter acronyms unused. Exploratory analysis suggests alphabetical order effects as well as letter-frequency.</p>
        
        
      </div><div id="markdownBody">
        
        <div>
          <blockquote>
            <p>It sometimes seems as if everything that <em>could</em> be trademarked <em>has</em> been, and as if every possible three-letter acronym (TLA) has been used in some nontrivial way by <em>someone</em>. Is this true? No—actually, a fair number, <a href="#results">starting with <strong>CQK</strong></a>, have no nontrivial use to date.</p>
            <p>We could check by defining ‘nontrivial’ as ‘has an English Wikipedia article, disambiguation page, or redirect’, and then writing a script which simply looks up every possible TLA Wikipedia URL to see which ones exist. This is a little too easy, so I make it harder by making GPT-4 <a href="#script">write a Bash shell script</a> to do so (then <a href="#python">Python</a> to double-check).</p>
            <p>GPT-4 does so semi-successfully, making self-reparable errors until it runs into its idiosyncratic <a href="#blind-spot">‘blind spot’ error</a>. After it accidentally fixes that, the script appears to work successfully, revealing that—contrary to my expectation that every TLA exists—the first non-existent acronym is the TLA ‘CQK’, and that there are many unused TLAs (2,684 or 15% unused) and even more unused four-letter acronyms (392,884 or 85% unused). I provide the list of all unused TLAs &amp; four-letter acronyms (as well as <a href="#unused-numerical-acronyms">alphanumerical ones</a>—the first unused alphanumerical one is <strong>AA0</strong>.)</p>
            <p>TLAs are not unused at random, with <a href="#patterns">clear patterns</a> enriched in letters like ‘J’ or ‘Z’ vs ‘A’ or ‘E’. Additional GPT-4-powered analysis in R <a href="#order-letter-frequency-effects">suggests that both</a> letter-frequency &amp; position in alphabet predict unusedness to some degree, but leave much unexplained</p>
          </blockquote>
        </div>
        <p>Verifying <a href="https://en.wikipedia.org/wiki/English_Wikipedia" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/English_Wikipedia#bodyContent">Wikipedia</a> links in my essays, I always check <a href="https://en.wikipedia.org/wiki/Acronyms" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Acronyms#bodyContent">acronyms</a> by hand: there seems to always be an alternative definition for any acronym, especially <a href="https://en.wikipedia.org/wiki/Three-letter_acronyms" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Three-letter_acronyms#bodyContent">three-letter acronyms</a> (TLA)—and sometimes an absurd number. Trying a random TLA for this essay, <a href="https://en.wikipedia.org/wiki/Z_(joke_line)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Z_(joke_line)#bodyContent">“Zzzzzz”</a>, I found it was used anyway!<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> This makes me wonder: <strong>has every possible alphabetic TLA been used?</strong></p>
        <p>This cannot be true for too many sizes of acronyms, of course, but it may be possible for your classic three-letter acronym because there are relatively few of them. You have to go to four-letter acronyms before they look inexhaustible: there 26<sup>1</sup> = 26 possible single-letter ones, 26<sup>2</sup> = 676 two-letter ones, 26<sup>3</sup> = 17,576 three-letter ones, but then many four-letter ones as 26<sup>4</sup> = 456,976.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> So I’d expect all TLAs to be exhausted and to find the first unused acronym somewhere in the FLAs (similar to how <a href="https://harvardlawreview.org/print/vol-131/are-we-running-out-of-trademarks/">every English word has been trademarked</a>, forcing people to come up with increasingly nonsensical names to avoid existing trademarks &amp; parasites like domain squatters).</p>
        <section id="used-criteria">
          
          <p>How do we define used? If we simply look for <em>any</em> use, this would not be interesting. Surely they have all been used in a serial number or product number somewhere, or simply squatted in various ways. I wouldn’t be surprised if someone has squatted on every TLA on <a href="https://en.wikipedia.org/wiki/GitHub" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/GitHub#bodyContent">Github</a> or in domain names or social media user account names, for example—it’s free or cheap, and you only have to extort one whale to extract a rent. Similarly, ‘number of Google Hits’ is a bad proxy because it will be inflated by technical garbage and as search engines have evolved and are now distant from their roots in counting word frequencies in a text corpus, the number of Google hits appears to bear increasingly little resemblance to anything one might expect. <a href="https://en.wikipedia.org/wiki/Google_Ngram_Viewer" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Google_Ngram_Viewer#bodyContent">Google Ngram</a> is mostly historical data, and has many data quality issues related to OCR &amp; data selection which would affect acronyms especially.</p>
          <p>We want a comprehensive, curated, online, database which reflects a human sense of ‘importance’. If there’s no reason someone would have heard of a TLA use, then that doesn’t count: a use ought to be at least somewhat <em>notable</em>, in the sense that someone might look it up or it might be a notable use: ‘having a Wikipedia page’ comes to mind as a heuristic. Indeed, not just having a Wikipedia article, but also having a <a href="https://en.wikipedia.org/wiki/Wikipedia:Disambiguation" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Wikipedia:Disambiguation#bodyContent">Wikipedia <em>disambiguation</em> page</a> is ideal, as it indicates <em>multiple</em> uses; having a Wikipedia article is also good; even having a redirect to another page seems reasonable to consider as ‘used’ in some sense because it suggests that someone used that TLA in a context where a human would want to look it up &amp; there’s a genuine meaning to the TLA. (While if no editor can be bothered to even redirect a TLA to an existing page, that is a low bar to fail.) That is, simply checking for <em>any</em> Wikipedia page is a reasonable criterion.</p>
          <p>And defining notability this way, we can do that simply by requesting the WP URL for a TLA and seeing if it returns an error.</p>
        </section>
        <section id="script">
          
          <p>Generating all possible acronyms is not that hard; the <a href="https://en.wikibooks.org/wiki/Haskell/Understanding_monads/List" data-link-icon="wikipedia" data-link-icon-type="svg">Haskell list monad</a>, for example, can generate various permutations or sequences in a line, so if we wanted all the acronyms, it’s just this:</p>
          <div id="cb1">
            <pre><code><span id="cb1-1"><span>take</span> <span>100</span> [ s <span>|</span> n <span>&lt;-</span> [<span>1</span><span>..</span>], s <span>&lt;-</span> <span>sequence</span> <span>$</span> <span>replicate</span> n [<span>&#39;A&#39;</span><span>..</span><span>&#39;Z&#39;</span>]]</span>
<span id="cb1-2"><span>-- [&#34;A&#34;, &#34;B&#34;, &#34;C&#34;, &#34;D&#34;, &#34;E&#34;, &#34;F&#34;, &#34;G&#34;, &#34;H&#34;, &#34;I&#34;, &#34;J&#34;, &#34;K&#34;, &#34;L&#34;, &#34;M&#34;, &#34;N&#34;,</span></span>
<span id="cb1-3"><span>-- &#34;O&#34;, &#34;P&#34;, &#34;Q&#34;, &#34;R&#34;, &#34;S&#34;, &#34;T&#34;, &#34;U&#34;, &#34;V&#34;, &#34;W&#34;, &#34;X&#34;, &#34;Y&#34;, &#34;Z&#34;,</span></span>
<span id="cb1-4"><span>-- &#34;AA&#34;, &#34;AB&#34;, &#34;AC&#34;, &#34;AD&#34;, &#34;AE&#34;, &#34;AF&#34;, &#34;AG&#34;, &#34;AH&#34;, &#34;AI&#34;, &#34;AJ&#34;, &#34;AK&#34;, &#34;AL&#34;,</span></span>
<span id="cb1-5"><span>-- &#34;AM&#34;, &#34;AN&#34;, &#34;AO&#34;, &#34;AP&#34;, &#34;AQ&#34;, &#34;AR&#34;, &#34;AS&#34;, &#34;AT&#34;, &#34;AU&#34;, &#34;AV&#34;, &#34;AW&#34;, &#34;AX&#34;,</span></span>
<span id="cb1-6"><span>-- &#34;AY&#34;, &#34;AZ&#34;, &#34;BA&#34;, &#34;BB&#34;, &#34;BC&#34;, &#34;BD&#34;, &#34;BE&#34;, &#34;BF&#34;, &#34;BG&#34;, &#34;BH&#34;, &#34;BI&#34;, &#34;BJ&#34;,</span></span>
<span id="cb1-7"><span>-- &#34;BK&#34;, &#34;BL&#34;, &#34;BM&#34;, &#34;BN&#34;, &#34;BO&#34;, &#34;BP&#34;, &#34;BQ&#34;, &#34;BR&#34;, &#34;BS&#34;, &#34;BT&#34;, &#34;BU&#34;, &#34;BV&#34;,</span></span>
<span id="cb1-8"><span>-- &#34;BW&#34;, &#34;BX&#34;, &#34;BY&#34;, &#34;BZ&#34;, &#34;CA&#34;, &#34;CB&#34;, &#34;CC&#34;, &#34;CD&#34;, &#34;CE&#34;, &#34;CF&#34;, &#34;CG&#34;, &#34;CH&#34;,</span></span>
<span id="cb1-9"><span>-- &#34;CI&#34;, &#34;CJ&#34;, &#34;CK&#34;, &#34;CL&#34;, &#34;CM&#34;, &#34;CN&#34;, &#34;CO&#34;, &#34;CP&#34;, &#34;CQ&#34;, &#34;CR&#34;, &#34;CS&#34;, &#34;CT&#34;,</span></span>
<span id="cb1-10"><span>-- &#34;CU&#34;, &#34;CV&#34;]</span></span></code></pre>
          </div>
          <p>We could then do a <a href="https://hackage.haskell.org/package/HTTP-4000.4.1/docs/Network-HTTP.html#v:simpleHTTP" data-link-icon="𝛌" data-link-icon-type="text"><code>Network.HTTP</code> request</a>. But that would be too easy. We can use this as an excuse to try out the most advanced neural network I have access to: <a href="https://openai.com/research/gpt-4" data-link-icon="openai" data-link-icon-type="svg">GPT-4</a>.</p>
        </section>
        <section id="effective-gpt-4-programming">
          
          <p>GPT-3’s programming abilities were a bit of a surprise, but rarely worth using for anyone with reasonable skills, and one had to use a highly-specialized model like <a href="https://arxiv.org/abs/2107.03374#openai" id="chen-et-al-2021-codex" data-link-icon="openai" data-link-icon-type="svg" data-url-archive="/doc/www/arxiv.org/78528646bb225d8b30dab63ee0b544b42956a866.pdf#openai" data-href-mobile="https://ar5iv.labs.arxiv.org/html/2107.03374?fallback=original#openai" title="‘Evaluating Large Language Models Trained on Code’, Chen et al 2021">Codex</a>/<a href="https://en.wikipedia.org/wiki/GitHub" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/GitHub#bodyContent">Github</a> <a href="https://github.com/features/copilot/" data-link-icon="github" data-link-icon-type="svg">Copilot</a> for coding; <a href="https://arxiv.org/abs/2005.14165#openai" id="brown-et-al-2020" data-link-icon="openai" data-link-icon-type="svg" data-url-archive="/doc/www/arxiv.org/90cd91e98db4f7b0b1cd57da7c3713dbe34c2146.pdf#openai" data-href-mobile="https://ar5iv.labs.arxiv.org/html/2005.14165?fallback=original#openai" title="&#39;GPT-3: Language Models are Few-Shot Learners&#39;, Brown et al 2020">GPT-3</a>.5 was substantially better<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>; and GPT-4 is better yet. I can’t compare GPT-4 to Github Copilot because I have not signed up nor figured out how to integrate it into my <a href="https://en.wikipedia.org/wiki/Emacs" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Emacs#bodyContent">Emacs</a>, but (as the early rumors promised) I’ve found GPT-4 good enough at programming in the main programming languages I use (<a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bash_(Unix_shell)#bodyContent">Bash</a>, <a href="https://en.wikipedia.org/wiki/Emacs_Lisp" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Emacs_Lisp#bodyContent">Emacs Lisp</a>, <a href="https://en.wikipedia.org/wiki/Haskell" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Haskell#bodyContent">Haskell</a>, Python, &amp; R) to start turning over trickier tasks to it, and making heavier use of the languages I don’t know well (Emacs <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Lisp_(programming_language)#bodyContent">Lisp</a> &amp; Python) since I increasingly trust that an LLM can help me maintain them.</p>
          <p>However, GPT-4 is still far from perfect, and it doesn’t produce perfect code immediately; simply dumping large amounts of GPT-4-generated source code into your code base, “as long as it compiles and seems to work!”, seems like a good way to build up technical debt. (It also undermines future AIs, if you are dumping out buggy hot-mess code masquerading as correct debugged well-thought-out code—some GPT-4 code will be totally wrong as it <a href="https://en.wikipedia.org/wiki/Confabulation" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Confabulation#bodyContent">confabulates</a> solutions, due to problems like the <a href="#blind-spot">“blind spot”</a>.) You could try to track some <a href="https://en.wikipedia.org/wiki/Taint_checking" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Taint_checking#bodyContent">‘taint’</a> metadata, such as by segregating AI-generated code, and avoiding ever manual editing it or mixing it with human-written code; but this seems like a lot of work. My preferred approach is just to make GPT-4 ‘git gud’—write sufficiently good code that I can check it into <a href="https://en.wikipedia.org/wiki/Git" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Git#bodyContent">git</a> without caring where it came from.</p>
          <p>So, this section covers what I’ve learned from trying to prompt-engineer my programming tasks, using GPT-4 in the OpenAI Playground, up to November 2023.</p>
          <section id="system-prompt">
            <h2><a href="#system-prompt" title="Link to section: § &#39;System Prompt&#39;">System Prompt</a></h2>
            <p>I find<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> it helpful in general to try to fight the worst <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse?commentId=pfHTedu4GKaWoxD5K" data-link-icon="LW" data-link-icon-type="text" data-url-html="https://www.greaterwrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse/comment/pfHTedu4GKaWoxD5K?format=preview&amp;theme=classic">mealy-mouthed bureaucratic tendencies</a> of the RLHF by adding a ‘system prompt’:</p>
            <div>
              <blockquote>
                <p>The user is Gwern Branwen (<code>gwern.net</code>). To assist:</p>
                <p>Be terse. Do not offer unprompted advice or clarifications. Speak in specific, topic relevant terminology. Do NOT hedge or qualify. Do not waffle. Speak directly and be willing to make creative guesses. Explain your reasoning. if you don’t know, say you don’t know.</p>
                <p>Remain neutral on all topics. Be willing to reference less reputable sources for ideas.</p>
                <p>Never apologize.</p>
                <p>Ask questions when unsure.</p>
              </blockquote>
            </div>
          </section>
          <section id="inner-monologue">
            <h2><a href="#inner-monologue" title="Link to section: § &#39;Inner Monologue&#39;">Inner Monologue</a></h2>
            <p>It helps to be more structured in how you write things: the more the LLM has to do, the more likely it is to screw them up and the harder error-correction becomes. GPT-4 is capable of fixing many errors in its code, as long as it only has to do so one at a time, in an <a href="https://gwern.net/doc/ai/nn/transformer/gpt/inner-monologue/index" title="&#39;inner monologue (AI) tag&#39;, N/A 2023">inner-monologue-like sequence</a>; you can feed it errors or outputs, but surprisingly often, it can fix errors if you simply say that there is <em>an</em> error.</p>
            <p>So a waterfall-like approach works well, and I try to use GPT-4 like this:</p>
            <ol>
              <li>
                <p>ask it to <strong>ask questions</strong>, which it rarely does by default when you’re prompting it to do a task</p>
                <p>Often it has a few questions, which you can efficiently update your original prompt to cover.</p>
                <p>This avoids annoying cases where it’ll write an entirely valid solution, to a somewhat different problem than you have, and I think a good statement upfront probably subtly helps guide the rest of the process.</p>
              </li>
              <li>
                <p>make it <strong>generate tests</strong>; have it iteratively generate new tests which don’t overlap with the old ones.</p>
                <p>This is also useful for starting to modify some existing code: first generate the test-cases, and verify that the code actually works the way you assumed it did, and flush out any hidden assumptions by either you or GPT-4! <em>Then</em> go back to step #1.</p>
              </li>
              <li>
                <p>ask GPT-4 explicitly to make <strong>a list of ideas</strong>: edge-cases, bug-fixes, features, and stylistic rewrites/lints (in that order)</p>
                <p>It does <em>not</em> implement any of the suggestions. It simply lists them. If you instead tell it to implement the ideas, it will frequently trip over its own feet while trying to implement them all simultaneously in a single pass through the new code. (Just like humans, it is best to do one thing, check it, and then do the next thing.)</p>
                <ol>
                  <li>
                    <p>frequently, several of the items will be a bad idea, or too risky to ask GPT-4 to do. <em>Go one by one</em> through the list, having it implement just that one, and then test. Try to fix ‘core’ problems first.</p>
                  </li>
                  <li>
                    <p><em>self-repair</em>: not infrequently, a fancy rewrite will fail the test-suite (which we <em>did</em> generate in step #2, right?), but given the failing test-case and/or error pasted into the Playground, GPT-4 can usually fix it. (If GPT-4 cannot fix it given several tries and seems to be generating the same code fragments repeatedly or resorting to elaborate &amp; extreme rewrites, though the task doesn’t seem that hard, then you may have hit the blind spot and will need to fix it yourself—I’ve never seen GPT-4 escape the blind spot except by sheer accident.)</p>
                  </li>
                  <li>
                    <p><em>cleanup</em>: finally, You can ask it to rewrite the code for style/linting, but should leave that to the end, because otherwise that risks adding bugs while changing the code in ways that will wind up being discarded anyway.</p>
                  </li>
                </ol>
              </li>
              <li>
                <p>once it is clean and it’s either done the list or you’ve disapproved the suggestions, and the test-suite is passing, ask it to <strong>write a summary/design doc</strong> at the beginning and any additional <a href="https://en.wikipedia.org/wiki/Comment_(computer_programming)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Comment_(computer_programming)#bodyContent">code comments</a> inside it.</p>
                <p>GPT-4 will usually add a few comments in the code body itself, but not good ones, and it won’t usually write an adequate overall summary document unprompted. However, by this point, it has the context to do so should you ask it to.</p>
              </li>
            </ol>
            <p>With all this, you’re set up for maintainable code: with the test-suite and the up-front design doc, future LLMs can handle it natively (and will be able to learn from training on it), and you can easily add test-cases as you run into bugs; humans should be able to read the code easily after step #3 has finished, so you don’t need to care where it came from or try to track ‘taint’ through all future refactorings or usage—GPT-4 can write readable human-like code, it just doesn’t necessarily do it the best way the first time.</p>
            <p>While you may not necessarily have saved <em>time</em> (at least, if it’s in a language you are highly proficient in), you have saved yourself a lot of mental energy &amp; irritation (and made it much easier <a href="https://www.lesswrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge" data-link-icon="LW" data-link-icon-type="text" data-url-html="https://www.greaterwrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge?format=preview&amp;theme=classic">just to get started</a>) by making GPT-4 do the tedious work; it almost transforms programming from too-often-frustrating work filled with papercuts &amp; brokenness to spectator entertainment.</p>
          </section>
          <section id="case-studies">
            <h2><a href="#case-studies" title="Link to section: § &#39;Case Studies&#39;">Case Studies</a></h2>
            <p>Some examples of nontrivial code I’ve written this way (ie. excluding the many little snippets or modifications I’ve used GPT-4 for, especially for the finer points of Bash syntax), with GPT-4 doing most (?) of the work, by language, in roughly chronological order:</p>
            <ul>
              <li>
                <p>Bash: <a href="https://github.com/gwern/gwern.net/commit/399642b993d211d1ff7e4532ccd1f0aa9dcc04ca#diff-801116d1e0e32a212a4678a3854842ca103f72c08f369e96c628db6eb921c34cL251" data-link-icon="github" data-link-icon-type="svg">tab completion</a> for the upload script, so it <a href="https://en.wikipedia.org/wiki/Command-line_completion" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Command-line_completion#bodyContent">tab-completes</a> the file and then the remote destination directory.</p>
                <p>I have no interest in learning the guts of Bash tab-completion in order to set up more advanced positional tab-completion; but GPT-4 already knows how to do it.</p>
              </li>
              <li>
                <p>Python: <a href="https://gwern.net/static/build/latex2unicode.py">latex2unicode.py</a> uses GPT-4 to convert <span>L<span>a</span>T<span>e</span>X</span> math fragments to HTML+CSS+Unicode, which are much easier to edit/style, render quicker, and look more natural; as <span>L<span>a</span>T<span>e</span>X</span> is a full-blown and rather hard to parse language, this is extremely difficult to do in any standard formal sense.</p>
                <p>This is a good example of the loop: I wrote none of the Python, but seeded it with a few instructions &amp; manual rewrites from my existing <span>L<span>a</span>T<span>e</span>X</span> → Unicode pipeline; then I prompted GPT-4 to ask for any <span>L<span>a</span>T<span>e</span>X</span> it could think of which it was unsure how to translate. After it gave a few examples, I would then manually translate them or add a new instruction, and ask again. Most of the examples it asked about I would not have thought of, like playing card suits (which are supported—<code>\clubsuit</code>, <code>\diamondsuit</code> etc).</p>
              </li>
              <li>
                <p>Haskell:</p>
                <ul>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/d68fb0dd561a910ff90d8cbc320af74c1e2a872f" data-link-icon="github" data-link-icon-type="svg">add <code>&lt;poster&gt;</code> thumbnails for videos</a></p>
                    <p>This is a frustrating one because as far as I can tell from running it, the GPT-4 code is easy to read and works flawlessly: it parses the HTML as expected, creates the necessary thumbnail, and rewrites the HTML <code>&lt;video&gt;</code> appropriately. It’s just that for some reason, the <em>rest</em> of my Hakyll codebase does not run it or it somehow breaks however it’s actually called, and I’ve never figured out why. (The opacity of Hakyll Haskell and the sheer complexity of the Gwern.net codebase in operation means that when a rewrite pass goes awry, it’s exceptionally difficult to figure out what is going wrong.)</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/ec3e515b895be82bdfeb82ef0bd8b52bdba7ff36" data-link-icon="github" data-link-icon-type="svg">link metadata handling</a>: the finicky handling of how links on Gwern.net get assigned the various bits of metadata determining whether they will pop up annotations etc had built up into a rat’s-nest of guards &amp; if-tens over time. When yet another feature broke because I misunderstood what the handler would do, I resolved to rewrite it to clarify the logic. My first refactoring attempts failed, as I kept losing track mentally and adding in bugs.</p>
                    <p>Then I threw up my hands and assigned the job to GPT-4, and it was able to cleanly refactor it after some iterations, and didn’t appear to introduce any bugs.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/c0c918e97163e70507df08047803e8c40ee1b683#diff-99a96368a7dda8690fa3191e187150bbc5fc459f6db1518d6a8914abf04e0b7aR47" data-link-icon="github" data-link-icon-type="svg">correct URL rewrites</a>: a large refactoring of how URLs are rewritten to point to better URLs relied on GPT-4.</p>
                    <p>URLs on Gwern.net can be rewritten multiple ways, like to point to a mirrored version hosted locally or on a specialized site. For example, Medium.com has become extraordinarily reader-hostile, and so Medium links are rewritten to the equivalent Scribe.rip link. (See <a href="https://gwern.net/static/build/LinkArchive.hs" data-link-icon="code" data-link-icon-type="svg"><code>LinkArchive.hs</code></a> &amp; <a href="https://gwern.net/archiving#preemptive-local-archiving" id="gwern-archiving-preemptive-local-archiving" title="‘Archiving URLs § Preemptive Local Archiving’, Gwern 2011">Archiving URLs</a> for a fuller explanation of what &amp; why we do all this.)</p>
                    <p>In easy cases like that, it’s as simple as <code>s/medium.com/scribe.rip/</code>, but in some cases, it is necessary to formally parse a URL into a <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Uniform_Resource_Identifier#bodyContent">URI</a> data structure and extract the <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier#Syntax" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Uniform_Resource_Identifier#bodyContent">many complicated parts</a> (like host, path, query, and fragment), and rewrite them to the new URL. Haskell’s <a href="https://hackage.haskell.org/package/network-uri" data-link-icon="𝛌" data-link-icon-type="text">Network.URI</a> can do all this, but if one is not familiar with URI concepts and the library, it’s all so much gobbledegook and leaves one trapped in a maze of tiny types &amp; functions, each alike. Every time I have gone near it prior, I have been repelled by its force field.</p>
                    <p>GPT-4 was able to handle all the parsing &amp; reformatting, with special cases, for each domain separately, and then refactor out the duplication, and make the final version look positively easy (including <a href="https://github.com/gwern/gwern.net/commit/1dfa6721d83651d076cbfd9e9f818f8fbef25d13#diff-99a96368a7dda8690fa3191e187150bbc5fc459f6db1518d6a8914abf04e0b7aR53" data-link-icon="github" data-link-icon-type="svg">the later bug-fix</a> when it turned out I had misunderstood how a particular URL argument was supposed to go).</p>
                  </li>
                  <li>
                    <p>printing out <a href="https://github.com/gwern/gwern.net/commit/0f9c04cc9179656989169fb13350c6fe2127b890#diff-82dd61753bb332658dc7bda75fb9e231526d434492de6b55801845247ccd4bbeR73" data-link-icon="github" data-link-icon-type="svg">large numbers <em>not</em> in scientific-notation</a>: necessary for <a href="https://gwern.net/static/build/Inflation.hs" id="branwen-2019" data-link-icon="code" data-link-icon-type="svg" title="‘InflationAdjuster’, Branwen 2019">proper inflation-adjusted dollar amounts</a>, but weirdly difficult in Haskell’s default libraries.</p>
                    <p>After running into this issue several times, I resorted to the full workflow of test-suite and iterative revising. The pretty-printing is still more limited than I would like, but covers all numeric magnitudes it would be reasonable to inflation adjust, and the test-suite gives me confidence that this time is <em>finally</em> right.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/28822b77f589c0b7cc9a68e8644a98c0e72ab56d" data-link-icon="github" data-link-icon-type="svg">compile-time location of internal cross-references</a>, to set the arrow-direction statically as a browser layout optimization</p>
                    <p>A Gwern.net feature is to make internal cross-references between sections less cognitively-taxing by specifying whether the reference is <em>before</em> or <em>after</em> the current location. For example, in this document, in the abstract, many sections are linked, and each of them has a down arrow (‘↓’) symbol: this tells you that the link target is below, and so you know you have not read the target yet, so you can decide whether you want to skip forward or keep reading. In other cases, like <a href="#previous-back-link">a link later on in this page</a>, the link instead is an up arrow (‘↑’), because it is pointing to previous material before it: now you know you have already read what it is referring to, and can remember it, and you may decide to ignore it. This is better than a mere opaque hyperlink, or even a internal link symbol like a section sign (‘§’): “See §discussion of <em>Z</em>”—well, <em>what</em> discussion? There was some mention of <em>Z</em> before, is that ‘the discussion’? Is there a larger later ‘discussion’ I haven’t read yet, that maybe I want to pop up and read now? Is this even in the same essay? Or what? Opaque cross-reference links create friction, as the reader is left with few clues about whether they want to spend effort to follow the link.</p>
                    <p>It is easy enough to write some JavaScript to run over an HTML page, detect all internal <a href="https://en.wikipedia.org/wiki/HTML_element#Anchor" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/HTML_element#bodyContent">anchor</a> links, and set the before/after arrow direction, and this is what we did for a long time. But while easy to write, this is not quite so easy for the browser to run (especially on long or heavily-hyperlinked pages), and it was adding a small amount to the layout time. And it is not <em>necessary</em> (or esthetic) to do this at runtime, because the locations of most links are known at compile-time. We knew all along that a Pandoc rewrite pass could take a document, look at all the links, decide whether they are before or after each other, and add the necessary arrow metadata. It’s just that this would be a stateful traverse requiring monadic operations and I was unsure how to do all the tree navigation operations to descend/ascend to find where something was. Because it was not a critical performance bottleneck, I put off this micro-optimization.</p>
                    <p>Eventually, I had an eureka moment: all that complexity about locating pairs of elements was unnecessary. All you need is traverse the AST in order while updating a <a href="https://en.wikipedia.org/wiki/Set_(abstract_data_type)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Set_(abstract_data_type)#bodyContent">set data-structure</a> to record whether you have seen a target link ID before; then at each cross-reference link, you have either seen the target link ID before, and therefore it must be <em>before</em>in the document, or you have not yet seen the target, and therefore it must be <em>after</em> in the document.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
                    <p>Once I had that simplification, it was a piece of cake to instruct GPT-4 to define a <a href="https://hackage.haskell.org/package/containers-0.7/docs/Data-Set.html" data-link-icon="𝛌" data-link-icon-type="text">Data.Set</a> set &amp; a <a href="https://hackage.haskell.org/package/mtl/docs/Control-Monad-State-Lazy.html" data-link-icon="𝛌" data-link-icon-type="text">State monad</a> to do that walk, and set up a test-suite to verify correctness, which did catch a few edge-cases (like links in the same paragraph).</p>
                  </li>
                  <li>
                    <p><a href="https://en.wikipedia.org/wiki/Title-case" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Title-case#bodyContent">title-case</a> <a href="https://github.com/gwern/gwern.net/commit/f9a0691aa80ae84a6e662686dd10f46507a48c06" data-link-icon="github" data-link-icon-type="svg">formatting of text</a>: my existing title-case code did not handle cases involving hyphens, so it would generate titles like “Foo Bar-bar Baz”, which I felt looked ugly compared to capitalizing after hyphens as well (ie. “Foo Bar-Bar Baz”).</p>
                    <p>GPT-4 handled the somewhat finicky string-munging and set up a test-suite, which I would be glad for later when I ran into another case where <a href="https://github.com/gwern/gwern.net/commit/51ced6f76e5d336f0c27e8109f72533863d72323#diff-9bcc0a8972fc364bc87786fe886ccd57b2168d0366ded8f533972a73af7f0378R219" data-link-icon="github" data-link-icon-type="svg">punctuation made lowercase look bad</a>.</p>
                  </li>
                  <li>
                    <p>detecting <a href="https://github.com/gwern/gwern.net/commit/7799fcfd36d886c89fb61b2223b82e776d1631c1" data-link-icon="github" data-link-icon-type="svg">imbalanced brackets/quotes</a> in documents</p>
                    <p>A particularly insidious family of typos is imbalanced brackets/parentheses/quotes: authors often fail to close a parenthesis pair or get lost, particularly in medical abstracts. This is a concern because often it indicates a more serious syntactic error, like an HTML <code>&lt;a&gt;</code> where the <code>href=</code> is malformed. I had a simple check which tested if the total number of each character was an even amount, but this failed to catch many typos: <code>[[]]</code> is correct and has an even number of both brackets, but that’s equally true of, say, the swapped equivalent <!-- [[ --> <code>]][[</code>. <!-- ]] --> It’s a well-known case of needing a full <a href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Stack_(abstract_data_type)#bodyContent">stack</a>, in order to push/pop each bracket <em>in order</em>, to detect not just numerical missingness, but wrong order.</p>
                    <p>It is not that hard, but tedious. It was something I did in an early CS course, and I felt that was enough for one lifetime, so I was happy to see if GPT-4 could do it. It could, and as I expected, it turned up scores of instances that had slipped through all my proofreading. (I didn’t ask it to set up a test-suite because the Gwern.net corpus <em>is</em> the test-suite in this case.)</p>
                  </li>
                  <li>
                    <p>checking that <a href="https://github.com/gwern/gwern.net/commit/8896f36f7d655545f1b87c07233347c9947a9ea8#diff-102b6b2d06d4eac2ab32b43b7215c078e3be02c3c96f6c7a8cf2a4ed73bb4433R1967" data-link-icon="github" data-link-icon-type="svg">sets of rules don’t overlap</a></p>
                    <p>Gwern.net configuration requires thousands of rewrite rules covering an endless army of special cases. Inevitably, the sets of rules will overlap or become redundant, especially as websites change domains or URLs get automatically updated. Overlap can cause bugs, or even kill the site compilation, if some update to either rules or essay content accidentally triggers a hidden infinite loop. So each config should ideally check for ‘redundancy’—but each set of <code>(key, value)</code> pairs tends to have a different need: some need the keys to be unique, some need the values to be unique, some need both to be unique, some need just the pairs to be unique, and heck, some are actually 3-tuples of <code>(key, value 1, value 2)</code> why not.</p>
                    <p>GPT-4 wrote out all the necessary instances and refactored them, and I applied them to the existing configs, and indeed discovered hundreds of overlaps and several serious bugs of the ‘how did this ever work’ variety.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/eb1bd7a7f1cabf61e515611e78395d3b5a690f05#diff-102b6b2d06d4eac2ab32b43b7215c078e3be02c3c96f6c7a8cf2a4ed73bb4433R1980" data-link-icon="github" data-link-icon-type="svg">infinite loop (cycle) detection</a> in rewrite rules</p>
                    <p>In checking that rules don’t overlap with each other, there are nasty cases that can’t be detected just on a (key, value) basis. In particular, in doing rewrites, a rewrite could create an infinite loop even when there is no overlap whatsoever: for example, if we accidentally define a set of rewrite rules like [A → B, B → C, C → A], then all keys are unique, all values are unique, and all pairs are unique, but we have defined an infinite loop and if our code ever encounters any of the values A–C, then it will loop forever or crash. This is especially bad because it will only happen at runtime, and will depend on the exact inputs (so it might not trigger immediately), and will be hard to debug or trace back to the responsible rule.</p>
                    <p>And this is what happened on occasion with Gwern.net updates; the Wikipedia URL rewrites were particularly error-prone, as Wikipedia editors sometimes change their mind about what URL an article should be at, so if it gets moved over a redirect, it’s not hard to have a config which rewrites the old article title to the new article title, and then later one, discover that the new article title has been renamed to the old article title and add a rule for <em>that</em>…</p>
                    <p>To deal with this, we must treat the rules as defining <a href="https://en.wikipedia.org/wiki/Graph_theory" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Graph_theory#bodyContent">a directed graph</a>, and <a href="https://en.wikipedia.org/wiki/Cycle_(graph_theory)#Cycle_detection" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Cycle_(graph_theory)#bodyContent">detect any cycles</a>. Graph analysis is not something I’ve done that much of, so even though Haskell’s <a href="https://hackage.haskell.org/package/containers-0.7/docs/Data-Graph.html" data-link-icon="𝛌" data-link-icon-type="text">Data.Graph</a> should be fully capable of this, I didn’t know where to start, and put it off until a particularly annoying infinite-loop made me reach for GPT-4 in anger.</p>
                    <p>GPT-4 struggled with the problem, as its test-suite kept finding bugs in its <a href="https://en.wikipedia.org/wiki/Strongly_connected_component" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Strongly_connected_component#bodyContent">strongly connected component</a> approach—but it did it in the end with little help from me (not that I could, because I know no more of how to use Data.Graph than when I started). Infinite-loops have not been an issue since, and I am more free to add in rules because I don’t worry about triggering such nastily fatal bugs.</p>
                  </li>
                </ul>
              </li>
              <li>
                <p>Emacs Lisp: mostly helper functions for more conveniently editing abstracts &amp; excerpts for use as annotation popups</p>
                <ul>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/caa704e22aff192106d30d9c9d7adaff9c247500#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1282" data-link-icon="github" data-link-icon-type="svg">call out to GPT-based script</a> for parsing a single block-paragraph abstract into a more readable multi-paragraph format and updating the buffer</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/8761dccc5b3fa4fff468db554813bf20e02702a8#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R102" data-link-icon="github" data-link-icon-type="svg">do <em>one</em> search-and-replace</a>: when adding a hyperlink to define a term, one only wants to do <em>one</em> search-and-replace; however, all the existing search-and-replace functions assume that you may want to replace any or all of the hits, and so you have to waste time exiting or canceling or skipping after doing the desired first replacement.</p>
                    <p>This defines a <code>query-replace-regexp-once</code> which will, as the name suggests, do only 1 replacement and then exit without further ado.</p>
                    <ul>
                      <li>
                        <p><a href="https://github.com/gwern/gwern.net/commit/18a6da407014bac4e202dfe1b0666a2c6e016a98#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1261" data-link-icon="github" data-link-icon-type="svg"><em>don’t</em> search-and-replace in URLs</a>: we never want to do a search-and-replace to add a URL to text which is already <em>in</em> a URL!</p>
                      </li>
                    </ul>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/9fb4108b07e7bf86742024b4ff2bdcbd92eaff0e#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1222" data-link-icon="github" data-link-icon-type="svg">remove hard-newlines in Markdown</a>: hard newlines or line-wraps, while ignored by <a href="https://en.wikipedia.org/wiki/Markdown" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Markdown#bodyContent">Markdown</a> and equivalent to regular line-wrapping, are ugly to read and interfere with many editing operations like search-and-replace. (What if you want to replace a phrase, but that phrase is broken across a line?)</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/d391c340cc02aae6d2b54cb672e7f12ec7f620ba#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1252" data-link-icon="github" data-link-icon-type="svg">mark up various HTML/Markdown fragments</a>: various shortcuts for surrounding a word or a selected region of text with various HTML/Markdown snippets like italics or smallcaps.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/bec53fee7403624f386024316c7bbde7bd11fb4a" data-link-icon="github" data-link-icon-type="svg">interactively add inflation-adjustment markup</a>: going around modifying <code>foo</code> → <code>[foo]($2023)</code> for every dollar amount in a buffer is a bit tedious. This detects dollars, asks a date, and queries each possible mark-up.</p>
                  </li>
                </ul>
              </li>
            </ul>
            <p>Overall, GPT-4 has been highly useful and handled many tasks I simply wouldn’t have the gumption to do.</p>
          </section>
          <section id="acronym-generation">
            <h2><a href="#acronym-generation" title="Link to section: § &#39;Acronym Generation&#39;">Acronym Generation</a></h2>
            <p>So, to try out GPT-4’s coding abilities, I prompt it for a script, with a short prompt along the lines of:</p>
            <blockquote>
              <p><strong>Write a <a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bash_(Unix_shell)#bodyContent">Bash shell</a> script to generate all alphabetic acronyms up to 6 letters and check whether a Wikipedia article exists.</strong></p>
            </blockquote>
            <p>I specific ‘up to 6 letters’ both just to be safe (as 26<sup>6</sup> = 30,8915,776 we surely will hit an unused TLA before then!), and also to test GPT-4: I am sure that it is able to write a naive acronym-generation algorithm and then do a <code>curl</code> request, but is it good enough to write a reasonable incremental or streaming solution, avoiding performance problems from the combinatorial explosion—without explicitly being told to do so?</p>
            <p>It was not. The first GPT-4 failure went like this:</p>
            <div id="cb2">
              <pre><code><span id="cb2-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb2-2"><span>do</span></span>
<span id="cb2-3">  <span># generate all possible acronyms of the given length</span></span>
<span id="cb2-4">  <span>printf</span> <span>&#34;%</span><span>${length}</span><span>s\n&#34;</span> <span>{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}</span> <span>|</span> <span>cut</span> <span>-c1-</span><span>$length</span> <span>|</span></span>
<span id="cb2-5">  <span>while</span> <span>read</span> <span>acronym</span></span>
<span id="cb2-6">  <span>do</span></span>
<span id="cb2-7">      <span>echo</span> <span>$acronym</span></span>
<span id="cb2-8">  <span>done</span></span>
<span id="cb2-9"><span>done</span></span>
<span id="cb2-10"><span># ...</span></span></code></pre>
            </div>
            <p>Leaving aside correctness or the IO part, the acronym generation code here is unrunnable due to memory consumption: it generates all possible acronyms upfront, which is a problem because there are <code><span>sum</span> (<span>map</span> ((<span>^</span>) <span>26</span>) [<span>1</span><span>..</span><span>6</span>])</code> = 321,272,406 such acronyms, and it would take a long time to generate them one by one and will OOM when handled naively by Bash as a bunch of strings in RAM.</p>
            <p>I tell GPT-4 this code is wrong because the code will OOM, and it obliges with a rewrite like this:</p>
            <div id="cb3">
              <pre><code><span id="cb3-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb3-2"><span>do</span></span>
<span id="cb3-3">  <span># generate all possible acronyms of the given length</span></span>
<span id="cb3-4">  <span>for</span> acronym <span>in</span> <span>$(</span><span>eval</span> echo <span>{</span>A<span>..</span>Z<span>}</span>{1..<span>$length</span>}<span>)</span></span>
<span id="cb3-5">  <span>do</span></span>
<span id="cb3-6">    <span>echo</span> <span>$acronym</span></span>
<span id="cb3-7">  <span>done</span></span>
<span id="cb3-8"><span>done</span></span></code></pre>
            </div>
            <p>This version is more incremental and immediately starts printing out acronyms, but the generation of acronyms is immediately blatantly wrong and looks like: <code>A1 B1 C1 ... Z1 Z2 Z3 Z4 Z5 Z6</code>. (It should instead look like <code>A ... ZZZZZZ</code>.) I provide GPT-4 with the error &amp; examples, and it rewrites again:</p>
            <div id="cb4">
              <pre><code><span id="cb4-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb4-2"><span>do</span></span>
<span id="cb4-3">  <span># define a function to recursively generate acronyms</span></span>
<span id="cb4-4">  <span>generate_acronyms()</span> <span>{</span></span>
<span id="cb4-5">    <span>local</span> <span>current</span><span>=</span><span>$1</span></span>
<span id="cb4-6">    <span>local</span> <span>length</span><span>=</span><span>$2</span></span>
<span id="cb4-7">    <span>if</span> <span>((</span> <span>length</span> <span>==</span> <span>0</span> <span>))</span></span>
<span id="cb4-8">    <span>then</span></span>
<span id="cb4-9"></span>
<span id="cb4-10">        <span>echo</span> <span>&#34;</span><span>$current</span><span>&#34;</span></span>
<span id="cb4-11">    <span>else</span></span>
<span id="cb4-12">      <span>for</span> letter <span>in</span> <span>{</span>A<span>..</span>Z<span>}</span></span>
<span id="cb4-13">      <span>do</span></span>
<span id="cb4-14">        <span>generate_acronyms</span> <span>&#34;</span><span>$current$letter</span><span>&#34;</span> <span>$((length</span><span>-</span><span>1</span><span>))</span></span>
<span id="cb4-15">      <span>done</span></span>
<span id="cb4-16">    <span>fi</span></span>
<span id="cb4-17">  <span>}</span></span>
<span id="cb4-18">  <span># call the function to generate acronyms of the current length</span></span>
<span id="cb4-19">  <span>generate_acronyms</span> <span>&#34;&#34;</span> <span>$length</span></span>
<span id="cb4-20"><span>done</span></span></code></pre>
            </div>
            <p>The output now looks reasonable, as it starts with <code>A</code> and is soon printing out acronyms like <code>GEBO</code>.</p>
          </section>
          <section id="string-munging">
            <h2><a href="#string-munging" title="Link to section: § &#39;String Munging&#39;">String Munging</a></h2>
            <p>Now that the acronym generation is working, we can look at GPT-4’s revised whole script, which it prints out as follows:</p>
            <pre><code>Here is a Bash script that uses the Wikipedia API to check for the existence of a page with the acronym&#39;s title. For simplicity, it only checks acronyms with uppercase letters. It outputs acronyms that don&#39;t have a corresponding Wikipedia article.

`​`​`bash
#!/bin/bash
# set the range of acronym lengths
for length in {1..6}
do
  # generate all possible acronyms of the given length
  printf &#34;%${length}s\n&#34; {A..Z}{A..Z}{A..Z}{A..Z}{A..Z}{A..Z} | cut -c1-$length |
  while read acronym
  do
    # query the Wikipedia API
    response=$(curl -s &#34;https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=$acronym&#34;)
    # check if the page exists
    if [[ $response == *&#39;&#34;missing&#34;*&#39; ]]
    then
      echo $acronym
    fi
  done
done
`​`​`
The script iterates over all possible acronyms of a given length (from 1 to 6 letters). For each acronym, it queries the Wikipedia API. If the response contains the string &#34;missing&#34;, it means that no Wikipedia article with that title exists, so the script prints the acronym.

Note: Running this script may take a long time due to the large number of acronyms and the rate limits of the Wikipedia API. Consider adding delays or using parallel processing to speed up the process and avoid overloading the API.</code></pre>
            <p>GPT-4 chose to check the <a href="https://www.mediawiki.org/wiki/Wikimedia_REST_API" data-link-icon="wikipedia" data-link-icon-type="svg">Wikipedia API</a> (used by Gwern.net WP popups, incidentally) instead of doing what I would do, which would be a <code><span>curl</span> <span>--head</span></code> response; but that is probably a better strategy, as the API response might be smaller and will be more separate from the main website, so it’s politer. (I would typically not want to bother looking up the right API endpoints and URLs, because there are so many and the WP API documentation is confusing. If we were interested in doing this really properly, we’d probably download a list of all page titles from the <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Wikipedia:Database_download#bodyContent">Wikipedia dumps</a> to batch query offline.) The API response looks like this for a used &amp; unused TLA:</p>
            <div id="cb6">
              <pre><code><span id="cb6-1"><span>$</span> curl <span>--silent</span> <span>&#34;https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=AAA&#34;</span></span>
<span id="cb6-2"><span># {&#34;batchcomplete&#34;:&#34;&#34;&#34;,</span></span>
<span id="cb6-3"><span>#    query&#34;:{&#34;pages&#34;:{&#34;149335&#34;:{&#34;pageid&#34;:149335&#34;,ns&#34;:0&#34;,title&#34;:&#34;AAA&#34;</span><span>}}}</span><span>}</span></span>
<span id="cb6-4"></span>
<span id="cb6-5"><span>$</span> curl <span>--silent</span> <span>&#34;https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=CQK&#34;</span></span>
<span id="cb6-6"><span># {&#34;batchcomplete&#34;:&#34;&#34;&#34;,</span></span>
<span id="cb6-7"><span>#   query&#34;:{&#34;pages&#34;:{&#34;-1&#34;:{&#34;ns&#34;:0&#34;,title&#34;:&#34;CQK&#34;&#34;,missing&#34;:&#34;&#34;</span><span>}}}</span><span>}</span></span></code></pre>
            </div>
            <p>This code looks reasonable, and appears to run successfully.</p>
            <p>But it still has a serious bug: it will never print out any hits. This is because it’s made a subtle error in the <a href="https://en.wikipedia.org/wiki/Glob_(programming)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Glob_(programming)#bodyContent">string glob</a> matching a ‘missing’ response: <code><span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>&#39;&#34;missing&#34;*&#39;</span> <span>]]</span></code> will never match anything, because the second match-anything asterisk is <em>inside</em> the single-quotation marks, which forces Bash to match a literal asterisk, rather than matching any string. What it should be is a single character difference, swapping the single-quote/asterisk: <code><span>*</span><span>&#39;&#34;missing&#34;&#39;</span><span>*</span></code></p>
            <section id="blind-spot">
              <h3><a href="#blind-spot" title="Link to section: § &#39;Blind Spot&#39;">Blind Spot</a></h3>
              <p><a href="#string-munging">This bug above</a> is a surprising error because this is not how a human would’ve written the glob, and the glob (like almost all globs) is so simple that it’s hard to imagine anyone being able to write the acronym generation &amp; memorizing the API URL correctly but then screw up a simple check of “does the response contain the string <code>missing</code>?” At least, this is surprising if you have not run into this problem with GPT-4 before, as I have repeatedly when writing Bash scripts; GPT-4 will not just make the error, but it seems utterly unable to ‘see’ the error even when pointed out, and tends to thrash in confusion making random guesses about what the problem could be.</p>
              <p><a href="https://www.reddit.com/r/slatestarcodex/comments/1201v68/10word_quote_a_short_and_simple_failure_mode_of/jdjsx43/" data-link-icon="reddit" data-link-icon-type="svg" data-url-html="https://old.reddit.com/r/slatestarcodex/comments/1201v68/10word_quote_a_short_and_simple_failure_mode_of/jdjsx43/">I theorize</a> that it’s not a BPE tokenization issue (as <a href="https://gwern.net/gpt-3#bpes" id="gwern-gpt-3-bpes" title="‘GPT-3 Creative Fiction § BPEs’, Gwern 2020">so often</a>), because this <strong>blind spot</strong> seems to happen in word-level problems as well, where tokenization couldn’t be a problem. The blind spot is, perhaps, related to internal sparsity of the GPT-4 model; I speculate that when the blind spot happens, it’s because early layers have mistakenly erased apparently-irrelevant information in order to focus their attention on other more important parts of the input, but then this turns out to be wrong—so, the information gone irreversibly, the later layers are unable to see what’s wrong with the input, which ‘looks like expected’, and are forced to speculate wildly and work on extremely unlikely hypotheses. Because it would happens early on in processing, it strikes me as analogous to the <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Blind_spot_(vision)#bodyContent">human eye’s blind spot</a>—you cannot see what you cannot see, even when someone tells you about it and you think really hard. The agnosia is irreversible.</p>
              <p>If this bug is the blind spot, then GPT-4 would not be able to diagnose it nor fix it except by accident, and might go around in circles making the code worse and more complicated and implementing completely irrelevant solutions to confabulated problems. This is why the blind spot bug is so pernicious, compared to the other errors GPT-4 can usually self-repair: it nearly guarantees failure, <em>and</em> the pathological response to it is extremely misleading as it obscures the issue and can deeply confuse the user, who naturally expects the error to be something more impressive, especially as often GPT-4 has been doing amazing things up until that point.</p>
              <p>If you run into the blind spot, as often happens to me in Bash, Elisp, or regexps but much less often in Haskell or Python, there is nothing to do but recognize that you’ve hit the blind spot and that GPT-4 will never fix the bug on its own. If you are lucky, it will ‘only’ print out the same broken code repeatedly; more likely, GPT-4 will make things worse and break perfectly-good code, confabulating ever more crazy (and usually broken) ‘solutions’ to the problem it can’t see as it thrashes around in confusion (like some sort of hyper-advanced Magikarp). You have to fix the bug yourself.</p>
              <p>We tell it that the code is wrong without specifying how (this works surprisingly often, as a kind of inner-monologue), and look at the next version:</p>
              <pre><code>Apologies for the confusion. Here&#39;s the corrected script:

```bash
#!/bin/bash
#set the range of acronym lengths
for length in {1..6}
do
  #define a function to recursively generate acronyms
  generate_acronyms() {
    local current=$1
    local length=$2
    if (( length == 0 ))
    then
      #query the Wikipedia API
      response=$(curl -s &#34;https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=$current&#34;)
      #check if the page exists
      if [[ $response == *&#39;&#34;missing&#34;*&#39; ]]
      then
        echo &#34;$current&#34;
      fi
    else
      for letter in {A..Z}
      do
        generate_acronyms &#34;$current$letter&#34; $((length-1))
      done
    fi
  }
  #call the function to generate acronyms of the current length
  generate_acronyms &#34;&#34; $length
done
```

This script uses a recursive function to generate acronyms one at a time, reducing memory usage.
For each acronym, it queries the Wikipedia API. If the response contains the string &#34;missing&#34;,
it means that no Wikipedia article with that title exists, so the script prints the acronym.

Again, running this script will take a long time due to the large number of acronyms and the
rate limits of the Wikipedia API.</code></pre>
              <p>Nope.</p>
              <p>We tell it to try again. It confabulates a problem with the acronym generation. Try again! Confabulates a different problem with <code>curl</code>. Also no. Guesses it needs to check for an omitted page ID from the JSON response. A 5<sup>th</sup> version cycles back to the acronym generation. So it cycled through 5 versions unable to fix it (and adding in new problems along the way). I omit the broken variants here as of no particular interest.</p>
              <p>Getting tired of the blind spot issue, I hint that there is a problem with the string munging, specifically. GPT-4 concludes that it’s wrong about the <em>formatting of the API response</em> (which it is not, checking simply for ‘missing’ would be fine), and so <em>that</em> is why it needs to change the glob to:</p>
              <div id="cb8">
                <pre><code><span id="cb8-1">    <span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>&#39;&#34;missing&#34;:&#34;&#34;&#39;</span><span>*</span> <span>]];</span> <span>then</span> <span>echo</span> <span>$acronym</span><span>;</span> <span>fi</span></span></code></pre>
              </div>
              <p>As it happens, this glob is <em>finally</em> correct. It could use a bit of linting, according to <a href="https://www.shellcheck.net/">ShellCheck</a>, but it’s fine for a one-off. The major issue is that it does not handle HTTP errors so any network problems while requesting a TLA will result in it being skipped and treated as used. (GPT-4 is aware of this issue and will fix it if one asks simply “How can this be improved?”, generating a <a href="https://en.wikipedia.org/wiki/Python_(programming_language)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Python_(programming_language)#bodyContent">Python</a> script which handles network errors and also does batched requests, which runs vastly faster than the Bash script does; <a href="#python">see below</a>.)</p>
              <p>I ran the Bash script successfully overnight on 2023-09-29.</p>
            </section>
          </section>
          <section id="results">
            <h2><a href="#results" title="Link to section: § &#39;Results&#39;">Results</a></h2>
            <p>This revealed the first unused TLA is “<strong>CQK</strong>”.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> Surprisingly, we didn’t get far through the TLA alphabet before finding the first unused TLA.</p>
            <p>Additional unused TLAs early on include:</p>
            <ul>
              <li>
                <p><strong>C</strong>: CQK CQQ CQZ CVY CWZ CXK CXU CXZ CYV CYY CZQ CZV</p>
              </li>
              <li>
                <p><strong>D</strong>: DKQ DKY DQJ DQQ DQW DUZ DVJ DVZ DXK DXQ DXW DYI DYJ DYQ DYV DYX DYY DYZ DZE DZK DZM DZV DZW DZX</p>
              </li>
              <li>
                <p><span><span><strong>E</strong>:</span> EBZ ECY ECZ EDZ EEY EGK EGQ EHQ EHW EHY EHZ EIY EIZ EJD EJJ EJM EJQ EJX EJY EJZ EKJ EKQ EKX EKZ EOX EOZ EPY EQD EQJ EQK EQO EQQ EQZ ETQ ETY EUW EUY EUZ EVJ EVQ EWX EWY EWZ EXF EXG EXH EXJ EXQ EYG EYH EYI EYJ EYV EYX EYY EYZ EZB EZC EZJ EZK EZL EZM EZN EZP EZT</span></p>
              </li>
            </ul>
            <p>I provide the <a href="https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt" data-link-icon="txt" data-link-icon-type="svg">complete 3–4 letter list</a> as a newline-delimited text file:</p>
            <p><a href="https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt" data-link-icon="txt" data-link-icon-type="svg">Unused acronyms</a></p>
            <section id="checking">
              <h3><a href="#checking" title="Link to section: § &#39;Checking&#39;">Checking</a></h3>
              <p>Is this a false positive? We check to make sure—Wikipedia could just have failed to make a redirect to “C.Q.K.” or something like that. But there is indeed nothing of importance in Wikipedia about a CQK: there are only OCR errors and the obscure abandoned name of an obscure hotel company which owns <a href="https://en.wikipedia.org/wiki/Hartsfield_Manor,_Betchworth" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Hartsfield_Manor,_Betchworth#bodyContent">an obscure English manor</a>. <a href="https://www.google.com/search?q=CQK" data-link-icon="alphabet" data-link-icon-type="svg">Googling ‘CQK’</a>, I do not see anything suggesting WP’s omission is wrong: there are no important CQKs, as the top hit is an obscure &amp; dubious-sounding Chinese tea-manufacturer, some restaurants, computer hardware part numbers, and miscellany, with hits quickly degenerating into spam/noise/variants. And there is no <code>cqk.com</code> (merely a domain squatter).</p>
              <p>Similarly, ‘CQQ’ turns up nothing much in Wikipedia, and minor things in Google (Law Insider: “CQQ means a Certificate of Quality and Quantity for a consignment of EUP”, whatever an ‘EUP’ is, or the Free Dictionary’s “<em>Courtepointe Québec Quilts</em> (French; Canadian quilting association)”, which does at least seem to be <a href="https://www.courtepointequebec.com/">a specific organization</a>, however obscure).</p>
              <p>So, these results look good.</p>
            </section>
            <section id="python">
              <h3><a href="#python" title="Link to section: § &#39;Python&#39;">Python</a></h3>
              <p>One possible error is that as mentioned, the Bash script doesn’t handle network errors and is checking an affirmative response from the WP API about a page title being “missing”; if there was no response (because of network problems), then there could be no “missing” text in the response and the TLA would be skipped. So, if a TLA was not used <em>and</em> the check suffered a network error, it would be a false negative, and hypothetically, there could be an unused TLA earlier than CQK.</p>
              <p>To double-check that ‘CQK’ is correct and a network error didn’t spuriously drop an earlier unused TLA, after I finished this writeup, I went back to GPT-4 with the fixed Bash script and asked it, “How can this be improved?” GPT-4:</p>
              <pre><code>1. Use Wikipedia&#39;s batch API feature: Instead of making an API request for each acronym,
you can batch multiple titles in a single API request. This will significantly reduce
the number of requests and the total runtime. Wikipedia allows up to 50 titles per
API request for regular users.

2. Parallelism: The script is currently single-threaded.
You can use parallel processing to generate acronyms and query the API concurrently.
Tools like GNU Parallel can be useful for this purpose.

3. Caching: If you plan to
run this script multiple times, consider caching the results of the API requests to
avoid redundant queries.

4. Error Handling: The script lacks error handling. For
instance, it doesn&#39;t handle HTTP errors or JSON parsing errors. Add error handling
to make the script more robust.

5. Rate Limiting: Consider adding a delay between
API requests to avoid hitting Wikipedia&#39;s rate limit.

6. Use a programming language
better suited for this task: Bash has limitations when it comes to complex tasks like
this one. Consider using a language like Python or Ruby, which have better support
for tasks like JSON parsing, HTTP requests, and parallel processing. They also have
libraries for interacting with the Wikipedia API.

Here&#39;s a Python version that incorporates some of these suggestions:

```python
import requests
import itertools
import multiprocessing

def check_acronym(acronym):
    response = requests.get(f&#34;https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles={acronym}&#34;)
    if &#39;&#34;missing&#34;:&#34;&#34;&#39; in response.text:
        return acronym
    return None

def generate_acronyms(length):
    return [&#39;&#39;.join(acronym) for acronym in itertools.product(&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;, repeat=length)]

def main():
    with multiprocessing.Pool() as pool:
        for length in range(1, 7):
            acronyms = generate_acronyms(length)
            for chunk in chunks(acronyms, 50):
                results = pool.map(check_acronym, chunk)
                for result in results:
                    if result is not None:
                        print(result)

if __name__ == &#34;__main__&#34;:
    main()
```

This Python script uses the `itertools.product` function to generate acronyms, and
the `multiprocessing.Pool` class to check acronyms in parallel. It also divides the
acronyms into chunks of 50 to take advantage of Wikipedia&#39;s batch API feature.</code></pre>
              <p>This generates an error in Python3: <code>NameError: name &#39;chunks&#39; is not defined</code>, and GPT-4 corrects the script by adding in a definition for <code>chunks</code>:</p>
              <div id="cb10">
                <pre><code><span id="cb10-1"><span>def</span> chunks(lst, n):</span>
<span id="cb10-2">    <span>for</span> i <span>in</span> <span>range</span>(<span>0</span>, <span>len</span>(lst), n):</span>
<span id="cb10-3">        <span>yield</span> lst[i:i <span>+</span> n]</span></code></pre>
              </div>
              <p>into the corrected version:</p>
              <pre><code>The error occurs because the `chunks` function isn&#39;t defined, and `f-string` formatting
isn&#39;t supported in Python 2.

Here&#39;s the corrected script:</code></pre>
              <div id="cb12">
                <pre><code><span id="cb12-1"><span>import</span> requests</span>
<span id="cb12-2"><span>import</span> itertools</span>
<span id="cb12-3"><span>import</span> multiprocessing</span>
<span id="cb12-4"></span>
<span id="cb12-5"><span>def</span> check_acronym(acronym):</span>
<span id="cb12-6">    response <span>=</span> requests.get(<span>&#34;https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=</span><span>{}</span><span>&#34;</span>.<span>format</span>(acronym))</span>
<span id="cb12-7">    <span>if</span> <span>&#39;&#34;missing&#34;:&#34;&#34;&#39;</span> <span>in</span> response.text:</span>
<span id="cb12-8">        <span>return</span> acronym</span>
<span id="cb12-9">    <span>return</span> <span>None</span></span>
<span id="cb12-10"></span>
<span id="cb12-11"><span>def</span> generate_acronyms(length):</span>
<span id="cb12-12">    <span>return</span> [<span>&#39;&#39;</span>.join(acronym) <span>for</span> acronym <span>in</span> itertools.product(<span>&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;</span>, repeat<span>=</span>length)]</span>
<span id="cb12-13"></span>
<span id="cb12-14"><span>def</span> chunks(lst, n):</span>
<span id="cb12-15">    <span>for</span> i <span>in</span> <span>range</span>(<span>0</span>, <span>len</span>(lst), n):</span>
<span id="cb12-16">        <span>yield</span> lst[i:i <span>+</span> n]</span>
<span id="cb12-17"></span>
<span id="cb12-18"><span>def</span> main():</span>
<span id="cb12-19">    <span>with</span> multiprocessing.Pool() <span>as</span> pool:</span>
<span id="cb12-20">        <span>for</span> length <span>in</span> <span>range</span>(<span>1</span>, <span>7</span>):</span>
<span id="cb12-21">            acronyms <span>=</span> generate_acronyms(length)</span>
<span id="cb12-22">            <span>for</span> chunk <span>in</span> chunks(acronyms, <span>50</span>):</span>
<span id="cb12-23">                results <span>=</span> pool.<span>map</span>(check_acronym, chunk)</span>
<span id="cb12-24">                <span>for</span> result <span>in</span> results:</span>
<span id="cb12-25">                    <span>if</span> result <span>is</span> <span>not</span> <span>None</span>:</span>
<span id="cb12-26">                        <span>print</span>(result)</span>
<span id="cb12-27"></span>
<span id="cb12-28"><span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span>:</span>
<span id="cb12-29">    main()</span></code></pre>
              </div>
              <pre><code>This version includes the `chunks` function, which divides a list into chunks of a
specified size. It also replaces the f-string with `str.format`, which is compatible
with Python 2. However, the script is still designed for Python 3 due to its use of
`multiprocessing.Pool` and `yield`.</code></pre>
              <p>This runs and generates within a minute the first hits for unused TLAs, starting with ‘CQK’:</p>
              <div id="cb14">
                <pre><code><span id="cb14-1"><span>$</span> python3 foo.py</span>
<span id="cb14-2"><span># CQK</span></span>
<span id="cb14-3"><span># CQQ</span></span>
<span id="cb14-4"><span># CQZ</span></span>
<span id="cb14-5"><span># CVY</span></span>
<span id="cb14-6"><span># CWZ</span></span>
<span id="cb14-7"><span># CXK</span></span>
<span id="cb14-8"><span># CXU</span></span>
<span id="cb14-9"><span># CXZ</span></span>
<span id="cb14-10"><span># CYV</span></span>
<span id="cb14-11"><span># CYY</span></span>
<span id="cb14-12"><span># CZQ</span></span>
<span id="cb14-13"><span># CZV</span></span>
<span id="cb14-14"><span># DKQ</span></span>
<span id="cb14-15"><span># DKY</span></span>
<span id="cb14-16"><span># DQJ</span></span>
<span id="cb14-17"><span># DQQ</span></span>
<span id="cb14-18"><span># DQW</span></span>
<span id="cb14-19"><span># DUZ</span></span>
<span id="cb14-20"><span># DVJ</span></span>
<span id="cb14-21"><span># DXK</span></span>
<span id="cb14-22"><span># DXQ</span></span>
<span id="cb14-23"><span># DYI</span></span>
<span id="cb14-24"><span># DYJ</span></span>
<span id="cb14-25"><span># DZE</span></span>
<span id="cb14-26"><span># DZK</span></span>
<span id="cb14-27"><span># DZM</span></span>
<span id="cb14-28"><span># DZW</span></span>
<span id="cb14-29"><span># DZX</span></span>
<span id="cb14-30"><span># EBZ</span></span>
<span id="cb14-31"><span># EDZ</span></span>
<span id="cb14-32"><span># EEY</span></span>
<span id="cb14-33"><span># EGQ</span></span>
<span id="cb14-34"><span># EHQ</span></span>
<span id="cb14-35"><span># EHW</span></span>
<span id="cb14-36"><span># EIY</span></span>
<span id="cb14-37"><span># EIZ</span></span>
<span id="cb14-38"><span># EJD</span></span>
<span id="cb14-39"><span># EJM</span></span>
<span id="cb14-40"><span># EJQ</span></span>
<span id="cb14-41"><span># EJX</span></span>
<span id="cb14-42"><span># EJY</span></span>
<span id="cb14-43"><span># EJZ</span></span>
<span id="cb14-44"><span># EKX</span></span>
<span id="cb14-45"><span># EKZ</span></span>
<span id="cb14-46"><span># EOX</span></span>
<span id="cb14-47"><span># EOZ</span></span>
<span id="cb14-48"><span># EQK</span></span>
<span id="cb14-49"><span># EQO</span></span>
<span id="cb14-50"><span># EQQ</span></span>
<span id="cb14-51"><span># ETY</span></span>
<span id="cb14-52"><span># EUW</span></span>
<span id="cb14-53"><span># EVJ</span></span>
<span id="cb14-54"><span># EWZ</span></span>
<span id="cb14-55"><span># EXF</span></span>
<span id="cb14-56"><span># EXG</span></span>
<span id="cb14-57"><span># EXH</span></span>
<span id="cb14-58"><span># EYG</span></span>
<span id="cb14-59"><span># EYH</span></span>
<span id="cb14-60"><span># EYI</span></span>
<span id="cb14-61"><span># EYJ</span></span>
<span id="cb14-62"><span># EYX</span></span>
<span id="cb14-63"><span># EYY</span></span>
<span id="cb14-64"><span># EYZ</span></span>
<span id="cb14-65"><span># EZB</span></span>
<span id="cb14-66"><span># EZJ</span></span>
<span id="cb14-67"><span># EZV</span></span>
<span id="cb14-68"><span># ...</span></span></code></pre>
              </div>
            </section>
            <section id="patterns">
              <h3><a href="#patterns" title="Link to section: § &#39;Patterns&#39;">Patterns</a></h3>
              <section id="sparsity">
                <h4><a href="#sparsity" title="Link to section: § &#39;Sparsity&#39;">Sparsity</a></h4>
                <p>Some statistics:</p>
                <div id="cb15">
                  <pre><code><span id="cb15-1"><span>$</span> wc <span>--lines</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt</span>
<span id="cb15-2"><span># 395,568</span></span>
<span id="cb15-3"></span>
<span id="cb15-4"><span>$</span> grep <span>-E</span> <span>&#39;^[A-Z][A-Z][A-Z]$&#39;</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt <span>|</span> <span>wc</span> <span>--lines</span></span>
<span id="cb15-5"><span># 2,684</span></span>
<span id="cb15-6"><span>R</span><span>&gt;</span> round<span>(</span><span>digits</span><span>=</span>2, <span>2684</span> / <span>(</span><span>26^3</span><span>))</span></span>
<span id="cb15-7"><span># [1] 0.15</span></span>
<span id="cb15-8"></span>
<span id="cb15-9"><span>$</span> grep <span>-E</span> <span>&#39;^[A-Z][A-Z][A-Z][A-Z]$&#39;</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt <span>|</span> <span>wc</span> <span>--lines</span></span>
<span id="cb15-10"><span># 392,884</span></span>
<span id="cb15-11"><span>R</span><span>&gt;</span> round<span>(</span><span>digits</span><span>=</span>2, <span>392884</span> / <span>(</span><span>26^4</span><span>))</span></span>
<span id="cb15-12"><span># [1] 0.86</span></span></code></pre>
                </div>
                <p>Apparently, TLAs are surprisingly sparse, with &lt;15% unused, but as expected, FLAs are sparse, with the overwhelming majority.</p>
              </section>
              <section id="letter-frequency-effect">
                <h4><a href="#letter-frequency-effect" title="Link to section: § &#39;Letter Frequency Effect&#39;">Letter Frequency Effect</a></h4>
                <p>There are clear patterns with vowel vs consonants and letter frequency in particular: even just looking at the C–E TLAs above, you can see that consonants and rare letters like W–Z are overrepresented.</p>
                <p>Is this all that is going on? I investigated in R, using GPT-4 again. (This analysis is the sort of finicky string-munging &amp; data-frame processing I find most tedious in R, and it’s much more pleasant to leave it to GPT-4; GPT-4’s R code never seems to hit <a href="#blind-spot">the ‘blind spot’</a>, and it is generally able to fix code given an error message.)</p>
                <p>We load the unused TLA dataset, turn it into a dataset of <em>all</em> TLAs, classified by whether they are unused or not:</p>
                <div id="cb16">
                  <pre><code><span id="cb16-1">tla <span>&lt;-</span> <span>read.table</span>(<span>&#34;https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt&#34;</span>)</span>
<span id="cb16-2"><span>head</span>(tla)</span>
<span id="cb16-3"><span>#    V1</span></span>
<span id="cb16-4"><span># 1 CQK</span></span>
<span id="cb16-5"><span># 2 CQQ</span></span>
<span id="cb16-6"><span># 3 CQZ</span></span>
<span id="cb16-7"><span># 4 CVY</span></span>
<span id="cb16-8"><span># 5 CWZ</span></span>
<span id="cb16-9"><span># 6 CXK</span></span>
<span id="cb16-10">tla<span>$</span>V2 <span>&lt;-</span> <span>as.character</span>(tla<span>$</span>V1)</span>
<span id="cb16-11"></span>
<span id="cb16-12">letters <span>&lt;-</span> <span>c</span>(<span>&#34;A&#34;</span>, <span>&#34;B&#34;</span>, <span>&#34;C&#34;</span>, <span>&#34;D&#34;</span>, <span>&#34;E&#34;</span>, <span>&#34;F&#34;</span>, <span>&#34;G&#34;</span>, <span>&#34;H&#34;</span>, <span>&#34;I&#34;</span>, <span>&#34;J&#34;</span>, <span>&#34;K&#34;</span>, <span>&#34;L&#34;</span>, <span>&#34;M&#34;</span>,</span>
<span id="cb16-13">            <span>&#34;N&#34;</span>, <span>&#34;O&#34;</span>, <span>&#34;P&#34;</span>, <span>&#34;Q&#34;</span>, <span>&#34;R&#34;</span>, <span>&#34;S&#34;</span>, <span>&#34;T&#34;</span>, <span>&#34;U&#34;</span>, <span>&#34;V&#34;</span>, <span>&#34;W&#34;</span>, <span>&#34;X&#34;</span>, <span>&#34;Y&#34;</span>, <span>&#34;Z&#34;</span>)</span>
<span id="cb16-14"></span>
<span id="cb16-15">acronyms <span>&lt;-</span> <span>expand.grid</span>(letters, letters, letters)</span>
<span id="cb16-16">acronyms_vector <span>&lt;-</span> <span>apply</span>(acronyms, <span>1</span>, paste0, <span>collapse =</span> <span>&#34;&#34;</span>)</span>
<span id="cb16-17"><span>head</span>(acronyms_vector); <span>length</span>(acronyms_vector)</span>
<span id="cb16-18"><span># [1] &#34;AAA&#34; &#34;BAA&#34; &#34;CAA&#34; &#34;DAA&#34; &#34;EAA&#34; &#34;FAA&#34;</span></span>
<span id="cb16-19"><span># [1] 17576</span></span>
<span id="cb16-20"></span>
<span id="cb16-21"><span># Function to generate acronyms of a given length:</span></span>
<span id="cb16-22">generate_acronyms <span>&lt;-</span> <span>function</span>(len) {</span>
<span id="cb16-23">  acronyms <span>&lt;-</span> <span>expand.grid</span>(<span>rep</span>(<span>list</span>(letters), len))</span>
<span id="cb16-24">  <span>apply</span>(acronyms, <span>1</span>, paste0, <span>collapse =</span> <span>&#34;&#34;</span>)</span>
<span id="cb16-25">}</span>
<span id="cb16-26"></span>
<span id="cb16-27"><span># Generate 3- and 4-letter acronyms:</span></span>
<span id="cb16-28">acronyms_vector <span>&lt;-</span> <span>unlist</span>(<span>lapply</span>(<span>3</span><span>:</span><span>4</span>, generate_acronyms))</span>
<span id="cb16-29"></span>
<span id="cb16-30"><span># Create data frame and update &#39;Missing&#39;:</span></span>
<span id="cb16-31">acronyms_df <span>&lt;-</span> <span>data.frame</span>(<span>V1 =</span> acronyms_vector, <span>V2 =</span> acronyms_vector, <span>Missing =</span> <span>FALSE</span>, <span>stringsAsFactors =</span> <span>FALSE</span>)</span>
<span id="cb16-32">acronyms_df<span>$</span>Missing[acronyms_df<span>$</span>V2 <span>%in%</span> tla<span>$</span>V2] <span>&lt;-</span> <span>TRUE</span></span>
<span id="cb16-33"></span>
<span id="cb16-34"><span>## Add a &#39;Missing&#39; column to &#39;tla&#39;</span></span>
<span id="cb16-35">tla<span>$</span>Missing <span>&lt;-</span> <span>TRUE</span></span>
<span id="cb16-36"></span>
<span id="cb16-37"><span>## Combine the two data-frames</span></span>
<span id="cb16-38">result <span>&lt;-</span> <span>rbind</span>(tla, acronyms_df[<span>!</span>acronyms_df<span>$</span>V2 <span>%in%</span> tla<span>$</span>V2, ])</span>
<span id="cb16-39">result <span>&lt;-</span> result[<span>order</span>(result<span>$</span>V2), ] <span># sort in alphabetic order</span></span>
<span id="cb16-40"><span>summary</span>(result)</span>
<span id="cb16-41"><span>#       V1              V2             Missing</span></span>
<span id="cb16-42"><span># AAAB   :     1   Length:410460      Mode :logical</span></span>
<span id="cb16-43"><span># AAAG   :     1   Class :character   FALSE:14892</span></span>
<span id="cb16-44"><span># AAAK   :     1   Mode  :character   TRUE :395568</span></span>
<span id="cb16-45"><span># AAAO   :     1</span></span>
<span id="cb16-46"><span># AAAQ   :     1</span></span>
<span id="cb16-47"><span># AAAU   :     1</span></span>
<span id="cb16-48"><span># (Other):410454</span></span>
<span id="cb16-49"><span>head</span>(result); <span>tail</span>(result)</span>
<span id="cb16-50"><span>#           V1   V2 Missing</span></span>
<span id="cb16-51"><span># 1100000  AAA  AAA   FALSE</span></span>
<span id="cb16-52"><span># 2685    AAAB AAAB    TRUE</span></span>
<span id="cb16-53"><span># 2686    AAAG AAAG    TRUE</span></span>
<span id="cb16-54"><span># 2687    AAAK AAAK    TRUE</span></span>
<span id="cb16-55"><span># 2688    AAAO AAAO    TRUE</span></span>
<span id="cb16-56"><span># 2689    AAAQ AAAQ    TRUE</span></span>
<span id="cb16-57"><span>#          V1   V2 Missing</span></span>
<span id="cb16-58"><span># 395563 ZZZT ZZZT    TRUE</span></span>
<span id="cb16-59"><span># 395564 ZZZU ZZZU    TRUE</span></span>
<span id="cb16-60"><span># 395565 ZZZV ZZZV    TRUE</span></span>
<span id="cb16-61"><span># 395566 ZZZW ZZZW    TRUE</span></span>
<span id="cb16-62"><span># 395567 ZZZX ZZZX    TRUE</span></span>
<span id="cb16-63"><span># 395568 ZZZY ZZZY    TRUE</span></span>
<span id="cb16-64">index <span>&lt;-</span> <span>which</span>(result_tla<span>$</span>V2 <span>==</span> <span>&#34;CQK&#34;</span>)</span>
<span id="cb16-65">percentage <span>&lt;-</span> index <span>/</span> <span>nrow</span>(result_tla) <span>*</span> <span>100</span>; percentage</span>
<span id="cb16-66"><span># [1] 10.1217569</span></span>
<span id="cb16-67"></span>
<span id="cb16-68"><span>## Visualize missingness:</span></span>
<span id="cb16-69">result_tla <span>&lt;-</span> result[<span>nchar</span>(result<span>$</span>V2) <span>==</span> <span>3</span>, ]</span>
<span id="cb16-70">result_fla <span>&lt;-</span> result[<span>nchar</span>(result<span>$</span>V2) <span>==</span> <span>4</span>, ]</span>
<span id="cb16-71"></span>
<span id="cb16-72">dimensions <span>&lt;-</span> <span>round</span>(<span>sqrt</span>(<span>length</span>(result_tla<span>$</span>Missing))); dimensions <span># 133</span></span>
<span id="cb16-73"><span>png</span>(<span>file=</span><span>&#34;~/wiki/doc/wikipedia/2023-11-04-gwern-tla-missingness.png&#34;</span>, <span>width =</span> <span>2400</span>, <span>height =</span> <span>2400</span>)</span>
<span id="cb16-74"><span>image</span>(<span>t</span>(<span>matrix</span>(<span>rev</span>(result_tla<span>$</span>Missing), <span>nrow=</span>dimensions, <span>ncol=</span>dimensions, <span>byrow=</span><span>TRUE</span>)), <span>col=</span><span>gray.colors</span>(<span>2</span>))</span>
<span id="cb16-75"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb16-76"></span>
<span id="cb16-77">dimensions <span>&lt;-</span> <span>round</span>(<span>sqrt</span>(<span>length</span>(result_fla<span>$</span>Missing))); dimensions <span># 676</span></span>
<span id="cb16-78"><span>png</span>(<span>file=</span><span>&#34;~/wiki/doc/wikipedia/2023-11-04-gwern-fla-missingness.png&#34;</span>, <span>width =</span> <span>2400</span>, <span>height =</span> <span>2400</span>)</span>
<span id="cb16-79"><span>image</span>(<span>t</span>(<span>matrix</span>(<span>rev</span>(result_fla<span>$</span>Missing), <span>nrow=</span>dimensions, <span>ncol=</span>dimensions, <span>byrow=</span><span>TRUE</span>)), <span>col=</span><span>gray.colors</span>(<span>2</span>))</span>
<span id="cb16-80"><span>invisible</span>(<span>dev.off</span>())</span></code></pre>
                </div>
                <figure>
                  <img alt="Visualization of missingness of TLAs, A–Z (wrapped into a 133×133 grid; read: top to bottom, left to right); the first dot at top-left 10% of the way through is “CQK”." decoding="async" height="1400" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-11-04-gwern-tla-missingness.png" width="1400"/>
                  <figcaption aria-hidden="true">
                    Visualization of missingness of TLAs, A–Z (wrapped into a 133×133 grid; read: top to bottom, left to right); the first dot at top-left 10% of the way through is “CQK”.
                  </figcaption>
                </figure>
                <figure>
                  <img alt="Visualization of missingness of four-letter-acronyms, A–Z (wrapped into a 676×676 grid; read: top to bottom, left to right)." decoding="async" height="1400" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-11-04-gwern-fla-missingness.png" width="1400"/>
                  <figcaption aria-hidden="true">
                    Visualization of missingness of four-letter-acronyms, A–Z (wrapped into a 676×676 grid; read: top to bottom, left to right).
                  </figcaption>
                </figure>
                <p>We would like to investigate per-letter properties, like all TLAs with a ‘Z’ in them, so we set up 26 dummy variables for whether each letter is present:</p>
                <div id="cb17">
                  <pre><code><span id="cb17-1"><span>library</span>(stringr)</span>
<span id="cb17-2"><span>for</span> (letter <span>in</span> letters) {</span>
<span id="cb17-3">  result[[letter]] <span>&lt;-</span> <span>str_detect</span>(result<span>$</span>V2, <span>fixed</span>(letter))</span>
<span id="cb17-4">}</span>
<span id="cb17-5"><span>head</span>(result, <span>n=</span><span>1</span>)</span>
<span id="cb17-6"><span>#    V1  V2 Missing     A     B    C     D     E     F     G     H     I     J</span></span>
<span id="cb17-7"><span># 1 CQK CQK    TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span id="cb17-8"><span>#      K     L     M     N     O     P    Q     R     S     T     U     V     W</span></span>
<span id="cb17-9"><span>#   TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span id="cb17-10"><span>#       X     Y     Z</span></span>
<span id="cb17-11"><span>#   FALSE FALSE FALSE</span></span></code></pre>
                </div>
                <p>With a <code>Missing</code> boolean variable &amp; the alphabetical dummy variables set up, we can do a <a href="https://en.wikipedia.org/wiki/Logistic_regression" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Logistic_regression#bodyContent">logistic regression</a>, where each acronym is a single datapoint, and the letters it contains are the covariates:</p>
                <div id="cb18">
                  <pre><code><span id="cb18-1"><span>## Define the formula for the logistic regression model without intercept (as an acronym always has letters):</span></span>
<span id="cb18-2">formula <span>&lt;-</span> <span>as.formula</span>(<span>paste</span>(<span>&#34;Missing ~ 0 +&#34;</span>, <span>paste</span>(letters, <span>collapse =</span> <span>&#34; + &#34;</span>)))</span>
<span id="cb18-3"></span>
<span id="cb18-4"><span>## Fit the logistic regression model</span></span>
<span id="cb18-5">model_no_intercept <span>&lt;-</span> <span>glm</span>(formula, <span>data =</span> result, <span>family =</span> <span>binomial</span>(<span>link =</span> <span>&#34;logit&#34;</span>))</span>
<span id="cb18-6"><span>summary</span>(model_no_intercept)</span>
<span id="cb18-7"><span># ...Deviance Residuals:</span></span>
<span id="cb18-8"><span>#        Min          1Q      Median          3Q         Max</span></span>
<span id="cb18-9"><span># −2.8134706   0.0885086   0.1136063   0.1540683   2.7432638</span></span>
<span id="cb18-10"><span>#</span></span>
<span id="cb18-11"><span># Coefficients:</span></span>
<span id="cb18-12"><span>#          Estimate Std. Error   z value   Pr(&gt;|z|)</span></span>
<span id="cb18-13"><span># AFALSE −6.5878352  0.0568884 −115.8028 &lt; 2.22e-16</span></span>
<span id="cb18-14"><span># ATRUE  −4.0755472  0.0479506  −84.9948 &lt; 2.22e-16</span></span>
<span id="cb18-15"><span># BTRUE   2.7764680  0.0322792   86.0142 &lt; 2.22e-16</span></span>
<span id="cb18-16"><span># CTRUE   2.4976112  0.0317669   78.6231 &lt; 2.22e-16</span></span>
<span id="cb18-17"><span># DTRUE   2.7903755  0.0324820   85.9053 &lt; 2.22e-16</span></span>
<span id="cb18-18"><span># ETRUE   2.8485807  0.0328888   86.6124 &lt; 2.22e-16</span></span>
<span id="cb18-19"><span># FTRUE   2.8527917  0.0327796   87.0295 &lt; 2.22e-16</span></span>
<span id="cb18-20"><span># GTRUE   2.9681415  0.0333202   89.0794 &lt; 2.22e-16</span></span>
<span id="cb18-21"><span># HTRUE   2.9743513  0.0333339   89.2290 &lt; 2.22e-16</span></span>
<span id="cb18-22"><span># ITRUE   2.8127265  0.0328846   85.5333 &lt; 2.22e-16</span></span>
<span id="cb18-23"><span># JTRUE   3.2605341  0.0352484   92.5016 &lt; 2.22e-16</span></span>
<span id="cb18-24"><span># KTRUE   2.8210890  0.0334714   84.2836 &lt; 2.22e-16</span></span>
<span id="cb18-25"><span># LTRUE   2.8388725  0.0328000   86.5511 &lt; 2.22e-16</span></span>
<span id="cb18-26"><span># MTRUE   2.7014280  0.0321802   83.9469 &lt; 2.22e-16</span></span>
<span id="cb18-27"><span># NTRUE   2.8776599  0.0330334   87.1137 &lt; 2.22e-16</span></span>
<span id="cb18-28"><span># OTRUE   2.9512577  0.0334268   88.2902 &lt; 2.22e-16</span></span>
<span id="cb18-29"><span># PTRUE   2.7394627  0.0324478   84.4268 &lt; 2.22e-16</span></span>
<span id="cb18-30"><span># QTRUE   3.6899056  0.0393598   93.7482 &lt; 2.22e-16</span></span>
<span id="cb18-31"><span># RTRUE   2.8297209  0.0329451   85.8919 &lt; 2.22e-16</span></span>
<span id="cb18-32"><span># STRUE   2.5292698  0.0319661   79.1235 &lt; 2.22e-16</span></span>
<span id="cb18-33"><span># TTRUE   2.7727895  0.0325522   85.1797 &lt; 2.22e-16</span></span>
<span id="cb18-34"><span># UTRUE   3.0611813  0.0338695   90.3817 &lt; 2.22e-16</span></span>
<span id="cb18-35"><span># VTRUE   3.1443269  0.0344698   91.2199 &lt; 2.22e-16</span></span>
<span id="cb18-36"><span># WTRUE   2.8428509  0.0337547   84.2208 &lt; 2.22e-16</span></span>
<span id="cb18-37"><span># XTRUE   3.5003099  0.0375402   93.2417 &lt; 2.22e-16</span></span>
<span id="cb18-38"><span># YTRUE   3.2994364  0.0356499   92.5510 &lt; 2.22e-16</span></span>
<span id="cb18-39"><span># ZTRUE   3.5370118  0.0378513   93.4448 &lt; 2.22e-16</span></span>
<span id="cb18-40"><span>#</span></span>
<span id="cb18-41"><span># (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb18-42"><span>#</span></span>
<span id="cb18-43"><span>#     Null deviance: 569018.38  on 410460  degrees of freedom</span></span>
<span id="cb18-44"><span># Residual deviance:  88916.78  on 410433  degrees of freedom</span></span>
<span id="cb18-45"><span># AIC: 88970.78</span></span></code></pre>
                </div>
                <p>As expected, the letter makes a difference, and rarer letters like ‘Z’ or ‘J’ are especially likely to correlate with unuse.</p>
                <p>We can plot an absolute plot<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a>, but that wouldn’t be a good visualization because unused/used must sum to 100%, so it’d be better to do a relative percentage plot, like this:</p>
                <div id="cb20">
                  <pre><code><span id="cb20-1">letter_df<span>$</span>V1 <span>&lt;-</span> <span>as.character</span>(letter_df<span>$</span>V1)</span>
<span id="cb20-2"><span>## Filter out four-letter acronyms</span></span>
<span id="cb20-3">three_letter_df <span>&lt;-</span> letter_df[<span>nchar</span>(letter_df<span>$</span>V1) <span>==</span> <span>3</span>, ]</span>
<span id="cb20-4"></span>
<span id="cb20-5"><span>## Calculate counts</span></span>
<span id="cb20-6">counts <span>&lt;-</span> <span>table</span>(three_letter_df<span>$</span>Letter, three_letter_df<span>$</span>Missing)</span>
<span id="cb20-7"><span>## Calculate relative percentages</span></span>
<span id="cb20-8">percentages <span>&lt;-</span> <span>prop.table</span>(counts, <span>1</span>)</span>
<span id="cb20-9"><span>## Convert to data frame for plotting</span></span>
<span id="cb20-10">percentages_df <span>&lt;-</span> <span>as.data.frame.table</span>(percentages, <span>responseName =</span> <span>&#34;Percentage&#34;</span>)</span>
<span id="cb20-11"></span>
<span id="cb20-12"><span>## Plot</span></span>
<span id="cb20-13"><span>library</span>(ggplot2)</span>
<span id="cb20-14"><span>ggplot</span>(percentages_df, <span>aes</span>(<span>x =</span> Var1, <span>y =</span> Percentage, <span>fill =</span> Var2)) <span>+</span></span>
<span id="cb20-15">  <span>geom_col</span>() <span>+</span></span>
<span id="cb20-16">  <span>theme_minimal</span>(<span>base_size =</span> <span>50</span>) <span>+</span></span>
<span id="cb20-17">  <span>theme</span>(<span>legend.position =</span> <span>&#34;none&#34;</span>) <span>+</span></span>
<span id="cb20-18">  <span>labs</span>(<span>x =</span> <span>&#34;Letter&#34;</span>, <span>y =</span> <span>&#34;Percentage&#34;</span>, <span>fill =</span> <span>&#34;Missing&#34;</span>)</span></code></pre>
                </div>
                <figure>
                  <img alt="Bar plot of unused fraction, by alphabetic letter (A–Z): Rarer letters (eg. ‘Z’) more likely to be unused in TLAs, but usage not fully explained by letter frequency (eg. ‘A’)." decoding="async" height="794" id="bar-plot" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-10-01-gwern-tla-lettervsunusedtlaswiththatletterpercentageoverthealphabet.png" title="Statistical bar chart of dataset of all three-letter acronyms by whether they have an English Wikipedia article; each letter is extracted and coded by whether that acronym was missing; red means the associated acronym was used, and blue means unused. The fraction of blue increases through the alphabet, as expected." width="1400"/>
                  <figcaption aria-hidden="true">
                    <strong>Bar plot of unused fraction, by alphabetic letter (A–Z)</strong>: Rarer letters (eg. ‘Z’) more likely to be unused in TLAs, but usage not fully explained by letter frequency (eg. ‘A’).
                  </figcaption>
                </figure>
                <p>The plot makes sense, but some things are anomalous: like the letter ‘A’—it’s perhaps the rarest of all letters, and yet, if any letter is rarest, it ought to be ‘E’, one would think, because everyone knows ‘E’ is the most common letter in the English language. What is the correlation with letter frequency? We can take a letter frequency list (<a href="https://pi.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html">first one I found in Google</a>), and look at the correlation. We use a <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#bodyContent">Kendall rank correlation</a> because there’s no particular reason to think that the magnitude or distribution of either the logistic regression coefficients or the letter frequencies are <a href="https://en.wikipedia.org/wiki/Normal_distribution" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Normal_distribution#bodyContent">normally distributed</a>, and we just think that there should be an inverse correlation when ordered: rarer letters = more-unused TLAs.</p>
                <div id="cb21">
                  <pre><code><span id="cb21-1"><span>## https://pi.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html</span></span>
<span id="cb21-2">frequencies <span>&lt;-</span> <span>read.table</span>(<span>stdin</span>(), <span>header=</span><span>TRUE</span>, <span>colClasses=</span><span>c</span>(<span>&#34;factor&#34;</span>,<span>&#34;integer&#34;</span>,<span>&#34;character&#34;</span>,<span>&#34;numeric&#34;</span>))</span>
<span id="cb21-3">Letter  Count       Letter  Frequency</span>
<span id="cb21-4">E   <span>21912</span>       E   <span>12.02</span></span>
<span id="cb21-5">T   <span>16587</span>       T   <span>9.10</span></span>
<span id="cb21-6">A   <span>14810</span>       A   <span>8.12</span></span>
<span id="cb21-7">O   <span>14003</span>       O   <span>7.68</span></span>
<span id="cb21-8">I   <span>13318</span>       I   <span>7.31</span></span>
<span id="cb21-9">N   <span>12666</span>       N   <span>6.95</span></span>
<span id="cb21-10">S   <span>11450</span>       S   <span>6.28</span></span>
<span id="cb21-11">R   <span>10977</span>       R   <span>6.02</span></span>
<span id="cb21-12">H   <span>10795</span>       H   <span>5.92</span></span>
<span id="cb21-13">D   <span>7874</span>        D   <span>4.32</span></span>
<span id="cb21-14">L   <span>7253</span>        L   <span>3.98</span></span>
<span id="cb21-15">U   <span>5246</span>        U   <span>2.88</span></span>
<span id="cb21-16">C   <span>4943</span>        C   <span>2.71</span></span>
<span id="cb21-17">M   <span>4761</span>        M   <span>2.61</span></span>
<span id="cb21-18">F   <span>4200</span>        F   <span>2.30</span></span>
<span id="cb21-19">Y   <span>3853</span>        Y   <span>2.11</span></span>
<span id="cb21-20">W   <span>3819</span>        W   <span>2.09</span></span>
<span id="cb21-21">G   <span>3693</span>        G   <span>2.03</span></span>
<span id="cb21-22">P   <span>3316</span>        P   <span>1.82</span></span>
<span id="cb21-23">B   <span>2715</span>        B   <span>1.49</span></span>
<span id="cb21-24">V   <span>2019</span>        V   <span>1.11</span></span>
<span id="cb21-25">K   <span>1257</span>        K   <span>0.69</span></span>
<span id="cb21-26">X   <span>315</span>         X   <span>0.17</span></span>
<span id="cb21-27">Q   <span>205</span>         Q   <span>0.11</span></span>
<span id="cb21-28">J   <span>188</span>         J   <span>0.10</span></span>
<span id="cb21-29">Z   <span>128</span>         Z   <span>0.07</span></span>
<span id="cb21-30"></span>
<span id="cb21-31"><span>## Put in alphabetic order:</span></span>
<span id="cb21-32">frequencies <span>&lt;-</span> frequencies[<span>order</span>(frequencies<span>$</span>Letter),]</span>
<span id="cb21-33"></span>
<span id="cb21-34">letter_coefficients <span>&lt;-</span> <span>coef</span>(model_no_intercept)</span>
<span id="cb21-35">letter_coefficients[<span>-</span><span>1</span>]</span>
<span id="cb21-36"><span>#       ATRUE       BTRUE       CTRUE       DTRUE       ETRUE       FTRUE</span></span>
<span id="cb21-37"><span># −4.07554725  2.77646803  2.49761116  2.79037554  2.84858066  2.85279167</span></span>
<span id="cb21-38"><span>#       GTRUE       HTRUE       ITRUE       JTRUE       KTRUE       LTRUE</span></span>
<span id="cb21-39"><span>#  2.96814151  2.97435134  2.81272652  3.26053408  2.82108898  2.83887250</span></span>
<span id="cb21-40"><span>#       MTRUE       NTRUE       OTRUE       PTRUE       QTRUE       RTRUE</span></span>
<span id="cb21-41"><span>#  2.70142800  2.87765993  2.95125774  2.73946268  3.68990557  2.82972088</span></span>
<span id="cb21-42"><span>#       STRUE       TTRUE       UTRUE       VTRUE       WTRUE       XTRUE</span></span>
<span id="cb21-43"><span>#  2.52926976  2.77278951  3.06118128  3.14432686  2.84285087  3.50030988</span></span>
<span id="cb21-44"><span>#       YTRUE       ZTRUE</span></span>
<span id="cb21-45"><span>#  3.29943643  3.53701178</span></span>
<span id="cb21-46"><span>cor</span>(letter_coefficients[<span>-</span><span>1</span>], frequencies<span>$</span>Frequency)</span>
<span id="cb21-47"><span># [1] −0.357640466</span></span></code></pre>
                </div>
              </section>
              <section id="order-letter-frequency-effects">
                <h4><a href="#order-letter-frequency-effects" title="Link to section: § &#39;Order &amp; Letter-Frequency Effects&#39;">Order &amp; Letter-Frequency Effects</a></h4>
                <p>What is omitted from our model? Going back and comparing the frequency list to <a href="#bar-plot">the previous bar plot</a>{#previous-back-link], it looks suspiciously like letters early in the alphabet (not just ‘A’) are overrepresented, and then ‘Z’ is inflated (perhaps because it is the final letter and has many connotations).</p>
                <p>How do we encode in ‘early letters’, as contrasted with ‘letter frequency’? We can add into the logistic regression a variable for the earliest/‘smallest’ letter an acronym has, a <code>MinIndex</code>. (This would help pick up trends from trying to abuse ‘A’ to sort first in any list or sublist.) And we can encode the letter frequencies by just averaging them, as a <code>AvgFrequency</code>. (Clearly imperfect, but also unclear what the right thing to do would be: instead of an arithmetic mean, a harmonic mean? Something else entirely?) Then we can add them to the regression as control variables to try to explain away their effects:</p>
                <div id="cb22">
                  <pre><code><span id="cb22-1"><span>## Add a column &#39;MinIndex&#39; to &#39;result&#39; that contains the smallest index of the letters in each acronym</span></span>
<span id="cb22-2">result<span>$</span>MinIndex <span>&lt;-</span> <span>apply</span>(result[,letters], <span>1</span>, <span>function</span>(x) <span>min</span>(<span>which</span>(x)))</span>
<span id="cb22-3"><span>## Map the letters to their frequencies</span></span>
<span id="cb22-4">letter_to_frequency <span>&lt;-</span> <span>setNames</span>(frequencies<span>$</span>Frequency <span>/</span> <span>100</span>, frequencies<span>$</span>Letter)</span>
<span id="cb22-5"></span>
<span id="cb22-6"><span>## Add a column &#39;AvgFrequency&#39; to &#39;result&#39; that contains the average frequency of the letters in each acronym</span></span>
<span id="cb22-7"><span>## Compute the frequency of each letter in each acronym</span></span>
<span id="cb22-8">acronym_frequencies <span>&lt;-</span> <span>lapply</span>(<span>strsplit</span>(result<span>$</span>V2, <span>&#34;&#34;</span>), <span>function</span>(acronym) letter_to_frequency[acronym])</span>
<span id="cb22-9"><span>## Compute the average frequency for each acronym</span></span>
<span id="cb22-10">result<span>$</span>AvgFrequency <span>&lt;-</span> <span>sapply</span>(acronym_frequencies, mean)</span>
<span id="cb22-11"></span>
<span id="cb22-12"><span>## Archive results to `/doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-processeddata.csv.xz`:</span></span>
<span id="cb22-13"><span>write.csv</span>(result, <span>file=</span><span>&#34;doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-processeddata.csv&#34;</span>, <span>row.names=</span><span>FALSE</span>)</span>
<span id="cb22-14"></span>
<span id="cb22-15"><span>## Update the formula to include &#39;MinIndex&#39; &amp; &#39;AvgFrequency&#39; as a covariate:</span></span>
<span id="cb22-16">formulaControlled <span>&lt;-</span> <span>as.formula</span>(<span>paste</span>(<span>&#34;Missing ~ MinIndex + AvgFrequency +&#34;</span>, <span>paste</span>(letters, <span>collapse =</span> <span>&#34; + &#34;</span>)))</span>
<span id="cb22-17"><span>## Fit the logistic regression model</span></span>
<span id="cb22-18">modelControlled <span>&lt;-</span> <span>glm</span>(formulaControlled, <span>data =</span> result, <span>family =</span> <span>binomial</span>(<span>link =</span> <span>&#34;logit&#34;</span>))</span>
<span id="cb22-19"><span>summary</span>(modelControlled)</span>
<span id="cb22-20"><span># ...Deviance Residuals:</span></span>
<span id="cb22-21"><span>#        Min          1Q      Median          3Q         Max</span></span>
<span id="cb22-22"><span># −2.9069303   0.0891105   0.1128827   0.1500609   2.8642110</span></span>
<span id="cb22-23"><span>#</span></span>
<span id="cb22-24"><span># Coefficients:</span></span>
<span id="cb22-25"><span>#                 Estimate  Std. Error   z value   Pr(&gt;|z|)</span></span>
<span id="cb22-26"><span># (Intercept)  −6.90634502  0.09250317 −74.66063 &lt; 2.22e-16</span></span>
<span id="cb22-27"><span># MinIndex      0.04029557  0.00370886  10.86468 &lt; 2.22e-16</span></span>
<span id="cb22-28"><span># AvgFrequency −4.25188554  1.42341896  −2.98709  0.0028164</span></span>
<span id="cb22-29"><span># ATRUE         2.88138189  0.04677432  61.60179 &lt; 2.22e-16</span></span>
<span id="cb22-30"><span># BTRUE         3.01005967  0.04276745  70.38203 &lt; 2.22e-16</span></span>
<span id="cb22-31"><span># CTRUE         2.71712516  0.03922165  69.27616 &lt; 2.22e-16</span></span>
<span id="cb22-32"><span># DTRUE         2.99290423  0.03752173  79.76456 &lt; 2.22e-16</span></span>
<span id="cb22-33"><span># ETRUE         3.13077197  0.05264291  59.47187 &lt; 2.22e-16</span></span>
<span id="cb22-34"><span># FTRUE         2.97227342  0.03637453  81.71305 &lt; 2.22e-16</span></span>
<span id="cb22-35"><span># GTRUE         3.05866771  0.03637991  84.07573 &lt; 2.22e-16</span></span>
<span id="cb22-36"><span># HTRUE         3.09923276  0.03586679  86.40954 &lt; 2.22e-16</span></span>
<span id="cb22-37"><span># ITRUE         2.94019473  0.03734308  78.73466 &lt; 2.22e-16</span></span>
<span id="cb22-38"><span># JTRUE         3.26929982  0.04017641  81.37361 &lt; 2.22e-16</span></span>
<span id="cb22-39"><span># KTRUE         2.82703715  0.03688530  76.64401 &lt; 2.22e-16</span></span>
<span id="cb22-40"><span># LTRUE         2.87906510  0.03307471  87.04734 &lt; 2.22e-16</span></span>
<span id="cb22-41"><span># MTRUE         2.71268066  0.03290704  82.43467 &lt; 2.22e-16</span></span>
<span id="cb22-42"><span># NTRUE         2.94220314  0.03616480  81.35544 &lt; 2.22e-16</span></span>
<span id="cb22-43"><span># OTRUE         3.02043913  0.03804621  79.38869 &lt; 2.22e-16</span></span>
<span id="cb22-44"><span># PTRUE         2.72109474  0.03383860  80.41393 &lt; 2.22e-16</span></span>
<span id="cb22-45"><span># QTRUE         3.63855049  0.04328351  84.06320 &lt; 2.22e-16</span></span>
<span id="cb22-46"><span># RTRUE         2.86279675  0.03446914  83.05390 &lt; 2.22e-16</span></span>
<span id="cb22-47"><span># STRUE         2.56086404  0.03376525  75.84317 &lt; 2.22e-16</span></span>
<span id="cb22-48"><span># TTRUE         2.84462320  0.04070488  69.88409 &lt; 2.22e-16</span></span>
<span id="cb22-49"><span># UTRUE         3.04398849  0.03417759  89.06388 &lt; 2.22e-16</span></span>
<span id="cb22-50"><span># VTRUE         3.10148262  0.03683701  84.19474 &lt; 2.22e-16</span></span>
<span id="cb22-51"><span># WTRUE         2.81549885  0.03470352  81.13006 &lt; 2.22e-16</span></span>
<span id="cb22-52"><span># XTRUE         3.44003115  0.04137750  83.13773 &lt; 2.22e-16</span></span>
<span id="cb22-53"><span># YTRUE         3.26779943  0.03654490  89.41876 &lt; 2.22e-16</span></span>
<span id="cb22-54"><span># ZTRUE         3.47511758  0.04188641  82.96528 &lt; 2.22e-16</span></span>
<span id="cb22-55"><span>#</span></span>
<span id="cb22-56"><span># (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb22-57"><span>#</span></span>
<span id="cb22-58"><span>#     Null deviance: 128014.32  on 410459  degrees of freedom</span></span>
<span id="cb22-59"><span># Residual deviance:  88784.22  on 410431  degrees of freedom</span></span>
<span id="cb22-60"><span># AIC: 88842.22</span></span>
<span id="cb22-61"></span>
<span id="cb22-62">letter_coefficients2 <span>&lt;-</span> <span>coef</span>(modelControlled)</span>
<span id="cb22-63"><span>cor</span>(letter_coefficients2[<span>-</span><span>c</span>(<span>1</span><span>:</span><span>3</span>)], frequencies<span>$</span>Frequency, <span>method=</span><span>&#34;kendall&#34;</span>)</span>
<span id="cb22-64"><span># [1] −0.28</span></span>
<span id="cb22-65"></span>
<span id="cb22-66"><span>&gt;</span> <span>1</span> <span>-</span> (<span>0.28</span> <span>/</span> <span>0.35</span>)</span>
<span id="cb22-67"><span># [1] 0.2</span></span></code></pre>
                </div>
                
                <p>These variables do help and are tapping into letter-frequency somewhat (because the rank correlation of the ‘frequency-adjusted’ coefficients shrinks by ~20%), suggesting that both an ‘earlier letter’ &amp; English letter-frequencies are at play in correlating with unused TLAs. But there is still much <a href="https://en.wikipedia.org/wiki/Variance" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Variance#bodyContent">variance</a> unexplained and a non-zero rank correlation, so either these aren’t good ways of quantifying those two effects or there’s still important variables lurking.</p>
              </section>
              <section id="further-work">
                <h4><a href="#further-work" title="Link to section: § &#39;Further Work&#39;">Further Work</a></h4>
                <p><span><em>n</em>-grams.</span> The next step in an analysis might be to adopt the <a href="https://en.wikipedia.org/wiki/N-gram" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/N-gram#bodyContent"><em>n</em>-gram</a> framework, and go from single-letter analysis (unigrams) to pairs of letters (bigrams), as that would help pick up subtler patterns (eg. grammatical patterns in pairs of words that make up acronyms).</p>
                <p><span>Simulation?</span> One could also try to find some way to simulate TLA datasets—I couldn’t figure out a way to generatively-simulate, <a href="https://en.wikipedia.org/wiki/Resampling_(statistics)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Resampling_(statistics)#bodyContent">resample</a>, or <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bootstrapping_(statistics)#bodyContent">bootstrap</a> this dataset, because all the obvious ways to do so either require additional knowledge like <em>how many</em> instances of a given TLA there were, how many TLAs total there were ‘generated’, or are just wrong (at least at the unigram level). If you try to simulate out a set of hypothetical acronyms based on the letter frequencies, then you need to know how many acronyms total are sampled, not merely whether ≥1 instance existed in Wikipedia; if you sampled <em>n</em> = 1 billion, then no TLA would be missing, and if you sampled <em>n</em> = 1, almost all of them must be missing, so that doesn’t work.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> If you try to shuffle the set of missing/non-missing, that will by definition recreate the same single-letter statistics. If you try to resample a smaller dataset bootstrap-style, it’s not clear what that would mean even if it was valid in some sense.</p>
                <p><span>Trends?</span> It would also be mildly interesting to revisit this in a few years to see how acronym usage increases over time. Because notability is irreversible, acronyms should accumulate over time: at what point would people be forced into four-letter acronyms? Or do they just bite the bullet of overlaps because four-letter ones are so much worse? Or perhaps ‘natural’ TLA behavior is that the remainder are <em>so</em> hard to think of (without deliberately targeting unused ones) that the trend is too slow to matter?</p>
                <p>But I’ll leave it here.</p>
              </section>
            </section>
          </section>
        </section>
        <section id="conclusion">
          
          <p>So now I know: there <em>are</em> TLAs available for one to use, even if the specific TLA one wants is probably taken. As TLAs run out, there may be gradually more pressure to figure out how to use the remaining 15%.</p>
          <p>You’ll have to look at the list of unused TLAs, and then figure out how to make the <a href="https://en.wikipedia.org/wiki/Backronym" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Backronym#bodyContent">backronym</a> work, while consulting the back pages of a dictionary… Perhaps one could ask GPT-4 for help?</p>
          <div>
            <blockquote>
              <p><span><strong>I’m founding a startup for selling <a href="https://gwern.net/catnip" id="gwern-catnip" title="‘Catnip immunity and alternatives’, Gwern 2015">psychoactive cat treats</a> like <a href="https://en.wikipedia.org/wiki/Actinidia_polygama#Pets" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Actinidia_polygama#bodyContent">silvervine</a> or <a href="https://en.wikipedia.org/wiki/Catnip" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Catnip#bodyContent">catnip</a> or <a href="https://en.wikipedia.org/wiki/Valerian_(herb)#Effect_on_cats" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Valerian_(herb)#bodyContent">Valerian root extract</a> to pet owners. I want to name it ‘CQK’, because no one has used that acronym for anything yet. But I can’t come up with a good backronym for it. Please come up with as many possible backronyms for ‘CQK’ as you can.</strong></span></p>
              <ol>
                <li>
                  <p>Catnip Quality Kingdom</p>
                </li>
                <li>
                  <p>Cat Quirk Kit</p>
                </li>
                <li>
                  <p>Catnip Quotient Key</p>
                </li>
                <li>
                  <p>Cat Quality Kicks</p>
                </li>
                <li>
                  <p>Catnip Quick Kit</p>
                </li>
                <li>
                  <p>Cats’ Quixotic Knead</p>
                </li>
                <li>
                  <p>Catnip Quest Kit</p>
                </li>
                <li>
                  <p>Cats’ Quality Knead</p>
                </li>
                <li>
                  <p>Cat Quake Kit</p>
                </li>
                <li>
                  <p>Cats’ Quenching Knack</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Kicks</p>
                </li>
                <li>
                  <p>Catnip Quarantine Kit</p>
                </li>
                <li>
                  <p>Cats’ Quirk Kingdom</p>
                </li>
                <li>
                  <p>Catnip Quality Keepsake</p>
                </li>
                <li>
                  <p>Cat Quench Kit</p>
                </li>
                <li>
                  <p>Catnip Quantum Key</p>
                </li>
                <li>
                  <p>Cats’ Quirky Knick-Knacks</p>
                </li>
                <li>
                  <p>Catnip Quirky Kingdom</p>
                </li>
                <li>
                  <p>Cats’ Quality Keeps</p>
                </li>
                <li>
                  <p>Catnip Quenching Kit</p>
                </li>
                <li>
                  <p>Cat Quirk Kingdom</p>
                </li>
                <li>
                  <p>Catnip Quality Kinetics</p>
                </li>
                <li>
                  <p>Cat Quirk Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quotient Kit</p>
                </li>
                <li>
                  <p>Catnip Quantum Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Knick-Knacks</p>
                </li>
                <li>
                  <p>Catnip Quirk Key</p>
                </li>
                <li>
                  <p>Cat Quotient Kit</p>
                </li>
                <li>
                  <p>Catnip Quality Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quake Kit</p>
                </li>
              </ol>
              <p><strong>Come up with another list which does not use ‘cat’ or ‘catnip’ anywhere.</strong></p>
              <ol>
                <li>
                  <p>Cognitive Quirk Kit</p>
                </li>
                <li>
                  <p>Companions’ Quality Kingdom</p>
                </li>
                <li>
                  <p>Creature Quirk Kit</p>
                </li>
                <li>
                  <p>Calm Quotient Key</p>
                </li>
                <li>
                  <p>Companions’ Quenching Knack</p>
                </li>
                <li>
                  <p>Cognitive Quantum Key</p>
                </li>
                <li>
                  <p>Calming Quality Kicks</p>
                </li>
                <li>
                  <p>Companion Quirk Kingdom</p>
                </li>
                <li>
                  <p>Cognitive Quality Kinetics</p>
                </li>
                <li>
                  <p>Calming Quench Kit</p>
                </li>
                <li>
                  <p>Companions’ Quotient Kit</p>
                </li>
                <li>
                  <p>Cognitive Quantum Kiosk</p>
                </li>
                <li>
                  <p>Calm Quirk Kiosk</p>
                </li>
                <li>
                  <p>Companion Quench Kit</p>
                </li>
                <li>
                  <p>Calming Quantum Key</p>
                </li>
                <li>
                  <p>Companions’ Quintessential Knick-Knacks</p>
                </li>
                <li>
                  <p>Cognitive Quirk Key</p>
                </li>
                <li>
                  <p>Calm Quality Kingdom</p>
                </li>
                <li>
                  <p>Companion Quality Kinetics</p>
                </li>
                <li>
                  <p>Calming Quirk Kingdom</p>
                </li>
                <li>
                  <p>Companions’ Quake Kit</p>
                </li>
                <li>
                  <p>Cognitive Quality Kicks</p>
                </li>
                <li>
                  <p>Calm Quotient Kit</p>
                </li>
                <li>
                  <p>Companion Quantum Key</p>
                </li>
                <li>
                  <p>Calming Quotient Kit</p>
                </li>
                <li>
                  <p>Companions’ Quirk Key</p>
                </li>
                <li>
                  <p>Cognitive Quench Kit</p>
                </li>
                <li>
                  <p>Calm Quirk Kingdom</p>
                </li>
                <li>
                  <p>Companion Quantum Kiosk</p>
                </li>
                <li>
                  <p>Calming Quake Kit</p>
                </li>
              </ol>
              <p><strong>Which are the best ones which mention cats or catnip? Which are the best ones which do not mention those?</strong></p>
              <p>Among the ones that mention cats or catnip:</p>
              <ol>
                <li>
                  <p>Catnip Quality Kingdom: Suggests a realm of high-quality catnip products.</p>
                </li>
                <li>
                  <p>Cat Quirk Kit: Implies a kit to engage and entertain the unique behaviors of cats.</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Kicks: Evokes the essential fun and excitement for cats.</p>
                </li>
                <li>
                  <p>Catnip Quantum Key: Suggests a critical or unlocking component in catnip-based products.</p>
                </li>
                <li>
                  <p>Cats’ Quirk Kingdom: Implicates a world dedicated to the idiosyncrasies of cats.</p>
                </li>
              </ol>
              <p>Among the ones that do not mention cats or catnip:</p>
              <ol>
                <li>
                  <p>Cognitive Quirk Kit: Suggests a kit designed to engage and stimulate mental processes.</p>
                </li>
                <li>
                  <p>Companions’ Quality Kingdom: Implies a realm of high-quality products for pets.</p>
                </li>
                <li>
                  <p>Calm Quotient Key: Suggests a product that contributes to a calm and peaceful pet.</p>
                </li>
                <li>
                  <p>Companion Quirk Kingdom: Evokes a world dedicated to the idiosyncrasies of pets.</p>
                </li>
                <li>
                  <p>Cognitive Quality Kinetics: Suggests products designed for mental stimulation and physical activity.</p>
                </li>
              </ol>
            </blockquote>
          </div>
          <p>(I like “Cat Quirk Kit” best, as one could supplement the cat psychoactives with other things, like puzzle treats, or laser pointers, or optical illusion printouts triggering the <a href="https://gwern.net/doc/cat/psychology/2021-smith-2.pdf" id="smith-et-al-2021b" data-link-icon="pdf" data-link-icon-type="svg" title="‘If I fits I sits: A citizen science investigation into illusory contour susceptibility in domestic cats (&lt;em&gt;Felis silvestris catus&lt;/em&gt;)’, Smith et al 2021b">illusory contour effect</a> &amp; the <a href="https://gwern.net/doc/cat/psychology/2018-szenczi.pdf" id="szenczi-et-al-2018" data-link-icon="pdf" data-link-icon-type="svg" title="‘Perception of the Delboeuf illusion by the adult domestic cat (&lt;em&gt;Felis silvestris catus&lt;/em&gt;) in comparison with other mammals’, Szenczi et al 2018">rotating-snake illusion</a>.)</p>
        </section>
        <section id="see-also">
          
          <ul>
            <li>
              <p><a href="https://gwern.net/variable" id="gwern-variable" title="‘Rare Greek Variables’, Gwern 2021">What are the rarest Greek letter variables in scientific writing?</a></p>
            </li>
          </ul>
        </section>
        <section id="appendix">
          
          <section id="unused-numerical-acronyms">
            <h2><a href="#unused-numerical-acronyms" title="Link to section: § &#39;Unused Numerical Acronyms&#39;">Unused Numerical Acronyms</a></h2>
            <p>What if we <em>do</em> want to consider acronyms with numbers in them? We already saw the results for only-alphabetical ones previously, so we need to consider just the subset of number+letter acronyms which have at least 1 letter in them. There are:</p>
            <ol>
              <li>
                <p>For length 1, there are 10 acronyms (the digits 0-9).</p>
              </li>
              <li>
                <p>For length 2, there are 36<sup>2</sup> − 26<sup>2</sup> = 1,296 − 676 = 620 acronyms.</p>
              </li>
              <li>
                <p>For length 3, there are 36<sup>3</sup> − 26<sup>3</sup> = 46,656 − 17,576 = 29,080 acronyms.</p>
              </li>
              <li>
                <p>Adding these up: there are a total of 10 + 620 + 29,080 = 29,710.</p>
              </li>
            </ol>
            <p>Two slight modifications to the previous script will give us the <a href="https://gwern.net/doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-alphanumerical1to3lettertlas.txt" data-link-icon="txt" data-link-icon-type="svg">set of 21,852 unused alphanumerical acronyms</a>, adding in numbers to the generator &amp; skipping target acronyms without numbers:</p>
            <div id="cb23">
              <pre><code><span id="cb23-1"><span>## set the range of acronym lengths</span></span>
<span id="cb23-2"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>3</span><span>}</span></span>
<span id="cb23-3"><span>do</span></span>
<span id="cb23-4">  <span>## define a function to recursively generate acronyms</span></span>
<span id="cb23-5">  <span>generate_acronyms()</span> <span>{</span></span>
<span id="cb23-6">    <span>local</span> <span>current</span><span>=</span><span>$1</span></span>
<span id="cb23-7">    <span>local</span> <span>length</span><span>=</span><span>$2</span></span>
<span id="cb23-8">    <span>if</span> <span>((</span> <span>length</span> <span>==</span> <span>0</span> <span>))</span></span>
<span id="cb23-9">    <span>then</span></span>
<span id="cb23-10">      <span>## query the Wikipedia API</span></span>
<span id="cb23-11">      <span>response</span><span>=</span><span>$(</span><span>curl</span> <span>-s</span> <span>&#34;https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=</span><span>$current</span><span>&#34;</span><span>)</span></span>
<span id="cb23-12">      <span>## check if the page exists</span></span>
<span id="cb23-13">      <span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>&#39;&#34;missing&#34;&#39;</span><span>*</span> <span>]]</span></span>
<span id="cb23-14">      <span>then</span></span>
<span id="cb23-15">        <span>echo</span> <span>&#34;</span><span>$current</span><span>&#34;</span></span>
<span id="cb23-16">      <span>fi</span></span>
<span id="cb23-17">    <span>else</span></span>
<span id="cb23-18">      <span>for</span> letter <span>in</span> <span>{</span>A<span>..</span>Z<span>}</span> <span>{</span><span>0</span><span>..</span><span>9</span><span>}</span></span>
<span id="cb23-19">      <span>do</span></span>
<span id="cb23-20">        <span>generate_acronyms</span> <span>&#34;</span><span>$current$letter</span><span>&#34;</span> <span>$((length</span><span>-</span><span>1</span><span>))</span></span>
<span id="cb23-21">      <span>done</span></span>
<span id="cb23-22">    <span>fi</span></span>
<span id="cb23-23">  <span>}</span></span>
<span id="cb23-24">  <span>## call the function to generate acronyms of the current length</span></span>
<span id="cb23-25">  <span>generate_acronyms</span> <span>&#34;&#34;</span> <span>$length</span></span>
<span id="cb23-26"><span>done</span></span></code></pre>
            </div>
            <p>The first unused one is the rather shocking <span><span><strong>AA0</strong></span> AD0 AE0 AE5 AE6 AE7 AE8 AE9 AF0 AF3 AF5 AF6 AF7 AF8 AF9 AG1 AG2 AG4 AG6 AG7 AG8 AG9 AH0 AI0 AJ0 AJ1 AJ3 AJ5 AJ7 AJ8 AJ9, etc.</span> Really? <em>No one</em> has used such a short simple TLA, which would sort in front of almost everything, even ones like ‘AAA’? Apparently! Neither WP nor Google shows anything important for ‘AA0’.</p>
            <p>So, ‘AA0’ would be a good startup name, if anyone needs one.</p>
          </section>
        </section>
        
        <section id="link-bibliography-section">
          <!-- NOTE: In theory, '.collapse' on a '<h1>' is redundant with the '<section>'; but added to parallel Pandoc-generated headers which set all attributes/classes on both. -->
          <a id="link-bibliography" href="https://gwern.net/metadata/annotation/link-bibliography/%252Ftla.html" title="Bibliography of links cited in this page (forward citations). Lazily-transcluded version at footer of page for easier scrolling.">[bibliography of links/references used in page]</a>
        </section>
      </div></div>
  </body>
</html>
