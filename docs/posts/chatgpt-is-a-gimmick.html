<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hedgehogreview.com/web-features/thr/posts/chatgpt-is-a-gimmick">Original</a>
    <h1>ChatGPT Is a Gimmick</h1>
    
    <div id="readability-page-1" class="page"><div>
                <article>
                    
                    <p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I recently attended a workshop on teaching with artificial intelligence at the university where I teach writing as a part-time adjunct. I had low hopes for the workshop, but I was also desperate. My students keep turning in essays that were obviously generated by AI, and I need to figure out what to do. I looked forward to hearing the keynote speaker, a former university president who made his name arguing that instructors should move all digital technology out of their classrooms so they and their students can focus on the human interactions that technology cannot replace. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>At the workshop, though, the first thing he asked us to do was open our laptops, navigate to a couple of LLMs, and enter a prompt. As we did this, he kept talking. What was I supposed to pay attention to? Him, or my screen? While he jabbered, I prompted Claude.ai to write a short essay in response to one of the “innovative” topics it had proposed to me: a “Change My View Challenge,” based on a Reddit forum. It was important that I use the word <em>innovative</em> in my prompt, the presenter insisted. Omit <em>innovative</em> and you get different, presumably more pedestrian, results. Claude spat out the paper and told me it was proud of its work, which, after all, had a “clear thesis statement.” When I said I couldn’t find the statement, Claude replied, “You’re right to question this. Looking at the essay more carefully, there isn’t a single, explicit thesis statement that clearly states the central argument.” Thanks, genius.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Later, the presenter wanted to demonstrate the amazing feats of pedagogical customization AI is capable of. So he asked a model to create, on the spot, a brief podcast summary of his own book, adding that the model should employ baseball metaphors, because, in this experiment, the user is a jock who cares only about baseball. The idea seemed to be that students would better appreciate the book’s content if it were expressed in terms they are already familiar with. He pressed play. The resulting summary, offered by credibly bland digital hosts, was astonishingly shallow and stupid, wedding one tired cliché (education is “lighting a lamp”) to another (“beware of the curveballs”). Was anybody really learning anything from this?</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The presenter seemed to be trying much too hard. <em>AI can do this! AI can do that!</em> He moved randomly from topic to topic, bouncing around the stage and making faux-shocked faces at his own pronouncements about the marvelous, career-disrupting implications of large language models. He seemed overstimulated and came across as impatient with the pace of human life and thought as it has been until now. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>That speaker was not the most frenzied AI advocate I came across this spring. A recent issue of the <em>Chronicle of Higher Education</em> ran a fevered </span><a href="https://www.chronicle.com/article/are-you-ready-for-the-ai-university"><span>fantasy</span></a><span> by one Scott Latham, a professor of strategy at the Manning School of Business at the University of Massachusetts at Lowell. It is a vision featuring AI “agents” that will provide students a bespoke experience running from orientation through course instruction to job placement, all the while tracking their every frown and furrowed brow (because students will stare endlessly into cameras) and responding at each moment with the perfect remedy. “Human interaction is not as important to today’s students,” Latham claims, and so presumably the AI university will offer little. Compared to the current model of college, he promises, all of this will be better and cheaper—never mind growing evidence that LLMs are deteriorating and becoming </span><a href="https://futurism.com/the-byte/openai-o3-cost-per-query"><span>more expensive</span></a><span>.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>And it <em>will</em> happen. In fact, the word <em>will</em> appears more than 130 times in the 4,000-word article. <em>Could</em>, only twice. “Predicting AI’s disruption is the easy part,” Latham writes. “The tough part is making people realize the inevitable.” </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The claim of inevitability is crucial to technology hype cycles, from the railroad to television to AI. “A key strategy for a technology to gain market share and buy-in,” the scholars David Gray Widder and Mar Hicks </span><a href="https://ash.harvard.edu/resources/watching-the-generative-ai-hype-bubble-deflate/"><span>write</span></a><span>, “is to present it as an inevitable and necessary part of future infrastructure, encouraging the development of new, anticipatory infrastructures around it.” </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The infrastructure demanded by AI is neither neutral nor cheap. It has well-known environmental costs, given the vast amount of electricity and water its data centers demand. Nor is the infrastructure only physical. It has already been built in the minds of students, who are becoming informational lotus-eaters, addicted to immediate, effort-free homework answers and adequate-seeming essays on demand. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Tot up the overeager salesmen, the questionable prophecies, and the clearly exorbitant costs, and it becomes clear: AI is not revolutionary. It’s a gimmick. It entices us with the prospect of sparing us drudgery, but it ultimately disappoints. </span>AI apologists are like hucksters at a county fair, fast-talking about some newfangled marvel. Universities are their gape-mouthed marks,  <a href="https://www.insidehighered.com/news/tech-innovation/artificial-intelligence/2023/09/05/risks-and-rewards-higher-ed-should-know"><span>emptying their pockets</span></a><span> even while they are unsure about what they are buying or whether students will use it to learn or simply cheat with.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>We call something a gimmick, the literary scholar </span><a href="https://www.chronicle.com/article/the-professor-of-gimmicks"><span>Sianne Ngai</span></a><span> points out, when it seems to be simultaneously working too hard and not hard enough. It appears both to save labor and to inflate it, like a fanciful Rube Goldberg device that allows you to sharpen a pencil merely by raising the sash on a window, which only initiates a chain of causation involving strings, pulleys, weights, levers, fire, flora, and fauna, including an opossum. The apparatus of a large language model really is remarkable. It takes in billions of pages of writing and figures out the configuration of words that will delight me just enough to feed it another prompt. There’s nothing else like it.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>But look at what people actually use this wonder for: brain-dead books and videos, scam-filled ads, polished but boring homework essays. Another presenter at the workshop I attended said he used AI to help him decide what to give his kids for breakfast that morning. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Such disappointment inevitably accompanies the gimmick. “The gimmick lets us down,” Ngai writes, “only because it has also managed to pump us up.” <em>A universal culture-producing machine? Remarkable!</em> Then we see its results. Widder and Hicks note that the failed promise of AI “is not surprising, as generative AI does not so much represent the wave of the future as it does the ebb and flow of waves past.” MOOCs, NFTs, AR: We should be wise to the tricks by now. AI progress in cultural production already seems to have slowed because the models have run out of human-generated writing to “learn” from and increasingly feed on AI-produced content, gulping down a vile soup of their own ever-concentrating ordure.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The AI apologists must deny, or at least forestall, such disappointment. They insist that the time scales on AI’s technical progress are shortening—artificial general intelligence will be here in ten years; no, five; no, we’re just months away—even as they implore skeptics to give the technology a chance, because it’s still early days. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>But do the apologists even believe it themselves? Latham, the professor of strategy, gives away the game at the end of his reverie. “None of this can happen, though,” he writes, “if professors and administrators continue to have their heads in the sand.” So it’s not inevitable after all? Whoops.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The scholars hectoring their colleagues to adopt AI are not all so ham-fisted. A more subtle and psychologically interesting argument about AI in higher ed shows how fine the line is between the huckster and the mark. Writing recently in <em>The New Yorker</em>, Princeton history professor D. Graham Burnett takes up where Latham leaves off, drawing a distinction between himself and his denialist colleagues: “[E]everyone seems intent on pretending that the most significant revolution in the world of thought in the past century <em>isn’t happening</em>.” This pretense, he writes, “is, simply, madness. And it won’t hold for long.” </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett’s essay appears to issue from deep doubts about the value of humanities research. Given the capabilities of AI to sift through archives, detect patterns within the contents, and perhaps incrementally advance what has been said about them, the value of an academic monograph seems to fall to zero. “The making of books such as those on my shelves,” Burnett writes, “each the labor of years or decades, is quickly becoming a matter of well-designed prompts. The question is no longer whether we can write such books; they can be written endlessly, for us. The question is, do we want to read them?”</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The answer depends on the “we.” Does the median <em>New Yorker</em> subscriber want to read a monograph? No. Does a scholar fifty or five hundred years hence want to? Let’s put it out there and let them decide. That has been the value proposition of humanistic research for centuries. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett supposes there might still be merit in teaching, even if scholarship is dead. It is a maneuver many an Ivy League PhD has made who finds himself or herself in a job with a heavy teaching load. Indeed, the overwhelming majority of humanities PhDs are in this position. Most have teaching-intensive jobs as adjuncts or, if they are on the tenure track, they teach four or five or six courses per semester at community colleges and regional universities. They never publish a single monograph and read few if any after they finish graduate school. For them, academia already looks like the near-future Burnett envisions. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett decided to merge his teaching with his interest in AI. He assigned students in an undergraduate class—so far as I can tell, the </span><a href="https://registrar.princeton.edu/course-offerings?term=1254&amp;keywords=burnett"><span>only one</span></a><span> he taught this spring—to engage with a chatbot about human attention and turn the text into a short paper. He marvels at their output:</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<blockquote>
<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Reading the results, on my living-room couch, turned out to be the most profound experience of my teaching career. I’m not sure how to describe it. In a basic way, I felt I was watching a new kind of creature being born, and also watching a generation come face to face with that birth: an encounter with something part sibling, part rival, part careless child-god, part mechanomorphic shadow—an alien familiar.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>
</blockquote>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>In Burnett’s eyes, his students are not just creative in the ordinary sense of being able to turn nice phrases or make clever connections—already feats that lighten a grading load of dreary essays. No, Burnett’s students are conjurers, evokers, maybe minor deities, able to break the old material laws most of us labor under. I know this impulse. In moments of professional self-doubt, I have often tried to convince myself that my students were amazing, that their halting efforts were in fact brilliant, that my class, unique among the courses listed on their transcripts, unlocked something in them, that the students were teaching <em>me</em>, that, indeed, all I needed to do was get out of their way. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>It is a lie many teachers tell themselves. And why not? It is not as if someone can fact-check it. Your scholarship, or lack thereof, is public; your students’ work occurs behind the high fence of the Family Educational Rights and Privacy Act. You tell yourself a story at once self-abnegating and self-aggrandizing. You tell it to raise the students up; you’re there for <em>them</em>, after all. And that much is true; you are there for them. But you tell the story as well to put down your unfeeling, ossified colleagues, the ones who don’t get it. You tell it so you won’t feel as old as they are, because you are spiritually closer to the young. To Burnett, the AI dialogs offered something genuinely new under the sun. “Each sheaf of paper I picked up,” he writes, “was more astonishing than the last.” Sure.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>This is not to say I have never been astonished by my students’ writing. Burnett reports being moved to tears by one of his students’ interactions with a chatbot. I, too, have cried while reading student essays. I once had a student who started college only after he had retired from four decades working for the phone company. He was not a great writer, but he wrote from the heart. In a class on religious autobiography, he described going to the hospital to speak to the man, by then on his deathbed, who had murdered my student’s mother. The man asked my student to forgive him. My student did so. He forgave. The essay testified to a form of love few of us would be capable of. Sitting at my kitchen table, I read the essay a second time; I cried a second time. It was the rare piece of student writing that improved the world by its existence. It made mercy more widely known.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>In the end, Burnett is essentially in the same place as his ostrich-headed colleagues, though it is not clear he realizes it. “You can no longer <em>make</em> students do the reading or the writing,” he writes, because they can make a machine do it for them. “So what’s left? Only this: give them work they want to do. And help them want to do it.” Is this a slip-up? A signal that AI-savvy students don’t need teachers after all? If students want to do the work, then they don’t need help wanting it. No, the only task remains the paradoxical one identified as far back as in Plato’s <em>Meno</em>: Give students work they don’t know they need to do. And yes, help them want to do it. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I have found, to overcome students’ resistance to learning, you often have to trick them. There’s the old bait-and-switch of offering grades, then seeing a few students learn to love learning itself. Worksheets are tricks, as are small-group discussions and even a teacher’s charisma. I’m sure I have used baseball analogies in class, too. In the face of the difficulty of reforming students’ desires, you can trick yourself into believing you’re doing it, and sleep well at night. I don’t know anyone for whom it’s a straightforward task. It’s <em>the</em> challenge for any teacher, and AI offers a tempting illusion to students—and evidently to some teachers—that there could be a shortcut. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The week Burnett’s article appeared, I visited the classroom of Ted Hadzi-Antich at Austin Community College. His honors political philosophy students were discussing the final section of James Baldwin’s <em>The Fire Next Time</em>. They wanted to talk about death. They sat in chairs in a circle, no desks walling them off from each other. I had hoped I could sit unobtrusively in the corner and take notes on the scene. “Learning doesn’t happen in the corner!” a student scolded. OK, fine. I was present. So I was implicated in what was about to occur.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>For eighty minutes, the eleven students and Hadzi-Antich talked. I chimed in at the end. No students texted. None disappeared into a screen. Two cried. They held their highlighted copies of the reading in their laps, but they didn’t talk much about it. Even so, they connected Baldwin’s ideas to their varied life experiences, including loss, addiction, and bigotry. Two students disagreed with each other about the uses of the past. Later, they told me they each describe the other as their “antagonist.” They seemed to respect each other, having read each other’s writing and debated in class all semester. They know each other’s voice.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>This is what Burnett seems to think is new, or newly exciting, thanks to his engagement with AI—something teachers such as Hadzi-Antich have been orchestrating in their classes since well before the dawn of AI. And in fact, Hadzi-Antich promotes text- and discussion-based education at community colleges through </span><a href="https://www.tgqf.org/"><span>The Great Questions Foundation</span></a><span>, which he directs. You don’t need to experience the technological sublime in order to see the value of giving students a book and asking them to read and discuss it. You do need to accept something like Max Weber’s idea of the teacher’s vocation, that of helping others “<em>to reckon with the ultimate meaning of</em> [their] <em>own actions</em>.” This reckoning is not a task you can expedite with a machine. If you try to spare yourself the labor, you fail.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Appropriately enough, this essay took me forever to write. I had a great deal of anger, frustration and sadness to draw on. I had evidence and critique, but I could not find an argument. I kept working. I changed the whole focus after I attended the AI workshop at school. That opened me up. I worked for days in the caesura between reading my students’ drafts and reading their finished research essays, knowing for sure that some had done all the work themselves because I had seen them put up the scaffolding and then build the essay, brick by brick. I knew that others would likely do very little, but I would be unable to prove it. As I worked, new articles and outrages about AI in education appeared daily, making me feel as though I were falling behind. Believe me, I wanted a shortcut. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I had not read <em>Theory of the Gimmick</em> before starting this essay, having only heard about it from my wife, a professor of literature. To get up to speed, I read an earlier article that Ngai turned into a chapter. I then skimmed my wife’s well-marked-up copy of the book, noting her underlines, her starred passages, her “hm!” in the margins. I leaned on the index to find passages on belief. I left most of the book unread.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I did these things because I know I am mortal. I cannot afford to read a 400-page work of frankly oblique theoretical prose. That is to cast no shade on Ngai. Her book deserves a close reading. I have only so much time. Even if that sounds like the same excuse students give for why they take AI shortcuts, I don’t think we mean the same thing. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Once I had a full draft, I asked my wife to read and comment on it. At the time, she was working on a paper about Herman Melville’s <em>The Confidence Man</em> and Max Weber’s “Science as a Vocation.” The latter is a locus classicus for our dinner-table conversation. She needed only to make a brief mention of a key passage in Weber for me to know what move the essay was missing. Her concerns, her attention, and the contingencies of her life and thought are all over this essay. It would have been different if she had been reading, say, William or Henry James that week. To a large extent, the intellectual circuit running through her life and mine, across two decades of talking about literature and culture, love and death, constitutes our life together. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>After I got her feedback, I finally asked ChatGPT if generative AI could be considered a gimmick in Ngai’s sense. I did not read its answer carefully. Whenever I see the words cascade down my computer screen, I get a sinking feeling. <em>Do I really have to read this?</em> I know I am unlikely to find anything truly interesting or surprising, and the ease with which the words appear really does cheapen them. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>ChatGPT reported back that it could indeed be considered a gimmick. Then I asked the gimmick how educators should approach its implementation in universities and schools. “The worst thing educational institutions could do is embrace AI uncritically as an inevitable ‘efficiency upgrade,’” it wrote, “because that would mean compounding the very gimmickry Ngai diagnoses: mistaking ease for value, and output for understanding.” Take that, Scott Latham. This is, of course, just what I hoped the machine would say. I have argued enough with the model that it probably knows I want it to be self-critical. I suspect it admits such things only to me. I wish it would tell university presidents, information officers, professors, and certainly students that even ChatGPT thinks it is a gimmick. But it only ever tells them what they want to hear, not what I want. </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Ted Hadzi-Antich’s students read and discussed these words by James Baldwin: “Perhaps the whole root of our trouble, the human trouble, is that we will sacrifice all the beauty of our lives, will imprison ourselves in totems, taboos, crosses, blood sacrifices, steeples, races, mosques, armies, flags, nations” —I might add technologies and hype bubbles—“in order to deny the fact of death, which is the only fact we have.” </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Part of a teacher’s job—certainly in the humanities, but even in professional fields like business—is to help students break out of their prisons, at least for an hour, so they can see and enhance the beauty of their own minds. It is to help them learn, together, to defend how they want to live, precisely because they, too, unlike a machine, will one day die.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I will sacrifice some length of my days to add depth to another person’s experience of the rest of theirs. Many did this for me. The work is slow. Its results often go unseen for years. But it is no gimmick.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>




                </article>

                            </div></div>
  </body>
</html>
