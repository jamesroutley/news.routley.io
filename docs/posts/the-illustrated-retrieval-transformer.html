<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://jalammar.github.io/illustrated-retrieval-transformer/">Original</a>
    <h1>The Illustrated Retrieval Transformer</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p><span>Discussion: <a href="https://github.com/jalammar/jalammar.github.io/discussions/21">Discussion Thread</a> for comments, corrections, or any feedback. </span></p>



<p><strong>Summary</strong>: The latest batch of language models can be much smaller yet achieve GPT-3 like performance by being able to query a database or search the web for information. A key indication is that building larger and larger models is not the only way to improve performance.</p>

<hr/>

<p>The last few years saw the rise of Large Language Models (LLMs) – machine learning models that rapidly improve how machines process and generate language. Some of the highlights since 2017 include:</p>

<ul>
  <li>The original <a href="http://jalammar.github.io/illustrated-transformer/">Transformer</a> breaks previous performance records for machine translation.</li>
  <li><a href="http://jalammar.github.io/illustrated-bert/">BERT</a> popularizes the pre-training then finetuning process, as well as Transformer-based contextualized word embeddings. It then rapidly starts to power <a href="https://blog.google/products/search/search-language-understanding-bert/">Google Search</a> and <a href="https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/">Bing Search</a>.</li>
  <li><a href="http://jalammar.github.io/illustrated-gpt2/">GPT-2</a> demonstrates the machine’s ability to write as well as humans do.</li>
  <li>First <a href="https://arxiv.org/abs/1910.10683">T5</a>, then <a href="https://huggingface.co/bigscience/T0pp">T0</a> push the boundaries of transfer learning (training a model on one task, and then having it do well on other adjacent tasks) and posing a lot of different tasks as text-to-text tasks.</li>
  <li><a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/">GPT-3</a> showed that massive scaling of generative models can lead to shocking emergent applications (the industry continues to train larger models like <a href="https://deepmind.com/research/publications/2021/scaling-language-models-methods-analysis-insights-from-training-gopher">Gopher</a>, <a href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/">MT-NLG</a>…etc).</li>
</ul>

<p>For a while, it seemed like scaling larger and larger models is the main way to improve performance. Recent developments in the field, like DeepMind’s <a href="https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens">RETRO Transformer</a> and OpenAI’s <a href="https://openai.com/blog/improving-factual-accuracy/">WebGPT</a>, reverse this trend by showing that smaller generative language models can perform on par with massive models if we augment them with a way to search/query for information.</p>

<p>This article breaks down DeepMind’s RETRO (<strong>R</strong>etrieval-<strong>E</strong>nhanced <strong>TR</strong>ansf<strong>O</strong>rmer) and how it works. The model performs on par with GPT-3 despite being 4% its size (7.5 billion parameters vs. 185 billion for GPT-3 Da Vinci).</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/deepmind-retro-retrieval-transformer.png"/></p></div>

<p>RETRO was presented in the paper <a href="https://arxiv.org/abs/2112.04426">Improving Language Models by Retrieving from Trillions of Tokens</a>. It continues and builds on a wide variety of retrieval <a href="http://www.crm.umontreal.ca/2018/Langue18/pdf/Cheung.pdf">work</a> <a href="https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/">in</a> <a href="https://openreview.net/forum?id=HklBjCEKvH">the</a> <a href="https://arxiv.org/abs/2102.02557">research</a> <a href="https://openreview.net/forum?id=B184E5qee">community</a>. This article explains the model and not what is especially novel about it.</p>

<!--more-->

<h2 id="why-this-is-important-separating-language-information-from-world-knowledge-information">Why This is Important: Separating Language Information from World Knowledge Information</h2>

<p>Language modeling trains models to predict the next word–to fill-in-the-blank at the end of the sentence, essentially.</p>

<p>Filling the blank sometimes requires knowledge of factual information (e.g. names or dates). For example:</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/prompt-1.png"/></p></div>

<p>Other times, familiarity with the language is enough to guess what goes in the blank. For example:</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/prompt-2.png"/></p></div>

<p>This distinction is important because LLMs encoded everything they know in their model parameters. While this makes sense for language information, it is inefficient for factual and world-knowledge information.</p>

<p>By including a retrieval method in the language model, the model can be much smaller. A neural database aids it with retrieving factual information it needs during text generation.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/Large-GPT-vs-Retro-transformer-world-knowledge-information.png"/></p></div>

<p>Training becomes fast with small language models, as training data memorization is reduced. Anyone can deploy these models on smaller and more affordable GPUs and tweak them as per need.</p>

<p>Mechanically, RETRO is an encoder-decoder model just like the original transformer. However, it augments the input sequence with the help of a retrieval database. The model finds the most probable sequences in the database and adds them to the input. RETRO works its magic to generate the output prediction.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/dune-prompt-into-retro-transformer-4.png"/></p></div>

<p>Before we explore the model architecture, let’s dig deeper into the retrieval database.</p>

<h2 id="inspecting-retros-retrieval-database">Inspecting RETRO’s Retrieval Database</h2>

<p>The database is a key-value store.</p>

<p>The key is a standard BERT sentence embedding.</p>

<p>The value is text in two parts:</p>

<ol>
  <li>
    <p>Neighbor, which is used to compute the key</p>
  </li>
  <li>
    <p>Completion, the continuation of the text in the original document.</p>
  </li>
</ol>

<p>RETRO’s database contains 2 trillion multi-lingual tokens based on the <em>MassiveText</em> dataset. Both the neighbor and completion chunks are at most 64 tokens long.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/database-key-value-examples.png"/></p></div>

<p>RETRO breaks the input prompt into multiple chunks. For simplicity, we’ll focus on how one chunk is augmented with retrieved text. The model, however, does this process for each chunk (except the first) in the input prompt.</p>

<h2 id="the-database-lookup">The Database Lookup</h2>

<p>Before hitting RETRO, the input prompt goes into BERT. The output contextualized vectors are then averaged to construct a sentence embedding vector. That vector is then used to query the database.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/bert-sentence-embedding.png"/></p></div>

<p>That sentence embedding is then used in an approximate nearest neighbor search (<a href="https://github.com/google-research/google-research/tree/master/scann">https://github.com/google-research/google-research/tree/master/scann</a>).</p>

<p>The two nearest neighbors are retrieved, and their text becomes a part of the input into RETRO.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/neighbor-retrieval-from-retro-neural-database-with-bert-embeddings.png"/></p></div>

<p>This is now the input to RETRO. The input prompt and its two nearest neighbors from the database (and their continuations).</p>

<p>From here, the Transformer and RETRO Blocks incorporate the information into their processing.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/input-prompt-and-retrieved-text-retro-transformer.png"/></p></div>

<h2 id="retro-architecture-at-a-high-level">RETRO Architecture at a High Level</h2>

<p>RETRO’s architecture is an encoder stack and a decoder stack.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/Retro-transformer-encoder-decoder-stacks-2.png"/></p></div>

<p>The encoder is made up of standard Transformer encoder blocks (self-attention + FFNN). To my best understanding, Retro uses an encoder made up of two Transformer Encoder Blocks.</p>

<p>The decoder stack interleaves two kinds of decoder blocks:</p>

<ul>
  <li>Standard transformer decoder block (ATTN + FFNN)</li>
  <li>RETRO decoder block (ATTN + Chunked cross attention (CCA) + FFNN)</li>
</ul>

<div>
  <p><img src="http://jalammar.github.io/images/retro/retro-transformer-blocks-4.png"/></p></div>

<p>Let’s start by looking at the encoder stack, which processes the retrieved neighbors, resulting in KEYS and VALUES matrices that will later be used for attention (see <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> for a refresher).</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/retro-encoder-block-keys-values-2.png"/></p></div>

<p>Decoder blocks process the input text just like a GPT would. It applies self-attention on the prompt token (causally, so only attending to previous tokens), then passes through a FFNN layer.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/retro-transformer-decoders-2.png"/></p></div>

<p>It’s only when a RETRO decoder is reached do we start to incorporate the retrieved information. Every third block starting from 9 is a RETRO block (that allows its input to attend to the neighbors). So layers 9, 12, 15…32 are RETRO blocks.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/retro-decoder-attention-2.png"/></p></div>

<p>So effectively, this is the step where the retrieved information can glance at the dates it needs to complete the prompt.</p>

<div>
  <p><img src="http://jalammar.github.io/images/retro/retro-decoder-chunked-cross-attention.png"/></p></div>

<h2 id="previous-work">Previous Work</h2>

<p>Aiding language models with retrieval techniques has been an active area of research. Some of the previous work in the space includes:</p>

<ul>
  <li><a href="https://openreview.net/forum?id=B184E5qee">Improving Neural Language Models with a Continuous Cache</a></li>
  <li><a href="https://openreview.net/forum?id=HklBjCEKvH">Generalization through Memorization: Nearest Neighbor Language Models</a></li>
  <li>Read the <a href="https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/">Retrieval Augmented Generation</a> blog from Meta AI and go through Jackie Chi Kit Cheung’s lecture on <a href="http://www.crm.umontreal.ca/2018/Langue18/pdf/Cheung.pdf">Leveraging External Knowledge in Natural Language Understanding Systems</a></li>
  <li>SPALM: <a href="https://arxiv.org/abs/2102.02557">Adaptive Semiparametric Language Models</a></li>
  <li>DPR: <a href="https://aclanthology.org/2020.emnlp-main.550/">Dense Passage Retrieval for Open-Domain Question Answering</a></li>
  <li><a href="https://arxiv.org/abs/2002.08909">REALM: Retrieval-Augmented Language Model Pre-Training</a></li>
  <li>FiD: <a href="https://aclanthology.org/2021.eacl-main.74/">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a></li>
  <li>EMDR: <a href="https://arxiv.org/abs/2106.05346">End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering</a></li>
  <li>BlenderBot 2.0: <a href="https://arxiv.org/abs/2107.07566">Internet-Augmented Dialogue Generation</a></li>
</ul>

<p>Please post in <a href="https://github.com/jalammar/jalammar.github.io/discussions/21">this thread</a> or reach out to me on <a href="https://twitter.com/JayAlammar">Twitter</a> for any corrections or feedback.</p>

  </div></div>
  </body>
</html>
