<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/TrevorS/voxtral-mini-realtime-rs">Original</a>
    <h1>Rust implementation of Mistral&#39;s Voxtral Mini 4B Realtime runs in your browser</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://huggingface.co/TrevorJS/voxtral-mini-realtime-gguf" rel="nofollow"><img src="https://camo.githubusercontent.com/cfd5aad45986b4e3bfee0bc62c43362730814bb78217911b0dce1acfcf6f9c35/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d4d6f64656c5f6f6e5f48756767696e67466163652d79656c6c6f77" alt="HuggingFace" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-Model_on_HuggingFace-yellow"/></a>
<a href="https://huggingface.co/spaces/TrevorJS/voxtral-mini-realtime" rel="nofollow"><img src="https://camo.githubusercontent.com/4e4d92ddbe9e446e395e32125f4f1bbbacad5126c7c08b6e7f110e1f3a0e7b32/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462539342538412d4c6976655f44656d6f2d626c7565" alt="Live Demo" data-canonical-src="https://img.shields.io/badge/%F0%9F%94%8A-Live_Demo-blue"/></a></p>
<p dir="auto">Streaming speech recognition running natively and in the browser. A pure Rust implementation of <a href="https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602" rel="nofollow">Mistral&#39;s Voxtral Mini 4B Realtime</a> model using the <a href="https://burn.dev" rel="nofollow">Burn</a> ML framework.</p>
<p dir="auto">The Q4 GGUF quantized path (2.5 GB) runs entirely client-side in a browser tab via WASM + WebGPU. <a href="https://huggingface.co/spaces/TrevorJS/voxtral-mini-realtime" rel="nofollow">Try it live.</a></p>


<div dir="auto" data-snippet-clipboard-copy-content="# Download model weights (~9 GB)
uv run --with huggingface_hub \
  hf download mistralai/Voxtral-Mini-4B-Realtime-2602 --local-dir models/voxtral

# Transcribe an audio file (f32 SafeTensors path)
cargo run --release --features &#34;wgpu,cli,hub&#34; --bin voxtral-transcribe -- \
  --audio audio.wav --model models/voxtral

# Or use the Q4 quantized path (~2.5 GB)
cargo run --release --features &#34;wgpu,cli,hub&#34; --bin voxtral-transcribe -- \
  --audio audio.wav --gguf models/voxtral-q4.gguf --tokenizer models/voxtral/tekken.json"><pre><span><span>#</span> Download model weights (~9 GB)</span>
uv run --with huggingface_hub \
  hf download mistralai/Voxtral-Mini-4B-Realtime-2602 --local-dir models/voxtral

<span><span>#</span> Transcribe an audio file (f32 SafeTensors path)</span>
cargo run --release --features <span><span>&#34;</span>wgpu,cli,hub<span>&#34;</span></span> --bin voxtral-transcribe -- \
  --audio audio.wav --model models/voxtral

<span><span>#</span> Or use the Q4 quantized path (~2.5 GB)</span>
cargo run --release --features <span><span>&#34;</span>wgpu,cli,hub<span>&#34;</span></span> --bin voxtral-transcribe -- \
  --audio audio.wav --gguf models/voxtral-q4.gguf --tokenizer models/voxtral/tekken.json</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Build WASM package
wasm-pack build --target web --no-default-features --features wasm

# Generate self-signed cert (WebGPU requires secure context)
openssl req -x509 -newkey ec -pkeyopt ec_paramgen_curve:prime256v1 \
  -keyout /tmp/voxtral-key.pem -out /tmp/voxtral-cert.pem \
  -days 7 -nodes -subj &#34;/CN=localhost&#34;

# Start dev server
bun serve.mjs"><pre><span><span>#</span> Build WASM package</span>
wasm-pack build --target web --no-default-features --features wasm

<span><span>#</span> Generate self-signed cert (WebGPU requires secure context)</span>
openssl req -x509 -newkey ec -pkeyopt ec_paramgen_curve:prime256v1 \
  -keyout /tmp/voxtral-key.pem -out /tmp/voxtral-cert.pem \
  -days 7 -nodes -subj <span><span>&#34;</span>/CN=localhost<span>&#34;</span></span>

<span><span>#</span> Start dev server</span>
bun serve.mjs</pre></div>
<p dir="auto">Open <code>https://localhost:8443</code>, accept the certificate, and click <strong>Load from Server</strong> to download the model shards. Record from your microphone or upload a WAV file to transcribe.</p>
<p dir="auto"><a href="https://huggingface.co/spaces/TrevorJS/voxtral-mini-realtime" rel="nofollow">Hosted demo on HuggingFace Spaces</a> if you want to skip local setup.</p>

<div data-snippet-clipboard-copy-content="Audio (16kHz mono)
  -&gt; Mel spectrogram [B, 128, T]
    -&gt; Causal encoder (32 layers, 1280 dim, sliding window 750)
      -&gt; Conv 4x downsample -&gt; Reshape [B, T/16, 5120]
        -&gt; Adapter [B, T/16, 3072]
          -&gt; Autoregressive decoder (26 layers, 3072 dim, GQA 32Q/8KV)
            -&gt; Token IDs -&gt; Text"><pre><code>Audio (16kHz mono)
  -&gt; Mel spectrogram [B, 128, T]
    -&gt; Causal encoder (32 layers, 1280 dim, sliding window 750)
      -&gt; Conv 4x downsample -&gt; Reshape [B, T/16, 5120]
        -&gt; Adapter [B, T/16, 3072]
          -&gt; Autoregressive decoder (26 layers, 3072 dim, GQA 32Q/8KV)
            -&gt; Token IDs -&gt; Text
</code></pre></div>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th>F32 (native)</th>
<th>Q4 GGUF (native + browser)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Weights</td>
<td>SafeTensors (~9 GB)</td>
<td>GGUF Q4_0 (~2.5 GB)</td>
</tr>
<tr>
<td>Linear ops</td>
<td>Burn tensor matmul</td>
<td>Custom WGSL shader (fused dequant + matmul)</td>
</tr>
<tr>
<td>Embeddings</td>
<td>f32 tensor (1.5 GiB)</td>
<td>Q4 on GPU (216 MB) + CPU bytes for lookups</td>
</tr>
<tr>
<td>Browser</td>
<td>No</td>
<td>Yes (WASM + WebGPU)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">The upstream mistral-common library left-pads audio with 32 silence tokens (at 12.5 Hz). After the mel/conv/reshape pipeline, this covers only 16 of the 38 decoder prefix positions with silence — the remaining 22 contain actual audio. The f32 model handles this fine, but Q4_0 quantization makes the decoder sensitive to speech content in the prefix: audio that starts immediately with speech (mic recordings, clips with no leading silence) produces all-pad tokens instead of text.</p>
<p dir="auto">The left padding is increased to 76 tokens, which maps to exactly 38 decoder tokens of silence and covers the full streaming prefix. See <a href="https://github.com/TrevorS/voxtral-mini-realtime-rs/blob/main/src/audio/pad.rs"><code>src/audio/pad.rs</code></a> for details.</p>

<p dir="auto">Running a 4B model in a browser tab required solving five hard constraints:</p>
<ol dir="auto">
<li><strong>2 GB allocation limit</strong> — <code>ShardedCursor</code> reads across multiple <code>Vec&lt;u8&gt;</code> buffers</li>
<li><strong>4 GB address space</strong> — Two-phase loading: parse weights, drop reader, then finalize</li>
<li><strong>1.5 GiB embedding table</strong> — Q4 embeddings on GPU + CPU-side row lookups</li>
<li><strong>No sync GPU readback</strong> — All tensor reads use <code>into_data_async().await</code></li>
<li><strong>256 workgroup invocation limit</strong> — Patched cubecl-wgpu to cap reduce kernel workgroups</li>
</ol>

<div dir="auto" data-snippet-clipboard-copy-content="# Native (default features: wgpu + native-tokenizer)
cargo build --release

# With all features
cargo build --release --features &#34;wgpu,cli,hub&#34;

# WASM
wasm-pack build --target web --no-default-features --features wasm"><pre><span><span>#</span> Native (default features: wgpu + native-tokenizer)</span>
cargo build --release

<span><span>#</span> With all features</span>
cargo build --release --features <span><span>&#34;</span>wgpu,cli,hub<span>&#34;</span></span>

<span><span>#</span> WASM</span>
wasm-pack build --target web --no-default-features --features wasm</pre></div>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>wgpu</code> (default)</td>
<td>GPU backend via Burn/CubeCL (WebGPU, Vulkan, Metal)</td>
</tr>
<tr>
<td><code>native-tokenizer</code> (default)</td>
<td>Tekken tokenizer (C deps, not WASM-compatible)</td>
</tr>
<tr>
<td><code>wasm</code></td>
<td>Browser support: wasm-bindgen, WebGPU device init, JS bindings</td>
</tr>
<tr>
<td><code>cli</code></td>
<td>CLI binary with clap + indicatif</td>
</tr>
<tr>
<td><code>hub</code></td>
<td>HuggingFace Hub model downloads</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<div dir="auto" data-snippet-clipboard-copy-content="# Unit + integration tests (requires GPU for full suite)
cargo test --features &#34;wgpu,cli,hub&#34;

# Lint
cargo clippy --features &#34;wgpu,cli,hub&#34; -- -D warnings
cargo clippy --no-default-features --features wasm --target wasm32-unknown-unknown -- -D warnings

# E2E browser test (requires Playwright + model shards)
bunx playwright test tests/e2e_browser.spec.ts"><pre><span><span>#</span> Unit + integration tests (requires GPU for full suite)</span>
cargo <span>test</span> --features <span><span>&#34;</span>wgpu,cli,hub<span>&#34;</span></span>

<span><span>#</span> Lint</span>
cargo clippy --features <span><span>&#34;</span>wgpu,cli,hub<span>&#34;</span></span> -- -D warnings
cargo clippy --no-default-features --features wasm --target wasm32-unknown-unknown -- -D warnings

<span><span>#</span> E2E browser test (requires Playwright + model shards)</span>
bunx playwright <span>test</span> tests/e2e_browser.spec.ts</pre></div>
<p dir="auto">GPU-dependent tests (model layer shapes, Q4 matmul, WGSL shader correctness) are skipped in CI since GitHub Actions runners lack a GPU adapter. These tests run locally on any machine with Vulkan, Metal, or WebGPU support.</p>

<div dir="auto"><h3 tabindex="-1" dir="auto">Q4 GGUF Sharding (for browser)</h3><a id="user-content-q4-gguf-sharding-for-browser" aria-label="Permalink: Q4 GGUF Sharding (for browser)" href="#q4-gguf-sharding-for-browser"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The GGUF file must be split into shards of 512 MB or less to stay under the browser&#39;s <code>ArrayBuffer</code> limit:</p>
<div dir="auto" data-snippet-clipboard-copy-content="split -b 512m models/voxtral-q4.gguf models/voxtral-q4-shards/shard-"><pre>split -b 512m models/voxtral-q4.gguf models/voxtral-q4-shards/shard-</pre></div>
<p dir="auto">The dev server and E2E test discover shards automatically from <code>models/voxtral-q4-shards/</code>.</p>

<p dir="auto">Coming soon: accuracy (WER) and inference speed benchmarks across native and browser targets.</p>

<div data-snippet-clipboard-copy-content="src/
  audio/          # Mel spectrogram, chunking, resampling, padding
  models/         # F32 model: encoder, decoder, adapter, attention, RoPE, KV cache
  gguf/           # Q4 GGUF: reader, loader, model, tensor, WGSL shader, tests
  web/            # WASM bindings: VoxtralQ4, initWgpuDevice, async decode loop
  tokenizer/      # Tekken tokenizer wrapper (native only)
  bin/transcribe  # CLI binary

web/              # Browser demo: index.html, worker.js, voxtral-client.js
tests/            # Integration tests + Playwright E2E spec
scripts/          # Dev scripts: reference implementations, weight inspection, E2E helpers
patches/          # cubecl-wgpu workgroup size fix for WebGPU"><pre><code>src/
  audio/          # Mel spectrogram, chunking, resampling, padding
  models/         # F32 model: encoder, decoder, adapter, attention, RoPE, KV cache
  gguf/           # Q4 GGUF: reader, loader, model, tensor, WGSL shader, tests
  web/            # WASM bindings: VoxtralQ4, initWgpuDevice, async decode loop
  tokenizer/      # Tekken tokenizer wrapper (native only)
  bin/transcribe  # CLI binary

web/              # Browser demo: index.html, worker.js, voxtral-client.js
tests/            # Integration tests + Playwright E2E spec
scripts/          # Dev scripts: reference implementations, weight inspection, E2E helpers
patches/          # cubecl-wgpu workgroup size fix for WebGPU
</code></pre></div>

<p dir="auto">Apache-2.0</p>
</article></div></div>
  </body>
</html>
