<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/brianmg/voynich-nlp-analysis">Original</a>
    <h1>Show HN: I modeled the Voynich Manuscript with SBERT to test for structure</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto">This started as a personal challenge to figure out what modern NLP could tell us about the Voynich Manuscript — without falling into translation speculation or pattern hallucination. I&#39;m not a linguist or cryptographer. I just wanted to see if something as strange as Voynichese would hold up under real language modeling: clustering, POS inference, Markov transitions, and section-specific patterns.</p>
<p dir="auto">Spoiler: it kinda did.</p>
<p dir="auto">This repo walks through everything — from suffix stripping to SBERT embeddings to building a lexicon hypothesis. No magic, no GPT guessing. Just a skeptical test of whether the manuscript has <em>structure that behaves like language</em>, even if we don’t know what it’s saying.</p>
<hr/>

<p dir="auto">The Voynich Manuscript remains undeciphered, with no agreed linguistic or cryptographic solution. Traditional analyses often fall into two camps: <em>statistical entropy checks</em> or <em>wild guesswork</em>. This project offers a middle path — using computational linguistics to assess whether the manuscript encodes real, structured language-like behavior.</p>
<hr/>

<div data-snippet-clipboard-copy-content="/data/
  AB.docx                         # Full transliteration with folio/line tags
  voynichese/                     # Root word .txt files
  stripped_cluster_lookup.json    # Cluster ID per stripped root
  unique_stripped_words.json      # All stripped root forms
  voynich_line_clusters.csv       # Cluster sequences per line

/scripts/
  cluster_roots.py                # SBERT clustering + suffix stripping
  map_lines_to_clusters.py        # Maps manuscript lines to cluster IDs
  pos_model.py                    # Infers grammatical roles from cluster behavior
  transition_matrix.py            # Builds and visualizes cluster transitions
  lexicon_builder.py              # Creates a candidate lexicon by section and role
  cluster_language_similarity.py  # (Optional) Compares clusters to real-world languages

/results/
  Figure_1.png                    # SBERT clusters (PCA reduced)
  transition_matrix_heatmap.png  # Markov transition matrix
  cluster_role_summary.csv
  cluster_transition_matrix.csv
  lexicon_candidates.csv"><pre><code>/data/
  AB.docx                         # Full transliteration with folio/line tags
  voynichese/                     # Root word .txt files
  stripped_cluster_lookup.json    # Cluster ID per stripped root
  unique_stripped_words.json      # All stripped root forms
  voynich_line_clusters.csv       # Cluster sequences per line

/scripts/
  cluster_roots.py                # SBERT clustering + suffix stripping
  map_lines_to_clusters.py        # Maps manuscript lines to cluster IDs
  pos_model.py                    # Infers grammatical roles from cluster behavior
  transition_matrix.py            # Builds and visualizes cluster transitions
  lexicon_builder.py              # Creates a candidate lexicon by section and role
  cluster_language_similarity.py  # (Optional) Compares clusters to real-world languages

/results/
  Figure_1.png                    # SBERT clusters (PCA reduced)
  transition_matrix_heatmap.png  # Markov transition matrix
  cluster_role_summary.csv
  cluster_transition_matrix.csv
  lexicon_candidates.csv
</code></pre></div>
<hr/>

<ul dir="auto">
<li>Clustering of stripped root words using multilingual SBERT</li>
<li>Identification of function-word-like vs. content-word-like clusters</li>
<li>Markov-style transition modeling of cluster sequences</li>
<li>Folio-based syntactic structure mapping (Botanical, Biological, etc.)</li>
<li>Generation of a data-driven lexicon hypothesis table</li>
</ul>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">🔧 Preprocessing Choices</h2><a id="user-content--preprocessing-choices" aria-label="Permalink: 🔧 Preprocessing Choices" href="#-preprocessing-choices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">One of the most important assumptions I made was about how to handle the Voynich words before clustering. Specifically: I stripped a set of recurring suffix-like endings from each word — things like aiin, dy, chy, and similar variants. The goal was to isolate what looked like root forms that repeated with variation, under the assumption that these suffixes might be:</p>
<ul dir="auto">
<li>Phonetic padding</li>
<li>Grammatical particles</li>
<li>Chant-like or mnemonic repetition</li>
<li>Or… just noise</li>
</ul>
<p dir="auto">This definitely improved the clustering behavior — similar stems grouped more tightly, and the transition matrix showed cleaner structural patterns. But it&#39;s also a strong preprocessing decision that may have:</p>
<ul dir="auto">
<li>Removed actual morphological information</li>
<li>Disguised meaningful inflectional variants</li>
<li>Introduced a bias toward function over content</li>
</ul>
<p dir="auto">So it’s not neutral — it helped, but it also shaped the results.
If someone wants to fork this repo and re-run the pipeline without suffix stripping — or treat suffixes as their own token class — I’d be genuinely interested in the comparison.</p>
<hr/>

<ul dir="auto">
<li><strong>Cluster 8</strong> exhibits high frequency, low diversity, and frequent line-starts — likely a <em>function word group</em></li>
<li><strong>Cluster 3</strong> has high diversity and flexible positioning — likely a <em>root content class</em></li>
<li><strong>Transition matrix</strong> shows strong internal structure, far from random</li>
<li>Cluster usage and POS patterns differ by <em>manuscript section</em> (e.g., Biological vs Botanical)</li>
</ul>
<hr/>

<p dir="auto">The manuscript encodes a structured constructed or mnemonic language using syllabic padding and positional repetition. It exhibits syntax, function/content separation, and section-aware linguistic shifts — even in the absence of direct translation.</p>
<hr/>

<div data-snippet-clipboard-copy-content="# 1. Install dependencies
pip install -r requirements.txt

# 2. Run each stage of the pipeline
python scripts/cluster_roots.py
python scripts/map_lines_to_clusters.py
python scripts/pos_model.py
python scripts/transition_matrix.py
python scripts/lexicon_builder.py"><pre><code># 1. Install dependencies
pip install -r requirements.txt

# 2. Run each stage of the pipeline
python scripts/cluster_roots.py
python scripts/map_lines_to_clusters.py
python scripts/pos_model.py
python scripts/transition_matrix.py
python scripts/lexicon_builder.py
</code></pre></div>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">📊 Example Visualizations</h2><a id="user-content--example-visualizations" aria-label="Permalink: 📊 Example Visualizations" href="#-example-visualizations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">📌 Figure 1: SBERT cluster embeddings (PCA-reduced)</h4><a id="user-content--figure-1-sbert-cluster-embeddings-pca-reduced" aria-label="Permalink: 📌 Figure 1: SBERT cluster embeddings (PCA-reduced)" href="#-figure-1-sbert-cluster-embeddings-pca-reduced"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/brianmg/voynich-nlp-analysis/blob/main/results/Figure_1.png"><img src="https://github.com/brianmg/voynich-nlp-analysis/raw/main/results/Figure_1.png" alt="Cluster visualization"/></a></p>
<div dir="auto"><h4 tabindex="-1" dir="auto">📌 Figure 2: Transition Matrix Heatmap</h4><a id="user-content--figure-2-transition-matrix-heatmap" aria-label="Permalink: 📌 Figure 2: Transition Matrix Heatmap" href="#-figure-2-transition-matrix-heatmap"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/brianmg/voynich-nlp-analysis/blob/main/results/transition_matrix_heatmap.png"><img src="https://github.com/brianmg/voynich-nlp-analysis/raw/main/results/transition_matrix_heatmap.png" alt="Transition matrix heatmap"/></a></p>
<hr/>

<ul dir="auto">
<li>Cluster-to-word mappings are indirect — frequency estimates may overlap</li>
<li>Suffix stripping is heuristic and may remove meaningful endings</li>
<li>No semantic translation attempted — only structural modeling</li>
</ul>
<hr/>

<p dir="auto">This project was built as a way to learn — about AI, NLP, and how far structured analysis can get you without assuming what you&#39;re looking at. I’m not here to crack the Voynich. But I do believe that modeling its structure with modern tools is a better path than either wishful translation or academic dismissal.</p>
<p dir="auto">So if you&#39;re here for a Rosetta Stone, you&#39;re out of luck.</p>
<p dir="auto">If you&#39;re here to model a language that may not want to be modeled — welcome.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributions Welcome</h2><a id="user-content--contributions-welcome" aria-label="Permalink: 🤝 Contributions Welcome" href="#-contributions-welcome"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This project is open to extensions, critiques, and collaboration — especially from linguists, cryptographers, conlang enthusiasts, and computational language researchers.</p>
</article></div></div>
  </body>
</html>
