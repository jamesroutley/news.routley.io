<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/brianmg/voynich-nlp-analysis">Original</a>
    <h1>Show HN: I modeled the Voynich Manuscript with SBERT to test for structure</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto">This started as a personal challenge to figure out what modern NLP could tell us about the Voynich Manuscript ‚Äî without falling into translation speculation or pattern hallucination. I&#39;m not a linguist or cryptographer. I just wanted to see if something as strange as Voynichese would hold up under real language modeling: clustering, POS inference, Markov transitions, and section-specific patterns.</p>
<p dir="auto">Spoiler: it kinda did.</p>
<p dir="auto">This repo walks through everything ‚Äî from suffix stripping to SBERT embeddings to building a lexicon hypothesis. No magic, no GPT guessing. Just a skeptical test of whether the manuscript has <em>structure that behaves like language</em>, even if we don‚Äôt know what it‚Äôs saying.</p>
<hr/>

<p dir="auto">The Voynich Manuscript remains undeciphered, with no agreed linguistic or cryptographic solution. Traditional analyses often fall into two camps: <em>statistical entropy checks</em> or <em>wild guesswork</em>. This project offers a middle path ‚Äî using computational linguistics to assess whether the manuscript encodes real, structured language-like behavior.</p>
<hr/>

<div data-snippet-clipboard-copy-content="/data/
  AB.docx                         # Full transliteration with folio/line tags
  voynichese/                     # Root word .txt files
  stripped_cluster_lookup.json    # Cluster ID per stripped root
  unique_stripped_words.json      # All stripped root forms
  voynich_line_clusters.csv       # Cluster sequences per line

/scripts/
  cluster_roots.py                # SBERT clustering + suffix stripping
  map_lines_to_clusters.py        # Maps manuscript lines to cluster IDs
  pos_model.py                    # Infers grammatical roles from cluster behavior
  transition_matrix.py            # Builds and visualizes cluster transitions
  lexicon_builder.py              # Creates a candidate lexicon by section and role
  cluster_language_similarity.py  # (Optional) Compares clusters to real-world languages

/results/
  Figure_1.png                    # SBERT clusters (PCA reduced)
  transition_matrix_heatmap.png  # Markov transition matrix
  cluster_role_summary.csv
  cluster_transition_matrix.csv
  lexicon_candidates.csv"><pre><code>/data/
  AB.docx                         # Full transliteration with folio/line tags
  voynichese/                     # Root word .txt files
  stripped_cluster_lookup.json    # Cluster ID per stripped root
  unique_stripped_words.json      # All stripped root forms
  voynich_line_clusters.csv       # Cluster sequences per line

/scripts/
  cluster_roots.py                # SBERT clustering + suffix stripping
  map_lines_to_clusters.py        # Maps manuscript lines to cluster IDs
  pos_model.py                    # Infers grammatical roles from cluster behavior
  transition_matrix.py            # Builds and visualizes cluster transitions
  lexicon_builder.py              # Creates a candidate lexicon by section and role
  cluster_language_similarity.py  # (Optional) Compares clusters to real-world languages

/results/
  Figure_1.png                    # SBERT clusters (PCA reduced)
  transition_matrix_heatmap.png  # Markov transition matrix
  cluster_role_summary.csv
  cluster_transition_matrix.csv
  lexicon_candidates.csv
</code></pre></div>
<hr/>

<ul dir="auto">
<li>Clustering of stripped root words using multilingual SBERT</li>
<li>Identification of function-word-like vs. content-word-like clusters</li>
<li>Markov-style transition modeling of cluster sequences</li>
<li>Folio-based syntactic structure mapping (Botanical, Biological, etc.)</li>
<li>Generation of a data-driven lexicon hypothesis table</li>
</ul>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">üîß Preprocessing Choices</h2><a id="user-content--preprocessing-choices" aria-label="Permalink: üîß Preprocessing Choices" href="#-preprocessing-choices"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">One of the most important assumptions I made was about how to handle the Voynich words before clustering. Specifically: I stripped a set of recurring suffix-like endings from each word ‚Äî things like aiin, dy, chy, and similar variants. The goal was to isolate what looked like root forms that repeated with variation, under the assumption that these suffixes might be:</p>
<ul dir="auto">
<li>Phonetic padding</li>
<li>Grammatical particles</li>
<li>Chant-like or mnemonic repetition</li>
<li>Or‚Ä¶ just noise</li>
</ul>
<p dir="auto">This definitely improved the clustering behavior ‚Äî similar stems grouped more tightly, and the transition matrix showed cleaner structural patterns. But it&#39;s also a strong preprocessing decision that may have:</p>
<ul dir="auto">
<li>Removed actual morphological information</li>
<li>Disguised meaningful inflectional variants</li>
<li>Introduced a bias toward function over content</li>
</ul>
<p dir="auto">So it‚Äôs not neutral ‚Äî it helped, but it also shaped the results.
If someone wants to fork this repo and re-run the pipeline without suffix stripping ‚Äî or treat suffixes as their own token class ‚Äî I‚Äôd be genuinely interested in the comparison.</p>
<hr/>

<ul dir="auto">
<li><strong>Cluster 8</strong> exhibits high frequency, low diversity, and frequent line-starts ‚Äî likely a <em>function word group</em></li>
<li><strong>Cluster 3</strong> has high diversity and flexible positioning ‚Äî likely a <em>root content class</em></li>
<li><strong>Transition matrix</strong> shows strong internal structure, far from random</li>
<li>Cluster usage and POS patterns differ by <em>manuscript section</em> (e.g., Biological vs Botanical)</li>
</ul>
<hr/>

<p dir="auto">The manuscript encodes a structured constructed or mnemonic language using syllabic padding and positional repetition. It exhibits syntax, function/content separation, and section-aware linguistic shifts ‚Äî even in the absence of direct translation.</p>
<hr/>

<div data-snippet-clipboard-copy-content="# 1. Install dependencies
pip install -r requirements.txt

# 2. Run each stage of the pipeline
python scripts/cluster_roots.py
python scripts/map_lines_to_clusters.py
python scripts/pos_model.py
python scripts/transition_matrix.py
python scripts/lexicon_builder.py"><pre><code># 1. Install dependencies
pip install -r requirements.txt

# 2. Run each stage of the pipeline
python scripts/cluster_roots.py
python scripts/map_lines_to_clusters.py
python scripts/pos_model.py
python scripts/transition_matrix.py
python scripts/lexicon_builder.py
</code></pre></div>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">üìä Example Visualizations</h2><a id="user-content--example-visualizations" aria-label="Permalink: üìä Example Visualizations" href="#-example-visualizations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h4 tabindex="-1" dir="auto">üìå Figure 1: SBERT cluster embeddings (PCA-reduced)</h4><a id="user-content--figure-1-sbert-cluster-embeddings-pca-reduced" aria-label="Permalink: üìå Figure 1: SBERT cluster embeddings (PCA-reduced)" href="#-figure-1-sbert-cluster-embeddings-pca-reduced"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/brianmg/voynich-nlp-analysis/blob/main/results/Figure_1.png"><img src="https://github.com/brianmg/voynich-nlp-analysis/raw/main/results/Figure_1.png" alt="Cluster visualization"/></a></p>
<div dir="auto"><h4 tabindex="-1" dir="auto">üìå Figure 2: Transition Matrix Heatmap</h4><a id="user-content--figure-2-transition-matrix-heatmap" aria-label="Permalink: üìå Figure 2: Transition Matrix Heatmap" href="#-figure-2-transition-matrix-heatmap"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/brianmg/voynich-nlp-analysis/blob/main/results/transition_matrix_heatmap.png"><img src="https://github.com/brianmg/voynich-nlp-analysis/raw/main/results/transition_matrix_heatmap.png" alt="Transition matrix heatmap"/></a></p>
<hr/>

<ul dir="auto">
<li>Cluster-to-word mappings are indirect ‚Äî frequency estimates may overlap</li>
<li>Suffix stripping is heuristic and may remove meaningful endings</li>
<li>No semantic translation attempted ‚Äî only structural modeling</li>
</ul>
<hr/>

<p dir="auto">This project was built as a way to learn ‚Äî about AI, NLP, and how far structured analysis can get you without assuming what you&#39;re looking at. I‚Äôm not here to crack the Voynich. But I do believe that modeling its structure with modern tools is a better path than either wishful translation or academic dismissal.</p>
<p dir="auto">So if you&#39;re here for a Rosetta Stone, you&#39;re out of luck.</p>
<p dir="auto">If you&#39;re here to model a language that may not want to be modeled ‚Äî welcome.</p>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">ü§ù Contributions Welcome</h2><a id="user-content--contributions-welcome" aria-label="Permalink: ü§ù Contributions Welcome" href="#-contributions-welcome"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This project is open to extensions, critiques, and collaboration ‚Äî especially from linguists, cryptographers, conlang enthusiasts, and computational language researchers.</p>
</article></div></div>
  </body>
</html>
