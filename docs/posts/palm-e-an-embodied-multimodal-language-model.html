<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://palm-e.github.io/">Original</a>
    <h1>Palm-E: An Embodied Multimodal Language Model</h1>
    
    <div id="readability-page-1" class="page"><div id="main">
        
        <div>
             
            <p><strong><SPAN size="+3">PaLM-E: An Embodied Multimodal Language Model</SPAN></strong> 
            </p>
        </div>
        


        

        <div>
            
            <div>
                <video id="v0" width="100%" playsinline="" muted="" loop="" autoplay="" onclick="setAttribute(&#39;controls&#39;, &#39;true&#39;);">
                    <source src="videos/palm-e-teaser.mp4" type="video/mp4"/>
                </video>		

                <h3>
                    Abstract
                </h3>
                <p>

                    Large language models have been demonstrated to perform complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks, including sequential robotic manipulation planning, visual question answering, and captioning.
                    
                    Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits <i>positive transfer</i>: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains.
                    Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.
                </p>
            </div>
        </div>

        <div>
            <div>
                <h3>
                    Approach
                </h3>
                <p>
                    <img src="https://palm-e.github.io/img/approach.png"/>
                </p>

                <p>
                    The main architectural idea of PaLM-E is to inject continuous, embodied observations such as images, 
                    state estimates, or other sensor modalities into the language embedding space of a pre-trained language model. 
                    This is realized by encoding the continuous observations into a sequence of vectors with the same dimension 
                    as the embedding space of the language tokens. The continuous information is hence injected into the language model 
                    in an analogous way to language tokens. PaLM-E is a decoder-only LLM that generates textual completions 
                    autoregressively given a prefix or prompt. We call our model PaLM-<b>E</b>, since we use <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">PaLM (Chowdhery et al., 2022)</a> as 
                    the pre-trained language model, and make it <b>E</b>mbodied.
                </p>    
            </div>
        </div>




    <div>
            <div>
                <h3>
                    Results
                </h3>

        <p> 
		We show a few example videos showing how PaLM-E can be used to plan and execute long horizon tasks on two different real embodiments. Please note, that all of these results were obtained using the same model trained on all data.
		In the first video, we execute a long-horizon instruction &#34;bring me the rice chips from the drawer&#34; that includes multiple planning steps as well as incorporating visual feedback from the robot&#39;s camera. 
		Finally, show another example on the same robot where the instruction is &#34;bring me a green star&#34;. Green star is an object that this robot wasn&#39;t directly exposed to.
		</p>
		

                <div id="carouselExampleCaptions3" data-bs-ride="carousel">
                    
                    <div>
                      <div data-bs-interval="100000000000">
                        <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                            <source src="videos/meta/planning_4x_compressed.mp4" type="video/mp4"/>
                        </video>	
                        <p>
                            <h5>&#34;Bring me the rice chips from the drawer.&#34;</h5>
                          <!-- <p>Some representative placeholder content for the second slide.</p> -->
                        </p>
                      </div>
                      <div data-bs-interval="100000000000">
                        <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                            <source src="videos/meta/green_star.mp4" type="video/mp4"/>
                        </video>	
                        <p>
                          <h5>&#34;Bring me the green star.&#34;</h5>
                          <!-- <p>Some representative placeholder content for the second slide.</p> -->
                        </p>
                      </div>
                    </div>
                    </div>

                  
    
        
        
        <p>
         In the following part, we show PaLM-E controlling a table top robot arranging blocks. We show the PaLM-E can successfully plan over multiple stages based on visual and language input. Our model is able to successfully plan a long-horizon task &#34;sort blocks by colors into different corners&#34; . 
         Another example of planning over multiple stages and incorporating visual feedback over long time horizons.
         Finally, we demonstrate another example of long-horizon pushing tasks on this robot. The first instruction is &#34;move remaining blocks to the group&#34;.  
         PaLM-E sequences step-by-step commands to the low-level policy such as &#34;move the yellow hexagon to the green star&#34;, and &#34;move the blue triangle to the group&#34;.
        </p>

        <div id="carouselExampleCaptions" data-bs-ride="carousel">
            
            <div>
              
              <div data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/xarm/sort_by_color_iphone.mp4" type="video/mp4"/>
                </video>		
                <p>
                  <h5>sort blocks by colors into different corners</h5>
                  <!-- <p>Some representative placeholder content for the first slide.</p> -->
                </p>
              </div>
              <div data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/xarm/sort_colors_into_corners_long.mp4" type="video/mp4"/>
                </video>	
                <p>
                  <h5>Incorporating visual feedback over long time horizons</h5>
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </p>
              </div>
              <div data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/xarm/remaining_blocks_to_group.mp4" type="video/mp4"/>
                </video>		
                <p>
                  <h5>Move remaining blocks to the group</h5>
                  <!-- <p>Some representative placeholder content for the third slide.</p> -->
                </p>
              </div>
              <div data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/xarm/ocean_colored_blocks_together.mp4" type="video/mp4"/>
                </video>		
                <p>
                  <h5>Push the ocean colored blocks together</h5>
                  <!-- <p>Some representative placeholder content for the third slide.</p> -->
                </p>
              </div>
            </div>
            </div>

        
        
        <p>
        Next, we demonstrate two examples of generalization. In the case below the instruction is &#34;push red blocks to the coffee cup&#34;. The dataset contains only three demonstrations with the coffee cup in them, and none of them included red blocks.
		We show another generalization example, where the instruction is &#34;push green blocks to the turtle&#34;. The robot is able to successfully execute this task even though it has never seen the turtle before.
		</p>

        <div id="carouselExampleCaptions2" data-bs-ride="carousel">
            
            <div>
              <div data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/xarm/red_blocks_to_coffee.mp4" type="video/mp4"/>
                </video>		
                <p>
                  <h5>Push red blocks to the coffee cup</h5>
                  <!-- <p>Some representative placeholder content for the first slide.</p> -->
                </p>
              </div>
              <div data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/xarm/green_blocks_to_turtle.mp4" type="video/mp4"/>
                </video>	
                <p>
                  <h5>Push green blocks to the turtle</h5>
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </p>
              </div>
            </div>
            </div>
            <p>In addition to unlocking new capabilities in robot planning. PaLM-E is a competent Vision-Language Model. Please check out our <a href="https://arxiv.org/abs/2303.03378">paper</a> for more details and see the dmeo below.</p>
	    </div>
        </div>

        <div id="demo">
            <div>
                <h3>
                    Demo
                </h3>
                <p>
                    The examples below are all example completions (in orange) from PaLM-E.  The prompt is the one or more images and the text in gray.
                </p>

                
                

                <div>
                    <p><img id="expandedImg" src="https://palm-e.github.io/img/placeholder.png"/>
                    </p>
                    <div>
                    <p>Prompt text in gray.</p>
                    <p id="answer">PaLM-E response in orange shade.</p>
                    </div>
                    
                </div>
                

                
        
            </div>
        </div>

        


        <div>
            <div>
                <h3>
                    Acknowledgements
                </h3>
                <p>
                    The authors would like to thank, for their advice, help
                    and support: Xi Chen, Etienne Pot, Sebastian Goodman,
                    Ted Xiao, Keerthana Gopalakrishnan, Kehang Han, Henryk
                    Michalewski, Neil Houlsby, Basil Mustafa, Justin Gilmer,
                    Yonghui Wu, Erica Moreira, Victor Gomes, Tom Duerig,
                    and Kendra Byrne.
                </p>
            </div>
        </div>



    </div></div>
  </body>
</html>
