<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://purplesyringa.moe/blog/the-ram-myth/">Original</a>
    <h1>The RAM Myth</h1>
    
    <div id="readability-page-1" class="page"><section><div><h2>The RAM myth</h2><p><time>December 19, 2024</time><a href="https://www.reddit.com/r/programming/comments/1hhds9c/the_ram_myth/"> Reddit</a><a href="https://news.ycombinator.com/item?id=42456310"> Hacker News</a></p><p>The RAM myth is a belief that modern computer memory resembles perfect random-access memory. Cache is seen as an optimization for small data: if it fits in L2, it‚Äôs going to be processed faster; if it doesn‚Äôt, there‚Äôs nothing we can do.</p><p>Most likely, you believe that code like this is the fastest way to shard data (I‚Äôm using Python as pseudocode; pretend I used your favorite low-level language):</p><pre><code>groups = [[] <span>for</span> _ <span>in</span> <span>range</span>(n_groups)]
<span>for</span> element <span>in</span> elements:
    groups[element.group].append(element)
</code></pre><p>Indeed, it‚Äôs linear (i.e. asymptotically optimal), and we have to access random indices anyway, so cache isn‚Äôt going to help us in any case.</p><p>In reality, when the number of groups is high, this is leaving a lot of performance on the table, and certain <em>asymptotically slower</em> algorithms can perform sharding significantly faster. They are mostly used by on-disk databases, but, surprisingly, they are useful even for in-RAM data.</p><p>The algorithm from above has an almost-guaranteed cache miss on every iteration. The only way to prevent cache misses is to make the memory accesses more ordered. If you can ensure the elements are ordered by <code>group</code>, that‚Äôs great. If you can‚Äôt, you can still sort the accesses before the <code>for</code> loop:</p><pre><code>elements.sort(key = <span>lambda</span> element: element.group)
</code></pre><p>Sorting costs some time, but in return, removes cache misses from the <code>for</code> loop entirely. If the data is large enough that it doesn‚Äôt fit in cache, this is a net win. As a bonus, creating individual lists can be replaced with a group-by operation:</p><pre><code>elements.sort(key = <span>lambda</span> element: element.group)
groups = [
    group_elements
    <span>for</span> _, group_elements
    <span>in</span> itertools.groupby(elements, key = <span>lambda</span> element: element.group)
]
</code></pre><p>There‚Äôs many cache-aware sorting algorithms, but as indices are just integers, radix sort works best here. Among off-the-shelf implementations, <a href="https://docs.rs/radsort/latest/radsort/">radsort</a> worked the best for me in Rust.</p><h2>Speedups</h2><p>This is already better than the straightforward algorithm on large data, but there‚Äôs many tricks to make it faster.</p><p>Sorting APIs try to make it seem like the data is sorted in-place, even when that‚Äôs not the case. This requires sorted data to be explicitly written to memory in a particular format. But if we only need to iterate over groups, generators or callbacks help avoid this:</p><pre><code>
<span>def</span> <span>radix_sort</span>(<span>elements, bits = <span>32</span></span>):
    
    <span>if</span> <span>len</span>(elements) &lt;= <span>1</span> <span>or</span> bits &lt;= <span>0</span>:
        <span>yield</span> <span>from</span> elements
        <span>return</span>

    
    buckets = [[] <span>for</span> _ <span>in</span> <span>range</span>(<span>256</span>)]
    <span>for</span> element <span>in</span> elements:
        buckets[(element.index &gt;&gt; <span>max</span>(<span>0</span>, bits - <span>8</span>)) &amp; <span>0xff</span>].append(element)

    
    <span>for</span> bucket <span>in</span> buckets:
        <span>yield</span> <span>from</span> radix_sort(bucket, bits - <span>8</span>)
</code></pre><p>We can even remove the <code>groupby</code> step by yielding individual groups:</p><pre><code>
<span>if</span> bits &lt;= <span>0</span>:
    <span>if</span> elements:
        
        <span>yield</span> elements
    <span>return</span>
</code></pre><p>The next problem with this code is constantly reallocating the <code>bucket</code> arrays on <code>append</code>. This invokes <code>memcpy</code> more often than necessary and is bad for cache. A common fix is to compute sizes beforehand:</p><pre><code><span>def</span> <span>get_bucket</span>(<span>element</span>):
    <span>return</span> (element.index &gt;&gt; <span>max</span>(<span>0</span>, bits - <span>8</span>)) &amp; <span>0xff</span>

sizes = Counter(<span>map</span>(get_bucket, elements))



buckets = [reserve(sizes[i]) <span>for</span> i <span>in</span> <span>range</span>(<span>256</span>)]
<span>for</span> element <span>in</span> elements:
    buckets[get_bucket(element)].append(element)
</code></pre><p>This, however, requires two iterations, and ideally we‚Äôd keep the code single-pass. If the index is random, we can have our cake and eat it too: <em>estimate</em> the size of each bucket as <code>len(elements) / 256</code> and reserve that much space. There‚Äôs going to be some leftovers if we underestimate, which we‚Äôll store in a small separate storage:</p><pre><code><span>class</span> <span>Bucket</span>:
    reserved: <span>list</span>
    leftovers: <span>list</span>

    <span>def</span> <span>__init__</span>(<span>self, capacity</span>):
        self.reserved = reserve(capacity) 
        self.leftovers = []

    <span>def</span> <span>append</span>(<span>self, element</span>):
        <span>if</span> <span>len</span>(self.reserved) &lt; self.reserved.capacity(): 
            self.reserved.append(element)
        <span>else</span>:
            self.leftovers.append(element)

    <span>def</span> <span>__len__</span>(<span>self</span>):
        <span>return</span> <span>len</span>(self.reserved) + <span>len</span>(self.leftovers)

    <span>def</span> <span>__iter__</span>(<span>self</span>):
        <span>yield</span> <span>from</span> self.reserved
        <span>yield</span> <span>from</span> self.leftovers
</code></pre><p>The probability distribution plays ball here: on large input, only a tiny percentage of the elements overflow into <code>leftovers</code>, so the memory overhead is pretty small, reallocations on pushing into <code>leftovers</code> are fast, and bucketing (and iterating over a bucket) is cache-friendly.</p><p>One simple micro-optimization is to allocate once and split the returned memory into chunks instead of invoking <code>malloc</code> (or creating vectors) multiple times. Allocations are pretty slow, and this is a cheap way to reduce the effect.</p><p>Switching to the straightforward algorithm on small inputs increases performance, as the effects of <eq><math><mrow><mi>ùí™</mi><mo form="prefix" stretchy="false">(</mo><mi>n</mi><mrow><mspace width="0.1667em"></mspace><mi>log</mi><mo>‚Å°</mo><mspace width="0.1667em"></mspace></mrow><mi>n</mi><mo form="postfix" stretchy="false">)</mo></mrow></math></eq> code are more pronounced there. However, as <code>radix_sort</code> is recursive, we can perform this check on every level of recursion, scoring a win even on large data:</p><pre><code>
<span>if</span> <span>len</span>(elements) &lt;= CUTOFF <span>or</span> bits &lt;= <span>8</span>:
    counts = [<span>0</span>] * <span>256</span>
    <span>for</span> element <span>in</span> elements:
        counts[element.index &amp; <span>0xff</span>] += <span>1</span>

    groups = [[] <span>for</span> _ <span>in</span> <span>range</span>(<span>256</span>)]
    <span>for</span> element <span>in</span> elements:
        groups[get_bucket(element)].append(element)

    <span>for</span> group <span>in</span> groups:
        <span>if</span> group:
            <span>yield</span> group
    <span>return</span>
</code></pre><p>The optimal <code>CUTOFF</code> is heavily machine-dependent. It depends on the relative speed of cache levels and RAM, as well as cache size and data types. For 64-bit integers, I‚Äôve seen machines where the optimal value was <code>50k</code>, <code>200k</code>, and <code>1M</code>. The best way to determine it is to benchmark in runtime ‚Äì an acceptable solution for long-running software, like databases.</p><h2>Benchmark</h2><p>Here‚Äôs a small benchmark.</p><p>The input data is an array of random 64-bit integers. We want to group them by a simple multiplicative hash and perform a simple analysis on the buckets ‚Äì say, compute the sum of minimums among buckets. (In reality, you‚Äôd consume the buckets with some other cache-friendly algorithm down the pipeline.)</p><p>We‚Äôll compare two implementations:</p><ol><li>The straightforward algorithm with optimized allocations.</li><li>Radix sort-based grouping, with all optimizations from above and the optimal cutoff.</li></ol><p>The average group size is <eq><math><mn>10</mn></math></eq>.</p><p>The code is available on <a href="https://github.com/purplesyringa/site/blob/master/blog/the-ram-myth/benchmark.rs">GitHub</a>.</p><p>The relative efficiency of the optimized algorithm grows as the data gets larger. Both the straightforward algorithm and the optimized one eventually settle at a fixed throughput. Depending on the machine, the improvement can be anywhere between <eq><math><mrow><mn>2.5</mn><mo>√ó</mo></mrow></math></eq> and <eq><math><mrow><mn>9</mn><mo>√ó</mo></mrow></math></eq> in the limit.</p><p>The results are (<code>A</code>, <code>Y</code>, <code>M</code> indicate different devices):</p><p><img alt="Grouping performance is benchmarked on three devices on element counts from 80k to 40M (with power-of-two steps). The cutoffs are 50k for A, 200k for Y, and 1M for M. On all three device, the throughput of the two algorithms is equivalent up to the cutoff, with radix sort getting faster and faster above the cutoff. The throughput of the straightforward algorithm degrades faster, while radix sort is way more stable." title="Grouping performance is benchmarked on three devices on element counts from 80k to 40M (with power-of-two steps). The cutoffs are 50k for A, 200k for Y, and 1M for M. On all three device, the throughput of the two algorithms is equivalent up to the cutoff, with radix sort getting faster and faster above the cutoff. The throughput of the straightforward algorithm degrades faster, while radix sort is way more stable." src="https://purplesyringa.moe/blog/the-ram-myth/benchmark.svg"/></p><p><img alt="Performance improvement of the new algorithm on different element counts, as measured on three devices. On A, the improvement slowly increases from 1x to 3x up to 5M elements and then quickly raises to 8x at 40M elements. On Y and M, the improvement is not so drastic, slowly but surely raising to 2.5x -- 3x." title="Performance improvement of the new algorithm on different element counts, as measured on three devices. On A, the improvement slowly increases from 1x to 3x up to 5M elements and then quickly raises to 8x at 40M elements. On Y and M, the improvement is not so drastic, slowly but surely raising to 2.5x -- 3x." src="https://purplesyringa.moe/blog/the-ram-myth/improvement.svg"/></p><p>Is it worth it? If you absolutely need performance and sharding is a large part of your pipeline, by all means, use this. For example, I use this to find a collision-free hash on a given dataset. But just like with any optimization, you need to consider if increasing the code complexity is worth the hassle.</p><p>At the very least, if you work with big data, this trick is good to keep in mind.</p><p>Here‚Äôs another takeaway lesson. Everyone knows that, when working with on-disk data, you shouldn‚Äôt just map it to memory and run typical in-memory algorithms. It‚Äôs <em>possible</em>, but the performance are going to be bad. The take-away lesson here is that this applies to RAM and cache too: if you‚Äôve got more than, say, <eq><math><mn>32</mn></math></eq> MiB of data, you need to seriously consider partitioning your data or switching to external memory algorithms.</p></div></section></div>
  </body>
</html>
