<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.microcenter.com/site/mc-news/article/micro-center-santa-clara-photos.aspx">Original</a>
    <h1>Silicon Valley finally has a big electronics retailer again: Micro Center opens</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>
For a while now, one of my biggest responsibilities in <a href="https://gtoolkit.com/">Glamorous Toolkit</a> has been to develop <a href="https://github.com/feenkcom/gt4llm">gt4llm</a>, a workbench for large language models that is intended to help develop assistants and also comes with a set of integrated assistants for code, prose, and other subsystems with GT.
  </p>
<p>
I haven’t written much about the workbench outside of the Glamorous Toolkit book, and if you want to learn more about it, <a href="https://book.gtoolkit.com/gt4llm-270ytb3y5mi1voswipfyc5hti">I suggest reading there</a>. It has all the context I know to talk about<sup><a href="#1">1</a></sup>.
  </p>
<p>
Instead, today I want to talk about a silly little idea that I had a long while back, and finally realized using the workbench: LlmL, the language that lets you shell out to an LLM when you don’t know how to solve a problem<sup><a href="#2">2</a></sup>. Let’s start by looking at the language, and then talk a bit about the implementation, and see what we can learn from it. Beware that the implementation bit is written in GT-specific Smalltalk, but the code should be short and simple enough that the idea can get across.
  </p>

<p>
The language &#34;design&#34; of LlmL was primarily shaped by the one hour timebox I set for myself when working on this project. As such, It only has numbers as types, and it uses an S-expression-based syntax (for ease of parsing).
  </p>
<p>
Here is a sample:
  </p>
<div>
  <div>
      <pre><code>(def x
  (fn (n)
    (* n 2)))

(+ (x 10) 3)</code></pre>
    </div>
  
</div>
<p>
This should result in <code>23</code>.
  </p>
<p>
So far, so simple. We have a language with functions, variables and numbers, using an S expression syntax. So, what is different?
  </p>
<p>
Well, consider factorial. How would you usually write factorial in a language like that? Here is one way:
  </p>
<div>
  <div>
      <pre><code>(def factorial
  (fn (n)
    (if (= n 0)
      1
      (* n (factorial (- n 1))))))

(factorial 5)</code></pre>
    </div>
  
</div>
<p>
Except this wouldn’t work, because we don’t have <code>if</code> and conditionals and all that jazz. Instead of trying to implement all of that using Church encodings and fancy lambda calculus thinga<sup><a href="#3">3</a></sup>, we can instead just call <code>infer</code>.
  </p>
<div>
  <div><pre><code>(def factorial (infer))
(factorial 5)</code></pre>
    </div>
  
</div>
<p>
And it will give us the right answer, 120!
  </p>
<p>
So, how does that work? Well, the basic idea was that in the age of just blindly copying a bunch of code that a language model generates and putting that in our code base, how about we just cut out the middleman and just ask it for the answer at runtime instead?
  </p>
<p>
So, we give the LLM the name of the function as well as the arguments it was called with and ask it to generate an answer, and then pass that back into the interpreter. Cool, right?
  </p>
<p>
I think we just accidentally stumbled into the future, where solving problems is outsourced in real-time.
  </p>

<p>
Now that we’ve talked about     <i>
what
    </i>
 LlmL is, let’s talk about how it works.
  </p>
<p>
The parser uses a framework called SmaCC that is used for virtually every parser in GT. It’s a parser generator framework and it’s quite awesome. Once you know how to wield it (which might take a bit), it takes something like 5-10 minutes to write a parser for something like LlmL that produces a neat little AST.
  </p>
<p>
From there, we build a simple evaluator that is just a tree-walking interpreter that keeps track of the variables in an environment and allows for environments to be pushed and popped as needed.
  </p>
<p>
Really the only novel and mildly interesting bit is inside the evaluation for the <code>infer</code> primitive, which in turn happens in a class called <code>LlmlInfer</code>    <span>
<code>Object subclass: #LlmlInfer
	instanceVariableNames: &#39;connection&#39;
	classVariableNames: &#39;&#39;
	package: &#39;LlmL-Model&#39;</code>
    </span>
.  Its method <code>LlmlInfer&gt;&gt;#evaluate:in:</code>    <span>
<code>evaluate: aCollection in: aLlmlEvaluator
	| chat |
	chat := GtLlmChat new
			provider: (self connection buildProvider format: self format).
	chat
		sendMessage: &#39;Provide the output of the following expression: &#39;
				, aLlmlEvaluator callStack last source.
	chat provider executions last wait.
	^ (chat messages last contentJson at: &#39;result&#39;) asNumber</code>
    </span>
 is the key to the LLM connection.
  </p>
<p>
It uses <a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">structured outputs</a> to guarantee that we always get a well-formed result. For those not in the know, it’s basically a way to make the LLM to adhere to a user-controlled JSON schema in the output.
  </p>
<p>
To transcribe it as a snippet and annotate, it basically does this:
  </p>
<div>
  <pre>&#34;Providers hold onto all the low-level bits and are
different for every LLM API. Currently we support Ollama,
OpenAI, Anthropic, and Google Gemini. These providers also
understand how to add a structured output format to the API
call.&#34;
provider := self connection buildProvider
  format: self format.

&#34;Then we create a chat and send a message.&#34;
chat := GtLlmChat new provider: provider.
chat sendMessage:
  &#39;Provide the output of the following expression: &#39;, 
  aLlmlEvaluator callStack last source.
&#34;^We get the last section from the callstack here.
It also has the arguments.&#34;

&#34;These executions happen asynchronously, so we force
synchronicity by waiting.&#34;
chat provider executions last wait.

&#34;We get the result from the JSON we got back and just
confidently convert it to a number.&#34;
(chat messages last contentJson at: &#39;result&#39;) asNumber</pre>
</div>
<p>
Usually in <code>gt4llm</code>, the format that we would use would be autogenerated by a higher level schema, but I didn’t want to bother with it, so I just wrote a simple structured output format by hand:
  </p>
<div><pre>{&#39;type&#39; -&gt; &#39;object&#39;.
 &#39;properties&#39; -&gt;
    {&#39;result&#39; -&gt;
      {&#39;type&#39; -&gt; &#39;number&#39;} asDictionary} asDictionary.
  &#39;required&#39; -&gt; {&#39;result&#39;}.
  &#39;additionalProperties&#39; -&gt; false} asDictionary</pre></div>
<p>
This won’t work with all providers because they are ever so slightly different in how they want their schema to be formatted. It does work with Ollama and OpenAI, though, and that was good enough for me here. For anything more, resort to the higher level.
  </p>
<p>
Still, the core of the system works with all providers we’ve provided in <code>gt4llm</code>, so whatever you set up as your default connection will be used by the evaluator by default. You can also override it to inject your own models or providers per evaluation:
  </p>
<div>
  <pre>LlmlEvaluator new
	connection: (GtLlmConnection new
			provider: GtOllamaProvider;
			model: &#39;llama3.2&#39;);
	evaluate: (LlmlParser
			parse: &#39;(def factorial (infer)) (factorial 4)&#39;)</pre>
</div>
<p>
And I think that’s all of it!
  </p>

<p>
I hope you enjoyed this little tour of my groundbreaking new research in programming language design!
  </p>
<p>
Okay, admittedly, LlmL is silly, but maybe not as silly as one might think at first glance: when people copy-paste code they found online or that an LLM spit out, what they actually want, at the heart of it, is an     <i>
oracle
    </i>
. Someone to just tell them the answer. And instead of generating an algorithm that won’t be reviewed anyway, this just gives you the answer right away. But it’s also a reductio ad absurdum of this idea that, somehow, this will solve our problems, because it will not. It just creates the ultimate black box.
  </p>
<p>
I really wanted to write a more philosophical article about LLMs, but I thought you deserved a bit of technical whimsy before that. I hope you liked the appetizer, the main course is to follow!
  </p>

<p>
<span id="1">1.</span> If you find anything missing, do not hesitate to contact me or <a href="https://github.com/feenkcom/gtoolkit/issues/">file an issue</a>!
  </p>
<div>
  <p>
<span id="2">2.</span> For a more manual way of approaching problem solving, <a href="https://blog.veitheller.de/On_starting_hard_things.html">read my last blog post</a>.
  </p>
</div>
<p>
<span id="3">3.</span> You couldn’t actually do that, either, because you’d have to start with a Church encoding instead of numbers to get to equality. If you have equality, it’s all smooth sailing from there.
  </p>
      
    </div></div>
  </body>
</html>
