<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mind-vis.github.io/">Original</a>
    <h1>Reconstructing images a person sees via non-invasive brain scans</h1>
    
    <div id="readability-page-1" class="page"><div id="main">
        
        

        <div>
            <div>
                <ul>
                    <li>
                        <sup>1</sup>National University of Singapore, Center for Sleep and Cognition, Centre for Translational Magnetic Resonance Research
                    </li>
                    </ul>
            </div>
        </div>

        

        <div>
            <div>
                <h3>
                    Overview
                </h3>
                <p><img src="https://blog.zulip.com/2022/11/16/zulip-server-5-7-security-release/index_files/images/first_fig.png" alt="overview"/><br/>
            </p></div>
        </div>

        <div>
            <div>
                <h3>
                    Motivation
                </h3>
                <p>
                    Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human vision and computer vision through the Brain-Computer Interface. 
                    However, due to the scarcity of data annotations and the complexity of underlying brain information, it is challenging to decode images with faithful details and meaningful semantics. 
                </p>
            </div>
        </div>

        <div>
            <div>
                <h3>
                    Contribution
                </h3>
                <p>
                    In this work, we present <b>MinD-Vis</b>: Sparse <b>M</b>asked Bra<b>in</b> Modeling with <b>D</b>ouble<b>-</b>Conditioned Diffusion Model for <b>Vis</b>ion Decoding.                   
                    Specifically, by boosting the information capacity of representations learned in a large-scale resting-state fMRI dataset, we show that our MinD-Vis framework reconstructed <b>highly plausible images with semantically matching details</b> from brain recordings with very few training pairs. 
                    We benchmarked our model and our method outperformed state-of-the-arts in both <b>semantic mapping</b> (100-way semantic classification) and <b>generation quality</b> (FID) by <b>66%</b> and <b>41%</b>, respectively.
                    Exhaustive ablation studies are conducted to analyze our framework. 
                </p>
            </div>
        </div>

        <!-- <div class="row" id="video">
            <div class="col-md-8 col-md-offset-2">
                <h3>Video</h3><p style="color:blue;font-size:11px;">(contains audio w/ subtitles)</p>
                <div class="text-center">
                    <video id="video_id" width="100%" controls="" controlsList="nodownload">>
                        <source src="./index_files/videos/video.mp4" type="video/mp4">
                        <track label="English" kind="subtitles" srclang="en" src="./index_files/videos/captions.vtt">
                    </video>

                    <script type="text/javascript">
                        $(document).ready(function() {
                        var video = document.querySelector('#video_id'); // get the video element
                        var tracks = video.textTracks; // one for each track element
                        var track = tracks[0]; // corresponds to the first track element
                        track.mode = 'hidden';});
                    </script>

                </div>
            </div>
        </div> -->
        
        <div>
            <div>
                <h3>
                    Highlights
                </h3>
                <ul>
                        <li>
                            A human visual decoding system that only reply on limited annotations.
                        </li>
                        <li>
                            State-of-the-art 100-way top-1 classification accuracy on GOD dataset: <b>23.9%</b>, outperforming the previous best by <b>66%</b>.
                        </li>
                        <li>
                            State-of-the-art generation quality (FID) on GOD dataset: <b>1.67</b>, outperforming the previous best by <b>41%</b>.
                        </li>
                        <li>
                            For the first time, we show that non-invasive brain recordings can be used to decode images with similar performance as invasive measures.
                        </li>
                    </ul>
                
            </div>
        </div>

        <div>
            <div>
                <h3>
                    MinD-Vis
                </h3>
                <p><img src="https://blog.zulip.com/2022/11/16/zulip-server-5-7-security-release/index_files/images/flowchart.png" alt="method"/></p></div>
        </div>

        <div>
            <div>
                <h3>
                    Results compared with benchmarks
                </h3>
                <p><img src="https://blog.zulip.com/2022/11/16/zulip-server-5-7-security-release/index_files/images/compare_figs.png" alt="result with sota"/><br/>
            </p></div>
        </div>

        <div>
            <div>
                <h3>
                    Generation Consistency              Replication Dataset
                </h3>
                <p><img src="https://blog.zulip.com/2022/11/16/zulip-server-5-7-security-release/index_files/images/more_result.png" alt="consistency and bold5000"/><br/>
            </p></div>
        </div>


        <div id="BibTeX">
            <div>
                <h3>
                    BibTeX
                </h3><p>
               If you find our data or project useful in your research, please cite:
                </p><pre>@InProceedings{chen_2022_arXiv,
    author    = {Chen, Zijiao and Qing, Jiaxin and Xiang, Tiange and Yue, Wan Lin and Zhou, Juan Helen},
    title     = {Seeing Beyond the Brain: Masked Modeling Conditioned Diffusion Model for Human Vision Decoding},
    booktitle = {arXiv},
    month     = {November},
    year      = {2022},
    url       = {https://arxiv.org/abs/2211.06956}
}</pre>
            </div>
        </div>
        <div>
            <div>
                <h3>
                    Acknowledgments
                </h3><p>
                The website template was borrowed from <a href="https://curvenet.github.io">Tiange Xiang</a>.
                </p>
            </div>
        </div>
    </div></div>
  </body>
</html>
