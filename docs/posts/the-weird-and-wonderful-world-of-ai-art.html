<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jxmo.notion.site/The-Weird-and-Wonderful-World-of-AI-Art-b9615a2e7278435b98380ff81ae1cf09">Original</a>
    <h1>The Weird and Wonderful World of AI Art</h1>
    
    <div id="readability-page-1" class="page">
		<p>&lt;aside&gt;
üëâ Hi everyone! My name is Jack Morris. I‚Äôm a PhD student at Cornell studying NLP, and I‚Äôm interested in AI in general. To read more of my writing, check out my blog: <a href="https://jxmo.io/"></a><a href="https://jxmo.io/">https://jxmo.io/</a></p>
<p>&lt;/aside&gt;</p>
<p>The world of Artificial-Intelligence generated art has exploded over the last twelve months. In January 2021, OpenAI released two models that changed the game: DALL-E and CLIP. These models showed what might be possible by generating visual art from text-based prompts.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8659ceef-eb87-4a88-ab93-d264d968a7f7/Screen_Shot_2022-02-24_at_2.09.24_PM.png" alt="‚ÄúKowloon City, in the style of Wes Anderson‚Äù, from Twitter user @somnai_dreams"/></p>
<p>‚ÄúKowloon City, in the style of Wes Anderson‚Äù, from Twitter user <a href="https://twitter.com/somnai_dreams/">@somnai_dreams</a></p>
<p>The release of DALL-E and CLIP kickstarted a new wave of work in AI art. Digital artists organized on Twitter, Github, and Discord developed tools for prototyping and generating art. For the technically proficient, artists shared their work in the form of Colab notebooks. For those without coding skills, art can be created through new websites and tools that allow you to harness the power of these deep learning models without writing any code.</p>
<p>How much have things really changed? Take a look at some ‚ÄúAI art‚Äù generated by pre-2021 techniques:</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4243a983-53e9-4640-840a-610c56f3dd49/Untitled.png" alt="Untitled"/></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f5fa1b73-3f09-4d00-bdba-b4260f00f99b/Untitled.png" alt="Untitled"/></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ca49f362-5729-4af3-89dd-cec71d79c457/Untitled.png" alt="Untitled"/></p>
<p>Now consider the following three images, generated in 2021 with CLIP, DALL-E, or related technologies (feel free to zoom):</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/adc23d40-5ab5-4615-aee4-85fddd3103c8/Screen_Shot_2022-01-28_at_2.20.54_PM.png" alt="Screen Shot 2022-01-28 at 2.20.54 PM.png"/></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5ee23ff1-ea5c-44f2-b1b0-e9f70f252f4e/Screen_Shot_2022-01-28_at_2.21.34_PM.png" alt="Screen Shot 2022-01-28 at 2.21.34 PM.png"/></p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/de231af5-fe1a-46b7-93ff-d6e098a717f4/Untitled.png" alt="Untitled"/></p>
<p>The reality right now is really, really crazy.</p>
<p>I don‚Äôt think the majority of AI researchers would even have suspected that these images could be created with current tools. The rapidity of the past year‚Äôs developments have surprised even some of the most bullish technologists.</p>
<p>The AI art we had <em>before</em> 2021 was intriguing, but tended to be abstract, esoteric, and just not that relatable to a human. The AI art we have <em>now</em> is fully controllable, and can be about whatever you want it to be.</p>
<p>What changed? Well, there‚Äôs something to be said for the new wave of publicity and interest, which certainly accelerated the pace of our art-generation techniques. But the main development is the rise of <strong>multimodal learning</strong>.</p>
<p>Multimodal learning, in this case, is learning to match up text and images. Our new models are really good at learning to write captions for images, and (more importantly for artistic purposes) to generate images that correspond to a given caption.</p>
<p>We already had AI-generated images that were high-resolution (see <a href="http://thispersondoesnotexist.com">thispersondoesnotexist.com</a> for an example). What‚Äôs changed is that our new joint image-text models give us some <em>control</em> over image generation. And when we combine multimodal models with really good image generation models that we already had, we get results like this:</p>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4e34eee8-528e-4f89-b4ab-bcf12cf4cf80/Screen_Shot_2022-01-28_at_2.34.19_PM.png" alt="Screen Shot 2022-01-28 at 2.34.19 PM.png"/></p>
<p>The rise of multimodal models has jumpstarted research into the best way to create beautiful AI art from a text-based prompt. A whole new generation of deep-learning-researcher-slash-artists have emerged from the depths of the Internet and joined forces to push the science forward. We‚Äôve observed an interesting confluence of empirical science (deep learning works) plus engineering (so many hyperparameters to set: smoothness, color range, clipping, optimization learning rate, initialization...).</p>
<p>Before we take a look at some art generated with CLIP and DALL-E and related models, let‚Äôs consider some of the art generated from before we developed text-based prompting. (Or, if you‚Äôre not interested in any of the history, scroll to the bottom to see some cool pictures!)</p>
<h2>Early forms of AI art</h2>

	

</div>
  </body>
</html>
