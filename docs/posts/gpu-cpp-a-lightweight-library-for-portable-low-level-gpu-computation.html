<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.answer.ai/posts/2024-07-11--gpu-cpp.html">Original</a>
    <h1>Gpu.cpp: A lightweight library for portable low-level GPU computation</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">




<p>We’re thrilled to announce the release of <a href="https://github.com/AnswerDotAI/gpu.cpp">gpu.cpp</a>. gpu.cpp is a lightweight, open-source library that makes portable GPU compute with C++ simple.</p>
<p>gpu.cpp focuses on general purpose native GPU computation, leveraging the WebGPU specification as a portable low-level GPU interface. This means we can drop in GPU code in C++ projects and have it run on Nvidia, Intel, AMD, and other GPUs. The same C++ code can work on a wide variety of laptops, workstations, mobile devices or virtually any hardware with Vulkan, Metal, or DirectX support.</p>
<p>The project is located at <a href="https://github.com/AnswerDotAI/gpu.cpp">github.com/AnswerDotAI/gpu.cpp</a>.</p>
<section id="use-cases">
<h2 data-anchor-id="use-cases">Use Cases</h2>
<p>gpu.cpp is aimed at enabling projects requiring portable on-device GPU computation with minimal implementation complexity and friction. Some example use cases are:</p>
<ul>
<li>Development of GPU algorithms to be run on personal computing devices</li>
<li>Direct standalone implementations of neural network models</li>
<li>Physics simulations and simulation environments</li>
<li>Multimodal applications - audio and video processing</li>
<li>Offline graphics rendering</li>
<li>ML inference engines and runtimes</li>
<li>Parallel compute intensive data processing applications</li>
</ul>
<p>Although gpu.cpp is meant for any general purpose GPU computation and not strictly AI, one area we’re interested in is pushing the limits exploring the intersection of new algorithms for post-training and on-device compute.</p>
<p>To date, AI research has primarily been built with CUDA as the priveledged first-class target. CUDA has been dominant at large scale training and inference but at the other end of the the spectrum in the world of GPU compute on personal devices, there exists far more heterogeneity in the hardware and software stack.</p>
<p>GPU compute in this personal device ecosystem has been largely limited to a small group of experts such as game engine developers and engineers working directly on ML compilers or inference runtimes. Along with that, implementing against the Vulkan or even WebGPU API directly tends to be targeted mostly towards infrastrcture scale efforts - game engines, production ML inference engines, large software packages.</p>
<p>We want to make it easier for a broader range of projects to harness the power of GPUs on personal devices. With a small amount of code, we can access the GPU at a low-level, focusing on directly implementing algorithms rather than the scaffolding and tech stack around the GPU. For example, in our AI research there’s much to explore with the various forms of dynamic/conditional post-training computation - dynamic use of adapters, sparsity, model compression, realtime multimodal integrations etc.</p>
<p>gpu.cpp lets us implement and drop-in any algorithm with fine-grained control of data movement and GPU code, and explore outside boundaries of what is supported by existing production-oriented inference runtimes. At the same time we can write code that is portable and immediately usable on a wide variety of and GPU vendors and compute form factors - workstations, laptops, mobile, or even emerging hardware platforms such as AR/VR and robotics.</p>
</section>
<section id="technical-objectives-lightweight-fast-iteration-and-low-boilerplate">
<h2 data-anchor-id="technical-objectives-lightweight-fast-iteration-and-low-boilerplate">Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate</h2>
<p>With gpu.cpp we want to enable a high-leverage library for individual developers and researchers to incorporate GPU computation into programs relying on nothing more than a standard C++ compiler as tooling. Our goals are:</p>
<ul>
<li>High power-to-weight ratio API: Provide the smallest API surface area that can cover the full range of GPU compute needs.</li>
<li>Fast compile/run cycles: Ensure projects can build nearly instantaneously, compile/run cycles should be &lt;5 seconds on a modern laptop.</li>
<li>Minimal dependencies and tooling overhead: A standard clang C++ compiler should be enough, no external library dependencies beyond the WebGPU native implementation.</li>
</ul>
<p>The implementation aims for a small API surface area with minimum boilerplate. There are a small number (about a dozen) of library operations to carry out an broad range of low-level GPU operations. We avoid abstractions that add layers of indirection, making the mapping between the gpu.cpp library to raw WebGPU API clear when it’s needed.</p>
<p>In this spirit of lightweight experimentation, we also want fast iteration - instantaneous C++ builds taking no more than a second or two even on modestly capable personal computing devices. With this in mind, we not only keep the API surface area small, but also keep the implementation small and we also provide a prebuilt binary of the Dawn native WebGPU implementation.</p>
<p>The core library implementation in the header-only <code>gpu.h</code> source code is around 1000 lines of code. In addition to enabling instantaneous, semi-interactive compilation cycles, the small implementation surface area keeps maintenance burden low and the velocity of improvements high. We also pre-build Google’s Dawn WebGPU implementation as a shared library binary. This allows builds to link the shared library with each build and incorporate Google’s powerful native WebGPU implementation without paying the cost of re-compiling Dawn during development cycles. For more advanced users and release deployments, we include <code>cmake</code> examples for building both Dawn with gpu.cpp end-to-end.</p>
</section>
<section id="hello-world-a-gelu-kernel">
<h2 data-anchor-id="hello-world-a-gelu-kernel">Hello World: A GELU Kernel</h2>
<p>As a real-world example for how to use gpu.cpp, let’s start with a practical-but-simple example of a GPU kernel from neural networks.</p>
<p>GELU is a non-linear embarassingly parallel operation often used in modern large language model transformer-based architectures.</p>
<p>It takes as input a vector of floats and applies the GELU function to each element of the vector. The function is nonlinear, attenuating values below zero to near zero, approximating the y = x identity function for large positive values. For values close to zero, GELU smoothly interpolates between the identity function and the zero function.</p>
<p>The GELU code below will illustrate the three main aspects of setting up a GPU computation with gpu.cpp:</p>
<ol type="1">
<li><p>The code that runs on the GPU (in WebGPU Shading Language, or WGSL), implementing the compute opporation.</p></li>
<li><p>The code that runs on the CPU (in C++) that sets up the GPU computation by allocating and preparing resources. For high performance, this code should be run ahead-of-time from the hot paths of the application.</p></li>
<li><p>The code that runs on the CPU (in C++) that dispatches the GPU computation and retrieves the results. The key concern of hot-path dispatch code is to eliminate or minimize any unnecessary resource allocation or data movement (offloading such concerns to step 2). A secondary consideration is that GPU dispatches are asynchronous. We work with standard C++ asynchronous primitives to manage the asynchronous aspect of kernel dispatch.</p></li>
</ol>
<p>Here’s a GELU kernel implemented (based on the CUDA implementation in <a href="https://github.com/karpathy/llm.c">llm.c</a>) as an on-device WGSL shader and invoked from the host using gpu.cpp library functions and types. It can be compiled using a standard C++ compiler (we recommend Clang):</p>
<pre><code>#include &lt;array&gt;
#include &lt;cstdio&gt;
#include &lt;future&gt;

#include &#34;gpu.h&#34;

using namespace gpu; // createContext, createTensor, createKernel,
                     // createShader, dispatchKernel, wait, toCPU
                     // Bindings, Tensor, Kernel, Context, Shape, kf32

static const char *kGelu = R&#34;(
const GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI)
@group(0) @binding(0) var&lt;storage, read_write&gt; inp: array&lt;{{precision}}&gt;;
@group(0) @binding(1) var&lt;storage, read_write&gt; out: array&lt;{{precision}}&gt;;
@compute @workgroup_size({{workgroupSize}})
fn main(
    @builtin(global_invocation_id) GlobalInvocationID: vec3&lt;u32&gt;) {
    let i: u32 = GlobalInvocationID.x;
    if (i &lt; arrayLength(&amp;inp)) {
        let x: f32 = inp[i];
        out[i] = select(0.5 * x * (1.0 + tanh(GELU_SCALING_FACTOR 
                 * (x + .044715 * x * x * x))), x, x &gt; 10.0);
    }
}
)&#34;;

int main(int argc, char **argv) {
  Context ctx = createContext();
  static constexpr size_t N = 10000;
  std::array&lt;float, N&gt; inputArr, outputArr;
  for (int i = 0; i &lt; N; ++i) {
    inputArr[i] = static_cast&lt;float&gt;(i) / 10.0; // dummy input data
  }
  Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data());
  Tensor output = createTensor(ctx, Shape{N}, kf32);
  std::promise&lt;void&gt; promise;
  std::future&lt;void&gt; future = promise.get_future();
  Kernel op = createKernel(ctx, createShader(kGelu, /* 1-D workgroup size */ 256, kf32),
                           Bindings{input, output},
                           /* number of workgroups */ {cdiv(N, 256), 1, 1});
  dispatchKernel(ctx, op, promise);
  wait(ctx, future);
  toCPU(ctx, output, outputArr.data(), sizeof(outputArr));
  for (int i = 0; i &lt; 16; ++i) {
    printf(&#34;  gelu(%.2f) = %.2f\n&#34;, inputArr[i], outputArr[i]);
  }
  return 0;
}</code></pre>
<p>Here we see the GPU code is quoted in a domain specific language called WGSL (WebGPU Shading Language). In a larger project, you might store this code in a separate file to be loaded at runtime (see <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui">examples/shadertui</a> for a demonstration of live WGSL code re-loading).</p>
<p>The CPU code in main() sets up the host coordination for the GPU computation. We can think of the use of gpu.cpp library as a collection of GPU nouns and verbs.</p>
<p>The “nouns” are GPU resources modeled by the type definitions of the library and the “verbs” actions on GPU resources, modeled by the functions of the library. The ahead-of-time resource acquisition functions are prefaced with <code>create*</code>, such as:</p>
<ul>
<li><code>createContext()</code> - constructs a reference to the GPU device context (<code>Context</code>).</li>
<li><code>createTensor()</code> - acquires a contiguous buffer on the GPU (<code>Tensor</code>).</li>
<li><code>createShader()</code> - constructs WGSL code string to run on the GPU) (<code>ShaderCode</code>)</li>
<li><code>createKernel()</code> - constructs a handle to resources for the GPU computation (<code>Kernel</code>), which combines bindings to GPU buffers from <code>createTensor()</code> with the computation definition from <code>createShader()</code>.</li>
</ul>
<p>These resource acquisition functions are tied to resource types for interacting with the GPU:</p>
<ul>
<li><code>Context</code> - a handle to the state of resources for interacting with the GPU device.</li>
<li><code>Tensor</code> - a buffer of data on the GPU.</li>
<li><code>ShaderCode</code> - the code for a shader program that can be dispatched to the GPU. This is a thin wrapper around a WGSL string but also includes the workgroup size the code is designed to run with.</li>
<li><code>Kernel</code> - a GPU program that can be dispatched to the GPU. This accepts a <code>ShaderCode</code> and a list of <code>Tensor</code> resources to bind for the dispatch computation. This takes an argument <code>Bindings</code> that is a list of <code>Tensor</code> instances and should map the bindings declared at the top of the WGSL code. In this example there’s two bindings corresponding to the <code>input</code> buffer on the GPU and the <code>ouptut</code> buffer on the GPU.</li>
</ul>
<p>In this example, the GELU computation is performed only once and the program immediately exits so preparing resources and dispatch are side-by-side. Other examples in the <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/">examples/</a> directory illustrate how resource acquisition is prepared ahead of time and dispatch occurs in the hot path like a render, model inference, or simulation loop.</p>
<p>Besides the <code>create*</code> resource acquisition functions, there are a few more “verbs” in the gpu.cpp library for handling dispatching execution to the GPU and data movement:</p>
<ul>
<li><code>dispatchKernel()</code> - dispatches a <code>Kernel</code> to the GPU for computation. This is an asynchronous operation that returns immediately.</li>
<li><code>wait()</code> - blocks until the GPU computation is complete. This is a standard C++ future/promise pattern.</li>
<li><code>toCPU()</code> - moves data from the GPU to the CPU. This is a synchronous operation that blocks until the data is copied.</li>
<li><code>toGPU()</code> - moves data from the CPU to the GPU. This is a synchronous operation that blocks until the data is copied. In this particular example, <code>toGPU()</code> is not used because there’s only one data movement from CPU to GPU in the program and that happens when the <code>createTensor()</code> function is called.</li>
</ul>
<p>This example is available in <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/hello_world/run.cpp">examples/hello_world/run.cpp</a>.</p>
</section>
<section id="building-clang-is-almost-all-you-need">
<h2 data-anchor-id="building-clang-is-almost-all-you-need">Building: Clang is (Almost) All You Need</h2>
<p>To build a gpu.cpp project, you will need to have installed on your system:</p>
<ul>
<li><code>clang++</code> compiler installed with support for C++17.</li>
<li><code>python3</code> and above, to run the script which downloads the Dawn shared library. make to build the project.</li>
<li><code>make</code> to build the project.</li>
<li>Only on Linux systems - Vulkan drivers. If Vulkan is not installed, you can run <code>sudo apt install libvulkan1 mesa-vulkan-drivers vulkan-tools</code> to install them.</li>
</ul>
<p>The only library dependency of gpu.cpp is a WebGPU implementation. Currently we support the Dawn native backend, but we plan to support other targets and WebGPU implementations (web browsers or other native implementations such as wgpu). Currently we support MacOS, Linux, and Windows (via WSL).</p>
<p>Optionally, Dawn can be built from scratch with gpu.cpp using the cmake build scripts provided - see the -cmake targets in the Makefile. However, this is recommended for advanced users only. Building Dawn dependencies with cmake takes much longer than using the precompiled Dawn shared library.</p>
<p>After cloning the repo, from the top-level gpu.cpp, you should be able to build and run the hello world GELU example by typing:</p>
<pre><code>make</code></pre>
<p>The first time you build and run the project this way, it will download a prebuilt shared library for the Dawn native WebGPU implementation automatically (using the setup.py script). This places the Dawn shared library in the third_party/lib directory. Afterwards you should see <code>libdawn.dylib</code> on MacOS or <code>libdawn.so</code> on Linux. This download only occurs once.</p>
<p>The build process itself should take a few seconds. If the build and executions is successful, you should see the output of the GELU computation:</p>
<pre><code>Hello gpu.cpp!
--------------

  gelu(0.00) = 0.00
  gelu(0.10) = 0.05
  gelu(0.20) = 0.12
  gelu(0.30) = 0.19
  gelu(0.40) = 0.26
  gelu(0.50) = 0.35
  gelu(0.60) = 0.44
  gelu(0.70) = 0.53
  gelu(0.80) = 0.63
  gelu(0.90) = 0.73
  gelu(1.00) = 0.84
  gelu(1.10) = 0.95
  gelu(1.20) = 1.06
  gelu(1.30) = 1.17
  gelu(1.40) = 1.29
  gelu(1.50) = 1.40
  ...

Computed 10000 values of GELU(x)</code></pre>
</section>
<section id="other-examples-matrix-multiplication-physics-sim-and-sdf-rendering">
<h2 data-anchor-id="other-examples-matrix-multiplication-physics-sim-and-sdf-rendering">Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering</h2>
<p>You can explore the example projects in <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/">examples/</a> which illustrate how to use gpu.cpp as a library.</p>
<p>Assuming you’ve already run <code>make</code> in the top-level directory which retrieves the prebuilt Dawn shared library, you can run each example by navigating to its directory and running <code>make</code> from the example’s directory.</p>
<p>An example of tiled matrix multiplication is shown in <a href="https://github.com/AnswerDotAI/gpu.cpp/blob/main/examples/matmul/">examples/matmul</a>. This implements a WebGPU version of the first few kernels of Simon Boehm’s now-famous <a href="https://siboehm.com/articles/22/CUDA-MMM">How to Opitmize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</a> post. It is only weakly optimized (up to 1D blocktiling, kernel number 4), but nonetheless already achieves an estimated ~ 1.2+ TFLOPs on a Macbook Pro M1 laptop, which has a theoretical peak of 10.4 TFLOPs. Contributions to optimize this further are welcome - kernels 5-9 of Simon’s post would be a natural starting point.</p>
<div>
<figure>
<p><img src="https://www.answer.ai/posts/images-2024-07-11--gpu-cpp/matmul.png"/></p>
</figure>
</div>
<p>A parallel physics simulation of N double pendulums running simultaneously on the GPU is shown in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/physics">examples/physics</a>.</p>
<div>
<figure>
<p><img src="https://www.answer.ai/posts/images-2024-07-11--gpu-cpp/pendulum.gif"/></p>
</figure>
</div>
<p>We also show some examples of signed distance function computations, rendered in the terminal as ascii. A 3D SDF of spheres is shown in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/render%5D">examples/render</a> and a shadertoy-like live-reloading example is in <a href="https://github.com/AnswerDotAI/gpu.cpp/tree/main/examples/shadertui">examples/shadertui</a>. Interestingly, with a starting example, LLMs such as Claude 3.5 Sonnet can be quite capable at writing low-level WGSL code for you - the other shaders in the shadertui example are written by the LLM.</p>
<div>
<figure>
<p><img src="https://www.answer.ai/posts/images-2024-07-11--gpu-cpp/shadertui.gif"/></p>
</figure>
</div>
</section>
<section id="what-gpu.cpp-is-not">
<h2 data-anchor-id="what-gpu.cpp-is-not">What gpu.cpp is Not</h2>
<p>gpu.cpp is meant for developers with some familiarity with C++ and GPU programming. It is not a high-level numerical computing or machine learning framework or inference engine, though it can be used in support of such implementations.</p>
<p>Second, in spite of the name, WebGPU has native implementations decoupled from the web and the browser. gpu.cpp leverages WebGPU as a portable <em>native</em> GPU API first and foremost, with the possibility of running in the browser being being a convenient additional benefit in the future.</p>
<p>If you find it counerintuitive, as many do, that WebGPU is a native technology and not just for the web, watch Elie Michel’s excellent talk <a href="https://www.youtube.com/watch?v=qHrx41aOTUQ">“WebGPU is Not Just About the Web”</a>.</p>
<p>Finally, the focus of gpu.cpp is general-purpose GPU computation rather than rendering/graphics on the GPU, although it can be useful for offline rendering or video processing use cases. We may explore directions with graphics in the future, but for now our focus is GPU compute.</p>
</section>
<section id="limitations-and-current-work">
<h2 data-anchor-id="limitations-and-current-work">Limitations and Current Work</h2>
<p><em>API Improvements</em> - gpu.cpp is a work-in-progress and there are many features and improvements to come. At this early stage, we expect the API design to evolve as we identify improvements / needs from use cases. In particular, the handling of structured parameters and asynchronous dispatch will undergo refinement and maturation in the short-term.</p>
<p><em>Browser Targets</em> - In spite of using WebGPU we haven’t tested builds targeting the browser yet though this is a short-term priority.</p>
<p><em>Reusable Kernels and Shader Library</em> - Currently the core library is strictly the operations and types for interfacing with the WebGPU API, with some specific use case example WGSL implementations in <code>examples/</code>. Over time, as kernel implementations mature we may migrate some of the reusable operations from specific examples into a small reusable kernel library.</p>
<p><em>More Use Case Examples and Tests</em> - Expect an iteration loop of use cases to design tweaks and improvements, which in turn make the use cases cleaner and easier to write. One short term use cases to flesh out the kernels from <a href="https://github.com/karpathy/llm.c">llm.c</a> in WebGPU form. As these mature into a reusable kernel library, we hope to help realize the potential for WebGPU compute in AI.</p>
</section>
<section id="closing-thoughts-and-getting-involved">
<h2 data-anchor-id="closing-thoughts-and-getting-involved">Closing Thoughts and Getting Involved</h2>
<p>We welcome collaborators - if you’re interested in getting involved, feedback and pull requests are welcome at <a href="https://github.com/AnswerDotAI/gpu.cpp">github.com/AnswerDotAI/gpu.cpp</a>. You can also join our discord to chat at <a href="https://discord.gg/Q9PWDckbnR">#gpu-cpp</a>.</p>
<p>GPUs are arguably the most empowering technology in the world today. However, it can feel like the joy of creating and exploring ideas through GPU code has been somewhat elusive. Somewhere between having to write a 1000 lines of boilerplate to dispatch a small compute kernel and operationalizing a new algorithm where 99% of the effort is squeezing a small implementation into a massive tower of machine learning technology stacks, the tools around the technology get in the way so much that GPUs on personal devices are often underutilized.</p>
<p>gpu.cpp is a shared experiment to make low-level on-device GPU computation more productive and fun while enabling you to run GPU code anywhere and everywhere.</p>


</section>

</main> <!-- /main -->

</div></div>
  </body>
</html>
