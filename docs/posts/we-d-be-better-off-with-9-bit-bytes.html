<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pavpanchekha.com/blog/9bit.html">Original</a>
    <h1>We&#39;d be better off with 9-bit bytes</h1>
    
    <div id="readability-page-1" class="page"><div id="content">

<p>
A number of 70s computing systems had nine-bit bytes, most prominently
the PDP-10, but today<sup><a id="fnr.1" href="#fn.1" role="doc-backlink">1</a></sup><span id="fn.1"><sup>1</sup> Apparently, it was the System/360 that
really set the standard here.</span> all systems use 8-bit bytes and that
now seems natural.<sup><a id="fnr.2" href="#fn.2" role="doc-backlink">2</a></sup><span id="fn.2"><sup>2</sup> Though you still see RFCs use &#34;octet&#34;, and the
C standard has a <code>CHAR_BITS</code> macro, to handle the possibility of a
different-sized byte.</span> As a power of two, eight is definitely nicer.
But I think a series of historical coincidences would actually go our
way with 9-bit bytes.
</p>

<p>
<b>IPv4</b>: Everyone knows the story: IPv4 had 32-bit addresses, so about 4
billion total.<sup><a id="fnr.3" href="#fn.3" role="doc-backlink">3</a></sup><span id="fn.3"><sup>3</sup> Less due to various reserved subnets.</span> That&#39;s not
enough in a world with 8 billion humans, and that&#39;s lead to NATs, more
active network middleware, and the impossibly glacial pace of IPv6
roll-out. It&#39;s 2025 and Github—Github!—doesn&#39;t support IPv6. But in a
world with 9-bit bytes IPv4 would have had 36-bit addresses, about 64
billion total. That would still be enough right now, and even with
continuing growth in India and Africa it would probably be enough for
about a decade more.<sup><a id="fnr.4" href="#fn.4" role="doc-backlink">4</a></sup><span id="fn.4"><sup>4</sup> In our timeline, exhaustion hit in 2011, when
demand was doubling every five years. 16× more addresses gets us to
2031 projecting linearly, and probably a little later with growth
slowing.</span> When exhaustion does set in, it would plausibly at a time
where there&#39;s not a lot of growth left in penetration, population, or
devices, and mild market mechanisms instead of NATs would be the
solution.
</p>

<p>
<b>UNIX time</b>: In our timeline, 32-bit UNIX timestamps run out in 2038, so
again all software has to painfully transition to larger, 64-bit
structures. Equivalent 36-bit timestamps last until 3058, so no hurry.
Negative timestamps would represent any time since 882, so could cover
the founding of Kievan Rus&#39;, the death of Alfred the Great, the
collapse of the Classic Maya,<sup><a id="fnr.5" href="#fn.5" role="doc-backlink">5</a></sup><span id="fn.5"><sup>5</sup> The people stuck around, but they
stopped building cool cities.</span> and the movement of Magyar tribes into
the Carpathian basin.
</p>

<p>
<b>Unicode</b>: In our universe, there are 65 thousand 16-bit characters,
which looked like <i>maybe</i> enough for all the world&#39;s languages, assuming
you&#39;re really careful about which Chinese characters you let
in.<sup><a id="fnr.6" href="#fn.6" role="doc-backlink">6</a></sup><span id="fn.6"><sup>6</sup> Known as CJK unification, a real design flaw in Unicode that
we&#39;re stuck with.</span> With 9-bit bytes we&#39;d have 262 thousand 18-bit
characters instead, which would totally be enough—there are only 155
thousand Unicode characters today, and that&#39;s with all the cat smileys
and emojis we can dream of. UTF-9 would be thought of more as a
compression format and largely sidelined by GZip.<sup><a id="fnr.7" href="#fn.7" role="doc-backlink">7</a></sup><span id="fn.7"><sup>7</sup> Alternatively,
we could lose a bit, be sparing with the cat smileys, and UTF-9 could
be one/two byte like Shift-JIS. That would be pretty attractive.</span>
</p>

<p>
<b>Pointers</b>: In 8-bit byte land, 32-bit operating systems impose a 2 GB
cap on processes,<sup><a id="fnr.8" href="#fn.8" role="doc-backlink">8</a></sup><span id="fn.8"><sup>8</sup> Because the kernel needs the top half of the
memory space.</span> which turns out to be pretty restrictive. 36-bit
operating systems would allow up to 32 GB per process, which even
today would be a big machine; I&#39;m writing this on a four-year-old
Macbook Pro and it only has 16 GB of RAM. Server-class machines would
still need to address more memory than that, but they&#39;re usually
running specialized software or virtualizing; databases and
hypervisors are already tricky code and segmentation wouldn&#39;t be the
end of the world.<sup><a id="fnr.9" href="#fn.9" role="doc-backlink">9</a></sup><span id="fn.9"><sup>9</sup> Basically, it&#39;d be <a href="https://en.wikipedia.org/wiki/X32_ABI">x32 as the standard</a>.</span> Memory
usage, even measured in bits, would be lower thanks to smaller
pointers<sup><a id="fnr.10" href="#fn.10" role="doc-backlink">10</a></sup><span id="fn.10"><sup>10</sup> So maybe 5% faster per x32 benchmarks?</span> though strings
would be bigger.<sup><a id="fnr.11" href="#fn.11" role="doc-backlink">11</a></sup><span id="fn.11"><sup>11</sup> So overall, maybe a wash?</span>
</p>

<p>
There are more obscure wins too. 16-bit AS numbers ran out years ago;
18-bit numbers would still be enough. 18-bit ports and process IDs and
user IDs would be a bit roomier. X86 and A64 instruction encodings
would be a bit saner.<sup><a id="fnr.12" href="#fn.12" role="doc-backlink">12</a></sup><span id="fn.12"><sup>12</sup> Thumb would work better?</span> Half-precision
(18-bit) floats might be prominent earlier.<sup><a id="fnr.13" href="#fn.13" role="doc-backlink">13</a></sup><span id="fn.13"><sup>13</sup> Today&#39;s manic 4- and
5-bit floats wouldn&#39;t work, and 3-bit floats seem impossible. Maybe
6-bit floats, 6-in-4, would be the consensus OCP float.</span> Extended
ASCII would have room for Greek and would become a kind of NATO code
page.<sup><a id="fnr.14" href="#fn.14" role="doc-backlink">14</a></sup><span id="fn.14"><sup>14</sup> And UTF-9 would privilege most of Western Europe, not just
the US.</span> Unix permissions would be one byte, so would lack sticky
bits. Octal, not hex, would be standard.<sup><a id="fnr.15" href="#fn.15" role="doc-backlink">15</a></sup><span id="fn.15"><sup>15</sup> It all comes from the
PDP-10!</span> Probably there are other benefits too.<sup><a id="fnr.16" href="#fn.16" role="doc-backlink">16</a></sup><span id="fn.16"><sup>16</sup> I measured ΔE for
18-bit color, which nicely splits 6/6/6. ChatGPT says the numbers I&#39;m
getting are imperceptible, but I don&#39;t really know, and losing an
alpha channel would hurt.</span>
</p>

<p>
Would there be costs? No system has bit addressing; if a byte isn&#39;t a
power of two it doesn&#39;t actually matter.<sup><a id="fnr.17" href="#fn.17" role="doc-backlink">17</a></sup><span id="fn.17"><sup>17</sup> No CPU would be dividing
by nine or anything like that.</span> Page sizes and block sizes probably
wouldn&#39;t change, the kernel wouldn&#39;t be doing anything different from
now.<sup><a id="fnr.18" href="#fn.18" role="doc-backlink">18</a></sup><span id="fn.18"><sup>18</sup> Though kernels would need to support some kind of 54-bit
segment selector plus pointer memory mapping.</span> Would something else
exhaust in ugly ways because it would look like it might fit? A bunch
of single-system stuff, probably; one-byte UID/GID might be tempting,
or two-byte inode numbers, but these happened in our universe and
didn&#39;t cause painful transitions.
</p>

<p>
The scariest I&#39;ve come up with<sup><a id="fnr.19" href="#fn.19" role="doc-backlink">19</a></sup><span id="fn.19"><sup>19</sup> ChatGPT o4 came up with it.</span> is TCP
sequence numbers, where 18-bit sequence numbers might look appealing
but would cause real problems for high-bandwidth connections. You&#39;d
need window scaling by the early 90s and a bump to 36-bit sequence
numbers by the mid 90s, culminating in an IPv6-like TCPv2 effort. Or
maybe instead of IPv6&#39;s &#34;skinny upgrade&#34; strategy TCPv2 would
incorporate networking concerns of the era; maybe ECN would be on by
default. But it&#39;s still not as bad as IPv6: ISPs would need to support
TCPv2 to offer higher speeds, which was the main way ISPs competed.
They&#39;d make the investment. And since it all happens in the mid-90s,
HTTP might end up requiring TCPv2. We wouldn&#39;t dual-stack.
</p>

<p>
<i>Update</i>: This post hit the <a href="https://news.ycombinator.com/item?id=44816692">HN front page</a>. Mostly the comments were
&#34;like always&#34; as we say,<sup><a id="fnr.20" href="#fn.20" role="doc-backlink">20</a></sup><span id="fn.20"><sup>20</sup> As usual, reading the post and looking up
the author&#39;s identity instead of guessing gets you a long way.</span> but I
wanted to highlight <a href="https://news.ycombinator.com/item?id=44819306">JdeBP&#39;s wonderful comment</a> sketching more of this
9-bit alternate history. Do read it. My gestalt impression is that
this alternate world sounds pretty good! Fewer annoying limits, lame
protocol extensions, US-specificity, and so on. So much of the early
computing era was shaped by numerological limits.
</p>

<p>
<i>Thank you to GPT 4o and o4 for discussions, research, and drafting.</i>
</p>
</div></div>
  </body>
</html>
