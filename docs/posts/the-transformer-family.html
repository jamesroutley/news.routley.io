<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">Original</a>
    <h1>The Transformer Family</h1>
    
    <div id="readability-page-1" class="page"><div><p>Many new Transformer architecture improvements have been proposed since my last post on <a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/"><ins>“The Transformer Family”</ins></a> about three years ago. Here I did a big refactoring and enrichment of that 2020 post — restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.</p>
<h2 id="notations">Notations</h2>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>$d$</td>
<td>The model size / hidden state dimension / positional encoding size.</td>
</tr>
<tr>
<td>$h$</td>
<td>The number of heads in multi-head attention layer.</td>
</tr>
<tr>
<td>$L$</td>
<td>The segment length of input sequence.</td>
</tr>
<tr>
<td>$N$</td>
<td>The total number of attention layers in the model; not considering MoE.</td>
</tr>
<tr>
<td>$\mathbf{X} \in \mathbb{R}^{L \times d}$</td>
<td>The input sequence where each element has been mapped into an embedding vector of shape $d$, same as the model size.</td>
</tr>
<tr>
<td>$\mathbf{W}^k \in \mathbb{R}^{d \times d_k}$</td>
<td>The key weight matrix.</td>
</tr>
<tr>
<td>$\mathbf{W}^q \in \mathbb{R}^{d \times d_k}$</td>
<td>The query weight matrix.</td>
</tr>
<tr>
<td>$\mathbf{W}^v \in \mathbb{R}^{d \times d_v}$</td>
<td>The value weight matrix. Often we have $d_k = d_v = d$.</td>
</tr>
<tr>
<td>$\mathbf{W}^k_i, \mathbf{W}^q_i \in \mathbb{R}^{d \times d_k/h}; \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$</td>
<td>The weight matrices per head.</td>
</tr>
<tr>
<td>$\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$</td>
<td>The output weight matrix.</td>
</tr>
<tr>
<td>$\mathbf{Q} = \mathbf{X}\mathbf{W}^q \in \mathbb{R}^{L \times d_k}$</td>
<td>The query embedding inputs.</td>
</tr>
<tr>
<td>$\mathbf{K} = \mathbf{X}\mathbf{W}^k \in \mathbb{R}^{L \times d_k}$</td>
<td>The key embedding inputs.</td>
</tr>
<tr>
<td>$\mathbf{V} = \mathbf{X}\mathbf{W}^v \in \mathbb{R}^{L \times d_v}$</td>
<td>The value embedding inputs.</td>
</tr>
<tr>
<td>$\mathbf{q}_i, \mathbf{k}_i \in \mathbb{R}^{d_k}, \mathbf{v}_i \in \mathbb{R}^{d_v}$</td>
<td>Row vectors in query, key, value matrices, $\mathbf{Q}$, $\mathbf{K}$ and $\mathbf{V}$.</td>
</tr>
<tr>
<td>$S_i$</td>
<td>A collection of key positions for the $i$-th query $\mathbf{q}_i$ to attend to.</td>
</tr>
<tr>
<td>$\mathbf{A} \in \mathbb{R}^{L \times L}$</td>
<td>The self-attention matrix between a input sequence of lenght $L$ and itself. $\mathbf{A} = \text{softmax}(\mathbf{Q}\mathbf{K}^\top / \sqrt{d_k})$.</td>
</tr>
<tr>
<td>$a_{ij} \in \mathbf{A}$</td>
<td>The scalar attention score between query $\mathbf{q}_i$ and key $\mathbf{k}_j$.</td>
</tr>
<tr>
<td>$\mathbf{P} \in \mathbb{R}^{L \times d}$</td>
<td>position encoding matrix, where the $i$-th row $\mathbf{p}_i$ is the positional encoding for input $\mathbf{x}_i$.</td>
</tr>
</tbody>
</table>

<p>The <strong>Transformer</strong> (which will be referred to as “vanilla Transformer” to distinguish it from other enhanced versions; <a href="https://arxiv.org/abs/1706.03762">Vaswani, et al., 2017</a>) model has an encoder-decoder architecture, as commonly used in many <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#born-for-translation">NMT</a> models. Later decoder-only Transformer was shown to achieve great performance in language modeling tasks, like in <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#openai-gpt">GPT</a> and <a href="https://lilianweng.github.io/posts/2019-01-31-lm/#bert">BERT</a>.</p>
<h2 id="attention-and-self-attention">Attention and Self-Attention</h2>
<p><strong>Attention</strong> is a mechanism in neural network that a model can learn to make predictions by selectively attending to a given set of data. The amount of attention is quantified by learned weights and thus the output is usually formed as a weighted average.</p>
<p><strong>Self-attention</strong> is a type of attention mechanism where the model makes prediction for one part of a data sample using other parts of the observation about the same sample. Conceptually, it feels quite similar to <a href="https://en.wikipedia.org/wiki/Non-local_means">non-local means</a>. Also note that self-attention is permutation-invariant; in other words, it is an operation on sets.</p>
<p>There are various forms of attention / self-attention, Transformer (<a href="https://arxiv.org/abs/1706.03762">Vaswani et al., 2017</a>) relies on the <em>scaled dot-product attention</em>: given a query matrix $\mathbf{Q}$, a key matrix $\mathbf{K}$ and a value matrix $\mathbf{V}$, the output is a weighted sum of the value vectors, where the weight assigned to each value slot is determined by the dot-product of the query with the corresponding key:</p>
<p>
$$
\text{attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q} {\mathbf{K}}^\top}{\sqrt{d_k}})\mathbf{V}
$$
</p>
<p>And for a query and a key vector $\mathbf{q}_i, \mathbf{k}_j \in \mathbb{R}^d$ (row vectors in query and key matrices), we have a scalar score:</p>
<p>
$$
a_{ij} = \text{softmax}(\frac{\mathbf{q}_i {\mathbf{k}_j}^\top}{\sqrt{d_k}})
= \frac{\exp(\mathbf{q}_i {\mathbf{k}_j}^\top)}{ \sqrt{d_k} \sum_{r \in \mathcal{S}_i} \exp(\mathbf{q}_i {\mathbf{k}_r}^\top) }
$$ 
</p>
<p>where $\mathcal{S}_i$ is a collection of key positions for the $i$-th query to attend to.</p>
<p>See my old <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#a-family-of-attention-mechanisms">post for other types of attention</a> if interested.</p>
<h2 id="multi-head-self-attention">Multi-Head Self-Attention</h2>
<p>The <strong>multi-head self-attention</strong> module is a key component in Transformer. Rather than only computing the attention once, the multi-head mechanism splits the inputs into smaller chunks and then computes the scaled dot-product attention over each subspace in parallel. The independent attention outputs are simply concatenated and linearly transformed into expected dimensions.</p>
<p>
$$
\begin{aligned}
\text{MultiHeadAttn}(\mathbf{X}_q, \mathbf{X}_k, \mathbf{X}_v) &amp;= [\text{head}_1; \dots; \text{head}_h] \mathbf{W}^o \\ 
\text{where head}_i &amp;= \text{Attention}(\mathbf{X}_q\mathbf{W}^q_i, \mathbf{X}_k\mathbf{W}^k_i, \mathbf{X}_v\mathbf{W}^v_i)
\end{aligned}
$$
</p>
<p>where $[.;.]$ is a concatenation operation. $\mathbf{W}^q_i, \mathbf{W}^k_i \in \mathbb{R}^{d \times d_k/h}, \mathbf{W}^v_i \in \mathbb{R}^{d \times d_v/h}$ are weight matrices to map input embeddings of size $L \times d$ into query, key and value matrices. And $\mathbf{W}^o \in \mathbb{R}^{d_v \times d}$ is the output linear transformation. All the weights should be learned during training.</p>
<p><img src="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/multi-head-attention.png"/></p><figcaption>Fig. 1. Illustration of the multi-head scaled dot-product attention mechanism. (Image source: Figure 2 in <a href="https://arxiv.org/abs/1706.03762" target="_blank">Vaswani, et al., 2017</a>)</figcaption>
<h2 id="encoder-decoder-architecture">Encoder-Decoder Architecture</h2>
<p>The <strong>encoder</strong> generates an attention-based representation with capability to locate a specific piece of information from a large context. It consists of a stack of 6 identity modules, each containing two submodules, a <em>multi-head self-attention</em> layer and a <em>point-wise</em> fully connected feed-forward network. By point-wise, it means that it applies the same linear transformation (with same weights) to each element in the sequence. This can also be viewed as a convolutional layer with filter size 1. Each submodule has a residual connection and layer normalization. All the submodules output data of the same dimension $d$.</p>
<p>The function of Transformer <strong>decoder</strong> is to retrieve information from the encoded representation. The architecture is quite similar to the encoder, except that the decoder contains two multi-head attention submodules instead of one in each identical repeating module. The first multi-head attention submodule is <em>masked</em> to prevent positions from attending to the future.</p>
<p><img src="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/transformer.png"/></p><figcaption>Fig. 2. The architecture of the vanilla Transformer model. (Image source: <a href="https://lilianweng.github.io/posts/2018-06-24-attention/#full-architecture" target="_blank">Figure 17</a>)</figcaption>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Because self-attention operation is permutation invariant, it is important to use proper <strong>positional encoding</strong> to provide <em>order information</em> to the model. The positional encoding $\mathbf{P} \in \mathbb{R}^{L \times d}$ has the same dimension as the input embedding, so it can be added on the input directly. The vanilla Transformer considered two types of encodings:</p>
<h3 id="sinusoidal-positional-encoding">Sinusoidal Positional Encoding</h3>
<p>Sinusoidal positional encoding is defined as follows, given the token position $i=1,\dots,L$ and the dimension $\delta=1,\dots,d$:</p>
<p>
$$
\text{PE}(i,\delta) = 
\begin{cases}
\sin(\frac{i}{10000^{2\delta&#39;/d}}) &amp; \text{if } \delta = 2\delta&#39;\\
\cos(\frac{i}{10000^{2\delta&#39;/d}}) &amp; \text{if } \delta = 2\delta&#39; + 1\\
\end{cases}
$$
</p>
<p>In this way each dimension of the positional encoding corresponds to a sinusoid of different wavelengths in different dimensions, from $2\pi$ to $10000 \cdot 2\pi$.</p>
<p><img src="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/sinoidual-positional-encoding.png"/></p><figcaption>Fig. 3. Sinusoidal positional encoding with $L=32$ and $d=128$. The value is between -1 (black) and 1 (white) and the value 0 is in gray.</figcaption>
<h3 id="learned-positional-encoding">Learned Positional Encoding</h3>
<p>Learned positional encoding assigns each element with a <em>learned</em> column vector which encodes its absolute position (<a href="https://arxiv.org/abs/1705.03122">Gehring, et al. 2017</a>) and furthermroe this encoding can be learned differently per layer (<a href="https://arxiv.org/abs/1808.04444">Al-Rfou et al. 2018</a>).</p>
<h3 id="relative-position-encoding">Relative Position Encoding</h3>
<p><a href="https://arxiv.org/abs/1803.02155">Shaw et al. (2018)</a>) incorporated relative positional information into $\mathbf{W}^k$ and $\mathbf{W}^v$. Maximum relative position is clipped to a maximum absolute value of $k$ and this clipping operation enables the model to generalize to unseen sequence lengths. Therefore, $2k + 1$ unique edge labels are considered and let us denote $\mathbf{P}^k, \mathbf{P}^v \in \mathbb{R}^{2k+1}$ as learnable relative position representations.</p>
<p>
$$
A_{ij}^k = P^k_{\text{clip}(j - i, k)} \quad
A_{ij}^v = P^v_{\text{clip}(j - i, k)} \quad
\text{where }\text{clip}(x, k) = \text{clip}(x, -k, k)
$$
</p><!-- If we omit the scalar in self-attention and summarize the denominator into a normalizing term $Z(.)$, an normal attention output looks as follows:


<div>
$$
\mathbf{o}_i = \sum_{j \in S_i} \exp(\mathbf{q}_i \cdot \mathbf{k}_j - Z(i, S_i)) \mathbf{v}_j \text{, where } S_i = \{j: j \leq i\}
$$ 
</div>

-->

































































































  </div></div>
  </body>
</html>
