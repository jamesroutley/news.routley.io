<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.docker.com/blog/introducing-docker-model-runner/">Original</a>
    <h1>Docker Model Runner</h1>
    
    <div id="readability-page-1" class="page"><div>
				
				
				
				
				<div>
			<div>
		
<p>Generative AI is transforming software development, but building and running AI models locally is still harder than it should be. Today’s developers face fragmented tooling, hardware compatibility headaches, and disconnected application development workflows, all of which hinder iteration and slow down progress.  </p>



<p>That’s why we’re launching Docker Model Runner — a faster, simpler way to <a href="https://www.docker.com/blog/run-llms-locally/">run and test AI models locally</a>, right from your existing workflow. Whether you’re experimenting with the latest LLMs or deploying to production, Model Runner brings the performance and control you need, without the friction.</p>



<p>We’re also teaming up with some of the most influential names in AI and software development, including <a href="https://ai.google.dev/gemma/docs/core" target="_blank" rel="noreferrer noopener nofollow">Google</a>, <a href="https://www.continue.dev/" target="_blank" rel="noreferrer noopener nofollow">Continue</a>, <a href="https://dagger.io/" target="_blank" rel="noreferrer noopener nofollow">Dagger</a>, <a href="https://www.qualcomm.com/" target="_blank" rel="noreferrer noopener nofollow">Qualcomm Technologies</a>, <a href="https://huggingface.co/" target="_blank" rel="noreferrer noopener nofollow">HuggingFace</a>, <a href="https://spring.io/projects/spring-ai" target="_blank" rel="noreferrer noopener nofollow">Spring AI</a>, and <a href="https://www.vmware.com/solutions/app-platform/ai" target="_blank" rel="noreferrer noopener nofollow">VMware Tanzu AI Solutions</a>, to give developers direct access to the latest models, frameworks, and tools. These partnerships aren’t just integrations, they’re a shared commitment to making AI innovation more accessible, powerful, and developer-friendly. With Docker Model Runner, you can tap into the best of the AI ecosystem from right inside your Docker workflow.</p>



<h2>LLM development is evolving: We’re making it local-first </h2>



<p>Local development for applications powered by LLMs is gaining momentum, and for good reason. It offers several advantages on key dimensions such as performance, cost, and data privacy. But today, local setup is complex.  </p>



<p>Developers are often forced to manually integrate multiple tools, configure environments, and manage models separately from container workflows. Running a model varies by platform and depends on available hardware. Model storage is fragmented because there is no standard way to store, share, or serve models. </p>



<p>The result? Rising cloud inference costs and a disjoined developer experience. With our first release, we’re focused on reducing that friction, making local model execution simpler, faster, and easier to fit into the way developers already build.</p>



<div><div>
				
				
				
				
				
				
				<div>
				<div>
				
				
				
				
				<div>
				
				
				
				
				<p><iframe title="Introducing Docker Model Runner" width="1080" height="608" src="https://www.youtube.com/embed/zQi-8mCTNf8?feature=oembed" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
				
			</div>
			</div>
				
				
				
				
			</div>
				
				
			</div></div>



<h2>Docker Model Runner: The simple, secure way to run AI models locally</h2>



<p>Docker Model Runner is designed to make AI model execution as simple as running a container. With this Beta release, we’re giving developers a fast, low-friction way to run models, test them, and iterate on application code that uses models locally, without all the usual setup headaches. Here’s how:</p>



<h3>Running models locally </h3>



<p>With Docker Model Runner, running AI models locally is now as simple as running any other service in your inner loop. Docker Model Runner delivers this by including an inference engine as part of Docker Desktop, built on top of llama.cpp and accessible through the familiar OpenAI API. No extra tools, no extra setup, and no disconnected workflows. Everything stays in one place, so you can test and iterate quickly, right on your machine.</p>



<h3>Enabling GPU acceleration (Apple silicon)</h3>



<p>GPU acceleration on Apple silicon helps developers get fast inference and the most out of their local hardware. By using host-based execution, we avoid the performance limitations of running models inside virtual machines. This translates to faster inference, smoother testing, and better feedback loops.</p>



<h3>Standardizing model packaging with OCI Artifacts</h3>



<p>Model distribution today is messy. Models are often shared as loose files or behind proprietary download tools with custom authentication. With Docker Model Runner, we package models as <strong>OCI Artifacts</strong>, an open standard that allows you to distribute and version them through the same registries and workflows you already use for containers. Today, you can easily pull ready-to-use models from Docker Hub. Soon, you’ll also be able to push your own models, integrate with any container registry, connect them to your CI/CD pipelines, and use familiar tools for access control and automation.</p>



<h2>Building momentum with a thriving GenAI ecosystem</h2>



<p>To make local development seamless, it needs an ecosystem. That starts with meeting developers where they are, whether they’re testing model performance on their local machines or building applications that run these models. </p>



<p>That’s why we’re launching Docker Model Runner with a powerful ecosystem of partners on both sides of the AI application development process. On the model side, we’re collaborating with industry leaders like Google and community platforms like HuggingFace to bring you high-quality, optimized models ready for local use. These models are published as OCI artifacts, so you can pull and run them using standard Docker commands, just like any container image.</p>



<p>But we aren’t stopping at models. We’re also working with application, language, and tooling partners like Dagger, Continue, and Spring AI and VMware Tanzu to ensure applications built with Model Runner integrate seamlessly into real-world developer workflows. Additionally, we’re working with hardware partners like Qualcomm Technologies to ensure high performance inference on all platforms.</p>



<p>As Docker Model Runner evolves, we’ll work to expand its ecosystem of partners, allowing for ample distribution and added functionality.</p>



<h2>Where We’re Going</h2>



<p>This is just the beginning. With Docker Model Runner, we’re making it easier for developers to bring AI model execution into everyday workflows, securely, locally, and with a low barrier of entry. Soon, you’ll be able to run models on more platforms, including Windows with GPU acceleration, customize and publish your own models, and integrate AI into your dev loop with even greater flexibility (including Compose and Testcontainers). With each Docker Desktop release, we’ll continue to unlock new capabilities that make GenAI development easier, faster, and way more fun to build with.</p>



<h3>Try it out now! </h3>



<p>Docker Model Runner is now available as a Beta feature in <a href="https://www.docker.com/blog/docker-desktop-4-40/">Docker Desktop 4.40</a>. To get started:</p>



<ol>
<li>On a Mac with Apple silicon</li>



<li>Update to Docker Desktop 4.40</li>



<li>Pull models developed by our partners at Docker’s <a href="https://hub.docker.com/catalogs/gen-ai" target="_blank" rel="noreferrer noopener nofollow">GenAI Hub</a> and start experimenting</li>



<li>For more information, check out our documentation <a href="https://docs.docker.com/desktop/features/model-runner/" rel="nofollow noopener" target="_blank">here</a>.</li>
</ol>



<p>Try it out and let us know what you think!</p>



<h3>How can I learn more about Docker Model Runner?</h3>



<p>Check out our available assets today! </p>



<p><a href="https://www.youtube.com/watch?v=rGGZJT3ZCvo" rel="nofollow noopener" target="_blank">Turn your Mac into an AI playground YouTube tutorial<br/></a><a href="https://www.docker.com/blog/run-llms-locally/">A Quickstart Guide to Docker Model Runner <br/></a><a href="https://docs.docker.com/desktop/features/model-runner/" rel="nofollow noopener" target="_blank">Docker Model Runner on Docker Docs </a></p>



<p>Come meet us at Google Cloud Next! </p>



<p>Swing by booth 1530 in the Mandalay Convention Center for hands-on demos and exclusive content.</p>

		</div>
	</div>
	
			</div></div>
  </body>
</html>
