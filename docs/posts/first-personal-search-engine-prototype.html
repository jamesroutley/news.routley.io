<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rsdoiel.github.io/blog/2023/03/10/first-prototype-pse.html">Original</a>
    <h1>First personal search engine prototype</h1>
    
    <div id="readability-page-1" class="page"><section>
  <article>
  <a data-pocket-label="pocket" data-pocket-count="none" data-lang="en" alt="save article to Pocket"></a>


<p>By R. S. Doiel, 2023-08-10</p>
<p>I’ve implemented a first prototype of my personal search engine which
I will abbreviate as “pse” from here on out. I implemented it using
three <a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)">Bash</a>
scripts relying on <a href="https://sqlite.org">sqlite3</a>, <a href="https://en.wikipedia.org/wiki/Wget">wget</a> and <a href="https://pagefind.app">PageFind</a> to do the heavy lifting.</p>
<p>Both Firefox and newsboat store useful information in sqlite
databases. Firefox’s <code>moz_places.sqlite</code> holds both all the
URLs visited as well as those that are associated with bookmarks
(i.e. the SQLite database <code>moz_bookmarks.sqlite</code>). I had
about 2000 bookmarks, less than I thought with many being stale from
link rot. Stale page URLs really slow down the harvest process because
of the need for wget to wait on various timeouts (e.g. DNS, server
response, download times). The “history” URLs would make an interesting
collection to spider but you’d probably want to have an exclude list
(e.g. there’s no point in saving queries to search engines, web mail,
shopping sites). Exploring that will wait for another prototype.</p>
<p>The <code>cache.db</code> associated with Newsboat provided a rich
resource of content and much fewer stale links (not surprising because I
maintain that URL list more much activity then reviewing my bookmarks).
Between the two I had 16,000 pages. I used SQLite 3 to query the url
values from the various DB into sorting for unique URLs into a single
text file one URL per line.</p>
<p>The next thing after creating a list of pages I wanted to search was
to download them into a directory using wget. Wget has many options, I
choose to enable timestamping, create a protocol directory and then a
domain and path directory for each item. This has the advantage of being
able to transform the Path into a URL later.</p>
<p>Once the content was harvested I then used PageFind to index the all
the harvested content. Since I started using PageFind originally the
tool has gained an option called <code>--serve</code> which provides a
localhost web service on port 1414. All I needed to do was add an
index.html file to the directory where I harvested the content and saved
the PageFind indexes. Then I used PageFind to again to provide a
localhost based personal search engine.</p>
<p>While the total number of pages was small (16k pages) I did find
interesting results just trying out random words. This makes the
prototype look promising.</p>
<h2 id="current-prototype-components">Current prototype components</h2>
<p>I have simple Bash script that gets the URLs from both Firefox
bookmarks and Newsboat’s cache then generates a single text file of
unique URLs I’ve named “pages.txt”.</p>
<p>I then use the “pages.txt” file to harvest content with wget into a
tree structure like</p>
<ul>
<li>htdocs
<ul>
<li>http (all the http based URLs I harvest go in here)</li>
<li>https (all the https based URLs I harvest go in here)</li>
<li>pagefind (this holds the PageFind indexes and JavaScript to
implement the search UI)</li>
<li>index.html (this holds the webpage for the search UI using the
libraries in <code>pagefind</code>)</li>
</ul></li>
</ul>
<p>Since I’m only downloaded the HTML the 16k pages does not take up
significant disk space yet.</p>
<h2 id="prototype-implementation">Prototype Implementation</h2>
<p>Here’s the bash scripts I use to get the URLs, harvest content and
launch my localhost search engine based on PageFind.</p>
<p>Get the URLs I want to be searchable. I use to environment variables
for finding the various SQLite 3 databases (i.e. PSE_MOZ_PLACES,
PSE_NEWSBOAT).</p>
<pre><code>#!/bin/bash

if [ &#34;$PSE_MOZ_PLACES&#34; = &#34;&#34; ]; then
    printf &#34;the PSE_MOZ_PLACES environment variable is not set.&#34;
    exit 1
fi
if [ &#34;$PSE_NEWSBOAT&#34; = &#34;&#34; ]; then
    printf &#34;the PSE_NEWSBOAT environment variable is not set.&#34;
    exit 1
fi

sqlite3 &#34;$PSE_MOZ_PLACES&#34; \
    &#39;SELECT moz_places.url AS url FROM moz_bookmarks JOIN moz_places ON moz_bookmarks.fk = moz_places.id WHERE moz_bookmarks.type = 1 AND moz_bookmarks.fk IS NOT NULL&#39; \
    &gt;moz_places.txt
sqlite3 &#34;$PSE_NEWSBOAT&#34; &#39;SELECT url FROM rss_item&#39; &gt;newsboat.txt
cat moz_places.txt newsboat.txt |
    grep -E &#39;^(http|https):&#39; |
    grep -v &#39;://127.0.&#39; |
    grep -v &#39;://192.&#39; |
    grep -v &#39;view-source:&#39; |
    sort -u &gt;pages.txt</code></pre>
<p>The next step is to have the pages. I use wget for that.</p>
<pre><code>#!/bin/bash
#
if [ ! -f &#34;pages.txt&#34; ]; then
    echo &#34;missing pages.txt, skipping harvest&#34;
    exit 1
fi
echo &#34;Output is logged to pages.log&#34;
wget --input-file pages.txt \
    --timestamping \
    --append-output pages.log \
    --directory-prefix htdocs \
    --max-redirect=5 \
    --force-directories \
    --protocol-directories \
    --convert-links \
    --no-cache --no-cookies</code></pre>
<p>Finally I have a bash script that generates the index.html page, an
Open Search Description XML file, indexes the harvested sites and
launches PageFind in server mode.</p>
<pre><code>#!/bin/bash
mkdir -p htdocs

cat &lt;&lt;OPENSEARCH_XML &gt;htdocs/pse.osdx
&lt;OpenSearchDescription xmlns=&#34;http://a9.com/-/spec/opensearch/1.1/&#34;
                       xmlns:moz=&#34;http://www.mozilla.org/2006/browser/search/&#34;&gt;
  &lt;ShortName&gt;PSE&lt;/ShortName&gt;
  &lt;Description&gt;A Personal Search Engine implemented via wget and PageFind&lt;/Description&gt;
  &lt;InputEncoding&gt;UTF-8&lt;/InputEncoding&gt;
  &lt;Url rel=&#34;self&#34; type=&#34;text/html&#34; method=&#34;get&#34; template=&#34;http://localhost:1414/index.html?q={searchTerms}&#34; /&gt;
  &lt;moz:SearchForm&gt;http://localhost:1414/index.html&lt;/moz:SearchForm&gt;
&lt;/OpenSearchDescription&gt;
OPENSEARCH_XML

cat &lt;&lt;HTML &gt;htdocs/index.html
&lt;html&gt;
&lt;head&gt;
&lt;link
  rel=&#34;search&#34;
  type=&#34;application/opensearchdescription+xml&#34;
  title=&#34;A Personal Search Engine&#34;
  href=&#34;http://localhost:1414/pse.osdx&#34; /&gt;
&lt;link href=&#34;/pagefind/pagefind-ui.css&#34; rel=&#34;stylesheet&#34;&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;A personal search engine&lt;/h1&gt;
&lt;div id=&#34;search&#34;&gt;&lt;/div&gt;
&lt;script src=&#34;/pagefind/pagefind-ui.js&#34; type=&#34;text/javascript&#34;&gt;&lt;/script&gt;
&lt;script&gt;
    window.addEventListener(&#39;DOMContentLoaded&#39;, function(event) {
        let page_url = new URL(window.location.href),
            query_string = page_url.searchParams.get(&#39;q&#39;),
            pse = new PagefindUI({ element: &#34;#search&#34; });
        if (query_string !== null) {
            pse.triggerSearch(query_string);
        }
    });
&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
HTML

pagefind \
--source htdocs \
--serve</code></pre>
<p>Then I just language my web browser pointing at
<code>http://localhost:1414/index.html</code>. I can even pass the URL a
<code>?q=...</code> query string if I want.</p>
<p>From a functionality point of view this is very bare bones and I
don’t think 16K pages is enough to make it compelling (I think I need
closer to 100K for that).</p>
<h2 id="what-i-learned-from-the-prototype-so-far">What I learned from
the prototype so far</h2>
<p>This prototype suffers from several limitations.</p>
<ol type="1">
<li>Stale links in my pages.txt make the harvest process really really
slow, I need to have a way to avoid stale links getting into the
pages.txt or have them removed from the pages.txt</li>
<li>PageFind’s result display uses the pages I downloaded to my local
machine. It would be better if the result link was translated to point
at the actual source of the pages, I think this can be done via
JavaScript in my index.html when I setup the PageFind search/results
element. Needs more exploration</li>
</ol>
<p>16K pages is a very tiny corpus. I get interesting results from my
testing but not good enough to make me use first. I’m guessing I need a
corpus of at least 100K pages to be compelling for first search use.</p>
<p>It is really nice having a localhost personal search engine. It means
that I can keep working with my home network connection is problematic.
I like that. Since the website generated for my localhost system is a
“static site” I could easily replicate that to net and make it available
to other machines.</p>
<p>Right now the big time sync is harvesting content to index. I’m not
certain yet how much space disk space will be needed for my 100K page
target corpus.</p>
<p>Setting up indexing and the search UI were the easiest part of the
process. PageFind is so easy to work with compare to enterprise search
applications.</p>
<h2 id="things-to-explore">Things to explore</h2>
<p>I can think of several ways to enlarge my search corpus. The first is
there are a few websites I use for reference that are small enough to
mirror. Wget provides a mirror function. Working from a “sites.txt” list
I could mirror those sites periodically and have their content available
for indexing.</p>
<p>When experimenting with the mirror option I notice I wind up with PDF
that are linked in the pages being mirrored. If I used the Unix find
command to locate all the PDF I could use another tool to extract there
text. Doing that would enlarge my search beyond plain text and HTML. I
would need to think this through as ultimately I’d want to be able to
recover the path to the PDF when those results are displayed.</p>
<p>Another approach would be to work with my full web browsers’ history
as well as it’s bookmarks. This would significantly expand the corpus.
If I did this I could also check the “head” of the HTML for references
to feeds that could be folded into my feed link harvests. This would
have the advantage of capture content from sources I find useful to read
but would catch blog posts I might have skipped due to limited reading
time.</p>
<p>I use Pocket to read the pages I find interesting in my feed reader.
Pocket has an API and I could get some additional interesting pages from
it. Pocket also has various curated lists and they might have
interesting pages to harvest and index. I think the trick would be to
use those suggests against an exclude list of some sort. E.g. Makes not
sense to try to harvest paywall stuff or commercial sites more
generally. One of the values I see in pse is that it is a personal
search engine not a replacement for commercial search engines
generally.</p>
  </article>
</section></div>
  </body>
</html>
