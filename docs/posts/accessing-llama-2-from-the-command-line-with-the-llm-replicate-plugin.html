<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Jul/18/accessing-llama-2/">Original</a>
    <h1>Accessing Llama 2 from the command-line with the LLM-replicate plugin</h1>
    
    <div id="readability-page-1" class="page"><div>


<h2>Accessing Llama 2 from the command-line with the llm-replicate plugin</h2>

<p>The big news today is <a href="https://ai.meta.com/llama/">Llama 2</a>, the new openly licensed Large Language Model from Meta AI. It’s a really big deal:</p>
<blockquote>
<p>Llama 2 is available for free for research and commercial use.</p>
</blockquote>
<p>(swyy <a href="https://twitter.com/swyx/status/1681364325469143040">spotted</a> that it was rebranded from LLaMA 2 to Llama 2 <a href="https://github.com/facebookresearch/llama/commit/6d4c0c290aeec1fa4399694fefb864be5a153bb6">a few hours ago</a>.)</p>
<p>Facebook’s original LLaMA model, released in February, kicked off a seismic wave of innovation in the world of open source LLMs—from fine-tuned variants to from-scratch recreations. I wrote about the impact this was having a few weeks after its original release in <a href="https://simonwillison.net/2023/Mar/11/llama/">Large language models are having their Stable Diffusion moment</a>. If anything I under-estimated how significant this was going to be.</p>
<p>There was just one catch: you couldn’t use it for commercial purposes.</p>
<p>That’s changed with Llama 2!</p>
<p>It does still have <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">a couple of interesting limitations</a>, emphasis mine:</p>
<blockquote>
<p>You will not use the Llama Materials or any output or results of the Llama Materials <strong>to improve any other large language model</strong> (excluding Llama 2 or derivative works thereof).</p>
<p>If, on the Llama 2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, <strong>is greater than 700 million monthly active users</strong> in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion[...]</p>
</blockquote>
<p>This still makes it available for most commercial purposes. Let’s try it out.</p>
<h4>Accessing Llama 2 using LLM</h4>
<p>My <a href="https://llm.datasette.io/">LLM</a> tool provides command-line access to a wide variety of language models, both via web APIs and self-hosted on your own machine.</p>
<p>Last week <a href="https://simonwillison.net/2023/Jul/12/llm/">I added plugin support</a> for extending it to support new models. An independently released plugin can now add support for new models to the existing CLI tool.</p>
<p>The brand new <a href="https://github.com/simonw/llm-replicate">llm-replicate</a> plugin provides CLI access to models hosted on <a href="https://replicate.com/">Replicate</a>, and this morning a16z-infra released <a href="https://replicate.com/a16z-infra/llama13b-v2-chat">a16z-infra/llama13b-v2-chat</a> which provides Replicate API access to the new Llama 2 13B chat model.</p>
<p>You’ll need a <a href="https://replicate.com/account/api-tokens">Replicate API key</a>.</p>
<p>Install LLM like this:</p>

<p>(I like to use <a href="https://pypa.github.io/pipx/">pipx</a>.)</p>
<p>Or if you don’t have <code>pip</code> setup but can use <a href="https://brew.sh/">Homebrew</a>:</p>
<div><pre>brew install simonw/llm/llm</pre></div>
<p>Run <code>llm --version</code> to confirm it’s installed.</p>
<p>Next, install the <code>llm-replicate</code> plugin:</p>
<div><pre>llm install llm-replicate</pre></div>
<p>The plugin adds a new <code>llm replicate</code> set of commands—see them with <code>llm --help</code>.</p>
<p>Configure your Replicate API key so the plugin can talk to the API:</p>

<pre><code>Enter key: &lt;paste key here&gt;
</code></pre>
<p>Register the new <code>a16z-infra/llama13b-v2-chat</code> model with the plugin:</p>
<div><pre>llm replicate add a16z-infra/llama13b-v2-chat \
  --chat --alias llama2</pre></div>
<p>This tells the plugin that it’s a “chat” model, which means you can have continuing conversations with it, rather than just sending single prompts.</p>
<p>Now we can send a prompt, using the <code>llama2</code> alias we configured there:</p>
<pre><code>llm -m llama2 &#34;Ten great names for a pet pelican&#34;
</code></pre>
<p>I get this:</p>
<blockquote>
<p>Certainly! Here are ten great names for a pet pelican:</p>
<ol>
<li>Peanut: A cute and playful name for a small pelican.</li>
<li>Puddles: A fun and silly name for a pelican that loves to splash around in water.</li>
<li>Pete: A classic name that suits a friendly and outgoing pelican.</li>
<li>Penelope: A pretty and elegant name for a female pelican.</li>
<li>Percy: A charming and sophisticated name for a male pelican.</li>
<li>Pippy: A playful and affectionate name for a pelican with a lively personality.</li>
<li>Pelty: A fun and quirky name for a pelican with a unique sense of style.</li>
<li>Pip: A sweet and endearing name for a small pelican.</li>
<li>Plucky: A feisty and adventurous name for a pelican with a big personality.</li>
<li>Poppy: A lovely and delicate name for a female pelican with a sweet disposition.</li>
</ol>
<p>I hope these suggestions help you find the perfect name for your pet pelican!</p>
</blockquote>
<p>And because it’s a chat model, we can send a follow-up prompt to the <a href="https://llm.datasette.io/en/stable/usage.html#continuing-a-conversation">same conversation</a>:</p>
<div><pre>llm -c <span><span>&#34;</span>Five more and make them more nautical<span>&#34;</span></span></pre></div>
<blockquote>
<p>Ahoy! Here be five more nautical names for yer pet pelican:</p>
<ol>
<li>Seabird: A fitting name for a pelican that loves the ocean and all its wonders.</li>
<li>Captain Hook: A swashbuckling name for a pelican with a bold and fearless spirit.</li>
<li>Anchor: A sturdy and dependable name for a pelican that’s always grounded and stable.</li>
<li>Finley: A fishy name for a pelican with a love for the sea and all its creatures.</li>
<li>Buoyant: A bouncy and cheerful name for a pelican with a lighthearted and carefree attitude.</li>
</ol>
<p>Hope these suggestions float yer boat, matey!</p>
</blockquote>
<p>LLM logs every prompt and response sent through it to a SQLite database. I’m using <code>--truncate</code> here to shorten the output to include in this post:</p>

<div><pre>[
  {
    <span>&#34;id&#34;</span>: <span><span>&#34;</span>01h5n5b3q6s54e03wtvqe90w2g<span>&#34;</span></span>,
    <span>&#34;model&#34;</span>: <span><span>&#34;</span>replicate-a16z-infra-llama13b-v2-chat<span>&#34;</span></span>,
    <span>&#34;prompt&#34;</span>: <span><span>&#34;</span>Five more and make them more nautical<span>&#34;</span></span>,
    <span>&#34;system&#34;</span>: <span>null</span>,
    <span>&#34;response&#34;</span>: <span><span>&#34;</span>Ahoy matey! Here be five more nautical-themed names for yer pet pelican:<span>\n\n</span>1. Captain Hook<span>\n</span>2. Anch...<span>&#34;</span></span>,
    <span>&#34;response_json&#34;</span>: <span>null</span>,
    <span>&#34;conversation_id&#34;</span>: <span><span>&#34;</span>01h5n5atsqfg4bhxc53j3ymnx2<span>&#34;</span></span>,
    <span>&#34;duration_ms&#34;</span>: <span>7364</span>,
    <span>&#34;datetime_utc&#34;</span>: <span><span>&#34;</span>2023-07-18T18:48:51.999944<span>&#34;</span></span>,
    <span>&#34;conversation_name&#34;</span>: <span><span>&#34;</span>Ten great names for a pet pelic<span>\u2026</span><span>&#34;</span></span>,
    <span>&#34;conversation_model&#34;</span>: <span><span>&#34;</span>replicate-a16z-infra-llama13b-v2-chat<span>&#34;</span></span>
  },
  {
    <span>&#34;id&#34;</span>: <span><span>&#34;</span>01h5n5atss6k9zsr5qs5hb2wzd<span>&#34;</span></span>,
    <span>&#34;model&#34;</span>: <span><span>&#34;</span>replicate-a16z-infra-llama13b-v2-chat<span>&#34;</span></span>,
    <span>&#34;prompt&#34;</span>: <span><span>&#34;</span>Ten great names for a pet pelican<span>&#34;</span></span>,
    <span>&#34;system&#34;</span>: <span>null</span>,
    <span>&#34;response&#34;</span>: <span><span>&#34;</span>Sure, here are ten great names for a pet pelican:<span>\n\n</span>1. Pelty<span>\n</span>2. Peanut<span>\n</span>3. Puddles<span>\n</span>4. Nibbles<span>\n</span>5. Fu...<span>&#34;</span></span>,
    <span>&#34;response_json&#34;</span>: <span>null</span>,
    <span>&#34;conversation_id&#34;</span>: <span><span>&#34;</span>01h5n5atsqfg4bhxc53j3ymnx2<span>&#34;</span></span>,
    <span>&#34;duration_ms&#34;</span>: <span>7873</span>,
    <span>&#34;datetime_utc&#34;</span>: <span><span>&#34;</span>2023-07-18T18:48:42.356413<span>&#34;</span></span>,
    <span>&#34;conversation_name&#34;</span>: <span><span>&#34;</span>Ten great names for a pet pelic<span>\u2026</span><span>&#34;</span></span>,
    <span>&#34;conversation_model&#34;</span>: <span><span>&#34;</span>replicate-a16z-infra-llama13b-v2-chat<span>&#34;</span></span>
  }
]</pre></div>
<p>Or run <code>llm logs path</code> to see the path to the SQLite database file. If you have <a href="https://datasette.io/">Datasette</a> installed you can open the logs database like this:</p>
<pre><code>datasette &#34;$(llm logs path)&#34;
</code></pre>
<p>Or with <a href="https://datasette.io/desktop">Datasette Desktop</a>:</p>
<pre><code>open &#34;$(llm logs path)&#34;
</code></pre>
<h4>What’s next</h4>
<p>I’m really excited about Llama 2. I expect we’ll start seeing all kinds of innovation on top of it as the model weights themselves start to roll out.</p>
<p>I’m particularly looking forward to:</p>
<ul>
<li>The fine-tuned models. Alpaca, Vicuna et-al did amazing things with fine-tuning on top of the original LLaMA. I expect we’ll see dozens of fine-tuned models over the next few weeks.</li>
<li>Implementation of the <a href="https://til.simonwillison.net/llms/python-react-pattern">reAct</a> and <a href="https://arxiv.org/abs/2302.04761">Toolformer</a> patterns, which will enable Llama 2 to drive external tools in a similar way to <a href="https://openai.com/blog/function-calling-and-other-api-updates">OpenAI Functions</a>. The <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2 paper</a> mentions testing against Toolformer prompts already, so this hopefully won’t be too difficult.</li>
</ul>
<p>Hopefully this time we’ll start to see a concerted effort from people to document what kind of prompts work well with Llama 2 for practical purposes, since I expect this model will be an influential piece of the ecosystem for some time to come.</p>




</div></div>
  </body>
</html>
