<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pytorch.org/blog/introducing-torchrec/">Original</a>
    <h1>TorchRec, a library for modern production recommendation systems</h1>
    
    <div id="readability-page-1" class="page"><div>
        <div>
            <div>
                <p><img src="https://pytorch.org/assets/images/logo-icon.svg"/></p><article>
                    <p>
                      by
                      
                        Meta AI - Donny Greenberg, Colin Taylor, Dmytro Ivchenko, Xing Liu
                      
                    </p>
                    <p>We are excited to announce <a href="https://github.com/pytorch/torchrec">TorchRec</a>, a PyTorch domain library for Recommendation Systems. This new library provides common sparsity and parallelism primitives, enabling researchers to build state-of-the-art personalization models and deploy them in production.</p>

<p>
  <img src="https://pytorch.org/assets/images/introducing-torchrec/torchrec_lockup.png" width="40%"/>
</p>

<h2 id="how-did-we-get-here">How did we get here?</h2>
<p>Recommendation Systems (RecSys) comprise a large footprint of production-deployed AI today, but you might not know it from looking at Github. Unlike areas like Vision and NLP, much of the ongoing innovation and development in RecSys is behind closed company doors. For academic researchers studying these techniques or companies building personalized user experiences, the field is far from democratized. Further, RecSys as an area is largely defined by learning models over sparse and/or sequential events, which has large overlaps with other areas of AI. Many of the techniques are transferable, particularly for scaling and distributed execution. A large portion of the global investment in AI is in developing these RecSys techniques, so cordoning them off blocks this investment from flowing into the broader AI field.</p>

<p>By mid-2020, the PyTorch team received a lot of feedback that there hasn’t been a large-scale  production-quality recommender systems package in the open-source PyTorch ecosystem. While we were trying to find a good answer, a group of engineers at Meta wanted to contribute Meta’s production RecSys stack as a PyTorch domain library, with a strong commitment to growing an ecosystem around it. This seemed like a good idea that benefits researchers and companies across the RecSys domain. So, starting from Meta’s stack, we began modularizing and designing a fully-scalable codebase that is adaptable for diverse recommendation use-cases. Our goal was to extract the key building blocks from across Meta’s software stack to simultaneously enable creative exploration and scale. After nearly two years, a battery of benchmarks, migrations, and testing across Meta, we’re excited to finally embark on this journey together with the RecSys community. We want this package to open a dialogue and collaboration across the RecSys industry, starting with Meta as the first sizable contributor.</p>

<h2 id="introducing-torchrec">Introducing TorchRec</h2>
<p>TorchRec includes a scalable low-level modeling foundation alongside rich batteries-included modules. We initially target “two-tower” ([<a href="https://research.google/pubs/pub48840/">1</a>], [<a href="https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/">2</a>]) architectures that have separate submodules to learn representations of candidate items and the query or context. Input signals can be a mix of floating point “dense” features or high-cardinality categorical “sparse” features that require large embedding tables to be trained. Efficient training of such architectures involves combining data parallelism that replicates the “dense” part of computation and model parallelism that  partitions large embedding tables across many nodes.</p>

<p>In particular, the library includes:</p>
<ul>
  <li>Modeling primitives, such as embedding bags and jagged tensors, that enable easy authoring of large, performant multi-device/multi-node models using hybrid data-parallelism and model-parallelism.</li>
  <li>Optimized RecSys kernels powered by <a href="https://github.com/pytorch/FBGEMM">FBGEMM</a> , including support for sparse and quantized operations.</li>
  <li>A sharder which can partition embedding tables with a variety of different strategies including data-parallel, table-wise, row-wise, table-wise-row-wise, and column-wise sharding.</li>
  <li>A planner which can automatically generate optimized sharding plans for models.</li>
  <li>Pipelining to overlap dataloading device transfer (copy to GPU), inter-device communications (input_dist), and computation (forward, backward) for increased performance.</li>
  <li>GPU inference support.</li>
  <li>Common modules for RecSys, such as models and public datasets (Criteo &amp; Movielens).</li>
</ul>

<p>To showcase the flexibility of this tooling, let’s look at the following code snippet, pulled from our DLRM Event Prediction example:</p>
<div><div><pre><code><span># Specify the sparse embedding layers
</span><span>eb_configs</span> <span>=</span> <span>[</span>
   <span>EmbeddingBagConfig</span><span>(</span>
       <span>name</span><span>=</span><span>f</span><span>&#34;t_</span><span>{</span><span>feature_name</span><span>}</span><span>&#34;</span><span>,</span>
       <span>embedding_dim</span><span>=</span><span>64</span><span>,</span>
       <span>num_embeddings</span><span>=</span><span>100_000</span><span>,</span>
       <span>feature_names</span><span>=</span><span>[</span><span>feature_name</span><span>],</span>
   <span>)</span>
   <span>for</span> <span>feature_idx</span><span>,</span> <span>feature_name</span> <span>in</span> <span>enumerate</span><span>(</span><span>DEFAULT_CAT_NAMES</span><span>)</span>
<span>]</span>

<span># Import and instantiate the model with the embedding configuration
# The &#34;meta&#34; device indicates lazy instantiation, with no memory allocated
</span><span>train_model</span> <span>=</span> <span>DLRM</span><span>(</span>
   <span>embedding_bag_collection</span><span>=</span><span>EmbeddingBagCollection</span><span>(</span>
       <span>tables</span><span>=</span><span>eb_configs</span><span>,</span> <span>device</span><span>=</span><span>torch</span><span>.</span><span>device</span><span>(</span><span>&#34;meta&#34;</span><span>)</span>
   <span>),</span>
   <span>dense_in_features</span><span>=</span><span>len</span><span>(</span><span>DEFAULT_INT_NAMES</span><span>),</span>
   <span>dense_arch_layer_sizes</span><span>=</span><span>[</span><span>512</span><span>,</span> <span>256</span><span>,</span> <span>64</span><span>],</span>
   <span>over_arch_layer_sizes</span><span>=</span><span>[</span><span>512</span><span>,</span> <span>512</span><span>,</span> <span>256</span><span>,</span> <span>1</span><span>],</span>
   <span>dense_device</span><span>=</span><span>device</span><span>,</span>
<span>)</span>

<span># Distribute the model over many devices, just as one would with DDP.
</span><span>model</span> <span>=</span> <span>DistributedModelParallel</span><span>(</span>
   <span>module</span><span>=</span><span>train_model</span><span>,</span>
   <span>device</span><span>=</span><span>device</span><span>,</span>
<span>)</span>

<span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>SGD</span><span>(</span><span>params</span><span>,</span> <span>lr</span><span>=</span><span>args</span><span>.</span><span>learning_rate</span><span>)</span>
<span># Optimize the model in a standard loop just as you would any other model!
# Or, you can use the pipeliner to synchronize communication and compute
</span><span>for</span> <span>epoch</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
   <span># Train
</span></code></pre></div></div>

<h2 id="scaling-performance">Scaling Performance</h2>
<p>TorchRec has state-of-the-art infrastructure for scaled Recommendations AI, powering some of the largest models at Meta. It was used to train a 1.25 trillion parameter model, pushed to production in January, and a 3 trillion parameter model which will be in production soon. This should be a good indication that PyTorch is fully capable of the largest scale RecSys problems in industry. We’ve heard from many in the community that sharded embeddings are a pain point. TorchRec cleanly addresses that. Unfortunately it is challenging to provide large-scale benchmarks with public datasets, as most open-source benchmarks are too small to show performance at scale.</p>

<h2 id="looking-ahead">Looking ahead</h2>
<p>Open-source and open-technology have universal benefits. Meta is seeding the PyTorch community with a state-of-the-art RecSys package, with the hope that many join in on building it forward, enabling new research and helping many companies. The team behind TorchRec plan to continue this program indefinitely, building up TorchRec to meet the needs of the RecSys community, to welcome new contributors, and to continue to power personalization at Meta. We’re excited to begin this journey and look forward to contributions, ideas, and feedback!</p>

<h2 id="references">References</h2>
<p>[<a href="https://research.google/pubs/pub48840/">1</a>] Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations</p>

<p>[<a href="https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/">2</a>] DLRM: An advanced, open source deep learning recommendation model</p>


                </article>
            </div>
        </div>
    </div></div>
  </body>
</html>
