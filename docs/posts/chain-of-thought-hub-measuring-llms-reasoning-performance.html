<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/FranxYao/chain-of-thought-hub">Original</a>
    <h1>Chain-of-Thought Hub: Measuring LLMs&#39; Reasoning Performance</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/FranxYao/chain-of-thought-hub/blob/main/resources/title.png"><img src="https://github.com/FranxYao/chain-of-thought-hub/raw/main/resources/title.png" alt="Title"/></a>
&#34;A fantasy graph illustrating a chain of stars in a dark night with blue sky, digital art, super resolution&#34;. Midjourney V5</p>
<hr/>
<p dir="auto">By <a href="https://franxyao.github.io/" rel="nofollow">Yao Fu</a>, <a href="https://github.com/Leonard907">Litu Ou</a>, <a href="https://github.com/Spehhhhh">Mingyu Chen</a>, <a href="https://github.com/Yuhao-Wan">Yuhao Wan</a>, <a href="https://haopeng-nlp.github.io/" rel="nofollow">Hao Peng</a>, <a href="https://allenai.org/team/tushark" rel="nofollow">Tushar Khot</a>, <a href="https://wenhuchen.github.io/" rel="nofollow">Wenhu Chen</a></p>
<p dir="auto">From University of Edinburgh, University of Washington, Allen Institute for AI, University of Waterloo</p>
<p dir="auto">Recently, there are a lot of progress in LLMs. Many claim that a small model less than 10B can achieve comparable performance to GPT-3.5. Really?</p>
<blockquote>
<p dir="auto">In a casual conversation, the distinction between GPT-3.5 and GPT-4 can be subtle. The difference comes out when <strong>*the complexity of the task reaches a sufficient threshold*</strong> â€” GPT-4 is more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.  --  <em>GPT-4 release blog</em></p>
</blockquote>
<p dir="auto">The key differentiator is whether a model can do <strong>complex tasks</strong>, like the old saying: &#34;chit-chat is cheap, show me the reasoning.&#34; This is why we compile a list of complex reasoning tasks including math (GSM8K), science (MATH, TheoremQA), symbolic (BBH), knowledge (MMLU, C-Eval), coding (HumanEval) to measure the models&#39; performance on challenging tasks.</p>
<p dir="auto">For more detailed discussion about complex reasoning, see article <a href="https://yaofu.notion.site/Towards-Complex-Reasoning-the-Polaris-of-Large-Language-Models-c2b4a51355b44764975f88e6a42d4e75" rel="nofollow">Towards Complex Reasoning: the Polaris of Large Language Models</a></p>
<p dir="auto"><strong>[UPDATE 20230527]</strong>: Call for contribution! If you are interested in fill in a missing number in our table, feel free to send a PR (especially for smaller models like Vicuna), much appreciated!</p>
<p dir="auto"><strong>[UPDATE 20230527]</strong>: Add TheoremQA, add Vicuna, Alpaca, InstructCodeT5. Yet still many numbers missing ...</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-results---overall" aria-hidden="true" href="#results---overall"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Results - Overall</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Param.</th>
<th>Type</th>
<th>GSM8K</th>
<th>MATH</th>
<th>MMLU</th>
<th>BBH</th>
<th>HumanEval</th>
<th>C-Eval</th>
<th>TheoremQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt-4</td>
<td>?</td>
<td>RLHF</td>
<td>92.0</td>
<td>42.5</td>
<td>86.4</td>
<td>-</td>
<td>67.0</td>
<td>68.7*</td>
<td>43.4</td>
</tr>
<tr>
<td>claude-v1.3</td>
<td>?</td>
<td>RLHF</td>
<td>81.8*</td>
<td>-</td>
<td>74.8*</td>
<td>67.3*</td>
<td>-</td>
<td>54.2*</td>
<td>24.9</td>
</tr>
<tr>
<td>PaLM-2</td>
<td>?</td>
<td>Base</td>
<td>80.7</td>
<td>34.3</td>
<td>78.3</td>
<td>78.1</td>
<td>-</td>
<td>-</td>
<td>31.8</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>?</td>
<td>RLHF</td>
<td>74.9*</td>
<td>-</td>
<td>67.3*</td>
<td>70.1*</td>
<td>48.1</td>
<td>54.4*</td>
<td>30.2</td>
</tr>
<tr>
<td>claude-instant</td>
<td>?</td>
<td>RLHF</td>
<td>70.8*</td>
<td>-</td>
<td>-</td>
<td>66.9*</td>
<td>-</td>
<td>45.9*</td>
<td>23.6</td>
</tr>
<tr>
<td>text-davinci-003</td>
<td>?</td>
<td>RLHF</td>
<td>-</td>
<td>-</td>
<td>64.6</td>
<td>70.7</td>
<td>-</td>
<td>-</td>
<td>22.8</td>
</tr>
<tr>
<td>code-davinci-002</td>
<td>?</td>
<td>Base</td>
<td>66.6</td>
<td>19.1</td>
<td>64.5</td>
<td>73.7</td>
<td>47.0</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>text-davinci-002</td>
<td>?</td>
<td>SIFT</td>
<td>55.4</td>
<td>-</td>
<td>60.0</td>
<td>67.2</td>
<td>-</td>
<td>-</td>
<td>16.6</td>
</tr>
<tr>
<td>Minerva</td>
<td>540B</td>
<td>SIFT</td>
<td>58.8</td>
<td>33.6</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Flan-PaLM</td>
<td>540B</td>
<td>SIFT</td>
<td>-</td>
<td>-</td>
<td>70.9</td>
<td>66.3</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Flan-U-PaLM</td>
<td>540B</td>
<td>SIFT</td>
<td>-</td>
<td>-</td>
<td>69.8</td>
<td>64.9</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PaLM</td>
<td>540B</td>
<td>Base</td>
<td>56.9</td>
<td>8.8</td>
<td>62.9</td>
<td>62.0</td>
<td>26.2</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>LLaMA</td>
<td>65B</td>
<td>Base</td>
<td>50.9</td>
<td>10.6</td>
<td>63.4</td>
<td>-</td>
<td>23.7</td>
<td>38.8*</td>
<td>-</td>
</tr>
<tr>
<td>PaLM</td>
<td>64B</td>
<td>Base</td>
<td>52.4</td>
<td>4.4</td>
<td>49.0</td>
<td>42.3</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>LLaMA</td>
<td>33B</td>
<td>Base</td>
<td>35.6</td>
<td>7.1</td>
<td>57.8</td>
<td>-</td>
<td>21.7</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>InstructCodeT5+</td>
<td>16B</td>
<td>SIFT</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>35.0</td>
<td>-</td>
<td>11.6</td>
</tr>
<tr>
<td>StarCoder</td>
<td>15B</td>
<td>Base</td>
<td>8.4</td>
<td>15.1</td>
<td>33.9</td>
<td>-</td>
<td>33.6</td>
<td>-</td>
<td>12.2</td>
</tr>
<tr>
<td>Vicuna</td>
<td>13B</td>
<td>SIFT</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>12.9</td>
</tr>
<tr>
<td>LLaMA</td>
<td>13B</td>
<td>Base</td>
<td>17.8</td>
<td>3.9</td>
<td>46.9</td>
<td>-</td>
<td>15.8</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Flan-T5</td>
<td>11B</td>
<td>SIFT</td>
<td>16.1*</td>
<td>-</td>
<td>48.6</td>
<td>41.4</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Alpaca</td>
<td>7B</td>
<td>SIFT</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>13.5</td>
</tr>
<tr>
<td>LLaMA</td>
<td>7B</td>
<td>Base</td>
<td>11.0</td>
<td>2.9</td>
<td>35.1</td>
<td>-</td>
<td>10.5</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Flan-T5</td>
<td>3B</td>
<td>SIFT</td>
<td>13.5*</td>
<td>-</td>
<td>45.5</td>
<td>35.2</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p dir="auto">Base means the pretrained checkpoint. SIFT means the checkpoint after supervised instruction finetuning. RLHF means the checkpoint after Reinforcement Learning from Human Feedback. Numbers marked with an asterisk * are from our own run, otherwise from multiple sources which we explain below.</p>
<p dir="auto">What&#39;s different than <a href="https://crfm.stanford.edu/helm/latest/" rel="nofollow">HeLM</a> and other evaluation?</p>
<ul dir="auto">
<li>HeLM uses answer-only prompting, we use chain-of-thought promoting</li>
<li>HeLM evaluates everything. We only focus on complex reasoning, the key differentiator of LLMs&#39; capability.</li>
</ul>
<p dir="auto">How the models are ranked</p>
<ul dir="auto">
<li>If we know model scale, we rank it by scale.</li>
<li>If we do not know model scale, we rank it by GSM8K, the classical benchmark measuring chain-of-thought math reasoning performance.
<ul dir="auto">
<li>This is definitely not the only metric, but a good interpretation is &#34;how good the model can do math while maintaining other generic abilities&#34; -- which is also very hard.</li>
<li>GPT-4 is already pretrained on GSM8k training split, others may not. So for GPT-4, its perf. on GSM8k is in-distribution generalization, while for others are ood. generalization. Yet even for in-dist. FlanT5 is also trained on GSM8k, still shows perf. difference.</li>
</ul>
</li>
<li>Generally it is very hard to rigiously compare model perf. due to multiple factors (whether trained on the corresponding training split, whether trained on code, whether optimize prompt .etc). View our results as approximate reference.</li>
</ul>
<p dir="auto">Source of numbers</p>
<ul dir="auto">
<li>GPT-4 from its <a href="https://openai.com/research/gpt-4" rel="nofollow">website</a> and <a href="https://arxiv.org/abs/2303.12712" rel="nofollow">Bubeck et al Mar 2023</a>. Note that the version that Bubeck uses is GPT-4 Early which is supposedly to be more powerful than GPT-4 Launch (OpenAI paid a lot of alignment tax to make GPT-4 safer).</li>
<li>*-davinci-00* and *PaLM are from the <a href="https://arxiv.org/abs/2210.11416" rel="nofollow">Flan-PaLM</a> paper appendix.</li>
<li>LLaMA from <a href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/" rel="nofollow">LLaMA</a> paper (TODO: test LLaMA on BBH). Note that the prompt of LLaMA used in these tasks are not released so reproduction may have varied numbers, see <a href="https://github.com/OpenLMLab/MOSS">this twitter thread</a> for more discussions.</li>
<li>PaLM-2 from <a href="https://ai.google/static/documents/palm2techreport.pdf" rel="nofollow">their tech report</a>.</li>
<li>Claude is from our own test script, see below about how to run it.</li>
<li>The HumanEval results for LLaMA models, PaLM and StartCoder are from <a href="https://huggingface.co/blog/starcoder" rel="nofollow">HuggingFace report</a>. Code-davinci-002&#39;s performance on HumanEval is from <a href="https://arxiv.org/pdf/2305.07922.pdf" rel="nofollow">CodeT5+ paper</a></li>
<li>C-Eval is from their <a href="https://cevalbenchmark.com/static/leaderboard.html" rel="nofollow">website</a></li>
<li>TheoremQA is from their <a href="https://github.com/wenhuchen/TheoremQA">github</a></li>
</ul>
<p dir="auto">Current results:</p>
<ul dir="auto">
<li>GPT-4 clearly outperforms all other models on GSM8K and MMLU.</li>
<li>**<strong>The 65B LLaMA is very close to text/code-davinci-002, which means that based on it, if SFT and RLHF are done correctly, it is very likely that we could reproduce ChatGPT based on the 65B LLaMA</strong>**</li>
<li>Claude is the only model family that is comparable to GPT family.</li>
<li>On GSM8K, gpt-3.5-turbo improves over text-davinci-003. This confirms OpenAI&#39;s Jan 30 2023 release notes &#34;improved mathematical capabilities.&#34;</li>
<li>On MMLU, gpt-3.5-turbo is slightly better than text-davinci-003. But this level of margin is NOT SIGNIFICANT</li>
<li>Also remember that gpt-3.5-turbo is 10 times cheaper than text-davinci-003</li>
<li>Also be careful that GPT-4/ 3.5&#39;s performance on GSM8K is not true few-shot -- in <a href="https://cdn.openai.com/papers/gpt-4.pdf" rel="nofollow">GPT-4 report</a> they said that they mixed a portion of GSM8K training set to train the model</li>
<li>LLaMA performance on MMLU is from their paper and probably not CoT but AO. Generally on MMLU, AO is better than CoT but just slightly better. So the LLaMA numbers on MMLU might be slightly overestimated.</li>
</ul>
<p dir="auto">Why choosing the above tasks?</p>
<ul dir="auto">
<li>We mostly care about complex reasoning.
<ul dir="auto">
<li>Other abilities of LLMs such as summarization or translation are not considered here as they are rather standard and probably not challenging enough.</li>
<li>We consider</li>
<li><a href="https://arxiv.org/abs/2210.11416" rel="nofollow">MMLU</a>: high school and college knowledge</li>
<li><a href="https://arxiv.org/abs/2201.11903" rel="nofollow">GSM8K</a>: elementary school math. -- Performance improvements on this dataset directly translate to daily math abilities when interacting with LLMs</li>
<li><a href="https://arxiv.org/abs/2206.14858" rel="nofollow">MATH</a> (Hard!): very hard math and natural science. All current models struggle.</li>
<li><a href="https://arxiv.org/abs/2210.09261" rel="nofollow">BBH</a>: a collection of 27 hard reasoning problems</li>
<li><a href="https://github.com/openai/human-eval">HumanEval</a>: a classical dataset for evaluating coding capability.</li>
<li><a href="https://cevalbenchmark.com/" rel="nofollow">C-Eval</a>: a collection of 52 disciplines of knowledge test in Chinese</li>
<li><a href="https://github.com/wenhuchen/TheoremQA">TheoremQA</a> (Hard!): a question-answering dataset driven by STEM theorems</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-run" aria-hidden="true" href="#run"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Run</h2>
<p dir="auto">MMLU</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd MMLU
mkdir outputs
API_KEY=&lt;your_api_key&gt;
python run_mmlu_gpt_3.5_turbo.py --api_key=${API_KEY}
python run_mmlu_claude.py --api_key=${API_KEY} --engine=claude-v1.3"><pre><span>cd</span> MMLU
mkdir outputs
API_KEY=<span>&lt;</span>your_api_key<span>&gt;</span>
python run_mmlu_gpt_3.5_turbo.py --api_key=<span>${API_KEY}</span>
python run_mmlu_claude.py --api_key=<span>${API_KEY}</span> --engine=claude-v1.3</pre></div>
<p dir="auto">GSM8k</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd gsm8k 
mkdir outputs

# run gpt-3.5
# codex_gsm8k_complex.ipynb         -- code-davinci-002 + complex prompt
# gpt3.5turbo_gsm8k_complex.ipynb   -- gpt-3.5-turbo + complex prompt

# run claude
python run_gsm8k_claude.py\
  --anthropic_key=${API_KEY}\
  --prompt_file=lib_prompt/prompt_original.txt\
  --engine=claude-v1.3\
  --output_file=outputs/gsm8k_claude_v1.3_original_test.txt

# run FlanT5
# flan_t5_11b_gsm8k.ipynb"><pre><span>cd</span> gsm8k 
mkdir outputs

<span><span>#</span> run gpt-3.5</span>
<span><span>#</span> codex_gsm8k_complex.ipynb         -- code-davinci-002 + complex prompt</span>
<span><span>#</span> gpt3.5turbo_gsm8k_complex.ipynb   -- gpt-3.5-turbo + complex prompt</span>

<span><span>#</span> run claude</span>
python run_gsm8k_claude.py\
  --anthropic_key=<span>${API_KEY}</span>\
  --prompt_file=lib_prompt/prompt_original.txt\
  --engine=claude-v1.3\
  --output_file=outputs/gsm8k_claude_v1.3_original_test.txt

<span><span>#</span> run FlanT5</span>
<span><span>#</span> flan_t5_11b_gsm8k.ipynb</span></pre></div>
<p dir="auto">BBH</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd BBH
mkdir outputs
# then run jupyter notebook to see an example penguins dataset
cd penguins
# gpt3.5trubo_penguins_original.ipynb

# Or run the script for all datasets
API_KEY=&lt;your_api_key&gt;
TASK=&lt;all | multiple_choice | free_form&gt;
python run_bbh_gpt_3.5_turbo.py --api_key=${API_KEY} --task=${TASK} # task=all by default
python run_bbh_claude_v1.3.py --api_key=${API_KEY} --model_index=claude-v1.3 --task=${TASK} # task=all by default"><pre><span>cd</span> BBH
mkdir outputs
<span><span>#</span> then run jupyter notebook to see an example penguins dataset</span>
<span>cd</span> penguins
<span><span>#</span> gpt3.5trubo_penguins_original.ipynb</span>

<span><span>#</span> Or run the script for all datasets</span>
API_KEY=<span>&lt;</span>your_api_key<span>&gt;</span>
TASK=<span>&lt;</span>all <span>|</span> multiple_choice <span>|</span> free_form<span>&gt;</span>
python run_bbh_gpt_3.5_turbo.py --api_key=<span>${API_KEY}</span> --task=<span>${TASK}</span> <span><span>#</span> task=all by default</span>
python run_bbh_claude_v1.3.py --api_key=<span>${API_KEY}</span> --model_index=claude-v1.3 --task=<span>${TASK}</span> <span><span>#</span> task=all by default</span></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-faq" aria-hidden="true" href="#faq"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FAQ</h2>
<ul dir="auto">
<li>What are the prompts used in the <em>complexity-based prompting</em> paper?
<ul dir="auto">
<li>See <code>research/complexity_based_prompting/</code></li>
</ul>
</li>
<li>I want to try some open-sourced model
<ul dir="auto">
<li>See <code>gsm8k/flan_t5_11b_gsm8k.ipynb</code> for a place to start</li>
</ul>
</li>
<li>There are some prompts that have wrong answer
<ul dir="auto">
<li>Yes, but we keep it as they are used in the original papers</li>
<li>Generally the model can be robust under prompt perturbation: even if sometimes there are errors in the prompt, as long as the format of the prompt is about the corresponding task, the model tend to only look at the format, ignore the prompt error, and make its own prediction.</li>
<li>See <a href="https://arxiv.org/abs/2202.12837" rel="nofollow">https://arxiv.org/abs/2202.12837</a> and <a href="https://arxiv.org/abs/2212.10001" rel="nofollow">https://arxiv.org/abs/2212.10001</a> about more analysis how the model can ignore errors in the prompt</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-references" aria-hidden="true" href="#references"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>References</h2>
<p dir="auto">We first discuss the recipe of building models of strong reasoning abilities, which is the same as generic LLM recipe: pretraining, finetuning, reinforcement learning. Then we discuss prompting methods for releasing the reasoning power of large language models.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-pretraining-continue-training" aria-hidden="true" href="#pretraining-continue-training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Pretraining/ Continue Training</h4>
<ul dir="auto">
<li>Lewkowycz et. al. 2022. Minerva: <a href="https://arxiv.org/abs/2206.14858" rel="nofollow">Solving Quantitative Reasoning Problems with Language Models</a></li>
<li>Taylor et. al. 2022. <a href="https://arxiv.org/abs/2211.09085" rel="nofollow">Galactica: A Large Language Model for Science</a></li>
</ul>
<h4 tabindex="-1" dir="auto"><a id="user-content-finetuning" aria-hidden="true" href="#finetuning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Finetuning</h4>
<ul dir="auto">
<li>Chung et. al. 2022. <a href="https://arxiv.org/abs/2210.11416" rel="nofollow">Scaling Instruction-Finetuned Language Models</a></li>
<li>Li et. al. 2022. <a href="https://arxiv.org/abs/2203.07814" rel="nofollow">Competition-Level Code Generation with AlphaCode</a></li>
<li>Fu et. al. 2023. <a href="https://arxiv.org/abs/2301.12726" rel="nofollow">Specializing Smaller Language Models towards Multi-Step Reasoning</a></li>
</ul>
<h4 tabindex="-1" dir="auto"><a id="user-content-reinforcement-learning" aria-hidden="true" href="#reinforcement-learning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reinforcement Learning</h4>
<ul dir="auto">
<li>Uesato et. al. 2022. <a href="https://arxiv.org/abs/2211.14275" rel="nofollow">Solving math word problems with process- and outcome-based feedback</a></li>
<li>Le et. al. 2022. <a href="https://arxiv.org/abs/2207.01780" rel="nofollow">CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning</a></li>
</ul>
<h4 tabindex="-1" dir="auto"><a id="user-content-prompting" aria-hidden="true" href="#prompting"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Prompting</h4>
<ul dir="auto">
<li>Wei et. al. 2022. <a href="https://arxiv.org/abs/2201.11903" rel="nofollow">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
<li>Suzgun et. al. 2022. <a href="https://arxiv.org/abs/2210.09261" rel="nofollow">Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them</a></li>
<li>Fu et. al. 2023. <a href="https://arxiv.org/abs/2210.00720" rel="nofollow">Complexity-Based Prompting for Multi-Step Reasoning</a></li>
</ul>
<p dir="auto"><a href="https://yaofu.notion.site/Towards-Complex-Reasoning-the-Polaris-of-Large-Language-Models-c2b4a51355b44764975f88e6a42d4e75" rel="nofollow">More detailed discussions</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-under-development" aria-hidden="true" href="#under-development"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Under Development</h2>
<p dir="auto"><a href="https://github.com/FranxYao/chain-of-thought-hub/blob/main/resources/todo.md">TODOs</a></p>
<p dir="auto"><a href="https://github.com/FranxYao/chain-of-thought-hub/blob/main/resources/literature.md">Literature</a></p>
<p dir="auto"><a href="https://github.com/FranxYao/chain-of-thought-hub/blob/main/resources/prompt_tricks.md">Prompting Tricks</a></p>
<p dir="auto"><a href="https://github.com/FranxYao/chain-of-thought-hub/blob/main/resources/detailed_results.md">Detailed Results</a></p>
</article>
          </div></div>
  </body>
</html>
