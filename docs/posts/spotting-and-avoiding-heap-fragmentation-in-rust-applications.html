<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.svix.com/blog/heap-fragmentation-in-rust-applications/">Original</a>
    <h1>Spotting and Avoiding Heap Fragmentation in Rust Applications</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><img alt="Cover image" src="https://www.svix.com/blog/static/images/generated/heap-fragmentation-in-rust-applications/cover-SX2UWUXG.png"/></p>
<h3 id="mystery-of-the-stair-step-profile"><a href="#mystery-of-the-stair-step-profile" aria-hidden="true" tabindex="-1"><span></span></a>Mystery of the Stair-step profile.</h3>
<p>We recently saw <a target="_blank" rel="noopener noreferrer" href="https://github.com/svix/svix-webhooks">one of our Rust projects</a>, an <code>axum</code> service, exhibit some odd behavior when it came to memory usage.
An odd-looking memory profile is the last thing I’d expect from a Rust program, but here we are.</p>
<p><img alt="Memory usage graph showing a sudden jump up, but otherwise flat" src="https://www.svix.com/blog/static/images/generated/heap-fragmentation-in-rust-applications/dd-stair-step-WHFN6CZC.png"/></p>
<p>The service would run with &#34;flat&#34; memory for a period of time, then suddenly jump up to a new plateau.
This pattern would repeat over hours, sometimes under load, but not always.
The worrying part was once we saw a sharp increase, it was rare for the memory to drop back down.
It was as if the memory was lost, or otherwise &#34;leaked&#34; once in a while.</p>
<p>Under normal circumstances, this &#34;stair-step&#34; profile was just odd looking, but at one point the memory usage climbed
disproportionately.
Unbounded memory growth can lead to services being forced to exit. When services exit abruptly, this can lower
availability... which is <em>bad for business</em>. I wanted to dig in and figure out what was happening.</p>
<p>Normally when I think of unexpected memory growth in a program, I think of leaks. Still, this seemed different.
With a leak, you tend to see a more steady, regular, pattern of growth.</p>
<p><img alt="Memory usage graph showing a steady rise then sharp drop, repeated, indicative of a leaky process" src="https://www.svix.com/blog/static/images/generated/heap-fragmentation-in-rust-applications/dd-leak-KDT5AVVV.png"/></p>
<p>Often this looks like a line sloping up and to the right. So, if our service wasn’t leaking, what was it doing?</p>
<p>If I could identify conditions that caused the jump in memory usage, maybe I could mitigate whatever was happening.</p>
<h3 id="digging-in"><a href="#digging-in" aria-hidden="true" tabindex="-1"><span></span></a>Digging in</h3>
<p>I had two burning questions:</p>
<ul>
<li>Did something change in our code to promote this behavior?</li>
<li>Otherwise, did a new traffic pattern emerge?</li>
</ul>
<p>Looking at historical metrics, I could see similar patterns of sharp increases between long flat periods, but never
before did we have this sort of growth.
To know if the growth itself was new (in spite of the &#34;stair-step&#34; pattern being <em>normal</em> for us), I’d need a reliable
way to reproduce this behavior.</p>
<p>If I could force the &#34;step&#34; to show itself, then I’ll have a way to verify a change in behavior when I take steps to
curb the memory growth.
I’d also be able to backtrack through our git history and look for a point in time when the service didn’t exhibit
seemingly unbounded growth.</p>
<p>The dimensions I used when running my load tests were:</p>
<ul>
<li>The size of POST bodies sent to the service.</li>
<li>The request rate (ie, requests per second).</li>
<li>The number of concurrent client connections.</li>
</ul>
<p>The magic combination for me was: <em>larger request bodies</em> and <em>higher concurrency</em>.</p>
<p>When running load tests on a local system, there are all sorts of limiting factors, including the finite number of
processors available for running both clients and the server itself. Still, I was able to see the &#34;stair-step&#34; in the
memory on my local machine given just the right circumstances, even at a lower overall request rate.</p>
<p>Using a fixed-sized payload and sending requests in batches, with a brief rest between them, I was able to drive up the
memory of the service repeatedly, a step at a time.</p>
<p>I found it interesting that while I could grow the memory over time, I’d eventually hit a point of diminishing returns.
Eventually, there’d be some (still much higher than expected) ceiling to the growth.
Playing around a little more, I found I could reach an even higher ceiling by sending requests with varying payload sizes.</p>
<p>Once I had identified my input, I was able to work backward through our git history, eventually learning that our
production scare was not likely to be the result of recent changes on our end.</p>
<p>The particulars of the workload to trigger this &#34;stair-step&#34; are specific to the application itself, though I was able
to force a similar graph to happen with a <a target="_blank" rel="noopener noreferrer" href="https://github.com/svix-onelson/stair-stepper">toy project</a>.</p>
<div><pre><code><span><span>#[derive(serde::Deserialize, Clone)]</span>
</span><span><span>struct</span> <span>Widget</span> <span>{</span>
</span><span>    payload<span>:</span> <span>serde_json<span>::</span></span><span>Value</span><span>,</span>
</span><span><span>}</span>
</span><span>
</span><span><span>#[derive(serde::Serialize)]</span>
</span><span><span>struct</span> <span>WidgetCreateResponse</span> <span>{</span>
</span><span>    id<span>:</span> <span>String</span><span>,</span>
</span><span>    size<span>:</span> <span>usize</span><span>,</span>
</span><span><span>}</span>
</span><span>
</span><span><span>async</span> <span>fn</span> <span>create_widget</span><span>(</span><span>Json</span><span>(</span>widget<span>)</span><span>:</span> <span>Json</span><span>&lt;</span><span>Widget</span><span>&gt;</span><span>)</span> <span>-&gt;</span> <span>Response</span> <span>{</span>
</span><span>    <span>(</span>
</span><span>        <span>StatusCode</span><span>::</span><span>CREATED</span><span>,</span>
</span><span>        <span>Json</span><span>(</span><span>process_widget</span><span>(</span>widget<span>.</span><span>clone</span><span>(</span><span>)</span><span>)</span><span>.</span><span>await</span><span>)</span><span>,</span>
</span><span>    <span>)</span>
</span><span>        <span>.</span><span>into_response</span><span>(</span><span>)</span>
</span><span><span>}</span>
</span><span>
</span><span><span>async</span> <span>fn</span> <span>process_widget</span><span>(</span>widget<span>:</span> <span>Widget</span><span>)</span> <span>-&gt;</span> <span>WidgetCreateResponse</span> <span>{</span>
</span><span>    <span>let</span> widget_id <span>=</span> <span>uuid<span>::</span></span><span>Uuid</span><span>::</span><span>new_v4</span><span>(</span><span>)</span><span>;</span>
</span><span>    <span>let</span> bytes <span>=</span> <span>serde_json<span>::</span></span><span>to_vec</span><span>(</span><span>&amp;</span>widget<span>.</span>payload<span>)</span><span>.</span><span>unwrap_or_default</span><span>(</span><span>)</span><span>;</span>
</span><span>    
</span><span>    
</span><span>    
</span><span>    <span>tokio<span>::</span>time<span>::</span></span><span>sleep</span><span>(</span><span>std<span>::</span>time<span>::</span></span><span>Duration</span><span>::</span><span>from_millis</span><span>(</span>
</span><span>        <span>std<span>::</span>env<span>::</span></span><span>var</span><span>(</span><span>&#34;SLEEP_MS&#34;</span><span>)</span>
</span><span>            <span>.</span><span>as_deref</span><span>(</span><span>)</span>
</span><span>            <span>.</span><span>unwrap_or</span><span>(</span><span>&#34;150&#34;</span><span>)</span>
</span><span>            <span>.</span><span>parse</span><span>(</span><span>)</span>
</span><span>            <span>.</span><span>expect</span><span>(</span><span>&#34;invalid SLEEP_MS&#34;</span><span>)</span><span>,</span>
</span><span>    <span>)</span><span>)</span>
</span><span>    <span>.</span><span>await</span><span>;</span>
</span><span>    <span>WidgetCreateResponse</span> <span>{</span>
</span><span>        id<span>:</span> widget_id<span>.</span><span>to_string</span><span>(</span><span>)</span><span>,</span>
</span><span>        size<span>:</span> bytes<span>.</span><span>len</span><span>(</span><span>)</span><span>,</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div>
<p>It turned out that you didn’t need much to get there. I managed to see a similar sharp (but in this case much smaller)
increase from an <code>axum</code> app with a single handler receiving a JSON body.</p>
<p><img alt="Memory usage graph with a sharp jump up circled in red" src="https://www.svix.com/blog/static/images/generated/heap-fragmentation-in-rust-applications/stair-step-marked-X5K3HIOX.png"/></p>
<p>While the memory increases in my toy project were nowhere near as dramatic as we saw in the production service, it was
enough to help me compare and contrast during the next phase of my investigation. It also helped me to have the tighter
iteration loop of a smaller codebase while I experimented with different workloads. See the <a target="_blank" rel="noopener noreferrer" href="https://github.com/svix-onelson/stair-stepper">README</a> for
details on how I ran my load tests.</p>
<p>I spent some time searching the web for bug reports or discussions that might describe a similar behavior.
A term that came up repeatedly was <strong>Heap Fragmentation</strong> and after reading a bit more on the topic, it seemed like it
could fit what I was seeing.</p>
<h3 id="what-is-heap-fragmentation"><a href="#what-is-heap-fragmentation" aria-hidden="true" tabindex="-1"><span></span></a>What is Heap Fragmentation?</h3>
<p>Folks of a certain age may have had the experience of watching a <em>defrag utility</em> on DOS or Windows move blocks around
on a hard disk to consolidate the &#34;used&#34; and &#34;free&#34; areas.</p>
<p><img alt="Defrag utility in action" src="https://www.svix.com/blog/static/images/generated/heap-fragmentation-in-rust-applications/defragger-M7C7WWZU.png"/></p>
<p>In the case of this old PC hard drive, files of varying sizes were written to disk then later moved or deleted, leaving
a &#34;hole&#34; of available space between other used regions. As the disk starts to fill up, you might try to create a new file
that doesn’t quite fit in one of those smaller areas. In heap fragmentation scenario, that will lead to an allocation failure, though the failure mode of disk fragmentation will be slightly less drastic. On disk, the file will then need to be split to smaller chunks which makes access much less efficient (thanks <code>wongarsu</code> <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=35470910">for the correction</a>). The solution for the disk drive is to &#34;defrag&#34; (de-fragment) the drive in order to re-arrange those open blocks co continuous spaces.</p>
<p>Something similar can <em>happen</em> when the allocator (the thing responsible for managing memory allocation in your program) adds and removes values of varying sizes over a period of time.
Gaps that are too small and scattered throughout the heap can lead to new &#34;fresh&#34; blocks of memory being allocated to
accommodate a new value that won’t fit otherwise. Though unfortunately because of how memory management works a &#34;defrag&#34; is not possible.</p>
<p>The specific cause for the fragmentation could be any number of things: JSON parsing with <code>serde</code>, something at the
framework-level in <code>axum</code>, something deeper in <code>tokio</code>, or even just a quirk of the specific allocator implementation
for the given system.
Even without knowing the root cause (if there is such a thing) the behavior is observable in our environment and
somewhat reproducible in a bare-bones app.</p>
<p>If this is what was happening to the process memory, what can be done about it?
It seems like it would be hard to change the workload to avoid fragmentation.
It also seems like it’d be tricky to unwind all the dependencies in my project to possibly find a root cause in the code
for how the fragmentation events are occurring. So, what can be done?</p>
<h3 id="jemalloc-to-the-rescue"><a href="#jemalloc-to-the-rescue" aria-hidden="true" tabindex="-1"><span></span></a><code>Jemalloc</code> to the rescue</h3>
<p><a target="_blank" rel="noopener noreferrer" href="https://jemalloc.net/"><code>jemalloc</code></a> is self-described as aiming to <em>&#34;[emphasize] fragmentation avoidance and scalable
concurrency support.&#34;</em>
Concurrency was indeed a part of the problem for my program, and avoiding fragmentation is the name of the game.
<code>jemalloc</code> sounds like it could be just what I need.</p>
<p>Since <code>jemalloc</code> is an allocator that goes out of its way to avoid fragmentation in the first place, the hope was our
service might be able to run longer without gradually increasing the memory.</p>
<p>It’s not so trivial to change the inputs to my program, or the pile of application dependencies. It is, however, trivial
to swap out the allocator.
Following the examples in the <a target="_blank" rel="noopener noreferrer" href="https://github.com/tikv/jemallocator">https://github.com/tikv/jemallocator</a> readme, very little work was required to take it for
a test drive.</p>
<p>For my <a target="_blank" rel="noopener noreferrer" href="https://github.com/svix-onelson/stair-stepper">toy project</a>, I added a cargo feature to optionally swap out the default allocator for <code>jemalloc</code> and re-ran my
load tests.</p>
<p><img alt="Two memory usage graphs side by side. On the left is the default allocator with the &#34;stair-step&#34; jump up in usage, on the
right is the same program with jemalloc in use. The jemalloc graph shows a ragged line where memory rises and falls over
the duration of the test" src="https://www.svix.com/blog/static/images/generated/heap-fragmentation-in-rust-applications/side-by-side-YZMQFXI7.png"/></p>
<p>Recording the resident memory during my simulated load shows the two distinct memory profiles.</p>
<p>Without <code>jemalloc</code>, we see the familiar stair-step profile. With <code>jemalloc</code>, we see the memory rise and fall repeatedly
as the test runs. More importantly, while there is a considerable difference between the memory usage with <code>jemalloc</code>
during load versus idle times, we don’t &#34;lose ground&#34; as we did before since the memory always comes back down to the
baseline.</p>
<h3 id="wrapping-up"><a href="#wrapping-up" aria-hidden="true" tabindex="-1"><span></span></a>Wrapping Up</h3>
<p>If you happen to see a &#34;stair-step&#34; profile on a Rust service, consider taking <code>jemalloc</code> for a test drive.
If you happen to have a workload that promotes heap fragmentation, <code>jemalloc</code> may give a better result overall.</p>
<p>Separately, included in the <a target="_blank" rel="noopener noreferrer" href="https://github.com/svix-onelson/stair-stepper">toy project</a> repo is a <code>benchmark.yml</code> for use with the <a target="_blank" rel="noopener noreferrer" href="https://github.com/fcsonline/drill">https://github.com/fcsonline/drill</a>
load testing tool.
Try changing the concurrency, body size (and the arbitrary handler sleep duration in the service itself), etc to see
how the change in allocator impacts the memory profile.</p>
<p>As for real-world impact, you can clearly see the profile change when we rolled out the switch to <code>jemalloc</code>.</p>
<p><img alt="Memory usage graph before and after switching to jemalloc" src="https://www.svix.com/blog/static/images/generated/heap-fragmentation-in-rust-applications/dd-jemalloc-adoption-annotated-7UAMVLR7.png"/></p>
<p>Where the service used to show flat lines and large steps, often regardless of load, we now see a more ragged line that
follows the active workload more closely. Aside from the benefit of helping the service avoid needless memory growth,
this change gave us better insight into how our service responds to load so all in all, this was a positive outcome.</p>
<hr/>
<p>If you&#39;re interested in building a robust and scalable service using Rust, we are hiring! Check out <a target="_blank" rel="noopener noreferrer" href="https://www.svix.com/careers/">our careers page</a> for more information.</p>
<p>For more content like this, make sure to follow us on <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/SvixHQ">Twitter</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/svix">Github</a> or <a target="_blank" rel="noopener noreferrer" href="https://www.svix.com/blog/rss/">RSS</a> for the latest updates for the <a target="_blank" rel="noopener noreferrer" href="https://www.svix.com">Svix webhook service</a>, or join the discussion on <a target="_blank" rel="noopener noreferrer" href="https://www.svix.com/slack/">our community Slack</a>.</p></div></div></div>
  </body>
</html>
