<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Foreseerr/TScale">Original</a>
    <h1>TScale â€“ distributed training on consumer GPUs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This repo contains transformer train and inference code written in C++ and CUDA.</p>
<p dir="auto">TScale is designed to run on consumer hardware. To achive best results it features</p>
<ul dir="auto">
<li>Optimized transformer architecture with faster convergence and ~2x reduced attention costs</li>
<li>Support for fp8 and int8 model weights and activations precision</li>
<li>Optimized for consumer nVidia GPUs including fast reduced precision training without sacrificing model quality</li>
<li>CPU offload reduces GPU memory requirements for training</li>
<li>Sync distributed training on several same config hosts</li>
<li>1-bit gradient compression allowing using regular ethernet links for interconnect</li>
<li>Async distributed training on arbitrary hosts with negligible network traffic. In this mode training can be run on geographically separated hosts</li>
</ul>

<p dir="auto">By using inexpensive GPUs and async distributed mode TScale trains LLMs fast and affordable. Log loss for the 1.5B model trained on fineweb-edu for 2 days and $500 on several spot instances with 4090:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Foreseerr/TScale/blob/main/img/fed_hellaswag.png"><img src="https://github.com/Foreseerr/TScale/raw/main/img/fed_hellaswag.png" alt="Nice train loss graph"/></a></p>

<p dir="auto">1T model size sounds beyond reach for most people and even organisations. However if we consider creative ways to count model size then there is nothing impossible. In this case we build a model with 1T index which we lookup for every token to make prediction with much smaller model. In terms of logloss/perplexity this construction easily achieves stellar results. Index for fineweb-edu occupies about 1T of disk space. Training run of 125M model with this ~1T index achieves <strong>x8</strong> perplexity reduction:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>125M</td>
<td>19.02</td>
</tr>
<tr>
<td>125M + 1T index</td>
<td>2.28</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/125M_model.md">Training 125M model</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/1.5B_model.md">Training 1.5B model</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/1T_model.md">Training 1T (!) model in your kitchen</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/fed.md">Async distributed train</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/precision.md">Notes on model and compute precision</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/model.md">TScale transformer model</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/lm_search.md">Data indexing</a></p>
<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/doc/tokenizer.md">Tokenizer</a></p>

<p dir="auto">To build the the code CUDA v12.3 and C++ compiler are required, msvc for windows,  cmake+clang for Linux. To support cross platform build files generation this repo uses <a href="https://github.com/Foreseerr/TScale/blob/main/doc/fo.md">fo</a>, lightweight solution/build files generator. To generate build files you need to compile fo/fo.cpp and run it with two arguments. First argument is root of source tree, second argument is directory to store build files to.</p>

<div dir="auto" data-snippet-clipboard-copy-content="D:\TScale&gt;fo.exe code sln"><pre>D:<span>\T</span>Scale<span>&gt;</span>fo.exe code sln</pre></div>
<p dir="auto">Then open code.sln from d:\TScale\sln\code.sln.</p>

<p dir="auto">To compile TScale for linux you need to compile fo.cpp, generate CMakeLists.txt file, run cmake, run make.</p>
<div dir="auto" data-snippet-clipboard-copy-content="~/TScale/fo$ clang++17 fo.cpp -o fo
~/TScale/fo$ cd ..
~/TScale$ ./fo/fo code make.dir
~/TScale$ cd make.dir
~/TScale/make.dir$ cmake -D CMAKE_BUILD_TYPE=RelWithDebInfo .
~/TScale/make.dir$ make"><pre><span>~</span>/TScale/fo$ clang++17 fo.cpp -o fo
<span>~</span>/TScale/fo$ <span>cd</span> ..
<span>~</span>/TScale$ ./fo/fo code make.dir
<span>~</span>/TScale$ <span>cd</span> make.dir
<span>~</span>/TScale/make.dir$ cmake -D CMAKE_BUILD_TYPE=RelWithDebInfo <span>.</span>
<span>~</span>/TScale/make.dir$ make</pre></div>

<p dir="auto">Examples in the code use <a href="https://mattmahoney.net/dc/textdata.html" rel="nofollow">enwik9</a> dataset and its truncacted version enwik8. Also Hugging Face hosted datasets openwebtext, ontocord/CulturaY, danasone/librusec are used in examples. To import them use <a href="https://github.com/Foreseerr/TScale/blob/main/pysrc/hf_import/import.py">hf_import</a>.</p>

<p dir="auto"><a href="https://github.com/Foreseerr/TScale/blob/main/code/gpt/train">gpt_train</a> is used to train a model. It is controlled by the <a href="https://github.com/Foreseerr/TScale/blob/main/doc/train_script.md">train script</a> and <a href="https://github.com/Foreseerr/TScale/blob/main/doc/data_script.md">data script</a>. Default scripts are stored in <a href="https://github.com/Foreseerr/TScale/blob/main/code/gpt/train/main_gpt.cpp">main_gpt.cpp</a>. To load train script from file run gpt_train with &#39;-d data_script.txt -s train_script.txt&#39; arguments.</p>

<p dir="auto">Compile gpt-train. Run it in the root directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="~/TScale$ ./make.dir/gpt-train"><pre><span>~</span>/TScale$ ./make.dir/gpt-train</pre></div>

<p dir="auto">Currently training can be distributed only among pow2 number of worker hosts.</p>
<p dir="auto">To start a worker process run gpt_train with &#39;-w 10000&#39; argument. 10000 specifies port number to use.</p>
<p dir="auto">To run master process call net_train(&#39;worker.txt&#39;) function in train script. List worker IP addresses in the file provided to net_train().</p>

<p dir="auto">To use multiple GPU devices set DEVICE_COUNT variable in train script to number of GPUs to use. For distributed runs DEVICE_COUNT is applied on each worker, heterogeneous configurations are not supported.</p>

<p dir="auto">Description of scripts used in training: <a href="https://github.com/Foreseerr/TScale/blob/main/doc/data_script.md">data script</a>, <a href="https://github.com/Foreseerr/TScale/blob/main/doc/train_script.md">train script</a></p>

<p dir="auto">To try inferencing from the trained model you can use <a href="https://github.com/Foreseerr/TScale/blob/main/code/gpt/infer">gpt_infer</a>. It runs basic http server on 11311 port and allows sampling continuations from the model. Current implementation is slow and designed for demonstration purposes only.</p>

<p dir="auto">MIT</p>
</article></div></div>
  </body>
</html>
