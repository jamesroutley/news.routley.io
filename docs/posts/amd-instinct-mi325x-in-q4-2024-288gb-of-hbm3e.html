<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ir.amd.com/news-events/press-releases/detail/1201/amd-accelerates-pace-of-data-center-ai-innovation-and">Original</a>
    <h1>AMD Instinct MI325X in Q4 2024, 288GB of HBM3E</h1>
    
    <div id="readability-page-1" class="page"><div>
                <div>
                                                    <main>
                        <div>
                                <article>
    
    
    
     
      
    
	
<p><em>— Updated AMD Instinct accelerator roadmap brings annual cadence of leadership AI performance and memory capabilities —</em><br/></p>    <p><em>— New AMD Instinct MI325X accelerator expected to be available in Q4 2024 with up to 288GB of HBM3E memory; new AMD Instinct MI350 series accelerators based on AMD CDNA 4 architecture expected to be available in 2025 with 35x generational increase in AI inference performance —</em></p>    <p>TAIPEI, Taiwan, June  02, 2024  (GLOBE NEWSWIRE) -- At Computex 2024, <a href="http://www.amd.com/" rel="nofollow" target="_blank">AMD</a> (NASDAQ: AMD) showcased the growing momentum of the AMD Instinct™ accelerator family during the opening keynote by Chair and CEO Dr. Lisa Su. AMD unveiled a multiyear, expanded AMD Instinct accelerator roadmap which will bring an annual cadence of leadership AI performance and memory capabilities at every generation.<br/></p>    <p>The updated roadmap starts with the new AMD Instinct MI325X accelerator, which will be available in Q4 2024. Following that, the AMD Instinct MI350 series, powered by the new AMD CDNA™ 4 architecture, is expected to be available in 2025 bringing up to a 35x increase in AI inference performance compared to AMD Instinct MI300 Series with AMD CDNA 3 architecture<sup>1</sup>. Expected to arrive in 2026, the AMD Instinct MI400 series is based on the AMD CDNA “Next” architecture.<br/></p>    <p>“The AMD Instinct MI300X accelerators continue their strong adoption from numerous partners and customers including Microsoft Azure, Meta, Dell Technologies, HPE, Lenovo and others, a direct result of the AMD Instinct MI300X accelerator exceptional performance and value proposition,” said Brad McCredie, corporate vice president, Data Center Accelerated Compute, AMD. “With our updated annual cadence of products, we are relentless in our pace of innovation, providing the leadership capabilities and performance the AI industry and our customers expect to drive the next evolution of data center AI training and inference.”</p>    <p><strong>AMD AI Software Ecosystem Matures</strong></p>    <p><strong>AMD Previews New Accelerators and Reveals Annual Cadence Roadmap</strong></p>    <ul type="disc">
<li>The new AMD Instinct MI325X accelerator, which will bring 288GB of HBM3E memory and 6 terabytes per second of memory bandwidth, use the same industry standard Universal Baseboard server design used by the AMD Instinct MI300 series, and be generally available in Q4 2024. The accelerator will have industry leading memory capacity and bandwidth, 2x and 1.3x better than the competition respectively<sup>4</sup>, and 1.3x better<sup>5</sup> compute performance than competition.</li>
<li>The first product in the AMD Instinct MI350 Series, the AMD Instinct MI350X accelerator, is based on the AMD CDNA 4 architecture and is expected to be available in 2025. It will use the same industry standard Universal Baseboard server design as other MI300 Series accelerators and will be built using advanced 3nm process technology, support the FP4 and FP6 AI datatypes and have up to 288 GB of HBM3E memory.</li>
<li>AMD CDNA “Next” architecture, which will power the AMD Instinct MI400 Series accelerators, is expected to be available in 2026 providing the latest features and capabilities that will help unlock additional performance and efficiency for inference and large-scale AI training.<br/>
</li>
</ul>    <p>Finally, AMD highlighted the demand for AMD Instinct MI300X accelerators continues to grow with numerous partners and customers using the accelerators to power their demanding AI workloads, including:</p>    <ul type="disc">
<li>Microsoft Azure using the accelerators for <a href="https://techcommunity.microsoft.com/t5/azure-high-performance-computing/introducing-the-new-azure-ai-infrastructure-vm-series-nd-mi300x/ba-p/4145152" rel="nofollow" target="_blank">Azure OpenAI services and the new Azure ND MI300X V5 virtual machines</a>.</li>
<li>Dell Technologies using MI300X accelerators in the PowerEdge <a href="https://www.dell.com/en-hk/blog/accelerate-modern-workloads-with-amd-and-dell-ai-innovation/" rel="nofollow" target="_blank">XE9680 for enterprise AI workloads</a>.</li>
<li>Supermicro <a href="https://www.supermicro.com/en/accelerators/amd" rel="nofollow" target="_blank">providing multiple solutions with AMD Instinct accelerators</a>.</li>
<li>Lenovo powering Hybrid AI innovation with the <a href="https://news.lenovo.com/pressroom/press-releases/lenovo-advances-hybrid-ai-to-meet-demands-of-most-compute-intensive-workloads/" rel="nofollow" target="_blank">ThinkSystem SR685a V3</a>
</li>
<li>HPE is using them to accelerate AI workloads in the <a href="https://www.hpe.com/us/en/hpe-cray-xd675.html" rel="nofollow" target="_blank">HPE Cray XD675</a>.<br/>
</li>
</ul>    <p>Read more AMD AI announcements at Computex <a href="https://www.amd.com/en/corporate/events/computex.html" rel="nofollow" target="_blank">here</a> and watch a video replay of the keynote on the <a href="https://youtube.com/live/MCi8jgALPYA" rel="nofollow" target="_blank">AMD YouTube page</a>.</p>    <p><strong>Supporting Resources</strong></p>    <ul type="disc">
<li>Follow AMD on <a href="https://www.linkedin.com/company/amd/" rel="nofollow" target="_blank">LinkedIn</a>
</li>
<li>Follow AMD on <a href="https://twitter.com/AMD" rel="nofollow" target="_blank">X</a>
</li>
</ul>    <p><strong>About AMD</strong></p>    <p>©2024 Advanced Micro Devices, Inc. All rights reserved. AMD, AMD Instinct, AMD CDNA, ROCm and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other names used herein are for informational purposes only and may be trademarks of their respective owners.</p>    <p>CAUTIONARY STATEMENT  </p>        <p><sup>1</sup>MI300-55: Inference performance projections as of May 31, 2024 using engineering estimates based on the design of a future AMD CDNA 4-based Instinct MI350 Series accelerator as proxy for projected AMD CDNA™ 4 performance. A 1.8T GPT MoE model was evaluated assuming a token-to-token latency = 70ms real time, first token latency = 5s, input sequence length = 8k, output sequence length = 256, assuming a 4x 8-mode MI350 series proxy (CDNA4) vs. 8x MI300X per GPU performance comparison. Actual performance will vary based on factors including but not limited to final specifications of production silicon, system configuration and inference model and size used.</p>    <p><sup>2</sup> MI300-54: Testing completed on 05/28/2024 by AMD performance lab attempting text generated Llama3-70B using batch size 1 and 2048 input tokens and 128 output tokens for each system.</p>    <p>Configurations: </p>    <p>Server manufacturers may vary configurations, yielding different results. Performance may vary based on use of latest drivers and optimizations.</p>    <p>Configurations: </p>    <p>Only 1 GPU on each system was used in this test.</p>    <p>Server manufacturers may vary configurations, yielding different results. Performance may vary based on use of latest drivers and optimizations.</p>    <p>The highest published results on the NVidia Hopper H200 (141GB) SXM GPU accelerator resulted in 141GB HBM3e memory capacity and 4.8 TB/s GPU memory bandwidth performance. </p>    <p>The highest published results on the NVidia Blackwell HGX B100 (192GB) 700W GPU accelerator resulted in 192GB HBM3e memory capacity and 8 TB/s GPU memory bandwidth performance.     </p>    <p>https://resources.nvidia.com/en-us-blackwell-architecture?_gl=1*1r4pme7*_gcl_aw*R0NMLjE3MTM5NjQ3NTAuQ2p3S0NBancyNkt4we know QmhCREVpd0F1NktYdDlweXY1dlUtaHNKNmhPdHM4UVdPSlM3dFdQaE40WkI4THZBaWFVajFyTGhYd3hLQmlZQ3pCb0NsVElRQXZEX0J3RQ..*_gcl_au*MTIwNjg4NjU0Ny4xNzExMDM1NTQ3</p>    <p>The highest published results on the NVidia Blackwell HGX B200 (192GB) GPU accelerator resulted in 192GB HBM3e memory capacity and 8 TB/s GPU memory bandwidth performance.</p>    <p>https://resources.nvidia.com/en-us-blackwell-architecture?_gl=1*1r4pme7*_gcl_aw*R0NMLjE3MTM5NjQ3NTAuQ2p3S0NBancyNkt4QmhCREVpd0F1NktYdDlweXY1dlUtaHNKNmhPdHM4UVdPSlM3dFdQaE40WkI4THZBaWFVajFyTGhYd3hLQmlZQ3pCb0NsVElRQXZEX0J3RQ..*_gcl_au*MTIwNjg4NjU0Ny4xNzExMDM1NTQ3</p>    <p><sup>5</sup>MI300-49: Calculations conducted by AMD Performance Labs as of May 28th, 2024 for the AMD Instinct™ MI325X GPU resulted in 1307.4 TFLOPS peak theoretical half precision (FP16), 1307.4 TFLOPS peak theoretical Bfloat16 format precision (BF16), 2614.9 TFLOPS peak theoretical 8-bit precision (FP8), 2614.9 TOPs INT8 floating-point performance. Actual performance will vary based on final specifications and system configuration.</p>    <p>Published results on Nvidia H200 SXM (141GB) GPU: 989.4 TFLOPS peak theoretical half precision tensor (FP16 Tensor), 989.4 TFLOPS peak theoretical Bfloat16 tensor format precision (BF16 Tensor), 1,978.9 TFLOPS peak theoretical 8-bit precision (FP8), 1,978.9 TOPs peak theoretical INT8 floating-point performance. BFLOAT16 Tensor Core, FP16 Tensor Core, FP8 Tensor Core and INT8 Tensor Core performance were published by Nvidia using sparsity; for the purposes of comparison, AMD converted these numbers to non-sparsity/dense by dividing by 2, and these numbers appear above. </p>    <p>Nvidia H200 source:  https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446 and https://www.anandtech.com/show/21136/nvidia-at-sc23-h200-accelerator-with-hbm3e-and-jupiter-supercomputer-for-2024 </p>    <p>Note: Nvidia H200 GPUs have the same published FLOPs performance as H100 products https://resources.nvidia.com/en-us-tensor-core/ </p> <img src="https://www.globenewswire.com/newsroom/ti?nf=OTE0NzM2NSM2Mjk5MDk2IzIwMDcxMTY="/> </article>
    	                        </div>
                    </main>
                                </div>
            </div></div>
  </body>
</html>
