<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tratt.net/laurie/blog/2023/four_kinds_of_optimisation.html">Original</a>
    <h1>Four Kinds of Optimisation</h1>
    
    <div id="readability-page-1" class="page"><div id="article-body"><p>



Premature optimisation might be the root of all evil, but
overdue optimisation is the root of all frustration. No matter
how fast hardware becomes, we find it easy to write programs which run too
slow. Often this is not immediately apparent. Users can go for years
without considering a program&#39;s performance to be an issue before it
suddenly becomes so — often in the space of a single working day.

</p><p>I have devoted more of my life to optimisation than I care to think about,
and that experience has led me to make two observations:

</p><ol>
<li>Human optimism leads us to believe that we can easily know where a program
spends most of its time.

</li><li>Human optimism leads us to believe that we can easily know how to make the
slow parts of a program run faster.
</li></ol>
<p>You will not be surprised to learn that I think both forms of
optimism misplaced. Partly this is because, as hardware and software have
become more sophisticated, it has become harder to understand their effects
on performance. But, perhaps more fundamentally, we tend to overestimate how
much we know about the software we&#39;re working on. We overemphasise the parts of
the system we&#39;ve personally worked on, particularly those we&#39;ve most recently
worked on. We downplay other parts of the system, including the impact of
dependencies (e.g. libraries).

</p><p>The solution to the first of these observations is fairly widely known — one
should rigorously profile a program before assuming one knows where it is
spending the majority of its time. I deliberately say &#34;rigorously profile&#34; because people
often confuse &#34;I have profiled a program once&#34; with &#34;I have built up a good
model of a program&#39;s performance in a variety of situations&#34;. Sometimes, a
quick profiling job is adequate, but it can also mislead. Often it is necessary
to profile a program with different inputs, sometimes on different machines or
network configurations, and to use a variety of sampling and non-sampling
approaches .

</p><p>However, the multiple solutions, and their inevitable trade-offs, to the
second observation are, I believe, underappreciated. I tend to think that
there are four main solutions:

</p><ol>
<li>Use a better algorithm.
</li><li>Use a better data-structure.
</li><li>Use a lower-level system.
</li><li>Accept a less precise solution.
</li></ol><p>

In the rest of this post I&#39;m going to go through each of these and give some
suggestions for the trade-offs involved.


</p><h3>Use a better algorithm</h3><p>

Let&#39;s imagine – and I&#39;ve genuinely seen this happen! – that after
careful profiling of a Python program, I find that I&#39;m spending most of my time
in a function which looks like this:

</p><pre><code>def f1(l):
  while True:
    c = False
    for i in range(0, len(l) - 1):
      if l[i+1] &lt; l[i]:
        t = l[i]
        l[i] = l[i+1]
        l[i+1] = t
        c = True
    if not c: return l</code></pre><p>

It&#39;s a <a href="https://en.wikipedia.org/wiki/Bubble_sort">bubble sort</a>! At
this point, many people will start guffawing, because it&#39;s an obviously slow
way of sorting elements. However, bubble sort has an often-forgotten advantage
over many &#34;better&#34; algorithms: it runs in constant memory . I could
gamble that my program doesn&#39;t need to use constant memory, but if I&#39;m unsure,
I can use an alternative algorithm which preserves this property. Let&#39;s
try a <a href="https://en.wikipedia.org/wiki/Selection_sort">selection sort</a>:

</p><pre><code>def f2(l):
  for i in range(0, len(l) - 1):
    m = i
    for j in range(i + 1, len(l)):
      if l[j] &lt; l[m]: m = j
    if m != i:
      t = l[i]
      l[i] = l[m]
      l[m] = t
  return l</code>
</pre><p>

If I use this quick testing code:

</p><pre><code>import random, time
l = [random.random() for _ in range(1000)]
before = time.time()
l1 = f1(l[:])
print(time.time() - before)
before = time.time()
l2 = f2(l[:])
print(time.time() - before)</code>
</pre><p>

and run it on CPython 3.11 on a Linux server I consistently get timings along the
lines of:

</p><pre>0.0643463134765625
0.020025014877319336
</pre><p>

In other words, selection sort is about three times faster than bubble sort
in my test.

</p><p>You don&#39;t need me to tell you that selection sort isn&#39;t the fastest
possible sorting algorithm, but &#34;fastest&#34; is a more slippery concept than it first
appears. For example, the selection sort algorithm above is faster than the bubble
sort for random data, but the bubble sort is much faster for sorted data . The relationship between inputs and algorithmic performance
can be subtle. Famously, if you choose an unfortunate
&#34;pivot&#34; when implementing <a href="https://en.wikipedia.org/wiki/Quicksort">quicksort</a>,
you&#39;ll find that it is very non-quick (e.g. you can make it as slow on already-sorted data
as the selection sort above).

</p><p>We can generalise from this that &#34;use a better algorithm&#34; requires
understanding the wider context of your system and the nature of the algorithm
you&#39;re thinking of using. For example, I&#39;ve often seen people conflate an
algorithm&#39;s best-case, average-case, and worst-case performance — but the
differences between those three pieces of information can be vital when I&#39;m
optimising a program. Sometimes I might know something about my program (e.g.
the nature of its inputs) that makes me confident that the worst case can&#39;t happen, or I
don&#39;t consider the worst case to be a problem (e.g. its a batch job and no-one will
notice occasional latency). But, generally, I care more about the worst case
than the best case, and I select algorithms accordingly.

</p><p>It&#39;s also not uncommon
that algorithms that have good theoretical performance have poor real-world
performance (big O notation can hide many sins). If in doubt, I try
gradually more test data until I feel I have truly understood the practical
consequences of different choices.

</p><p>It&#39;s also easy to overlook complexity. Fundamentally, faster algorithms are faster because they observe that some
steps in a calculation can be side-stepped. I can still remember the
first time I read the <a href="https://github.com/python/cpython/blob/main/Objects/listsort.txt">description
for timsort</a>: the beauty of its algorithmic observations has stayed with me
ever since. But verifying those observations is harder than we imagine —
even timsort, created by one of the greatest programmers I have ever come
across, had a subtle bug in it .

</p><p>When us mortals implement faster algorithms, they are
often slightly incorrect, particularly when newly implemented,
either producing wrong results or not having the expected performance
characteristics .
For example, parallelising an algorithm can often lead to huge speedups,
particularly as CPUs gain more cores, but how many of us understand
the C11 memory model well enough to feel confident of the consequences or parallelisation?

</p><p>The combination of (in)correctness and the difficulty in understanding the
context in which an algorithm is fast means that I frequently encourage people
to start with a simple algorithm and only move to something &#34;faster&#34; if they
really find they need to. Picking (and, if necessary, implementing) the right
algorithm for the task at hand is a surprisingly difficult skill!


</p><h3>Use a better data-structure</h3><p>

Let&#39;s imagine that I profile another program and find that I spend most of my time
in the following function:

</p><pre><code>def f3(l, e):
  for x in l:
    if x == e: return True
  return False</code>
</pre><p>

It&#39;s an existence check function! Optimising these can be quite interesting,
because my choices will depend on how the lists passed to this function are
used. I could change the list to a <a href="">binary tree</a>, for example. But
if I can tell, as is not uncommon, that we repeatedly check for the existence
of elements in a list that is never mutated after initial creation, I might be
able to get away with a very simple data-structure: a sorted list. That might
sound odd, because &#34;sorted list&#34; doesn&#39;t sound like much of a data-structure,
but that then allows me to do a <a href="https://en.wikipedia.org/wiki/Binary_search_algorithm">binary search</a>.
For anything but the smallest lists , binary
search is much quicker than the linear search above.

</p><p>Just as with &#34;use a better
algorithm&#34;, &#34;use a better data-structure&#34; requires careful thought and measurement
. In general, while I often find it necessary to implement my own &#34;better
algorithms&#34;, I rarely find it necessary to implement my own &#34;better
data-structures&#34;. Partly this is laziness on my part, but it&#39;s mostly because
data-structures are more easily packaged in a library than better algorithms
.

</p><p>There is an important tactical variant on &#34;better data-structures&#34; that is perhaps
best thought of as &#34;put your structs/classes on a diet&#34;. If a program is
allocating vast numbers of a given struct/class, the size of that struct/class
in bytes can become a significant cost in its own right. When I was working on <a href="https://tratt.net/laurie/blog/2020/automatic_syntax_error_recovery.html">error
recovery in grmtools</a>, I found that simply reducing the most commonly allocated
struct by 8 bytes in size improved total program performance by 5% — a
trick that, from memory, I repeated twice!

</p><p>There are many similar tactics to this,
for example reducing &#34;pointer chasing&#34; (typically by folding multiple structs/classes
into one), encouraging memory locality and so on. However, while it&#39;s easy to
measure the size of a struct/class and how often it&#39;s allocated, it&#39;s
difficult to measure the indirect impact of things like memory locality —
I have heard such factors blamed for poor performance much more often than I have
seen such factors proven as responsible for poor performance. In general, I
only look to such factors when I&#39;m getting desperate.


</p><h3>Use a lower-level system</h3><p>

A time-honoured tradition is to rewrite parts of a program in a lower-level
programming language. Let&#39;s rewrite our Python bubble sort into Rust:

</p><pre><code>use std::cmp::PartialOrd;
fn f1&lt;T: Copy + PartialOrd&gt;(l: &amp;mut Vec&lt;T&gt;) {
  loop {
    let mut c = false;
    for i in 0..l.len() - 1 {
      if l[i + 1] &lt; l[i] {
        let t = l[i];
        l[i] = l[i + 1];
        l[i + 1] = t;
        c = true;
      }
    }
    if !c {
      return;
    }
  }
}</code>
</pre><p>

I mildly adopted my Python program from earlier to save out 1000 random
floating point numbers, and added this testing code in Rust:

</p><pre><code>use {env::args, fs::read_to_string, time::Instant};
fn main() {
  let mut l = read_to_string(args().nth(1).unwrap())
    .unwrap()
    .lines()
    .map(|x| x.parse::<f64>().unwrap())
    .collect::<vec<_>&gt;();
  let before = Instant::now();
  f1(&amp;mut l);
  println!(&#34;{}&#34;, (Instant::now() - before).as_secs_f64());
}</vec<_></f64></code>
</pre><p>

My Rust bubble sort runs in 0.001s, about 60x faster than the Python version.
This looks like a great success for &#34;rewrite in a lower-level programming
language&#34; — but you may have noticed that I titled this section &#34;Use a
lower-level system&#34;.

</p><p>Instead of spending 15 minutes writing the Rust code, it would have
been smarter of me to recognise that my Python bubble sort is likely
to emphasise CPython&#39;s (the most common implementation of Python) weaknesses.
In particular, CPython will represent what I conceptually thought of as a list
of floating point numbers as an array of pointers to individually
heap-allocated Python objects. That representation has the virtue of
generality, but not efficiency.

</p><p>Although it&#39;s often forgotten, CPython isn&#39;t the only implementation of
Python. Amongst the alternatives is <a href="https://pypy.org">PyPy</a>,
which just so happens to
<a href="https://tratt.net/laurie/research/pubs/html/bolz_diekmann_tratt__storage_strategies_for_collections_in_dynamically_typed_languages/">represent lists
of floats as efficiently as Rust</a>. Simply typing
<code>pypy</code> instead of <code>python</code> speeds my bubble sort up by
4x! There are few changes I can make that give me such a big performance
improvement for such little effort. That&#39;s not to say that PyPy runs my program
as fast as Rust (PyPy is still about 15x slower) but it may well be fast
enough, which is what really matters.

</p><p>I have seen multiple organisations
make the mistake of trying to solve performance problems by
rewriting their software in lower-level programming languages, when they would
have got sufficient benefit from working out how to run their existing software
a little faster. There are often multiple things one can do here, from using
different language implementations, to checking that you&#39;ve got compiler
optimisations turned on , to using faster libraries
or databases, and so on. Sometimes rewriting in a lower-level programming language
really is the right thing to do, but it is rarely a quick job, and it
inevitably introduces a period of instability while bugs are shaken out of the
new version.


</p><h3>Accept a less precise solution</h3><p>

A common problem we face is that we have <em>n</em> elements of something and
we want to understand the best subset or ordering of those for our situation.
Let&#39;s imagine that I&#39;ve implemented a compiler and 30 separate optimisation
passes. I know that some optimisation passes are more effective if they run
after other optimisation passes, but I don&#39;t know what the most effective ordering of
all the passes is.

</p><p>I could write a program to enumerate all the
permutations of those 30 passes, run them against a benchmark suite I possess,
and then select the fastest permutation. But if my benchmark suite takes 1 second
to run then it will take roughly 2<sup>82</sup> years to evaluate all the
possibilities — which is rather longer than the current age of the
universe. Clearly I can&#39;t wait that long for an answer: I can only run a
subset of all the permutations. In situations such as this, I have to accept
that I&#39;ll never be able to know for sure what the best possible answer is: but,
that said, I can at least make sure I end up with a better answer than not
trying anything at all.

</p><p>There are various ways of tackling this but most boil down to <a href="https://en.wikipedia.org/wiki/Local_search_(optimization)">local
search</a>. In essence, we define a metric (in our running example, how fast
our benchmark suite runs) that allows us to compare two solutions (in our case,
faster is better) and discard the worst. We then need a way of generating a
neighbour solution to the one we already have, at which point we recalculate
the metric and discard the worse of the old and new solution. After either a
fixed time-limit, or if we can&#39;t find solutions which improve our metric, we
return the best solution we&#39;ve found. The effectiveness of this simple
technique (the core algorithm is a few lines of code) tends to stun newcomers,
since the obvious problem of <a href="https://en.wikipedia.org/wiki/Local_optimum">local optima</a>
seems like it should undermine the whole idea.

</p><p>As typically implemented, local search as I&#39;ve outlined
it above produces correct but possibly non-optimal solutions. Sometimes,
however, we&#39;re prepared to accept an answer which is less precise in the
sense that it is possibly &#34;incorrect&#34;. By this I don&#39;t mean that the
program is buggy, but that the program may deliberately
produce outputs that do not fully match what we would consider the &#34;full and proper&#34; answer.

</p><p>Exactly what constitutes &#34;correct&#34;
varies from one situation to another. For example,
<a href="https://en.wikipedia.org/wiki/Fast_inverse_square_root">fast inverse
square root</a> approximates multiplicative inverse: for
situations such as games, its fast nearly-correct answer is a
better trade-off than a slow definitely-correct answer. A <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a> can give false
positives: accepting that possibility allows it to be exceptionally
frugal with memory. JPEG image compression
deliberately throws away some of an image&#39;s fine details in order to make the
image more compressible. Unlike other image compression approaches I cannot
recover the original imagine perfectly from a JPEG, but by foregoing a little
bit of image quality, I end up with much smaller files to transmit.

</p><p>I think that, in general, most programmers struggle to accept that
correctness can sometimes be traded-off — personally, it offends a
deep internal conviction of mine that programs should be correct. Probably
because of that, I think the technique is used less often than it
should be.

</p><p>Recently, though, we&#39;ve become much more willing to accept incorrect
answers thanks to the explosion of ML (Machine Learning). Whereas local search
requires us to explicitly state how to create new solutions, ML is trained
on previous data, and then generates new solutions from that data. This can
be a very powerful technique, but ML&#39;s inevitable &#34;hallucinations&#34; are
really just a form of incorrectness.

</p><p>We can thus see that there are two different ways of accepting imprecise
solutions: possibly non-optimal; and possibly incorrect. I&#39;ve come to realise
that many people think they&#39;re the same thing, but possible incorrectness more
often causes problems. I might be happy trading off a bit of
image-quality for better compression, but if an ML system rewrites my code and
leaves off a &#34;not&#34; I&#39;m unhappy. My rule of thumb is that unless you are
convinced you can tolerate incorrectness, you&#39;re best off assuming that you
can&#39;t.


</p><h3>Summary</h3><p>

I&#39;ve listed the four optimisation approaches above in the frequency with which I&#39;ve
seen them used (from most to least used).

</p><p>It will probably not surprise you that my least favourite approach is
&#34;rewrite in a lower-level programming language&#34;, in the sense that it tends to
offer the poorest ratio of improvement/cost. That doesn&#39;t mean that it&#39;s always
the wrong approach, but we tend to reach for it before we&#39;ve adequately
considered cheaper alternatives. In contrast, I think that until recently we
have too rarely reached for &#34;accept a less precise solution&#34;, though the ML
explosion has rapidly changed that.

</p><p>Personally, when I&#39;m trying to optimise a program I tend to reach for the
simplest tricks first. One thing that I&#39;ve found surprises people is how often
my first attempt at optimisation will be to hunt for places to use hashmaps — only rarely
do I go hunting for exotic data-structures to use. I less often turn to clever
algorithms. Of those clever algorithms I tend to implement myself, I suspect
that binary search is the one I use the most often, and I probably do so at
most once or twice a year — each time I implement it, I have to look up the
correct way to do so !

</p><p>Ultimately, having written this post, I&#39;ve come to realise that there are
three lessons that cut across all of the approaches.

</p><p>First, when correctness can be sacrificed for performance, it&#39;s a powerful
technique — but we often sacrifice correctness for performance
unintentionally. When we need to optimise a program, it&#39;s best to use the least
complex optimisation that will give us the performance we want, because that&#39;s
likely to introduce the fewest bugs.

</p><p>Second, human time matters. Because us programmers enjoy complexity so much,
it&#39;s tempting for us to reach for the complex optimisations too soon. Even if
they succeed in improving performance – which they often don&#39;t! – they tend to consume much
more time than is necessary for the performance improvement we needed.

</p><p>Third, I think that breadth of optimisation knowledge is more important than
depth of optimisation knowledge. Within each of the approaches I&#39;ve listed in
this post I have a couple of tricks that I regularly deploy. That has helped
give me a reasonable intuition about what the most appropriate overall approach
to my current performance woes might be, even if I don&#39;t know the specifics.

</p><p><b>Acknowledgements:</b> Thanks to
<a href="https://cfbolz.de/">Carl Friedrich Bolz-Tereick</a>, and
<a href="https://jakehughes.uk/">Jake Hughes</a>
for comments.

</p><p><b>Update (2023-11-14)</b>: My original phrasing of a Bloom filter could be
read in a way that seemed to be a contradiction. I&#39;ve tweak the phrasing to
avoid this.

</p>



<h3>Footnotes</h3>
<p><a name="12846661"><span>[1]</span>Often, when I&#39;m first looking at performance, I only need a rough sense of
what&#39;s going on, as I narrow down what parts I need to investigate in detail. I
often start with normal Unix <code>time</code>, move to something like </a><a href="https://tratt.net/laurie/src/multitime">multitime</a>, then perhaps a sampling profiler
like Linux&#39;x <code>perf</code>, before finally using a more detailed profiler
like Valgrind. Each of these tools takes a bit more time to run and understand
than the previous, which is why I don&#39;t start with (say) Valgrind.

</p><p>A harder challenge is what inputs to use. It is always tempting to benchmark
a program with a single input, and I fall into that trap often, despite knowing
that it is a trap.
</p>


<h3>Comments</h3>



</div></div>
  </body>
</html>
