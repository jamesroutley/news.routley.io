<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ar5iv.labs.arxiv.org/html/2304.06035">Original</a>
    <h1>Choose your weapon: Survival strategies for depressed AI academics</h1>
    
    <div id="readability-page-1" class="page"><div>
<div>
<article lang="en">

<p><span>
<span>
Julian Togelius, <em id="id3.1.id1">Senior Member, IEEE</em>,
Georgios N. Yannakakis, <em id="id4.2.id2">Fellow, IEEE</em>

<br/>
</span><span>JT is with New York University, GNY is with University of Malta. The word “depressed” in the title does not refer to either the clinical or economic concept of depression, but rather the word’s everyday use as signifying an unhappy and/or hopeless state of mind.</span></span>
</p>

<section id="S1">
<h2>
<span>I </span><span id="S1.1.1">Introduction</span>
</h2>

<p id="S1.p1.1">As someone who does Artificial Intelligence (AI) research in a university, you develop a complicated relationship to the corporate AI research powerhouses, such as Googe DeepMind, OpenAI, and Meta AI. Whenever you see one of these papers that train some kind of gigantic neural net model to do something you were not even sure a neural network could do, unquestionably pushing the state of the art and reconfiguring your ideas of what is possible, you get conflicting emotions. On the one hand: it is very impressive. Good on you for pushing AI forward. On the other hand: how could we possibly keep up? As an AI academic, leading a lab with a few PhD students and (if you’re lucky) some postdoctoral fellows, perhaps with a few dozen Graphics Processing Units (GPUs) in your lab, this kind of research is simply not possible to do.</p>
<p id="S1.p2.1">To be clear, this was not always the case. As recently as ten years ago, if you had a decent desktop computer and an internet connection you had everything you needed to compete with the best of researchers out there. Ground-breaking papers were often written by one or two people who ran all the experiments on their regular workstations. It is useful to point this out particularly for those who have come into the research field within the last decade, and for which the need for gigantic compute resources is a given.</p>
<p id="S1.p3.1">If we have learned one thing from deep learning <cite>[<a href="#bib.bib9" title="">9</a>]</cite>, it is that scaling works. From the ImageNet <cite>[<a href="#bib.bib19" title="">19</a>]</cite> competitions and their various winners to ChatGPT, Gato <cite>[<a href="#bib.bib17" title="">17</a>]</cite>, and most recently to GPT-4 <cite>[<a href="#bib.bib1" title="">1</a>]</cite>, we have seen that more data and more compute yield quantitatively and often even qualitatively better results. (By the time you are reading this, that list of very recent AI milestones might very well be outdated.). Of course there are improvements to learning algorithms and network architectures as well, but these improvements are mostly useful in the context of the massive scale of experiments. (Sutton talks about the “Bitter Pill”, referring to the insight that simple methods that scale well always win the day when more compute becomes available <cite>[<a href="#bib.bib22" title="">22</a>]</cite>.) A scale that is not achievable by academic researchers nowadays. As far as we can tell, the gap between the amount of compute available to ordinary researchers and the amount available to stay competitive is growing every year.</p>
<p id="S1.p4.1">This goes a long way to explain the resentment that many AI researchers in academia feel towards these companies. Healthy competition from your peers is one thing, but competition from someone that has so much resources that they can easily do things you could never do, no matter how good your ideas are, is another thing. When you have been working on a research topic for a while and, say, DeepMind or OpenAI decides to work on the same thing, you will likely feel the same way as the owner of a small-town general store feels when Walmart sets up shop next door. Which is sad, because we want to believe in research as an open and collaborative endeavor where everybody gets their contribution recognized, don’t we?</p>
<p id="S1.p5.1">So, if you are but a Professor, with a limited team size and limited compute resources, what can you do to stay relevant in face of the onslaught of incredibly well-funded research companies? This is a question that has been troubling us and many of our colleagues for years now. Recent events, with models such as GPT-4 being shockingly capable and shockingly closed-sourced and devoid of published details, has made the question even more urgent. We have heard from multiple researchers at various levels of seniority, both in-person and via social media, who worry about the prospects of doing meaningful research given the lack of resources and the unfair competition from big tech companies.</p>
<p id="S1.p6.1">Let us make this clear at the outset: both of us are secure. We hold tenured academic Professorships and we rose up on the academic ladder pretty fast, in part because of finding an academic niche: we systematically pushed the envelope of AI in the domain of video games. While we obviously care about continuing to do relevant AI research ourselves, we are writing this mostly for our more junior colleagues, postdocs and doctoral students, who may wonder about which career path to choose. Is it worthwhile to go into academia, or is it better to join a big tech company, or maybe kick off a startup? Is a career in AI a good idea, or is it better to become a plumber? Should you be a cog in the machinery, or a rebel? (It’s usually easier to be a rebel when you have nothing to lose, which is either at the beginning of your career or when you have tenure.) As skilled as one may be, is this glorious battle to stay competitive lost already? Are we about to lie here, obedient to our laws? This Point of View article is partly meant as serious advice, and partly as emotional encouragement, but perhaps most of all to start a discussion with all of you so we improve our position as academics before the battle is long lost. We do not wish to stop the evolution of AI technology (even if we could); quite the contrary: we wish to discuss the strategies that will equip as many as possible to be part of this journey. While the challenges are real and many, we both feel the are even more opportunities and the time is right to grab them!</p>
<p id="S1.p7.1">In the remainder of this article we list a number of ideas (or strategies) for what to do if you are an AI academic despairing about your options. These options are presented in no particular order. We also don’t make any particular recommendations here or ranking the options for you. It is up to you to pick one, more than one, or none of them as your favourite direction. Towards the end of the article, however, we discuss what big tech companies and universities can do to help the situation. There, we make some specific suggestions.</p>
</section>
<section id="S2">
<h2>
<span>II </span><span id="S2.1.1">Give Up!</span>
</h2>

<p id="S2.p1.1">Giving up is always an option. Not giving up on doing research, but giving up on doing things that are really impactful and pushing the envelope. There are still plenty of technical details and sub-sub-questions to publish papers about in mid-tier journals and conferences. Please note, however: (1) This works best if you already have a secure permanent position and you do not care much about promotions, (2) this wasn’t really what you dreamed of doing when you decided on a research career, right? Forcing yourself to reframe your research agenda because of this fierce competition is similar to adjusting your research to the priorities of funding bodies like the European Commission or US National Science Foundation. At least going for the latter might secure some funding for your lab which can, in turn, help you work with some talented AI researchers and doctoral students. It is important to note that we both consider ourselves lucky enough as we have coordinated or have been part of several small- and large-scale research projects that allowed us to support our research agendas and helped us (in part) to secure our positions.</p>
</section>
<section id="S3">
<h2>
<span>III </span><span id="S3.1.1">Try Scaling Anyway</span>
</h2>

<p id="S3.p1.1">Going head-to-head with an overwhelming competition is an admirable sentiment. If scaling works, let’s do it in our university labs! Let’s go tilting at windmills (GPU fans)!</p>
<p id="S3.p2.1">The most obvious problem is access to central processing units (CPUs) and GPUs. So, let’s say you secure $50k of funding for cloud compute from somewhere and go ahead running your big experiment. But this is a very small amount of money compared to what training something like GPT-3 costs. The recent open AI agent that learned to craft a diamond pickaxe in Minecraft required training of 9 days on 720 V100 GPUs <cite>[<a href="#bib.bib2" title="">2</a>]</cite>; this amounts to a few hundred thousand dollars for a single experiment. Not even prestigious European Research Council (EU) or National Science Foundation (US) grants can support such a level of investment. Still, spending $50k on cloud compute will give you significantly more compute than a bunch of gaming PCs taped together, so you could scale at least a little bit. At least for that very experiment. But as we all know, most experiments don’t work the first time you try them. For every big successful experiment we see reported, we have unreported months or maybe years of prototypes, proofs of concept, debugging, parameter tuning, and failed starts. You need this level of compute available constantly.</p>
<p id="S3.p3.1">The less obvious problem is that you need the right kind of team to build experimental software that scales, and that is generally not compatible with academic career structures. Most of the members of a typical academic research lab in computer science are PhD students that need to graduate within a few years, and need to have an individual project to work on which results in multiple first-author papers so they can get a job afterwards. A large-scale AI project typically means that most members of the team work for many months or years on the same project, where only one of them can be the first author on the paper. The team will probably also include people who do “mundane” software engineering tasks that are crucial to the success of the project, but which are not seen as AI research in themselves. The structures needed for successful large scale projects are simply not compatible with the structures of academia.</p>
</section>
<section id="S4">
<h2>
<span>IV </span><span id="S4.1.1">Scale Down</span>
</h2>

<p id="S4.p1.1">One popular way to bypass the issue is to focus on simple yet representative (toy) problems that will either prove the benefits of a new approach theoretically or showcase the comparative advantages of a novel method. Indicatively, a recent paper on Behaviour Transformers <cite>[<a href="#bib.bib21" title="">21</a>]</cite> showcased the benefits of the method on a toy navigation task that only took a simple multi layer perceptron to solve. A similar approach was later used in <cite>[<a href="#bib.bib15" title="">15</a>]</cite>. Both studies will likely be impactful despite the limited scale because they demonstrated the capacity of the algorithms in popular game and robotic benchmark problems that require large models and significant compute to train. In <cite>[<a href="#bib.bib14" title="">14</a>]</cite> we observe the same pattern once again: a case is made in a toy (gambling) environment but the impact, one would argue, comes from the comparative advantages the algorithm shows in more complex but computationally heavy problems.</p>
<p id="S4.p2.1">A downside with this approach is that people are wowed by pretty colors in high resolution, and take a real car navigating a road more seriously than a toy car, even though the challenges may be the same. So you will get less media exposure, perhaps less funding. There are also domains, such as language, which are very hard to scale down beyond some limit.</p>
</section>
<section id="S5">
<h2>
<span>V </span><span id="S5.1.1">Reuse and Remaster</span>
</h2>

<p id="S5.p1.1">A key reason that AI has advanced so rapidly over the last decade is that researchers make their code and models available to the scientific community. Model sharing and code accessibility was neither the norm nor the priority of AI researchers back in the days. Having access to pretrained large models like ViT in vision <cite>[<a href="#bib.bib4" title="">4</a>]</cite> or the Llama family for text <cite>[<a href="#bib.bib25" title="">25</a>]</cite> saves you time and effort as you can simply resue them, and fine-tune them for your own specific problem. Arguably, one needs to assume that the representations of those large models is general enough to be able to perform well to your downstream task with limited training. Unfortunately the fine-tuning and post-hoc analysis of a large model is sometimes not enough for good performance, especially if your domain is quite different from what they were pre-trained for. Relying on pre-trained models is therefore limiting the scope of research you can do.</p>
</section>
<section id="S6">
<h2>
<span>VI </span><span id="S6.1.1">Analysis Instead of Synthesis</span>
</h2>

<p id="S6.p1.1">Another thing one can do with the publicly available pretrained models is to analyze them. While this may not directly contribute to new capabilities, it can still make scientific progress. The current state of things is that we have great models for text and image generation publicly available, but we don’t understand them very well. You could even argue that we barely understand them all. Let’s face it: a transformer is not an intuitive thing to anyone, and the scale of data these models are trained on is almost incomprehensible in itself. There is plenty of work to do in analyzing them, for example by probing them in creative ways, and developing visualizations and conceptual machinery to help us understand them.</p>
<p id="S6.p2.1">One can do analysis with different mindsets. Trying to find and describe specific circuits and mechanisms that have been learnt is useful, and can help us (well, someone else, with resources) to create better models in the future. But one can also play the role of the gadfly, incessantly finding ways to break them! This is scientifically and societally valuable, no matter what those who try to make a business out of large models say. But it might not be the kind of research you want to do.</p>
</section>
<section id="S7">
<h2>
<span>VII </span><span id="S7.1.1">RL! No Data!</span>
</h2>

<p id="S7.p1.1">One might scale down one’s requirements with respect to data and instead approach AI problems through the lens of (online) reinforcement learning (RL). Following the RL path might allow you to bypass issues related to data availability, analysis, storage and handling; it does not however minimize the computational effort required necessarily. In fact, even the most efficient RL methods are known to be computationally heavy as the very process of exploration is costly. Moreover, shaping a reward function often involves forms of black art (informally) or practical wisdom (more formally). That is, a researcher often needs to continuously run lengthy experiments with different types of reward (among other hyperparameters) for a breakthrough result. So ultimately one has to downscale the complexity of the problem once again. The bottom line is that if you want to break free from large data sets you might be still faced with large compute requirements unless you work on simple (toy) problems, specialized domains, or work with small models; the next section is dedicated to the latter strategy.</p>
</section>
<section id="S8">
<h2>
<span>VIII </span><span id="S8.1.1">Small Models! No Compute!</span>
</h2>

<p id="S8.p1.1">Another valid strategy is to compromise on model scale to save on compute. There are many circumstances where you want or need a smaller model. Think of the smallest possible models that are capable of solving a problem or completing a task. This is particularly important to and relevant for real-world applications. <em id="S8.p1.1.1">In-the-wild</em> domains such as games, internet of things, and autonomous vehicles could allow AI to be deployed next to their end user and the data the user generates, i.e. at the edge of the network. This is often called <em id="S8.p1.1.2">edge AI</em> <cite>[<a href="#bib.bib10" title="">10</a>]</cite>, the operation of AI applications in devices of the physical world is possible when memory requirements are low and inference occurs rapidly. Neuroevolution and neural architecture search <cite>[<a href="#bib.bib10" title="">10</a>]</cite>, and knowledge distillation <cite>[<a href="#bib.bib6" title="">6</a>, <a href="#bib.bib13" title="">13</a>]</cite> methods are only a few of the available methods for edge AI. Note that beyond learning more from smaller models one could also attempt to learn more from less data <cite>[<a href="#bib.bib7" title="">7</a>]</cite>.
Following this research path may lead to significant into models’ inner workings. Studying small AI models makes the analysis far easier and increases the explainability of whatever the model does. Moreover, deploying such models on devices helps with privacy concerns. You can also argue for small models from the perspective of <em id="S8.p1.1.3">green AI</em> <cite>[<a href="#bib.bib20" title="">20</a>]</cite> , as it minimizes the environmental footprint of the research. Obviously there are limits to what a small model is capable of doing but the importance of this research direction, we feel, will be growing drastically over the years.</p>
</section>
<section id="S9">
<h2>
<span>IX </span><span id="S9.1.1">Work on Specialized Application Areas or Domains</span>
</h2>

<p id="S9.p1.1">One rather efficient strategy is to pick a niche but somewhat established area of research––that is likely beyond the immediate interest of the industry—and try to innovate within and through that area. It is often a successful strategy to bring and test your ideas to an entirely new domain but it is less often that the outcomes will have a large impact beyond that domain. There are plenty of examples of niche areas eventually becoming dominant due to the push of a few dedicated researchers. We are both currently mostly taking this strategy: we have the AI for games community as primary scientific community where we can perform state-of-art work, as few large companies put serious efforts into modern AI for games.</p>
<p id="S9.p2.1">Think of video games as a domain that penetrated the research communities of robotics and computer vision back in early 00s, and again with video games as deep RL benchmarks after 2015. Think of neural networks and deep learning methods that came to dominate communities invested in support vector machines and regression models (e.g. NeurIPS a decade ago). Also think of the ways reinforcement learning and deep learning have altered the core principles of multi-agent learning and cognitive/affect modeling in communities represented by the AAMAS, ACII and IVA conferences, for instance.</p>
<p id="S9.p3.1">A core downside to this strategy is the difficulty getting your paper accepted in the kind of large venues that are most influential in AI, such as NeurIPS, AAAI, ICML and IJCAI. Your paper and its results might end up sitting out-of-the-interest-distribution. It is, however, very possible to start your own community with its own publication venues.</p>
<p id="S9.p4.1">If you do not have the requisite domain expertise—and/or datasets—yourself, you can fruitfully approach domain experts to collaborate. The good news is that as an academic, you have plenty of such experts in other departments of your university or institute and they all have interesting AI problems to solve if you spend some time talking to them. One of the authors recently ran in to an anthropologist and an analytical chemist in a corridor, and started discussing projects that would include all three. Another example is a recent collaboration of one of the authors with urban designers resulting in the reconstruction of urban areas around MIT and Harvard for improving the comfort levels of Bostonians <cite>[<a href="#bib.bib5" title="">5</a>]</cite>.</p>
<p id="S9.p5.1">These projects may not end up advancing the state of AI much, but may make big differences in the particular disciplines. And sometimes big AI advances come from application-specific work.</p>
</section>
<section id="S10">
<h2>
<span>X </span><span id="S10.1.1">Solve Problems Few Care About (For Now!)</span>
</h2>

<p id="S10.p1.1">While focusing on an established niche or application field is a relatively safe strategy, a somewhat riskier one is to find a niche or application that does not exist yet. Basically, focus on a problem that almost no-one sees the importance of, or a method that nobody finds promising.</p>
<p id="S10.p2.1">One approach is to go looking for applications that people have not seriously applied AI to. A good idea is to look into a field that is neither timely nor “sexy”. The bet here is that this particular application domain will become important in the future, either in its own right or because it enables something else. We both took this path. Procedural content generation for games was a very niche topic 15 years ago and we helped nuild a research community around it<cite>[<a href="#bib.bib24" title="">24</a>, <a href="#bib.bib28" title="">28</a>]</cite>; recently it has become more important not only for the games industry, but also as a way to help generalize (deep) reinforcement learning <cite>[<a href="#bib.bib18" title="">18</a>, <a href="#bib.bib23" title="">23</a>]</cite>. Research on reinforcement learning is a core AI topic with thousands of papers published per year, lending more importance to this once somewhat obscure topic. This high-risk high-gain mindset might lead to a lonely path that nevertheless could end up being highly rewarding in the long run.</p>
<p id="S10.p3.1">So, look around you, and talk to people who are not AI researchers. What problem domains do you see where AI is rarely applied, and which AI researchers seem to not know or care about? Might someone care about these domains in the future? If so, you may want to dig deeper in one of those domains.</p>
</section>
<section id="S11">
<h2>
<span>XI </span><span id="S11.1.1">Try Things that Shouldn’t Work</span>
</h2>

<p id="S11.p1.1">Another comparative advantage of small academic teams is the ability to try things that “shouldn’t work”, in the sense that they are unsupported by theory or experimental evidence. The dynamics of large industry research labs are typically such that researchers are incentivized to try things that are likely to work; if not, money is lost. In academia, failure can be as instructive and valuable as success and the stakes are lower overall. Many important inventions and ideas in AI come from trying the “wrong” thing. In particular, all of deep learning stems from researchers stubbornly working on neural networks even though there were good theoretical reasons why they shouldn’t work.</p>
</section>
<section id="S12">
<h2>
<span>XII </span><span id="S12.1.1">Do Things that Have Bad Optics</span>
</h2>

<p id="S12.p1.1">The larger and more important a company is, the more constrained it is by ethics and optics. Any company is ultimately responsible to their shareholders, and if the shareholders perceive that the company suffers “reputational damage” they can easily fire the CEO. So large companies will try to avoid to do anything that looks bad. To get around this, large companies sometimes fund startups to do their more experimental work that might go wrong (think Microsoft and OpenAI). But even such plays have limits, as bad PR can come washing back like the tide in San Francisco Bay.</p>
<p id="S12.p2.1">As an individual researcher, who either has no position or who already has a secure position, you have nothing to lose. You can do things that are as crazy as you like. You are only constrained by the law and your own personality. Now, we are in no way arguing that you should do research that is unethical. By all means, try to do the right thing. But what you find objectionable might be very different from what a group of mostly-white liberal overeducated engineers in coastal USA find objectionable. The PR departments, ethics committees, and boards of directors of the rich tech companies espouse a very particular set of values. But the world is large, and full of very different people and cultures. So there is a big opportunity to do research that these tech companies will not do even though they could.</p>
<p id="S12.p3.1">As an example of a project that exploits such an opportunity, one of us participated in a project critically examining the normativity of the “neutral English” in current writing support systems by creating an autocomplete system with a language model that assumes you write in the tone of Chuck Tingle, the famous author of absurd sci-fi political satire gay erotica <cite>[<a href="#bib.bib8" title="">8</a>]</cite>. Our guess is that this project would not have been cleared for publication by Amazon or Google. Another example is this very paper.</p>
<p id="S12.p4.1">Similarly, you may find that you deviate from the cultural consensus in big tech companies regarding topics relating to nudity, sexuality, rudeness, religion, capitalism, communism, law and order, justice, equality, welfare, representation, history, reproduction, violence, or something else. As all AI research happens in and is influenced by a cultural and political context, see your deviation from the norm as an opportunity. If you can’t do the research they couldn’t do, do the research they wouldn’t do.</p>
</section>
<section id="S13">
<h2>
<span>XIII </span><span id="S13.1.1">Start it Up; Spin it Out!</span>
</h2>

<p id="S13.p1.1">By now it should be rather clear that academia is somewhat, paradoxically, limiting academic AI research. Even if one manages to secure large-scale multimillion projects this covers only a fraction of human and computational resources that are necessary for contemporary AI research, and the career structures and IP rights regimes of universities often impose further limits. One popular alternative among AI scientists is to spin out their idea from their university lab and found a company that will gradually transfer AI research to a set of commercial-standard services or products. Both authors have been part of this journey through co-founding modl.ai <cite>[<a href="#bib.bib16" title="">16</a>]</cite> and have learned a lot from this.</p>
<p id="S13.p2.1">Being part of the applied AI world offers many benefits. In principle you get access to rich data from real-world applications that you wouldn’t be able to have otherwise. Moreover your AI algorithms are tested on challenging commercial-standard, applications and have to be operational in the wild. Finally, you usually gain access to more compute and, if the start-up scales up, growing access to human resources.</p>
<p id="S13.p3.1">This journey is far from straightforward, however, as there are several limiting factors to consider. First, not all research ideas are directly applicable to a startup business model. Your best research ideas might be brilliant in terms of understanding the world, or at least getting published in highly prestigious venues, but that does not mean that one can easily make products out of them. Second, many outstanding results one obtained in the lab today may have to go through a long runway until they turn into a business case of some sort. Most startups do development rather than research, as the runways are short and you need to have a functioning product, preferably with some market traction, before the next funding round in two years or so. Third, even if you do get some investment, this does not mean you have an unlimited compute budget. With seed grants often in the range of a few millions, this does not buy you the capacity to do OpenAI-level experiments, especially as you need to pay real salaries (not PhD stipends) to your employees. Fourth, not every AI academic enjoys this type of an adventure. At the end of the day most academics have long agreed on their priorities when they opted to follow the academic career path. You don’t become a professor for the money. The security of an academic environment (given that it is both safe and creative), means to some far more than any potentially higher salary or other corporate benefits.</p>
<p id="S13.p4.1">Here, we might point out that both of us publish many more papers with our academic research teams than with the company we co-founded and work part-time at. On the other hand, we believe we have more direct impact on the games industry through our company.</p>
</section>
<section id="S14">
<h2>
<span>XIV </span><span id="S14.1.1">Collaborate, or Jump Ship!</span>
</h2>

<p id="S14.p1.1">If none of the above options work for you and you still want to innovate though large scale methods that are trained on lots of data you can always collaborate with those that have them both: compute and data. There are several ways to move forward with this approach.</p>
<p id="S14.p2.1">Universities in the vicinity of leading AI companies have a comparative advantage as local social networks and in-person meetings make the collaboration easier. Researchers from remote universities can still establish collaborations though research visits, placements and internships as part of a joint-research project. More radically, some established AI professors decide to dedicate some (if not all) of their research time to an industrial partner or even move their entire lab in there. Results from such partnerships, placements or lab transfers can be astonishing <cite>[<a href="#bib.bib26" title="">26</a>, <a href="#bib.bib27" title="">27</a>]</cite>. At a glance, this looks like the best way forward for AI academics, however, 1) the generated IP cannot always be published and 2) not everyone can or want to work in an industry-based AI lab.</p>
<p id="S14.p3.1">One might even argue that innovation should be driven by public institutions as supported by the industry, not the other way around. It is arguably the university’s responsibility to maintain (part of, or some of) the talented AI researchers it educates (academics and students) and the IP they generate. Otherwise AI education and research will eventually become redundant within a University environment. This would be bad for everyone, as knowledge would be less open, and there would be no-one to train the next generation of AI researchers. Next, let’s look at this relationship more closely and outline ways industrial corporations and universities may be able to help.</p>
</section>
<section id="S15">
<h2>
<span>XV </span><span id="S15.1.1">How Can Large Players in Industry Help?</span>
</h2>

<p id="S15.p1.1">It is not clear that large companies with well-financed AI labs actually want to help alleviate this situation. Individual researchers and managers might care about the depression of academic AI research, but what the companies care about is the bottom line and shareholder value, and having a competitive academic research community might or might not be in their best interest. However, to the extent that large private sector actors do care, there are multiple things they can do.</p>
<p id="S15.p2.1">At the most basic level, open-sourcing models, including both weights and training scripts, helps a lot. It allows academic AI researchers to study the trained models, fine-tune them, and build systems around them. It still leaves academic researchers uncompetitive when it comes to training new models, but it is a start. To their credit, several large industrial research organizations regularly release their most capable models publicly; Meta in particular stands out. Others don’t, and could rightly be shamed for not doing so. In particular if their name implies some degree of openness.</p>
<p id="S15.p3.1">The next step for remedying this situation is to collaborate with academia. As discussed earlier (see Section <a href="#S14" title="XIV Collaborate, or Jump Ship! ‣ Choose Your Weapon: Survival Strategies for Depressed AI Academics"><span>XIV</span></a>) some large institutions regularly do this, mostly through accepting current PhD students as interns, allowing these students to do large-scale work. Some offer joint appointments to certain academic researchers, and a few even occasionally offer research grants. All of this is good, but more can be done. In particular there could be mechanisms where academics initiate collaborations by proposing work they would do collaboratively, and there could be more stable research funding mechanisms.</p>
<p id="S15.p4.1">Going even further, private companies that really wanted to help mend this academia-industry divide could choose to work in public: post their plans, commit code, models, and development updates to public repositories and allowing academics to contribute freely. This is not how most companies work, and often they have good reasons for their secrecy. On the other hand, a lot could be gained from having academics contributing to your code and training for free.</p>
</section>
<section id="S16">
<h2>
<span>XVI </span><span id="S16.1.1">How Can Universities Help?</span>
</h2>

<p id="S16.p1.1">As much as industry might be willing to help, the primary initiative should come from those universities that wish to drive innovation. Universities have a strong initiative to stay on top of (or if possible be in the driving seat for) AI research for many reasons, including their role in educating students who will look for jobs in a world transformed by AI, and the many ways in which AI systems transform education <cite>[<a href="#bib.bib12" title="">12</a>]</cite>. It is worth noting that many of the most influential papers in AI involve a university department. Those papers are typically co-authored by researchers that either collaborate with or are involved in a company. The successful examples are out there <cite>[<a href="#bib.bib26" title="">26</a>, <a href="#bib.bib27" title="">27</a>, <a href="#bib.bib3" title="">3</a>]</cite>, but more is needed from the university’s end to enable such partnerships. And actually, there are many ways an academic institution can initiate and foster collaboration with the industry.</p>
<p id="S16.p2.1">Universities can also help their faculty manage the changed competitive landscape by encouraging and allowing them to be more risk-taking. The comparative advantage of academic researchers in AI is to do more high-risk exploration, and incentive structures at universities must change to account for this. For example, it is unreasonable to expect a steady stream of papers at top-tier conferences such as NeurIPS and AAAI; large, well-funded industry research labs will have large advantages at writing such papers. Similarly, the grant funding structure is such that it rewards safe and incremental research on popular topics; this seems to be an inherent feature of the way grant applications are evaluated, and it is unlikely to change however often funding agencies use words like “disruptive”. The kind of research that is favored by some of the most traditional (closed-call) grant mechanisms is mostly the kind of research where academic AI researchers will not be able to compete with industry. Therefore, universities should probably avoid making grant funding a condition for hires and promotions. If universities are serious about incentivizing their faculty to leverage their competitive advantage, they should reward trying and failing and promote high-risk high-gain funding schemes and research initiatives. It is then likely that funding agencies will follow the trend and invest even more on basic and blue sky research.</p>
<p id="S16.p3.1">Such a mindset might further open the possibilities for academics to attract large amounts of funding and collectively start building their own large (foundation) models that would be entirely open to any researcher. European research funding, for instance, has long supported the AI-on-Demand Platform—a community-driven channel featuring open access AI tools—that could host such collaborative efforts on model building and sharing. The seeds of collaborative open-source projects are already planted; think of StarCoder, the recent large model built by an open-science community involving both universities and industrial partners <cite>[<a href="#bib.bib11" title="">11</a>]</cite>. We feel it is only a matter of time that more and larger academic-driven models and data will be shared openly.</p>
</section>
<section id="S17">
<h2>
<span>XVII </span><span id="S17.1.1">Parting Words</span>
</h2>

<p id="S17.p1.1">We wrote this Point of View article with several purposes in mind. First, to share our concerns with other fellow AI researchers with a hope of finding a common cause (and a collective remedy?) as a community. Second, to offer a set of guidelines based on our own experiences but also the discussions we had in the academic and industrial AI venues we participate or organize. Third, to spur an open dialogue and solicit ideas for potentially more efficient strategies for us all. Arguably, the list of strategies we ended up discussing here are far from inclusive of all possibilities that are available out there; we believe, however, that they are seeds of a conversation that—in our opinion—is very timely.</p>
</section>
<section id="S18">
<h2>
<span>XVIII </span><span id="S18.1.1">Acknowledgements</span>
</h2>

<p id="S18.p1.1">We would like to thank all anonymous and eponymous reviewers for their insightful comments. This work has been supported by NSF Award 1956200 and by a GoodAI award (JT) and from the European Union’s Horizon 2020 programme under grant agreement No 951911 (GNY).</p>
</section>
<section id="bib">
<h2>References</h2>

<ul>
<li id="bib.bib1">
<span>[1]</span>
<span>
Rick Astley.

</span>
<span><span id="bib.bib1.1.1">Never Gonna Give You Up</span>.

</span>
<span>RCA, New York, 1987.

</span>
</li>
<li id="bib.bib2">
<span>[2]</span>
<span>
Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien
Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.

</span>
<span>Video pretraining (vpt): Learning to act by watching unlabeled online
videos.

</span>
<span><span id="bib.bib2.1.1">Advances in Neural Information Processing Systems</span>,
35:24639–24654, 2022.

</span>
</li>
<li id="bib.bib3">
<span>[3]</span>
<span>
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.

</span>
<span>Decision transformer: Reinforcement learning via sequence modeling.

</span>
<span><span id="bib.bib3.1.1">Advances in neural information processing systems</span>,
34:15084–15097, 2021.

</span>
</li>
<li id="bib.bib4">
<span>[4]</span>
<span>
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.

</span>
<span>An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span><span id="bib.bib4.1.1">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li id="bib.bib5">
<span>[5]</span>
<span>
Theodoros Galanos, Antonios Liapis, Georgios N Yannakakis, and Reinhard Koenig.

</span>
<span>Arch-elites: Quality-diversity for urban design.

</span>
<span>In <span id="bib.bib5.1.1">Proceedings of the Genetic and Evolutionary Computation
Conference Companion</span>, pages 313–314, 2021.

</span>
</li>
<li id="bib.bib6">
<span>[6]</span>
<span>
Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao.

</span>
<span>Knowledge distillation: A survey.

</span>
<span><span id="bib.bib6.1.1">International Journal of Computer Vision</span>, 129:1789–1819, 2021.

</span>
</li>
<li id="bib.bib7">
<span>[7]</span>
<span>
Souhila Kaci.

</span>
<span><span id="bib.bib7.1.1">Working with preferences: Less is more</span>.

</span>
<span>Springer Science &amp; Business Media, 2011.

</span>
</li>
<li id="bib.bib8">
<span>[8]</span>
<span>
Ahmed Khalifa, Gabriella AB Barros, and Julian Togelius.

</span>
<span>Deeptingle.

</span>
<span>In <span id="bib.bib8.1.1">International Conference on Computational Creativity</span>, 2017.

</span>
</li>
<li id="bib.bib9">
<span>[9]</span>
<span>
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.

</span>
<span>Deep learning.

</span>
<span><span id="bib.bib9.1.1">nature</span>, 521(7553):436–444, 2015.

</span>
</li>
<li id="bib.bib10">
<span>[10]</span>
<span>
En Li, Liekang Zeng, Zhi Zhou, and Xu Chen.

</span>
<span>Edge ai: On-demand accelerating deep neural network inference via
edge computing.

</span>
<span><span id="bib.bib10.1.1">IEEE Transactions on Wireless Communications</span>, 19(1):447–457,
2019.

</span>
</li>
<li id="bib.bib11">
<span>[11]</span>
<span>
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.

</span>
<span>Starcoder: may the source be with you!

</span>
<span><span id="bib.bib11.1.1">arXiv preprint arXiv:2305.06161</span>, 2023.

</span>
</li>
<li id="bib.bib12">
<span>[12]</span>
<span>
Weng Marc Lim, Asanka Gunasekara, Jessica Leigh Pallant, Jason Ian Pallant, and
Ekaterina Pechenkina.

</span>
<span>Generative ai and the future of education: Ragnarök or
reformation? a paradoxical perspective from management educators.

</span>
<span><span id="bib.bib12.1.1">The International Journal of Management Education</span>,
21(2):100790, 2023.

</span>
</li>
<li id="bib.bib13">
<span>[13]</span>
<span>
Konstantinos Makantasis, David Melhart, Antonios Liapis, and Georgios N
Yannakakis.

</span>
<span>Privileged information for modeling affect in the wild.

</span>
<span>In <span id="bib.bib13.1.1">2021 9th International Conference on Affective Computing and
Intelligent Interaction (ACII)</span>, pages 1–8. IEEE, 2021.

</span>
</li>
<li id="bib.bib14">
<span>[14]</span>
<span>
Keiran Paster, Sheila McIlraith, and Jimmy Ba.

</span>
<span>You can’t count on luck: Why decision transformers fail in stochastic
environments.

</span>
<span><span id="bib.bib14.1.1">arXiv preprint arXiv:2205.15967</span>, 2022.

</span>
</li>
<li id="bib.bib15">
<span>[15]</span>
<span>
Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca
Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja
Hofmann, et al.

</span>
<span>Imitating human behaviour with diffusion models.

</span>
<span><span id="bib.bib15.1.1">arXiv preprint arXiv:2301.10677</span>, 2023.

</span>
</li>
<li id="bib.bib16">
<span>[16]</span>
<span>
Christoffer Holmgård Pedersen, Benedikte Mikkelsen, Julian Togelius,
Georgios N Yannakakis, Sebastian Risi, and Lars Henriksen.

</span>
<span>Experience based game development and methods for use therewith,
May 17 2022.

</span>
<span>US Patent 11,331,581.

</span>
</li>
<li id="bib.bib17">
<span>[17]</span>
<span>
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander
Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
Jost Tobias Springenberg, et al.

</span>
<span>A generalist agent.

</span>
<span><span id="bib.bib17.1.1">arXiv preprint arXiv:2205.06175</span>, 2022.

</span>
</li>
<li id="bib.bib18">
<span>[18]</span>
<span>
Sebastian Risi and Julian Togelius.

</span>
<span>Increasing generality in machine learning through procedural content
generation.

</span>
<span><span id="bib.bib18.1.1">Nature Machine Intelligence</span>, 2(8):428–436, 2020.

</span>
</li>
<li id="bib.bib19">
<span>[19]</span>
<span>
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.

</span>
<span>Imagenet large scale visual recognition challenge.

</span>
<span><span id="bib.bib19.1.1">International journal of computer vision</span>, 115:211–252, 2015.

</span>
</li>
<li id="bib.bib20">
<span>[20]</span>
<span>
Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni.

</span>
<span>Green ai.

</span>
<span><span id="bib.bib20.1.1">Communications of the ACM</span>, 63(12):54–63, 2020.

</span>
</li>
<li id="bib.bib21">
<span>[21]</span>
<span>
Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel
Pinto.

</span>
<span>Behavior transformers: Cloning <math id="bib.bib21.1.m1.1" alttext="k" display="inline"><semantics id="bib.bib21.1.m1.1a"><mi id="bib.bib21.1.m1.1.1" xref="bib.bib21.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="bib.bib21.1.m1.1b"><ci id="bib.bib21.1.m1.1.1.cmml" xref="bib.bib21.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib21.1.m1.1c">k</annotation></semantics></math> modes with one stone.

</span>
<span><span id="bib.bib21.2.1">Advances in neural information processing systems</span>,
35:22955–22968, 2022.

</span>
</li>
<li id="bib.bib22">
<span>[22]</span>
<span>
Richard Sutton.

</span>
<span>The bitter lesson.

</span>
<span><span id="bib.bib22.1.1">Incomplete Ideas (blog)</span>, 13(1), 2019.

</span>
</li>
<li id="bib.bib23">
<span>[23]</span>
<span>
Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal
Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang,
Natalie Clay, Adrian Collister, et al.

</span>
<span>Human-timescale adaptation in an open-ended task space.

</span>
<span><span id="bib.bib23.1.1">arXiv preprint arXiv:2301.07608</span>, 2023.

</span>
</li>
<li id="bib.bib24">
<span>[24]</span>
<span>
Julian Togelius, Georgios N Yannakakis, Kenneth O Stanley, and Cameron Browne.

</span>
<span>Search-based procedural content generation: A taxonomy and survey.

</span>
<span><span id="bib.bib24.1.1">IEEE Transactions on Computational Intelligence and AI in
Games</span>, 3(3):172–186, 2011.

</span>
</li>
<li id="bib.bib25">
<span>[25]</span>
<span>
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al.

</span>
<span>Llama 2: Open foundation and fine-tuned chat models.

</span>
<span><span id="bib.bib25.1.1">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li id="bib.bib26">
<span>[26]</span>
<span>
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span>Attention is all you need.

</span>
<span><span id="bib.bib26.1.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib27">
<span>[27]</span>
<span>
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando
Freitas.

</span>
<span>Dueling network architectures for deep reinforcement learning.

</span>
<span>In <span id="bib.bib27.1.1">International conference on machine learning</span>, pages
1995–2003. PMLR, 2016.

</span>
</li>
<li id="bib.bib28">
<span>[28]</span>
<span>
Georgios N Yannakakis and Julian Togelius.

</span>
<span>Experience-driven procedural content generation.

</span>
<span><span id="bib.bib28.1.1">IEEE Transactions on Affective Computing</span>, 2(3):147–161, 2011.

</span>
</li>
</ul>
</section>
<figure id="id1">
<table id="id1.1">
<tbody><tr id="id1.1.1">
<td id="id1.1.1.1"><img src="https://ar5iv.labs.arxiv.org/html/2304.06035/assets/figures/togelius.jpg" id="id1.1.1.1.g1" width="96" height="125" alt="[Uncaptioned image]"/></td>
<td id="id1.1.1.2">
<span id="id1.1.1.2.1">
<span id="id1.1.1.2.1.1"><span id="id1.1.1.2.1.1.1">Julian Togelius</span>  (S’05-M’07-SM’22) is an Associate Professor in the Department of Computer Science and Engineering, New York University, and a co-founder of modl.ai. He works on artificial intelligence for games and on games for artificial intelligence. His current main research directions involve procedural content generation in games, general video game playing, player modeling, and fair and relevant benchmarking of AI through game-based competitions. Additionally, he works on topics in evolutionary computation, quality-diversity algorithms, and reinforcement learning. From 2018 to 2021, he was the Editor-in-Chief of the IEEE Transactions on Games. Togelius holds a BA from Lund University, an MSc from the University of Sussex, and a PhD from the University of Essex. He has previously worked at IDSIA in Lugano and at the IT University of Copenhagen.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="id2">
<table id="id2.1">
<tbody><tr id="id2.1.1">
<td id="id2.1.1.1"><img src="https://ar5iv.labs.arxiv.org/html/2304.06035/assets/figures/yannakakis.jpg" id="id2.1.1.1.g1" width="100" height="100" alt="[Uncaptioned image]"/></td>
<td id="id2.1.1.2">
<span id="id2.1.1.2.1">
<span id="id2.1.1.2.1.1"><span id="id2.1.1.2.1.1.1">Georgios N. Yannakakis</span> (S’04-M’05-SM’14-F’24) is a Professor at the Institute of Digital Games, University of Malta, and a co-founder of modl.ai. He does research at the crossroads of artificial intelligence, computational creativity, affective computing, advanced game technology, and human-computer interaction. He has published more than 350 papers in the aforementioned fields and his work has been cited broadly. He is currently the Editor in Chief of <span id="id2.1.1.2.1.1.2">IEEE Transactions on Games</span> and an Associate Editor of <span id="id2.1.1.2.1.1.3">IEEE Transactions on Evolutionary Computation</span>.</span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</article>
</div>

</div></div>
  </body>
</html>
