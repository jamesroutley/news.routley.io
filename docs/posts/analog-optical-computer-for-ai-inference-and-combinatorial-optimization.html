<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/s41586-025-09430-z">Original</a>
    <h1>Analog optical computer for AI inference and combinatorial optimization</h1>
    
    <div id="readability-page-1" class="page"><div>
                    
                        <section data-title="Main"><div id="Sec1-section"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>Computing today is digital, but analog has a future. Exponential advances in digital hardware have both driven and benefited from the rise of artificial intelligence (AI), but its escalating energy and latency demands push digital specialization to its limits<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Dally, B. Trends in deep learning hardware: Bill Dally (NVIDIA) YouTube 
                https://www.youtube.com/watch?v=kLiwvnr4L80&amp;t=770s
                
               (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR8" id="ref-link-section-d430713338e612">8</a></sup>. Analog approaches—leveraging optics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shastri, B. J. et al. Photonics for artificial intelligence and neuromorphic computing. Nat. Photon. 15, 102–114 (2021)." href="#ref-CR1" id="ref-link-section-d430713338e616">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shen, Y. et al. Deep learning with coherent nanophotonic circuits. Nat. Photon. 11, 441–446 (2017)." href="#ref-CR2" id="ref-link-section-d430713338e616_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chen, Y. et al. All-analog photoelectronic chip for high-speed vision tasks. Nature 623, 48–57 (2023)." href="#ref-CR3" id="ref-link-section-d430713338e616_2">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="McMahon, P. L. et al. A fully programmable 100-spin coherent ising machine with all-to-all connections. Science 354, 614–617 (2016)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR4" id="ref-link-section-d430713338e619">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Psaltis, D. &amp; Farhat, N. Optical information processing based on an associative memory model of neural nets with thresholding and feedback. Opt. Lett. 10, 98–100 (1985)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR9" id="ref-link-section-d430713338e622">9</a></sup>, analog electronic crossbars<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Cai, F. et al. Power-efficient combinatorial optimization using intrinsic noise in memristor Hopfield neural networks. Nat. Electron. 3, 409–418 (2020)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR5" id="ref-link-section-d430713338e626">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Hao, L., William, M., Hanzhao, Y., Sachin, S. &amp; Kim, C. H. An Ising solver chip based on coupled ring oscillators with a 48-node all-to-all connected array architecture. Nat. Electron. 6, 771–778 (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR6" id="ref-link-section-d430713338e629">6</a></sup> and quantum annealers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Johnson, M. W. et al. Quantum annealing with manufactured spins. Nature 473, 194–198 (2011)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR7" id="ref-link-section-d430713338e633">7</a></sup>—promise orders-of-magnitude gains in efficiency and speed. Existing hardware demonstrations focus either on AI inference<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shastri, B. J. et al. Photonics for artificial intelligence and neuromorphic computing. Nat. Photon. 15, 102–114 (2021)." href="#ref-CR1" id="ref-link-section-d430713338e637">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Shen, Y. et al. Deep learning with coherent nanophotonic circuits. Nat. Photon. 11, 441–446 (2017)." href="#ref-CR2" id="ref-link-section-d430713338e637_1">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Chen, Y. et al. All-analog photoelectronic chip for high-speed vision tasks. Nature 623, 48–57 (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR3" id="ref-link-section-d430713338e640">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Feldmann, J. et al. Parallel convolutional processing using an integrated photonic tensor core. Nature 589, 52–58 (2021)." href="#ref-CR10" id="ref-link-section-d430713338e643">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wetzstein, G. et al. Inference in artificial intelligence with deep optics and photonics. Nature 588, 39–47 (2020)." href="#ref-CR11" id="ref-link-section-d430713338e643_1">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Hamerly, R., Bernstein, L., Sludds, A., Soljacic, M. &amp; Englund, D. Large-scale optical neural networks based on photoelectric multiplication. Phys. Rev. X 9, 021032 (2019)." href="#ref-CR12" id="ref-link-section-d430713338e643_2">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Wang, T. et al. An optical neural network using less than 1 photon per multiplication. Nat. Commun. 13, 123 (2022)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR13" id="ref-link-section-d430713338e646">13</a></sup>, which accounts for 90% of energy in commercial deployments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Ivanov, A., Dryden, N., Ben-Nun, T., Li, S. &amp; Hoefler, T. Data movement is all you need: a case study on optimizing transformers. Proc. Mach. Learn. Syst. 3, 711–732 (2021)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR14" id="ref-link-section-d430713338e651">14</a></sup>, or combinatorial optimization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Johnson, M. W. et al. Quantum annealing with manufactured spins. Nature 473, 194–198 (2011)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR7" id="ref-link-section-d430713338e655">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Mohseni, N., McMahon, P. L. &amp; Byrnes, T. Ising machines as hardware solvers of combinatorial optimization problems. Nat. Rev. Phys. 4, 363–379 (2022)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR15" id="ref-link-section-d430713338e658">15</a></sup>, but none efficiently accelerate both on the same analog hardware.</p><p>Here we introduce the analog optical computer (AOC), a non-traditional computing platform designed for both AI inference and combinatorial optimization. By combining optical and analog electronic components within a feedback loop, the AOC rapidly performs a fixed-point search without digital conversions. In each loop iteration of approximately 20 ns, optics handle matrix–vector multiplications, whereas analog electronics perform nonlinear operations, subtraction and annealing (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig1">1a–c</a>). Over multiple iterations, the fixed-point nature of the AOC enhances noise robustness, which is essential for analog hardware.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="The AOC and its applications."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: The AOC and its applications.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-025-09430-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="606"/></picture></a></div><p><b>a</b>, A schematic of the AOC hardware architecture with key components 1–4. <b>b</b>, The AOC hardware architecture. <b>c</b>, Key hardware components. The microLED array (1) is the light source and represents neural network activations or optimization variables. The spatial light modulator (2) stores neural network weights or optimization problem coefficients, and multiplies them with the incoming light. The photodetector array (3) adds and transfers optical signals into the analog electronic domain. The nonlinearity, subtraction, annealing and other computations are applied in analog electronics (4). <b>d</b>, ML inference as a fixed-point search. The AOC hardware is used to accelerate ML inference, using an ML equilibrium model that entails finding their fixed points. <b>e</b>, Quadratic optimization as a fixed-point search. A schematic of the convergence to minima over time for an optimization problem with continuous (horizontal bars) and binary (vertical arrows) variables, experiencing gradients (red) from other variables until convergence to the fixed point (green). <b>f</b>, Applications realized on the AOC hardware. The AOC hardware performs inference for MNIST and Fashion-MNIST classification tasks and nonlinear regression, and also solves industrial optimization problems, including medical image reconstruction and transaction settlement between financial institutions.</p></div></figure></div><p>The AOC’s fully analog architecture and fixed-point abstraction address two key challenges in unconventional computing. First, hybrid architectures typically accelerate linear operations but rely on digital nonlinearities, resulting in energy-intensive conversions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Shen, Y. et al. Deep learning with coherent nanophotonic circuits. Nat. Photon. 11, 441–446 (2017)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR2" id="ref-link-section-d430713338e708">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Wang, T. et al. An optical neural network using less than 1 photon per multiplication. Nat. Commun. 13, 123 (2022)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR13" id="ref-link-section-d430713338e711">13</a></sup>, which are eliminated in the AOC. Second, they often face an application-hardware gap: memory-bound AI models are hard to accelerate<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Ivanov, A., Dryden, N., Ben-Nun, T., Li, S. &amp; Hoefler, T. Data movement is all you need: a case study on optimizing transformers. Proc. Mach. Learn. Syst. 3, 711–732 (2021)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR14" id="ref-link-section-d430713338e715">14</a></sup>, and prevalent binary optimization formulations limit practical applicability<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Jacobs, B. Quantum-Inspired Classical Computing (QuICC) HR001121S0041 Call for Proposals Report (DARPA, Microsystems Technology Office, 2021)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR16" id="ref-link-section-d430713338e719">16</a></sup>. With its unifying fixed-point abstraction, the AOC closes this gap (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig1">1d,e</a>).</p><p>For inference, the AOC accelerates emerging iterative models, including fixed-point models such as deep-equilibrium networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Bai, S., Kolter, J. Z. &amp; Koltun, V. Deep equilibrium models. Adv. Neural Inf. Process. Syst. 32, 690–701 (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR17" id="ref-link-section-d430713338e729">17</a></sup>, which are compute-bound and costly on digital chips but naturally suited for the AOC. These models enable iterative reasoning with dynamic inference time computation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Graves, A. Adaptive computation time for recurrent neural networks. Preprint at 
                https://arxiv.org/abs/1603.08983
                
               (2016)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR18" id="ref-link-section-d430713338e733">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Nye, M. et al. Show your work: scratchpads for intermediate computation with language models. Preprint at 
                https://arxiv.org/abs/2112.00114
                
               (2021)" href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR19" id="ref-link-section-d430713338e736">19</a></sup>. For optimization, the AOC supports quadratic unconstrained mixed optimization (QUMO), a flexible formulation with binary and continuous variables that captures real-world problems<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Kalinin, K. P. et al. Analog iterative machine (AIM): using light to solve quadratic optimization problems with mixed variables. Preprint at 
                https://arxiv.org/abs/2304.12594
                
               (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR20" id="ref-link-section-d430713338e740">20</a></sup>.</p><p>The current small-scale AOC accelerates equilibrium models with up to 4,096 weights at 9-bit precision, performing image classification and nonlinear regression tasks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig1">1f</a>). To enable these inference tasks, we design a differentiable digital twin (AOC-DT) that achieves over 99% correspondence with the physical hardware. For optimization, the AOC solves QUMO problems with up to 64 variables, tackling real-world applications such as medical image reconstruction and financial transaction settlement. The AOC-DT is used to demonstrate scalable solutions for industrial problems, such as reconstruction of a brain scan with over 200,000 problem variables. Compared with well-known heuristics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Microsoft Quantum Inspired Optimisation (QIO) provider (2022); 
                https://learn.microsoft.com/en-us/azure/quantum/provider-microsoft-qio
                
              ." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR21" id="ref-link-section-d430713338e751">21</a></sup> and commercial solvers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Gurobi Optimizer Reference Manual (2023); 
                http://www.gurobi.com
                
              ." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR22" id="ref-link-section-d430713338e755">22</a></sup>, we set state-of-the-art results on several instances from a standard quadratic optimization benchmark<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Furini, F. et al. QPLIB: a library of quadratic programming instances. Math. Program. Comput. 11, 237–265 (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR23" id="ref-link-section-d430713338e759">23</a></sup>.</p><p>Designed with consumer-grade optical and electronic components, the AOC leverages mature manufacturing processes, with future scalability relying on tighter coupling of integrated analog electronics with integrated three-dimensional (3D) optics. By eliminating digital–analog conversions and merging compute and memory to bypass the von Neumann bottleneck, the AOC can achieve substantial efficiency gains albeit specialized. With projected performance around 500 tera-operations per second (TOPS) per watt at 8-bit precision—over 100-times more efficient than leading graphics processing units (GPUs)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Nvidia H100 Tensor Core GPU. Nvidia 
                https://www.nvidia.com/en-us/data-center/h100/
                
               (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR24" id="ref-link-section-d430713338e766">24</a></sup>—the AOC represents a promising step towards sustainable computing.</p></div></div></section><section data-title="Fixed-point abstraction"><div id="Sec2-section"><h2 id="Sec2">Fixed-point abstraction</h2><div id="Sec2-content"><p>The AOC hardware unifies machine learning (ML) inference and optimization paradigms through an iterative fixed-point search. In ML, the inference of equilibrium<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Bai, S., Kolter, J. Z. &amp; Koltun, V. Deep equilibrium models. Adv. Neural Inf. Process. Syst. 32, 690–701 (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR17" id="ref-link-section-d430713338e778">17</a></sup> and energy-based<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="LeCun, Y. et al. A tutorial on energy-based learning. In Predicting Structured Data, Vol. 1 (eds Bakir, G., Hofman, T., Scholkopt, B., Smola, A. &amp; Taskar, B.) (MIT Press, 2006)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR25" id="ref-link-section-d430713338e782">25</a></sup> models entails finding their fixed points, whereas in optimization, objective minima represent fixed points of gradient-descent-based methods. The core AOC abstraction realizes the following iterative update rule:</p><div id="Equ1"><p><span>$${{\bf{s}}}_{t+1}=\alpha (t){{\bf{s}}}_{t}+\beta W\,f({{\bf{s}}}_{t})+\gamma ({{\bf{s}}}_{t}-{{\bf{s}}}_{t-1})+{\bf{b}}.$$</span></p><p>
                    (1)
                </p></div><p>At each iteration <i>t</i>, the continuous real-valued state vector <span>\({{\bf{s}}}_{t}\in {{\mathbb{R}}}^{N}\)</span> is updated to <b>s</b><sub><i>t</i>+1</sub>, with each iteration corresponding to a signal round-trip time in hardware. Although equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ1">1</a>) is discrete, the AOC operates in a continuous clock-free manner. The annealing schedule <span>\(\alpha (t):[0,T]\to {\mathbb{R}}\)</span> controls the state magnitude reduction per iteration, similar to residual connections in neural networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition 770–778 (IEEE, 2016)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR26" id="ref-link-section-d430713338e1052">26</a>, where <i>T</i></sup> is the number of timesteps in the annealing schedule. The factor <i>β</i> determines the matrix–vector product scale, where the matrix <i>W</i> encodes neural network weights or an optimization problem, and <span>\(f:{{\mathbb{R}}}^{N}\to {{\mathbb{R}}}^{N}\)</span> is an element-wise nonlinear function. The coefficient <i>γ</i> <span>∈</span> (0, 1) introduces momentum which, in continuous time, corresponds to the second-order differential equation dynamics (Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.10</a>), generalizing the AOC abstraction beyond first-order models such as Hopfield networks. The bias vector <span>\({\bf{b}}\in {{\mathbb{R}}}^{N}\)</span> represents additional problem-specific information.</p><p>The introduced fixed-point abstraction is ideally suited for analog feedback devices, such as the AOC, as it is compute-bound, requires no intermediate memory and is noise-tolerant: the attracting fixed point pulls the trajectory closer at every iteration, counteracting analog noise.</p></div></div></section><section data-title="Hardware"><div id="Sec3-section"><h2 id="Sec3">Hardware</h2><div id="Sec3-content"><p>The AOC hardware combines 3D optical and analog electronic technologies to accelerate all the compute operations in the fixed-point abstraction described by equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ1">1</a>): matrix–vector multiplication, nonlinearity, annealing, addition and subtraction. In each fixed-point iteration, the analog signal alternates between optical and electrical domains, giving the system state <b>s</b><sub><i>t</i></sub> a dual opto-electronic nature.</p><p>Matrix–vector multiplication occurs in the optical domain, where the state vector <b>s</b><sub><i>t</i></sub> is encoded in the light intensity of arrays of micro light-emitting diodes (microLEDs), whereas the weight matrix <i>W</i> is represented by spatial light modulator (SLM) pixels (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig1">1a</a>). Light from each microLED fans out across an SLM row for element-wise multiplication, and the resulting light signals are summed column-wise by a photodetector array<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Goodman, J. W., Dias, A. R., Woody, L. M. &amp; Erickson, J. Application of optical communication technology to optical information processing. In Los Alamos Conference on Optics 1979 Vol. 0190 (ed. Liebenberg, D. H.) 485–496 (SPIE, 1980); 
                https://doi.org/10.1117/12.957795
                
              ." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR27" id="ref-link-section-d430713338e1196">27</a></sup>. In contrast to planar optical architectures, the AOC leverages 3D optics with its efficient fan-in and fan-out of light in the third dimension through the use of spherical and cylindrical optics, thus enabling inherently parallel and scalable multiplication operations of larger matrices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="McMahon, P. L. The physics of optical computing. Nat. Rev. Phys. 5, 717–734 (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR28" id="ref-link-section-d430713338e1200">28</a></sup>.</p><p>The result of the optical matrix–vector multiplication is measured in the electrical domain using a photodetector array, where the state vector <b>s</b><sub><i>t</i></sub> is represented as a voltage per detector. The remaining operations in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ1">1</a>) are implemented via analog electronics: a hyperbolic tangent (tanh) function for the element-wise nonlinearity, summing and difference amplifiers for addition and subtraction of analog signals, and variable gain amplifiers for the annealing schedule <i>α</i>(<i>t</i>) and the <i>β</i> factor. The circuit layout, with highlighted voltage readout position, is detailed in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig7">3</a>.</p><p>The AOC hardware executes the iterative update rule from several to thousands of iterations until convergence to a fixed point, when the signal amplitudes are read out digitally. This all-analog operation minimizes the overhead of analog-to-digital conversions. The current hardware includes 16 microLEDs and 16 photodetectors, supporting a 16-variable state vector <b>s</b><sub><i>t</i></sub>, along with two SLMs to handle positive and negative entries of the matrix <i>W</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig1">1b,c</a>). This configuration is sufficient to support inference for ML models and optimization tasks with up to 256 weights and can extend to 4,096 weights via problem decomposition, maintaining fully analog fixed-point iterations.</p><p>The AOC fixed-point abstraction targets a balance between generality and efficient hardware implementation. To illustrate its versatility, the remainder of the paper presents four case studies highlighting how equilibrium ML models can be applied to classification and regression tasks, and how the QUMO paradigm can represent real-world applications in finance and healthcare, while utilizing the same AOC hardware.</p></div></div></section><section data-title="AOC for machine learning"><div id="Sec4-section"><h2 id="Sec4">AOC for machine learning</h2><div id="Sec4-content"><h3 id="Sec5">Analog equilibrium model</h3><p>The AOC supports neural equilibrium models, which have been widely applied across various domains from language<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Bai, S., Kolter, J. Z. &amp; Koltun, V. Deep equilibrium models. Adv. Neural Inf. Process. Syst. 32, 690–701 (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR17" id="ref-link-section-d430713338e1260">17</a></sup> to vision<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Bai, S., Koltun, V. &amp; Kolter, J. Z. Multiscale deep equilibrium models. Adv. Neural Inf. Process. Syst. 33, 5238–5250 (2020)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR29" id="ref-link-section-d430713338e1264">29</a></sup>. Equilibrium models typically follow a fixed-point iterative update rule, <b>s</b><sub><i>t</i>+1</sub> = Network(<b>s</b><sub><i>t</i></sub>), with examples including classic Hopfield networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl Acad. Sci. USA 79, 2554–2558 (1982)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR30" id="ref-link-section-d430713338e1282">30</a></sup> and their modern variants<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Krotov, D. &amp; Hopfield, J. J. Dense associative memory for pattern recognition. Adv. Neural Inf. Process. Syst. 29, 1180–1188 (2016)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR31" id="ref-link-section-d430713338e1286">31</a></sup>, as well as deep-equilibrium models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Bai, S., Kolter, J. Z. &amp; Koltun, V. Deep equilibrium models. Adv. Neural Inf. Process. Syst. 32, 690–701 (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR17" id="ref-link-section-d430713338e1290">17</a></sup>. These models operate as self-recurrent neural networks with constant input, driving the hidden state to a fixed point that represents the network output (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig1">1d</a>). Their dynamic depth enables recursive reasoning, leads to improved scaling laws<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J. &amp; Kaiser, L. Universal transformers. In International Conference on Learning Representations (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR32" id="ref-link-section-d430713338e1297">32</a></sup> and enhances out-of-distribution generalization compared with feedforward models<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Graves, A. Adaptive computation time for recurrent neural networks. Preprint at 
                https://arxiv.org/abs/1603.08983
                
               (2016)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR18" id="ref-link-section-d430713338e1301">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Du, Y., Li, S., Tenenbaum, J. &amp; Mordatch, I. Learning iterative reasoning through energy minimization. In International Conference on Machine Learning 5570–5582 (PMLR, 2022)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR33" id="ref-link-section-d430713338e1304">33</a></sup>. Recent self-recurrent language models with billions of parameters show impressive reasoning capabilities, surpassing fixed-depth models in representation power<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Geiping, J. et al. Scaling up test-time compute with latent reasoning: a recurrent depth approach. Preprint at 
                https://arxiv.org/abs/2502.05171
                
               (2025)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR34" id="ref-link-section-d430713338e1309">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Schone, M. et al. Implicit language models are RNNs: balancing parallelization and expressivity. In International Conference on Machine Learning (PMLR, 2025)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR35" id="ref-link-section-d430713338e1312">35</a></sup>. We demonstrate that the AOC supports models with such recurrent nature and achieves greater out-of-distribution generalization (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig10">6</a>).</p><p>As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig2">2a</a>, the complete neural network architecture includes an input projection (IP) layer, the equilibrium model and an output projection (OP) layer. The network training is performed digitally using the AOC-DT, whereas the equilibrium model is deployed on the AOC hardware for inference.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="AOC for ML inference."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: AOC for ML inference.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-025-09430-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="670"/></picture></a></div><div data-test="bottom-caption" id="figure-2-desc"><p><b>a</b>, Top: the neural network architecture used for training and inference. During training, input data are projected into the latent space via an IP layer, processed by the AOC hardware’s digital twin (AOC-DT) and passed through the OP layer. Bottom: the inference process on the AOC hardware: the IP and OP layers interface with the hardware, which updates its initial state <b>s</b><sub>0</sub> until reaching the fixed-point state <b>s</b>*. <b>b</b>, During inference, MNIST and Fashion-MNIST images are passed through the IP layer and are fed into the AOC hardware. Intermediate states may be projected out to monitor progress, schematically showing evolution from a higher to a lower entropy distribution. Once converged, a softmax nonlinearity in the OP layer selects the highest-probability class, whereas in regression, the OP layer outputs a continuous value <i>y</i>, with the MSE evaluated against ground truth (GNDTH) to assess performance. GNDTH is the true curve we want to regress against. <b>c</b>, The nonlinear regression results are demonstrated for the AOC hardware over a Gaussian curve (left) and sinusoidal curve (right) with MSE losses of 3.75 × 10<sup>−3</sup> and 1.21 × 10<sup>−2</sup>, respectively. The shaded area around the AOC predictions represents the observed variability, that is, the standard deviation across the sampling window and repeated AOC runs with the same input (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Sec15">Methods</a>). <b>d</b>, For the full test datasets of MNIST (left) and Fashion-MNIST (right), the AOC classification accuracy is compared with the performance of the AOC-DT and the feedforward model (FFM). Higher accuracies are achieved for larger models (4,096 weights), which are realized with a time-multiplexing technique. Hardware results align with AOC-DT simulations; the 4,096-weight hardware result slightly exceeds the AOC-DT, as it reflects the best of 2 seeds, whereas the AOC-DT accuracy is averaged. The dashed lines show the linear classifier performance; the error bars reflect the random-seed variability for the AOC-DT and experimental repeats for hardware (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Sec15">Methods</a>).</p><p><a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM4">Source Data</a></p></div></div></figure></div><p>For equilibrium models, the fixed-point iterative update follows from equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ1">1</a>) by setting <i>α</i>(<i>t</i>) → <i>α</i>, <i>γ</i>(<i>t</i>) → 0, <b>b</b> → <b>b</b> + <b>x</b><sub>proj</sub>, and using the element-wise tanh nonlinearity:</p><div id="Equ2"><p><span>$${{\bf{s}}}_{t+1}=\alpha {{\bf{s}}}_{t}+\beta W\tanh ({{\bf{s}}}_{t})+{\bf{b}}+{{\bf{x}}}_{{\rm{proj}}}.$$</span></p><p>
                    (2)
                </p></div><p>Here <b>b</b> is the trained bias and <b>x</b><sub>proj</sub> encodes the equilibrium model input. The trained weight matrix <i>W</i> is quantized to 9-bit integers (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Sec15">Methods</a>), separated into positive and negative components, and loaded into the corresponding SLMs. The initial state is set to <b>s</b><sub>0</sub> = <b>b</b> + <b>x</b><sub>proj</sub>.</p><p>During inference, the original data <b>x</b> go through the IP layer as <b>x</b><sub>proj</sub> = <i>W</i><sub>IP</sub><b>x</b> + <b>b</b><sub>IP</sub>, where <i>W</i><sub>IP</sub> and <b>b</b><sub>IP</sub> are the trained IP weights and biases. For the given <b>x</b><sub>proj</sub>, the equilibrium model iterates on the AOC hardware until convergence, with the fixed-point state <b>s</b>* read out as voltages (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig9">5</a>). Finally, the inference result is obtained by applying the OP layer to the AOC solution as <b>y</b> = <i>W</i><sub>OP</sub><b>s</b>*  + <b>b</b><sub>OP</sub>, where <i>W</i><sub>OP</sub> and <b>b</b><sub>OP</sub> are the trained OP weights and bias.</p><p>In the current hardware, the equilibrium model is implemented for a single-layer network with a 256-weight matrix, without symmetry constraints. A recurrent multilayer neural network can be constructed using a lower subdiagonal block-wise matrix (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig3">3</a>), whereas a continuous-valued Hopfield network arises for a symmetric matrix. We note that neural networks conventionally apply the activation function and weight matrix in a different order than in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ2">2</a>). However, this leads to only a minor difference in practice owing to the self-recursive process<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Xu, Z.-B., Qiao, H., Peng, J. &amp; Zhang, B. A comparative study of two modeling approaches in neural networks. Neural Netw. 17, 73–85 (2004)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR36" id="ref-link-section-d430713338e1625">36</a></sup>.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Multilayer neural networks with recurrence on the AOC."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: Multilayer neural networks with recurrence on the AOC<i>.</i></b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-025-09430-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="123"/></picture></a></div><p>The left panel shows a multilayer deep neural network (DNN) with recurrence, where orange, green and blue denote different layers. Here, <b>s</b><sub>0</sub> is the initial network state, <b>x</b> is the network input and <b>s</b>* represents the fixed point to which the network converges. This network structure can be realized on the AOC hardware by arranging the layers along the lower subdiagonal of a larger matrix <i>W</i>, placing the final layer as a recurrent block in the top-right corner, with its time evolution shown in the middle panel. The right panel further represents this architecture through a single block iterated over time, capturing the recurrent multilayered model structure. All three panels depict equivalent representations of the recurrent multilayer network.</p></div></figure></div><p>The hardware realizes the equilibrium model as in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ2">2</a>) for two ML inference tasks: image classification and nonlinear regression (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig2">2b</a>). In both cases, models are trained digitally through the AOC-DT and are deployed on the hardware without further calibration, which requires high hardware precision and high fidelity of the AOC-DT (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Sec15">Methods</a>).</p><h3 id="Sec6">Regression case study</h3><p>We show that the AOC hardware can run nonlinear regression models. Regression tasks require continuous-valued outputs, which are challenging owing to the inherently noisy nature of analog computations. This is in contrast to classification tasks discussed below where the output labels are discrete and only the class of the largest probability is selected. From a model perspective, nonlinear regression tasks are well suited to the 256-weight AOC as they require small input (<span>\({W}_{{\rm{IP}}}\in {{\mathbb{R}}}^{16\times 1}\)</span>) and output (<span>\({W}_{{\rm{OP}}}\in {{\mathbb{R}}}^{1\times 16}\)</span>) projection matrices.</p><p>We select two nonlinear functions for regression: Gaussian and sinusoidal curves. In agreement with the AOC-DT results (see Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">2</a>), the hardware reproduces accurately both functions, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig2">2c</a>. The sinusoidal curve presents a greater challenge for accurate fitting than the Gaussian curve owing to its multiple minima and maxima, requiring higher AOC-DT fidelity. This may explain why the AOC hardware struggles to accurately fit the region near the right minimum of the sinusoidal curve. However, at no point does the AOC-DT curve fall outside the AOC standard deviation. We note the digital IP and OP layers alone would only be able to fit linear functions, highlighting the contribution of the equilibrium model running on the AOC hardware.</p><h3 id="Sec7">Classification case study</h3><p>For the Modified National Institute of Standards and Technology (MNIST) and Fashion-MNIST datasets, the inputs <span>\({\bf{x}}\in {{\mathbb{R}}}^{28\times 28}\)</span> are rescaled to [−1, 1] range and flattened into vectors. Hence, the IP and OP layers have dimensions 16 × 784 and 10 × 16, respectively. The test dataset results on the 256-weight AOC hardware are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig2">2d</a> (see also Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">4</a>). The AOC-predicted labels match the AOC-DT results for 99.8% of the inputs.</p><p>The AOC results demonstrate the viability of digital training with subsequent weight transfer for opto-electronic analog inference. The contribution of the equilibrium model running on the AOC further becomes apparent when comparing the AOC results with a linear classifier, which consists of digitally trained IP layer, a middle layer and an OP layer. We also train a simple feedforward model comprising an IP layer, a middle layer with tanh nonlinearity and an OP layer. Both linear classifier and feedforward models have the same number of parameters as the AOC hardware. Although the AOC achieves slightly higher accuracy (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig2">2d</a>), the simple nature of the MNIST and Fashion-MNIST datasets is unlikely to demonstrate the full potential of self-recurrent models. Looking ahead, this potential may materialize in a form of test-time compute in sequence modelling tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Geiping, J. et al. Scaling up test-time compute with latent reasoning: a recurrent depth approach. Preprint at 
                https://arxiv.org/abs/2502.05171
                
               (2025)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR34" id="ref-link-section-d430713338e1871">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Schone, M. et al. Implicit language models are RNNs: balancing parallelization and expressivity. In International Conference on Machine Learning (PMLR, 2025)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR35" id="ref-link-section-d430713338e1874">35</a></sup> or inference of generative diffusion models.</p><p>In practice, model sizes tend to exceed what a given hardware can support, including traditional GPUs. To address this, we demonstrate time-multiplexing on the AOC by training a 4,096-weight ensemble equilibrium model, composed of 16 independent 256-weight equilibrium models. The overall architecture mirrors the previous 256-weight model, but the middle layer now consists of 16 independent equilibrium models, executed sequentially on the AOC for each slice of the input <span>\({{\bf{x}}}_{\mathrm{proj}}\in {{\mathbb{R}}}^{4096}\)</span>. Classification accuracies for these time-multiplexed models are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig2">2d</a>. The time-to-solution increases linearly with the number of independent blocks in the ensemble. Across all architectures considered for classification tasks, the IP layer accounts for the majority of parameters, whereas nonlinearities within the AOC primarily drive performance differences compared with a linear classifier. Additional classification results, including ablation studies of the optical and electronic contributions with untrained IP layers, are provided in Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">C.1</a>.</p><p>Across all classification and regression tasks, the AOC-DT requires around nine iterations per input to reach convergence. On the AOC hardware, these fixed points can be achieved in 180 ns and, ideally, their sampling would occur immediately afterwards. In practice, to ensure the state stability and mitigate noise, we sample over a fixed 6.4-μs window at 6.25-MHz frequency (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig9">5</a>). As the sampling rate is about eight-times slower than the hardware round-trip time, individual iterations cannot be resolved on the AOC. Compared with classification tasks, the regression tasks show greater sensitivity to noise, requiring up to 11 repeated runs for averaging to obtain smooth curves (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">6</a>). We note that owing to the iterative process, the training and inference times of the AOC-DT running in silico are approximately nine-times slower than an equivalent feedforward model.</p><h3 id="Sec8">Noise robustness</h3><p>Equilibrium models, beyond being compute heavy, are also suitable for analog acceleration owing to the attractor nature of their iterative inference process. This provides enhanced robustness to analog noise compared with deep feedforward networks, which is a critical property for the AOC performance at scale (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig10">6b</a>).</p></div></div></section><section data-title="AOC for optimization"><div id="Sec9-section"><h2 id="Sec9">AOC for optimization</h2><div id="Sec9-content"><h3 id="Sec10">Quadratic unconstrained mixed optimization</h3><p>The QUMO formulation represents a wide class of combinatorial optimization problems aimed at minimizing the objective function <span>\(F({\bf{x}})=-\frac{1}{2}{{\bf{x}}}^{{\rm{T}}}W{\bf{x}}-{{\bf{b}}}^{{\rm{T}}}{\bf{x}}\)</span>, where the vector <b>x</b> includes binary and continuous variables, and the information about the optimization problem is encoded in the weight matrix <i>W</i> and the constant vector <b>b</b>. Without loss of generality, one may consider the values {0, 1} for binary and the interval [0, 1] for continuous variables. These variables are represented through the element-wise nonlinearity over the system state vector <b>s</b><sub><i>t</i></sub> in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ1">1</a>). The solution to the QUMO problem is the assignment of the variables <b>x</b> that minimizes the objective <i>F</i>(<b>x</b>). If the components of <b>x</b> are all binary variables, the problem reduces to the quadratic unconstrained binary optimization (QUBO) formulation. We note that the QUBO problem is equivalent to the well-known problems of minimizing the Hamiltonian of the Ising model<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Lucas, A. Ising formulations of many NP problems. Front. Phys. 2, 5 (2014)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR37" id="ref-link-section-d430713338e2090">37</a></sup> and finding the maximum cut of a weighted graph<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Barahona, F., Jünger, M. &amp; Reinelt, G. Experiments in quadratic 0-1 programming. Math. Program. 44, 127–137 (1989)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR38" id="ref-link-section-d430713338e2095">38</a></sup>.</p><p>Besides being nonlinear, most optimization problems are constrained. A problem with linear inequality constraints highlights the greater expressiveness of the QUMO over the standard QUBO formulation, commonly used across many non-traditional platforms. For example, only one additional continuous variable, typically referred to as slack variable, is required for mapping one inequality constraint to the QUMO problem with a penalty method. In contrast, the QUBO formulation suffers from a large mapping overhead: 10 to 100 binary variables are needed to represent a single constraint with either binary or unary encoding (Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.8</a>). We next demonstrate solving QUMO problems on the AOC hardware for two applications: medical image reconstruction and transaction settlement.</p><h3 id="Sec11">Medical image reconstruction case study</h3><p>We implement compressed sensing on the AOC hardware, a technique enabling accurate signal reconstruction from fewer measurements than traditionally required<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Donoho, D. L. Compressed sensing. IEEE Trans. Inf. Theory 52, 1289–1306 (2006)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR39" id="ref-link-section-d430713338e2113">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Candes, E. J. &amp; Tao, T. Near-optimal signal recovery from random projections: universal encoding strategies? IEEE Trans. Inf. Theory 52, 5406–5425 (2006)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR40" id="ref-link-section-d430713338e2116">40</a></sup>. Compressed sensing accelerates image acquisition, reducing scan times and enhancing patient comfort. For magnetic resonance imaging (MRI), a sparse image representation is typically achieved using techniques such as wavelet regularization that penalize ‘unnatural’ reconstructions. The standard regularization choice is the <i>ℓ</i><sub>1</sub>-norm, which promotes sparsity and enables optimization via convex solvers. However, the original compressed-sensing method employs the ‘<i>ℓ</i><sub>0</sub>-norm’, which counts the number of non-zero elements in a vector. Minimizing the <i>ℓ</i><sub>0</sub>-norm may yield better reconstruction in theory<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Kabashima, Y., Wadayama, T. &amp; Tanaka, T. A typical reconstruction limit for compressed sensing based on Lp-norm minimization. J. Stat. Mech. Theory Exp. 2009, L09003 (2009)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR41" id="ref-link-section-d430713338e2133">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Nakanishi-Ohno, Y., Obuchi, T., Okada, M. &amp; Kabashima, Y. Sparse approximation based on a random overcomplete basis. J. Stat. Mech. Theory Exp. 2016, 063302 (2016)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR42" id="ref-link-section-d430713338e2136">42</a></sup>, although the optimization problem is deemed impractical in this case and, hence, remains largely unexplored in applied image reconstruction tasks. With the AOC hardware, we can address this original hard problem by formulating the compressed-sensing approach as the QUMO optimization problem:</p><div id="Equ3"><p><span>$$\mathop{\min }\limits_{{\bf{x}}}\,\frac{1}{2}{\parallel {\bf{y}}-{\bf{A}}{\bf{x}}\parallel }_{2}^{2}+{\lambda }_{1}{{\bf{1}}}^{{\rm{T}}}{\boldsymbol{\sigma }}+{\lambda }_{2}{({\bf{1}}-{\boldsymbol{\sigma }})}^{{\rm{T}}}{\bf{x}}.$$</span></p><p>
                    (3)
                </p></div><p>Here the first term ensures data fidelity between measurements <span>\({\bf{y}}\in {{\mathbb{R}}}^{M}\)</span> and the image <span>\({\bf{x}}\in {{\mathbb{R}}}^{N}\)</span> in the wavelet domain, and the matrix <span>\({\bf{A}}\in {{\mathbb{R}}}^{M\times N}\)</span> represents the MRI acquisition process consisting of Fourier and inverse wavelet transforms with an undersampling mask (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Sec15">Methods</a>). To reduce the MRI scan time, the number of measurements <i>M</i> needs to be smaller than the number of pixels <i>N</i>; hence this data-fidelity term has infinitely many solutions on its own. The image pixels are normalized to <b>x</b> <span>∈</span> [0, 1]<sup><i>N</i></sup> in the wavelet domain and <b>σ</b> <span>∈</span> {0, 1}<sup><i>N</i></sup> is a binary vector that controls the sparsity of <b>x</b>. When <b>σ</b><sub><i>i</i></sub> = 0, the <i>λ</i><sub>1</sub> penalty disappears and the corresponding non-zero pixel value <b>x</b><sub><i>i</i></sub> penalizes the objective owing to the <i>λ</i><sub>2</sub> penalty. For <i>σ</i><sub><i>i</i></sub> = 1, the <i>λ</i><sub>2</sub> penalty disappears and the pixel <b>x</b><sub><i>i</i></sub> can take any value to match the measurements, albeit at <i>λ</i><sub>1</sub> penalty cost to the objective. Lastly, <b>1</b> and (<span>⋅</span>)<sup>T</sup> denote the vector of ones and the transpose operation in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ3">3</a>), respectively. The generalization of this reconstruction problem to the complex-valued variables is presented in Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.3</a>.</p><p>We realize the compressed-sensing approach on the AOC hardware for a line of the Shepp–Logan phantom image of 32 × 32 pixels (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4a</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">10</a>), formulated as a 64-variable 9-bit QUMO problem with equal split between binary and continuous variables. As an example of a realistic MRI process, we omit 37.5% of the measurements. By minimizing the data-fidelity term only, we obtain a poor reconstruction (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4a</a>), highlighting the importance of the interactions between continuous and binary variables in the QUMO formulation.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="The AOC for optimization."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: The AOC for optimization.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41586-025-09430-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="578"/></picture></a></div><div data-test="bottom-caption" id="figure-4-desc"><p><b>a</b>, Medical image reconstruction. Top: the AOC hardware realizes a compressed-sensing algorithm with the ‘<i>ℓ</i><sub>0</sub>-norm’ to reconstruct a line of the Shepp–Logan phantom image. The reconstruction process is formulated as optimization of the 64-variable QUMO instance and compared with the minimization of the data-fidelity term only in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ3">3</a>), achieving an MSE of 0.008 in the former and 0.079 in the latter cases. The greyscale bar denotes brightness values in the image, with 0 corresponding to black and 1 to white. Bottom: reconstructions by the AOC-DT for a real brain image from the FastMRI dataset at typical acceleration rates. <b>b</b>, Transaction settlement. Top: a schematic of the transaction-settlement process among multiple financial parties, with settled (green) and unsettled (red) transactions. Bottom: the number of settled transactions achieved by the AOC hardware for a 41-variable QUMO instance as a function of block coordinate descent steps. <b>c</b>, The AOC hardware solves synthetic 3-bit to 8-bit precision QUMO and QUBO instances with 16 variables, requiring fewer than 1,000 samples to reach at least ≥95% and 100% objective proximities for the QUMO and QUBO instances, respectively. The shaded regions indicate the 50% confidence interval. <b>d</b>, The time trace of 16 variables during the optimization process on the AOC hardware for a QUMO instance. The system converges to the fixed point within 30 μs, with the sampling window occurring over the last 6 μs. <b>e</b>, The relative speed-up of the AOC-DT compared with the Gurobi solver is shown for a QUMO-reformulated subset of QPLIB benchmark instances (QBL). The state-of-the-art solutions are found for instances 3,860 and 3,584. <b>f</b>, The hardest QBL instances require more than 60 s for the Gurobi solver to find the best-known solutions of 10 QBL instances in their original formulation.</p><p><a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM5">Source Data</a></p></div></div></figure></div><p>For the non-zero <i>λ</i><sub>1</sub> and <i>λ</i><sub>2</sub> penalties, we split the 64-variable QUMO problem into smaller subproblems and solve them all in the AOC hardware using the block coordinate descent (BCD) method<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Tseng, P. Convergence of a block coordinate descent method for nondifferentiable minimization. J. Optim. Theory Appl. 109, 475–494 (2001)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR43" id="ref-link-section-d430713338e2579">43</a></sup>. Solutions to each subproblem, corresponding to one step of BCD, are used to create the subsequent ones, all of which are solved using the fixed-point abstraction realized on the AOC hardware. The convergence to the optimal solution takes around 30 –40 BCD steps with 1,000–1,500 AOC iterations per BCD step. The final image reconstruction closely matches the original line, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4a</a>. We note that all QUMO instances are solved in an entirely analog manner without any digital post-processing.</p><p>To validate the QUMO formulation for compressed sensing at scale, we use the AOC-DT to reconstruct a brain scan image of 320 × 320 pixels from the FastMRI dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Zbontar, J. et al. FastMRI: an open dataset and benchmarks for accelerated MRI. Preprint at 
                https://arxiv.org/abs/1811.08839
                
               (2018)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR44" id="ref-link-section-d430713338e2589">44</a></sup>, which results in the QUMO problem with more than 200,000 variables. For the typical undersampling rates of 4 and 8, we achieve reconstructions with mean squared error (MSE) below 0.07 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4a</a>).</p><h3 id="Sec12">Transaction settlement problem case study</h3><p>For optimization in the financial domain, we use the AOC hardware to solve a transaction-settlement problem. Each securities transaction is an exchange of securities for a payment, known as a delivery-versus-payment transaction. Clearing houses process batches of such transactions; for example, the subsidiaries of the Depository Trust and Clearing Corporation (DTCC) processed securities transactions valued at US$3 quadrillion in 2023<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="DTCC 2023 Annual Report (Depository Trust &amp; Clearing Corp., 2023); 
                https://www.dtcc.com/-/media/Files/Downloads/Annual%20Report/2023/DTCC-2023-AR-Print.pdf
                
              ." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR45" id="ref-link-section-d430713338e2604">45</a></sup>. Within each batch, the transaction-settlement objective is to maximize the total number or total value of settled transactions, which is NP-hard<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Gedin, S. Securities settlement optimization using an optimization software solution. MSc thesis, KTH Royal Institute of Technology (2020)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR46" id="ref-link-section-d430713338e2608">46</a></sup>. This is a difficult optimization problem given the volume of transactions, legal constraints and additional requirements (for example, collateral and credit facilities).</p><p>A prevalent approach for solving the transaction-settlement problem is to formulate it as a linear optimization problem with binary variables and linear inequality constraints<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Gedin, S. Securities settlement optimization using an optimization software solution. MSc thesis, KTH Royal Institute of Technology (2020)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR46" id="ref-link-section-d430713338e2615">46</a></sup>. This formulation can be mapped to a QUMO problem, where the inequality constraints are efficiently incorporated into the objective function by introducing continuous slack variables (Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.8</a>).</p><p>We design a transaction-settlement instance generator that produces industrially relevant transaction-settlement scenarios for the given numbers of transactions, financial parties and assets. We also implement a pre-processing technique to eliminate trivial constraints. As an example, we generate a scenario with 46 transactions between 37 parties (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4b</a>), resulting in 30 constraints, which is reduced to an effective 41-variable QUMO instance. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4b</a>, the AOC hardware finds the globally optimal solution in seven BCD steps for this transaction-settlement scenario. Similar to reconstruction of the Shepp–Logan image, the QUMO instances across all transaction-settlement scenarios are solved in an entirely analog manner.</p><p>In addition, we evaluate several smaller-size scenarios derived from real settlement data<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Braine, L. et al. Quantum algorithms for mixed binary optimization applied to transaction settlement. IEEE Trans. Quantum Eng. 2, 1–8 (2021)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR47" id="ref-link-section-d430713338e2634">47</a></sup>. After pre-processing, these reduce to QUMO instances of 8 variables, on which the AOC hardware achieves a 100% success rate (see Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">8</a>). In contrast, quantum hardware performance on the same problems yields success rates of 40–60% (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Braine, L. et al. Quantum algorithms for mixed binary optimization applied to transaction settlement. IEEE Trans. Quantum Eng. 2, 1–8 (2021)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR47" id="ref-link-section-d430713338e2641">47</a></sup>).</p><h3 id="Sec13">Comprehensive benchmarking</h3><p>For the AOC hardware, a comprehensive evaluation is conducted on a diverse set of challenging synthetic QUMO and QUBO problems. The optimization problems include instances with 16 binary and continuous variables, with dense and sparse weight matrices up to 8-bit precision. For 100 instances, the AOC hardware achieves over 95% and 100% proximity to the optimal objectives of QUMO and QUBO instances, respectively, under 1,000 samples (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4c</a>). The typical time trace of 16 variables during the optimization process on the AOC hardware is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4d</a>.</p><p>Using the AOC-DT, algorithmic performance is validated on the hardest quadratic binary problems with linear inequality constraints from the quadratic programming library (QPLIB) benchmark<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Furini, F. et al. QPLIB: a library of quadratic programming instances. Math. Program. Comput. 11, 237–265 (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR23" id="ref-link-section-d430713338e2662">23</a></sup>, formulated as QUMO instances. The AOC approach is compared with the commercial Gurobi solver<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Gurobi Optimizer Reference Manual (2023); 
                http://www.gurobi.com
                
              ." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR22" id="ref-link-section-d430713338e2666">22</a></sup>, which requires over a minute to reach the best-known solutions for these problems. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig4">4e,f</a> shows that the AOC-DT is up to three orders of magnitude faster in all instances, except for two, one of which it is unable to solve. Moreover, the AOC solver discovers the new best solutions for two heavily constrained instances (3,584 and 3,860), each with over 500 binary and 10,000 continuous variables in the QUMO formulation, in about 40 s. For instance, for 3,584, Gurobi matches the AOC solution in about 54,000 s, whereas proving its global optimality takes 4.5 days. The details of the AOC-DT parameters are provided in Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.4</a> with additional benchmarks in Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.5</a>.</p></div></div></section><section data-title="Discussion"><div id="Sec14-section"><h2 id="Sec14">Discussion</h2><div id="Sec14-content"><p>Addressing practical applications with the AOC necessitates hardware scalability from hundreds of millions to billions of weights. For example, typical MRI scans with resolutions around 100,000 pixels require systems capable of processing around 20,000 variables when using decomposition techniques such as BCD, which is equivalent to handling around 400 million weights. Similarly, deep learning models with a few billion weights are standard for real-world applications<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Javaheripi, M. et al. Phi-2: the surprising power of small language models. Microsoft Research Blog 
                https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models
                
               (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR48" id="ref-link-section-d430713338e2689">48</a></sup>, which, with mixture of experts models or techniques such as ensembling, could be reduced to many parallel models that are an order of magnitude smaller. The AOC hardware has the potential to scale to these requirements through a modular architecture that decomposes the core optical matrix–vector multiplication operation into multiplication of smaller subvectors and submatrices.</p><p>At scale, the AOC hardware will consist of multiple modules, each performing a part of the full-weight matrix multiplication. Each module will include a microLED array, a photodetector array and an SLM. With current SLMs supporting 4 million pixels, matrices with up to 4 million weights could be realized. When combined with integrated driving electronics, this results in a miniaturized module with dimensions of around 4 cm. Utilizing the third dimension for matrix–vector multiplication thus enables scalable in-memory computing. In contrast, emerging planar optical computers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Shastri, B. J. et al. Photonics for artificial intelligence and neuromorphic computing. Nat. Photon. 15, 102–114 (2021)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR1" id="ref-link-section-d430713338e2696">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Shen, Y. et al. Deep learning with coherent nanophotonic circuits. Nat. Photon. 11, 441–446 (2017)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR2" id="ref-link-section-d430713338e2699">2</a></sup> are constrained by the reticle-size limit of the chip area that is used for both routing and computing, which limits the matrix size<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="McMahon, P. L. The physics of optical computing. Nat. Rev. Phys. 5, 717–734 (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR28" id="ref-link-section-d430713338e2703">28</a></sup>. Furthermore, as microLEDs are incoherent light sources, optical paths need to be matched only within the system bandwidth (gigahertz) rather than the source wavelength (hundreds of terahertz), which is a fundamental manufacturability advantage over coherent systems. Achieving the required miniaturization, however, is both a challenge and an opportunity to drive advancements in 3D optical technologies with broader applications. The miniaturized optical modules are coupled with integrated analog electronic models in a 3D mesh (Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig8">4</a>). These modules further aggregate the output vectors from the optical modules and perform the remaining compute primitives.</p><p>We envision that the AOC can support models with 0.1 billion to 2 billion weights, requiring 50 to 1,000 optical modules. The module count can be halved if a single optical module supports both positive and negative weights<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Wang, T. et al. An optical neural network using less than 1 photon per multiplication. Nat. Commun. 13, 123 (2022)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR13" id="ref-link-section-d430713338e2713">13</a></sup>. All AOC components, including microLEDs, photodetectors, SLMs and analog electronics, have an existing and growing manufacturing ecosystem with wafer-scale production. At the same time, complementing optics with analog electronics offers numerous opportunities to expand the compute primitives, including nonlinearities, the hardware can support, thereby enhancing its expressiveness.</p><p>The operational speed and power consumption of the AOC dictate its energy efficiency. The speed is limited by the bandwidth of the opto-electronic components, typically 2 GHz or higher<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Pezeshki, B., Tselikov, A., Kalman, R. &amp; Danesh, C. Wide and parallel LED-based optical links using multi-core fiber for chip-to-chip communications. In Optical Fiber Communication Conference (OFC) 2021 
                https://opg.optica.org/abstract.cfm?URI=OFC-2021-F3A.1
                
               (2021)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR49" id="ref-link-section-d430713338e2720">49</a></sup>. For a 100-million-weight matrix with 25 AOC modules, the power consumption is estimated to be 800 W, resulting in a computing speed of 400 peta-OPS and an efficiency of 500 TOPS W<sup>−1</sup> (2 fJ per operation) at 8-bit weight precision (Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">A.1</a>). In contrast, the latest GPUs achieve a system efficiency of up to 4.5 TOPS W<sup>−1</sup> at the same precision for dense matrices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Nvidia H100 Tensor Core GPU. Nvidia 
                https://www.nvidia.com/en-us/data-center/h100/
                
               (2023)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR24" id="ref-link-section-d430713338e2731">24</a></sup>.</p><p>In conclusion, the AOC architecture shows promise for scaling to practical ML and optimization tasks, offering a potential 100-fold improvement in power efficiency. The current AOC hardware uses a rapid fixed-point search to power inference tasks, such as regression and classification, using equilibrium models with promising reasoning capabilities, and to successfully solve QUMO problems including medical image reconstruction and transaction settlement. Cross-validation with the digital twin, coupled with evaluation on large problems, offers confidence in the hardware’s performance as it scales. Looking ahead, the AOC’s co-design approach—aligning the hardware with the ML and optimization algorithms—could spur a flywheel of future innovations in hardware and algorithms, pivotal for a sustainable future of computing.</p></div></div></section><section data-title="Methods"><div id="Sec15-section"><h2 id="Sec15">Methods</h2><div id="Sec15-content"><h3 id="Sec16">Experimental set-up</h3><p>The key components of our experimental set-up are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig1">1a</a> and Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig5">1</a>.</p><h4 id="Sec17">Optical subsystem</h4><p>The optical subsystem performs matrix–vector multiplication. The basic components are the optical sources (input vector), a system of fan-out optics to project the light onto the modulator matrix and a system of fan-in optics to project the light onto a photodetector array (output vector). The corresponding schematic is shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig6">2</a>.</p><p>The incoherent light sources are an array of 16 independently addressable microLEDs. Each microLED is driven with a bias current and an offset voltage. The variable value is encoded by the light intensity, with a value of zero corresponding to the microLED bias point. Mathematical positive values are represented by microLED drive currents greater than the bias value. Negative values are represented by drive currents less than the bias value. The diameter of each emitter is 50 μm and the pitch is 75 μm. The sources are fabricated in gallium nitride wafers on a sapphire substrate and the die is wire-bonded onto a printed circuit board (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig1">1c</a>). The emission spectrum is centred at 520 nm with a full-width of half-maximum of 35 nm and the operational −3-dB bandwidth is 200 MHz at 20 mA, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">1</a>.</p><p>After the sources, there is a polarizing beamsplitter (PBS). From this point, there are two equivalent optical paths in this set-up. Each path performs two functions: first, they allow us to use both polarizations of the unpolarized light output; second, they allow us to perform non-negative and non-positive multiplications with only intensity modulation. Each path contains one amplitude modulator matrix and one photodetector array. The modulator matrix is a reflective parallel-aligned nematic liquid-crystal SLM. We refer to the first part of the optical system as the fan-out system. The task of this fan-out system is to image the microLEDs onto the SLM, where the weights are displayed, and to spread the light horizontally into lines. The microLEDs are arranged in a one-dimensional line (let this be the <i>y</i> axis) and are imaged onto the SLM using a 4F system composed of a high-numerical-aperture (Thorlabs TL10X-2P, numerical aperture 0.5, ×10 magnification, 22-mm field number) collection objective and a lower-numerical-aperture lens group composed of 2 achromatic doublets with combined focal length 77 mm. There is a cylindrical lens, Thorlabs LJ1558L1, in infinity space of this 4F system. This lens adds defocus to the image of the source array on the SLM but only in the <i>x</i> direction, so that the projected light pattern is a set of long horizontal lines, one per microLED. Each matrix element occupies a patch of 12 (height) × 10 (width) pixels of the modulator array. An 8-bit look-up table is used to linearize the SLM response as a function of grey level.</p><p>The SLM is imaged onto the photodetector array using a 4F system (the fan-in system). The first lens group of the fan-in is the same as the second lens group of the fan-out system as this is in double pass. From here, the light is directed towards the intended photodetector array through a second PBS. The light from each column of the SLM is collected by an array of 16 silicon photodetectors to perform the required summation operation. The active area of each element is 3.6 × 0.075 mm<sup>2</sup>. The photodetectors are on a pitch of 0.125 mm. The operation bandwidth is 490 MHz at −10 V measured at 600 nm.</p><h4 id="Sec18">Analog electronic subsystem</h4><p>After the photodetector array, the signals are in the analog electronic domain. The photocurrents from each photodetector element are amplified by a linear trans-impedance amplifier (Analog Devices MAX4066). Each trans-impedance amplifier provides 25-kΩ gain and is characterized by an input referred noise of <span>\(3\,{\rm{pA}}\,\sqrt{\text{Hz}}\)</span> and has differential outputs. The corresponding 2 sets (1 per photodetector board) of 16 differential pairs of signals are fed to the main boards where the per-channel nonlinear operation and other analog electronic processing is carried out. Each of the 16 signals sees the following circuitry: (1) a variable gain amplifier (VGA; Texas Instruments VCA824) to allow the input signal range to be set and equalized across channels; (2) a difference amplifier to perform the operation of subtracting the negative input signal from the positive one and achieve signed voltages (signed multiplications); (3) a VGA that adds and subtracts signals from the described path, referred to as gradient term, to the annealing and momentum terms, as per equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ1">1</a>), while providing a common gain control to all these paths; (4) an electronic switch (ADG659) to open and close the loop to set and reset the solving state; (5) a buffer amplifier to distribute the signal to the gradient, annealing and momentum paths; (6) a bipolar differential pair to implement the tanh nonlinearity; (7) a VGA to adjust the signal level between the nonlinearity and the required voltage and current onto the microLED alternating-current input circuit. Both the annealing and momentum paths have VGAs with a common external control so that we can implement time-varying annealing and momentum schedules.</p><p>Each channel also has an offset to the common control signal added to allow minor adjustment or correction of channel-to-channel variations. The other VGAs are set with digital-to-analog converters controlled over an inter-integrated circuit (I2C) bus. This allows slower control at per-experiment timescales.</p><h4 id="Sec19">Nonlinearity</h4><p>The per-channel nonlinear function is an approximation to a tanh. This is shown in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">5d</a>. The system is designed so that all signals follow the same path through the solver. For ML workloads, the input domain of the tanh function is unrestricted by hardware; there are no gain variations across channels. The trained weights and equilibrium model input ensure that signals evolve accurately. For optimization workloads, binary and continuous variables require different handling in hardware. Here we set the gain after the trans-impedance amplifier and before the tanh nonlinearity to be lower for continuous variables than for binary variables. This adjustment ensures that the input domain of the nonlinearity results in a more linear output for continuous variables than for binary variables.</p><h4 id="Sec20">Evaluation of matrix–vector multiplication accuracy</h4><p>We characterized and calibrated the key opto-electronic and electronic components to equalize the gain of each AOC path. For example, we calibrate the optical paths by applying a set of 93 reference matrices and for each we digitally compute the result of the vector–matrix product. We then adjust the gain per channel slightly so that, averaged over the set of 93 computed vectors, the AOC result is as close as possible to the digital result.</p><p>Following this, the accuracy of the matrix–vector multiplication is characterized using the same 93 reference matrices on each SLM and measuring the output of the system, shown in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">3a</a>. For each reference matrix in the set, we calculate the MSE between the known and the measured output. The mean MSE across all dot products is 5.5 × 10<sup>−3</sup>, and the matrix–vector multiplication MSE as a function of matrix (instance) is shown in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">3b</a>. For these experiments, we configure the system in open-loop mode without feedback and turn off the annealing and momentum paths.</p><h3 id="Sec21">ML methods</h3><h4 id="Sec22">Training and digital twin</h4><p>In commercial deployments, training consumes less than 10% of the energy and, hence, is not targeted by the AOC. The equilibrium models are trained through our digital twin, which is based on equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ2">2</a>). In the digital domain during training, the convergence criterion is set to <span>∣</span><span>∣</span><b>s</b><sub><i>t</i>+1</sub> − <b>s</b><sub><i>t</i></sub><span>∣</span><span>∣</span> &lt; <i>ε</i>, with <i>ε</i> = 10<sup>−3</sup>. The AOC-DT models up to seven non-idealities measured on the AOC device; each non-ideality can be switched on and off (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">5</a>). The AOC-DT is implemented as a Pytorch module with the weight matrix <i>W</i> and bias terms <i>b</i>, as well as the gain <i>β</i> as trainable parameters. The weight matrix is normalized to fulfil <span>∥</span><i>W</i><span>∥</span><sub><i>∞</i></sub> = 1 throughout training to simulate the passive SLM. The numeric scale of the matrix is instead modelled by the gain <i>β</i>. This separation of scale is necessary as several nonlinear non-idealities occur between the matrix multiplication and the gain in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ2">2</a>), as discussed in Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">D</a>.</p><p>The weight matrix is initialized with the default Pytorch initialization for a 16 × 16 matrix, the bias term is initialized to 0 and <i>β</i> is initialized at 1. We trained all models with a batch size of <i>B</i> = 8, at a learning rate of <i>η</i> = 3 × 10<sup>−4</sup> for MNIST and Fashion-MNIST and <i>η</i> = 7 × 10<sup>−4</sup> for regression tasks. We used the Adam optimizer<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Kingma, D. P. Adam: a method for stochastic optimization. Preprint at 
                https://arxiv.org/abs/1412.6980
                
               (2014)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR50" id="ref-link-section-d430713338e2957">50</a></sup>. In all cases, models are trained end-to-end, with the equilibrium-section trained through our AOC-DT using the implicit gradient method<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Bai, S., Kolter, J. Z. &amp; Koltun, V. Deep equilibrium models. Adv. Neural Inf. Process. Syst. 32, 690–701 (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR17" id="ref-link-section-d430713338e2961">17</a></sup>, which avoids storing activations for the fixed-point iterations. This decouples memory cost from iteration depth as intermediate activations do not need to be stored. In all experiments, the <i>α</i> gain in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ2">2</a>) is set to 0.5 to strike a balance between overall signal amplitude and speed of convergence. Low <i>α</i> values cause the signal to be too weak, resulting in a low signal-to-noise ratio (Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">D</a>).</p><h4 id="Sec23">Inference and export to the AOC</h4><p>Once training has completed, the weight matrix <i>W</i> is quantized to signed 9-bit integers using</p><div id="Equ4"><p><span>$$W\approx \frac{\text{max}(W)}{255}\,\text{clamp}\,{\left[\text{round}\,\left(\frac{W}{\text{max}(W)}\times 255\right)\right]}_{\text{min}=-256}^{\text{max}=255}\,=\,\frac{\text{max}(W)}{255}{W}_{{\rm{Q}}},$$</span></p><p>
                    (4)
                </p></div><p>with the rounded and clamped matrix on the right-hand side being the quantized weight matrix <i>W</i><sub>Q</sub>. Whenever we report AOC-DT results, we report results obtained with the quantized matrix.</p><p>Exporting trained models to the AOC requires several further steps. First, the model inputs <b>x</b> and the bias term <b>b</b> need to be condensed into a single vector <b>b</b><sub>AOC</sub> = <b>b</b> + <b>x</b> followed by clamp to ensure the values fit into the dynamic range of the AOC device (Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">D</a>). Second, as the optical matrix multiplication is implemented using SLMs, elements of the weight matrix are bounded by one such that all quantization-related factors disappear. However, the original maximum element of the matrix max(<i>W</i>) needs to be re-injected, which we achieve via the <i>β</i> gain in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ2">2</a>), approximately restoring the original matrix <i>W</i>.</p><p>The quantized matrix is split into positive and negative parts, <span>\({W}_{{\rm{Q}}}={W}_{{\rm{Q}}}^{+}-{W}_{{\rm{Q}}}^{-}\)</span>, and each part is displayed on its respective SLM.</p><h4 id="Sec24">AOC sampling and workflow</h4><p>Each classification instance (that is, MNIST or Fashion-MNIST test image) is run once on the AOC, and the fixed point is sampled at the point marked in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">3</a> after a short 2.5<i>-</i>μs cooldown window after the switch is closed, as shown in Extended Data Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig9">5a,b</a>. The sampling window extends over 40 samples at 6.25 MHz, corresponding to 6.4 μs. This ensures that the search of fixed points for the equilibrium models happens entirely in the analog domain. Once sampled, we digitally project the vector into the output space. For classification, the input is projected from 784 to 16 dimensions, the output is projected from 16 to 10 classes. The label is then determined by the index of the largest element in the output vector (argument-max). For regression tasks, the IP and OP layers transform a scalar to 16 dimensions and back, respectively. The MSE results in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig2">2c</a> were obtained by averaging over 11 repeats for each input. This means that we restart the solution process 11 times, including the sampling window, and average the resulting latent fixed-point vectors. Importantly, the solve-to-solve variability appears to be centred close to the curve produced by the AOC-DT, enabling us to average this variability out (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">6</a>).</p><h4 id="Sec25">The 4,096-weight ensemble model</h4><p>We can expand the model sizes supported by the hardware by using an ensemble of small models that fit on it. These smaller 256-weight models are independent at inference time but are trained jointly by receiving slices 16-sized slices of a larger input vector and stacking their outputs before the OP. To scale to a 4,096-weight equilibrium model, we expand the input space from 16 to 16 × 16 = 4,096 dimensions and the output space from 10 to 10 × 16 = 160 dimensions. The IP matrix is consequently a 784 × 4,096-shaped matrix and the OP matrix is shaped 160 × 10. MNIST or Fashion-MNIST images are scaled to the range [−1, 1] and, projected to 4,096 dimensions and split into 16 slices of 16 dimensions. Each of the 16 equilibrium models then runs its respective slice of input vectors to a fixed-point. Once all 16 models are run on the AOC, we concatenate outputs and project them into the 10-dimensional output space where the largest dimension determines the predicted cipher.</p><h4 id="Sec26">Nonlinear regression</h4><p>The first curve (I) is a Gaussian rescaled such that the Gaussian curve approximately stretches from −1 to 1, <span>\({f}_{{\rm{I}}}(x)=2{{\rm{e}}}^{-{x}^{2}/2{\sigma }^{2}}-1\)</span> for <i>σ</i> = 0.25 and <i>x</i> <span>∈</span> [−1, 1]. The second curve (II) is given by <span>\({f}_{{\rm{II}}}(x)=\sqrt{| x| }\,\sin (3{\rm{\pi }}x)\)</span>. For training sets, we choose 10,000 equidistant points <i>x</i><sub><i>i</i></sub> in the range [−1, 1] whereas for test regression datasets, we choose 200 points randomly <i>x</i><sub><i>i</i></sub> ≈ <i>U</i>([−1, 1]).</p><h4 id="Sec27">Error estimation</h4><p>For regression tasks, we concatenate the 40 samples from all 11 repeats and calculate the standard deviation per point on the curve.</p><h4 id="Sec28">Classification datasets</h4><p>We trained the MNIST and Fashion-MNIST models on 48,000 images from their respective training set, validated on a set of 12,000 images and tested them on the full test set comprising 10,000 images.</p><h4 id="Sec29">Error estimation</h4><p>For experimental results, the error bars in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Fig2">2d</a> were estimated using a Bayesian approach for the decision variable <i>c</i><sub><i>t</i></sub> <span>∈</span> {0, 1, …, 9} for each sample <i>t</i> along the sampling window per image. We assume an uninformative prior <i>p</i>(<i>c</i><sub><i>t</i></sub>) = beta(1, 1), which we then update with the observed number of correct decisions <i>n</i><sub>success</sub> and failures <i>n</i><sub>failure</sub> over the sampling window. The variance of the conjugate posterior of a beta distribution is given by <span>\(\mathrm{Var}({c}_{t}| {n}_{\mathrm{success}},{n}_{{\rm{failure}}})=\frac{(1+{n}_{\mathrm{success}})(1+{n}_{\mathrm{failure}})}{{(2+{n}_{\mathrm{success}}+{n}_{\mathrm{failure}})}^{2}(3+{n}_{\mathrm{success}}+{n}_{\mathrm{failure}})}\)</span>. We use this to estimate the variance and, by taking the square root, the standard deviation per input image. The dataset error bars are then estimated as the mean of the standard deviations over all members of the dataset.</p><h3 id="Sec30">Optimization methods</h3><h4 id="Sec31">Positive and negative problem weights</h4><p>To address optimization problems involving positive and negative weights on the AOC hardware, QUMO instances without linear terms can have up to eight variables, which applies to both transaction-settlement scenarios and reconstruction of one-dimensional line of the Shepp–Logan phantom image. The weight matrices are unsigned in synthetic QUMO and QUBO hardware benchmarks; hence the AOC hardware can accommodate up to 16-variable instances in the absence of linear terms. Such instance size difference arises because, when both positive and negative weights are present, non-idealities in the dual-SLM configuration reduce the accuracy of matrix–vector multiplication. To mitigate this, a single SLM is used to process both positive and negative weights, effectively halving the number of variables per instance.</p><h4 id="Sec32">Industrial optimization problems</h4><p>For the transaction-settlement scenario and the Shepp–Logan phantom image slice, their 41-variable and 64-variable QUMO instances are decomposed into smaller 7-variable QUMO instances. For each of these subinstances, the 7 variables are connected with the rest of the variables via a linear vector <b>b</b>, which is incorporated into the quadratic matrix <i>W</i> via an additional binary variable. This decomposition is repeated for each subinstance and the linear vector <b>b</b> is updated at the end of each BCD iteration to create the next QUMO instance. Such an approach yields 8-variable QUMO instances and a single SLM is used to represent their positive and negative matrix elements, with analog electronics handling their subtraction, which effectively utilizes the full 16-variable capacity available in hardware. The required number of BCD iterations varies depending on factors such as the initial random state of the optimization instance variables, the selection of variable blocks among subinstances, and the order in which they are optimized.</p><p>For the one-dimensional Shepp–Logan phantom image, 12 out of 32 measurements are omitted, corresponding to a 37.5% data loss or a 1.6 undersampling (acceleration) rate. Although typical MRI acceleration ranges from 2 to 8, this rate is used here owing to the image’s non-smoothness at a 32-pixel resolution.</p><h4 id="Sec33">Binary and continuous variables</h4><p>In the AOC, binary variables are encoded using a hyperbolic tangent function, whereas continuous variables utilize the near-linear region of the function, connecting optimization variables to state variables via <b>x</b> = <i>f</i>(<b>s</b>). In simulations at scale with the AOC-DT, linear and sign functions are used for continuous and binary variables, respectively.</p><h4 id="Sec34">Hardware QUMO instances</h4><p>To ensure that some variables take indeed continuous values in the global optimal solution, we plant random continuous values and generate synthetic 16-variable QUMO instances. As the number of continuous variables increases for a given problem size, the problem instances become computationally easier to solve. Consequently, we consider instances with up to eight continuous variables.</p><h4 id="Sec35">Hardware QUBO instances</h4><p>We generate up to 8-bit dense and sparse instances. The sparse instances belong to the QUBO model on three-regular graphs that are NP-hard<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Garey, M. R., Johnson, D. S. &amp; Stockmeyer, L. Some simplified NP-complete problems. In Proc. 6th Annual ACM Symposium on Theory of Computing 47–63 (1974)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR51" id="ref-link-section-d430713338e3860">51</a></sup>, although NP-hardness does not imply that every random instance is difficult to solve. To make these instances more challenging to solve, we verify that their global objective minimizer is distinct from the signs of the principal eigenvector of the weight matrix<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Kalinin, K. P. &amp; Berloff, N. G. Computational complexity continuum within Ising formulation of NP problems. Commun. Phys. 5, 20 (2022)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR52" id="ref-link-section-d430713338e3864">52</a></sup>.</p><h4 id="Sec36">QPLIB benchmark</h4><p>The QPLIB is a library of quadratic programming instances<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Furini, F. et al. QPLIB: a library of quadratic programming instances. Math. Program. Comput. 11, 237–265 (2019)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR23" id="ref-link-section-d430713338e3876">23</a></sup> collected over almost a year-long open call from various communities, with the selected instances being challenging for state-of-the-art solvers. As described in the main part of the paper, we consider only the hardest instances within the QPLIB:QBL class of problems, which contains instances with quadratic objective and linear inequality constraints. The QPLIB:QCBO class of problems, which contains instances with quadratic objective and linear equality constraints, and the QPLIB:QBN class of problems, which contains QUBO instances, are considered in Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.5</a>.</p><h4 id="Sec37">AOC-DT operation and parameters</h4><p>The distinction of the AOC-DT algorithm is the simultaneous inclusion of both momentum and annealing terms, which markedly improves the performance of the standard steepest gradient-descent method on non-convex optimization problems. Typically, multiple hyperparameters need to be calibrated for heuristic methods to achieve their best performance in solving optimization problems. We consider <span>\(\alpha (t)=1-\widehat{\alpha }(t)\)</span>, where <span>\(\widehat{\alpha }(t)\)</span> is a linearly decreasing function from some initial value <i>α</i><sub>0</sub> to 0 over time. From the hardware perspective, such an annealing schedule provides an explicit stopping criteria, which is an advantage for an all-analog hardware implementation as it avoids the complexity of multiple intermediate readouts that stochastic heuristic approaches suffer from<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Reifenstein, S., Kako, S., Khoyratee, F., Leleu, T. &amp; Yamamoto, Y. Coherent Ising machines with optical error correction circuits. Adv. Quantum Technol. 4, 2100077 (2021)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR53" id="ref-link-section-d430713338e3984">53</a></sup>. In principle, the three main parameters {<i>α</i><sub>0</sub>, <i>β</i>, <i>γ</i>} of the AOC fixed-point update rule need to be adjusted for each optimization instance. In our simulations, we notice that the algorithm is less sensitive to the momentum parameter value, whereas the <i>α</i><sub>0</sub> and <i>β</i> values substantially affect the solution quality. We further perform a linear stability analysis of the algorithm to evaluate reasonable exploration regions for these two parameters and find that by scaling the <i>β</i> parameter as <i>β</i> = <i>β</i><sub>0</sub>/<i>λ</i><sub>largest</sub>, where <i>λ</i><sub>largest</sub> is the largest eigenvalue of the weight matrix <i>W</i>, we get scaled parameters <i>β</i><sub>0</sub> and <i>α</i><sub>0</sub> being in a similar optimal unit range across a wide range of problems.</p><p>We design a two-phase approach for the AOC-DT to operate similar to a black-box solver that can quickly adjust the critical parameters within the given time limit. During the ‘exploration’ phase, we evaluate the relative algorithm performance across a vast range of parameters (<i>α</i><sub>0</sub>, <i>β</i><sub>0</sub>). A subset of ‘good’ parameters is then passed for more extensive investigation in the ‘deep search’ phase (Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.1</a>).</p><p>We note that for two QPLIB:QUMO instances, namely, 5,935 and 5,962, we developed a pre-processing technique that greedily picks variables with the highest impact on the objective functions and considers their possible values, which is accounted in the reported time speed-up of the AOC-DT.</p><h4 id="Sec38">Competing solvers</h4><p>For a fair comparison, we ensure that all methods use similar computing resources. Although the implementation of GPU- or central-processing-unit-based solvers can require highly varying engineering efforts, we try to estimate the cost of running solvers on the hardware, on which they are designed to run, and vary the time limit across solvers accordingly to ensure similar cost per solver run. In what follows, the Julia-based AOC-DT runs on a GV100 GPU for 5–300 s per instance across all benchmarks. In the case of Gurobi, our licence allows us to use only up to eight cores, and its time to achieve the best solution for the first time is used (not the time to prove its optimality).</p><p>More details about the AOC hardware and the AOC-DT performance on different optimization instances are provided in Supplementary Information section <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM1">G.5</a>.</p></div></div></section>
                    
                </div><div>
                <section data-title="Data availability"><div id="data-availability-section"><h2 id="data-availability">Data availability</h2><p>All data supporting the findings of this study are available in source data provided with the paper and via Zenodo at <a href="https://doi.org/10.5281/zenodo.15088326">https://doi.org/10.5281/zenodo.15088326</a> (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Chi, J. et al. Datasets for “The analog optical computer for machine learning inference and combinatorial optimization”. Zenodo 
                https://doi.org/10.5281/zenodo.15088326
                
               (2025)." href="https://www.nature.com/articles/s41586-025-09430-z#ref-CR54" id="ref-link-section-d430713338e4176">54</a></sup>). <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Sec42">Source data</a> are provided with this paper.</p></div></section><section data-title="Code availability"><div id="code-availability-section"><h2 id="code-availability">Code availability</h2><div id="code-availability-content">
            
            <p>Code for the AOC digital twin supporting optimization and machine learning models will be released upon publication at <a href="https://microsoft.github.io/AOCoptimizer.jl">https://microsoft.github.io/AOCoptimizer.jl</a> and <a href="https://github.com/microsoft/aoc">https://github.com/microsoft/aoc</a> (under MIT license).</p>
          </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Shastri, B. J. et al. Photonics for artificial intelligence and neuromorphic computing. <i>Nat. Photon.</i> <b>15</b>, 102–114 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41566-020-00754-y" data-track-item_id="10.1038/s41566-020-00754-y" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41566-020-00754-y" aria-label="Article reference 1" data-doi="10.1038/s41566-020-00754-y">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021NaPho..15..102S" aria-label="ADS reference 1">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXis12htLs%3D" aria-label="CAS reference 1">CAS</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Photonics%20for%20artificial%20intelligence%20and%20neuromorphic%20computing&amp;journal=Nat.%20Photon.&amp;doi=10.1038%2Fs41566-020-00754-y&amp;volume=15&amp;pages=102-114&amp;publication_year=2021&amp;author=Shastri%2CBJ">
                    Google Scholar</a> 
                </p></li><li data-counter="2."><p id="ref-CR2">Shen, Y. et al. Deep learning with coherent nanophotonic circuits. <i>Nat. Photon.</i> <b>11</b>, 441–446 (2017).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nphoton.2017.93" data-track-item_id="10.1038/nphoton.2017.93" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnphoton.2017.93" aria-label="Article reference 2" data-doi="10.1038/nphoton.2017.93">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2017NaPho..11..441S" aria-label="ADS reference 2">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXhtVSjt7bJ" aria-label="CAS reference 2">CAS</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20with%20coherent%20nanophotonic%20circuits&amp;journal=Nat.%20Photon.&amp;doi=10.1038%2Fnphoton.2017.93&amp;volume=11&amp;pages=441-446&amp;publication_year=2017&amp;author=Shen%2CY">
                    Google Scholar</a> 
                </p></li><li data-counter="3."><p id="ref-CR3">Chen, Y. et al. All-analog photoelectronic chip for high-speed vision tasks. <i>Nature</i> <b>623</b>, 48–57 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-023-06558-8" data-track-item_id="10.1038/s41586-023-06558-8" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-023-06558-8" aria-label="Article reference 3" data-doi="10.1038/s41586-023-06558-8">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2023Natur.623...48C" aria-label="ADS reference 3">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXit1SlsbjN" aria-label="CAS reference 3">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37880362" aria-label="PubMed reference 3">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10620079" aria-label="PubMed Central reference 3">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=All-analog%20photoelectronic%20chip%20for%20high-speed%20vision%20tasks&amp;journal=Nature&amp;doi=10.1038%2Fs41586-023-06558-8&amp;volume=623&amp;pages=48-57&amp;publication_year=2023&amp;author=Chen%2CY">
                    Google Scholar</a> 
                </p></li><li data-counter="4."><p id="ref-CR4">McMahon, P. L. et al. A fully programmable 100-spin coherent ising machine with all-to-all connections. <i>Science</i> <b>354</b>, 614–617 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1126/science.aah5178" data-track-item_id="10.1126/science.aah5178" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.aah5178" aria-label="Article reference 4" data-doi="10.1126/science.aah5178">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016Sci...354..614M" aria-label="ADS reference 4">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3494769" aria-label="MathSciNet reference 4">MathSciNet</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XhslKgt7vJ" aria-label="CAS reference 4">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27811274" aria-label="PubMed reference 4">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20fully%20programmable%20100-spin%20coherent%20ising%20machine%20with%20all-to-all%20connections&amp;journal=Science&amp;doi=10.1126%2Fscience.aah5178&amp;volume=354&amp;pages=614-617&amp;publication_year=2016&amp;author=McMahon%2CPL">
                    Google Scholar</a> 
                </p></li><li data-counter="5."><p id="ref-CR5">Cai, F. et al. Power-efficient combinatorial optimization using intrinsic noise in memristor Hopfield neural networks. <i>Nat. Electron.</i> <b>3</b>, 409–418 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41928-020-0436-6" data-track-item_id="10.1038/s41928-020-0436-6" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41928-020-0436-6" aria-label="Article reference 5" data-doi="10.1038/s41928-020-0436-6">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Power-efficient%20combinatorial%20optimization%20using%20intrinsic%20noise%20in%20memristor%20Hopfield%20neural%20networks&amp;journal=Nat.%20Electron.&amp;doi=10.1038%2Fs41928-020-0436-6&amp;volume=3&amp;pages=409-418&amp;publication_year=2020&amp;author=Cai%2CF">
                    Google Scholar</a> 
                </p></li><li data-counter="6."><p id="ref-CR6">Hao, L., William, M., Hanzhao, Y., Sachin, S. &amp; Kim, C. H. An Ising solver chip based on coupled ring oscillators with a 48-node all-to-all connected array architecture. <i>Nat. Electron.</i> <b>6</b>, 771–778 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41928-023-01021-y" data-track-item_id="10.1038/s41928-023-01021-y" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41928-023-01021-y" aria-label="Article reference 6" data-doi="10.1038/s41928-023-01021-y">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20Ising%20solver%20chip%20based%20on%20coupled%20ring%20oscillators%20with%20a%2048-node%20all-to-all%20connected%20array%20architecture&amp;journal=Nat.%20Electron.&amp;doi=10.1038%2Fs41928-023-01021-y&amp;volume=6&amp;pages=771-778&amp;publication_year=2023&amp;author=Hao%2CL&amp;author=William%2CM&amp;author=Hanzhao%2CY&amp;author=Sachin%2CS&amp;author=Kim%2CCH">
                    Google Scholar</a> 
                </p></li><li data-counter="7."><p id="ref-CR7">Johnson, M. W. et al. Quantum annealing with manufactured spins. <i>Nature</i> <b>473</b>, 194–198 (2011).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nature10012" data-track-item_id="10.1038/nature10012" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnature10012" aria-label="Article reference 7" data-doi="10.1038/nature10012">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2011Natur.473..194J" aria-label="ADS reference 7">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3MXlvFGmurw%3D" aria-label="CAS reference 7">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21562559" aria-label="PubMed reference 7">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantum%20annealing%20with%20manufactured%20spins&amp;journal=Nature&amp;doi=10.1038%2Fnature10012&amp;volume=473&amp;pages=194-198&amp;publication_year=2011&amp;author=Johnson%2CMW">
                    Google Scholar</a> 
                </p></li><li data-counter="8."><p id="ref-CR8">Dally, B. Trends in deep learning hardware: Bill Dally (NVIDIA) <i>YouTube</i> <a href="https://www.youtube.com/watch?v=kLiwvnr4L80&amp;t=770s" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.youtube.com/watch?v=kLiwvnr4L80&amp;t=770s">https://www.youtube.com/watch?v=kLiwvnr4L80&amp;t=770s</a> (2023).</p></li><li data-counter="9."><p id="ref-CR9">Psaltis, D. &amp; Farhat, N. Optical information processing based on an associative memory model of neural nets with thresholding and feedback. <i>Opt. Lett.</i> <b>10</b>, 98–100 (1985).</p></li><li data-counter="10."><p id="ref-CR10">Feldmann, J. et al. Parallel convolutional processing using an integrated photonic tensor core. <i>Nature</i> <b>589</b>, 52–58 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-020-03070-1" data-track-item_id="10.1038/s41586-020-03070-1" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-020-03070-1" aria-label="Article reference 10" data-doi="10.1038/s41586-020-03070-1">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021Natur.589...52F" aria-label="ADS reference 10">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXnsFWisw%3D%3D" aria-label="CAS reference 10">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33408373" aria-label="PubMed reference 10">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Parallel%20convolutional%20processing%20using%20an%20integrated%20photonic%20tensor%20core&amp;journal=Nature&amp;doi=10.1038%2Fs41586-020-03070-1&amp;volume=589&amp;pages=52-58&amp;publication_year=2021&amp;author=Feldmann%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="11."><p id="ref-CR11">Wetzstein, G. et al. Inference in artificial intelligence with deep optics and photonics. <i>Nature</i> <b>588</b>, 39–47 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41586-020-2973-6" data-track-item_id="10.1038/s41586-020-2973-6" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41586-020-2973-6" aria-label="Article reference 11" data-doi="10.1038/s41586-020-2973-6">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2020Natur.588...39W" aria-label="ADS reference 11">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXisVOks7rI" aria-label="CAS reference 11">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33268862" aria-label="PubMed reference 11">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Inference%20in%20artificial%20intelligence%20with%20deep%20optics%20and%20photonics&amp;journal=Nature&amp;doi=10.1038%2Fs41586-020-2973-6&amp;volume=588&amp;pages=39-47&amp;publication_year=2020&amp;author=Wetzstein%2CG">
                    Google Scholar</a> 
                </p></li><li data-counter="12."><p id="ref-CR12">Hamerly, R., Bernstein, L., Sludds, A., Soljacic, M. &amp; Englund, D. Large-scale optical neural networks based on photoelectric multiplication. <i>Phys. Rev. X</i> <b>9</b>, 021032 (2019).</p></li><li data-counter="13."><p id="ref-CR13">Wang, T. et al. An optical neural network using less than 1 photon per multiplication. <i>Nat. Commun.</i> <b>13</b>, 123 (2022).</p></li><li data-counter="14."><p id="ref-CR14">Ivanov, A., Dryden, N., Ben-Nun, T., Li, S. &amp; Hoefler, T. Data movement is all you need: a case study on optimizing transformers. <i>Proc. Mach. Learn. Syst.</i> <b>3</b>, 711–732 (2021).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20movement%20is%20all%20you%20need%3A%20a%20case%20study%20on%20optimizing%20transformers&amp;journal=Proc.%20Mach.%20Learn.%20Syst.&amp;volume=3&amp;pages=711-732&amp;publication_year=2021&amp;author=Ivanov%2CA&amp;author=Dryden%2CN&amp;author=Ben-Nun%2CT&amp;author=Li%2CS&amp;author=Hoefler%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="15."><p id="ref-CR15">Mohseni, N., McMahon, P. L. &amp; Byrnes, T. Ising machines as hardware solvers of combinatorial optimization problems. <i>Nat. Rev. Phys.</i> <b>4</b>, 363–379 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s42254-022-00440-8" data-track-item_id="10.1038/s42254-022-00440-8" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42254-022-00440-8" aria-label="Article reference 15" data-doi="10.1038/s42254-022-00440-8">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Ising%20machines%20as%20hardware%20solvers%20of%20combinatorial%20optimization%20problems&amp;journal=Nat.%20Rev.%20Phys.&amp;doi=10.1038%2Fs42254-022-00440-8&amp;volume=4&amp;pages=363-379&amp;publication_year=2022&amp;author=Mohseni%2CN&amp;author=McMahon%2CPL&amp;author=Byrnes%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="16."><p id="ref-CR16">Jacobs, B. <i>Quantum-Inspired Classical Computing (QuICC)</i> HR001121S0041 Call for Proposals Report (DARPA, Microsystems Technology Office, 2021).</p></li><li data-counter="17."><p id="ref-CR17">Bai, S., Kolter, J. Z. &amp; Koltun, V. Deep equilibrium models. <i>Adv. Neural Inf. Process. Syst.</i> <b>32</b>, 690–701 (2019).</p></li><li data-counter="18."><p id="ref-CR18">Graves, A. Adaptive computation time for recurrent neural networks. Preprint at <a href="https://arxiv.org/abs/1603.08983" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1603.08983">https://arxiv.org/abs/1603.08983</a> (2016).</p></li><li data-counter="19."><p id="ref-CR19">Nye, M. et al. Show your work: scratchpads for intermediate computation with language models. Preprint at <a href="https://arxiv.org/abs/2112.00114" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2112.00114">https://arxiv.org/abs/2112.00114</a> (2021)</p></li><li data-counter="20."><p id="ref-CR20">Kalinin, K. P. et al. Analog iterative machine (AIM): using light to solve quadratic optimization problems with mixed variables. Preprint at <a href="https://arxiv.org/abs/2304.12594" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2304.12594">https://arxiv.org/abs/2304.12594</a> (2023).</p></li><li data-counter="21."><p id="ref-CR21">Microsoft Quantum Inspired Optimisation (QIO) provider (2022); <a href="https://learn.microsoft.com/en-us/azure/quantum/provider-microsoft-qio" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://learn.microsoft.com/en-us/azure/quantum/provider-microsoft-qio">https://learn.microsoft.com/en-us/azure/quantum/provider-microsoft-qio</a>.</p></li><li data-counter="22."><p id="ref-CR22"><i>Gurobi Optimizer Reference Manual</i> (2023); <a href="http://www.gurobi.com" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://www.gurobi.com">http://www.gurobi.com</a>.</p></li><li data-counter="23."><p id="ref-CR23">Furini, F. et al. QPLIB: a library of quadratic programming instances. <i>Math. Program. Comput.</i> <b>11</b>, 237–265 (2019).</p><p><a data-track="click_references" rel="noopener" data-track-label="10.1007/s12532-018-0147-4" data-track-item_id="10.1007/s12532-018-0147-4" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s12532-018-0147-4" aria-label="Article reference 23" data-doi="10.1007/s12532-018-0147-4">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3946537" aria-label="MathSciNet reference 23">MathSciNet</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=QPLIB%3A%20a%20library%20of%20quadratic%20programming%20instances&amp;journal=Math.%20Program.%20Comput.&amp;doi=10.1007%2Fs12532-018-0147-4&amp;volume=11&amp;pages=237-265&amp;publication_year=2019&amp;author=Furini%2CF">
                    Google Scholar</a> 
                </p></li><li data-counter="24."><p id="ref-CR24">Nvidia H100 Tensor Core GPU. <i>Nvidia</i> <a href="https://www.nvidia.com/en-us/data-center/h100/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.nvidia.com/en-us/data-center/h100/">https://www.nvidia.com/en-us/data-center/h100/</a> (2023).</p></li><li data-counter="25."><p id="ref-CR25">LeCun, Y. et al. A tutorial on energy-based learning. In <i>Predicting Structured Data</i>, Vol. 1 (eds Bakir, G., Hofman, T., Scholkopt, B., Smola, A. &amp; Taskar, B.) (MIT Press, 2006).</p></li><li data-counter="26."><p id="ref-CR26">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition</i> 770–778 (IEEE, 2016).</p></li><li data-counter="27."><p id="ref-CR27">Goodman, J. W., Dias, A. R., Woody, L. M. &amp; Erickson, J. Application of optical communication technology to optical information processing. In <i>Los Alamos Conference on Optics 1979</i> Vol. 0190 (ed. Liebenberg, D. H.) 485–496 (SPIE, 1980); <a href="https://doi.org/10.1117/12.957795" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1117/12.957795">https://doi.org/10.1117/12.957795</a>.</p></li><li data-counter="28."><p id="ref-CR28">McMahon, P. L. The physics of optical computing. <i>Nat. Rev. Phys.</i> <b>5</b>, 717–734 (2023).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s42254-023-00645-5" data-track-item_id="10.1038/s42254-023-00645-5" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42254-023-00645-5" aria-label="Article reference 28" data-doi="10.1038/s42254-023-00645-5">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20physics%20of%20optical%20computing&amp;journal=Nat.%20Rev.%20Phys.&amp;doi=10.1038%2Fs42254-023-00645-5&amp;volume=5&amp;pages=717-734&amp;publication_year=2023&amp;author=McMahon%2CPL">
                    Google Scholar</a> 
                </p></li><li data-counter="29."><p id="ref-CR29">Bai, S., Koltun, V. &amp; Kolter, J. Z. Multiscale deep equilibrium models. <i>Adv. Neural Inf. Process. Syst.</i> <b>33</b>, 5238–5250 (2020).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Multiscale%20deep%20equilibrium%20models&amp;journal=Adv.%20Neural%20Inf.%20Process.%20Syst.&amp;volume=33&amp;pages=5238-5250&amp;publication_year=2020&amp;author=Bai%2CS&amp;author=Koltun%2CV&amp;author=Kolter%2CJZ">
                    Google Scholar</a> 
                </p></li><li data-counter="30."><p id="ref-CR30">Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. <i>Proc. Natl Acad. Sci. USA</i> <b>79</b>, 2554–2558 (1982).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1073/pnas.79.8.2554" data-track-item_id="10.1073/pnas.79.8.2554" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.79.8.2554" aria-label="Article reference 30" data-doi="10.1073/pnas.79.8.2554">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="ads reference" data-track-action="ads reference" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1982PNAS...79.2554H" aria-label="ADS reference 30">ADS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=652033" aria-label="MathSciNet reference 30">MathSciNet</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DyaL383it1WktQ%3D%3D" aria-label="CAS reference 30">CAS</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=6953413" aria-label="PubMed reference 30">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238" aria-label="PubMed Central reference 30">PubMed Central</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1369.92007" aria-label="MATH reference 30">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20networks%20and%20physical%20systems%20with%20emergent%20collective%20computational%20abilities&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.79.8.2554&amp;volume=79&amp;pages=2554-2558&amp;publication_year=1982&amp;author=Hopfield%2CJJ">
                    Google Scholar</a> 
                </p></li><li data-counter="31."><p id="ref-CR31">Krotov, D. &amp; Hopfield, J. J. Dense associative memory for pattern recognition. <i>Adv. Neural Inf. Process. Syst.</i> <b>29</b>, 1180–1188 (2016).</p></li><li data-counter="32."><p id="ref-CR32">Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J. &amp; Kaiser, L. Universal transformers. In <i>International Conference on Learning Representations</i> (2019).</p></li><li data-counter="33."><p id="ref-CR33">Du, Y., Li, S., Tenenbaum, J. &amp; Mordatch, I. Learning iterative reasoning through energy minimization. In <i>International Conference on Machine Learning</i> 5570–5582 (PMLR, 2022).</p></li><li data-counter="34."><p id="ref-CR34">Geiping, J. et al. Scaling up test-time compute with latent reasoning: a recurrent depth approach. Preprint at <a href="https://arxiv.org/abs/2502.05171" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2502.05171">https://arxiv.org/abs/2502.05171</a> (2025).</p></li><li data-counter="35."><p id="ref-CR35">Schone, M. et al. Implicit language models are RNNs: balancing parallelization and expressivity. In <i>International Conference on Machine Learning</i> (PMLR, 2025).</p></li><li data-counter="36."><p id="ref-CR36">Xu, Z.-B., Qiao, H., Peng, J. &amp; Zhang, B. A comparative study of two modeling approaches in neural networks. <i>Neural Netw.</i> <b>17</b>, 73–85 (2004).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/S0893-6080(03)00192-8" data-track-item_id="10.1016/S0893-6080(03)00192-8" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS0893-6080%2803%2900192-8" aria-label="Article reference 36" data-doi="10.1016/S0893-6080(03)00192-8">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=14690709" aria-label="PubMed reference 36">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1082.68099" aria-label="MATH reference 36">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparative%20study%20of%20two%20modeling%20approaches%20in%20neural%20networks&amp;journal=Neural%20Netw.&amp;doi=10.1016%2FS0893-6080%2803%2900192-8&amp;volume=17&amp;pages=73-85&amp;publication_year=2004&amp;author=Xu%2CZ-B&amp;author=Qiao%2CH&amp;author=Peng%2CJ&amp;author=Zhang%2CB">
                    Google Scholar</a> 
                </p></li><li data-counter="37."><p id="ref-CR37">Lucas, A. Ising formulations of many NP problems. <i>Front. Phys.</i> <b>2</b>, 5 (2014).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3389/fphy.2014.00005" data-track-item_id="10.3389/fphy.2014.00005" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3389%2Ffphy.2014.00005" aria-label="Article reference 37" data-doi="10.3389/fphy.2014.00005">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Ising%20formulations%20of%20many%20NP%20problems&amp;journal=Front.%20Phys.&amp;doi=10.3389%2Ffphy.2014.00005&amp;volume=2&amp;publication_year=2014&amp;author=Lucas%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="38."><p id="ref-CR38">Barahona, F., Jünger, M. &amp; Reinelt, G. Experiments in quadratic 0-1 programming. <i>Math. Program.</i> <b>44</b>, 127–137 (1989).</p><p><a data-track="click_references" rel="noopener" data-track-label="10.1007/BF01587084" data-track-item_id="10.1007/BF01587084" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/BF01587084" aria-label="Article reference 38" data-doi="10.1007/BF01587084">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=1003556" aria-label="MathSciNet reference 38">MathSciNet</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0677.90046" aria-label="MATH reference 38">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Experiments%20in%20quadratic%200-1%20programming&amp;journal=Math.%20Program.&amp;doi=10.1007%2FBF01587084&amp;volume=44&amp;pages=127-137&amp;publication_year=1989&amp;author=Barahona%2CF&amp;author=J%C3%BCnger%2CM&amp;author=Reinelt%2CG">
                    Google Scholar</a> 
                </p></li><li data-counter="39."><p id="ref-CR39">Donoho, D. L. Compressed sensing. <i>IEEE Trans. Inf. Theory</i> <b>52</b>, 1289–1306 (2006).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TIT.2006.871582" data-track-item_id="10.1109/TIT.2006.871582" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTIT.2006.871582" aria-label="Article reference 39" data-doi="10.1109/TIT.2006.871582">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=2241189" aria-label="MathSciNet reference 39">MathSciNet</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1288.94016" aria-label="MATH reference 39">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Compressed%20sensing&amp;journal=IEEE%20Trans.%20Inf.%20Theory&amp;doi=10.1109%2FTIT.2006.871582&amp;volume=52&amp;pages=1289-1306&amp;publication_year=2006&amp;author=Donoho%2CDL">
                    Google Scholar</a> 
                </p></li><li data-counter="40."><p id="ref-CR40">Candes, E. J. &amp; Tao, T. Near-optimal signal recovery from random projections: universal encoding strategies? <i>IEEE Trans. Inf. Theory</i> <b>52</b>, 5406–5425 (2006).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TIT.2006.885507" data-track-item_id="10.1109/TIT.2006.885507" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTIT.2006.885507" aria-label="Article reference 40" data-doi="10.1109/TIT.2006.885507">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=2300700" aria-label="MathSciNet reference 40">MathSciNet</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Near-optimal%20signal%20recovery%20from%20random%20projections%3A%20universal%20encoding%20strategies%3F&amp;journal=IEEE%20Trans.%20Inf.%20Theory&amp;doi=10.1109%2FTIT.2006.885507&amp;volume=52&amp;pages=5406-5425&amp;publication_year=2006&amp;author=Candes%2CEJ&amp;author=Tao%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="41."><p id="ref-CR41">Kabashima, Y., Wadayama, T. &amp; Tanaka, T. A typical reconstruction limit for compressed sensing based on <i>L</i><sub><i>p</i></sub>-norm minimization. <i>J. Stat. Mech. Theory Exp.</i> <b>2009</b>, L09003 (2009).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1088/1742-5468/2009/09/L09003" data-track-item_id="10.1088/1742-5468/2009/09/L09003" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1088%2F1742-5468%2F2009%2F09%2FL09003" aria-label="Article reference 41" data-doi="10.1088/1742-5468/2009/09/L09003">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20typical%20reconstruction%20limit%20for%20compressed%20sensing%20based%20on%20Lp-norm%20minimization&amp;journal=J.%20Stat.%20Mech.%20Theory%20Exp.&amp;doi=10.1088%2F1742-5468%2F2009%2F09%2FL09003&amp;volume=2009&amp;publication_year=2009&amp;author=Kabashima%2CY&amp;author=Wadayama%2CT&amp;author=Tanaka%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="42."><p id="ref-CR42">Nakanishi-Ohno, Y., Obuchi, T., Okada, M. &amp; Kabashima, Y. Sparse approximation based on a random overcomplete basis. <i>J. Stat. Mech. Theory Exp.</i> <b>2016</b>, 063302 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1088/1742-5468/2016/06/063302" data-track-item_id="10.1088/1742-5468/2016/06/063302" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1088%2F1742-5468%2F2016%2F06%2F063302" aria-label="Article reference 42" data-doi="10.1088/1742-5468/2016/06/063302">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3522782" aria-label="MathSciNet reference 42">MathSciNet</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1456.62014" aria-label="MATH reference 42">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Sparse%20approximation%20based%20on%20a%20random%20overcomplete%20basis&amp;journal=J.%20Stat.%20Mech.%20Theory%20Exp.&amp;doi=10.1088%2F1742-5468%2F2016%2F06%2F063302&amp;volume=2016&amp;publication_year=2016&amp;author=Nakanishi-Ohno%2CY&amp;author=Obuchi%2CT&amp;author=Okada%2CM&amp;author=Kabashima%2CY">
                    Google Scholar</a> 
                </p></li><li data-counter="43."><p id="ref-CR43">Tseng, P. Convergence of a block coordinate descent method for nondifferentiable minimization. <i>J. Optim. Theory Appl.</i> <b>109</b>, 475–494 (2001).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1023/A:1017501703105" data-track-item_id="10.1023/A:1017501703105" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FA%3A1017501703105" aria-label="Article reference 43" data-doi="10.1023/A:1017501703105">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="mathscinet reference" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=1835069" aria-label="MathSciNet reference 43">MathSciNet</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1006.65062" aria-label="MATH reference 43">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Convergence%20of%20a%20block%20coordinate%20descent%20method%20for%20nondifferentiable%20minimization&amp;journal=J.%20Optim.%20Theory%20Appl.&amp;doi=10.1023%2FA%3A1017501703105&amp;volume=109&amp;pages=475-494&amp;publication_year=2001&amp;author=Tseng%2CP">
                    Google Scholar</a> 
                </p></li><li data-counter="44."><p id="ref-CR44">Zbontar, J. et al. FastMRI: an open dataset and benchmarks for accelerated MRI. Preprint at <a href="https://arxiv.org/abs/1811.08839" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1811.08839">https://arxiv.org/abs/1811.08839</a> (2018).</p></li><li data-counter="45."><p id="ref-CR45"><i>DTCC 2023 Annual Report</i> (Depository Trust &amp; Clearing Corp., 2023); <a href="https://www.dtcc.com/-/media/Files/Downloads/Annual%20Report/2023/DTCC-2023-AR-Print.pdf" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.dtcc.com/-/media/Files/Downloads/Annual%20Report/2023/DTCC-2023-AR-Print.pdf">https://www.dtcc.com/-/media/Files/Downloads/Annual%20Report/2023/DTCC-2023-AR-Print.pdf</a>.</p></li><li data-counter="46."><p id="ref-CR46">Gedin, S. <i>Securities settlement optimization using an optimization software solution</i>. MSc thesis, KTH Royal Institute of Technology (2020).</p></li><li data-counter="47."><p id="ref-CR47">Braine, L. et al. Quantum algorithms for mixed binary optimization applied to transaction settlement. <i>IEEE Trans. Quantum Eng.</i> <b>2</b>, 1–8 (2021).</p></li><li data-counter="48."><p id="ref-CR48">Javaheripi, M. et al. Phi-2: the surprising power of small language models. <i>Microsoft Research Blog</i> <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models">https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models</a> (2023).</p></li><li data-counter="49."><p id="ref-CR49">Pezeshki, B., Tselikov, A., Kalman, R. &amp; Danesh, C. Wide and parallel LED-based optical links using multi-core fiber for chip-to-chip communications. In <i>Optical Fiber Communication Conference (OFC) 2021</i> <a href="https://opg.optica.org/abstract.cfm?URI=OFC-2021-F3A.1" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://opg.optica.org/abstract.cfm?URI=OFC-2021-F3A.1">https://opg.optica.org/abstract.cfm?URI=OFC-2021-F3A.1</a> (2021).</p></li><li data-counter="50."><p id="ref-CR50">Kingma, D. P. Adam: a method for stochastic optimization. Preprint at <a href="https://arxiv.org/abs/1412.6980" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a> (2014).</p></li><li data-counter="51."><p id="ref-CR51">Garey, M. R., Johnson, D. S. &amp; Stockmeyer, L. Some simplified NP-complete problems. In <i>Proc. 6th Annual ACM Symposium on Theory of Computing</i> 47–63 (1974).</p></li><li data-counter="52."><p id="ref-CR52">Kalinin, K. P. &amp; Berloff, N. G. Computational complexity continuum within Ising formulation of NP problems. <i>Commun. Phys.</i> <b>5</b>, 20 (2022).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s42005-021-00792-0" data-track-item_id="10.1038/s42005-021-00792-0" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42005-021-00792-0" aria-label="Article reference 52" data-doi="10.1038/s42005-021-00792-0">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Computational%20complexity%20continuum%20within%20Ising%20formulation%20of%20NP%20problems&amp;journal=Commun.%20Phys.&amp;doi=10.1038%2Fs42005-021-00792-0&amp;volume=5&amp;publication_year=2022&amp;author=Kalinin%2CKP&amp;author=Berloff%2CNG">
                    Google Scholar</a> 
                </p></li><li data-counter="53."><p id="ref-CR53">Reifenstein, S., Kako, S., Khoyratee, F., Leleu, T. &amp; Yamamoto, Y. Coherent Ising machines with optical error correction circuits. <i>Adv. Quantum Technol.</i> <b>4</b>, 2100077 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1002/qute.202100077" data-track-item_id="10.1002/qute.202100077" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1002%2Fqute.202100077" aria-label="Article reference 53" data-doi="10.1002/qute.202100077">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="cas reference" data-track-action="cas reference" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXhvFyju7rP" aria-label="CAS reference 53">CAS</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Coherent%20Ising%20machines%20with%20optical%20error%20correction%20circuits&amp;journal=Adv.%20Quantum%20Technol.&amp;doi=10.1002%2Fqute.202100077&amp;volume=4&amp;publication_year=2021&amp;author=Reifenstein%2CS&amp;author=Kako%2CS&amp;author=Khoyratee%2CF&amp;author=Leleu%2CT&amp;author=Yamamoto%2CY">
                    Google Scholar</a> 
                </p></li><li data-counter="54."><p id="ref-CR54">Chi, J. et al. Datasets for “The analog optical computer for machine learning inference and combinatorial optimization”. <i>Zenodo</i> <a href="https://doi.org/10.5281/zenodo.15088326" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.5281/zenodo.15088326">https://doi.org/10.5281/zenodo.15088326</a> (2025).</p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-025-09430-z?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>We acknowledge G. Mourgias-Alexandris and I. Haller for contributions to the first-generation AOC hardware; S. Jordan, B. Lackey, A. Barzegar, F. Hamze and M. Troyer for discussions about optimization problems and for providing us with the QUBO benchmarks (Wishart, Tile3D, RCDP); J. Cummins for contributions to medical image reconstruction; A. Grace for contributions to the AOC hardware; J. Westcott, N. Farrell and T. Burridge for the AOC mechanical parts; S. Y. Siew for help with microLEDs; M. Schapira and colleagues at Microsoft Research, Cambridge, UK, and S. Bramhavar from ARIA for discussions. N.G.B. acknowledges the support from HORIZON EIC-2022-PATHFINDERCHALLENGES-01 HEISINGBERG Project 101114978 and the support from Weizmann-UK Make Connection Grant 142568.</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Microsoft Research, Cambridge, UK</p><p>Kirill P. Kalinin, Jannes Gladrow, Jiaqi Chu, James H. Clegg, Daniel Cletheroe, Douglas J. Kelly, Babak Rahmani, Grace Brennan, Burcu Canakci, Fabian Falck, Heiner Kremer, Greg O’Shea, Lucinda Pickup, Ant Rowstron, Christos Gkantsidis, Francesca Parmigiani &amp; Hitesh Ballani</p></li><li id="Aff2"><p>Microsoft, Redmond, WA, USA</p><p>Michael Hansen, Jim Kleewein, Saravan Rajmohan &amp; Victor Ruhle</p></li><li id="Aff3"><p>Chief Technology Office, Barclays, London, UK</p><p>Lee Braine &amp; Shrirang Khedekar</p></li><li id="Aff4"><p>Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, UK</p><p>Natalia G. Berloff</p></li></ol><div data-test="author-info"><p><span>Authors</span></p><ol><li id="auth-Kirill_P_-Kalinin-Aff1"><span>Kirill P. Kalinin</span></li><li id="auth-Jannes-Gladrow-Aff1"><span>Jannes Gladrow</span></li><li id="auth-Jiaqi-Chu-Aff1"><span>Jiaqi Chu</span></li><li id="auth-James_H_-Clegg-Aff1"><span>James H. Clegg</span></li><li id="auth-Daniel-Cletheroe-Aff1"><span>Daniel Cletheroe</span></li><li id="auth-Douglas_J_-Kelly-Aff1"><span>Douglas J. Kelly</span></li><li id="auth-Babak-Rahmani-Aff1"><span>Babak Rahmani</span></li><li id="auth-Grace-Brennan-Aff1"><span>Grace Brennan</span></li><li id="auth-Burcu-Canakci-Aff1"><span>Burcu Canakci</span></li><li id="auth-Fabian-Falck-Aff1"><span>Fabian Falck</span></li><li id="auth-Michael-Hansen-Aff2"><span>Michael Hansen</span></li><li id="auth-Jim-Kleewein-Aff2"><span>Jim Kleewein</span></li><li id="auth-Heiner-Kremer-Aff1"><span>Heiner Kremer</span></li><li id="auth-Greg-O_Shea-Aff1"><span>Greg O’Shea</span></li><li id="auth-Lucinda-Pickup-Aff1"><span>Lucinda Pickup</span></li><li id="auth-Saravan-Rajmohan-Aff2"><span>Saravan Rajmohan</span></li><li id="auth-Ant-Rowstron-Aff1"><span>Ant Rowstron</span></li><li id="auth-Victor-Ruhle-Aff2"><span>Victor Ruhle</span></li><li id="auth-Lee-Braine-Aff3"><span>Lee Braine</span></li><li id="auth-Shrirang-Khedekar-Aff3"><span>Shrirang Khedekar</span></li><li id="auth-Natalia_G_-Berloff-Aff4"><span>Natalia G. Berloff</span></li><li id="auth-Christos-Gkantsidis-Aff1"><span>Christos Gkantsidis</span></li><li id="auth-Francesca-Parmigiani-Aff1"><span>Francesca Parmigiani</span></li><li id="auth-Hitesh-Ballani-Aff1"><span>Hitesh Ballani</span></li></ol></div><h3 id="contributions">Contributions</h3><p>K.P.K., J.G., J.C., J.H.C., D.C., D.J.K. and B.R. contributed equally to this work. F.P., C.G. and H.B. conceived of the project. K.P.K., J.G. and C.G. developed and designed the abstraction and corresponding unification with the support of N.G.B. and H.B. F.P., J.H.C., D.C. and L.P. developed the hardware and designed its optical and electrical parts. J.C., J.G. and D.J.K. performed the ML experiments and analysed their data. J.C., K.P.K., J.H.C., C.G. and F.P. performed the optimization experiments and analysed their data. C.G., J.G., K.P.K., J.C. and D.C. implemented the digital twin. J.G. designed and ran the digital twin experiments and analysed the data for ML models. K.P.K. and C.G. designed and ran the digital twin experiments and analysed the data for optimization problems. B.R. and J.G. designed and ran the generalization and noise robustness experiments. J.C., J.H.C., D.J.K., G.B., L.P., D.C. and F.P. implemented and characterized the AOC components and technologies. D.J.K., J.H.C., B.C., F.P., H.B., L.P. and D.C. developed the roadmap and the AOC calculations. D.J.K., J.H.C., J.C., G.B., L.P. and G.O’S. developed the control software. B.R., K.P.K., F.F., H.K., S.R., V.R. and H.B. provided technical input to ML experiments. M.H., K.P.K., C.G. and H.B. designed the MRI experiments. L.B., S.K., C.G., K.P.K. and H.B. designed the transaction-settlement experiments. A.R., N.G.B., S.R., V.R. and J.K. provided technical input to the hardware applications. K.P.K., J.G., H.B., J.H.C., F.P., B.R. and C.G. wrote the paper with input from all authors.</p><h3 id="corresponding-author">Corresponding authors</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:kkalinin@microsoft.com">Kirill P. Kalinin</a>, <a id="corresp-c2" href="mailto:francesca.parmigiani@microsoft.com">Francesca Parmigiani</a> or <a id="corresp-c3" href="mailto:hitesh.ballani@microsoft.com">Hitesh Ballani</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
            
              <h3 id="FPar2">Competing interests</h3>
              <p>The authors of the paper have filed several patents relating to the subject matter contained in this paper in the name of Microsoft Co.</p>
            
          </div></div></section><section data-title="Peer review"><div id="peer-review-section"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
            
            
              <h3 id="FPar1">Peer review information</h3>
              <p><i>Nature</i> thanks Yang Wang and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM3">Peer reviewer reports</a> are available.</p>
            
          </div></div></section><section data-title="Additional information"><div id="additional-information-section"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></section><section data-title="Extended data figures and tables"><div id="Sec40-section"><h2 id="Sec40">Extended data figures and tables</h2><div id="Sec40-content"><div data-test="supplementary-info"><div data-test="supp-item" id="Fig5"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 1 experimental layout of aoc." href="https://www.nature.com/articles/s41586-025-09430-z/figures/5" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig5_ESM.jpg">Extended Data Fig. 1 Experimental layout of <i>AOC.</i></a></h3><p><b>a</b>, Photo of <i>AOC</i>, used to simultaneous run ML models and solve optimization instances. Key highlighted components are: the microLED array, the SLMs, the photodetector array, and the analog electronic block. <b>b</b>, <i>AOC </i>photo inside a rack, with all required equipment included.</p></div><div data-test="supp-item" id="Fig6"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 2 detailed schematic diagram of" href="https://www.nature.com/articles/s41586-025-09430-z/figures/6" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig6_ESM.jpg">Extended Data Fig. 2 Detailed Schematic diagram of the optical vector-matrix multiplication in <i>AOC.</i></a></h3><p>Light from the sources is collected by the objective. The sources are imaged onto the SLM using the combined 4F system of the objective and lens group 1 (LG1). Polarizing beamsplitter 1 (PBS 1) splits the light by linear polarization state and sends light to one of two modulators. The reflected light is modulated by polarization (multiplication) and the action of PBS 1 makes this an amplitude modulation. Each SLM is imaged onto its corresponding detector using LG1 and LG2 through PBS 1 and PBS 2. Summation happens at the detector.</p></div><div data-test="supp-item" id="Fig7"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 3 schematic diagram of aoc." href="https://www.nature.com/articles/s41586-025-09430-z/figures/7" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig7_ESM.jpg">Extended Data Fig. 3 Schematic diagram of <i>AOC.</i></a></h3><p>Schematic diagram of the analog electronic parts (in the orange box) and the optical parts (in the green boxes), where the key functions and operations are highlighted. The state vector <i>s</i>(<i>t</i>) in equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ1">1</a>) and equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="https://www.nature.com/articles/s41586-025-09430-z#Equ2">2</a>) is measured at the pointed indicated as output.</p></div><div data-test="supp-item" id="Fig8"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 4 aoc at scale." href="https://www.nature.com/articles/s41586-025-09430-z/figures/8" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig8_ESM.jpg">Extended Data Fig. 4 <i>AOC</i> at scale.</a></h3><p>Artistic representation of <i>AOC</i> at 100 million weights scale, showing the 3D mesh structure of the required 50 modules, with each module with size of 4 × 10<sup>6</sup> weights (2000 variables) and distinct optical modules for positive and negative weights.</p></div><div data-test="supp-item" id="Fig9"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 5 traces and multi-seed-dt-aoc " href="https://www.nature.com/articles/s41586-025-09430-z/figures/9" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig9_ESM.jpg">Extended Data Fig. 5 Traces and Multi-Seed-DT-AOC comparison.</a></h3><div data-component="thumbnail-container"><p><b>a</b>, Example trace of an MNIST example. We selected four time points of the trace where we projected the measured state into classification space using the output projection and applied a softmax to obtain probabilities. The most likely class stays on the wrong label 7 until the fully-sampled result is evaluated in which the correct class label 9 is reached. <b>b</b>, Example trace of a Fashion MNIST example. Here, the initial entropy of the class-probability distribution is high but drastically falls off over iterations. The correct class with label 2 emerges early on and is maintained throughout the iterations and in the sampled result. <b>c</b>, Example traces of a sinusoidal regression task for <i>x</i> = − 0.55. While the trace sampled during evolution averages multiple iterations and is asynchronously sampled, we can observe some correspondence between the evolution of the <i>AOC-DT</i> for the same point. The plot on the right shows the evolution of the entire curve over <i>AOC-DT</i> iterations. The point of interest is marked by the vertical dashed line and appears to follow the same trajectory from close to −1 to its target value. <b>d</b>, Multi-seed study of the Gaussian regression task for 100 independently trained and tested equilibrium models to test how reliable the <i>AOC-DT</i> models the AOC device.</p><p><a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM6">Source Data</a></p></div></div><div data-test="supp-item" id="Fig10"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="extended data fig. 6 increased out-of-distribution" href="https://www.nature.com/articles/s41586-025-09430-z/figures/10" data-supp-info-image="//media.springernature.com/lw685/springer-static/esm/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_Fig10_ESM.jpg">Extended Data Fig. 6 Increased out-of-distribution generalization and equilibrium model robustness.</a></h3><div data-component="thumbnail-container"><p><b>a</b>, Out-of-distribution generalization of the equilibrium model. <b>b</b>, Robustness of the equilibrium model to noise for feedforward models with varying number of layers and parameter-matched single-layer DEQs.</p><p><a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41586-025-09430-z#MOESM7">Source Data</a></p></div></div></div></div></div></section><section data-title="Supplementary information"><div id="Sec41-section"><h2 id="Sec41">Supplementary information</h2><div id="Sec41-content"><div data-test="supplementary-info"><div data-test="supp-item" id="MOESM1"><h3><a data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary information" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09430-z/MediaObjects/41586_2025_9430_MOESM1_ESM.pdf" data-supp-info-image="">Supplementary Information</a></h3><p>This file contains sections A–G, including Supplementary Figs. 1–13, 4 Supplementary Tables and References.</p></div></div></div></div></section><section data-title="Source data"></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
              <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
            <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Analog%20optical%20computer%20for%20AI%20inference%20and%20combinatorial%20optimization&amp;author=Kirill%20P.%20Kalinin%20et%20al&amp;contentID=10.1038%2Fs41586-025-09430-z&amp;copyright=Microsoft%20Corporation%20and%20Barclays%20Bank%20plc.%202025&amp;publication=0028-0836&amp;publicationDate=2025-09-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1038/s41586-025-09430-z" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41586-025-09430-z" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Kalinin, K.P., Gladrow, J., Chu, J. <i>et al.</i> Analog optical computer for AI inference and combinatorial optimization.
                    <i>Nature</i>  (2025). https://doi.org/10.1038/s41586-025-09430-z</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41586-025-09430-z?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2024-11-21">21 November 2024</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2025-07-18">18 July 2025</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2025-09-03">03 September 2025</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41586-025-09430-z</span></p></li></ul></div></div></div></div></section>
            </div></div>
  </body>
</html>
