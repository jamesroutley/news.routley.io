<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/smol-ai/developer">Original</a>
    <h1>Smol Developer</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><em><strong>Human-centric &amp; Coherent Whole Program Synthesis</strong></em> aka your own personal junior developer</p>
<blockquote>
<p dir="auto"><a href="https://twitter.com/swyx/status/1657578738345979905" rel="nofollow">Build the thing that builds the thing!</a> a <code>smol dev</code> for every dev in every situation</p>
</blockquote>
<p dir="auto">this is a prototype of a &#34;junior developer&#34; agent (aka <code>smol dev</code>) that scaffolds an entire codebase out for you once you give it a product spec, but does not end the world or overpromise AGI. instead of making and maintaining specific, rigid, one-shot starters, like <code>create-react-app</code>, or <code>create-nextjs-app</code>, this is basically <a href="https://news.ycombinator.com/item?id=35942352" rel="nofollow"><code>create-anything-app</code></a> where you develop your scaffolding prompt in a tight loop with your smol dev.</p>
<p dir="auto">AI that is helpful, harmless, and honest is complemented by a codebase that is simple, safe, and smol - &lt;200 lines of Python and Prompts, so this is easy to understand and customize.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c32a1b1f6f8b20c9ffca4ef5e1935427b3bb76c6c28de42cdc9ff64249b0bbbb/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4677457a564363614d4145377434683f666f726d61743d6a7067266e616d653d6c61726765"><img height="200" src="https://camo.githubusercontent.com/c32a1b1f6f8b20c9ffca4ef5e1935427b3bb76c6c28de42cdc9ff64249b0bbbb/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4677457a564363614d4145377434683f666f726d61743d6a7067266e616d653d6c61726765" data-canonical-src="https://pbs.twimg.com/media/FwEzVCcaMAE7t4h?format=jpg&amp;name=large"/></a>
</p>
<p dir="auto"><em>engineering with prompts, rather than prompt engineering</em></p>
<p dir="auto">The demo example in <code>prompt.md</code> shows the potential of AI-enabled, but still firmly human developer centric, workflow:</p>
<ul dir="auto">
<li>Human writes a basic prompt for the app they want to build</li>
<li><code>main.py</code> generates code</li>
<li>Human runs/reads the code</li>
<li>Human can:
<ul dir="auto">
<li>simply add to the prompt as they discover underspecified parts of the prompt</li>
<li>manually runs the code and identifies errors</li>
<li><em>paste the error into the prompt</em> just like they would file a github issue</li>
<li>for extra help, they can use <code>debugger.py</code> which reads the whole codebase to make specific code change suggestions</li>
</ul>
</li>
</ul>
<p dir="auto">Loop until happiness is attained. Notice that AI is only used as long as it is adding value - once it gets in your way, just take over the codebase from your smol junior developer with no fuss and no hurt feelings. (<em>we could also have smol-dev take over an existing codebase and bootstrap its own prompt... but that&#39;s a Future Direction</em>)</p>
<p dir="auto"><em>Not no code, not low code, but some third thing.</em></p>
<p dir="auto">Perhaps a higher order evolution of programming where you still need to be technical, but no longer have to implement every detail at least to scaffold things out.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-video-demo" aria-hidden="true" href="#video-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>video demo</h2>
<p dir="auto"><a href="https://youtu.be/UCo7YeTy-aE" rel="nofollow"><img src="https://camo.githubusercontent.com/c01e263cceb4861f2968d8f0e4d8f20636792da8bfeb357f53e67157dab65434/68747470733a2f2f69332e7974696d672e636f6d2f76692f55436f37596554792d61452f687164656661756c742e6a7067" alt="https://i3.ytimg.com/vi/UCo7YeTy-aE/hqdefault.jpg" data-canonical-src="https://i3.ytimg.com/vi/UCo7YeTy-aE/hqdefault.jpg"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-arch-diagram" aria-hidden="true" href="#arch-diagram"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>arch diagram</h2>
<p dir="auto">naturally generated with gpt4, like <a href="https://twitter.com/swyx/status/1648724820316786688" rel="nofollow">we did for babyagi</a>
<a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/6764957/238257413-f8fc68f4-77f6-43ee-852f-a35fb195430a.png"><img src="https://user-images.githubusercontent.com/6764957/238257413-f8fc68f4-77f6-43ee-852f-a35fb195430a.png" alt="image"/></a></p>
<h3 tabindex="-1" dir="auto"><a id="user-content-innovations-and-insights" aria-hidden="true" href="#innovations-and-insights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>innovations and insights</h3>
<blockquote>
<p dir="auto">Please subscribe to <a href="https://latent.space/" rel="nofollow">https://latent.space/</a> for a fuller writeup and insights and reflections</p>
</blockquote>
<ul dir="auto">
<li><strong>Markdown is all you need</strong> - Markdown is the perfect way to prompt for whole program synthesis because it is easy to mix english and code (whether <code>variable_names</code> or entire ``` code fenced code samples)
<ul dir="auto">
<li>turns out you can specify prompts in code in prompts and gpt4 obeys that to the letter</li>
</ul>
</li>
<li><strong>Copy and paste programming</strong>
<ul dir="auto">
<li>teaching the program to understand how to code around a new API (Anthropic&#39;s API is after GPT3&#39;s knowledge cutoff) by just pasting in the <code>curl</code> input and output</li>
<li>pasting error messages into the prompt and vaguely telling the program how you&#39;d like it handled. it kind of feels like &#34;logbook driven programming&#34;.</li>
</ul>
</li>
<li><strong>Debugging by <code>cat</code>ing</strong> the whole codebase with your error message and getting specific fix suggestions - particularly delightful!</li>
<li><strong>Tricks for whole program coherence</strong> - our chosen example usecase, Chrome extensions, have a lot of indirect dependencies across files. Any hallucination of cross dependencies causes the whole program to error.
<ul dir="auto">
<li>We solved this by adding an intermediate step asking GPT to think through <code>shared_dependencies.md</code>, and then insisting on using that in generating each file. This basically means GPT is able to talk to itself...</li>
<li>... but it&#39;s not perfect, yet. <code>shared_dependencies.md</code> is sometimes not comperehensive in understanding what are hard dependencies between files. So we just solved it by specifying a specific <code>name</code> in the prompt. felt dirty at first but it works, and really it&#39;s just clear unambiguous communication at the end of the day.</li>
<li>see <code>prompt.md</code> for SOTA smol-dev prompting</li>
</ul>
</li>
<li><strong>Low activation energy for unfamiliar APIs</strong>
<ul dir="auto">
<li>we have never really learned css animations, but now can just say we want a &#34;juicy css animated red and white candy stripe loading indicator&#34; and it does the thing.</li>
<li>ditto for Chrome Extension Manifest v3 - the docs are an abject mess, but fortunately we don&#39;t have to read them now to just get a basic thing done</li>
<li>the Anthropic docs (bad bad) were missing guidance on what return signature they have. so just curl it and dump it in the prompt lol.</li>
</ul>
</li>
<li><strong>Modal is all you need</strong> - we chose Modal to solve 4 things:
<ul dir="auto">
<li>solve python dependency hell in dev and prod</li>
<li>parallelizable code generation</li>
<li>simple upgrade path from local dev to cloud hosted endpoints (in future)</li>
<li>fault tolerant openai api calls with retries/backoff, and attached storage (for future use)</li>
</ul>
</li>
</ul>
<blockquote>
<p dir="auto">Please subscribe to <a href="https://latent.space/" rel="nofollow">https://latent.space/</a> for a fuller writeup and insights and reflections</p>
</blockquote>
<h3 tabindex="-1" dir="auto"><a id="user-content-caveats" aria-hidden="true" href="#caveats"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>caveats</h3>
<p dir="auto">We were working on a Chrome Extension, which requires images to be generated, so we added some usecase specific code in there to skip destroying/regenerating them, that we haven&#39;t decided how to generalize.</p>
<p dir="auto">We dont have access to GPT4-32k, but if we did, we&#39;d explore dumping entire API/SDK documentation into context.</p>
<p dir="auto">The feedback loop is very slow right now (<code>time</code> says about 2-4 mins to generate a program with GPT4, even with parallelization due to Modal (occasionally spiking higher)), but it&#39;s a safe bet that it will go down over time (see also &#34;future directions&#34; below).</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-install" aria-hidden="true" href="#install"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>install</h2>
<p dir="auto">it&#39;s basically:</p>
<ul dir="auto">
<li><code>git clone https://github.com/smol-ai/developer</code>.</li>
<li>copy over <code>.example.env</code> to <code>.env</code> filling in your API keys.</li>
</ul>
<p dir="auto">There are no python dependencies to wrangle thanks to using Modal as a <a href="https://www.google.com/search?q=self+provisioning+runtime" rel="nofollow">self-provisioning runtime</a>.</p>
<p dir="auto">Unfortunately this project also uses 3 waitlisted things:</p>
<ul dir="auto">
<li>Modal.com - <code>pip install modal-client</code> (private beta - hit up the modal team to get an invite, and login)</li>
<li>GPT-4 api (private beta) - can use 3.5 but obviously wont be as good</li>
<li>(for the demo project) anthropic claude 100k context api (private beta)</li>
</ul>
<blockquote>
<p dir="auto">yes, the most important skill in being an ai engineer is social engineering to get off waitlists. Modal will let you in if you say the keyword &#34;swyx&#34;</p>
</blockquote>
<p dir="auto">you&#39;ll have to adapt this code on a fork if you want to use it on other infra. please open issues/PRs and i&#39;ll happily highlight your fork here.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-trying-the-example-chrome-extension" aria-hidden="true" href="#trying-the-example-chrome-extension"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>trying the example chrome extension</h3>
<p dir="auto">the <code>/generated</code> and <code>/exampleChromeExtension</code> folder contains <code>a Chrome Manifest V3 extension that reads the current page, and offers a popup UI that has the page title+content and a textarea for a prompt (with a default value we specify). When the user hits submit, it sends the page title+content to the Anthropic Claude API along with the up to date prompt to summarize it. The user can modify that prompt and re-send the prompt+content to get another summary view of the content.</code></p>
<ul dir="auto">
<li>go to Manage Extensions in Chrome</li>
<li>load unpacked</li>
<li>find the relevant folder in your file system and load it</li>
<li>go to any content heavy site</li>
<li>click the cute bird</li>
<li>see it work</li>
</ul>
<p dir="auto">this entire extension was generated by the prompt in <code>prompt.md</code> (except for the images), and was built up over time by adding more words to the prompt in an iterative process.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-smol-dev" aria-hidden="true" href="#smol-dev"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>smol dev</h2>
<p dir="auto">basic usage</p>
<div dir="auto" data-snippet-clipboard-copy-content="modal run main.py --prompt &#34;a Chrome extension that, when clicked, opens a small window with a page where you can enter a prompt for reading the currently open page and generating some response from openai&#34;   "><pre>modal run main.py --prompt <span><span>&#34;</span>a Chrome extension that, when clicked, opens a small window with a page where you can enter a prompt for reading the currently open page and generating some response from openai<span>&#34;</span></span>   </pre></div>
<p dir="auto">after a while of adding to your prompt, you can extract your prompt to a file, as long as your &#34;prompt&#34; ends in a .md extension we&#39;ll go look for that file</p>
<div dir="auto" data-snippet-clipboard-copy-content="modal run main.py --prompt prompt.md   "><pre>modal run main.py --prompt prompt.md   </pre></div>
<p dir="auto">each time you run this, the generated directory is deleted (except for images) and all files are rewritten from scratch.</p>
<p dir="auto">In the <code>shared_dependencies.md</code> file is a helper file that ensures coherence between files.</p>
<p dir="auto">if you make a tweak to the prompt and only want it to affect one file, and keep the rest of the files, specify the file param:</p>
<div dir="auto" data-snippet-clipboard-copy-content="modal run main.py --prompt prompt.md  --file popup.js"><pre>modal run main.py --prompt prompt.md  --file popup.js</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-smol-debugger" aria-hidden="true" href="#smol-debugger"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>smol debugger</h2>
<p dir="auto">take the entire contents of the generated directory in context, feed in an error, get a response. this basically takes advantage of longer (32k-100k) context so we basically dont have to do any embedding of the source.</p>
<div dir="auto" data-snippet-clipboard-copy-content="modal run debugger.py --prompt &#34;Uncaught (in promise) TypeError: Cannot destructure property &#39;pageTitle&#39; of &#39;(intermediate value)&#39; as it is undefined.    at init (popup.js:59:11)&#34;

# gpt4
modal run debugger.py --prompt &#34;your_error msg_here&#34; --model=gpt-4"><pre>modal run debugger.py --prompt <span><span>&#34;</span>Uncaught (in promise) TypeError: Cannot destructure property &#39;pageTitle&#39; of &#39;(intermediate value)&#39; as it is undefined.    at init (popup.js:59:11)<span>&#34;</span></span>

<span><span>#</span> gpt4</span>
modal run debugger.py --prompt <span><span>&#34;</span>your_error msg_here<span>&#34;</span></span> --model=gpt-4</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-smol-pm" aria-hidden="true" href="#smol-pm"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>smol pm</h2>
<p dir="auto">take the entire contents of the generated directory in context, and get a prompt back that could synthesize the whole program. basically <code>smol dev</code>, in reverse.</p>
<div dir="auto" data-snippet-clipboard-copy-content="modal run code2prompt.py # ~0.5 second

# use gpt4
modal run code2prompt.py --model=gpt-4 # 2 mins, MUCH better results"><pre>modal run code2prompt.py <span><span>#</span> ~0.5 second</span>

<span><span>#</span> use gpt4</span>
modal run code2prompt.py --model=gpt-4 <span><span>#</span> 2 mins, MUCH better results</span></pre></div>
<p dir="auto">We have done indicative runs of both, stored in <code>code2prompt-gpt3.md</code> vs <code>code2prompt-gpt4.md</code>. Note how incredibly better gpt4 is at prompt engineering its future self.</p>
<p dir="auto">Naturally, we had to try <code>code2prompt2code</code>...</p>
<div dir="auto" data-snippet-clipboard-copy-content="# add prompt... this needed a few iterations to get right
modal run code2prompt.py --prompt &#34;make sure all the id&#39;s of the DOM elements, and the data structure of the page content (stored with {pageTitle, pageContent }) , referenced/shared by the js files match up exactly. take note to only use Chrome Manifest V3 apis. rename the extension to code2prompt2code&#34; --model=gpt-4 # takes 4 mins. produces semi working chrome extension copy based purely on the model-generated description of a different codebase

# must go deeper
modal run main.py --prompt code2prompt-gpt4.md --directory code2prompt2code"><pre><span><span>#</span> add prompt... this needed a few iterations to get right</span>
modal run code2prompt.py --prompt <span><span>&#34;</span>make sure all the id&#39;s of the DOM elements, and the data structure of the page content (stored with {pageTitle, pageContent }) , referenced/shared by the js files match up exactly. take note to only use Chrome Manifest V3 apis. rename the extension to code2prompt2code<span>&#34;</span></span> --model=gpt-4 <span><span>#</span> takes 4 mins. produces semi working chrome extension copy based purely on the model-generated description of a different codebase</span>

<span><span>#</span> must go deeper</span>
modal run main.py --prompt code2prompt-gpt4.md --directory code2prompt2code</pre></div>
<p dir="auto">We leave the social and technical impacts of multilayer generative deep-frying of codebases as an exercise to the reader.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-future-directions" aria-hidden="true" href="#future-directions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>future directions</h2>
<p dir="auto">things to try/would accept open issue discussions and PRs:</p>
<ul dir="auto">
<li><strong>specify .md files for each generated file</strong>, with further prompts that could finetune the output in each of them
<ul dir="auto">
<li>so basically like <code>popup.html.md</code> and <code>content_script.js.md</code> and so on</li>
</ul>
</li>
<li><strong>bootstrap the <code>prompt.md</code></strong> for existing codebases - write a script to read in a codebase and write a descriptive, bullet pointed prompt that generates it
<ul dir="auto">
<li>done by <code>smol pm</code>, but its not very good yet - would love for some focused polish/effort until we have quine smol developer that can generate itself lmao</li>
</ul>
</li>
<li><strong>ability to install its own dependencies</strong>
<ul dir="auto">
<li>this leaks into depending on the execution environment, which we all know is the path to dependency madness. how to avoid? dockerize? nix? <a href="https://twitter.com/litbid/status/1658154530385670150" rel="nofollow">web container</a>?</li>
<li>Modal has an interesting possibility: generate functions that speak modal which also solves the dependency thing <a href="https://twitter.com/akshat_b/status/1658146096902811657" rel="nofollow">https://twitter.com/akshat_b/status/1658146096902811657</a></li>
</ul>
</li>
<li><strong>self-heal</strong> by running the code itself and use errors as information for reprompting
<ul dir="auto">
<li>however its a bit hard to get errors from the chrome extension environment so we did not try this</li>
</ul>
</li>
<li><strong>using anthropic as the coding layer</strong>
<ul dir="auto">
<li>you can run <code>modal run anthropic.py --prompt prompt.md --outputdir=anthropic</code> to try it</li>
<li>but it doesnt work because anthropic doesnt follow instructions to generate file code very well.</li>
</ul>
</li>
<li><strong>make agents that autonomously run this code in a loop/watch the prompt file</strong> and regenerate code each time, on a new git branch
<ul dir="auto">
<li>the code could be generated on 5 simultaneous git branches and checking their output would just involve switching git branches</li>
</ul>
</li>
</ul>
</article>
          </div></div>
  </body>
</html>
