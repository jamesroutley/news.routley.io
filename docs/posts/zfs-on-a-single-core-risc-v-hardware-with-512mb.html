<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://andreas.welcomes-you.com/zfs-risc-v-512mb-lichee-rv/">Original</a>
    <h1>ZFS on a single core RISC-V hardware with 512MB</h1>
    
    <div id="readability-page-1" class="page"><div>           
            <h2>Is this feasible?</h2>
<p>Yes it is! OpenZFS on Linux compiled from sources for RISC-V, it will work with less than 250MB of RAM for a nearly fully filled 2TB zfs volume with 225.000 files! </p>
<p>Even stressing the system and run in parallel (via Wifi): a scp copy (with 650kB/s), a samba transfer (with 1.3MB/s), writing random data with dd on disk (with 2.8MB/s), finding files, running apt update, zfs auto snapshot and doing a zpool scrub, the board needs well below 450MB of RAM.</p>
<div><div data-is-empty="false" data-columns="1">
<figure><a href="https://andreas.welcomes-you.com/media/posts/4/gallery/Overview-htop-iotop-etc-in-stress-test.png" data-size="1554x1161"><img loading="lazy" src="https://andreas.welcomes-you.com/media/posts/4/gallery/Overview-htop-iotop-etc-in-stress-test-thumbnail.png" alt="" width="500"/></a>
<figcaption>Overview giving info from htp, iotop and zfs</figcaption>
</figure>
</div></div>
<h3>My conclusion</h3>
<p>Linux and OpenZFS are an engineering marvel! And the OpenZFS community has ported it successfully to RISC-V hardware, considering in particular that there isn&#39;t a lot of hardware available yet. Well done Linux and OpenZFS community!</p>
<p><strong>...and the recommendation of 1GB of RAM per TB of storage?</strong></p>
<p>I think I showed that having 256MB per TB is feasible -and probably one can manage even more TB of storage with the same amount of memory- hence you don&#39;t <strong><em>need</em> </strong>1GB of RAM per TB, but you might <strong><em>want</em> </strong>to have more for performance reasons.</p>
<p><strong>[Update-2022-03-13]</strong> As I learned from Durval Menezes on the <a href="https://zfsonlinux.topicbox.com/groups/zfs-discuss/T85363c84178b7180/zfs-on-a-single-core-risc-v-hardware-with-512mb-sipeed-lichee-rv-d1" target="_blank" rel="noopener noreferrer">zfsonlinux forum</a>, the: <em>&#34;1GB RAM per 1TB Disk&#34; rule used to be the case until a few years ago (around ZFS 0.7.x-0.8.x IIRC), but then our cherished devs did a round of specific memory optimizations and since then it&#39;s no longer the case</em>. <strong>[/Update]</strong></p>

<h2>The details</h2>
<p>Below I have some more details on how I have configured the system, including some tricks I had to apply to get ZFS running on limited memory without getting a kernel panic, the performances I measured and how I have build the ZFS kernel module and tools on the Sipeed Lichee RV RISC-V board.</p>
<h3>Performances</h3>
<p>To estimate the maximum read/write speeds (in MB/s) I used 1GB sequential reads/writes on the Sipeed Lichee RV carrier board with RISC-V D1 processor. 1GB was chosen to ensure that any possible caching will not have a big impact on the measurements (the board has only 512MB of RAM). </p>
<table>
<tbody>
<tr>
<td> </td>
<td>Write</td>
<td>random data</td>
<td>Read </td>
<td></td>
</tr>
<tr>
<td colspan="5">256GB SSD via USB2</td>
</tr>
<tr>
<td>ext4</td>
<td>35.2</td>
<td>12.4</td>
<td>41.3</td>
<td>40.9</td>
</tr>
<tr>
<td>zfs</td>
<td>38.2</td>
<td>15.1</td>
<td>42.0</td>
<td>41.7</td>
</tr>
<tr>
<td>zfs native encryption</td>
<td>4.0</td>
<td>3.0</td>
<td>3.7</td>
<td>3.7</td>
</tr>
<tr>
<td>LUKS + zfs</td>
<td>8.0</td>
<td>5.7</td>
<td>7.6</td>
<td>7.6</td>
</tr>
<tr>
<td colspan="5">2TB HDD via USB2</td>
</tr>
<tr>
<td>zfs</td>
<td>22.5</td>
<td>13.3</td>
<td>27.1</td>
<td>26.1</td>
</tr>
<tr>
<td colspan="5">Internal SD card (MMC)</td>
</tr>
<tr>
<td>ext4</td>
<td>12.8</td>
<td>10.4</td>
<td>12.1</td>
<td>12.1</td>
</tr>
</tbody>
</table>
<p>It is impressive that zfs is more performant than ext4. However, the native encryption of ZFS lags much behind the combination LUKS+zfs. This is not specific to the RISC-V implementation, from my experience this is also the case on Intel architecture.   </p>

<h3>The tricks</h3>
<h4>ZFS ARC size</h4>
<p>The ZFS kernel modue by default allocates half of the system RAM for caching/buffers. This would amounts to 256MB for the Lichee RV board. One can query the min and max ZFS ARC sizes directly from the kernel module </p>
<pre>cat /sys/module/zfs/parameters/zfs_arc_min</pre>
<p>or get a summary with the arc_summary tool</p>
<pre>arc_summary -s arc</pre>
<p>The sizes can be adapted, either by defining them in <code>/etc/modprobe.d/zfs.conf</code>, or by updating them in the running system with  </p>
<pre>sudo echo size_in_bytes &gt;&gt; /sys/module/zfs/parameters/zfs_arc_max</pre>
<p>It seems that the lowest value one can set is 64MB (67108864 bytes). Hence this is what I have used in the tests above.</p>
<h4>Dropping caches</h4>
<p>Reducing the ZFS caches helps already a lot, but to ensure that the kernel memory will not get fragmented and as a consequence the system runs out of memory, one can use a feature that tells the kernel to drop the caches. A crontab did the job on the RISC-V board by having the following line in the root crontab (dropping every 10min the caches):</p>
<pre>*/10 * * * * /usr/bin/sync; /usr/bin/echo 3 &gt; /proc/sys/vm/drop_caches</pre>
<p>This approach was needed to prevent kernel panics, before I discoved that one can reduce the ZFS caches. But honestly it doesn&#39;t hurt - except that one might get less performance. I didn&#39;t mind, since this whole process was more to experiment than really setting up a NAS based on RISC-V. Althought it is not impossible to use this board as a NAS, if you don&#39;t mind maybe a bit lower transfer speeds than you normally might expect (I get around 12MB/s via samba from an unencrypted ZFS volume).</p>

<h3>How to build OpenZFS on RISC-V</h3>
<p>As basis I have used the <a href="https://andreas.welcomes-you.com/boot-sw-debian-risc-v-lichee-rv/" target="_blank" rel="noopener noreferrer">boot software and Debian root file system</a> which I had documented in my other blog post.</p>
<p>On Debian unstable (at time of writing, March 2022: bookworm/sid), OpenZFS is not yet available as debian package (zfsutils-linux). Hence you have to compile and build OpenZFS yourself.</p>
<p><span>Unfortunately OpenZFS seems not to support (yet) cross-compiling for the RISC-V platform, hence you have to build the kernel on the RISC-V board. But since the ZFS kernel module has to be compiled with the same compiler as the kernel -and I used gcc 9 to cross-compile the kernel and gcc 11 is provided with Debian bookworm/sid- I had also to recompile the Linux kernel on this single core RISC-V processor. This took 4.5h, but ok, I really wanted to get ZFS running :-).  </span></p>
<p>To build, you should follow the <a href="https://openzfs.github.io/openzfs-docs/Developer%20Resources/Building%20ZFS.html" target="_blank" rel="noopener noreferrer">instructions</a> given by OpenZFS which first tells you the packages that are needed</p>
<pre>sudo apt install build-essential autoconf automake libtool gawk alien fakeroot dkms libblkid-dev uuid-dev libudev-dev libssl-dev zlib1g-dev libaio-dev libattr1-dev libelf-dev python3 python3-dev python3-setuptools python3-cffi libffi-dev python3-packaging git libcurl4-openssl-dev</pre>
<p>then you clone the repository</p>
<pre>git clone https://github.com/openzfs/zfs.git</pre>
<p>and execute</p>
<pre>cd ./zfs</pre>
<p>Finall you can install the kernel module</p>
<pre>sudo make install; sudo ldconfig; sudo depmod</pre>
<p>To test the module, you can run</p>
<pre>modprobe zfs</pre>
<p>and if successful, you can add a new line in /etc/modules with &#34;zfs&#34;. Done!</p>

<h3>ZFS auto snapshot</h3>
<p>Since the debian package zfs-auto-snapshot is not yet available on RISC-V, probably due to its dependency to zfsutils-linux, I downloaded it from <a href="https://github.com/zfsonlinux/zfs-auto-snapshot" target="_blank" rel="noopener noreferrer">zfsonlinux/zfs-auto-snapshot</a> and followed the instruction in the README:</p>
<pre>wget https://github.com/zfsonlinux/zfs-auto-snapshot/archive/upstream/1.2.4.tar.gz</pre>
<p>This will create backups every 15min (4x), hourly (24x), daily (31x), weekly(8x) and monthly (12x).</p>            
         </div></div>
  </body>
</html>
