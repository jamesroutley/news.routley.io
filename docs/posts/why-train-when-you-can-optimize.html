<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://justinmeiners.github.io/why-train-when-you-can-optimize/">Original</a>
    <h1>Why train when you can optimize?</h1>
    
    <div id="readability-page-1" class="page">
<!-- Generated by srcweave https://github.com/justinmeiners/srcweave -->



<p><strong>By:</strong> <a href="https://justinmeiners.github.io">Justin Meiners</a></p>

<p>View the final code at the <a href="https://github.com/justinmeiners/why-train-when-you-can-optimize">GitHub repo</a></p>

<p><a href="https://en.wikipedia.org/wiki/Mathematical_optimization">Multi-variable optimization</a> is an incredibly useful tool for writing software.
It allows you to specify a desired result and delegate finding the solution to a reliable algorithm.
If you know what you want but not how to get it, optimization can probably help.
Problems with many interacting variables that are unmanageable with traditional programming approaches become straightforward when using optimization.</p>

<p>Optimization has been researched by mathematicians for decades and plays an essential role in fields like <a href="https://en.wikipedia.org/wiki/Stationary-action_principle">classical mechanics</a> and <a href="https://en.wikipedia.org/wiki/Optimal_control">control theory</a>.
But, only recently has it become popular in software due to its role in machine learning.
Unfortunately, most programmers&#39; understanding of optimization is restricted to <a href="https://en.wikipedia.org/wiki/Backpropagation">training neural networks</a>.
The potential for optimization as a general software tool isn’t being realized,
despite being very general and accessible.
Furthermore, many deep learning applications can be restated as optimization problems,
with better results.</p>

<p>In this article, I will introduce optimization by creating a <a href="https://justinmeiners.github.io/why-train-when-you-can-optimize/src/">drawing assistant</a> that refines hand-drawn figures into perfect shapes. This project is written in JavaScript using the <code>&lt;canvas&gt;</code> element.</p>

<ol><li><a href="#s0:0">Project setup</a></li><li><a href="#s0:1">Optimization overview</a></li><li><a href="#s0:2">Drawing assistant overview</a></li><li><a href="#s0:3">Lines</a></li><li><a href="#s0:4">How do optimizers work?</a></li><li><a href="#s0:5">When do optimizers fail?</a></li><li><a href="#s0:6">Circles</a></li><li><a href="#s0:7">Simple rectangles</a></li><li><a href="#s0:8">Transformed rectangles</a></li><li><a href="#s0:9">Combining all shape fits</a></li><li><a href="#s0:10">What is the connection between deep learning and optimization?</a></li><li><a href="#s0:11">Further study</a></li></ol>




<h2>1. Project setup<a id="s0:0"></a></h2>


<p>This project is inspired by the drawing feature in the iPad notes app (with Apple Pencil).
As you draw a figure similar to a line, circle, etc,
it will detect the intended shape and offer to replace the rough drawing with a perfect one.</p>

<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/ipad-notes.gif" alt="ipad pencil drawing correction"/></p>

<p>This is great software design.
It complements the abilities of a human with the precision of a computer.
It manages to be even easier than drawing with a real pencil and paper
by restricting the set of possible choices to those that are actually useful.</p>

<p>It’s also not the kind of feature you would typically see in a desktop application.
Just think of how hard it would be to program with a traditional approach.
“Look for angles close to 90 degrees, if the shape has exactly four of them arranged in the following way.. then.. else.”
What a mess!
Even if we get something working, it will have so many edge cases that it will never work well enough to trust.</p>

<h2>2. Optimization overview<a id="s0:1"></a></h2>


<p><a href="https://en.wikipedia.org/wiki/Mathematical_optimization"><strong>Multi-variable optimization</strong></a> is about finding minima of functions that take many inputs and produce a single output.
Given a real valued function <span><code>f\colon \mathbb{R}^{n} \rightarrow \mathbb{R}</code></span>,
we want to find a point <span><code>a = (a_{1}, \ldots, a_{n}) \in \mathbb{R}^n</code></span> so that
<span><code>f(a) \leq f(x)</code></span> for all <span><code>x</code></span>, or at least the <span><code>x</code></span> in some region.</p>

<p>An <strong>optimizer</strong> is an algorithm that attempts to do this automatically.
It intelligently tests many different variable assignments
until it finds one that minimizes <span><code>f</code></span>.
Later, we will provide more detail about how it works and identify a few mathematical requirements for <span><code>f</code></span> that allow it to be optimized.</p>

<p>So, how can optimization help our drawing project?
As mentioned before, optimization is useful for solving problems where you know what you want, but not all the steps to get there.
First, we need a model of the problem.
You may already have one in the form of a black box.
This may be a piece of code or a physical system, and you just want it to behave
a certain way.
Otherwise, you may have an intuitive idea of a problem and need to construct your own model.
The important part is describing all the variables at play (there may be hundreds!) and how they relate.
With a good model, solving the problem is then a manner of finding
inputs that give the intended results.</p>

<p>So, how do we configure all the variables in the models to get the desired outcome?
Depending on the problem, you may be able to solve it directly,
like the math and physics homework practiced in school.
But, with optimization, all we have to do is describe <em>what a good configuration looks like</em>.
We do this by defining a <strong>cost or preference function</strong>
that takes all the variables as input and
measures how well they match our desired outcome.
This is the function <span><code>f \colon \mathbb{R}^{n} \rightarrow \mathbb{R}</code></span>
that we will eventually want to optimize.</p>

<p>The idea is to define the cost function <span><code>f</code></span> carefully, such that the variables that minimize <code>f</code> will also be good solutions to our application.
This is harder than it sounds.
It’s always difficult to express an intuitive idea in a precise quantitative manner.
Often you think you want one thing, only to learn you actually need something else entirely.
But, with a little thought and experimentation, we can become confident that
<span><code>f</code></span> expresses what we want.</p>

<p>When <span><code>f</code></span> is defined, we hand it off to the optimizer to find a minimum.
Assuming the algorithm is able to find a
minimum, this result will satisfy the preferences expressed by our function
and consequently solve the problem.</p>

<h2>3. Drawing assistant overview<a id="s0:2"></a></h2>


<p>Let’s try applying this approach to the drawing assistant.
First, let’s describe part of the model.</p>

<ol>
<li><p>The user will draw a path by hand.
This is represented by an array of points, in order, that form a stroke.</p>

<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/stroke.png" alt="stroke"/></p></li>
<li><p>We then want to classify whether its intended shape is a line, circle, rectangle, or neither.
The path is then replaced with the ideal shape that matches it.</p></li>
<li><p>The drawn path could be positioned anywhere in the canvas, as well as rotated, scaled, etc.
How do we find the particular shape and transformation which best matches the drawing?</p></li>
</ol>


<p>This sounds hard!
But, once again, with optimization, we don’t have to worry too much about actually
finding the shape.
If we can clearly specify a cost function that expresses how well a shape matches our drawing,
then we can let the optimizer do the work of actually finding the shape.</p>

<h3>Template</h3>

<p>I have provided a template that takes care of the drawing part.
If you want to follow along by writing your own code, here are the files you will need:</p>

<ul>
<li><a href="https://justinmeiners.github.io/why-train-when-you-can-optimize/src/math.js">math.js</a>: A vector math library that also includes an optimization algorithm. (<a href="https://justinmeiners.github.io/why-train-when-you-can-optimize/math.html">docs</a>)</li>
<li><a href="https://justinmeiners.github.io/why-train-when-you-can-optimize/src/draw.js">draw.js</a>: Simple canvas code for mouse input and drawing. (<a href="https://justinmeiners.github.io/why-train-when-you-can-optimize/template.html">docs</a>)</li>
</ul>


<p>These can be loaded in a trivial HTML template, with <code>&lt;canvas id=&#34;main-canvas&#34;&gt;</code>
somewhere in the body.</p>

<h2>4. Lines<a id="s0:3"></a></h2>


<p>The line is a simple shape to start with.
We first need a mathematical model to describe it.
I have chosen to describe it as an <code>origin</code> and <code>angle</code>.
The formula <span><code>y = mx + b</code></span> could be used in other contexts,
but doesn’t work for vertical lines (undefined slope).
Two points in the plane could also work,
but you have to be careful to avoid the case of them overlapping.
So to represent a line in this form we need 3 real variables:</p>

<ul>
<li><code>origin.x</code></li>
<li><code>origin.y</code></li>
<li><code>angle</code></li>
</ul>


<p>Now we need to define a cost function <code>f</code> which measures
how well a given line, specified by these variables, fits our drawing.
A natural choice to start with is the <a href="https://en.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance"><strong>sum of squares</strong></a>
of the distances from each point to the line:</p>

<p><code>\begin{equation}    f(x_{1}, \ldots,  x_{n}) = \sum_{i=1}^{n} d(x, L)^{2}\end{equation}</code></p>


<p>where <span><code>L</code></span> is the set of points in the line.
Here is this formula written in code:</p>

<div>
<pre><code>function sumOfSquares(distances) {
    return distances.reduce((total, term) =&gt; total + term * term, 0);
};

function lineDistance2(points, line) {
    const normal = line.direction.orthogonal();
    const distances = points.map(p =&gt; Vec.sub(p, line.origin).dot(normal));
    return sumOfSquares(distances);
};
</code></pre>
<p><small>Used by <a href="#bline_optimization:207" title="index.lit:208 line optimization">1</a> </small></p></div>


<p>The code uses a direction vector instead of the angle,
This is more convenient for measuring distance and can be derived immediately from the angle.</p>

<p>This is a good start, but it’s not a cost function yet.
To make it one, we need to remove all parameters that are not variables.
Recall that <code>origin</code> and <code>angle</code> are the only variables we want to determine.
The points in the drawn path are fixed and should not be changed.
We do this by constructing a closure that captures the constant inputs (also known as <a href="https://en.wikipedia.org/wiki/Currying">currying</a>).
The closure will take a single array as input (this is what the optimizer expects).
We then need to pull it apart to get all the variables to pass to the function above.</p>

<div>
<pre><code>function varsToLine(vars) {
    return {
        origin: new Vec(vars[0], vars[1]),
        direction: Vec.fromAngle(vars[2])
    }
}

function makeLineCost(points) {
    return vars =&gt; lineDistance2(points, varsToLine(vars));
};
</code></pre>
</div>


<p>This is now a cost function, ready to pass to an optimizer!</p>

<p>We are almost ready.
However, all optimizers require some configuration and tuning.
For ours, we need an <strong>initial point</strong>.
This is a rough guess of where the minima will actually be.
The optimizer will start by looking in a region around this point.
The center of the drawing (the centroid of points) is a good guess for the <code>origin</code>.
But, we don’t have a good guess for what the <code>angle</code> is.</p>

<p>We also need an <strong>iteration limit</strong>, which specifies the
number of steps the optimizer will take to find a minimum.
Be warned that this is proportional to how
many times the cost function will be called!
Efficient cost functions are important to good performance.</p>

<div>
<pre><code><em title="index.lit:157"><a href="#bline_cost_function:156">@{line cost function}</a></em>

function tryLineFit(points) {
    const centroid = Vec.centroid(points);

    const initial = [ centroid.x, centroid.y, 0 ];
    const result = multivarOptimize(initial, makeLineCost(points), {
        maxIterations: 1000
    });

    return {
        cost: result.cost,
        path: lineToDrawing(points, varsToLine(result.vars))
    };
};

<em title="index.lit:232"><a href="#bline_to_drawing:231">@{line to drawing}</a></em>
</code></pre>
<p><small>Used by <a href="#bshape_fit:644" title="index.lit:645 shape fit">1</a> </small></p></div>


<p>We need one more function to help turn our final answer into an actual drawing to display.
Note that our line model describes the <em>infinite line</em> that fits the drawing.
After the optimizer finds the best one, we need to trim it to the proper length to match the drawing.</p>

<div>
<pre><code>function lineToDrawing(points, line) {
    const distances = points.map(p =&gt; Vec.sub(p, line.origin).dot(line.direction));
    const start = Math.min.apply(Math, distances);
    const end = Math.max.apply(Math, distances);

    return [
        Vec.add(line.origin, Vec.scale(line.direction, start)),
        Vec.add(line.origin, Vec.scale(line.direction, end))
    ];
};
</code></pre>
<p><small>Used by <a href="#bline_optimization:207" title="index.lit:208 line optimization">1</a> </small></p></div>


<p>To see this code work in the template, define the following function (which will be called
in the <code>draw.js</code> file):</p>

<div>
<pre><code>function tryShapeFit(path) {
    return tryLineFit(path);
}
</code></pre>
<p><small>Used by <a href="#b/assist.js:677" title="index.lit:678 /assist.js">1</a> </small></p></div>


<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/line-drawing.gif" alt="line drawing"/></p>

<p>Isn’t that exciting?</p>

<p><strong>Note:</strong> Linear regression might come to mind as a more direct solution to the line fit problem.
It is probably better for lines, but it does not generalize to all the shapes that we want to draw,
nor does it demonstrate the general problem-solving approach.</p>

<h2>5. How do optimizers work?<a id="s0:4"></a></h2>


<p>Now that you have seen an optimizer solve a problem, let’s review how they work.
If you look at a graph of a function, you’ll notice the maxima and minima points always occur at peaks and valleys.
A little thought reveals this to be obvious.
If a point is on a slope, it can always get smaller by walking downhill.
You may recall from calculus that these points can be found by finding where the slope (derivative) is 0.</p>

<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/function-extrema.png" alt="function extrema"/></p>

<p>So, most optimizers tend to work by “stepping” down the slope of a function until they find a valley.
They also use some tricks to avoid getting trapped in small valleys
(in hopes of finding big valleys),
but <a href="https://en.wikipedia.org/wiki/Gradient_descent">following slopes downhill</a> is their basic mode of operation.</p>

<p>In the case of multiple variables, more tricky situations can occur,
so finding minima is more difficult and time-consuming, but the idea is the same.</p>

<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/function-3d-1.png" alt="3d function"/></p>

<p><strong>Note:</strong> For those interested, this collection of <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization">test functions</a> illustrate some difficult situations.</p>

<p>The important thing to remember is that all optimizers make assumptions about the structure of the cost function.
For general optimizers, that usually means the function is <strong>continuous</strong> (it doesn’t have breaks and hopefully has some hills), and some number of <strong>derivatives</strong> exist (we can compute downhill directions).
In practice, small discontinuities and other minor imperfections tend not to matter,
but broadly, there must be peaks and valleys that the optimizer can actually follow.
Given a totally random function, the optimizer can’t do anything but test values in a brute force manner.</p>

<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/function-not-continuous.png" alt="function not continuous"/></p>

<p><strong>Exercise:</strong> Verify that the line cost function we defined is differentiable.</p>

<h3>Which optimizer is used here?</h3>

<p>In this article, I use the <a href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method">Nelder-Mead</a> algorithm.
It’s convenient because it doesn’t require much extra information besides the function itself (many algorithms require providing an explicit derivative).
The tradeoff is it doesn’t have a great <a href="http://www.scholarpedia.org/article/Nelder-Mead_algorithm">theoretical reputation</a> and can be slower.
A good heuristic is that it should give similar results to gradient descent,
but will be much slower (especially with many variables).
I reach for it first for experimentation and then upgrade if it is no longer adequate.</p>

<h2>6. When do optimizers fail?<a id="s0:5"></a></h2>


<p>The challenge for mathematicians who design optimization algorithms
is to ensure they find minima, and that they do so in a reasonable amount of time.
In this article, we don’t have to worry much about the optimizer failing,
as the problem only involves a few variables with simple cost functions.
Furthermore, if it does fail, the results aren’t catastrophic.
We either get a slightly worse shape fit, or if it fails entirely,
 the cost of the output will be high,
and we can simply reject and ignore it.</p>

<p>However, in optimization problems with many variables and complex cost functions,
finding a minimum in a timely manner, or even at all, becomes a real challenge.
This is where a great amount of theory has been developed to classify
cost functions and pair them with algorithms that are guaranteed to give meaningful results.
(It’s a field you can get a Ph.D in.)</p>

<p>One way to verify that the optimizer is failing is by manually constructing examples with lower
cost than the optimizer is finding and checking their cost against the cost function.
If it’s what you expect, then the optimizer is just not finding it.
For most software applications, it’s just a matter of tuning the cost function
and the parameters (like the initial guess) to get things working.</p>

<p><strong>Exercise:</strong> Experiment with the iteration limit parameter for line fit. When does it start failing to converge?</p>

<h2>7. Circles<a id="s0:6"></a></h2>


<p>Now we return to the drawing assistant.</p>

<p>We will make things more interesting by adding a few more shapes.
Circles are almost as easy to represent as lines.
Once again, we have three variables:</p>

<ul>
<li><code>origin.x</code></li>
<li><code>origin.y</code></li>
<li><code>radius</code></li>
</ul>


<p>Just like for the line, we can find the distance from each point in the drawing to the circle.
The cost function is constructed using the sum of squares.</p>

<div>
<pre><code>function circleDistance(point, circle) {
    var v = Vec.sub(point, circle.origin);
    return Math.abs(v.len() - circle.radius);
};

function circleDistance2(points, circle) {
    return sumOfSquares(points.map(p =&gt; circleDistance(p, circle)));
};

function varsToCircle(vars) {
    return {
        origin: new Vec(vars[0], vars[1]),
        radius: vars[2]
    }
};

function makeCircleCost(points) {
    return vars =&gt; circleDistance2(points, varsToCircle(vars));
};
</code></pre>
<p><small>Used by <a href="#bcircle_optimization:369" title="index.lit:370 circle optimization">1</a> </small></p></div>


<p>For the optimization, we can once again use the centroid of the drawing
as a guess for the origin of the circle.
A good guess for the radius of the circle is the size of the bounding
rectangle around the drawing.</p>

<div>
<pre><code><em title="index.lit:343"><a href="#bcircle_cost_function:342">@{circle cost function}</a></em>

function tryCircleFit(points) {
    const centroid = Vec.centroid(points);
    const [minPoint, maxPoint] = Vec.bounds(points);

    const initial = [
        centroid.x, centroid.y,
        Math.max(maxPoint.x - minPoint.x, maxPoint.y - minPoint.y)
    ];
    const result = multivarOptimize(initial, makeCircleCost(points), {
        maxIterations: 1000
    });

    const circle = varsToCircle(result.vars);
    if (!circleMatches(points, circle)) return null;

    return {
        cost: result.cost,
        circle: circle
    };
};

<em title="index.lit:409"><a href="#bcircle_matches:408">@{circle matches}</a></em>
</code></pre>
<p><small>Used by <a href="#bshape_fit:644" title="index.lit:645 shape fit">1</a> </small></p></div>


<p>With the circle, we have a new problem to solve.
Sometimes the optimizer finds a circle which is close to all the points in the drawing,
but is much larger.
For example, if we draw a slightly curved line, the optimizer
can fit it closely by making a giant circle that is mostly off-screen.</p>

<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/circle-fail.gif" alt="circle fail"/></p>

<p>This behaviour is usually unexpected.
To correct for it, we can ensure the size of the circle is close to the drawing.
We do this by measuring their relative path lengths and verifying they are similar.</p>

<div>
<pre><code>function circleMatches(path, circle) {
    const circumference = 2.0 * Math.PI * circle.radius;
    if (circumference &lt; 10) return false;

    const ratio = Vec.pathLen(path) / circumference;
    return Math.abs(ratio - 1.0) &lt; 0.15;
};
</code></pre>
<p><small>Used by <a href="#bcircle_optimization:369" title="index.lit:370 circle optimization">1</a> </small></p></div>


<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/circle-drawing.gif" alt="circle drawing"/></p>

<p><strong>Exercise:</strong> Experiment with different initial variables. How do they affect output quality?</p>

<h2>8. Simple rectangles<a id="s0:7"></a></h2>


<p>The circle was about as simple as the line due to symmetry.
Now we come to a real challenge.
A rectangle can come in any position, size, and angle.
How can we model this?</p>

<p>Like any complex problem, we can break it down into manageable parts.
In this case, we can focus on defining the model for a simple rectangle first,
and worry about transforming it later.
The simple rectangle model we start with is an axis-aligned box, with one corner at the origin.
It’s defined entirely defined by two variables:</p>

<ul>
<li><code>width</code></li>
<li><code>height</code></li>
</ul>


<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/rect-model.png" alt="rect model"/></p>

<p>Let’s define a function that measures the distance between a simple axis-aligned box and a point.
Unfortunately, even for this simplified model, there is no clean formula.
We have to handle nine different cases,
which are the possible sections of the plane
the point could be in.</p>

<div>
<pre><code>function classify(x, min, max) {
    if (x &lt; min) {
        return -1;
    } else if (x &gt; max) {
        return 1;
    } else {
        return 0;
    }
}
const sideX = classify(point.x, 0, size.x);
const sideY = classify(point.y, 0, size.y);
</code></pre>
<p><small>Used by <a href="#bsimple_rect_distance:464" title="index.lit:465 simple rect distance">1</a> </small></p></div>


<p>When the point lies within a corner section,
its distance to the box is the distance from the corner.
Otherwise, the distance is one of its coordinate components.</p>

<div>
<pre><code>function simpleRectDistance(point, size) {
    <em title="index.lit:447"><a href="#brect_side_classification:446">@{rect side classification}</a></em>
    switch (sideX) {
    case -1:
        switch (sideY) {
        case -1: return point.len();
        case 0:  return -point.x;
        case 1:  return Vec.dist(point, new Vec(0, size.y));
        }
    case 0:
        switch (sideY) {
        case -1: return -point.y;
        case 0:  return Math.min(point.x, point.y, size.x - point.x, size.y - point.y);
        case 1:  return point.y - size.y;
        }
    case 1:
        switch (sideY) {
        case -1: return Vec.dist(point, new Vec(size.x, 0));
        case 0:  return point.x - size.x;
        case 1:  return Vec.dist(point, size);
        }
    }
};
</code></pre>
<p><small>Used by <a href="#brect_cost_function:533" title="index.lit:534 rect cost function">1</a> </small></p></div>


<p><strong>Exercise:</strong> Describe what a “heat map” style visualization of this distance function would look like.</p>

<p>The conditional statements are a good indicator that the function we are defining
might be problematic for the optimizer.
We need to be careful about its continuity and differentiability.
A little analysis will show it is indeed continuous and in only a few places not differentiable.
So, this is likely a safe function.</p>

<p><strong>Exercise:</strong> Determine the set of points where <code>simpleRectDistance</code> is not differentiable.</p>

<p>With a distance function, we can now define a measure of fit,
using the sum of squares.</p>

<div>
<pre><code>function simpleRectDistances2(points, size) {
    return sumOfSquares(points.map(p =&gt; simpleRectDistance(p, size)));
};
</code></pre>
</div>




<h2>9. Transformed rectangles<a id="s0:8"></a></h2>


<p>That works for simple axis-aligned rectangles, but we still need to handle the general rectangle case.
Let’s introduce an <a href="https://en.wikipedia.org/wiki/Affine_transformation">affine matrix</a> transformation to rotate and position the rectangle.
Don’t worry if you aren’t too familiar with matrices.
The important idea is it combines the rotation and translation into one operation.</p>

<p>Including the size of the rectangle and the transformation matrix,
there will be five total variables to optimize:</p>

<ul>
<li><code>size.x</code></li>
<li><code>size.y</code></li>
<li><code>translate.x</code></li>
<li><code>translate.y</code></li>
<li><code>angle</code></li>
</ul>


<p>To measure the cost, we will apply the inverse transformation
to transform all of the points in the canvas into the local
coordinate system of the box.
In local space, the rectangle is a simple axis-aligned box!
Then we can use the simple cost functions we just defined.</p>

<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/inverse-transform.png" alt="inverse transform"/></p>

<div>
<pre><code><em title="index.lit:465"><a href="#bsimple_rect_distance:464">@{simple rect distance}</a></em>

function buildTransform(translate, angle) {
    const A = new AffineTransform();
    A.t = translate;
    A.m = Matrix.fromAngle(angle);
    return A;
};

function orientedRectDistances2(points, rect) {
    const A = buildTransform(rect.translate, rect.angle);
    const B = AffineTransform.inverse(A);

    const localPoints = points.map(p =&gt; B.transform(p));
    return simpleRectDistances2(localPoints, rect.size);
};

function varsToRect(vars) {
    return {
        translate: new Vec(vars[0], vars[1]),
        size: new Vec(vars[2], vars[3]),
        angle: vars[4]
    };
};

function makeRectCost(points) {
    return vars =&gt; orientedRectDistances2(points, varsToRect(vars));
};
</code></pre>
<p><small>Used by <a href="#brect_optimization:570" title="index.lit:571 rect optimization">1</a> </small></p></div>


<p>This rectangle has taken more work to build the cost function,
but the optimization isn’t any harder than before.
We use the bounds of the drawing as the guess
for the box size and translation.</p>

<div>
<pre><code><em title="index.lit:534"><a href="#brect_cost_function:533">@{rect cost function}</a></em>

function tryRectFit(points) {
    const [ minPoint, maxPoint ] = Vec.bounds(points);

    const initial = [
        minPoint.x, minPoint.y,
        maxPoint.x - minPoint.x, maxPoint.y - minPoint.y,
        0
    ];

    const result = multivarOptimize(initial, makeRectCost(points), {
        maxIterations: 1000
    });

    const rect = varsToRect(result.vars);
    if (!rectMatches(points, rect)) return null;

    return {
        cost: result.cost,
        path: rectToDrawing(rect)
    };
};
</code></pre>
<p><small>Used by <a href="#bshape_fit:644" title="index.lit:645 shape fit">1</a> </small></p></div>


<p>Like the circle, we also need to reject boxes
which do not plausibly match the drawing.</p>

<div>
<pre><code>function rectMatches(path, rect) {
    if (rect.size.x &lt; 6 || rect.size.y &lt; 6) {
        return false;
    }

    const length = rect.size.x * 2 + rect.size.y * 2;
    const ratio = Vec.pathLen(path) / length;
    return Math.abs(ratio - 1.0) &lt; 0.15;
};
</code></pre>
</div>


<p>Lastly, we need to convert the rect to a drawing.
Unlike circles which have a special drawing type,
we can represent the rect as a path.</p>

<div>
<pre><code>function rectToDrawing(rect) {
    const points = [
        new Vec(0, 0),
        new Vec(rect.size.x, 0),
        new Vec(rect.size.x, rect.size.y),
        new Vec(0, rect.size.y),
        new Vec(0, 0)
    ];

    const A = buildTransform(rect.translate, rect.angle);
    return points.map(p =&gt; A.transform(p));
};
</code></pre>
</div>


<p><img src="https://justinmeiners.github.io/why-train-when-you-can-optimize/img/rect-drawing.gif" alt="rect drawing"/></p>

<p><strong>Exercise:</strong> Explain why it would be difficult to measure distance if we performed scaling in the transformation,
instead of sizing the simple rectangle before the transformation.</p>

<h2>10. Combining all shape fits<a id="s0:9"></a></h2>


<p>Now that we have all our shape functions,
we will redefine the <code>tryShapeFit</code> function to pick the best one.</p>




<p>We also add a check to ensure the total cost is within a threshold.
This permits the drawing of other shapes besides the correctable ones.</p>

<div>
<pre><code>function acceptableTolerance(path) {
    const perPointTolerance = 6.0;
    return path.length * perPointTolerance * perPointTolerance;
}
</code></pre>
<p><small>Used by <a href="#bshape_fit:644" title="index.lit:645 shape fit">1</a> </small></p></div>







<p>That’s all for the tutorial!</p>

<p><strong>Exercise:</strong> Try adding another shape to the drawing assistant. Note that you might
want to try defining the distance function first. Some shapes, <a href="https://blog.chatfield.io/simple-method-for-distance-to-ellipse/">like ellipses</a>, have tricky distance functions. Consider approximating.</p>

<h2>11. What is the connection between deep learning and optimization?<a id="s0:10"></a></h2>


<p>You may have noticed some similarities between this project and <a href="http://neuralnetworksanddeeplearning.com/">traditional deep learning</a>.
Optimization is a fundamental part of deep learning, but it’s not the main focus,
and is in turn, much more separated from the problem.
Instead, the approach we just used emphasizes modeling a problem mathematically and using an optimizer directly.
There is a lot of overlap between the two approaches, so let’s compare and contrast.</p>

<h3>Differences in cost functions</h3>

<p>The first difference is in how cost functions are constructed.
In this project, we analyzed geometric properties and designed cost
functions that reflect the outcome we wanted.
These functions were not just rough guesses.
With a little analysis, we can confirm that any shape with low cost
actually give us the desired results.</p>

<p>In deep learning, the cost function is constructed implicitly through training data.
Each example in a training set is a single input, and
the expected output is assigned by a person in a manner that is hopefully consistent.
The optimizer then attempts to train the neural network to fit those examples.
But, examples are discrete and usually sparse.
The gaps between examples are inferred.</p>

<h3>Tradeoffs of training</h3>

<p>Using training data, rather than direct modeling,
is motivated by the difficulty of describing desired output directly.
Transforming a human-understandable thought into an explicit cost function, that works as expected, is hard.
It requires careful thought, trial and error, and some physical or mathematical
modeling knowledge.</p>

<p>Often it’s just not feasible.
Try to imagine what the cost function must look like for a program
that identifies animals from a collection of pixels!
In such cases, deep learning allows the cost function to be developed interactively, from much less information.</p>

<p>Of course, acquiring training data is its own challenge.
Designing a data collection and storage process
in a way that prevents bias, gets good results, and is able to generalize
beyond the examples in the training set, is a significant amount of work.
What is gained with the simplicity of the approach must be made up with old-fashioned manual labor.</p>

<p>Inferring from examples is always going to be less precise than an explicit function.
But, perhaps the most worrisome problem with deep learning is the <a href="https://en.wikipedia.org/wiki/Problem_of_induction">problem of induction</a>.
A neural network may be trained on carefully picked examples
and verified using others, but how do you really know it works?
How do you know it’s going to respond well in new situations?
The only way to increase reliability is with more training and verification,
all of which are expensive and difficult.
These efforts only improve our confidence in the models, but never give certainty.
There may always be cases where they blow up and behave totally unexpectedly.
(This is part of why self-driving cars are so difficult!)</p>

<p>An additional challenge of training is it doesn’t behave
at all like traditional software.
A trained network is entirely opaque.
It’s difficult to inspect, debug, or describe how it works.
There is no mechanism to report how well its performing.
On the other hand, optimizing a good cost function looks more like programming.
(The kind we are comfortable with.)
We can inspect and reason about its state.
We can prove a minimum gives desired results,
and receive feedback in the form of high cost,
to communicate when it’s failing to find solutions.</p>

<h3>Differences in problem modeling</h3>

<p>In this project, we optimized mathematical models of lines, circles, and rectangles.
The optimizer helped find angles, coefficients, coordinates, etc
that construct shapes that fit the desired results.
Programming all of this requires some domain knowledge about geometry.
A lot of it is familiar from middle school, but some parts (like matrix transformations)
may still be a little out of reach for some readers.
Needless to say, if you haven’t studied a subject, it’s hard to model it.</p>

<p>The deep learning approach is much less concerned with modeling the underlying subject matter.
The variables to be optimized are typically neurons in a neural network.
Each neuron function is determined by a weighting of its inputs.
Instead of finding angles, coordinates, etc,
the optimizer just searches for the weights that make the neurons fire as the training
data expects.
This same neuron model is used regardless of the problem, in addition to a bit of layering
and feedback design.</p>

<p>The hope is that rather than constructing problem-specific mathematical models,
neural networks can be used as general function representations, taking on any form needed.
We have some good reasons to hope multi-layer neural networks should be able to fill this role:</p>

<ol>
<li>They are proven to be <a href="http://neuralnetworksanddeeplearning.com/chap4.html">universal function approximators</a>.</li>
<li>They can be implemented efficiently on modern parallel compute hardware.</li>
<li>There is an <a href="https://justinmeiners.github.io/neural-nets-sim/">analogy</a> between organic neurons and computer neurons, so there is
 a hope that advantages will come from mimicking brains.</li>
<li>Optimization algorithms like gradient descent work well on them (back propagation).</li>
</ol>


<h3>Tradeoffs of neural networks</h3>

<p>These are all true and useful properties of neural networks, but they also aren’t as special as they might appear. For example, we could consider approximating coefficients of polynomials or trigonometric series instead.</p>

<p>Neural networks can approximate any function, but that doesn’t mean they do so efficiently.
Depending on the function, they can require incredible amounts of neurons and training.
At their worst, they devolve into a lookup table.
It’s not hard to find these examples either.
Just try training a neural network to compute <code>sin(x)</code>!</p>

<p>One challenge of designing a neural network is knowing how to structure it.
How many layers are needed? How wide? What points should have feedback, etc.
The reason why the answer to these questions is fuzzy is because if the
answer was fully known, there would be no need to make a network.
Ultimately, any attempt to tinker with these components in the network design is an attempt to more closely mirror the underlying mathematical model.</p>

<p>In fact, the most successful machine learning approaches (such as convolution neural networks
for image processing) use neural networks on top of the underlying mathematical model.
Image processing algorithms are based on convolving a <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">kernel matrix</a> with an image.
Many of these deep learning programs are simply tuning traditional kernels with an optimizer.</p>

<p>Applying a mathematical model tailored to a problem is much more likely to be successful,
not to mention efficient and understandable.
If you are solving a geometric problem, it’s going to work best if you construct models based on geometry, equations of lines, matrices, etc.
If you are solving a physical problem, use equations from physics and then optimize
to figure out what the parameters of those models are.
The deep learning approach is a bit like looking at a few graphs of motion and
expecting to rediscover Newton’s laws.</p>

<h3>Conclusion</h3>

<p>Hopefully, this discussion has clarified the roles and tradeoffs of these two approaches.
To summarize, deep learning and related machine learning algorithms
are unique in their ability to make progress on problems we have no better tools for.
Being able to identify objects in images, or generate new images or videos, is absolutely incredible.
But perhaps it is being overused.
For many problems, more direct modeling and optimization methods are available and more likely to be reliable and successful.</p>

<p>I hope this tutorial has given you ideas for upcoming projects.
Remember to keep optimization readily available in your toolbox!</p>

<h2>12. Further study<a id="s0:11"></a></h2>


<p>Interested in learning more about optimization?
Here are several free resources, each offering a very different path to explore:</p>

<ul>
<li><p><a href="https://mitpress.mit.edu/sites/default/files/titles/content/sicm_edition_2/book.html"><strong>Structure and Interpretation of Classical Mechanics</strong></a></p>

<p>  This book is about physics, but explains it using code.
  Unfortunately, it is still quite challenging and requires a solid math background.</p>

<p>  As it turns out, optimization algorithms are important for finding motion paths.
  This is actually where I learned about optimization for the first time,
  and it happens to also use the Nelder-Mead algorithm.</p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Linear_programming"><strong>Linear Programming</strong></a></p>

<p>  Linear programming is a special class of optimization problems that are generally
  simpler, but have algorithms guaranteed to give answers in reasonable amounts of time.</p>

<p>  This is a great way to learn about other optimization approaches.</p></li>
<li><p><a href="https://github.com/apple/swift/blob/main/docs/DifferentiableProgramming.md"><strong>Differentiable Programming Manifesto</strong></a></p>

<p>  As you get into more complex problems, getting an optimizer to work efficiently
  and reliably is harder.
  Providing explicit and accurate gradients is one way to help the optimizer out a lot.</p>

<p>  Automatic differentiation is a language feature that computes derivatives of the code you write at compile time.
  It makes optimization very easy and accessible.</p></li>
<li><p><a href="http://neuralnetworksanddeeplearning.com/"><strong>Neural Networks and Deep Learning</strong></a></p>

<p>  Looking to understand deep learning better? This popular resource is an excellent
  introduction that hits on all the main ideas and basic techniques.</p></li>
</ul>



</div>
  </body>
</html>
