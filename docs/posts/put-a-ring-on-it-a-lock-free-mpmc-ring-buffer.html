<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://h4x0r.org/ring/">Original</a>
    <h1>Put a ring on it: a lock-free MPMC ring buffer</h1>
    
    <div id="readability-page-1" class="page"><div><article><section><div><p>One of the reasons few security products work well in busy Linux environments is that they amplify performance risk. You‚Äôre popular and your backend‚Äôs load is skyrocketing? Well, the typical product is just going to collect more data and do more analysis, which amplifies the degradation.</p><p>In the real world, one of the key ways everyone deals with being overloaded is by dropping less essential things.</p><p>We can do the same thing with ring buffers, which are fixed-size queues that typically drop old data once they fill up. Yet, they rarely get used outside of single-reader, single-writer scenarios, because it‚Äôs hard to build something correct that scales to 1-to-many scenarios, never mind many-to-many scenarios.</p><p>But, what if we told you, you can have a scalable ring buffer that doesn‚Äôt need any locking, and works with multiple readers and multiple writers at the same time? You might say, ‚Äúthere‚Äôs no such thing‚Äù, except that now there is.</p><p>Ring buffers are fixed-size first-in-first-out (FIFO) queues. Fixed size queues that separately track the front and back are a category of algorithm called the <strong>circular buffer</strong>.</p><p>Some people treat the term <strong>circular buffer</strong> and <strong>ring buffer</strong> the same. For me, a ring buffer is a type of circular buffer, but one that explicitly <em>drops data</em> when the queue fills up.</p><p>That‚Äôs not the only option for a circular buffer. For instance:</p><ol><li><p>You can just block until space becomes available.</p></li><li><p>You can grow the backing store.</p></li><li><p>You can just do nothing, except signal an insertion error and let the programmer figure it out.</p></li></ol><p>Ring buffers are more resiliant, since they proactively drop when needed. Typically, it‚Äôs the oldest data gets dropped. However, there are ring buffers out there that give the option to drop new data instead.</p><p>Sure, for some situations, dropping data is the wrong call. But for many situations, especially ones requiring performance, lossy is still much better than bringing the system to a halt due to too much data.</p><p>For instance, in the Linux kernel, ring buffers are used in many places. A well known example is for relaying events from the kernel to the userland handler, when <code>ebpf</code> probes are attached.</p><p>When workloads are having performance issues, probes often end up with <em>more</em> work to do, and if there‚Äôs not some form of backpressure, things will end badly. And since <code>ebpf</code> probes are intended for observability, and since observability is generally less important than availability, dropping data as a first line of defense is a good idea.</p><p>And because the kernel‚Äôs ring buffer allows you to choose whether to drop off the front or the back, <code>ebpf</code> users get that choose too. Still, dropping older data is generally more common.</p><p>When operating on any kind of queue, lock-contention can slow things down significantly, because there are two bottlenecks‚Äì the <em>head</em> pointer (for enqueuers) or the <em>tail</em> pointer (for dequeuers). But rings have life even worse, because the head pointer can circle around the ring, and meet up with the tail pointer.</p><p>Our requirements for a ring buffer:</p><ol><li>An ordered set <code>S</code>, with a maximum size <code>n</code>, with items of a fixed length <code>l</code>.</li><li>An operation, <code>enqueue(item)</code>, where <code>item</code> is an arbitrary item of length <code>l</code>.</li><li>An operation, <code>dequeue()</code>, which returns the next item of length <code>l,</code> or indicates that the buffer is empty.</li><li>No thread should be able to detect an inconsistent ordering of operations on the ring, under any circumstances (We‚Äôll cover memory ordering after we build our ring buffer).</li></ol><p>Trying to minimize the impact of bottlenecks is hard, and so people tend to make compromises somewhere. For instance, ring buffers will try to avoid locks, but will accept some constraints, for instance:</p><ul><li>single producer, single consumer (SPSC)</li><li>single producer, multiple consumer (SPMC)</li><li>multiple-producer, single consumer (MPSC)</li></ul><p>Here, ‚Äúproducer‚Äù means ‚Äúenqueuer‚Äù, and ‚Äúconsumer‚Äù means ‚Äúdequeuer‚Äù. At some point, I wanted a true, lock-free multiple-producer, multiple-consumer (MPMC) ring buffer, but there was nothing out there that would scale, so I came up with an algorithm.</p><table><tbody><tr><td>‚õìÔ∏è‚Äçüí•</td><td><p>When we say <em>lock free</em>, we mean that, for any given thread performing an operation, no other thread can cause a thread to suspend. With respect to an algorithm only, this is often referred to as a system-wide progress guarantee.</p><p>That doesn&#39;t mean any given thread will make &#39;progress&#39; in a comfortable amount of time; lock-free algorithms usually perform operations that fail, and need to keep trying them until successful. They could conceptually lose their race till the end of eternity.</p><p>To get per-thread progress, we need <em>wait freedom</em>, which is often achievable with exponential backoff. But, when OS scheduling tends to be fair, lock free algorithms essentially can expect not to contend forever, and the extra overhead of wait freedom is often not worth it.</p></td></tr></tbody></table><p>At some point after I‚Äôd come up with my algorithm, I did stumble across a vein of literature, calling it a ‚Äúring buffer‚Äù where old data couldn‚Äôt be dropped. The user was left to resubit. To my mind, that‚Äôs a fixed-size circular FIFO, but <em>not a ring buffer</em>.</p><p>Today, we‚Äôll build a true MPMC ring buffer. We‚Äôll focus on dropping old data, but this is one place where extending it yourself would be really pretty simple.</p><p>The full code is <a href="https://codeberg.org/h4x0r/mpmc_lf_ring">available at codeberg</a>.</p><p>The last of our above requirements for a ring is often referred to as <em>linearization</em>. All operations map to a point on a conceptual timeline, where no thread can ‚Äòsee‚Äô operations in an order that would be different from that timeline.</p><p>For instance, if thread <code>A</code> enqueued <code>I1</code> then <code>I2</code>, and thread <code>B</code> is the one to dequeue both, it must always dequeue in the expected order‚Äì <code>I1</code> first, then <code>I2</code>.</p><p>Every insertion thus needs to have a well-defined ordering, as does every removal. But it doesn‚Äôt have to map to wall time.</p><p>For instance, let‚Äôs say threads <code>B</code> and <code>C</code> are both dequeuers. <code>B</code> shows up first and starts dequeueing <code>I1</code>, but the scheduler suspends it in the middle of the operation. Meanwhile, <code>C</code> comes in and quickly pulls out <code>I2</code> before <code>B</code> wakes, and returns its value.</p><p>While <code>B</code> might return after <code>C</code> in wall-clock time, that shouldn‚Äôt worry us, as long as there‚Äôs a well-defined ordering. That well defined ordering requires a well-defined <em>linearization point</em>. By that, we mean an atomic operation that is considered the point where the overall operation ‚Äòoccurs‚Äô or ‚Äòis committed‚Äô.</p><p>So if <code>B</code> hasn‚Äôt returned, but is suspended after the linearization point, it‚Äôs no big deal. The algorithm already considers the item dequeued.</p><p>Similarly, if <code>B</code> starts its operation before <code>C</code> does, but is suspended BEFORE the linearization point, and C is never suspended, <code>C</code> can absolutely claim <code>I1</code>; when <code>B</code> wakes up, it cannot get <code>I1</code>, and nobody has an inconsistent view of the world.</p><p>That works because all threads get equal treatment‚Äì it doesn‚Äôt matter when the functions they call start or end; the ordering is based on when the operation at the linearization point occurs.</p><p>We‚Äôll make sure to clearly identify our linearization points for each operation, which will always be on atomic operations.</p><p>We‚Äôre going to start with modest expectations and build a ring that operates on values that are exactly one word long. We‚Äôre going to go ahead and assume you‚Äôre on modern hardware, with a 64-bit word.</p><p>When designing our algorithm, in order to ensure correctness, we will want to think through all the places where there‚Äôs contention, meaning, we need to cover all cases where threads might try to operate on the same memory at the same time.</p><p>The following scenarios are all very realistic things for us to consider:</p><ul><li>Multiple enqueuers can be performing enqueue operations in parallel, and thus compete with each other to get onto the list ‚Äòfirst‚Äô.</li><li>If one enqueuer gets suspended, other enqueuers may wrap around the queue before its operation completes.</li><li>Multiple dequeuers can also run in parallel, and fight over which item they get.</li><li>Dequeuers can drain a list to the point that they‚Äôve caught up to writers, and might be reading their state before an operation completes.</li><li>Or, dequeuers could lag way behind, trying to dequeue from a slot in the ring that a writer is now trying to use for a much more recent value.</li></ul><p>To be able to arbitrate disputes around this kind of contention, we‚Äôre going to be explicit about our linearization timeline. Items in the array will be associated with an <em>epoch</em>, which is simply a point of time on our timeline. But every enqueue operation will be tied to an epoch.</p><p>We will make sure that each enqueue operation is associated with a unique epoch (though it is perfectly fine if we find ourselves needing to skip epochs).</p><p>When our dequeuers go to dequeue, they will also be looking to own the dequeue of an item that‚Äôs associated with a particular <em>epoch</em>.</p><p>When a thread examines a slot in the ring, it will need to know what epoch is associated with a cell, and whether there‚Äôs an enqueued item in that cell or not. We‚Äôll want to make sure all that information is definitely tied to a specific value, and that the whole kit-and-kaboodle needs to always be read from (and written to) atomically.</p><p>When a thread wants to update the same state from a cell in the ring, we need to atomically read the existing state, create a copy that has the state we want, and then try to replace the state inside the cell atomically.</p><p>However, if, when we go to update the state, it‚Äôs changed from what we expected (based on the copy we made), then we need the operation to FAIL, and we should start the update process over again, based on those changes.</p><h2 id="lets-swap">Let‚Äôs Swap</h2><p>Thankfully, there‚Äôs a universally available atomic operation that does all of those things, often referred to as a <em>compare-and-swap</em> operation (CAS). The <code>C</code> language standard provides an API call to do this, although they use the word <em>exchange</em> instead of swap (also a common name for this operation).</p><p>Any CAS operation we perform on a cell will be a linearization point for us.</p><p>Most hardware platforms can do a compare-and-swap atomically, but not for any arbitrary size. Modern hardware usually limits us to 128 bits that we can atomically operate on. Other atomic operations may limit us to just 64-bit operands.</p><table><tbody><tr><td>üíª</td><td>The x86 family has long supported a 128-bit compare-and-swap, but until recently, it required instruction-level locking to use, because it did not support atomically loading 128 bits into a register otherwise. So on old hardware, you&#39;re technically using a lock with a 128-bit CAS, but ü§∑‚Äç‚ôÇÔ∏è.</td></tr></tbody></table><p>The CAS operation conceptually takes three operands (it can differ, as we‚Äôll see later):</p><ol><li>A pointer to the bytes we want to swap (i.e., the <em>object</em> to swap)</li><li>A pointer to the value we‚Äôre expecting to be in that memory before the operation begins (i.e., the <em>expected</em> value)</li><li>The actual value we want to leave behind (i.e., the <em>desired</em> value).</li></ol><p>The processor will check that the value in the 2nd field is right; if it is, the memory address pointed to in parameter 1 gets the value you passed into parameter 3, and the OLD value gets written into the memory address pointed to by parameter 2, overwriting the <em>expected</em> field.</p><p>In this case, the operation returns <code>true</code>.</p><p>If the operation fails because the memory has changed since your last load:</p><ol><li>Your desired value does <em>not</em> get installed.</li><li>The memory holding the expected value is updated to contain what the actual value was that differed from the expected value.</li><li>The function returns <code>false</code>.</li></ol><p>On some platforms, there can be two variations of this operation, the <em>strong</em> CAS and the <em>weak</em> CAS. The <em>weak</em> CAS actually is allowed to return <code>false</code> and skip the swap, even if the expected value does match the object to swap. Generally, that operation will often get it right, but you might occasionally end up re-trying where you shouldn‚Äôt have needed to retry.</p><p>Why the heck would anyone want that behavior? It turns out, some platforms can make this weaker version faster. Even with the potential for failures, if you are using a CAS in a loop, until it succeeds, this weaker version is what people will recommend.</p><p>However, if you have a use for a CAS operation that <em>doesn‚Äôt</em> require testing for success after, using the weaker version would require testing. If you have to add a loop where there wouldn‚Äôt have been one otherwise, then you definitely want to use the strong variant.</p><p>But, while that‚Äôs the guidance you‚Äôll find all over the internet, I don‚Äôt actually know which CPUs this would affect. Maybe it‚Äôs old news, I dunno. But it does still seem to make a shade of difference in real-world tests, so ü§∑.</p><h2 id="picking-your-venue-">Picking your venue ‚õ™Ô∏è</h2><p>How do threads decide where to operate inside the ring buffer, with a minimum of fighting?</p><p>Imagine our ring is a large butcher shop with multiple counters. We take a number, and then go to the counter associated with that number.</p><p>Our ring will give out tickets to epochs, giving out each number no more than two times‚Äì once to an enqueuer, and once to a dequeuer. To do that, we‚Äôll need two counters that we can atomically update.</p><p>If we need to implement giving out tickets like this, we can use an atomic CAS operation to do it, sitting in a loop until our new value is swapped in. Each time we lose, we‚Äôll have to recompute the next value, but that‚Äôll certainly work.</p><p>However, we can avoid the loop altogether if we do it through an atomic <em>fetch-and-add</em> (FAA) operation. When implemented in hardware, FAA is guaranteed to be completed. You pass in two operands:</p><ol><li>A pointer to the object containing the current value. The result will be stored here at the end of the operation.</li><li>The value you‚Äôd like to add.</li></ol><p>At the end of the operation, the old value gets returned to the caller.</p><p>So, to implement a ticketing system, we can have a variable that holds the value of the next ticket to give out. Each thread gets a ticket by calling FAA, adding the number <code>1</code> to the ticket value, but returning the number that was there at the start, which becomes our ticket.</p><p>No two enqueuers will get the same ticket. We can then map each ticket to a cell in the ring (easiest done by limiting the number of cells in the ring to powers of two). We take the ticket, divide by the number of cells, and the remainder is the cell index associated with that ticket (i.e., we compute the modulus).</p><p><a name="powers-of-two"></a>When we have a number of cells that‚Äôs a power of two, we can use a cheap binary AND operation to get the modulus.</p><p>That trick only works for powers of two, because of some specific properties:</p><ol><li><p>Every power of two is represented with a single bit set to 1; all other bits are zero.</p></li><li><p>If you subtract <code>1</code> from a power of two, all bits to the RIGHT of that one bit set for the power of two will be set, and nothing else.</p></li></ol><p>So we get a cheap modulo via a binary AND operation. To be fair, it could be the case that modern processors can compute the modulus just as quickly as a bitwise AND, even though it‚Äôs a much more complicated operation. But, that‚Äôs why many data structures are backed by arrays of size to powers of two. It‚Äôs such a common paradigm, we‚Äôll just roll with it.</p><p>Now, we also need a second ticket for dequeuers; those tickets should be associated with values that have already been enqueued.</p><p>However, if we keep those two tickets separate, then it will be really hard for us to build any confidence that we‚Äôre detecting some of the contention scenarios we talked about above.</p><p>For example, if we read each counter separately, how do we know if the queue is empty? Or, how can we be certain that readers are way behind (we don‚Äôt want to waste a lot of time with readers grabbing tickets for values we already dropped).</p><p>The answer for us is to operate on those tickets (epochs) in one atomic operation.</p><p>We can still do that with an FAA. For example, if we define our epoch-of-record datatype like this:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>struct</span> {
</span></span><span><span>    <span>uint32_t</span> epoch_q; <span>// The next epoch assoc. w/ an enqueue op.
</span></span></span><span><span><span></span>    <span>uint32_t</span> epoch_d; <span>// The next epoch assoc. w/ an enqueue op.
</span></span></span><span><span><span></span>} <span>h4x0r_word_ring_epochs_t</span>;
</span></span></code></pre></div><p>The compiler is smart enough that it will let you perform operations on structs as if they were integers, as long as the sizes are acceptable, and as long as you are explicit enough with your use of types to convince the compiler you know what you‚Äôre doing.</p><p>One way to do this is to use a <code>union</code>, which we might declare like this:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>union</span> {
</span></span><span><span>    <span>h4x0r_word_ring_epochs_t</span> epochs;
</span></span><span><span>    <span>uint64_t</span>                 integer;
</span></span><span><span>} <span>h4x0r_converter_t</span>;
</span></span></code></pre></div><p>The fields in unions may be individually addressed, but they <em>share the same memory</em>. The bits won‚Äôt change, but we can get the compiler to recognize the change-of-type based on the field we end up accessing.</p><p>Let‚Äôs say we want to add 1 to the queue epoch. We don‚Äôt need to understand endianness or how the two fields in that struct are ordered. Here‚Äôs one way we could do that:</p><div><pre tabindex="0"><code data-lang="c"><span><span>    <span>h4c0r_word_ring_epochs_t</span> to_add_struct <span>=</span> {
</span></span><span><span>                                               .epoch_q <span>=</span> <span>1</span>,
</span></span><span><span>                                               .epoch_d <span>=</span> <span>0</span>,
</span></span><span><span>                                             };
</span></span><span><span>    <span>union</span> <span>h4x0r_converter_t</span> conv           <span>=</span> {
</span></span><span><span>                                               .epochs <span>=</span> my_epochs,
</span></span><span><span>                                             };
</span></span><span><span>    <span>uint64_t</span>                to_add_num     <span>=</span> conv.integer;
</span></span></code></pre></div><p>We can then convert it back to a struct. And really, those conversions we expect to be free; it‚Äôs just a mechanism for expressing our intent to the compiler.</p><p>While 128-bit CAS operations are commonly supported in hardware, FAA (and similar operations) are more likely to have a 64-bit cap on their operands.</p><p>That‚Äôs why the two epochs in our data structure are kept to 32 bits.</p><p>However, 32 bits isn‚Äôt all that large a number, and it wouldn‚Äôt take too long for a busy system to overflow a counter. Dealing with that kind of overflow wouldn‚Äôt be fun.</p><p>If you are confident you‚Äôve got a situation where you won‚Äôt use a ring enough to overflow, then by all means, use 32-bit epochs. But we recommend that, by default, you use 64-bit epochs. You can emulate a 128-bit FAA pretty easily with a CAS operation, as we described above.</p><p>While a 64-bit FAA should be faster, the CAS probably will be fine (on my machine, it‚Äôs a tiny smidge better, but not enough to crow about).</p><p>Our full implementation allows you to toggle between the two epoch sizes at compile-time, so you can compare the results if you like.</p><p>Anyway, atomically updating the epoch state gets you a ticket, and shows you what the next ticket is for the opposite operation, at the time we were handing your ticket.</p><h2 id="a-simple-ring-">A Simple ring üç©</h2><p>As we said above, the core state of the cells inside a ring must be no more than 128 bits if we want to operate on it without requiring a lock. So we need to be careful about what we try to jam in a cell.</p><p>When we load a cell, we want to know if we‚Äôre too slow, whatever our operation. At a bare minimum, we‚Äôre going to need:</p><ol><li>Whether an item is enqueued in the slot or has been dequeued.</li><li>Room for that value, which probably needs to be a whole word so we can fit a pointer in (usually 64 bits).</li><li>The epoch associated with the cell, which is how we‚Äôll know if a writer has been lapped (if the epoch is higher than the epoch for our operation, we got lapped). It‚Äôs also how readers will figure out the writer is slow and hasn‚Äôt finished.</li></ol><p>As we said above, 32 bits is not enough space for the epoch if we are building something general-purpose. But, we clearly don‚Äôt have room for 64-bit epochs, so we‚Äôll need to compromise.</p><p>A boolean generally will take up at least 8 bits, and a 56-bit epoch probably is good enough not to worry about. Though C doesn‚Äôt have 56-bit ints.</p><p>However, we can instead use C bit slicing, which would even let us get the flag down to a single bit, leaving 63 bits for our epoch:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>struct</span> {
</span></span><span><span>    <span>_Atomic</span>(<span>void</span> <span>*</span>)item;
</span></span><span><span>    <span>uint64_t</span> enqueued : <span>1</span>;
</span></span><span><span>    <span>uint64_t</span> epoch    : <span>63</span>;
</span></span><span><span>} <span>h4x0r_word_ring_item_t</span>;
</span></span><span><span>
</span></span><span><span><span>static_assert</span>(<span>sizeof</span>(<span>h4x0r_word_ring_item_t</span>) <span>==</span> <span>16</span>,
</span></span><span><span>              <span>&#34;Bitfield implementation doesn&#39;t pack as expected&#34;</span>);
</span></span><span><span>
</span></span><span><span><span>typedef</span> <span>_Atomic</span>(<span>h4x0r_word_ring_item_t</span>)   <span>h4x0r_word_ring_cell_t</span>;
</span></span><span><span><span>typedef</span> <span>_Atomic</span>(<span>h4x0r_word_ring_epochs_t</span>) <span>h4x0r_atomic_epochs_t</span>;
</span></span></code></pre></div><p>The C standard doesn‚Äôt really mandate layout for bit slices, so we check at compile type with the <code>static_assert</code>.</p><p>For values we need to be shared between threads, we need to label them as <code>_Atomic</code>. Otherwise, the code that the compiler generates will assume none of our variables are shared across threads, and we are bound to have all sorts of nasty race conditions.</p><p>But, we don‚Äôt tag <code>_Atomic</code> on <code>h4x0r_word_ring_item_t</code> or <code>h4x0r_word_ring_epochs_t</code> directly, because threads will be keeping their own private versions for updating.</p><p>My version of the top-level ring looks like this:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>void</span> (<span>*</span>h4x0r_word_ring_drop_handler)(<span>h4x0r_word_ring_t</span> <span>*</span>, <span>void</span> <span>*</span>);
</span></span><span><span>
</span></span><span><span><span>struct</span> <span>h4x0r_word_ring_t</span> {
</span></span><span><span>    h4x0r_word_ring_drop_handler drop_handler;
</span></span><span><span>    <span>uint32_t</span>                     num_cells;
</span></span><span><span>    <span>uint32_t</span>                     last_slot;
</span></span><span><span>    <span>h4x0r_atomic_epochs_t</span>        epochs;
</span></span><span><span>    <span>h4x0r_word_ring_cell_t</span>       cells[];
</span></span><span><span>};
</span></span></code></pre></div><p>The drop handler is a function pointer, and we‚Äôd expect it to be set when initializing the ring, defaulting to <code>NULL</code> when we don‚Äôt need to do anything in particular to deal with drops (either because we‚Äôre not using dynamic memory, or because we have something like a garbage collector to clean up if needed).</p><p>The <code>epochs</code> field is tagged as being atomic, though it‚Äôs hidden behind the typedef.</p><p>Similarly, the variable-length array of cells is really an array of items we want to be atomic. The fact that cell loading and storing requires using the C atomic API is in there, but hidden behind a <code>typedef</code>.</p><p>However, the <code>num_cells</code> field and <code>last_slot</code> field are <strong>not</strong> tagged <code>_Atomic</code>. That‚Äôs because these should be set by one thread during initialization, and then never changed. As long as the memory has synced before other threads start to use these fields, we definitely don‚Äôt need them to be treated specially.</p><p>Usually, if we do initialization in a proper function, the call boundary is going to be a <em>memory barrier</em> that makes sure they‚Äôre sync‚Äôd when other threads start getting a handle on our ring.</p><p>But, if initialization might be inlined, you should probably flag these things as <code>_Atomic</code>, but when you access them via the C11 atomic API, ask for <code>memory_order_relaxed</code>, which essentially means, ‚Äúno atomic op necessary here, so don‚Äôt add instructions to sync here‚Äù.</p><p>In that case, the <code>_Atomic</code> modifier will make sure our writes to those fields are seen by subsequent loads, but we don‚Äôt have to slow down those loads once the values are written.</p><p>The <code>num_cells</code> field is always going to be the number of cells in our ring, and 2^32 cells should be plenty, thus 32 bits. But because of C alignment rules, our struct is going to end up with a 32-bit hole somewhere. So, we fill that space with <code>last_slot</code>, which is always going to be one less than the value of <code>num_cells</code>, allowing us to perform our fast modulo without first having to subtract one from <code>num_cells</code>.</p><h2 id="the-dequeue-operation">The dequeue operation</h2><p>So far, our queue has nothing in it, and we don‚Äôt know how to put anything in it yet.</p><p>Still, let‚Äôs start with our dequeue operation.</p><p>Our first order of business is to the epochs, and if the tail and head are in the same place (or if the tail somehow lapped the head), then the queue is empty, and we shouldn‚Äôt even bother taking a ticket, so to speak.</p><p>But, if we do see items to dequeue, we‚Äôll play the game! We‚Äôll use FAA to get a unique read epoch. However, it‚Äôs 100% possible that other readers beat us, with no writers coming along.</p><p>That means, we could actually increment the counter <em>past</em> where the next writer is going to write. We will need to make sure that when we deal with writers, we try to solve that problem.</p><p>The epoch we read in won‚Äôt ever get another reader, but the writer will need to make sure it doesn‚Äôt use the slot we just accidentally burned.</p><p>Now, we‚Äôll use our epoch to find the associated cell. We‚Äôll load it, and look at the contents to figure out what to do.</p><p>Once we have loaded the cell, if the epoch is the one we‚Äôre expecting, and there‚Äôs an item in there, then we try to dequeue it by doing a CAS to mark it as dequeued (and generally attempting to set the item to <code>NULL</code>).</p><p>If the dequeue succeeds in the right epoch, we‚Äôre done.</p><p>If we found that another reader wasn‚Äôt keeping up, and our writer hasn‚Äôt written over it yet, we‚Äôd still try the CAS, and if it succeeds, that epoch is invalidated; the writer will eventually figure out that they cannot complete their write, and needs to try again.</p><p>But, if we can prove via the epochs that we‚Äôre the only writer at the time of our CAS, then, even though we know someone is trying to queue, we take that successful CAS as our linearization point, and declare that the ring was empty.</p><p>If we lose the race against a slow writer, no big deal, we start over with the same epoch; the writer probably left us a present.</p><p>If our CAS operation fails, before we try again, we look at the value of the <code>expected</code> item, which is the current stored value.</p><p>If we find the epoch is lower than ours, then we try to invalidate it with a CAS. If we fail, it could be that the slow writer finished, or it could be that we were suspended and are now behind. If it‚Äôs the former, we try again with the same epoch. If it‚Äôs the latter, we‚Äôll know because the epoch is higher than ours, and we need to go get another read epoch (we can‚Äôt return empty unless we know that because of some CAS that we did, there‚Äôs a moment on our timeline where the ring was empty).</p><p>That‚Äôs already several corner cases we need to worry about. But the most difficult concern here is the last one.</p><p>Consider when we use the CAS to dequeue, and that CAS operation failed. But the expected value was the epoch we were looking for. We can just go home and call it a day, right?</p><p>It depends:</p><ul><li><p>If we are running with a drop-handler, we absolutely cannot, because the writer will have known it was overwriting, and will be calling the drop handler. <em>We don‚Äôt want to risk a double free if there‚Äôs dynamic deallocation.</em></p></li><li><p>Otherwise, yes; the writer we were competing with absolutely doesn‚Äôt care.</p></li></ul><p>For the API call to dequeue, some callers are going to need to distinguish between returning a <code>NULL</code> because the ring is empty, and the case where <code>NULL</code> was enqueued.</p><p>The way to deal with this is to have our API call accept a pointer to a boolean. If <code>NULL</code> is passed, then we ignore the parameter. Otherwise, we‚Äôll store a value in the memory pointed to.</p><p>The logic around whether to store the pointer is easily lifted out into inline functions:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>inline</span> <span>void</span> <span>*</span>
</span></span><span><span><span>h4x0r_word_ring_empty</span>(<span>bool</span> <span>*</span>found)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (found) {
</span></span><span><span>        <span>*</span>found <span>=</span> false;
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> NULL;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span> <span>*</span>
</span></span><span><span><span>h4x0r_word_ring_found</span>(<span>h4x0r_word_ring_item_t</span> item,
</span></span><span><span>                      <span>uint64_t</span>              <span>*</span>epoch_ptr,
</span></span><span><span>                      <span>bool</span>                  <span>*</span>found)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (found) {
</span></span><span><span>        <span>*</span>found <span>=</span> true;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>h4x0r_atomic_fence</span>();
</span></span><span><span>
</span></span><span><span>    <span>if</span> (epoch_ptr) {
</span></span><span><span>        <span>*</span>epoch_ptr <span>=</span> item.epoch;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> item.item;
</span></span><span><span>}
</span></span></code></pre></div><p>Notice that <code>h4x0r_word_ring_found</code> takes a second pointer parameter‚Äì that‚Äôs for the caller to get the associated epoch. Day-to-day that may not be too useful. However, it‚Äôs very useful for correctness testing, to make sure one thread never sees out-of-order dequeues.</p><p>We‚Äôll use it when we do our testing.</p><p>With all that, here‚Äôs the body of our dequeue operation:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>void</span> <span>*</span>
</span></span><span><span><span>h4x0r_word_ring_dequeue</span>(<span>h4x0r_word_ring_t</span> <span>*</span>ring, <span>bool</span> <span>*</span>found, <span>uint64_t</span> <span>*</span>ep)
</span></span><span><span>{
</span></span><span><span>    <span>h4x0r_word_ring_cell_t</span>  <span>*</span>cell;
</span></span><span><span>    <span>h4x0r_word_ring_item_t</span>   expected;
</span></span><span><span>    <span>h4x0r_word_ring_item_t</span>   last;
</span></span><span><span>    <span>h4x0r_word_ring_item_t</span>   candidate;
</span></span><span><span>    <span>h4x0r_word_ring_epochs_t</span> epochs <span>=</span> <span>h4x0r_atomic_load</span>(<span>&amp;</span>ring<span>-&gt;</span>epochs);
</span></span><span><span>
</span></span><span><span>    <span>while</span> (true) {
</span></span><span><span>        <span>if</span> (epochs.epoch_d <span>&gt;=</span> epochs.epoch_q) {
</span></span><span><span>            <span>return</span> <span>h4x0r_word_ring_empty</span>(found);
</span></span><span><span>        }
</span></span><span><span>        epochs   <span>=</span> <span>h4x0r_epochs_increment</span>(<span>&amp;</span>ring<span>-&gt;</span>epochs, read_incr);
</span></span><span><span>        cell     <span>=</span> <span>h4x0r_word_ring_slot_addr</span>(ring, epochs.epoch_d);
</span></span><span><span>        expected <span>=</span> <span>h4x0r_atomic_load</span>(cell);
</span></span><span><span>
</span></span><span><span>        candidate <span>=</span> (<span>h4x0r_word_ring_item_t</span>){
</span></span><span><span>            .item     <span>=</span> NULL,
</span></span><span><span>            .epoch    <span>=</span> epochs.epoch_d,
</span></span><span><span>            .enqueued <span>=</span> false,
</span></span><span><span>            .dequeued <span>=</span> true,
</span></span><span><span>        };
</span></span><span><span>
</span></span><span><span>        <span>while</span> (expected.epoch <span>&lt;=</span> epochs.epoch_d) {
</span></span><span><span>            last <span>=</span> expected;
</span></span><span><span>
</span></span><span><span>            <span>if</span> (<span>h4x0r_atomic_cas</span>(cell, <span>&amp;</span>expected, candidate)) {
</span></span><span><span>                <span>if</span> (epochs.epoch_d <span>==</span> last.epoch) {
</span></span><span><span>                    <span>return</span> <span>h4x0r_word_ring_found</span>(last, ep, found);
</span></span><span><span>                }
</span></span><span><span>                <span>if</span> (epochs.epoch_d <span>&gt;</span> last.epoch) {
</span></span><span><span>                    <span>h4x0r_word_ring_drop</span>(ring, last);
</span></span><span><span>                    <span>break</span>;
</span></span><span><span>                }
</span></span><span><span>                <span>return</span> <span>h4x0r_word_ring_found</span>(last, ep, found);
</span></span><span><span>            }
</span></span><span><span>            <span>else</span> {
</span></span><span><span>                <span>if</span> (last.epoch <span>==</span> epochs.epoch_d <span>&amp;&amp;</span> <span>!</span>ring<span>-&gt;</span>drop_handler) {
</span></span><span><span>                    <span>return</span> <span>h4x0r_word_ring_found</span>(last, ep, found);
</span></span><span><span>                }
</span></span><span><span>                epochs <span>=</span> <span>h4x0r_atomic_load</span>(<span>&amp;</span>ring<span>-&gt;</span>epochs);
</span></span><span><span>                <span>continue</span>;
</span></span><span><span>            }
</span></span><span><span>            <span>if</span> (epochs.epoch_q <span>-</span> epochs.epoch_d <span>&lt;=</span> <span>1</span>) {
</span></span><span><span>                <span>return</span> <span>h4x0r_word_ring_empty</span>(found);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        <span>// We got lapped and need a new epoch.
</span></span></span><span><span><span></span>        epochs <span>=</span> <span>h4x0r_atomic_load</span>(<span>&amp;</span>ring<span>-&gt;</span>epochs);
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>You‚Äôll notice there are a few more helper functions in there:</p><ul><li><code>h4x0r_epochs_increment()</code> uses some inline code to do the 128-bit FAA using a union for conversion, as discussed above.</li></ul><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>inline</span> <span>h4x0r_word_ring_epochs_t</span>
</span></span><span><span><span>h4x0r_epochs_increment</span>(_Atomic <span>h4x0r_word_ring_epochs_t</span> <span>*</span>p,
</span></span><span><span>                       <span>h4x0r_epoch_info_t</span>                counter)
</span></span><span><span>{
</span></span><span><span>    <span>h4x0r_epoch_info_t</span> result;
</span></span><span><span>
</span></span><span><span>    result.i <span>=</span> <span>h4x0r_atomic_add</span>((<span>_Atomic</span>(__uint128_t) <span>*</span>)p, counter.i);
</span></span><span><span>
</span></span><span><span>    <span>return</span> result.s;
</span></span><span><span>}
</span></span></code></pre></div><ul><li><code>h4x0r_word_ring_slot_addr()</code> takes our epoch, performs the modulo operation, and gets us a pointer to our cell. The code to get the address inside that inline function is more compact than the call, but we prefer the extra clarity, and the compiler is expected to inline it, especially if we put it in a header file (or we can annotate it to always inline).</li></ul><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>inline</span> <span>_Atomic</span>(<span>h4x0r_word_ring_item_t</span>) <span>*</span>
</span></span><span><span><span>h4x0r_word_ring_slot_addr</span>(<span>h4x0r_word_ring_t</span> <span>*</span>ring, <span>uint64_t</span> epoch)
</span></span><span><span>{
</span></span><span><span>    <span>return</span> <span>&amp;</span>ring<span>-&gt;</span>cells[epoch <span>&amp;</span> ring<span>-&gt;</span>last_slot];
</span></span><span><span>}
</span></span></code></pre></div><ul><li>There are several <code>h4x0r_atomic_*()</code> calls. Those wrap the C11 API calls so we can apply consistent memory ordering that‚Äôs different from C‚Äôs default. We‚Äôll discuss this a bit at the end of the article for those who want to understand. We do use C‚Äôs new-ish <code>_Generic</code> feature to abstract away over <code>fetch-and-add</code>, selecting our function for 128 bit values:</li></ul><div><pre tabindex="0"><code data-lang="c"><span><span><span>#define h4x0r_atomic_add(x, y)                    \
</span></span></span><span><span><span>    _Generic((y),                                 \
</span></span></span><span><span><span>        __uint128_t: _h4x0r_atomic_faa_128(x, y), \
</span></span></span><span><span><span>        default: atomic_fetch_add_explicit(x, y, H4X0R_MO_RW))
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>static</span> <span>inline</span> __uint128_t                                 
</span></span><span><span>        <span>_h4x0r_atomic_faa_128</span>(<span>void</span> <span>*</span>v, __uint128_t val) 
</span></span><span><span>    {                                                         
</span></span><span><span>        _Atomic __uint128_t <span>*</span>var <span>=</span> v;                         
</span></span><span><span>                                                             
</span></span><span><span>        __uint128_t expected;                                 
</span></span><span><span>        __uint128_t desired;                                  
</span></span><span><span>                                                              
</span></span><span><span>        expected <span>=</span> <span>h4x0r_atomic_load</span>(var);                    
</span></span><span><span>                                                              
</span></span><span><span>        <span>do</span> {                                                  
</span></span><span><span>            desired <span>=</span> expected <span>+</span> val;                 
</span></span><span><span>        } <span>while</span> (<span>!</span><span>h4x0r_atomic_cas</span>(var, <span>&amp;</span>expected, desired)); 
</span></span><span><span>        <span>return</span> expected;                                      
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span><span>h4x0r_decl_binopu128</span>(faa, <span>+</span>);
</span></span></code></pre></div><ul><li><code>h4x0r_word_ring_drop()</code> is about as simple as you‚Äôd want it to be:</li></ul><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_word_ring_drop</span>(<span>h4x0r_word_ring_t</span> <span>*</span>ring, <span>h4x0r_word_ring_item_t</span> cell)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (ring<span>-&gt;</span>drop_handler <span>&amp;&amp;</span> cell.enqueued) {
</span></span><span><span>        (<span>*</span>ring<span>-&gt;</span>drop_handler)(ring, cell.item);
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><h2 id="the-enqueue-operation">The enqueue operation</h2><p>We‚Äôre already more than halfway done with our first ring. All we really need to do is get stuff into it.</p><p>Our first order of business when a thread calls our enqueue function is to load the existing epochs, grabbing its own ticket (epoch) in the process, via our fetch-and-add.</p><p>Second, we check the epochs to see if they need repair. That could be one of two scenarios:</p><ol><li>We see that the tail is lagging (meaning, the epochs indicate the tail is farther behind than the number of slots); this is due to a relative lack of dequeuers.</li><li>We see that some dequeue operation accidentally look a read-slot when the queue was empty, during a race condition (We do not want to write into a slot that no reader could ever read).</li></ol><p>If the enqueuer sees either of these two scenarios, it will attempt to ‚Äòfix‚Äô the tail by moving the dequeue epoch to the lowest epoch that might still be enqueued. Here are my helper functions to deal with these scenarios:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#define H4X0R_BACKOFF_START_NS 1000
</span></span></span><span><span><span>#define H4X0R_BACKOFF_MAX_NS   65536
</span></span></span><span><span><span></span>
</span></span><span><span><span>static</span> <span>const</span> <span>struct</span> timespec start_sleep <span>=</span> {
</span></span><span><span>    .tv_sec  <span>=</span> <span>0</span>,
</span></span><span><span>    .tv_nsec <span>=</span> H4X0R_BACKOFF_START_NS,
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_word_ring_needs_repair</span>(<span>h4x0r_word_ring_epochs_t</span> epochs,
</span></span><span><span>                             <span>uint32_t</span>                 ring_size)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (epochs.epoch_d <span>+</span> ring_size <span>&lt;</span> epochs.epoch_q) {
</span></span><span><span>        <span>return</span> true;
</span></span><span><span>    }
</span></span><span><span>    <span>if</span> (epochs.epoch_d <span>&gt;</span> epochs.epoch_q) {
</span></span><span><span>        <span>return</span> true;
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> false;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>void</span>
</span></span><span><span><span>h4x0r_ring_lag_sleep</span>(<span>struct</span> timespec <span>*</span>sleep_time)
</span></span><span><span>{
</span></span><span><span>    <span>// We don&#39;t really care if we sleep the whole time or not.
</span></span></span><span><span><span></span>    <span>nanosleep</span>(sleep_time, NULL);
</span></span><span><span>    sleep_time<span>-&gt;</span>tv_nsec <span>&lt;&lt;=</span> <span>1</span>;
</span></span><span><span>
</span></span><span><span>    <span>if</span> (sleep_time<span>-&gt;</span>tv_nsec <span>&gt;</span> H4X0R_BACKOFF_MAX_NS) {
</span></span><span><span>        sleep_time<span>-&gt;</span>tv_nsec <span>=</span> H4X0R_BACKOFF_MAX_NS;
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// Returns true if we ever go through the loop, indicating
</span></span></span><span><span><span>// we may need to  update our own epoch.
</span></span></span><span><span><span></span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_word_ring_repair</span>(<span>h4x0r_word_ring_epochs_t</span> epochs,
</span></span><span><span>                       <span>h4x0r_word_ring_t</span>       <span>*</span>ring)
</span></span><span><span>{
</span></span><span><span>    <span>struct</span> timespec          sleep_time <span>=</span> start_sleep;
</span></span><span><span>    <span>bool</span>                     repair     <span>=</span> false;    
</span></span><span><span>    <span>h4x0r_word_ring_epochs_t</span> candidate;
</span></span><span><span>
</span></span><span><span>    <span>while</span> (<span>h4x0r_word_ring_needs_repair</span>(epochs, ring<span>-&gt;</span>num_cells)) {
</span></span><span><span>        repair <span>=</span> true;
</span></span><span><span>        
</span></span><span><span>        <span>if</span> (epochs.epoch_d <span>&gt;</span> epochs.epoch_q) {
</span></span><span><span>            candidate <span>=</span> (<span>h4x0r_word_ring_epochs_t</span>){
</span></span><span><span>                .epoch_q <span>=</span> epochs.epoch_q,
</span></span><span><span>                .epoch_d <span>=</span> epochs.epoch_q,
</span></span><span><span>            };
</span></span><span><span>        }
</span></span><span><span>        <span>else</span> {
</span></span><span><span>            candidate <span>=</span> (<span>h4x0r_word_ring_epochs_t</span>){
</span></span><span><span>                .epoch_q <span>=</span> epochs.epoch_q,
</span></span><span><span>                .epoch_d <span>=</span> epochs.epoch_q <span>-</span> ring<span>-&gt;</span>num_cells,
</span></span><span><span>            };
</span></span><span><span>        }
</span></span><span><span>        <span>if</span> (<span>!</span><span>h4x0r_atomic_cas</span>(<span>&amp;</span>ring<span>-&gt;</span>epochs, <span>&amp;</span>epochs, candidate)) {
</span></span><span><span>            <span>return</span> true;
</span></span><span><span>        }
</span></span><span><span>        <span>h4x0r_ring_lag_sleep</span>(<span>&amp;</span>sleep_time);
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> repair;
</span></span><span><span>}
</span></span></code></pre></div><p>If the enqueuers don‚Äôt do the tail-correction, it penalizes dequeuers who are already behind; they‚Äôll pay the price of going back for tickets, only to find they‚Äôre out of date, which can exacerbate problems when they‚Äôre behind.</p><p>If we do see lag, after attempting to fix it, we should help even more by taking a really short snooze to go ahead and help any pending reader succeed. If we don‚Äôt, then we‚Äôre at more risk of dequeuers continually being forced to retry because writers are starving them. Here is one place in our algorithm where, if we want to go for full wait-freedom, we can turn this process into an exponential backoff loop.</p><p>Once we leave the readers acceptably far behind, we then go to the cell we‚Äôre supposed to be writing to. There, we‚Äôre going to want to load the state to see what‚Äôs what.</p><ul><li>If we see our epoch is already in there, we were too slow, and some reader invalidated the slot; we need to grab a new epoch and try everything again.</li><li>We do exactly the same thing if we see a HIGHER epoch than ours (writers lapped us, probably because we got suspended).</li></ul><p>Next, we add the value, move the state to <code>enqueued</code>, and attempt to install the item via a CAS.</p><p>We keep trying until that succeeds.</p><p>Once our CAS is successful, then the write is successful. However, we still are not always done.</p><p>Specifically, we may need to look at what we overwrote (which should be installed in the <code>expected</code> field).</p><p>If we overwrote a queued item, then we need to pass that item to the drop handler. This is a particularly important thing to do when the item we found is a pointer to heap memory.</p><p>If we simply overwrite without checking, we might be leaking the memory the pointer references.</p><p>Of course, if we have no drop handler, we don‚Äôt need the extra step; there‚Äôs no problem.</p><p>We can just return, successful in our mission.</p><p>Here‚Äôs my implementation:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>uint64_t</span>
</span></span><span><span><span>h4x0r_word_ring_enqueue</span>(<span>h4x0r_word_ring_t</span> <span>*</span>ring, <span>void</span> <span>*</span>item)
</span></span><span><span>{
</span></span><span><span>    <span>h4x0r_word_ring_cell_t</span>  <span>*</span>cell;
</span></span><span><span>    <span>h4x0r_word_ring_epochs_t</span> epochs;
</span></span><span><span>    <span>uint64_t</span>                 write_epoch;
</span></span><span><span>    <span>h4x0r_word_ring_item_t</span>   expected;
</span></span><span><span>
</span></span><span><span>    <span>while</span> (true) {
</span></span><span><span>        epochs      <span>=</span> <span>h4x0r_epochs_increment</span>(<span>&amp;</span>ring<span>-&gt;</span>epochs, write_incr);
</span></span><span><span>        write_epoch <span>=</span> epochs.epoch_q;
</span></span><span><span>        
</span></span><span><span>        <span>if</span> (<span>h4x0r_word_ring_repair</span>(epochs, ring)) {
</span></span><span><span>            <span>if</span> (write_epoch <span>+</span> ring<span>-&gt;</span>num_cells <span>&lt;</span> epochs.epoch_q) {
</span></span><span><span>                <span>continue</span>;
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        cell     <span>=</span> <span>h4x0r_word_ring_slot_addr</span>(ring, write_epoch);
</span></span><span><span>        expected <span>=</span> <span>h4x0r_atomic_load</span>(cell);
</span></span><span><span>
</span></span><span><span>        <span>// This has to be a CAS; we might have another writer who
</span></span></span><span><span><span></span>        <span>// laps us between the last epoch check and the coming op.
</span></span></span><span><span><span></span>        <span>h4x0r_word_ring_item_t</span> new <span>=</span> {
</span></span><span><span>            .item     <span>=</span> item,
</span></span><span><span>            .epoch    <span>=</span> write_epoch,
</span></span><span><span>            .enqueued <span>=</span> true,
</span></span><span><span>        };
</span></span><span><span>
</span></span><span><span>        <span>while</span> (expected.epoch <span>&lt;</span> write_epoch) {
</span></span><span><span>            <span>if</span> (<span>h4x0r_atomic_cas</span>(cell, <span>&amp;</span>expected, new)) {
</span></span><span><span>                <span>// If we overwrote something, it&#39;ll need to be dropped.
</span></span></span><span><span><span></span>                <span>h4x0r_word_ring_drop</span>(ring, expected);
</span></span><span><span>                <span>return</span> write_epoch;
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        <span>continue</span>; <span>// too slow; get a new epoch and retry.
</span></span></span><span><span><span></span>    }
</span></span><span><span>}
</span></span></code></pre></div><h2 id="weve-inscribed-some-words-on-your-ring">We‚Äôve Inscribed some words on your ring</h2><p>We have our first lock-free ring. But so far, it‚Äôs like we got the ring out of a cracker-jack box. It‚Äôs not yet something nice enough to use, since we‚Äôre limited to putting in 64-bit values.</p><p>However, we will 100% solve that problem.</p><p>One thing about a ring that‚Äôs often valued is that you can operate in a fixed amount of statically allocated memory. Only giving 64 bits of space for the ring will push us towards dynamic allocation, which is a shame.</p><p>But we can use our word ring to make a big ring with larger, fixed-sized memory cells.</p><p>The basic idea is that we have two circular buffers, one of them being the word ring (we‚Äôll call it <code>R</code>). Then, we‚Äôll create a second circular buffer to store our arbitrary-sized records. We‚Äôll call this one <code>S</code> (for store).</p><p>The entries we enqueue into our word ring <code>R</code> will simply be an epoch that points to a spot in <code>S</code>.</p><p>Write threads will peek at the ring‚Äôs global epoch info, for two reasons:</p><ol><li>As a hint for where to start in the larger array, and</li><li>So that we know which records conceptually aren‚Äôt in the ring anymore and can be overwritten.</li></ol><p>Here are the data structures I used in my implementation, along with the state flags I use throughout.</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>struct</span> <span>h4x0r_ring_t</span> <span>h4x0r_ring_t</span>;
</span></span><span><span><span>typedef</span> <span>struct</span> <span>h4x0r_ring_entry_info_t</span> <span>h4x0r_ring_entry_info_t</span>;
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>// The full cell definition for the outer ring cells.
</span></span></span><span><span><span></span>
</span></span><span><span><span>struct</span> <span>h4x0r_ring_entry_t</span> {
</span></span><span><span>    _Atomic <span>h4x0r_ring_entry_info_t</span> info;
</span></span><span><span>    <span>uint64_t</span>                        len;
</span></span><span><span>    <span>char</span>                            data[];
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>// This the first item in the outer ring cell, 
</span></span></span><span><span><span></span><span>struct</span> <span>h4x0r_ring_entry_info_t</span> {
</span></span><span><span>    <span>uint64_t</span> write_epoch;
</span></span><span><span>    <span>uint64_t</span> state;
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>struct</span> <span>h4x0r_ring_t</span> {
</span></span><span><span>    <span>h4x0r_word_ring_t</span>  <span>*</span>word_ring;
</span></span><span><span>    <span>h4x0r_ring_entry_t</span> <span>*</span>entries;
</span></span><span><span>    _Atomic <span>uint64_t</span>    entry_ix;
</span></span><span><span>    <span>uint64_t</span>            last_entry;
</span></span><span><span>    <span>uint64_t</span>            entry_len;
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>enum</span> <span>:</span> <span>uint64_t</span> {
</span></span><span><span>    H4X0R_RING_EMPTY             <span>=</span> <span>0x00</span>,
</span></span><span><span>    H4X0R_RING_RESERVED          <span>=</span> <span>0x01</span>,
</span></span><span><span>    H4X0R_RING_DEQUEUE_RESERVE   <span>=</span> <span>0x02</span>,
</span></span><span><span>    H4X0R_RING_ENQUEUE_DONE      <span>=</span> <span>0x04</span>,
</span></span><span><span>    H4X0R_RING_USED              <span>=</span> <span>0x07</span>,
</span></span><span><span>    H4X0R_RING_DEQUEUE_DONE_MASK <span>=</span> <span>~</span><span>0x06</span>,
</span></span><span><span>};
</span></span></code></pre></div><p>I‚Äôm going to skip initialization here, but two important notes if you‚Äôre going to DIY:</p><ol><li>Double-check that the ‚Äúcore‚Äù word ring‚Äôs size in bits is <a href="#powers-of-two">a power of two</a>.</li><li>Ensure that cell sizes are properly aligned (probably to a 16-byte boundary to be safe).</li></ol><h2 id="the-enqueue-operation-1">The enqueue operation</h2><p>The write thread starts by grabbing the underlying word ring‚Äôs epoch information. It takes the write epoch it finds, and maps that into <code>S</code> (<code>e % len(S)</code>, if <code>e</code> is the epoch)`.</p><p>Starting at that position, the writer scans <code>S</code> to find the first valid spot it can claim.</p><p>That means, if it notices an operation in progress, it skips that cell and keeps probing until it can claim a cell that‚Äôs safe to write. Once the write completes, we enqueue the position into our word ring. Adding it to the word ring gives us the epoch; we add that into our slot inside <code>S</code>, before we remove the flag that indicates we‚Äôre writing to the cell.</p><p>As a result, entries in <code>S</code> can be out of order, and that‚Äôs totally fine. The correct order will be kept in the word ring.</p><p>More specifically, the steps for enqueuers (writers) are as follows:</p><ol><li>Find a spot in <code>S</code>, and reserve it, so no other writer will touch it.</li><li>Copy data into the reserved cell.</li><li>Enqueue a pointer to <code>S</code> into <code>R</code> (our linearization point).</li><li>Write into <code>S</code> the epoch that <code>R</code> returned to us when we enqueued.</li><li>Update the slot in <code>S</code> with the epoch, and indicate we‚Äôre done with their enqueue.</li></ol><p>To make this all work, we really should have <code>S</code> contain more entries than <code>R</code>. We want to have enough to ensure the right number of items can all be enqueued at once in a full queue, and that any write thread will still have a space to write. If we don‚Äôt do that, writers will be roaming around a full parking garage until a spot opens up and they‚Äôre lucky enough to nab it.</p><p>If we have a ceiling on the number of threads we allow, we can use that value. But, if not, just doubling the number of entries should be more than good enough.
There will be no competition for the enqueuer‚Äôs slot from other enqueuers.</p><p>However, a dequeuer can come in after step 3 completes and before step 4 completes, while we‚Äôre suspended.</p><p>That‚Äôs not a problem for us‚Äì the linearization point is the enqueue into <code>R</code>. We just need to make sure that enqueuers can only claim a slot if BOTH enqueuers and dequeuers are done with their operation (and, if it‚Äôs not in <code>R</code>, of course).</p><p>Note that dequeuers will set a state bit when they start dequeuing, so we can easily avoid taking those slots. However, when figuring out whether we can overwrite a state no thread is in, we need to check the stored epoch, to make sure it‚Äôs far enough behind ours that it‚Äôs not conceptually in the queue anymore.</p><p>Here‚Äôs the core of the enqueue operation:</p><div><pre tabindex="0"><code data-lang="c"><span><span> <span>void</span>
</span></span><span><span><span>h4x0r_ring_enqueue</span>(<span>h4x0r_ring_t</span> <span>*</span>self, <span>void</span> <span>*</span>item, <span>uint64_t</span> len)
</span></span><span><span>{
</span></span><span><span>    <span>uint64_t</span>                 ix;
</span></span><span><span>    <span>uint64_t</span>                 byte_ix;
</span></span><span><span>    <span>uint64_t</span>                 start_epoch;
</span></span><span><span>    <span>h4x0r_ring_entry_info_t</span>  expected;
</span></span><span><span>    <span>h4x0r_ring_entry_info_t</span>  candidate;
</span></span><span><span>    <span>h4x0r_ring_entry_t</span>      <span>*</span>cur;
</span></span><span><span>    <span>h4x0r_word_ring_epochs_t</span> epochs;
</span></span><span><span>
</span></span><span><span>    <span>if</span> (len <span>&gt;</span> self<span>-&gt;</span>entry_len) {
</span></span><span><span>        len <span>=</span> self<span>-&gt;</span>entry_len;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    epochs      <span>=</span> <span>h4x0r_atomic_load</span>(<span>&amp;</span>self<span>-&gt;</span>word_ring<span>-&gt;</span>epochs);
</span></span><span><span>    start_epoch <span>=</span> epochs.epoch_q;
</span></span><span><span>    ix          <span>=</span> start_epoch <span>&amp;</span> self<span>-&gt;</span>last_entry;
</span></span><span><span>
</span></span><span><span>    <span>while</span> (true) {
</span></span><span><span>        byte_ix <span>=</span> ix <span>*</span> (<span>sizeof</span>(<span>h4x0r_ring_entry_t</span>) <span>+</span> self<span>-&gt;</span>entry_len);
</span></span><span><span>        cur     <span>=</span> (<span>h4x0r_ring_entry_t</span> <span>*</span>)<span>&amp;</span>(((<span>char</span> <span>*</span>)self<span>-&gt;</span>entries)[byte_ix]);
</span></span><span><span>
</span></span><span><span>        expected              <span>=</span> <span>h4x0r_atomic_load</span>(<span>&amp;</span>cur<span>-&gt;</span>info);
</span></span><span><span>        candidate.write_epoch <span>=</span> <span>0</span>;
</span></span><span><span>        candidate.state       <span>=</span> H4X0R_RING_RESERVED;
</span></span><span><span>
</span></span><span><span>        <span>if</span> (<span>h4x0r_atomic_cas</span>(<span>&amp;</span>cur<span>-&gt;</span>info, <span>&amp;</span>expected, candidate)) {
</span></span><span><span>            <span>break</span>;
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        <span>if</span> (<span>!</span><span>h4x0r_ring_can_write_here</span>(expected,
</span></span><span><span>                                       start_epoch,
</span></span><span><span>                                       self<span>-&gt;</span>last_entry)) {
</span></span><span><span>            ix <span>=</span> (ix <span>+</span> <span>1</span>) <span>&amp;</span> self<span>-&gt;</span>last_entry;
</span></span><span><span>            <span>continue</span>;
</span></span><span><span>        }
</span></span><span><span>
</span></span><span><span>        <span>if</span> (<span>h4x0r_atomic_cas</span>(<span>&amp;</span>cur<span>-&gt;</span>info, <span>&amp;</span>expected, candidate)) {
</span></span><span><span>            <span>break</span>;
</span></span><span><span>        }
</span></span><span><span>        ix <span>=</span> (ix <span>+</span> <span>1</span>) <span>&amp;</span> self<span>-&gt;</span>last_entry;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>memcpy</span>(cur<span>-&gt;</span>data, item, len);
</span></span><span><span>
</span></span><span><span>    candidate.write_epoch <span>=</span> <span>h4x0r_word_ring_enqueue</span>(self<span>-&gt;</span>word_ring,
</span></span><span><span>                                                    (<span>void</span> <span>*</span>)ix);
</span></span><span><span>    candidate.state       <span>=</span> H4X0R_RING_ENQUEUE_DONE;
</span></span><span><span>    cur<span>-&gt;</span>len              <span>=</span> len;
</span></span><span><span>
</span></span><span><span>    <span>h4x0r_atomic_store</span>(<span>&amp;</span>cur<span>-&gt;</span>info, candidate);
</span></span><span><span>}
</span></span></code></pre></div><p>The only new helper function here is <code>h4x0r_ring_can_write_here()</code> and its helper:</p><div><pre tabindex="0"><code data-lang="c"><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_ring_entry_is_being_used</span>(<span>h4x0r_ring_entry_info_t</span> info)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (info.state <span>&amp;</span> H4X0R_RING_USED) {
</span></span><span><span>        <span>return</span> true;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> false;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_ring_can_write_here</span>(<span>h4x0r_ring_entry_info_t</span> info,
</span></span><span><span>                          <span>uint64_t</span>                my_write_epoch,
</span></span><span><span>                          <span>uint32_t</span>                last_entry)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (<span>h4x0r_ring_entry_is_being_used</span>(info)) {
</span></span><span><span>        <span>return</span> false;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>if</span> (info.write_epoch <span>&gt;</span> my_write_epoch) {
</span></span><span><span>        <span>return</span> false;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>if</span> (info.write_epoch <span>&gt;</span> (my_write_epoch <span>-</span> (last_entry <span>+</span> <span>1</span>))) {
</span></span><span><span>        <span>return</span> false;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> true;
</span></span><span><span>}
</span></span></code></pre></div><h2 id="the-dequeue-operation-1">The dequeue operation</h2><p>Dequeuers (readers) take the following steps:</p><ol><li>Dequeue a value from <code>R</code>, which gives us the index into <code>S</code> we need; at the same time, we use our reference parameter to capture the epoch the writer used to make sure we don‚Äôt read from the future.</li><li>Attempt to mark the cell in <code>S</code> for reading and validating. If validation fails, we restart.</li><li>Actually perform the read.</li><li>They mark the cell state in <code>S</code> to indicate the read is done.</li></ol><p>Note that a slow dequeuer might find that by the time they attempt to flag the cell in <code>L</code> for read, someone has already claimed that cell for writing a newer log message. In such cases, the slow dequeuer just needs to try again.</p><p>Or, the writer may not have stored its epoch yet. We know if they got a ticket, we can go find the right slot. If the epoch is anything less than the epoch we dequeued, it‚Äôs definitely our value to dequeue.</p><p>For the dequeue, we‚Äôll use these two very simple helpers:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>static</span> <span>inline</span> <span>bool</span>
</span></span><span><span><span>h4x0r_ring_can_dequeue_here</span>(<span>h4x0r_ring_entry_info_t</span> info,
</span></span><span><span>                            <span>uint64_t</span>                expected_epoch)
</span></span><span><span>{
</span></span><span><span>    <span>if</span> (info.write_epoch <span>&gt;</span> expected_epoch) {
</span></span><span><span>        <span>return</span> false;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>return</span> true;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>static</span> <span>inline</span> <span>uint64_t</span>
</span></span><span><span><span>h4x0r_ring_set_dequeue_done</span>(<span>uint64_t</span> state)
</span></span><span><span>{
</span></span><span><span>    <span>return</span> state <span>&amp;</span> H4X0R_RING_DEQUEUE_DONE_MASK;
</span></span><span><span>}
</span></span></code></pre></div><p>Our dequeue function is going to return whether there was a dequeue or not, instead of returning a value, and use a parameter for people to check if the queue was empty.</p><p>That‚Äôs because we‚Äôre going to need the caller to pass in a buffer for the result. We DEFINITELY don‚Äôt want to pass back a pointer to the ring entry; that‚Äôs a recipe for disaster.</p><p>Especially since we‚Äôre dealing with concurrency, we want to make sure to test thoroughly. We should run in a number of different configurations, and should thoroughly test to make sure that we only ever see linearized results when we dequeue.</p><p>We‚Äôre also going to want to count some stuff, then check it all for consistency:</p><ol><li>We should count successful dequeues.</li><li>We should <em>independently</em> count the number of drops, too.</li><li>We should capture the number of times fast readers find an empty queue.</li></ol><p>Each thread should collect metrics privately, and then add them to totals, when it‚Äôs done with the work.</p><p>If we add independently collected dequeues to drops, we should get the total number of enqueues; otherwise we have a bug.</p><p>You‚Äôll need to know when to stop trying to dequeue. The simplest path is to have the main thread first <code>join()</code> on all enqueuers; at that point, we know there‚Äôs nothing else to enqueue. So when a dequeue thread sees all writers are done, the first time they encounter an empty queue, they know they‚Äôre done. That way is easy to implement, but leaves a small window where the empty dequeue time will push up. You can instead have individual writer threads decrement a global counter, which will shorten that window.</p><p>Also, even if it‚Äôs not real world, we should test for worst case, and run enqueues and dequeues as back to back as possible, to help us understand worst case performance, or any other considerations we might need.</p><p>I‚Äôll spare you the code, because you can go get it in the <a href="https://codeberg.org/h4x0r/mpmc_lf_ring">codeberg repo</a>. But, it does iterate over both types of ring, using five different sizes of buffer, and with a variety of (mostly imbalanced) readers and writers.</p><p>Let‚Äôs look at some example output though (I‚Äôve deleted a few columns to keep us under 80 characters).</p><p>First, for the main, arbitrary ring, let‚Äôs look at our most minimal ring size:</p><pre tabindex="0"><code>Tests for queue with 16 items:
Test   Th+    Th-   Time(sec)     Q-             Qüíß    Mop/s (‚úÖ)
------------------------------------------------------------------
# 1    1      1      0.0246     251,355       10,781      10.23   
# 2    2      2      0.0505     235,606        3,705       5.11   
# 3    4      4      0.1124      68,417      187,282       0.67   
# 4    8      8      0.2179     199,100       34,775       1.04   
# 5    2      1      0.0364     110,678      132,274       3.57   
# 6    4      1      0.0624      40,678      210,810       0.82   
# 7    8      1      0.0855       7,927      251,875       0.12   
# 8    1      2      0.0271     176,210       85,934       6.51   
# 9    1      4      0.0472     183,065       79,079       3.88   
#10    1      8      0.0930     183,732       78,412       1.97   
</code></pre><p>Here, <code>Th+</code> is the number of enqueue threads. The <code>-</code> sign denotes the dequeue size.</p><p><code>Qüíß</code> is the number of drops. Then, the last column denotes how many millions of ops per second we performed in that test case.</p><p>The three columns we omitted that you‚Äôll see in the output:</p><ul><li><code>Q+</code> is the total number of enqueues, which is always <code>262,144</code> in my runs.</li><li><code>Q‚àÖ</code>, the number of times a dequeuer attempted to dequeue, and found the queue was empty.</li><li><code>Mop/s (‚úÖ+‚àÖ)</code> which recomputes Mop/sec, including dequeues that find the queue empty.</li></ul><p>One thing that should jump out to you is that there are an absurd number of drops in there. And when we get those absurd drops, our overall performance tends to plummet. The table makes it clear that we need to do more to deal with the contention.</p><p>That‚Äôs the kind of concern you should be looking for when testing parallel algorithms.</p><p>The insight makes sense; if the queue is full, the writers help with the tail, but then go back to competing where the table comes together.</p><p>The conclusion I drew is that, before the queue fills, enqueuers should briefly pause to give preference to readers, so as not to contend with them. In the <a href="https://codeberg.org/h4x0r/mpmc_lf_ring">code repo</a> for this article, I did just that for you, setting the threshold to 75%, which gives much better results:</p><pre tabindex="0"><code>Tests for queue with 16 items:
Test   Th+    Th-   Time(sec)     Q-         Qüíß    Mop/s (‚úÖ)
--------------------------------------------------------------
# 1    1      1      0.0248     262,142        2      10.55
# 2    2      2      0.0450     260,071       43       5.82
# 3    4      4      0.0923     236,809      434       2.83
# 4    8      8      0.2132     208,501      177       1.23
# 5    2      1      0.0400     256,786       32       6.55
# 6    4      1      0.0479     253,364       20       5.47
# 7    8      1      0.0989     219,220       53       2.65
# 8    1      2      0.0384     262,101       43       6.83
# 9    1      4      0.0489     262,084       60       5.36
#10    1      8      0.0868     261,814      330       3.02
</code></pre><p>If we run more tests, we may see some significant drops, but we‚Äôd expect big numbers only when the number of writers greatly outweighs the number of readers. And in that case, the drops are expected. On my machine, this only ever happens for 16-item queues though. Even at 128 items, it looks good (on my machine):</p><pre tabindex="0"><code>Tests for queue with 128 items:
Test   Th+    Th-   Time(sec)      Q-           Qüíß     Mop/s (‚úÖ)  
------------------------------------------------------------------
#11    1      1      0.0147     262,145          0      17.82
#12    2      2      0.0428     262,145          0       6.13
#13    4      4      0.0765     262,144          1       3.42
#14    8      8      0.1879     262,140          5       1.39
#15    2      1      0.0263     262,145          0       9.97
#16    4      1      0.0517     262,145          0       5.07
#17    8      1      0.0936     262,145          0       2.80
#18    1      2      0.0251     262,144          1      10.43
#19    1      4      0.0417     262,142          3       6.28
#20    1      8      0.0859     262,145          0       3.05
</code></pre><p>If we study these charts, we can compare to see exactly how big an impact that contention actually has. In tests where we were dropping, the number of operations plummeted massively due to the contention.</p><p>With the above tweak to the enqueuer‚Äôs help rules, my laptop tests never fail to top 1 million operations a second, and raw word-queue performance peaks at about 24 Mop/sec, and stays above 3 MOp/sec for all but a couple of configurations (the ones where I‚Äôve overcommitted my cores w 8q+/8q-, so past the point where I‚Äôve maxed out potential parallelism).</p><p>Not bad, considering we haven‚Äôt really done anything to optimize (I do have a more optimized implementation of the base word-ring algorithm that can get as high at 40Mop/sec with one enqueuer and one dequeuer, and bottoms out at 4Mops/sec when the number of dequeuers is lopsided, all on the same machine).</p><p>My test bed is <a href="https://codeberg.org/h4x0r/mpmc_lf_ring">there in the repo</a> for you to use if you like.</p><p>When I wrote about futexes and mutexes, I glossed over the topic of memory ordering, because it‚Äôs notoriously hard to communicate well. But given we‚Äôre into the world of concurrency without locks, I think it‚Äôs remiss not to cover it. I‚Äôll try to make it as straightforward as possible.</p><p>If you are still confused after reading this section (which is likely), <a href="https://codeberg.org/h4x0r/mpmc_lf_ring/issues/new">give me feedback</a> on what questions it leaves you with, and I‚Äôll try again.</p><h2 id="your-compiler-may-be-gaslighting-you">Your compiler may be gaslighting you</h2><p>Your compiler wants you to think your code will execute in the order you‚Äôd expect while staring at the source code.</p><p>Meanwhile, behind your back, it‚Äôs generally going way out of its way to thwart your expectations. Though, to be fair, it‚Äôs doing it for you. It knows how disappointed you‚Äôd be if it performs poorly for you.</p><p>So yes, the compiler will absolutely rewrite your code (particularly when you turn on any level of optimization). It has no qualms changing the order you‚Äôd expect, with hopes of it running faster for you. But, it‚Äôs hoping you won‚Äôt notice. The transformations often aren‚Äôt semantically identical to what you might have intended, but, at least in the context of a single thread, you‚Äôre not likely to notice the difference.</p><p>Even for multi-threaded programs, compilers often try hard to optimize what you‚Äôre doing, transforming and reordering to take advantage of the CPU.</p><p>Still, there are things the compiler won‚Äôt do (just like Meatloaf). Specifically, compilers have this idea of ‚Äúbarriers‚Äù, which are features in your code that the compiler won‚Äôt move stuff past.</p><p>For instance, compilers will not move code across the boundary of a mutex, or any op on a variable labeled <code>_Atomic</code> (unless you explicitly allow it at the call site).</p><p>The compiler is conservative here, because you expressed your intent. Event if the compiler thinks you‚Äôre walking away from a big performance gain, and even if they could ‚Äúprove‚Äù that it‚Äôs not going to change the semantics of your program. Generally, function boundaries result in a barrier as well, as long as they‚Äôre not inlined.</p><p>But, even with multi-processor programs, the compiler does all that analysis as if a single thread is running. So it can move data around across threads in many frustrating ways. If you don‚Äôt pay attention to how you handle concurrency, compiler transformations can definitely make race conditions far worse.</p><h2 id="the-processor-is-a-bad-influence">The processor is a bad influence</h2><p>Making matters worse, your compiler‚Äôs friend, the processor, tries to apply parallelism everywhere it can, because of performance. So there are also plenty of architectural considerations that can lead to data races.</p><p>For instance, you probably know that the CPU is fast, and memory accesses are slow. And when many threads are competing to load memory at the same time, things can get chaotic, because memory cells loaded into registers on one core don‚Äôt magically sync instantly across multiple cores.</p><p>And, the processor may do its own reordering of instructions, for instance, to achieve fine-grained parallelism via things like instruction pipelining, which can definitely run your code out of order.</p><p>Processors not only have a very complex memory model, but that model can be vastly different across architectures (e.g., x86 and ARM).</p><p>Very few programmers are going to be aware of most of that subtlety.</p><p>Languages could generate code to make sure everything always happens in a well-defined order (full sequential consistency), but generally they do not. Processors go out of their way to make things faster by moving your code around, and compilers often do a lot to make your code faster too.</p><p>So no language is going to find it an acceptable hit to force the processor to run everything in a well-defined order, at least in the case of multiple threads.</p><h2 id="how-to-make-this-relationship-work">How to make this relationship work</h2><p>Programmers typically will need to tell their compiler where to get conservative, and sacrifice performance for correctness. In C, you can do that by labeling variables as <code>_Atomic</code>, which tells the compiler that variables will be used across threads.</p><p>But <code>_Atomic</code> doesn‚Äôt truly mean, ‚Äúalways atomic‚Äù, primarily because you can explicitly ask for different behavior on a case-by-case basis.</p><p>If you don‚Äôt specify anything other than <code>_Atomic</code>, it does mean that you‚Äôre not going to get corrupting data races, and it does mean that, by default, the compiler will ensure all operations on that variable will happen in the same order, from the perspective of any thread.</p><p>Enforcing that kind of order generally slows things down, so, you can specify to the compiler cases where you want different behavior. For instance, if you‚Äôre initializing memory, you probably have exclusive access to the data. Your own view on the ordering is consistent anyway, so at this point you may not care to slow down the processor.</p><p>However, that <em>could</em> be problematic for the next thread to read. Since the first access didn‚Äôt care, it‚Äôs definitely possible for a thread to get a reference to the object, and see the pre-initialization value, but only if that second access explicitly relaxes its requirement for getting access to the variable.</p><p>Generally, those kinds of surprises are easy to find, especially when they involve fields in an object whose pointer you read atomically, but whose fields are not marked as being <code>_Atomic</code>. It‚Äôs a great recipe for Heisenbugs.</p><p>So generally, if you know multiple threads will handle a variable, not only should you go ahead and declare it as <code>_Atomic</code>, but also you should avoid giving up most of its protection‚Äì you‚Äôre just inviting disaster.</p><h3 id="my-memory-order-arrived-damaged">My memory order arrived damaged</h3><p>By default, accessing a single <code>_Atomic</code> variable will make sure that any changes to the variable happen in a well-defined (linearized) order. For example, let‚Äôs say we have a huge number of threads, and thread <code>0</code> goes to modify <code>_Atomic</code> variable <code>A</code>.</p><p>Just by declaring the variable to be <code>_Atomic</code>, the compiler will, if necessary, generate code that makes sure that any changes to <code>A</code> that were in flight when thread <code>0</code> goes to modify it, all end before its operation. Similarly, any subsequent loads of that variable will see the reflected value.</p><p><code>_Atomic</code> variable access (unless relaxed) acts like a barrier that the compiler will enforce for the variable in question. That enforcement, though, is really done by generating an assembly that helps get the proper semantics, which is very platform dependent.</p><p>If you do <em>not</em> mark a variable as <code>_Atomic</code>, then you <em>should not be using the variable across threads</em>. The compiler will certainly generate code under the assumption that those variables won‚Äôt have to deal with concurrent access.</p><p>But we do have some options:</p><ul><li><p><strong>Relaxed Memory Order.</strong> In C/C++ parlance, the semantics of variables that are not marked <code>_Atomic</code> is called <em>relaxed memory order</em>. It‚Äôs a weird name that means there are no ordering guarantees outside of what would be promised by the underlying architecture for a non-atomic variable, and the promise of non-corrupting data races. That‚Äôs scary.</p></li><li><p><strong>Sequentially Consistent Ordering.</strong> This lives on the other end of the spectrum from relaxed ordering. Using this is supposed to ensure a global ordering for all atomic data by default, at the price of some efficiency. This is the default memory ordering, and is the strongest, but it does have some slight issues we‚Äôll discuss in a minute.</p></li><li><p><strong>Acquire / Release Ordering.</strong> This is in between relaxed and sequentially consistent. It basically does what you want it to do on a variable-by-variable basis, forcing a well defined ordering.</p></li></ul><p>You‚Äôll often see ‚ÄúAcquire‚Äù and ‚ÄúRelease‚Äù listed as separate strategies. They‚Äôre more like two sides of the same coin:</p><ul><li><p><em>acquire</em> semantics apply only to <em>loading</em> an <code>_Atomic</code> variable (i.e., acquiring it). The guarantee is that any modifications to a memory address that were made by other threads will be reflected by the time the value loads, with nothing still in progress.</p></li><li><p><em>release</em> semantics only apply to <em>storing</em> an <code>_Atomic</code> value (i.e., releasing it). The guarantee here is that the store operation will be reflected in the next subsequent load of that address. That is to say, once the store finishes, no other thread will be able to load the previous value.</p></li></ul><p>For some operations, only one of these two things makes sense. For instance, acquire semantics make sense for an <code>atomic_load()</code> call, but that doesn‚Äôt update the value, so <em>release</em> semantics don‚Äôt make sense here (and thus, acquire/release doesn‚Äôt make sense either).</p><p>You don‚Äôt specify memory ordering when you declare something <code>_Atomic</code>. By default, every operation for such variables will get the strongest memory ordering.</p><p>If you want anything else, you can get it on a call-by-call basis every time the variable is used, by calling a call in the C atomic library that allows you to explicitly change to another ordering, but only at that one slot.</p><p>Generally, the defaults are going to be least surprising (in a world where surprises are common, and understanding what‚Äôs going on is hard).</p><p>That doesn‚Äôt mean that most strict memory ordering is perfect: sequential consistency has some issues that prevent it from living up to its name, particularly when you end up mixing access with different memory orderings (see section 2 of <a href="https://plv.mpi-sws.org/scfix/paper.pdf">this paper</a> for more details).</p><p>Plus, sequential consistency generally isn‚Äôt much stronger than <em>acquire / release</em> semantics, and it can be slower, depending on the architecture. So it‚Äôs pretty reasonable to use acquire/release semantics as the default.</p><p>But this stuff is trickier than we might think.</p><p>For instance, you may have noticed that I declared the <code>item</code> field inside the struct <code>h4x0r_word_ring_item_t</code> to be <code>_Atomic</code>.</p><p>If you remove that qualifier, on some platforms our tests will occasionally fail. Why? How is that possible??</p><p>Sure, we dequeue an entire <code>h4x0r_word_ring_item_t</code> atomically. But, we store the result of that dequeue into a second memory location, that isn‚Äôt itself marked to be atomic. In our case, in our word ring dequeue algorithm, that‚Äôs going to be the variable called <code>last</code>.</p><p>So, when we return <code>last.item</code>, we might end up returning the value that was stored in that field before the CAS operation.</p><p>Since we also return the associated epoch, we could possibly get an old value there, too. However, since they are only unloaded into the <code>last</code> field together, (atomically), we can be pretty confident that, if the item is available, then the epoch will be too.</p><p>Still, C doesn‚Äôt guarantee that; it‚Äôs just a matter of having some knowledge of what‚Äôs going on under the hood (and experience to back it up).</p><p>But, if you wanted to be really careful, you would want to tag the epoch field as <code>_Atomic</code> too. If you try though, you‚Äôll get an error, because <code>_Atomic</code> doesn‚Äôt work with bit slices directly. You‚Äôd have to do something else. Some options are:</p><ol><li>Use a union, with one of the types in the union being an <code>_Atomic uint64_t</code></li><li>Declare the thing as a <code>uint64_t</code>, and manage the flag bit manually (but you do need the flag to be a bit).</li><li>Leave it unsynced, and tell the compiler to sync all relevant variables before pulling the epoch out of the item.</li></ol><p>This third option you can do with a memory fence, putting it before the assignment in <code>h4x0r_word_ring_found</code>. In our associated code, <code>h4x0r_atomic_fence()</code> will do the trick; this is a very trivial wrapper around a C atomic builtin.</p><p>By the way, if we tag <code>item</code> as being <code>_Atomic</code>, but specify relaxed memory ordering when we copy it out, we could absolutely end up with the same problem, because <code>_Atomic</code> is only atomic until you explicitly tell it otherwise.</p><p>Yes, there are a <em>lot</em> of subtleties. Nobody said it would be easy.</p><p><em>Good luck, you‚Äôre going to need it.</em></p><p>‚Äì Lee T. Solo (tho with a ring on it)</p></div></section></article></div></div>
  </body>
</html>
