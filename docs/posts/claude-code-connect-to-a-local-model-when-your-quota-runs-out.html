<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://boxc.net/blog/2026/claude-code-connecting-to-local-models-when-your-quota-runs-out/">Original</a>
    <h1>Claude Code: connect to a local model when your quota runs out</h1>
    
    <div id="readability-page-1" class="page"><article id="post-227">
	
		<p><img width="825" height="510" src="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_limit_new_option_header2-825x510.png" alt="" decoding="async" fetchpriority="high"/>	</p><!-- .post-thumbnail -->

	
	<!-- .entry-header -->

	<div>
		
<p>If you’re on one of the cheaper Anthropic plans like me, it’s a pretty common scenario when you’re deep into Claude coding an idea, to hit a daily or weekly quota limit. If you want to keep going, you can connect to a local open source model instead of Anthropic. To monitor your current quota, type: <code>/usage</code></p>



<figure><a href="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_usage.png"><img decoding="async" width="902" height="632" src="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_usage.png" alt="" srcset="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_usage.png 902w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_usage-300x210.png 300w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_usage-768x538.png 768w" sizes="(max-width: 902px) 100vw, 902px"/></a><figcaption>Type <code>/usage</code> to monitor how much quota you have left and how quick you burn it.</figcaption></figure>



<p>The best open source model is changing pretty frequently, but at the time of writing this post, I recommend <a href="https://docs.z.ai/guides/llm/glm-4.7#glm-4-7-flash" target="_blank" rel="noopener"><strong>GLM-4.7-Flash</strong> from Z.AI</a> or <strong><a href="https://qwen.ai/blog?id=qwen3-coder-next" target="_blank" rel="noopener">Qwen3-Coder-Next</a></strong>. If you want or need to save some disk space and GPU memory, try a smaller quantized version which will load and run quicker with a quality cost. I’ll save another detailed post for how to find the best open source model for your task and machine constraints.</p>



<h3>Method 1: LM Studio</h3>



<figure><img decoding="async" width="1024" height="619" src="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_lmstudio_model_search-1024x619.png" alt="" srcset="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_lmstudio_model_search-1024x619.png 1024w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_lmstudio_model_search-300x181.png 300w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_lmstudio_model_search-768x465.png 768w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_lmstudio_model_search-1536x929.png 1536w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_lmstudio_model_search.png 1954w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption>Accessing open source models in LM Studio</figcaption></figure>



<p>If you haven’t used LM Studio before, it’s an accessible way to find and run open source LLMs and vision models locally on your machine. In version 0.4.1, they introduce support to connect to Claude Code (CC). See here: <a href="https://lmstudio.ai/blog/claudecode" target="_blank" rel="noopener">https://lmstudio.ai/blog/claudecode</a> or follow the instructions below:</p>



<ol>
<li><a href="https://lmstudio.ai/download" target="_blank" rel="noopener">Install and run LM Studio</a></li>



<li>Find the model search button to install a model (see image above). LM Studio recommends running the model with a context of &gt; 25K.</li>



<li>Open a new terminal session to:</li>



<li>Reduce your expectations about speed and performance! </li>



<li>To confirm which model you are using or when you want to switch back, type <code>/model</code></li>
</ol>



<figure><a href="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_running_oss_model.png"><img loading="lazy" decoding="async" width="1024" height="318" src="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_running_oss_model-1024x318.png" alt="" srcset="https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_running_oss_model-1024x318.png 1024w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_running_oss_model-300x93.png 300w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_running_oss_model-768x239.png 768w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_running_oss_model-1536x478.png 1536w, https://boxc.net/blog/wp-content/uploads/2026/02/claude_code_running_oss_model.png 2032w" sizes="auto, (max-width: 1024px) 100vw, 1024px"/></a><figcaption>Enter /model to confirm which model you are using or to switch back</figcaption></figure>



<h3>Method 2: Connecting directly to Llama.CPP</h3>



<p>LM Studio is built on top of the open source project <a href="https://github.com/ggml-org/llama.cpp" target="_blank" rel="noopener">llama.cpp</a>.</p>



<h3>Conclusion</h3>



<p>For the moment, this is a backup solution. Unless you have a monster of a machine, you’re going to notice the time it takes to do things and a drop in code quality but it works(!) and it’s easy enough to switch between your local OSS model and Claude when you’re quota limit is back, so it’s a good way to keep coding when you’re stuck or you just want to save some quota. If you try it let me know how you go and which model works for you.</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>
  </body>
</html>
