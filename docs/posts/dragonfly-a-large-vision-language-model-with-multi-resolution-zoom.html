<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.together.ai/blog/dragonfly-v1">Original</a>
    <h1>Dragonfly: A large vision-language model with multi-resolution zoom</h1>
    
    <div id="readability-page-1" class="page"><div><section><div><div><div><div><p><img loading="eager" src="" alt=""/></p></div></div></div></div></section><section><div><div><div><div><div><div fs-richtext-element="rich-text"><p>We are excited to announce the launch of Dragonfly, a breakthrough instruction-tuning Vision-language architecture, that enhances fine-grained visual understanding and reasoning about image regions. </p><p>We are releasing the Dragonfly architecture, which uses multi-resolution zoom-and-select to enhance multi-modal reasoning while being context-efficient. We are also launching two new open-source models  <a href="https://huggingface.co/togethercomputer/Llama-3-8B-Dragonfly-v1">Llama-3-8b-Dragonfly-v1</a> a general-domain model trained on 5.5 million image-instruction pairs and <a href="https://huggingface.co/togethercomputer/Llama-3-8B-Dragonfly-Med-v1">Llama-3-8b-Dragonfly-Med-v1</a> finetuned on additional 1.4 biomedical image-instruction data. Dragonfly demonstrates promising performance on vision-language benchmarks like commonsense visual QA and image captioning. Dragonfly-Med outperforms prior models, including Med-Gemini on multiple medical imaging tasks, showcasing its capabilities for high-resolution medical data.</p><p>Figure 1: Generated example from Llama-3-8B-Dragonfly</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661eeab92ebdcff8cda76f2_image%20(5).png" loading="lazy" alt=""/></p><figcaption><strong>Question</strong>: Summarize the visual contents of the image.</figcaption></figure><p>Figure 2: Generated examples from  Llama-3-8B-Dragonfly-Med</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661eed892ebdcff8cda8fc9_image%20(9).png" loading="lazy" alt=""/></p><figcaption><strong>Question</strong>: Write a detailed radiology note based on the chest X-ray.</figcaption></figure><h2>Dragonfly Model Architecture</h2><p><a href="https://arxiv.org/abs/2406.00977"><em>Arxiv Paper</em></a><em> with technical details and </em><a href="https://github.com/togethercomputer/Dragonfly"><em>codebase</em></a><em> for architecture implementation</em></p><p>Dragonfly is special because:</p><ol role="list"><li>Dragonfly employs two key strategies: multi-resolution visual encoding and zoom-in patch selection, which enables the model to focus more fine-grained details on image regions and provide better commonsense reasoning. Despite the model being optimized for capturing fine-grained image details, the model achieves good zero-shot performance at standard image-understanding benchmarks such as visual question-answering or natural-image-captioning.</li><li>We demonstrate our model&#39;s understanding and reasoning capabilities on biomedical tasks, which often require a fine-grained understanding of high-resolution image regions. By finetuning our general-domain model on a biomedical instruction-tuning dataset with 1.4 million biomedical image-text pairs, Dragonfly-med, our biomedical version of the model, achieves state-of-the-art or competitive performance on multiple biomedical benchmarks, including visual question answering, image captioning, and radiology report generation.</li></ol><p>Figure 3: Overview of the Dragonfly architecture</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66614db787f49c335e5c4118_image3.png" loading="lazy" alt=""/></p></figure><p><strong>Multi-resolution Visual Encoding:</strong> Dragonfly processes images at multiple resolutions—low, medium, and high. Each image, depending on its resolution, is divided into sub-images that are encoded into visual tokens. These tokens are then projected into a language space, forming a concatenated sequence that feeds into the LLM. This method allows the model to handle larger images efficiently and improves the granularity of visual data processing.</p><p><strong>Zoom-in Patch Selection:</strong> To further refine the model&#39;s focus on important visual details, Dragonfly employs a selective approach for high-resolution images. It uses a novel zoom-in patch selection strategy to identify and retain only those high-resolution sub-images that provide the most significant visual information. This is achieved by comparing summary embeddings of medium and high-resolution sub-images to select the most semantically relevant patches. This targeted selection reduces redundancy and focuses on crucial content areas, thereby enhancing the overall model efficiency and fine-grained understanding on these areas.</p><p>Together, these strategies enable Dragonfly to achieve a detailed and efficient visual understanding, making it particularly adept at processing complex image data in specific domains.</p><p>Figures 4 and 5: Generated examples from Llama-3-8B-Dragonfly</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661f9c35d300e04e8ef323f_image%20(6).png" loading="lazy" alt=""/></p><figcaption><strong>Figure 4 Question:</strong> Summarize the visual content of the image first and then answer the questions. What process does this diagram portray and which solar body is portrayed in this diagram?</figcaption></figure><h2>Dragonfly Model Evaluation</h2><p>We evaluate Dragonfly trained based on LLaMA-8B on five popular vision-language benchmarks that require strong commonsense reasoning and detailed image understanding, AI2D, ScienceQA, MMMU, MMVet, and POPE. AI2D and ScienceQA evaluate visual commonsense reasoning in the science domain. MMMU and MMVet focus on providing a comprehensive evaluation of the vision-language capabilities. POPE evaluates the object-level hallucination of the regional details. Dragonfly achieved competitive performance compared with other popular vision-language models, demonstrating the effectiveness of Dragonfly on commonsense reasoning and fine-grained understanding of image regions. The results are presented in the following table:</p><div><div>
<table>
  <tbody><tr>
    <th>Model</th>
    <th>AI2D</th>
    <th>ScienceQA</th>
    <th>MMMU</th>
    <th>MMVet</th>
    <th>POPE</th>
  </tr>
  <tr>
    <td>VILA</td>
    <td>-</td>
    <td>68.2</td>
    <td></td>
    <td>34.9</td>
    <td>85.5</td>
  </tr>
  <tr>
    <td>LLaVA-v1.5 (Vicuna-7B)</td>
    <td>54.8</td>
    <td>70.4</td>
    <td>35.3</td>
    <td>30.5</td>
    <td>85.9</td>
  </tr>
  <tr>
    <td>LLaVA-v1.6 (Mistral-7B)</td>
    <td>60.8</td>
    <td>72.8</td>
    <td>33.4</td>
    <td>44.8</td>
    <td>86.7</td>
  </tr>
  <tr>
    <td>QWEN-VL-chat</td>
    <td>52.3</td>
    <td>68.2</td>
    <td>35.9</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Dragonfly (LLaMA-8B)</td>
    <td>63.6</td>
    <td>80.5</td>
    <td>37.8</td>
    <td>35.9</td>
    <td>91.2</td>
  </tr>
</tbody></table>
</div></div><h2>Dragonfly-Med</h2><p>We partnered with the Zou group at Stanford Medicine to finetune Dragonfly on additional 1.4 million biomedical image instructions to develop the biomedical version, Dragonfly-Med. </p><p>Figures 6 and 7: Generated examples from  Llama-3-8B-Dragonfly-Med</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661f97f1204614802b7b80f_image%20(7).png" loading="lazy" alt=""/></p><figcaption><strong>Figure 4 Question: </strong>Please provide brief description of the image.</figcaption></figure><p>Dragonfly-Med is evaluated on both visual question-answering, medical image captioning, and clinical report generation evaluation. Our model outperforms Med-Gemini, which was finetuned a customized version of Gemini 1.5 with about 5x more image-instruction data, on all the three question-answering datasets. The following two tables show the results on these benchmarks. The first table shows the performance on question-answering benchmarks and the second table presents the performance of biomedical image caption and report generation. </p><div><div>
<table>
  <tbody><tr>
    <th>Dataset</th>
    <th>Metric</th>
    <th>LLaVA-Med</th>
    <th>Med-Gemini</th>
    <th>SOTA</th>
    <th>Dragonfly-Med (LLaMA-8B)</th>
  </tr>
  <tr>
    <td rowspan="2">VQA-RAD</td>
    <td>Acc (closed)</td>
    <td>84.2</td>
    <td>69.7</td>
    <td>87.1</td>
    <td>77.4</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>50.1</td>
    <td>62.1</td>
    <td>59.6</td>
  </tr>
  <tr>
    <td rowspan="2">SLAKE</td>
    <td>Acc (closed)</td>
    <td>83.2</td>
    <td>84.8</td>
    <td>91.6</td>
    <td>90.4</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>75.8</td>
    <td>89.3</td>
    <td>88.8</td>
  </tr>
  <tr>
    <td rowspan="2">Path-VQA</td>
    <td>Acc (closed)</td>
    <td>91.7</td>
    <td>83.3</td>
    <td>91.7</td>
    <td>92.3</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>58.7</td>
    <td>62.7</td>
    <td>67.6</td>
  </tr>
</tbody></table>
</div></div><p>‍</p><div><table>
  <tbody><tr>
    <th>Dataset</th>
    <th>Metric</th>
    <th colspan="2">BiomedGPT SOTA</th>
    <th>Dragonfly-Med (LLaMA-8B)</th>
  </tr>
  <tr>
    <td rowspan="3">IU X-Ray</td>
    <td rowspan="3">ROUGE-L</td>
    <td rowspan="3">28.5</td>
    <td rowspan="3">44.8</td>
    <td rowspan="3">28.5</td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">Peir Gross</td>
    <td rowspan="3">ROUGE-L</td>
    <td rowspan="3">36.0</td>
    <td rowspan="3">36.0</td>
    <td rowspan="3">40.3</td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">ROCO</td>
    <td rowspan="3">ROUGE-L</td>
    <td rowspan="3">18.2</td>
    <td rowspan="3">18.2</td>
    <td rowspan="3">19.3</td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">MIMIC CXR</td>
    <td rowspan="3">ROUGE-L</td>
    <td rowspan="3">23.8</td>
    <td rowspan="3">33.5</td>
    <td rowspan="3">24.2</td>
  </tr>
  <tr></tr>
  <tr></tr>
</tbody></table></div><p>‍</p><p>Dragonfly-Med outperforms the existing state-of-the-art models across multiple benchmarks on the Path-VQA dataset for both accuracy (closed) and token F1 metrics. It attains an accuracy of 90.4% on the SLAKE dataset, which is close to the current state-of-the-art of 91.6%. Notably, Dragonfly-Med outperforms Med-Gemini, a model finetuned from Gemini-1.5 on 7 million biomedical data samples, on all VQA tasks we evaluated. On the image captioning task, Dragonfly-Med achieves state-of-the-art or competitive results on several metrics across these datasets. Notably, on the Peir Gross and ROCO datasets, Dragonfly-Med outperforms existing methods on all three metrics: ROUGE-L, METEOR, and CIDEr. Some of the baseline models are much larger than our current implementation. The zoom-and-select architecture of Dragonfly is especially powerful for medical image understanding as medical images are often very large and the salient regions are found in small patches.</p><h2>Conclusion and Future Work</h2><p>Dragonfly architecture provides a potential research direction on zooming in image regions to focus more selected fine-grained visual information. We trained two checkpoints based on LLaMA3-8B-Instruct and achieved promising results on both general-domain and biomedical-domain tasks. We hope this work could benefit the research community to explore more open-sourced multimodal research and apply AI on real-world problems.</p><p>We will keep improving the comprehensive capabilities of open-source multimodal models. In the future, we will explore new architectures, better visual encoding strategies, more comprehensive studies on how the data mixture should be and more scientific domains to provide benefits for broader fields.</p><h2>Acknowledgements</h2><p>We especially appreciate the contributions and collaborations of our partner, Stanford Medicine. We would also like to acknowledge the following resources that were instrumental in the development of Dragonfly:</p><ul role="list"><li>Meta LLaMA3: We utilized the LLaMA3 as our language model backbone for our current two checkpoints.</li><li>CLIP: Our current visual backbone is CLIP model from OpenAI</li><li>Our codebase is built upon codebases of Otter and LLaVA-UHD.</li></ul></div><div><div><div><div><p><h2>Q: Should I use the RedPajama-V2 Dataset out of the box?</h2></p></div><div><div><p>RedPajama-V2 is conceptualized as a pool of data that serves as a foundation for creating high quality datasets. The dataset is thus not intended to be used out of the box and, depending on the application, data should be filtered out using the quality signals that accompany the data. With this dataset, we take the view that the optimal filtering of data is dependent on the intended use. Our goal is to provide all the signals and tooling that enables this.</p></div></div></div></div></div></div></div></div></div></div></section></div></div>
  </body>
</html>
