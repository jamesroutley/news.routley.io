<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://differ.blog/p/here-s-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-ace4ba">Original</a>
    <h1>Build and train GPT-2 from scratch using PyTorch</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>Are you tired of always using ChatGPT and curious about how to build your own language model? Well, you’re in the right place! Today, we’re going to create GPT-2 , a powerful language model developed by OpenAI, from scratch that can generate human-like text by predicting the next word in a sequence.</p>
<p>To dive deeper into the theory and architecture of GPT-2, I highly recommend reading <a rel="noopener noreferrer nofollow ugc" href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2</a> by Jay Alammar. This article provides an excellent visual and intuitive explanation of GPT-2 and its inner workings. I’ll be referring to some of the visuals from the article to explain things better.</p>
<blockquote>
<p>I have tried to make this as simpler as possible. Anyone with any level of Python or machine learning can follow along and build the model.</p>
</blockquote>
<h3>Resources</h3>
<p>This project will take you through all the steps for building a simple GPT-2 model and train on bunch of Taylor Swift and Ed Sheeran songs. We’ll see what it will come up at the end :).</p>
<p>The dataset and source codes for this article will be available in <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/r?url=https%3A%2F%2Fgithub.com%2Fajeetkharel%2Fgpt2-from-scratch">Github</a>.</p>
<blockquote>
<p>I’ll also add a Jupyter Notebook which replicates this article so you can follow along with running code and understanding side-by-side.</p>
</blockquote>
<h3>Building GPT-2 Architecture</h3>
<p>We will take this project step-by-step by continuously improving a bare-bone model and adding layers based on the original <a rel="noopener noreferrer nofollow ugc" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a> implementation.</p>
<p>Here are the steps we will follow:</p>
<ol>
<li><strong>Building a custom Tokenizer</strong></li>
<li><strong>Building a Data Loader</strong></li>
<li><strong>Train a simple language model</strong></li>
<li><strong>Implement GPT-2 architecture (part 2)</strong> <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/@mramitkharel/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-part-2-9b41d15baf62">🔗</a></li>
</ol>
<p>This project is divided into two parts, the first one goes through the basics of language modelling and <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/@mramitkharel/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-part-2-9b41d15baf62">Part 2</a> jumps straight into GPT-2 implementation. I suggest you to follow along with the article and build it yourself which makes learning GPT-2 more interesting and fun.</p>
<blockquote>
<p>Note: This whole project will be done in a single python file so it will be easy for you to follow along block by block.</p>
</blockquote>
<p><strong>Final Model:</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*35AaHBa5imxVIbbjByIE2Q.png" alt=""/></p>
<p><strong>Final Model output:</strong></p>
<blockquote>
<p>Your summer has a matter likely you trying
I wish you would call
Oh-oh,
I&#39;ll be a lot of everyoneI just walked
You&#39;re sorry&#34;Your standing in love out,
And something would wait forever bring &#39;Don&#39;t you think about the storyIf you&#39;re perfectly
I want your beautiful
You had sneak for you make me
This ain&#39;t think that it wanted you this enough for lonely thing
It&#39;s a duchess and I did nothin&#39; home was no head
Oh, but you left me
Was all the less pair of the applause
Honey, he owns me now
But&#39;ve looks for us?&#34;
If I see you&#39;ll be alright
You understand, a out of theWait for me I can&#39;t call
Everything
Oh, no words don&#39;t read about me
You should&#39;ve been so
You&#39;re doing what you so tired,
If you, you got perfect fall</p>
</blockquote>
<p>Like the song? Then let’s get building..</p>
<h3><strong>1. Building a custom Tokenizer</strong></h3>
<p>Language models don’t see text like us. Instead they recognize sequence of numbers as tokens of specific text. So, the first step is to import our data and build our own character level Tokenizer.</p>
<pre><code>data_dir = <span>&#34;data.txt&#34;</span>
text = <span>open</span>(data_dir, <span>&#39;r&#39;</span>).<span>read</span>() # load all the data <span>as</span> simple string

# <span>Get</span> all unique characters <span>in</span> the text <span>as</span> vocabulary
chars = <span>list</span>(<span>set</span>(text))
vocab_size = <span>len</span>(chars)
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*34WkqssQKHKpdO1yTH0n-g.png" alt=""/></p>
<p>If you see the output above, we have a list of all unique characters extracted from the text data in the initialization process. Character tokenization is basically using the index position of characters from the vocabulary and mapping it to corresponding character in the input text.</p>
<pre><code># build the character level tokenizer
chr_to_idx = {<span>c</span>:i <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}
idx_to_chr = {<span>i</span>:c <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}

def <span>encode</span>(<span>input_text</span>: str) -&gt; list[int]:
    <span>return</span> [chr_to_idx[t] <span>for</span> t <span>in</span> input_text]

def <span>decode</span>(<span>input_tokens</span>: list[int]) -&gt; <span>str</span>:
    <span>return</span> <span>&#34;&#34;</span>.<span>join</span>([idx_to_chr[i] <span>for</span> i <span>in</span> input_tokens])
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*adFjPqc2Ks1MY2uXuB9DGQ.png" alt=""/></p>
<p>Convert our text data into tokens:</p>
<p><strong>Installation</strong>:</p>
<p><code>pip install torch</code></p>
<p><strong>Code</strong>:</p>
<pre><code><span>import</span> torch
# use cpu or gpu based on your system
device = <span>&#34;cpu&#34;</span>
<span>if</span> torch.<span>cuda</span>.<span>is_available</span>():
    device = <span>&#34;cuda&#34;</span>

# convert our text data into tokenized tensor
data = torch.<span>tensor</span>(<span>encode</span>(text), dtyppe=torch.<span>long</span>, device=device)
</code></pre>
<p>Now, we have the tokenized tensor <code>data</code> where each characters in the text is converted to the respective tokens.</p>
<p><strong>So far:</strong></p>
<pre><code><span>import</span> torch

data_dir = <span>&#34;data.txt&#34;</span>
text = <span>open</span>(data_dir, <span>&#39;r&#39;</span>).<span>read</span>() # load all the data <span>as</span> simple string

# <span>Get</span> all unique characters <span>in</span> the text <span>as</span> vocabulary
chars = <span>list</span>(<span>set</span>(text))
vocab_size = <span>len</span>(chars)

# build the character level tokenizer
chr_to_idx = {<span>c</span>:i <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}
idx_to_chr = {<span>i</span>:c <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}

def <span>encode</span>(<span>input_text</span>: str) -&gt; list[int]:
    <span>return</span> [chr_to_idx[t] <span>for</span> t <span>in</span> input_text]

def <span>decode</span>(<span>input_tokens</span>: list[int]) -&gt; <span>str</span>:
    <span>return</span> <span>&#34;&#34;</span>.<span>join</span>([idx_to_chr[i] <span>for</span> i <span>in</span> input_tokens])


# convert our text data into tokenized tensor
data = torch.<span>tensor</span>(<span>encode</span>(text), dtyppe=torch.<span>long</span>, device=device)
</code></pre>
<h3><strong>2. Building a Data Loader</strong></h3>
<p>Now, before building our model, we have to define how we are going to feed the data into the model for training and what the data looks like in terms of dimensions and batch size.</p>
<p>Let’s define our data loader as below:</p>
<pre><code>train_batch_size = <span>16</span>  # training batch size
eval_batch_size = <span>8</span>  # evaluation batch size
context_length = <span>256</span>  # number <span>of</span> tokens processed <span>in</span> a single batch
train_split = <span>0.8</span>  # percentage <span>of</span> data to use <span>from</span> total data <span>for</span> training

# split data into trian and <span>eval</span>
n_data = <span>len</span>(data)
train_data = data[:<span>int</span>(n_data * train_split)]
eval_data = data[<span>int</span>(n_data * train_split):]


<span>class</span> <span>DataLoader</span>:
    def <span>__init__</span>(self, tokens, batch_size, context_length) -&gt; <span>None</span>:
        self.<span>tokens</span> = tokens
        self.<span>batch_size</span> = batch_size
        self.<span>context_length</span> = context_length

        self.<span>current_position</span> = <span>0</span>

    def <span>get_batch</span>(self) -&gt; torch.<span>tensor</span>:
        b, c = self.<span>batch_size</span>, self.<span>context_length</span>

        start_pos = self.<span>current_position</span>
        end_pos = self.<span>current_position</span> + b * c + <span>1</span>

        # <span>if</span> the batch exceeds total length, get the data till last token
        # and take remaining <span>from</span> starting token to avoid always excluding some data
        add_data = -<span>1</span> # n, <span>if</span> length exceeds and we need <span>`n`</span> additional tokens <span>from</span> start
        <span>if</span> end_pos &gt; <span>len</span>(self.<span>tokens</span>):
            add_data = end_pos - <span>len</span>(self.<span>tokens</span>) - <span>1</span>
            end_pos = <span>len</span>(self.<span>tokens</span>) - <span>1</span>

        d = self.<span>tokens</span>[<span>start_pos</span>:end_pos]
        <span>if</span> add_data != -<span>1</span>:
            d = torch.<span>cat</span>([d, self.<span>tokens</span>[:add_data]])
        x = (d[:-<span>1</span>]).<span>view</span>(b, c)  # inputs
        y = (d[<span>1</span>:]).<span>view</span>(b, c)  # targets

        self.<span>current_position</span> += b * c # set the next position
        <span>return</span> x, y

train_loader = <span>DataLoader</span>(train_data, train_batch_size, context_length)
eval_loader = <span>DataLoader</span>(eval_data, eval_batch_size, context_length)
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*GMpC_jFxFpk_1xK19YvbrA.png" alt=""/></p>
<p>Now we have our own customized data loader for both training and evaluation. The loader has a <code>get_batch</code> function which returns batches of <code>batch_size * context_length</code>.</p>
<p>If you are wondering why <code>x</code> is from <code>start</code> to <code>end</code> and <code>y</code> is from <code>start+1</code> to <code>end+1</code>, it’s because the main task for this model will be to predict next sequence given the previous. So there will be an extra token in <code>y</code> for it to predict the (n+1) token given last n tokens of <code>x</code>. If it sounds complicated look at the below visual:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/0*jTrSzRD-KGPs3v5E.gif" alt=""/><em>Figure 2: GPT-2 Input &amp; Output flow from “The Illustrated GPT-2” by Jay Alammar.</em></p>
<h3><strong>3. Train a simple language model</strong></h3>
<p>Now we are ready to build and train a simple language model using the data we have just loaded.</p>
<p>For this section, we will keep it very simple and implement a simple Bi-Gram Model where given the last token predict the next token. As you can see below we will be using just the Embedding layer while ignoring the main decoder block.</p>
<p>An Embedding layer represents <code>n = d_model</code> unique properties of all the characters in our vocabulary and based on which the layer pops out the property using the token index or in our case the index of our character in the vocabulary.</p>
<p>You will be amazed how well the model will behave just by using the Embeddings. And we will be improving the model step by step by adding more layers, so sit tight and follow along.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*9cYT2nBANRzBr3vqQVLBsw.png" alt=""/></p>
<p><strong>Initialization</strong>:</p>

<p>d_model = vocab_size </p>
<p>The embedding dimension or <code>d_model</code> is <code>vocab_size</code> currently because the final output has to map to the logits for each character in vocab to calculate their probabilities. Later on we will introduce a <code>Linear</code> layer which will map <code>d_model</code> to <code>vocab_size</code> and then we can have a custom embedding_dimension.</p>
<p><strong>Model</strong>:</p>
<pre><code><span>import</span> torch.<span>nn</span> <span>as</span> nn
<span>import</span> torch.<span>nn</span>.<span>functional</span> <span>as</span> F

<span>class</span> <span>GPT</span>(nn.<span>Module</span>):
    def <span>__init__</span>(self, vocab_size, d_model):
        <span>super</span>().<span>__init__</span>()
        self.<span>wte</span> = nn.<span>Embedding</span>(vocab_size, d_model) # word token embeddings
    
    def <span>forward</span>(self, inputs, targets = <span>None</span>):
        logits = self.<span>wte</span>(inputs) # dim -&gt; batch_size, sequence_length, d_model
        loss = <span>None</span>
        <span>if</span> targets != <span>None</span>:
            batch_size, sequence_length, d_model = logits.<span>shape</span>
            # to calculate loss <span>for</span> all token embeddings <span>in</span> a batch
            # kind <span>of</span> a requirement <span>for</span> cross_entropy
            logits = logits.<span>view</span>(batch_size * sequence_length, d_model)
            targets = targets.<span>view</span>(batch_size * sequence_length)
            loss = F.<span>cross_entropy</span>(logits, targets)
        <span>return</span> logits, loss
    
    def <span>generate</span>(self, inputs, max_new_tokens):
        # <span>this</span> will store the model outputs along <span>with</span> the initial input sequence
        # make a copy so that it doesn<span>&#39;t interfare with model 
        for _ in range(max_new_tokens):
            # we only pass targets on training to calculate loss
            logits, _ = self(inputs)  
            # for all the batches, get the embeds for last predicted sequence
            logits = logits[:, -1, :] 
            probs = F.softmax(logits, dim=1)            
            # get the probable token based on the input probs
            idx_next = torch.multinomial(probs, num_samples=1) 
            
            inputs = torch.cat([inputs, idx_next], dim=1)
        # as the inputs has all model outputs + initial inputs, we can use it as final output
        return inputs

m = GPT(vocab_size=vocab_size, d_model=d_model).to(device)
</span></code></pre>
<p>We have now successfully defined our model with just one <code>Embedding</code> layer and <code>Softmax</code> for token generation. Let’s see how our model behaves when given some input characters.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*dQpkxEARkvXbpJpI7dCRYA.png" alt=""/></p>
<p>😄 Pretty interesting!! But we are not quite there yet.</p>
<p>Now the final step is to train our model and give it some knowledge about the characters. Let’s setup our optimizer. We will use a simple <code>AdamW</code> optimizer for now with <code>0.001</code> learning rate. We will go through improving the optimization in later sections.</p>
<pre><code>lr = <span>1e-3</span>
optim = torch.<span>optim</span>.<span>AdamW</span>(m.<span>parameters</span>(), lr=lr)
<span>Below</span> is a very simple training loop.
epochs = <span>5000</span>
eval_steps = <span>1000</span> # perform evaluation <span>in</span> every n steps
<span>for</span> ep <span>in</span> <span>range</span>(epochs):
    xb, yb = train_loader.<span>get_batch</span>()

    logits, loss = <span>m</span>(xb, yb)
    optim.<span>zero_grad</span>(set_to_none=<span>True</span>)
    loss.<span>backward</span>()
    optim.<span>step</span>()

    <span>if</span> ep % eval_steps == <span>0</span> or ep == epochs-<span>1</span>:
        m.<span>eval</span>()
        <span>with</span> torch.<span>no_grad</span>():
            xvb, yvb = eval_loader.<span>get_batch</span>()
            _, e_loss = <span>m</span>(xvb, yvb)

            <span>print</span>(f<span>&#34;Epoch: {ep}tlr: {lr}ttrain_loss: {loss}teval_loss: {e_loss}&#34;</span>)
        m.<span>train</span>() # back to training mode
</code></pre>
<p>Let’s run:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*ikOrVlB0KHzrTWTpOLi9Lw.png" alt=""/></p>
<p>So we got a pretty good loss result. But we are not there yet. As you can see, the error decreased by a higher amount until epoch 2000 and not much improvements afterwards. It’s because the model doesn’t yet have much brain power (or layers/neural networks) and it’s just comparing embedding of one character with another.</p>
<p>The output now looks like below:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*fEEJXUZrhAIORdD0tXk_wA.png" alt=""/></p>
<p>😮 OK!! Not very pleasing but definitely some improvements than the first generation which was without any training (Obviously). The model is starting to know how the songs are formatted and the lines and everything which is pretty impressive.</p>
<p>Now, as this article is getting too longer, I will add rest of the sections in the Part 2 below:</p>

<p>Thanks for reading the article. I hope you learned something new. If you have any questions/feedback, feel free to leave a comment.</p>
<h3>References</h3>
<p><em>Automatic Arabic Poem Generation with GPT-2 — Scientific Figure on ResearchGate. Available from:</em> <a rel="noopener noreferrer nofollow ugc" href="https://www.researchgate.net/figure/GPT-2-architecture-Heilbron-et-al-2019_fig1_358654229"><em>https://www.researchgate.net/figure/GPT-2-architecture-Heilbron-et-al-2019_fig1_358654229</em></a></p>
<p><em>Alammar, J (2018). The Illustrated GPT-2 [Blog post]. Retrieved from</em> <a rel="noopener noreferrer nofollow ugc" href="https://jalammar.github.io/illustrated-gpt2/">https://jalammar.github.io/illustrated-gpt2/</a></p></div></div></div>
  </body>
</html>
