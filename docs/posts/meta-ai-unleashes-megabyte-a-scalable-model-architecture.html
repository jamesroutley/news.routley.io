<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.artisana.ai/articles/meta-ai-unleashes-megabyte-a-revolutionary-scalable-model-architecture">Original</a>
    <h1>Meta AI Unleashes Megabyte, a Scalable Model Architecture</h1>
    
    <div id="readability-page-1" class="page"><div>
        <p>A Meta team of AI researchers has proposed <a href="https://arxiv.org/abs/2305.07185">an innovative architecture for AI models</a>, capable of generating expansive content in text, image, and audio formats, stretching to over 1 million tokens. This groundbreaking proposal, if embraced, could pave the way for the next generation of proficient AI models, transcending the Transformer architecture that underpins models such as GPT-4 and Bard, and unleashing novel capacities in content generation.</p>
<h3>The Constraints of Current Models</h3>
<p>Contemporary high-performing generative AI models, like OpenAI&#39;s GPT-4, are grounded in the Transformer architecture. <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"><u>Initially introduced by Google researchers in 2017</u></a>, this architecture forms the backbone of emergent AI models, facilitating an understanding of nuanced inputs and generating extensive sentences and documents.</p>
<p>Nonetheless, Meta&#39;s AI research team posits that the prevailing Transformer architecture might be reaching its threshold. They highlight two significant flaws inherent in the design:</p>
<ol><li><p><b>With the increase in the length of inputs and outputs, self-attention scales dramatically.</b> As each word processed or produced by a Transformer language model requires attention to all other words, the computation becomes highly intensive for thousands of words, whereas it&#39;s less problematic for smaller word counts.</p></li><li><p><b>Feedforward networks, which aid language models in comprehending and processing words through a sequence of mathematical operations and transformations, struggle with scalability on a per-position basis</b>. These networks operate on character groups or &#34;positions&#34; independently, leading to substantial computational expenses.
</p></li></ol>
<h3>Megabyte Model: The Game Changer</h3>
<p>The Megabyte model, introduced by Meta AI, showcases a uniquely different architecture, dividing a sequence of inputs and outputs into &#34;patches&#34; rather than individual tokens. Within each patch, a local AI model generates results, while a global model manages and harmonizes the final output across all patches.</p>
<p>This methodology addresses the scalability challenges prevalent in today&#39;s AI models. The Megabyte model&#39;s patch system permits a single feedforward network to operate on a patch encompassing multiple tokens. Researchers found that this patch approach effectively counters the issue of self-attention scaling.</p>
<p>The patch model enables Megabyte to perform calculations in parallel, a stark contrast to traditional Transformers performing computations serially. Even when a base model has more parameters, this results in significant efficiencies. Experiments indicated that Megabyte, utilizing a 1.5B parameter model, could generate sequences 40% quicker than a Transformer model operating on 350M parameters.</p>
<p>Using several tests to determine the limits of this approach, researchers discovered that the Megabyte model&#39;s maximum capacity exceeded 1.2M tokens. For comparison, OpenAI&#39;s GPT-4 has a limit of 32,000 tokens, while Anthropic&#39;s Claude has a limit of 100,000 tokens.</p>
<h3>Shaping the AI Future</h3>
<p>As the AI arms race progresses, AI model enhancements largely stem from training on an ever-growing number of parameters, which are the values learned during an AI model&#39;s training phase. While GPT-3.5 was trained on 175B parameters, there&#39;s speculation that the more capable GPT-4 was trained on 1 trillion parameters.</p>
<p>OpenAI CEO Sam Altman recently suggested a shift in strategy, confirming that the company is thinking <a href="https://techcrunch.com/2023/04/14/sam-altman-size-of-llms-wont-matter-as-much-moving-forward/"><u>beyond training colossal models</u></a> and is zeroing in on other optimizations. He equated the future of AI models to iPhone chips, where the majority of consumers are oblivious to the raw technical specifications. Altman envisioned a similar future for AI, emphasizing the continual increase in capability.</p>
<p>Meta’s researchers believe their innovative architecture arrives at an opportune time, but also acknowledge there are other pathways to optimization. Promising research areas such as more efficient encoder models adopting patching techniques, decode models breaking down sequences into smaller blocks, and preprocessing sequences into compressed tokens are on the horizon, and could extend the capabilities of the existing Transformer architecture for a new generation of models</p>
<p>Nonetheless, Meta’s recent research has AI experts excited. Andrej Karpathy, the former Sr. Director of AI at Tesla and now a lead AI engineer at OpenAI, <a href="https://twitter.com/karpathy/status/1657949234535211009"><u>chimed in as well on the paper</u></a>. This is “promising,” he wrote on Twitter. “Everyone should hope that we can throw away tokenization in LLMs. Doing so naively creates (byte-level) sequences that are too long.”</p>
      </div></div>
  </body>
</html>
