<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ashvardanian.com/posts/stringwars-on-gpus/">Original</a>
    <h1>Processing Strings 109x Faster Than Nvidia on H100</h1>
    
    <div id="readability-page-1" class="page"><div><p><a href="http://github.com/ashvardanian/StringZilla"><img alt="StringZilla banner" loading="lazy" src="https://ashvardanian.com/stringwars-on-gpus/StringZilla-v4.jpg"/></a></p><p>I‚Äôve just shipped <a href="https://github.com/ashvardanian/StringZilla/releases/tag/v4.0.0">StringZilla v4</a>, the first <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a>-capable release of my <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a>-first string processing library.
Which in English means that it is now fast not only on CPUs, but also on GPUs!</p><ul><li>I‚Äôve wanted to add <a href="https://en.wikipedia.org/wiki/ROCm">ROCm</a>-acceleration for AMD GPUs ü§¶‚Äç‚ôÇÔ∏è</li><li>I‚Äôve wanted to include a parallel multi-pattern search algorithm ü§¶‚Äç‚ôÇÔ∏è</li><li>I‚Äôve wanted to publish it back in December 2024 ü§¶‚Äç‚ôÇÔ∏è</li></ul><p>So not everything went to plan, but ‚ÄúStringZilla 4 CUDA‚Äù is finally here, bringing 500+ GigaCUPS of edit-distance calculations in a <code>pip install</code>-able package, and a few more tricks up its sleeve, aimed at large-scale Information Retrieval, <strong>Databases</strong> and Datalake systems, as well as <strong>Bioinformatics</strong> workloads.
All under a permissive Apache 2.0 open-source license, free for commercial use.
So in this post, we‚Äôll cover some of the most interesting parts of this release, including:</p><ul><li>Fast evaluation of dynamic-programming algorithms on <strong>GPUs</strong>,</li><li>Hashing beyond <code>CRC32</code>, <code>MurMurHash</code>, <code>xxHash</code>, and <code>aHash</code>, and</li><li>Fingerprinting biological sequences with <strong>52-bit</strong> integers?!</li></ul><h2 id="background--inspiration">Background &amp; Inspiration</h2><p>Historically, StringZilla started as conference talk material in the late 2010s, showcasing the power of AVX-512 and the intricacies of vectorizing non-data-parallel workloads (‚Ä¶ pretty much the opposite of my <a href="https://github.com/ashvardanian/SimSIMD">SimSIMD</a>).
Over the years, it expanded from a few substring search kernels into a beast competing with <code>GLibC</code> for the fastest <a href="https://en.cppreference.com/w/cpp/string/byte/memcpy.html"><code>memcpy</code></a> (yes, I know it‚Äôs a popular claim).
It later added support for little- &amp; big-endian platforms; several generations of AVX-512 on x86, Arm NEON, SVE, and SVE2 extensions; dynamic dispatch; and first-party bindings for Python, Rust, JavaScript, and even Swift, translating the underlying C99 implementation.</p><p>Now, StringZilla v4 adds even more functionality:</p><ul><li>Yet another non-cryptographic hash function and string <a href="https://en.wikipedia.org/wiki/Pseudorandom_number_generator">PRNGs</a> on <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">AES</a> and other port-parallel SIMD instructions.</li><li>New intersection and sorting algorithms for extensive collections of strings standard in DBMS <a href="https://en.wikipedia.org/wiki/Join_(SQL)"><code>JOIN</code></a>s and <a href="https://en.wikipedia.org/wiki/Order_by"><code>ORDER BY</code></a>s.</li><li>GPU- and CPU-accelerated string similarity kernels, covering Levenshtein distances, <a href="https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm">Needleman-Wunsch</a> and <a href="https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm">Smith-Waterman</a> scores with <a href="https://scholar.google.com/citations?user=Ryknh9AAAAAJ&amp;hl=en">Gotoh‚Äôs</a> extensions for <a href="https://github.com/gata-bio/affine-gaps">‚Äúaffine gaps‚Äù</a>, needed in bioinformatics.</li><li>GPU- and CPU-accelerated fingerprinting <a href="https://en.wikipedia.org/wiki/MinHash">MinHashing</a> kernels for large-scale Information Retrieval and deduplication.</li></ul><p>Depending on the chosen implementation and input data, these may be an order of magnitude faster than what you are using today.
Not all of the new kernels are State-of-the-Art, as I partially demonstrate in my refactored <strong><a href="http://github.com/ashvardanian/StringWa.rs">StringWa.rs</a></strong> benchmarks repository ‚Äî they‚Äôre solving a different problem: providing a reliable, fast, and easy-to-use baseline for large-scale workloads.</p><p><a href="http://github.com/ashvardanian/StringWa.rs"><img alt="StringWa.rs banner" loading="lazy" src="https://ashvardanian.com/stringwars-on-gpus/StringWa.rs.jpg"/></a></p><h2 id="traditional-string-similarity-measures">Traditional String Similarity Measures</h2><h3 id="dynamic-programming-and-levenshtein-evaluation-order">Dynamic Programming and Levenshtein Evaluation Order</h3><p>If we look at the classic description of the Levenshtein distance computing methodology, it suggests incrementally populating an $N + 1$ by $M + 1$ matrix, where $N$ and $M$ are the lengths of the two strings being compared.
The order in which we fill this matrix is crucial for the algorithm‚Äôs efficiency and correctness:</p><p>$$
L_{i,j}=\min\left[ \begin{array}{l} L_{i-1,j}+1, \ L_{i,j-1}+1, \ L_{i-1,j-1}+[x_i \ne y_j] \end{array} \right]
$$</p><p>$$
L_{0,j}=j, L_{i,0}=i
$$</p><blockquote><p>Here, $L_{i,j}$ represents the edit distance between the first $i$ characters of string $x$ and the first $j$ characters of string $y$.
The three terms in the minimum represent deletion, insertion, and substitution costs respectively, where $[x_i \ne y_j]$ equals 1 if characters differ and 0 if they match.</p></blockquote><p>If we look up the Wikipedia article or the absolute majority of code-snippets they describe the Wagner-Fischer algorithm, filling that matrix top-to-bottom, left-to-right.
More memory-efficient implementations suggest storing only 2 rows of the matrix at any time, significantly reducing the space complexity from $O(NM)$ to $O(\min(N, M))$.
That, however, does nothing to remove the sequential data dependency in the lower row - we can‚Äôt process $L_{i,j}$ without having computed all $L_{i,j-1}$.</p><p>The smarter way, when parallelizing such algorithms is to look into dependency chains that break vectorization potential.
In the case of such string similarity measures, including Levenshtein distance, it‚Äôs simple - <strong>evaluate diagonals instead of rows</strong>!
We‚Äôll store 3 diagonals instead of the 2 rows, and each consecutive diagonal will be computed from the previous two.
Substitution costs will come from the sooner diagonal, while insertion and deletion costs will come from the later diagonal.</p><table><tbody><tr><td><strong>Row-by-Row Algorithm</strong></td><td><strong>Anti-Diagonal Algorithm</strong></td></tr><tr><td colspan="2"><strong>Legend:</strong></td></tr></tbody></table><p>Did it help?
Performance of such algorithms is measured in Cell Updates Per Second, or CUPS.
Comparing ‚âÖ 1&#39;000-byte strings resulted in:</p><ul><li><code>rapidfuzz::levenshtein</code>: <strong>14&#39;316 MCUPS</strong> on an Intel Sapphire Rapids core,</li><li><code>stringzillas::LevenshteinDistances</code>: <strong>13&#39;084 MCUPS</strong> on the same 1 core,</li><li><code>stringzillas::LevenshteinDistances</code>: <strong>624&#39;730 MCUPS</strong> on an Nvidia H100 GPU.</li></ul><p>NLTK, one of Pythons‚Äô historically most used libraries with <a href="https://clickpy.clickhouse.com/dashboard/nltk">over 1 Billion downloads</a>, yields around ‚âÖ <strong>2 MCUPS</strong>.
RapidFuzz, wrapped into Python, of course performs much better, but most still loses some throughput in the binding layer.
StringZilla C to Python bindings are some of the <a href="https://github.com/ashvardanian/StringZilla/tree/main/python">thinnest on GitHub</a>, so the degradation is minimal compared to other packages, including Nvidia‚Äôs own CuDF.
Even after pre-constructing the <code>cudf.Series</code> objects, the performance for different string lengths in MCUPS looks like this:</p><table><thead><tr><th>Library</th><th>‚âÖ 100 bytes</th><th>‚âÖ 1&#39;000 bytes</th><th>‚âÖ 10&#39;000 bytes</th></tr></thead><tbody><tr><td><code>cudf.edit_distance</code> üêç</td><td>24&#39;754</td><td>6&#39;976</td><td>1&#39;447</td></tr><tr><td><code>stringzillas.LevenshteinDistances</code> üêç</td><td>18&#39;081</td><td>320&#39;109</td><td>157&#39;968</td></tr><tr><td><code>stringzillas::LevenshteinDistances</code> ü¶Ä</td><td>20&#39;780</td><td>624&#39;730</td><td>173&#39;160</td></tr></tbody></table><blockquote><p>üêç denotes Python bindings, ü¶Ä denotes Rust.</p></blockquote><p>CuDF calls <code>nvtext::levenshtein</code> under the hood.
It works well on collections of small inputs, but not larger ones.
Most likely, it doesn‚Äôt distribute large inputs across the GPU cores as aggressively as StringZilla does.
End result?</p><ul><li><strong>46x performance improvement on ‚âÖ 1&#39;000-byte strings</strong>.</li><li><strong>109x performance improvement on ‚âÖ 10&#39;000-byte strings</strong>.</li></ul><p>To be fair, <code>cudf</code> is not exactly a string-similarity library, and there is a separate commercial offering from Nvidia called <a href="https://www.nvidia.com/en-gb/clara/genomics/">Clara Parabricks</a> that probably does better.
Still, 109x over CuDF is a good start and will be a perfect foundation for future improvements!</p><h3 id="substitution-costs-and-affine-gap-penalties">Substitution Costs and Affine Gap Penalties</h3><p>In Bioinformatics, Levenshtein distance is used to compare DNA strings.
Character insertions and deletions in string pairs signal physical breaks in really long molecules.
The presence of those gaps is a bigger factor than the inferred length of the gap, so those are scored differently - using <strong>‚Äúaffine gap penalties‚Äù</strong>.
It‚Äôs not enough to store 1 matrix anymore, we need to store 3 matrices, including 2 new ones to differentiate gap extension and openings.</p><p>Similarly, Needleman-Wunsch score generalizes the Levenshtein distance to handle <strong>variable substitution costs</strong>.
That‚Äôs handy when dealing with protein sequences.
Those are represented as strings over a much smaller alphabet of 20 amino acids: ACDEFGHIKLMNPQRSTVWY.
So one needs to store a 20x20 substitution matrix, or a larger one if we want to include ambiguous amino acids.</p><p>On the bright side, StringZilla already uses <a href="https://github.com/ashvardanian/less_slow.cpp/releases/tag/v0.11.0">Nvidia‚Äôs DP4A and DPX instructions</a> for SIMD processing of small integers in such dynamic programming workloads.
On the other side, StringZilla‚Äôs current implementation mistakenly uses constant memory to store the substitution matrix, which is a bottleneck and will be improved in the future.
Still, the results are not too bad compared to other <code>pip install</code>-able solutions.
On ‚âÖ 1000-long amino-acid sequences, we get:</p><ul><li><code>biopython</code> achieving ‚âÖ <strong>303 MCUPS</strong>,</li><li><code>stringzillas-cpus</code> achieving ‚âÖ <strong>276 MCUPS</strong>,</li><li><code>stringzillas-cuda</code> achieving ‚âÖ <strong>10&#39;098 MCUPS</strong>.</li></ul><h2 id="two-more-hash-functions">Two More Hash Functions?!</h2><h3 id="design-goals">Design Goals</h3><p>There are a lot of hash functions around!
Some of them are really-really good!</p><ul><li>The <a href="https://github.com/Cyan4973/xxHash"><code>XXH3</code> by Yann Collet</a>, for example, is a remarkable production-ready codebase in which even very experienced developers may find something new.</li><li><code>MurMurHash</code> is a classic, released by Austin Appleby in 2008 together with the <a href="https://github.com/aappleby/smhasher">SMHasher framework</a>.</li><li>There is also a very well crafted <a href="https://github.com/tkaitchuck/aHash"><code>ahash</code> package for Rust by Tom Kaitchuck</a>, popularizing AES instructions, also previously suggested in <a href="https://github.com/jandrewrogers/AquaHash"><code>AquaHash</code> by J. Andrew Rogers</a>.</li></ul><p>Overall, I wanted a hash function that:</p><ul><li>Is fast for both short <strong>(velocity)</strong> and long strings <strong>(throughput)</strong>.</li><li>Supports incremental <strong>(streaming)</strong> hashing, when the data arrives in chunks.</li><li>Supports custom <strong>seeds</strong> for hashes and have it affecting every bit of the output.</li><li>Provides <strong>dynamic-dispatch</strong> for different architectures to simplify deployment.</li><li>Uses not just AVX2 &amp; NEON <strong>SIMD</strong>, but also masked AVX-512 &amp; predicated SVE2.</li><li>Documents its logic and produces the <strong>same output</strong> across different platforms.</li><li>Outputs 64-bit or 128-bit hashes and passes the <strong>SMHasher</strong> <code>--extra</code> tests.</li></ul><h3 id="aes-and-port-parallelism-recipe">AES and Port-Parallelism Recipe</h3><p>Using AES instructions for hashing is not a new idea, but it makes a lot of sense.
Just look at the amount of signal mixing that happens in a single round of AES:</p><p><img alt="AES Encryption Rounds used in StringZilla" loading="lazy" src="https://ashvardanian.com/stringwars-on-gpus/AES-Encryption-Rounds.jpg"/></p><p>In every iteration, AES performs four operations on a $4*4=16$ byte-matrix:</p><ol><li><code>SubBytes</code> - a non-linear substitution step where each byte is mapped to another.</li><li><code>ShiftRows</code> ‚Äì a transposition where the last three rows are shifted cyclically.</li><li><code>MixColumns</code> ‚Äì a linear mixing operation that operates on the columns of the state.</li><li><code>AddRoundKey</code> ‚Äì a simple XOR operation with a round key.</li></ol><p>In the second step, we mix bytes between consecutive 32-bit words of the state.
In the third step, we mix bytes within a single 32-bit word.
And the rest of the logic applies to the entire 128-bit matrix.
Great instructions for mixing, but not the cheapest ones:</p><ul><li><code>VAESENC (ZMM, ZMM, ZMM)</code> and <code>VAESDEC (ZMM, ZMM, ZMM)</code>:<ul><li>on Intel Ice Lake: 5 cycles on port 0.</li><li>On AMD Zen4: 4 cycles on ports 0 or 1.</li></ul></li></ul><p>As it often happens with integer workloads on Intel, a SIMD instruction is always routed to just one execution port, often port 0 or 5.
AES routes to 0, so to mix more data in parallel we need to combine it with cheap instructions mapping to other ports, like:</p><ul><li><code>VPSHUFB_Z (ZMM, K, ZMM, ZMM)</code><ul><li>on Intel Ice Lake: 3 cycles on port 5.</li><li>On AMD Zen4: 2 cycles on ports 1 or 2.</li></ul></li><li><code>VPADDQ (ZMM, ZMM, ZMM)</code>:<ul><li>on Intel Ice Lake: 1 cycle on ports 0 or 5.</li><li>On AMD Zen4: 1 cycle on ports 0, 1, 2, 3.</li></ul></li></ul><p>That‚Äôs a solid recipe, and is ideologically close to <code>ahash</code>, but with a few twists:</p><ul><li>A larger state and a larger block size is used for inputs longer than 64 bytes, benefiting from wider registers on current CPUs. Like many other hash functions, the state is initialized with the seed and a set of <a href="https://en.wikipedia.org/wiki/Pi">Pi</a> constants. Unlike others, we pull more Pi bits (1024), but only 64-bits of the seed, to keep the API sane.</li><li>The length of the input is not mixed into the AES block at the start to allow incremental construction, when the final length is not known in advance.</li><li>The vector-loads are not interleaved, meaning that each byte of input has exactly the same weight in the hash. On the implementation side it requires some extra shuffling on older platforms, but on newer platforms it can be done with ‚Äúmasked‚Äù loads in AVX-512 and ‚Äúpredicated‚Äù instructions in SVE2.</li></ul><p>Fun fact: AES instructions on x86 and Arm do slightly different things, but once you compensate for that, the performance is still great.
Running StringWa.rs on an Intel Sapphire Rapids core, we get:</p><table><thead><tr><th>Library</th><th>Bits</th><th>Ports ¬π</th><th>Short Words</th><th>Long Lines</th></tr></thead><tbody><tr><td><code>std::hash</code></td><td>64</td><td>‚ùå</td><td>0.43 GiB/s</td><td>3.74 GiB/s</td></tr><tr><td><code>crc32fast::hash</code></td><td>32</td><td>‚úÖ</td><td>0.49 GiB/s</td><td>8.45 GiB/s</td></tr><tr><td><code>xxh3::xxh3_64</code></td><td>64</td><td>‚úÖ</td><td>1.08 GiB/s</td><td>9.48 GiB/s</td></tr><tr><td><code>aHash::hash_one</code></td><td>64</td><td>‚ùå</td><td>1.23 GiB/s</td><td>8.61 GiB/s</td></tr><tr><td><code>gxhash::gxhash64</code></td><td>64</td><td>‚ùå</td><td><strong>2.68 GiB/s</strong></td><td>9.19 GiB/s</td></tr><tr><td><code>stringzilla::hash</code></td><td>64</td><td>‚úÖ</td><td>1.84 GiB/s</td><td><strong>11.23 GiB/s</strong></td></tr></tbody></table><blockquote><p>¬π Portability means availability in multiple other programming languages, like C, C++, Python, Java, Go, JavaScript, etc.</p></blockquote><h3 id="generating-random-strings">Generating Random Strings</h3><p>The same hashing AES primitives can easily be used for high-throughput generation of random strings at speeds far exceeding what <code>std::random_device</code> and <code>std::mt19937</code> can do.
That‚Äôs typically achieved in one of <a href="https://crypto.stackexchange.com/a/32512">3 different modes</a>:</p><ol><li>CTR (Counter Mode)</li><li>OFB (Output Feedback Mode)</li><li>CFB (Cipher Feedback Mode)</li></ol><p>The first is easily parallelizable, which can be handy if we want to scramble all RAM on our machine, so the choice was clear.
But it‚Äôs still tricky if every nano-second counts.</p><p>The reason why some of the StringZilla logic exceeds <code>memcpy</code> speeds, is the use of non-temporal stores, which bypass the CPU caches and write data directly to RAM.
This reduces cache pollution and improves energy-efficiency on IO-heavy workloads.
So for PRNGs it makes a lot of sense, but the extra complexity of handling misaligned writes wasn‚Äôt worth it, if you want to keep the PRNG seed-able and reproducible!
Even without those tricks, the results are impressive, compared to other Rusty options:</p><table><thead><tr><th>Library</th><th>‚âÖ 100 bytes lines</th><th>‚âÖ 1000 bytes lines</th></tr></thead><tbody><tr><td><code>getrandom::fill</code></td><td>0.18 GiB/s</td><td>0.40 GiB/s</td></tr><tr><td><code>rand_chacha::ChaCha20Rng</code></td><td>0.62 GiB/s</td><td>1.72 GiB/s</td></tr><tr><td><code>rand_xoshiro::Xoshiro128Plus</code></td><td>2.66 GiB/s</td><td>3.72 GiB/s</td></tr><tr><td><code>zeroize::zeroize</code></td><td>4.62 GiB/s</td><td>4.35 GiB/s</td></tr><tr><td><code>sz::fill_random</code></td><td><strong>17.30 GiB/s</strong></td><td><strong>10.57 GiB/s</strong></td></tr></tbody></table><h3 id="52-bit-math-for-bioinformatics">52-bit Math for Bioinformatics?!</h3><p>There is, however, a kind of hashing where I still avoid AES primitives in favor of modulo integer arithmetic, but with a twist!</p><p><a href="https://github.com/unum-cloud/usearch/">USearch</a>, my search engine, uses 40-bit integers to identify entries - a non-standard size chosen to balance memory efficiency with a large enough address space to fit 1 trillion of vectors on 1 machine.
Continuing the trend of obscure integer sizes, StringZilla v4 uses 52-bit integers to compute <a href="https://en.wikipedia.org/wiki/MinHash">MinHashes</a>, or ‚ÄúMin-wise independent permutations Locality Sensitive Hashing‚Äù.</p><p>First of all, I wasn‚Äôt sure if MinHash is a big thing.
I doubted it even more once I looked at available software.
But the community pressure was too high, so I caved in.</p><hr/><p>Looking at a text document of length $T$ bytes, we can extract $T - N + 1$ N-grams of length $N$.
For example, the string ‚Äúhello‚Äù contains the following 3-grams: ‚Äúhel‚Äù, ‚Äúell‚Äù, and ‚Äúllo‚Äù.
Then, MinHash defines $D$ different hash functions $h_1, h_2, ‚Ä¶, h_D$.
For each hash function $h_i$, we compute $T - N + 1$ hashes - one for each N-gram - and take the minimum value:</p><p>$$
\text{MinHash}(h_i) = \min_{j=1}^{T - N + 1} h_i(\text{N-gram}_j)
$$</p><p>Those $D$ minimum hash values form the MinHash signature of the document, a $D$-dimensional vector.
Each of those hashes becomes more computationally expensive as the $N$ increases.
So the overall complexity of computing the MinHash signature is $O(D \cdot (T - N) \cdot N)$.
That‚Äôs a lot!</p><p>To reduce the complexity to $O(D \cdot (T - N))$, one can switch to Rabin-style rolling hashes, which I was doing in StringZilla v3 as well, but it wasn‚Äôt enough!
Computing high-quality signatures required good enough intermediate hashes, either:</p><ol><li>a simple mixing scheme with larger 64-bit representations of hasher states, or</li><li>a more complex mixing scheme with smaller 32-bit representations of hasher states.</li></ol><p>But the third option is always the best one, right?
As many of us, including readers of this blog and <a href="https://github.com/ashvardanian/less_slow.cpp">Less Slow C++</a>, using <code>float</code>-s to do the dirty integer work is a common trick for tiny integers.
What everyone forgets is that <code>double</code>-s are also a thing, and they have 53 bits of precision (52 stored bits plus 1 implicit bit).
So if we can fit our hasher state in 52 bits, we can do all the mixing and modulo arithmetic using <code>double</code>-s, and then store the final result as a 32-bit integer.</p><table><thead><tr><th></th><th>Compute in</th><th>Store in</th><th>Quality</th><th>CPU-friendly</th><th>GPU-friendly</th></tr></thead><tbody><tr><td>1</td><td><code>uint32_t</code></td><td><code>uint32_t</code></td><td>‚òÖ‚òÜ‚òÜ</td><td>‚òÖ‚òÖ‚òÖ</td><td>‚òÖ‚òÖ‚òÜ</td></tr><tr><td>2</td><td><code>uint64_t</code></td><td><code>uint64_t</code></td><td>‚òÖ‚òÖ‚òÖ</td><td>‚òÖ‚òÖ‚òÜ</td><td>‚òÖ‚òÜ‚òÜ</td></tr><tr><td>3</td><td><code>double</code></td><td><code>uint32_t</code></td><td>‚òÖ‚òÖ‚òÖ</td><td>‚òÖ‚òÖ‚òÜ</td><td>‚òÖ‚òÖ‚òÜ</td></tr></tbody></table><p>Modern vectorized CPUs with 512-bit wide FMA units are surprisingly good at this, but GPUs are even better!
It took a while to get rid of all the edge cases and ensuring the CPU and GPU kernels report the same fingerprints.
At the end of the day:</p><ul><li>single-threaded Rust code: <strong>0.5 MiB/s</strong>.</li><li>H100 CUDA code: <strong>392.37 MiB/s</strong>.</li></ul><p><a href="https://github.com/rapidsai/cudf/pull/12961">In 2023</a>, Nvidia also added MinHash to their <a href="https://docs.rapids.ai/api/cudf/stable/libcudf_docs/api_docs/nvtext_minhash/"><code>nvtext::</code></a> package.
It uses the <code>MurmurHash3_32</code> algorithm, as a foundation, and is somewhat more similar in design to the Rust‚Äôs <code>probabilistic_collections</code> package.
Comparing these solutions purely on throughput isn‚Äôt fair, as the quality of the produced signatures matters and differs a lot.</p><table><thead><tr><th>Library</th><th>‚âÖ 100 bytes lines</th><th>‚âÖ 1000 bytes lines</th></tr></thead><tbody><tr><td>Serial MinHash for <code>&lt;ByteGrams&gt;</code></td><td>0.44 MiB/s</td><td>0.47 MiB/s</td></tr><tr><td></td><td>92.81% collisions</td><td>94.58% collisions</td></tr><tr><td></td><td>0.8528 entropy</td><td>0.7979 entropy</td></tr><tr><td></td><td></td><td></td></tr><tr><td><code>pc::MinHash&lt;ByteGrams&gt;</code></td><td>2.41 MiB/s</td><td>2.37 MiB/s</td></tr><tr><td></td><td>91.80% collisions</td><td>93.17% collisions</td></tr><tr><td></td><td>0.9343 entropy</td><td>0.8779 entropy</td></tr><tr><td></td><td></td><td></td></tr><tr><td><code>szs::Fingerprints</code> on 1x CPU</td><td>0.56 MiB/s</td><td>0.51 MiB/s</td></tr><tr><td><code>szs::Fingerprints</code> on 16x CPUs</td><td>6.62 MiB/s</td><td>8.03 MiB/s</td></tr><tr><td><code>szs::Fingerprints</code> on 1x GPU</td><td><strong>102.07 MiB/s</strong></td><td><strong>392.37 MiB/s</strong></td></tr><tr><td></td><td>86.80% collisions</td><td>93.21% collisions</td></tr><tr><td></td><td>0.9992 entropy</td><td>0.9967 entropy</td></tr></tbody></table><p>Within StringWa.rs, I only currently look at the collision rate of individual dimensions across the dataset and the entropy of the bit distribution within signatures.
The entropy is <em>somewhat</em> absolute, but the collision rate can only be compared within the same dataset.
The ultimate benchmark, is of course, the quality of the nearest-neighbor search results, but that‚Äôs a topic for another post and a longer list of Information Retrieval techniques.</p><h2 id="sorting--batch-operations">Sorting &amp; Batch Operations</h2><p>Most sorting algorithms are comparison-based, meaning that they rely on a series of comparisons between elements to determine their order.
Comparing two integers is a single CPU instruction, but comparing two strings is much more complex.
It involves a loop over the characters of both strings, comparing them one by one until a difference is found or the end of one string is reached:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-0-1"><a href="#hl-0-1">1</a>
</span><span id="hl-0-2"><a href="#hl-0-2">2</a>
</span><span id="hl-0-3"><a href="#hl-0-3">3</a>
</span><span id="hl-0-4"><a href="#hl-0-4">4</a>
</span><span id="hl-0-5"><a href="#hl-0-5">5</a>
</span><span id="hl-0-6"><a href="#hl-0-6">6</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="cpp"><span><span><span>bool</span> <span>is_less</span><span>(</span><span>char</span> <span>const</span><span>*</span> <span>a</span><span>,</span> <span>size_t</span> <span>a_len</span><span>,</span> <span>char</span> <span>const</span><span>*</span> <span>b</span><span>,</span> <span>size_t</span> <span>b_len</span><span>)</span> <span>noexcept</span> <span>{</span>
</span></span><span><span>    <span>char</span> <span>const</span><span>*</span> <span>a_comparison_end</span> <span>=</span> <span>a</span> <span>+</span> <span>std</span><span>::</span><span>min</span><span>(</span><span>a_len</span><span>,</span> <span>b_len</span><span>);</span>
</span></span><span><span>    <span>for</span> <span>(;</span> <span>a</span> <span>&lt;</span> <span>a_comparison_end</span><span>;</span> <span>++</span><span>a</span><span>,</span> <span>++</span><span>b</span><span>)</span> <span>// ! Loop to be avoided !
</span></span></span><span><span><span></span>        <span>if</span> <span>(</span><span>*</span><span>a</span> <span>!=</span> <span>*</span><span>b</span><span>)</span> <span>return</span> <span>*</span><span>a</span> <span>&lt;</span> <span>*</span><span>b</span><span>;</span>
</span></span><span><span>    <span>return</span> <span>a_len</span> <span>&lt;</span> <span>b_len</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>In older StringZilla versions I‚Äôve shown that the trivial optimization of sorting integer-represented prefixes before the rest of the string can yield significant speedups.
The implementation was, however, a proof of concept, but now it‚Äôs a lot more usable and yields even better results:</p><table><thead><tr><th>Library</th><th>Shorter Words</th><th>Longer Lines</th></tr></thead><tbody><tr><td><code>std::sort_unstable_by_key</code></td><td>54.35 M comparisons/s</td><td>57.70 M comparisons/s</td></tr><tr><td><code>rayon::par_sort_unstable_by_key</code> on 1x CPU</td><td>47.08 M comparisons/s</td><td>50.35 M comparisons/s</td></tr><tr><td><code>arrow::lexsort_to_indices</code></td><td>122.20 M comparisons/s</td><td><strong>84.73 M comparisons/s</strong></td></tr><tr><td><code>sz::argsort_permutation</code></td><td><strong>182.88 M comparisons/s</strong></td><td>74.64 M comparisons/s</td></tr></tbody></table><p>It definitely still has room for improvement, and will pave the way to a broader range of batch-processing operations.
Now that I have a <a href="https://ashvardanian.com/posts/beyond-openmp-in-cpp-rust/">fast enough thread-pool implementation</a>, scaling will be easier!</p><h2 id="try-yourself">Try Yourself</h2><p>Beyond the algorithmic innovations, the engineering challenge was shipping across platforms and languages:</p><ul><li>For Python bindings alone, I ship to PyPI more platform-specific wheels per release than NumPy‚Ä¶</li><li>And unlike NumPy, my libraries ship their own SIMD and GPGPU kernels, that need to be re-compiled for each platform, instead of relying on C interfaces of existing BLAS libraries‚Ä¶</li><li>And unlike NumPy, StringZilla also ships for C, C++, CUDA, Rust, JavaScript, Go, and Swift, which all have their own peculiarities‚Ä¶</li></ul><p>‚Ä¶ all from the same repository‚Äôs CI on a free-tier GitHub Actions plan.
So I won‚Äôt be surprised if there is still some weird issue propagating build flags and inferring a weird SIMD capability support on some platforms.
To check if it works as expected on your end, for Python:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-1-1"><a href="#hl-1-1">1</a>
</span><span id="hl-1-2"><a href="#hl-1-2">2</a>
</span><span id="hl-1-3"><a href="#hl-1-3">3</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="bash"><span><span>pip install stringzilla         <span># for serial algorithms</span>
</span></span><span><span>pip install stringzillas-cpus   <span># for parallel multi-CPU backends</span>
</span></span><span><span>pip install stringzillas-cuda   <span># for parallel Nvidia GPU backend</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>To check for detected capabilities:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-2-1"><a href="#hl-2-1">1</a>
</span><span id="hl-2-2"><a href="#hl-2-2">2</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="bash"><span><span>python -c <span>&#34;import stringzilla as sz; print(sz.__version__, sz.__capabilities__)&#34;</span>       <span># for serial algorithms</span>
</span></span><span><span>python -c <span>&#34;import stringzillas as szs; print(szs.__version__, szs.__capabilities__)&#34;</span>   <span># for parallel algorithms</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>For other programming languages, refer to the README.md and if something doesn‚Äôt work - share your issues on <a href="https://github.com/ashvardanian/StringZilla/issues/236">this thread on GitHub</a>.
For Rust, however, stuff should work fine, as its been used extensively for the StringWa.rs that you can pull and reproduce on your own hardware:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-3-1"><a href="#hl-3-1">1</a>
</span><span id="hl-3-2"><a href="#hl-3-2">2</a>
</span><span id="hl-3-3"><a href="#hl-3-3">3</a>
</span><span id="hl-3-4"><a href="#hl-3-4">4</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="bash"><span><span><span>RUSTFLAGS</span><span>=</span><span>&#34;-C target-cpu=native&#34;</span> <span>\
</span></span></span><span><span><span></span>    <span>STRINGWARS_DATASET</span><span>=</span>your-favorite-dataset <span>\
</span></span></span><span><span><span></span>    <span>STRINGWARS_TOKENS</span><span>=</span>lines <span>\
</span></span></span><span><span><span></span>    cargo criterion --features bench_hash bench_hash --jobs <span>$(</span>nproc<span>)</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>This work moves on small pushes from the community: a minimal repro, a perf trace, or a note to keep going.
My thanks to everyone who contributed!</p><p>Special thanks to <a href="https://nebius.com/">Nebius</a>, my most-used cloud, for dependable, long-haul GPU capacity powering both personal experiments like this and <a href="https://github.com/unum-cloud/">Unum‚Äôs open-source work</a>, from <a href="https://nebius.com/customer-stories/unum">pre-training new perception &amp; generative architectures</a> to this kind of performance engineering.</p><p>Spread the word if you‚Äôd like to see more such open-source work ü§ó</p><blockquote><p lang="en" dir="ltr"><a href="https://t.co/KbIf9uMQLa">https://t.co/KbIf9uMQLa</a> on GPUs: Databases &amp; Bioinformatics ü¶†</p>‚Äî Ash Vardanian (@ashvardanian) <a href="https://twitter.com/ashvardanian/status/1967654348340785558?ref_src=twsrc%5Etfw">September 15, 2025</a></blockquote></div></div>
  </body>
</html>
