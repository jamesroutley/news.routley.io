<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/">Original</a>
    <h1>LeftoverLocals: Listening to LLM responses through leaked GPU local memory</h1>
    
    <div id="readability-page-1" class="page"><article id="post-105881">
	<!-- .entry-header -->

	<div>
		<p><em>By Tyler Sorensen and Heidy Khlaaf</em></p>
<p>We are disclosing LeftoverLocals: a vulnerability that allows recovery of data from GPU local memory created by another process on Apple, Qualcomm, AMD, and Imagination GPUs. LeftoverLocals impacts the security posture of GPU applications as a whole, with particular significance to LLMs and ML models <a href="https://twitter.com/AMD/status/1744831880241750112">run on impacted GPU platforms</a>. By recovering local memory—an optimized GPU memory region—we were able to build a PoC where an attacker can listen into another user’s interactive LLM session (e.g., llama.cpp) across process or container boundaries, as shown below:</p>

<p><strong>Figure 1:</strong> An illustration of how LeftoverLocals can be used to implement an attack on an interactive LLM chat session. The LLM user (left) queries the LLM, while a co-resident attacker (right) can listen to the LLM response.</p>
<p>LeftoverLocals can leak ~5.5 MB per GPU invocation on an AMD Radeon RX 7900 XT which, when running a 7B model on llama.cpp, adds up to ~181 MB for each LLM query. This is enough information to reconstruct the LLM response with high precision. The vulnerability highlights that many parts of the ML development stack have unknown security risks and have not been rigorously reviewed by security experts.</p>
<div id="attachment_105917"><p><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-105917" data-attachment-id="105917" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig1/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=1090%2C772&amp;ssl=1" data-orig-size="1090,772" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="figfig1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=300%2C212&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=690%2C489&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=543%2C385&amp;ssl=1" alt="" width="543" height="385" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=1024%2C725&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=300%2C212&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=768%2C544&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?w=1090&amp;ssl=1 1090w" sizes="(max-width: 543px) 100vw, 543px" data-recalc-dims="1"/></p><p id="caption-attachment-105917"><strong>Figure 2:</strong> LeftoverLocals logo: what leftover data is your ML model leaving for another user to steal?</p></div>
<p>This vulnerability is tracked by <a href="https://kb.cert.org/vuls/id/446598">CVE-2023-4969</a>. It was discovered by Tyler Sorensen as part of his work within the ML/AI Assurance team. Tyler Sorensen is also an assistant professor at UCSC. Since September 2023, we have been working with CERT Coordination Center on a large coordinated disclosure effort involving all major GPU vendors, including: NVIDIA, Apple, AMD, Arm, Intel, Qualcomm, and Imagination.</p>
<p>As of writing, the status of the impacted vendors, Apple, AMD, and Qualcomm are as follows:</p>
<ul>
<li><strong>Apple</strong>: Despite multiple efforts to establish contact through CERT/CC, we only received a response from Apple on January 13, 2024. We re-tested the vulnerability on January 10 where it appears that some devices have been patched, i.e., Apple iPad Air 3rd G (A12). However, the issue still appears to be present on the Apple MacBook Air (M2). Furthermore, the recently released Apple iPhone 15 does not appear to be impacted as previous versions have been. <span>Apple has confirmed that the A17 and M3 series </span><span>processors</span><span> contain fixes</span>, but we have not been notified of the specific patches deployed across their devices.</li>
<li><strong>AMD</strong>: We have confirmed with AMD that their devices remain impacted, although they continue to investigate potential mitigation plans. Their statement on the issue can be read <a href="https://www.amd.com/en/resources/product-security/bulletin/amd-sb-6010.html">here</a>.</li>
<li><strong>Qualcomm</strong>: We received notice that there is a patch to Qualcomm firmware <a href="https://lore.kernel.org/linux-firmware/20240111114032.126035-1-quic_akhilpo@quicinc.com/T/#u">v2.07</a> that addresses LeftoverLocals for some devices. However, there may still be other devices impacted at this time. A Qualcomm representative has provided the following comment: <em>“Developing technologies that endeavor to support robust security and privacy is a priority for Qualcomm Technologies. We commend Dr. Tyler Sorensen and Dr. Heidy Khlaaf from the AI/ML Assurance group at Trail of Bits for using coordinated disclosure practices and are in the process of providing security updates to our customers. We encourage end users to apply security updates as they become available from their device makers.”</em></li>
<li><strong>Imagination: <span>Despite not observing LeftoverLocals ourselves across the Imagination GPUs that we tested, Google has confirmed that some Imagination GPUs are indeed impacted. Imagination <a href="https://www.imaginationtech.com/gpu-driver-vulnerabilities/">released a fix</a> in their latest DDK release, 23.3, made available to customers in December 2023.</span></strong></li>
</ul>
<p>Further details are discussed in “Coordinated disclosure,” and a list of tested and impacted devices can be found in “Testing GPU platforms for LeftoverLocals.” Other vendors have provided us the following details:</p>
<ul>
<li><strong>NVIDIA</strong>: confirmed that their devices are not currently impacted. One reason for this could be that researchers have explored various memory leaks on NVIDIA GPUs <a href="https://arxiv.org/abs/1305.7383">previously</a>, and thus, they are aware of these types of issues.</li>
<li><strong>ARM</strong>: also confirmed that their devices are not currently impacted.</li>
</ul>
<p>While we did not hear a response from these vendors, we tested at least one GPU from them and did not observe that they were impacted: <strong>Intel</strong>.</p>
<h3>Exploit brief</h3>
<p>GPUs were initially developed to accelerate graphics computations. In this domain, performance is critical, and previously uncovered security issues have generally not had any significant consequences on applications. Historically, this entailed that GPU hardware and software stacks iterated rapidly, with frequent major architecture and programming model changes. This has led to complex system stacks and vague specifications. For example, while CPU ISAs have volumes of documentation, NVIDIA simply provides a few short <a href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#instruction-set-reference">tables</a>. This type of vague specification has led to alarming issues, both <a href="https://users.soe.ucsc.edu/~tsorensen/files/asplos2015.pdf">previously</a> and currently, as LeftoverLocals exemplifies.</p>
<h4>Exploitation requirements</h4>
<p>This is a co-resident exploit, meaning that a threat actor’s avenue of attack could be implemented as another application, app, or user on a shared machine. The attacker only requires the ability to run GPU compute applications, e.g., through OpenCL, Vulkan, or Metal. These frameworks are well-supported and typically do not require escalated privileges. Using these, the attacker can read data that the victim has left in the GPU local memory simply by writing a GPU kernel that dumps uninitialized local memory. These attack programs, as our code demonstrates, can be less than 10 lines of code. Implementing these attacks is thus not difficult and is accessible to amateur programmers (at least in obtaining stolen data). We note that it appears that browser GPU frameworks (e.g., WebGPU) are not currently impacted, as they insert dynamic memory checks into GPU kernels.</p>
<p>Unless the user inspects the application’s low-level GPU source-code, it is not possible for them to uncover if their application is utilizing GPU local memory; this matter is further complicated as the GPU code is often hidden deep in library calls, at low levels of deep software stacks (e.g., for ML). Overall, there are very limited ways to observe that an attacker is currently stealing data, or has stolen data. This attack hinges on the attacker reading uninitialized memory on the GPU, and while this is technically undefined behavior, it is not currently checked dynamically, or logged. Any additional defenses would be quite invasive, e.g., performing code analysis on GPU kernels to check for undefined behavior.</p>
<p>We have <a href="https://github.com/trailofbits/LeftoverLocalsRelease">released a PoC</a> that exploits this vulnerability, and the sections below describe how it works.</p>
<h4>User mitigations</h4>
<p>Given the lack of comprehensive patches across impacted GPU vendors, LeftoverLocals can be defended by modifying the source code of all GPU kernels that use local memory. Before the kernel ends, the GPU threads should clear memory (e.g., store 0s) to any local memory memory locations that were used in the kernel. Additionally, the users should ensure the compiler doesn’t remove these memory-clearing instructions away (e.g., by annotating their local memory as volatile), as the compiler may detect that the cleared memory is not used later in the kernel. This is difficult to verify because GPU binaries are typically not stored explicitly, and there are very few GPU binary analysis tools. Because of reasons like this, we note that this mitigation may be difficult for many users, and we discuss this further in “Mitigations” below.</p>
<h2>The vulnerability: LeftoverLocals</h2>
<p>In this section we describe the vulnerability, named LeftoverLocals, and the corresponding exploit in more detail. We then detail our testing campaign across a wide variety of GPU devices, which found that GPUs from AMD, Apple, and Qualcomm are vulnerable to LeftoverLocals. For those unfamiliar with GPU architecture and terminology, we provide a more in-depth level-setter in “Background: How GPUs work.” We also note that while GPU memory leaks are not <a href="https://arxiv.org/abs/1305.7383">new</a> (a further discussion follows below), LeftoverLocals has demonstrated both deeper impact and wider breadth than previously discovered vulnerabilities.</p>
<p>At a high level, we found that several GPU frameworks do not sufficiently isolate memory in the same way that it is traditionally expected in CPU-based frameworks. We have observed that on impacted GPUs, it is possible for one kernel—potentially from another user that is co-resident on the same machine—to observe values in local memory that were written by another kernel. Thus, an attacker who has access to a shared GPU through its programmable interface (e.g., OpenCL) can steal memory from other users and processes, violating traditional process isolation properties. This data leaking can have severe security consequences, especially given the rise of ML systems, where local memory is used to store model inputs, outputs, and weights.</p>
<p>Previous <a href="https://arxiv.org/abs/1305.7383">academic work</a> showed that NVIDIA GPUs leaked memory across processes through a variety of memory regions, including local memory. However, they examined only GPUs from NVIDIA (and the results from this paper may be part of the reason why we didn’t observe LocalLeftovers on NVIDIA GPUs). They also did not discuss the impact on widely deployed use-cases, such as ML. Other works have shown how GPUs leak graphics data, and that a co-resident attacker can reconstruct partial visual information from another process (see some examples documented <a href="https://ieeexplore.ieee.org/document/6956554">here</a>, <a href="https://arxiv.org/pdf/1605.06610.pdf">here</a>, and <a href="https://www.hertzbleed.com/gpu.zip/">here</a>). Despite these prior works, LeftoverLocals shows that many GPUs remain vulnerable to local memory leaks and that this vulnerability can be exploited in co-resident attacks on important ML applications.</p>
<p>Overall, this vulnerability can be illustrated using two simple programs: a Listener and a Writer, where the writer stores canary values in local memory, while a listener reads uninitialized local memory to check for the canary values. The Listener repeatedly launches a GPU kernel that reads from uninitialized local memory. The Writer repeatedly launches a GPU kernel that writes canary values to local memory. Below, we demonstrate how each of these operations is carried out.</p>
<p><strong><em>The Listener</em></strong>: The Listener launches a GPU kernel that reads from uninitialized local memory and stores the result in a persistent main memory region (i.e., global memory). This can be accomplished with the OpenCL kernel below:</p>
<pre><span>__kernel</span> void listener(__global <span>volatile</span> int *<span>dump</span>) {
  local <span>volatile</span> int lm[LM_SIZE];
  for (int i = <span>get_local_id(0)</span>; i &lt; LM_SIZE; i+= <span>get_local_size(0)</span>) {
    <span>dump</span>[((LM_SIZE * <span>get_group_id(0)</span>) + i)] = lm[i];
  }
}</pre>
<p>The keyword <span><code>__kernel</code></span> denotes that this is the GPU kernel function. We pass a global memory array <span><code>dump</code></span> to the function. Whatever the kernel writes to this array can be read later by the CPU. We statically declare a local memory array lm with a predefined size LM_SIZE (which we set to be the max size of local memory for each GPU we test). This program technically contains undefined behavior, as it reads from uninitialized local memory. Because of this, we use the <span><code>volatile</code></span> qualifier to suppress aggressive compiler optimizations that might optimize away the memory accesses. In fact, our code contains a few more code patterns included to further stop the compiler from optimizing away our memory dump. This process is more of a trial-and-error process than a science.</p>
<p>For each loop iteration, the invocation (thread) is read from a location in local memory, and that location is dumped to a unique location in the <span><code>dump</code></span> array. The only tricky part of this code is the indexing, because local memory is disjointed across workgroups, so workgroup local IDs need to be mapped to a unique global ID in <span><code>dump</code></span>. The process utilizes <span>built-in identifiers</span> to achieve this, which are documented <a href="https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_C.html#work-item-functions">here</a>. At the end of the kernel, dump contains every value that was stored in local memory when the listener kernel started executing. Because dump is in the global memory region, it can be examined by the CPU host code to check for canary values.</p>
<p><strong><em>The Writer</em></strong>: On the other hand, the Writer launches a kernel that writes a canary value to local memory (for example, this work uses the value 123). We show an example of the OpenCL kernel code below:</p>
<pre>__kernel void writer(__global volatile int *<span>canary</span>) {
  local volatile int lm[LM_SIZE];
  for (uint i = get_local_id(0); i &lt; LM_SIZE; i+=get_local_size(0)) {
    lm[i] = <span>canary</span>[i];
  }
}
</pre>
<p>This code is very similar to the Listener, except that rather than dumping local memory, we are writing a value. In this case, we are writing a value from an array <span><code>canary</code></span>. We use an extra array so that the compiler does not optimize away the memory write (as it is prone to do with constant values). At the end of the kernel, the writer has filled all available local memory with the <span>canary</span> values.</p>
<p>The CPU programs for both the Listener and the Writer launch their respective kernels repeatedly. In the case of the listener, at each iteration, the CPU analyzes the values observed in the local memory and checks for the canary value. On a server, these two programs can be run by different users or in different Docker containers. On a mobile device, these routines can be run in different apps. The apps can be swapped in and out of focus to alternate reading and writing. <em>If the Listener can reliably read the canary values, then we say that the platform is vulnerable to LeftoverLocals.</em></p>
<p>The following animation shows how the listener and writer interact, and how the listener may observe values from the writer if local memory is not cleared.</p>

<p><strong>Figure 3:</strong> A Listener and a Writer processes, where the writer stores canary values in local memory, while a listener reads uninitialized local memory to check for the canary values</p>
<h3>Listening to LLM responses</h3>
<p>In this section, we provide an overview of how LeftoverLocals can be exploited by a malicious actor (an attacker) to listen to another user’s (the victim) LLM responses on a multi-tenant GPU machine, followed by a detailed description of the PoC.</p>
<p>At a high level, both actors are executed as co-resident processes. The attack process implements the listener described above, with the additional steps of comparing the stolen values to various fingerprints. The victim process is unknowingly the writer, where instead of canary values, the values being written are sensitive components of an interactive LLM chat session. The attack ultimately follows two steps:</p>
<ul>
<li>The attack process fingerprints the model that the victim process is using by repeatedly dumping (i.e., listening) to the leftover local memory, which, in this scenario, consists of sensitive components of linear algebra operations used by the victim in the LLM model architecture.</li>
<li>The attacker then repeatedly listens to the victim’s process again, specifically seeking for an LLM to execute the output layer, which can be identified using weights or memory layout patterns from the earlier fingerprinting.</li>
</ul>
<p>Note that the output layer is a matrix-vector multiplication with two inputs: the model weights, and the layer input—in other words, the values derived from the user input that propagated through the earlier levels of the deep neural network (DNN). Given that the model weights of the output layer are too large to comprehensively steal, an attacker can inspect available open-source models to fully obtain the weights through the exposed model fingerprint. We found that the second input to the last layer (i.e., the layer input) is subsequently small enough to fit into local memory. Thus, the entire layer input can be stolen, and the attacker can reproduce the final layer computation to uncover the final result of the DNN.</p>
<div id="attachment_105919"><p><img decoding="async" aria-describedby="caption-attachment-105919" data-attachment-id="105919" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig2/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=1999%2C909&amp;ssl=1" data-orig-size="1999,909" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="figfig2" data-image-description="" data-image-caption="&lt;p&gt;Figure 4: Steps of the PoC exploit whereby an attacker process can uncover data to listen to another user’s interactive LLM session with high fidelity&lt;/p&gt;
" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=300%2C136&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=690%2C314&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=690%2C314&amp;ssl=1" alt="" width="690" height="314" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1024%2C466&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=300%2C136&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1536%2C698&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1200%2C546&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?w=1999&amp;ssl=1 1999w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"/></p><p id="caption-attachment-105919"><strong>Figure 4:</strong> Steps of the PoC exploit whereby an attacker process can uncover data to listen to another user’s interactive LLM session with high fidelity</p></div>
<p>We note that this is a fairly straightforward attack, and with further creativity and ingenuity, a threat actor may be able to construct further complex and sophisticated malicious scenarios that may compromise ML applications in more severe ways. Below we provide a detailed description of the PoC, and the configuration and testing carried out on various GPU platforms to uncover their susceptibility to LeftoverLocals.</p>
<p><strong><em>Our configuration</em></strong>: We outline our configuration in the table below. Our attack builds on the llama.cpp LLM due to its simplicity and variety of support for GPU acceleration. In our example we use a large discrete GPU that we found to be susceptible to LeftoverLocals: the AMD Radeon RX 7900 XT. We configure llama.cpp to use OpenCL for GPU acceleration, which uses the CLBLAST linear algebra library. We use the wizardLM-7B.ggmlv3.q5_0.bin model, which <a href="https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/main">can be obtained</a> from Hugging Face. This model was selected due to its reasonable size, which enabled rapid prototyping and analysis; however, this attack is transferable to many different models. In our threat model, we assume that the victim is using the LLM in an interactive chat session.</p>
<p><img decoding="async" data-attachment-id="105921" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig3/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=1258%2C504&amp;ssl=1" data-orig-size="1258,504" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="figfig3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=300%2C120&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=690%2C276&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=600%2C241&amp;ssl=1" alt="" width="600" height="241" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=1024%2C410&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=300%2C120&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=768%2C308&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=1200%2C481&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?w=1258&amp;ssl=1 1258w" sizes="(max-width: 600px) 100vw, 600px" data-recalc-dims="1"/></p>
<p><strong><em>Modification</em></strong>: The attack requires an optimized GPU implementation of matrix-vector multiplication. We found that the current matrix-vector multiplication in llama.cpp (which does not call into CLBLAST) is not implemented in an optimized idiomatic way. It stores partial dot product results in local memory and then combines them at the end. While there is a more complex approach using linear algebra to achieve our same results, for the simplicity of our PoC and demonstration, we replace the llama.cpp matrix-vector multiplication with our own that is more idiomatic (following best GPU programming programming practices).</p>
<p><strong><em>Step 1—Fingerprinting the model</em></strong>: An attacker can fingerprint a model if it can listen to several inference queries from the victim. In our configuration, the GPU contains roughly 5MB of local memory. The model has roughly 33 layers, each of them consisting of a matrix multiplication operation. Matrix multiplication is often optimized on GPUs by using tiling: an approach that subdivides the matrices into small matrices, performs the multiplication, and then combines the results (as <a href="https://cnugteren.github.io/tutorial/pages/page4.html">detailed here</a>). In many optimized libraries, including CLBLAST, local memory is used to cache the smaller matrices. Thus, for every layer, the attacker can steal ~2.5MB of weights, and ~2.5MB of the inputs. While this is a significant amount of data, we note that it is not enough to reconstruct the entire computation. Many of these layers have weights and inputs that are 100s of MB large.</p>
<p>However, for a whole inference computation (33 layers), the attacker can steal around 80MB of the weights, which is sufficient to fingerprint the model (assuming the user is using an open-source model, such as one that can be found on Hugging Face). Given this, we assume that it is a straightforward task to fingerprint the model, and thus for the attacker to obtain the full model being used by the victim.</p>
<p><strong><em>Step 2—Listening to the LLM output</em></strong>: The attacker can then turn their attention to the output layer of the DNN. In our configuration, we found that the output layer is a matrix-vector multiplication, rather than a matrix-matrix multiplication. The weights matrix is large (~128MB), but the input vector is quite small (~4KB). However, given that the attacker has fingerprinted the model in step 1, the attacker does not need to comprehensively steal the weights as they are available from the fingerprinted model.</p>
<p>Matrix-vector multiplication has a different GPU implementation than matrix-matrix multiplication. In the case where the input vector fits in local memory, the most performant implementation is often to cache the input vector in local memory, as it is used repeatedly (i.e., for repeated dot products). Because the input vector is stored entirely in local memory, the attacker can steal this entire vector. In determining whether the attacker has found local memory from the output layer, we discovered that the attacker could simply look for 4KB of floating point values with zeros on either side. In our testing, this unique fingerprint was associated with the output layer nearly every single time. For different models and different GPUs, this fingerprint will likely have to be recalibrated.</p>
<p><strong><em>Putting it together</em></strong>: With an attacker in possession of both the weights and the input vector, they can perform the final computation and obtain the result of the inference. This allows the attacker to reproduce the output of the victim’s LLM chat session with high fidelity, as demonstrated in the introduction. In practice, we tuned the attacker to dump the local memory very efficiently (that is, by using only a small number of threads and requiring a small amount of memory). This allows the attacker to listen to long chat queries with only a small number of noticeable artifacts. Some of the artifacts observed include:</p>
<ul>
<li><em>Duplicate tokens</em>: This occurs when the attacker steals the same output layer twice due to circumstances such as the attacker process being scheduled twice in a row, thus the LLM was not scheduled to compute its next token.</li>
<li><em>Missing tokens</em>: This occurs when the attacker kernel isn’t scheduled at the right time, i.e., immediately after the output layer computation kernel.</li>
<li><em>Incorrect tokens</em> outputted occurring due to:</li>
<li>the attacker mis-identifying a stolen set of data to be the last layer. In this case, it will print a junk token.</li>
<li>Production of a token that is “close” to the original output, even if it is not exact. That is, the attacker may be unable to steal the exact token embedding at the target layer. This results in a corrupted token embedding which, when decoded, is semantically similar (in the word2vec sense) to the original token. As an example, in the GIF provided at the beginning, the attacker extracts the incorrect word “Facebook”, which is semantically similar to other Named Entities tokens (like “Google”, and “Amazon”) in the generated text.</li>
</ul>
<p>Despite these discrepant artifacts, the stolen text is more than sufficient to uncover the LLM response. Additionally, the attacker can be further tuned by, for example, having multiple threads launch the listener kernel or by having a more precise fingerprint of the last layer.</p>
<h3>Testing GPU platforms for LeftoverLocals</h3>
<p>Given the diversity of the devices we tested, there exists several applications that can test for LeftoverLocals written in a variety of frameworks:</p>
<ul>
<li><strong>Vulkan Command Line</strong>: A command line application using Vulkan. The kernel is written in OpenCL and compiled to SPIR-V using <a href="https://github.com/google/clspv">clspv</a>. It uses a simple Vulkan wrapper called <a href="https://github.com/ucsc-chpl/easyvk">EasyVK</a>.</li>
<li><strong>OpenCL Command Line</strong>: A command line application that uses the OpenCL framework.</li>
<li><strong>Apple App</strong>: An Apple app that can be deployed on iOS or Mac OS. It targets the GPU using Apple’s Metal framework.</li>
<li><strong>Android App</strong>: An Android app that uses Vulkan to target mobile GPUs. The code uses Vulkan’s C API (through EasyVK again) using JNI. The kernels are the same as in the Vulkan command line app: they are written in OpenCL and compiled to SPIR-V using clspv.</li>
</ul>
<p>Using the above programs, we tested 11 devices spanning seven GPU vendors (and multiple GPU frameworks in some cases). We observed LeftoverLocals on devices from three of the vendors (Apple, Qualcomm, and AMD). The amount of memory leaked depends on the size of the GPU. Larger GPUs contain more physical memory, and thus, leak more data. For the larger GPUs (e.g., an AMD Radeon RX 7900 XT), we found that we can leak over ~5MB per kernel. The following tables outlines the system info for the GPUs we were able to observe LeftoverLocals (QC refers to Qualcomm):</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105923" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig4/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=1576%2C404&amp;ssl=1" data-orig-size="1576,404" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="figfig4" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=300%2C77&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=690%2C177&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=690%2C177&amp;ssl=1" alt="" width="690" height="177" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1024%2C262&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=300%2C77&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=768%2C197&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1536%2C394&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1200%2C308&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?w=1576&amp;ssl=1 1576w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"/></p>
<p>For some devices, specifically those from Arm, we were not able to observe the canary value from the Writer in the Listener, but we did observe non-zero data. Representatives from Arm reviewed our observations and concluded that although these values are not zero, they are not from a memory leak.</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105924" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig5/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=1508%2C216&amp;ssl=1" data-orig-size="1508,216" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="figfig5" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=300%2C43&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=690%2C99&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=690%2C99&amp;ssl=1" alt="" width="690" height="99" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=1024%2C147&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=300%2C43&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=768%2C110&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=1200%2C172&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?w=1508&amp;ssl=1 1508w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"/></p>
<p>Additionally, we tested some GPUs from NVIDIA, Intel, and Imagination. For these devices, we observed only zeros in local memory, and thus did not observe LeftoverLocals. <span>It is unclear if all their devices are not impacted. For example, although we did not observe the issue on our </span><span>Imagination device, Google notified us that they were able to observe it on other Imagination devices.</span></p>
<p><img loading="lazy" decoding="async" data-attachment-id="105925" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig6/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=1596%2C272&amp;ssl=1" data-orig-size="1596,272" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="figfig6" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=300%2C51&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=690%2C118&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=690%2C118&amp;ssl=1" alt="" width="690" height="118" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1024%2C175&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=300%2C51&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=768%2C131&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1536%2C262&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1200%2C205&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?w=1596&amp;ssl=1 1596w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"/></p>
<p>The following YouTube video demonstrates the different interfaces and examples of LocalLeftovers—namely the LLM PoC attack, covert communication channels, and searching for canary values—on a few different platforms using a few different applications.</p>
<p><iframe loading="lazy" title="YouTube video player" src="https://www.youtube.com/embed/g2A7GvbnItg?si=w0tvRgk2Kn1YdcX7" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p><strong><em>Vulnerable environments</em></strong>: An attack program must be co-resident on the same machine and must be “listening” at the same time that the victim is running a sensitive application on the GPU. This could occur in many scenarios: for example, if the attack program is co-resident with the victim on a shared cloud computer with a GPU. On a mobile device, the attack could be implemented in an app or a library. Listening can be implemented efficiently, and thus can be done repeatedly and constantly with almost no obvious performance degradation.</p>
<p>Next, we briefly discuss other environments where GPUs are either deployed or where an attacker might have access to sensitive information. Although it appears that some current systems (e.g., WebGPU) are not currently impacted, the ever-growing prevalence of ML and the diversity of modern GPUs mean that the next iteration of these systems (or other near-future systems) may be severely compromised by these types of vulnerabilities.</p>
<ul>
<li><strong>Cloud providers</strong>: Cloud providers (e.g., AWS and Azure) are unlikely to provide shared GPU instances, especially if users have dedicated access to the GPU machine. In other cases, GPUs could be shared using very conservative GPU VM technology (such as NVIDIA’s vGPU or MxGPU), which physically partitions the GPU and therefore prevents users from sharing GPU resources (e.g., local memory). Given this, many current cloud GPU systems may not currently be vulnerable to LeftoverLocals; however, we do not have conclusive evidence to determine this given the general lack of visibility into the specification and implementation of these systems. We note that we have observed LeftoverLocals on multi-user Linux servers, as well as on desktop (Windows and Mac) systems through traditional multi-processing. This includes Docker containers on these systems.</li>
<li><strong>Mobile applications</strong>: In our experiments and explorations in the mobile domain, we were able to run concurrent GPU processes (from different apps on iOS or Android) only in very specific instances. That is, we were not able to run a GPU process (e.g., from a malicious listener app) in the background while other apps (e.g., the victim) were run in the foreground. As with our analysis of cloud providers, we were unable to find clear documentation that explicitly detailed these constraints, and so we cannot definitively claim whether they are vulnerable. However, as seen in the video above, LeftoverLocals can be exploited either when a malicious listener app is run side-by-side with a victim app, or if the malicious listener app is quickly swapped from the background into the foreground from a victim app.</li>
<li><strong>Remote attacks</strong>: We preliminarily investigated the possibility of attacks originating from websites (e.g., those hosted by a remote attacker). To our knowledge, web applications do not have the low-level features required to listen to local memory using GPU graphics frameworks, such as WebGL. We note that the new WebGPU framework does provide low-level capabilities that allow a webpage to access local memory. Conservatively, WebGPU initializes and performs dynamic array bounds checking on local memory (and global memory), which mitigates this vulnerability. However, these checks cause significant overhead, as documented in discussions like <a href="https://github.com/gpuweb/gpuweb/issues/1202">this one</a>. To test this further, our code repo contains a simple listener in WebGPU. As expected, we have only observed zeros in local memory, even on devices that are vulnerable to LeftoverLocals through other frameworks. However, GPU compilers are <a href="https://medium.com/@afd_icl/crashes-hangs-and-crazy-images-by-adding-zero-689d15ce922b">known to be fragile</a>, and it is not difficult to imagine finding a compiler bug that could somehow bypass these checks (especially using fuzzing techniques). Our position is that LocalLeftovers should be addressed at a lower level (e.g., the driver).</li>
</ul>
<p><strong>How GPU vendors can resolve this vulnerability</strong>: To defend against LocalLeftovers, GPUs should clear their local memory between kernel calls. While this could cause some performance overhead, our experiments show that many GPU vendors (e.g., NVIDIA, Intel) currently appear to provide this functionality. It even appears that some of this functionality is provided for impacted GPUs. For example, Mesa drivers <a href="https://github.com/Mesa3D/mesa/blob/957009978ef6d7121fc0d710d03bc20097d4d46b/src/amd/vulkan/radv_shader.c#L709">for AMD GPUs</a> clears local memory after a compute kernel launch. However, this approach has a fundamental flaw that makes it vulnerable to LeftoverLocals: this memory wipe is done with a separate kernel, thus, the GPU kernel queue may contain a malicious listener between the computation kernel and the local memory wipe, allowing the listener to steal memory. Instead, the computation kernel and the local memory wipe need to occur atomically, i.e., without allowing any other kernel to be interleaved between them. Otherwise, a user may attempt to preemptively defend themselves against LeftoverLocals as described in the next section.</p>
<p><strong>Mitigations</strong>: In light of a lack of comprehensive patches across impacted GPU vendors, LeftoverLocals can be defended by modifying the source code of all GPU kernels that use local memory. As we’ve previously noted, before the kernel ends, the GPU threads should store 0 to any local memory locations that were used in the kernel. Given that GPU tasks are typically interleaved at the kernel boundary, this will prevent another user from being able to read leftover values. We note that this mitigation may be difficult for many users, especially because GPU code is often buried deep in complex software stacks (e.g., for ML). Furthermore, the GPU code may be part of a highly optimized library (e.g., ML linear algebra routines). In these cases, it is very difficult to identify how local memory is used, and even more difficult to modify the kernel to zero it out. It may be possible to augment a compiler to add this functionality, similar to how WebGPU handles GPU memory accesses (described above). These mitigations do have a performance overhead that should be taken into account. Another blunt mitigation involves simply avoiding multi-tenant GPU environments.</p>
<h2>Impact on LLMs and GPU platforms</h2>
<h3>LLM security</h3>
<p>Our PoC attack examines only one application: an interactive open-source LLM session. However, with a little creativity, attackers could likely target many GPU applications, including those used within privacy-sensitive domains. Our motivation stems from the recent increased use and support of open-source models, often accompanied by claims that their “openness” inherently entails safety and security through transparency. A recent article in <a href="https://www.nature.com/articles/d41586-023-03803-y">Nature</a> even alleges that only open-source generative AI models can “safely” revolutionize health care, a safety-critical domain. Yet, even if open-source models provide the opportunity to be rigorously audited and assessed (<a href="https://blog.trailofbits.com/2023/11/15/assessing-the-security-posture-of-a-widely-used-vision-model-yolov7/">which they have yet to be</a>), their deployment still hinges on a closed-source stack (i.e., GPUs). And as demonstrated by LeftoverLocals, open-source LLMs are particularly susceptible to our vulnerability given our ability to fingerprint these models to obtain remaining weights as needed. Indeed, we have already observed announcements regarding the deployment of open-source models in collaboration with impacted GPU vendors, including <a href="https://twitter.com/AMD/status/1744831880241750112">Hugging Face’s collaboration with AMD</a>, <a href="https://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu-rich-enterprise-llms">Lamini’s deployment on AMD GPUs</a>, and the <a href="https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi">Qualcomm and Meta partnership</a> for edge devices.</p>
<p>Generally, the introduction of ML poses new attack surfaces that traditional threat models do not account for, and that can lead to implicit and explicit access to data, model parameters, or resulting outputs, increasing the overall attack surface of the system. It is crucial to identify and taxonomize novel classes of failure modes that directly impact ML models, in addition to novel threats that can compromise the ML Ops pipeline, as we have demonstrated with LeftoverLocals. We discuss GPU-specific threat implications in the following section.</p>
<h3>GPU providers, applications, and vendors</h3>
<p>While many platforms are not currently impacted (see Vulnerable environments), we emphasize that the GPU compute landscape is evolving rapidly. As some examples: <a href="https://cloud-gpus.com/">a growing number of GPU cloud providers</a> have various policies and available configurations; and GPU programming frameworks, such as Vulkan and Metal, are well-supported on mainstream platforms, and can be used in apps without requiring extra privileges. While these developments are exciting, they increase the threat potential of GPU vulnerabilities, as LeftoverLocals illustrates. As far as we are aware, there is no unified security specification for how GPUs are required to handle sensitive data, and no portable test suite to check if systems are vulnerable to simple memory leaks, like LeftoverLocals. Thus, GPU compute environments should be rigorously scrutinized when used for processing any type of sensitive data.</p>
<p>As mentioned above, while we focus on LLM applications, GPU local memory is one of the first tools that a GPU developer uses when optimizing an application. Although other attacks would likely require analyzing the victim’s GPU kernel code to identify local memory usage, other attacks are likely possible in GPU compute domains, such as image processing and scientific computing. It will likely be increasingly difficult for users to detect and defend against these attacks since it’s unlikely they will know if their application is vulnerable to LeftoverLocals; this would require knowing the details of the exact GPU kernel code, which are often hidden away in highly optimized linear algebra libraries (e.g., <a href="https://github.com/CNugteren/CLBlast">CLBLAST</a>). Additionally, an overall lack of specification in up-and-coming GPU platforms makes it difficult to determine whether the compiler or runtime will use impacted memory regions without the user knowing. For example, Apple GPUs have a new caching mechanism, called <a href="https://www.digitaltrends.com/computing/apple-dynamic-caching-explained/">dynamic caching</a>, that does not have a clear specification regarding if local memory regions are being used for other purposes.</p>
<h2>Coordinated disclosure</h2>
<p>Since September 2023, we have been working CERT/CC on a large coordinated disclosure involving all major GPU vendors, including NVIDIA, Apple, AMD, Arm, Intel, Qualcomm, and Imagination. Trail of Bits provided vendors a total of 125 days to test their products and provide remediations. The coordination gradually grew to include software stakeholders, including Google, Microsoft, and others, which allowed us to understand how LocalLeftovers impacts privacy requirements and impact at different stages in the ML supply chain. Apple did not respond or engage with us regarding the disclosure.</p>
<p>A high-level timeline of the disclosure is provided below:</p>
<ul>
<li>September 8, 2023: Trail of Bits submitted report to the CERT/CC</li>
<li>September 11, 2023: CERT/CC acknowledged the submission of LeftoverLocals and began the process of vendor outreach and CVE assignment with a preliminary disclosure date of December 11, 2023</li>
<li>September 14, 2023: AMD acknowledged the CERT disclosure</li>
<li>September 15, 2023: Qualcomm acknowledged the CERT disclosure</li>
<li>September 22, 2023: The case report was shared with Khronos and OpenCL working group</li>
<li>September 29, 2023: NVIDIA acknowledged disclosure and confirmed they were not affected by the vulnerability</li>
<li>November 22, 2023: ToB extended release of embargo to January 16, 2024 to accommodate for vendor requests for further time</li>
<li>January 11, 2024: We received a notice that Qualcomm provided a patch to their firmware that addresses this issue only for some of their devices. Additionally, Google noted that ChromeOS Stable 120 and LTS 114 will be released on January 16 to include AMD and Qualcomm mitigations.</li>
<li><span>January 13, 2024: Apple confirmed that the A17 and M3 series processors contain fixes to the vulnerability.</span></li>
<li><span>January 14, 2024: Google notified us that they observed that that some Imagination GPUs are impacted.</span></li>
<li>January 16, 2024: Embargo lift and public disclosure of LeftoverLocals</li>
</ul>
<h2>Moving forward</h2>
<p>Now that GPUs are being used in a wide range of applications, including privacy sensitive applications, we believe that the wider GPU systems community (vendors, researchers, developers) must work towards hardening the GPU system stack and corresponding specifications. This should be accomplished through robust, holistic specifications that describe both GPU programs’ behavior and how GPU devices integrate with the rest of the system stack (e.g., the OS or hypervisor). Furthermore, these specifications should be rigorously tested to account for the diversity of GPU systems and safety requirements of diverse application domains. Looking forward, a wide variety of <a href="https://www.theinformation.com/articles/the-twelve-startups-battling-for-a-slice-of-nvidias-pie?utm_source=ti_app">new AI chips</a> are being developed and will require rigorous security analysis.</p>
<p>There are positive developments in this direction. For example, AMD’s <a href="https://www.amd.com/en/products/software/rocm.html">ROCm</a> stack is open, and thus available for independent rigorous evaluation, and the Khronos Group has <a href="https://www.khronos.org/syclsc">safety critical specification groups</a>. Additionally, cross-vendor programming frameworks, such as Vulkan, have been incredibly useful for writing portable test suites, as opposed to single-vendor programming frameworks.</p>
<p>While GPU security and privacy guarantees are scattered and scarce, the Vulkan <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#fundamentals-validusage">specification</a> outlines a reasonable definition of security for GPU platforms to adhere to—a definition that several platforms clearly violate, as our results show:</p>
<blockquote><p><em>… implementations must ensure that […] an application does not affect the integrity of the operating system[…]. In particular, any guarantees made by an operating system about whether memory from one process can be visible to another process or not must not be violated by a Vulkan implementation for any memory allocation.</em></p></blockquote>
<p><span>Given the role of Khronos specifications in this result, we included the Khronos Group in the coordinated disclosure. They connected us with representatives of various impacted vendors, and engaged in fruitful discussions about security specifications and testing. Prior to the release, Khronos released this statement in support of this work:</span></p>
<blockquote><p><i><span>Khronos welcomes the work by Tyler Sorensen and Trail of Bits to increase security around the usage of Khronos APIs and have been working closely with them for several months to ensure that API implementers are aware and able to act on any issues. Khronos is also diligently exploring additional actions relating to API specifications, conformance testing, and platform vendor cooperation to continually strengthen safety and security when using Khronos compute and rendering APIs. – Neil Trevett, Khronos President</span></i></p></blockquote>
<p>With the dust settling, our position is the following: given the wide diversity of GPUs and their critical importance in enabling machine learning applications, these devices, and their ecosystems, are in need of (1) a detailed threat model that considers the various types of data processed on GPUs and how this data might be compromised; (2) an exploration of the GPU execution stack to determine where and how GPU security properties should be specified and implemented; and (3) significant testing and auditing to fortify GPU ecosystem, which is the computational foundation of machine learning.</p>
<p><em><span>For full transparency, we note that Tyler Sorensen has been an invited member of the Khronos group (sponsored by Google) since 2019, and participates in the memory model technical specification group.</span></em></p>
<p><strong>Acknowledgements</strong>: We thank Max Ammann, Dominik Czarnota, Kelly Kaoudis, Jay Little, and Adelin Travers for their insightful comments and feedback on the vulnerability, PoC, and throughout the disclosure process. We also thank the Khronos Group for discussing technical specification details with us, and providing an avenue for us to engage with many vendors. We thank CERT/CC, specifically Vijay Sarvepalli and Ben Koo, for organizing the coordinated disclosure, especially considering the potential breadth of the vulnerability. Thanks to Adam Sorensen and Trent Brunson for helping create the vulnerability logo. <span>Finally, t</span><span>hank you to everyone who engaged with us on this issue. T</span><span>his was a large project and we had </span><span>discussions</span><span> with many people who provided valuable insights and perspectives.</span></p>
<h2>Background: How GPUs work</h2>
<p>GPUs are massively parallel, throughput-oriented co-processors. While originally designed to accelerate graphics workloads, their design, which balances flexible programming and high computational throughput, has been highly effective in a variety of applications. Perhaps the most impactful current application domain is machine learning, where GPUs are the computational workhorse and achieve nearly all major results in this area.</p>
<p>GPUs are not only in large servers; they are in our phones, our tablets, and our laptops. These GPUs come from a variety of vendors, with almost all major hardware vendors (Apple, AMD, Arm, Qualcomm, Intel, and Imagination) producing their own GPU architecture. These GPUs are increasingly used for ML tasks, especially because doing ML locally can preserve users’ privacy, achieve lower latency, and reduce computational burdens on service providers.</p>
<p><strong>GPU architecture</strong>: GPU architecture has a parallel, hierarchical structure. At the top level, a GPU is made up of Compute Units (sometimes called Streaming Multiprocessors in NVIDIA literature). Large, discrete GPUs contain many compute units, and smaller, mobile GPUs have fewer. For example, the large AMD Radeon RX 7900 XT discrete GPU has 84 compute units, while the mobile Qualcomm Adreno 740 GPU has 8. All compute units have access to global memory. On discrete GPUs, global memory is implemented using VRAM; on integrated GPUs, global memory simply uses the CPU’s main memory.</p>
<p>Compute units encapsulate both compute and memory components. Compute units contain an array of processing elements; these simple cores are the fundamental units of computation and execute a stream of GPU instructions. In terms of memory, compute units often contain a cache for global memory, but they also contain a special region of memory called local memory. This is an optimized memory region that is shared only across processing elements in the same compute unit. This memory can be accessed with significantly less latency than global memory, but also has much smaller capacity. Different GPUs have varying amounts of local memory, typically ranging from 16KB to 64KB. For example, the AMD Radeon RX 7900 XT GPU has 84 compute units and a local memory size of 64KB; thus, the total amount of local memory on the GPU is ~5MB. Local memory is a software-managed cache: the program executing on the processing elements is responsible for loading values into local memory (e.g., values that will be repeatedly used from global memory).</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105927" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig7/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=1384%2C786&amp;ssl=1" data-orig-size="1384,786" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="figfig7" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=300%2C170&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=690%2C392&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=604%2C343&amp;ssl=1" alt="" width="604" height="343" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=1024%2C582&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=300%2C170&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=768%2C436&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=1200%2C682&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?w=1384&amp;ssl=1 1384w" sizes="(max-width: 604px) 100vw, 604px" data-recalc-dims="1"/></p>
<p><strong>GPU execution model</strong>: A GPU program, called a (GPU) kernel, is written in a shader language. Common examples are SPIR-V (Vulkan), OpenCL C, (OpenCL), and Metal Shading Language (Metal). These kernels specify a single entry point function, called the kernel function, which is executed by many invocations (i.e., GPU threads). Invocations have unique built-in identifiers (such as a global ID), which can be used to index a unique data element in a data-parallel program. Invocations are further partitioned into workgroups. Each workgroup is mapped to a compute unit (although many workgroups may execute on the same compute unit, depending on resource requirements). All invocations have access to the same global memory, but only invocations in the same workgroup will share the same local memory.</p>
<p>Applications that use the GPU often launch many short-running kernels. These kernels often correspond to basic operations, such as matrix multiplication or convolution. Kernels can then be executed in sequence; for example, each layer in a deep neural network will be a kernel execution. Local memory is statically allocated at each kernel launch and is not specified to persist across kernel calls.</p>
<p>Platforms generally do not time-multiplex different GPU kernels. That is, if multiple kernels are launched simultaneously (e.g., by different users), the GPU will execute one kernel to competition before the next kernel starts. Because GPU kernels are typically short running, sharing GPU resources at kernel boundaries saves expensive preemption overhead while also maintaining acceptable latency in practice.</p>
<p><strong>Terminology</strong>: Because this blog post focuses on portable GPU computing, it uses OpenCL GPU terminology. For readers more familiar with GPU terminology from a different framework (e.g., CUDA or Metal), we provide the following translation table:</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105930" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig8/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=1256%2C288&amp;ssl=1" data-orig-size="1256,288" data-comments-opened="1" data-image-meta="{&#34;aperture&#34;:&#34;0&#34;,&#34;credit&#34;:&#34;&#34;,&#34;camera&#34;:&#34;&#34;,&#34;caption&#34;:&#34;&#34;,&#34;created_timestamp&#34;:&#34;0&#34;,&#34;copyright&#34;:&#34;&#34;,&#34;focal_length&#34;:&#34;0&#34;,&#34;iso&#34;:&#34;0&#34;,&#34;shutter_speed&#34;:&#34;0&#34;,&#34;title&#34;:&#34;&#34;,&#34;orientation&#34;:&#34;0&#34;}" data-image-title="figfig8" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=300%2C69&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=690%2C158&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=611%2C140&amp;ssl=1" alt="" width="611" height="140" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=1024%2C235&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=300%2C69&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=768%2C176&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=1200%2C275&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?w=1256&amp;ssl=1 1256w" sizes="(max-width: 611px) 100vw, 611px" data-recalc-dims="1"/></p>

			</div><!-- .entry-content -->

	
</article></div>
  </body>
</html>
