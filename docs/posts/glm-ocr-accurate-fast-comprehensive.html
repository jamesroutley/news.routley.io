<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/zai-org/GLM-OCR">Original</a>
    <h1>GLM-OCR: Accurate Ã— Fast Ã— Comprehensive</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p><a target="_blank" rel="noopener noreferrer" href="https://seinwave.com/zai-org/GLM-OCR/blob/main/resources/logo.svg"><img src="https://seinwave.com/zai-org/GLM-OCR/raw/main/resources/logo.svg" width="40%"/></a>
</p>
<p dir="auto">
    ðŸ‘‹ Join our <a href="https://seinwave.com/zai-org/GLM-OCR/blob/main/resources/WECHAT.md">WeChat</a> and <a href="https://discord.gg/QR7SARHRxK" rel="nofollow">Discord</a> community
    </p>

<p dir="auto">GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoderâ€“decoder architecture. It introduces Multi-Token Prediction (MTP) loss and stable full-task reinforcement learning to improve training efficiency, recognition accuracy, and generalization. The model integrates the CogViT visual encoder pre-trained on large-scale imageâ€“text data, a lightweight cross-modal connector with efficient token downsampling, and a GLM-0.5B language decoder. Combined with a two-stage pipeline of layout analysis and parallel recognition based on PP-DocLayout-V3, GLM-OCR delivers robust and high-quality OCR performance across diverse document layouts.</p>
<p dir="auto"><strong>Key Features</strong></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>State-of-the-Art Performance</strong>: Achieves a score of 94.62 on OmniDocBench V1.5, ranking #1 overall, and delivers state-of-the-art results across major document understanding benchmarks, including formula recognition, table recognition, and information extraction.</p>
</li>
<li>
<p dir="auto"><strong>Optimized for Real-World Scenarios</strong>: Designed and optimized for practical business use cases, maintaining robust performance on complex tables, code-heavy documents, seals, and other challenging real-world layouts.</p>
</li>
<li>
<p dir="auto"><strong>Efficient Inference</strong>: With only 0.9B parameters, GLM-OCR supports deployment via vLLM, SGLang, and Ollama, significantly reducing inference latency and compute cost, making it ideal for high-concurrency services and edge deployments.</p>
</li>
<li>
<p dir="auto"><strong>Easy to Use</strong>: Fully open-sourced and equipped with a comprehensive <a href="https://github.com/zai-org/GLM-OCR">SDK</a> and inference toolchain, offering simple installation, one-line invocation, and smooth integration into existing production pipelines.</p>
</li>
</ul>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Download Links</th>
<th>Precision</th>
</tr>
</thead>
<tbody>
<tr>
<td>GLM-OCR</td>
<td><a href="https://huggingface.co/zai-org/GLM-OCR" rel="nofollow">ðŸ¤— Hugging Face</a></td>
<td>BF16</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">We provide an SDK for using GLM-OCR more efficiently and conveniently.</p>

<blockquote>
<p dir="auto"><a href="https://docs.astral.sh/uv/getting-started/installation/" rel="nofollow">UV Installation</a></p>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="# Install from source
git clone https://github.com/zai-org/glm-ocr.git
cd glm-ocr
uv venv --python 3.12 --seed &amp;&amp; source .venv/bin/activate
uv pip install -e ."><pre><span><span>#</span> Install from source</span>
git clone https://github.com/zai-org/glm-ocr.git
<span>cd</span> glm-ocr
uv venv --python 3.12 --seed <span>&amp;&amp;</span> <span>source</span> .venv/bin/activate
uv pip install -e <span>.</span></pre></div>

<p dir="auto">Two ways to use GLM-OCR:</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Option 1: Zhipu MaaS API (Recommended for Quick Start)</h4><a id="user-content-option-1-zhipu-maas-api-recommended-for-quick-start" aria-label="Permalink: Option 1: Zhipu MaaS API (Recommended for Quick Start)" href="#option-1-zhipu-maas-api-recommended-for-quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Use the hosted cloud API â€“ no GPU needed. The cloud service runs the complete GLM-OCR pipeline internally, so the SDK simply forwards your request and returns the result.</p>
<ol dir="auto">
<li>Get an API key from <a href="https://open.bigmodel.cn" rel="nofollow">https://open.bigmodel.cn</a></li>
<li>Configure <code>config.yaml</code>:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="pipeline:
  maas:
    enabled: true # Enable MaaS mode
    api_key: your-api-key # Required"><pre><span>pipeline</span>:
  <span>maas</span>:
    <span>enabled</span>: <span>true </span><span><span>#</span> Enable MaaS mode</span>
    <span>api_key</span>: <span>your-api-key </span><span><span>#</span> Required</span></pre></div>
<p dir="auto">That&#39;s it! When <code>maas.enabled=true</code>, the SDK acts as a thin wrapper that:</p>
<ul dir="auto">
<li>Forwards your documents to the Zhipu cloud API</li>
<li>Returns the results directly (Markdown + JSON layout details)</li>
<li>No local processing, no GPU required</li>
</ul>
<p dir="auto">Input note (MaaS): the upstream API accepts <code>file</code> as a URL or a <code>data:&lt;mime&gt;;base64,...</code> data URI.
If you have raw base64 without the <code>data:</code> prefix, wrap it as a data URI (recommended). The SDK will
auto-wrap local file paths / bytes / raw base64 into a data URI when calling MaaS.</p>
<p dir="auto">API documentation: <a href="https://docs.bigmodel.cn/cn/guide/models/vlm/glm-ocr" rel="nofollow">https://docs.bigmodel.cn/cn/guide/models/vlm/glm-ocr</a></p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Option 2: Self-host with vLLM / SGLang</h4><a id="user-content-option-2-self-host-with-vllm--sglang" aria-label="Permalink: Option 2: Self-host with vLLM / SGLang" href="#option-2-self-host-with-vllm--sglang"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Deploy the GLM-OCR model locally for full control. The SDK provides the complete pipeline: layout detection, parallel region OCR, and result formatting.</p>

<p dir="auto">Install vLLM:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv pip install -U vllm --torch-backend=auto --extra-index-url https://wheels.vllm.ai/nightly
# Or use Docker
docker pull vllm/vllm-openai:nightly"><pre>uv pip install -U vllm --torch-backend=auto --extra-index-url https://wheels.vllm.ai/nightly
<span><span>#</span> Or use Docker</span>
docker pull vllm/vllm-openai:nightly</pre></div>
<p dir="auto">Launch the service:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# In docker container, uv may not be need for transformers install
uv pip install git+https://github.com/huggingface/transformers.git

# Run with MTP for better performance
vllm serve zai-org/GLM-OCR --allowed-local-media-path / --port 8080 --speculative-config &#39;{&#34;method&#34;: &#34;mtp&#34;, &#34;num_speculative_tokens&#34;: 1}&#39; --served-model-name glm-ocr"><pre><span><span>#</span> In docker container, uv may not be need for transformers install</span>
uv pip install git+https://github.com/huggingface/transformers.git

<span><span>#</span> Run with MTP for better performance</span>
vllm serve zai-org/GLM-OCR --allowed-local-media-path / --port 8080 --speculative-config <span><span>&#39;</span>{&#34;method&#34;: &#34;mtp&#34;, &#34;num_speculative_tokens&#34;: 1}<span>&#39;</span></span> --served-model-name glm-ocr</pre></div>

<p dir="auto">Install SGLang:</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pull lmsysorg/sglang:dev
# Or build from source
uv pip install git+https://github.com/sgl-project/sglang.git#subdirectory=python"><pre>docker pull lmsysorg/sglang:dev
<span><span>#</span> Or build from source</span>
uv pip install git+https://github.com/sgl-project/sglang.git#subdirectory=python</pre></div>
<p dir="auto">Launch the service:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# In docker container, uv may not be need for transformers install
uv pip install git+https://github.com/huggingface/transformers.git

# Run with MTP for better performance
python -m sglang.launch_server --model zai-org/GLM-OCR --port 8080 --speculative-algorithm NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --served-model-name glm-ocr
# Modify the speculative config base on your device"><pre><span><span>#</span> In docker container, uv may not be need for transformers install</span>
uv pip install git+https://github.com/huggingface/transformers.git

<span><span>#</span> Run with MTP for better performance</span>
python -m sglang.launch_server --model zai-org/GLM-OCR --port 8080 --speculative-algorithm NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --served-model-name glm-ocr
<span><span>#</span> Modify the speculative config base on your device</span></pre></div>

<p dir="auto">After launching the service, configure <code>config.yaml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipeline:
  maas:
    enabled: false # Disable MaaS mode (default)
  ocr_api:
    api_host: localhost # or your vLLM/SGLang server address
    api_port: 8080"><pre><span>pipeline</span>:
  <span>maas</span>:
    <span>enabled</span>: <span>false </span><span><span>#</span> Disable MaaS mode (default)</span>
  <span>ocr_api</span>:
    <span>api_host</span>: <span>localhost </span><span><span>#</span> or your vLLM/SGLang server address</span>
    <span>api_port</span>: <span>8080</span></pre></div>

<p dir="auto">For specialized deployment scenarios, see the detailed guides:</p>
<ul dir="auto">
<li><strong><a href="https://seinwave.com/zai-org/GLM-OCR/blob/main/examples/mlx-deploy/README.md">Apple Silicon with mlx-vlm</a></strong> - Optimized for Apple Silicon Macs</li>
<li><strong><a href="https://seinwave.com/zai-org/GLM-OCR/blob/main/examples/ollama-deploy/README.md">Ollama Deployment</a></strong> - Simple local deployment with Ollama</li>
</ul>


<div dir="auto" data-snippet-clipboard-copy-content="# Parse a single image
glmocr parse examples/source/code.png

# Parse a directory
glmocr parse examples/source/

# Set output directory
glmocr parse examples/source/code.png --output ./results/

# Use a custom config
glmocr parse examples/source/code.png --config my_config.yaml

# Enable debug logging with profiling
glmocr parse examples/source/code.png --log-level DEBUG"><pre><span><span>#</span> Parse a single image</span>
glmocr parse examples/source/code.png

<span><span>#</span> Parse a directory</span>
glmocr parse examples/source/

<span><span>#</span> Set output directory</span>
glmocr parse examples/source/code.png --output ./results/

<span><span>#</span> Use a custom config</span>
glmocr parse examples/source/code.png --config my_config.yaml

<span><span>#</span> Enable debug logging with profiling</span>
glmocr parse examples/source/code.png --log-level DEBUG</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="from glmocr import GlmOcr, parse

# Simple function
result = parse(&#34;image.png&#34;)
result = parse([&#34;img1.png&#34;, &#34;img2.jpg&#34;])
result = parse(&#34;https://example.com/image.png&#34;)
result.save(output_dir=&#34;./results&#34;)

# Note: a list is treated as pages of a single document.

# Class-based API
with GlmOcr() as parser:
    result = parser.parse(&#34;image.png&#34;)
    print(result.json_result)
    result.save()"><pre><span>from</span> <span>glmocr</span> <span>import</span> <span>GlmOcr</span>, <span>parse</span>

<span># Simple function</span>
<span>result</span> <span>=</span> <span>parse</span>(<span>&#34;image.png&#34;</span>)
<span>result</span> <span>=</span> <span>parse</span>([<span>&#34;img1.png&#34;</span>, <span>&#34;img2.jpg&#34;</span>])
<span>result</span> <span>=</span> <span>parse</span>(<span>&#34;https://example.com/image.png&#34;</span>)
<span>result</span>.<span>save</span>(<span>output_dir</span><span>=</span><span>&#34;./results&#34;</span>)

<span># Note: a list is treated as pages of a single document.</span>

<span># Class-based API</span>
<span>with</span> <span>GlmOcr</span>() <span>as</span> <span>parser</span>:
    <span>result</span> <span>=</span> <span>parser</span>.<span>parse</span>(<span>&#34;image.png&#34;</span>)
    <span>print</span>(<span>result</span>.<span>json_result</span>)
    <span>result</span>.<span>save</span>()</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Start service
python -m glmocr.server

# With debug logging
python -m glmocr.server --log-level DEBUG

# Call API
curl -X POST http://localhost:5002/glmocr/parse \
  -H &#34;Content-Type: application/json&#34; \
  -d &#39;{&#34;images&#34;: [&#34;./example/source/code.png&#34;]}&#39;"><pre><span><span>#</span> Start service</span>
python -m glmocr.server

<span><span>#</span> With debug logging</span>
python -m glmocr.server --log-level DEBUG

<span><span>#</span> Call API</span>
curl -X POST http://localhost:5002/glmocr/parse \
  -H <span><span>&#34;</span>Content-Type: application/json<span>&#34;</span></span> \
  -d <span><span>&#39;</span>{&#34;images&#34;: [&#34;./example/source/code.png&#34;]}<span>&#39;</span></span></pre></div>
<p dir="auto">Semantics:</p>
<ul dir="auto">
<li><code>images</code> can be a string or a list.</li>
<li>A list is treated as pages of a single document.</li>
<li>For multiple independent documents, call the endpoint multiple times (one document per request).</li>
</ul>

<p dir="auto">Full configuration in <code>glmocr/config.yaml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Server (for glmocr.server)
server:
  host: &#34;0.0.0.0&#34;
  port: 5002
  debug: false

# Logging
logging:
  level: INFO # DEBUG enables profiling

# Pipeline
pipeline:
  # OCR API connection
  ocr_api:
    api_host: localhost
    api_port: 8080
    api_key: null # or set API_KEY env var
    connect_timeout: 300
    request_timeout: 300

  # Page loader settings
  page_loader:
    max_tokens: 16384
    temperature: 0.01
    image_format: JPEG
    min_pixels: 12544
    max_pixels: 71372800

  # Result formatting
  result_formatter:
    output_format: both # json, markdown, or both

  # Layout detection (optional)
  enable_layout: false"><pre><span><span>#</span> Server (for glmocr.server)</span>
<span>server</span>:
  <span>host</span>: <span><span>&#34;</span>0.0.0.0<span>&#34;</span></span>
  <span>port</span>: <span>5002</span>
  <span>debug</span>: <span>false</span>

<span><span>#</span> Logging</span>
<span>logging</span>:
  <span>level</span>: <span>INFO </span><span><span>#</span> DEBUG enables profiling</span>

<span><span>#</span> Pipeline</span>
<span>pipeline</span>:
  <span><span>#</span> OCR API connection</span>
  <span>ocr_api</span>:
    <span>api_host</span>: <span>localhost</span>
    <span>api_port</span>: <span>8080</span>
    <span>api_key</span>: <span>null </span><span><span>#</span> or set API_KEY env var</span>
    <span>connect_timeout</span>: <span>300</span>
    <span>request_timeout</span>: <span>300</span>

  <span><span>#</span> Page loader settings</span>
  <span>page_loader</span>:
    <span>max_tokens</span>: <span>16384</span>
    <span>temperature</span>: <span>0.01</span>
    <span>image_format</span>: <span>JPEG</span>
    <span>min_pixels</span>: <span>12544</span>
    <span>max_pixels</span>: <span>71372800</span>

  <span><span>#</span> Result formatting</span>
  <span>result_formatter</span>:
    <span>output_format</span>: <span>both </span><span><span>#</span> json, markdown, or both</span>

  <span><span>#</span> Layout detection (optional)</span>
  <span>enable_layout</span>: <span>false</span></pre></div>
<p dir="auto">See <a href="https://seinwave.com/zai-org/GLM-OCR/blob/main/glmocr/config.yaml">config.yaml</a> for all options.</p>

<p dir="auto">Here are two examples of output formats:</p>
<ul dir="auto">
<li>JSON</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="[[{ &#34;index&#34;: 0, &#34;label&#34;: &#34;text&#34;, &#34;content&#34;: &#34;...&#34;, &#34;bbox_2d&#34;: null }]]"><pre>[[{ <span>&#34;index&#34;</span>: <span>0</span>, <span>&#34;label&#34;</span>: <span><span>&#34;</span>text<span>&#34;</span></span>, <span>&#34;content&#34;</span>: <span><span>&#34;</span>...<span>&#34;</span></span>, <span>&#34;bbox_2d&#34;</span>: <span>null</span> }]]</pre></div>
<ul dir="auto">
<li>Markdown</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# Document Title

Body...

| Table | Content |
| ----- | ------- |
| ...   | ...     |"><pre><span># <span>Document Title</span></span>

Body...

<span>|</span> Table <span>|</span> Content <span>|</span>
<span>|</span> ----- <span>|</span> ------- <span>|</span>
<span>|</span> ...   <span>|</span> ...     <span>|</span></pre></div>

<p dir="auto">you can run example code likeï¼š</p>
<div dir="auto" data-snippet-clipboard-copy-content="python examples/example.py"><pre>python examples/example.py</pre></div>
<p dir="auto">Output structure (one folder per input):</p>
<ul dir="auto">
<li><code>result.json</code> â€“ structured OCR result</li>
<li><code>result.md</code> â€“ Markdown result</li>
<li><code>imgs/</code> â€“ cropped image regions (when layout mode is enabled)</li>
</ul>

<p dir="auto">GLM-OCR uses composable modules for easy customization:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>PageLoader</code></td>
<td>Preprocessing and image encoding</td>
</tr>
<tr>
<td><code>OCRClient</code></td>
<td>Calls the GLM-OCR model service</td>
</tr>
<tr>
<td><code>PPDocLayoutDetector</code></td>
<td>PP-DocLayout layout detection</td>
</tr>
<tr>
<td><code>ResultFormatter</code></td>
<td>Post-processing, outputs JSON/Markdown</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">You can extend the behavior by creating custom pipelines:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from glmocr.dataloader import PageLoader
from glmocr.ocr_client import OCRClient
from glmocr.postprocess import ResultFormatter


class MyPipeline:
  def __init__(self, config):
    self.page_loader = PageLoader(config)
    self.ocr_client = OCRClient(config)
    self.formatter = ResultFormatter(config)

  def process(self, request_data):
    # Implement your own processing logic
    pass"><pre><span>from</span> <span>glmocr</span>.<span>dataloader</span> <span>import</span> <span>PageLoader</span>
<span>from</span> <span>glmocr</span>.<span>ocr_client</span> <span>import</span> <span>OCRClient</span>
<span>from</span> <span>glmocr</span>.<span>postprocess</span> <span>import</span> <span>ResultFormatter</span>


<span>class</span> <span>MyPipeline</span>:
  <span>def</span> <span>__init__</span>(<span>self</span>, <span>config</span>):
    <span>self</span>.<span>page_loader</span> <span>=</span> <span>PageLoader</span>(<span>config</span>)
    <span>self</span>.<span>ocr_client</span> <span>=</span> <span>OCRClient</span>(<span>config</span>)
    <span>self</span>.<span>formatter</span> <span>=</span> <span>ResultFormatter</span>(<span>config</span>)

  <span>def</span> <span>process</span>(<span>self</span>, <span>request_data</span>):
    <span># Implement your own processing logic</span>
    <span>pass</span></pre></div>

<p dir="auto">This project is inspired by the excellent work of the following projects and communities:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/PaddlePaddle/PP-DocLayoutV3" rel="nofollow">PP-DocLayout-V3</a></li>
<li><a href="https://github.com/PaddlePaddle/PaddleOCR">PaddleOCR</a></li>
<li><a href="https://github.com/opendatalab/MinerU">MinerU</a></li>
</ul>

<p dir="auto">The Code of this repo is under Apache License 2.0.</p>
<p dir="auto">The GLM-OCR model is released under the MIT License.</p>
<p dir="auto">The complete OCR pipeline integrates <a href="https://huggingface.co/PaddlePaddle/PP-DocLayoutV3" rel="nofollow">PP-DocLayoutV3</a> for document layout analysis, which is licensed under the Apache License 2.0. Users should comply with both licenses when using this project.</p>
</article></div></div>
  </body>
</html>
