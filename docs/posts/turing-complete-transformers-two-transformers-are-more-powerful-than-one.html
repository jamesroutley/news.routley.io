<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openreview.net/forum?id=MGWsPGogLH">Original</a>
    <h1>Turing Complete Transformers: Two Transformers Are More Powerful Than One</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>Keywords<!-- -->:</strong> <span>transformers, computational complexity, computation, generalization, agents, multi-model</span></p><p><strong>TL;DR<!-- -->:</strong> <span>We prove transformers are not Turing complete, propose a new architecture that is Turing complete, and empirically demonstrate that the new architecture can generalize more effectively than transformers.</span></p><p><strong>Abstract<!-- -->:</strong> <span>This paper presents Find+Replace transformers, a family of multi-transformer architectures that can provably do things no single transformer can, and which outperforms GPT-4 on several challenging tasks. We first establish that traditional transformers and similar architectures are not Turing Complete, while Find+Replace transformers are. Using this fact, we show how arbitrary programs can be compiled into Find+Replace transformers, potentially aiding interpretability research. We also demonstrate the superior performance of Find+Replace transformers over GPT-4 on a set of composition challenge problems. This work aims to provide a theoretical basis for multi-transformer architectures, and to encourage their further exploration.</span></p><p><strong>Primary Area<!-- -->:</strong> <span>general machine learning (i.e., none of the above)</span></p><p><strong>Code Of Ethics<!-- -->:</strong> <span>I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.</span></p><p><strong>Submission Guidelines<!-- -->:</strong> <span>I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide.</span></p><p><strong>Anonymous Url<!-- -->:</strong> <span>I certify that there is no URL (e.g., github page) that could be used to find authors&#39; identity.</span></p><p><strong>No Acknowledgement Section<!-- -->:</strong> <span>I certify that there is no acknowledgement section in this submission for double blind review.</span></p><p><strong>Submission Number<!-- -->:</strong> <span>8781</span></p></div></div>
  </body>
</html>
