<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/">Original</a>
    <h1>PyTorch at the Edge: Deploy 964 TIMM Models on Android with TorchScript</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><hr/><h3>Table of Contents</h3><nav id="TableOfContents"><ul><li><a href="#-motivation">🔥 Motivation</a></li><li><a href="#-dataset">🌿 Dataset</a></li><li><a href="#-pytorch-image-models">🥇 PyTorch Image Models</a></li><li><a href="#-training-with-fastai">🏋️‍♀️ Training with Fastai</a></li><li><a href="#-exporting-with-torchscript">📀 Exporting with TorchScript</a></li><li><a href="#-inference-in-flutter">📲 Inference in Flutter</a></li><li><a href="#-comments--feedback">🙏 Comments &amp; Feedback</a></li></ul></nav><hr/><h3 id="-motivation">🔥 Motivation</h3><p>With various high-level libraries like <a href="https://keras.io/" target="_blank" rel="nofollow noopener noreferrer">Keras</a>, <a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="nofollow noopener noreferrer">Transformer</a>, and <a href="https://www.fast.ai/" target="_blank" rel="nofollow noopener noreferrer">Fastai</a>, the barrier to training SOTA models has never been lower.</p><p>On top of that with platforms like <a href="https://colab.research.google.com/" target="_blank" rel="nofollow noopener noreferrer">Google Colab</a> and <a href="https://www.kaggle.com/" target="_blank" rel="nofollow noopener noreferrer">Kaggle</a>, pretty much anyone can train a reasonably good model using an old laptop or even a mobile phone (with some patience).</p><blockquote><p>The question is no longer “<strong>can we train a SOTA model?</strong>”, but “<strong>what happens after that?</strong>”</p></blockquote><p>Unfortunately, after getting the model trained, most people wash their hands off at this point claiming their model works.
But, what good would SOTA models do if it’s just in notebooks and Kaggle leaderboards?</p><p>Unless the model is deployed and put to use, it’s of little benefit to anyone out there.</p><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme.jpg" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_360x0_resize_q75_box.jpg 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_720x0_resize_q75_box.jpg 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><p>But deployment is painful. Running a model on a mobile phone?</p><p>Forget it 🤷‍♂️.</p><p>The frustration is real. I remember spending nights exporting models into <code>ONNX</code> and it still failed me.
Deploying models on mobile for edge inference used to be complex.</p><p>Not anymore.</p><p>In this post, I’m going to show you how you can pick from over 900+ SOTA models on <a href="https://github.com/rwightman/pytorch-image-models" target="_blank" rel="nofollow noopener noreferrer">TIMM</a>, train them using best practices with <a href="https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/" target="_blank" rel="nofollow noopener noreferrer">Fastai</a>, and deploy them on Android using <a href="https://flutter.dev/" target="_blank" rel="nofollow noopener noreferrer">Flutter</a>.</p><p>✅ Yes, for free.</p><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>⚡ By the end of this post you will learn how to:</p><ul><li>Load a SOTA classification model from TIMM and train it with Fastai.</li><li>Export the trained model with TorchScript for inference.</li><li>Create a functional Android app and run the inference on your device.</li></ul><p>🔥 The inference time averages at about <strong>200ms</strong> on my Pixel 3 XL!</p><p>💡 <strong>NOTE</strong>: Code and data for this post are available on my GitHub repo <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div><p>Demo of the app 👇</p><video controls="" preload="auto" width="400px" autoplay="" loop="" muted="" playsinline="">
<source src="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/vids/inference_edgenext.mp4" type="video/mp4"/><span></span></video><p>If that looks interesting, read on 👇</p><h3 id="-dataset">🌿 Dataset</h3><p>We will be working with the Paddy Disease Classification <a href="https://www.kaggle.com/competitions/paddy-disease-classification" target="_blank" rel="nofollow noopener noreferrer">dataset</a> from Kaggle.
The dataset consists of <code>10,407</code> labeled images across ten classes (9 diseases and 1 normal):</p><ol><li><code>bacterial_leaf_blight</code></li><li><code>bacterial_leaf_streak</code></li><li><code>bacterial_panicle_blight</code></li><li><code>blast</code></li><li><code>brown_spot</code></li><li><code>dead_heart</code></li><li><code>downy_mildew</code></li><li><code>hispa</code></li><li><code>tungro</code></li><li><code>normal</code></li></ol><p>The task is to classify the paddy images into <code>1</code> of the <code>9</code> diseases or <code>normal</code>.
Few sample images shown below.</p><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img.jpg" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_360x0_resize_q75_box.jpg 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_720x0_resize_q75_box.jpg 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><p>Next, I download the data locally and organize them in a folder structure.
Here’s the structure I have on my computer.</p><pre tabindex="0"><code data-lang="tree">├── data
│   ├── test_images
│   └── train_images
│       ├── bacterial_leaf_blight 
│       ├── bacterial_leaf_streak 
│       ├── bacterial_panicle_blight 
│       ├── blast 
│       ├── brown_spot 
│       ├── dead_heart 
│       ├── downy_mildew 
│       ├── hispa 
│       ├── models
│       ├── normal 
│       └── tungro 
└── train
    └── train.ipynb
</code></pre><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>Descriptions of the folders:</p><ul><li><code>data/</code> - A folder to store train and test images.</li><li><code>train/</code> - A folder to store training-related files and notebooks.</li></ul><p>View the full structure by browsing my GitHub <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost" target="_blank" rel="nofollow noopener noreferrer">repo</a>.</p></div><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>🔔 If you’d like to explore the dataset and excel in the competition, I’d encourage you to check out a series of Kaggle notebooks by Jeremy Howard.</p><ul><li><a href="https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1" target="_blank" rel="nofollow noopener noreferrer">First Steps.</a> - Setting up, looking at the data and training your first model.</li><li><a href="https://www.kaggle.com/code/jhoward/small-models-road-to-the-top-part-2" target="_blank" rel="nofollow noopener noreferrer">Small Models.</a> - Iterate faster with small models, test time augmentation, and then scale up.</li><li><a href="https://www.kaggle.com/code/jhoward/scaling-up-road-to-the-top-part-3" target="_blank" rel="nofollow noopener noreferrer">Scaling Up.</a> - Testing various models, Vision Transformers, and Ensembles.</li><li><a href="https://www.kaggle.com/code/jhoward/multi-target-road-to-the-top-part-4" target="_blank" rel="nofollow noopener noreferrer">Multi-target.</a> - Train a multi-target model with Fastai.</li></ul><p>I’ve personally learned a lot from the notebooks. Part of the codes in the post is adapted from the notebooks.</p></div><p>Now that we’ve got the data, let’s see how to start building a model out of it</p><p>For that we need 👇</p><h3 id="-pytorch-image-models">🥇 PyTorch Image Models</h3><p>There are many libraries to model computer vision tasks but PyTorch Image Models or <a href="https://github.com/rwightman/pytorch-image-models" target="_blank" rel="nofollow noopener noreferrer">TIMM</a> by <a href="https://www.linkedin.com/in/wightmanr/" target="_blank" rel="nofollow noopener noreferrer">Ross Wightman</a> is arguably the most prominent one today.</p><p>The TIMM repository hosts hundreds of recent SOTA models maintained by Ross.
At this point (January 2023) we have 964 pre-trained models on TIMM and increasing as we speak.</p><p>You can install TIMM by simply:</p><p>One line of code, and we’d have access to all models on TIMM!</p><p>With such a massive collection, it can be disorienting which model to start from.
Worry not, TIMM provides a function to search for model architectures with a <a href="https://www.delftstack.com/howto/python/python-wildcard/" target="_blank" rel="nofollow noopener noreferrer">wildcard</a>.</p><p>Since we will be running the model on a mobile device, let’s search for model names that contain the word <em>edge</em>:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> timm
</span></span><span><span>timm<span>.</span>list_models(<span>&#39;*edge*&#39;</span>)
</span></span></code></pre></div><p>This outputs all models that match the wildcard.</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span>[</span><span>&#39;cs3edgenet_x&#39;</span>,
</span></span><span><span> <span>&#39;cs3se_edgenet_x&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_base&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_small&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_small_rw&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_x_small&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_xx_small&#39;</span><span>]</span>
</span></span></code></pre></div><p>Looks like we have something related to the EdgeNeXt model.</p><p>With a simple search and reading through the preprint <a href="https://arxiv.org/abs/2206.10589" target="_blank" rel="nofollow noopener noreferrer">EdgeNeXt - Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications</a>, looks like it’s a fitting model for our application!</p><p>With the model name, you can now start training.
The TIMM repo provides various utility functions and training scripts. Feel free to use them.</p><p>In this post, I’m going to show you an easy way to train a TIMM model using Fastai 👇</p><h3 id="-training-with-fastai">🏋️‍♀️ Training with Fastai</h3><p><a href="https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/" target="_blank" rel="nofollow noopener noreferrer">Fastai</a> is a deep learning library that provides practitioners with high high-level components that can quickly provide SOTA results.
Under the hood Fastai uses PyTorch but it abstracts away the details and incorporates various best practices in training a model.</p><p>Install Fastai with:</p><p>Since, we’d run our model on a mobile device, let’s select the smallest model we got from the previous section - <code>edgenext_xx_small</code>.</p><p>Let’s import all the necessary packages with:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> fastai.vision.all <span>import</span> <span>*</span>
</span></span></code></pre></div><p>Next, load the images into a <code>DataLoader</code>.</p><div><pre tabindex="0"><code data-lang="python"><span><span>trn_path <span>=</span> Path(<span>&#39;../data/train_images&#39;</span>)
</span></span><span><span>dls <span>=</span> ImageDataLoaders<span>.</span>from_folder(trn_path, seed<span>=</span><span>316</span>, 
</span></span><span><span>                                   valid_pct<span>=</span><span>0.2</span>, bs<span>=</span><span>128</span>,
</span></span><span><span>                                   item_tfms<span>=</span>[Resize((<span>224</span>, <span>224</span>))], 
</span></span><span><span>                                   batch_tfms<span>=</span>aug_transforms(min_scale<span>=</span><span>0.75</span>))
</span></span></code></pre></div><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>Parameters for the <code>from_folder</code> method:</p><ul><li><code>trn_path</code> – A <code>Path</code> to the training images.</li><li><code>valid_pct</code> – The percentage of dataset to allocate as the validation set.</li><li><code>bs</code> – Batch size to use during training.</li><li><code>item_tfms</code> – Transformation applied to each item.</li><li><code>batch_tfms</code> – Random transformations applied to each batch to augment the dataset. Read more <a href="https://docs.fast.ai/vision.augment.html#aug_transforms" target="_blank" rel="nofollow noopener noreferrer">here</a>.</li></ul><p>📝 <strong>NOTE</strong>: Check out the Fastai <a href="https://docs.fast.ai/" target="_blank" rel="nofollow noopener noreferrer">docs</a> for more information on the parameters.</p></div><p>You can show a batch of the train images loaded into the <code>DataLoader</code> with:</p><div><pre tabindex="0"><code data-lang="python"><span><span>dls<span>.</span>train<span>.</span>show_batch(max_n<span>=</span><span>8</span>, nrows<span>=</span><span>2</span>)
</span></span></code></pre></div><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch.png" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_1920x0_resize_box_3.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><p>Next create a <code>Learner</code> object which stores the model, dataloaders, and loss function to train a model.
Read more about the <code>Learner</code> <a href="https://docs.fast.ai/learner.html#learner" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p><p>For vision classification tasks we can create a <code>Learner</code> by calling the <code>vision_learner</code> function and providing the necessary parameters:</p><div><pre tabindex="0"><code data-lang="python"><span><span>learn <span>=</span> vision_learner(dls, <span>&#39;edgenext_xx_small&#39;</span>, metrics<span>=</span>accuracy)<span>.</span>to_fp16()
</span></span></code></pre></div><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>Parameters for <code>vision_learner</code>:</p><ul><li><strong>dls</strong> - The <code>Dataloader</code> object.</li><li><strong>edgenext_xx_small</strong> - Model name from TIMM.</li></ul><p>📝 <strong>NOTE</strong>: Read more on vision_learner <a href="https://docs.fast.ai/vision.learner.html#vision_learner" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p><p>In Fastai, you can easily incorporate <a href="https://on-demand.gputechconf.com/gtc/2019/video/_/S9143/" target="_blank" rel="nofollow noopener noreferrer">Mixed Precision Training</a> by adding the <code>.to_fp16()</code> method. This little trick reduces memory usage and trains your model faster at the cost of precision.</p></div><p>One of my favorite features in Fastai is the learning rate finder.
It lets you estimate the range of learning rate to train the model for the best results.</p><p>Find the best learning rate with:</p><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find.png" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_1920x0_resize_box_3.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>The orange dot 🟠 shows the suggested learning rate which is approximately at <code>2e-3</code>.</p><p>A good learning rate lies at the point where the loss is <strong>decreasing most rapidly</strong>. On the plot, it’s anywhere
from the orange dot 🟠 to the point where the loss starts increasing again approximately at <code>1e-1</code>. I’ll pick <code>1e-2</code> as my learning rate.</p><p>Read a post by Zach Mueller on <a href="https://walkwithfastai.com/lr_finder" target="_blank" rel="nofollow noopener noreferrer">how to pick a good learning rate</a>.</p></div><p>Now train the model for 5 <code>epochs</code> and a base learning rate of <code>0.002</code> with the <a href="https://arxiv.org/pdf/1803.09820.pdf" target="_blank" rel="nofollow noopener noreferrer">1cycle policy</a>.
The <code>ShowGraphCallback</code> callback plots the progress of the training.</p><div><pre tabindex="0"><code data-lang="python"><span><span>learn<span>.</span>fine_tune(<span>5</span>, base_lr<span>=</span><span>1e-2</span>, cbs<span>=</span>[ShowGraphCallback()])
</span></span></code></pre></div><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train.png" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_1920x0_resize_box_3.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><p>With just a few lines of code, we can train a reasonably good model with Fastai.
For completeness, here are the few lines of codes you need to load and train the model:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> fastai.vision.all <span>import</span> <span>*</span>
</span></span><span><span>trn_path <span>=</span> Path(<span>&#39;../data/train_images&#39;</span>)
</span></span><span><span>dls <span>=</span> ImageDataLoaders<span>.</span>from_folder(trn_path, seed<span>=</span><span>316</span>,
</span></span><span><span>                                  valid_pct<span>=</span><span>0.2</span>, bs<span>=</span><span>128</span>,
</span></span><span><span>                                  item_tfms<span>=</span>[Resize((<span>224</span>, <span>224</span>))], 
</span></span><span><span>                                  batch_tfms<span>=</span>aug_transforms(min_scale<span>=</span><span>0.75</span>))
</span></span><span><span>learn <span>=</span> vision_learner(dls, <span>&#39;edgenext_xx_small&#39;</span>, metrics<span>=</span>accuracy)<span>.</span>to_fp16()
</span></span><span><span>learn<span>.</span>fine_tune(<span>5</span>, base_lr<span>=</span><span>1e-2</span>, cbs<span>=</span>[ShowGraphCallback()])
</span></span></code></pre></td></tr></tbody></table></div></div><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>For demonstration purposes, I’ve only with only 5 <code>epochs</code>. You can train for longer to obtain better accuracy and model performance.</p><p>📝 <strong>NOTE</strong>: View my training notebook <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/train/train.ipynb" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div><p>You can optionally export the <code>Learner</code> object and import it from another script or notebook with:</p><div><pre tabindex="0"><code data-lang="python"><span><span>learn<span>.</span>export(<span>&#34;../../train/export.pkl&#34;</span>)
</span></span></code></pre></div><p>Once done, now it’s time we transform the model into a form we can use for mobile inference.</p><p>For that, we’ll need 👇</p><h3 id="-exporting-with-torchscript">📀 Exporting with TorchScript</h3><p>In this section, we export the model into a form suitable for a mobile device.
We can do that easily with <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="nofollow noopener noreferrer">TorchScript</a>.</p><blockquote><p>TorchScript is a way to create serializable and optimizable models from PyTorch code on
a variety of platforms, including desktop and mobile devices, without requiring a Python runtime.</p></blockquote><p>With TorchScript, the model’s code is converted into a static graph that can be optimized for faster performance, and then saved and loaded as a serialized representation of the model.</p><p>This allows for deployment to a variety of platforms and acceleration with hardware such as GPUs, TPUs, and mobile devices.</p><p>All the models on TIMM can be exported with TorchScript using the following code snippet.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch
</span></span><span><span><span>from</span> torch.utils.mobile_optimizer <span>import</span> optimize_for_mobile
</span></span><span><span>
</span></span><span><span>learn<span>.</span>model<span>.</span>cpu()
</span></span><span><span>learn<span>.</span>model<span>.</span>eval()
</span></span><span><span>example <span>=</span> torch<span>.</span>rand(<span>1</span>, <span>3</span>, <span>224</span>, <span>224</span>)
</span></span><span><span>traced_script_module <span>=</span> torch<span>.</span>jit<span>.</span>trace(learn<span>.</span>model, example)
</span></span><span><span>optimized_traced_model <span>=</span> optimize_for_mobile(traced_script_module)
</span></span><span><span>optimized_traced_model<span>.</span>_save_for_lite_interpreter(<span>&#34;torchscript_edgenext_xx_small.pt&#34;</span>)
</span></span></code></pre></td></tr></tbody></table></div></div><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>From the snippet above we need to specify a few things:</p><ul><li><code>Line 6</code>: The shape of the input image tensor.</li><li><code>Line 9</code>: “torchscript_edgenext_xx_small.pt” is the name of the resulting TorchScript serialized model.</li></ul><p>📝 <strong>NOTE</strong>: View the full notebook from training to exporting the model on my GitHub repo <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/train/train.ipynb" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div><p>Once completed, you’ll have a file <code>torchscript_edgenext_xx_small.pt</code> that can be ported to other devices for inference.
In this post, I will be porting it to Android using a framework known as <a href="https://flutter.dev/" target="_blank" rel="nofollow noopener noreferrer">Flutter</a>.</p><h3 id="-inference-in-flutter">📲 Inference in Flutter</h3><p><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/vids/flutter.gif" alt="img"/></p><blockquote><p>Flutter is an open-source framework by Google for building beautiful, natively compiled, multi-platform applications from a single codebase.</p></blockquote><p>We can load the <code>torchscript_edgenext_xx_small.pt</code> and use if for inference.
To do so, we will use the <a href="https://github.com/zezo357/pytorch_lite" target="_blank" rel="nofollow noopener noreferrer">pytorch_lite</a> Flutter package.
The <code>pytorch_lite</code> package supports image classification and detection with TorchScript.</p><p>The following code snippet shows a function to load our serialized model <code>torchscript_edgenext_xx_small.pt</code>.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td><td><pre tabindex="0"><code data-lang="dart"><span><span>Future loadModel() <span>async</span> {
</span></span><span><span>    <span>String</span> pathImageModel <span>=</span> <span>&#34;assets/models/torchscript_edgenext_xx_small.pt&#34;</span>;
</span></span><span><span>    <span>try</span> {
</span></span><span><span>        _imageModel <span>=</span> <span>await</span> PytorchLite.loadClassificationModel(
</span></span><span><span>            pathImageModel, <span>224</span>, <span>224</span>,
</span></span><span><span>            labelPath: <span>&#34;assets/labels/label_classification_paddy.txt&#34;</span>);
</span></span><span><span>    } on PlatformException {
</span></span><span><span>        print(<span>&#34;only supported for Android&#34;</span>);
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></td></tr></tbody></table></div></div><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>From the snippet above we need to specify a few things:</p><ul><li><code>Line 2</code>: Path to the serialized model.</li><li><code>Line 5</code>: The input image size - <code>224</code> by <code>224</code> pixels.</li><li><code>Line 6</code>: A text file with labels associated with each class.</li></ul><p>View the full code on my GitHub <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/flutter_app/lib/main.dart" target="_blank" rel="nofollow noopener noreferrer">repo</a>.</p></div><p>The following code snippet shows a function to run the inference.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span></code></pre></td><td><pre tabindex="0"><code data-lang="dart"><span><span>Future runClassification() <span>async</span> {
</span></span><span><span>    objDetect <span>=</span> [];
</span></span><span><span>    <span>//pick a random image
</span></span></span><span><span><span></span>    <span>final</span> PickedFile<span>?</span> image <span>=</span>
</span></span><span><span>        <span>await</span> _picker.getImage(<span>source</span><span>:</span> ImageSource.gallery);
</span></span><span><span>
</span></span><span><span>    <span>//get prediction
</span></span></span><span><span><span></span>    _imagePrediction <span>=</span> <span>await</span> _imageModel<span>!</span>
</span></span><span><span>        .getImagePrediction(<span>await</span> File(image<span>!</span>.path).readAsBytes());
</span></span><span><span>
</span></span><span><span>    List<span>&lt;</span><span>double</span><span>?&gt;?</span> predictionList <span>=</span> <span>await</span> _imageModel<span>!</span>.getImagePredictionList(
</span></span><span><span>      <span>await</span> File(image.path).readAsBytes(),
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    List<span>&lt;</span><span>double</span><span>?&gt;?</span> predictionListProbabilites <span>=</span>
</span></span><span><span>        <span>await</span> _imageModel<span>!</span>.getImagePredictionListProbabilities(
</span></span><span><span>      <span>await</span> File(image.path).readAsBytes(),
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    <span>//Gettting the highest Probability
</span></span></span><span><span><span></span>    <span>double</span> maxScoreProbability <span>=</span> <span>double</span>.negativeInfinity;
</span></span><span><span>    <span>double</span> sumOfProbabilites <span>=</span> <span>0</span>;
</span></span><span><span>    <span>int</span> index <span>=</span> <span>0</span>;
</span></span><span><span>    <span>for</span> (<span>int</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> predictionListProbabilites<span>!</span>.length; i<span>++</span>) {
</span></span><span><span>      <span>if</span> (predictionListProbabilites[i]<span>!</span> <span>&gt;</span> maxScoreProbability) {
</span></span><span><span>        maxScoreProbability <span>=</span> predictionListProbabilites[i]<span>!</span>;
</span></span><span><span>        sumOfProbabilites <span>=</span> sumOfProbabilites <span>+</span> predictionListProbabilites[i]<span>!</span>;
</span></span><span><span>        index <span>=</span> i;
</span></span><span><span>      }
</span></span><span><span>    }
</span></span><span><span>    _predictionConfidence <span>=</span> (maxScoreProbability <span>*</span> <span>100</span>).toStringAsFixed(<span>2</span>);
</span></span><span><span>
</span></span><span><span>  }
</span></span></code></pre></td></tr></tbody></table></div></div><p>Those are the two important functions to load and run the TorchScript model.</p><p>The following screen capture shows the Flutter app in action.
The clip runs in real-time and is <strong>NOT sped up</strong>!</p><video controls="" preload="auto" width="400px" autoplay="" loop="" muted="" playsinline="">
<source src="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/vids/inference_edgenext.mp4" type="video/mp4"/><span></span></video><p>The compiled <code>.apk</code> file is about <strong>77MB</strong> in size and the inference time is approximately <strong>200 ms</strong> on my Pixel 3 XL.</p><p>Try it out and install the pre-built <code>.apk</code> file on your Android phone <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/app-release.apk?raw=true" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p><p>That’s a wrap! In this post, I’ve shown you how you can start from a model, train it, and deploy it on a mobile device for edge inference.</p><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>⚡ In short we learned how to:</p><ul><li>Load a SOTA classification model from TIMM and train it with Fastai.</li><li>Export the trained model with TorchScript for inference.</li><li>Create a functional Android app and run the model inference on your device.</li></ul><p>📝 <strong>NOTE</strong>: View the codes for the entire post on my GitHub repo <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div><p>I hope you’ve learned a thing or two from this blog post.
If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or <a href="https://dicksonneoh.com/contact/" target="_blank" rel="nofollow noopener noreferrer">drop me a message</a>.</p><blockquote><p lang="en" dir="ltr">Tired of training models that never see the light of day? Don&#39;t let your hard work go to waste!</p>— Dickson Neoh 🚀 (@dicksonneoh7) <a href="https://twitter.com/dicksonneoh7/status/1625367344712388609?ref_src=twsrc%5Etfw">February 14, 2023</a></blockquote>
</div></div></div></div>
  </body>
</html>
