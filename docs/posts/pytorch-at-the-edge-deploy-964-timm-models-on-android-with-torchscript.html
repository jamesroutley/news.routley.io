<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/">Original</a>
    <h1>PyTorch at the Edge: Deploy 964 TIMM Models on Android with TorchScript</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><hr/><h3>Table of Contents</h3><nav id="TableOfContents"><ul><li><a href="#-motivation">ğŸ”¥ Motivation</a></li><li><a href="#-dataset">ğŸŒ¿ Dataset</a></li><li><a href="#-pytorch-image-models">ğŸ¥‡ PyTorch Image Models</a></li><li><a href="#-training-with-fastai">ğŸ‹ï¸â€â™€ï¸ Training with Fastai</a></li><li><a href="#-exporting-with-torchscript">ğŸ“€ Exporting with TorchScript</a></li><li><a href="#-inference-in-flutter">ğŸ“² Inference in Flutter</a></li><li><a href="#-comments--feedback">ğŸ™ Comments &amp; Feedback</a></li></ul></nav><hr/><h3 id="-motivation">ğŸ”¥ Motivation</h3><p>With various high-level libraries like <a href="https://keras.io/" target="_blank" rel="nofollow noopener noreferrer">Keras</a>, <a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="nofollow noopener noreferrer">Transformer</a>, and <a href="https://www.fast.ai/" target="_blank" rel="nofollow noopener noreferrer">Fastai</a>, the barrier to training SOTA models has never been lower.</p><p>On top of that with platforms like <a href="https://colab.research.google.com/" target="_blank" rel="nofollow noopener noreferrer">Google Colab</a> and <a href="https://www.kaggle.com/" target="_blank" rel="nofollow noopener noreferrer">Kaggle</a>, pretty much anyone can train a reasonably good model using an old laptop or even a mobile phone (with some patience).</p><blockquote><p>The question is no longer â€œ<strong>can we train a SOTA model?</strong>â€, but â€œ<strong>what happens after that?</strong>â€</p></blockquote><p>Unfortunately, after getting the model trained, most people wash their hands off at this point claiming their model works.
But, what good would SOTA models do if itâ€™s just in notebooks and Kaggle leaderboards?</p><p>Unless the model is deployed and put to use, itâ€™s of little benefit to anyone out there.</p><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme.jpg" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_360x0_resize_q75_box.jpg 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_720x0_resize_q75_box.jpg 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/meme_hu837a4392153a49e1573ab877c8748e3c_65319_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><p>But deployment is painful. Running a model on a mobile phone?</p><p>Forget it ğŸ¤·â€â™‚ï¸.</p><p>The frustration is real. I remember spending nights exporting models into <code>ONNX</code> and it still failed me.
Deploying models on mobile for edge inference used to be complex.</p><p>Not anymore.</p><p>In this post, Iâ€™m going to show you how you can pick from over 900+ SOTA models on <a href="https://github.com/rwightman/pytorch-image-models" target="_blank" rel="nofollow noopener noreferrer">TIMM</a>, train them using best practices with <a href="https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/" target="_blank" rel="nofollow noopener noreferrer">Fastai</a>, and deploy them on Android using <a href="https://flutter.dev/" target="_blank" rel="nofollow noopener noreferrer">Flutter</a>.</p><p>âœ… Yes, for free.</p><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>âš¡ By the end of this post you will learn how to:</p><ul><li>Load a SOTA classification model from TIMM and train it with Fastai.</li><li>Export the trained model with TorchScript for inference.</li><li>Create a functional Android app and run the inference on your device.</li></ul><p>ğŸ”¥ The inference time averages at about <strong>200ms</strong> on my Pixel 3 XL!</p><p>ğŸ’¡ <strong>NOTE</strong>: Code and data for this post are available on my GitHub repo <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div><p>Demo of the app ğŸ‘‡</p><video controls="" preload="auto" width="400px" autoplay="" loop="" muted="" playsinline="">
<source src="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/vids/inference_edgenext.mp4" type="video/mp4"/><span></span></video><p>If that looks interesting, read on ğŸ‘‡</p><h3 id="-dataset">ğŸŒ¿ Dataset</h3><p>We will be working with the Paddy Disease Classification <a href="https://www.kaggle.com/competitions/paddy-disease-classification" target="_blank" rel="nofollow noopener noreferrer">dataset</a> from Kaggle.
The dataset consists of <code>10,407</code> labeled images across ten classes (9 diseases and 1 normal):</p><ol><li><code>bacterial_leaf_blight</code></li><li><code>bacterial_leaf_streak</code></li><li><code>bacterial_panicle_blight</code></li><li><code>blast</code></li><li><code>brown_spot</code></li><li><code>dead_heart</code></li><li><code>downy_mildew</code></li><li><code>hispa</code></li><li><code>tungro</code></li><li><code>normal</code></li></ol><p>The task is to classify the paddy images into <code>1</code> of the <code>9</code> diseases or <code>normal</code>.
Few sample images shown below.</p><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img.jpg" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_360x0_resize_q75_box.jpg 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_720x0_resize_q75_box.jpg 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/test_img_huf9611a5d8b83fa8276183d52e772329a_134184_1920x0_resize_q75_box.jpg 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><p>Next, I download the data locally and organize them in a folder structure.
Hereâ€™s the structure I have on my computer.</p><pre tabindex="0"><code data-lang="tree">â”œâ”€â”€ data
â”‚   â”œâ”€â”€ test_images
â”‚   â””â”€â”€ train_images
â”‚       â”œâ”€â”€ bacterial_leaf_blight 
â”‚       â”œâ”€â”€ bacterial_leaf_streak 
â”‚       â”œâ”€â”€ bacterial_panicle_blight 
â”‚       â”œâ”€â”€ blast 
â”‚       â”œâ”€â”€ brown_spot 
â”‚       â”œâ”€â”€ dead_heart 
â”‚       â”œâ”€â”€ downy_mildew 
â”‚       â”œâ”€â”€ hispa 
â”‚       â”œâ”€â”€ models
â”‚       â”œâ”€â”€ normal 
â”‚       â””â”€â”€ tungro 
â””â”€â”€ train
    â””â”€â”€ train.ipynb
</code></pre><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>Descriptions of the folders:</p><ul><li><code>data/</code> - A folder to store train and test images.</li><li><code>train/</code> - A folder to store training-related files and notebooks.</li></ul><p>View the full structure by browsing my GitHub <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost" target="_blank" rel="nofollow noopener noreferrer">repo</a>.</p></div><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>ğŸ”” If youâ€™d like to explore the dataset and excel in the competition, Iâ€™d encourage you to check out a series of Kaggle notebooks by Jeremy Howard.</p><ul><li><a href="https://www.kaggle.com/code/jhoward/first-steps-road-to-the-top-part-1" target="_blank" rel="nofollow noopener noreferrer">First Steps.</a> - Setting up, looking at the data and training your first model.</li><li><a href="https://www.kaggle.com/code/jhoward/small-models-road-to-the-top-part-2" target="_blank" rel="nofollow noopener noreferrer">Small Models.</a> - Iterate faster with small models, test time augmentation, and then scale up.</li><li><a href="https://www.kaggle.com/code/jhoward/scaling-up-road-to-the-top-part-3" target="_blank" rel="nofollow noopener noreferrer">Scaling Up.</a> - Testing various models, Vision Transformers, and Ensembles.</li><li><a href="https://www.kaggle.com/code/jhoward/multi-target-road-to-the-top-part-4" target="_blank" rel="nofollow noopener noreferrer">Multi-target.</a> - Train a multi-target model with Fastai.</li></ul><p>Iâ€™ve personally learned a lot from the notebooks. Part of the codes in the post is adapted from the notebooks.</p></div><p>Now that weâ€™ve got the data, letâ€™s see how to start building a model out of it</p><p>For that we need ğŸ‘‡</p><h3 id="-pytorch-image-models">ğŸ¥‡ PyTorch Image Models</h3><p>There are many libraries to model computer vision tasks but PyTorch Image Models or <a href="https://github.com/rwightman/pytorch-image-models" target="_blank" rel="nofollow noopener noreferrer">TIMM</a> by <a href="https://www.linkedin.com/in/wightmanr/" target="_blank" rel="nofollow noopener noreferrer">Ross Wightman</a> is arguably the most prominent one today.</p><p>The TIMM repository hosts hundreds of recent SOTA models maintained by Ross.
At this point (January 2023) we have 964 pre-trained models on TIMM and increasing as we speak.</p><p>You can install TIMM by simply:</p><p>One line of code, and weâ€™d have access to all models on TIMM!</p><p>With such a massive collection, it can be disorienting which model to start from.
Worry not, TIMM provides a function to search for model architectures with a <a href="https://www.delftstack.com/howto/python/python-wildcard/" target="_blank" rel="nofollow noopener noreferrer">wildcard</a>.</p><p>Since we will be running the model on a mobile device, letâ€™s search for model names that contain the word <em>edge</em>:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> timm
</span></span><span><span>timm<span>.</span>list_models(<span>&#39;*edge*&#39;</span>)
</span></span></code></pre></div><p>This outputs all models that match the wildcard.</p><div><pre tabindex="0"><code data-lang="bash"><span><span><span>[</span><span>&#39;cs3edgenet_x&#39;</span>,
</span></span><span><span> <span>&#39;cs3se_edgenet_x&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_base&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_small&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_small_rw&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_x_small&#39;</span>,
</span></span><span><span> <span>&#39;edgenext_xx_small&#39;</span><span>]</span>
</span></span></code></pre></div><p>Looks like we have something related to the EdgeNeXt model.</p><p>With a simple search and reading through the preprint <a href="https://arxiv.org/abs/2206.10589" target="_blank" rel="nofollow noopener noreferrer">EdgeNeXt - Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications</a>, looks like itâ€™s a fitting model for our application!</p><p>With the model name, you can now start training.
The TIMM repo provides various utility functions and training scripts. Feel free to use them.</p><p>In this post, Iâ€™m going to show you an easy way to train a TIMM model using Fastai ğŸ‘‡</p><h3 id="-training-with-fastai">ğŸ‹ï¸â€â™€ï¸ Training with Fastai</h3><p><a href="https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/" target="_blank" rel="nofollow noopener noreferrer">Fastai</a> is a deep learning library that provides practitioners with high high-level components that can quickly provide SOTA results.
Under the hood Fastai uses PyTorch but it abstracts away the details and incorporates various best practices in training a model.</p><p>Install Fastai with:</p><p>Since, weâ€™d run our model on a mobile device, letâ€™s select the smallest model we got from the previous section - <code>edgenext_xx_small</code>.</p><p>Letâ€™s import all the necessary packages with:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> fastai.vision.all <span>import</span> <span>*</span>
</span></span></code></pre></div><p>Next, load the images into a <code>DataLoader</code>.</p><div><pre tabindex="0"><code data-lang="python"><span><span>trn_path <span>=</span> Path(<span>&#39;../data/train_images&#39;</span>)
</span></span><span><span>dls <span>=</span> ImageDataLoaders<span>.</span>from_folder(trn_path, seed<span>=</span><span>316</span>, 
</span></span><span><span>                                   valid_pct<span>=</span><span>0.2</span>, bs<span>=</span><span>128</span>,
</span></span><span><span>                                   item_tfms<span>=</span>[Resize((<span>224</span>, <span>224</span>))], 
</span></span><span><span>                                   batch_tfms<span>=</span>aug_transforms(min_scale<span>=</span><span>0.75</span>))
</span></span></code></pre></div><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>Parameters for the <code>from_folder</code> method:</p><ul><li><code>trn_path</code> â€“ A <code>Path</code> to the training images.</li><li><code>valid_pct</code> â€“ The percentage of dataset to allocate as the validation set.</li><li><code>bs</code> â€“ Batch size to use during training.</li><li><code>item_tfms</code> â€“ Transformation applied to each item.</li><li><code>batch_tfms</code> â€“ Random transformations applied to each batch to augment the dataset. Read more <a href="https://docs.fast.ai/vision.augment.html#aug_transforms" target="_blank" rel="nofollow noopener noreferrer">here</a>.</li></ul><p>ğŸ“ <strong>NOTE</strong>: Check out the Fastai <a href="https://docs.fast.ai/" target="_blank" rel="nofollow noopener noreferrer">docs</a> for more information on the parameters.</p></div><p>You can show a batch of the train images loaded into the <code>DataLoader</code> with:</p><div><pre tabindex="0"><code data-lang="python"><span><span>dls<span>.</span>train<span>.</span>show_batch(max_n<span>=</span><span>8</span>, nrows<span>=</span><span>2</span>)
</span></span></code></pre></div><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch.png" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/show_batch_hucecf67f548be7e303644e798dd8afd17_730278_1920x0_resize_box_3.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><p>Next create a <code>Learner</code> object which stores the model, dataloaders, and loss function to train a model.
Read more about the <code>Learner</code> <a href="https://docs.fast.ai/learner.html#learner" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p><p>For vision classification tasks we can create a <code>Learner</code> by calling the <code>vision_learner</code> function and providing the necessary parameters:</p><div><pre tabindex="0"><code data-lang="python"><span><span>learn <span>=</span> vision_learner(dls, <span>&#39;edgenext_xx_small&#39;</span>, metrics<span>=</span>accuracy)<span>.</span>to_fp16()
</span></span></code></pre></div><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>Parameters for <code>vision_learner</code>:</p><ul><li><strong>dls</strong> - The <code>Dataloader</code> object.</li><li><strong>edgenext_xx_small</strong> - Model name from TIMM.</li></ul><p>ğŸ“ <strong>NOTE</strong>: Read more on vision_learner <a href="https://docs.fast.ai/vision.learner.html#vision_learner" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p><p>In Fastai, you can easily incorporate <a href="https://on-demand.gputechconf.com/gtc/2019/video/_/S9143/" target="_blank" rel="nofollow noopener noreferrer">Mixed Precision Training</a> by adding the <code>.to_fp16()</code> method. This little trick reduces memory usage and trains your model faster at the cost of precision.</p></div><p>One of my favorite features in Fastai is the learning rate finder.
It lets you estimate the range of learning rate to train the model for the best results.</p><p>Find the best learning rate with:</p><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find.png" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/lr_find_hu99e6eb1b20f728b920631645a7f5b897_23295_1920x0_resize_box_3.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>The orange dot ğŸŸ  shows the suggested learning rate which is approximately at <code>2e-3</code>.</p><p>A good learning rate lies at the point where the loss is <strong>decreasing most rapidly</strong>. On the plot, itâ€™s anywhere
from the orange dot ğŸŸ  to the point where the loss starts increasing again approximately at <code>1e-1</code>. Iâ€™ll pick <code>1e-2</code> as my learning rate.</p><p>Read a post by Zach Mueller on <a href="https://walkwithfastai.com/lr_finder" target="_blank" rel="nofollow noopener noreferrer">how to pick a good learning rate</a>.</p></div><p>Now train the model for 5 <code>epochs</code> and a base learning rate of <code>0.002</code> with the <a href="https://arxiv.org/pdf/1803.09820.pdf" target="_blank" rel="nofollow noopener noreferrer">1cycle policy</a>.
The <code>ShowGraphCallback</code> callback plots the progress of the training.</p><div><pre tabindex="0"><code data-lang="python"><span><span>learn<span>.</span>fine_tune(<span>5</span>, base_lr<span>=</span><span>1e-2</span>, cbs<span>=</span>[ShowGraphCallback()])
</span></span></code></pre></div><figure><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train.png" srcset="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_360x0_resize_box_3.png 360w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_720x0_resize_box_3.png 720w, /portfolio/pytorch_at_the_edge_timm_torchscript_flutter/train_hue163433df998bfba1575a9b983a6d3bf_57741_1920x0_resize_box_3.png 1920w" sizes="(max-width: 37.5em) 360px, (min-width: 75em) 720px, (min-width: 112.5em) 1200px"/></figure><p>With just a few lines of code, we can train a reasonably good model with Fastai.
For completeness, here are the few lines of codes you need to load and train the model:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> fastai.vision.all <span>import</span> <span>*</span>
</span></span><span><span>trn_path <span>=</span> Path(<span>&#39;../data/train_images&#39;</span>)
</span></span><span><span>dls <span>=</span> ImageDataLoaders<span>.</span>from_folder(trn_path, seed<span>=</span><span>316</span>,
</span></span><span><span>                                  valid_pct<span>=</span><span>0.2</span>, bs<span>=</span><span>128</span>,
</span></span><span><span>                                  item_tfms<span>=</span>[Resize((<span>224</span>, <span>224</span>))], 
</span></span><span><span>                                  batch_tfms<span>=</span>aug_transforms(min_scale<span>=</span><span>0.75</span>))
</span></span><span><span>learn <span>=</span> vision_learner(dls, <span>&#39;edgenext_xx_small&#39;</span>, metrics<span>=</span>accuracy)<span>.</span>to_fp16()
</span></span><span><span>learn<span>.</span>fine_tune(<span>5</span>, base_lr<span>=</span><span>1e-2</span>, cbs<span>=</span>[ShowGraphCallback()])
</span></span></code></pre></td></tr></tbody></table></div></div><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>For demonstration purposes, Iâ€™ve only with only 5 <code>epochs</code>. You can train for longer to obtain better accuracy and model performance.</p><p>ğŸ“ <strong>NOTE</strong>: View my training notebook <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/train/train.ipynb" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div><p>You can optionally export the <code>Learner</code> object and import it from another script or notebook with:</p><div><pre tabindex="0"><code data-lang="python"><span><span>learn<span>.</span>export(<span>&#34;../../train/export.pkl&#34;</span>)
</span></span></code></pre></div><p>Once done, now itâ€™s time we transform the model into a form we can use for mobile inference.</p><p>For that, weâ€™ll need ğŸ‘‡</p><h3 id="-exporting-with-torchscript">ğŸ“€ Exporting with TorchScript</h3><p>In this section, we export the model into a form suitable for a mobile device.
We can do that easily with <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="nofollow noopener noreferrer">TorchScript</a>.</p><blockquote><p>TorchScript is a way to create serializable and optimizable models from PyTorch code on
a variety of platforms, including desktop and mobile devices, without requiring a Python runtime.</p></blockquote><p>With TorchScript, the modelâ€™s code is converted into a static graph that can be optimized for faster performance, and then saved and loaded as a serialized representation of the model.</p><p>This allows for deployment to a variety of platforms and acceleration with hardware such as GPUs, TPUs, and mobile devices.</p><p>All the models on TIMM can be exported with TorchScript using the following code snippet.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> torch
</span></span><span><span><span>from</span> torch.utils.mobile_optimizer <span>import</span> optimize_for_mobile
</span></span><span><span>
</span></span><span><span>learn<span>.</span>model<span>.</span>cpu()
</span></span><span><span>learn<span>.</span>model<span>.</span>eval()
</span></span><span><span>example <span>=</span> torch<span>.</span>rand(<span>1</span>, <span>3</span>, <span>224</span>, <span>224</span>)
</span></span><span><span>traced_script_module <span>=</span> torch<span>.</span>jit<span>.</span>trace(learn<span>.</span>model, example)
</span></span><span><span>optimized_traced_model <span>=</span> optimize_for_mobile(traced_script_module)
</span></span><span><span>optimized_traced_model<span>.</span>_save_for_lite_interpreter(<span>&#34;torchscript_edgenext_xx_small.pt&#34;</span>)
</span></span></code></pre></td></tr></tbody></table></div></div><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>From the snippet above we need to specify a few things:</p><ul><li><code>Line 6</code>: The shape of the input image tensor.</li><li><code>Line 9</code>: â€œtorchscript_edgenext_xx_small.ptâ€ is the name of the resulting TorchScript serialized model.</li></ul><p>ğŸ“ <strong>NOTE</strong>: View the full notebook from training to exporting the model on my GitHub repo <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/train/train.ipynb" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div><p>Once completed, youâ€™ll have a file <code>torchscript_edgenext_xx_small.pt</code> that can be ported to other devices for inference.
In this post, I will be porting it to Android using a framework known as <a href="https://flutter.dev/" target="_blank" rel="nofollow noopener noreferrer">Flutter</a>.</p><h3 id="-inference-in-flutter">ğŸ“² Inference in Flutter</h3><p><img src="https://dicksonneoh.com/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/vids/flutter.gif" alt="img"/></p><blockquote><p>Flutter is an open-source framework by Google for building beautiful, natively compiled, multi-platform applications from a single codebase.</p></blockquote><p>We can load the <code>torchscript_edgenext_xx_small.pt</code> and use if for inference.
To do so, we will use the <a href="https://github.com/zezo357/pytorch_lite" target="_blank" rel="nofollow noopener noreferrer">pytorch_lite</a> Flutter package.
The <code>pytorch_lite</code> package supports image classification and detection with TorchScript.</p><p>The following code snippet shows a function to load our serialized model <code>torchscript_edgenext_xx_small.pt</code>.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td><td><pre tabindex="0"><code data-lang="dart"><span><span>Future loadModel() <span>async</span> {
</span></span><span><span>    <span>String</span> pathImageModel <span>=</span> <span>&#34;assets/models/torchscript_edgenext_xx_small.pt&#34;</span>;
</span></span><span><span>    <span>try</span> {
</span></span><span><span>        _imageModel <span>=</span> <span>await</span> PytorchLite.loadClassificationModel(
</span></span><span><span>            pathImageModel, <span>224</span>, <span>224</span>,
</span></span><span><span>            labelPath: <span>&#34;assets/labels/label_classification_paddy.txt&#34;</span>);
</span></span><span><span>    } on PlatformException {
</span></span><span><span>        print(<span>&#34;only supported for Android&#34;</span>);
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></td></tr></tbody></table></div></div><div><p><span><svg><use href="#note-notice"></use></svg></span>note</p><p>From the snippet above we need to specify a few things:</p><ul><li><code>Line 2</code>: Path to the serialized model.</li><li><code>Line 5</code>: The input image size - <code>224</code> by <code>224</code> pixels.</li><li><code>Line 6</code>: A text file with labels associated with each class.</li></ul><p>View the full code on my GitHub <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/flutter_app/lib/main.dart" target="_blank" rel="nofollow noopener noreferrer">repo</a>.</p></div><p>The following code snippet shows a function to run the inference.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span></code></pre></td><td><pre tabindex="0"><code data-lang="dart"><span><span>Future runClassification() <span>async</span> {
</span></span><span><span>    objDetect <span>=</span> [];
</span></span><span><span>    <span>//pick a random image
</span></span></span><span><span><span></span>    <span>final</span> PickedFile<span>?</span> image <span>=</span>
</span></span><span><span>        <span>await</span> _picker.getImage(<span>source</span><span>:</span> ImageSource.gallery);
</span></span><span><span>
</span></span><span><span>    <span>//get prediction
</span></span></span><span><span><span></span>    _imagePrediction <span>=</span> <span>await</span> _imageModel<span>!</span>
</span></span><span><span>        .getImagePrediction(<span>await</span> File(image<span>!</span>.path).readAsBytes());
</span></span><span><span>
</span></span><span><span>    List<span>&lt;</span><span>double</span><span>?&gt;?</span> predictionList <span>=</span> <span>await</span> _imageModel<span>!</span>.getImagePredictionList(
</span></span><span><span>      <span>await</span> File(image.path).readAsBytes(),
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    List<span>&lt;</span><span>double</span><span>?&gt;?</span> predictionListProbabilites <span>=</span>
</span></span><span><span>        <span>await</span> _imageModel<span>!</span>.getImagePredictionListProbabilities(
</span></span><span><span>      <span>await</span> File(image.path).readAsBytes(),
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    <span>//Gettting the highest Probability
</span></span></span><span><span><span></span>    <span>double</span> maxScoreProbability <span>=</span> <span>double</span>.negativeInfinity;
</span></span><span><span>    <span>double</span> sumOfProbabilites <span>=</span> <span>0</span>;
</span></span><span><span>    <span>int</span> index <span>=</span> <span>0</span>;
</span></span><span><span>    <span>for</span> (<span>int</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> predictionListProbabilites<span>!</span>.length; i<span>++</span>) {
</span></span><span><span>      <span>if</span> (predictionListProbabilites[i]<span>!</span> <span>&gt;</span> maxScoreProbability) {
</span></span><span><span>        maxScoreProbability <span>=</span> predictionListProbabilites[i]<span>!</span>;
</span></span><span><span>        sumOfProbabilites <span>=</span> sumOfProbabilites <span>+</span> predictionListProbabilites[i]<span>!</span>;
</span></span><span><span>        index <span>=</span> i;
</span></span><span><span>      }
</span></span><span><span>    }
</span></span><span><span>    _predictionConfidence <span>=</span> (maxScoreProbability <span>*</span> <span>100</span>).toStringAsFixed(<span>2</span>);
</span></span><span><span>
</span></span><span><span>  }
</span></span></code></pre></td></tr></tbody></table></div></div><p>Those are the two important functions to load and run the TorchScript model.</p><p>The following screen capture shows the Flutter app in action.
The clip runs in real-time and is <strong>NOT sped up</strong>!</p><video controls="" preload="auto" width="400px" autoplay="" loop="" muted="" playsinline="">
<source src="/portfolio/pytorch_at_the_edge_timm_torchscript_flutter/vids/inference_edgenext.mp4" type="video/mp4"/><span></span></video><p>The compiled <code>.apk</code> file is about <strong>77MB</strong> in size and the inference time is approximately <strong>200 ms</strong> on my Pixel 3 XL.</p><p>Try it out and install the pre-built <code>.apk</code> file on your Android phone <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/blob/main/app-release.apk?raw=true" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p><p>Thatâ€™s a wrap! In this post, Iâ€™ve shown you how you can start from a model, train it, and deploy it on a mobile device for edge inference.</p><div><p><span><svg><use href="#tip-notice"></use></svg></span>tip</p><p>âš¡ In short we learned how to:</p><ul><li>Load a SOTA classification model from TIMM and train it with Fastai.</li><li>Export the trained model with TorchScript for inference.</li><li>Create a functional Android app and run the model inference on your device.</li></ul><p>ğŸ“ <strong>NOTE</strong>: View the codes for the entire post on my GitHub repo <a href="https://github.com/dnth/timm-flutter-pytorch-lite-blogpost/" target="_blank" rel="nofollow noopener noreferrer">here</a>.</p></div><p>I hope youâ€™ve learned a thing or two from this blog post.
If you have any questions, comments, or feedback, please leave them on the following Twitter/LinkedIn post or <a href="https://dicksonneoh.com/contact/" target="_blank" rel="nofollow noopener noreferrer">drop me a message</a>.</p><blockquote><p lang="en" dir="ltr">Tired of training models that never see the light of day? Don&#39;t let your hard work go to waste!</p>â€” Dickson Neoh ğŸš€ (@dicksonneoh7) <a href="https://twitter.com/dicksonneoh7/status/1625367344712388609?ref_src=twsrc%5Etfw">February 14, 2023</a></blockquote>
</div></div></div></div>
  </body>
</html>
