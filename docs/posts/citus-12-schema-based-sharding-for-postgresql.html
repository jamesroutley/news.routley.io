<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.citusdata.com/blog/2023/07/18/citus-12-schema-based-sharding-for-postgres/">Original</a>
    <h1>Citus 12: Schema-based sharding for PostgreSQL</h1>
    
    <div id="readability-page-1" class="page"><div data-sticky-container="">  <div>  <section> <div> <p>What if you could automatically shard your PostgreSQL database across any number of servers and get <a href="https://www.citusdata.com/blog/2023/06/21/distributed-postgres-benchmarks-using-hammerdb-by-gigaom/">industry-leading performance at scale</a> without any special data modelling steps?</p> <p>Our latest <a href="https://github.com/citusdata/citus">Citus open source</a> release, Citus 12, adds a new and easy way to transparently scale your Postgres database: Schema-based sharding, where the database is transparently sharded by schema name.</p> <p>Schema-based sharding gives an easy path for scaling out several important classes of applications that can divide their data across schemas:</p> <ul> <li>Multi-tenant SaaS applications</li> <li>Microservices that use the same database</li> <li>Vertical partitioning by groups of tables</li> </ul> <p>Each of these scenarios can now be enabled on Citus using regular CREATE SCHEMA commands. That way, many existing applications and libraries (e.g. <a href="https://github.com/django-tenants/django-tenants">django-tenants</a>) can scale out without any changes, and developing new applications can be much easier. Moreover, you keep all the other benefits of Citus, including distributed transactions, reference tables, rebalancing, and more. </p> <p>In this blog post, you’ll get a high-level overview of schema-based sharding and other new Citus 12 features:</p> <ul> <li><a href="#what-is-schema-based-sharding">What is schema-based sharding?</a></li> <li><a href="#how-to-use-citus-schema-based-sharding">How to use Citus schema-based sharding for Postgres</a></li> <li><a href="#benefits-of-schema-based-sharding">Benefits of schema-based sharding</a></li> <li><a href="#choosing-a-sharding-model-for-multi-tenant-saas-apps">Choosing a sharding model for multi-tenant applications (schema-based vs. row-based)</a></li> <li><a href="#migrating-an-existing-schema-per-tenant-app-to-citus">Migrating an existing schema-per-tenant application to Citus</a></li> <li><a href="#merge-improvements">MERGE improvements</a></li> </ul> <p>Even more details available in the release notes on the <a href="https://www.citusdata.com/updates/v12-0">12.0 Updates page</a>. And if you want to see demos of some of this functionality, be sure to join us for the <a href="https://www.youtube.com/watch?v=_3UPL-EVwKA">livestream of the Citus 12.0 release party</a> on Wed 02 Aug (<a href="https://www.addevent.com/event/oq18048843">mark your calendar</a> and join us.</p> <p>Let’s dive in!</p> <h2 id="what-is-schema-based-sharding">What is schema-based sharding?</h2> <p><strong>Schema-based sharding means that tables from the same schema are placed on the same node, while different schemas may be on different nodes.</strong> That way, queries and transactions that involve a single schema can always be evaluated efficiently by a single node (read: without network overhead), while the system can transparently scale out to accommodate an arbitrarily large amount of data and high rate of queries across different schemas. Moreover, the cluster can be <a href="https://docs.citusdata.com/en/stable/admin_guide/cluster_management.html#rebalance-shards-without-downtime">rebalanced</a> based on disk usage, such that large schemas automatically get more resources dedicated to them, while small schemas are efficiently packed together.</p> <p><strong>So far, Citus primarily focused on “row-based sharding”, where tables are hash-distributed across nodes based on the value in the distribution column.</strong> Row-based sharding enables arbitrarily large PostgreSQL tables, with parallel distributed queries, query routing based on filters, and distributed DML and DDL. Row-based sharding is very suitable for analytical applications (e.g. IoT, time series) and other scenarios in which you need very large tables. It also works well for multi-tenant applications that keep their data in a set of shared tables, but to be able to distribute your database effectively you to add a “tenant ID” column to all your tables, and include that column in all filters, inserts, foreign keys, etc. You also need to explicitly run <a href="https://docs.citusdata.com/en/stable/develop/reference_ddl.html#creating-and-distributing-tables">create_distributed_table</a> to select a distribution column for each table. If you skip one of these steps, performance might be poor due to network overhead, or you might run into <a href="https://docs.citusdata.com/en/stable/develop/reference_workarounds.html">distributed SQL limitations</a>.</p> <p><strong>Schema-based sharding has almost no data modelling restrictions or special steps compared to unsharded PostgreSQL.</strong> That makes it easy to build a scalable multi-tenant application with a schema per tenant, or other applications where different parts of your data model can live in different schemas. The main restriction of schema-based sharding is that joins and foreign keys should only involve tables from the same schema (or <a href="https://docs.citusdata.com/en/stable/develop/reference_ddl.html#reference-tables">reference tables</a>), but transactions across schemas still work fine. That makes schema-based sharding much easier to use than row-based sharding, though it only makes sense for specific workload patterns. For applications that can use either model (e.g. multi-tenant apps), a downside of schema-based sharding is the need to manage many tables and performance overhead. A more detailed comparison is given <a href="#choosing-a-sharding-model-for-multi-tenant-saas-apps">below</a>.</p> <h2 id="how-to-use-citus-schema-based-sharding">How to use Citus schema-based sharding for Postgres</h2> <p>You can easily get started with schema-based sharding by enabling the <code>citus.enable_schema_based_sharding</code> setting. When enabled, any schema you create will become a “distributed schema”.</p> <div><pre><code><span>-- Enable schema-based sharding!</span>
<span>set</span> <span>citus</span><span>.</span><span>enable_schema_based_sharding</span> <span>to</span> <span>on</span><span>;</span>

<span>-- Create distributed schemas for two of my tenants</span>
<span>create</span> <span>schema</span> <span>tenant1</span><span>;</span>
<span>create</span> <span>schema</span> <span>tenant2</span><span>;</span>

<span>-- See my distributed schemas</span>
<span>select</span> <span>*</span> <span>from</span> <span>citus_schemas</span><span>;</span>

<span>schema_name</span>  <span>|</span> <span>colocation_id</span> <span>|</span> <span>schema_size</span> <span>|</span> <span>schema_owner</span>
<span>-------------+---------------+-------------+--------------</span>
 <span>tenant1</span>     <span>|</span>             <span>3</span> <span>|</span> <span>0</span> <span>bytes</span>     <span>|</span> <span>marco</span>
 <span>tenant2</span>     <span>|</span>             <span>4</span> <span>|</span> <span>0</span> <span>bytes</span>     <span>|</span> <span>marco</span>
<span>(</span><span>1</span> <span>row</span><span>)</span>
</code></pre></div> <p>Any table created in the <code>tenant1</code> or <code>tenant2</code> schema will now automatically become a “single shard” table that is “co-located” with all other tables in the schema. Compared to <a href="https://docs.citusdata.com/en/stable/develop/reference_ddl.html#creating-and-distributing-tables">hash-distributed tables</a> in Citus, single shard tables do not have a distribution column. There is only one shard per table, and all shards in the same schema live on the same node.</p> <div><pre><code><span>-- Create single shard tables in schema tenant1 (automatically co-located)</span>
<span>set</span> <span>search_path</span> <span>to</span> <span>tenant1</span><span>;</span>
<span>create</span> <span>table</span> <span>note_categories</span> <span>(</span>
   <span>category_id</span> <span>bigserial</span> <span>primary</span> <span>key</span><span>,</span>
   <span>category_name</span> <span>text</span> <span>not</span> <span>null</span><span>);</span>

<span>create</span> <span>table</span> <span>notes</span> <span>(</span>
   <span>note_id</span> <span>bigserial</span> <span>primary</span> <span>key</span><span>,</span>
   <span>category_id</span> <span>bigint</span> <span>references</span> <span>note_categories</span> <span>(</span><span>category_id</span><span>),</span>
   <span>message</span> <span>text</span> <span>not</span> <span>null</span><span>);</span>

<span>-- Create single shard tables in schema tenant2 (automatically co-located)</span>
<span>set</span> <span>search_path</span> <span>to</span> <span>tenant2</span><span>;</span>
<span>create</span> <span>table</span> <span>note_categories</span> <span>(</span>
   <span>category_id</span> <span>bigserial</span> <span>primary</span> <span>key</span><span>,</span>
   <span>category_name</span> <span>text</span> <span>not</span> <span>null</span><span>);</span>

<span>create</span> <span>table</span> <span>notes</span> <span>(</span>
   <span>note_id</span> <span>bigserial</span> <span>primary</span> <span>key</span><span>,</span>
   <span>category_id</span> <span>bigint</span> <span>references</span> <span>note_categories</span> <span>(</span><span>category_id</span><span>),</span>
   <span>message</span> <span>text</span> <span>not</span> <span>null</span><span>);</span>
</code></pre></div> <p>You can easily see where the tables live in the <code>citus_shards</code> view</p> <div><pre><code><span>select</span> <span>table_name</span><span>,</span> <span>shardid</span><span>,</span> <span>colocation_id</span><span>,</span> <span>nodename</span><span>,</span> <span>nodeport</span><span>,</span> <span>shard_size</span> 
<span>from</span> <span>citus_shards</span> <span>where</span> <span>citus_table_type</span> <span>=</span> <span>&#39;schema&#39;</span><span>;</span>

       <span>table_name</span>       <span>|</span> <span>shardid</span> <span>|</span> <span>colocation_id</span> <span>|</span> <span>nodename</span>  <span>|</span> <span>nodeport</span> <span>|</span> <span>shard_size</span>
<span>------------------------+---------+---------------+-----------+----------+------------</span>
<span>tenant1</span><span>.</span><span>note_categories</span> <span>|</span>  <span>102310</span> <span>|</span>             <span>3</span> <span>|</span> <span>wrk1</span><span>.</span><span>host</span> <span>|</span>     <span>5432</span> <span>|</span>      <span>16384</span>
<span>tenant1</span><span>.</span><span>notes</span>           <span>|</span>  <span>102311</span> <span>|</span>             <span>3</span> <span>|</span> <span>wrk1</span><span>.</span><span>host</span> <span>|</span>     <span>5432</span> <span>|</span>      <span>16384</span>
<span>tenant2</span><span>.</span><span>note_categories</span> <span>|</span>  <span>102312</span> <span>|</span>             <span>4</span> <span>|</span> <span>wrk2</span><span>.</span><span>host</span> <span>|</span>     <span>5432</span> <span>|</span>      <span>16384</span>
<span>tenant2</span><span>.</span><span>notes</span>           <span>|</span>  <span>102313</span> <span>|</span>             <span>4</span> <span>|</span> <span>wrk2</span><span>.</span><span>host</span> <span>|</span>     <span>5432</span> <span>|</span>      <span>16384</span>
<span>(</span><span>4</span> <span>rows</span><span>)</span>
</code></pre></div> <p>Now, any query that involves only tables from the same schema will be transparently delegated to the right node. You can either set the search_path or use fully qualified names in your queries:</p> <div><pre><code><span>-- perform a query on different tenants using search_path and relative paths</span>
<span>set</span> <span>search_path</span> <span>to</span> <span>tenant1</span><span>;</span>
<span>select</span> <span>*</span> <span>from</span> <span>notes</span> <span>join</span> <span>note_categories</span> <span>using</span> <span>(</span><span>category_id</span><span>);</span>

<span>set</span> <span>search_path</span> <span>to</span> <span>tenant2</span><span>;</span>
<span>select</span> <span>*</span> <span>from</span> <span>notes</span> <span>join</span> <span>note_categories</span> <span>using</span> <span>(</span><span>category_id</span><span>);</span>

<span>-- perform a query on different tenants using fully-qualified names</span>
<span>select</span> <span>*</span> <span>from</span> <span>tenant1</span><span>.</span><span>notes</span> <span>join</span> <span>tenant1</span><span>.</span><span>note_categories</span> <span>using</span> <span>(</span><span>category_id</span><span>);</span>
<span>select</span> <span>*</span> <span>from</span> <span>tenant2</span><span>.</span><span>notes</span> <span>join</span> <span>tenant2</span><span>.</span><span>note_categories</span> <span>using</span> <span>(</span><span>category_id</span><span>);</span>
</code></pre></div> <p>You can run these <a href="https://www.citusdata.com/blog/2022/06/17/citus-11-goes-fully-open-source/">queries from any node</a>. We are also <a href="https://github.com/pgbouncer/pgbouncer/pull/867">improving PgBouncer to handle search_path</a> correctly when using schema-based sharding.</p> <p>That’s pretty much it. To use schema-based sharding in Citus 12 or later, you create multiple schemas, and use <code>search_path</code> or fully qualified names to switch between schemas when creating tables or running queries. The schemas are transparently spread across nodes (starting on a single node) and can be <a href="https://docs.citusdata.com/en/stable/admin_guide/cluster_management.html#rebalance-shards-without-downtime">rebalanced</a>. No additional data modelling steps (like <code>create_distributed_table</code>) required!</p> <p>In a follow up blog post, we’ll dive deeper into different usage scenarios and schema management.</p> <h2 id="benefits-of-schema-based-sharding">Benefits of schema-based sharding in Citus and Postgres</h2> <p>Schema-based sharding gives you an easy path for sharding if your application and database model fits into one of these classes:</p> <p><strong>Multi-tenant applications (schema per tenant)</strong>: Software as a service (SaaS) applications serve many different tenants from a single deployment, often backed by a single database for ease-of-management and cost efficiency. Most queries done by these SaaS applications can be scoped to a single tenant, which makes the tenant a natural shard key. You can shard your Citus database by creating a schema per tenant, as an alternative to distributing tables by a tenant ID column.</p> <figure> <img src="https://www.citusdata.com/assets/images/blog/marco-blog-diagram-citus12-fig1.png" width="800" height="380" loading="lazy" alt="schema per tenant app diagram"/> <figcaption><strong>Figure 1:</strong> Schema per tenant application where each tenant has its own schema, typically with the same set of tables</figcaption> </figure> <p><strong>Microservices (schema per microservice)</strong>: Many cloud-native applications are broken down into microservices that are developed and deployed separately. Each microservice has its own state, which is usually quite small, and simple. Rather than creating and managing a separate database system for each microservice, it is useful to consolidate the state of different microservices into a single distributed database. That way, you have a single place to manage and query all data, can share data and perform transactions across microservices, and can minimize cost by sharing resources, without the database becoming a bottleneck.</p> <figure> <img src="https://www.citusdata.com/assets/images/blog/marco-blog-diagram-citus12-fig2.png" width="800" height="380" loading="lazy" alt="schema per microservice app diagram"/> <figcaption><strong>Figure 2:</strong> Schema per microservice application, with each microservice having its state in a separate schema in the same distributed database.</figcaption> </figure> <p><strong>Vertical partitioning (schema per table group):</strong> The databases of complex OLTP applications often involve many different tables, and not all of them are closely related. “Vertical partitioning” refers to the practice of sharding your database into groups related tables with each group living on its own database server. With schema-based sharding, you can easily achieve this or prepared for it upfront by assigning each group to its own schema and scale out only when necessary (and avoid all the <a href="https://www.figma.com/blog/how-figma-scaled-to-multiple-databases/">growing pains</a>).</p> <figure> <img src="https://www.citusdata.com/assets/images/blog/marco-blog-diagram-citus12-fig3.png" width="800" height="380" loading="lazy" alt="vertical partitioning diagram"/> <figcaption><strong>Figure 3:</strong> Vertical partitioning with groups of related tables combined in a schema</figcaption> </figure> <p>Within a distributed schema, you can use arbitrary joins and foreign keys. Transactions that remain scoped to a schema will be efficiently delegated to the node that stores the schema, which minimizes overhead and gives you PostgreSQL performance characteristics at any scale. Distributed transactions across schemas are still possible, with some additional network round trips.</p> <p>Schema-based sharding offers many other benefits over manual, application-level sharding:</p> <ul> <li>Very easy shard management: Just CREATE/DROP SCHEMA, and Citus does the rest.</li> <li><a href="https://docs.citusdata.com/en/stable/admin_guide/cluster_management.html#rebalance-shards-without-downtime">Automatic rebalancing</a> of schemas across nodes without downtime.</li> <li>Share data across schemas using <a href="https://docs.citusdata.com/en/stable/develop/reference_ddl.html?highlight=reference%20tables#reference-tables">reference tables</a> with support for local joins and foreign keys.</li> <li><a href="https://www.citusdata.com/blog/2023/05/12/tenant-monitoring-in-citus-and-postgres-with-citus-stat-tenants/">Tenant-level statistics</a>, which has been expanded in Citus 12 to treat schemas as tenants.</li> <li>Mixed distributed database modes, e.g. multi-tenancy apps where small tenants live in a shared table and large tenants have their own distributed schema. Or ultra-large tenants / microservices live in a regular schema with distributed tables, while others have a distributed schema.</li> <li>Transparent <a href="https://www.citusdata.com/blog/2020/11/21/making-postgres-stored-procedures-9x-faster-in-citus/">stored procedure call delegation</a> by co-locating with a table in a distributed schema.</li> <li>Can manage all schemas via transactions.</li> </ul> <p>And with all the flexibility that PostgreSQL and Citus already offer, the overall list of benefits can go on for a while.</p> <p>Of course, everything in distributed databases is ultimately still a trade-off. One thing to consider is that having a very large number of schemas (or rather, tables) can create certain performance issues in PostgreSQL. For instance, each process keeps a separate catalog cache, which can cause high memory consumption when there are many tables. We recommend carefully considering your sharding model if you expect to have over 10k schemas. Queries and transactions that span multiple schemas may also be slower than on a single PostgreSQL node, and may incur certain <a href="https://docs.citusdata.com/en/stable/develop/reference_workarounds.html">SQL limitations</a>.</p> <h2 id="choosing-a-sharding-model-for-multi-tenant-saas-apps">Choosing a sharding model for multi-tenant SaaS applications</h2> <p>Schema-based sharding is only applicable if your application naturally divides into distinct groups of tables. For applications with a single large table (e.g. IoT measurements), row-based sharding is the obvious choice. However, for multi-tenant applications, row-based sharding (by tenant ID) and schema-based sharding (with a schema per tenant) are both very reasonable options, with different characteristics.</p> <figure> <img src="https://www.citusdata.com/assets/images/blog/marco-blog-diagram-citus12-fig4.png" width="800" height="822" loading="lazy" alt="schema-based vs row-based sharding"/> <figcaption><strong>Figure 4:</strong>Side-by-side comparison of Schema-based sharding vs. Row-based sharding. In this diagram, the same colors are used on both sides of the diagram to depict data for each of the 5 tenants (green for tenant1, blue for tenant2, yellow for tenant3, grey for tenant4, orange for tenant5)—so you can visually see how the tenant data is distributed in a schema-based sharding model vs. row-based sharding.</figcaption> </figure> <p>Each of these models has different pros and cons to consider:</p> <p><strong>Schema-based sharding</strong>: Each tenant has a separate schema with its own set of tables, in the same database. Tables do not have any restrictions with regards to constraints or foreign keys, except that foreign keys should not cross the schema boundary, unless they are foreign keys to reference tables. Schema-based sharding works especially well to scale applications that have many large tenants but might see performance degradation with a very large number of tenants (&gt;10k).</p> <p><strong>Row-based sharding</strong>: The data from all tenants is in the same set of tables. Each table has a tenant ID column (or equivalent) which acts as the distribution column. Tables are co-located such that the same tenant ID lives on the same node, across different tables. Primary keys, foreign keys, joins, and filter should include the tenant ID column to ensure operations can be evaluated locally. Very large tenants should be <a href="https://docs.citusdata.com/en/v11.3/develop/api_udf.html#isolate-tenant-to-new-shard">isolated to their own shard</a>.</p> <p>A high-level comparison of these sharding models (in the context of multi-tenant applications) is given below:</p> <div> <table><thead> <tr> <th></th> <th><strong>Schema-based sharding</strong></th> <th><strong>Row-based sharding</strong></th> </tr> </thead><tbody> <tr> <td><strong>Multi-tenancy model</strong></td> <td>Separate schema per tenant</td> <td>Shared tables with tenant ID columns</td> </tr> <tr> <td><strong>Citus version</strong></td> <td>12.0+</td> <td>All versions</td> </tr> <tr> <td><strong>Additional steps compared to vanilla PostgreSQL</strong></td> <td>None, only a config change</td> <td>Use create_<wbr/>distributed_<wbr/>table on each table to distribute &amp; co-locate tables by tenant ID</td> </tr> <tr> <td><strong>Number of tenants</strong></td> <td>1-10k</td> <td>100-1M+</td> </tr> <tr> <td><strong>Data modelling requirement</strong></td> <td>No foreign keys across distributed schemas.</td> <td>Need to include a tenant ID column and use it as the a distribution column in each table, and include it in primary keys, foreign keys.</td> </tr> <tr> <td><strong>SQL requirement for single node queries</strong></td> <td>Use a single distributed schema per query.</td> <td>Joins and WHERE clauses should include tenant_id column</td> </tr> <tr> <td><strong>Parallel cross-tenant queries</strong></td> <td>No</td> <td>Yes</td> </tr> <tr> <td><strong>Custom table definitions per tenant</strong></td> <td>Yes</td> <td>No</td> </tr> <tr> <td><strong>Access control</strong></td> <td>Schema permissions</td> <td>Row-level security</td> </tr> <tr> <td><strong>Data sharing across tenants</strong></td> <td>Yes, using reference tables (in a separate schema)</td> <td>Yes, using reference tables</td> </tr> <tr> <td><strong>Tenant to shard isolation</strong></td> <td>Every tenant has its own shard group by definition</td> <td>Can give specific tenant IDs their own shard group via <a href="https://docs.citusdata.com/en/stable/develop/api_udf.html#isolate-tenant-to-new-shard">isolate_<wbr/>tenant_<wbr/>to_<wbr/>new_shard</a></td> </tr> </tbody></table> </div> <p>Which model to choose depends on your requirements. The biggest benefit of schema-based sharding is ease-of-use; no additional data modelling steps are required, only a single setting change. If you have a smaller number of large tenants (B2B), and some require a custom table definition or permissions, then schema-based sharding is also a great fit. If you have a very large number of small tenants (B2C) and want to simplify schema management and cross-tenant queries, then row-based sharding is likely to be a better fit.</p> <p>If you need both, then consider that nothing prevents you from using one set of distributed tables for small tenants with row-based sharding, and separate distributed schemas for large tenants.</p> <h2 id="migrating-an-existing-schema-per-tenant-app-to-citus">Migrating an existing schema-per-tenant application to Citus</h2> <p>One more thing… if you already have an existing PostgreSQL application that uses a schema per tenant, then you can easily migrate into a Citus cluster that uses schema-based sharding using <a href="https://github.com/dimitri/pgcopydb">pgcopydb</a>, a powerful open-source database migration tool created by my colleague Dimitri Fontaine.</p> <p>To see it in action, make sure <code>citus.enable_schema_based_sharding = on</code> is in your postgresql.conf and then run:</p> <div><pre><code>pgcopydb clone <span>--source</span> <span>&#34;host=source.host&#34;</span> <span>--target</span> <span>&#34;host=target.host&#34;</span> <span>--restart</span> <span>--drop-if-exists</span> <span>--skip-extensions</span>
</code></pre></div> <p>You can also use the <code>--follow</code> option to replay writes that happen during the migration to avoid downtime. You should not make schema changes or create/drop schemas during the migration. We do recommend extensively testing your application before switching over, and reporting any issues via the <a href="https://github.com/citusdata/citus">Citus GitHub repo</a>.</p> <h2 id="merge-improvements">Extending the MERGE superpower in Citus 12</h2> <p>Schema-based sharding is super exciting and the biggest enhancement in Citus 12, but we also continue to improve Citus for other scenarios, including row-based multi-tenancy and Internet-of-things (IoT) scenarios.</p> <p>PostgreSQL 15 introduced the <a href="https://www.postgresql.org/docs/current/sql-merge.html">MERGE command</a>, which is especially useful when you receive a batch of records and you not only want to insert new records, but also update or delete existing records.</p> <p>For instance, consider the following IoT scenario where we want to maintain a table of “active alerts” based on incoming measurements:</p> <div><pre><code><span>create</span> <span>table</span> <span>alerts</span> <span>(</span>
   <span>alert_id</span> <span>bigserial</span> <span>primary</span> <span>key</span><span>,</span>
   <span>metric</span> <span>text</span> <span>not</span> <span>null</span><span>,</span>
   <span>upper_bound</span> <span>double</span> <span>precision</span> <span>not</span> <span>null</span><span>);</span>
<span>create</span> <span>index</span> <span>on</span> <span>alerts</span> <span>(</span><span>metric</span><span>);</span>
<span>select</span> <span>create_reference_table</span><span>(</span><span>&#39;alerts&#39;</span><span>);</span>

<span>create</span> <span>table</span> <span>active_alerts</span> <span>(</span>
   <span>alert_id</span> <span>bigint</span> <span>not</span> <span>null</span><span>,</span>
   <span>device_id</span> <span>bigint</span> <span>not</span> <span>null</span><span>,</span>
   <span>alert_start_time</span> <span>timestamptz</span> <span>not</span> <span>null</span><span>,</span>
   <span>max_value</span> <span>double</span> <span>precision</span><span>,</span>
   <span>primary</span> <span>key</span> <span>(</span><span>alert_id</span><span>,</span> <span>device_id</span><span>));</span>
<span>-- distributed by alert_id for fast lookup of all devices that are exceeding the threshold for a particular alert</span>
<span>select</span> <span>create_distributed_table</span><span>(</span><span>&#39;active_alerts&#39;</span><span>,</span> <span>&#39;alert_id&#39;</span><span>);</span>

<span>create</span> <span>table</span> <span>measurements</span> <span>(</span>
   <span>measurement_id</span> <span>uuid</span> <span>not</span> <span>null</span> <span>default</span> <span>uuid_generate_v4</span><span>(),</span>
   <span>device_id</span> <span>bigint</span> <span>not</span> <span>null</span><span>,</span>
   <span>payload</span> <span>jsonb</span> <span>not</span> <span>null</span><span>,</span>
   <span>measurement_time</span> <span>timestamptz</span> <span>default</span> <span>now</span><span>(),</span>
   <span>primary</span> <span>key</span> <span>(</span><span>device_id</span><span>,</span> <span>measurement_id</span><span>));</span>
<span>-- distributed by device_id_id for fast grouping by device_id</span>
<span>select</span> <span>create_distributed_table</span><span>(</span><span>&#39;measurements&#39;</span><span>,</span> <span>&#39;device_id&#39;</span><span>);</span>

<span>insert</span> <span>into</span> <span>alerts</span> <span>(</span><span>metric</span><span>,</span> <span>upper_bound</span><span>)</span> <span>values</span> <span>(</span><span>&#39;temperature-9177&#39;</span><span>,</span> <span>38</span><span>.</span><span>0</span><span>);</span>
<span>insert</span> <span>into</span> <span>measurements</span> <span>(</span><span>device_id</span><span>,</span> <span>payload</span><span>)</span> <span>values</span> <span>(</span><span>1</span><span>,</span> <span>&#39;{&#34;value&#34;:37.5, &#34;metric&#34;:&#34;temperature-9177&#34;}&#39;</span><span>);</span>
<span>insert</span> <span>into</span> <span>measurements</span> <span>(</span><span>device_id</span><span>,</span> <span>payload</span><span>)</span> <span>values</span> <span>(</span><span>1</span><span>,</span> <span>&#39;{&#34;value&#34;:38.5, &#34;metric&#34;:&#34;temperature-9177&#34;}&#39;</span><span>);</span>
<span>insert</span> <span>into</span> <span>measurements</span> <span>(</span><span>device_id</span><span>,</span> <span>payload</span><span>)</span> <span>values</span> <span>(</span><span>1</span><span>,</span> <span>&#39;{&#34;value&#34;:38.6, &#34;metric&#34;:&#34;temperature-9177&#34;}&#39;</span><span>);</span>
<span>insert</span> <span>into</span> <span>measurements</span> <span>(</span><span>device_id</span><span>,</span> <span>payload</span><span>)</span> <span>values</span> <span>(</span><span>1</span><span>,</span> <span>&#39;{&#34;value&#34;:38.7, &#34;metric&#34;:&#34;temperature-9177&#34;}&#39;</span><span>);</span>
</code></pre></div> <p>We want to periodically register all devices for which the last 3 measurements exceeded a particular threshold. When the last 3 measurements were all below the threshold, we want to delete it. We can do that using a MERGE command:</p> <div><pre><code><span>/* run every minute using pg_cron */</span>
<span>select</span> <span>cron</span><span>.</span><span>schedule</span><span>(</span><span>&#39;periodic-merge&#39;</span><span>,</span> <span>&#39;* * * * * &#39;</span><span>,</span> <span>$$</span>

<span>/* lets have some fun with (distributed) SQL */</span>
<span>merge</span> <span>into</span> <span>active_alerts</span> <span>aa</span>
<span>using</span> <span>(</span>
  <span>/* select minimum and maximum value across last 3 measurements, per device */</span>
  <span>select</span> <span>alert_id</span><span>,</span> <span>device_id</span><span>,</span> <span>min</span><span>(</span><span>value</span><span>)</span> <span>as</span> <span>min_value</span><span>,</span> <span>max</span><span>(</span><span>value</span><span>)</span> <span>as</span> <span>max_value</span><span>,</span> <span>upper_bound</span><span>,</span> <span>max</span><span>(</span><span>measurement_time</span><span>)</span> <span>as</span> <span>measurement_time</span>
  <span>from</span> <span>(</span>
    <span>/* get time-ordered measurements by device (latest is row_number 1) */</span>
    <span>select</span>
      <span>device_id</span><span>,</span>
      <span>payload</span><span>-&gt;&gt;</span><span>&#39;metric&#39;</span> <span>as</span> <span>metric</span><span>,</span>
      <span>(</span><span>payload</span><span>-&gt;&gt;</span><span>&#39;value&#39;</span><span>)::</span><span>double</span> <span>precision</span> <span>as</span> <span>value</span><span>,</span>
      <span>measurement_time</span><span>,</span>
      <span>row_number</span><span>()</span> <span>over</span><span>(</span><span>partition</span> <span>by</span> <span>device_id</span> <span>order</span> <span>by</span> <span>measurement_time</span> <span>desc</span><span>)</span> <span>as</span> <span>row_number</span>
    <span>from</span> <span>measurements</span>
  <span>)</span> <span>meas</span>
  <span>join</span> <span>alerts</span> <span>on</span> <span>(</span><span>meas</span><span>.</span><span>metric</span> <span>=</span> <span>alerts</span><span>.</span><span>metric</span><span>)</span>
  <span>where</span> <span>row_number</span> <span>&lt;=</span> <span>3</span>
  <span>group</span> <span>by</span> <span>alert_id</span><span>,</span> <span>device_id</span>
<span>)</span> <span>m</span>
<span>on</span> <span>aa</span><span>.</span><span>alert_id</span> <span>=</span> <span>m</span><span>.</span><span>alert_id</span> <span>AND</span> <span>aa</span><span>.</span><span>device_id</span> <span>=</span> <span>m</span><span>.</span><span>device_id</span>

<span>/* if the threshold is broken and the alert is not yet active, insert it */</span>
<span>when</span> <span>not</span> <span>matched</span> <span>and</span> <span>m</span><span>.</span><span>min_value</span> <span>&gt;</span> <span>m</span><span>.</span><span>upper_bound</span> <span>then</span>
<span>insert</span> <span>values</span> <span>(</span><span>m</span><span>.</span><span>alert_id</span><span>,</span> <span>m</span><span>.</span><span>device_id</span><span>,</span> <span>m</span><span>.</span><span>measurement_time</span><span>,</span> <span>m</span><span>.</span><span>max_value</span><span>)</span>

<span>/* if the threshold is broken and the alert is already active, update it */</span>
<span>when</span> <span>matched</span> <span>and</span> <span>m</span><span>.</span><span>max_value</span> <span>&gt;</span> <span>aa</span><span>.</span><span>max_value</span> <span>then</span> <span>update</span> <span>set</span> <span>max_value</span> <span>=</span> <span>m</span><span>.</span><span>max_value</span>

<span>/* if the threshold is no longer broken but the alert is still active, delete it */</span>
<span>when</span> <span>matched</span> <span>and</span> <span>m</span><span>.</span><span>max_value</span> <span>&lt;=</span> <span>m</span><span>.</span><span>upper_bound</span> <span>then</span> <span>delete</span><span>;</span>

<span>$$</span><span>);</span>
</code></pre></div> <p>What’s new in Citus 12 is that this MERGE works even though the measurements table and the active_alerts table are not co-located (their distribution columns do not match). The MERGE will re-partition the data across the cluster on the fly, in one parallel, distributed transaction. That makes MERGE the most advanced distributed database command available in Citus.</p> <p>If you use MERGE in combination with schema-based sharding, then it will be fully pushed down to the node that stores the schema and handled by PostgreSQL.</p> <h2>Start building scalable apps</h2> <p>Schema-based sharding in Citus 12 gives you a new and easy way to scale out your PostgreSQL database. Whether you are an ISV building a multi-tenant SaaS application, microservices, or a complex OLTP workload that can use vertical partitioning, you can now easily tell Citus how to distribute the database by grouping tables into schemas without any special syntax. There are various application development libraries that will help you do this, but you can also easily build it yourself through simple commands like <code>SET search_path TO &lt;schema name&gt;</code> or using variables schema names in your queries.</p> <p>To learn even more about what Citus 12.0 can do for you:</p> <ul> <li>Check out the <a href="https://www.citusdata.com/updates/v12-0/">12.0 Updates page</a> if you want to read through the detailed release notes</li> <li>Join the <a href="https://www.youtube.com/watch?v=_3UPL-EVwKA">Livestream of the Citus 12.0 Release Party, with demos</a>: The livestream will include a demo of schema-based sharding and is happening on Wed 02 Aug @ 9:00am PDT (<a href="https://www.addevent.com/event/oq18048843">mark your calendar</a>.)</li> </ul> <p>And if you want to get started distributing Postgres with Citus, the links in the bullets below should be useful:</p> <ul> <li><a href="https://github.com/citusdata/citus">Citus open source repo on GitHub</a></li> <li><a href="https://www.citusdata.com/getting-started/">Getting started page for Citus</a></li> <li><a href="https://www.citusdata.com/download/">Download Citus open source</a></li> <li><a href="https://docs.citusdata.com/">Citus open source docs</a></li> <li><a href="https://slack.citusdata.com/">Join the Citus Public Slack</a></li> <li><a href="https://learn.microsoft.com/azure/cosmos-db/postgresql/quickstart-create-portal">Quickstart docs for Citus on Azure</a>, now known as Azure Cosmos DB for PostgreSQL… Since Citus 12.0 is just released, it is not yet available on Azure but it will be soon. Oh, and there is a <a href="https://aka.ms/trycosmosdb">free trial</a>, too.</li> </ul>   <div> <h3> Enjoy what you’re reading? </h3> <p>If you want to read more posts from our Citus database and Postgres teams, sign up for our monthly newsletter and get the latest content delivered straight to your inbox.</p>    </div>     </div> </section>  </div> </div></div>
  </body>
</html>
