<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.blog/2023-02-06-the-technology-behind-githubs-new-code-search/">Original</a>
    <h1>The technology behind GitHub’s new code search</h1>
    
    <div id="readability-page-1" class="page"><div>
  <div>
    


<main role="main" id="post-69904">
  
<p>From launching our <a href="https://github.blog/2021-12-08-improving-github-code-search/">technology preview</a> of the new and improved code search experience a year ago, to the <a href="https://github.blog/2022-11-15-a-better-way-to-search-navigate-and-understand-code-on-github/">public beta</a> we released at GitHub Universe last November, there’s been a flurry of innovation and dramatic changes to some of the core GitHub product experiences around how we, as developers, find, read, and navigate code.</p>
<p>One question we hear about the new code search experience is, “How does it work?” And to complement <a href="https://www.youtube.com/watch?v=QCs76SC1ZZ0">a talk I gave at GitHub Universe</a>, this post gives a high-level answer to that question and provides a small window into the system architecture and technical underpinnings of the product.</p>
<p>So, how <em>does</em> it work? The short answer is that we built our own search engine from scratch, in Rust, specifically for the domain of code search. We call this search engine Blackbird, but before I explain how it works, I think it helps to understand our motivation a little bit. At first glance, building a search engine from scratch seems like a questionable decision. Why would you do that? Aren’t there plenty of existing, open source solutions out there already? Why build something new?</p>
<p>To be fair, we’ve tried and have been trying, for almost the entire history of GitHub, to use existing solutions for this problem. You can read a bit more about our journey in Pavel Avgustinov’s post, <a href="https://github.blog/2021-12-15-a-brief-history-of-code-search-at-github/">A brief history of code search at GitHub</a>, but one thing sticks out: we haven’t had a lot of luck using general text search products to power <strong><em>code</em></strong> search. The user experience is poor, indexing is slow, and it’s expensive to host. There are some newer, code-specific open source projects out there, but they definitely don’t work at GitHub’s scale. So, knowing all that, we were motivated to create our own solution by three things:</p>
<ol>
<li>We’ve got a vision for an entirely new user experience that’s about being able to ask questions of code and get answers through iteratively searching, browsing, navigating, and reading code.</li>
<li>We understand that <strong><em>code</em></strong> search is uniquely different from general text search. Code is already designed to be understood by machines and we should be able to take advantage of that structure and relevance. Searching code also has unique requirements: we want to search for punctuation (for example, a period or open parenthesis); we don’t want stemming; we don’t want stop words to be stripped from queries; and, we want to search with regular expressions.</li>
<li>GitHub’s scale is truly a unique challenge. When we first deployed Elasticsearch, it took months to index all of the code on GitHub (about 8 million repositories at the time). Today, that number is north of 200 million, and that code isn’t static: it’s constantly changing and that’s quite challenging for search engines to handle. For the beta, you can currently search almost 45 million repositories, representing 115 TB of code and 15.5 billion documents. </li>
</ol>
<p>At the end of the day, nothing off the shelf met our needs, so we built something from scratch.</p>
<h2 id="just-use-grep">Just use grep?<a href="#just-use-grep" aria-label="Just use grep?"></a></h2>
<p>First though, let’s explore the brute force approach to the problem. We get this question a lot: “Why don’t you just use grep?” To answer that, let’s do a little napkin math using <a href="https://github.com/BurntSushi/ripgrep">ripgrep</a> on that 115 TB of content. On a machine with an eight core Intel CPU, ripgrep <a href="https://github.com/BurntSushi/ripgrep#quick-examples-comparing-tools">can run an exhaustive regular expression query</a> on a 13 GB file cached in memory in 2.769 seconds, or about 0.6 GB/sec/core.</p>
<p>We can see pretty quickly that this really isn’t going to work for the larger amount of data we have. Code search runs on 64 core, 32 machine clusters. Even if we managed to put 115 TB of code in memory and assume we can perfectly parallelize the work, we’re going to saturate 2,048 CPU cores for 96 seconds to serve a single query! Only that one query can run. Everybody else has to get in line. This comes out to a whopping 0.01 queries per second (QPS) and good luck doubling your QPS—that’s going to be a fun conversation with leadership about your infrastructure bill.</p>
<p>There’s just no cost-effective way to scale this approach to all of GitHub’s code and all of GitHub’s users. Even if we threw a ton of money at the problem, it still wouldn’t meet our user experience goals.</p>
<p>You can see where this is going: we need to build an index.</p>
<h2 id="a-search-index-primer">A search index primer<a href="#a-search-index-primer" aria-label="A search index primer"></a></h2>
<p>We can only make queries fast if we pre-compute a bunch of information in the form of indices, which you can think of as maps from keys to sorted lists of document IDs (called “posting lists”) where that key appears. As an example, here’s a small index for programming languages. We scan each document to detect what programming language it’s written in, assign a document ID, and then create an inverted index where language is the key and the value is a posting list of document IDs.</p>
<h3 id="forward-index">Forward index<a href="#forward-index" aria-label="Forward index"></a></h3>
<div><table>
<tbody><tr>
<td><strong>Doc ID</strong>
   </td>
<td><strong>Content</strong>
   </td>
</tr>
<tr>
<td>1
   </td>
<td>def lim</td>
</tr>
<tr>
<td>2
   </td>
<td>fn limits() {
   </td>
</tr>
<tr>
<td>3
   </td>
<td>function mits() {
   </td>
</tr>
</tbody></table></div>
<h3 id="inverted-index">Inverted index<a href="#inverted-index" aria-label="Inverted index"></a></h3>
<div><table>
<tbody><tr>
<td><strong>Language</strong>
   </td>
<td><strong>Doc IDs (postings)</strong>
   </td>
</tr>
<tr>
<td>JavaScript
   </td>
<td>3, 8, 12, …
   </td>
</tr>
<tr>
<td>Ruby
   </td>
<td>1, 10, 13, …
   </td>
</tr>
<tr>
<td>Rust
   </td>
<td>2, 5, 11, …
   </td>
</tr>
</tbody></table></div>
<p>For code search, we need a special type of inverted index called an ngram index, which is useful for looking up substrings of content. An <a href="https://en.wikipedia.org/wiki/N-gram">ngram</a> is a sequence of characters of length <em>n</em>. For example, if we choose n=3 (trigrams), the ngrams that make up the content “limits” are <code>lim</code>, <code>imi</code>, <code>mit</code>, <code>its</code>. With our documents above, the index for those trigrams would look like this:</p>
<div><table>
<tbody><tr>
<td><strong>ngram</strong>
   </td>
<td><strong>Doc IDs (postings)</strong>
   </td>
</tr>
<tr>
<td>lim
   </td>
<td>1, 2, …
   </td>
</tr>
<tr>
<td>imi
   </td>
<td>2, …
   </td>
</tr>
<tr>
<td>mit
   </td>
<td>1, 2, 3, …
   </td>
</tr>
<tr>
<td>its
   </td>
<td>2, 3, …
   </td>
</tr>
</tbody></table></div>
<p>To perform a search, we intersect the results of multiple lookups to give us the list of documents where the string appears. With a trigram index you need four lookups: <code>lim</code>, <code>imi</code>, <code>mit</code>, and <code>its</code> in order to fulfill the query for <code>limits</code>.</p>
<p>Unlike a hashmap though, these indices are too big to fit in memory, so instead, we build iterators for each index we need to access. These lazily return sorted document IDs (the IDs are assigned based on the ranking of each document) and we intersect and union the iterators (as demanded by the specific query) and only read far enough to fetch the requested number of results. That way we never have to keep entire posting lists in memory.</p>
<h2 id="indexing-45-million-repositories">Indexing 45 million repositories<a href="#indexing-45-million-repositories" aria-label="Indexing 45 million repositories"></a></h2>
<p>The next problem we have is how to build this index in a reasonable amount of time (remember, this took months in our first iteration). As is often the case, the trick here is to identify some insight into the specific data we’re working with to guide our approach. In our case it’s two things: Git’s use of <a href="https://en.wikipedia.org/wiki/K-way_merge_algorithm">content addressable hashing</a> and the fact that there’s actually quite a lot of duplicate content on GitHub. Those two insights lead us the the following decisions:</p>
<ol>
<li><strong>Shard by Git blob object ID</strong> which gives us a nice way of evenly distributing documents between the shards while avoiding any duplication. There won’t be any hot servers due to special repositories and we can easily scale the number of shards as necessary.</li>
<li><strong>Model the index as a tree</strong> and use delta encoding to reduce the amount of crawling and to optimize the metadata in our index. For us, metadata are things like the list of locations where a document appears (which path, branch, and repository) and information about those objects (repository name, owner, visibility, etc.). This data can be quite large for popular content.</li>
</ol>
<p>We also designed the system so that query results are consistent on a commit-level basis. If you search a repository while your teammate is pushing code, your results shouldn’t include documents from the new commit until it has been fully processed by the system. In fact, while you’re getting back results from a repository-scoped query, someone else could be paging through global results and looking at a different, prior, but still <strong><em>consistent</em></strong> state of the index. This is tricky to do with other search engines. Blackbird provides this level of query consistency as a core part of its design.</p>
<h2 id="lets-build-an-index">Let’s build an index<a href="#lets-build-an-index" aria-label="Let’s build an index"></a></h2>
<p>Armed with those insights, let’s turn our attention to building an index with Blackbird. This diagram represents a high level overview of the ingest and indexing side of the system.</p>
<p><img decoding="async" src="https://github.blog/wp-content/uploads/2023/02/Canvas_1-1.png?w=1024&amp;resize=1024%2C692" alt="a high level overview of the ingest and indexing side of the system" width="1024" height="692" srcset="https://github.blog/wp-content/uploads/2023/02/Canvas_1-1.png?w=1376 1376w, https://github.blog/wp-content/uploads/2023/02/Canvas_1-1.png?w=300 300w, https://github.blog/wp-content/uploads/2023/02/Canvas_1-1.png?w=768 768w, https://github.blog/wp-content/uploads/2023/02/Canvas_1-1.png?w=1024&amp;resize=1024%2C692 1024w" sizes="(max-width: 1000px) 100vw, 1000px" loading="lazy" data-recalc-dims="1"/></p>
<p>Kafka provides events that tell us to go index something. There are a bunch of crawlers that interact with Git and a service for extracting symbols from code, and then we use Kafka, again, to allow each shard to consume documents for indexing at its own pace.</p>
<p>Though the system generally just responds to events like <code>git push</code> to crawl changed content, we have some work to do to ingest all the repositories for the first time. One key property of the system is that we optimize the order in which we do this initial ingest to make the most of our delta encoding. We do this with a novel probabilistic data structure representing repository similarity and by driving ingest order from a level order traversal of a <a href="https://en.wikipedia.org/wiki/Minimum_spanning_tree">minimum spanning tree</a> of a graph of repository similarity<sup id="fnref-69904-1"><a href="#fn-69904-1" title="Read footnote.">1</a></sup>.</p>
<p>Using our optimized ingest order, each repository is then crawled by diffing it against its parent in the delta tree we’ve constructed. This means we only need to crawl the blobs unique to that repository (not the entire repository). Crawling involves fetching blob content from Git, analyzing it to extract symbols, and creating documents that will be the input to indexing.</p>
<p>These documents are then published to another Kafka topic. This is where we partition<sup id="fnref-69904-2"><a href="#fn-69904-2" title="Read footnote.">2</a></sup> the data between shards. Each shard consumes a single Kafka partition in the topic. Indexing is decoupled from crawling through the use of Kafka and the ordering of the messages in Kafka is how we gain query consistency.</p>
<p>The indexer shards then take these documents and build their indices: tokenizing to construct ngram indices<sup id="fnref-69904-bignote"><a href="#fn-69904-bignote" title="Read footnote.">3</a></sup> (for content, symbols, and paths) and other useful indices (languages, owners, repositories, etc) before serializing and flushing to disk when enough work has accumulated.</p>
<p>Finally, the shards run compaction to collapse up smaller indices into larger ones that are more efficient to query and easier to move around (for example, to a read replica or for backups). Compaction also <a href="https://en.wikipedia.org/wiki/Content-addressable_storage">k-merges</a> the posting lists by score so relevant documents have lower IDs and will be returned first by the lazy iterators. During the initial ingest, we delay compaction and do one big run at the end, but then as the index keeps up with incremental changes, we run compaction on a shorter interval as this is where we handle things like document deletions.</p>
<h2 id="life-of-a-query">Life of a query<a href="#life-of-a-query" aria-label="Life of a query"></a></h2>
<p>Now that we have an index, it’s interesting to trace a query through the system. The query we’re going to follow is a regular expression qualified to the <a href="https://github.com/rails">Rails organization</a> looking for code written in the Ruby programming language: <code>/arguments?/ org:rails lang:Ruby</code>. The high level architecture of the query path looks a little bit like this:</p>
<p><img decoding="async" loading="lazy" src="https://github.blog/wp-content/uploads/2023/02/code-search.png?w=1024&amp;resize=1024%2C508" alt="Architecture diagram of a query path." width="1024" height="508" srcset="https://github.blog/wp-content/uploads/2023/02/code-search.png?w=1376 1376w, https://github.blog/wp-content/uploads/2023/02/code-search.png?w=300 300w, https://github.blog/wp-content/uploads/2023/02/code-search.png?w=768 768w, https://github.blog/wp-content/uploads/2023/02/code-search.png?w=1024&amp;resize=1024%2C508 1024w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1"/></p>
<p>In between GitHub.com and the shards is a service that coordinates taking user queries and fanning out requests to each host in the search cluster. We use Redis to manage quotas and cache some access control data.</p>
<p>The front end accepts the user query and passes it along to the Blackbird query service where we parse the query into an abstract syntax tree and then rewrite it, resolving things like languages to their canonical <a href="https://github.com/github/linguist">Linguist</a> language ID and tagging on extra clauses for permissions and scopes. In this case, you can see how rewriting ensures that I’ll get results from public repositories or any private repositories that I have access to.</p>
<pre><code>And(
    Owner(&#34;rails&#34;),
    LanguageID(326),
    Regex(&#34;arguments?&#34;),
    Or(
        RepoIDs(...),
        PublicRepo(),
    ),
)
</code></pre>
<p>Next, we fan out and send <em>n</em> concurrent requests: one to each shard in the search cluster. Due to our sharding strategy, a query request must be sent to each shard in the cluster.</p>
<p>On each individual shard, we then do some further conversion of the query in order to lookup information in the indices. Here, you can see that the regex gets translated into a series of substring queries on the ngram indices.</p>
<pre><code>and(
  owners_iter(&#34;rails&#34;),
  languages_iter(326),
  or(
    and(
      content_grams_iter(&#34;arg&#34;),
      content_grams_iter(&#34;rgu&#34;),
      content_grams_iter(&#34;gum&#34;),
      or(
        and(
         content_grams_iter(&#34;ume&#34;),
         content_grams_iter(&#34;ment&#34;)
        )
        content_grams_iter(&#34;uments&#34;),
      )
    ),
    or(paths_grams_iter…)
    or(symbols_grams_iter…)
  ), 
  …
)
</code></pre>
<p>If you want to learn more about a method to turn regular expressions into substring queries, see Russ Cox’s article on <a href="https://swtch.com/~rsc/regexp/regexp4.html">Regular Expression Matching with a Trigram Index</a>. We use a different algorithm and dynamic gram sizes instead of trigrams (see below<sup id="fnref2:69904-bignote"><a href="#fn-69904-bignote" title="Read footnote.">3</a></sup>). In this case the engine uses the following grams: <code>arg</code>,<code>rgu</code>, <code>gum</code>, and then either <code>ume</code> and <code>ment</code>, or the 6 gram <code>uments</code>.</p>
<p>The iterators from each clause are run: <em>and</em> means intersect, <em>or</em> means union. The result is a list of documents. We still have to double check each document (to validate matches and detect ranges for them) before scoring, sorting, and returning the requested number of results.</p>
<p>Back in the query service, we aggregate the results from all shards, re-sort by score, filter (to double-check permissions), and return the top 100. The GitHub.com front end then still has to do syntax highlighting, term highlighting, pagination, and finally we can render the results to the page.</p>
<p>Our p99 response times from individual shards are on the order of 100 ms, but total response times are a bit longer due to aggregating responses, checking permissions, and things like syntax highlighting. A query ties up a single CPU core on the index server for that 100 ms, so our 64 core hosts have an upper bound of something like 640 queries per second. Compared to the grep approach (0.01 QPS), that’s screaming fast with plenty of room for simultaneous user queries and future growth.</p>
<h2 id="in-summary">In summary<a href="#in-summary" aria-label="In summary"></a></h2>
<p>Now that we’ve seen the full system, let’s revisit the scale of the problem. Our ingest pipeline can publish around 120,000 documents per second, so working through those 15.5 billion documents should take about 36 hours. But delta indexing reduces the number of documents we have to crawl by over 50%, which allows us to re-index the entire corpus in about 18 hours.</p>
<p>There are some big wins on the size of the index as well. Remember that we started with 115 TB of content that we want to search. Content deduplication and delta indexing brings that down to around 28 TB of <strong>unique</strong> content. And the index itself clocks in at just 25 TB, which includes not only all the indices (including the ngrams), but also a compressed copy of all unique content. This means our total index size including the content is roughly a quarter the size of the original data!</p>
<p>If you haven’t signed up already, we’d love for you to <a href="https://github.com/features/code-search">join our beta</a> and try out the new code search experience. Let us know what you think! We’re actively adding more repositories and fixing up the rough edges based on feedback from people just like you.</p>

<h3 id="notes">Notes<a href="#notes" aria-label="Notes"></a></h3>


      
  </main>


  </div>
</div></div>
  </body>
</html>
