<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/MILS">Original</a>
    <h1>LLMs can see and hear without any training</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Official implementation of the paper <strong>LLMs can see and hear without any training</strong>.</p>
<p dir="auto"><a href="https://arxiv.org/pdf/2501.18096.pdf" rel="nofollow"><img src="https://camo.githubusercontent.com/5c9d87323bc669f5d0a4e2e3b1f532a11656a0da9b087c291796e6fefb493f4b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323530312e31383039362d3030666630302e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2501.18096-00ff00.svg"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/MILS/blob/main/teaser.png"><img src="https://github.com/facebookresearch/MILS/raw/main/teaser.png" alt="Teaser"/></a></p>

<p dir="auto">Install the conda environment using</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda env create -f environment.yml
conda activate MILS"><pre>conda env create -f environment.yml
conda activate MILS</pre></div>

<p dir="auto">Download the following datasets, annotations, and checkpoints</p>
<p dir="auto"><strong>MS-COCO</strong>: Download the MS-COCO validation dataset from the official website <a href="https://cocodataset.org/#download" rel="nofollow">here</a>. Also, download the 5000 samples test split used in Karpathy et al., <em>Deep visual-semantic alignments for generating image descriptions</em>, CVPR 2015.</p>
<div dir="auto" data-snippet-clipboard-copy-content="wget http://images.cocodataset.org/zips/val2014.zip
wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip 

unzip val2014.zip
unzip annotations_trainval2014.zip"><pre>wget http://images.cocodataset.org/zips/val2014.zip
wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip 

unzip val2014.zip
unzip annotations_trainval2014.zip</pre></div>
<p dir="auto"><strong>Clotho</strong>: Download the clotho dataset from the official website <a href="https://zenodo.org/records/3490684" rel="nofollow">here</a>. We use the test split of this dataset for our benchmarking.</p>
<div dir="auto" data-snippet-clipboard-copy-content="wget https://zenodo.org/records/3490684/files/clotho_audio_evaluation.7z
pip3 install dtrx
wget https://www.7-zip.org/a/7z2107-linux-x64.tar.xz
tar xf 7z2107-linux-x64.tar.xz
./7zz e clotho_audio_evaluation.7z
wget https://zenodo.org/records/3490684/files/clotho_captions_evaluation.csv
"><pre>wget https://zenodo.org/records/3490684/files/clotho_audio_evaluation.7z
pip3 install dtrx
wget https://www.7-zip.org/a/7z2107-linux-x64.tar.xz
tar xf 7z2107-linux-x64.tar.xz
./7zz e clotho_audio_evaluation.7z
wget https://zenodo.org/records/3490684/files/clotho_captions_evaluation.csv
</pre></div>
<p dir="auto"><strong>MSR-VTT</strong>: Download the dataset from <a href="https://cove.thecvf.com/datasets/839" rel="nofollow">here</a>. We use the test split of this dataset.</p>
<div dir="auto" data-snippet-clipboard-copy-content="wget https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip
unzip MSRVTT.zip"><pre>wget https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip
unzip MSRVTT.zip</pre></div>
<p dir="auto"><strong>ViClip-InternVid-10M-FLT.pth</strong>: Download from
<a href="https://huggingface.co/OpenGVLab/ViCLIP/blob/main/ViClip-InternVid-10M-FLT.pth" rel="nofollow">here</a> and set the correct path in <code>task_utils/video/viclip.py</code>.</p>

<p dir="auto">Update the variables in <a href="https://github.com/facebookresearch/MILS/blob/main/paths.py">paths.py</a> to set the dataset directory, and the output folder.</p>

<p dir="auto">MILS is an inference-only method that can be run on a single A100 GPU. We run the experiments on eight A100 GPUs, and the code below can be adjusted for any number of GPUs.</p>

<p dir="auto">Generate captions using</p>
<div dir="auto" data-snippet-clipboard-copy-content="CUDA_VISIBLE_DEVICES=0 python main_image_captioning.py --process 0 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=1 python main_image_captioning.py --process 1 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=2 python main_image_captioning.py --process 2 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=3 python main_image_captioning.py --process 3 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=4 python main_image_captioning.py --process 4 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=5 python main_image_captioning.py --process 5 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=6 python main_image_captioning.py --process 6 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=7 python main_image_captioning.py --process 7 --num_processes 8 --batch_size 32 &amp;"><pre>CUDA_VISIBLE_DEVICES=0 python main_image_captioning.py --process 0 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=1 python main_image_captioning.py --process 1 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=2 python main_image_captioning.py --process 2 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=3 python main_image_captioning.py --process 3 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=4 python main_image_captioning.py --process 4 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=5 python main_image_captioning.py --process 5 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=6 python main_image_captioning.py --process 6 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=7 python main_image_captioning.py --process 7 --num_processes 8 --batch_size 32 <span>&amp;</span></pre></div>
<p dir="auto">The captions are saved in <code>OUTPUT_DIR</code>. Specify this path in <code>ours_result_path</code> variable in <code>eval/image_captioning.py</code> and then obtain captioning metrics as</p>
<div dir="auto" data-snippet-clipboard-copy-content="python eval/image_captioning.py"><pre>python eval/image_captioning.py</pre></div>

<p dir="auto">Generate captions using</p>
<div dir="auto" data-snippet-clipboard-copy-content="CUDA_VISIBLE_DEVICES=0 python main_audio_captioning.py --process 0 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=1 python main_audio_captioning.py --process 1 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=2 python main_audio_captioning.py --process 2 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=3 python main_audio_captioning.py --process 3 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=4 python main_audio_captioning.py --process 4 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=5 python main_audio_captioning.py --process 5 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=6 python main_audio_captioning.py --process 6 --num_processes 8 --batch_size 32 &amp;
CUDA_VISIBLE_DEVICES=7 python main_audio_captioning.py --process 7 --num_processes 8 --batch_size 32 &amp;"><pre>CUDA_VISIBLE_DEVICES=0 python main_audio_captioning.py --process 0 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=1 python main_audio_captioning.py --process 1 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=2 python main_audio_captioning.py --process 2 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=3 python main_audio_captioning.py --process 3 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=4 python main_audio_captioning.py --process 4 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=5 python main_audio_captioning.py --process 5 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=6 python main_audio_captioning.py --process 6 --num_processes 8 --batch_size 32 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=7 python main_audio_captioning.py --process 7 --num_processes 8 --batch_size 32 <span>&amp;</span></pre></div>
<p dir="auto">The captions are saved in <code>OUTPUT_DIR</code>. Specify this path in <code>address</code> variable in <code>eval/audio_captioning.py</code> and then obtain captioning metrics as</p>
<div dir="auto" data-snippet-clipboard-copy-content="python eval/audio_captioning.py"><pre>python eval/audio_captioning.py</pre></div>

<p dir="auto">Generate captions using</p>
<div dir="auto" data-snippet-clipboard-copy-content="CUDA_VISIBLE_DEVICES=0 python main_video_captioning.py --process 0 --num_processes 8 --batch_size 8 &amp;
CUDA_VISIBLE_DEVICES=1 python main_video_captioning.py --process 1 --num_processes 8 --batch_size 8 &amp;
CUDA_VISIBLE_DEVICES=2 python main_video_captioning.py --process 2 --num_processes 8 --batch_size 8 &amp;
CUDA_VISIBLE_DEVICES=3 python main_video_captioning.py --process 3 --num_processes 8 --batch_size 8 &amp;
CUDA_VISIBLE_DEVICES=4 python main_video_captioning.py --process 4 --num_processes 8 --batch_size 8 &amp;
CUDA_VISIBLE_DEVICES=5 python main_video_captioning.py --process 5 --num_processes 8 --batch_size 8 &amp;
CUDA_VISIBLE_DEVICES=6 python main_video_captioning.py --process 6 --num_processes 8 --batch_size 8 &amp;
CUDA_VISIBLE_DEVICES=7 python main_video_captioning.py --process 7 --num_processes 8 --batch_size 8 &amp;"><pre>CUDA_VISIBLE_DEVICES=0 python main_video_captioning.py --process 0 --num_processes 8 --batch_size 8 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=1 python main_video_captioning.py --process 1 --num_processes 8 --batch_size 8 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=2 python main_video_captioning.py --process 2 --num_processes 8 --batch_size 8 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=3 python main_video_captioning.py --process 3 --num_processes 8 --batch_size 8 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=4 python main_video_captioning.py --process 4 --num_processes 8 --batch_size 8 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=5 python main_video_captioning.py --process 5 --num_processes 8 --batch_size 8 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=6 python main_video_captioning.py --process 6 --num_processes 8 --batch_size 8 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=7 python main_video_captioning.py --process 7 --num_processes 8 --batch_size 8 <span>&amp;</span></pre></div>
<p dir="auto">The captions are saved in <code>OUTPUT_DIR</code>. Specify this path in <code>ours_result_path</code> variable in <code>eval/video_captioning.py</code> and then obtain captioning metrics as</p>
<div dir="auto" data-snippet-clipboard-copy-content="python eval/video_captioning.py"><pre>python eval/video_captioning.py</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">High-quality image generation</h3><a id="user-content-high-quality-image-generation" aria-label="Permalink: High-quality image generation" href="#high-quality-image-generation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Generate high-quality image using</p>
<div dir="auto" data-snippet-clipboard-copy-content="CUDA_VISIBLE_DEVICES=0 python main_image_generation_enhancement.py --process 0 --num_processes 8 --batch_size 4 &amp;
CUDA_VISIBLE_DEVICES=1 python main_image_generation_enhancement.py --process 1 --num_processes 8 --batch_size 4 &amp;
CUDA_VISIBLE_DEVICES=2 python main_image_generation_enhancement.py --process 2 --num_processes 8 --batch_size 4 &amp;
CUDA_VISIBLE_DEVICES=3 python main_image_generation_enhancement.py --process 3 --num_processes 8 --batch_size 4 &amp;
CUDA_VISIBLE_DEVICES=4 python main_image_generation_enhancement.py --process 4 --num_processes 8 --batch_size 4 &amp;
CUDA_VISIBLE_DEVICES=5 python main_image_generation_enhancement.py --process 5 --num_processes 8 --batch_size 4 &amp;
CUDA_VISIBLE_DEVICES=6 python main_image_generation_enhancement.py --process 6 --num_processes 8 --batch_size 4 &amp;
CUDA_VISIBLE_DEVICES=7 python main_image_generation_enhancement.py --process 7 --num_processes 8 --batch_size 4 &amp;"><pre>CUDA_VISIBLE_DEVICES=0 python main_image_generation_enhancement.py --process 0 --num_processes 8 --batch_size 4 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=1 python main_image_generation_enhancement.py --process 1 --num_processes 8 --batch_size 4 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=2 python main_image_generation_enhancement.py --process 2 --num_processes 8 --batch_size 4 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=3 python main_image_generation_enhancement.py --process 3 --num_processes 8 --batch_size 4 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=4 python main_image_generation_enhancement.py --process 4 --num_processes 8 --batch_size 4 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=5 python main_image_generation_enhancement.py --process 5 --num_processes 8 --batch_size 4 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=6 python main_image_generation_enhancement.py --process 6 --num_processes 8 --batch_size 4 <span>&amp;</span>
CUDA_VISIBLE_DEVICES=7 python main_image_generation_enhancement.py --process 7 --num_processes 8 --batch_size 4 <span>&amp;</span></pre></div>
<p dir="auto">The image generations are saved in <code>OUTPUT_DIR</code>.</p>

<p dir="auto">Put the style and content image in the <code>images/</code> folder, and run</p>
<div dir="auto" data-snippet-clipboard-copy-content="python main_style_transfer.py --style_image &lt;style_image&gt; --content_image &lt;content_image&gt;"><pre>python main_style_transfer.py --style_image <span>&lt;</span>style_image<span>&gt;</span> --content_image <span>&lt;</span>content_image<span>&gt;</span></pre></div>
<p dir="auto">The output is saved in <code>OUTPUT_DIR</code>.</p>

<p dir="auto">We first use image captioning to convert image to text. Also, we use audio captioning to convert audio to text. Next, we combine the captions into an image generation prompt from the LLM, please see the paper for exact prompt details. The prompt is then fed to the high-quality image generation using <code>CustomArithmetic.csv</code> as the initial prompt.</p>

<p dir="auto">Please open an issue in this repository (preferred for better visibility) or reach out to <a href="mailto:kumar.ashutosh@utexas.edu">kumar.ashutosh@utexas.edu</a>.</p>

<p dir="auto">See the <a href="https://github.com/facebookresearch/MILS/blob/main/CONTRIBUTING.md">CONTRIBUTING</a> file for how to help out.</p>

<p dir="auto"><code>MILS</code> is made available under a <a href="https://github.com/facebookresearch/MILS/blob/main/LICENSE.md">CC-by-NC 4.0 license</a>, however third party content pulled from other locations are subject to their own licenses and you may have other legal obligations or restrictions that govern your use of that content.</p>

<p dir="auto">If you use this work, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{ashutosh2025llms,
  title={LLMs can see and hear without any training},
  author={Ashutosh, Kumar and Gandelsman, Yossi and Chen, Xinlei and Misra, Ishan and Girdhar, Rohit},
  journal={arXiv preprint arXiv:2501.18096},
  year={2025}
}"><pre><span>@article</span>{<span>ashutosh2025llms</span>,
  <span>title</span>=<span><span>{</span>LLMs can see and hear without any training<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Ashutosh, Kumar and Gandelsman, Yossi and Chen, Xinlei and Misra, Ishan and Girdhar, Rohit<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2501.18096<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
