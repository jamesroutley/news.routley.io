<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/schedule_free">Original</a>
    <h1>Schedule-Free Learning â€“ A New Way to Train</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Schedule-Free Optimizers in PyTorch.</p>
<p dir="auto">Authors: Aaron Defazio, Xingyu Yang, Konstantin Mishchenko, Ashok Cutkosky, Harsh Mehta, Ahmed Khaled</p>
<p dir="auto"><strong>TLDR</strong> Faster training without schedules - no need to specify the stopping time/steps in advance!</p>
<p dir="auto"><code>pip install schedulefree</code></p>
<p dir="auto">Primary implementations are <code>SGDScheduleFree</code> and <code>AdamWScheduleFree</code>.</p>

<p dir="auto">Schedule-Free learning replaces the momentum of an underlying optimizer with a combination of interpolation and averaging. In the case of gradient descent, the Schedule-free update is:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$$
\begin{align*}
y_{t} &amp; = (1-\beta)z_{t} + \beta x_{t},\\
z_{t+1} &amp; =z_{t}-\gamma\nabla f(y_{t}),\\
x_{t+1} &amp; =\left(1-\frac{1}{t}\right)x_{t}+\frac{1}{t}z_{t+1},
\end{align*}
$$</math-renderer></p>
<p dir="auto">Here <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$x$</math-renderer> is the sequence that evaluations of test/val loss should occur at, which differs from the primary iterates <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$z$</math-renderer> and the gradient evaluation locations <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$y$</math-renderer>. The updates to <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$z$</math-renderer> correspond to the underlying optimizer, in this case a simple gradient step.</p>
<p dir="auto">As the name suggests, Schedule-free learning does not require a decreasing learning rate schedule, yet typically out-performs, or at worst matches, SOTA schedules such as cosine-decay and linear decay. Only two sequences need to be stored at a time (the third can be computed from the other two on the fly) so this method has the same memory requirements as the base optimizer (parameter buffer + momentum).</p>
<p dir="auto">We provide both AdamW and SGD versions in this repo.</p>

<p dir="auto">Since our optimizer uses two different points for gradient calls and test/val loss calculations, it&#39;s necessary to switch the param buffer between the two during training. This is done by calling <code>optimizer.train()</code> at the same place you call <code>model.train()</code> and <code>optimizer.eval()</code> at the same place you call <code>model.eval()</code>.</p>
<p dir="auto">If your code supports PyTorch Optimizer step closures, you can use the closure forms of the optimizers, which do not require the <code>.train()</code> and <code>.eval()</code> calls.</p>

<p dir="auto">Examples of using the <code>schedulefree</code> package can be found in the <code>examples</code> folder. These include:</p>
<ul dir="auto">
<li><a href="https://github.com/facebookresearch/schedule_free/blob/main/examples/mnist/README.md">Image classification (MNIST) using Convnets</a>*</li>
<li>More examples to be added</li>
</ul>
<p dir="auto">*Example is modified from <a href="https://github.com/pytorch/examples">Pytorch Examples Repo</a>.</p>

<ul dir="auto">
<li>If your model uses BatchNorm, additional modifications are required for test/val evaluations to work correctly. Right before eval, something like the following:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content=" model.train()
 optimizer.eval()
 for batch in itertools.islice(train_loader, 50):
   _ = self.model(batch)
 model.eval()"><pre> <span>model</span>.<span>train</span>()
 <span>optimizer</span>.<span>eval</span>()
 <span>for</span> <span>batch</span> <span>in</span> <span>itertools</span>.<span>islice</span>(<span>train_loader</span>, <span>50</span>):
   <span>_</span> <span>=</span> <span>self</span>.<span>model</span>(<span>batch</span>)
 <span>model</span>.<span>eval</span>()</pre></div>
<p dir="auto">This will replace the <code>training_mean</code>/<code>training_var</code> cache (which is updated in each forward pass when in model.train() mode) with values calculated at <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$x$</math-renderer> instead of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$y$</math-renderer>. Using PreciseBN will also avoid this issue.</p>
<ul dir="auto">
<li>Many code bases use additional features that may not be compatible without additional changes. For instance, if the parameters are cached in fp16, the cached versions will need to be updated manually to ensure the correct <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$x$</math-renderer> sequence is used for evaluation, not the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$y$</math-renderer> sequence. Some GradScalers do this.</li>
<li>Training is more sensitive to the choice of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$\beta$</math-renderer> than you may expect from standard momentum. Our default of <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$0.9$</math-renderer> works on most problems but it may be necessary to increase the value to <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$0.95$</math-renderer> or <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$0.98$</math-renderer> particually for very long training runs.</li>
<li>There is no need to use a learning rate scheduler, however the code is compatible with one.</li>
<li>Using learning rate warmup is recommended. This is supported through the <code>warmup_steps</code> parameter.</li>
<li>This method does require tuning - it won&#39;t necessarily out-perform a schedule approach without also tuning regularization and learning rate parameters.</li>
<li>For SGD, a learning rate 10x-50x larger than classical rates seems to be a good starting point.</li>
<li>For AdamW, learnings rates in the range 1x-10x larger than with schedule based approaches seem to work.</li>
<li>Our method can also be implemented as a wrapper around a base optimizer, where the momentum of the base optimizer is disabled. We didn&#39;t do that as PyTorch&#39;s Adam implementation would still allocate memory for it&#39;s momentum buffer <code>exp_avg</code> even if we don&#39;t use it.</li>
</ul>

<p dir="auto">See the <a href="https://github.com/facebookresearch/schedule_free/blob/main/LICENSE">License file</a>.</p>

<p dir="auto">Schedule-Free learning can be seen as an interpolation between primal averaging (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$\beta=1$</math-renderer>) and Polyak-Ruppert averaging (<math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$\beta=0)$</math-renderer>. The advantage of this interpolation is that it allows us to get the best of both worlds. We can achieve the fast early stage convergence of Polyak-Ruppert averaging (since the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$z$</math-renderer> sequence moves quicker than the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$x$</math-renderer> sequence), without the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$x$</math-renderer> sequence straying too far from the <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$z$</math-renderer> sequence, which causes instability.</p>
<p dir="auto">Our method is also related to Nesterov&#39;s accelerated method (Nesterov, 1983), which can be written in the following form:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="6e53a9c53f56065aa1e956ceb87c03d4">$$
\begin{align*}
y_{t} &amp; =(1-2/(t+1))x_{t} + (2/(t+1))z_{t}\\
z_{t+1} &amp; =z_{t}-\frac{t}{2L}\nabla f(y_{t})\\
x_{t+1} &amp; =(1-2/(t+1))x_{t}+(2/(t+1))z_{t+1}
\end{align*}
$$</math-renderer></p>
<p dir="auto">Our approach has the same three sequences, but uses very different weights, and crucially, does not include an increasing learning rate over time, which is essential for accelerated rates with Nesterov&#39;s method. We also use different weight sequences for the interpolation operation versus the averaging operation.</p>
<p dir="auto">Tail averaging approaches such as Stochastic Weight Averaging (Izmailov et al., 2018) and LAtest Weight Averaging (Kaddour, 2022; Sanyal et al., 2023) combine averaging with large or cyclic learning rates. They still require the use of a schedule, introduce additional hyper-parameters to tune, and require additional memory compared to our technique. It is also possible to use SWA and LAWA on top of our approach, potentially giving further gains.</p>
<p dir="auto">Portes Et. Al. (2022) use cyclic learning rate schedules with increasing cycle periods to give a method that explores multiple points along the Pareto frontier of training time vs eval performance. Each point at the end of a cycle is an approximation to the model from a tuned schedule ending at that time. Our method gives the entire frontier, rather than just a few points along the path.</p>
<p dir="auto">Exponential moving averages (EMA) of the iterate sequence are used in the popular Lookahead optimizer (Zhang et al., 2019). The Lookahead method can be seen as the EMA version of primal averaging, just as exponential weight averaging is the EMA version of Polyak-Ruppert averaging. Our extra interpolation step can potentially be used in combination with the lookahead optimizer also.</p>
</article></div></div>
  </body>
</html>
