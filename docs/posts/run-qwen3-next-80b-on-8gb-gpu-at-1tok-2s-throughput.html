<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Mega4alik/ollm">Original</a>
    <h1>Show HN: Run Qwen3-Next-80B on 8GB GPU at 1tok/2s throughput</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/dd088163642ae8858d1a680350b3e941518f8521d1d3a1f3d3f754b89f2d8bab/68747470733a2f2f6f6c6c6d2e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f66696c65732f6c6f676f322e706e67" data-canonical-src="https://ollm.s3.us-east-1.amazonaws.com/files/logo2.png"/>
    <img alt="vLLM" src="https://camo.githubusercontent.com/dd088163642ae8858d1a680350b3e941518f8521d1d3a1f3d3f754b89f2d8bab/68747470733a2f2f6f6c6c6d2e73332e75732d656173742d312e616d617a6f6e6177732e636f6d2f66696c65732f6c6f676f322e706e67" width="52%" data-canonical-src="https://ollm.s3.us-east-1.amazonaws.com/files/logo2.png"/>
  </picture></themed-picture>
</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">
LLM Inference for Large-Context Offline Workloads
</h3><a id="user-content-llm-inference-for-large-context-offline-workloads" aria-label="Permalink: 
LLM Inference for Large-Context Offline Workloads
" href="#llm-inference-for-large-context-offline-workloads"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">oLLM is a lightweight Python library for large-context LLM inference, built on top of Huggingface Transformers and PyTorch. It enables running models like <a href="https://huggingface.co/openai/gpt-oss-20b" rel="nofollow">gpt-oss-20B</a>, <a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct" rel="nofollow">qwen3-next-80B</a> or <a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct" rel="nofollow">Llama-3.1-8B-Instruct</a> on 100k context using ~$200 consumer GPU with 8GB VRAM.  No quantization is used‚Äîonly fp16/bf16 precision.</p>
<p dir="auto"><em>Latest updates (0.4.0)</em> üî•</p>
<ul dir="auto">
<li><b>qwen3-next-80B</b> (160GB model) added with <span>‚ö°Ô∏è1tok/2s</span> throughput (fastest model so far)</li>
<li>Llama3 custom chunked attention replaced with flash-attention2 for stability</li>
<li>gpt-oss-20B flash-attention-like implementation added to reduce VRAM usage </li>
<li>gpt-oss-20B chunked MLP added to reduce VRAM usage </li>
<li>KVCache is replaced with DiskCache.</li>
</ul>
<hr/>
<div dir="auto"><h3 tabindex="-1" dir="auto">8GB Nvidia 3060 Ti Inference memory usage:</h3><a id="user-content-8gb-nvidia-3060-ti-inference-memory-usage" aria-label="Permalink: 8GB Nvidia 3060 Ti Inference memory usage:" href="#8gb-nvidia-3060-ti-inference-memory-usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Weights</th>
<th>Context length</th>
<th>KV cache</th>
<th>Baseline VRAM (no offload)</th>
<th>oLLM GPU VRAM</th>
<th>oLLM Disk (SSD)</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct" rel="nofollow">qwen3-next-80B</a></td>
<td>160 GB (bf16)</td>
<td>10k</td>
<td>1.4 GB</td>
<td>~170 GB</td>
<td>~5.4 GB</td>
<td>162 GB</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openai/gpt-oss-20b" rel="nofollow">gpt-oss-20B</a></td>
<td>13 GB (packed bf16)</td>
<td>10k</td>
<td>1.4 GB</td>
<td>~40 GB</td>
<td>~7.3GB</td>
<td>15 GB</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct" rel="nofollow">llama3-1B-chat</a></td>
<td>2 GB (fp16)</td>
<td>100k</td>
<td>12.6 GB</td>
<td>~16 GB</td>
<td>~5 GB</td>
<td>15 GB</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct" rel="nofollow">llama3-3B-chat</a></td>
<td>7 GB (fp16)</td>
<td>100k</td>
<td>34.1 GB</td>
<td>~42 GB</td>
<td>~5.3 GB</td>
<td>42 GB</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct" rel="nofollow">llama3-8B-chat</a></td>
<td>16 GB (fp16)</td>
<td>100k</td>
<td>52.4 GB</td>
<td>~71 GB</td>
<td>~6.6 GB</td>
<td>69 GB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">By &#34;Baseline&#34; we mean typical inference without any offloading</p>
<p dir="auto">How do we achieve this:</p>
<ul dir="auto">
<li>Loading layer weights from SSD directly to GPU one by one</li>
<li>Offloading KV cache to SSD and loading back directly to GPU, no quantization or PagedAttention</li>
<li>Offloading layer weights to CPU if needed</li>
<li>FlashAttention-2 with online softmax. Full attention matrix is never materialized.</li>
<li>Chunked MLP. Intermediate upper projection layers may get large, so we chunk MLP as well</li>
</ul>
<hr/>
<p dir="auto">Typical use cases include:</p>
<ul dir="auto">
<li>Analyze contracts, regulations, and compliance reports in one pass</li>
<li>Summarize or extract insights from massive patient histories or medical literature</li>
<li>Process very large log files or threat reports locally</li>
<li>Analyze historical chats to extract the most common issues/questions users have</li>
</ul>
<hr/>
<p dir="auto">Supported <strong>Nvidia GPUs</strong>: Ampere (RTX 30xx, A30, A4000,  A10),  Ada Lovelace (RTX 40xx,  L4), Hopper (H100), and newer</p>

<p dir="auto">It is recommended to create venv or conda environment first</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m venv ollm_env
source ollm_env/bin/activate"><pre>python3 -m venv ollm_env
<span>source</span> ollm_env/bin/activate</pre></div>
<p dir="auto">Install oLLM with <code>pip install ollm</code> or <a href="https://github.com/Mega4alik/ollm">from source</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Mega4alik/ollm.git
cd ollm
pip install -e .
pip install kvikio-cu{cuda_version} Ex, kvikio-cu12"><pre>git clone https://github.com/Mega4alik/ollm.git
<span>cd</span> ollm
pip install -e <span>.</span>
pip install kvikio-cu{cuda_version} Ex, kvikio-cu12</pre></div>
<blockquote>
<p dir="auto">üí° <strong>Note</strong></p>
</blockquote>

<p dir="auto">Code snippet sample</p>
<div dir="auto" data-snippet-clipboard-copy-content="from ollm import Inference, file_get_contents, TextStreamer
o = Inference(&#34;llama3-1B-chat&#34;, device=&#34;cuda:0&#34;) #llama3-1B/3B/8B-chat, gpt-oss-20B, qwen3-next-80B
o.ini_model(models_dir=&#34;./models/&#34;, force_download=False)
o.offload_layers_to_cpu(layers_num=2) #(optional) offload some layers to CPU for speed boost
past_key_values = o.DiskCache(cache_dir=&#34;./kv_cache/&#34;) #set None if context is small
text_streamer = TextStreamer(o.tokenizer, skip_prompt=True, skip_special_tokens=False)

messages = [{&#34;role&#34;:&#34;system&#34;, &#34;content&#34;:&#34;You are helpful AI assistant&#34;}, {&#34;role&#34;:&#34;user&#34;, &#34;content&#34;:&#34;List planets&#34;}]
input_ids = o.tokenizer.apply_chat_template(messages, reasoning_effort=&#34;minimal&#34;, tokenize=True, add_generation_prompt=True, return_tensors=&#34;pt&#34;).to(o.device)
outputs = o.model.generate(input_ids=input_ids,  past_key_values=past_key_values, max_new_tokens=500, streamer=text_streamer).cpu()
answer = o.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=False)
print(answer)"><pre>from ollm import Inference, file_get_contents, TextStreamer
o = Inference(<span><span>&#34;</span>llama3-1B-chat<span>&#34;</span></span>, device=<span><span>&#34;</span>cuda:0<span>&#34;</span></span>) <span><span>#</span>llama3-1B/3B/8B-chat, gpt-oss-20B, qwen3-next-80B</span>
o.ini_model(models_dir=<span><span>&#34;</span>./models/<span>&#34;</span></span>, force_download=False)
o.offload_layers_to_cpu(layers_num=2) <span><span>#</span>(optional) offload some layers to CPU for speed boost</span>
past_key_values = o.DiskCache(cache_dir=<span><span>&#34;</span>./kv_cache/<span>&#34;</span></span>) <span><span>#</span>set None if context is small</span>
text_streamer = TextStreamer(o.tokenizer, skip_prompt=True, skip_special_tokens=False)

messages = [{<span><span>&#34;</span>role<span>&#34;</span></span>:<span><span>&#34;</span>system<span>&#34;</span></span>, <span><span>&#34;</span>content<span>&#34;</span></span>:<span><span>&#34;</span>You are helpful AI assistant<span>&#34;</span></span>}, {<span><span>&#34;</span>role<span>&#34;</span></span>:<span><span>&#34;</span>user<span>&#34;</span></span>, <span><span>&#34;</span>content<span>&#34;</span></span>:<span><span>&#34;</span>List planets<span>&#34;</span></span>}]
input_ids = o.tokenizer.apply_chat_template(messages, reasoning_effort=<span><span>&#34;</span>minimal<span>&#34;</span></span>, tokenize=True, add_generation_prompt=True, return_tensors=<span><span>&#34;</span>pt<span>&#34;</span></span>).to(o.device)
outputs = o.model.generate(input_ids=input_ids,  past_key_values=past_key_values, max_new_tokens=500, streamer=text_streamer).cpu()
answer = o.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=False)
print(answer)</pre></div>
<p dir="auto">or run sample python script as <code>PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python example.py</code></p>

<p dir="auto">If there‚Äôs a model you‚Äôd like to see supported, feel free to reach out at <a href="mailto:anuarsh@ailabs.us">anuarsh@ailabs.us</a>‚ÄîI‚Äôll do my best to make it happen.</p>
</article></div></div>
  </body>
</html>
