<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://mathvoices.ams.org/featurecolumn/2022/02/01/the-kalman-filter-helping-chickens-cross-the-road/">Original</a>
    <h1>The Kalman Filter. Helping Chickens Cross the Road</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">

			<div>
				
<p> David Austin</p>
<p>
	I was running some errands recently when I spied <a href="https://www.youtube.com/watch?v=j5o1GCQpti4" target="blank" rel="noopener"> the Chicken Robot </a> ambling down the sidewalk, then veering into the bike lane.  A local grocery chain needs to transport rotisserie chickens from one of its larger stores to its downtown market.  Their solution?  A semi-autonomous delivery vehicle that makes the three-mile journey navigating with GPS.
      </p>
<p>
	I was curious how this worked.  The locations provided by most GPS systems are only accurate to within a meter or so.  How could those juicy chickens know how to stay so carefully in the center of the bike lane?
      </p>
<p>
	A typical solution to problems like this is an elegant algorithm, known as Kalman filtering, that’s embedded in a wide range of technology that most of us frequently either use or benefit from in some way.  The algorithm allows us to efficiently combine our expectations about the state of a system, the chickens’ physical location, for instance, with imperfect measurements about the system to develop a highly accurate picture of the system.
      </p>
<p>
	This column will describe some simple versions of Kalman filtering, the main observation that makes it work, and why it’s such a great idea.
      </p>
<h3>
	<SPAN color="#993300"></SPAN><br/>
      </h3>
<p>
	Let’s begin with a simple example.  Suppose we would like to determine the weight of some object.  Of course, any scale is imperfect so we might weigh it repeatedly on the same scale and find the following weights, perhaps in grams.
      </p>
<table>
<tbody><tr>
<td> 50.4 </td>
<td> 46.0 </td>
<td> 46.1 </td>
<td> 45.1 </td>
<td> 48.9 </td>
<td> 42.6 </td>
<td> 50.7 </td>
<td> 45.7 </td>
<td> 47.8 </td>
<td> 46.7 </td>
</tr>
</tbody></table>
<p>
	As the result of each weighing is recorded, we update our estimate of the weight as the average of all the measurements we’ve seen so far.  That is, after obtaining $z_n$, the $n^{th}$ weight, we could estimate the object’s weight by finding the average $$ x_n = \frac1n(z_1 + z_2 + \ldots + z_n).  $$ The result is shown below.
      </p>
<p><img src="https://i1.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/weights.jpg?resize=300%2C167&amp;ssl=1" alt="" width="300" height="167" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/weights.jpg?resize=300%2C167&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/>
      </p>
<p>
	As the next weight $z_{n}$ is recorded, we update our estimate:</p>
<p>
	This expression tells us how each new measurement influences the next estimate.  More specifically, the new estimate $x_{n}$ is obtained from the previous estimate $x_{n-1}$ by adding in $\frac1{n}(z_{n} – x_{n-1})$.  Notice that this term is proportional to the difference between the new measurement and the previous estimate with the proportionality constant being $K_n = \frac1{n}$.  We call $K_n$ the <em> Kalman gain;</em> the fact that it decreases as we include more 	measurements reflects our increasing confidence in the estimates $x_n$ so that, as $n$ increases, new measurements have less effect on the updated estimates.
      </p>
<p>
	Suppose now that we have some additional information about the 	accuracy of our measurements $z_n$.  For example, if $z_n$ is the chickens’ location reported by a GPS system, the true location is most likely within a meter or so.  More generally, we might imagine that the true value is normally distributed about $z_n$ with standard deviation $\sigma_{z_n}$.
      </p>
<p>
	Returning to our weighings, we could perhaps estimate $\sigma_{z_n}$ by repeatedly weighing an object of known weight. 	The probability that the true value is near $z_n$ is given by the distribution</p>
<p>
	Let’s also suppose that we have some estimate of the uncertainty of our estimates $x_n$.  In particular, we’ll assume the probability that the true value is near $x_n$ is given by the normal distribution having mean $x_n$ and standard deviation $\sigma_{x_n}$:</p>
<p>
	These distributions could be as shown below.
      </p>
<p><img loading="lazy" src="https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/distros-1.jpg?resize=300%2C172&amp;ssl=1" alt="" width="300" height="172" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/distros-1.jpg?resize=300%2C172&amp;ssl=1 300w, https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/distros-1.jpg?w=365&amp;ssl=1 365w" data-lazy-sizes="(max-width: 300px) 100vw, 300px" data-lazy-src="https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/distros-1.jpg?resize=300%2C172&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/>
      </p>
<p>
	Now here’s the beautiful idea that underlies Kalman’s filtering algorithm.  Both distributions $p_{x_n}$ and $p_{z_n}$ describe the location of the true value we seek.  We can combine them to obtain a better estimate of the true value by multiplying them.  That is, we combine or “fuse” these two distributions using the product</p>
<p><img loading="lazy" src="https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/distros-2.jpg?resize=300%2C172&amp;ssl=1" alt="" width="300" height="172" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/distros-2.jpg?resize=300%2C172&amp;ssl=1 300w, https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/distros-2.jpg?w=365&amp;ssl=1 365w" data-lazy-sizes="(max-width: 300px) 100vw, 300px" data-lazy-src="https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/distros-2.jpg?resize=300%2C172&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/>
      </p>
<p>
	More specifically, the product of the two distributions is</p>
<p>
	In addition, the product of the Gaussians leads to the new standard deviation</p>
<p>
	Notice that the Kalman gain satisfies $0\lt K_n \lt 1$.   In the case that $\sigma_{x_n} \ll \sigma_{z_n}$, we feel much more confident in our estimate $x_n$ than our new measurement $z_n$.  Therefore, $K_n\approx 0$ and so $x_{n+1} \approx x_n$, reflecting the fact that the new measurement has little influence on our next estimate.
      </p>
<p>
	However, if $\sigma_{x_n} \gg \sigma_{z_n}$, we feel much more confident in our new measurement $z_n$ than in our estimate $x_n$.  Then $K_n\approx 1$ and $x_{n+1}\approx z_n$.
      </p>
<p>
	In both cases, the uncertainty in our new estimate, as measured by $\sigma_{x_{n+1}}^2 = (1-K_n)\sigma_{x_n}^2$, has decreased.
      </p>
<p>
	This leads to the following algorithm:</p>
<ol>
<li>
<p>
	      Initialize the first estimate $x_1$ and the uncertainty $\sigma_{x_1}$.  We could use the first measurement as the first estimate or we could simply make a guess for the estimate and choose a large uncertainty.
	    </p>
</li>
<li>
<p>
	      Each time a new measurement $z_{n}$ arrives, update the estimate $x_{n+1}$ nd uncertainty $\sigma_{x_{n+1}}$ as follows:</p>
<ul>
<li>
<p>
		    Find the Kalman gain $K_n = \frac{\sigma_{x_n}^2}</p>
</li>
<li>
<p>
		    Update our estimate $x_{n+1} = x_n + K_n(z_{n}-x_n)$.
		  </p>
</li>
<li>
<p>
		    Update our uncertainty $\sigma_{x_{n+1}}^2 =</p>
</li>
</ul>
</li>
</ol>
<p>
	To illustrate, suppose that we make many measurements of a known weight with our scale and determine that the standard deviation of its measurements is constant $\sigma_z=2$. 	Initializing so that $x_1 = z_1$ and $\sigma_{x_1} = \sigma_z = 2$, we have the following sequence of estimates along with the decreasing</p>
<p><img loading="lazy" src="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/weights-kalman.jpg?resize=300%2C167&amp;ssl=1" alt="" width="300" height="167" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/weights-kalman.jpg?resize=300%2C167&amp;ssl=1 300w, https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/weights-kalman.jpg?w=391&amp;ssl=1 391w" data-lazy-sizes="(max-width: 300px) 100vw, 300px" data-lazy-src="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/weights-kalman.jpg?resize=300%2C167&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/>
      </p>
<h3>
	<SPAN color="#993300"></SPAN><br/>
      </h3>
<p>
	In our first example, the quantity we’re tracking, the weight of some object, doesn’t change.  Let’s now consider a dynamic process where the quantities 	are changing.  For instance, suppose we’re tracking the height and vertical velocity of a moving object.  The true height of the object could look like this, although</p>
<p><img loading="lazy" src="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-height.jpg?resize=292%2C292&amp;ssl=1" alt="" width="292" height="292" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-height.jpg?w=292&amp;ssl=1 292w, https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-height.jpg?resize=150%2C150&amp;ssl=1 150w" data-lazy-sizes="(max-width: 292px) 100vw, 292px" data-lazy-src="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-height.jpg?resize=292%2C292&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/>
      </p>
<p>
	In addition to recording the height $h$, we will also track the vertical velocity $v$ so that the state of the object at some time is given by the vector</p>
<p>
	Moreover, we will assume that there is some uncertainty in both the position and the velocity.  Initially, these uncertainties may be uncorrelated with one another
      </p>
<div>
<p><img loading="lazy" src="https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/position.jpg?resize=235%2C191&amp;ssl=1" alt="" width="235" height="191" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/position.jpg?resize=235%2C191&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/></p></div>
<p>
	so that we arrive at a Gaussian</p>
<p><img loading="lazy" src="https://i1.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/dist-2.jpg?resize=278%2C191&amp;ssl=1" alt="" width="278" height="191" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/dist-2.jpg?resize=278%2C191&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/>
      </p>
<p>
	Now if we know $x_{n,n}$, the state at some time, we can predict the state at a later time using some assumption about the system.  For instance, we might assume, after $\Delta t$ time units have passed, that the new state is ${\mathbf x}_{n+1,n} = \begin{bmatrix}h_{n+1} \\ v_{n+1} \end{bmatrix}$ where</p>
<p>
	This is a particular model we are choosing; in other situations, another model may be more appropriate.  For instance, if we have information about the object’s acceleration, we may want to incorporate it.  In any case, we assume there is a matrix $F_n$ from which we extrapolate the next state:</p>
<p>
	Of course, uncertainty in the state ${\mathbf x}_{n,n}$ will translate into uncertainty in the extrapolated state ${\mathbf x}_{n+1,n}$.  It is straightforward to verify that the covariance matrix is transformed as</p>

<p>
	These two transformations form the extrapolation phase of the algorithm:</p>
<p>
	Next, suppose we have a new measurement ${\mathbf z}_{n}$ whose uncertainty is described by the covariance matrix $R_{n}$.  We imagine that the normal distribution centered on ${\mathbf z}_{n}$ and with covariance matrix $R_n$ describes the distribution of the true state.
      </p>
<p>
	As before, we will fuse our predicted state ${\mathbf x}_{n+1,n}$ with the measured state ${\mathbf z}_{n}$ by multiplying the two normal distributions and rewriting as a single Gaussian.  With some work, one finds the expression for the Kalman gain $$ K_{n} = P_{n+1,n}(P_{n+1,n} + R_{n})^{-1}, $$ which should be compared to our earlier expression.
      </p>
<p>
	We also obtain the updated state and covariance matrix</p>
<p>
	So now we arrive at a new version of the algorithm:
      </p>
<ol>
<li>
<p>
	    Initialize the initial state ${\mathbf x}_{1,1}$ and covariance matrix $P_{1,1}$.
	  </p>
</li>
<li>
<p>
	    As new measurements become available, repeat the following steps:
	  </p>
<ul>
<li>
<p>
		Form the extrapolated state and covariance:</p>
</li>
<li>
<p>
		Use the result to find the Kalman gain</p>
</li>
<li>
<p>
		Use the Kalman gain to fuse the predicted state with the measured state and update the covariance matrix.</p>
</li>
</ul>
</li>
</ol>
<p>
	Let’s see how this plays out in an example.  Imagine we are tracking the height and vertical velocity of an object whose true height as is shown.
      </p>
<p><img loading="lazy" src="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-height.jpg?resize=292%2C292&amp;ssl=1" alt="" width="292" height="292" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-height.jpg?w=292&amp;ssl=1 292w, https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-height.jpg?resize=150%2C150&amp;ssl=1 150w" data-lazy-sizes="(max-width: 292px) 100vw, 292px" data-lazy-src="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-height.jpg?resize=292%2C292&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/>
      </p>
<p>
	Remember that we don’t know the true height, but we do have some noisy measurements that reflect the object’s height and its velocity.
      </p>
<div>
<p><img loading="lazy" src="https://i1.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-noise.jpg?resize=292%2C292&amp;ssl=1" alt="" width="292" height="292" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-noise.jpg?resize=292%2C292&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/></p></div>
<p>
	Applying the Kalman filtering algorithm naively, we obtain the green curve describing the object’s height.  Notice how there is a time lag between the filtered height and the true height.
      </p>
<div>
<p><img loading="lazy" src="https://i1.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-filtered-noQ.jpg?resize=292%2C292&amp;ssl=1" alt="" width="292" height="292" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-filtered-noQ.jpg?resize=292%2C292&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/></p></div>
<p>
	What’s the problem here?  Clearly, the object is experiencing a significant amount of acceleration, which is not built into the model.  As we saw in our earlier static example, the uncertainty in the estimated state ${\mathbf x}_{n,n}$ continually decreases, which means the confidence we have in our estimates grows and causes us to downplay the new measurements.
      </p>
<p>
	There are several ways to deal with this.  If we have measurements about the acceleration, we could rebuild our model so that the state vector ${\mathbf x}_{n,n}$ includes the acceleration in addition to the position and velocity.  Alternatively, if we have information about how an operator is controlling the object we’re tracking, we could build it into the extrapolation phase using the update</p>
<p>
	Finally, we can simply build additional uncertainty into the model by adding it into the extrapolation phase.  For instance, we could define</p>
<p>
	Adding some process noise into the extrapolation phase prevents us from becoming overly confident in the extrapolated states and continuing to give sufficient weight to new measurements.  This leads to the filtered state as shown below, which is clearly much better than the measured signal.
      </p>
<div>
<p><img loading="lazy" src="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-filtered.jpg?resize=292%2C292&amp;ssl=1" alt="" width="292" height="292" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/mathvoices.ams.org/featurecolumn/wp-content/uploads/sites/2/2022/01/full-kalman-filtered.jpg?resize=292%2C292&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"/></p></div>
<h3>
	<SPAN color="#993300"></SPAN><br/>
      </h3>
<p>
	Kalman developed this algorithm in 1960, though it seems to have appeared earlier in other guises, and found a significant early use in the Apollo guidance computer.  Indeed, the algorithm is well suited for this application.  While the guidance computer was a marvel of both hardware and software engineering, its memory and processing power were modest by our current standards.  As the algorithm only relies on our current estimate of the state, its demands on memory are slight, and the computational complexity is similarly small.
      </p>
<p>
	In addition to being fast and efficient, the algorithm is also optimal in the sense that, under certain assumptions on the system being modeled, the algorithm has the smallest possible expected error obtained from a given set of measurements.
      </p>
<p>
	Kalman filtering is now ubiquitous in navigation and guidance applications.  In fact, it is used to smooth the motion of computer trackpads so you may have used it while reading this article.  If you are driving while navigating with an app like Google Maps, you may notice the effect of the algorithm when you come to a stop at a traffic light.  It sometimes happens that your location continues with constant speed into the intersection and then quickly snaps back to your actual 	location.  The extrapolation phase of the algorithm would lead us to believe that we continue with constant speed into the intersection before new measurements 	pull the extrapolated locations back to our true location.
      </p>
<p>
	Finally, the Chicken Robot needs your help to <a href="https://www.bridgestreetmarket.com/tortoise" target="_blank" rel="noopener"> find a new name.<br/>
	</a>
      </p>
<h3>
	<SPAN color="#993300"></SPAN><br/>
      </h3>
<p>
	There are lots of relevant references on the internet, but few give an intuitive sense of what makes this algorithm work.  Besides Kalman’s original paper, I’ve given a few of those here.
      </p>
<ul>
<li>
<p> <b> Rudolph Kalman. </b> <a href="https://www.cs.unc.edu/~welch/kalman/media/pdf/Kalman1960.pdf" target="_blank" rel="noopener">  <em> A New Approach to Linear Filtering and Prediction Problems,</em> </a>  Journal of Basic Engineering <b>82</b>, 1960.  Pages 35-45.  </p>
</li>
<li>
<p>  <b> Tim Babb. </b> <a href="https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/" target="_blank" rel="noopener"> How a Kalman filter works, in pictures. </a> A lovely introduction with a lot of visual explanations.
	  </p>
</li>
<li>
<p> <b>Alex Becker.</b> <a href="https://www.kalmanfilter.net/default.aspx" target="_blank" rel="noopener"> Kalman Filter Tutorial. </a> A gentle introduction that includes many motivating examples.
	  </p>
</li>
<li>
<p> <b> Ramsey Faragher. </b> <a href="https://drive.google.com/file/d/1nVtDUrfcBN9zwKlGuAclK-F8Gnf2M_to/view" target="_blank" rel="noopener"> 	      Understanding the Basis of the Kalman Filter Via a Simple and Intuitive Derivation. </a> <em>Signal Processing Magazine</em>, IEEE <b>29</b>, 2012. Pages 128-132.
	  </p>
</li>
</ul>
			</div>
					</div></div>
  </body>
</html>
