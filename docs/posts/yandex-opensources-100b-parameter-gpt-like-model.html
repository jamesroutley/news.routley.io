<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/yandex/YaLM-100B">Original</a>
    <h1>Yandex opensources 100B parameter GPT-like model</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><strong>YaLM 100B</strong> is a GPT-like neural network for generating and processing text. It can be used freely by developers and researchers from all over the world.</p>
<p dir="auto">The model leverages 100 billion parameters. It took 65 days to train the model on a cluster of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources in both English and Russian.</p>
<p dir="auto">Training details and best practices on acceleration and stabilizations can be found on <strong><a href="https://medium.com/p/d1df53d0e9a6" rel="nofollow">Medium</a></strong> (English) and <strong><a href="https://habr.com/ru/company/yandex/blog/672396/" rel="nofollow">Habr</a></strong> (Russian) articles.</p>
<h2 dir="auto"><a id="user-content-setup" aria-hidden="true" href="#setup"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Setup</h2>
<p dir="auto">Make sure to have 200GB of free disk space before downloading weights. The model <em>(code is based on <a href="https://github.com/microsoft/DeepSpeedExamples/tree/068e6561188e9192104e014f70fbe25224b5eb62/Megatron-LM-v1.1.5-ZeRO3">microsoft/DeepSpeedExamples/Megatron-LM-v1.1.5-ZeRO3</a>)</em> is supposed to run on multiple GPUs with tensor parallelism. It was tested on 4 (A100 80g) and 8 (V100 32g) GPUs, but is able to work with different configurations with â‰ˆ200GB of GPU memory in total which divide weight dimensions correctly (e.g. 16, 64, 128).</p>
<h3 dir="auto"><a id="user-content-downloading-checkpoint" aria-hidden="true" href="#downloading-checkpoint"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Downloading checkpoint</h3>
<ul dir="auto">
<li>Run <code>bash download/download.sh</code> to download model weights and vocabulary.</li>
<li>By default, weights will be downloaded to <code>./yalm100b_checkpoint/weights/</code>, and vocabulary will be downloaded to <code>./yalm100b_checkpoint/vocab/</code>.</li>
</ul>
<h3 dir="auto"><a id="user-content-docker" aria-hidden="true" href="#docker"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Docker</h3>
<ul dir="auto">
<li>We <a href="https://hub.docker.com/r/yandex/yalm-cuda11-ds" rel="nofollow">published</a> image on Docker Hub, it can be pulled with <code>docker/pull.sh</code>. It is compatible with A100 and V100.</li>
<li>Alternatively, you can build docker image from source using <code>docker/build.sh</code> (which will just build docker image from <code>docker/Dockerfile</code>).</li>
<li>To run container, use <code>docker/run.sh</code> <em>(volumes, name and other parameters can be changed)</em>.</li>
</ul>
<h2 dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Usage</h2>
<p dir="auto">You can start with the following scripts:</p>
<ul dir="auto">
<li><code>examples/generate_interactive.sh</code>: interactive generation from command line, the simplest way to try the model.</li>
<li><code>examples/generate_conditional_sampling.sh</code>: conditional generation with sampling strategy. Top-p is used by default, feel free to change temperature or use top-k. Input is jsonlines (example: <code>examples/example_cond_input.json</code>), output will be the same jsonlines with generated text field added to each line.</li>
<li><code>examples/generate_conditional_greedy.sh</code>: same as previous, but generation is greedy. Suitable for solving problems with few-shot.</li>
<li><code>examples/generate_unconditional.sh</code>: unconditional generation. No input is used, output will be jsonlines.</li>
</ul>
<h2 dir="auto"><a id="user-content-license" aria-hidden="true" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License</h2>
<p dir="auto">The model is published under the Apache 2.0 license that permits both research and commercial use, Megatron-LM is licensed under the <a href="https://github.com/yandex/YaLM-100B/blob/main/megatron_lm/LICENSE">Megatron-LM license</a>.</p>
</article>
          </div></div>
  </body>
</html>
