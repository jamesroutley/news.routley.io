<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://samsunglabs.github.io/rome/">Original</a>
    <h1>Realistic one-shot mesh-based head avatars</h1>
    
    <div id="readability-page-1" class="page"><div>

            <p>
              We present a system for the creation of realistic one-shot mesh-based (ROME) human head avatars.
              From a single photograph, our system estimates the head mesh (with person-specific details in both the facial and non-facial head parts) as well as the neural texture encoding, local photometric and geometric details. 
              The resulting avatars are rigged and can be rendered using a deep rendering network, which is trained alongside the mesh and texture estimators on a dataset of in-the-wild videos. 
              In the experiments, we observe that our system performs competitively both in terms of head geometry recovery and the quality of renders, especially for cross-person reenactment.
            </p>
            
            
        </div><div>
        <p>
          We use a neural texture map to represent both the geometry and appearance.
          This texture is estimated from a single source image by a texture encoder.
          Also, we estimate facial blendshape parameters, and the camera parameters from both the source and the driving images using a pre-trained system for facial reconstruction (e.g. <a href="https://arxiv.org/abs/2012.04012">DECA</a>).          
        </p>
        <p>
          Both the neural texture and the head mesh are fed into our head reconstruction pipeline, which predicts displacements to the input head mesh. 
          We use a combination of a geometry autoencoding network that produces latent geometry features, and a local geometry decoding MLP to predict displacements. 
          These displacements reconstruct the geometry, such as hair and shoulders. 
        </p>      
        <p>
          The reconstructed mesh is then used for neural rendering to produce photo-realistic images.
          We use a standard <a href="https://arxiv.org/abs/1904.12356">deferred neural rendering</a> pipeline, which renders a neural texture instead of a regular RGB texture, and decodes it into the image via an image-to-image network.
         </p>
        </div></div>
  </body>
</html>
