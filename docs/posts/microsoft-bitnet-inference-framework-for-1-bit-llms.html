<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/microsoft/BitNet">Original</a>
    <h1>Microsoft BitNet: inference framework for 1-bit LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6581c31c16c1b13ddc2efb92e2ad69a93ddc4a92fd871ff15d401c4c6c9155a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/license-MIT-blue.svg"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/93172b08c863df96ed7d00e798caaddd15891f35d34596605dad69bf0ebf5d04/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d312e302d626c7565"><img src="https://camo.githubusercontent.com/93172b08c863df96ed7d00e798caaddd15891f35d34596605dad69bf0ebf5d04/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f76657273696f6e2d312e302d626c7565" alt="version" data-canonical-src="https://img.shields.io/badge/version-1.0-blue"/></a></p>
<p dir="auto">bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support <strong>fast</strong> and <strong>lossless</strong> inference of 1.58-bit models on CPU (with NPU and GPU support coming next).</p>
<p dir="auto">The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of <strong>1.37x</strong> to <strong>5.07x</strong> on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by <strong>55.4%</strong> to <strong>70.0%</strong>, further boosting overall efficiency. On x86 CPUs, speedups range from <strong>2.37x</strong> to <strong>6.17x</strong> with energy reductions between <strong>71.9%</strong> to <strong>82.2%</strong>. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. More details will be provided soon.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/BitNet/blob/main/assets/m2_performance.jpg"><img src="https://github.com/microsoft/BitNet/raw/main/assets/m2_performance.jpg" alt="m2_performance" width="800"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/BitNet/blob/main/assets/intel_performance.jpg"><img src="https://github.com/microsoft/BitNet/raw/main/assets/intel_performance.jpg" alt="m2_performance" width="800"/></a></p>
<blockquote>
<p dir="auto">The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.</p>
</blockquote>

<p dir="auto">A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/54800242/377447164-7f46b736-edec-4828-b809-4be780a3e5b1.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkyNjg3NDIsIm5iZiI6MTcyOTI2ODQ0MiwicGF0aCI6Ii81NDgwMDI0Mi8zNzc0NDcxNjQtN2Y0NmI3MzYtZWRlYy00ODI4LWI4MDktNGJlNzgwYTNlNWIxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE4VDE2MjA0MlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQxOGNhMGM3ZDM4OTQ5MGY3YmJmZTI3YTE2ZTA3NTUzZTRjMTc5NzQ4ZTRlYzE4ZTk0NDhlZjM5YmI1OTk0NzAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.D_F7u8y_v7D9vO3Wxsw4WBbEgYYsrCCc7RybQYA8gbw" data-canonical-src="https://private-user-images.githubusercontent.com/54800242/377447164-7f46b736-edec-4828-b809-4be780a3e5b1.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkyNjg3NDIsIm5iZiI6MTcyOTI2ODQ0MiwicGF0aCI6Ii81NDgwMDI0Mi8zNzc0NDcxNjQtN2Y0NmI3MzYtZWRlYy00ODI4LWI4MDktNGJlNzgwYTNlNWIxLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE4VDE2MjA0MlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQxOGNhMGM3ZDM4OTQ5MGY3YmJmZTI3YTE2ZTA3NTUzZTRjMTc5NzQ4ZTRlYzE4ZTk0NDhlZjM5YmI1OTk0NzAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.D_F7u8y_v7D9vO3Wxsw4WBbEgYYsrCCc7RybQYA8gbw" controls="controls" muted="muted">

  </video>
</details>


<ul dir="auto">
<li>10/17/2024 bitnet.cpp 1.0 released.</li>
<li>02/27/2024 <a href="https://arxiv.org/abs/2402.17764" rel="nofollow">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li>
<li>10/17/2023 <a href="https://arxiv.org/abs/2310.11453" rel="nofollow">BitNet: Scaling 1-bit Transformers for Large Language Models</a></li>
</ul>

<p dir="auto">bitnet.cpp supports a list of 1-bit models available on <a href="https://huggingface.co/" rel="nofollow">Hugging Face</a>, which are trained with research settings. We hope the release of bitnet.cpp can inspire more 1-bit LLMs trained in large-scale settings.</p>
<markdown-accessiblity-table><table>
    
    <tbody><tr>
        <th rowspan="2">Model</th>
        <th rowspan="2">Parameters</th>
        <th rowspan="2">CPU</th>
        <th colspan="3">Kernel</th>
    </tr>
    <tr>
        <th>I2_S</th>
        <th>TL1</th>
        <th>TL2</th>
    </tr>
    <tr>
        <td rowspan="2"><a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large" rel="nofollow">bitnet_b1_58-large</a></td>
        <td rowspan="2">0.7B</td>
        <td>x86</td>
        <td>✔</td>
        <td>✘</td>
        <td>✔</td>
    </tr>
    <tr>
        <td>ARM</td>
        <td>✔</td>
        <td>✔</td>
        <td>✘</td>
    </tr>
    <tr>
        <td rowspan="2"><a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B" rel="nofollow">bitnet_b1_58-3B</a></td>
        <td rowspan="2">3.3B</td>
        <td>x86</td>
        <td>✘</td>
        <td>✘</td>
        <td>✔</td>
    </tr>
    <tr>
        <td>ARM</td>
        <td>✘</td>
        <td>✔</td>
        <td>✘</td>
    </tr>
    <tr>
        <td rowspan="2"><a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens" rel="nofollow">Llama3-8B-1.58-100B-tokens</a></td>
        <td rowspan="2">8.0B</td>
        <td>x86</td>
        <td>✔</td>
        <td>✘</td>
        <td>✔</td>
    </tr>
    <tr>
        <td>ARM</td>
        <td>✔</td>
        <td>✔</td>
        <td>✘</td>
    </tr>
</tbody></table></markdown-accessiblity-table>


<ul dir="auto">
<li>python&gt;=3.9</li>
<li>cmake&gt;=3.22</li>
<li>clang&gt;=18
<ul dir="auto">
<li>
<p dir="auto">For Windows users, install <a href="https://visualstudio.microsoft.com/downloads/" rel="nofollow">Visual Studio 2022</a>. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):</p>
<ul dir="auto">
<li>Desktop-development with C++</li>
<li>C++-CMake Tools for Windows</li>
<li>Git for Windows</li>
<li>C++-Clang Compiler for Windows</li>
<li>MS-Build Support for LLVM-Toolset (clang)</li>
</ul>
</li>
<li>
<p dir="auto">For Debian/Ubuntu users, you can download with <a href="https://apt.llvm.org/" rel="nofollow">Automatic installation script</a></p>
<p dir="auto"><code> bash -c &#34;$(wget -O - https://apt.llvm.org/llvm.sh)&#34;</code></p>
</li>
</ul>
</li>
<li>conda (highly recommend)</li>
</ul>

<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p dir="auto">If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands</p>
</div>
<ol dir="auto">
<li>Clone the repo</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet"><pre>git clone --recursive https://github.com/microsoft/BitNet.git
<span>cd</span> BitNet</pre></div>
<ol start="2" dir="auto">
<li>Install the dependencies</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt"><pre><span><span>#</span> (Recommended) Create a new conda environment</span>
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt</pre></div>
<ol start="3" dir="auto">
<li>Build the project</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Download the model from Hugging Face, convert it to quantized gguf format, and build the project
python setup_env.py --hf-repo HF1BitLLM/Llama3-8B-1.58-100B-tokens -q i2_s

# Or you can manually download the model and run with local path
huggingface-cli download HF1BitLLM/Llama3-8B-1.58-100B-tokens --local-dir models/Llama3-8B-1.58-100B-tokens
python setup_env.py -md models/Llama3-8B-1.58-100B-tokens -q i2_s"><pre><span><span>#</span> Download the model from Hugging Face, convert it to quantized gguf format, and build the project</span>
python setup_env.py --hf-repo HF1BitLLM/Llama3-8B-1.58-100B-tokens -q i2_s

<span><span>#</span> Or you can manually download the model and run with local path</span>
huggingface-cli download HF1BitLLM/Llama3-8B-1.58-100B-tokens --local-dir models/Llama3-8B-1.58-100B-tokens
python setup_env.py -md models/Llama3-8B-1.58-100B-tokens -q i2_s</pre></div>
<pre>usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
</pre>


<div dir="auto" data-snippet-clipboard-copy-content="# Run inference with the quantized model
python run_inference.py -m models/Llama3-8B-1.58-100B-tokens/ggml-model-i2_s.gguf -p &#34;Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\nAnswer:&#34; -n 6 -temp 0

# Output:
# Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?
# Answer: Mary is in the garden.
"><pre><span><span>#</span> Run inference with the quantized model</span>
python run_inference.py -m models/Llama3-8B-1.58-100B-tokens/ggml-model-i2_s.gguf -p <span><span>&#34;</span>Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\nAnswer:<span>&#34;</span></span> -n 6 -temp 0

<span><span>#</span> Output:</span>
<span><span>#</span> Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?</span>
<span><span>#</span> Answer: Mary is in the garden.</span>
</pre></div>
<pre>usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
</pre>

<p dir="auto">We provide scripts to run the inference benchmark providing a model.</p>
<div data-snippet-clipboard-copy-content="usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. "><pre><code>usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
</code></pre></div>
<p dir="auto">Here&#39;s a brief explanation of each argument:</p>
<ul dir="auto">
<li><code>-m</code>, <code>--model</code>: The path to the model file. This is a required argument that must be provided when running the script.</li>
<li><code>-n</code>, <code>--n-token</code>: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.</li>
<li><code>-p</code>, <code>--n-prompt</code>: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.</li>
<li><code>-t</code>, <code>--threads</code>: The number of threads to use for running the inference. It is an optional argument with a default value of 2.</li>
<li><code>-h</code>, <code>--help</code>: Show the help message and exit. Use this argument to display usage information.</li>
</ul>
<p dir="auto">For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  "><pre>python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  </pre></div>
<p dir="auto">This command would run the inference benchmark using the model located at <code>/path/to/model</code>, generating 200 tokens from a 256 token prompt, utilizing 4 threads.</p>
<p dir="auto">For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128"><pre>python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

<span><span>#</span> Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate</span>
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128</pre></div>

<p dir="auto">This project is based on the <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> framework. We would like to thank all the authors for their contributions to the open-source community. We also thank <a href="https://github.com/microsoft/T-MAC/">T-MAC</a> team for the helpful discussion on the LUT method for low-bit LLM inference.</p>
</article></div></div>
  </body>
</html>
