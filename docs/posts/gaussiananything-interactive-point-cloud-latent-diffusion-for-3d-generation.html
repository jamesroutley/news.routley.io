<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nirvanalan.github.io/projects/GA/">Original</a>
    <h1>GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation</h1>
    
    <div id="readability-page-1" class="page"><div>
        <h2>Abstract</h2>
        <p> While 3D content generation has advanced significantly, existing methods still face challenges with
            input formats, latent space design, and output representations. This paper introduces a novel 3D
            generation framework that addresses these challenges, offering scalable, high-quality 3D generation
            with an interactive <i>Point Cloud-structured Latent</i> space. Our framework employs a
            Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using
            a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent
            diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything,
            supports multi-modal conditional 3D generation, allowing for point cloud, caption, and
            single/multi-view image inputs. Notably, the newly proposed latent space naturally enables
            geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate
            the effectiveness of our approach on multiple datasets, outperforming existing methods in both text-
            and image-conditioned 3D generation.</p>
        <!-- <a>
            Our method generates <i>high-quality</i> and <i>editable</i> surfel Gaussians through a cascaded 3D
            diffusion pipeline, given single-view images or texts as the conditions.
        </a> -->
    </div><div>
        <h2>Mulit-stage Native 3D Diffusion</h2>
        <!-- <p> An intriguing question arises - could SD features offer valuable and complementary semantic correspondences
            compared to widely explored discriminative features, such as those from the newly released DINOv2 model?</p>
        <img class="summary-img" src="./sd_dino_files/sd_dino_analysis.png" style="width:100%;">
        <h3>
            <center>Analysis of different features for correspondence.</center>
        </h3>
        <a> We present visualization of PCA for the inputs from DAVIS (left) and dense correspondence for SPair-71k
            (right).
            The figures show the performance of SD and DINO features under different inputs: identical instance (top
            left),
            pure object masks (bottom left), challenging inputs requiring semantic understanding (right top) and spatial
            information (right bottom).</a>
        <br>
        <p> Our qualitative analysis reveals that SD features have a strong sense of spatial layout and generate smooth
            correspondences, but its pixel level matching between two objects can often be inaccurate.
            While DINOv2 generates sparse but accurate matches, which surprisingly, form a natural complement to the
            higher
            spatial information from SD features.</p> -->

        <p><img src="https://nirvanalan.github.io/projects/GA/static/images/ga-diffusion.jpg"/></p><h3>
            <center>Diffusion training of GaussianAnything.</center>
        </h3>
        <p>
            Based on the point-cloud structure 3D VAE, we perform cascaded 3D diffusion learning given text (a) and
            image (b) conditions. We adopt DiT architecture with AdaLN-single and
            QK-Norm. For both condition modality, we send in the conditional feature
            with cross attention block, but at different positions. The 3D generation is achieved in two stages (c),
            where a point cloud diffusion model first generates the 3D layout \(\mathbf{z}_{x,0}\), and a texture diffusion model
            further generates the corresponding point-cloud features \(\mathbf{z}_{h,0}\). The generated latent code \(\mathbf{z}_0\) is
            decoded into the final 3D object with the pre-trained VAE decoder. </p>
    </div><div>
        <h2>Generation Results</h2>
        <p>Results for Image-conditioned 3D Generation. </p>
        <!-- <img class="summary-img" src="./static/images/ga-i23d.png" style="width:100%;"> -->
        <!-- <img class="summary-img" src="./static/images/ga-i23d.png" style="width:100%;"> -->
        <video width="100%" controls="" loop="" autoplay="" muted="">
            <source src="static/videos/concat-i23d-vid.mp4" type="video/mp4"/>
            Your browser does not support the video tag.
        </video>

        <div>
            <p>Input</p>
            <p>Open-LRM</p>
            <p>Splatter Image</p>
            <p>One-2-3-45</p>
            <p>CRM</p>
            <p>Lara</p>
            <p>LGM</p>
            <p>Shape-E</p>
            <p>LN3Diff</p>
            <p>Ours</p>
        </div>

        <!-- <strong>Qualitative Comparison of Image-to-3D</strong> -->
        <p>
            We showcase the novel view 3D reconstruction of all methods given a single image from unseen GSO dataset.
            Our proposed method achieves consistently stable performance across all cases.
            Note that though feed-forward 3D reconstruction methods achieve sharper texture reconstruction, these method
            fail to yield intact 3D predictions under challenging cases (\eg, the rhino in row 2).
            In contrast, our proposed native 3D diffusion model achieve consistently better performance.
            Better zoom in. </p>
    </div><div>
        <h2>Discussions</h2>
        <p>Compared to existing 3D generation framework such as SDS-based
            (DreamFusion), mulit-view generation-based (MVDream, Zero123++, Instant3D) and feedforward 3D
            reconstruction-based (LRM, InstantMesh, LGM), GaussianAnything is an native 3D Diffusion framework. Like
            2D/Video
            AIGC pipeline, GaussianAnything first trains a 3D-VAE and then conduct LDM training (text/image conditioned)
            on the
            learned latent space. Native 3D diffusion model shows better 3D consistency and higher success rate
            compared to feedforward 3D reconstruction model, as shown in the qualitative results above. We believe the
            proposed method has much potential
            and
            scales better with more
            data and compute resources, and yield better 3D editing performance due to its compatability with
            diffusion model.</p>
    </div><div>
        <h2>Concurrent Work</h2>
        <p>Concurrently, several impressive studies also leverage native 3D
            diffusion for 3D object generation:</p>
        <p><a href="https://sites.google.com/view/clay-3dlm">CLAY</a>
            proposes a comprehensive 3D generation framework that supports flexible conditional 3D generation, and is
            the state-of-the-art 3D generative model that supports <a href="https://hyperhuman.deemos.com/rodin?gad_source=1&amp;gclid=Cj0KCQiA88a5BhDPARIsAFj595horONDmE1pRCsn-04EL45dJcqZIlSyAgFRZ_CC3IhrjUjQP9jrvA4aAgPhEALw_wcB">Rodin</a>
            .</p>
        <p><a href="https://nju-3dv.github.io/projects/Direct3D/">Direct3D</a> proposes a native 3D diffusion model
            with high-quality surface generation. A hybrid conditional pipeline that leverages both CLIP and DINO
            features are employed.</p>
        <p><a href="https://github.com/wyysf-98/CraftsMan">Craftsman</a> also
            proposes a multi-view conditioned 3D surface diffusion model, along with an interactive refinement pipeline.
        </p>
        <p><a href="https://nirvanalan.github.io/projects/ln3diff/">LN3Diff</a>, our previous work, generates triplane
            given text or image as the condition. However, it only supports up to 192x192 resolution output due to the
            costly volume rendering.</p>
    </div></div>
  </body>
</html>
