<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kracekumar.com/post/deepseek_r1_aider_benchmark/">Original</a>
    <h1>DeepSeek R1 Aider Benchmark</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
  
  <p><time datetime="2025-01-26T00:52:45+0530">Sun, Jan 26, 2025</time></p><p><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">DeepSeek recently released its R1 model</a>, a state-of-the-art LLM that outperforms all available reasoning models on the market.
The accompanying paper includes a comprehensive comparison across 21 benchmarks in four categories:
<code>English, Code, Math, and Chinese</code> .</p>
<p><img src="https://kracekumar.com/images/aider/deepseek-comparision" alt="R1 benchamark results"/></p>
<p>As a software engineer, I was particularly curious about the Code category and
decided to explore the datasets and evaluation criteria.
While many benchmarks in this category were either poorly documented or required extensive dataset downloads.
Aider-polyglot stood out for its clear documentation and ease of use, <a href="https://github.com/Aider-AI/aider/blob/main/benchmark/benchmark.py">benchmark script</a></p>
<h3 id="what-is-aider">What is Aider?</h3>
<p>The benchmark is based on programming problems from exercism.io and covers six popular languages: <code>Python, Java, JavaScript, C++, Rust, and Go</code>.
The <a href="https://github.com/Aider-AI/aider/blob/main/benchmark/README.md">README</a> provides step-by-step instructions for running the benchmarks,
making it accessible even for those new to the AI.</p>
<h3 id="running-the-benchmark">Running the benchmark</h3>
<p>Set the <code>DEEPSEEK_API_KEY</code> while running the benchmark command.
I used the hosted version of DeepSeek to run the benchmark.
Hereâ€™s the command I executed for the Python benchmarks:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>$ ./benchmark/benchmark.py test-deepseek-r1-run --model r1 --edit-format whole --threads <span>10</span> --exercises-dir polyglot-benchmark --verbose --new --languages python
</span></span></code></pre></div><p><strong>Key CLI Parameters:</strong></p>
<ul>
<li><code>model: r1</code> (indicating the DeepSeek R1 model).</li>
<li><code>edit-format: whole</code> (the other option is edit, which was used in the original paper).</li>
<li><code>threads: 10</code> (number of Python threads to run in parallel).</li>
<li><code>languages</code>: python (by default, all languages are benchmarked).</li>
</ul>
<p><strong>Output:</strong></p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>- dirname: 2025-01-25-19-03-46--test-deepseek-r1-run
</span></span><span><span>  test_cases: <span>34</span>
</span></span><span><span>  model: deepseek/deepseek-reasoner
</span></span><span><span>  edit_format: whole
</span></span><span><span>  commit_hash: b276d48
</span></span><span><span>  pass_rate_1: 35.3
</span></span><span><span>  pass_rate_2: 64.7
</span></span><span><span>  pass_num_1: <span>12</span>
</span></span><span><span>  pass_num_2: <span>22</span>
</span></span><span><span>  percent_cases_well_formed: 100.0
</span></span><span><span>  error_outputs: <span>0</span>
</span></span><span><span>  num_malformed_responses: <span>0</span>
</span></span><span><span>  num_with_malformed_responses: <span>0</span>
</span></span><span><span>  user_asks: <span>0</span>
</span></span><span><span>  lazy_comments: <span>0</span>
</span></span><span><span>  syntax_errors: <span>0</span>
</span></span><span><span>  indentation_errors: <span>0</span>
</span></span><span><span>  exhausted_context_windows: <span>0</span>
</span></span><span><span>  test_timeouts: <span>1</span>
</span></span><span><span>  total_tests: <span>225</span>
</span></span><span><span>  command: aider --model deepseek/deepseek-reasoner
</span></span><span><span>  date: 2025-01-25
</span></span><span><span>  versions: 0.72.3.dev
</span></span><span><span>  seconds_per_case: 226.0
</span></span><span><span>  total_cost: 0.9313
</span></span><span><span>
</span></span><span><span>costs: <span>$0</span>.0274/test-case, <span>$0</span>.93 total, <span>$6</span>.16 projected
</span></span></code></pre></div><p>Most fields are self-explanatory, but two key metrics stand out: <code>pass_rate_1</code> and <code>pass_rate_2</code>,
which indicate the percentage of problems solved on the first and second attempts, respectively.
The R1 model achieved a <code>64.7%</code> pass rate across 34 exercises.
From the <a href="https://aider.chat/2024/12/21/polyglot.html">official leaderboard</a> the pass rate of <code>56.9%</code> across langauges.
This is not like to like comparision but for illustrative purpose.
Notably, the official website does not distinguish between pass rates for the first and second attempts.</p>
<p><img src="https://kracekumar.com/images/aider/polyglot-benchmark" alt="images/aider/polyglot-benchmark"/></p>
<h3 id="conclusion">Conclusion</h3>
<p>During the benchmark, I encountered a temporary issue where the DeepSeek API returned a 503 error.
While Aider employs exponential backoff to retry failed exercises, recovery can be time-consuming.</p>
<p>Following are some of the results from other language benchmarks except Java.</p>
<h3 id="c">C++</h3>
<pre tabindex="0"><code>$./benchmark/benchmark.py test-deepseek-r1-run-cpp --model r1 --edit-format whole --threads 10 --exercises-dir polyglot-benchmark --verbose --new --languages cpp
- dirname: 2025-01-25-19-26-20--test-deepseek-r1-run-cpp
  test_cases: 26
  model: deepseek/deepseek-reasoner
  edit_format: whole
  commit_hash: b276d48
  pass_rate_1: 19.2
  pass_rate_2: 69.2
  pass_num_1: 5
  pass_num_2: 18
  percent_cases_well_formed: 100.0
  error_outputs: 0
  num_malformed_responses: 0
  num_with_malformed_responses: 0
  user_asks: 0
  lazy_comments: 0
  syntax_errors: 0
  indentation_errors: 0
  exhausted_context_windows: 0
  test_timeouts: 0
  total_tests: 225
  command: aider --model deepseek/deepseek-reasoner
  date: 2025-01-25
  versions: 0.72.3.dev
  seconds_per_case: 410.2
  total_cost: 0.4168

costs: $0.0160/test-case, $0.42 total, $3.61 projected
</code></pre><h3 id="go">Go</h3>
<pre tabindex="0"><code>$./benchmark/benchmark.py test-deepseek-r1-run-go --model r1 --edit-format whole --threads 10 --exercises-dir polyglot-benchmark --verbose --new --languages go
rname: 2025-01-26-07-44-16--test-deepseek-r1-run-go
  test_cases: 39
  model: deepseek/deepseek-reasoner
  edit_format: whole
  commit_hash: b276d48
  pass_rate_1: 41.0
  pass_rate_2: 66.7
  pass_num_1: 16
  pass_num_2: 26
  percent_cases_well_formed: 100.0
  error_outputs: 0
  num_malformed_responses: 0
  num_with_malformed_responses: 0
  user_asks: 3
  lazy_comments: 0
  syntax_errors: 0
  indentation_errors: 0
  exhausted_context_windows: 0
  test_timeouts: 1
  total_tests: 225
  command: aider --model deepseek/deepseek-reasoner
  date: 2025-01-26
  versions: 0.72.3.dev
  seconds_per_case: 204.4
  total_cost: 0.8196

costs: $0.0210/test-case, $0.82 total, $4.73 projected
</code></pre><h3 id="javascript">Javascript</h3>
<pre tabindex="0"><code>./benchmark/benchmark.py test-deepseek-r1-run-javascript --model r1 --edit-format whole --threads 10 --exercises-dir polyglot-benchmark --verbose  --languages javascript --new
- dirname: 2025-01-26-14-52-31--test-deepseek-r1-run-javascript
  test_cases: 49
  model: deepseek/deepseek-reasoner
  edit_format: whole
  commit_hash: b276d48
  pass_rate_1: 22.4
  pass_rate_2: 57.1
  pass_num_1: 11
  pass_num_2: 28
  percent_cases_well_formed: 100.0
  error_outputs: 0
  num_malformed_responses: 0
  num_with_malformed_responses: 0
  user_asks: 2
  lazy_comments: 0
  syntax_errors: 0
  indentation_errors: 0
  exhausted_context_windows: 0
  test_timeouts: 1
  total_tests: 225
  command: aider --model deepseek/deepseek-reasoner
  date: 2025-01-26
  versions: 0.72.3.dev
  seconds_per_case: 236.6
  total_cost: 1.2589

costs: $0.0257/test-case, $1.26 total, $5.78 projected
</code></pre><h3 id="rust">Rust</h3>
<pre tabindex="0"><code>./benchmark/benchmark.py test-deepseek-r1-run-rust --model r1 --edit-format whole --threads 10 --exercises-dir polyglot-benchmark --verbose  --languages rust --new

- dirname: 2025-01-26-15-18-05--test-deepseek-r1-run-rust
  test_cases: 30
  model: deepseek/deepseek-reasoner
  edit_format: whole
  commit_hash: b276d48
  pass_rate_1: 50.0
  pass_rate_2: 63.3
  pass_num_1: 15
  pass_num_2: 19
  percent_cases_well_formed: 100.0
  error_outputs: 0
  num_malformed_responses: 0
  num_with_malformed_responses: 0
  user_asks: 3
  lazy_comments: 0
  syntax_errors: 0
  indentation_errors: 0
  exhausted_context_windows: 0
  test_timeouts: 0
  total_tests: 225
  command: aider --model deepseek/deepseek-reasoner
  date: 2025-01-26
  versions: 0.72.3.dev
  seconds_per_case: 174.1
  total_cost: 0.7162

costs: $0.0239/test-case, $0.72 total, $5.37 projected
</code></pre>
</div>


    </div></div>
  </body>
</html>
