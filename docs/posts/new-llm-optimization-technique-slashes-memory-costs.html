<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://venturebeat.com/ai/new-llm-optimization-technique-slashes-memory-costs-up-to-75/">Original</a>
    <h1>New LLM optimization technique slashes memory costs</h1>
    
    <div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-12-13T14:46:56+00:00" datetime="2024-12-13T14:46:56+00:00">December 13, 2024 6:46 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="422" src="https://venturebeat.com/wp-content/uploads/2024/12/transformer-memory.jpg?w=750" alt="transformer memory"/></p><p><span>Image credit: Sakana AI</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary">

		<article id="content">
			<div>
				<div id="boilerplate_2682874"><!-- wp:paragraph -->
<p><em>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav" data-type="link" data-id="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav">Learn More</a></em></p>
<!-- /wp:paragraph -->

<!-- wp:separator {"opacity":"css","className":"is-style-wide"} -->
<hr/>
<!-- /wp:separator --></div><p>Researchers at the Tokyo-based startup Sakana AI have developed a new technique that enables language models to use memory more efficiently, helping enterprises cut the costs of building applications on top of large language models (LLMs) and other Transformer-based models.</p>



<p>The technique, called “<a href="https://sakana.ai/namm/" target="_blank" rel="noreferrer noopener">universal</a><a href="https://sakana.ai/namm/"> transformer memory</a>,” uses special neural networks to optimize LLMs to keep bits of information that matter and discard redundant details from their context. </p>



<h2 id="h-optimizing-transformer-memory">Optimizing Transformer memory</h2>



<p>The responses of Transformer models, the backbone of LLMs, depend on the content of their “<a href="https://venturebeat.com/ai/googles-new-technique-gives-llms-infinite-context/" target="_blank" rel="noreferrer noopener">context</a><a href="https://venturebeat.com/ai/googles-new-technique-gives-llms-infinite-context/"> </a><a href="https://venturebeat.com/ai/googles-new-technique-gives-llms-infinite-context/" target="_blank" rel="noreferrer noopener">window</a>” — that is, what they receive as input from users.</p>



<p>The context window can be considered the model’s working memory. Tweaking the content of the context window can have a tremendous impact on the model’s performance, which has given rise to an entire field of “<a href="https://venturebeat.com/ai/why-prompt-engineering-is-one-of-the-most-valuable-skills-today/">prompt </a><a href="https://venturebeat.com/ai/why-prompt-engineering-is-one-of-the-most-valuable-skills-today/" target="_blank" rel="noreferrer noopener">engineering</a>.”</p>



<p>Current models support <a href="https://venturebeat.com/ai/google-unveils-gemini-1-5-a-next-gen-ai-model-with-million-token-context-window/">very long context </a><a href="https://venturebeat.com/ai/google-unveils-gemini-1-5-a-next-gen-ai-model-with-million-token-context-window/" target="_blank" rel="noreferrer noopener">windows</a> with hundreds of thousands, or even millions, of tokens (an LLM’s numerical representations of the words, word parts, phrases, concepts and numbers inputted by users in their prompts).</p>



<p>This enables users to cram more information into their prompts. However, longer prompts can result in higher compute costs and slower performance. Optimizing prompts to remove unnecessary tokens while keeping important information can reduce costs and increase speed.</p>



<p>Current prompt optimization techniques are resource-intensive or require users to manually test different configurations to reduce the size of their prompts.</p>



<h2 id="h-neural-attention-memory-modules">Neural attention memory modules</h2>



<p>Universal transformer memory optimizes prompts using neural attention memory models (NAMMs), simple neural networks that decide whether to “remember” or “forget” each given token stored in the LLM’s memory. </p>



<p>“This new capability allows Transformers to discard unhelpful or redundant details, and focus on the most critical information, something we find to be crucial for tasks requiring long-context reasoning,” the researchers write.</p>


<div>
<figure><img fetchpriority="high" decoding="async" width="2048" height="1412" src="https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?w=800" alt="Universal Transformer Memory" srcset="https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png 2048w, https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?resize=300,207 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?resize=768,530 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?resize=800,552 800w, https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?resize=1536,1059 1536w, https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?resize=400,276 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?resize=750,517 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?resize=578,399 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_55947c.png?resize=930,641 930w" sizes="(max-width: 2048px) 100vw, 2048px"/><figcaption>Universal transformer memory (source: Sakana AI)</figcaption></figure></div>


<p>NAMMs are trained separately from the LLM and are combined with the pre-trained model at inference time, which makes them flexible and easy to deploy. However, they need access to the inner activations of the model, which means they can only be applied to open-source models.</p>



<p>Like other techniques developed by Sakana AI, NAMMs are trained through <a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">evolutionary</a><a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm" target="_blank" rel="noreferrer noopener"> </a><a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">algorithms</a> instead of gradient-based optimization methods. By iteratively mutating and selecting the best-performing models through trial and error, evolution algorithms optimize NAMMs for efficiency and performance. This is especially important since NAMMs are trying to achieve a non-differentiable goal: keeping or discarding tokens.</p>



<p>NAMMs operate on the attention layers of LLMs, one of the key components of the Transformer architecture that determines the relations and importance of each token in the model’s context window. Based on attention values, NAMMs determine which tokens should be preserved and which can be discarded from the LLM’s context window. This attention-based mechanism makes it possible to use a trained NAMM on various models without further modification. For example, a NAMM trained on text-only data can be applied to vision or multi-modal models without additional training.</p>



<figure><img decoding="async" width="1576" height="555" src="https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?w=800" alt="NAMM" srcset="https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png 1576w, https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?resize=300,106 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?resize=768,270 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?resize=800,282 800w, https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?resize=1536,541 1536w, https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?resize=400,141 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?resize=750,264 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?resize=578,204 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_79a868.png?resize=930,328 930w" sizes="(max-width: 1576px) 100vw, 1576px"/><figcaption>Neural attention memory models (NAMMs) examine attention layers to determine which tokens should be kept or discarded from the context window (source: Sakana AI)</figcaption></figure>



<h2 id="h-universal-memory-in-action">Universal memory in action</h2>



<p>To test the universal transformer memory concept in action, the researchers trained a NAMM on top of an open-source Meta <a href="https://venturebeat.com/ai/llama-3-launches-alongside-new-stand-alone-meta-ai-chatbot/">Llama 3-8B model</a>. Their experiments show that with NAMMs, Transformer-based models perform better on natural language and coding problems on very long sequences. Meanwhile, by discarding unnecessary tokens, NAMM enabled the LLM model to save up to 75% of its cache memory while performing the tasks.</p>



<p>“Across our benchmarks, NAMMs provide clear performance improvements to the Llama 3-8B transformer,” the researchers write. “Furthermore, our memory systems yield notable side benefits, reducing the context size of each layer, while never being explicitly optimized for memory efficiency.” </p>



<figure><img decoding="async" width="1879" height="408" src="https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?w=800" alt="NAMM" srcset="https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png 1879w, https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?resize=300,65 300w, https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?resize=768,167 768w, https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?resize=800,174 800w, https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?resize=1536,334 1536w, https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?resize=400,87 400w, https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?resize=750,163 750w, https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?resize=578,126 578w, https://venturebeat.com/wp-content/uploads/2024/12/image_02bd17.png?resize=930,202 930w" sizes="(max-width: 1879px) 100vw, 1879px"/><figcaption>NAMM models compete with leading prompt optimization techniques while improving the model’s performance (source: Sakana AI)</figcaption></figure>



<p>They also tested the model on the 70B version of Llama as well as Transformer models designed for other modalities and tasks, such as <a href="https://venturebeat.com/ai/the-open-source-alternatives-to-gpt-4-vision-are-coming/" target="_blank" rel="noreferrer noopener">Llava</a> (computer vision) and Decision Transformer (reinforcement learning). </p>



<p>“Even in these out-of-distribution settings, NAMMs retain their benefits by discarding tokens such as redundant video frames and suboptimal actions, allowing their new base models to focus on the most relevant information to improve performance,” the researchers write.</p>



<h2 id="h-task-dependent-behavior">Task-dependent behavior</h2>



<p>Another interesting finding is that NAMMs automatically adjust their behavior based on the task. </p>



<p>For example, for coding tasks, the model discards contiguous chunks of tokens that correspond to comments and whitespaces that don’t affect the code’s execution. </p>



<p>On the other hand, in natural language tasks, the model discards tokens that represent grammatical redundancies and don’t affect the meaning of the sequence.</p>



<p>The researchers released the <a href="https://github.com/SakanaAI/evo-memory">code for creating your own NAMMs</a>. Techniques such as universal transformer memory can be very useful for enterprise applications that process millions of tokens and can benefit from speed boosts and cost reduction. The reusability of a trained NAMM also makes it a versatile tool to use across different applications in an enterprise.</p>



<p>For the future, the researchers suggest more advanced techniques, such as using NAMMs during the training of LLMs to further extend their memory capabilities.</p>



<p>“This work has only begun to tap into the potential of our new class of memory models, which we anticipate might offer many new opportunities to advance future generations of transformers,” the researchers write.  </p>
<div id="boilerplate_2660155"><!-- wp:shortcode -->
		<div>
			<div>
				<p><strong>Daily insights on business use cases with VB Daily</strong></p>
				<p>If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.</p>
				
				<p>Read our <a href="https://venturebeat.com/terms-of-service/">Privacy Policy</a></p>
				<p id="boilerplateNewsletterConfirmation">
					Thanks for subscribing. Check out more <a href="https://venturebeat.com/newsletters/">VB newsletters here</a>.
				</p>
				<p>An error occured.</p>
			</div>

							<p><img src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" alt=""/>
				</p>
			
		</div>
		
<!-- /wp:shortcode --></div>			</div><!-- .article-content -->

							
			
		</article><!-- #content .article-wrapper -->

	</div></div>
  </body>
</html>
