<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.alexmolas.com/2025/07/28/unexpected-benefit-llm.html">Original</a>
    <h1>Who needs Git when you have 1M context windows?</h1>
    
    <div id="readability-page-1" class="page"><div>
    


<p>
  <span>July 28, 2025 · <span title="Estimated read time">
  
  
  
  

  
  
  

  
  

  
    2 mins · 380 words
  
</span>
</span>
  <span>
    Share on:
    <a href="https://twitter.com/intent/tweet?text=Who+needs+git+when+you+have+1M+context+windows%3F+by+%40MolasAlex&amp;url=https://alexmolas.com%2F2025%2F07%2F28%2Funexpected-benefit-llm.html" target="_blank">X</a> ·
    <a href="https://news.ycombinator.com/submitlink?u=https://alexmolas.com%2F2025%2F07%2F28%2Funexpected-benefit-llm.html&amp;t=Who%20needs%20git%20when%20you%20have%201M%20context%20windows?" target="_blank">HN</a>
  </span>
</p>

<hr/>

<p>Lately I’ve heard a lot of stories of AI accidentally deleting entire codebases or <a href="https://x.com/jasonlk/status/1946069562723897802">wiping production databases</a>. But in my case it was the other way around. I removed some working code and the LLM helped me to recover it.</p>

<hr/>

<p>I joined RevenueCat a couple of months ago to work on LTV predictions. My first few projects were straightforward: fix small things, ship some low-hanging fruit. After getting familiar with the code, I decided to extend the LTV model and add some cool machine learning. So I dove in. Spent a couple of days wrangling data, cleaning it, understanding what was going on, and the usual standard pre-training stuff. I was in “research mode”, so all my code lived in notebooks and ugly scripts. But after enough trial and error, I managed to <strong>improve the main metric by 5%</strong>. I was hyped. Told the team. Felt great.</p>

<p>Then came the cleanup. I refactored all the sketchy code into a clean Python package, added tests, formatted everything nicely, added type hints, and got it ready for production. Just before opening the PR, I ran the pipeline again to double-check everything… and the <strong>results had dropped by 2%</strong>.</p>

<p>Oh shit… My ML model was now making worse predictions than the original model… And I never committed the changes that got me the +5%. Noob mistake. My teammate wasted no time laughing at my blunder</p>

<!-- _includes/image.html -->
<figure>
  <img src="https://www.alexmolas.com/docs/unexpected-benefit-llm/slack.png" alt="My colleague having a good fun" width="600"/>
  <figcaption>My colleague having a good fun</figcaption>
</figure>

<p>I spent the next few days trying to reproduce the original results, but it was impossible. Whatever secret sauce I’d stumbled on was gone. Then the weekend came and I went to the beach with my kids, and while making sand-castles I had an epiphany. I wasn’t working alone while developing the ML model. There was someone else helping me: my good old friend <code>gemini-2.5-pro</code>, with an incredible 1M token context window, was there all the time. Maybe, just maybe, it remembered. On Monday, after a great and relaxing weeked, I opened Cursor and asked</p>

<blockquote>
  <p>give me the exact original file of ml_ltv_training.py i passed you in the first message</p>
</blockquote>

<p>Boom. There it was, the original script that gave me the +5% uplift. An unexpected benefit of long-context LLMs. Who needs git best practices when you have an LLM that remembers everything?</p>



  </div></div>
  </body>
</html>
