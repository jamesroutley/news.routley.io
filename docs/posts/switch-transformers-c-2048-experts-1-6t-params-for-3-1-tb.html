<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/google/switch-c-2048">Original</a>
    <h1>Switch Transformers C – 2048 experts (1.6T params for 3.1 TB)</h1>
    
    <div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START -->
<p><a rel="noopener nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1666966931908-62441d1d9fdefb55a0b7d12c.png"><img alt="model image" src="https://cdn-uploads.huggingface.co/production/uploads/1666966931908-62441d1d9fdefb55a0b7d12c.png"/></a></p>

<ol start="0">
<li><a rel="noopener nofollow" href="#TL;DR">TL;DR</a></li>
<li><a rel="noopener nofollow" href="#model-details">Model Details</a></li>
<li><a rel="noopener nofollow" href="#usage">Usage</a></li>
<li><a rel="noopener nofollow" href="#uses">Uses</a></li>
<li><a rel="noopener nofollow" href="#bias-risks-and-limitations">Bias, Risks, and Limitations</a></li>
<li><a rel="noopener nofollow" href="#training-details">Training Details</a></li>
<li><a rel="noopener nofollow" href="#evaluation">Evaluation</a></li>
<li><a rel="noopener nofollow" href="#environmental-impact">Environmental Impact</a></li>
<li><a rel="noopener nofollow" href="#citation">Citation</a></li>
<li><a rel="noopener nofollow" href="#model-card-authors">Model Card Authors</a></li>
</ol>

<p>Switch Transformers is a Mixture of Experts (MoE) model trained on Masked Language Modeling (MLM) task. The model architecture is similar to the classic T5, but with the Feed Forward layers replaced by the Sparse MLP layers containing &#34;experts&#34; MLP. According to the <a rel="noopener nofollow" href="https://arxiv.org/pdf/2101.03961.pdf">original paper</a> the model enables faster training (scaling properties) while being better than T5 on fine-tuned tasks. 
As mentioned in the first few lines of the abstract : </p>
<blockquote>
<p> we advance the current scale of language models by pre-training up to trillion parameter models on the “Colossal Clean Crawled Corpus”, and achieve a 4x speedup over the T5-XXL model.</p>
</blockquote>
<p><strong>Disclaimer</strong>: Content from <strong>this</strong> model card has been written by the Hugging Face team, and parts of it were copy pasted from the <a rel="noopener nofollow" href="https://arxiv.org/pdf/2101.03961.pdf">original paper</a>.</p>

<h2>
	<a rel="noopener nofollow" href="#model-description" id="model-description">
		
	</a>
	<span>
		Model Description
	</span>
</h2>
<ul>
<li><strong>Model type:</strong> Language model</li>
<li><strong>Language(s) (NLP):</strong> English</li>
<li><strong>License:</strong> Apache 2.0</li>
<li><strong>Related Models:</strong> <a rel="noopener nofollow" href="https://huggingface.co/models?search=switch">All FLAN-T5 Checkpoints</a></li>
<li><strong>Original Checkpoints:</strong> <a rel="noopener nofollow" href="https://github.com/google-research/t5x/blob/main/docs/models.md#mixture-of-experts-moe-checkpoints">All Original FLAN-T5 Checkpoints</a></li>
<li><strong>Resources for more information:</strong><ul>
<li><a rel="noopener nofollow" href="https://arxiv.org/pdf/2101.03961.pdf">Research paper</a></li>
<li><a rel="noopener nofollow" href="https://github.com/google-research/t5x">GitHub Repo</a></li>
<li><a rel="noopener nofollow" href="https://huggingface.co/docs/transformers/model_doc/switch_transformers">Hugging Face Switch Transformers Docs (Similar to T5) </a></li>
</ul>
</li>
</ul>

<p>Note that these checkpoints has been trained on Masked-Language Modeling (MLM) task. Therefore the checkpoints are not &#34;ready-to-use&#34; for downstream tasks. You may want to check <code>FLAN-T5</code> for running fine-tuned weights or fine-tune your own MoE following <a rel="noopener nofollow" href="https://colab.research.google.com/drive/1aGGVHZmtKmcNBbAwa9hbu58DDpIuB5O4?usp=sharing">this notebook</a></p>
<p>Find below some example scripts on how to use the model in <code>transformers</code> - bear in mind that the model is <strong>extremely</strong> large, so you may consider using disk offload from <code>accelerate</code>:</p>
<h2>
	<a rel="noopener nofollow" href="#using-the-pytorch-model" id="using-the-pytorch-model">
		
	</a>
	<span>
		Using the Pytorch model
	</span>
</h2>
<h3>
	<a rel="noopener nofollow" href="#running-the-model-on-a-cpu" id="running-the-model-on-a-cpu">
		
	</a>
	<span>
		Running the model on a CPU
	</span>
</h3>
<details>
<summary> Click to expand </summary>

<pre><code>
<span>from</span> transformers <span>import</span> AutoTokenizer, SwitchTransformersForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained(<span>&#34;google/switch-c-2048&#34;</span>)
model = SwitchTransformersForConditionalGeneration.from_pretrained(<span>&#34;google/switch-c-2048&#34;</span>, device_map=<span>&#34;auto&#34;</span>, offload_folder=&lt;OFFLOAD_FOLDER&gt;)

input_text = <span>&#34;A &lt;extra_id_0&gt; walks into a bar a orders a &lt;extra_id_1&gt; with &lt;extra_id_2&gt; pinch of &lt;extra_id_3&gt;.&#34;</span>
input_ids = tokenizer(input_text, return_tensors=<span>&#34;pt&#34;</span>).input_ids

outputs = model.generate(input_ids)
<span>print</span>(tokenizer.decode(outputs[<span>0</span>]))
<span>&gt;&gt;&gt; </span>&lt;pad&gt; &lt;extra_id_0&gt; man&lt;extra_id_1&gt; beer&lt;extra_id_2&gt; a&lt;extra_id_3&gt; salt&lt;extra_id_4&gt;.&lt;/s&gt;
</code></pre>
</details>

<h3>
	<a rel="noopener nofollow" href="#running-the-model-on-a-gpu" id="running-the-model-on-a-gpu">
		
	</a>
	<span>
		Running the model on a GPU
	</span>
</h3>
<details>
<summary> Click to expand </summary>

<pre><code>
<span>from</span> transformers <span>import</span> AutoTokenizer, SwitchTransformersForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained(<span>&#34;google/switch-c-2048&#34;</span>)
model = SwitchTransformersForConditionalGeneration.from_pretrained(<span>&#34;google/switch-c-2048&#34;</span>, device_map=<span>&#34;auto&#34;</span>, offload_folder=&lt;OFFLOAD_FOLDER&gt;)

input_text = <span>&#34;A &lt;extra_id_0&gt; walks into a bar a orders a &lt;extra_id_1&gt; with &lt;extra_id_2&gt; pinch of &lt;extra_id_3&gt;.&#34;</span>
input_ids = tokenizer(input_text, return_tensors=<span>&#34;pt&#34;</span>).input_ids.to(<span>0</span>)

outputs = model.generate(input_ids)
<span>print</span>(tokenizer.decode(outputs[<span>0</span>]))
<span>&gt;&gt;&gt; </span>&lt;pad&gt; &lt;extra_id_0&gt; man&lt;extra_id_1&gt; beer&lt;extra_id_2&gt; a&lt;extra_id_3&gt; salt&lt;extra_id_4&gt;.&lt;/s&gt;
</code></pre>
</details>

<h3>
	<a rel="noopener nofollow" href="#running-the-model-on-a-gpu-using-different-precisions" id="running-the-model-on-a-gpu-using-different-precisions">
		
	</a>
	<span>
		Running the model on a GPU using different precisions
	</span>
</h3>
<h4>
	<a rel="noopener nofollow" href="#bp16" id="bp16">
		
	</a>
	<span>
		BP16
	</span>
</h4>
<details>
<summary> Click to expand </summary>

<pre><code>
<span>from</span> transformers <span>import</span> AutoTokenizer, SwitchTransformersForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained(<span>&#34;google/switch-c-2048&#34;</span>)
model = SwitchTransformersForConditionalGeneration.from_pretrained(<span>&#34;google/switch-c-2048&#34;</span>, device_map=<span>&#34;auto&#34;</span>, torch_dtype=torch.bfloat16, offload_folder=&lt;OFFLOAD_FOLDER&gt;)

input_text = <span>&#34;A &lt;extra_id_0&gt; walks into a bar a orders a &lt;extra_id_1&gt; with &lt;extra_id_2&gt; pinch of &lt;extra_id_3&gt;.&#34;</span>
input_ids = tokenizer(input_text, return_tensors=<span>&#34;pt&#34;</span>).input_ids.to(<span>0</span>)

outputs = model.generate(input_ids)
<span>print</span>(tokenizer.decode(outputs[<span>0</span>]))
<span>&gt;&gt;&gt; </span>&lt;pad&gt; &lt;extra_id_0&gt; man&lt;extra_id_1&gt; beer&lt;extra_id_2&gt; a&lt;extra_id_3&gt; salt&lt;extra_id_4&gt;.&lt;/s&gt;
</code></pre>
</details>

<h4>
	<a rel="noopener nofollow" href="#int8" id="int8">
		
	</a>
	<span>
		INT8
	</span>
</h4>
<details>
<summary> Click to expand </summary>

<pre><code>
<span>from</span> transformers <span>import</span> AutoTokenizer, SwitchTransformersForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained(<span>&#34;google/switch-c-2048&#34;</span>)
model = SwitchTransformersForConditionalGeneration.from_pretrained(<span>&#34;google/switch-c-2048&#34;</span>, device_map=<span>&#34;auto&#34;</span>, offload_folder=&lt;OFFLOAD_FOLDER&gt;)

input_text = <span>&#34;A &lt;extra_id_0&gt; walks into a bar a orders a &lt;extra_id_1&gt; with &lt;extra_id_2&gt; pinch of &lt;extra_id_3&gt;.&#34;</span>
input_ids = tokenizer(input_text, return_tensors=<span>&#34;pt&#34;</span>).input_ids.to(<span>0</span>)

outputs = model.generate(input_ids)
<span>print</span>(tokenizer.decode(outputs[<span>0</span>]))
<span>&gt;&gt;&gt; </span>&lt;pad&gt; &lt;extra_id_0&gt; man&lt;extra_id_1&gt; beer&lt;extra_id_2&gt; a&lt;extra_id_3&gt; salt&lt;extra_id_4&gt;.&lt;/s&gt;
</code></pre>
</details>


<h2>
	<a rel="noopener nofollow" href="#direct-use-and-downstream-use" id="direct-use-and-downstream-use">
		
	</a>
	<span>
		Direct Use and Downstream Use
	</span>
</h2>
<p>See the <a rel="noopener nofollow" href="https://arxiv.org/pdf/2101.03961.pdf">research paper</a> for further details.</p>
<h2>
	<a rel="noopener nofollow" href="#out-of-scope-use" id="out-of-scope-use">
		
	</a>
	<span>
		Out-of-Scope Use
	</span>
</h2>
<p>More information needed.</p>

<p>More information needed.</p>
<h2>
	<a rel="noopener nofollow" href="#ethical-considerations-and-risks" id="ethical-considerations-and-risks">
		
	</a>
	<span>
		Ethical considerations and risks
	</span>
</h2>
<p>More information needed.</p>
<h2>
	<a rel="noopener nofollow" href="#known-limitations" id="known-limitations">
		
	</a>
	<span>
		Known Limitations
	</span>
</h2>
<p>More information needed.</p>
<h2>
	<a rel="noopener nofollow" href="#sensitive-use" id="sensitive-use">
		
	</a>
	<span>
		Sensitive Use:
	</span>
</h2>
<p>More information needed.</p>

<h2>
	<a rel="noopener nofollow" href="#training-data" id="training-data">
		
	</a>
	<span>
		Training Data
	</span>
</h2>
<p>The model was trained on a Masked Language Modeling task, on Colossal Clean Crawled Corpus (C4) dataset, following the same procedure as <code>T5</code>.</p>
<h2>
	<a rel="noopener nofollow" href="#training-procedure" id="training-procedure">
		
	</a>
	<span>
		Training Procedure
	</span>
</h2>
<p>According to the model card from the <a rel="noopener nofollow" href="https://arxiv.org/pdf/2101.03961.pdf">original paper</a> the model has been trained on TPU v3 or TPU v4 pods, using <a rel="noopener nofollow" href="https://github.com/google-research/t5x"><code>t5x</code></a> codebase together with <a rel="noopener nofollow" href="https://github.com/google/jax"><code>jax</code></a>.</p>

<h2>
	<a rel="noopener nofollow" href="#testing-data-factors--metrics" id="testing-data-factors--metrics">
		
	</a>
	<span>
		Testing Data, Factors &amp; Metrics
	</span>
</h2>
<p>The authors evaluated the model on various tasks and compared the results against T5. See the table below for some quantitative evaluation:
<a rel="noopener nofollow" href="https://s3.amazonaws.com/moonup/production/uploads/1666967660372-62441d1d9fdefb55a0b7d12c.png"><img alt="image.png" src="https://s3.amazonaws.com/moonup/production/uploads/1666967660372-62441d1d9fdefb55a0b7d12c.png"/></a>
For full details, please check the <a rel="noopener nofollow" href="https://arxiv.org/pdf/2101.03961.pdf">research paper</a>.</p>
<h2>
	<a rel="noopener nofollow" href="#results" id="results">
		
	</a>
	<span>
		Results
	</span>
</h2>
<p>For full results for Switch Transformers, see the <a rel="noopener nofollow" href="https://arxiv.org/pdf/2101.03961.pdf">research paper</a>, Table 5.</p>

<p>Carbon emissions can be estimated using the <a rel="noopener nofollow" href="https://mlco2.github.io/impact#compute">Machine Learning Impact calculator</a> presented in <a rel="noopener nofollow" href="https://arxiv.org/abs/1910.09700">Lacoste et al. (2019)</a>.</p>
<ul>
<li><strong>Hardware Type:</strong> Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips ≥ 4.</li>
<li><strong>Hours used:</strong> More information needed</li>
<li><strong>Cloud Provider:</strong> GCP</li>
<li><strong>Compute Region:</strong> More information needed</li>
<li><strong>Carbon Emitted:</strong> More information needed</li>
</ul>

<p><strong>BibTeX:</strong></p>
<pre><code>@misc{https://doi.org/10.48550/arxiv.2101.03961,
  doi = {10.48550/ARXIV.2101.03961},
  
  url = {https://arxiv.org/abs/2101.03961},
  
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
</code></pre>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
