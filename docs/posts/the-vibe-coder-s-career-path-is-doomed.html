<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.florianherrengt.com/vibe-coder-career-path.html">Original</a>
    <h1>The vibe coder&#39;s career path is doomed</h1>
    
    <div id="readability-page-1" class="page"><article>
      <p>
        Let me get one thing out of the way immediately: LLMs are helpful. This
        isn&#39;t about whether LLMs can write code. They can. It&#39;s about why vibe
        coding might be the worst career investment you&#39;ll make.
      </p>
      <p>
        I started noticing this shift when developer conversations changed
        completely. Now it&#39;s all about getting Claude to write code for you. Or
        the holy grail: getting AI to do everything without your intervention.
      </p>
      <p>
        Until recently, I&#39;d been mostly ignoring the hype. I&#39;d read headlines,
        occasionally ask Claude or ChatGPT to help me debug something, but not
        much else. Time to learn vibe coding!
      </p>
      <h2>“What are you vibe coding?”</h2>
      <p>
        A Telegram bot. It&#39;s a greenfield project. Some dashboards with
        real-time updates. Nothing too complex but not trivial either. Just a
        standard REST API with a React frontend.
      </p>
      <p>
        I set up the full AI coding workflow: Claude, Playwright and Postgres
        MCPs, multiple agents working on different branches and comprehensive
        documentation files. Then I started vibe coding.
      </p>
      <p>
        Claude was updating schemas, writing endpoints, clicking buttons in
        Chrome, checking Postgres data and opening pull requests. It worked. My
        first reaction:
      </p>
      <p>&#34;Holy shit! This is crazy!&#34;</p>
      <p>
        There&#39;s clearly going to be a gold rush. I don’t have to spend time
        writing code anymore. I need more agents, more automation. The factory
        must grow! I now have an army of junior devs available 24/7.
      </p>
      <p>
        I was easily adding 2 -3 features every day. The barrier between
        thinking and implementing just disappeared. It was so satisfying.
      </p>
      <p>
        As the project got more complex, things changed. Claude kept repeating
        the same mistakes, getting stuck in loops. The context switching became
        brutal. </p>
      <p>
        In the end, I&#39;m still limited by mental energy. Context switching
        between multiple AI-generated branches only works for small tasks. For
        complex problems, I still had to think through the solution myself.
        Claude was just doing the typing for me.
      </p>
      <p>
        I spent more time testing and writing instruction files for Claude than
        I&#39;ve ever spent on any project of this size. I&#39;ve worked with junior
        devs straight out of bootcamp and none required this level of
        hand-holding.
      </p>
      <p>
        Anyway, I shipped it to my 3 test users and everything caught fire.
        Messages wouldn&#39;t sync, users got assigned to the wrong accounts. I
        found myself begging Claude to fix bug after bug. How did I get myself
        into this? This sucked. It was just chaos.
      </p>
      <p>
        The last time this happened to me was when I worked with an offshore
        team. Nobody really cared about the code quality and everyone was solely
        focused on shipping fast. I had too many PRs to review, opened by 5
        different people who didn’t really know or care about what they were
        doing. I only had a surface-level understanding of what was going on,
        turning into some kind of orchestrator who… wait. This sounds familiar.
      </p>
      <p>
        Is this the future of software engineering? Was I missing something? Why
        would anyone want to invest in this?
      </p>
      <h2>“Early adopters will have an advantage”</h2>
      <p>
        Vibe coding skills aren&#39;t particularly hard to acquire. I went from
        knowing nothing to being competent in a few weeks. Even if it became the
        industry’s standard, anyone can be up to speed pretty quickly. </p>
      <p>
        Meanwhile, whatever I learned about vibe coding is already obsolete. I
        checked Hacker News this morning. Companies are shipping products that
        automate away the exact workflows I just mastered. There&#39;s no
        first-mover advantage when the entire playing field gets bulldozed.
      </p>
      <p>
        There&#39;s no lasting competitive advantage. No deep technical skills to
        master.
      </p>
      <p>
        The vibe coding barrier to entry is collapsing so fast that “early
        adopters” are just beta testers. You&#39;re subsidising R&amp;D for tools that
        will commoditise your skills.
      </p>
      <h2>“It’s all about knowing how to prompt”</h2>
      <p>
        My prompting approach? I switch to plan mode and describe what I want.
        Then keep replying with &#34;If anything is ambiguous or unclear, please ask
        for clarification&#34; until I&#39;m satisfied. That&#39;s it. It works.
      </p>
      <p>
        Compare that to learning something like Rust, which I&#39;ve been struggling
        with for months now. It’s not just syntax. It&#39;s completely new concepts.
        That&#39;s something you can&#39;t just pick up.
      </p>
      <p>
        Prompting is not a sophisticated skill requiring extensive training.
      </p>
      <p>
        People spend thousands of hours mastering how to write code. They learn
        how to design data schemas that can adapt to new requirements, structure
        systems where bugs are easy to hunt down and fix. That&#39;s nowhere near
        prompting skills.
      </p>
      <h2>“I don’t care, it makes me 10x faster”</h2>
      <p>
        Faster at what? Prototyping? Boilerplate? That&#39;s very short-lived. The
        vast majority of software engineers work on production systems, not
        greenfield projects.
      </p>
      <p>
        What LLMs are really good at is writing code very fast. Imagine you have
        two novelists. One types 50wpm, the other 200wpm. Does the fast typist
        finish 4x sooner? No. Because they spend most of their time on the plot,
        characters and creating a coherent story.
      </p>
      <p>
        Did you ever work on a project where nothing moves forward? Everything&#39;s
        just slow. The app is slow. Adding features is slow. Fixing bugs takes
        forever. Did you think “this is because devs can&#39;t write code fast
        enough”? Or was it wrong architecture, wrong culture, broken
        communication, unclear requirements, poor technology choices?
      </p>
      <p>
        At the very least, the assumption that AI makes development dramatically
        faster deserves scrutiny. </p>
      <p>
        It&#39;s a different way to build software, which comes with its own
        trade-offs.
      </p>
      <h2>“It makes my job easier”</h2>
      <p>Vibe coding trades clarity for velocity.</p>
      <p>
        You ship fast but lose your mental map. It’s a delicate balance. During
        my experiment, I watched myself building resistance to manually changing
        code. It was easier to tell the LLM &#34;It doesn&#39;t work&#34; and paste a stack
        trace. I found myself asking for tiny changes like &#34;Now make it blue&#34;.
      </p>
      <p>
        Why? Because I lost track of where things were and what they did. I
        don’t even remember in which file this button is. Yes, of course I
        reviewed the PRs. Do you know how hard it is to properly review code? To
        build a mental model of what’s going on? Now there are a dozen PRs in
        your queue. </p>
      <p>
        At some point, I hit a wall. Claude couldn&#39;t fix a bug after begging it
        many times. I was forced to jump in. And fuck me, this was hard work.
        Thinking is hard work and I&#39;ve been avoiding it for a while now. Like
        trying to run a marathon after months on the couch. It took me so long
        to get up to speed that I lost all my productivity gains.
      </p>
      <p>
        As I&#39;m writing this, I just received another bug report. I have zero
        idea why it happens or where to start.
      </p>
      <p>So much for making my job easier.</p>
      <h2>“So you don’t use LLMs then?”</h2>
      <p>
        After reading this, you might think I&#39;m a die-hard anti-AI. I&#39;m not.
      </p>
      <p>
        AI helped me write this. English is my second language and my writing
        skills aren&#39;t that great. I used it to clean up grammar, improve
        sentence flow and make my ideas clearer. But I&#39;m not a professional
        writer and I&#39;m not claiming to be one. This is just a blog post, not an
        essay or a book.
      </p>
      <p>I also use AI for coding. Shocking, I know.</p>
      <p>
        But that&#39;s nothing like vibe coding. I don&#39;t mind using Claude Code on a
        VERY short leash with a specific purpose and I understand it costs me
        more than tokens. </p>
      <p>
        I’m not even against vibe coding itself. Sometimes you just have to cut
        corners and get shit done. Perfect can wait because you need the feature
        yesterday. Tech debt is a tool and it’s totally reasonable to use it.
        I&#39;ve watched too many products slowly die while developers polished code
        that no one ever used. But full-time vibe coding? Cut me some slack.
      </p>
      <p>
        The idea of autonomous AI development is just a fantasy. You can’t just
        replace expertise with tools. The most valuable developers are the ones
        with a strong mental map of where things are and what they do.
      </p>
      <p>
        Using LLMs isn&#39;t the same as writing code. It doesn&#39;t create the same
        value and certainly doesn&#39;t produce better results. It’s technical debt.
      </p>
      <h2>“Soon, everyone will be a developer”</h2>
      <p>
        I’ve seen some amazing businesses built on top of Excel and no-code. Of
        course, you can build an app with Claude. It doesn’t make you a software
        engineer. Yes, I am gatekeeping. It’s for your own good because...
      </p>
      <p>
        Unlike people assembling tools to create products, the vibe coder
        creates a huge mess. I&#39;ve lost count of the horror stories from people
        who&#39;ve inherited AI-generated codebases. No one&#39;s thinking about
        anything. After all, why bother when the AI can just do everything
        anyway?
      </p>
      <p>
        The real difference is what professional developers actually do:
        architecture, creating and debugging complex systems, security,
        maintenance. They don&#39;t get six-figure salaries because they can quickly
        spin up an MVP.
      </p>
      <p>
        Creating something special still takes domain knowledge, acquired
        through time and effort with or without AI.
      </p>
      <h2>“AI Won&#39;t Take Your Job, Someone Using AI Will”</h2>
      <p>
        This is yet another empty claim telling people to quickly rush into
        using AI. This implies that if you don’t learn how to use AI today,
        you’ll be irrelevant.
      </p>
      <p>
        I don’t believe this is true but if you truly believe AI is soon to
        become good enough to handle complex development work, why are you
        investing in it? What happens to your salary when the skills required
        drop significantly? If AI writes better code than you, why would anyone
        hire you specifically?
      </p>
      <p>
        Either AI is years away from writing production-quality code and there&#39;s
        no urgency or it will soon make coding so trivial that it becomes
        minimum-wage work. There&#39;s no lucrative middle ground where &#39;AI
        whispering&#39; is a high-value skill.
      </p>
      <p>
        If the future is “AI-augmented” development, even with gradual adoption,
        you&#39;re not coding anymore. You&#39;re babysitting. Your day consists of
        reviewing AI-generated PRs you barely understand and working on a
        codebase you can&#39;t mentally model.
      </p>
      <p>
        That&#39;s not engineering. It&#39;s middle management cosplaying as QA
        reviewing tickets they can&#39;t solve from workers who can&#39;t think.
      </p>
      <h2>“It’s only going to get better”</h2>
      <p>
        For LLMs to keep improving, we need one of three things: more data, more
        power or a breakthrough.
      </p>
      <p>
        Data is getting harder to find. Regulatory constraints, ethical
        considerations and public scrutiny move much slower than technology.
        They are also likely to run out of high-quality text data between 2026
        and 2032 and synthetic data (using LLMs to generate more data) causes
        model collapse and bias amplification.
      </p>
      <p>
        Power isn&#39;t unlimited either. Data centres are concentrated in specific
        regions. We don&#39;t have the electrical grid infrastructure to deliver
        power to them at the scale they need. Other energy-intensive
        technologies, like electric vehicles, are also competing for grid
        capacity. And if we take a broader view, like meeting our climate goals,
        diverting more power to GPUs may not be the most pressing political
        priority.
      </p>
      <p>
        Breakthroughs are rare. Modern LLMs are based on Google&#39;s papers:
        &#34;Attention Is All You Need&#34; (2017) and BERT (2018). Almost a decade ago.
        Since then, improvements have come from scaling, not new architectures.
        Every new release is becoming less and less impressive because
        transformers are just hitting fundamental limitations that incremental
        improvements can&#39;t solve.
      </p>
      <p>
        We can be hopeful for breakthroughs but they&#39;re unpredictable by nature.
        More likely, we&#39;ll see smaller models become more capable rather than
        dramatically more powerful ones.
      </p>
      <h2>“You’re just a sceptic”</h2>
      <p>
        The AI industry is built on subsidised resources while burning VC money
        with no clear path to profit. Datacenters get discounted land, tax
        breaks and infrastructure upgrades paid by the public.
      </p>
      <p>
        They socialised the costs, privatised the profits and they&#39;re still
        nowhere near profitability.
      </p>
      <p>
        They claim to make everyone 10x, even 100x, more productive but they
        have no path to profit. Why are they all failing to capture that value?
      </p>
      <p>
        When Google launched, it had better algorithms. Yahoo and AltaVista,
        despite their vastly superior resources, couldn’t keep up. After Apple
        released the iPhone, it was such a great product that Blackberry and
        Nokia just slowly died.
      </p>
      <p>
        Today, every billionaire has their own pet AI. None are significantly
        better. Each release slightly one-ups the others on arbitrary
        benchmarks, quickly followed by similar open-source models. This can&#39;t
        keep going forever.
      </p>
      <h2>“What if you’re wrong?”</h2>
      <p>
        If AI soon becomes good enough at building software on its own, software
        engineering as we know it is dead. I have no interest in becoming a
        glorified project manager, orchestrating AI agents all day long. If it
        does happen, I am now competing with anyone who can type a prompt. I’m
        not betting my career on being slightly better at prompting than
        millions of others.
      </p>
      <p>
        Since I see no clear path to this happening any time soon, my bet is
        that we&#39;re actually much further away from this scenario than AI
        companies want us to think and they keep making extraordinary claims to
        raise more funding.
      </p>
      <p>
        If I&#39;m right, I didn&#39;t waste my time learning temporary skills instead
        of building real expertise.
      </p>
      <h2>Sources</h2>
      <p>
        <a href="https://arxiv.org/abs/2211.04325">Will we run out of data? Limits of LLM scaling based on
          human-generated data</a>
        </p>
    </article></div>
  </body>
</html>
