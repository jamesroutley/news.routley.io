<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.atlassian.com/engineering/april-2022-outage-update">Original</a>
    <h1>Update on the Atlassian outage affecting some customers</h1>
    
    <div id="readability-page-1" class="page"><div>
					
<p>On Tuesday, April 4th, 20:12 UTC, approximately 400 Atlassian Cloud customers experienced a full outage across their Atlassian products. We are in the process of restoring sites, and have restored service to ~45% of impacted users. We expect we will make a full recovery for the rest of our customers within the next two weeks.</p>



<p>To be clear, this incident was not a cyber attack nor was it a failure of our systems to scale. Additionally, the majority of restored customers have had no data loss, while some have reported data loss for up to 5 minutes prior to the incident.</p>



<p><strong>Let me start by saying that this incident and our response time are not up to our standard, and I apologize on behalf of Atlassian. </strong>We know that our products are mission-critical to your teams and that your business is impacted when our services are unavailable. We are working around the clock to get our customers back up and running and this is the top priority.</p>



<h2 id="What-happened">What happened</h2>



<p>One of our standalone apps for Jira Service Management and Jira Software, called &#34;Insight – Asset Management,&#34; was fully integrated into our products as native functionality. Because of this, we needed to deactivate the standalone legacy app on customer sites that had it installed. Our engineering teams planned to use an existing script to deactivate instances of this standalone application. However, two critical problems ensued:</p>



<ul><li><strong>Communication gap. </strong>First, there was a communication gap between the team that requested the deactivation and the team that ran the deactivation. Instead of providing the IDs of the intended app being marked for deactivation, the team provided the IDs of the entire cloud site where the apps were to be deactivated.</li><li><strong>Faulty script. </strong>Second, the script we used provided both the &#34;mark for deletion&#34; capability used in normal day-to-day operations (where recoverability is desirable), and the &#34;permanently delete&#34; capability that is required to permanently remove data when required for compliance reasons. The script was executed with the wrong execution mode and the wrong list of IDs. The result was that sites for approximately 400 customers were improperly deleted.</li></ul>



<p>To recover from this incident, our global engineering team has implemented a methodical process for restoring our impacted customers.</p>



<h2>What we are doing to recover</h2>



<p><a href="https://www.atlassian.com/trust/security/data-management">Atlassian Data Management</a> describes our data management processes in detail.</p>



<p>To ensure high availability, we provision and maintain a synchronous standby replica in multiple AWS Availability Zones (AZ). The AZ failover is automated and typically takes 60-120 seconds, and we regularly handle data center outages and other common disruptions with no customer impact.</p>



<p>We also maintain immutable backups that are designed to be resilient against data corruption events, which enable recovery to a previous point in time. Backups are retained for 30 days, and Atlassian continuously tests and audits storage backups for restoration.</p>



<p>Using these backups, we regularly roll-back individual customers or a small set of customers who accidentally delete their own data. And, if required, we can restore all customers at once into a new environment.</p>



<p>What we have not (yet) automated is restoring a large subset of customers into our existing (and currently in use) environment without affecting any of our other customers.</p>



<p>Within our cloud environment, each data store contains data from multiple customers. Because the data deleted in this incident was only a portion of data stores that are continuing to be used by other customers, we have to manually extract and restore individual pieces from our backups. Each customer site recovery is a lengthy and complex process, requiring internal validation and final customer verification when the site is restored.</p>



<h3 id="Steps-in-the-process">Steps in the process</h3>



<p>Our initial approach to restoring customer sites was only semi-automated. It involved a series of complex steps that were time-consuming, in part because it required us to manually validate customer data for each site that was restored.</p>



<p>We are now shifting to a more automated process, as follows:</p>



<ol><li>Re-enable metadata for deleted sites in our centralized orchestration system</li><li>Restore customer data extracted from backups, including users, permissions, etc.</li><li>Re-enable ecosystem apps, billing data and other data not directly associated with customer-generated data</li></ol>



<p>Because multiple data stores must be extracted and recovered for each site, we test the site and work closely with each individual customer to ensure the accuracy of their restoration.</p>



<p>Currently, we are restoring customers in batches of up to 60 tenants at a time. End-to-end, it takes between 4 and 5 elapsed days to hand a site back to a customer. Our teams have now developed the capability to run multiple batches in parallel, which has helped to reduce our overall restore time.</p>



<h2 id="Restoring-customers-is-our-top-priority">Restoring customers is our top priority</h2>



<p>We know that incidents like this can erode trust. We are not meeting the high standards that we set for ourselves. This includes our communications efforts, which until now were entirely focused on reaching our impacted customers directly.</p>



<p>This incident remains the top priority for my engineering team and for our entire company. We will continue working around the clock until every single customer&#39;s site is restored.</p>



<p>Here&#39;s what you can expect next:</p>



<ul><li><strong>Restoring customer sites.</strong> We will continue to work directly with affected customers to restore their sites, communicating 1:1 via support tickets and through our Customer Support team. We are moving through this as fast as possible.</li><li><strong>Daily updates. </strong>We are committed to daily updates to our impacted customers via their tickets – and via daily <a href="https://status.atlassian.com/">status page updates</a>.</li><li><strong>Post-incident review. </strong>Following this incident, we will conduct and share a post-incident review with our findings and next steps. This report will be public.</li></ul>



<p>Finally to our customers: Thank you for your partnership as we navigate each and every step together. We know that you have stakeholders to answer to within your organization and that our failure has resulted in major disruptions to your business. My teams and I are committed to restoring service for each customer as soon as possible, and doing what we can to make this right for you.</p>
				</div></div>
  </body>
</html>
