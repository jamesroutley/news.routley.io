<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://charlie-volow.com/blog/posts/endpoints.html">Original</a>
    <h1>Jackdaw Endpoints</h1>
    
    <div id="readability-page-1" class="page"><div id="indexable">
	    <h3 id="context-contents">Context / Contents</h3>
    <p><a href="https://github.com/chvolow24/jackdaw/#readme">Jackdaw</a> is
    my free digital audio workstation (DAW), currently in progress. This
    post, like all programming, is basically about abstraction; it
    dissects a very big and useful abstraction (the Jackdaw “endpoint”),
    addressing in particular some issues related to concurrency.</p>
    
    <p>In my <a href="https://charlie-volow.com/blog/posts/ctrl_z.html">last
    post</a>, I introduced a problem that arose in the course of
    developing an undo/redo feature for Jackdaw. I will briefly restate
    the problem here:</p>
    <p>Some project parameters – values that have significance in the
    DAW project – are associated with GUI components like sliders, which
    can be used to modify the values. These parameters can usually be
    modified in other ways as well. For example: a track’s volume can be
    changed by clicking and dragging a slider that appears on the track
    interface, by holding Shift + “-” or Shift + “=” while the track is
    selected, or with mix automation. It can also be changed by
    executing an undo/redo.</p>
    <p><img src="https://charlie-volow.com/assets/blog_imgs/vol_methods.gif" width="100%"/></p><p>
    Track volume being modified first with the keyboard, then the mouse
    </p>
    <p>In order to make an undo/redo possible, we must at some point
    during the execution of the change push a <code>UserEvent</code>
    struct describing how the undo/redo should be executed to a
    project-level data structure. But where do we do the push? If we put
    it in the slider code, then we also have to put it in the keyboard
    code, and in the automation code, and all of these instances need a
    way to “find” the associated slider and modify its appearance to
    reflect the new value. There ought to be some symmetry between these
    different methods of modifying the same parameter, but the unifying
    thing here – the track volume – is just a <code>float</code> living
    in the parent <code>Track</code> struct. So how and where do we
    capture the symmetry?</p>
    <h3 id="clusters-of-associated-behavior">Clusters of associated
    behavior</h3>
    <p>It became clear that some values, like the track volume, are
    <em>not just values:</em> they have clusters of associated behavior.
    Any time the track volume is modified – regardless of how it is
    modified – the associated GUI slider must change, and an undoable
    <code>UserEvent</code> must be registered.</p>
    <p>Other parameters have other types of associated behavior. If the
    cutoff frequency of a lowpass filter effect is modified, several
    things must happen:</p>
    <ol type="1">
    <li>the frequency response of the filter is recomputed;</li>
    <li>a slider representing that cutoff frequency changes appearance,
    <em>if the slider currently exists</em>;</li>
    <li>an undoable <code>UserEvent</code> is registered.</li>
    </ol>
    <h3 id="endpoints">Endpoints</h3>
    <p>I had, months before this juncture, mused to a friend that it
    would be cool to design my DAW with a taxonomy of API endpoints,
    which could be modified safely from outside of the program. I was
    thinking a lot at the time about integrating with <a href="https://puredata.info/">pure data</a>, and allowing it to
    control Jackdaw parameters.</p>
    <p>I did not anticipate that such a concept would be crucial not
    just for inter-process communication (IPC), but for internal program
    operation as well. Here, in this
    parameters-with-associated-behaviors problem, was the perfect
    impetus to finally implement endpoints, and with a much richer
    understanding of their value.</p>
    <p>So, thus far, we know that an endpoint must contain a pointer to
    its target value and pointers to one or more functions that describe
    the associated behaviors. When an endpoint write operation is
    triggered, those functions (“callbacks”) will run after the value
    itself has been modified. That’s all simple enough. But now
    concurrency enters the picture and wantonly swirls the fresh paint
    until we can scarcely make out the original subject.</p>
    <h3 id="concurrency">Concurrency</h3>
    <p>Due to the original IPC concept, it was clear at the outset that
    endpoint operations would need to be thread-safe. In fact, thread
    safety generally was due some serious attention; I had been
    developing the program until this time with an uneasy awareness that
    there was a class of race conditions that I had quietly deemed
    acceptable, because in practice they never caused any problems. To
    return to the archetypal example, I was allowing direct modification
    of the track volume on the main thread all over the place, with that
    very same value being read on another thread (the “DSP thread”,
    which governs real-time audio playback) to scale arrays of audio
    samples, all with nary a mutex in sight. I’m embarrassed to admit
    that I still don’t understand theoretically what distinguishes those
    race conditions that cause crashes or other undesired behavior from
    the ones that seemingly don’t. (Are float reads/writes reliably
    atomic on x86 and ARM?) I’ll read up on this eventually, but in the
    meantime, I knew I could leverage the endpoint abstraction to
    finally expunge my shameful secret locked (or rather, not locked) in
    the attic.</p>
    <p>What I wanted to avoid, however, was any solution that
    meaningfully delayed the operation of the “DSP thread.” The
    real-time audio component of the application needs to run
    <em>fast,</em> and there are many parameters that can be modified on
    the main thread that must be read on the DSP thread to process audio
    for playback. I didn’t want to put locks on every single shared
    parameter. (More on this at <a href="#is-my-clever-concurrency-model-actually-kind-of-dumb">the
    bottom</a>).</p>
    <p>So how do I get read operations on a shared variable to run as
    fast as possible on the DSP thread without introducing race
    conditions?</p>
    <h3 id="thread-ownership">Thread ownership</h3>
    <p>Fortunately, there is no stringent requirement that reads/writes
    to these audio parameters be fast on the <em>main</em> thread, or on
    any thread other than the DSP thread. There is also a convenient
    design paradigm that we can exploit: both the main thread and the
    DSP thread run on a loop. This means that a change requested on one
    thread can be queued for execution on the next iteration of the
    <em>other</em> thread’s loop. An endpoint can be assigned an “owner
    thread”, such that a read operation on the owner thread always
    occurs immediately. This is possible if direct modification of the
    target variable only occurs on the owner thread.</p>
    <pre><code>* Writes:
    - on owner thread: in mutex critical region
    - on other thread: queued for later execution on the owner thread.
* Reads:
    - on owner thread: occurs immediately; no lock
    - on other thread: in mutex critical region
  </code></pre>
    <p>Writes on the owner thread – including those that were requested
    and queued by other threads – still require a lock to guarantee
    thread safety. But that’s ok; typically, the DSP thread, which owns
    all sensitive audio-related parameters, only requests read access to
    those values, and it is the main thread (which handles user inputs)
    that writes to them. Lock contention will only occur if a read
    operation is requested on some non-owner thread concurrent with a
    write operation on the owner thread, and that scenario is generally
    improbable.</p>
    <h3 id="queued-value-changes">Queued value changes</h3>
    <p>The list of queued value changes, which is flushed once per loop
    iteration, must be shared by all threads and and protected by a
    mutex lock. In a more basic model, where every single shared
    variable is protected by its own lock, the owner thread would need
    to lock on every single read of a shared variable; in this model,
    those reads are lock-free, at the cost of a single lock on the
    flushing of the queue that occurs once per loop iteration.</p>
    <h3 id="concurrency-and-callbacks">Concurrency and callbacks</h3>
    <p>Those clusters of behavior discussed above – the endpoint
    callbacks – fall into two main categories:</p>
    <ol type="1">
    <li>Actions that ought to occur on the DSP thread, because they
    relate to audio processing</li>
    <li>Actions that ought to occur on the main thread, because they
    relate to the GUI (drawing to the screen)</li>
    </ol>
    <p>Therefore, endpoint callbacks are specified per-thread; one each
    for the DSP thread (the “dsp callback”) and for main (the “gui
    callback”). Relegating all GUI-related behavior to the main thread
    and audio-related behavior to the DSP thread <em>regardless of which
    thread initiates those changes</em> saves us from a lot of granular
    locking operations and greatly reduces the complexity of the program
    overall, at the cost of some latency and a somewhat complicated
    endpoint design.</p>
    <p>The callbacks are handled in a similar manner to the value
    changes themselves. Callbacks that must run on a non-owner thread
    are queued to be executed on the next iteration of that thread’s
    loop, after the value has been changed.</p>
    <p>Complexity arises when, for example, the main thread writes to a
    DSP-owned endpoint, but also includes a GUI callback. The GUI
    callback can’t run immediately, because the target value must first
    be modified on the DSP thread. Instead, the GUI callback is
    <em>queued to be queued</em> later, after the value change is
    done.</p>
    <table>
    <thead>
    <tr>
    <th>Main</th>
    <th>DSP</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>1. Initiate write</td>
    <td>…</td>
    </tr>
    <tr>
    <td>2. Queue value change</td>
    <td>…</td>
    </tr>
    <tr>
    <td>…</td>
    <td>3. Execute value change</td>
    </tr>
    <tr>
    <td>…</td>
    <td>4. Queue GUI callback</td>
    </tr>
    <tr>
    <td>5. Execute GUI callback</td>
    <td>…</td>
    </tr>
    </tbody>
    </table>
    <p>This design causes latency in most GUI updates related to audio
    parameters. It might seem like a big cost, given the importance of
    GUI responsiveness, but in practice, it’s barely noticeable.</p>
    <h3 id="taking-stock">Taking stock</h3>
    <p>So far, we know that an endpoint must store the following:</p>
    <ul>
    <li>a pointer to its target value</li>
    <li>the identity of the owner thread (either main or DSP)</li>
    <li>a DSP callback, if applicable</li>
    <li>a GUI callback, if applicable</li>
    <li>data to be used as arguments to the callbacks, if required</li>
    </ul>
    <h3 id="what-about-undoredo">What about undo/redo?</h3>
    <p>Ah yes. Now we temporarily banish concurrency and unswirl the
    paint so we can revisit the original subject.</p>
    <p>As discussed in the <a href="https://charlie-volow.com/blog/posts/ctrl_z.html">previous
    post</a>, an undoable event requires instructions on how to execute
    both an undo and redo of that event. For some such events, these
    instructions are unique; in other words, new undo and redo functions
    must be defined for that particular type of event.</p>
    <p>But for a large new class of events – namely, endpoint writes –
    there is enough symmetry that we can handle the undo logic
    abstractly. <strong>This is possible because the individual
    variations in behavior are already captured in the endpoint
    callbacks.</strong></p>
    <p>At the most basic level, it is quite easy to understand how this
    will work: anytime we do an <code>endpoint_write</code>, we will
    push a <code>UserEvent</code> that stores both the old and new value
    of the target. When an undo occurs, a new
    <code>endpoint_write</code> is triggered to reset the endpoint to
    the old value, and a redo will do the same, but for the new
    value.</p>
    <p>We don’t want undo/redo operations to push new
    <code>UserEvent</code>s themselves (we are not using the <a href="https://charlie-volow.com/blog/posts/ctrl_z.html#aside-emacs">emacs
    model</a>), and there are some other scenarios where an
    <code>endpoint_write</code> should <em>not</em> be undoable, so we
    add a boolean <code>undoable</code> parameter to
    <code>endpoint_write</code> that specifies whether or not a
    <code>UserEvent</code> should be pushed. The calls to
    <code>endpoint_write</code> that occur inside the undo/redo
    functions will always pass <code>false</code> for that
    parameter.</p>
    <p>Many parameter changes, like clicking and dragging a slider, are
    <em>continuous changes</em>, where a single “event” encompasses many
    per-frame endpoint writes. In those cases, a <code>UserEvent</code>
    should only be registered when the continuous change has completed.
    Therefore, in addition to <code>endpoint_write</code>, we add
    <code>endpoint_start_continuous_change</code> and
    <code>endpoint_stop_continuous_change</code> functions to the
    endpoint interface. When a continuous change starts, the current
    value of the endpoint is cached; the writes that occur during the
    continuous change (e.g. with the dragging of the slider) are not
    considered undoable; and when the continuous change <em>stops</em>
    (when the mouse button is released), a final write occurs that
    <em>is</em> undoable, and uses the cached value as the undo
    value.</p>
    <h3 id="value-ranges-and-automation">Value ranges and
    automation</h3>
    <p>Adding minimum, maximum, and default values to endpoints allows
    us to create sliders and automation tracks more or less directly
    from endpoints. In my original automation design, details like the
    range, default value, and a pointer to the target value were tied to
    an <code>AutomationType</code>, which could be one of
    <code>AUTO_VOL</code>, <code>AUTO_PAN</code>,
    <code>AUTO_FIR_FILTER_CUTOFF</code>, etc. If I wanted to add support
    for a new type of automation, I would need to expand the automation
    feature to accommodate it. Automations also had their own logic to
    account for concurrency.</p>
    <p>Now, when the user goes to add a new automation, the program
    simply displays a list of endpoints associated with the given track,
    and can create an automation from any of them. In this new design,
    that list of available automations is also dynamic; if a new effect
    is added to a track, new endpoints for each of its parameters are
    registered to the track, and those parameters will appear in the
    list of available automations. If the effect is deleted, they will
    disappear from the list.</p>
    <h3 id="the-jackdaw-api">The Jackdaw API</h3>
    <p>The immediate impetus to create endpoints was to solve problems
    in the internal program design, but recall that the prototypical
    concept was motivated by an interest in allowing Jackdaw to receive
    inputs from other programs. I knew during the build that once I had
    endpoints working internally, it would not be too difficult to
    expose them to the outside world.</p>
    <p>All I needed to add was some kind of taxonomic data structure for
    the endpoints (which would mirror a tree inherent in the program
    design), a hash table, and a UDP server.</p>
    <p><img src="https://charlie-volow.com/assets/blog_imgs/jackdaw_api_tree.jpg" width="100%"/></p><div><p>
    (Yes, you can have <a href="https://github.com/chvolow24/jackdaw?tab=readme-ov-file#project-navigation--multiple-timelines">multiple
    timelines!</a>)
    </p></div>
    <p>Each endpoint has a “local id” (e.g. <code>&#34;vol&#34;</code>,
    <code>&#34;pan&#34;</code>), and each node in the tree
    (<code>APINode</code>) points to the related object name, like the
    track name. The tree lends itself naturally to the filepath-style
    route names,
    e.g. <code>/{Timeline name}/{Track name}/{Endpoint local id}</code>.
    It is these routes that are hashed when endpoints are inserted into
    a global hash table. If an object tied to an API node is renamed,
    the related routes are reconstructed.</p>
    <p>(I learned from friends subsequently that this route syntax is
    very similar to that used by OSC, but that’s a coincidence. I think
    it’s just the most natural way to do it.)</p>
    <p>The API request syntax is simply the endpoint route followed by a
    string representing the desired new value. E.g.:</p>
    <pre><code>/main/piano/vol 0.765</code></pre>
    <p>The user can start a UDP server on a port of their choice
    (function <code>Start API server</code>, which you can find with the
    <a href="https://github.com/chvolow24/jackdaw?tab=readme-ov-file#function-lookup">function
    lookup</a>). Incoming messages are parsed into a route and value.
    The route is hashed, and if a matching endpoint is found, the server
    thread calls <code>endpoint_write</code> to set the new value.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> That’s it!</p>
    <p>This setup can be used for IPC on a single machine, and since it
    uses a network protocol, it can also be used over the network.
    Here’s pure data, running an LFO on my macbook, sending UDP
    datagrams over the network to modify the track pan on an instance of
    Jackdaw running on my old thinkpad:</p>
    <p><img src="https://charlie-volow.com/assets/blog_imgs/jackdaw_api_network.gif" width="100%"/></p>
    <h3 id="complexity">Complexity</h3>
    <p>The endpoint build is a bit of a horror. There are many details
    I’ve elided in this post for brevity’s sake (believe it or not). I
    merged the feature several months ago, and in writing this, had some
    difficulty reading the code and remembering exactly how everything
    works. I should thoroughly document that code.</p>
    <p>On the other hand, I have been using endpoints extensively since
    implementing them, and writing this post is the <em>first</em> time
    I have had to reopen the black box. The endpoint abstraction
    <em>contains</em> a lot of complexity, and silently bears the burden
    of difficult problems related to concurrency so that I no longer
    have to.</p>
    
    <h3 id="is-this-oop">Is this OOP?</h3>
    <p>I don’t know. “Data with associated behavior” certainly
    <em>sounds</em> like OOP, but I don’t feel that using a language
    with classes would particularly behoove me here. It’s worth noting
    that, in my design, the endpoint value itself does not live inside
    the <code>Endpoint</code> struct; the <code>Endpoint</code> just
    points to it. The two live side-by-side in some parent struct:</p>
    <div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span>typedef</span> <span>struct</span> track <span>{</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span>...</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span>float</span> vol<span>;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span>float</span> pan<span>;</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span>...</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    Endpoint vol_ep<span>;</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    Endpoint pan_ep<span>;</span>    </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span>...</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span>}</span> Track<span>;</span></span></code></pre></div>
    <p>I am free to access the track volume directly on the track and
    modify it with normal floating point operations where appropriate
    (e.g. when loading a project file at start time), and only use the
    higher-level <code>Endpoint</code> operations when required. This
    also makes it easy to start working on a new feature with a simple
    design, adding higher-level structure later. For example, I’m
    implementing a compressor right now, and the attack and release
    times, ratio, and threshold are all just <code>float</code>s on a
    <code>Compressor</code> struct. I’ll add endpoints for each of these
    parameters later, and doing so will not require any redesign; the
    simple version of the build still exists and works, with the
    higher-level concept layered neatly on top of it.</p>
    <h3 id="is-my-clever-concurrency-model-actually-kind-of-dumb">Is my
    clever concurrency model actually kind of dumb?</h3>
    <p>It might be.</p>
    <p>It would take many, many hours of research for me to understand
    the performance profile of concurrency primitives like mutex locks.
    And even if I spent those hours, would I be able to account for all
    the variables? Context switches, cache coherence, syscall overhead,
    parallelism, probability of lock contention, …? Jackdaw is built to
    run on different platforms, and the relevance of each of these
    things varies between them. I therefore design guided by heuristics,
    like “the use of mutex locks in the real-time audio thread (the DSP
    thread) should be minimized.” Is that a good heuristic?</p>
    <p>Recall that one aspect of my design is the fact that the DSP
    thread contains only a single locking operation – on the shared
    queue of asynchronous endpoint value changes – where it might have
    contained many. At first glance, the single-lock design seems like
    it would be more performant than a hundred-lock design. But is
    it?</p>
    <p>If the cost of a lock is paid mainly when there is contention,
    then I might have a problem. Each lock in the hundred-lock design
    has a relatively low likelihood of contention, since access to the
    exact matching parameter would need to be requested concurrently on
    another thread for a conflict to occur; but the single lock has a
    high likelihood of contention, since every single DSP-thread-owned
    endpoint write operation must pass through it.</p>
    <p>But is it true that “the cost of a lock is paid mainly when there
    is contention?” That’s another heuristic. And if it is true, how
    would I compute the overall probability of contention in each of the
    two models? And are the meager drops of juice<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>
    worth all this squeezing? From a performance perspective, have I
    just built and blogged about a <a href="https://www.youtube.com/watch?v=viejY6UZ5Bk">juice
    loosener?</a></p>
    <p>I ended my last post with a question that I intended to answer,
    but this time, I don’t have an answer. It will be months before I
    reach a phase in development where it’s important to scrutinize
    designs like this one with an eye toward optimization. For now, I’m
    happy to have an abstraction that accomplishes all of its other
    objectives, which turn out to be more important anyway.</p>
    
	  </div></div>
  </body>
</html>
