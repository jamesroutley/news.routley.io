<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.phind.com/blog/introducing-phind-70b">Original</a>
    <h1>Phind-70B: Closing the code quality gap with GPT-4 Turbo while running 4x faster</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><div><div><div><h2>Introducing Phind-70B – closing the code quality gap with GPT-4 Turbo while running 4x faster</h2><h3>We&#39;re excited to announce<!-- --> <a href="https://www.phind.com/" target="_blank" rel="noreferrer">Phind-70B</a>, our largest and most performant model to date. Running at up to 80 tokens per second, Phind-70B gives high-quality answers for technical topics without making users make a cup of coffee while they wait. We think it offers the<!-- --> <span>best overall user experience for developers</span> <!-- -->amongst state-of-the-art models.<br/></h3><p>Phind-70B is based on the CodeLlama-70B model and is fine-tuned on an additional 50 billion tokens, yielding significant improvements. It also supports a context window of<!-- --> <span>32K tokens</span>.</p><p>Phind-70B scores 82.3% on HumanEval,<!-- --> <span>beating the latest GPT-4 Turbo</span> <!-- -->(gpt-4-0125-preview) score of 81.1% in our evaluation. On Meta&#39;s CRUXEval dataset, Phind-70B scores 59% to GPT-4&#39;s reported score of 62% on the output prediction benchmark. However, neither of these public datasets fully captures how our users use Phind for real-world workloads. We find that Phind-70B is in the same quality realm as GPT-4 Turbo for code generation and exceeds it on some tasks. Phind-70B is also<!-- --> <span>less “lazy”</span> <!-- -->than GPT-4 Turbo and doesn&#39;t hesistate to generate detailed code examples.</p><p><img alt="Phind 70B HumanEval" loading="lazy" width="1200" height="777" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=3840&amp;q=75 3840w" src="https://www.phind.com/_next/image?url=%2Fimages%2Fphind-70b-humaneval.png&amp;w=3840&amp;q=75"/><img alt="Phind 70B CRUXEval" loading="lazy" width="1200" height="777" decoding="async" data-nimg="1" sizes="100vw" srcset="/_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=3840&amp;q=75 3840w" src="https://www.phind.com/_next/image?url=%2Fimages%2Fphind-70b-cruxeval.png&amp;w=3840&amp;q=75"/></p><p>Phind-70B is significantly faster than GPT-4 Turbo, running at<!-- --> <span>80+ tokens per second</span> <!-- -->to GPT-4 Turbo&#39;s ~20 tokens per second. We&#39;re able to achieve this by running NVIDIA&#39;s TensorRT-LLM library on H100 GPUs, and we&#39;re working on optimizations to further increase Phind-70B&#39;s inference speed.</p><p>Phind-70B is available today to<!-- --> <a href="https://www.phind.com/" target="_blank" rel="noreferrer">try for free and without a login</a>. You can get higher limits by subscribing to Phind Pro.</p><p>We love the open-source community and will be releasing the weights for the latest Phind-34B model in the coming weeks. We intend to release the weights for Phind-70B in time as well.</p><p>We&#39;d like to thank our cloud partners, SF Compute and AWS, for helping us get the infrastructure right for training and serving Phind-70B. We&#39;d also like to thank our partners at Meta and NVIDIA for their support.</p><p>Fun fact: We melted an H100 during Phind 70B&#39;s training!</p><p><a target="_blank" href="https://www.phind.com/">Try Phind-70B</a></p></div></div></div></div></div></div></div>
  </body>
</html>
