<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://vorpus.org/blog/why-does-calloc-exist/">Original</a>
    <h1>Why does calloc exist? (2016)</h1>
    
    <div id="readability-page-1" class="page"><div><p>[Edit: Welcome Hacker News readers! Before we dive into the neat
memory management esoterica, I want to briefly note that as engineers
we have an <a href="https://en.wikipedia.org/wiki/Engineering_ethics#Obligation_to_society">ethical obligation</a>
in our work to consider the <a href="http://www.ieee.org/about/corporate/governance/p7-8.html">&#34;safety, health, and welfare of the
public&#34;</a>,
because if we don&#39;t, <a href="https://en.wikipedia.org/wiki/Engineering_ethics#Case_studies_and_key_individuals">terrible things</a>
<a href="http://www.jewishvirtuallibrary.org/jsource/Holocaust/IBM.html">happen</a>. This
is a challenging responsibility that requires we all stay thoughtful
and informed â€“ but that&#39;s difficult if popular technical news
aggregators choose to <a href="https://news.ycombinator.com/item?id=13108404">censor links and discussions about the societal
implications of technology</a>. I sympathize with
their moderation challenges, but this idea of creating a politics-free
safe space is the cowards&#39; way out, quite literally choosing the
<a href="https://www.africa.upenn.edu/Articles_Gen/Letter_Birmingham.html">&#34;absence of tension&#34; over &#34;the presence of justice&#34;</a>. I
hope the HN moderators find a way to step up to the responsibility
their position entails; in the mean time, you might consider also
subscribing to to <a href="https://recompilermag.com/">The Recompiler</a> and
<a href="https://modelviewculture.com/">Model View Culture</a>, and checking
out <a href="https://www.safetypinbox.com/">Safety Pin Box</a>,
<a href="https://sfbay.techsolidarity.org/">techsolidarity.org</a>, or <a href="http://joinfundclub.com/">Fund
Club</a>. Anyway, thanks for listening! We
now return to our regularly scheduled <tt>calloc</tt>-related programming,
and I hope you enjoy my essay. And if you like this, you might also
enjoy <a href="https://lukasa.co.uk/2016/12/Debugging_Your_Operating_System/">Cory Benfield&#39;s related post</a>.]</p>
<hr/>
<p>When programming in C, there are two standard ways to allocate some
new memory on the heap:</p>
<div><pre><span></span><span>void</span><span>*</span> <span>buffer1</span> <span>=</span> <span>malloc</span><span>(</span><span>size</span><span>);</span>
<span>void</span><span>*</span> <span>buffer2</span> <span>=</span> <span>calloc</span><span>(</span><span>count</span><span>,</span> <span>size</span><span>);</span>
</pre></div>
<p><tt>malloc</tt> allocates an <em>uninitialized</em> array with the given number of
bytes, i.e., <tt>buffer1</tt> could contain anything. In terms of its
public API, <tt>calloc</tt> is different in two ways: first, it takes two
arguments instead of one, and second, it returns memory that is
pre-initialized to be all-zeros. So there are lots of books and
webpages out there that will claim that the <tt>calloc</tt> call above is
equivalent to calling <tt>malloc</tt> and then calling <tt>memset</tt> to fill
the memory with zeros:</p>
<div><pre><span></span><span>/* Equivalent to the calloc() call above -- OR IS IT?? */</span>
<span>void</span><span>*</span> <span>buffer3</span> <span>=</span> <span>malloc</span><span>(</span><span>count</span> <span>*</span> <span>size</span><span>);</span>
<span>memset</span><span>(</span><span>buffer3</span><span>,</span> <span>0</span><span>,</span> <span>count</span> <span>*</span> <span>size</span><span>);</span>
</pre></div>
<p>So... why does <tt>calloc</tt> exist, if it&#39;s equivalent to these 2 lines?
The C library is not known for its excessive focus on providing
convenient shorthands!</p>
<p>It turns out the answer is less widely known than I had realized! If I
were <a href="https://drawings.jvns.ca/">Julia Evans</a> at this point I&#39;d
make a neat little comic ðŸ˜Š. But I&#39;m not, so... here&#39;s a wall of text.</p>
<p>It turns out there are actually two differences between calling
<tt>calloc</tt>, versus calling <tt>malloc</tt> + <tt>memset</tt>.</p>
<div id="difference-1-computers-are-bad-at-arithmetic">
<h2>Difference #1: computers are bad at arithmetic</h2>
<p>When <tt>calloc</tt> multiplies <tt>count * size</tt>, it checks for overflow,
and errors out if the multiplication returns a value that can&#39;t fit
into a 32- or 64-bit integer (whichever one is relevant for your
platform). This is good. If you do the multiplication the naive way I
did it above by just writing <tt>count * size</tt>, then if the values are
too large then the multiplication will silently wrap around, and
<tt>malloc</tt> will happily allocate a smaller buffer than we
expected. That&#39;s bad. &#34;<em>This</em> part of the code thought the buffer was
<em>this</em> long but <em>that</em> part of the code thought it was <em>that</em> long&#34; is
the beginning of, like, eleventy-billion security advisories every
year. (<a href="http://undeadly.org/cgi?action=article&amp;sid=20060330071917">Example</a>)</p>
<p>I wrote a little program to demonstrate. It tries to allocate an
buffer containing <span>2<sup>63</sup>â€…Ã—â€…2<sup>63</sup>â€…=â€…2<sup>126</sup></span> bytes, first using
<tt>malloc</tt> and then using <tt>calloc</tt>:</p>

        <!-- silly hard-coded styles from njs_code_include, will need changing
             if theme changes -->
        
        <div><pre><span></span><span> 6</span><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span>
<span> 7</span><span>{</span>
<span> 8</span>    <span>size_t</span> <span>huge</span> <span>=</span> <span>INTPTR_MAX</span><span>;</span>
<span> 9</span>
<span>10</span>    <span>void</span><span>*</span> <span>buf</span> <span>=</span> <span>malloc</span><span>(</span><span>huge</span> <span>*</span> <span>huge</span><span>);</span>
<span>11</span>    <span>if</span> <span>(</span><span>!</span><span>buf</span><span>)</span> <span>perror</span><span>(</span><span>&#34;malloc failed&#34;</span><span>);</span>
<span>12</span>    <span>printf</span><span>(</span><span>&#34;malloc(huge * huge) returned: %p</span><span>\n</span><span>&#34;</span><span>,</span> <span>buf</span><span>);</span>
<span>13</span>    <span>free</span><span>(</span><span>buf</span><span>);</span>
<span>14</span>
<span>15</span>    <span>buf</span> <span>=</span> <span>calloc</span><span>(</span><span>huge</span><span>,</span> <span>huge</span><span>);</span>
<span>16</span>    <span>if</span> <span>(</span><span>!</span><span>buf</span><span>)</span> <span>perror</span><span>(</span><span>&#34;calloc failed&#34;</span><span>);</span>
<span>17</span>    <span>printf</span><span>(</span><span>&#34;calloc(huge, huge) returned: %p</span><span>\n</span><span>&#34;</span><span>,</span> <span>buf</span><span>);</span>
<span>18</span>    <span>free</span><span>(</span><span>buf</span><span>);</span>
<span>19</span><span>}</span>
</pre></div>
<p>On my computer, I get:</p>
<div><pre><span></span><span>~$ gcc calloc-overflow-demo.c -o calloc-overflow-demo</span>
<span>~$ ./calloc-overflow-demo</span>
<span>malloc(huge * huge) returned: 0x55c389d94010</span>
<span>calloc failed: Cannot allocate memory</span>
<span>calloc(huge, huge) returned: (nil)</span>
</pre></div>
<p>So yeah, apparently <tt>malloc</tt> successfully allocated a
73786976294838206464 exbiyte array? I&#39;m sure that will work out
well. This is a nice thing about <tt>calloc</tt>: it helps avoid terrible
security flaws.</p>
<p>But, it&#39;s not <em>that</em> exciting. (I mean, let&#39;s be honest: if we really
cared about security we wouldn&#39;t be writing in C.) It only helps in
the particular case where you&#39;re deciding how much memory to allocate
by multiplying two numbers together. This happens, it&#39;s an important
case, but there are lots of other cases where we either aren&#39;t doing
any arithmetic at all, or where we&#39;re doing some more complex
arithmetic and need a more general solution. Plus, if we wanted to, we
could certainly write our own wrapper for <tt>malloc</tt> that took two
arguments and multiplied them together with overflow checking. And in
fact if we want an overflow-safe version of <tt>realloc</tt>, or if we
don&#39;t want the memory to be zero-initialized, then... we still have to
do that. So, it&#39;s... nice?  But it doesn&#39;t really justify
<tt>calloc</tt>&#39;s existence.</p>
<p>The other difference, though? Is super, super important.</p>
</div>
<div id="difference-2-lies-damned-lies-and-virtual-memory">
<h2>Difference #2: lies, damned lies, and virtual memory</h2>
<p>Here&#39;s <a href="https://vorpus.org/blog/why-does-calloc-exist/calloc-1GiB-demo.c">a little benchmark program</a>
that measures how long it takes to <tt>calloc</tt> a 1 gibibyte buffer
versus <tt>malloc+memset</tt> a 1 gibibyte buffer. (Make sure you compile
without optimization, because modern compilers are clever enough to
know that <tt><span>free(calloc(...))</span></tt> is a no-op and optimize it out!) On my
laptop I get:</p>
<div><pre><span></span><span>~$ gcc calloc-1GiB-demo.c -o calloc-1GiB-demo</span>
<span>~$ ./calloc-1GiB-demo</span>
<span>calloc+free 1 GiB: 3.44 ms</span>
<span>malloc+memset+free 1 GiB: 365.00 ms</span>
</pre></div>
<p>i.e., <tt>calloc</tt> is more than 100x faster. Our textbooks and manual
pages says they&#39;re equivalent. What the heck is going on?</p>
<p>The answer, of course, is that <tt>calloc</tt> is cheating.</p>
<p>For small allocations, <tt>calloc</tt> literally will just call
<tt>malloc+memset</tt>, so it&#39;ll be the same speed. But for larger
allocations, most memory allocators will for various reasons make a
special request to the operating system to fetch more memory just for
this allocation. (&#34;Small&#34; and &#34;large&#34; here are determined by some
heuristics inside your memory allocator; for glibc &#34;large&#34; is anything
&gt;128 KiB, <a href="https://www.gnu.org/software/libc/manual/html_node/Malloc-Tunable-Parameters.html">at least in its default configuration</a>).</p>
<p>When the operating system hands out memory to a process, it always
zeros it out first, because otherwise our process would be able to
peek at whatever detritus was left in that memory by the last process
to use it, which might include, like, crypto keys, or embarrassing
fanfiction. So that&#39;s the first way that <tt>calloc</tt> cheats: when you
call <tt>malloc</tt> to allocate a large buffer, then <em>probably</em> the memory
will come from the operating system and already be zeroed, so there&#39;s
no need to call <tt>memset</tt>. But you don&#39;t know that for sure! Memory
allocators are pretty inscrutable. So <em>you</em> have to call <tt>memset</tt>
every time just in case. But <tt>calloc</tt> lives inside the memory
allocator, so <em>it</em> knows whether the memory it&#39;s returning is fresh
from the operating system, and if it is then it skips calling
<tt>memset</tt>. And this is why <tt>calloc</tt> has to be built into the
standard library, and you can&#39;t efficiently fake it yourself as a
layer on top of <tt>malloc</tt>.</p>
<p>But this only explains part of the speedup: <tt>memset+malloc</tt> is
actually clearing the memory twice, and <tt>calloc</tt> is clearing it
once, so we might expect <tt>calloc</tt> to be 2x faster at
best. Instead... it&#39;s 100x faster. What the heck?</p>
<p>It turns out that the kernel is also cheating! When we ask it for 1
GiB of memory, it doesn&#39;t actually go out and find that much RAM and
write zeros to it and then hand it to our process. Instead, it fakes
it, using virtual memory: it takes a single 4 KiB <a href="https://drawings.jvns.ca/pagetable/">page</a> of memory that is already
full of zeros (which it keeps around for just this purpose), and maps
1 GiB / 4 KiB = 262144 <a href="https://drawings.jvns.ca/copyonwrite/">copy-on-write</a> copies of it into our
process&#39;s address space. So the first time we actually <em>write</em> to each
of those 262144 pages, then at that point the kernel has to go and
find a real page of RAM, write zeros to it, and then quickly swap it
in place of the &#34;virtual&#34; page that was there before. But this happens
lazily, on a page-by-page basis.</p>
<p>So in real life, the difference won&#39;t be as stark as it looks in our
benchmark up above â€“ part of the trick is that <tt>calloc</tt> is shifting
some of the cost of zero&#39;ing out pages until later, while
<tt>malloc+memset</tt> is paying the full price up front. BUT, at least we
aren&#39;t zero&#39;ing them out twice. And at least we aren&#39;t trashing the
cache hierarchy up front â€“ if we delay the zero&#39;ing until we were
going to write to the pages anyway, then that means both writes happen
at the same time, so we only have to pay one set of TLB / L2 cache /
etc. misses. And, most importantly, it&#39;s possible we might never get
around to writing to all of those pages at all, in which case
<tt>calloc</tt> + the kernel&#39;s sneaky trickery is a huge win!</p>
<p>Of course, the exact set of optimizations <tt>calloc</tt> uses will vary
depending on your environment. A neat trick that used to be popular
was that the kernel would go around and speculatively zero out pages
when the system was idle, so that they&#39;d be fresh and ready when
needed â€“ but this is <a href="http://lists.dragonflybsd.org/pipermail/commits/2016-August/624202.html">out of fashion</a>
on current systems. Tiny embedded systems without virtual memory
obviously won&#39;t use virtual memory trickery. But in general,
<tt>calloc</tt> is never worse than <tt>malloc+memset</tt>, and on mainstream
systems it can do much better.</p>
<p>One real life example is a recent <a href="https://github.com/kennethreitz/requests/issues/3729">bug in requests</a>, where doing
streaming downloads over HTTPS with a large receive block size was
chewing up 100% CPU. It turns out that the problem was that when the
user said they were willing to handle up to 100 MiB chunks at a time,
then requests passed that on <a href="https://github.com/pyca/pyopenssl/issues/577">to pyopenssl</a>, and then pyopenssl
used <tt>cffi.new</tt> to allocate a 100 MiB buffer to hold the incoming
data. But most of the time, there wasn&#39;t actually 100 MiB ready to
read on the connection; so pyopenssl would allocate this large buffer,
but then would only use a small part of it. Except... it turns out
that <tt>cffi.new</tt> <a href="https://bitbucket.org/cffi/cffi/issues/295/cffinew-is-way-slower-than-it-should-be-it">emulates calloc by doing malloc+memset</a>,
so they were paying to allocate and zero the whole buffer anyway. If
<tt>cffi.new</tt> had used <tt>calloc</tt> instead, then the bug never would
have happened! Hopefully they&#39;ll fix that soon.</p>
<p>Or here&#39;s another example that comes up in <a href="http://www.numpy.org/">numpy</a>: suppose you want to make a big <a href="https://en.wikipedia.org/wiki/Identity_matrix">identity
matrix</a>, one with
16384 rows and 16384 columns. That requires allocating a buffer to
hold 16384 * 16384 floating point numbers, and each float is 8 bytes,
so that comes to 2 GiB of memory total.</p>
<p>Before we create the matrix, our process is using 24 MiB of memory:</p>
<div><pre><span></span><span>&gt;&gt;&gt; </span><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>&gt;&gt;&gt; </span><span>import</span> <span>resource</span>
<span>&gt;&gt;&gt; </span><span># this way of fetching memory usage probably only works right on Linux:</span>
<span>&gt;&gt;&gt; </span><span>def</span> <span>mebibytes_used</span><span>():</span>
<span>... </span>    <span>return</span> <span>resource</span><span>.</span><span>getrusage</span><span>(</span><span>resource</span><span>.</span><span>RUSAGE_SELF</span><span>)</span><span>.</span><span>ru_maxrss</span> <span>/</span> <span>1024</span>
<span>...</span>
<span>&gt;&gt;&gt; </span><span>mebibytes_used</span><span>()</span>
<span>24.35546875</span>
</pre></div>
<p>Then we allocate a 2 GiB dense matrix:</p>
<div><pre><span></span><span>&gt;&gt;&gt; </span><span>big_identity_matrix</span> <span>=</span> <span>np</span><span>.</span><span>eye</span><span>(</span><span>16384</span><span>)</span>
<span>&gt;&gt;&gt; </span><span>big_identity_matrix</span>
<span>array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],</span>
<span>       [ 0.,  1.,  0., ...,  0.,  0.,  0.],</span>
<span>       [ 0.,  0.,  1., ...,  0.,  0.,  0.],</span>
<span>       ...,</span>
<span>       [ 0.,  0.,  0., ...,  1.,  0.,  0.],</span>
<span>       [ 0.,  0.,  0., ...,  0.,  1.,  0.],</span>
<span>       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])</span>
<span>&gt;&gt;&gt; </span><span>big_identity_matrix</span><span>.</span><span>shape</span>
<span>(16384, 16384)</span>
</pre></div>
<p>How much memory is our process using now? The Answer May Surprise You
(Learn This One Weird Trick To Slim Down Your Processes Now):</p>
<div><pre><span></span><span>&gt;&gt;&gt; </span><span>mebibytes_used</span><span>()</span>
<span>88.3515625</span>
</pre></div>
<p>Numpy allocated the array using <tt>calloc</tt>, and then it wrote 1s in
the diagonal... but most of the array is still zeros, so it isn&#39;t
actually taking up any memory, and our 2 GiB matrix fits into ~60 MiB
of actual RAM. Of course there are other ways to accomplish the same
thing, like using a real <a href="https://en.wikipedia.org/wiki/Sparse_matrix">sparse matrix library</a>, but that&#39;s not the
point. The point is that <em>if</em> you do something like this, <tt>calloc</tt>
will magically make everything more efficient â€“ and it&#39;s always at
least as fast as the alternative.</p>
<p>So basically, <tt>calloc</tt> exists because it lets the memory allocator
and kernel engage in a sneaky conspiracy to make your code faster and
use less memory. You should let it! Don&#39;t use <tt>malloc+memset</tt>!</p>
<hr/>
<p><em>Changes history:</em></p>
<ul>
<li>2016-12-05 14:00 PST: Fix typo: HTTP where I meant HTTPS.</li>
<li>2016-12-05 17:00 PST: Add the HN note.</li>
<li>2016-12-07 01:45 PST: Several changes:<ul>
<li><a href="https://www.reddit.com/r/C_Programming/comments/5grwep/why_does_calloc_exist/daur0r9/">Use better error checking style</a>
in <tt><span>calloc-overflow-demo.c</span></tt></li>
<li>Clarify that I&#39;m not saying you <em>can&#39;t</em> reimplement <tt>calloc</tt>
yourself and get good performance, just that if you want to do
that you have to reimplement <tt>malloc</tt> too â€“ the point is that
<tt>calloc</tt> might look like it can be implemented in terms of
<tt>malloc</tt>, but this is misleading.</li>
<li>Add a paragraph noting more explicitly that <tt>calloc</tt>
optimizations vary across systems.</li>
</ul>
</li>
</ul>
</div>
</div></div>
  </body>
</html>
