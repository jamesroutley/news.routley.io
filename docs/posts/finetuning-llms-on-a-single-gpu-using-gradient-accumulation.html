<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lightning.ai/pages/blog/gradient-accumulation/">Original</a>
    <h1>Finetuning LLMs on a Single GPU Using Gradient Accumulation</h1>
    
    <div id="readability-page-1" class="page"><div><article id="post-5647630" role="article" itemscope="" itemtype="http://schema.org/BlogPosting"><header><canvas id="gradient-canvas"></canvas></header><section itemprop="text"><div><div><div><h3>Key takeaway</h3><p> Learn how to use gradient accumulation to train models with large batch sizes in order to work around hardware limitations when GPU memory is a concern.</p></div><p>Previously, <a href="https://sebastianraschka.com/blog/2023/pytorch-faster.html">I shared an article using multi-GPU training strategies to speed up the finetuning of large language models</a>. Several of these strategies include mechanisms such as model or tensor sharding that distributes the model weights and computations across different devices to work around GPU memory limitations.</p><p>However, many of us don’t have access to multi-GPU resources. This article therefore demonstrates a great workaround to train models with larger batch sizes when GPU memory is a concern: gradient accumulation.</p><h2><strong>Let’s Finetune BLOOM for Classification</strong></h2><p>Let’s suppose we are interested in adopting a recent pretrained large language model for a downstream task such as text classification. We are going to work with <a href="https://arxiv.org/abs/2211.05100">BLOOM</a>, which is an open-source alternative to GPT-3. In particular, we are going to use a version of BLOOM that “only” has 560 million parameters — it should fit into the RAM of conventional GPUs without problems (for reference, the free tier of Google Colab has a GPU with 15 Gb of RAM.)</p><p>Once we start, however, we bump into problems: our memory explodes during training or finetuning; we find that the only way to train this model is using a batch size of 1.</p><p><img decoding="async" src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/bloom-image-1.png" alt="" width="1650" height="1100" srcset="https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/bloom-image-1.png 1650w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/bloom-image-1-300x200.png 300w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/bloom-image-1-1024x683.png 1024w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/bloom-image-1-1536x1024.png 1536w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/bloom-image-1-300x200@2x.png 600w" sizes="(max-width: 1650px) 100vw, 1650px"/></p><pre><code>

# pip install torch lightning matplotlib pandas torchmetrics watermark transformers datasets -U

import os</code></pre><p>I am using <a href="https://lightning.ai/fabric">Lightning Fabric</a> because it allows me to flexibly change the number of GPUs and multi-GPU training strategy when running this code on different hardware. It also lets me enable mixed-precision training by only adjusting the precision flag. In this case, mixed-precision training can triple the training speed and reduce memory requirements by roughly 25%.</p><p>The main code shown above is executed in the <code>if __name__ == &#34;__main__&#34;</code> context, which is recommended when running Python scripts for multi-GPU training with PyTorch — even though we are only using a single GPU, it’s a best practice that we adopt. Then, the following three code sections within the <code>if __name__ == &#34;__main__&#34;</code>, take care of the data loading:</p><p><code># 1 Loading the Dataset</code></p><p><code># 2 Tokenization and Numericalization</code></p><p><code># 3 Setting Up DataLoaders</code></p><p>In section <code># 4 Initializing the Model</code>, we initialize the model. Then, in section <code># 5 Finetuning</code>, we call the train function, which is where things get interesting. In the <code>train(...)</code> function, we implement our standard PyTorch loop. An annotated version of the core training loop is shown below.</p><p><img decoding="async" loading="lazy" src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Untitled-5.png" alt="" width="1356" height="1256" srcset="https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Untitled-5.png 1356w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Untitled-5-300x278.png 300w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Untitled-5-1024x948.png 1024w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Untitled-5-300x278@2x.png 600w" sizes="(max-width: 1356px) 100vw, 1356px"/></p><p>The problem with batch sizes of 1 is that the gradient updates will be extremely noisy, as we can see based on the fluctuating training loss and poor test set performance below when we train the model:</p><pre><code>

...</code></pre><p>Since we don’t have multiple GPUs available for tensor sharding, what can we do to train the model with larger batch sizes?</p><p>One workaround is gradient accumulation, where we modify the aforementioned training loop.</p><div><h3>What is gradient accumulation?</h3><p> Gradient accumulation is a way to virtually increase the batch size during training, which is very useful when the available GPU memory is insufficient to accommodate the desired batch size. In gradient accumulation, gradients are computed for smaller batches and accumulated (usually summed or averaged) over multiple iterations instead of updating the model weights after every batch. Once the accumulated gradients reach the target “virtual” batch size, the model weights are updated with the accumulated gradients.</p><p>To illustrate this, consider the updated PyTorch training loop below. (<a href="https://github.com/rasbt/gradient-accumulation-blog/blob/main/src/2_batchsize-16.py">The full script is available here on GitHub.</a>)</p></div><p><img decoding="async" loading="lazy" src="https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Screenshot-2023-03-27-at-11.35.47-AM.png" alt="" width="1356" height="1320" srcset="https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Screenshot-2023-03-27-at-11.35.47-AM.png 1356w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Screenshot-2023-03-27-at-11.35.47-AM-300x292.png 300w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Screenshot-2023-03-27-at-11.35.47-AM-1024x997.png 1024w, https://lightningaidev.wpengine.com/wp-content/uploads/2023/03/Screenshot-2023-03-27-at-11.35.47-AM-300x292@2x.png 600w" sizes="(max-width: 1356px) 100vw, 1356px"/></p><p>If we set <code>accumulation_steps</code> to 2, then <code>zero_grad()</code> and <code>optimizer.step()</code> will only be called every second epoch. Consequently, running the modified training loop with <code>accumulation_steps=2</code> will have the same effect as doubling the batch size.</p><p>For example, if we want to use a batch size of 256 but can only fit a batch size of 64 into GPU memory, we can perform gradient accumulation over four batches of size 64. (After processing all four batches, we will have the accumulated gradients equivalent to a single batch of size 256.) This allows us to effectively emulate a larger batch size without requiring larger GPU memory or tensor sharding across different devices.</p><p>While gradient accumulation can help us train models with larger batch sizes, it does not reduce the total computation required. In fact, it can sometimes lead to a slightly slower training process, as the weight updates are performed less frequently. Nevertheless, it allows us to work around limitations where we have very small batch sizes that lead to noisy updates.</p><p>For example, let’s now run the code from above, where we have a batch size of 1, with 16 accumulation steps to simulate a batch size of 16. You can download the code here.</p><p>The output is as follows:</p><pre><code>

...</code></pre><p>As we can see, based on the results above, the loss fluctuates less than before. In addition, the test set performance increased by 10%! We are only iterating through the training set once, so each training example is only encountered a single time. Training the model for multiple epochs can further improve the predictive performance, but I’ll leave this as an exercise for you to try out (and let me know how it goes on <a href="https://discord.gg/tfXFetEZxv">Discord</a>!).</p><p>You may have also noticed that this code also executed faster than the code we used previously with a batch size of 1. If we increase the virtual batch size to 8 using gradient accumulation, we still have the same number of forward passes. However, since we only update the model every eighth epoch, we have fewer backward passes, which lets us iterate through the examples in a single epoch faster.</p><h2><strong>Conclusion</strong></h2><p>Gradient accumulation is a technique that simulates a larger batch size by accumulating gradients from multiple small batches before performing a weight update. This technique can be helpful in scenarios where the available memory is limited, and the batch size that can fit in memory is small.</p><p>However, consider a scenario in which you can run the batch size in the first place, meaning the available memory is large enough to accommodate the desired batch size. In that case, gradient accumulation may not be necessary. In fact, running a larger batch size can be more efficient because it allows for more parallelism and reduces the number of weight updates required to train the model.</p><p>In summary, gradient accumulation can be a useful technique for reducing the impact of noise in small batch sizes on the accuracy of gradient updates. It’s a simple yet effective technique that lets us work around hardware limitations.</p><p>For reference, all code accompanying this blog post is available <a href="https://github.com/rasbt/gradient-accumulation-blog/tree/main/src">here</a> on GitHub.</p></div></div></section>    </article></div></div>
  </body>
</html>
