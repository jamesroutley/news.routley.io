<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://fly.io/blog/wal-mode-in-litefs/">Original</a>
    <h1>WAL Mode in LiteFS</h1>
    
    <div id="readability-page-1" class="page"><article> <dl> <dt>Author</dt> <dd> <img src="https://fly.io/static/images/ben.jpg" alt="Ben Johnson" srcset=""/> <dl> <dt>Name</dt> <dd> Ben Johnson </dd> <dt>Twitter</dt> <dd> <a href="https://twitter.com/benbjohnson" target="_blank"> @benbjohnson </a> </dd> </dl> </dd> </dl> <section> <figure> <img src="https://fly.io/blog/2023-01-04/wal-mode-cover.jpg" alt=""/> <figcaption> <span>Image by</span> <svg role="img" style="pointer-events: none; width: 17px; height: 17px;" viewBox="0 0 20 20" fill="currentColor" fill-rule="evenodd"> <g buffered-rendering="static"> <path fill-rule="evenodd" d="M1 8a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 018.07 3h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0016.07 6H17a2 2 0 012 2v7a2 2 0 01-2 2H3a2 2 0 01-2-2V8zm13.5 3a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM10 14a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd"></path> </g> </svg> <a href="https://annieruygtillustration.com/" target="_blank"> Annie Ruygt </a> </figcaption> </figure> <p> <a href="https://fly.io/docs/litefs/">LiteFS</a> is a distributed file system that magically replicates your SQLite databases. Make an update on one server and, voil√†, your change is instantly available to your other servers on the edge. Take a look at our <a href="https://fly.io/docs/litefs/getting-started/"><em>Getting Started</em></a> guide to see how to add LiteFS to your application.</p><p>By and large, SQLite is configuration-free. You can get pretty far by just using the default settings. As your application grows and you start tweaking settings, one of the first knobs you&#39;ll come across is the <a href="https://www.sqlite.org/pragma.html#pragma_journal_mode">journal mode</a>. This setting determines how SQLite performs transactions on disk and there are essentially two modes: the rollback journal &amp; the write-ahead log, or WAL.</p> <p>The rollback journal was the original transaction mechanism and it&#39;s still the default. The WAL mode is the shiny new transaction mode. If you start reading blog posts &amp; forums about SQLite, one tip you will repeatedly hear is, <em>&#34;use WAL mode!&#34;</em></p> <p>If your database is slow, you should use the WAL mode.</p> <p>If you have concurrent users, you should use the WAL mode.</p> <p>WAL mode. WAL mode. WAL mode.</p> <p>In the SQLite world, the write-ahead log is as close to a <a href="https://en.wikipedia.org/wiki/No_Silver_Bullet">silver bullet</a> as you can find. It&#39;s basically magic fairy dust that makes your database better and you should <a href="https://sqlite.org/wal.html#overview">almost</a> always use it.</p> <p>However, <a href="https://github.com/superfly/litefs">LiteFS</a>, our distributed SQLite file system, only supported the rollback journal mode. Until now! With the release of <a href="https://github.com/superfly/litefs/releases/tag/v0.3.0">LiteFS v0.3.0</a>, we now support all journaling modes.</p> <h2 id="quick-primer-on-journal-modes"><a href="#quick-primer-on-journal-modes" aria-label="Anchor"></a>Quick Primer on Journal Modes</h2><p>We&#39;ve written about the internals of the <a href="https://fly.io/blog/sqlite-internals-rollback-journal/">rollback journal</a> and the <a href="https://fly.io/blog/sqlite-internals-wal/">WAL mode</a> in previous posts, but here&#39;s a refresher.</p> <p>With the rollback journal, SQLite:</p> <ul> <li>Writes new pages directly to the database file. </li><li>Copies the previous contents to a separate rollback journal file. </li><li>Deletes the rollback journal on commit. </li></ul> <p>Because the pages in the database file are moving around and being deleted, this mode does not allow read transactions &amp; write transactions to occur at the same time.</p> <p>The WAL works the opposite way:</p> <ul> <li>New pages are written to a separate write-ahead log file. </li><li>The last page written has a &#34;commit&#34; flag to indicate the end of the transaction. </li></ul> <p>Since the original data is never changed during the transaction, readers can continue running in parallel while another process is writing to the database. In addition to improved concurrency, the WAL also tends to have better write performance.</p> <h2 id="databases-as-a-history-of-change-sets"><a href="#databases-as-a-history-of-change-sets" aria-label="Anchor"></a>Databases as a History of Change Sets</h2><p>Most developers think of databases as just a collection of tables &amp; rows. And that&#39;s how you should view it when you&#39;re building an application. However, when designing database tooling like LiteFS, it&#39;s better to think in terms of change sets.</p> <p>A good analogy is baseball card collections. You might start off buying a pack of cards to start your collection. Over time, you may buy more packs or you might trade cards with friends. Each of these actions is a &#34;change set&#34;, adding and/or removing a set of cards from your collection.</p> <p>Eventually, word gets out about your sweet baseball card collection and your friends want to have the same set. So each time you make a change, you send each friend a list of which cards were added and removed so they can update their collections. Now everyone has the same collection just by communicating change sets.</p> <p>That, in a nutshell, is how LiteFS nodes keep distributed copies of your database in sync. However, instead of baseball cards, these LiteFS nodes communicate change sets of fixed-sized blocks called <em>pages</em>.</p> <p>SQLite applies these change sets of pages safely &amp; atomically by using either a rollback journal or the write-ahead log. These two methods have a different approach but, at the end of they day, they both transactionally update a set of pages in a SQLite database.</p> <p>In LiteFS, we track the beginning and end of these transactions through the file system API. We can see which pages have changed and bundle them up in an internal file format called <a href="https://github.com/superfly/ltx">LTX</a>.</p> <h3 id="detecting-page-sets-with-the-rollback-journal"><a href="#detecting-page-sets-with-the-rollback-journal" aria-label="Anchor"></a>Detecting Page Sets With the Rollback Journal</h3><p>The rollback journal is a simple mechanism, which makes it easy for LiteFS to determine when write transactions start &amp; end. From a high-level, SQLite implements transactions like this:</p> <ol> <li>Obtain an exclusive lock on the <code>SHARED</code> &amp; <code>RESERVED</code> lock bytes. </li><li>Create a <code>-journal</code> file. </li><li>Write changes to the database file with <code>write(2)</code> &amp; copy old versions of pages to the journal. </li><li><code>fsync(2)</code> the database file &amp; <code>unlink(2)</code> the journal file. </li><li>Release the locks. </li></ol> <p>LiteFS acts as a passthrough file system so it can see all these file system calls. On the initial journal creation, it begins watching for page changes. On <code>write(2)</code>, it marks a page as changed. And finally, on <code>unlink(2)</code> it will copy the page change set to an LTX file and then delete the journal.</p> <h3 id="detecting-page-sets-with-the-wal"><a href="#detecting-page-sets-with-the-wal" aria-label="Anchor"></a>Detecting Page Sets With the WAL</h3><p>SQLite&#39;s operations when it uses the WAL mode are a bit more complicated but it still has similar start &amp; end triggers.</p> <ol> <li>Obtain the <code>SHARED</code> lock byte in the database file but also obtain WAL-specific locks such as <code>WAL_WRITE_LOCK</code>. </li><li>Write new pages to the end of the WAL using <code>write(2)</code>. </li><li>On the last page write, the <code>commit</code> field is set in the WAL frame header. This indicates the end of the transaction and also the ending size of the database. </li><li>Release locks. </li></ol> <p>LiteFS can read the list of changed pages from the WAL and copy them out to an LTX file when the final WAL write for the transaction comes in. Again, both the rollback journal and WAL are implementation details so we end up with the same LTX format with either one.</p> <p>In the WAL mode, SQLite will also maintain a shared-memory file (aka SHM) and uses it as an index to look up pages in the WAL. This piece is managed by SQLite so LiteFS doesn&#39;t touch it during a write.</p> <h3 id="applying-transactions-to-the-replica"><a href="#applying-transactions-to-the-replica" aria-label="Anchor"></a>Applying Transactions to the Replica</h3><p>Once an LTX file is created on the primary LiteFS node, it will send it to all connected replica LiteFS nodes. These replicas will validate the file, perform some consistency checks, and then apply the change set to the SQLite database.</p> <p>The LiteFS replica imitates a SQLite client and takes the same locks in order to apply the transaction. That means it looks like just another SQLite client doing an update so it&#39;s safe across other processes using the database.</p> <h2 id="bootstrapping-made-easy"><a href="#bootstrapping-made-easy" aria-label="Anchor"></a>Bootstrapping Made Easy</h2><p>Previously, it was tough to convert an existing application to use LiteFS. You&#39;d need to create a SQL dump of your database and import in using the <code>sqlite3</code> command line. That was a pain.</p> <p>We&#39;ve improved this workflow with the new <a href="https://fly.io/docs/litefs/import/"><code>litefs import</code></a> command. This command lets you remotely send a SQLite database to your LiteFS cluster and it will transactionally replace it. That means you can start a cluster with an existing database or you can even revert to an old snapshot on a live application.</p> <div><pre><code><span>$ </span>litefs import <span>-name</span> my.db /path/to/my.db
</code></pre></div><h2 id="reworking-checksumming"><a href="#reworking-checksumming" aria-label="Anchor"></a>Reworking Checksumming</h2><p>LiteFS uses a fast, incremental checksum for ensuring the state of the entire database is consistent across all nodes at every transaction. The method is simple: we XOR the <a href="https://en.wikipedia.org/wiki/Cyclic_redundancy_check">CRC64</a> checksums of every page in the database together. This approach let us incrementally update individual pages by XOR&#39;ing out the old checksum for a page and XOR&#39;ing in the new checksum for the page. That&#39;s pretty cool.</p> <p>However, in practice, it was difficult to ensure we were calculating the correct previous checksum for a page every time we performed an update as page data is spread across the database file, journal file, &amp; WAL file. The edge cases for determining the previous page data were too easy to get wrong.</p> <p>So in v0.3.0, we decided to rework the database checksum. It still uses the same algorithm of XOR&#39;ing page checksums but now we maintain a map of the current checksum of every page in the database so they can be XOR&#39;d together on commit. We no longer need to track the previous checksum and this change made a lot of edge cases disappear.</p> <p>This approach is not without its trade-offs though. First, it requires additional memory. The map keys are 4-byte unsigned integers and the values are 8-byte hash values so we need about 12 bytes per page. SQLite uses 4KB pages by default so that&#39;s 262,144 pages per gigabyte. Our total memory overhead for our map of page hashes ends up being about 3MB of RAM per gigabyte of on-disk SQLite database data. LiteFS targets database sizes between 1 to 10 GB so that seemed like a reasonable trade-off.</p> <p>Second, this approach adds CPU overhead after each commit. Map iteration and XOR computation are quite fast but these do begin to show up in performance profiles as the database grows. In our tests, we&#39;ve found it adds about 5ms per gigabyte of SQLite data. That&#39;s pretty high. Fortunately, much of this iteration can be cached since XORs are associative. We&#39;ll be implementing this cache in the next version of LiteFS.</p> <h2 id="improving-debugging-with-the-trace-log"><a href="#improving-debugging-with-the-trace-log" aria-label="Anchor"></a>Improving Debugging With the Trace Log</h2><p>One benefit to having checksum bugs in v0.2.0 was that it gave us plenty of time to get our hands dirty with debugging. The best tools come out of necessity and the LiteFS trace log is one of those tools.</p> <p>Debugging a failed database or distributed system is <a href="https://twitter.com/honest_update/status/651897353889259520">a bit like a murder mystery</a> in that you know how it ended but you need to put the pieces together to figure out how it happened.</p> <p>In the previous version of LiteFS, we didn&#39;t have many clues when one of these failures happened so it required a Sherlock Holmes level of deductive reasoning to figure out the mystery. The trace log simplifies this process by writing out every internal event to a log file so we can see where things went awry after the fact.</p> <p>SQLite uses the POSIX file system API so debugging with a normal <code>strace</code> would look like a series of seemingly opaque system calls. LiteFS translates these system calls back into SQLite related actions such as <code>WriteDatabase()</code> or <code>LockSHM()</code>. When we write those events to the trace log, we can decorate the log lines with additional information such as page numbers and checksums. All this makes reading the trace much more straightforward.</p> <p>The trace log is not without its costs though. It will increase I/O to your disk as there are a lot of events that are written. It&#39;s typical to see your disk I/O double when you enable the trace log. However, it does cap the total size of the trace log by using a rolling log so you don&#39;t need much space available. By default, it will roll over to a new log file every 64MB and it will retain the last 10 logs in a gzipped format.</p> <p>The trace log is disabled by default, however, you review the <a href="https://fly.io/docs/litefs/config/#trace-log">trace log documentation</a> if you need it to debug any LiteFS issues.</p> <h2 id="upcoming-work"><a href="#upcoming-work" aria-label="Anchor"></a>Upcoming Work</h2><p>The WAL support &amp; stability improvements have been huge steps in moving LiteFS to be production ready but there&#39;s still more work to come. In the next release, we&#39;ll be focused on making LiteFS easier to integrate into your application by adding support for <a href="https://github.com/superfly/litefs/issues/56"><em>write forwarding</em></a>. That will let you write to your database from any node and have LiteFS automatically forward those writes to the primary instead of having your application redirect writes.</p> <p>We&#39;ll also be making performance improvements by adding <a href="https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)">LZ4 compression</a> to the LTX files. This will reduce latency between nodes and it will significantly cut down on bandwidth costs.</p> <h2 id="thank-you"><a href="#thank-you" aria-label="Anchor"></a>Thank You!</h2><p>Finally, we&#39;d like to give a huge shoutout for everyone who has tried LiteFS and given feedback. It makes a world of difference! <a href="https://www.youtube.com/@KentCDodds-vids">Kent C. Dodds</a> even live streamed his experience with LiteFS and it gave us incredible, detailed feedback. Thank you!</p>  </section> <dl> <dt> Previous post¬†¬†‚Üì </dt> <dd> <a href="https://fly.io/blog/launching-redis-by-upstash/"> Launching Redis by Upstash </a> </dd> </dl> </article></div>
  </body>
</html>
