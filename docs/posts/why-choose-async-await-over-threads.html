<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://notgull.net/why-not-threads/">Original</a>
    <h1>Why choose async/await over threads?</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p>A common refrain is that threads can do everything that <code>async</code>/<code>await</code> can, but
simpler. So why would anyone choose <code>async</code>/<code>await</code>?</p>

<p>This is a common question that I’ve seen a lot in the Rust community. Frankly, I completely understand
where it’s coming from.</p>

<p>Rust is a low-level language that doesn’t hide the
complexity of coroutines from you. This is in opposition to languages like Go,
where <code>async</code> happens by default, without the programmer needing to even
consider it.</p>

<p>Smart programmers try to avoid complexity. So, they see the extra complexity in
<code>async</code>/<code>await</code> and question why it is needed. This question is especially
pertinent when considering that a reasonable alternative exists in OS threads.</p>

<p>Let’s take a mind-journey through <code>async</code> and see how it stacks up.</p>

<h2 id="background-blitz">Background Blitz</h2>

<p>Rust is a low-level language. Normally, code is linear; one thing runs after
another. It looks like this:</p>

<div><div><pre><code><span>fn</span> <span>main</span><span>()</span> <span>{</span>
    <span>foo</span><span>();</span>
    <span>bar</span><span>();</span>
    <span>baz</span><span>();</span>
<span>}</span>
</code></pre></div></div>

<p>Nice and simple, right?</p>

<p>However, sometimes you will want to run many things at once. The canonical
example for this is a web server. Consider the following written in linear
code:</p>

<div><div><pre><code><span>fn</span> <span>main</span><span>()</span> <span>-&gt;</span> <span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>()</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>socket</span> <span>=</span> <span>TcpListener</span><span>::</span><span>bind</span><span>(</span><span>&#34;0.0.0.0:80&#34;</span><span>)</span><span>?</span><span>;</span>

    <span>loop</span> <span>{</span>
        <span>let</span> <span>(</span><span>client</span><span>,</span> <span>_</span><span>)</span> <span>=</span> <span>socket</span><span>.accept</span><span>()</span><span>?</span><span>;</span>
        <span>handle_client</span><span>(</span><span>client</span><span>)</span><span>?</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Imagine if <code>handle_client</code> takes a few milliseconds, and two clients try to
connect to your webserver at the same time. You’ll run into a serious
problem!</p>

<ul>
  <li>Client #1 connects to the webserver, and is accepted by the <code>accept()</code>
function. It starts running <code>handle_client()</code>.</li>
  <li>Client #2 connects to the webserver. However, since <code>accept()</code> is not
currently running, we have to wait for <code>handle_client()</code> for Client #1 to
finish running.</li>
  <li>After waiting a few milliseconds, we get back to <code>accept()</code>. Client #2 can
connect.</li>
</ul>

<p>Now imagine that instead of two clients, there are two million simultaneous
clients. At the end of the queue, you’ll have to wait several minutes
before the web server can help you. It becomes un-scalable very quickly.</p>

<p>Obviously, the embryonic web tried to solve this problem. The original solution was to introduce threading. By saving
the value of some registers and the program’s stack into memory, the operating
system can stop a program, run another program in its place, then resume running
that program later. Essentially, it allows for multiple routines (or “threads”,
or “processes”) to run on the same CPU.</p>

<p>Using threads, we can rewrite the above code as follows:</p>

<div><div><pre><code><span>fn</span> <span>main</span><span>()</span> <span>-&gt;</span> <span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>()</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>socket</span> <span>=</span> <span>TcpListener</span><span>::</span><span>bind</span><span>(</span><span>&#34;0.0.0.0:80&#34;</span><span>)</span><span>?</span><span>;</span>

    <span>loop</span> <span>{</span>
        <span>let</span> <span>(</span><span>client</span><span>,</span> <span>_</span><span>)</span> <span>=</span> <span>socket</span><span>.accept</span><span>()</span><span>?</span><span>;</span>
        <span>thread</span><span>::</span><span>spawn</span><span>(</span><span>move</span> <span>||</span> <span>handle_client</span><span>(</span><span>client</span><span>));</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Now, the client is being handled by a separate thread than the one handling
waiting for new connections. Great! This avoids the problem by allowing concurrent
thread access.</p>

<ul>
  <li>Client #1 is <code>accept</code>ed by the server. The server spawns a thread that calls
<code>handle_client</code>.</li>
  <li>Client #2 tries to connect to the server.</li>
  <li>Eventually, <code>handle_client</code> blocks on something. The OS saves the thread
handling Client #1 and brings back the main thread.</li>
  <li>The main thread <code>accept</code>s Client #2. It spawns a separate thread to handle
Client #2. With only a few microseconds of delay, Client #1 and Client #2
are run in parallel.</li>
</ul>

<p>Threads work especially well when you consider that production-grade web servers
have dozens of CPU cores. It’s not just that the OS can give the <em>illusion</em> that
all of these threads run at the same time; it’s that the OS can <em>actually</em> make
them all run at once.</p>

<p>Eventually, for reasons I’ll elaborate later, programmers wanted to bring this
concurrency out of the OS space and into the user space. There are many
different models for userspace concurrency. There is event-driven programming,
actors, and coroutines. The one Rust settled on is <code>async</code>/<code>await</code>.</p>

<p>To oversimplify, you compile the program as a grab-bag of state machines that
can all be run independently of another. Rust itself provides a mechanism for
creating state machines; the mechanism of <code>async</code> and <code>await</code>. The above program in terms of <code>async</code>/<code>await</code> would
look like this, written using <a href="https://crates.io/crates/smol"><code>smol</code></a>:</p>

<div><div><pre><code><span>#[apply(smol_macros::main</span><span>!</span><span>)]</span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>ex</span><span>:</span> <span>&amp;</span><span>smol</span><span>::</span><span>Executor</span><span>)</span> <span>-&gt;</span> <span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>()</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>socket</span> <span>=</span> <span>TcpListener</span><span>::</span><span>bind</span><span>(</span><span>&#34;0.0.0.0:80&#34;</span><span>)</span><span>.await</span><span>?</span><span>;</span>

    <span>loop</span> <span>{</span>
        <span>let</span> <span>(</span><span>client</span><span>,</span> <span>_</span><span>)</span> <span>=</span> <span>socket</span><span>.accept</span><span>()</span><span>.await</span><span>?</span><span>;</span>
        <span>ex</span><span>.spawn</span><span>(</span><span>async</span> <span>move</span> <span>{</span>
            <span>handle_client</span><span>(</span><span>client</span><span>)</span><span>.await</span><span>;</span>
        <span>})</span><span>.detach</span><span>();</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<ul>
  <li>The main function is preceded with the <code>async</code> keyword. This means that it is
not a traditional function, but one that returns a state machine. Roughly, the
function’s contents correspond to that state machine.</li>
  <li><code>await</code> includes another state machine as a part of the currently running
state machine. For <code>accept()</code>, it means that the state machine will include it
as a step.</li>
  <li>Eventually, one of the inner functions will <em>yield</em>, or give up control. For
example, when <code>accept()</code> waits
for a new connection. At this point the entire state machine will yield its execution to
the higher-level executor. For us, that is <code>smol::Executor</code>.</li>
  <li>Once execution is yielded, the <code>Executor</code> will replace the current state
machine with another one that is running concurrently, spawned through the
<code>spawn</code> function.</li>
  <li>We pass an <code>async</code> block to the <code>spawn</code> function. This block represents an entire new state
machine, independent of the one created by the <code>main</code> function. All this state
machine does is run the <code>handle_client</code> function.</li>
  <li>Once <code>main</code> yields, one of the clients is selected to run in its place. Once that
client yields, the cycle repeats.</li>
  <li>You can now handle millions of simultaneous clients.</li>
</ul>

<p>Of course, user-space concurrency like this introduces an uptick in
complexity. When you’re using threads, you don’t have to deal with executors
and tasks and state machines and all.</p>

<p>If you’re a reasonable person, you might be asking “why do we need to do all of
this? Threads work well; for 99% of programs, we don’t need to involve any kind
of user-space concurrency. Introducing new complexity is technical debt, and
technical debt costs us time and money.</p>

<p>“So why wouldn’t we use threads?”</p>

<h2 id="timeout-trouble">Timeout Trouble</h2>

<p>Perhaps one of Rust’s biggest strengths is <em>composability</em>. It provides a set of
abstractions that can be nested, built upon, put together, and expanded upon.</p>

<p>I recall that <em>the</em> thing that made me stick with Rust is the <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html"><code>Iterator</code></a>
trait. It blew my mind that you could make something an <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html"><code>Iterator</code></a>, apply
a handful of different combinators, then pass the resulting <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html"><code>Iterator</code></a> into
any function that took an <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html"><code>Iterator</code></a>.</p>

<p>It continues to impress me how powerful it is. Let’s say you want to receive a list of
integers from another thread, only take the ones that are immediately available,
discard any integers that aren’t even, add one to all of them, then push them
onto a new list.</p>

<p>That would be fifty lines and a helper function in some other languages. In Rust
it can be done in five:</p>

<div><div><pre><code><span>let</span> <span>(</span><span>send</span><span>,</span> <span>recv</span><span>)</span> <span>=</span> <span>mpsc</span><span>::</span><span>channel</span><span>();</span>
<span>my_list</span><span>.extend</span><span>(</span>
    <span>recv</span><span>.try_iter</span><span>()</span>
        <span>.filter</span><span>(|</span><span>x</span><span>|</span> <span>x</span> <span>&amp;</span> <span>1</span> <span>==</span> <span>0</span><span>)</span>
        <span>.map</span><span>(|</span><span>x</span><span>|</span> <span>x</span> <span>+</span> <span>1</span><span>)</span>
<span>);</span>
</code></pre></div></div>

<p>The best thing about <code>async</code>/<code>await</code> is that it lets you apply this composability
to I/O-bound functions. Let’s say you have a new client requirement; you want to add a timeout to your above
function. Assume that our <code>handle_client</code> above function looks like this:</p>

<div><div><pre><code><span>async</span> <span>fn</span> <span>handle_client</span><span>(</span><span>client</span><span>:</span> <span>TcpStream</span><span>)</span> <span>-&gt;</span> <span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>()</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>mut</span> <span>data</span> <span>=</span> <span>vec!</span><span>[];</span>
    <span>client</span><span>.read_to_end</span><span>(</span><span>&amp;</span><span>mut</span> <span>data</span><span>)</span><span>.await</span><span>?</span><span>;</span>
    
    <span>let</span> <span>response</span> <span>=</span> <span>do_something_with_data</span><span>(</span><span>data</span><span>)</span><span>.await</span><span>?</span>
    <span>client</span><span>.write_all</span><span>(</span><span>&amp;</span><span>response</span><span>)</span><span>.await</span><span>?</span><span>;</span>

    <span>Ok</span><span>(())</span>
<span>}</span>
</code></pre></div></div>

<p>If we want to add, say, a three-second timeout, we can combine two combinators
to do that:</p>

<ul>
  <li>The <a href="https://docs.rs/smol/latest/smol/future/fn.race.html"><code>race</code></a> function takes two futures and runs them at the same time.</li>
  <li>The <a href="https://docs.rs/smol/latest/smol/struct.Timer.html"><code>Timer</code></a> future waits for some time before returning.</li>
</ul>

<p>Here is what the final code looks like:</p>

<div><div><pre><code><span>async</span> <span>fn</span> <span>handle_client</span><span>(</span><span>client</span><span>:</span> <span>TcpStream</span><span>)</span> <span>-&gt;</span> <span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>()</span><span>&gt;</span> <span>{</span>
    <span>// Future that handles the actual connection.</span>
    <span>let</span> <span>driver</span> <span>=</span> <span>async</span> <span>move</span> <span>{</span>
        <span>let</span> <span>mut</span> <span>data</span> <span>=</span> <span>vec!</span><span>[];</span>
        <span>client</span><span>.read_to_end</span><span>(</span><span>&amp;</span><span>mut</span> <span>data</span><span>)</span><span>.await</span><span>?</span><span>;</span>
        
        <span>let</span> <span>response</span> <span>=</span> <span>do_something_with_data</span><span>(</span><span>data</span><span>)</span><span>.await</span><span>?</span>
        <span>client</span><span>.write_all</span><span>(</span><span>&amp;</span><span>response</span><span>)</span><span>.await</span><span>?</span><span>;</span>

        <span>Ok</span><span>(())</span>
    <span>};</span>

    <span>// Future that handles waiting for a timeout.</span>
    <span>let</span> <span>timeout</span> <span>=</span> <span>async</span> <span>{</span>
        <span>Timer</span><span>::</span><span>after</span><span>(</span><span>Duration</span><span>::</span><span>from_secs</span><span>(</span><span>3</span><span>))</span><span>.await</span><span>;</span>

        <span>// We just hit a timeout! Return an error.</span>
        <span>Err</span><span>(</span><span>io</span><span>::</span><span>ErrorKind</span><span>::</span><span>TimedOut</span><span>.into</span><span>())</span>
    <span>};</span>

    <span>// Run both in parallel.</span>
    <span>driver</span><span>.race</span><span>(</span><span>timeout</span><span>)</span><span>.await</span>
<span>}</span>
</code></pre></div></div>

<p>I find this to be a very easy process. All you have to do is wrap your existing
code in an <code>async</code> block and race it against another future.</p>

<p>An added bonus of this approach is that it works with any kind of stream. Here,
we use a <code>TcpStream</code>. However we can easily replace it with anything that
implements <code>impl AsyncRead + AsyncWrite</code>. It could be a GZIP stream on top of
the normal stream, or a Unix socket, or a file. <code>async</code> just slides into
whatever pattern you need from it.</p>

<h2 id="thematic-threads">Thematic Threads</h2>

<p>What if we wanted to implement this in our threaded example above?</p>

<div><div><pre><code><span>fn</span> <span>handle_client</span><span>(</span><span>client</span><span>:</span> <span>TcpStream</span><span>)</span> <span>-&gt;</span> <span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>()</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>mut</span> <span>data</span> <span>=</span> <span>vec!</span><span>[];</span>
    <span>client</span><span>.read_to_end</span><span>(</span><span>&amp;</span><span>mut</span> <span>data</span><span>)</span><span>?</span><span>;</span>
    
    <span>let</span> <span>response</span> <span>=</span> <span>do_something_with_data</span><span>(</span><span>data</span><span>)</span><span>?</span>
    <span>client</span><span>.write_all</span><span>(</span><span>&amp;</span><span>response</span><span>)</span><span>?</span><span>;</span>

    <span>Ok</span><span>(())</span>
<span>}</span>
</code></pre></div></div>

<p>Well, it’s not easy. Generally, you can’t interrupt the <code>read</code> or <code>write</code>
system calls in blocking code, without doing something catastrophic like
closing the file descriptor (which can’t be done in Rust).</p>

<p>Thankfully, <a href="https://doc.rust-lang.org/std/net/struct.TcpStream.html"><code>TcpStream</code></a> has two functions <a href="https://doc.rust-lang.org/std/net/struct.TcpStream.html#method.set_read_timeout"><code>set_read_timeout</code></a> and
<a href="https://doc.rust-lang.org/std/net/struct.TcpStream.html#method.set_write_timeout"><code>set_write_timeout</code></a> that can be used to set the timeouts for reading and
writing, respectively. However, we can’t just use it naively. Imagine a client
that sends one byte every 2.9 seconds, just to reset the timeout.</p>

<p>So we have to program a little defensively here. Due to the power of Rust combinators, we can write our own
type wrapping around the <a href="https://doc.rust-lang.org/std/net/struct.TcpStream.html"><code>TcpStream</code></a> to program the timeout.</p>

<div><div><pre><code><span>// Deadline-aware wrapper around `TcpStream.</span>
<span>struct</span> <span>DeadlineStream</span> <span>{</span>
    <span>tcp</span><span>:</span> <span>TcpStream</span><span>,</span>
    <span>deadline</span><span>:</span> <span>Instant</span>
<span>}</span>

<span>impl</span> <span>DeadlineStream</span> <span>{</span>
    <span>/// Create a new `DeadlineStream` that expires after some time.</span>
    <span>fn</span> <span>new</span><span>(</span><span>tcp</span><span>:</span> <span>TcpStream</span><span>,</span> <span>timeout</span><span>:</span> <span>Duration</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span>
        <span>Self</span> <span>{</span>
            <span>tcp</span><span>,</span>
            <span>deadline</span><span>:</span> <span>Instant</span><span>::</span><span>now</span><span>()</span> <span>+</span> <span>timeout</span><span>,</span>
        <span>}</span>
    <span>}</span>
<span>}</span>

<span>impl</span> <span>io</span><span>::</span><span>Read</span> <span>for</span> <span>DeadlineStream</span> <span>{</span>
    <span>fn</span> <span>read</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> <span>buf</span><span>:</span> <span>&amp;</span><span>mut</span> <span>[</span><span>u8</span><span>])</span> <span>-&gt;</span> <span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>usize</span><span>&gt;</span> <span>{</span>
        <span>// Set the deadline.</span>
        <span>let</span> <span>time_left</span> <span>=</span> <span>self</span><span>.deadline</span><span>.saturating_duration_since</span><span>(</span><span>Instant</span><span>::</span><span>now</span><span>());</span>
        <span>self</span><span>.tcp</span><span>.set_read_timeout</span><span>(</span><span>Some</span><span>(</span><span>time_left</span><span>))</span><span>?</span><span>;</span>

        <span>// Read from the stream.</span>
        <span>self</span><span>.tcp</span><span>.read</span><span>(</span><span>buf</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>impl</span> <span>io</span><span>::</span><span>Write</span> <span>for</span> <span>DeadlineStream</span> <span>{</span>
    <span>fn</span> <span>write</span><span>(</span><span>&amp;</span><span>mut</span> <span>self</span><span>,</span> <span>buf</span><span>:</span> <span>&amp;</span><span>[</span><span>u8</span><span>])</span> <span>-&gt;</span> <span>io</span><span>::</span><span>Result</span><span>&lt;</span><span>usize</span><span>&gt;</span> <span>{</span>
        <span>// Set the deadline.</span>
        <span>let</span> <span>time_left</span> <span>=</span> <span>self</span><span>.deadline</span><span>.saturating_duration_since</span><span>(</span><span>Instant</span><span>::</span><span>now</span><span>());</span>
        <span>self</span><span>.tcp</span><span>.set_write_timeout</span><span>(</span><span>Some</span><span>(</span><span>time_left</span><span>))</span><span>?</span><span>;</span>

        <span>// Read from the stream.</span>
        <span>self</span><span>.tcp</span><span>.write</span><span>(</span><span>buf</span><span>)</span>
    <span>}</span>
<span>}</span>

<span>// Create the wrapper.</span>
<span>let</span> <span>client</span> <span>=</span> <span>DeadlineStream</span><span>::</span><span>new</span><span>(</span><span>client</span><span>,</span> <span>Duration</span><span>::</span><span>from_secs</span><span>(</span><span>3</span><span>));</span>

<span>let</span> <span>mut</span> <span>data</span> <span>=</span> <span>vec!</span><span>[];</span>
<span>client</span><span>.read_to_end</span><span>(</span><span>&amp;</span><span>mut</span> <span>data</span><span>)</span><span>?</span><span>;</span>

<span>let</span> <span>response</span> <span>=</span> <span>do_something_with_data</span><span>(</span><span>data</span><span>)</span><span>?</span>
<span>client</span><span>.write_all</span><span>(</span><span>&amp;</span><span>response</span><span>)</span><span>?</span><span>;</span>

<span>Ok</span><span>(())</span>
</code></pre></div></div>

<p>On one hand, it could be argued that this is elegant. We used Rust’s
capabilities to solve the problem with a relatively simple combinator. I’m sure
it would work well enough.</p>

<p>On the other hand, it’s definitely hacky.</p>

<ul>
  <li>We’ve locked ourselves into using <a href="https://doc.rust-lang.org/std/net/struct.TcpStream.html"><code>TcpStream</code></a>. There’s no trait in Rust to
abstract over using the <a href="https://doc.rust-lang.org/std/net/struct.TcpStream.html#method.set_read_timeout"><code>set_read_timeout</code></a> and <a href="https://doc.rust-lang.org/std/net/struct.TcpStream.html#method.set_write_timeout"><code>set_write_timeout</code></a> types.
So it would take a lot of additional work to make it use any kind of writer.</li>
  <li>It involves an extra system call for setting the timeout.</li>
  <li>I imagine this type is much more unwieldy to use for the kinds of actual logic
that web servers demand.</li>
</ul>

<p>If I saw this code in production, I would ask the author why they avoided using
<code>async</code>/<code>await</code> to solve this problem. This is the phenomenon I was describing
in my post “<a href="https://notgull.net/why-you-want-async/">Why you might actually want async in your project</a>”.
Quite frequently I encounter a pattern where synchronous code can’t be used
without contortion, so I have to rewrite it in <code>async</code>.</p>

<h2 id="async-success-stories">Async Success Stories</h2>

<p>There’s a reason why the HTTP ecosystem has adopted <code>async</code>/<code>await</code> as its
primary runtime mechanism, even for clients. You can take any function that
makes an HTTP call, and make it fit whatever hole or use case you want it to.</p>

<p><a href="https://crates.io/crates/tower"><code>tower</code></a> is probably the best example of this phenomenon I can think of, and
it’s really <em>the</em> thing that made me realize how powerful <code>async</code>/<code>await</code> can
be. If you implement your service as an <code>async</code> function, you get timeouts,
rate limiting, load balancing, <a href="https://docs.rs/tower/0.4.13/tower/hedge/index.html">hedging</a>
and back-pressure handling. All of that for free.</p>

<p>It doesn’t matter what runtime you used, or what you’re actually doing in your
service. You can throw <a href="https://crates.io/crates/tower"><code>tower</code></a> at it to make it more robust.</p>

<p><a href="https://docs.rs/macroquad"><code>macroquad</code></a> is a miniature Rust game engine that aims to make game development
as easy as possible. Its main function uses <code>async</code>/<code>await</code> in order to run its
engine. This is because <code>async</code>/<code>await</code> is really the best way in Rust to
express a linear function that needs to be stopped in order to wait for
something else.</p>

<p>In practice, this can be extremely powerful. Imagine simultaneously polling
a network connection to your game server and your GUI framework, on the same
thread. The possibilities are endless.</p>

<h2 id="improving-asyncs-image">Improving Async’s Image</h2>

<p>I don’t think the issue is that some people think threads are better than
<code>async</code>. I think the issue is that the benefits of <code>async</code> aren’t widely
broadcast. This leads some people to be misinformed about the benefits of
<code>async</code>.</p>

<p>If this is an educational problem, I think it’s worth taking a look at the
educational material. Here’s what the <a href="https://rust-lang.github.io/async-book/01_getting_started/02_why_async.html">Rust Async Book</a> says when comparing
<code>async</code>/<code>await</code> to operating system threads.</p>

<blockquote>
  <p><strong>OS threads</strong> don’t require any changes to the programming model, which makes it very easy to express concurrency. However, synchronizing between threads can be difficult, and the performance overhead is large. Thread pools can mitigate some of these costs, but not enough to support massive IO-bound workloads.</p>

  <p><em>- <a href="https://rust-lang.github.io/async-book/01_getting_started/02_why_async.html">Rust Async Book</a>, various authors</em></p>
</blockquote>

<p>I think this is a consistent problem throughout the <code>async</code> community. When
someone asks the question of “why do we want to use this over OS threads”,
people have a tendency to kind of wave their hand and say “<code>async</code> has less
overhead. Other than that, everything’s the same.”</p>

<p>This is the reason why web server authors switched to <code>async</code>/<code>await</code>. It’s how
they solved the <a href="https://en.wikipedia.org/wiki/C10k_problem">C10k problem</a>. But, it’s not going to be the reason why
everyone else switches to <code>async</code>/<code>await</code>.</p>

<p>Performance gains are
fickle and can disappear in the wrong circumstances. There are plenty of cases
where a threaded workflow can be faster than an equivalent <code>async</code> workflow
(mostly, in the case of CPU bound tasks). I think that we, as a community, have
over-emphasized the ephemeral performance benefits of <code>async</code> Rust while
downplaying its semantic benefits.</p>

<p>In the worst case, it leads to people shrugging off <code>async</code>/<code>await</code> as
“<a href="https://shnatsel.medium.com/smoke-testing-rust-http-clients-b8f2ee5db4e6">a weird thing that you resort to for niche use cases</a>”. It should be seen as
a powerful programming model that lets you succinctly express patterns that
can’t be expressed in synchronous Rust without dozens of threads and channels.</p>

<p>I also think there’s a tendency to try to make <code>async</code> Rust “just like sync
Rust” in a way that encourages negative comparison. By “tendency”, I mean that
it’s <a href="https://blog.rust-lang.org/inside-rust/2022/02/03/async-in-2022.html">the stated roadmap for the Rust project</a>,
saying that “that writing async Rust code should be as easy as writing sync code, apart from the occasional <code>async</code> and <code>await</code> keyword.”.</p>

<p>I reject this framing because it’s fundamentally impossible. It’s like trying to
host a pizza party on a ski slope. Sure, you can probably get 99% of the way
there, especially if you’re really talented. But there are differences that the
average bear <em>will</em> notice, no matter how good you are.</p>

<p>We shouldn’t be trying to force our model into unfriendly idioms to
appease programmers who refuse to adopt another type of pattern. We should be
trying to highlight the strengths of Rust’s <code>async</code>/<code>await</code> ecosystem; its
composability and its power. We should be trying to make it so <code>async</code>/<code>await</code>
is the <em>default</em> choice whenever a programmer reaches for concurrency. Rather
than trying to make sync Rust and <code>async</code> Rust the same, we should embrace the
differences.</p>

<p>In short, we shouldn’t be using technical reasons to argue for a semantic model.
We should be using semantic reasons.</p>


  </div></div>
  </body>
</html>
