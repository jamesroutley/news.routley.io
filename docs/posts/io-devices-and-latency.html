<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://planetscale.com/blog/io-devices-and-latency">Original</a>
    <h1>IO Devices and Latency</h1>
    
    <div id="readability-page-1" class="page"><article><p>By <!-- -->Benjamin Dicken<!-- --> | <time datetime="2025-03-13">March 13, 2025</time></p><p>Non-volatile storage is a cornerstone of modern computer systems.<!-- --> <!-- -->Every modern photo, email, bank balance, medical record, and other critical pieces of data are kept on digital storage devices, often replicated many times over for added durability.</p><p><strong>Non-volatile</strong> storage, or colloquially just &#34;<strong>disk</strong>&#34;, can store binary data even when the computer it is attached to is powered off.<!-- --> <!-- -->Computers have other forms of <strong>volatile</strong> storage such as CPU registers, CPU cache, and random-access memory, all of which are faster but require continuous power to function.</p><p>Here, we&#39;re going to cover the history, functionality, and performance of non-volatile storage devices over the history of computing, all using fun and interactive visual elements.<!-- --> <!-- -->This blog is written in celebration of our latest product release: <a href="https://planetscale.com/blog/announcing-metal">PlanetScale Metal</a>.<!-- --> <!-- -->Metal uses locally attached NVMe drives to run your cloud database, as opposed to the slower and less consistent network-attached storage used by most cloud database providers.<!-- --> <!-- -->This results in a blazing fast queries, low latency, and unlimited IOPS.<!-- --> <!-- -->Check out <a href="https://planetscale.com/docs/metal">the docs</a> to learn more.</p><h2 id="tape-storage"><a href="#tape-storage">Tape Storage</a></h2><p>As early as the 1950s, computers were using <strong>tape drives</strong> for non-volatile digital storage.<!-- --> <!-- -->Tape storage systems have been produced in many form factors over the years, ranging from ones that take up an entire room to small drives that can fit in your pocket, such as the iconic Sony Walkman.<!-- --> <!-- -->A tape <strong>Reader</strong> is a box containing hardware specifically designed for reading <strong>tape cartridges</strong>.<!-- --> <!-- -->Tape cartridges are inserted and then unwound which causes the tape to move over the <strong>IO Head</strong>, which can <span>read</span> and <span>write</span> data.</p><p>Though tape started being used to store digital information over 70 years ago, it is still in use today for certain applications.<!-- --> <!-- -->A standard LTO tape cartridge has several-hundred meters of 0.5 inch wide tape.<!-- --> <!-- -->The tape has several tracks running along its length, each track being further divided up into many small cells.<!-- --> <!-- -->A single tape cartridge contains many trillions of cells.</p><p>Each cell can have its magnetic polarization set to up or down, corresponding to a binary 0 or 1.<!-- --> <!-- -->Technically, the magnetic field created by the <em>transition</em> between two cells is what makes the 1 or zero.<!-- --> <!-- -->A long sequence of bits on a tape forms a page of data.<!-- --> <!-- -->In the visualization of the tape reader, we simplify this by showing the tape as a simple sequence of data pages, rather than showing individual bits.</p><p>When a tape needs to be read, it is loaded into a reader, sometimes by hand and sometimes by robot.<!-- --> <!-- -->The reader then spins the cartridge with its motor and uses the <strong>reader head</strong> to read off the binary values as the tape passes underneath.</p><p>Give this a try with the (greatly slowed down) interactive visualization below.<!-- --> <!-- -->You can control the <span>speed of the tape</span> if you&#39;d like it faster or slower.<!-- --> <!-- -->You can also <span>issue read requests</span> and <span>write requests</span> and then monitor how long these take.<!-- --> <!-- -->You&#39;ll also be able to see the queue of pending IO operations pop up in the top-left corner.<!-- --> <!-- -->Try issuing a few requests to get a feel for how tape storage works:</p><p>If you spend enough time with this, you will notice that:</p><ol><li>If you read/write to a cell &#34;near&#34; the read head, it&#39;s fast.</li><li>If you read/write to a cell &#34;far&#34; from the read head, it&#39;s slow.</li></ol><p>Even with modern tape systems, reading data that is far away on a tape can take 10s of seconds, because it may need to spin the tape by hundreds of meters to reach the desired data.<!-- --> <!-- -->Let&#39;s compare two more specific, interactive examples to illustrate this further.</p><p>Say we need to <span>read</span> a total of 4 pages and <span>write</span> an additional 4 pages worth of data.<!-- --> <!-- -->In the first scenario, all 4 pages we need to read are in a neat sequence, and the 4 to write to are immediately after the reads.<!-- --> <!-- -->You can see the IO operations queued up in the white container on the top-left.<!-- --> <!-- -->Go ahead and click the <span>Time IO button</span> to see this in action, and observe the time it takes to complete.</p><p>As you can see, it takes somewhere around 3-4 seconds.<!-- --> <!-- -->On a real system, with an IO head that can operate much faster and motors that can drive the spools more quickly, it would be much faster.</p><p>Now consider another scenario where we need to read and write the same number of pages.<!-- --> <!-- -->However, these reads and writes are spread out throughout the tape.<!-- --> <!-- -->Go ahead and click the <span>Time IO button</span> again.</p><p>That took ~7x longer for the same total number of reads and writes!<!-- --> <!-- -->Imagine if this system was being used to load your social media feed or your email inbox.<!-- --> <!-- -->It might take 10s of seconds or even a full minute to display.<!-- --> <!-- -->This would be totally unacceptable.</p><p>Though the latency for random reads and writes is poor, tape systems operate quite well when reading or writing data in long sequences.<!-- --> <!-- -->In fact, tape storage still has many such use cases today in the modern tech world.<!-- --> <!-- -->Tape is particularly well-suited for situations where there is a need for <em>massive</em> amounts of storage that does not need to be read frequently, but needs to be safely stored.<!-- --> <!-- -->This is because tape is both cheaper per-gigabyte and has a longer shelf-life than its competition: solid state drives and hard disk drives.<!-- --> <!-- -->For example, CERN has a tape storage data warehouse with <a href="https://home.cern/science/computing/data-preservation">over 400 petabytes of data</a> under management.<!-- --> <!-- -->AWS also offers <a href="https://aws.amazon.com/storagegateway/vtl/">tape archiving</a> as a service.</p><p>What tape is not well suited for is high-traffic transactional databases.<!-- --> <!-- -->For these and many other high-performance tasks, other storage mediums are needed.</p><h2 id="hard-disk-drives"><a href="#hard-disk-drives">Hard Disk Drives</a></h2><p>The next major breakthrough in storage technology was the <strong>hard disk drive</strong>.</p><p>Instead of storing binary data on a tape, we store them on a small circular metal disk known as the <strong>Platter</strong>.<!-- --> <!-- -->This disk is placed inside of an <strong>enclosure</strong> with a special <strong>read/write head</strong>, and spins very fast (7200 RPM is common, for example).<!-- --> <!-- -->Like the tape, this disk is also divided into tracks.<!-- --> <!-- -->However, the tracks are <em>circular</em>, and a single disk will often have well over 100,000 tracks.<!-- --> <!-- -->Each track contains hundreds of thousands of pages, and each page containing 4k (or so) of data.</p><p>An HDD requires a mechanical spinning motion of both the reader and the platter to bring the data to the correct location for reading.<!-- --> <!-- -->One advantage of HDD over tape is that the entire surface area of the bits is available 100% of the time.<!-- --> <!-- -->It still takes time to move the needle + spin the disk to the correct location for a read or write, but it does not need to be &#34;uncovered&#34; like it needs to be for a tape.<!-- --> <!-- -->This combined with the fact that there are two different things that can spin, means data can be read and written with much lower latency.<!-- --> <!-- -->A typical random read can be performed in 1-3 milliseconds.</p><p>Below is an interactive hard drive.<!-- --> <!-- -->You can control the <span>speed of the platter</span> if you&#39;d like it faster or slower.<!-- --> <!-- -->You can request that the hard drive <span>read a page</span> and <span>write to a nearby available page</span>.<!-- --> <!-- -->If you request a read or write before the previous one is complete, a queue will be built up, and the disk will process the requests in the order it receives them.<!-- --> <!-- -->As before, you&#39;ll also be able to see the queue of pending IO operations in the white IO queue box.</p><p>As with the tape, the speed of the platter spin has been slowed down by orders of magnitude to make it easier to see what&#39;s going on.<!-- --> <!-- -->In real disks, there would also be many more tracks and sectors, enough to store multiple terabytes of data in some cases.</p><p>Let&#39;s again consider a few specific scenarios to see how the order of reads and writes affects latency.</p><p>Say we need to write a total of three pages of data and then read 3 pages afterward.<!-- --> <!-- -->The three writes will happen on nearby available pages, and the reads will be from tracks 1, 4, and 3.<!-- --> <!-- -->Go ahead and click the <span>Time IO button</span>.<!-- --> <!-- -->You&#39;ll see the requests hit the queue, the reads and writes get fulfilled, and then the total time at the end.</p><p>Due to the sequential nature of most of these operations, all the tasks were able to complete quickly.</p><p>Now consider the same set of 6 reads and writes, but with them being interleaved in a different order.<!-- --> <!-- -->Go ahead and click the <span>Time IO button</span> again.</p><p>If you had the patience to wait until the end, you should notice how the same total number of reads and writes took much longer.<!-- --> <!-- -->A lot of time was spent waiting for the platter to spin into the correct place under the read head.</p><p>Magnetic disks have supported command queueing directly on the disks for a long time (80s with SCSI, 2000s with SATA).<!-- --> <!-- -->Because of this, the OS can issue multiple commands that run in parallel and potentially out-of-order, similar to SSDs.<!-- --> <!-- -->Magnetic disks also improve their performance if they can build up a queue of operations that the disk controller can then schedule reads and writes to optimize for the geometry of the disk.</p><p>Here&#39;s a visualization to help us see the difference between the latency of a <span>random tape read</span> compared to a <span>random disk read</span>.<!-- --> <!-- -->A random tape read will often take multiple seconds (I put 1 second here to be generous) and a disk head seek takes closer to 2 milliseconds (one thousandth of a second)</p><p>Even though HDDs are an improvement over tape, they are still &#34;slow&#34; in some scenarios, especially random reads and writes.<!-- --> <!-- -->The next big breakthrough, and currently the most common storage format for transactional databases, are SSDs.</p><h2 id="solid-state-drives"><a href="#solid-state-drives">Solid State Drives</a></h2><p>Solid State Storage, or &#34;flash&#34; storage, was invented in the 1980s.<!-- --> <!-- -->It was around even while tape and hard disk drives dominated the commercial and consumer storage spaces.<!-- --> <!-- -->It didn&#39;t become mainstream for consumer storage until the 2000s due to technological limitations and cost.</p><p>The advantage of SSDs over both tape and disk is that they do not rely on any <em>mechanical</em> components to read data.<!-- --> <!-- -->All data is read, written, and erased electronically using a special type of non-volatile <em>transistor</em> known as NAND flash.<!-- --> <!-- -->This means that each 1 or 0 can be read or written without the need to move any physical components, but 100% through electrical signaling.</p><p>SSDs are organized into one or more <strong>targets</strong>, each of which contains many <strong>blocks</strong> which each contain some number of <strong>pages</strong>.<!-- --> <!-- -->SSDs read and write data at the page level, meaning they can only read or write full pages at a time.<!-- --> <!-- -->In the SSD below, you can see reads and writes happening via the <strong>lines</strong> between the controller and targets (also called &#34;traces&#34;).</p><p>The removal of mechanical components reduces the latency between when a request is made and when the drive can fulfill the request.<!-- --> <!-- -->There is no more waiting around for something to spin.</p><p>We&#39;re showing small examples in the visual to make it easier to follow along, but a single SSD is capable of storing multiple terabytes of data.<!-- --> <!-- -->For example, say each page holds 4096 bits of data (4k).<!-- --> <!-- -->Now, say each block stores 16k pages, each target stores 16k blocks, and our device has 8 targets.<!-- --> <!-- -->This comes out to <code>4k * 16k * 16k * 8 = 8,796,093,022,208</code> bits, or 8 terabytes.<!-- --> <!-- -->We could increase the capacity of this drive by adding more targets or packing more pages in per block.</p><p>Here&#39;s a visualization to help us see the difference between the latency of a random read on an <span>HDD</span> vs <span>SSD</span>.<!-- --> <!-- -->A random read on an SSD varies by model, but can execute as fast as 16μs (μs = microsecond, which is one millionth of a second).</p><p>It would be tempting to think that with the removal of mechanical parts, the organization of data on an SSD no longer matters.<!-- --> <!-- -->Since we don&#39;t have to wait for things to spin, we can access any data at any location with perfect speed, right?</p><p>Not quite.</p><p>There are other factors that impact the performance of IO operations on an SSD.<!-- --> <!-- -->We won&#39;t cover them all here, but two that we will discuss are <strong>parallelism</strong> and <strong>garbage collection</strong>.</p><h3 id="ssd-parallelism"><a href="#ssd-parallelism">SSD Parallelism</a></h3><p>Typically, each <strong>target</strong> has a dedicated <strong>line</strong> going from the control unit to the target.<!-- --> <!-- -->This line is what processes reads and writes, and only one page can be communicated by each line at a time.<!-- --> <!-- -->Pages can be communicated on these lines <em>really fast</em>, but it still does take a small slice of time.<!-- --> <!-- -->The organization of data and sequence of reads and writes has a significant impact on how efficiently these lines can be used.</p><p>In the interactive SSD below, we have 4 targets and a set of 8 write operations queued up.<!-- --> <!-- -->You can click the <span>Time IO button</span> to see what happens when we can use the lines in parallel to get these pages written.</p><p>In this case, we wrote 8 pages spread across the 4 targets.<!-- --> <!-- -->Because they were spread out, we were able to leverage parallelism to write 4 at a time in two time slices.</p><p>Compare that with another sequence where the SSD writes all 8 pages to the same target.<!-- --> <!-- -->The SSD can only utilize a single data line for the writes.<!-- --> <!-- -->Again, hit the <span>Time IO button</span> to see the timing.</p><p>Notice how only one line was used and it needed to write sequentially.<!-- --> <!-- -->All the other lines sat dormant.</p><p>This demonstrates that the order in which we read and write data matters for performance.<!-- --> <!-- -->Many software engineers don&#39;t have to think about this on a day-to-day basis, but those designing software like MySQL need to pay careful attention to what <a href="https://planetscale.com/blog/btrees-and-database-indexes">structures data is being stored in</a> and how data is laid out on disk.</p><h3 id="ssd-garbage-collection"><a href="#ssd-garbage-collection">SSD Garbage Collection</a></h3><p>The minimum &#34;chunk&#34; of data that can be read from or written to an SSD is the size of a page.<!-- --> <!-- -->Even if you only need a subset of the data within, that is the unit that requests to the drive must be made in.</p><p>Data can be read from a page any number of times.<!-- --> <!-- -->However, writes are a bit different.<!-- --> <!-- -->After a page is written to, it cannot be overwritten with new data until the old data has been explicitly <strong>erased</strong>.<!-- --> <!-- -->The tricky part is, individual pages cannot be erased.<!-- --> <!-- -->When you need to erase data, the entire block must be erased, and afterwards all of the pages within it can be reused.</p><p>Each SSD needs to have an internal algorithm for managing which pages are empty, which are in use, and which are <strong>dirty</strong>.<!-- --> <!-- -->A <strong>dirty</strong> page is one that has been written to but the data is no longer needed and ready to be erased.<!-- --> <!-- -->Data also sometimes needs to be re-organized to allow for new write traffic.<!-- --> <!-- -->The algorithm that manages this is called the <strong>garbage collector</strong>.</p><p>Let&#39;s see how this can have an impact by looking at another visualization.<!-- --> <!-- -->In the below SSD, all four of the targets are storing data.<!-- --> <!-- -->Some of the data is <span>dirty, indicated by red text</span>.<!-- --> <!-- -->We want to <span>write</span> 5 pages worth of data to this SSD.<!-- --> <!-- -->If we <span>time this sequence of writes</span>, the SSD can happily write them to free pages with no need for extra garbage collection.<!-- --> <!-- -->There are sufficient unused pages in the first target.</p><p>Now say we have a drive with different data already on it, but we want to <span>write</span> those same 5 pages of data to it.<!-- --> <!-- -->In this drive, we only have 2 pages that are unused, but a number of <span>dirty pages</span>.<!-- --> <!-- -->In order to write 5 pages of data, the SSD will need to spend some time doing garbage collection to make room for the new data.<!-- --> <!-- -->When attempting to <span>time another sequence of writes</span>, some garbage collection will take place to make room for the data, slowing down the write.</p><p>In this case, the drive had to move the two non-dirty pages from the top-left target to new locations.<!-- --> <!-- -->By doing this, it was able to make all of the pages on the top-left target dirty, making it safe to erase that data.<!-- --> <!-- -->This made room for the 5 new pages of data to be written.<!-- --> <!-- -->These additional steps significantly slowed down the performance of the write.</p><p>This shows how the organization of data on the drive can have an impact on performance.<!-- --> <!-- -->When SSDs have a lot of reads, writes, and deletes, we can end up with SSDs that have degraded performance due to garbage collection.<!-- --> <!-- -->Though you may not be aware, busy SSDs do garbage collection tasks regularly, which can slow down other operations.</p><p>These are just two of many reasons why the arrangement of data on a SSD affects its performance.</p><h2 id="storage-in-the-cloud"><a href="#storage-in-the-cloud">Storage in the cloud</a></h2><p>The shift from tape, to disk, to solid state has allowed durable IO performance to accelerate dramatically over the past several decades.<!-- --> <!-- -->However, there is another phenomenon that has caused an additional shift in IO performance: moving to the cloud.</p><p>Though there were companies offering cloud compute services before this, the mass move to cloud gained significant traction when Amazon AWS launched in 2006.<!-- --> <!-- -->Since that time, tens of thousands of companies have moved their app servers and database systems to their cloud and other similar services from Google, Microsoft, and others.</p><p>Though there are many upsides to this trend, there are several downsides.<!-- --> <!-- -->One of these is that servers tend to have less permanence.<!-- --> <!-- -->Users rent (virtualised) servers on arbitrary hardware within gigantic data centers.<!-- --> <!-- -->These servers can get shut down at any time for a variety of reasons - hardware failure, hardware replacement, network disconnects, etc.<!-- --> <!-- -->When building platforms on rented cloud infrastructure, computer systems need to be able to tolerate more frequent failures at any moment.<!-- --> <!-- -->This, along with many engineers&#39; desire for dynamically-scaleable storage volumes has led to a new sub-phenomenon: Separation of <strong>storage</strong> and <strong>compute</strong>.</p><h2 id="separating-storage-from-compute"><a href="#separating-storage-from-compute">Separating storage from compute</a></h2><p>Traditionally, most servers, desktops, laptops, phones and other computing devices have their non-volatile storage directly attached.<!-- --> <!-- -->These are attached with SATA cables, PCIe interfaces, or even built directly into the same SOC as the RAM, CPU, and other components.<!-- --> <!-- -->This is great for speed, but provides the following challenges:</p><ol><li>If the server goes down, the data goes down with it.</li><li>The storage is of a fixed size.</li></ol><p>For application servers, 1. and 2. are typically not a big deal since they work well in ephemeral environments by design.<!-- --> <!-- -->If one goes down, just spin up a new one.<!-- --> <!-- -->They also don&#39;t typically need much storage, as most of what they do happens in-memory.</p><p>Databases are a different story.<!-- --> <!-- -->If a server goes down, we don&#39;t want to lose our data, and data size grows quickly, meaning we may hit storage limits.<!-- --> <em>Partly</em> due to this, many cloud providers allow you to spin up compute instances with a separately-configurable storage system attached over the network.<!-- --> <!-- -->In other words, using network-attached storage as the default.</p><p>When you create a new server in EC2, the default is typically to attach an EBS network storage volume.<!-- --> <!-- -->Many database services including Amazon RDS, Amazon Aurora, Google Cloud SQL, and PlanetScale rely on these types of storage systems that have compute separated from storage over the network.<!-- --> <!-- -->This provides a nice advantage in the that the storage volume can be dynamically resized as data grows and shrinks.<!-- --> <!-- -->It also means that if a server goes down, the data is still safe, and can be re-attached to a different server.<!-- --> <!-- -->This simplicity has come at a cost, however.</p><h2 id="local-vs-network-storage"><a href="#local-vs-network-storage">Local vs network storage</a></h2><p>Consider the following simple configuration.<!-- --> <!-- -->In it, we have a server with a CPU, RAM, and direct-attached NVMe SSD.<!-- --> <!-- -->NVMe SSDs are a type of solid state disk that use the non-volatile memory host controller interface specification for blazing-fast IO speed and great bandwidth.<!-- --> <!-- -->In such a setup, the <span>round trip from CPU to memory (RAM)</span> takes about 100 nanoseconds (a nanosecond is 1 billionth of a second).<!-- --> <span>A round trip from the CPU to a locally-attached NVMe SSD</span> takes about 50,000 nanoseconds (50 microseconds).</p><p>This makes it pretty clear that it&#39;s best to keep as much data in memory as possible for faster IO times.<!-- --> <!-- -->However, we still need disk because (A) memory is more expensive and (B) we need to store our data somewhere permanent.<!-- --> <!-- -->As slow as it may seem here, a locally-attached NVMe SSD is about as fast as it gets for modern storage.</p><p>Let&#39;s compare this to the <span>speed of a network-attached storage volume</span>, such as EBS.<!-- --> <!-- -->Read and write requires a short network round trip within a data center.<!-- --> <!-- -->The round trip time is significantly worse, taking about 250,000 nanoseconds (250 microseconds, or 0.25 milliseconds).</p><p>Using the same cutting-edge SSD now takes an <em>order of magnitude</em> longer to fulfill individual read and write requests.<!-- --> <!-- -->When we have large amounts of sequential IO, the negative impact of this can be reduced, but not eliminated.<!-- --> <!-- -->We have introduced significant <em>latency</em> deterioration for every time we need to hit our storage system.</p><p>Another issue with network-attached storage in the cloud comes in the form of limiting IOPS.<!-- --> <!-- -->Many cloud providers that use this model, including AWS and Google Cloud, limit the amount of IO operations you can send over the wire.<!-- --> <!-- -->By default, a GP3 EBS instance on Amazon allows you to send 3000 IOPS per-second, with an additional pool that can be built up to allow for occasional bursts.<!-- --> <!-- -->The following visual shows how this works.<!-- --> <!-- -->Note that the burst balance size is smaller here than in reality to make it easier to see.</p><p>If instead you have your storage attached directly to your compute instance, there are no artificial limits placed on IO operations.<!-- --> <!-- -->You can read and write as fast as the hardware will allow for.</p><p>For as many steps as we&#39;ve taken forward in IO performance over the years, this seems like a step in the wrong direction.<!-- --> <!-- -->This separation buys some nice conveniences, but at what cost to performance?</p><p>How do we overcome issue 1 (data durability) and 2 (drive scalability) while keeping good IOPS performance?</p><p>Issue 1 can be overcome with replication.<!-- --> <!-- -->Instead of relying on a single server to store all data, we can replicate it onto several computers.<!-- --> <!-- -->One common way of doing this is to have one server act as the primary, which will receive all write requests.<!-- --> <!-- -->Then 2 or more additional servers get all the data replicated to them.<!-- --> <!-- -->With the data in three places, the likelihood of losing data becomes very small.</p><p>Let&#39;s look at concrete numbers.<!-- --> <!-- -->As a made up value, say in a given month, there is a 1% chance of a server failing.<!-- --> <!-- -->With a single server, this means we have a 1% chance of losing our data each month.<!-- --> <!-- -->This is an unacceptable for any serious business purpose.<!-- --> <!-- -->However, with three servers, this goes down to 1% × 1% × 1% = 0.0001% chance (1 in one million).<!-- --> <!-- -->At PlanetScale the protection is actually far stronger than even this, as we automatically detect and replace failed nodes in your cluster.<!-- --> <!-- -->We take frequent and reliable backups of the data in your database for added protection.</p><p>Problem 2. can be solved, though it takes a bit more manual intervention when working with directly-attached SSDs.<!-- --> <!-- -->We need to ensure that we monitor and get alerted when our disk approaches capacity limits, and then have tools to easily increase capacity when needed.<!-- --> <!-- -->With such a feature, we can have data permanence, scalability, and blazing fast performance. This is exactly what PlanetScale has built with Metal.</p><p>Planetscale just announced <strong><a href="https://planetscale.com/blog/announcing-metal">Metal</a></strong>, an industry-leading solution to this problem.</p><p>With Metal, you get a full-fledged Vitess+MySQL cluster set up, with each MySQL instance running with a direct-attached NVMe SSD drive.<!-- --> <!-- -->Each Metal cluster comes with a primary and two replicas by default for extremely durable data.<!-- --> <!-- -->We allow you to resize your servers with larger drives with just a few clicks of a button when you run up against storage limits.<!-- --> <!-- -->Behind the scenes, we handle spinning up new nodes and migrating your data from your old instances to the new ones with zero downtime.</p><p>Perhaps most importantly, with a Metal database, there is no artificial cap on IOPS.<!-- --> <!-- -->You can perform IO operations with minimal latency, and hammer it as hard as you want without being throttled or paying for expensive IOPS classes on your favorite cloud provider.</p><p>If you want the ultimate in performance and scalability, <strong><a href="https://planetscale.com/metal">try Metal today</a></strong>.</p></article></div>
  </body>
</html>
