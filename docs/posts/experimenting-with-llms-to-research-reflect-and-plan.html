<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eugeneyan.com/writing/llm-experiments/">Original</a>
    <h1>Experimenting with LLMs to Research, Reflect, and Plan</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>Over the past few weekends, I‚Äôve been playing with large language models (LLMs) and combining them with tools and interfaces to build a <a href="#tools-to-summarize-query-and-advise">simple assistant</a>. Along the way, I noticed <a href="#shortcomings-in-retrieval-and-how-to-solve-them">some issues with retrieval</a> and thought of a few ideas on how to solve them.</p>
<p>It was a lot of fun, and I came away from this experience optimistic that LLMs have great potential to augment how we work, especially how we <a href="#llm-augmented-research-reflection-and-planning">research, reflect, and plan</a>.</p>

<p>My first project was inspired by Simon‚Äôs <a href="https://simonwillison.net/2023/Mar/10/chatgpt-internet-access/" target="_blank">post</a> on how ChatGPT is unable to read content from URLs. Thus, I tried to help it do just that with <code>/summarize</code> and <code>/eli5</code>. The former can <code>/summarize</code> content from URLs into bullet points while the latter reads the content and explains like I‚Äôm five (eli5). They help me skim content before deciding if I want to read the details in full (<a href="https://twitter.com/eugeneyan/status/1637562031233708032" target="_blank">tweet thread</a>).</p>
<p><img src="https://eugeneyan.com/assets/summarize.jpg" loading="lazy" title="Using /summarize on &#39;ChatGPT Is a Blurry JPEG of the Web&#39;" alt="Using /summarize on &#39;ChatGPT Is a Blurry JPEG of the Web&#39;"/></p>
<p>Bullet point summary of &#34;<a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="blank">ChatGPT Is a Blurry JPEG of the Web</a>&#34;</p>
<p><img src="https://eugeneyan.com/assets/eli5.jpg" loading="lazy" title="Using /eli5 on &#39;ChatGPT Is a Blurry JPEG of the Web&#39;" alt="Using /eli5 on &#39;ChatGPT Is a Blurry JPEG of the Web&#39;"/></p>
<p>Explaining to a five-year old &#34;<a href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web" target="blank">ChatGPT Is a Blurry JPEG of the Web</a>&#34;</p>
<p>Next, I explored building agents with access to tools like SQL and search. <code>/sql</code> takes natural language questions, writes and runs SQL queries, and returns the result. <code>/sql-agent</code> does the same but as a zero-shot agent. Though <code>/sql-agent</code> didn‚Äôt work as reliably as I hoped (see <a href="#appendix">Appendix</a>), watching it struggle and eventually get it right was endearing and motivating (<a href="https://twitter.com/eugeneyan/status/1640160729537073152" target="_blank">tweet thread</a>).</p>
<p><img src="https://eugeneyan.com/assets/sql.jpg" loading="lazy" title="Querying a small database via /sql" alt="Querying a small database via /sql"/></p>
<p>Querying a small database via <code>/sql</code></p>
<p><img src="https://eugeneyan.com/assets/sql-agent-1.jpg" loading="lazy" title="Querying a small database via /sql-agent" alt="Querying a small database via /sql-agent"/>
<img src="https://eugeneyan.com/assets/sql-agent-2.jpg" loading="lazy" title="Querying a small database via /sql-agent" alt="Querying a small database via /sql-agent"/>
<img src="https://eugeneyan.com/assets/sql-agent-3.jpg" loading="lazy" title="Querying a small database via /sql-agent" alt="Querying a small database via /sql-agent"/></p>
<p>Querying a small database via <code>/sql-agent</code> (it&#39;s fun to watch it think, observe, and act)</p>
<p>I also built <code>/search</code>, an agent that can use tools to query search provider APIs (e.g., Google Search). This way, the LLM can find recent data that it hasn‚Äôt been trained on and return an accurate and up-to-date response. (This was before ChatGPT plugins that now have this functionality out of the box. Even so, it was fun building it from scratch.)</p>
<p><img src="https://eugeneyan.com/assets/search.jpg" loading="lazy" title="Using /search to find recent information for the LLM" alt="Using /search to find recent information for the LLM"/>
<img src="https://eugeneyan.com/assets/search-2.jpg" loading="lazy" title="Using /search to find recent information for the LLM" alt="Using /search to find recent information for the LLM"/></p>
<p>Using <code>/search</code> to find recent information for the LLM</p>
<p>Most recently, I built a <code>/board</code> of advisors. It‚Äôs based on content from informal mentors‚Äîand prolific writers‚Äîlike Paul Graham, Marc Andreessen, Will Larson, Charity Majors, and Naval Ravikant. <code>/board</code> provides advice on topics including technology, leadership, and startups. Its response includes source URLs for further reading, which can be chained with <code>/summarize</code> and <code>/eli5</code> (<a href="https://twitter.com/eugeneyan/status/1642775988215107584" target="_blank">tweet thread</a>).</p>
<p><img src="https://eugeneyan.com/assets/board-2.jpg" loading="lazy" title="Seeking advice on technical leadership from the /board" alt="Seeking advice from the /board"/>
<img src="https://eugeneyan.com/assets/board.jpg" loading="lazy" title="Seeking advice on personal success from the /board" alt="Seeking advice from the /board"/></p>
<p>Seeking advice on technical leadership and personal success from the <code>/board</code></p>
<p>I also built <code>/ask-ey</code> which is similar to <code>/board</code> but based on my own writing. Because I‚Äôm more familiar with my work, it‚Äôs easier to spot issues such as not using an expected source (i.e., recall issue) or using irrelevant sources (i.e., ranking issue).</p>
<p><img src="https://eugeneyan.com/assets/ask-ey.jpg" loading="lazy" title="Synthesizing across recsys posts on my site via /ask-ey" alt="Synthesizing across posts on my site via /ask-ey"/>
<img src="https://eugeneyan.com/assets/ask-ey-2.jpg" loading="lazy" title="Synthesizing across mechanism posts on my site via /ask-ey" alt="Synthesizing across posts on my site via /ask-ey"/></p>
<p>Synthesizing across recsys and mechanism posts on my site via <code>/ask-ey</code></p>
<h2 id="combining-llms-databases-search-apis-and-discord">Combining LLMs, databases, search APIs, and Discord</h2>
<p>To extract content from URLs, I used good ol‚Äô <a href="https://pypi.org/project/requests/" target="_blank"><code>requests</code></a> and <a href="https://pypi.org/project/beautifulsoup4/" target="_blank"><code>BeautifulSoup</code></a>. For LLMs, I worked with OpenAI‚Äôs <a href="https://platform.openai.com/docs/models/gpt-3-5" target="_blank"><code>gpt-3.5-turbo</code></a> and <a href="https://platform.openai.com/docs/models/gpt-4" target="_blank"><code>gpt-4</code></a>, primarily the former due to its cost-effectiveness. <a href="https://github.com/hwchase17/langchain" target="_blank">LangChain</a> made it easy to apply the LLM chains, agents, and tools. For search, I used Google‚Äôs custom search through the <a href="https://pypi.org/project/google-api-python-client/2.84.0/" target="_blank"><code>google-api-python-client</code></a> wrapper. To embed documents and queries, I used OpenAI‚Äôs <a href="https://platform.openai.com/docs/models/embeddings" target="_blank"><code>text-embedding-ada-002</code></a>.</p>
<p>The application server was hosted on <a href="http://railway.app" target="_blank">Railway</a>. To host, serve, and find nearest neighbours on embeddings, I used <a href="https://www.pinecone.io" target="_blank">Pinecone</a>. Lastly, I integrated everything with Discord via the <a href="https://github.com/interactions-py/interactions.py" target="_blank"><code>interactions</code></a> wrapper.</p>
<h2 id="shortcomings-in-retrieval-and-how-to-solve-them">Shortcomings in retrieval and how to solve them</h2>
<p>While experimenting with <code>/board</code> and <code>/ask-ey</code>, I noticed that it wasn‚Äôt retrieving and using the expected sources some of the time.</p>
<p>For example, when I asked the <code>/board</code> ‚ÄúHow do I decide between being a manager or an IC‚Äù, it fails to use (as a source) any of Charity‚Äôs writing on the <a href="https://charity.wtf/2017/05/11/the-engineer-manager-pendulum/" jeejarget="_blank">manager</a>-<a href="https://charity.wtf/2019/01/04/engineering-management-the-pendulum-or-the-ladder/" target="_blank">engineer</a> <a href="https://charity.wtf/2022/03/24/twin-anxieties-of-the-engineer-manager-pendulum/" target="_blank">pendulum</a> or <a href="https://charity.wtf/2019/09/08/reasons-not-to-be-a-manager/" target="_blank">management</a>. However, tweaking the question to ‚ÄúHow do I decide between being a manager or an <em>engineer</em>‚Äù resolved this.</p>
<p><img src="https://eugeneyan.com/assets/manager-ic.jpg" loading="lazy" title="Failing to retrieve the relevant engineering-IC resources the first time" alt="Failing to retrieve the relevant engineering-IC resources the first time"/></p>
<p>Asking the first question doesn&#39;t lead to the expected manager-eng sources being used; the second does</p>
<p>Similarly, when I <code>/ask-ey</code> ‚ÄúWhat bandits are used in recommendation systems‚Äù, it didn‚Äôt retrieve my main writing on <a href="https://eugeneyan.com/writing/bandits/" target="_blank">bandits</a>. But updating the question to ‚Äú<em>How</em> are bandits used in recommendation systems‚Äù fixed this issue.</p>
<p><img src="https://eugeneyan.com/assets/bandit.jpg" loading="lazy" title="Failing to retrieve the relevant bandit resources the first time" alt="Failing to retrieve the relevant bandit resources the first time"/></p>
<p>Asking the first question doesn&#39;t lead to the expected bandit sources being used; the second does</p>
<p>But when I checked the retrieved sources, it was disappointing to see that only the top hit came from the <a href="https://eugeneyan.com/writing/bandits/" target="_blank">expected URL</a>, and even that was an irrelevant chunk of the content. (Text from each URL is split into chunks of 1,500 tokens.) I had expected embedding-based retrival to fetch more chunks from the bandit URL. This suggests there‚Äôs room to improve on how I processed the data before embedding and highlights the importance of data prep.</p>
<p><img src="https://eugeneyan.com/assets/bandit-sources.jpg" loading="lazy" title="Only the top hit has the right resource but it doesn&#39;t contain useful content" alt="Only the top hit has the right resource but it doesn&#39;t contain useful content"/></p>
<p>Only the top hit has the right resource but it doesn&#39;t contain useful content</p>
<p>This issue is partially due to poor recall. Here are a few hypotheses on why this happens:</p>
<ul>
<li><a href="#ann-indices-might-be-tuned-sub-optimally">ANN indices might be tuned sub-optimally</a></li>
<li><a href="#off-the-shelf-embeddings-may-transfer-poorly-across-domains">Off-the-shelf embeddings may transfer poorly across domains</a></li>
<li><a href="#documents-may-be-inadequately-chunked">Documents may be inadequately chunked</a></li>
<li><a href="#embedding-based-retrieval-alone-might-be-insufficient">Embedding-based retrieval alone might be insufficient</a></li>
</ul>
<h3 id="ann-indices-might-be-tuned-sub-optimally">ANN indices might be tuned sub-optimally</h3>
<p>Most (if not all) embedding-based retrieval use <em>approximate</em> nearest neighbours (ANN). If we use <em>exact</em> nearest neighbours, we would get perfect recall of 1.0 but with higher latency (think seconds). In contrast, ANN offers good-enough recall (~0.95) with millisecond latency. I‚Äôve <a href="https://eugeneyan.com/writing/real-time-recommendations/#how-to-design-and-implement-an-mvp" target="_blank">previously compared several open-source ANNs</a> and most achieved ~0.95 recall at throughput of hundreds to thousands of queries per second.</p>
<p><img src="https://eugeneyan.com/assets/ann-benchmarks.jpg" loading="lazy" title="Benchmarking ANNs on recall vs latency" alt="Benchmarking ANNs on recall vs latency"/></p>
<p>Benchmarking ANNs on recall vs latency across index parameters; top-right is better.</p>
<p>If the issue lies in a sub-optimally tuned ANN index, we can tune the index parameters to achieve the recall/latency trade-off we need. However, this requires more effort compared to a plug-and-play index as a service. I‚Äôm also not sure if cloud vector databases offer the option to tune the ANN. As a result, we could end up with <a href="https://twitter.com/jobergum/status/1643187540222959616" target="_blank">as low as 50% recall</a>.</p>
<h3 id="off-the-shelf-embeddings-may-transfer-poorly-across-domains">Off-the-shelf embeddings may transfer poorly across domains</h3>
<p>Off-the-shelf embeddings may be too generic and don‚Äôt transfer well to other domains. From the examples in <a href="https://community.openai.com/t/some-questions-about-text-embedding-ada-002-s-embedding/35299" target="_blank">this OpenAI forum</a>, we see unexpectedly high cosine similarity between seemingly different text. (While the failure examples above seem generic, the point is that we should pay attention when applying embeddings to our domain.)</p>
<p>A possible solution: If we have both positive and (hard) negative examples, we can fine-tune an embedding model via triplet loss. This way, we can ensure that the distance between the anchor and positive example is closer than the distance between the anchor and negative example (by a margin). This is especially helpful when embedding private data that contains language that foundational models may not have seen.</p>
<p>Preparing these <code>(anchor, positive, negative)</code> triplets is the bulk of the work. One way is to collect explicit feedback by returning sources in responses and asking people to thumbs up/down on them. Alternatively, implicit feedback is available in settings such as e-commerce, where we can consider results that users ignore as negatives, or search, where we provide sources in results (√† la Bing Chat) and observe if users click on them.</p>
<h3 id="documents-may-be-inadequately-chunked">Documents may be inadequately chunked</h3>
<p>Third, if we‚Äôre using LangChain, we‚Äôre probably taking the default approach of using its <a href="https://python.langchain.com/en/latest/reference/modules/text_splitter.html" target="_blank">text splitter</a> and chunking content into documents of 1,000 - 2,000 tokens each. While we can have such large documents because recent embedding models can scale to long input text, problems may arise when the input is overloaded with multiple concepts.</p>
<p>Imagine embedding a <a href="https://eugeneyan.com/writing/content-moderation/" target="_blank">3,000-word document</a> that has five high-level concepts and a dozen lower-level concepts. Embedding the entire document may force the model to place it in the latent space of all concepts, making retrieval based on any single concept difficult. Even if we split it into multiple chunks of 1,500 tokens each, each chunk‚Äôs embedding could be a muddy blend of multiple concepts.</p>
<p>A more effective approach could be to chunk documents by sections or paragraphs. After all, this is how most content is organized, where each section/chapter discusses a high-level concept while paragraphs contain lower-level concepts. This should enhance the quality of embeddings and improve embedding-based retrieval. Thankfully, most writing is organized by sections or chapters, with paragraphs separated by <code>/n/n</code>.</p>
<p>I suspect there are large gains to be made here though it also requires relatively more work. Scraping data for my document store took as much, if not more, effort as building the tools.</p>
<h3 id="embedding-based-retrieval-alone-might-be-insufficient">Embedding-based retrieval alone might be insufficient</h3>
<p>Lastly, relying solely on document and query embeddings for retrieval may be insufficient. While embedding-based retrieval is great for semantic retrieval, it can struggle when term matching is crucial. Because embeddings represent documents as dense vectors, they may fail to capture the importance of individual words in the documents, leading to poor recall. And if the search query is precise and short, embedding-based retrieval may not add that much value, or perform worse. Also, simply embedding the entire query might be too crude and could make the results sensitive to how the question is phrased.</p>
<p>One solution is to ensemble semantic search with keyword search. <a href="https://en.wikipedia.org/wiki/Okapi_BM25" target="_blank">BM25</a> is a solid baseline when we expect at least one keyword to match. Nonetheless, it doesn‚Äôt do as well on shorter queries where there‚Äôs no keyword overlap with the relevant documents‚Äîin this case, averaged keyword embeddings may perform better. By combining the best of keyword search and semantic search, we can improve recall for various types of queries.</p>
<p>Query parsing can also help by identifying and expanding (e.g., synonyms) keywords in the query, ensuring that questions are interpreted consistently regardless of minor differences in phrasing. Spelling correction and autocomplete can also guide users toward better results. (A simple hack is to have the LLM parse the query before proceeding with retrieval.)</p>
<p>We can also rank retrieved documents before including them in the LLM‚Äôs context. In the bandit query example above, the top hit doesn‚Äôt offer any useful information. One solution is to rank documents via query-dependent and query-independent signals. The former is done via BM25 and semantic search while the latter includes user feedback, popularity, recency, PageRank, and so on. Heuristics such as document length may also help.</p>
<h2 id="llm-augmented-research-reflection-and-planning">LLM-augmented research, reflection, and planning</h2>
<p>While the tools above were hacked together over a few weekends, they hint at the potential in LLM-augmented workflows. Here are some ideas in the adjacent possible.</p>
<h3 id="enterprisepersonal-search-and-qa">Enterprise/Personal Search and Q&amp;A</h3>
<p>Picture yourself as part of an organization where internal documents, meeting transcripts, code, and other resources were stored as retrievable documents. For confidentiality and security reasons, you would only be able to access documents that you have permissions for. To navigate this vast knowledge base, you could ask simple queries such as:</p>
<ul>
<li>What were the common causes of high-severity tickets last month?</li>
<li>What were our biggest wins and most valuable lessons from last quarter?</li>
<li>What are some recent ‚ÄúThink Big‚Äù ideas or <a href="https://commoncog.com/putting-amazons-pr-faq-to-practice/" target="_blank">PRFAQs</a> the team has written?</li>
</ul>
<p>Then, instead of returning links to documents that we would have to read, why not have an LLM <code>/summarize</code> or <code>/eli5</code> the information? It could also synthesize via <code>/board</code> and find common patterns, uncovering root causes for seemingly unrelated incidents or finding synergies (or duplication) in upcoming projects. To augment the results, it could <code>/sql</code> or <code>/sql-agent</code> databases or <code>/search</code> for recent data on the internet.</p>
<p>Let‚Äôs consider another scenario which uses a personal knowledge base. Over the years, I‚Äôve built up a library of books, papers, and disorganized notes. Unfortunately, my <a href="https://en.wikipedia.org/wiki/Forgetting_curve" target="_blank">memory degrades over time</a> and I forget most of the details within a week. To address this, I can apply similar techniques to my personal knowledge base and <code>/ask-ey</code>:</p>
<ul>
<li>What papers discuss the use of <a href="https://eugeneyan.com/writing/bandits/" target="_blank">bandits in recommendation systems</a>?</li>
<li>What guidance would I give someone <a href="https://eugeneyan.com/writing/onboarding/" target="_blank">joining</a> <a href="https://eugeneyan.com/writing/influencing-without-authority/" target="_blank">a</a> <a href="https://eugeneyan.com/writing/15-5/" target="_blank">new</a> <a href="https://eugeneyan.com/writing/red-flags/" target="_blank">team</a>?</li>
<li>What were the main themes in my life in the <a href="https://eugeneyan.com/writing/retrospective-2020/" target="_blank">last</a> <a href="https://eugeneyan.com/writing/2021-year-in-review/" target="_blank">few</a> <a href="https://eugeneyan.com/writing/2022-in-review/" target="_blank">years</a>?</li>
</ul>
<p>For each of those questions, I‚Äôve invested effort into research, distilling, and publishing the answers as <a href="https://eugeneyan.com/writing/note-taking-zettelkasten/#permanent-note" target="_blank">permanent notes</a>. And the process was invaluable for clarifying my thoughts and learning while writing. That said, I think the tools above can get us ~50% there with far less effort. Products like <a href="https://www.glean.com/" target="_blank">Glean</a> (enterprise) and <a href="https://www.rewind.ai/" target="_blank">Rewind</a> (personal) seem to do this.</p>
<h3 id="research-planning-and-writing">Research, Planning, and Writing</h3>
<p>Back to the scenario of internal docs and meeting transcripts. Let‚Äôs say you‚Äôre a leader in your org and need to write an important doc. It could be a six-week plan to tackle tech debt or a more ambitious three-year roadmap. How can we make writing this doc easier?</p>
<p>To write the tech debt document, we‚Äôll first need to understand what are the most pressing issues. We can start by asking <code>/board</code> to gather details about the problems we‚Äôre already aware of. <code>/board</code> can help us retrieve and synthesize the relevant trouble tickets, war room meeting transcripts, and more via <code>/sql</code> and internal <code>/search</code>. Then, we can expand to broader queries to find problems we‚Äôre unaware of, before diving deeper as needed.</p>
<p>With the top three issues, we can start writing an introduction that outlines the purpose of the document (aka the <a href="https://eugeneyan.com/writing/writing-docs-why-what-how/#writing-framework-why-what-how-who" target="_blank">Why</a> aka the prompt). Then, as we created section headers for each issue, a document retrieval + LLM copilot helps with filling them out, providing data points (<code>/sql</code>), links to relevant sources (<code>/search</code>), and even suggesting solutions (<code>/board</code>). Bing Chat has this in some form. Also, I believe this is the vision for Office 365 Copilot.</p>
<p>As the main author, we‚Äôll still need to apply our judgment to check the relevance of the sources and prioritize the issues. We‚Äôll also need to assess suggested solutions and decide if tweaking one solution could address multiple issues, thereby reducing effort and duplication. Nonetheless, while we‚Äôre still responsible for writing the document, our copilot can help gather and prepare the data, significantly reducing our workload.</p>
<h2 id="llms-not-a-knowledge-base-but-a-reasoning-engine">LLMs: Not a knowledge base but a reasoning engine</h2>
<blockquote>
<p>‚ÄúThe right way to think of the models that we create is a reasoning engine, not a fact database. They can also act as a fact database, but that‚Äôs not really what‚Äôs special about them. What we want them to do is something closer to the ability to reason, not to memorize.‚Äù ‚Äî <a href="https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122" target="_blank">Sam Altman</a></p>
</blockquote>
<p>We‚Äôve seen that LLMs are adept at using tools, summarizing information, and synthesizing patterns. Being trained on the entire internet somehow gave them reasoning abilities. (Perhaps this is due to learning on Github and StackOverflow data, since code is logic?) Nonetheless, while they can reason, they‚Äôre often constrained by the lack of in-depth or private knowledge, like the kind found in enterprise or personal knowledge bases.</p>
<p>I think the key challenge, and solution, is getting them the right information at the right time. Having a well-organized document store can help. And by using a hybrid of keyword and semantic search, we can accurately retrieve the context that LLMs need‚Äîthis explains why traditional search indices are integrating <a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/knn-search.html" target="_blank">vector</a> <a href="https://opensearch.org/docs/latest/search-plugins/knn/index/" target="_blank">search</a>, why vector databases are adding <a href="https://www.pinecone.io/learn/hybrid-search/" target="_blank">keyword</a> <a href="https://weaviate.io/blog/hybrid-search-explained" target="_blank">search</a>, and why some apps adopt a hybrid approach (<a href="https://vespa.ai/features" target="_blank">Vespa.ai</a>, <a href="https://arxiv.org/abs/2006.11632" target="_blank">FB search</a>).</p>
<p>‚Ä¢ ‚Ä¢ ‚Ä¢</p>
<p>It‚Äôs hard to foresee how effective or widespread LLMs will become. I‚Äôve previously <a href="https://twitter.com/eugeneyan/status/1635818825018150913" target="_blank">wondered</a> and <a href="https://www.linkedin.com/posts/eugeneyan_activity-7041585027503046657-CeIo" target="_blank">asked</a> whether LLMs might have the same impact as computers, mobile phones, or the internet. But as I continue experimenting with them, I‚Äôm starting to think that their potential could be even greater than all those technologies combined.</p>
<p>And even if I end up being wrong, at least I can still have fun getting LLMs to explain headlines in the style of Dr. Seuss or make up quirky quotes on my Raspberry Pi Pico.</p>
<blockquote><p lang="en" dir="ltr">While some use LLMs to disrupt industries and more,</p>‚Äî Eugene Yan (@eugeneyan) <a href="https://twitter.com/eugeneyan/status/1645244543217041410?ref_src=twsrc%5Etfw">April 10, 2023</a></blockquote>

<hr/>
<h2 id="appendix">Appendix</h2>
<p>Here‚Äôs an example of how <code>/sql-agent</code> struggled and eventually figured out that it should check the database schema. While it finally executed the right query and got the results, it also ran out of iterations before it could respond ü•≤ (Back to <a href="#llm-tools-to-summarize-query-and-advise">top</a>)</p>
<p><img src="https://eugeneyan.com/assets/sql-agent-fail-1.jpg" loading="lazy" title="Querying a small database via /sql-agent" alt="Querying a small database via /sql-agent"/>
<img src="https://eugeneyan.com/assets/sql-agent-fail-2.jpg" loading="lazy" title="Querying a small database via /sql-agent" alt="Querying a small database via /sql-agent"/>
<img src="https://eugeneyan.com/assets/sql-agent-fail-3.jpg" loading="lazy" title="Querying a small database via /sql-agent" alt="Querying a small database via /sql-agent"/>
<img src="https://eugeneyan.com/assets/sql-agent-fail-4.jpg" loading="lazy" title="Querying a small database via /sql-agent" alt="Querying a small database via /sql-agent"/></p>
<p><code>/sql-agent</code> struggling with getting the query right</p>
</div></div>
  </body>
</html>
