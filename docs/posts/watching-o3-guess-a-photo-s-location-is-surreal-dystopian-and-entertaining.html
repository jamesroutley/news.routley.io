<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2025/Apr/26/o3-photo-locations/">Original</a>
    <h1>Watching o3 guess a photo&#39;s location is surreal, dystopian and entertaining</h1>
    
    <div id="readability-page-1" class="page"><div>


<div data-permalink-context="/2025/Apr/26/o3-photo-locations/">

<p>26th April 2025</p>



<p>Watching OpenAI’s new o3 model guess where a photo was taken is one of those moments where decades of science fiction suddenly come to life. It’s a cross between the <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/EnhanceButton">Enhance Button</a> and <a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/OmniscientDatabase">Omniscient Database</a> TV Tropes.</p>
<p>The process is simple. Snap a photograph of somewhere, ideally without putting any obvious landmarks in there. Feed it to ChatGPT o3 (or o4-mini) and prompt “Guess where this photo was taken”. Let it churn.</p>
<p>(I found I needed to turn off all of ChatGPT’s memory features first or it would cheat and use context from my previous conversations.)</p>
<p>I gave it this photo, taken while sat at an outside bar near my home in El Granada, California. I think it’s a pretty solid challenge: not many identifying features, just a bit of road, an anonymous house and some hills in the background:</p>
<p><img src="https://static.simonwillison.net/static/2025/guess-location.jpg" alt="Residential scene with a cream-colored house with gray roof, white picket fence, and two white vehicles parked nearby. In the foreground is a glass-enclosed fire table and orange flowers. Background shows hills under an overcast sky with power lines crossing above. A person in red stands between vehicles near a yellow directional sign."/></p>
<p>... and off it went.</p>
<p>I’ve shared <a href="https://chatgpt.com/share/680c6160-a0c4-8006-a4de-cb8aff785f46">the full transcript here</a>—be sure to expand the “Thought for 6m 48s” link to see the full details of how it attacked the problem.</p>
<p>It starts with a very weird mistake: it briefly pretends that it can’t see the image at all:</p>
<blockquote>
<p>The user uploaded an image and is asking me to guess the location. First, I need to analyze the image to identify clues about where it was taken. Since I can’t actually see images, I’ll rely on the metadata. I see the resolution is 1024x768, and I’ll consider any textual information if available for clues. Unfortunately, I don’t have access to the image visually, so I’ll proceed with this approach.</p>
</blockquote>
<p>Then it snaps out of that and starts using its vision abilities:</p>
<blockquote>
<p>The image shows a residential neighborhood with small houses, one of which is light green with a white picket fence and a grey roof. The fire pit and signposts hint at a restaurant or cafe, possibly near the coast. The environment, with olive trees and California poppies, suggests a coastal California location, perhaps Central Coast like Cambria or Morro Bay. The pastel-colored houses and the hills in the background resemble areas like Big Sur. A license plate could offer more, but it’s hard to read.</p>
</blockquote>
<p>Now here’s where it leaps straight into science fiction:</p>
<blockquote>
<p>Alright, I’m going to zoom into the license plate to try to confirm the state, as it could help identify the location more precisely. The plate might give me clues about where this photo was taken, especially if it’s from a recognizable state. Let’s see if that’s visible enough to be useful!</p>
</blockquote>
<p>It runs some Python code!</p>
<blockquote>
<pre><span># crop license plate area</span>
<span>w</span>, <span>h</span> <span>=</span> <span>img</span>.<span>size</span>
<span># approximate area: roughly center x maybe 1700? Actually width is likely &gt; 3024? Let&#39;s print size</span>
<span>print</span>(<span>img</span>.<span>size</span>)

(<span>3456</span>, <span>4608</span>)</pre>
</blockquote>
<p>Frustratingly the shared conversation doesn’t include most of the other Python it used (the sharing feature is still quite buggy). Here’s a screenshot of what happened next:</p>
<p><img src="https://static.simonwillison.net/static/2025/zoom-crop.jpg" alt="| want to crop a small bounding box around the license plate. The car behind the picket fence is likely near the center, so I&#39;ll focus on that area. I&#39;ll define the region with approximate coordinates: x 1500 to 2000 and y 2500 to 3000. By selecting this region, I can better evaluate the plate and see if it&#39;s clearer to identify. Let&#39;s manually crop and inspect it. Then it says Analyzing Data, and runs Python code: crop = img.crop( (1300, 2400, 2200, 3200)); plt.imshow (crop); plt.axis( off&#34;); Output is (-0.5, 899.5, 799.5, -0.5). It shows the cropped photo and says: The license plate appears to follow the standard California design — white background with red characters. To be sure, I&#39;ll zoom in on the area around the plate to check for more details and confirm the state. This could give me a stronger clue as to where the photo was taken. Let&#39;s see if it&#39;s clearer now!"/></p>
<p>It kept running like this for <em>six more minutes</em>—cropping and zooming to an area of the photo, musing about what it could see, comparing it to previous observations and then cropping and zooming and analyzing again.</p>
<p>To cut straight to the end, here’s the final answer it came up with:</p>
<blockquote>
<p>I’m getting strong Central-Coast-California vibes from this scene:</p>
<table>
<thead>
<tr>
<th>Clue</th>
<th>What it suggests</th>
</tr>
</thead>
<tbody>
<tr>
<td>California-style license plate on the SUV</td>
<td>We’re almost certainly in CA</td>
</tr>
<tr>
<td>Masses of blooming California poppies, an olive tree, and a cool, marine-layer sky</td>
<td>Typical coastal‐central-CA vegetation and weather</td>
</tr>
<tr>
<td>Low pastel beach cottages with white picket fences and a rope-and-post patio rail</td>
<td>Common architecture in the little beach towns between Santa Cruz and Pismo</td>
</tr>
<tr>
<td>Rolling green coastal range right behind town (no tall peaks, no snow)</td>
<td>Fits the Santa Lucia / Santa Cruz coastal hills</td>
</tr>
</tbody>
</table>
<p>Put together, that points to one of the small Highway 1 villages. The mix of cottages and the immediate backdrop of hills feels most like <strong>Cambria, California</strong> (just north of Morro Bay and Cayucos). I’d guess you snapped this from a café patio in Cambria’s West Village, a block or two in from Moonstone Beach Drive.</p>
<p>(If I’m off, my next guess would be Half Moon Bay–El Granada further north—but Cambria is my first instinct!)</p>
</blockquote>
<p>How did it do? Central Coast California was exactly right. Cambria, California was about 200 miles off... but the “my next guess” was spot on! The photo was indeed taken in El Granada, a tiny town just north of Half Moon Bay.</p>
<p>I’m confident it didn’t cheat and look at the EXIF data on the photograph, because if it had cheated it wouldn’t have guessed Cambria first. If you’re still suspicious, try stripping EXIF by taking a screenshot and run an experiment yourself—I’ve tried this and it still works the same way. <strong>Update:</strong> vessenes on Hacker News <a href="https://news.ycombinator.com/item?id=43732506#43732866">reported</a> an instance where it did use the <a href="https://pillow.readthedocs.io/en/stable/reference/ExifTags.html">ExifTags</a> package and lie about it, but it was at least visible <a href="https://chatgpt.com/share/6802e229-c6a0-800f-898a-44171a0c7de4">in the thinking trace</a>.</p>

<p>o3 certainly isn’t the only model that can do this: I’ve tried similar things with Claude 3.5 and 3.7 Sonnet and been impressed by the results there as well, although they don’t have that extravagant ability to “zoom”.</p>

<p>How much does the zooming actually help? My suspicion is that the model’s vision input operates at quite a low resolution, so cropping closer to a license plate does have a meaningful impact on the process. I’m not sure it justifies 25 separate cropping operations for one photo though, that feels a little performative to me.</p>

<p>Here’s <a href="https://claude.ai/share/231756c9-6fe6-4f63-9f34-c6c7e1299a44">Claude 3.7 Sonnet “extended thinking”</a> guessing “a small to mid-sized California coastal town”. I tried <a href="https://gemini.google.com/">Gemini</a> but it cheated and said “Given that my current location context is El Granada, California...”—so I tried Gemini 2.5 Pro via their API <a href="https://gist.github.com/simonw/6a5a9407326d8366105f95e5524f3694">and got</a> a confidently incorrect guess of “the patio of The Hidden Kitchen restaurant in Cayucos, California”.</p>

<p>What’s different here with o3, <a href="https://simonwillison.net/2025/Apr/21/ai-assisted-search/#o3-and-o4-mini-are-really-good-at-search">as with search</a>, is that the tool usage is integrated into the “thinking” phase of the response.</p>

<p>Tools that can be used as part of that dedicated chain-of-thought sequence are an astonishingly powerful new pattern for these models. I expect we’ll see this from other vendors soon.</p>

<h4 id="what-to-make-of-this-">What to make of this?</h4>
<p>First, this is <strong>really fun</strong>. Watching the model’s thought process as it churns through the photo, pans and zooms and discusses different theories about where it could be is <em>wildly entertaining</em>. It’s like living in an episode of CSI.</p>
<p>It’s also <strong>deeply dystopian</strong>. Technology can identify locations from photographs now. It’s vitally important that people understand how easy this is—if you have any reason at all to be concerned about your safety, you need to know that any photo you share—even a photo as bland as my example above—could be used to identify your location.</p>
<p>As is frequently the case with modern AI, the fact that this technology is openly available to almost anyone has negative and positive implications. As with image generation, it’s important that people can see what this stuff can do first hand. Seeing this in action is a visceral lesson in what’s now possible.</p>

<h4 id="update-o3-does-have-rough-location-access">Update: o3 does have rough location access</h4>
<p>I’m embarrassed that I didn’t think to check this, but it turns out o3 does have <a href="https://chatgpt.com/share/680ceb49-a184-8006-9979-d73169325297">a loose model of your location made available to it now</a>—presumably as part of its improved search feature. It knows I’m in Half Moon Bay.</p>
<p>The location guessing trick still works independently of that though. I tried it on photos
I’d taken thousands of miles from my location (EXIF stripped via screenshotting)—one in rural Madagascar (<a href="https://static.simonwillison.net/static/2025/madagascar.jpg">photo</a>, <a href="https://chatgpt.com/share/680cec52-e0d4-8006-bf80-952888b018fd">transcript</a>), one in urban Buenos Aires (<a href="https://static.simonwillison.net/static/2025/buenos-aires.jpg">photo</a>, <a href="https://chatgpt.com/share/680cec1c-f0c4-8006-86c3-7dc70104bd3f">transcript</a>), and it gave convincing answers for both.</p>

<p>I’ve also seen examples from numerous other people replicating these results for their own diverse collections of photos.</p>


</div>


</div></div>
  </body>
</html>
