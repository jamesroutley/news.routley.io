<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ohnadj.pages.dev/blog/2025/01/rc-batchviz/">Original</a>
    <h1>batches of batch: visualizing RC interests</h1>
    
    <div id="readability-page-1" class="page"><div>  <p data-astro-cid-2q5oecfc="">Tue Jan 07</p> <p data-astro-cid-2q5oecfc=""><em data-astro-cid-2q5oecfc="">in which many words are used to explain a small, ml-based data visualization</em></p>  
<p><img src="https://ohnadj.pages.dev/_astro/rc-batchviz-viz.DZp-StPf_Z1BeroH.webp" alt="A screenshot of a cluster from the final visualization created" width="1166" height="540" loading="lazy" decoding="async"/>
<em><span>Screenshot of my cluster from the final visualization created. My batch-mates are pseudonym-ized.</span></em></p>
<p>In this post, I’ll explain how I used some fundamental machine learning techniques to visualize the interests of my (anonymized) batch-mates at the Recurse Center (RC), at the beginning of our Winter 1, 2024 (W1’24) batch. This project demonstrates:</p>
<ul>
<li>Converting text into numerical representations (embeddings)</li>
<li>Simplifying complex data while preserving relationships (dimensionality reduction)</li>
<li>Finding natural groupings in data (clustering)</li>
<li>Visualizing multidimensional data with Python’s <a href="https://plotly.com/python/" rel="nofollow noopener noreferrer" target="_blank"><code>plotly</code><span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a> library</li>
</ul>
<p>My goal for this post is to provide an accessible explainer for this project, whether you are new to machine learning or just curious about analyzing text data!</p>
<p><em>Disclaimer</em>: This was a quick, fun exercise that I did to warm up my coding muscles at the beginning of batch. As such, I didn’t spend any additional time on improving final outputs here. Aside from the general pitfalls of attempting to group individuals by algorithm, expect the results to be kind of wonky and not tuned for accuracy!</p>
<p>For more on RC, check out the first few paragraphs that I wrote <a href="https://ohnadj.pages.dev/blog/2024/11/rc-reflection-1">here</a>.</p>

<p>“batches of batch” is a lightly interactive 2D/3D visualization that attempts to show how my batch-mates’ interests relate to one another, based on the content of their welcome posts and week 1 introductions. The full source code is available on GitHub: <a href="https://github.com/iconix/rc-batchviz" rel="nofollow noopener noreferrer" target="_blank">iconix/rc-batchviz<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a>.</p>
<p>This viz was inspired by the “Advice and Introductions” call on the first day of W1’24, where RC faculty advised us to take notes because:</p>
<blockquote>
<p>It’s very easy to forget who said the thing you were curious about once everybody’s gone</p>
</blockquote>
<p>This sparked a fun idea: what if I could create a durable “map” of everyone’s interests based on what they shared that day? This could theoretically help us all find potential collaborators and encourage connections.</p>
<p>As a spoiler, here’s what the live visualization looks like, in 2D (easier to manuever on bigger screens):</p>

<p>It shows batch-mates clustered by color, and hovering over points reveals batch-mate pseudonym, cluster assignment, and topical keywords for the cluster. You can also zoom and pan the visualization to be better see the dense clusters (see the controls at the top right). Also, one could toggle a flag on the generating script to render this in 3D (<code>--components 3</code>).</p>

<p>At a very high level:</p>
<blockquote>
<p>I manually scraped some text data, then pushed it through an embedding model + a dimensionality reducer + a clusterer + a topic modeler.</p>
</blockquote>
<p>Neat but huh?! My batch-mate, Karen, basically replied when I gave this description in my daily RC check-in post. Fair enough!</p>
<h3 id="step-1-gather-text"><a aria-hidden="true" tabindex="-1" href="#step-1-gather-text"><span><img src="https://ohnadj.pages.dev/assets/link.svg"/></span></a>Step 1. Gather text</h3>
<blockquote>
<p><code>some text data</code> ➤ my notes from the “Advice and Introductions” call + W1’24 welcome topic posts</p>
</blockquote>
<p>This bit is straightforward enough. I copy-pasted all the welcome posts from our messaging platform Zulip, then added my own notes from the call, to a Google Sheet and exported it to a CSV.</p>
<h3 id="step-2-turn-text-into-numbers-embeddings"><a aria-hidden="true" tabindex="-1" href="#step-2-turn-text-into-numbers-embeddings"><span><img src="https://ohnadj.pages.dev/assets/link.svg"/></span></a>Step 2. Turn text into numbers (embeddings)</h3>
<blockquote>
<p><code>pushed it through an embedding model</code> ➤ converted the text into lists of numbers that capture features (relationships and patterns) from the original data</p>
</blockquote>
<p>Here’s where the ML ‘magic’ starts! Modern machine learning models can translate text (discrete, symbolic, context-dependent text) into a continuous representation that other models, and computers in general, know how to process: that is, a ton of numbers (also known as vectors). The numbers aren’t random, either: they are tuned to encode similarity and relationships between different texts. An embedding model is what performs this encoding process.</p>
<p>Here’s a simplified visualization of how embeddings behave in 2D:</p>
<p><img src="https://ohnadj.pages.dev/_astro/rc-batchviz-embedding.DkYNqGEh_Z2nA3GK.svg" alt="a scatter plot of &#34;cat&#34; &#34;dog&#34; and &#34;airplane&#34; in embedding space" width="600" height="400" loading="lazy" decoding="async"/></p>
<p>In this example, instead of trying to understand “cat” as a word, the model understands it as a point in multidimensional mathematical space. Words with similar meanings or closer relationships appear closer together - so “cat” and “dog” are closer together than “cat” and “airplane” are.</p>
<p>An interesting effect of this is that since they are numbers with meaning, you can do math on words essentially, like measure <a href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="nofollow noopener noreferrer" target="_blank">cosine similarity<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a>. Pretty neat!</p>
<p>The embedding model learns these relationships by seeing lots and lots of examples. Nowadays, text-based models are taught on essentially as many books and as much of the internet as possible (for better or worse). And the teaching occurs by the model predicting some objective (like predicting what words might appear nearby - aka within the context of - the word “cat”), then slightly adjusting its internal numbers (“weights”) to do better at the next round of prediction. Doing better at this objective results in tweaking the numerical representations for co-occurring text to be more and more similar.</p>
<p>It is pretty amazing that models trained from scratch start by making predictions totally at random and then refine from there. If interested in diving deeper into the details, this prediction and adjustment cycle is known as <a href="https://en.wikipedia.org/wiki/Gradient_descent" rel="nofollow noopener noreferrer" target="_blank">gradient descent<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a>.</p>
<p>Anywho, I didn’t have to teach these models from scratch here. Instead I used a pretrained, open-source neural network called Nomic Embed (<code>nomic-embed-text-v1.5</code>, to be exact). This model was chosen because it is <a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5" rel="nofollow noopener noreferrer" target="_blank">available on the Hugging Face model repository<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a>, which makes usage simple, and it has strong performance in academic benchmarks (outperforming OpenAI’s paid embedding model, <a href="https://www.nomic.ai/blog/posts/nomic-embed-text-v1" rel="nofollow noopener noreferrer" target="_blank">thanks to how it was trained<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a>). It can also handle longer text sequences with its 8,192 token context window, although that wasn’t necessary for this project.</p>
<p>The Nomic Embed model converts each person’s associated text data into a 768-dimensional vector - meaning each person, and ideally for our purposes, their interests, are represented by 768 numbers!</p>
<h3 id="step-3-simplify-the-numbers-dimensionality-reduction"><a aria-hidden="true" tabindex="-1" href="#step-3-simplify-the-numbers-dimensionality-reduction"><span><img src="https://ohnadj.pages.dev/assets/link.svg"/></span></a>Step 3. Simplify the numbers (dimensionality reduction)</h3>
<blockquote>
<p><code>+ a dimensionality reducer</code> ➤ an algorithm that takes high-dimensional data and creates a meaningful low-dimensional representation</p>
</blockquote>
<p>So now we have lists of numbers imbued with semantic meaning. These lists of <code>K</code> numbers represent the text they encode in <code>K</code>-dimensional space.</p>
<p><code>K</code> is usually a fairly big number - like a multiple of 128. For <code>nomic-embed-text-v1.5</code>, <code>K=768</code>. So, how to make sense of this 768D web of words? One approach is to “flatten” this web into a simpler map, in a lower visual dimension (2D or 3D), which preserves the most important connections. Think of it like making a visual map of the Earth - you lose some information, but the important relationships (like which countries are near each other) stay mostly intact. This technique is called dimensionality reduction.</p>
<p><a href="https://umap-learn.readthedocs.io/en/latest/" rel="nofollow noopener noreferrer" target="_blank">UMAP<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a>, or Uniform Manifold Approximation and Projection, is the particular algorithm I used in this project for dimensionality reduction. UMAP is good at preserving both local and global structure in the data, which should allow the reduction to generalize better. It also tends to work well with the high-dimensional embeddings produced by modern language models.</p>
<p>Like other machine learning approaches, dimensionality reduction involves learning patterns from data. UMAP uses techniques similar to neural networks (e.g., gradient descent!), optimizing a mathematical objective that balances preserving local and global structure.</p>
<p>Here’s another simplified visualization, this time for dimensionality reduction:</p>
<p><img src="https://ohnadj.pages.dev/_astro/rc-batchviz-dimred.C7apxUyG_PBh0a.svg" alt="simple diagram showing 3D points being mapped to 2D while preserving relationships, with some information loss" width="800" height="400" loading="lazy" decoding="async"/></p>
<p>Note that some relationships between points are distorted in the projection due to information loss. For example, the tight cluster in 3D gets somewhat dispersed in 2D.</p>
<p>You may wonder: what do the dimensions mean after UMAP is applied? Unlike other methods like <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="nofollow noopener noreferrer" target="_blank">Principle Component Analysis<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a> (PCA), where dimensions can sometimes be interpreted, UMAP dimensions don’t have inherent meaning. Instead, they represent abstract combinations of features that UMAP found useful for preserving the important relationships in the high-dimensional space. What matters is the relative positions and distances between points, not the specific coordinate values.</p>
<h3 id="step-4-find-groups-clustering"><a aria-hidden="true" tabindex="-1" href="#step-4-find-groups-clustering"><span><img src="https://ohnadj.pages.dev/assets/link.svg"/></span></a>Step 4. Find groups (clustering)</h3>
<blockquote>
<p><code>+ a clusterer</code> ➤ an algorithm to identify natural groupings in the simplified number space</p>
</blockquote>
<p>Once we have our 2D or 3D representation, we can find natural groups in the data with a clustering algorithm called <a href="https://hdbscan.readthedocs.io/en/latest/" rel="nofollow noopener noreferrer" target="_blank">HDBSCAN<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a> (Hierarchical Density-Based Spatial Clustering of Applications with Noise).</p>
<p>HDBSCAN works by:</p>
<ol>
<li>Finding areas where points are densely packed together</li>
<li>Identifying these dense regions as clusters</li>
<li>Assigning points to their most likely cluster</li>
</ol>
<p>HDBSCAN was chosen over simpler clustering algorithms like <a href="https://en.wikipedia.org/wiki/K-means_clustering" rel="nofollow noopener noreferrer" target="_blank">k-means<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a> because it doesn’t require specifying the number of clusters in advance (which we don’t know!), can identify noise points or “outliers” that don’t belong to any cluster, and can find clusters of varying densities and shapes.</p>
<p>Clustering is a form of unsupervised machine learning, where the algorithm learns to identify patterns and structure in data without being given explicit examples of what constitutes a “correct” grouping. HDBSCAN learns to recognize dense regions in the data that likely represent meaningful groups.</p>
<h3 id="step-5-understand-group-themes-topic-modeling"><a aria-hidden="true" tabindex="-1" href="#step-5-understand-group-themes-topic-modeling"><span><img src="https://ohnadj.pages.dev/assets/link.svg"/></span></a>Step 5. Understand group themes (topic modeling)</h3>
<blockquote>
<p><code>+ a topic modeler</code> ➤ an algorithm to extract key themes and terms that characterize a group of documents</p>
</blockquote>
<p>In this step, text makes a comeback!</p>
<p>For each cluster, we want to understand what common themes (that ideally align with interests) bind it together. We use a technique called topic modeling to automatically extract key themes for each cluster from the underlying text data that now belongs to a specific cluster.</p>
<p>As an exercise and for comparison with automated methods, here is how I manually interpreted the clusters by interest:</p>
<ul>
<li>Cluster 1: Audio/Visual &amp; hardware</li>
<li>Cluster 2: CS fundamentals &amp; machine learning</li>
<li>Cluster 3: Systems programming &amp; hardware</li>
<li>Cluster 4: Graphics &amp; game development</li>
<li>Cluster 5: Web development &amp; creative coding</li>
<li>Cluster 6: Visual tools &amp; applications</li>
</ul>
<p>Topic modeling is another form of unsupervised machine learning that learns to discover abstract topics that occur in a collection of documents. The algorithm identifies which words tend to appear together frequently and uses this information to infer underlying themes or topics.</p>
<p>The results that I liked best here come from a modified <a href="https://jaketae.github.io/study/tf-idf/" rel="nofollow noopener noreferrer" target="_blank">TF-IDF<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a> method, despite it being considered an old school method. The script has a flag (<code>--topic-method</code>) that can be changed to run more modern methods, like KeyBERT.</p>
<p>I did a bit more comparison between the automated topic labeling methods available in my script, versus my own interpretation of the clusters:</p>

<p>So how did topic modeling do? Overall, it often “misses the forest for the trees.” Human interpretation remains valuable for synthesizing coherent, meaningful cluster labels that capture the essence of the grouped content. It just doesn’t scale as well!</p>
<h3 id="step-6-visualize-groups"><a aria-hidden="true" tabindex="-1" href="#step-6-visualize-groups"><span><img src="https://ohnadj.pages.dev/assets/link.svg"/></span></a>Step 6. Visualize groups</h3>
<p>The final step uses <a href="https://plotly.com/python/" rel="nofollow noopener noreferrer" target="_blank">Plotly<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a> to create an interactive visualization where:</p>
<ul>
<li>Points represent individual batch-mates</li>
<li>Colors indicate cluster membership</li>
<li>Distance between points represents similarity of interests</li>
<li>Hovering over points reveals pseudonyms, cluster membership, and key topics</li>
<li>The visualization can be zoomed and panned to explore different regions</li>
</ul>
<p>The interactive bit of the visualization allows for better visualizing the clusters, which can be a bit dense, as well as exploration of the relationships between different clusters and individual data points.</p>
<p>Quick asides: The only data point without a pseudonym is my own: <code>Nadja Rhodes</code>. I also had lots of dialogue with <a href="https://claude.ai/" rel="nofollow noopener noreferrer" target="_blank">Claude<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a> in order to generate the Plotly code.</p>
<h3 id="all-together-now"><a aria-hidden="true" tabindex="-1" href="#all-together-now"><span><img src="https://ohnadj.pages.dev/assets/link.svg"/></span></a>All together now</h3>
<p>Here’s a flow chart that puts the full pipeline together with a bit more detail. Data flows through the different steps, and each step performs a specific task, as described in the steps above.</p>
<p><img src="https://ohnadj.pages.dev/_astro/rc-batchviz-pipeline.Bi1zp07F_ZveafJ.svg" alt="full machine learning pipeline visualized" width="2314" height="222" loading="lazy" decoding="async"/></p>
<p>The first full pipeline run, from CSV, takes about a minute and a half on my laptop (no GPU) via the Python script.</p>

<p>So how representative of interests is this viz, really? Across the board, not terribly but also not spectacularly. The clusters seem fairly coherent after a brief inspection, but the topic labels to help with interpretation are lackluster.</p>
<p>Although, as seen in the first screenshot of this post, my particular cluster with topics <code>web, personal, server, building, creative</code> feels pretty spot-on (“works on my <del>machine</del> data” <em>*closes laptop*</em>).</p>
<p>Here are some other interesting patterns and data artifacts that I found:</p>
<ul>
<li>A “no welcome post” cluster (Cluster 4) emerged containing batch-mates who did not write a welcome post, clearly showing how the model picked up on more than just interests. If it can cluster around missing data, what else may it have picked up on?</li>
<li>Clusters are not set in stone and can change between runs, due to elements of randomness in both the UMAP and HDBSCAN algorithms (UMAP initializes with a random state and makes random choices during optimization; HDBSCAN does some random tie-breaking and can vary how it builds its cluster hierarchy)</li>
<li>Cluster boundaries are highly dependent on the HDBSCAN parameters selected. Subjectivity, interpretation, and experimentation were important, and it wasn’t just “plug and play” - I used my own judgment to tune parameters.</li>
<li>The meanings of clusters can be tricky to pin down automatically</li>
<li>Some information is inevitably lost during dimensionality reduction:
<ul>
<li>Data points that clustered together in higher dimensions could end up far apart in the 2D/3D visualization</li>
<li>Distinct groups might get merged if their distinguishing features were lost</li>
</ul>
</li>
<li>Topics like <code>rap</code> showing up (Cluster 3) are likely due to how the text tokenizer breaks up words. I checked, and no one mentioned rap music. So an interesting artifact!</li>
<li>Love my pseudonym generator 😍 Let’s goo <code>Exuberant Teapot</code></li>
</ul>
<p>Future improvements could include:</p>
<ul>
<li>Incorporating data from another Zulip scrape on actual interactions and collaborations during batch and seeing how the clusters change</li>
<li>Adding more interactive features and modifiable parameters to explore the data and techniques</li>
<li>Providing more raw data on hover: interests, background, and whether or not they filled out each data source, etc.</li>
<li>Using Sparse Autoencoders (SAEs) to potentially get more interpretable features from the embeddings

</li>
</ul>
<h3 id="notes-on-implementation"><a aria-hidden="true" tabindex="-1" href="#notes-on-implementation"><span><img src="https://ohnadj.pages.dev/assets/link.svg"/></span></a>Notes on implementation</h3>
<p>This project used the following Python packages:</p>
<ul>
<li><code>transformers</code> and <code>torch</code> for loading and running the embedding model</li>
<li><code>umap-learn</code> for dimensionality reduction</li>
<li><code>hdbscan</code> for clustering</li>
<li><code>plotly</code> and <code>scipy</code> for interactive visualization</li>
<li><code>pandas</code> and <code>numpy</code> for data processing</li>
<li><code>parquet</code> and <code>datasets</code> for efficient data storage of embeddings</li>
<li><code>keybert</code> and <code>sentence-transformers</code> for testing out more modern topic modeling methods</li>
</ul>
<p>Here is a direct link to the <a href="https://iconix.github.io/rc-batchviz/interactive_viz.html" rel="nofollow noopener noreferrer" target="_blank">final visualization<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a>. The full implementation is available on GitHub: <a href="https://github.com/iconix/rc-batchviz" rel="nofollow noopener noreferrer" target="_blank">iconix/rc-batchviz<span><img src="https://ohnadj.pages.dev/assets/external-link.svg" alt="External link icon"/></span></a>. I don’t share the underlying data used because it was shared in internal communications.</p>
<p>That’s all, folks! Thanks for reading.</p>

<p>Special thanks to:</p>


<p>In machine learning nowadays, things change so fast that finding the most up-to-date resources, pretrained models, and libraries can be half the battle. The following resources helped me out a lot in this respect:</p>
    </div></div>
  </body>
</html>
