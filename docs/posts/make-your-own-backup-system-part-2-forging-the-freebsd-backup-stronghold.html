<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://it-notes.dragas.net/2025/07/29/make-your-own-backup-system-part-2-forging-the-freebsd-backup-stronghold/">Original</a>
    <h1>Make Your Own Backup System â€“ Part 2: Forging the FreeBSD Backup Stronghold</h1>
    
    <div id="readability-page-1" class="page"><div><div role="main" itemprop="articleBody"><h2>Laying the Foundation</h2><p>With the <a href="https://it-notes.dragas.net/2025/07/18/make-your-own-backup-system-part-1-strategy-before-scripts/">primary backup strategies and methodologies introduced</a>, we&#39;ve reached the point where we can get specific: the Backup Server configuration.</p><p>When choosing the type of backup server to use, I tend to favor specific setups: either I trust a professional backup service provider (like Colin Percival&#39;s <a href="https://www.tarsnap.com/">Tarsnap</a>), or I want full control over the disks where the backups will be hosted. In both cases, for the past twenty years, my operating system of choice for backup servers has been FreeBSD. With a few rare exceptions for clients with special requests, it covers all my needs. When I require Linux-based solutions, such as the <a href="https://www.proxmox.com/en/products/proxmox-backup-server/overview">Proxmox Backup Server</a>, I create a VM and manage it within.</p><p>I typically use both IPv4 and IPv6. For IPv4, I &#34;play&#34; with NAT and port forwarding. For IPv6, I tend to assign a public IPv6 address to each jail or VM, which is then filtered by the physical server&#39;s firewall. Unfortunately, every provider, server, and setup has a different approach to IPv6, making it impossible to cover them all in this article. When a provider allows for routed setups, I use this approach: <a href="https://it-notes.dragas.net/2023/09/23/make-your-own-vpn-freebsd-wireguard-ipv6-and-ad-blocking-included/">Make your own VPN: FreeBSD, WireGuard, IPv6, and ad-blocking included</a> - assigning a /72 to the bridge for the jails and VMs.</p><p>In my opinion, FreeBSD is a perfect all-rounder for backups, thanks to its ability to completely partition services. You can separate backup services (or specific servers/clients) into different jails or even VMs. Furthermore, using ZFS greatly enhances both flexibility and the range of tools you can use.</p><p>The main distinction is usually between local backup servers (physically accessible, though not always attended, and in locations deemed secure) and remote ones, such as leased external servers. I personally use a combination of both. If the services I need to back up are external, in a datacenter, and need to be quickly restorable, I prefer to always have a copy on another server in a different datacenter with good outbound connectivity. This guarantees good bandwidth for restores, which isn&#39;t always available from a local connection to the outside world. However, an internal, nearby, and accessible backup server (even a Raspberry Pi or a mini PC) ensures physical access to the data. Whenever possible, I maintain both an external and an internal copy - and they are autonomous, meaning the internal copy is <em>not</em> a replica of the external one, but an additional, independent backup. This ensures that if a problem occurs with the external backup, it won&#39;t automatically propagate to the internal one. In any case, the backup must always be in a different datacenter from the one containing the production data. When <a href="https://www.reuters.com/article/idUSKBN2B20NT/">the fire at the OVH datacenter in Strasbourg</a> caused the entire complex to shut down, many people found themselves in trouble because their backups were in the same, now unreachable, location. I had a copy with another provider, in a different datacenter and country, as well as a local copy.</p><p>Despite it being &#34;just&#34; a backup server, I almost always use some form of disk redundancy. If I have two disks, I set up a mirror. With three or more, I use RaidZ1 or RaidZ2. This is because, in my view, backups are nearly as important as production data. The inability to recover data from a backup means it&#39;s lost forever. And it happens often, very often, that someone contacts me to recover a file (or a database, etc.) days or weeks after its accidental loss or deletion. Usually, pulling out a file from a two-month-old backup generates a mix of disbelief, admiration, but above all, a sense of security in the person requesting it. And that is what our work should instill in the people we collaborate with.</p><p>The backup server should be hardened. If possible, it should be protected and unreachable from the outside. My best backup servers are those accessible only via VPN, capable of pulling the data on their own. If they are on a LAN, it&#39;s even better if they are completely disconnected from the Internet.</p><p>For this very reason, <strong>backups must always be encrypted</strong>. Having a backup means having full access to the data, and the backup server is the prime target for being breached or stolen if the goal is to get your hands on that data. I&#39;ve seen healthcare facilities&#39; backup servers being targeted (in a rather trivial way, to be honest) by journalists looking for health details of important figures. It is therefore critical that the backup server be as secure as possible.</p><p>Based on the type of access, I use two types of encryption:</p><ul><li><strong>If the server is local</strong> (especially if the ZFS pool is on external disks), I usually install FreeBSD on UFS in read-only mode, as <a href="https://it-notes.dragas.net/2024/05/31/freebsd-tips-and-tricks-native-ro-rootfs/">I&#39;ve described in a previous article</a>, and encrypt the backup disks with <a href="https://man.freebsd.org/cgi/man.cgi?geli(8)">GELI</a>. This ensures that in the event of a &#34;dirty&#34; shutdown (more likely in unattended environments), I can reconnect to the host and then reactivate the ZFS pool. This approach makes it nearly impossible to retrieve even the pool&#39;s metadata if the disks are stolen, as GELI performs a full-device encryption. For example, an employee of a company I work with stole one of the secondary backup disks (which was located at a different, unmonitored company site) to steal information. He got nothing but a criminal complaint. With this approach, it&#39;s also not necessary to further encrypt the datasets, which avoids some issues (which I&#39;ll discuss later, in a future post).</li><li><strong>If the server is remote</strong>, in a datacenter, I usually use ZFS native encryption, encrypting the main backup dataset (and <a href="https://bastillebsd.org/">BastilleBSD</a>&#39;s, if applicable). Consequently, all child datasets containing backups will also be encrypted. In this case as well, a password will be required after a reboot to unlock those datasets, ensuring that the data cannot be extracted if control of the disks is lost.</li></ul><p>Here is an example of how to use GELI to encrypt an entire partition and then create a ZFS pool on it (in the example, the disk is <code>da1</code> - do not follow these commands blindly, or you will erase all content on the <code>da1</code> device!):</p><pre><code># WARNING: This destroys the existing partition table on disk da1
gpart destroy -F da1

# Create a new GPT partition table
gpart create -s gpt da1

# Add a freebsd-zfs partition that spans the entire disk
# The -a 1m flag ensures proper alignment
gpart add -t freebsd-zfs -a 1m da1

# Initialize GELI encryption on the new partition (da1p1)
# We use AES-XTS with 256-bit keys and a 4k sector size
# The -b flag means &#34;boot,&#34; prompting for the passphrase at boot time
geli init -b -l 256 -s 4096 da1p1
# You will be prompted for a passphrase: choose a strong one and save it!

# Attach the encrypted partition. A new device /dev/da1p1.eli will be created.
# You will be prompted for the passphrase you just set
geli attach da1p1

# (Optional) Check the status of the encrypted device
geli status da1p1

# Create the ZFS pool &#34;bckpool&#34; on the encrypted device
# We enable zstd compression (an excellent compromise) and disable atime
zpool create -O compression=zstd -O atime=off bckpool da1p1.eli
</code></pre><p>In this setup, the reference pool for everything related to backups will be <code>bckpool</code> - and you&#39;ll need to keep this in mind for the next steps. Additionally, after every server reboot, you&#39;ll need to &#34;unlock&#34; the disk and import the pool:</p><pre><code># Enter the passphrase when prompted
geli attach da1p1

# Import the ZFS pool, which is now visible
zpool import bckpool
</code></pre><p>With this method, it&#39;s not necessary to encrypt the ZFS datasets, as the underlying disk (or, more precisely, the partition containing the ZFS pool) is already encrypted.</p><p>If, instead, you choose to encrypt the ZFS dataset (for example, if you install FreeBSD on the same disks that will hold the data and don&#39;t want to use a multi-partition approach), you should create a base encrypted dataset. Inside it, you can create the various backup datasets, VMs, and the BastilleBSD mountpoint. Due to property inheritance, they will all be encrypted as well.</p><p>To create an encrypted dataset, a command like this will suffice:</p><pre><code># Creates a new dataset with encryption enabled.
# keylocation=prompt will ask for a passphrase every time it&#39;s mounted.
# keyformat=passphrase specifies the key type.
zfs create -o encryption=on -o keylocation=prompt -o keyformat=passphrase zfspool/dataset
</code></pre><p>In this case, after every reboot, you will need to load the key and mount the dataset:</p><pre><code>zfs load-key zfspool/dataset
zfs mount zfspool/dataset
</code></pre><p>Keep in mind the setup you choose, as many of the subsequent choices and commands will depend on it.</p><h2>Base System Setup</h2><p>I&#39;ll install BastilleBSD - a useful tool for separating services into jails. It will be helpful for isolating our backup services:</p><pre><code>pkg install -y bastille
</code></pre><p>If you used ZFS for the root filesystem, you can proceed directly with the setup. Otherwise (i.e., ZFS on other disks), you&#39;ll need to edit the <code>/usr/local/etc/bastille/bastille.conf</code> file and specify the correct dataset on which to install the jails. Then run:</p><pre><code>bastille setup
</code></pre><p>Once the automatic setup is complete, check the <code>/etc/pf.conf</code> file - it will be automatically configured to only accept SSH connections. Ensure the network interface is set correctly. When you activate <code>pf</code>, you will be kicked out of the server, but you can then reconnect.</p><pre><code>service pf start
</code></pre><p>Let&#39;s bootstrap a FreeBSD release for the jails - this will be useful later.</p><pre><code>bastille bootstrap 14.3-RELEASE update
</code></pre><p>Now, we create a local bridge. Jails and VMs can be attached to it, making them fully autonomous. Using VNET jails, for example, will allow the creation of VPNs or <code>tun</code> interfaces inside them, simplifying potential future setups (and increasing security by using a dedicated network stack).</p><p>Modify the <code>/etc/rc.conf</code> file and add:</p><pre><code># Add lo1 and bridge0 to the list of cloned interfaces
cloned_interfaces=&#34;lo1 bridge0&#34;
# Assign an IP address and netmask to the bridge
ifconfig_bridge0=&#34;inet 192.168.0.1 netmask 255.255.255.0&#34;
# Enable gateway functionality for routing
gateway_enable=&#34;yes&#34;
</code></pre><p>Let&#39;s also modify <code>/etc/pf.conf</code> to allow the <code>192.168.0.0/24</code> subnet to access the Internet via NAT. We will skip packet filtering on <code>bridge0</code> and enable NAT. This isn&#39;t the most secure setup, but it&#39;s sufficient to get started:</p><pre><code>#...
# Skip PF processing on the internal bridge interface
set skip on bridge0
#...
# NAT traffic from our internal network to the outside world
nat on $ext_if from 192.168.0.0/24 to any -&gt; ($ext_if:0)
#...
</code></pre><p>To ensure the new settings are correct, it&#39;s a good idea to test with a reboot.</p><p>Since I often configure <a href="https://github.com/freebsd/vm-bhyve">vm-bhyve</a> in my setups, I prefer to install it right away, creating the dataset that will contain the VMs and installation templates. Remember that <code>zroot</code> is only valid if you installed the entire system on ZFS; otherwise, you&#39;ll need to change it to your own dataset:</p><pre><code># Install required packages
pkg install vm-bhyve grub2-bhyve bhyve-firmware
# Create a dataset to store VMs
zfs create zroot/VMs
# Enable the vm service at boot
sysrc vm_enable=&#34;YES&#34;
# Set the directory for VMs, using the ZFS dataset
sysrc vm_dir=&#34;zfs:zroot/VMs&#34;
# Initialize vm-bhyve
vm init
# Copy the example templates
cp /usr/local/share/examples/vm-bhyve/* /zroot/VMs/.templates/
</code></pre><p>At this point, I usually enable the console via <code>tmux</code>. This means that when a VM is launched, it won&#39;t open a VNC port by default, but a <code>tmux</code> session connected to the VM&#39;s serial port. Let&#39;s install and configure <code>tmux</code>:</p><pre><code>pkg install -y tmux
vm set console=tmux
</code></pre><p>Let&#39;s also attach the switch we created (<code>bridge0</code>) to <code>vm-bhyve</code> so we can use it:</p><pre><code>vm switch create -t manual -b bridge0 public
</code></pre><p>Now, <code>vm-bhyve</code> is ready.</p><p>The basic infrastructure is complete. We now have:</p><ul><li><strong>ZFS</strong> to ensure data integrity, which will also handle redundancy, etc.</li><li><strong>BastilleBSD</strong> to manage jails, useful for backing up Linux, NetBSD, OpenBSD, and non-ZFS FreeBSD machines.</li><li><strong>vm-bhyve</strong> to install specific systems (like Proxmox Backup Server).</li></ul><h2>Backup Strategies</h2><p>I use various backup tools, too many to list in this article. So I&#39;ll make a broad distinction, describing how to use this server to achieve our goal: securing data.</p><ul><li>For <strong>FreeBSD servers with ZFS</strong> (hosts, VMs, jails, hypervisors, and their respective VMs), I use an extremely useful, efficient, and reliable tool: <a href="https://github.com/psy0rz/zfs_autobackup">zfs-autobackup</a>.</li><li>For <strong>Linux servers (without ZFS), NetBSD, OpenBSD</strong>, etc. (any non-ZFS OS), I usually use <a href="https://www.borgbackup.org/">BorgBackup</a>. There are other fantastic tools like <a href="https://restic.net/">restic</a>, <a href="https://kopia.io/">Kopia</a>, etc., but BorgBackup has never let me down and has served me well even on low-power devices and after incredibly complex disasters.</li><li>For <strong>Proxmox servers</strong> (a solution I&#39;ve used with satisfaction in production since 2013, although I&#39;m recently migrating to FreeBSD/bhyve where possible), I use two possible alternatives (often both at the same time): if the storage is ZFS, I use the <code>zfs-autobackup</code> approach. In either case, the most practical solution is the Proxmox Backup Server. And the Proxmox Backup Server is one of the reasons I proposed installing <code>vm-bhyve</code>: running it in a VM and storing the data on the FreeBSD host gives you the best of both worlds. Some time ago, I tried running it in a FreeBSD jail (via Linuxulator), but it didn&#39;t work.</li></ul><h3>Backups using zfs-autobackup</h3><p><code>zfs-autobackup</code> is an extremely useful and effective tool. It allows for &#34;pull&#34; type backups, as well as having an intermediary host that connects to both the source and destination, which is useful if you don&#39;t want direct contact between the source and destination. I won&#39;t describe the latter setup, but the documentation is clear, and I have several of them in production, ensuring that the production server and its backup server cannot communicate with each other.</p><p>I usually create a dataset for each server and instruct <code>zfs-autobackup</code> to keep that server&#39;s backups in that dataset. The snapshots taken and transferred will all be from the same instant, so as not to create a time skew (some tools of this kind snapshot a dataset, then transfer it, which can result in minutes of difference between two different datasets from the same server).</p><p>I&#39;ve described in detail how I perform this type of backup in a <a href="https://it-notes.dragas.net/2022/05/30/how-we-are-migrating-many-of-our-servers-from-linux-to-freebsd-part-2/">previous post</a>, so I suggest reading that post for reference.</p><p>Let&#39;s install zfs-autobackup on the FreeBSD server:</p><pre><code>pkg install py311-zfs-autobackup mbuffer
</code></pre><h3>Backups for other servers using BorgBackup</h3><p>When I don&#39;t have ZFS available or need to perform a file-based backup (all or partial), I use a different technique. <code>BorgBackup</code> backups are primarily &#34;push&#34; based, meaning the client will connect to the backup server. This is not optimal or the most secure approach, as the backup server should, in theory, be hardened. Even when protecting everything via VPN, the risk remains that a compromised server could connect to its backup server and alter or delete the backups. I have seen this happen in ransomware cases (especially in the Microsoft world), and so I try to be careful to minimize this type of problem, mainly through snapshots of the backup server (an operation that will be described later).</p><p>To ensure the highest possible security, I create a FreeBSD jail on the backup server for each server I need to back up. The advantage of this approach is the complete separation of all servers from each other. By using a regular user inside a jail, a compromised server that connects to its backup server would only be able to reach its own backups, as it would be confined to a user account and, even if it managed to escalate privileges, still be inside a jail.</p><p>Let&#39;s say, for example, we want to back up a server called &#34;ServerA&#34; (great imagination, I know). We create a dedicated jail on the backup server:</p><pre><code># Create a new VNET jail named &#34;servera&#34; attached to our bridge
bastille create -B servera 14.3-RELEASE 192.168.0.101/24 bridge0
</code></pre><p>BastilleBSD will automatically set the host&#39;s gateway for the jail. In our case, this is incorrect, so we need to modify it and set the jail&#39;s gateway to <code>192.168.0.1</code> in the <code>/usr/local/bastille/jails/servera/root/etc/rc.conf</code> file:</p><pre><code># ...
defaultrouter=&#34;192.168.0.1&#34;
# ...
</code></pre><p>Restart the jail and connect to it:</p><pre><code>bastille restart servera
bastille console servera
</code></pre><p>Now, inside the jail, we install <code>borgbackup</code>:</p><pre><code>pkg install py311-borgbackup
</code></pre><p>BorgBackup doesn&#39;t run a daemon; it&#39;s launched by the remote server (which sends its data to the backup server), so it&#39;s important that the installed version is compatible with the one on the remote host.</p><p>Since we&#39;ll be using SSH, let&#39;s enable it:</p><pre><code>service sshd enable
service sshd start
</code></pre><p>And create a non-privileged user for this purpose:</p><pre><code># The &#39;adduser&#39; utility provides an interactive way to create a user.
root@servera:~ # adduser
Username: servera
Full name: Server A
Uid (Leave empty for default): 
Login group [servera]: 
Login group is servera. Invite servera into other groups? []: 
Login class [default]: 
Shell (sh csh tcsh nologin) [sh]: 
Home directory [/home/servera]: 
Home directory permissions (Leave empty for default): 
Use password-based authentication? [yes]: 
Use an empty password? (yes/no) [no]: 
Use a random password? (yes/no) [no]: yes
Lock out the account after creation? [no]: 
Username    : servera
Password    : &lt;random&gt;
Full Name   : Server A
Uid         : 1001
Class       : 
Groups      : servera 
Home        : /home/servera
Home Mode   : 
Shell       : /bin/sh
Locked      : no
OK? (yes/no) [yes]: yes
adduser: INFO: Successfully added (servera) to the user database.
adduser: INFO: Password for (servera) is: JIkdq8Ex
</code></pre><p>The user is created and can receive SSH connections. After setting everything up, I suggest disabling password-based login in the jail&#39;s SSH configuration, using only public key authentication.</p><p>As mentioned, the biggest risk of a &#34;push&#34; backup is that a compromised client could access the backup server and delete or encrypt the backup history, rendering it useless.</p><p>To drastically mitigate this risk, we can configure SSH to force the client to operate in a special Borg mode called <strong>append-only</strong>. In this mode, the SSH key used by the client will only have permission to create new archives, not to read or delete old ones. However, this approach could complicate some client-side operations (like <code>mount</code>, <code>prune</code>, etc.), forcing them to be done on the server. For this reason, I won&#39;t describe it in this setup, &#34;limiting&#34; myself to taking snapshots of the repositories. It can be a very good practice, so I recommend considering it.</p><p>Let&#39;s initialize the BorgBackup repository. In this example, for simplicity, I won&#39;t set up repository encryption. If the jails are on an encrypted dataset or GELI-encrypted disks, there will still be data encryption on the disks, but there will be no protection against someone who could physically access the server while the disks are mounted. As usual, security is like an onion: every layer helps. Personally, I suggest enabling and using it ALWAYS.</p><pre><code># Switch to the new user
su -l servera
# Initialize a new Borg repo named &#34;servera&#34; with no encryption (for this example)
borg init -e none servera
</code></pre><p>The jail is ready, but it&#39;s unreachable from the outside. There are two ways to make it accessible:</p><ul><li><strong>Install a VPN system inside the jail itself.</strong> Using tools like Zerotier or Tailscale (which don&#39;t need to expose ports) will immediately create the conditions to connect to the jail, which will remain inaccessible from the outside. As the jail is a VNET jail, we&#39;re free to choose any of the supported VPN technologies.</li><li><strong>Expose a port on the backup server</strong>, i.e., on the host, to allow external connections. Many advise against this path as they consider it less secure. It is, but sometimes we don&#39;t have the luxury of installing whatever we want on the server we&#39;re backing up.</li></ul><p>To expose the port, go back to the host and modify the <code>/etc/pf.conf</code> file, creating the <code>rdr</code> and <code>pass</code> rules to let packets in:</p><pre><code># ...
# Redirect incoming traffic on port 1122 to the jail&#39;s SSH port (22)
rdr on $ext_if inet proto tcp from any to any port = 1122 -&gt; 192.168.0.101 port 22
# ...
# Allow incoming traffic on port 1122
pass in inet proto tcp from any to any port 1122 flags S/SA keep state
</code></pre><p>Reload the <code>pf</code> configuration:</p><pre><code>service pf reload
</code></pre><p>The jail will now be reachable on the server&#39;s public IP, on port 1122. Obviously, this port number is for illustrative purposes, and I used <code>from any</code>, but for better security, you should replace <code>any</code> with the IP address of the server that will be connecting to perform the backup.</p><p>By repeating this process for each server and creating a separate jail for each, you can have isolated jails in separate datasets with their backups, potentially setting space limits using ZFS quotas.</p><p>It&#39;s important to remember that backing up a live filesystem (i.e., without a snapshot or dumps) has a very high probability of being impossible to restore completely. Databases hate this approach because files will change while being copied and tend to get corrupted. Of course, it depends on the nature of the data (a backup of a static website will have no issues, but a WordPress database probably will), but it&#39;s crucial to think about a technique to snapshot the filesystem before proceeding. For example, I have already written about how to create snapshots on FreeBSD with UFS in a previous article: <a href="https://it-notes.dragas.net/2024/06/04/freebsd-tips-and-tricks-creating-snapshots-with-ufs/">FreeBSD tips and tricks: creating snapshots with UFS</a>.</p><p>I will cover other operating systems in a future, dedicated post.</p><h3>Proxmox Backup Server in a Dedicated VM</h3><p>Starting with version 4.0 (which is still in beta at the time of this writing), Proxmox Backup Server (PBS) supports storing its data in an S3 bucket. This is excellent news as it decouples the server from the data. There are great open-source S3 implementations, like <a href="https://min.io/">Minio</a> or <a href="https://github.com/seaweedfs/seaweedfs">SeaweedFS</a>, which allow for clustering, replication, etc. In this &#34;simple&#34; case, we will install Proxmox Backup Server in a small VM, while for the data, we&#39;ll install Minio in a native FreeBSD jail. The advantage is undeniable: the VM will only serve as an &#34;intermediary&#34;, but the data will rest directly on the FreeBSD host&#39;s dataset, natively. It will also be possible to specify other external endpoints, other repositories, etc.</p><p>As a philosophy, I tend not to use external providers unless for specific needs, so installing Minio in a jail is a perfect solution to manage this situation.</p><p>Let&#39;s install PBS by downloading the ISO from their website (https://enterprise.proxmox.com/iso/) - at this moment, the version that supports this setup is 4.0 Beta.</p><p>The directory to download to is the <code>vm-bhyve</code> ISOs directory. It&#39;s not strictly necessary, but it&#39;s useful for not &#34;losing&#34; it somewhere. So, go to the directory and download it:</p><pre><code>cd /zroot/VMs/.iso
fetch https://enterprise.proxmox.com/iso/proxmox-backup-server_4.0-BETA-1.iso
</code></pre><p>Now let&#39;s create a VM with <code>vm-bhyve</code>. We can start from the Debian template, but we&#39;ll make some modifications to optimize performance. In this example, I&#39;m giving it 30 GB of disk space, 2 GB of RAM, and 2 cores.</p><p>If you want to store all backups inside the VM, you&#39;ll need to size the virtual disk correctly (or create and attach another one). In this case, I will focus on the &#34;clean&#34; VM that will store its data on a dedicated jail with Minio.</p><pre><code>vm create -t debian -s 30G -m 2G -c 2 pbs
</code></pre><p>Once the empty VM is created, let&#39;s modify its options:</p><pre><code>vm configure pbs
</code></pre><p>We will modify the VM to be UEFI and to use the NVME disk driver - bhyve <a href="https://it-notes.dragas.net/2024/06/10/proxmox-vs-freebsd-which-virtualization-host-performs-better/">performs significantly better on NVME than virtio, as previously tested</a>:</p><pre><code>loader=&#34;uefi&#34;
cpu=&#34;2&#34;
memory=&#34;2G&#34;
network0_type=&#34;virtio-net&#34;
network0_switch=&#34;public&#34;
disk0_type=&#34;nvme&#34;
disk0_name=&#34;disk0.img&#34;
</code></pre><p>Fortunately, the Proxmox team has provided for the installation of the Backup Server on devices without a graphical interface, so the boot menu will allow installation via serial console. Let&#39;s launch the installation and connect to the virtual serial console:</p><pre><code>cd /zroot/VMs/.iso
vm install pbs proxmox-backup-server_4.0-BETA-1.iso
vm console pbs
</code></pre><p>Select the installation via <strong>Terminal UI (serial console)</strong> and proceed normally as if it were a physical host, assigning an IPv4 address from the <code>192.168.0.x</code> range (in this example, I&#39;ll use <code>192.168.0.3</code>).</p><p>This way, the Proxmox Backup Server will run in a VM, with the ability to take snapshots before updates, etc., without any worries.</p><p>Once the installation is complete, PBS will reboot and listen on port 8007 of its IP. Again, as with the jails, we have two options: install a VPN system within the VM itself (thus exposing it automatically only on that VPN - generally a more secure operation) or expose port 8007 on the server&#39;s public IP.</p><p>In the latter case, add the relevant lines to the <code>/etc/pf.conf</code> file on the FreeBSD backup server:</p><pre><code># ...
# Redirect incoming traffic on port 8007 to the PBS VM&#39;s web interface
rdr on $ext_if inet proto tcp from any to any port = 8007 -&gt; 192.168.0.3 port 8007
# ...
# Allow that traffic to pass
pass in inet proto tcp from any to any port 8007 flags S/SA keep state
</code></pre><p>Reload the <code>pf</code> configuration:</p><pre><code>service pf reload
</code></pre><p>The PBS VM configuration is complete. If you chose to use the PBS&#39;s internal disk as a repository, no further operations are necessary (other than the normal repository creation, etc., within PBS).</p><p>In this case, however, we will use a different approach.</p><h4>Creating a Minio Jail as a Data Repository for PBS</h4><p>This approach, in my opinion, has a number of important advantages. The first is that Minio will run in a dedicated jail on the host, at full performance, and will store the data directly on the physical ZFS datapool, thus removing any other layer in between. This jail could potentially be moved to other hosts (by connecting PBS and the jail via VPN or public IP), made redundant thanks to all of Minio&#39;s features, etc. Another solution I am successfully testing (in other setups) is SeaweedFS.</p><p>Let&#39;s create a dedicated jail with Minio and put it on the bridge, so that PBS can access it on the LAN.</p><pre><code>bastille create -B minio 14.3-RELEASE 192.168.0.11/24 bridge0
</code></pre><p>If not configured directly, BastilleBSD will use the host&#39;s gateway for the jail, which is incorrect in this case. So let&#39;s go modify it and restart the jail. Enter the jail with:</p><pre><code>bastille console minio
</code></pre><p>And modify the <code>/etc/rc.conf</code> file to have the correct gateway (following the example addresses):</p><pre><code># ...
ifconfig_vnet0=&#34; inet 192.168.0.11/24 &#34;
defaultrouter=&#34;192.168.0.1&#34;
# ...
</code></pre><p>Exit the jail and restart it:</p><pre><code>bastille restart minio
</code></pre><p>Enter the jail and install Minio:</p><pre><code>bastille console minio
pkg install -y minio
</code></pre><p>Minio is already able to start, but PBS, even on the LAN, wants an encrypted connection. Fortunately, there&#39;s a handy tool that can generate the certificates for us:</p><pre><code># Download the certgen tool
fetch https://github.com/minio/certgen/releases/latest/download/certgen-freebsd-amd64

# Make it executable and run it for our jail&#39;s IP
chmod a+rx certgen-freebsd-amd64
./certgen-freebsd-amd64  -host &#34;192.168.0.11&#34;

# Create the necessary directories and set permissions
mkdir -p /usr/local/etc/minio/certs
cp private.key public.crt /usr/local/etc/minio/certs/
chown -R minio:minio /usr/local/etc/minio/certs/
</code></pre><p>Let&#39;s view the certificate&#39;s fingerprint. Since it&#39;s self-signed, we&#39;ll need it for PBS later. For security reasons, PBS will ask for the fingerprint of non-directly verifiable certificates. Run the following command and take note of the result:</p><pre><code>openssl x509 -in /usr/local/etc/minio/certs/public.crt -noout -fingerprint -sha256
</code></pre><p>At this point, enable and configure Minio in <code>/etc/rc.conf</code>. <strong>WARNING</strong>: The username and password (access key and secret) used in this example are insecure and for testing purposes only. It is strongly recommended to use different values:</p><pre><code># Enable Minio service
minio_enable=&#34;YES&#34;
# Set the address for the Minio console
minio_console_address=&#34;:8751&#34;
# Set the root user and password as environment variables
minio_env=&#34;MINIO_ROOT_USER=testaccess MINIO_ROOT_PASSWORD=testsecret&#34;
</code></pre><p>Start Minio:</p><pre><code>service minio start
</code></pre><p>If everything went correctly, Minio is now running (with its certificates) and ready to receive connections.</p><p>It&#39;s now time to create the bucket(s) that PBS will use. There are several ways to do this, but to test that everything is working and to configure PBS, I suggest connecting via an SSH tunnel.</p><pre><code># Create an SSH tunnel from your local machine to the backup server
# Port 8007 is forwarded to the PBS web UI
# Port 8751 is forwarded to the Minio console
ssh user@backupServerIP -L8007:192.168.0.3:8007 -L8751:192.168.0.11:8751
</code></pre><p>This way, we&#39;ll create a tunnel between the FreeBSD backup server and our workstation, mapping <code>127.0.0.1:8007</code> to <code>192.168.0.3:8007</code> (the PBS web interface) and <code>127.0.0.1:8751</code> to <code>192.168.0.11:8751</code> (the Minio console port).</p><p>Now, connect to <code>https://127.0.0.1:8751</code>, enter the credentials specified in <code>/etc/rc.conf</code>, and create a bucket.</p><p>Once the bucket is created, you can configure PBS to use it. Connect to PBS via <code>https://127.0.0.1:8007</code> and go to <strong>S3 Endpoints</strong>. Set a name, use <code>192.168.0.11</code> as the IP and <code>9000</code> as the port, enter the access and secret keys, and the certificate fingerprint we generated earlier. <strong>Enable &#34;Path Style&#34;</strong> or it will not work.</p><p>Then go to <strong>Datastores</strong> and add it, as you would for any other S3 datastore, by specifying the created bucket and a local directory where the system will keep its cache.</p><p>If everything was set up correctly, PBS will create its structure in the bucket, and from that moment on, you can use it. Always keep in mind that this is still a &#34;technology preview&#34;, so there may be issues, but from my tests, it is sufficiently reliable.</p><h3>Taking Local Snapshots of Backups</h3><p>One of the most common techniques used in ransomware attacks is to also delete or encrypt backups. They often use automated methods, relying on the fact that many (too many!) consider a &#34;backup&#34; to be a simple copy of files to a network share. However, it&#39;s not impossible that, in specific cases, they might compromise the machine and connect to the backup server. This is nearly impossible with a &#34;pull&#34; type backup (like the one managed by <code>zfs-autobackup</code>) but is still possible with the &#34;push&#34; approach, which involves using BorgBackup or similar tools.</p><p>This happened to one of my clients once - in that case, the problem originated internally, from an employee who wanted to cover up his mistake, inadvertently creating a disaster - but that will be material for another post.</p><p>Fortunately, the client had a system that solved the problem: thanks to ZFS, we can have local snapshots on the backup server, which are invisible and inaccessible to the production server. Since we have already installed <code>zfs-autobackup</code>, it&#39;s easy to use it for this purpose as well. I&#39;ve already talked about this in a <a href="https://it-notes.dragas.net/2024/08/21/automating-zfs-snapshots-for-peace-of-mind/">previous article</a> and won&#39;t rewrite the steps here. Just consult that article, keeping in mind that in this case, it&#39;s not advisable to snapshot all the datasets on the backup server (the space would grow exponentially), but only those at risk. In the cases analyzed in this post, this applies only to the <code>push</code> part, as PBS will also be accessible only from the Proxmox servers and not from the VMs they contain. If, in this case too, you don&#39;t trust those who manage the Proxmox servers, just set up snapshots for the Minio jail as well.</p><h3>Conclusion</h3><p>This long post aims to analyze, in a general way, how I believe one can manage reasonably secure backups of their data. Obviously, there are many variables, additional precautions, possible optimizations, hardening, etc., that must be studied on a case-by-case basis. There are old rules, new rules, old and new philosophies. Recently, many people who have embraced the cloud have often stopped thinking about backups, only to realize it when something happens and the data has, indeed, vanished... into the clouds.</p><p>In this post, I have generically covered the setup of the backup server, and this demonstrates how FreeBSD, thanks to its features, can be considered an ideal platform for this type of task.</p><p>In the next articles in this series, I will examine the client side, i.e., how to structure them for a sufficiently reliable backup, and how to monitor everything - because I&#39;ve seen this too: people resting easy because the backup was supposedly running every night, but in fact, the backup had been failing every night for more than 4 years.</p><p>Stay Tuned and stay...backupped!</p></div></div></div>
  </body>
</html>
