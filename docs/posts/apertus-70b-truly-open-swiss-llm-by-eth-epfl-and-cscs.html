<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/swiss-ai/Apertus-70B-2509">Original</a>
    <h1>Apertus 70B: Truly Open - Swiss LLM by ETH, EPFL and CSCS</h1>
    
    <div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START -->
<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6639f08490b7db8dcbf1a2aa/YKux3SpTciL4O60L3Ol-6.jpeg"><img alt="image/jpeg" src="https://cdn-uploads.huggingface.co/production/uploads/6639f08490b7db8dcbf1a2aa/YKux3SpTciL4O60L3Ol-6.jpeg"/></a></p>
<h2>
	<a rel="nofollow" href="#table-of-contents" id="table-of-contents">
		
	</a>
	<span>
		Table of Contents
	</span>
</h2>
<ol>
<li><a rel="nofollow" href="#model-summary">Model Summary</a></li>
<li><a rel="nofollow" href="#how-to-use">How to use</a></li>
<li><a rel="nofollow" href="#evaluation">Evaluation</a></li>
<li><a rel="nofollow" href="#training">Training</a></li>
<li><a rel="nofollow" href="#limitations">Limitations</a></li>
<li><a rel="nofollow" href="#legal-aspects">Legal Aspects</a></li>
</ol>
<h2>
	<a rel="nofollow" href="#model-summary" id="model-summary">
		
	</a>
	<span>
		Model Summary
	</span>
</h2>
<p>Apertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models. 
The model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors.</p>
<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/654baf61d625e083383dfd00/gKDv_6dpIpvmgyquenbXt.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/654baf61d625e083383dfd00/gKDv_6dpIpvmgyquenbXt.png"/></a></p>
<p>The model is a decoder-only transformer, pretrained on 15T tokens with a staged curriculum of web, code and math data. The model uses a new xIELU activation function and is trained from scratch with the AdEMAMix optimizer. Post-training included supervised fine-tuning and alignment via QRPO.</p>
<h3>
	<a rel="nofollow" href="#key-features" id="key-features">
		
	</a>
	<span>
		Key features
	</span>
</h3>
<ul>
<li><strong>Fully open model</strong>: open weights + open data + full training details including all data and training recipes</li>
<li><strong>Massively Multilingual</strong>: 1811 natively supported languages</li>
<li><strong>Compliant</strong> Apertus is trained while respecting opt-out consent of data owners (even retrospectivey), and avoiding memorization of training data</li>
</ul>
<p>For more details refer to our <a rel="nofollow" href="https://github.com/swiss-ai/apertus-tech-report/blob/main/Apertus_Tech_Report.pdf">technical report</a></p>
<h2>
	<a rel="nofollow" href="#how-to-use" id="how-to-use">
		
	</a>
	<span>
		How to use
	</span>
</h2>
<p>The modeling code for Apertus is available in transformers <code>v4.56.0</code>, so make sure to upgrade your transformers version. You can also load the model with the latest <code>vLLM</code> which uses transformers as a backend.</p>
<pre><code>pip install -U transformers
</code></pre>
<pre><code><span>from</span> transformers <span>import</span> AutoModelForCausalLM, AutoTokenizer

model_name = <span>&#34;swiss-ai/Apertus-70B-2509&#34;</span>
device = <span>&#34;cuda&#34;</span>  


tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
).to(device)


prompt = <span>&#34;Give me a brief explanation of gravity in simple terms.&#34;</span>
messages_think = [
    {<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: prompt}
]

text = tokenizer.apply_chat_template(
    messages_think,
    tokenize=<span>False</span>,
    add_generation_prompt=<span>True</span>,
)
model_inputs = tokenizer([text], return_tensors=<span>&#34;pt&#34;</span>).to(model.device)


generated_ids = model.generate(**model_inputs, max_new_tokens=<span>32768</span>)


output_ids = generated_ids[<span>0</span>][<span>len</span>(model_inputs.input_ids[<span>0</span>]) :]
<span>print</span>(tokenizer.decode(output_ids, skip_special_tokens=<span>True</span>))
</code></pre>
<blockquote>
<p>We recommend setting <code>temperature=0.8</code> and <code>top_p=0.9</code> in the sampling parameters.</p>
</blockquote>
<h3>
	<a rel="nofollow" href="#long-context-processing" id="long-context-processing">
		
	</a>
	<span>
		Long context processing
	</span>
</h3>
<p>Apertus by default supports a context length up to 65,536 tokens.</p>
<h3>
	<a rel="nofollow" href="#agentic-usage" id="agentic-usage">
		
	</a>
	<span>
		Agentic Usage
	</span>
</h3>
<p>Apertus supports tool use</p>
<h3>
	<a rel="nofollow" href="#deployment" id="deployment">
		
	</a>
	<span>
		Deployment
	</span>
</h3>
<p>Deployment of the models is directly supported by the newest versions of <a rel="nofollow" href="https://github.com/huggingface/transformers">Transformers</a>, <a rel="nofollow" href="https://github.com/vllm-project/vllm">vLLM</a>, <a rel="nofollow" href="https://github.com/sgl-project/sglang">SGLang</a>, and also for running on-device with <a rel="nofollow" href="https://github.com/ml-explore/mlx-lm">MLX</a>, </p>
<h2>
	<a rel="nofollow" href="#evaluation" id="evaluation">
		
	</a>
	<span>
		Evaluation
	</span>
</h2>
<p><strong>Pretraining Evaluation:</strong> Performance (%) of Apertus models on <em>general language understanding</em> tasks (higher is better) compared to other pretrained models.</p>
<div>
	<table>
		<thead><tr>
<th><strong>Model</strong></th>
<th><strong>Avg</strong></th>
<th><strong>ARC</strong></th>
<th><strong>HellaSwag</strong></th>
<th><strong>WinoGrande</strong></th>
<th><strong>XNLI</strong></th>
<th><strong>XCOPA</strong></th>
<th><strong>PIQA</strong></th>
</tr>

		</thead><tbody><tr>
<td><strong>Fully Open Models</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Apertus-8B</strong></td>
<td>65.8</td>
<td>72.7</td>
<td>59.8</td>
<td>70.6</td>
<td>45.2</td>
<td>66.5</td>
<td>79.8</td>
</tr>
<tr>
<td><strong>Apertus-70B</strong></td>
<td>67.5</td>
<td>70.6</td>
<td>64.0</td>
<td>73.3</td>
<td>45.3</td>
<td>69.8</td>
<td>81.9</td>
</tr>
<tr>
<td>OLMo2-7B</td>
<td>64.0</td>
<td>72.9</td>
<td>60.4</td>
<td>74.5</td>
<td>40.4</td>
<td>55.2</td>
<td>80.9</td>
</tr>
<tr>
<td>OLMo2-32B</td>
<td>67.7</td>
<td>76.2</td>
<td>66.7</td>
<td>78.6</td>
<td>42.9</td>
<td>60.1</td>
<td>82.1</td>
</tr>
<tr>
<td>EuroLLM-1.7B</td>
<td>54.8</td>
<td>57.2</td>
<td>44.9</td>
<td>58.1</td>
<td>40.7</td>
<td>55.7</td>
<td>72.4</td>
</tr>
<tr>
<td>EuroLLM-9B</td>
<td>62.8</td>
<td>67.9</td>
<td>57.9</td>
<td>68.8</td>
<td>41.5</td>
<td>61.1</td>
<td>79.6</td>
</tr>
<tr>
<td>SmolLM2-1.7B</td>
<td>58.5</td>
<td>66.1</td>
<td>52.4</td>
<td>65.6</td>
<td>37.6</td>
<td>52.3</td>
<td>77.0</td>
</tr>
<tr>
<td>SmolLM3-3B</td>
<td>61.6</td>
<td>68.6</td>
<td>56.4</td>
<td>68.1</td>
<td>40.5</td>
<td>58.2</td>
<td>77.7</td>
</tr>
<tr>
<td>Poro-34B</td>
<td>61.7</td>
<td>65.7</td>
<td>57.9</td>
<td>70.6</td>
<td>41.6</td>
<td>56.0</td>
<td>78.5</td>
</tr>
<tr>
<td><strong>Open-Weight Models</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Llama3.1-8B</td>
<td>65.4</td>
<td>71.6</td>
<td>60.0</td>
<td>73.4</td>
<td>45.3</td>
<td>61.8</td>
<td>80.1</td>
</tr>
<tr>
<td>Llama3.1-70B</td>
<td>67.3</td>
<td>74.4</td>
<td>56.5</td>
<td>79.4</td>
<td>44.3</td>
<td>66.7</td>
<td>82.3</td>
</tr>
<tr>
<td>Qwen2.5-7B</td>
<td>64.4</td>
<td>69.6</td>
<td>60.1</td>
<td>72.8</td>
<td>43.3</td>
<td>61.7</td>
<td>78.7</td>
</tr>
<tr>
<td>Qwen2.5-72B</td>
<td>69.8</td>
<td>76.2</td>
<td>67.5</td>
<td>78.0</td>
<td>46.9</td>
<td>68.2</td>
<td>82.0</td>
</tr>
<tr>
<td>Qwen3-32B</td>
<td>67.8</td>
<td>75.6</td>
<td>64.0</td>
<td>73.8</td>
<td>44.4</td>
<td>67.9</td>
<td>80.9</td>
</tr>
<tr>
<td>Llama4-Scout-16x17B</td>
<td>67.9</td>
<td>74.7</td>
<td>66.8</td>
<td>73.2</td>
<td>43.5</td>
<td>67.7</td>
<td>81.2</td>
</tr>
<tr>
<td>GPT-OSS-20B</td>
<td>58.1</td>
<td>67.0</td>
<td>41.5</td>
<td>66.5</td>
<td>37.4</td>
<td>60.4</td>
<td>75.6</td>
</tr>
</tbody>
	</table>
</div>
<p>Many additional benchmark evaluations, for pretraining and posttraining phases, multilingual evaluations in around hundred languages, and long context evaluations are provided in Section 5 of the <a rel="nofollow" href="https://github.com/swiss-ai/apertus-tech-report/blob/main/Apertus_Tech_Report.pdf">Apertus_Tech_Report.pdf</a></p>
<h2>
	<a rel="nofollow" href="#training" id="training">
		
	</a>
	<span>
		Training
	</span>
</h2>
<h3>
	<a rel="nofollow" href="#model" id="model">
		
	</a>
	<span>
		Model
	</span>
</h3>
<ul>
<li><strong>Architecture:</strong> Transformer decoder</li>
<li><strong>Pretraining tokens:</strong> 15T</li>
<li><strong>Precision:</strong> bfloat16</li>
</ul>
<h3>
	<a rel="nofollow" href="#software--hardware" id="software--hardware">
		
	</a>
	<span>
		Software &amp; hardware
	</span>
</h3>
<ul>
<li><strong>GPUs:</strong> 4096 GH200</li>
<li><strong>Training Framework:</strong> <a rel="nofollow" href="https://github.com/swiss-ai/Megatron-LM">Megatron-LM</a></li>
<li>...</li>
</ul>
<h3>
	<a rel="nofollow" href="#open-resources" id="open-resources">
		
	</a>
	<span>
		Open resources
	</span>
</h3>
<p>All elements used in the training process are made openly available</p>
<ul>
<li><strong>Training data reconstruction scripts:</strong> <a rel="nofollow" href="https://github.com/swiss-ai/pretrain-data">github.com/swiss-ai/pretrain-data</a></li>
<li>The training intermediate checkpoints are available on the different branches of this same repository</li>
</ul>
<h2>
	<a rel="nofollow" href="#limitations" id="limitations">
		
	</a>
	<span>
		Limitations
	</span>
</h2>
<p>Apertus can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.</p>
<h2>
	<a rel="nofollow" href="#legal-aspects" id="legal-aspects">
		
	</a>
	<span>
		Legal Aspects
	</span>
</h2>
<h4>
	<a rel="nofollow" href="#eu-ai-act-transparency-documentation-and-code-of-practice" id="eu-ai-act-transparency-documentation-and-code-of-practice">
		
	</a>
	<span>
		EU AI Act Transparency Documentation and Code of Practice
	</span>
</h4>
<ul>
<li><a rel="nofollow" href="https://huggingface.co/swiss-ai/Apertus-70B-2509/blob/main/Apertus_EU_Public_Summary.pdf">Apertus_EU_Public_Summary.pdf</a></li>
<li><a rel="nofollow" href="https://huggingface.co/swiss-ai/Apertus-70B-2509/blob/main/Apertus_EU_Code_of_Practice.pdf">Apertus_EU_Code_of_Practice.pdf</a></li>
</ul>
<h4>
	<a rel="nofollow" href="#data-protection-and-copyright-requests" id="data-protection-and-copyright-requests">
		
	</a>
	<span>
		Data Protection and Copyright Requests
	</span>
</h4>
<p>For removal requests of personally identifiable information (PII) or of copyrighted content, please contact the respective dataset owners or us directly</p>
<ul>
<li><a rel="nofollow" href="mailto:llm-privacy-requests@swiss-ai.org">llm-privacy-requests@swiss-ai.org</a></li>
<li><a rel="nofollow" href="mailto:llm-copyright-requests@swiss-ai.org">llm-copyright-requests@swiss-ai.org</a></li>
</ul>
<h4>
	<a rel="nofollow" href="#output-filter-for-pii" id="output-filter-for-pii">
		
	</a>
	<span>
		Output Filter for PII
	</span>
</h4>
<ul>
<li>Currently no output filter is provided.</li>
<li>Please check this site regularly for an output filter that can be used on top of the Apertus LLM. The filter reflects data protection deletion requests which have been addressed to us as the developer of the Apertus LLM. It allows you to remove Personal Data contained in the model output. We strongly advise downloading and applying this output filter from this site every six months.</li>
</ul>
<h2>
	<a rel="nofollow" href="#contact" id="contact">
		
	</a>
	<span>
		Contact
	</span>
</h2>
<p>To contact us, please send an email to
<a rel="nofollow" href="mailto:llm-requests@swiss-ai.org">llm-requests@swiss-ai.org</a></p>
<h2>
	<a rel="nofollow" href="#citation" id="citation">
		
	</a>
	<span>
		Citation
	</span>
</h2>
<pre><code>@misc{swissai2025apertus,
  title={{Apertus: Democratizing Open and Compliant LLMs <span>for</span> Global Language Environments}},
  author={Apertus Team},
  year={2025},
  howpublished={\url{https://huggingface.co/swiss-ai/Apertus-70B-2509}}
}
</code></pre>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
