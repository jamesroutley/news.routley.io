<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html">Original</a>
    <h1>When gradient descent is a kernel method</h1>
    
    <div id="readability-page-1" class="page">
    
    
    <p>Suppose that we sample a large number <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> of independent random functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi mathvariant="double-struck">R</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f_i \colon \R \to \R</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span></span><span></span><span></span><span></span><span><span>:</span></span><span></span><span><span>R</span></span><span></span><span>→</span><span></span></span><span><span></span><span><span>R</span></span></span></span></span> from a certain distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>F</span></span></span></span></span> and propose to solve a regression problem by choosing a linear combination
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>λ</mi><mi>i</mi></msub><msub><mi>f</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f} = \sum_i \lambda_i f_i.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span></span>
For large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">N,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span>,</span></span></span></span> adjusting the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> to fit some fixed constraints of the form <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\bar{f}(t_i) = y_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> amounts to solving a highly underdetermined linear system, meaning that a high-dimensional space <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Λ</span></span></span></span> of vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>λ</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>λ</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\lambda_1, \dots, \lambda_N)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>λ</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span>N</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> fit our constraints perfectly. So, choosing one element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Λ</span></span></span></span> requires some additional decision-making. To use the picturesque idea of a
&#34;loss landscape&#34; over parameter space, our problem will have a <em>ridge</em> of equally performing parameters rather than just a single optimal <em>peak</em>.</p>
<p>Now, we make a very strange proposal. What if we simply initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><msqrt><mi>n</mi></msqrt></mrow><annotation encoding="application/x-tex">\lambda_i = 1/\sqrt{n}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>1/</span><span><span><span><span><span><span></span><span><span>n</span></span></span><span><span></span><span><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> for all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>i</span></span></span></span> and proceed by minimizing some loss function using gradient descent? If all goes well, we should end up with an element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Lambda.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Λ.</span></span></span></span> Of course, many elements of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Λ</span></span></span></span> give very bad models. To see this, it&#39;s enough to remember that we can expect a linear combination of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> random functions to fit <em>any</em> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> data points, so if we have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>m</span></span></span></span> data points, there exist models in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Λ</span></span></span></span> that perfectly interpolate any adversarial selection of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>−</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">N - m</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>−</span><span></span></span><span><span></span><span>m</span></span></span></span> additional data points! Does gradient descent tend to make a &#34;good&#34; choice?</p>
<p>Let&#39;s test this empirically. In the widget below, I&#39;ve chosen functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> by sampling <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>200</span></span></span></span> trajectories of a Wiener process, also known as Brownian noise. Click anywhere to introduce data points and click the play button in the top right to run gradient descent for the squared loss <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\sum_i (\bar{f}(t_i) - y_i)^2.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>∑</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>−</span><span></span></span><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span>.</span></span></span></span></p>

<p>Interestingly, the functions we obtain are not that bad. They seem to concentrate around piecewise linear interpolations of our data. In fact, in the limit <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">N \to \infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span></span><span>→</span><span></span></span><span><span></span><span>∞</span></span></span></span> of many random functions, it turns out that running gradient descent to convergence has a meaningful statistical interpretation. Specifically, if we view the Wiener process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>F</span></span></span></span></span> as a prior, then <strong>running gradient descent to convergence samples from the posterior for our data points.</strong> Since many optimal solutions to our minimization problem are meaningless, it is not possible to explain this fact if we see gradient descent as &#34;just some optimization method.&#34; What explains its relative success?</p>
<p>As we will show in this post, our intriguing Bayesian interpretation can be explained by the relationship between the behavior of gradient descent steps, the statistical properties of our random functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, and our initialization. In particular, it does <em>not</em> depend on the loss function—so long as it leads gradient descent to converge to an exact interpolation—but <em>does</em> depend significantly on our choice of parameters at initialization. Our analysis will rely on a &#34;tangent kernel&#34; of the sort introduced in the <em>Neural Tangent Kernel</em> paper by Jacot et al.. Specifically, viewing gradient descent as a process occurring in the function space of our regression problem, we will find that its dynamics can be described in terms of a certain kernel function, which in this case is just the kernel function of the process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Fc.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>F</span></span><span>.</span></span></span></span></p>
<p>Of course, there are much easier ways to sample posteriors for low-dimensional Gaussian processes. Nevertheless, it&#39;s interesting to notice a relationship between Bayesian inference and gradient descent methods at large, since the latter tend to apply to situations where direct estimation of a posterior distribution is not practical. Furthermore, the results of Jacot suggest that kernel-based interpretations may also hold for large, non-trivial neural networks. To the extent that this is true, we can use the kernel interpretation to reason about many sorts of neural network phenomena, including the benefits of early stopping, the existence of &#34;implicit regularization&#34;, and the fact that overparameterization often <em>increases</em> performance despite the apparent risk of overfitting.</p>
<p>In this post, we will focus exclusively on the toy problem introduced above. Our discussion (which admittedly takes a bit of a scenic route) is divided into three sections.</p>
<ul>
<li>First, we will discover how the covariance kernel of the process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>F</span></span></span></span></span> is related to the dynamics of gradient descent.</li>
<li>Next, we recall the theory of reproducing kernel Hilbert spaces and show how the kernel-based behavior of gradient descent is related to regularization.</li>
<li>Finally, we recall some special properties of Gaussian processes and explain why regularization is related to the Bayesian interpretation of our trained model.</li>
</ul>
<h2>Kernel Functions</h2>
<p>Let&#39;s begin by considering the effect that a single step of gradient descent has on our function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f}.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>.</span></span></span></span> In general, the differential of a loss can be written as a sum of differentials <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>d</span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the evaluation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> at an input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span>,</span></span></span></span> so by linearity it is enough for us to understand how <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> &#34;responds&#34; to differentials of this form.</p>
<p>In response to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">d \pi_t,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>d</span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span></span></span></span> the parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> are assigned differentials
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>λ</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial \pi_t}{\partial \lambda_i} = f_i(t).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>∂</span><span><span>λ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span>∂</span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>t</span><span>)</span><span>.</span></span></span></span></span>
So, gradient descent will increase <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> proportional to the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f_i(t).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>t</span><span>)</span><span>.</span></span></span></span> In terms of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bar{f},</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>,</span></span></span></span> we find that the value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\bar{f}(s)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span>s</span><span>)</span></span></span></span> at another input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>s</span></span></span></span> will increase proportional to
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\Delta_t(s) = \sum_{i = 1}^N f_i(s) f_i(t). \tag{1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Δ</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>s</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span><span>i</span><span>=</span><span>1</span></span></span></span><span><span></span><span><span>∑</span></span></span><span><span></span><span><span>N</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>s</span><span>)</span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>t</span><span>)</span><span>.</span></span><span><span></span><span><span>(</span><span><span>1</span></span><span>)</span></span></span></span></span></span>
Note that this expression is independent of the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\lambda_i.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span> This means that the gradient descent step we apply to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> depends only on our learning rate and differential of the loss at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f}.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>.</span></span></span></span> In other words, we can view gradient descent as a process happening in the function space of our regression problem.</p>
<p>For large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> we get the approximation
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>≈</mo><msub><mi>E</mi><mrow><mi>f</mi><mo>∼</mo><mi mathvariant="script">F</mi></mrow></msub><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\frac{1}{N} \Delta_t(s) \approx E_{f \sim \Fc}[f(s) f(t)]. \tag{2}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>N</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span><span>Δ</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>s</span><span>)</span><span></span><span>≈</span><span></span></span><span><span></span><span><span>E</span><span><span><span><span><span><span></span><span><span><span>f</span><span>∼</span><span><span>F</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>[</span><span>f</span><span>(</span><span>s</span><span>)</span><span>f</span><span>(</span><span>t</span><span>)]</span><span>.</span></span><span><span></span><span><span>(</span><span><span>2</span></span><span>)</span></span></span></span></span></span>
This last expression is familiar in the study of Gaussian processes; it is called the covariance kernel of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>F</span></span></span></span></span> and denoted <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>s</span><span>,</span><span></span><span>t</span><span>)</span><span>.</span></span></span></span> For the Wiener process, the covariance kernel takes the form
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t) = \min(s, t).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>s</span><span>,</span><span></span><span>t</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>min</span><span>(</span><span>s</span><span>,</span><span></span><span>t</span><span>)</span><span>.</span></span></span></span></span>
In the following, we&#39;ll assume <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> is large enough for us to make the approximation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(2)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>2</span><span>)</span></span></span></span> confidently. Then, we conclude that a request of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>d</span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> will cause gradient descent to push <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> in the direction of the function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>−</span><span>,</span><span></span><span>t</span><span>)</span><span>.</span></span></span></span></p>
<p>You can get a visual sense of this behavior in the widget below. As above, I&#39;ve generated <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>200</span></span></span></span> trials of the Wiener process to use as my functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f_i.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span> You can choose the request <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>d</span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> by clicking on the graph, and your browser will compute the corresponding response <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mi mathvariant="normal">/</mi><mi>N</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Delta_t/N.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>Δ</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>/</span><span>N</span><span>.</span></span></span></span> For comparison I&#39;ve also drawn the prediction <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>−</span><span>,</span><span></span><span>t</span><span>)</span><span>.</span></span></span></span></p>

<p>This is already a significant conclusion. In particular, it means that every step of gradient descent modifies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> by a linear combination of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K(-, t_i),</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>−</span><span>,</span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span></span></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> ranges over the inputs in our training set. Since the linear span of these functions is a certain space of piecewise affine functions, if we initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_i = 0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>λ</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>0</span></span></span></span> and run gradient descent to convergence with <em>any</em> reasonable loss function, we should approximately converge to a piecewise affine interpolation of our data points. We&#39;ve run this experiment in the widget below.</p>

<p>This new model has less variance than before. In fact, in the large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span> limit, its behavior will be exactly deterministic! In comparison, our previous model will <em>always</em> exhibit variance due to initialization of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>f</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> even for large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">N.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span><span>.</span></span></span></span> In other words, when given a finite training set, gradient descent cannot entirely &#34;forget&#34; its initialization even when run to convergence.</p>
<p>Another important conclusion is that, when we optimize least squares with gradient descent, the evolution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> is linear in the sense of approximately obeying a linear ODE. Indeed, for data points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(t_i, y_i)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> our loss differential will be
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>d</mi><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>d</mi><msub><mi>π</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{1}{2} d \sum_i (\bar{f}(t_i) - y_i)^2 = \sum_i d \pi_{t_i} (\bar{f}(t_i) - y_i).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>d</span><span></span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>−</span><span></span></span><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>d</span><span><span>π</span><span><span><span><span><span><span></span><span><span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>−</span><span></span></span><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>.</span></span></span></span></span>
So, if we view <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> as evolving under the flow of gradient descent with respect to a continuous parameter <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\tau,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>τ</span><span>,</span></span></span></span> we have
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><mrow><mi>d</mi><mi>τ</mi></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\frac{d \bar{f}}{d \tau} = \sum_i K(-, t_i) (y_i - \bar{f}(t_i)),</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>d</span><span>τ</span></span></span><span><span></span><span></span></span><span><span></span><span><span>d</span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span>K</span><span>(</span><span>−</span><span>,</span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>(</span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>))</span><span>,</span></span></span></span></span>
where the right-hand side is a linear function of the empirical error vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">h = (y_i - \bar{f}(t_i)).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>h</span><span></span><span>=</span><span></span></span><span><span></span><span>(</span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>−</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>))</span><span>.</span></span></span></span> Restricting this equation to the evaluation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span></span><span>f</span></span><span><span></span><span><span>ˉ</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span> over the input points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t_i,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span></span></span></span> we find that the error vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>h</span></span></span></span> solves the ODE
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>h</mi></mrow><mrow><mi>d</mi><mi>τ</mi></mrow></mfrac><mo>=</mo><mo>−</mo><mi>K</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">\frac{d h}{d \tau} = -K h</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span>d</span><span>τ</span></span></span><span><span></span><span></span></span><span><span></span><span><span>d</span><span>h</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>−</span><span>K</span><span>h</span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span> is the matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(K(t_i, t_j)).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>K</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>))</span><span>.</span></span></span></span> Since this matrix is positive-definite—it is a covariance matrix—we conclude that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>h</span></span></span></span> will converge to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>0</span></span></span></span> over the training process if our learning rate is sufficiently small. Furthermore, knowing the eigenvalues of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span> lets us understand the nature of our convergence; gradient descent will &#34;correct&#34; the error <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>h</span></span></span></span> along the components of the eigenbasis for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span> with largest eigenvalues first, and take longer to correct components with smaller eigenvalues.</p>
<p>Now, we could have chosen a distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>F</span></span></span></span></span> of random functions with a different covariance kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>.</span></span></span></span> Here the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(-, t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>−</span><span>,</span><span></span><span>t</span><span>)</span></span></span></span> were easy to interpret, but in general, what does it mean to fit a data set using a linear combination of functions like these? One interesting perspective comes from the idea of <em>regularization</em>, which we discuss next.</p>
<h2>Regularization</h2>
<p>Consider a Hilbert space <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> equipped with bounded projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>H</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\pi_t \colon H \to \R</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span></span><span></span><span></span><span></span><span><span>:</span></span><span></span><span>H</span><span></span><span>→</span><span></span></span><span><span></span><span><span>R</span></span></span></span></span> for each <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t \in \R,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span></span><span>∈</span><span></span></span><span><span></span><span><span>R</span></span><span>,</span></span></span></span> and suppose that we want to find an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">v \in H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span><span></span><span>∈</span><span></span></span><span><span></span><span>H</span></span></span></span> that minimizes some loss function depending on the values <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{t}(v)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>v</span><span>)</span></span></span></span> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span></span></span></span> belonging to some collection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\{t_i\}.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span>.</span></span></span></span> (Note that elements of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> can be viewed as functions from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\R</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>R</span></span></span></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\R</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>R</span></span></span></span></span> by viewing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> as the evaluation at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span>.</span></span></span></span>) If this problem is underdetermined—which necessarily will happen if <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> is infinite-dimensional and our collection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{t_i\}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span></span></span></span> is finite—then we may ask for an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span></span></span></span> that both minimizes our loss and has minimal norm in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span><span>.</span></span></span></span> In machine learning, this is called regularization.</p>
<p>Let&#39;s write <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span></span></span></span> for the product of the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msub><mi>t</mi><mi>i</mi></msub></msub></mrow><annotation encoding="application/x-tex">\pi_{t_i}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> Because <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span></span></span></span> is chosen with minimal norm, it cannot be made smaller by adjusting it by an element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\ker \Pi,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ker</span><span></span><span>Π</span><span>,</span></span></span></span> so <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span></span></span></span> is orthogonal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\ker \Pi.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ker</span><span></span><span>Π.</span></span></span></span> But since the maps <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{t}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> are continuous, they can be represented by vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> in the sense that
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mo>−</mo><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>−</mo><mo stretchy="false">⟩</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\pi_{t}(-) = \langle K_{t}, - \rangle.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>−</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>⟨</span><span><span>K</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>−</span><span>⟩</span><span>.</span></span></span></span></span>
(This is the Riesz representation theorem.) Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\ker \Pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ker</span><span></span><span>Π</span></span></span></span> can be described as the orthogonal complement to the set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>K</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mo stretchy="false">}</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\{K_{t_i}\},</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>{</span><span><span>K</span><span><span><span><span><span><span></span><span><span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>}</span><span>,</span></span></span></span> the orthogonal complement to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\ker \Pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ker</span><span></span><span>Π</span></span></span></span> is exactly the closure of the span of the vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K_{t_i}.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span> We conclude that any regularized solution to our loss function is a (limit of) linear combinations of these vectors.</p>
<p>What are the projections of the &#34;representative elements&#34; <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span>t</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>? By our own definition, we have
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>s</mi></msub><mo stretchy="false">(</mo><msub><mi>K</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>s</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>t</mi></msub><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\pi_s(K_t) = \langle K_s, K_t \rangle</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span>K</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>⟨</span><span><span>K</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⟩</span></span></span></span></span>
for any other <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">s \in \R.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>s</span><span></span><span>∈</span><span></span></span><span><span></span><span><span>R</span></span><span>.</span></span></span></span> This last expression is a positive semidefinite kernel, which we will denote <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>s</span><span>,</span><span></span><span>t</span><span>)</span><span>.</span></span></span></span> In other words, the norm on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> and the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> work together to produce a kernel function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span> whose partial evaluations <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(-, t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>−</span><span>,</span><span></span><span>t</span><span>)</span></span></span></span> help us solve optimization problems regularized by the norm of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span><span>.</span></span></span></span></p>
<p>In the literature, a Hilbert space equipped with bounded projections indexed over a set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>I</span></span></span></span> is called a reproducing kernel Hilbert space (or RKHS). In fact, we can also go in the other direction: every positive definite kernel on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>I</span></span></span></span> is &#34;reproduced&#34; by some RKHS, which also turns out to be unique in a certain sense. This is known as the Moore-Aronszajn theorem.</p>
<p>What RKHS corresponds to our kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(s, t) = \min(s, t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>s</span><span>,</span><span></span><span>t</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>min</span><span>(</span><span>s</span><span>,</span><span></span><span>t</span><span>)</span></span></span></span>? In general, determining the RKHS of a kernel is not entirely straightforward. In fact, notice that for positive definite kernels over a finite set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">I,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>I</span><span>,</span></span></span></span> the inner product for the RKHS expressed in the dual basis for our projections turns out to be the <em>inverse</em> of the matrix encoded by our kernel. Indeed, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">K_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> are representatives of the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\pi_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">e_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is a dual basis verifying <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\pi_i(e_j) = \delta_{i, j},</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span>e</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>δ</span><span><span><span><span><span><span></span><span><span><span>i</span><span>,</span><span>j</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span></span></span></span> we find that
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo><mi>K</mi><mo stretchy="false">(</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="false">(</mo><msub><mi>K</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>k</mi></msub><mo stretchy="false">⟩</mo><mo>=</mo><msub><mi>π</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle e_i, e_j \rangle K(j, k) = \langle e_i, e_j \rangle \pi_j(K_k) = \langle e_i, K_k \rangle = \pi_k(e_j) = \delta_{i, k}.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>⟨</span><span><span>e</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⟩</span><span>K</span><span>(</span><span>j</span><span>,</span><span></span><span>k</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>⟨</span><span><span>e</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>e</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⟩</span><span><span>π</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span>K</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>⟨</span><span><span>e</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>⟩</span><span></span><span>=</span><span></span></span><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span>e</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>δ</span><span><span><span><span><span><span></span><span><span><span>i</span><span>,</span><span>k</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span></span>
So, interpreting a RKHS norm in terms of projections of elements requires solving some sort of inverse problem.</p>
<p>The RKHS for a centered Gaussian process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(X_t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>X</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> can be viewed as an isometric embedding of the observables <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> with respect to the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span></span></span></span> norm for the process measure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Fc.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>F</span></span><span>.</span></span></span></span> Specifically, if we define <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">f(X_t) = K_t,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span><span>X</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span></span></span></span> then clearly
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>X</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>X</mi><mi>k</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><msup><mi>L</mi><mn>2</mn></msup><mi mathvariant="script">F</mi></mrow></msub><mo>=</mo><mo stretchy="false">⟨</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mo stretchy="false">⟩</mo><mtext>RKHS</mtext></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle X_t, X_k \rangle_{L^2 \Fc} = \langle f(X_t), f(X_k) \rangle_\text{RKHS}.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>⟨</span><span><span>X</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>X</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span>⟩</span><span><span><span><span><span><span></span><span><span><span><span>L</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span><span>F</span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span>⟨</span><span>f</span><span>(</span><span><span>X</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span><span></span><span>f</span><span>(</span><span><span>X</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span><span>⟩</span><span><span><span><span><span><span></span><span><span><span>RKHS</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span></span>
Indeed, the space of observables of a Gaussian process is already a RKHS for its covariance kernel, if we take the projections to be the maps <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>X</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>−</mo><mo stretchy="false">⟩</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle X_t, - \rangle.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>⟨</span><span><span>X</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>−</span><span>⟩</span><span>.</span></span></span></span> However, we would like to view the RKHS more directly as a space of functions.</p>
<p>We may begin by observing, then, that observables of the Wiener process can be isometrically mapped into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>1</mn></msup><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L^1 [0, \infty)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>L</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span></span></span></span></span><span>[</span><span>0</span><span>,</span><span></span><span>∞</span><span>)</span></span></span></span> by sending <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>X</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> to
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mi>s</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>≤</mo><mi>s</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn><mo>:</mo><mtext>otherwise.</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">K_s(t) = \begin{cases}
1 : t \le s \\
0 : \text{otherwise.}
\end{cases}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>t</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>{</span></span><span><span><span><span><span><span><span><span></span><span><span>1</span><span></span><span>:</span><span></span><span>t</span><span></span><span>≤</span><span></span><span>s</span></span></span><span><span></span><span><span>0</span><span></span><span>:</span><span></span><span><span>otherwise.</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span></span></span></span></span></span></span>
Under this perspective, our projections become
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">⟩</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>t</mi></msubsup><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>d</mi><mi>s</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\pi_t(f) = \langle K_t, f \rangle = \int_0^t f(s) \, ds.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>f</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>⟨</span><span><span>K</span><span><span><span><span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>f</span><span>⟩</span><span></span><span>=</span><span></span></span><span><span></span><span><span>∫</span><span><span><span><span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>t</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>f</span><span>(</span><span>s</span><span>)</span><span></span><span>d</span><span>s</span><span>.</span></span></span></span></span>
Ultimately, we are led to view the RKHS of the Wiener process as the Sobolev space of absolutely continuous functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f \colon [0, \infty) \to \R</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span></span><span></span><span></span><span></span><span></span><span><span>:</span></span><span></span><span>[</span><span>0</span><span>,</span><span></span><span>∞</span><span>)</span><span></span><span>→</span><span></span></span><span><span></span><span><span>R</span></span></span></span></span> such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(0) = 0</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>0</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>0</span></span></span></span> and such that the norm
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">∥</mo><mi>f</mi><mo stretchy="false">∥</mo><mo>=</mo><mrow><mo fence="true">(</mo><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><mo stretchy="false">(</mo><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>t</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lVert f \rVert = \left( \int_0^\infty (f&#39;(t))^2 \, dt \right)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∥</span><span>f</span><span>∥</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span>(</span></span><span><span>∫</span><span><span><span><span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>∞</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span>f</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>(</span><span>t</span><span>)</span><span><span>)</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>d</span><span>t</span><span><span>)</span></span></span></span></span></span></span>
is finite. In fact, solving the regularized interpolation problem
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext mathvariant="bold">minimize</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><mo stretchy="false">(</mo><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>t</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext mathvariant="bold">subject</mtext><mtext> </mtext><mtext mathvariant="bold">to</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mtext>  </mtext><mtext>for all </mtext><mi>i</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\textbf{minimize} &amp; \quad \int_0^\infty (f&#39;(s))^2 \, dt \\
\textbf{subject to} &amp; \quad f(0) = 0, f(t_i) = y_i \; \text{for all }i
\end{align*}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span>minimize</span></span></span></span><span><span></span><span><span><span>subject to</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span></span><span><span>∫</span><span><span><span><span><span><span></span><span><span>0</span></span></span><span><span></span><span><span>∞</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span><span>f</span><span><span><span><span><span><span></span><span><span><span>′</span></span></span></span></span></span></span></span></span><span>(</span><span>s</span><span>)</span><span><span>)</span><span><span><span><span><span><span></span><span><span>2</span></span></span></span></span></span></span></span><span></span><span>d</span><span>t</span></span></span><span><span></span><span><span></span><span></span><span>f</span><span>(</span><span>0</span><span>)</span><span></span><span>=</span><span></span><span>0</span><span>,</span><span></span><span>f</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>for all </span></span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span>
results in the piecewise affine interpolations we observed in the widget above.</p>
<p>So far, we have shown that its relationship with kernel functions gives gradient descent a distinct flavor of <em>implicit regularization</em>. We did not have a penalty function in mind when we set up our problem, but our distribution of random functions ended up making our gradient updates interpretable in terms of a RKHS for an associated kernel function. In the last section of this post, we address how this fact is related to the statistical idea of a conditional distribution for a Gaussian process.</p>
<h2>Bayesian Interpretation</h2>
<p>When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span></span></span></span> are jointly Gaussian distributed, we know that the remainder
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span><span></span><span>−</span><span></span></span><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span>)</span></span></span></span></span>
of the conditional expectation is independent from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">X.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span><span>.</span></span></span></span> (Remember that for other distributions, this remainder will have zero covariance with all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span></span></span></span>-measurable events but will not necessarily be independent!) So, we can decompose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span></span></span></span> into two components
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">(</mo><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y = (Y - E(Y|X)) + E(Y|X),</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span><span></span><span>=</span><span></span></span><span><span></span><span>(</span><span>Y</span><span></span><span>−</span><span></span></span><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span>))</span><span></span><span>+</span><span></span></span><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span>)</span><span>,</span></span></span></span></span>
the first being a Gaussian variable independent from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span></span></span></span> and the second being <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span></span></span></span>-measurable. This clarifies the nature of the conditional distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span></span></span></span> on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span><span></span><span>=</span><span></span></span><span><span></span><span>x</span></span></span></span>: it will have constant variance equal to the variance of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span><span></span><span>−</span><span></span></span><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span>)</span></span></span></span> and mean equal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">E(Y|X),</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span>)</span><span>,</span></span></span></span> a linear function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">X.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span><span>.</span></span></span></span> In particular, if we want to sample the conditional distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span></span></span></span> given <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X = x,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span><span></span><span>=</span><span></span></span><span><span></span><span>x</span><span>,</span></span></span></span> we could take
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Y</mi><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo>−</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y + E(Y|X = x) - E(Y|X) = Y + E(Y|X = x - X),</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span><span></span><span>+</span><span></span></span><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span></span><span>=</span><span></span></span><span><span></span><span>x</span><span>)</span><span></span><span>−</span><span></span></span><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>Y</span><span></span><span>+</span><span></span></span><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span></span><span>=</span><span></span></span><span><span></span><span>x</span><span></span><span>−</span><span></span></span><span><span></span><span>X</span><span>)</span><span>,</span></span></span></span></span>
where the apparently nonsensical conditional expectation on the RHS should be interpreted as the evaluation of the conditional expectation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">E(Y|X),</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span>)</span><span>,</span></span></span></span> viewed as a function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span><span>,</span></span></span></span> at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>−</mo><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">x - X.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span><span></span><span>−</span><span></span></span><span><span></span><span>X</span><span>.</span></span></span></span> Keep in mind that this is a very special property of Gaussian distributions; in general, the distribution of the remainder <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Y</span><span></span><span>−</span><span></span></span><span><span></span><span>E</span><span>(</span><span>Y</span><span>∣</span><span>X</span><span>)</span></span></span></span> conditional on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span></span></span></span> will depend on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span><span>,</span></span></span></span> and so we won&#39;t be able to sample the conditional distribution under another &#34;counterfactual&#34; value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>X</span><span></span><span>=</span><span></span></span><span><span></span><span>x</span></span></span></span> simply by translating a sample of the remainder.</p>
<p>Now, consider a random function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span> drawn from a Gaussian distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>F</span></span></span></span></span> and let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span></span></span></span> give the values of our trajectory on a finite set of inputs. If we want to produce a sample from the distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span> conditional on some data <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><mi>π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Pi = \pi,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span><span></span><span>=</span><span></span></span><span><span></span><span>π</span><span>,</span></span></span></span> we can take
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><mi>π</mi><mo>−</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f - E(f | \Pi = \pi - \Pi).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span></span><span>−</span><span></span></span><span><span></span><span>E</span><span>(</span><span>f</span><span>∣Π</span><span></span><span>=</span><span></span></span><span><span></span><span>π</span><span></span><span>−</span><span></span></span><span><span></span><span>Π</span><span>)</span><span>.</span></span></span></span></span>
But, as it turns out, the conditional expectation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f|\Pi = \pi^*)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>E</span><span>(</span><span>f</span><span>∣Π</span><span></span><span>=</span><span></span></span><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span><span>)</span></span></span></span> will be exactly the function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span></span></span></span> in the RKHS of our process that solves the constraint <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\Pi = \pi^*</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span><span></span><span>=</span><span></span></span><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span></span></span></span> regularized by the RKHS norm! This explains the Bayesian interpretation of our model.</p>
<p>One way to understand this is just to write out the expression for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>E</span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>∣Π</span><span>)</span></span></span></span> at a given value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span>.</span></span></span></span> We know that this will be the linear function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\lambda \Pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>λ</span><span>Π</span></span></span></span> of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span></span></span></span> uniquely determined by the equation
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Cov</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>−</mo><mi>λ</mi><mi mathvariant="normal">Π</mi><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.</mn></mrow><annotation encoding="application/x-tex">\Cov(f(t) - \lambda \Pi, \Pi) = 0.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span>Cov</span></span></span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span></span><span>−</span><span></span></span><span><span></span><span>λ</span><span>Π</span><span>,</span><span></span><span>Π</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>0.</span></span></span></span></span>
Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">K = [K(t_i, t_j)]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span></span><span>=</span><span></span></span><span><span></span><span>[</span><span>K</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)]</span></span></span></span> is the covariance matrix of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="normal">Cov</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">v = [\Cov(f(t), f(t_i))]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span><span></span><span>=</span><span></span></span><span><span></span><span>[</span><span><span><span>Cov</span></span></span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>,</span><span></span><span>f</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>))]</span></span></span></span> gives the covariance of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>t</span><span>)</span></span></span></span> with the components <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t_i)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Pi,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span><span>,</span></span></span></span> this equation can be written as
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>v</mi><mo>−</mo><mi>λ</mi><mi>K</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v - \lambda K = 0,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span><span></span><span>−</span><span></span></span><span><span></span><span>λ</span><span>K</span><span></span><span>=</span><span></span></span><span><span></span><span>0</span><span>,</span></span></span></span></span>
and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mo>=</mo><mi>v</mi><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="normal">Π</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">E(f(t) | \Pi) = v K^{-1} \Pi.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>E</span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>∣Π</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>v</span><span><span>K</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span><span>Π.</span></span></span></span> We conclude that the function
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mo>↦</mo><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t \mapsto E(f(t)|\Pi)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span></span><span>↦</span><span></span></span><span><span></span><span>E</span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>∣Π</span><span>)</span></span></span></span></span>
is a linear combination of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(t, t_i)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>t</span><span>,</span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span>—the coordinates of the vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span></span></span></span>—with constant coefficients, determined by the constraints that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>E</span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>∣Π</span><span>)</span></span></span></span> should agree with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span></span></span></span> at the points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><msub><mi>t</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t = t_i.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span></span><span>=</span><span></span></span><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span> But, as we saw above, this is the same as the solution to the problem of interpolating some constraints <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f(t_i) = y_i</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> regularized by the RKHS norm of our process.</p>
<p>To see this connection more directly, remember that the mean of a Gaussian distribution coincides with the mode—the point of highest probability density under linear coordinates. So, for example, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi = \pi^*)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>E</span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>∣Π</span><span></span><span>=</span><span></span></span><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span><span>)</span></span></span></span> is exactly the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>t</span><span>)</span></span></span></span> that minimizes
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mo>+</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext>  </mtext><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mtext>  </mtext><mo>…</mo><mtext>  </mtext><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\ln(p(f(t), \pi^*)) = C + -\frac{1}{2} 
[f(t)  \; \pi^*(1)  \; \dots \; \pi^*(m)] K^{-1}
\begin{bmatrix}
f(t)   \\
\pi^*(1)   \\
\vdots  \\
\pi^*(m)]
\end{bmatrix}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>ln</span><span>(</span><span>p</span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>,</span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span><span>))</span><span></span><span>=</span><span></span></span><span><span></span><span>C</span><span></span><span>+</span><span></span></span><span><span></span><span>−</span><span><span></span><span><span><span><span><span><span></span><span><span>2</span></span></span><span><span></span><span></span></span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span>[</span><span>f</span><span>(</span><span>t</span><span>)</span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span><span>(</span><span>1</span><span>)</span><span></span><span></span><span>…</span><span></span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span><span>(</span><span>m</span><span>)]</span><span><span>K</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span><span></span><span><span><span><span><span><span><span><span></span><span><svg xmlns="http://www.w3.org/2000/svg" width="0.667em" height="5.400em" viewBox="0 0 667 5400"><path d="M403 1759 V84 H666 V0 H319 V1759 v1800 v1759 h347 v-84
H403z M403 1759 V0 H319 V1759 v1800 v1759 h84z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span><span><span><span><span><span><span><span></span><span><span>f</span><span>(</span><span>t</span><span>)</span></span></span><span><span></span><span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span><span>(</span><span>1</span><span>)</span></span></span><span><span></span><span><span><span>⋮</span><span></span></span></span></span><span><span></span><span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span><span>(</span><span>m</span><span>)]</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span><span><span><span><span><span><span><span></span><span><svg xmlns="http://www.w3.org/2000/svg" width="0.667em" height="5.400em" viewBox="0 0 667 5400"><path d="M347 1759 V0 H0 V84 H263 V1759 v1800 v1759 H0 v84 H347z
M347 1759 V0 H263 V1759 v1800 v1759 h84z"></path></svg></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span></span></span></span> is the inverse of the covariance matrix for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(f(t), \Pi).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>,</span><span></span><span>Π</span><span>)</span><span>.</span></span></span></span> But from the previous section we know that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">K^{-1}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span><span>−</span><span>1</span></span></span></span></span></span></span></span></span></span></span></span> expresses the inner product of the RKHS derived from the covariance kernel of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t), \Pi)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>,</span><span></span><span>Π</span><span>)</span></span></span></span> in the dual basis for the projections. So in fact we are asking for the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t), \Pi)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>f</span><span>(</span><span>t</span><span>)</span><span>,</span><span></span><span>Π</span><span>)</span></span></span></span> that satisfies the constraint <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\Pi = \pi^*</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>Π</span><span></span><span>=</span><span></span></span><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>∗</span></span></span></span></span></span></span></span></span></span></span> and is regularized by the RKHS norm corresponding to a restriction of the covariance kernel of our process, which by the representer theorem will be a linear combination of restrictions of functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span>(</span><span>−</span><span>,</span><span></span><span>t</span><span>)</span><span>.</span></span></span></span></p>
<p>Abstractly, we are relying on the fact that whenever we have a positive definite kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>I</mi><mo>×</mo><mi>I</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">K \colon I \times I \to \R</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span></span><span></span><span></span><span></span><span></span><span><span>:</span></span><span></span><span>I</span><span></span><span>×</span><span></span></span><span><span></span><span>I</span><span></span><span>→</span><span></span></span><span><span></span><span><span>R</span></span></span></span></span> with RKHS <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> a finite subset <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>⊆</mo><mi>I</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">J \subseteq I,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>J</span><span></span><span>⊆</span><span></span></span><span><span></span><span>I</span><span>,</span></span></span></span> we get a natural projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>H</mi><mo>→</mo><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">P \colon H \to H_J</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>P</span><span></span><span></span><span></span><span></span><span></span><span><span>:</span></span><span></span><span>H</span><span></span><span>→</span><span></span></span><span><span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>J</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> onto the RKHS for the kernel restricted to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>×</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">J \times J</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>J</span><span></span><span>×</span><span></span></span><span><span></span><span>J</span></span></span></span> given by
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><msub><mi>K</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(v) = \sum_j \pi_j(v) K_j.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>v</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span><span><span><span><span></span><span><span>j</span></span></span><span><span></span><span><span>∑</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>v</span><span>)</span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span></span>
Given a vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>H</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v \in H,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>v</span><span></span><span>∈</span><span></span></span><span><span></span><span>H</span><span>,</span></span></span></span> what is the norm of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>v</span><span>)</span></span></span></span> in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>J</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>? Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>P</span></span></span></span> is an isometry over the span of the elements <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K_j,</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span></span></span></span> we can view <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∥</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><msub><mo stretchy="false">∥</mo><msub><mi>H</mi><mi>J</mi></msub></msub></mrow><annotation encoding="application/x-tex">\lVert P(v) \rVert_{H_J}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∥</span><span>P</span><span>(</span><span>v</span><span>)</span><span><span>∥</span><span><span><span><span><span><span></span><span><span><span><span>H</span><span><span><span><span><span><span></span><span><span>J</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> as the minimum possible norm for an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>∈</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">w \in H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>w</span><span></span><span>∈</span><span></span></span><span><span></span><span>H</span></span></span></span> solving the equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(w) = P(v).</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>P</span><span>(</span><span>w</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>P</span><span>(</span><span>v</span><span>)</span><span>.</span></span></span></span> In particular, solving a regularized problem over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>H</span></span></span></span> that depends on projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\pi_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>π</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>∈</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">j \in J</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>j</span><span></span><span>∈</span><span></span></span><span><span></span><span>J</span></span></span></span> and then restricting the solution to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>J</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the same as restricting to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>J</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and solving the regularized problem with respect to the norm on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H_J.</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>H</span><span><span><span><span><span><span></span><span><span>J</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span></p>
<p>As a final remark, note that we can informally imagine the RKHS of a Gaussian process as specifying the &#34;energy&#34; of the process in a statistical mechanics sense; although the norm of the RKHS is not defined over the same function space that the process takes values, we get the energy for the joint distribution of any finite projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t_1), \dots, f(t_m))</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span>f</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span>f</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>m</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>))</span></span></span></span> as a function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y_1, \dots, y_m)</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>y</span><span><span><span><span><span><span></span><span><span>1</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span>…</span><span></span><span>,</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>m</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> by solving
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext mathvariant="bold">minimize</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mo stretchy="false">∥</mo><mi>f</mi><msub><mo stretchy="false">∥</mo><mi>H</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext mathvariant="bold">subject</mtext><mtext> </mtext><mtext mathvariant="bold">to</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\textbf{minimize} &amp; \quad \lVert f \rVert_H \\
\textbf{subject to} &amp; \quad f(t_i) = y_i.
\end{align*}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span><span><span><span><span><span><span></span><span><span><span>minimize</span></span></span></span><span><span></span><span><span><span>subject to</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span><span><span><span><span><span></span><span><span></span><span></span><span>∥</span><span>f</span><span><span>∥</span><span><span><span><span><span><span></span><span><span>H</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span><span><span></span><span><span></span><span></span><span>f</span><span>(</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>)</span><span></span><span>=</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>.</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></span>
This is the most satisfactory way that I&#39;ve found to connect the interpretation of kernel functions in terms of regularization with their interpretation in terms of conditional expectations of a Gaussian process.</p>

    


</div>
  </body>
</html>
