<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ashawkey/stable-dreamfusion">Original</a>
    <h1>A working implementation of text-to-3D DreamFusion, powered by Stable Diffusion</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">A pytorch implementation of the text-to-3D model <strong>Dreamfusion</strong>, powered by the <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> text-to-2D model.</p>
<p dir="auto">The original paper&#39;s project page: <a href="https://dreamfusion3d.github.io/" rel="nofollow"><em>DreamFusion: Text-to-3D using 2D Diffusion</em></a>.</p>
<p dir="auto">Colab notebook for usage: <a href="https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUUvTKwUkrrlCHpF?usp=sharing" rel="nofollow"><img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<p dir="auto">Examples generated from text prompt <code>a high quality photo of a pineapple</code> viewed with the GUI in real time:</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path fill-rule="evenodd" d="M16 3.75a.75.75 0 00-1.136-.643L11 5.425V4.75A1.75 1.75 0 009.25 3h-7.5A1.75 1.75 0 000 4.75v6.5C0 12.216.784 13 1.75 13h7.5A1.75 1.75 0 0011 11.25v-.675l3.864 2.318A.75.75 0 0016 12.25v-8.5zm-5 5.075l3.5 2.1v-5.85l-3.5 2.1v1.65zM9.5 6.75v-2a.25.25 0 00-.25-.25h-7.5a.25.25 0 00-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-4.5z"></path>
</svg>
    <span aria-label="Video description pineapple.mp4">pineapple.mp4</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/25863658/194241493-f3e68f78-aefe-479e-a4a8-001424a61b37.mp4" data-canonical-src="https://user-images.githubusercontent.com/25863658/194241493-f3e68f78-aefe-479e-a4a8-001424a61b37.mp4" controls="controls" muted="muted">

  </video>
</details>

<h3 dir="auto"><a id="user-content-gallery--update-logs" aria-hidden="true" href="#gallery--update-logs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><a href="https://github.com/ashawkey/stable-dreamfusion/issues/1" data-hovercard-type="issue" data-hovercard-url="/ashawkey/stable-dreamfusion/issues/1/hovercard">Gallery</a> | <a href="https://github.com/ashawkey/stable-dreamfusion/blob/main/assets/update_logs.md">Update Logs</a></h3>

<p dir="auto">This project is a <strong>work-in-progress</strong>, and contains lots of differences from the paper. Also, many features are still not implemented now. <strong>The current generation quality cannot match the results from the original paper, and many prompts still fail badly!</strong></p>
<h2 dir="auto"><a id="user-content-notable-differences-from-the-paper" aria-hidden="true" href="#notable-differences-from-the-paper"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Notable differences from the paper</h2>
<ul dir="auto">
<li>Since the Imagen model is not publicly available, we use <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> to replace it (implementation from <a href="https://github.com/huggingface/diffusers">diffusers</a>). Different from Imagen, Stable-Diffusion is a latent diffusion model, which diffuses in a latent space instead of the original image space. Therefore, we need the loss to propagate back from the VAE&#39;s encoder part too, which introduces extra time cost in training. Currently, 15000 training steps take about 5 hours to train on a V100.</li>
<li>We use the <a href="https://github.com/NVlabs/instant-ngp/">multi-resolution grid encoder</a> to implement the NeRF backbone (implementation from <a href="https://github.com/ashawkey/torch-ngp">torch-ngp</a>), which enables much faster rendering (~10FPS at 800x800).</li>
<li>We use the Adam optimizer with a larger initial learning rate.</li>
</ul>
<h2 dir="auto"><a id="user-content-todos" aria-hidden="true" href="#todos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>TODOs</h2>
<ul dir="auto">
<li>The normal evaluation &amp; shading part.</li>
<li>Better mesh (improve the surface quality).</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/ashawkey/stable-dreamfusion.git
cd stable-dreamfusion"><pre>git clone https://github.com/ashawkey/stable-dreamfusion.git
<span>cd</span> stable-dreamfusion</pre></div>
<p dir="auto"><strong>Important</strong>: To download the Stable Diffusion model checkpoint, you should create a file called <code>TOKEN</code> under this directory (i.e., <code>stable-dreamfusion/TOKEN</code>) and copy your hugging face <a href="https://huggingface.co/docs/hub/security-tokens" rel="nofollow">access token</a> into it.</p>
<h3 dir="auto"><a id="user-content-install-with-pip" aria-hidden="true" href="#install-with-pip"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Install with pip</h3>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt

# (optional) install the tcnn backbone if using --tcnn
pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch

# (optional) install CLIP guidance for the dreamfield setting
pip install git+https://github.com/openai/CLIP.git

# (optional) install nvdiffrast for exporting textured mesh
pip install git+https://github.com/NVlabs/nvdiffrast/"><pre>pip install -r requirements.txt

<span><span>#</span> (optional) install the tcnn backbone if using --tcnn</span>
pip install git+https://github.com/NVlabs/tiny-cuda-nn/<span><span>#</span>subdirectory=bindings/torch</span>

<span><span>#</span> (optional) install CLIP guidance for the dreamfield setting</span>
pip install git+https://github.com/openai/CLIP.git

<span><span>#</span> (optional) install nvdiffrast for exporting textured mesh</span>
pip install git+https://github.com/NVlabs/nvdiffrast/</pre></div>
<h3 dir="auto"><a id="user-content-build-extension-optional" aria-hidden="true" href="#build-extension-optional"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Build extension (optional)</h3>
<p dir="auto">By default, we use <a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load" rel="nofollow"><code>load</code></a> to build the extension at runtime.
We also provide the <code>setup.py</code> to build each extension:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install all extension modules
bash scripts/install_ext.sh

# if you want to install manually, here is an example:
pip install ./raymarching # install to python path (you still need the raymarching/ folder, since this only installs the built extension.)"><pre><span><span>#</span> install all extension modules</span>
bash scripts/install_ext.sh

<span><span>#</span> if you want to install manually, here is an example:</span>
pip install ./raymarching <span><span>#</span> install to python path (you still need the raymarching/ folder, since this only installs the built extension.)</span></pre></div>
<h3 dir="auto"><a id="user-content-tested-environments" aria-hidden="true" href="#tested-environments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tested environments</h3>
<ul dir="auto">
<li>Ubuntu 22 with torch 1.12 &amp; CUDA 11.6 on a V100.</li>
</ul>

<p dir="auto">First time running will take some time to compile the CUDA extensions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="### stable-dreamfusion setting
## train with text prompt
# `-O` equals `--cuda_ray --fp16 --dir_text`
python main.py --text &#34;a hamburger&#34; --workspace trial -O

## after the training is finished:
# test (exporting 360 video, and an obj mesh with png texture)
python main.py --workspace trial -O --test

# test with a GUI (free view control!)
python main.py --workspace trial -O --test --gui

### dreamfields (CLIP) setting
python main.py --text &#34;a hamburger&#34; --workspace trial_clip -O --guidance clip
python main.py --text &#34;a hamburger&#34; --workspace trial_clip -O --test --gui --guidance clip"><pre><span><span>#</span>## stable-dreamfusion setting</span>
<span><span>#</span># train with text prompt</span>
<span><span>#</span> `-O` equals `--cuda_ray --fp16 --dir_text`</span>
python main.py --text <span><span>&#34;</span>a hamburger<span>&#34;</span></span> --workspace trial -O

<span><span>#</span># after the training is finished:</span>
<span><span>#</span> test (exporting 360 video, and an obj mesh with png texture)</span>
python main.py --workspace trial -O --test

<span><span>#</span> test with a GUI (free view control!)</span>
python main.py --workspace trial -O --test --gui

<span><span>#</span>## dreamfields (CLIP) setting</span>
python main.py --text <span><span>&#34;</span>a hamburger<span>&#34;</span></span> --workspace trial_clip -O --guidance clip
python main.py --text <span><span>&#34;</span>a hamburger<span>&#34;</span></span> --workspace trial_clip -O --test --gui --guidance clip</pre></div>

<p dir="auto">This is a simple description of the most important implementation details.
If you are interested in improving this repo, this might be a starting point.
Any contribution would be greatly appreciated!</p>
<ul dir="auto">
<li>The SDS loss is located at <code>./nerf/sd.py &gt; StableDiffusion &gt; train_step</code>:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# 1. we need to interpolate the NeRF rendering to 512x512, to feed it to SD&#39;s VAE.
pred_rgb_512 = F.interpolate(pred_rgb, (512, 512), mode=&#39;bilinear&#39;, align_corners=False)
# 2. image (512x512) --- VAE --&gt; latents (64x64), this is SD&#39;s difference from Imagen.
latents = self.encode_imgs(pred_rgb_512)
... # timestep sampling, noise adding and UNet noise predicting
# 3. the SDS loss, since UNet part is ignored and cannot simply audodiff, we manually set the grad for latents.
w = (1 - self.scheduler.alphas_cumprod[t]).to(self.device)
grad = w * (noise_pred - noise)
latents.backward(gradient=grad, retain_graph=True)"><pre><span># 1. we need to interpolate the NeRF rendering to 512x512, to feed it to SD&#39;s VAE.</span>
<span>pred_rgb_512</span> <span>=</span> <span>F</span>.<span>interpolate</span>(<span>pred_rgb</span>, (<span>512</span>, <span>512</span>), <span>mode</span><span>=</span><span>&#39;bilinear&#39;</span>, <span>align_corners</span><span>=</span><span>False</span>)
<span># 2. image (512x512) --- VAE --&gt; latents (64x64), this is SD&#39;s difference from Imagen.</span>
<span>latents</span> <span>=</span> <span>self</span>.<span>encode_imgs</span>(<span>pred_rgb_512</span>)
... <span># timestep sampling, noise adding and UNet noise predicting</span>
<span># 3. the SDS loss, since UNet part is ignored and cannot simply audodiff, we manually set the grad for latents.</span>
<span>w</span> <span>=</span> (<span>1</span> <span>-</span> <span>self</span>.<span>scheduler</span>.<span>alphas_cumprod</span>[<span>t</span>]).<span>to</span>(<span>self</span>.<span>device</span>)
<span>grad</span> <span>=</span> <span>w</span> <span>*</span> (<span>noise_pred</span> <span>-</span> <span>noise</span>)
<span>latents</span>.<span>backward</span>(<span>gradient</span><span>=</span><span>grad</span>, <span>retain_graph</span><span>=</span><span>True</span>)</pre></div>
<ul dir="auto">
<li>Other regularizations are in <code>./nerf/utils.py &gt; Trainer &gt; train_step</code>.
<ul dir="auto">
<li>The generation seems quite sensitive to regularizations on weights_sum (alphas for each ray). The original opacity loss tends to make NeRF disappear (zero density everywhere), so we use an entropy loss to replace it for now (encourages alpha to be either 0 or 1).</li>
</ul>
</li>
<li>NeRF Rendering core function: <code>./nerf/renderer.py &gt; NeRFRenderer &gt; run_cuda</code>.
<ul dir="auto">
<li>the occupancy grid based training acceleration (instant-ngp like, enabled by <code>--cuda_ray</code>) may harm the generation progress, since once a grid cell is marked as empty, rays won&#39;t pass it later...</li>
</ul>
</li>
<li>Shading &amp; normal evaluation: <code>./nerf/network*.py &gt; NeRFNetwork &gt; forward</code>. Current implementation harms training and is disabled.
<ul dir="auto">
<li>use <code>--albedo_iters 1000</code> to enable random shading mode after 1000 steps from albedo, lambertian, and textureless.</li>
<li>light direction: current implementation use a plane light source, instead of a point light source...</li>
</ul>
</li>
<li>View-dependent prompting: <code>./nerf/provider.py &gt; get_view_direction</code>.
<ul dir="auto">
<li>ues <code>--angle_overhead, --angle_front</code> to set the border. How to better divide front/back/side regions?</li>
</ul>
</li>
<li>Network backbone (<code>./nerf/network*.py</code>) can be chosen by the <code>--backbone</code> option, but <code>tcnn</code> and <code>vanilla</code> are not well tested.</li>
<li>Spatial density bias (gaussian density blob): <code>./nerf/network*.py &gt; NeRFNetwork &gt; gaussian</code>.</li>
</ul>

<ul dir="auto">
<li>
<p dir="auto">The amazing original work: <a href="https://dreamfusion3d.github.io/" rel="nofollow"><em>DreamFusion: Text-to-3D using 2D Diffusion</em></a>.</p>
<div data-snippet-clipboard-copy-content="@article{poole2022dreamfusion,
    author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
    title = {DreamFusion: Text-to-3D using 2D Diffusion},
    journal = {arXiv},
    year = {2022},
}"><pre><code>@article{poole2022dreamfusion,
    author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},
    title = {DreamFusion: Text-to-3D using 2D Diffusion},
    journal = {arXiv},
    year = {2022},
}
</code></pre></div>
</li>
<li>
<p dir="auto">Huge thanks to the <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> and the <a href="https://github.com/huggingface/diffusers">diffusers</a> library.</p>
<div data-snippet-clipboard-copy-content="@misc{rombach2021highresolution,
    title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
    author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
    year={2021},
    eprint={2112.10752},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{von-platen-etal-2022-diffusers,
    author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
    title = {Diffusers: State-of-the-art diffusion models},
    year = {2022},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/huggingface/diffusers}}
}"><pre><code>@misc{rombach2021highresolution,
    title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
    author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
    year={2021},
    eprint={2112.10752},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{von-platen-etal-2022-diffusers,
    author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},
    title = {Diffusers: State-of-the-art diffusion models},
    year = {2022},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/huggingface/diffusers}}
}
</code></pre></div>
</li>
<li>
<p dir="auto">The GUI is developed with <a href="https://github.com/hoffstadt/DearPyGui">DearPyGui</a>.</p>
</li>
</ul>
</article>
          </div></div>
  </body>
</html>
