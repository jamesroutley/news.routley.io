<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html">Original</a>
    <h1>Characterizing Emergent Phenomena in Large Language Models</h1>
    
    <div id="readability-page-1" class="page"><div>
<div id="post-body-732464377672747583">
<p><span>Posted by Jason Wei and Yi Tay, Research Scientists, Google Research, Brain Team</span>


</p><p>
The field of natural language processing (NLP) has been revolutionized by language models trained on large amounts of text data. Scaling up the size of language models often leads to improved performance and sample efficiency on a range of downstream NLP tasks. In many cases, the performance of a large language model can be predicted by extrapolating the performance trend of smaller models. For instance, the effect of scale on language model <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a> has been empirically shown to span more than <a href="https://arxiv.org/abs/2001.08361">seven orders of magnitude</a>.
</p>
<p>
On the other hand, performance for certain other tasks does not improve in a predictable fashion. For example, the <a href="https://arxiv.org/pdf/2005.14165.pdf">GPT-3 paper</a> showed that the ability of language models to perform multi-digit addition has a flat scaling curve (approximately random performance) for models from 100M to 13B parameters, at which point the performance jumped substantially. Given the growing use of language models in NLP research and applications, it is important to better understand abilities such as these that can arise unexpectedly.
</p>
<p>
In “<a href="https://openreview.net/forum?id=yzkSU5zdwD">Emergent Abilities of Large Language Models</a>,” recently published in the <em><a href="https://www.jmlr.org/tmlr/">Transactions on Machine Learning Research</a></em> (TMLR), we discuss the phenomena of <em>emergent abilities</em>, which we define as abilities that are not present in small models but are present in larger models. More specifically, we study emergence by analyzing the performance of language models as a function of language model scale, as measured by total <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">floating point operations</a> (FLOPs), or how much compute was used to train the language model. However, we also explore emergence as a function of other variables, such as dataset size or number of model parameters (see the paper for full details). Overall, we present dozens of examples of emergent abilities that result from scaling up language models. The existence of such emergent abilities raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.
</p>

<h2>Emergent Prompted Tasks</h2>

<p>
First we discuss emergent abilities that may arise in prompted tasks. In such tasks, a pre-trained language model is given a prompt for a task framed as next word prediction, and it performs the task by completing the response. Without any further fine-tuning, language models can often perform tasks that were not seen during training. 
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigsk_RT3zBrzSTftNq6czTHYkv3izej5wCEhxNrjnoUrvIPt0aJLsV8s4zIgpnyoPysHobWFhHuzCU-B30AItGMAmYRMEWY_Pp--lLmQ6--oMMWrRciyDDv7qD1zf4Y--i7avr9EHv2nsz4Q7hHTY5-JeXFKHhbUttmVruMd8Py_fqCUtaAKCwHyOF_A/s1288/image2.png"><img data-original-height="341" data-original-width="1288" height="169" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigsk_RT3zBrzSTftNq6czTHYkv3izej5wCEhxNrjnoUrvIPt0aJLsV8s4zIgpnyoPysHobWFhHuzCU-B30AItGMAmYRMEWY_Pp--lLmQ6--oMMWrRciyDDv7qD1zf4Y--i7avr9EHv2nsz4Q7hHTY5-JeXFKHhbUttmVruMd8Py_fqCUtaAKCwHyOF_A/w640-h169/image2.png" width="640"/></a></td></tr><tr><td>Example of few-shot prompting on movie review sentiment classification. The model is given one example of a task (classifying a movie review as positive or negative) and then performs the task on an unseen example.</td></tr></tbody></table>
<p>
We call a prompted task emergent when it unpredictably surges from random performance to above-random at a specific scale threshold. Below we show three examples of prompted tasks with emergent performance: <a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/modified_arithmetic">multi-step arithmetic</a>, taking <a href="https://arxiv.org/abs/2009.03300">college-level exams</a>, and <a href="https://pilehvar.github.io/wic/">identifying the intended meaning of a word</a>. In each case, language models perform poorly with very little dependence on model size up to a threshold at which point their performance suddenly begins to excel. 
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhl5PSqGGHMWNxwav2cdB6GaoiHCrKESFwkRXQ6VJmJxVGCjcuQhqJsey9EiCQW6WUKaHDaMCmYj9LGxZaVuU5DpHTh9-Wl0pRzlTybDC2WES0_jSjmyGHcHKku9XZECXceG1TCtH5DNocVj-0PQHTztf_5Zzo7Ijrj8jlT_kClaW72fxzj4-3SQOwtNQ/s1013/image4.png"><img data-original-height="487" data-original-width="1013" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhl5PSqGGHMWNxwav2cdB6GaoiHCrKESFwkRXQ6VJmJxVGCjcuQhqJsey9EiCQW6WUKaHDaMCmYj9LGxZaVuU5DpHTh9-Wl0pRzlTybDC2WES0_jSjmyGHcHKku9XZECXceG1TCtH5DNocVj-0PQHTztf_5Zzo7Ijrj8jlT_kClaW72fxzj4-3SQOwtNQ/s16000/image4.png"/></a></td></tr><tr><td>The ability to perform multi-step arithmetic (<strong>left</strong>), succeed on college-level exams (<strong>middle</strong>), and identify the intended meaning of a word in context (<strong>right</strong>) all emerge only for models of sufficiently large scale. The models shown include <a href="https://arxiv.org/abs/2201.08239">LaMDA</a>, <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, <a href="https://arxiv.org/abs/2112.11446">Gopher</a>, <a href="https://arxiv.org/abs/2203.15556">Chinchilla</a>, and <a href="https://arxiv.org/abs/2204.02311">PaLM</a>.</td></tr></tbody></table>
<p>
Performance on these tasks only becomes non-random for models of sufficient scale — for instance, above 10<sup>22</sup> training FLOPs for the arithmetic and multi-task NLU tasks, and above 10<sup>24</sup> training FLOPs for the word in context tasks. Note that although the scale at which emergence occurs can be different for different tasks and models, no model showed smooth improvement in behavior on any of these tasks. Dozens of other emergent prompted tasks are listed <a href="https://openreview.net/forum?id=yzkSU5zdwD">in our paper</a>.
</p>

<h2>Emergent Prompting Strategies</h2>


<p>
The second class of emergent abilities encompasses <em>prompting strategies</em> that augment the capabilities of language models. Prompting strategies are broad paradigms for prompting that can be applied to a range of different tasks. They are considered emergent when they fail for small models and can only be used by a sufficiently-large model.
</p>
<p>
One example of an emergent prompting strategy is called “<a href="https://twitter.com/Google/status/1525188695875366912">chain-of-thought prompting</a>”, for which the model is prompted to generate a series of intermediate steps before giving the final answer. Chain-of-thought prompting enables language models to perform tasks requiring complex reasoning, such as a multi-step math word problem. Notably, models acquire the ability to do chain-of-thought reasoning without being explicitly trained to do so. An example of chain-of-thought prompting is shown in the figure below.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjO6jryw6Oesg8YVxsa2hl2b5FSoBVfzuDou3U9LA9U6cBAIMbV1MZs5ZX5XLMHGg2jd29FPYpabC9hn7PgfC1qLDKMS7sWz6ay8XTKupyB0cB4EHu8ZpRkftQTMP5gFxyXiAPQ-dBscd6-QFEdp_P1qaUADthj0DOZ8zrZb1dBNd6nbzy4tFR-rtjkCw/s793/image3.png"><img data-original-height="370" data-original-width="793" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjO6jryw6Oesg8YVxsa2hl2b5FSoBVfzuDou3U9LA9U6cBAIMbV1MZs5ZX5XLMHGg2jd29FPYpabC9hn7PgfC1qLDKMS7sWz6ay8XTKupyB0cB4EHu8ZpRkftQTMP5gFxyXiAPQ-dBscd6-QFEdp_P1qaUADthj0DOZ8zrZb1dBNd6nbzy4tFR-rtjkCw/s16000/image3.png"/></a></td></tr><tr><td>Chain of thought prompting enables sufficiently large models to solve multi-step reasoning problems.</td></tr></tbody></table>
<p>
The empirical results of chain-of-thought prompting are shown below. For smaller models, applying chain-of-thought prompting does not outperform standard prompting, for example, when applied to <a href="https://arxiv.org/abs/2110.14168">GSM8K</a>, a challenging benchmark of math word problems. However, for large models (10<sup>24</sup> FLOPs), chain-of-thought prompting substantially improves performance in our tests, reaching a 57% solve rate on GSM8K.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDr6dLlvyfWclgmTuqlzEQ0Ge-x2CUywoKXoozXO5wMJgw5ZERIzqRy59_aDr_P9YOC3XEZ1wFqoPWmGgP26-DvdJUMzHx9-i2Nc8fyDGIwu9s5kYyhDkadS8s4azusiper7nDPk7fgUe4dNM9KVgbQkZoO3AiXQ8-rIJ4CN3YY4US2g3Us-oMNr9gPQ/s732/image1.png"><img data-original-height="561" data-original-width="732" height="306" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgDr6dLlvyfWclgmTuqlzEQ0Ge-x2CUywoKXoozXO5wMJgw5ZERIzqRy59_aDr_P9YOC3XEZ1wFqoPWmGgP26-DvdJUMzHx9-i2Nc8fyDGIwu9s5kYyhDkadS8s4azusiper7nDPk7fgUe4dNM9KVgbQkZoO3AiXQ8-rIJ4CN3YY4US2g3Us-oMNr9gPQ/w400-h306/image1.png" width="400"/></a></td></tr><tr><td>Chain-of-thought prompting is an emergent ability — it fails to improve performance for small language models, but substantially improves performance for large models. Here we illustrate the difference between standard and chain-of-thought prompting at different scales for two language models, <a href="https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html">LaMDA</a> and <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">PaLM</a>.</td></tr></tbody></table>



<h2>Implications of Emergent Abilities</h2>


<p>
The existence of emergent abilities has a range of implications. For example, because emergent few-shot prompted abilities and strategies are not explicitly encoded in pre-training, researchers may not know the full scope of few-shot prompted abilities of current language models. Moreover, the emergence of new abilities as a function of model scale raises the question of whether further scaling will potentially endow even larger models with new emergent abilities.
</p>
<p>
Identifying emergent abilities in large language models is a first step in understanding such phenomena and their potential impact on future model capabilities. Why does scaling unlock emergent abilities? Because computational resources are expensive, can emergent abilities be unlocked via other methods without increased scaling (e.g., better model architectures or training techniques)? Will new real-world applications of language models become unlocked when certain abilities emerge? Analyzing and understanding the behaviors of language models, including emergent behaviors that arise from scaling, is an important research question as the field of NLP continues to grow. 
</p>

<h2>Acknowledgements</h2>


<p>
<em>It was an honor and privilege to work with Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.</em>
</p>
</div>
</div></div>
  </body>
</html>
