<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.swtch.com/fp">Original</a>
    <h1>Floating-Point Printing and Parsing Can Be Simple and Fast</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        
       

<a href="#introduction"><h2 id="introduction">Introduction</h2></a>


<p>
A floating point number <math><mi>f</mi></math> has the form <math><mrow><mi>f</mi><mo>=</mo><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math>
where <math><mi>m</mi></math> is called the <i>mantissa</i>
and <math><mi>e</mi></math> is a signed integer <i>exponent</i>.
We like to read numbers scaled by powers of ten,
not two, so computers need algorithms to convert binary floating-point
to and from decimal text.
My 2011 post “<a href="https://research.swtch.com/ftoa">Floating Point to Decimal Conversion is Easy</a>”
argued that  these conversions can be simple as long as you
don’t care about them being fast.
But I was wrong: fast converters can be simple too,
and this post shows how.

</p><p>
The main idea of this post is to implement <i>fast unrounded scaling</i>,
which computes an approximation to <math><mrow><mi>x</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math>,
often in a single 64-bit multiplication.
On that foundation
we can build nearly trivial printing and parsing algorithms that run very fast.
In fact, the printing algorithms
run faster than all other known algorithms,
including
Dragon4 [<a id="fnref-30" href="#fn-30">30</a>],
Grisu3 [<a id="fnref-23" href="#fn-23">23</a>],
Errol3 [<a id="fnref-4" href="#fn-4">4</a>],
Ryū [<a id="fnref-2" href="#fn-2">2</a>],
Ryū Printf [<a id="fnref-3" href="#fn-3">3</a>],
Schubfach [<a id="fnref-12" href="#fn-12">12</a>],
and Dragonbox [<a id="fnref-17" href="#fn-17">17</a>],
and the parsing algorithm runs faster than
the Eisel-Lemire algorithm [<a id="fnref-22" href="#fn-22">22</a>].
This post presents both the algorithms and a concrete implementation in Go.
I expect some form of this Go code to ship in Go 1.27 (scheduled for August 2026).

</p><p>
This post is rather long—far longer than the implementations!—so here is a brief overview of the sections
for easier navigation and understanding where we’re headed.
</p><ul>
<li>
“<a href="#numbers">Fixed-Point and Floating-Point Numbers</a>”
briefly reviews fixed-point and floating-point numbers,
establishing some terminology and concepts needed for the rest of the post.
</li><li>
“<a href="#unround">Unrounded Numbers</a>” introduces the idea of unrounded numbers,
inspired by the IEEE754 floating-point extended format.
</li><li>
“<a href="#scale">Unrounded Scaling</a>” defines the unrounded scaling primitive.
</li><li>
“<a href="#fixed">Fixed-Width Printing</a>” formats floating-point numbers
with a given (fixed) number of decimal digits, at most 18.
</li><li>
“<a href="#parse">Parsing Decimals</a>” parses decimal numbers of
at most 19 digits into floating-point numbers.
</li><li>
“<a href="#short">Shortest-Width Printing</a>” formats floating-point numbers
using the shortest representation that parses back to the original number.
</li><li>
“<a href="#fast">Fast Unrounded Scaling</a>” reveals the
short but subtle implementation of fast unrounded scaling
that enables those simple algorithms.
</li><li>
“<a href="#proof">Sketch of a Proof of Fast Scaling</a>” briefly sketches the proof
that the fast unrounded scaling algorithm is correct.
A companion post, “<a href="https://cjwu.substack.com/p/fp-proof">Fast Unrounded Scaling: Proof by Ivy</a>”
provides the full details.
</li><li>
“<a href="#omit">Omit Needless Multiplications</a>” uses a key idea from the proof
to optimize the fast unrounded scaling implementation further,
reducing it to a single 64-bit multiplication in many cases.
</li><li>
“<a href="#perf">Performance</a>” compares the performance of the
implementation of these algorithms against earlier ones.
</li><li>
“<a href="#history">History and Related Work</a>” examines the history of
solutions to the floating-point printing and parsing problems
and traces the origins of the specific ideas used in this
post’s algorithms.</li></ul>


<p>
For the last decade, there has been a new algorithm for floating-point printing and parsing
every few years.
Given the simplicity and speed of the algorithms in this post
and the increasingly small deltas between successive algorithms,
perhaps we are nearing an optimal solution.
<a href="#numbers"></a></p><h2 id="numbers"><a href="#numbers">Fixed-Point and Floating-Point Numbers</a></h2>


<p>
Fixed-point numbers have the form <math><mrow><mi>f</mi><mo>=</mo><mi>m</mi><mo>·</mo><mi>B</mi><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math> for an integer mantissa <math><mi>m</mi></math>, constant base <math><mi>B</mi></math>, and constant (fixed) exponent <math><mi>e</mi></math>.
We can create fixed-point representations
in any base, but the most common are base 2 (for computers)
and base 10 (for people).
This diagram shows fixed-point numbers at various scales
that can represent numbers between 0 and 1:

</p><p>
<img name="fpfmt-ruler1" width="410" height="230" src="https://cjwu.substack.com/p/fpfmt-ruler1.svg"/>

</p><p>
Using a smaller scaling factor increases precision
at the cost of larger mantissas.
When representing very large numbers, we can use
larger scaling factors to reduce the mantissa size.
For example, here are various representations of
numbers around one billion:

</p><p>
<img name="fpfmt-ruler2" width="400" height="210" src="https://cjwu.substack.com/p/fpfmt-ruler2.svg"/>

</p><p>
Floating-point numbers are the same as base-2 fixed-point numbers except that
<math><mi>e</mi></math> changes with
the overall size of the number.
Small numbers use very small scaling factors
while large numbers use large scaling factors,
aiming to keep the mantissas a constant length.
For float64s, the exponent <math><mi>e</mi></math> is chosen so that the mantissa <math><mi>m</mi></math> has 53 bits,
meaning <math><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>53</mn></msup><mo stretchy="false">)</mo></mrow></math>.
For example, for numbers in <math><mrow><mo stretchy="false">[</mo><mn>½</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math>, float64s use <math><mrow><mi>e</mi><mo>=</mo><mo form="prefix">−</mo><mn>53</mn></mrow></math>;
for numbers in <math><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math> they use <math><mrow><mi>e</mi><mo>=</mo><mo form="prefix">−</mo><mn>52</mn></mrow></math>;
and so on.

</p><p>
[The notation <math><mrow><mo stretchy="false">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></math> is a <i>half-open interval</i>, which includes <math><mi>a</mi></math> but not <math><mi>b</mi></math>.
In contrast, the <i>closed interval</i> <math><mrow><mo stretchy="false">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">]</mo></mrow></math> includes both <math><mi>a</mi></math> and <math><mi>b</mi></math>.
We write <math><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></math> or <math><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">]</mo></mrow></math> to say that <math><mi>x</mi></math> is in that interval.
Using this notation, <math><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>53</mn></msup><mo stretchy="false">)</mo></mrow></math> means <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>≤</mo><mi>m</mi><mo>&lt;</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>53</mn></msup></mrow></math>.]

</p><p>
In addition to limiting the mantissa size, we must also limit the exponent,
to keep the overall number a fixed size.
For float64s, assuming <math><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>53</mn></msup><mo stretchy="false">)</mo></mrow></math>, the exponent <math><mrow><mi>e</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1074</mn><mo>,</mo><mn>971</mn><mo stretchy="false">]</mo></mrow></math>.

</p><p>
A float64 consists of 1 sign bit, 11 exponent bits, and 52 mantissa bits.
The <i>normal</i> 11-bit exponent encodings <code>0x001</code> through <code>0x3fe</code> denote <math><mrow><mi>e</mi><mo>=</mo><mo form="prefix">−</mo><mn>1074</mn></mrow></math> through <math><mrow><mi>e</mi><mo>=</mo><mn>971</mn></mrow></math>.
For those, the mantissa <math><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>53</mn></msup><mo stretchy="false">)</mo></mrow></math>,
and it is encoded into only 52 bits by omitting the leading 1 bit.
The special exponent encoding <code>0x3ff</code> is used for infinity and not-a-number.
That leaves the encoding <code>0x000</code>, which is also special.
It denotes <math><mrow><mi>e</mi><mo>=</mo><mo form="prefix">−</mo><mn>1074</mn></mrow></math> (like <code>0x001</code> does)
but with mantissas <math><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo stretchy="false">)</mo></mrow></math> without an implicit leading 1.
These <i>subnormals</i> or <i>denormalized numbers</i> [<a id="fnref-8" href="#fn-8">8</a>]
continue the fixed-point <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>1074</mn></mrow></msup></mrow></math> scale down to zero,
which ends up encoded (not coincidentally) as 64 zero bits.

</p><p>
Other definitions of floating point numbers use different interpretations.
For example the IEEE754 standard uses
<math><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math> with <math><mrow><mi>e</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1023</mn><mo>,</mo><mn>1023</mn><mo stretchy="false">]</mo></mrow></math>,
while the C standard libary  <i>frexp</i> function uses <math><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>½</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math> with <math><mrow><mi>e</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1022</mn><mo>,</mo><mn>1024</mn><mo stretchy="false">]</mo></mrow></math>.
Both of these choices make <math><mi>m</mi></math> itself a fixed-point number instead of an integer.
Our integer definition lets us use integer math.
These interpretations are all equivalent and differ only by a constant added to <math><mi>e</mi></math>.

</p><p>
This description of float64s applies to float32s as well, but with different constants. This table summarizes the two encodings:
</p><table id="_table1">
<tbody><tr><th></th><th>float32</th><th>float64</th></tr>
<tr><td>sign bits</td><td>1</td><td>1</td></tr>
<tr><td>encoded mantissa bits</td><td>23</td><td>52</td></tr>
<tr><td>encoded exponent bits</td><td>8</td><td>11</td></tr>
<tr><td>exponent range for <math><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></td><td><math><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>127</mn><mo>,</mo><mn>127</mn><mo stretchy="false">]</mo></mrow></math></td><td><math><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1023</mn><mo>,</mo><mn>1023</mn><mo stretchy="false">]</mo></mrow></math></td></tr>
<tr><td>exponent range for integer <math><mi>m</mi></math></td><td><math><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>150</mn><mo>,</mo><mn>104</mn><mo stretchy="false">]</mo></mrow></math></td><td><math><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1074</mn><mo>,</mo><mn>971</mn><mo stretchy="false">]</mo></mrow></math></td></tr>
<tr><td>normal numbers</td><td><math><mrow><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>23</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>24</mn></msup><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>150</mn><mo>,</mo><mn>104</mn><mo stretchy="false">]</mo></mrow></msup></mrow></math></td><td><math><mrow><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>53</mn></msup><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1074</mn><mo>,</mo><mn>971</mn><mo stretchy="false">]</mo></mrow></msup></mrow></math></td></tr>
<tr><td>subnormal numbers</td><td><math><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>23</mn></msup><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>150</mn></mrow></msup></mrow></math></td><td><math><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>1074</mn></mrow></msup></mrow></math></td></tr>
<tr><td>exponent range for 64-bit <math><mi>m</mi></math></td><td><math><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>190</mn><mo>,</mo><mn>64</mn><mo stretchy="false">]</mo></mrow></math></td><td><math><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1085</mn><mo>,</mo><mn>960</mn><mo stretchy="false">]</mo></mrow></math></td></tr>
<tr><td>normal numbers</td><td><math><mrow><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>63</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>190</mn><mo>,</mo><mn>64</mn><mo stretchy="false">]</mo></mrow></msup></mrow></math></td><td><math><mrow><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>63</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1085</mn><mo>,</mo><mn>960</mn><mo stretchy="false">]</mo></mrow></msup></mrow></math></td></tr>
<tr><td>subnormal numbers</td><td><math><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>63</mn></msup><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>190</mn></mrow></msup></mrow></math></td><td><math><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>63</mn></msup><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>1085</mn></mrow></msup></mrow></math></td></tr>
</tbody></table>


<p>
To convert a float64 to its bits, we use Go’s <a href="https://go.dev/pkg/math/#Float64bits"><code>math.Float64bits</code></a>.
</p><div><pre><span>// unpack64 returns m, e such that f = m * 2**e.</span>
<span>// The caller is expected to have handled 0, NaN, and ±Inf already.</span>
<span>// To unpack a float32 f, use unpack64(float64(f)).</span>
func unpack64(f float64) (uint64, int) {
	const shift = 64 - 53
	const minExp = -(1074 + shift)
	b := math.Float64bits(f)
	m := 1&lt;&lt;63 | (b&amp;(1&lt;&lt;52-1))&lt;&lt;shift
	e := int((b &gt;&gt; 52) &amp; (1&lt;&lt;shift - 1))
	if e == 0 {
		m &amp;^= 1 &lt;&lt; 63
		e = minExp
		s := 64 - bits.Len64(m)
		return m &lt;&lt; s, e - s
	}
	return m, (e - 1) + minExp
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L23-L39">fpfmt/fpfmt.go:23,39</a></p></div>

<p>
To convert back, we use Go’s <a href="https://go.dev/pkg/math/#Float64frombits"><code>math.Float64frombits</code></a>.
</p><div><pre><span>// pack64 takes m, e and returns f = m * 2**e.</span>
<span>// It assumes the caller has provided a 53-bit mantissa m</span>
<span>// and an exponent that is in range for the mantissa.</span>
func pack64(m uint64, e int) float64 {
	if m&amp;(1&lt;&lt;52) == 0 {
		return math.Float64frombits(m)
	}
	return math.Float64frombits(m&amp;^(1&lt;&lt;52) | uint64(1075+e)&lt;&lt;52)
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L41-L49">fpfmt/fpfmt.go:41,49</a></p></div>

<p>
[Other presentations use “fraction” and “significand” instead of “mantissa”.
This post uses mantissa for consistency with my 2011 post
and because I generally agree with Agatha Mallett’s excellent
“<a href="https://geometrian.com/projects/blog/in-defense-of-mantissa.html">In Defense of ‘Mantissa’</a>”.]
<a href="#unround"></a></p><h2 id="unround"><a href="#unround">Unrounded Numbers</a></h2>


<p>
Floating-point operations are defined as if computed exactly to infinite precision
and then rounded to the nearest actual floating-point number,
breaking ties by rounding to an even mantissa.
Of course, real implementations don’t use infinite precision;
they only keep enough precision to round properly.
We will use the same idea.
In our algorithms, we want the scaling operation to eventually evaluate to an integer,
but we want to give the caller control over the rounding step.
So instead of returning an integer, we will return an <i>unrounded number</i>,
which contains all the information needed to round it in a variety of ways.

</p><p>
The unrounded form of any real number <math><mi>x</mi></math>, which we will write as as <math><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow></math>,
is the truncated integer part of <math><mi>x</mi></math> followed by two more bits.
Those bits indicate (1) whether the fractional part of <math><mi>x</mi></math> was at least ½, and (2) whether the fractional part was not exactly 0 or ½.
If you think of <math><mi>x</mi></math> as a real number written in binary, the first extra bit is the bit immediately after the “binary point”—the bit that represents <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>1</mn></mrow></msup></mrow></math>, aka the ½ bit—and the second extra bit is the OR of all the bits after the ½ bit.

</p><p>
This definition applies even to numbers that require an infinite binary representation.
For example, just as 1/3 requires an infinite decimal representation ‘<math><mrow><mn>0.333</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace></mrow></math>’,
1.6 requires an infinite binary representation ‘<math><mrow><mn>1.1001100110011</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace></mrow></math>’.
The unrounded version <math><mrow><mtext>⟨</mtext><mn>1.6</mn><mtext>⟩</mtext></mrow></math> is finite: ‘<math><mn>1.11</mn></math>’.
But instead of reading unrounded numbers in binary,
let’s print <math><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow></math> as <math><mrow><mtext>‘</mtext><mtext><i>n</i></mtext><mn>.</mn><mtext><i>hs</i></mtext><mtext>’</mtext></mrow></math> where <math><mtext><i>n</i></mtext></math> is the integer part <math><mrow><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>&gt;&gt;</mo><mn>2</mn></mrow></math>,
<math><mtext><i>h</i></mtext></math> is 0 or 5, and <math><mtext><i>s</i></mtext></math> is ‘+’ when the second bit is 1.
Then <math><mrow><mtext>⟨</mtext><mn>1.6</mn><mtext>⟩</mtext></mrow></math> is written ‘<math><mrow><mn>1.5</mn><mtext>+</mtext></mrow></math>’.</p><p><math display="block"><mtable><mtr><mtd><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mrow><mo stretchy="false">⌊</mo><mn>4</mn><mi>x</mi><mo stretchy="false">⌋</mo></mrow><mo>|</mo><mo stretchy="false">(</mo><mn>4</mn><mi>x</mi><mo>≠</mo><mrow><mo stretchy="false">⌊</mo><mn>4</mn><mi>x</mi><mo stretchy="false">⌋</mo></mrow><mo stretchy="false">)</mo></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mn>6</mn><mspace width="0.166em"></mspace><mtext>exactly⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>24</mn><mo>=</mo><mtext>‘</mtext><mn>6.0</mn><mtext>’</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mn>6.000001</mn><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>25</mn><mo>=</mo><mtext>‘</mtext><mn>6.0</mn><mtext>+’</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mn>6.499999</mn><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>25</mn><mo>=</mo><mtext>‘</mtext><mn>6.0</mn><mtext>+’</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mn>6.5</mn><mspace width="0.166em"></mspace><mtext>exactly⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>26</mn><mo>=</mo><mtext>‘</mtext><mn>6.5</mn><mtext>’</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mn>6.500001</mn><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>27</mn><mo>=</mo><mtext>‘</mtext><mn>6.5</mn><mtext>+’</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mn>6.999999</mn><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>27</mn><mo>=</mo><mtext>‘</mtext><mn>6.5</mn><mtext>+’</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mn>7</mn><mspace width="0.166em"></mspace><mtext>exactly⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>28</mn><mo>=</mo><mtext>‘</mtext><mn>7.0</mn><mtext>’</mtext></mrow></mtd></mtr></mtable></math></p>


<p>
Let’s implement unrounded numbers in Go.
</p><div><pre>type unrounded uint64

func unround(x float64) unrounded {
	return unrounded(math.Floor(4*x)) | bool2[unrounded](math.Floor(4*x) != 4*x)
}

func (u unrounded) String() string {
	return fmt.Sprintf(&#34;⟨%d.%d%s⟩&#34;, u&gt;&gt;2, 5*((u&gt;&gt;1)&amp;1), &#34;+&#34;[1-u&amp;1:])
}
</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L52-L61">fpfmt/fpfmt.go:52,61</a></p></div>

<p>
The <code>bool2</code> function converts a boolean to an integer.
(The Go compiler will implement this using an inlined conditional move.)
</p><div><pre><span>// bool2 converts b to an integer: 1 for true, 0 for false.</span>
func bool2[T ~int | ~uint64](b bool) T {
	if b {
		return 1
	}
	return 0
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L15-L21">fpfmt/fpfmt.go:15,21</a></p></div>

<p>
We won’t use the <code>unround</code> constructor in our actual code, but it’s helpful for playing.
For example, we can try the examples we just saw:
</p><pre>row(&#34;x&#34;, &#34;raw&#34;, &#34;str&#34;)
for _, x := range []float64{6, 6.001, 6.499, 6.5, 6.501, 6.999, 7} {
    u := unround(x)
    row(x, uint64(u), u)
}
table()
</pre>

<pre>x      raw  str
6      24   ⟨6.0⟩
6.001  25   ⟨6.0+⟩
6.499  25   ⟨6.0+⟩
6.5    26   ⟨6.5⟩
6.501  27   ⟨6.5+⟩
6.999  27   ⟨6.5+⟩
7      28   ⟨7.0⟩
</pre>


<p>
The unrounded form <math><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow></math> holds the information needed by all the usual rounding operations.
Adding 0, 1, 2, or 3 and then dividing by four (or shifting right by two) yields: floor, round with ½ rounding down, round with ½ rounding up, and ceiling.
In floating-point math, we want to round with ½ rounding to even, meaning 1½ and 2½ both round to 2.
We can do that by adding <math><mrow><mn>1</mn><mo>+</mo><mtext>odd</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math>,
where <math><mrow><mtext>odd</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> is 0 or 1 according to whether <math><mi>x</mi></math> is odd.
That’s just the low bit of <math><mi>x</mi></math>:
<math><mrow><mtext>odd</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><mo>&amp;</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>&gt;&gt;</mo><mn>2</mn><mo stretchy="false">)</mo><mo>&amp;</mo><mn>1</mn></mrow></math>.

</p><p>
Putting that all together:</p><p><math display="block"><mtable><mtr><mtd><mrow><mo>⌊</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>⌋</mo></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>+</mo><mn>0</mn><mo stretchy="false">)</mo><mo>&gt;&gt;</mo><mn>2</mn></mrow></mtd><mtd><mtext>(floor)</mtext></mtd></mtr><mtr><mtd><msup><mrow><mo>[</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>]</mo></mrow><mo>−</mo></msup></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>&gt;&gt;</mo><mn>2</mn></mrow></mtd><mtd><mrow><mtext>(round,</mtext><mspace width="0.3em"></mspace><mtext>half</mtext><mspace width="0.3em"></mspace><mtext>down)</mtext></mrow></mtd></mtr><mtr><mtd><msup><mrow><mo>[</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>]</mo></mrow><mtext>even</mtext></msup></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>+</mo><mn>1</mn><mo>+</mo><mtext>odd</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>&gt;&gt;</mo><mn>2</mn></mrow></mtd><mtd><mrow><mtext>(round,</mtext><mspace width="0.3em"></mspace><mtext>half</mtext><mspace width="0.3em"></mspace><mtext>to</mtext><mspace width="0.3em"></mspace><mtext>even)</mtext></mrow></mtd></mtr><mtr><mtd><mrow></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>+</mo><mn>1</mn><mo>+</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>&gt;&gt;</mo><mn>2</mn><mo stretchy="false">)</mo><mo>&amp;</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>&gt;&gt;</mo><mn>2</mn></mrow></mtd><mtd><mrow></mrow></mtd></mtr><mtr><mtd><msup><mrow><mo>[</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>]</mo></mrow><mo>+</mo></msup></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>+</mo><mn>2</mn><mo stretchy="false">)</mo><mo>&gt;&gt;</mo><mn>2</mn></mrow></mtd><mtd><mrow><mtext>(round,</mtext><mspace width="0.3em"></mspace><mtext>half</mtext><mspace width="0.3em"></mspace><mtext>up)</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mo>⌈</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>⌉</mo></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>+</mo><mn>3</mn><mo stretchy="false">)</mo><mo>&gt;&gt;</mo><mn>2</mn></mrow></mtd><mtd><mtext>(ceiling)</mtext></mtd></mtr></mtable></math></p>


<p>
In Go:
</p><div><pre>func (u unrounded) floor() uint64         { return uint64((u + 0) &gt;&gt; 2) }
func (u unrounded) roundHalfDown() uint64 { return uint64((u + 1) &gt;&gt; 2) }
func (u unrounded) round() uint64         { return uint64((u + 1 + (u&gt;&gt;2)&amp;1) &gt;&gt; 2) }
func (u unrounded) roundHalfUp() uint64   { return uint64((u + 2) &gt;&gt; 2) }
func (u unrounded) ceil() uint64          { return uint64((u + 3) &gt;&gt; 2) }</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L62-L66">fpfmt/fpfmt.go:62,66</a></p></div>
<pre>row(&#34;x&#34;, &#34;floor&#34;, &#34;round½↓&#34;, &#34;round&#34;, &#34;round½↑&#34;, &#34;ceil&#34;)
for _, x := range []float64{6, 6.25, 6.5, 6.75, 7, 7.5, 8.5} {
    u := unround(x)
    row(u, u.floor(), u.roundHalfDown(), u.round(), u.roundHalfUp(), u.ceil())
}
table()
</pre>

<pre>x       floor  round½↓  round  round½↑  ceil
⟨6.0⟩   6      6        6      6        6
⟨6.0+⟩  6      6        6      6        7
⟨6.5⟩   6      6        6      7        7
⟨6.5+⟩  6      7        7      7        7
⟨7.0⟩   7      7        7      7        7
⟨7.5⟩   7      7        8      8        8
⟨8.5⟩   8      8        8      9        9
</pre>


<p>
Dividing unrounded numbers preserves correct rounding as long as the second extra bit
is maintained correctly: once it is set to 1, it has to stay a 1 in all future results.
This gives the second extra bit its shorter name: the <i>sticky bit</i>.

</p><p>
To divide an unrounded number, we do a normal divide but force the sticky bit to 1
when there is a remainder.
Right shift does the same.</p><p><math display="block"><mtable><mtr><mtd><mrow><mtext>⟨</mtext><mi>x</mi><mn>/</mn><mi>n</mi><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mn>/</mn><mi>n</mi><mo stretchy="false">)</mo><mo>|</mo><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>mod</mo><mi>n</mi><mo>≠</mo><mn>0</mn><mo stretchy="false">)</mo><mo>|</mo><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>&amp;</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mi>x</mi><mo>&gt;&gt;</mo><mi>n</mi><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>&gt;&gt;</mo><mi>n</mi><mo stretchy="false">)</mo><mo>|</mo><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>mod</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>n</mi></msup><mo>≠</mo><mn>0</mn><mo stretchy="false">)</mo><mo>|</mo><mo stretchy="false">(</mo><mrow><mtext>⟨</mtext><mi>x</mi><mtext>⟩</mtext></mrow><mo>&amp;</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mtd></mtr></mtable></math></p>


<p>
For example, if we rounded 15.4 to an integer 15 and then divided it by 6,
we’d get 2.5, which rounds down to 2,
but the more precise answer would be 15.4/6 = 2.57, which rounds up to 3.
An unrounded division handles this correctly:</p><p><math display="block"><mtable><mtr><mtd><mrow><mtext>⟨</mtext><mn>15.4</mn><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>61</mn><mspace width="0.3em"></mspace><mo>‘</mo><mn>15.0</mn><mtext>+</mtext><mo>’</mo><mrow><mspace width="0.3em"></mspace><mtext>“a</mtext><mspace width="0.3em"></mspace><mtext>little</mtext><mspace width="0.3em"></mspace><mtext>more</mtext><mspace width="0.3em"></mspace><mtext>than</mtext><mspace width="0.3em"></mspace><mtext>15”</mtext></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mtext>⟨</mtext><mn>15.4/6</mn><mtext>⟩</mtext></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>11</mn><mspace width="0.3em"></mspace><mo>‘</mo><mn>2.5</mn><mtext>+</mtext><mo>’</mo><mrow><mspace width="0.3em"></mspace><mtext>“a</mtext><mspace width="0.3em"></mspace><mtext>little</mtext><mspace width="0.3em"></mspace><mtext>more</mtext><mspace width="0.3em"></mspace><mtext>than</mtext><mspace width="0.3em"></mspace><mtext>2½”</mtext></mrow></mrow></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtext>⟨</mtext><mn>15.4/6</mn><mtext>⟩</mtext></mrow><mo>]</mo></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mn>3</mn></mtd></mtr></mtable></math></p>


<p>
Let’s implement division and right shift in Go:
</p><div><pre>func (u unrounded) div(d uint64) unrounded {
	x := uint64(u)
	return unrounded(x/d) | u&amp;1 | bool2[unrounded](x%d != 0)
}

func (u unrounded) rsh(s int) unrounded {
	return u&gt;&gt;s | u&amp;1 | bool2[unrounded](u&amp;((1&lt;&lt;s)-1) != 0)
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L69-L76">fpfmt/fpfmt.go:69,76</a></p></div>
<pre>u := unround(15.1).div(6)
fmt.Println(u, u.round())
</pre>

<pre>⟨2.5+⟩ 3
</pre>


<p>
Finally, we are going to need to be able to nudge an unrounded number
up or down before computing a ceiling or floor,
as if we added or subtracting a tiny amount.
Let’s add that:
</p><div><pre>func (u unrounded) nudge(δ int) unrounded { return u + unrounded(δ) }</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L67-L">fpfmt/fpfmt.go:67</a></p></div>
<pre>row(&#34;x&#34;, &#34;nudge(-1).floor&#34;, &#34;floor&#34;, &#34;ceil&#34;, &#34;nudge(+1).ceil&#34;)
for _, x := range []float64{15, 15.1, 15.9, 16} {
    u := unround(x)
    row(u, u.nudge(-1).floor(), u.floor(), u.ceil(), u.nudge(+1).ceil())
}
</pre>

<pre>x        nudge(-1).floor  floor  ceil  nudge(+1).ceil
⟨15.0⟩   14               15     15    16
⟨15.0+⟩  15               15     16    16
⟨15.5+⟩  15               15     16    16
⟨16.0⟩   15               16     16    17
</pre>


<p>
Floating-point hardware maintains three extra bits to round
all arithmetic operations correctly.
For just division and right shift, we can get by with only two bits.
<a href="#scale"></a></p><h2 id="scale"><a href="#scale">Unrounded Scaling</a></h2>


<p>
The fundamental insight of this post is that all
floating-point conversions can be written correctly
and simply using <i>unrounded scaling</i>,
which multiplies a number <math><mi>x</mi></math> by a power of two and a power of ten
and returns the unrounded product.</p><p><math display="block"><mrow><mtext>uscale</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>e</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mtext>⟨</mtext><mi>x</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mtext>⟩</mtext></mrow><mn>.</mn></mrow></math></p>


<p>
When <math><mi>p</mi></math> is negative, the value <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math>
cannot be stored exactly in any finite binary floating-point number,
so any implementation of uscale must be careful.

</p><p>
In Go, we can implement uscale using big integers and an unrounded division:
</p><pre>func uscale(x uint64, e, p int) unrounded {
    num :=   mul(big(4), big(x), pow(2, max(0, p)),  pow(10, max(0, e)))
    denom := mul(                pow(2, max(0, -p)), pow(10, max(0, -e)))
    div, mod := divmod(num, denom)
    return unrounded(div.uint64() | bool2[uint64](!mod.isZero()))
}
</pre>


<p>
The <code>max</code> expressions choose between multiplying <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math> into <code>num</code> when <math><mrow><mi>e</mi><mo>&gt;</mo><mn>0</mn></mrow></math>
or multiplying <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mi>e</mi></mrow></msup></mrow></math> into <code>denom</code> when <math><mrow><mi>e</mi><mo>&lt;</mo><mn>0</mn></mrow></math>,
and similarly for <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math>.
The <code>divmod</code> implements the floor, and <code>mod.isZero</code> reports
whether the floor was exact.

</p><p>
This implementation of uscale is correct but inefficient.
In our usage, <math><mi>e</mi></math> and <math><mi>p</mi></math> will mostly cancel out,
typically with opposite signs,
and the input <math><mi>x</mi></math> and result <math><mrow><mtext>uscale</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>e</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></math>,
will always fit in 64 bits.
That limited input domain and range makes it possible
to implement a very fast, completely accurate uscale,
and we’ll see that implementation later.

</p><p>
Our actual implementation will be split into two functions,
to allow sharing some computations derived from <math><mi>p</mi></math> and <math><mi>e</mi></math>.
Instead of <code>uscale(x, e, p)</code>, the fast Go version will be called as <code>uscale(x, prescale(e, p, log2Pow10(p)))</code>.
Also, callers are responsible for passing in an <math><mi>x</mi></math> left-shifted to have its
high bit set.
The <code>unpack</code> function we looked at already arranged that for its result,
but otherwise callers need to do something like:
</p><pre>shift = 64 - bits.Len64(x)
... uscale(x&lt;&lt;shift, prescale(e-shift, p, log2Pow10(p))) ...
</pre>


<p>
Conceptually, uscale maps numbers on one fixed-point scale to numbers on another,
including converting between binary and decimal scales.
For example, consider the scales <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>13</mn></mrow></msup></mrow></math> and <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>4</mn></mrow></msup></mrow></math>:

</p><p>
<img name="fpfmt-ruler-scale" width="210" height="250" src="https://cjwu.substack.com/p/fpfmt-ruler-scale.svg"/>

</p><p>
Given <math><mi>x</mi></math> from the <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>13</mn></mrow></msup></mrow></math> side,
<math><mrow><mtext>uscale</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mo form="prefix">−</mo><mn>13</mn><mo>,</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math> maps to the equivalent
point on the <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>4</mn></mrow></msup></mrow></math> side;
and given <math><mi>x</mi></math> from the <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>4</mn></mrow></msup></mrow></math>,
<math><mrow><mtext>uscale</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mn>13</mn><mo>,</mo><mo form="prefix">−</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math> maps to the equivalent
point on the <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>13</mn></mrow></msup></mrow></math> side.
Before we look at the fast implementation of <math><mtext>uscale</mtext></math>,
let’s look at how it simplifies all the floating-point printing
and parsing algorithms.
<a href="#fixed"></a></p><h2 id="fixed"><a href="#fixed">Fixed-Width Printing</a></h2>


<p>
Our first application of uscale is fixed-width printing.
Given <math><mrow><mi>f</mi><mo>=</mo><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math>, we want to compute its
approximate equivalent
<math><mrow><mi>d</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mtext><i>de</i></mtext></msup></mrow></math>, where <math><mi>d</mi></math> has exactly <math><mi>n</mi></math> digits.
It only takes 17 digits to uniquely identify any float64,
so we’re willing to limit <math><mrow><mi>n</mi><mo>≤</mo><mn>18</mn></mrow></math>,
which will ensure <math><mi>d</mi></math> fits in a uint64.
The strategy is to multiply <math><mi>f</mi></math> by <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math> for some <math><mi>p</mi></math>
and then round it to an integer <math><mi>d</mi></math>.
Then the result is <math><mrow><mi>d</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mi>p</mi></mrow></msup></mrow></math>.

</p><p>
The <math><mi>n</mi></math>-digit requirement means <math><mrow><mi>d</mi><mo>=</mo><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo>∈</mo><mo stretchy="false">[</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mi>n</mi><mo lspace="0" rspace="0">−</mo><mn>1</mn></mrow></msup><mo>,</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>n</mi></msup><mo stretchy="false">)</mo></mrow></math>.
From this we can derive <math><mi>p</mi></math>:</p><p><math display="block"><mtable><mtr><mtd><mrow><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo stretchy="false">[</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mi>n</mi><mo lspace="0" rspace="0">−</mo><mn>1</mn></mrow></msup><mo>,</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>n</mi></msup><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mi>n</mi><mo lspace="0" rspace="0">−</mo><mn>1</mn></mrow></msup><mo>·</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>10</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mtext>[factoring</mtext><mspace width="0.3em"></mspace><mtext>range]</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo stretchy="false">)</mo><mo>+</mo><mi>p</mi></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mi>n</mi><mo>−</mo><mn>1</mn><mo>+</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mtext>[taking</mtext><mspace width="0.3em"></mspace><mtext>log]</mtext></mrow></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mi>n</mi><mo>−</mo><mn>1</mn><mo>−</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mrow><mtext>[isolating</mtext><mspace width="0.3em"></mspace></mrow><mi>p</mi><mtext>]</mtext></mrow></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mi>n</mi><mo>−</mo><mn>1</mn><mo>−</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo stretchy="false">)</mo><mo>−</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mtd><mtd><mtext>[regrouping]</mtext></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mi>n</mi><mo>−</mo><mn>1</mn><mo>−</mo><mrow><mo stretchy="false">⌊</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo stretchy="false">⌋</mo></mrow></mrow></mtd><mtd><mrow><mtext>[</mtext><mi>p</mi><mrow><mspace width="0.3em"></mspace><mtext>is</mtext><mspace width="0.3em"></mspace><mtext>an</mtext><mspace width="0.3em"></mspace><mtext>integer]</mtext></mrow></mrow></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mi>n</mi><mo>−</mo><mn>1</mn><mo>−</mo><mrow><mo stretchy="false">⌊</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo stretchy="false">)</mo><mo>·</mo><mo stretchy="false">(</mo><mi>e</mi><mo>+</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">⌋</mo></mrow></mrow></mtd><mtd><mrow><mtext>[changing</mtext><mspace width="0.3em"></mspace><mtext>log</mtext><mspace width="0.3em"></mspace><mtext>base]</mtext></mrow></mtd></mtr></mtable></math></p>


<p>
It is okay for <math><mi>p</mi></math> to be too big—we will get an extra digit that we can divide away—so
we can approximate <math><mrow><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>m</mi></mrow></math> as <math><mrow><mtext>bits</mtext><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow></math>, where <math><mrow><mtext>bits</mtext><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></math> is the bit length of <math><mi>m</mi></math>.
That gives us <math><mrow><mi>p</mi><mo>=</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo>−</mo><mrow><mo stretchy="false">⌊</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo stretchy="false">)</mo><mo>·</mo><mo stretchy="false">(</mo><mi>e</mi><mo>+</mo><mtext>bits</mtext><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">⌋</mo></mrow></mrow></math>.
With this derivation of <math><mi>p</mi></math>, uscale does the rest of the work.

</p><p>
The floor expression is a simple linear function and can be computed
exactly for our inputs using fixed-point arithmetic:
</p><div><pre><span>// log10Pow2(x) returns ⌊log₁₀ 2**x⌋ = ⌊x * log₁₀ 2⌋.</span>
func log10Pow2(x int) int {
	<span>// log₁₀ 2 ≈ 0.30102999566 ≈ 78913 / 2^18</span>
	return (x * 78913) &gt;&gt; 18
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L78-L82">fpfmt/fpfmt.go:78,82</a></p></div>

<p>
The <code>log2Pow10</code> function, which we mentioned above and need to
use when calling <code>prescale</code>, is similar:
</p><div><pre><span>// log2Pow10(x) returns ⌊log₂ 10**x⌋ = ⌊x * log₂ 10⌋.</span>
func log2Pow10(x int) int {
	<span>// log₂ 10 ≈ 3.32192809489 ≈ 108853 / 2^15</span>
	return (x * 108853) &gt;&gt; 15
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L84-L88">fpfmt/fpfmt.go:84,88</a></p></div>

<p>
Now we can put everything together:
</p><div><pre><span>// FixedWidth returns the n-digit decimal form of f as d * 10**p.</span>
<span>// n can be at most 18.</span>
func FixedWidth(f float64, n int) (d uint64, p int) {
	if n &gt; 18 {
		panic(&#34;too many digits&#34;)
	}
	m, e := unpack64(f)
	p = n - 1 - log10Pow2(e+63)
	u := uscale(m, prescale(e, p, log2Pow10(p)))
	d = u.round()
	if d &gt;= uint64pow10[n] {
		d, p = u.div(10).round(), p-1
	}
	return d, -p
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L96-L110">fpfmt/fpfmt.go:96,110</a></p></div>

<p>
That’s the entire conversion!

</p><p>
The code splits <math><mi>f</mi></math> into <math><mi>m</mi></math>, <math><mi>e</mi></math>;
computes <math><mi>p</mi></math> as just described;
and then uses <code>uscale</code> and <code>round</code> to compute
<math><mrow><mi>d</mi><mo>=</mo><mi>f</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math>.
If the result has an extra digit,
either because our approximate log made <math><mi>p</mi></math> too big,
or because of rollover during rounding,
we divide the unrounded form by 10, round again, and update <math><mi>p</mi></math>.
When we approximated <math><mrow><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>m</mi></mrow></math> by counting bits,
we used the exact log of the greatest power of two less than or equal to <math><mi>m</mi></math>,
so the computed <math><mi>d</mi></math> must be less than twice the intended limit <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>n</mi></msup></mrow></math>,
meaning the leading digit (if there are too many digits) must be 1.
And rollover only happens for ‘<math><mrow><mn>999</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace><mn>.</mn><mspace width="0.166em"></mspace></mrow></math>’,
so it is not possible to have both an extra digit and rollover.

</p><p>
As an example conversion,
consider a float64 approximation of <math><mi>π</mi></math> (<math><mrow><mtext><code>0x1921fb54442d18</code></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>51</mn></mrow></msup></mrow></math>) to 15 decimal digits.
We have <math><mrow><mi>e</mi><mo>=</mo><mo form="prefix">−</mo><mn>51</mn></mrow></math>, <math><mrow><mi>n</mi><mo>=</mo><mn>15</mn></mrow></math>, and <math><mrow><mtext>bits</mtext><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo>=</mo><mn>53</mn></mrow></math>,
so <math><mrow><mi>p</mi><mo>=</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo>−</mo><mrow><mo stretchy="false">⌊</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo stretchy="false">)</mo><mo>·</mo><mo stretchy="false">(</mo><mi>e</mi><mo>+</mo><mtext>bits</mtext><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">⌋</mo></mrow><mo>=</mo><mn>14</mn></mrow></math>.

</p><p>
The <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>51</mn></mrow></msup></mrow></math> and <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>14</mn></mrow></msup></mrow></math> scales align like this:

</p><p>
<img name="fpfmt-ruler-pi" width="340" height="190" src="https://cjwu.substack.com/p/fpfmt-ruler-pi.svg"/>

</p><p>
Then <math><mrow><mtext>uscale</mtext><mo stretchy="false">(</mo><mtext><code>0x1921fb54442d18</code></mtext><mo>,</mo><mo form="prefix">−</mo><mn>51</mn><mo>,</mo><mn>14</mn><mo stretchy="false">)</mo></mrow></math> returns the unrounded number ‘314159265358979.0+’,
which rounds to 314159265358979.
Our answer is then <math><mrow><mn>314159265358979</mn><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>14</mn></mrow></msup></mrow></math>.
<a href="#parse"></a></p><h2 id="parse"><a href="#parse">Parsing Decimals</a></h2>


<p>
Unrounded scaling also lets us parse decimal representations of floating-point numbers efficiently.
Let’s assume we’ve taken care of parsing a string like ‘1.23e45’
and now have an integer and exponent like <math><mrow><mi>d</mi><mo>=</mo><mn>123</mn></mrow></math>, <math><mrow><mi>p</mi><mo>=</mo><mn>45</mn><mo>−</mo><mn>2</mn><mo>=</mo><mn>43</mn></mrow></math>.
To convert <math><mrow><mi>d</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math> to a float64,
we can choose an appropriate <math><mi>e</mi></math> so that <math><mrow><mi>d</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo>∈</mo><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>53</mn></msup><mo stretchy="false">)</mo></mrow></math>
and then return <math><mrow><mo stretchy="false">[</mo><mtext>uscale</mtext><mo stretchy="false">(</mo><mi>d</mi><mo>,</mo><mi>e</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mi>e</mi></mrow></msup></mrow></math>.

</p><p>
The derivation of <math><mi>e</mi></math> is similar to the derivation of <math><mi>p</mi></math> for printing:</p><p><math display="block"><mtable><mtr><mtd><mrow><mi>d</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>53</mn></msup><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow></mrow></mtd></mtr><mtr><mtd><mrow><mi>d</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo>·</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mtext>[factoring</mtext><mspace width="0.3em"></mspace><mtext>range]</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>d</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo stretchy="false">)</mo><mo>+</mo><mi>e</mi></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mn>52</mn><mo>+</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mtext>[taking</mtext><mspace width="0.3em"></mspace><mtext>log]</mtext></mrow></mtd></mtr><mtr><mtd><mi>e</mi></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mn>52</mn><mo>−</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>d</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mrow><mtext>[isolating</mtext><mspace width="0.3em"></mspace></mrow><mi>e</mi><mtext>]</mtext></mrow></mtd></mtr><mtr><mtd><mi>e</mi></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mn>52</mn><mo>−</mo><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>d</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo stretchy="false">)</mo><mo>−</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mtd><mtd><mtext>[regrouping]</mtext></mtd></mtr><mtr><mtd><mi>e</mi></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>52</mn><mo>−</mo><mrow><mo stretchy="false">⌊</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>d</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo stretchy="false">⌋</mo></mrow></mrow></mtd><mtd><mrow><mtext>[</mtext><mi>p</mi><mrow><mspace width="0.3em"></mspace><mtext>is</mtext><mspace width="0.3em"></mspace><mtext>an</mtext><mspace width="0.3em"></mspace><mtext>integer]</mtext></mrow></mrow></mtd></mtr><mtr><mtd><mi>e</mi></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mn>52</mn><mo>−</mo><mrow><mo stretchy="false">⌊</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>d</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mn>10</mn><mo stretchy="false">)</mo><mo>·</mo><mi>p</mi><mo stretchy="false">⌋</mo></mrow></mrow></mtd><mtd><mrow><mtext>[changing</mtext><mspace width="0.3em"></mspace><mtext>log</mtext><mspace width="0.3em"></mspace><mtext>base]</mtext></mrow></mtd></mtr></mtable></math></p>


<p>
Once again, it is okay to overestimate <math><mi>e</mi></math>, so we can approximate
<math><mrow><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>d</mi><mo>=</mo><mtext>bits</mtext><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow></math>, yielding <math><mrow><mi>e</mi><mo>=</mo><mn>53</mn><mo>−</mo><mtext>bits</mtext><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo><mo>−</mo><mrow><mo stretchy="false">⌊</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mn>10</mn><mo stretchy="false">)</mo><mo>·</mo><mi>p</mi><mo stretchy="false">⌋</mo></mrow></mrow></math>.
If <math><mi>e</mi></math> is very large, <math><mrow><mo>−</mo><mi>e</mi></mrow></math> will be very small,
meaning we will be creating a subnormal,
so we need to round to a smaller number of bits.
To handle this, we cap <math><mi>e</mi></math> at 1074,
which caps <math><mrow><mo>−</mo><mi>e</mi></mrow></math> at <math><mrow><mo>−</mo><mn>1074</mn></mrow></math>.
As before, due to the approximation of <math><mrow><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mi>d</mi></mrow></math>, the scaled result is at most twice as large as our target,
meaning it might have one extra bit to shift away.
</p><div><pre><span>// Parse rounds d * 10**p to the nearest float64 f.</span>
<span>// d can have at most 19 digits.</span>
func Parse(d uint64, p int) float64 {
	if d &gt; 1e19 {
		panic(&#34;too many digits&#34;)
	}
	b := bits.Len64(d)
	e := min(1074, 53-b-log2Pow10(p))
	u := uscale(d&lt;&lt;(64-b), prescale(e-(64-b), p, log2Pow10(p)))
	m := u.round()
	if m &gt;= 1&lt;&lt;53 {
		m, e = u.rsh(1).round(), e-1
	}
	return pack64(m, -e)
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/unopt/fpfmt.go#L111-L125">fpfmt/unopt/fpfmt.go:111,125</a></p></div>

<p>
<code>FixedWidth</code> and <code>Parse</code> demonstrate
exactly how similar printing and parsing really are.
In printing, we are given <math><mi>m</mi></math>, <math><mi>e</mi></math> and
find <math><mi>p</mi></math>; then <math><mrow><mtext>uscale</mtext><mo stretchy="false">(</mo><mi>m</mi><mo>,</mo><mi>e</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></math> converts binary to decimal.
In parsing, we are given <math><mi>d</mi></math>, <math><mi>p</mi></math> and find <math><mi>e</mi></math>;
then <math><mrow><mtext>uscale</mtext><mo stretchy="false">(</mo><mi>d</mi><mo>,</mo><mi>e</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></math> converts decimal to binary.

</p><p>
We can make parsing a little faster with a few hand optimizations.
This optimized version introduces <code>lp</code> to avoid calling <code>log2Pow10</code> twice,
and it implements the extra digit handling in branch-free code.
</p><div><pre><span>// Parse rounds d * 10**p to the nearest float64 f.</span>
<span>// d can have at most 19 digits.</span>
func Parse(d uint64, p int) float64 {
	if d &gt; 1e19 {
		panic(&#34;too many digits&#34;)
	}
	b := bits.Len64(d)
	lp := log2Pow10(p)
	e := min(1074, 53-b-lp)
	u := uscale(d&lt;&lt;(64-b), prescale(e-(64-b), p, lp))

	<span>// This block is branch-free code for:</span>
	<span>//	if u.round() &gt;= 1&lt;&lt;53 {</span>
	<span>//		u = u.rsh(1)</span>
	<span>//		e = e - 1</span>
	<span>//	}</span>
	s := bool2[int](u &gt;= unmin(1&lt;&lt;53))
	u = (u &gt;&gt; s) | u&amp;1
	e = e - s

	return pack64(u.round(), -e)
}

<span>// unmin returns the minimum unrounded that rounds to x.</span>
func unmin(x uint64) unrounded {
	return unrounded(x&lt;&lt;2 - 2)
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L112-L138">fpfmt/fpfmt.go:112,138</a></p></div>

<p>
Now we are ready for our next challenge: shortest-width printing.
<a href="#short"></a></p><h2 id="short"><a href="#short">Shortest-Width Printing</a></h2>


<p>
Shortest-width printing means to prepare a decimal representation
that a floating-point parser would convert back to the exact same <code>float64</code>,
using as few digits as possible.
When there are multiple possible shortest decimal outputs,
we insist on the one that is nearest the original input,
namely the correctly-rounded one.
In general, 17 digits are always enough to uniquely identify a <code>float64</code>,
but sometimes fewer can be used, even down to a single digit in numbers like 1, 2e10, and 3e−42.

</p><p>
An obvious approach would be to use <code>FixedPrint</code> for increasing values of <code>n</code>,
stopping when <code>Parse(FixedPrint(f, n)) == f</code>.
Or maybe we should derive an equation for <code>n</code> and then use <code>FixedPrint(f, n)</code> directly.
Surprisingly, neither approach works:
<code>Short(f)</code> is not necessarily <code>FixedPrint(f, n)</code> for some <code>n</code>.
The simplest demonstration of this is <math><mrow><mi>f</mi><mo>=</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>89</mn></msup><mo>=</mo><mn>6189700196426901</mn><mspace width="0.166em"></mspace><mn>37449562112</mn><mo>=</mo><mtext><code>0x10000000000000</code></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>37</mn></msup></mrow></math>,
which looks like this:

</p><p>
<img name="fpfmt-ruler-skew" width="370" height="190" src="https://cjwu.substack.com/p/fpfmt-ruler-skew.svg"/>

</p><p>
Because <math><mi>f</mi></math> is a power of two, the floating-point exponent
changes at <math><mi>f</mi></math>,
as does the spacing between floating-point numbers.
The next smallest value is <math><mrow><mtext><code>0x1ffffffffffff</code></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>38</mn></mrow></msup></mrow></math>,
marked on the diagram as <math><mrow><mtext><code>0xffffffffffff½</code></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mn>37</mn></mrow></msup></mrow></math>.
The dotted lines mark the halfway points between <math><mi>f</mi></math>
and its nearest floating point neighbors.
The accurate decimal answers are those at or between the dotted lines,
all of which convert back to <math><mi>f</mi></math>.

</p><p>
The correct rounding of <math><mi>f</mi></math> to 16 digits ends in …901: the next digit in <math><mi>f</mi></math> is 3,
so we should round down.
However, because of the spacing change around <math><mi>f</mi></math>,
that correct decimal rounding does not convert back to <math><mi>f</mi></math>.
A <code>FixedPrint</code> loop would choose a 17-digit form instead.
But there is an accurate 16-digit form, namely …902.
That decimal is closer to <math><mi>f</mi></math> than it is to any other float64,
making it an accurate <math><mi>d</mi></math>.
And since the closer 16-digit value …901 is not an accurate <math><mi>d</mi></math>,
<code>Short</code> should use …902 instead.

</p><p>
Assuming as usual that <math><mrow><mi>f</mi><mo>=</mo><mi>m</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math>,
let’s define
<math><mrow><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></math>
to be the distance between the midpoints from <math><mi>f</mi></math> to its
floating-point neighbors.
Normally those neighbors are <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math>
in either direction—the midpoints are <math><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>±</mo><mn>½</mn><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math>—so
<math><mrow><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math>.
At a power of two with an exponent change,
the lower midpoint is instead <math><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>−</mo><mn>¼</mn><mo stretchy="false">)</mo><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math>,
so <math><mrow><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mn>¾</mn><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math>.
The rounding paradox can only happen for powers of two
with this kind of skewed footprint.

</p><p>
All that is to say we cannot use <code>FixedWidth</code> with “the right <math><mi>n</mi></math>”.
But we can use scale directly with “the right <math><mi>p</mi></math>.”
Specifically, we can compute the midpoints between <math><mi>f</mi></math>
and its floating-point neighbors
and scale them to obtain the
minimum and maximum valid choices for <math><mi>d</mi></math>.
Then we can make the best choice:
</p><ul>
<li>
If one of the valid <math><mi>d</mi></math> ends in 0, use it after removing trailing zeros. </li><li>
If there is only one valid <math><mi>d</mi></math>, use it.
</li><li>
Otherwise there are at least two valid <math><mi>d</mi></math>, at least one on each side of <math><mi>f</mi></math>;
use the correctly rounded one.</li></ul>


<p>
Here is an example of the first case: one of the valid <math><mi>d</mi></math> ends in zero.

</p><p>
<img name="fpfmt-ruler-trimzero" width="370" height="190" src="https://cjwu.substack.com/p/fpfmt-ruler-trimzero.svg"/>

</p><p>
We already saw an example of the second case: only one valid <math><mi>d</mi></math>.
For numbers with symmetric footprints, that will be the
correctly rounded <math><mi>d</mi></math>.
As we saw for numbers with skewed footprints,
that may not be the correctly rounded <math><mi>d</mi></math>,
but it is still the correct answer.

</p><p>
Finally, here is an example of the third case: multiple valid <math><mi>d</mi></math>,
but none that end in zero.
Now we should use the correctly rounded one.

</p><p>
<img name="fpfmt-ruler-many" width="370" height="190" src="https://cjwu.substack.com/p/fpfmt-ruler-many.svg"/>

</p><p>
This sounds great, but how do we determine the right <math><mi>p</mi></math>?
We want <math><mrow><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></math> to allow at least one decimal integer,
but at most ten, meaning <math><mrow><mn>1</mn><mo>≤</mo><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mn>10</mn></mrow></math>.
Luckily, we can hit that target exactly.

</p><p>
For a symmetric footprint:</p><p><math display="block"><mtable><mtr><mtd><mrow><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>10</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow></mrow></mtd></mtr><mtr><mtd><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>10</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mrow><mtext>[definition</mtext><mspace width="0.3em"></mspace><mtext>of</mtext><mspace width="0.3em"></mspace></mrow><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mtext>]</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mn>1/2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo stretchy="false">)</mo><mo>·</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>10</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mrow><mtext>[isolating</mtext><mspace width="0.3em"></mspace></mrow><mi>p</mi><mtext>]</mtext></mrow></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo>−</mo><mrow><mo>(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo>)</mo></mrow><mo>·</mo><mi>e</mi><mo>+</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mtext>[taking</mtext><mspace width="0.3em"></mspace><mtext>log]</mtext></mrow></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo>−</mo><mrow><mo>(</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo stretchy="false">)</mo><mo>·</mo><mi>e</mi><mo>−</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><mo>)</mo></mrow></mrow></mtd><mtd><mtext>[regrouping]</mtext></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo>−</mo><mrow><mo>⌊</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo stretchy="false">)</mo><mo>·</mo><mi>e</mi><mo>⌋</mo></mrow></mrow></mtd><mtd><mrow><mtext>[</mtext><mi>p</mi><mrow><mspace width="0.3em"></mspace><mtext>is</mtext><mspace width="0.3em"></mspace><mtext>an</mtext><mspace width="0.3em"></mspace><mtext>integer]</mtext></mrow></mrow></mtd></mtr></mtable></math></p>


<p>
For a skewed footprint:</p><p><math display="block"><mtable><mtr><mtd><mrow><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>10</mn><mo stretchy="false">)</mo></mrow></mtd></mtr><mtr><mtd><mrow><mn>¾</mn><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>10</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mrow><mtext>[definition</mtext><mspace width="0.3em"></mspace><mtext>of</mtext><mspace width="0.3em"></mspace></mrow><mtext>footprint</mtext><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mtext>]</mtext></mrow></mtd></mtr><mtr><mtd><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo stretchy="false">(</mo><mn>1/</mn><mo stretchy="false">(</mo><mn>¾</mn><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>·</mo><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>10</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mrow><mtext>[isolating</mtext><mspace width="0.3em"></mspace></mrow><mi>p</mi><mtext>]</mtext></mrow></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo>−</mo><mrow><mo>(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>¾</mn><mo>+</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo stretchy="false">)</mo><mo>·</mo><mi>e</mi><mo>)</mo></mrow><mo>+</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mtd><mtd><mrow><mtext>[taking</mtext><mspace width="0.3em"></mspace><mtext>log]</mtext></mrow></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>∈</mo></mtd><mtd><mrow><mo>−</mo><mrow><mo>(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>¾</mn><mo>+</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo stretchy="false">)</mo><mo>·</mo><mi>e</mi><mo>−</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><mo>)</mo></mrow></mrow></mtd><mtd><mtext>[regrouping]</mtext></mtd></mtr><mtr><mtd><mi>p</mi></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo>−</mo><mrow><mo>⌊</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>¾</mn><mo>+</mo><mo stretchy="false">(</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>10</mn></msub><mspace width="0.166em"></mspace></mrow><mn>2</mn><mo stretchy="false">)</mo><mo>·</mo><mi>e</mi><mo>⌋</mo></mrow></mrow></mtd><mtd><mrow><mtext>[</mtext><mi>p</mi><mrow><mspace width="0.3em"></mspace><mtext>is</mtext><mspace width="0.3em"></mspace><mtext>an</mtext><mspace width="0.3em"></mspace><mtext>integer]</mtext></mrow></mrow></mtd></mtr></mtable></math></p>


<p>
For the symmetric footprint, we can use <code>log10Pow2</code>,
but for the skewed footprint, we need a new approximation:
</p><div><pre><span>// skewed computes the skewed footprint of m * 2**e,</span>
<span>// which is ⌊log₁₀ 3/4 * 2**e⌋ = ⌊e*(log₁₀ 2)-(log₁₀ 4/3)⌋.</span>
func skewed(e int) int {
	return (e*631305 - 261663) &gt;&gt; 21
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L234-L238">fpfmt/fpfmt.go:234,238</a></p></div>

<p>
We should worry about a footprint with decimal width exactly 1,
since if <math><mi>f</mi></math> had an odd mantissa,
the midpoints would be excluded.
In that case, if the decimals were the exact midpoints,
there would be no decimal between them,
making the conversion invalid.
But it turns out we should not worry too much.
For a skewed footprint, <math><mrow><mn>¾</mn><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math> can never be exactly 1,
because nothing can divide away the 3.
For a symmetric footprint, <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo>=</mo><mn>1</mn></mrow></math>
can only happen for <math><mrow><mi>e</mi><mo>=</mo><mi>p</mi><mo>=</mo><mn>0</mn></mrow></math>,
but then scaling is a no-op,
so that the decimal integers are exactly the binary integers.
The non-integer midpoints map to non-integer decimals.

</p><p>
When we compute the decimal equivalents of the midpoints,
we will use ceiling and floor instead of rounding them,
to make sure the integer results are valid decimal answers.
If the mantissa <math><mi>m</mi></math> is odd, we will nudge the unrounded forms
inward slightly before taking the ceiling or floor,
since rounding will be away from <math><mi>m</mi></math>.

</p><p>
The Go code is:
</p><div><pre><span>// Short computes the shortest formatting of f,</span>
<span>// using as few digits as possible that will still round trip</span>
<span>// back to the original float64.</span>
func Short(f float64) (d uint64, p int) {
	const minExp = -1085

	m, e := unpack64(f)

	var min uint64
	z := 11 <span>// extra zero bits at bottom of m; 11 for 53-bit m</span>
	if m == 1&lt;&lt;63 &amp;&amp; e &gt; minExp {
		p = -skewed(e + z)
		min = m - 1&lt;&lt;(z-2) <span>// min = m - 1/4 * 2**(e+z)</span>
	} else {
		if e &lt; minExp {
			z = 11 + (minExp - e)
		}
		p = -log10Pow2(e + z)
		min = m - 1&lt;&lt;(z-1) <span>// min = m - 1/2 * 2**(e+z)</span>
	}
	max := m + 1&lt;&lt;(z-1) <span>// max = m + 1/2 * 2**(e+z)</span>
	odd := int(m&gt;&gt;z) &amp; 1

	pre := prescale(e, p, log2Pow10(p))
	dmin := uscale(min, pre).nudge(+odd).ceil()
	dmax := uscale(max, pre).nudge(-odd).floor()

	if d = dmax / 10; d*10 &gt;= dmin {
		return trimZeros(d, -(p - 1))
	}
	if d = dmin; d &lt; dmax {
		d = uscale(m, pre).round()
	}
	return d, -p
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L198-L232">fpfmt/fpfmt.go:198,232</a></p></div>

<p>
Notice that this algorithm requires either two or three calls to <code>uscale</code>.
When the number being printed has only one valid representation
of the shortest length, we avoid the third call to <code>uscale</code>.
Also notice that the <code>prescale</code> result is shared by all three calls.

</p><p>
When <math><mrow><mi>m</mi><mo>=</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>63</mn></msup></mrow></math>, <math><mrow><mtext><i>min</i></mtext><mo>&lt;</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>63</mn></msup></mrow></math>,
meaning it won’t be left shifted as far as possible
during the call to <code>uscale</code>.
Although we could detect this case and call <code>uscale</code>
with <math><mrow><mn>2</mn><mo>·</mo><mtext><i>min</i></mtext></mrow></math> and <math><mrow><mi>e</mi><mo>−</mo><mn>1</mn></mrow></math>,
using <math><mtext><i>min</i></mtext></math> unmodified is fine:
it is still shifted enough that the bits <code>uscale</code>
needs to return will stay in the high 64 bits of the 192-bit product,
and using the same <math><mi>e</mi></math>
lets us use the same <code>prescale</code> work for all three calls.
<a href="#trimzero"></a></p><h3 id="trimzero"><a href="#trimzero">Trimming Zeros</a></h3>


<p>
The <code>trimZeros</code> function used in <code>Short</code> removes any trailing zeros from its argument,
updating the decimal power. An unoptimized version is:
</p><div><pre><span>// trimZeros removes trailing zeros from x * 10**p.</span>
<span>// If x ends in k zeros, trimZeros returns x/10**k, p+k.</span>
<span>// It assumes that x ends in at most 16 zeros.</span>
func trimZeros(x uint64, p int) (uint64, int) {
	if x%10 != 0 {
		return x, p
	}
	x /= 10
	p += 1

	if x%100000000 == 0 {
		x /= 100000000
		p += 8
	}
	if x%10000 == 0 {
		x /= 10000
		p += 4
	}
	if x%100 == 0 {
		x /= 100
		p += 2
	}
	if x%10 == 0 {
		x /= 10
		p += 1
	}
	return x, p
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/unopt/fpfmt.go#L227-L254">fpfmt/unopt/fpfmt.go:227,254</a></p></div>

<p>
The initial removal of a single zero gives an early return for
the common case of having no zeros.
Otherwise, the code makes four additional checks that
collectively remove up to 16 more zeros.
For outputs with many zeros, these four checks run faster
than a loop removing one zero at a time.

</p><p>
When compiling this code,
the Go compiler reduces the remainder checks to multiplications
using the following well-known optimization.
An exact <code>uint64</code> division <math><mrow><mi>x</mi><mn>/</mn><mi>c</mi></mrow></math> where <math><mrow><mi>x</mi><mo>mod</mo><mi>c</mi><mo>=</mo><mn>0</mn></mrow></math>
can be implemented by <math><mrow><mi>x</mi><mo>·</mo><mi>m</mi></mrow></math> where <math><mi>m</mi></math>
is the <code>uint64</code> multiplicative inverse of <math><mi>c</mi></math>, meaning <math><mrow><mi>m</mi><mo>·</mo><mi>c</mi><mo>mod</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup><mo>=</mo><mn>1</mn></mrow></math>:
Since <math><mi>c</mi></math> is also the multiplicative inverse of <math><mi>m</mi></math>, <math><mrow><mi>x</mi><mo>·</mo><mi>m</mi></mrow></math> is
lossless—all the exact multiples of <math><mi>c</mi></math> map to all of <math><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mo stretchy="false">(</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>/</mo><mi>c</mi><mo stretchy="false">]</mo></mrow></math>—so
the non-multiples are forced to map to larger values.
This observation gives a quick test for whether <math><mi>x</mi></math> is an exact multiple of <math><mi>c</mi></math>:
check whether <math><mrow><mi>x</mi><mo>·</mo><mi>m</mi><mo>≤</mo><mo stretchy="false">(</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>/</mo><mi>c</mi></mrow></math>.

</p><p>
Only odd <math><mi>c</mi></math> have multiplicative inverses modulo powers of two,
so even divisors require more work.
To compute an exact division <math><mrow><mi>x</mi><mo>/</mo><mo stretchy="false">(</mo><mi>c</mi><mo>&lt;&lt;</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></math>,
we can use <math><mrow><mo stretchy="false">(</mo><mi>x</mi><mn>/</mn><mi>c</mi><mo stretchy="false">)</mo><mo>&gt;&gt;</mo><mi>s</mi></mrow></math> instead.
To check for remainder, we need to check that those low <math><mi>s</mi></math>
bits are all zero before we shift them away.
We can merge that check with the range check by rotating those bits
into the high part instead of discarding them:
check whether <math><mrow><mi>x</mi><mo>·</mo><mi>m</mi><mo>↻&gt;</mo><mi>s</mi><mo>≤</mo><mo stretchy="false">(</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>/</mo><mi>c</mi></mrow></math>,
where <math><mo lspace="0" rspace="0">↻&gt;</mo></math> is right rotate.

</p><p>
The Go compiler does this transformation automatically
for the <code>if</code> conditions in <code>trimZeros</code>,
but inside the <code>if</code> bodies, it does not reuse the
exact quotient it just computed.
I considered changing the compiler to recognize that pattern,
but instead I wrote out the remainder check by hand
in the optimized version, allowing me to reuse the computed exact quotients:
</p><div><pre><span>// trimZeros removes trailing zeros from x * 10**p.</span>
<span>// If x ends in k zeros, trimZeros returns x/10**k, p+k.</span>
<span>// It assumes that x ends in at most 16 zeros.</span>
func trimZeros(x uint64, p int) (uint64, int) {
	const (
		maxUint64 = ^uint64(0)
		inv5p8    = 0xc767074b22e90e21 <span>// inverse of 5**8</span>
		inv5p4    = 0xd288ce703afb7e91 <span>// inverse of 5**4</span>
		inv5p2    = 0x8f5c28f5c28f5c29 <span>// inverse of 5**2</span>
		inv5      = 0xcccccccccccccccd <span>// inverse of 5</span>
	)

	<span>// Cut 1 zero, or else return.</span>
	if d := bits.RotateLeft64(x*inv5, -1); d &lt;= maxUint64/10 {
		x = d
		p += 1
	} else {
		return x, p
	}

	<span>// Cut 8 zeros, then 4, then 2, then 1.</span>
	if d := bits.RotateLeft64(x*inv5p8, -8); d &lt;= maxUint64/100000000 {
		x = d
		p += 8
	}
	if d := bits.RotateLeft64(x*inv5p4, -4); d &lt;= maxUint64/10000 {
		x = d
		p += 4
	}
	if d := bits.RotateLeft64(x*inv5p2, -2); d &lt;= maxUint64/100 {
		x = d
		p += 2
	}
	if d := bits.RotateLeft64(x*inv5, -1); d &lt;= maxUint64/10 {
		x = d
		p += 1
	}
	return x, p
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L240-L278">fpfmt/fpfmt.go:240,278</a></p></div>

<p>
This approach to trimming zeros is from Dragonbox.
For more about the general optimization,
see Warren’s <i>Hacker’s Delight</i>  [<a id="fnref-34" href="#fn-34">34</a>],
sections 10-16 and 10-17.
<a href="#fast"></a></p><h2 id="fast"><a href="#fast">Fast, Accurate Scaling</a></h2>


<p>
The conversion algorithms we examined are nice and simple.
For them to be fast, <code>uscale</code> needs to be fast while remaining correct.
Although multiplication by <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup></mrow></math> can be implemented by shifts,
<code>uscale</code> cannot actually compute or multiply by
<math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math>—that would take too long when <math><mi>p</mi></math> is a large positive or negative number.
Instead, we can approximate <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math> as a floating-point number <math><mrow><mtext><i>pm</i></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mtext><i>pe</i></mtext></msup></mrow></math> with a 128-bit mantissa,
looked up in a table indexed by <math><mi>p</mi></math>.
Specifically, we will use <math><mrow><mtext><i>pe</i></mtext><mo>=</mo><mrow><mo stretchy="false">⌊</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo stretchy="false">⌋</mo></mrow><mo>−</mo><mn>127</mn></mrow></math> and <math><mrow><mtext><i>pm</i></mtext><mo>=</mo><mrow><mo stretchy="false">⌈</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mn>/2</mn><msup><mspace height="0.66em"></mspace><mtext><i>pe</i></mtext></msup><mo stretchy="false">⌉</mo></mrow></mrow></math>,
ensuring that <math><mrow><mtext><i>pm</i></mtext><mo>∈</mo><mo stretchy="false">[</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>127</mn></msup><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>128</mn></msup><mo stretchy="false">)</mo></mrow></math>.
We will write a separate program to generate this table.
It emits Go code defining <code>pow10Min</code>, <code>pow10Max</code>, and <code>pow10Tab</code>:
<code>pow10Tab[0]</code> holds the entry for <math><mrow><mi>p</mi><mo>=</mo><mtext><code>pow10Min</code></mtext></mrow></math>.
To figure out how big the table needs to be,
we can analyze the three functions we just wrote.
</p><ul>
<li>
<code>FixedWidth</code> converts floating-point to decimal.
It needs to call <code>uscale</code> with a 53-bit <math><mi>x</mi></math>, <math><mrow><mi>e</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1137</mn><mo>,</mo><mn>960</mn><mo stretchy="false">]</mo></mrow></math>, and <math><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>307</mn><mo>,</mo><mn>341</mn><mo stretchy="false">]</mo></mrow></math>.
</li><li>
<code>Short</code> also converts floating-point to decimal.
It needs to call <code>uscale</code> with a 55-bit <math><mi>x</mi></math>, <math><mrow><mi>e</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>1137</mn><mo>,</mo><mn>960</mn><mo stretchy="false">]</mo></mrow></math>, and <math><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>292</mn><mo>,</mo><mn>324</mn><mo stretchy="false">]</mo></mrow></math>.
</li><li>
<code>Parse</code> converts decimal to floating-point.
It needs to call <code>uscale</code> with a 64-bit <math><mi>x</mi></math> and <math><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>343</mn><mo>,</mo><mn>289</mn><mo stretchy="false">]</mo></mrow></math>.
(Outside that range of <math><mi>p</mi></math>, <code>Parse</code> can return 0 or infinity.)</li></ul>


<p>
So the table needs to provide answers for <math><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>343</mn><mo>,</mo><mn>341</mn><mo stretchy="false">]</mo></mrow></math>.

</p><p>
If <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo>≈</mo><mtext><i>pm</i></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mtext><i>pe</i></mtext></msup></mrow></math>, then <math><mrow><mi>x</mi><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>e</mi></msup><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo>≈</mo><mi>x</mi><mo>·</mo><mtext><i>pm</i></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mrow><mi>e</mi><mo>+</mo><mtext><i>pe</i></mtext></mrow></msup></mrow></math>.
In all of our algorithms, the result of <code>uscale</code> was always small—at most 64 bits.
Since <math><mtext><i>pm</i></mtext></math> is 128 bits and <math><mrow><mi>x</mi><mo>·</mo><mtext><i>pm</i></mtext></mrow></math> is even bigger, <math><mrow><mi>e</mi><mo>+</mo><mtext><i>pe</i></mtext></mrow></math> must be negative,
so this computation is
<code>(x*pm) &gt;&gt; -(e+pe)</code>.
Because of the ceiling, <math><mtext><i>pm</i></mtext></math> may be too large by an error <math><mrow><mi>ε</mi><msub><mspace height="0em"></mspace><mn>0</mn></msub><mo>&lt;</mo><mn>1</mn></mrow></math>,
so <math><mrow><mi>x</mi><mo>·</mo><mtext><i>pm</i></mtext></mrow></math> may be too large by an error <math><mrow><mi>ε</mi><msub><mspace height="0em"></mspace><mn>1</mn></msub><mo>=</mo><mi>x</mi><mo>·</mo><mi>ε</mi><msub><mspace height="0em"></mspace><mn>0</mn></msub><mo>&lt;</mo><mi>x</mi></mrow></math>.
To round exactly, we care whether any of the shifted bits is 1,
but <math><mrow><mi>ε</mi><msub><mspace height="0em"></mspace><mn>1</mn></msub></mrow></math> may change the low <math><mrow><mtext>bits</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> bits,
so we can’t trust them.
Instead, we will throw them away.
and use only the upper bits to compute our unrounded number.
That is the entire idea!

</p><p>
Now let’s look at the implementation.
The <code>prescale</code> function returns a <code>scaler</code> with <math><mtext><i>pm</i></mtext></math> and a shift count <math><mi>s</mi></math>:
</p><div><pre><span>// A scaler holds derived scaling constants for a given e, p pair.</span>
type scaler struct {
	pm pmHiLo
	s  int
}

<span>// A pmHiLo represents hi&lt;&lt;64 + lo.</span>
type pmHiLo struct {
	hi uint64
	lo uint64
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/unopt/fpfmt.go#L256-L266">fpfmt/unopt/fpfmt.go:256,266</a></p></div>

<p>
We want the shift count to reserve two extra bits for the unrounded
representation and to apply to the top 64-bit word of the 192-bit product,
which gives this formula:</p><p><math display="block"><mtable><mtr><mtd><mi>s</mi></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo>−</mo><mo stretchy="false">(</mo><mi>e</mi><mo>+</mo><mtext><i>pe</i></mtext><mo stretchy="false">)</mo><mo>−</mo><mn>2</mn><mo>−</mo><mo stretchy="false">(</mo><mn>192</mn><mo>−</mo><mn>64</mn><mo stretchy="false">)</mo></mrow></mtd></mtr><mtr><mtd><mrow></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo>−</mo><mo stretchy="false">(</mo><mi>e</mi><mo>+</mo><mrow><mo stretchy="false">⌊</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo stretchy="false">⌋</mo></mrow><mo>−</mo><mn>127</mn><mo stretchy="false">)</mo><mo>−</mo><mn>2</mn><mo>−</mo><mn>128</mn></mrow></mtd></mtr><mtr><mtd><mrow></mrow></mtd><mtd><mo>=</mo></mtd><mtd><mrow><mo>−</mo><mo stretchy="false">(</mo><mi>e</mi><mo>+</mo><mrow><mo stretchy="false">⌊</mo><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub><mspace width="0.166em"></mspace></mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo stretchy="false">⌋</mo></mrow><mo>+</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></mtd></mtr></mtable></math></p>


<p>
That translates directly to Go:
</p><div><pre><span>// prescale returns the scaling constants for e, p.</span>
<span>// lp must be log2Pow10(p).</span>
func prescale(e, p, lp int) scaler {
	return scaler{pm: pow10Tab[p-pow10Min], s: -(e + lp + 3)}
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L292-L296">fpfmt/fpfmt.go:292,296</a></p></div>

<p>
In <code>uscale</code>, since the caller left-justified <math><mi>x</mi></math> to 64 bits,
discarding the low <math><mrow><mtext>bits</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> bits means discarding the
lowest 64 bits of the product, which we skip computing entirely.
Then we use the middle 64-bit word and the low <math><mi>s</mi></math> bits
of the upper word to set the sticky bit in the result.
</p><div><pre><span>// uscale returns unround(x * 2**e * 10**p).</span>
<span>// The caller should pass c = prescale(e, p, log2Pow10(p))</span>
<span>// and should have left-justified x so its high bit is set.</span>
func uscale(x uint64, c scaler) unrounded {
	hi, mid := bits.Mul64(x, c.pm.hi)
	mid2, _ := bits.Mul64(x, c.pm.lo)
	mid, carry := bits.Add64(mid, mid2, 0)
	hi += carry
	sticky := bool2[unrounded](mid != 0 || hi&amp;((1&lt;&lt;c.s)-1) != 0)
	return unrounded(hi&gt;&gt;c.s) | sticky
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/unopt/fpfmt.go#L302-L312">fpfmt/unopt/fpfmt.go:302,312</a></p></div>

<p>
It is mind-boggling that this works, but it does.
Of course, you shouldn’t take my word for it.
We have to prove it correct.
<a href="#proof"></a></p><h2 id="proof"><a href="#proof">Sketch of a Proof of Fast Scaling</a></h2>


<p>
To prove that our fast <code>uscale</code> algorithm is correct,
there are three cases: small positive <math><mi>p</mi></math>,
small negative <math><mi>p</mi></math>,
and large <math><mi>p</mi></math>.
The actual proof, especially for large <math><mi>p</mi></math>,
is non-trivial,
and the details are quite a detour from
our fast scaling implementations,
so this section only sketches the basic ideas.
For the details, see the accompanying post, “<a href="https://cjwu.substack.com/p/fp-proof">Fast Unrounded Scaling: Proof by Ivy</a>.”

</p><p>
Remember from the previous section that <math><mrow><mtext><i>pm</i></mtext><mo>=</mo><mrow><mo stretchy="false">⌈</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mn>/2</mn><msup><mspace height="0.66em"></mspace><mtext><i>pe</i></mtext></msup><mo stretchy="false">⌉</mo></mrow><mo>=</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mn>/2</mn><msup><mspace height="0.66em"></mspace><mtext><i>pe</i></mtext></msup><mo>+</mo><mi>ε</mi><msub><mspace height="0em"></mspace><mn>0</mn></msub></mrow></math> for some <math><mrow><mi>ε</mi><msub><mspace height="0em"></mspace><mn>0</mn></msub><mo>&lt;</mo><mn>1</mn></mrow></math>.
Since <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo>=</mo><mn>5</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math>,
<math><mtext><i>pm</i></mtext></math>’s 128 bits need only represent the <math><mrow><mn>5</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math> part; the <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math> can always be handled by <math><mtext><i>pe</i></mtext></math>.

</p><p>
For <math><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>27</mn><mo stretchy="false">)</mo></mrow></math>, <math><mrow><mn>5</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math> fits in the top 64 bits of the 128-bit <math><mtext><i>pm</i></mtext></math>.
Since <math><mtext><i>pm</i></mtext></math> is exact,
the only possible error is introduced by discarding the bottom <math><mrow><mtext>bits</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> bits.
Since the bottom 64 bits of <math><mtext><i>pm</i></mtext></math> are zero,
the bits we discard are all zero.
So <code>uscale</code> is correct for small positive <math><mi>p</mi></math>.

</p><p>
For <math><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">[</mo><mo form="prefix">−</mo><mn>27</mn><mo>,</mo><mo form="prefix">−</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></math>,
<math><mrow><mi>x</mi><mo>·</mo><mtext><i>pm</i></mtext></mrow></math> is approximating division by <math><mrow><mn>5</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mi>p</mi></mrow></msup></mrow></math> (remember that <math><mrow><mo form="prefix">−</mo><mi>p</mi></mrow></math> is a positive number!).
The 128-bit approximation is precise enough that when <math><mi>x</mi></math> is a
multiple of <math><mrow><mn>5</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mi>p</mi></mrow></msup></mrow></math>, only the lowest <math><mrow><mi>b</mi><mi>i</mi><mi>t</mi><mi>s</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></math> bits are non-zero;
discarding them keeps the unrounded form exact.
And when <math><mi>x</mi></math> is not a multiple of <math><mrow><mn>5</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mi>p</mi></mrow></msup></mrow></math>,
the result has a fractional part that must be at least
<math><mrow><mn>1/5</mn><msup><mspace height="0.66em"></mspace><mrow><mo form="prefix">−</mo><mi>p</mi></mrow></msup></mrow></math> away from an integer.
That fractional separation is much larger than the maximum error in the product,
so the high bits saved in the unrounded form are correct;
the fraction is also repeating, so that there is guaranteed
to be a 1 bit to cause the unrounded form to be marked inexact.
So <code>uscale</code> is correct for small negative <math><mi>p</mi></math>.

</p><p>
Finally, we must handle large <math><mi>p</mi></math>, which always have a non-zero error
and therefore should always return unrounded numbers marked inexact
(with the sticky bit set to 1).
Consider the effect of adding a small error to the idealized “correct” <math><mrow><mi>x</mi><mo>·</mo><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup><mn>/2</mn><msup><mspace height="0.66em"></mspace><mtext><i>pe</i></mtext></msup></mrow></math>,
producing <math><mrow><mi>x</mi><mo>·</mo><mtext><i>pm</i></mtext></mrow></math>.
The error is at most 64 bits.
Adding that error to the 192-bit product can certainly affect
the low 64 bits, and it may also generate a carry out of the low 64
into the middle 64 bits.
The carry turns 1 bits into 0 bits from right to left
until it hits a 0 bit;
that first 0 bit becomes a 1, and the carry stops.
The key insight is that seeing a 1 in the middle bits
is proof that the carry did not reach the high bits,
so the high bits are correct.
(Seeing a 1 in the middle bits also ensures that
the unrounded form is marked inexact, as it must be,
even though we discarded the low bits.)
Using a program backed by careful math, we can analyze all the <math><mtext><i>pm</i></mtext></math> in our table,
showing that every possible <math><mrow><mi>x</mi><mo>·</mo><mtext><i>pm</i></mtext></mrow></math> has a 1 in the middle bits.
So <code>uscale</code> is correct for large <math><mi>p</mi></math>.
<a href="#omit"></a></p><h2 id="omit"><a href="#omit">Omit Needless Multiplications</a></h2>


<p>
We have a fast and correct <code>uscale</code>, but we can make it faster
now that we understand the importance of carry bits.
The idea is to compute the high 64 bits of the product
and then use it directly whenever possible, avoiding the computation
of the remaining 64 bits at all.
To make this work, we need the high 64 bits to be rounded up,
a ceiling instead of a floor.
So we will change the <code>pmHiLo</code> from representing <math><mrow><mtext><i>hi</i></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup><mo>+</mo><mtext><i>lo</i></mtext></mrow></math>
to <math><mrow><mtext><i>hi</i></mtext><mo>·</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup><mo>−</mo><mtext><i>lo</i></mtext></mrow></math>.
</p><div><pre><span>// A pmHiLo represents hi&lt;&lt;64 - lo.</span>
type pmHiLo struct {
	hi uint64
	lo uint64
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L280-L284">fpfmt/fpfmt.go:280,284</a></p></div>

<p>
The exact computation using this form would be:
</p><pre>hi, mid := bits.Mul64(x, c.pm.hi)
mid2, lo := bits.Mul64(x, c.pm.lo)
mid, carry := bits.Sub64(mid, mid2, bool2[uint64](lo &gt; 0))
hi -= carry
return unrounded(hi &gt;&gt; c.s) | bool2[unrounded](hi&amp;((1&lt;&lt;c.s)-1) != 0 || mid != 0)
</pre>


<p>
The 128-bit product <math><mrow><mi>x</mi><mo>·</mo><mtext><i>pm</i></mtext><mn>.</mn><mtext><i>hi</i></mtext></mrow></math> computed on the first line
may be too big by an error of up to <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>64</mn></msup></mrow></math>,
which may or may not affect the high 64 bits;
The middle three lines correct the product,
possibly subtracting 1 from <math><mtext><i>hi</i></mtext></math>.
Like in the proof sketch, if any of the bottom <math><mi>s</mi></math> bits of the approximate <math><mtext><i>hi</i></mtext></math> is a 1 bit,
that 1 bit would stop the subtracted carry from
affecting the higher bits, indicating that we don’t need to correct the product.

</p><p>
Using this insight, the optimized <code>uscale</code> is:
</p><div><pre><span>// uscale returns unround(x * 2**e * 10**p).</span>
<span>// The caller should pass c = prescale(e, p, log2Pow10(p))</span>
<span>// and should have left-justified x so its high bit is set.</span>
func uscale(x uint64, c scaler) unrounded {
	hi, mid := bits.Mul64(x, c.pm.hi)
	sticky := uint64(1)
	if hi&amp;(1&lt;&lt;(c.s&amp;63)-1) == 0 {
		mid2, _ := bits.Mul64(x, c.pm.lo)
		sticky = bool2[uint64](mid-mid2 &gt; 1)
		hi -= bool2[uint64](mid &lt; mid2)
	}
	return unrounded(hi&gt;&gt;c.s | sticky)
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L298-L310">fpfmt/fpfmt.go:298,310</a></p></div>

<p>
The fix-up looks different from the exact computation above
but it has the same effect.
We don’t need the actual final value of <math><mtext><i>mid</i></mtext></math>, only the carry
and its effect on the sticky bit.

</p><p>
On some systems, notably x86-64, <code>bits.Mul64</code> computes both results in a single instruction.
On other systems, notably ARM64, <code>bits.Mul64</code> must use two different instructions;
it helps on those systems to write the code this way,
optimizing away the computation for the low half of <math><mrow><mi>x</mi><mo>·</mo><mtext><i>pm</i></mtext><mn>.</mn><mtext><i>lo</i></mtext></mrow></math>.

</p><p>
The more bits that are being shifted out of <code>hi</code>,
the more likely it is that a 1 bit is being shifted out,
so that we have an answer after only the first <code>bits.Mul64</code>.
When <code>Short</code> calls <code>uscale</code>, it passes two <math><mi>x</mi></math> that
differ only in a single bit
and multiplies them by the same <math><mrow><mtext><i>pm</i></mtext><mn>.</mn><mtext><i>hi</i></mtext></mrow></math>.
While one of them might clear the low <math><mi>s</mi></math> bits of <math><mtext><i>hi</i></mtext></math>,
the other is unlikely to also clear them,
so we are likely to hit the fast path at least once,
if not twice.
In the case where <code>Short</code> calls <code>uscale</code> three times,
we are likely to hit the fast path at least twice.
This optimization means that, most of the time, a <code>uscale</code>
is implemented by a single wide multiply.
This is the main reason that <code>Short</code> runs faster than
Ryū, Schubfach, and Dragonbox, as we will see in the next section.
<a href="#perf"></a></p><h2 id="perf"><a href="#perf">Performance</a></h2>


<p>
I promised these algorithms would be simple <i>and</i> fast.
I hope you are convinced about simple.
(If not, keep in mind that the implementations in widespread
use today are far more complicated!)
Now it is time to evaluate ‘fast’
by comparing against other implementations.
All the other implementations are written in C or C++ and compiled by a C/C++ compiler.
To isolate compilation differences,
I translated the Go code to C and measured
both the Go code and the C translation.

</p><p>
I ran the benchmarks on two systems.
</p><ul>
<li>
Apple M4 (2025 MacBook Air ‘Mac16,12’), 32 GB RAM, macOS 26.1, Apple clang 17.0.0 (clang-1700.6.3.2)
</li><li>
AMD Ryzen 9 7950X, 128 GB RAM, Linux 6.17.9 and libc6 2.39-0ubuntu8.6, Ubuntu clang 18.1.3 (1ubuntu1)</li></ul>


<p>
Both systems used Go 1.26rc1.
The full benchmark code is in the <a href="https://pkg.go.dev/rsc.io/fpfmt"><code>rsc.io/fpfmt</code> package</a>.
<a href="#printing_text"></a></p><h3 id="printing_text"><a href="#printing_text">Printing Text</a></h3>


<p>
Real implementations generate strings, so we need to write
code to convert the integers we have been returning into digit sequences,
like this:
</p><div><pre><span>// formatBase10 formats the decimal representation of u into a.</span>
<span>// The caller is responsible for ensuring that a is big enough to hold u.</span>
<span>// If a is too big, leading zeros will be filled in as needed.</span>
func formatBase10(a []byte, u uint64) {
	for nd := len(a) - 1; nd &gt;= 0; nd-- {
		a[nd] = byte(u%10 + &#39;0&#39;)
		u /= 10
	}
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/unopt/fpfmt.go#L368-L376">fpfmt/unopt/fpfmt.go:368,376</a></p></div>

<p>
Unfortunately, if we connect our fast <code>FixedWidth</code> and <code>Short</code> to this
version of <code>formatBase10</code>, benchmarks spend most of their time in the formatting loop.
There are a variety of clever ways to speed up digit formatting.
For our purposes, it suffices to use the old trick of
splitting the number into two-digit chunks and
then converting each chunk by
indexing a 200-byte lookup table (more precisely, a “lookup string”) of all 2-digit values from 00 to 99:
</p><div><pre><span>// i2a is the formatting of 00..99 concatenated,</span>
<span>// a lookup table for formatting [0, 99].</span>
const i2a = &#34;00010203040506070809&#34; +
	&#34;10111213141516171819&#34; +
	&#34;20212223242526272829&#34; +
	&#34;30313233343536373839&#34; +
	&#34;40414243444546474849&#34; +
	&#34;50515253545556575859&#34; +
	&#34;60616263646566676869&#34; +
	&#34;70717273747576777879&#34; +
	&#34;80818283848586878889&#34; +
	&#34;90919293949596979899&#34;</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L353-L364">fpfmt/fpfmt.go:353,364</a></p></div>

<p>
Using this table and unrolling the loop to allow the
compiler to optimize away bounds checks, we end up with <code>formatBase10</code>:
</p><div><pre><span>// formatBase10 formats the decimal representation of u into a.</span>
<span>// The caller is responsible for ensuring that a is big enough to hold u.</span>
<span>// If a is too big, leading zeros will be filled in as needed.</span>
func formatBase10(a []byte, u uint64) {
	nd := len(a)
	for nd &gt;= 8 {
		<span>// Format last 8 digits (4 pairs).</span>
		x3210 := uint32(u % 1e8)
		u /= 1e8
		x32, x10 := x3210/1e4, x3210%1e4
		x1, x0 := (x10/100)*2, (x10%100)*2
		x3, x2 := (x32/100)*2, (x32%100)*2
		a[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]
		a[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]
		a[nd-5], a[nd-6] = i2a[x2+1], i2a[x2]
		a[nd-7], a[nd-8] = i2a[x3+1], i2a[x3]
		nd -= 8
	}

	x := uint32(u)
	if nd &gt;= 4 {
		<span>// Format last 4 digits (2 pairs).</span>
		x10 := x % 1e4
		x /= 1e4
		x1, x0 := (x10/100)*2, (x10%100)*2
		a[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]
		a[nd-3], a[nd-4] = i2a[x1+1], i2a[x1]
		nd -= 4
	}
	if nd &gt;= 2 {
		<span>// Format last 2 digits.</span>
		x0 := (x % 1e2) * 2
		x /= 1e2
		a[nd-1], a[nd-2] = i2a[x0+1], i2a[x0]
		nd -= 2
	}
	if nd &gt; 0 {
		<span>// Format final digit.</span>
		a[0] = byte(&#39;0&#39; + x)
	}
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L366-L406">fpfmt/fpfmt.go:366,406</a></p></div>

<p>
This is more code than I’d prefer, but it is at least straightforward.
I’ve seen much more complex versions.

</p><p>
With <code>formatBase10</code>, we can build <code>Fmt</code>, which formats in standard exponential notation:
</p><div><pre><span>// Fmt formats d, p into s in exponential notation.</span>
<span>// The caller must pass nd set to the number of digits in d.</span>
<span>// It returns the number of bytes written to s.</span>
func Fmt(s []byte, d uint64, p, nd int) int {
	<span>// Put digits into s, leaving room for decimal point.</span>
	formatBase10(s[1:nd+1], d)
	p += nd - 1

	<span>// Move first digit up and insert decimal point.</span>
	s[0] = s[1]
	n := nd
	if n &gt; 1 {
		s[1] = &#39;.&#39;
		n++
	}

	<span>// Add 2- or 3-digit exponent.</span>
	s[n] = &#39;e&#39;
	if p &lt; 0 {
		s[n+1] = &#39;-&#39;
		p = -p
	} else {
		s[n+1] = &#39;+&#39;
	}
	if p &lt; 100 {
		s[n+2] = i2a[p*2]
		s[n+3] = i2a[p*2+1]
		return n + 4
	}
	s[n+2] = byte(&#39;0&#39; + p/100)
	s[n+3] = i2a[(p%100)*2]
	s[n+4] = i2a[(p%100)*2+1]
	return n + 5
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L312-L345">fpfmt/fpfmt.go:312,345</a></p></div>

<p>
When calling <code>Fmt</code> with a <code>FixedWidth</code> result, we know the digit count <code>nd</code> already.
For a <code>Short</code> result, we can compute the digit count easily from the bit length:
</p><div><pre><span>// Digits returns the number of decimal digits in d.</span>
func Digits(d uint64) int {
	nd := log10Pow2(bits.Len64(d))
	return nd + bool2[int](d &gt;= uint64pow10[nd])
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L347-L351">fpfmt/fpfmt.go:347,351</a></p></div>
<a href="#fixed-width_performance"><h3 id="fixed-width_performance">Fixed-Width Performance</h3></a>


<p>
To evaluate fixed-width printing,
we need to decide which floating-point values to convert.
I generated 10,000 uint64s in the range <math><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>63</mn></msup><mo>−</mo><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>52</mn></msup><mo stretchy="false">)</mo></mrow></math> and used them as
float64 bit patterns.
The limited range avoids negative numbers, infinities, and NaNs.
The benchmarks all use Go’s
<a href="https://go.dev/blog/chacha8rand">ChaCha8-based generator</a>
with a fixed seed for reproducibility.
To reduce timing overhead, the benchmark builds an array of 1000 copies of the value
and calls a function that converts every value in the array in sequence.
To reduce noise, the benchmark times that function call 25 times and uses the median timing.
We also have to decide how many digits to ask for:
longer sequences are more difficult.
Although I investigated a wider range, in this post I’ll show
two representative widths: 6 digits (C <code>printf</code>’s default) and 17 digits
(the minimum to guarantee accurate round trips, so widely used).

</p><p>
The implementations I timed are:
</p><ul>
<li>
<b>dblconv</b>: Loitsch’s <a href="https://github.com/google/double-conversion">double-conversion library</a>, using the <code>ToExponential</code> function.
This library, used in Google Chrome,
implements a handful of special cases for small binary exponents
and falls back to a bignum-based printer for larger exponents.
</li><li>
<b>dmg1997</b>: Gay’s <a href="https://netlib.org/fp/"><code>dtoa.c</code></a>, <a href="https://web.archive.org/web/19970415033207/https://www.netlib.org/fp/dtoa.c">archived in 1997</a>.
For our purposes, this represents Gay’s original C implementation
described in his technical report from 1990  [<a id="fnref-11" href="#fn-11">11</a>].
I confirmed that this 1997 snapshot runs at the same speed as
(and has no significant code changes compared to)
another copy dating back to May 1991 or earlier.
</li><li>
<b>dmg2017</b>: Gay’s <a href="https://netlib.org/fp/"><code>dtoa.c</code></a>, <a href="https://web.archive.org/web/20170421060916/https://www.netlib.org/fp/dtoa.c">archived in 2017</a>.
In 2017, Gay published an updated version of <code>dtoa.c</code> that uses <code>uint64</code> math and
a table of 96-bit powers of ten. It is significantly faster than the original version (see below).
In November 2025, I confirmed that the latest version runs at the same speed as this one.
</li><li>
<b>libc</b>:
The C standard library conversion using <code>sprintf(&#34;%.*e&#34;, prec-1)</code>.
The conversion algorithm varies by C library.
The macOS C library seems to wrap a pre-2017 version of <code>dtoa.c</code>,
while Linux’s glibc uses its own bignum-based code.
In general the C library implementations have not kept pace
with recent algorithms and are slower than any of the others.
</li><li>
<b>ryu</b>: Adams’s <a href="https://github.com/ulfjack/ryu">Ryū library</a>, using the <code>d2exp_buffered</code> function.
It uses the Ryū Printf algorithm [<a id="fnref-3-2" href="#fn-3">3</a>].
</li><li>
<b>uscale</b>: The unrounded scaling approach, using the Go code in this post.
</li><li>
<b>uscalec</b>: A C translation of the unrounded scaling Go code.</li></ul>


<p>
Here is a scatterplot showing the times required to format <math><mi>f</mi></math> to 17 digits,
running on the Linux system:

</p><p>
<a href="https://cjwu.substack.com/p/fpfmt/plot/fpfmt-ryzen-fixed17-scat-big.svg"><img name="fpfmt/plot/fpfmt-ryzen-fixed17-scat" width="600" height="300" src="https://cjwu.substack.com/p/fpfmt/plot/fpfmt-ryzen-fixed17-scat.png" srcset="fpfmt/plot/fpfmt-ryzen-fixed17-scat.png 1x, fpfmt/plot/fpfmt-ryzen-fixed17-scat@1.5x.png 1.5x, fpfmt/plot/fpfmt-ryzen-fixed17-scat@2x.png 2x, fpfmt/plot/fpfmt-ryzen-fixed17-scat@3x.png 3x, fpfmt/plot/fpfmt-ryzen-fixed17-scat@4x.png 4x, fpfmt/plot/fpfmt-ryzen-fixed17-scat@6.png 6, fpfmt/plot/fpfmt-ryzen-fixed17-scat@6x2.png 6x2"/></a>

</p><p>
(Click on any of the graphs in this post for a larger view.)

</p><p>
The X axis is the log of the floating point input <math><mi>f</mi></math>,
and
the Y axis is the time required for a single conversion of the given input.
The scatterplot makes many things clear. For example, it is obvious that
there are two kinds of implementations.
Those that use bignums take longer for large exponents and
have a “winged” scatterplot,
while those that avoid bignums run at a mostly constant speed across
the entire exponent range.
The scatterplot also highlights many interesting data-dependent patterns in the timings,
most of which I have not investigated.
A friend remarked that you could probably spend a whole career
analyzing the patterns in this one plot.

</p><p>
For our purposes, it would help to have a clearer comparison
of the speed of the different algorithms.
The right tool for that is a plot of the cumulative distribution function (CDF),
which looks like this:

</p><p>
<a href="https://cjwu.substack.com/p/fpfmt/plot/fpfmt-ryzen-fixed17-cdf-big.svg"><img name="fpfmt/plot/fpfmt-ryzen-fixed17-cdf" width="400" height="300" src="https://cjwu.substack.com/p/fpfmt/plot/fpfmt-ryzen-fixed17-cdf.png" srcset="fpfmt/plot/fpfmt-ryzen-fixed17-cdf.png 1x, fpfmt/plot/fpfmt-ryzen-fixed17-cdf@1.5x.png 1.5x, fpfmt/plot/fpfmt-ryzen-fixed17-cdf@2x.png 2x, fpfmt/plot/fpfmt-ryzen-fixed17-cdf@3x.png 3x, fpfmt/plot/fpfmt-ryzen-fixed17-cdf@4x.png 4x"/></a>

</p><p>
Now time is on the X axis (still log scale), and the Y axis plots what
fraction of the inputs ran in that time or less.
For example, we can see that dblconv’s fast path applies to most inputs,
but its slow path is much slower than Linux glibc or
even Gay’s original C library.

</p><p>
The CDF only plots the middle 99.9% of timings
(dropping the 0.05% fastest and slowest),
to avoid tails caused by measurement noise.
In general, measurements are noisier on the Mac because
ARM64 timers only provide ~20ns precision,
compared to the x86’s sub-nanosecond precision.

</p><p>
Here are the scatterplots and CDFs for 6-digit output on the two systems:
</p></div></div><div><div>


<p>
For short output, various special-case optimizations are possible
to avoid bignums, and the scatterplots make clear that
all the implementations do that,
except for Linux glibc.
It surprises me that both libc implementations are so much slower
than David Gay’s original dtoa from 1990 (dmg1997).
I expected that any new attempt at floating-point printing
would at least make sure it was as fast as the canonical
reference implementation.

</p><p>
Here are the results for 17-digit output:
</p></div></div><div><div>


<p>
In this case, fewer optimizations are available,
and libc has a winged scatterplot on both systems.
The dblconv library has a fast path that can be taken
about 99% of the time, but the scatterplot shows a shadow
of a wing for the remaining 1%.
The CDFs show the bignum-based implementations clearly:
they are slower and have a more gradual slope.
We can also read off the CDFs that dmg2017’s
table-based fast path handles about 95% of the inputs.

</p><p>
In general, fast fixed-width printing has not seen much
optimization attention.
Unrounded scaling almost has the field to itself
and is significantly faster than the other implementations.
<a href="#shortest-width_performance"></a></p><h3 id="shortest-width_performance"><a href="#shortest-width_performance">Shortest-Width Performance</a></h3>


<p>
For shortest-width printing, I used the same set of random inputs as for fixed-width printing.
The implementations are:
</p><ul>
<li>
<b>dblconv</b>: Loitsch’s <a href="https://github.com/google/double-conversion">double-conversion library</a>, using the <code>ToShortest</code> function.
It uses the Grisu3 algorithm [<a id="fnref-23-2" href="#fn-23">23</a>].
</li><li>
<b>dmg1997</b>: Gay’s 1997 <code>dtoa.c</code> in shortest-output mode.
</li><li>
<b>dmg2017</b>: Gay’s 2017 <code>dtoa.c</code> in shortest-output mode.
</li><li>
<b>dragonbox</b>: Jeon’s <a href="https://github.com/jk-jeon/dragonbox">dragonbox library</a>, using the <code>jkj::dragonbox::to_chars</code> function.
It uses the Dragonbox algorithm [<a id="fnref-17-2" href="#fn-17">17</a>].
</li><li>
<b>ryu</b>: Adams’s <a href="https://github.com/ulfjack/ryu">Ryū library</a>, using the <code>d2s_buffered</code> function.
It uses the Ryü algorithm [<a id="fnref-2-2" href="#fn-2">2</a>].
</li><li>
<b>schubfach</b>: A C++ translation of Giulietti’s Java implementation of Schubfach [<a id="fnref-12-2" href="#fn-12">12</a>].
</li><li>
<b>uscale</b>: The unrounded scaling approach, using the Go code for <code>Short</code> and <code>Fmt</code> in this post.
</li><li>
<b>uscalec</b>: A C translation of the unrounded scaling Go code.</li></ul>


<p>
All these implementations are running different code than for fixed-width printing.
The C library does not provide shortest-width printing,
so there is no libc implementation to compare against.
</p></div></div><div><div>


<p>
There is much more competition here.
Other than Gay’s 1990 dtoa, everything runs quickly.
From the CDFs, we can see that Gay’s 2017 dtoa fast path runs about 85% of the time.
The C and Go unrounded scalings run at about the same speed as Ryū
but a bit slower than Dragonbox.
This turns out to be due mainly to Dragonbox’s digit formatter,
not the actual floating-point conversion.

</p><p>
To remove digit formatting from the comparison, I ran a set of benchmarks
of just <code>Short</code> (which returns an integer, not a digit string)
and equivalent code from Dragonbox, Schubfach, and Ryū.
For Dragonbox, I used <code>jkj::dragonbox::to_decimal</code>.
For Schubfach and Ryū, I added new entry points that
return the integer and exponent instead of formatting them.
Schubfach delayed the trimming of zeros until after formatting,
so I added a call to the <code>trimZeros</code> used by <code>Short</code>,
which is in turn similar to the one used in Dragonbox.
</p></div></div><div><div>


<p>
The difference between these plots and the previous ones show that
most of Dragonbox’s apparent speed before was in its digit formatter,
not in the actual binary-to-decimal conversion.
<a href="https://github.com/jk-jeon/dragonbox/blob/e4a85ebee62750382bc7d1eef4bb72f9696d073f/source/dragonbox_to_chars.cpp">That converter</a> effectively has
different straight-line code path for each number length.
It’s not surprising that it’s faster,
but it’s more code than I’m willing to stomach myself.

</p><p>
The scatterplots show that the Ryū code’s special case for integer inputs
helps for a few inputs (at the bottom of the plot) but runs slower than
the general case for more inputs (at the top of the plot).
On the other hand, the vertical lines of blue points near <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>500</mn></msup></mrow></math>
and <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>700</mn></msup></mrow></math> are likely not algorithmic,
nor is the vertical line of black points near <math><mrow><mn>2</mn><msup><mspace height="0.66em"></mspace><mn>100</mn></msup></mrow></math>.
Both appear to be some kind of bad Apple M4 interaction when accessing a
specific table entry.
The specific inputs are executed in random order,
so a clustering like this is not interference like
a single overloaded moment on the machine.
For a given built executable, the slow inputs are consistent;
as the code and data sections move around when the program
is changed, the slow inputs move too.
There is also a general phenomenon that if you sample 10,000
points, some of them will run slower than others due to
random hardware interactions.
All this is to say that the tails of the CDFs
for these very quick operations are not entirely trustworthy.

</p><p>
On a more reliable note,
the CDFs show that Dragonbox has a fast path is taken about 60% of the time
and runs faster than unrounded scaling,
but the cost of that check is to make the remaining 40% slower
than unrounded scaling.
On average, they are about the same,
but unrounded scaling is more consistent and less code.

</p><p>
Overall, unrounded scaling runs faster than or at the same speed as
the others,
especially when focusing on the core conversion.
When formatting text, Dragonbox runs faster, but only because
of its digit formatting code, not the code we are focusing on
in this post.
<a href="#parsing_text"></a></p><h3 id="parsing_text"><a href="#parsing_text">Parsing Text</a></h3>


<p>
Like for printing, to compare against other parsing implementations
we need code to handle text, not just the integers passed to <code>Parse</code>.
Here is the parser I used.
It could be improved to handle arbitrary numbers of leading and trailing zeros,
negative numbers, and special values like zero, infinity and NaN,
but it is close enough for our purposes.
It is essentially a direct translation of the regular expression
<code>[0-9]*(\.[0-9]*)?([Ee][+-]?[0-9]*)</code>,
with checks on the digit counts.
</p><div><pre><span>// ParseText parses a decimal string s</span>
<span>// and returns the nearest floating point value.</span>
<span>// It returns 0, false if the input s is malformed.</span>
func ParseText(s []byte) (f float64, ok bool) {
	isDigit := func(c byte) bool { return c-&#39;0&#39; &lt;= 9 }

	<span>// Read digits.</span>
	const maxDigits = 19
	d := uint64(0) <span>// decimal value of digits</span>
	frac := 0      <span>// count of digits after decimal point</span>
	i := 0
	for ; i &lt; len(s) &amp;&amp; isDigit(s[i]); i++ {
		d = d*10 + uint64(s[i]) - &#39;0&#39;
	}
	if i &gt; maxDigits {
		return <span>// too many digits</span>
	}
	if i &lt; len(s) &amp;&amp; s[i] == &#39;.&#39; {
		i++
		for ; i &lt; len(s) &amp;&amp; isDigit(s[i]); i++ {
			d = d*10 + uint64(s[i]) - &#39;0&#39;
			frac++
		}
		if i == 1 || i &gt; maxDigits+1 {
			return <span>// no digits or too many digits</span>
		}
	}
	if i == 0 {
		return <span>// no digits</span>
	}

	<span>// Read exponent.</span>
	p := 0
	if i &lt; len(s) &amp;&amp; (s[i] == &#39;e&#39; || s[i] == &#39;E&#39;) {
		i++
		sign := +1
		if i &lt; len(s) {
			if s[i] == &#39;-&#39; {
				sign = -1
				i++
			} else if s[i] == &#39;+&#39; {
				i++
			}
		}
		if i &gt;= len(s) || len(s)-i &gt; 3 {
			return <span>// missing or too large exponent</span>
		}
		for ; i &lt; len(s) &amp;&amp; isDigit(s[i]); i++ {
			p = p*10 + int(s[i]) - &#39;0&#39;
		}
		p *= sign
	}
	if i != len(s) {
		return <span>// junk on end</span>
	}
	return Parse(d, p-frac), true
}</pre></div><div><p><a href="https://github.com/rsc/fpfmt/blob/blog1/fpfmt.go#L140-L196">fpfmt/fpfmt.go:140,196</a></p></div>
<a href="#parsing_performance"><h3 id="parsing_performance">Parsing Performance</h3></a>


<p>
Now we can compare <code>Parse</code> to other implementations.
I generated 10,000 random inputs,
each of which was a random 19-digit sequence
with a decimal point after the first digit,
along with a random decimal exponent in the range [-300, 300].
(The full float64 decimal exponent range is [-308, 308],
but narrowing it avoids generating numbers
that underflow to 0 or overflow to infinity.)

</p><p>
The implementations are:
</p><ul>
<li>
<b>abseil</b>: The <a href="https://github.com/abseil/abseil-cpp">Abseil library</a>, using <code>absl::from_chars</code> as of November 2025 (commit 48bf10f142). <a href="https://github.com/abseil/abseil-cpp/blob/20250814.1/absl/strings/charconv.cc">It uses</a> the Eisel-Lemire algorithm [<a id="fnref-22-2" href="#fn-22">22</a>].
</li><li>
<b>libc</b>: The C library’s <code>strtod</code>. <a href="https://github.com/bminor/glibc/blob/glibc-2.39/stdlib/strtod_l.c">Linux glibc uses</a> a bignum-based algorithm,
while <a href="https://github.com/apple-oss-distributions/Libc/blob/Libc-1725.40.4/stdlib/strtofp.c">macOS 26 libc uses</a> the Eisel-Lemire algorithm.
</li><li>
<b>dblconv</b>: The <a href="https://github.com/google/double-conversion">double-conversion library</a>’s <code>StringToDouble</code> function. It uses Clinger’s algorithm [<a id="fnref-6" href="#fn-6">6</a>] with simulated floating-point
using 64-bit mantissas.
</li><li>
<b>dmg1997</b>: Gay’s 1997 strtod, from <code>dtoa.c</code> in 1997.
It uses Clinger’s algorithm with hardware floating-point (float64s).
</li><li>
<b>dmg2017</b>: Gay’s 2017 strtod, from <code>dtoa.c</code> in 2017.
It uses Clinger’s algorithm with simulated floating-point using 96-bit mantissas.
</li><li>
<b>fast_float</b>: Lemire’s <a href="https://github.com/fastfloat/fast_float">fast_float library</a>, using the <code>fast_float::from_chars</code> function. Unsurprisingly, it uses the Eisel-Lemire algorithm.
</li><li>
<b>uscale</b>: The unrounded scaling approach, using the Go code for <code>Parse</code> and <code>Unfmt</code> in this post.
</li><li>
<b>uscalec</b>: A C translation of the Go code in this post.</li></ul>
</div></div><div><div>


<p>
The surprise here is the macOS libc, which is competitive with unrounded scaling and fast_float.
It turns out that macOS 26 shipped a new strtod based on the Eisel-Lemire algorithm.

</p><p>
Once again, to isolate the actual conversion from the text processing,
I also benchmarked <code>Parse</code> and equivalent code from fast_float.
</p></div></div><div><div>


<p>
I don’t understand the notch in the Go uscale on macOS,
nor do I understand why the C uscale is faster than fast_float on macOS
but only about the same speed on Linux.
Since each conversion takes only a few nanoseconds,
the answer may be subtle microarchitectural effects
that I’m not particularly skilled at chasing down.

</p><p>
Overall,
unrounded scaling is faster than—or in one case tied with—the other known
algorithms for converting floating-point numbers
to and from decimal representations.
<a href="#history"></a></p><h2 id="history"><a href="#history">History and Related Work</a></h2>
<blockquote>

<p>
The story is told of G. H. Hardy (and of other people) that during a lecture
he said “It is obvious. . . <i>Is</i> it obvious?” left the room, and returned fifteen minutes
later, saying “Yes, it’s obvious.”
I was present once when Rogosinski asked Hardy whether the story were true.
Hardy would admit only that he might have said “It’s obvious. . . <i>Is</i> it obvious?” (brief pause)
“Yes, it’s obvious.” </p></blockquote><blockquote>

<p>
If I have seen further, it is by standing on the shoulders of giants. </p></blockquote><blockquote>

<p>
So I picked up my staff </p></blockquote>

<p>
People have been studying the problem of floating-point printing
and parsing since the late 1940s.
The solutions in this post, based on a fast, accurate unrounded scaling primitive,
may seem obvious in retrospect,
but they were certainly not obvious to me when I started down this trail.
Nor were they obvious to the many people
who studied this problem before, or we’d already be using these faster, simpler algorithms!
As is often the case in computer science,
the algorithms in this post
connect individual ideas that have been known for decades.
This section traces the history of the relevant ideas.

</p><p>
The companion post “<a href="https://cjwu.substack.com/p/fp-proof">Fast Unrounded Scaling: Proof by Ivy</a>” has
its own <a href="https://cjwu.substack.com/p/fp-proof#related">related work section</a>
that covers the history of proofs that a table of
128-bit powers of ten is sufficient for accurate results.
<a href="#related.fixed"></a></p><h3 id="related.fixed"><a href="#related.fixed">Fixed-Point Printing</a></h3>


<p>
The earliest binary/decimal conversions in the literature are probably
the ones in Goldstein and Von Neumann’s 1947
<i>Planning and Coding Problems for an Electronic Computing Instrument</i> [<a id="fnref-13" href="#fn-13">13</a>].
They converted one digit at a time by repeated multiplication by 10
and modulo by 1,
as did many conversions that followed.

</p><p>
The alternative to repeated multiplication by 10
is multiplication by a single larger power of 10,
as we did in this post.
Many early systems did that as well.
In a 1966 article in <i>CACM</i>, Mancino [<a id="fnref-24" href="#fn-24">24</a>]
summarized the state of the art:
“Decimal-to-binary and binary-to-decimal
floating-point conversion is often performed by using a table of the powers
<math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>i</mi></msup></mrow></math> (<math><mi>i</mi></math> a positive integer) for converting from base 10 to base
2, and by using a table of the coefficients of a polynomial
approximation of <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>x</mi></msup></mrow></math> <math><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>≤</mo><mi>x</mi><mo>&lt;</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math> for converting from base
2 to base 10.”
Mancino’s article then showed that the powers-of-10 table could be
used for binary-to-decimal as well
and also discussed reducing its size.

</p><p>
During the development of IEEE 754 floating-point, Coonen published
an implementation guide  [<a id="fnref-7" href="#fn-7">7</a>] that defined
conversions in both directions using powers of 10 constructed on demand.
The powers <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mi>p</mi></msup></mrow></math> for <math><mrow><mn>1</mn><mo>≤</mo><mi>p</mi><mo>≤</mo><mn>27</mn></mrow></math> can be computed exactly,
and then Coonen suggested storing a table containing
<math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mn>54</mn></msup></mrow></math>, <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mn>108</mn></msup></mrow></math>, and <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mn>216</mn></msup></mrow></math>,
so that any power up to <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mn>308</mn></msup></mrow></math> can be
constructed as the product of at most three multiplications
involving at most two inexact values.
Coonen computed an approximate <math><mi>p</mi></math> using a different
approximation to <math><mrow><mtext>log</mtext><msub><mspace height="0em"></mspace><mn>2</mn></msub></mrow></math>;
if it was off by one, he repeated the process with <math><mi>p</mi></math>
incremented or decremented by 1.
The result was not exact but was provably within a very small
error margin, which became IEEE754’s required
conversion accuracy.
Coonen’s thesis  [<a id="fnref-9" href="#fn-9">9</a>] improved on that error margin
by changing the table to contain the exceptionally accurate powers
<math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mn>55</mn></msup></mrow></math>, <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mn>108</mn></msup></mrow></math>, and <math><mrow><mn>10</mn><msup><mspace height="0.66em"></mspace><mn>206</mn></msup></mrow></math>,
improved the log approximation,
and discussed how to use the floating-point hardware’s
rounding modes and inexact flag to reduce the error further.
In some ways, unrounded scaling is similar to
the hardware Coonen shows in this diagram from Chapter 2:

</p><p>
<img name="fpfmt-coonen1" width="396" height="252" src="https://cjwu.substack.com/p/fpfmt-coonen1.png" srcset="fpfmt-coonen1.png 1x, fpfmt-coonen1@1.5x.png 1.5x, fpfmt-coonen1@2x.png 2x"/>

</p><p>
The main difference is that unrounded scaling uses enough precision
to avoid any error at all,
that some of the chopped bits are discarded entirely
rather than feeding into the inexact flag (the sticky bit),
and all the details have to be implemented in software
instead of relying on floating-point hardware.
In an appendix to his thesis, Coonen also defined
bignum-based exact conversion routines
written in Pascal.
(It would be interesting to translate them to C and
add them to the benchmarks above!)

</p><p>
1990 was the <i>annus mirabilis</i> of floating-point formatting.
In April, Slishman  [<a id="fnref-29" href="#fn-29">29</a>] published table-based algorithms
for printing and parsing at fixed precision.
The algorithms computed 16 additional bits of precision,
falling back to a bignum-based implementation only
when those 16 bits were all 1’s.
This is analogous to unrounded scaling’s check
for whether the middle bits are all 0’s
and appears to be the earliest analysis of the effect
of error carries on the eventual result.
(Slishman used a table of powers rounded down,
while unrounded scaling uses a table of powers rounded up,
so the overflow conditions are inverted.)

</p><p>
In June at the ACM PLDI conference, Steele and White published
“How to Print Floating-Point Numbers Accurately”  [<a id="fnref-30-2" href="#fn-30">30</a>]
(and Clinger also published “How to Read Floating Point Numbers Accurately”  [<a id="fnref-6-2" href="#fn-6">6</a>], discussed later).
Although the paper is mainly cited for shortest-width formatting,
Steele and White do discuss fixed-width formatting briefly.
Their algorithms use repeated multiplication by 10
instead of a table.

</p><p>
In November, Gay  [<a id="fnref-11-2" href="#fn-11">11</a>] published important optimizations for
both printing and parsing
but left the basic algorithms unmodified.
Gay also published a portable, freely redistributable C implementation.
As noted earlier, that implementation is probably one of the
most widely copied software libraries ever.

</p><p>
In a 2004 retrospective  [<a id="fnref-31" href="#fn-31">31</a>], Steele and White explained:</p><blockquote>

<p>
During the 1980s, White investigated the question of whether one
could use limited-precision arithmetic after all rather than bignums.
He had earlier proved by exhaustive testing that just 7 extra bits suffice for correctly printing 36-bit PDP-10 floating-point numbers, if
powers of ten used for prescaling are precomputed using bignums
and rounded just once. But can one derive, without exhaustive testing, the necessary amount of extra precision solely as a function of
the precision and exponent range of a floating-point format? This
problem is still open, and appears to be very hard.</p></blockquote>

<p>
In 1991, Paxson  [<a id="fnref-28" href="#fn-28">28</a>] identified the necessary algorithms
to answer that question,
but he put them to use only for deriving difficult test cases,
not for identifying the precision needed to avoid bignums entirely.
My <a href="https://cjwu.substack.com/p/fp-post">proof post</a> covers that in detail.

</p><p>
It appears that the first table-based exact conversions without bignums
were developed by Kenton Hanson at Apple,
who documented them on his personal web site in 1997  [<a id="fnref-15" href="#fn-15">15</a>]
after retiring. He summarized:</p><blockquote>

<p>
Once this worst case is determined we have shown how we can
guarantee correct conversions using arithmetic that is slightly
more than double the precision of the target destinations.</p></blockquote>

<p>
Like Slishman’s work at IBM, Hanson’s work unfortunately went mostly
unnoticed by the broader research community.

</p><p>
In 2018, Adams  [<a id="fnref-2-3" href="#fn-2">2</a>] published Ryū, an algorithm for
shortest-width formatting that used 128-bit tables.
After reading that paper
(discussed more in the next section),
Remy Oudompheng
<a href="https://go.dev/change/0184b445c04a0f30e34ce624298547f12630f3aa">rewrote Go’s fixed-width printer</a>
to adopt a table-based single-multiplication strategy.
He originally described it as “a simplified version of [Ryū]
for printing floating-point numbers with a fixed number of decimal
digits,”
but he told me recently
that he meant only that the code made use of the Ryū paper’s observation
that 128-bit precision is generally sufficient for correct conversion.
Because the Ryū paper did not address fixed-width printing
nor prove the correction of the conversions in that context,
Oudompheng devised a new <a href="https://github.com/remyoudompheng/fptest">computational proof</a> based on
<a href="https://en.wikipedia.org/wiki/Stern%E2%80%93Brocot_tree">Stern-Brocot tree traversal</a>.
Oudompheng’s printer
uses Ryū’s fairly complex rounding implementation
and expensive exactness computation based on a
<a href="https://go.googlesource.com/go/+/refs/tags/go1.25.0/src/strconv/ftoaryu.go#546">“divide by five” loop</a>.
I wrote Go’s original floating-point printing routines in 2008
but had not kept up with recent advances.
In 2025, I happened to read Oudompheng’s printer
and realized that the calculation could be significantly simplified
using standard IEEE754 hardware implementation techniques,
including keeping a sticky bit during scaling.
That was the first step down the path to
the general approach of unrounded scaling.

</p><p>
I am not sure where IEEE754’s sticky bit originated.
The earliest use of it I have found is in Palmer’s 1977 paper
introducing Intel’s standard floating-point [<a id="fnref-27" href="#fn-27">27</a>],
but I don’t know whether the sticky bit was new in that hardware design.

</p><p>
The unrounded scaling approach to fixed-width printing
can be viewed as the same table-based approach
described by Mancino  [<a id="fnref-24-2" href="#fn-24">24</a>] and Coonen  [<a id="fnref-7-2" href="#fn-7">7</a>],
but using 128-bit precision
to produce exact results,
as first noted by Hanson  [<a id="fnref-15-2" href="#fn-15">15</a>]
and then by Hack  [<a id="fnref-14" href="#fn-14">14</a>] and Adams  [<a id="fnref-2-4" href="#fn-2">2</a>].
<a href="#related.short"></a></p><h3 id="related.short"><a href="#related.short">Shortest Printing</a></h3>


<p>
Shortest-width printing has a related but distinct history.
The idea may have begun with Taranto’s 1959 <i>CACM</i> article [<a id="fnref-33" href="#fn-33">33</a>],
which considered the problem of converting a fixed-point decimal fraction
into a fixed-point binary fraction of the shortest length to reach a given fixed decimal precision.
From that paper, Knuth derived the problem of converting between any two bases
with shortest output for a fixed precision, publishing it in 1969
as exercise 4.4–3 in the first edition of
<i>The Art of Computer Programming, Volume 2: Seminumerical Algorithms</i> [<a id="fnref-18" href="#fn-18">18</a>].
Knuth included his own solution with a reference to Taranto.
Knuth’s exercise was not quite the <a href="#short">Shortest-Width Printing</a>
problem considered above: first, the exercise is about fixed-point fractions,
so it avoids the complexity of skewed footprints;
and second, the exercise gave no requirement to round correctly,
and the solution did not.

</p><p>
Steele and White adapted Knuth’s exercise and solution as the basis
for floating-point printing routines in the mid-to-late 1970s.
At the time, they shared a draft paper with Knuth, but the final paper
was not published until PLDI 1990 [<a id="fnref-30-3" href="#fn-30">30</a>].
Their fixed-point printing algorithm (FP)³ is Knuth’s solution to exercise 4.4–3,
but updated to round correctly.
In the second edition of <i>Seminumerical Algorithms</i> [<a id="fnref-19" href="#fn-19">19</a>],
Knuth changed the exercise to specify rounding,
made Steele and White’s one-line change to the solution,
and cited their unpublished draft.
In the third edition in 1997 [<a id="fnref-20" href="#fn-20">20</a>], Knuth was able to cite the published paper.
In my post “<a href="https://cjwu.substack.com/p/fp-knuth">Pulling a New Proof from Knuth’s Fixed-Point Printer</a>”,
the section titled “<a href="https://cjwu.substack.com/p/fp-knuth#textbook">A Textbook Solution</a>”
examines Taranto’s, Knuth’s, and Steele and White’s fixed-point algorithms in detail.

</p><p>
Steele and White’s 1990 paper kicked off a flurry of activity focused mainly
on shortest-width printing.
Their converters were named Dragon2 and Dragon4 (Dragon1 is never described,
and they say they omitted Dragon3 for space),
which set a dragon-themed naming pattern continued by
the ever-more complex printing algorithms that followed.
Gay [<a id="fnref-11-3" href="#fn-11">11</a>] and Burger and Dybvig [<a id="fnref-5" href="#fn-5">5</a>] found important optimizations
for special cases but left
the core algorithms the same.
In their 2004 retrospective  [<a id="fnref-31-2" href="#fn-31">31</a>], Steele and White described those
as the only successor papers of note,
but that soon changed.

</p><p>
Loitsch  [<a id="fnref-23-3" href="#fn-23">23</a>] introduced the Grisu2 and Grisu3 algorithms at PLDI 2010.
Both use a “do-it-yourself floating-point” or “diy-fp” representation
limited to 64-bit mantissas
to calculate the minimum and maximum decimals
for a given floating point input,
like in our <a href="#short">shortest-width algorithm</a>.
64 bits is not enough to convert exactly,
so Grisu2 rounds the minimum and maximum inward
conservatively.
As a result, it always finds an accurate answer
but may not find the shortest one.
Grisu3 repeats that computation rounding outward.
If its answer also lies within the conservative Grisu2 bounds,
that answer must be shortest.
Otherwise, a fallback algorithm must be used instead.
Grisu3 avoids the fallback
about 99.5% of the time for random inputs.
Due to the importance of formatting speed in
JavaScript and especially JSON,
web browsers and programming languages quickly adopted Grisu3.

</p><p>
Andrysco, Jhala, and Lerner introduced the Errol algorithms at POPL 2016  [<a id="fnref-4-2" href="#fn-4">4</a>].
They extended Loitsch’s approach by replacing
the diy-fps with 106-bit double-double arithmetic  [<a id="fnref-10" href="#fn-10">10</a>] [<a id="fnref-18-2" href="#fn-18">18</a>],
which empirically handles 99.9999999% of all inputs.
They claim to show empirically that “further precision is useless”,
and by careful refinement and analysis
identified that their final version Errol3 only
failed for 45 float64 inputs (!),
which they handled with a special lookup table.
I am not sure what went wrong in their analysis
that kept them from finding that 128-bit precision would have
been completely exact.

</p><p>
Picking up a different thread,
Abbott <i>et al.</i>  [<a id="fnref-1" href="#fn-1">1</a>]
published a report in 1999 about IBM’s addition of
IEEE754 floating-point to System/390 and repeated
a description of Slishman’s algorithms  [<a id="fnref-29-2" href="#fn-29">29</a>].
After reader feedback, Hack  [<a id="fnref-14-2" href="#fn-14">14</a>] analyzed
the error behavior and in 2004 published the
first proof that 128-bit precision was sufficient
for parsing.
Nadhezin  [<a id="fnref-26" href="#fn-26">26</a>] adapted the proof to printing
and formally verified it in ACL2,
and Giulietti  [<a id="fnref-12-3" href="#fn-12">12</a>] used that result to create
the Schubfach shortest-width printing algorithm in 2018.
(In fact Giulietti and Nadhezin showed that
126-bit tables are sufficient,
which is important because Java lacks unsigned 64-bit integers.)

</p><p>
Unrounded scaling’s shortest-width algorithm
is adapted from Schubfach,
which introduced the critical observation
that with the right choice of <math><mi>p</mi></math>,
at most one valid decimal can end in zero.
Schubfach’s main scaling operation ‘rop’
can be viewed as a special case of unrounded scaling;
Giulietti seems to have even invented the
unrounded form from first principles
(as opposed to adapting IEEE754 implementation techniques as I did).
Schubfach’s rop does not make use of the carry bit optimization,
which is the main reason it runs slower than unrounded scaling.
The Schubfach implementation was adopted
by Java’s OpenJDK <a href="https://bugs.openjdk.org/browse/JDK-8202555">after being reviewed by Steele</a>.

</p><p>
Apparently independently of Hack, Nadhezin, and Giulietti,
Adams  [<a id="fnref-2-5" href="#fn-2">2</a>] also discovered that 126-bit precision sufficed
and used that fact to build the Ryū algorithm in 2018.
Ryū does not make use of the carry bit optimization;
its rounding and exactness computations are more complex
than needed; and it finds shortest-width outputs
by repeated division by 10 of the scaled minimum
and maximum decimals, which adds to the expense.
Even so, the improvement over Grisu was clear,
and Adams’s paper was more succinct than Giulietti’s.
Many languages and browsers adopted Ryū.

</p><p>
In 2020, Jeon  [<a id="fnref-16" href="#fn-16">16</a>] proposed a new algorithm Grisu-Exact,
applying Ryū’s 128-bit results to Loitsch’s Grisu2 algorithm.
The result does remove the fallback, but it is quite complex.
In 2024, Jeon  [<a id="fnref-17-3" href="#fn-17">17</a>] proposed Dragonbox,
which applied the Grisu-Exact approach to
optimizing Schubfach.
The result does run faster but once again adds significant
complexity.
The unrounded scaling approach to shortest-width printing
in this post can also be viewed as a 128-bit Grisu2 like Grisu-Exact
or as an optimized Schubfach like Dragonbox,
but it is simpler than either.
The zero-trimming algorithm in this post
is adapted from Dragonbox’s.
<a href="#related.parse"></a></p><h3 id="related.parse"><a href="#related.parse">Parsing</a></h3>


<p>
Parsing has a much shorter history than printing.
As noted earlier,
Mancino  [<a id="fnref-24-3" href="#fn-24">24</a>] wrote in 1966 that
table-based multiplication algorithms
were “often” used for decimal-to-binary conversions.
Coonen’s 1984 thesis  [<a id="fnref-9-2" href="#fn-9">9</a>] gave a precise error analysis
for the kinds of inexact algorithms
that were used until Clinger’s 1990 publication of
“How to read floating-point numbers correctly”  [<a id="fnref-6-3" href="#fn-6">6</a>].
Clinger’s approach is to pair an inexact IEEE754
extended-precision floating-point calculation (using a float80 with a 64-bit mantissa)
to get an answer that is either the correct float64 or adjacent to the correct float64,
and then to check it with a single bignum calculation
and adjust upward or downward as needed.
Gay  [<a id="fnref-11-4" href="#fn-11">11</a>] quickly improved on this by identifying new special cases
and removing the dependence on extended precision
(replacing float80s with float64s).

</p><p>
As noted already, Slishman  [<a id="fnref-29-3" href="#fn-29">29</a>] published in 1990
a table-based parser with carry-bit-based fallback to bignums,
and then Hack  [<a id="fnref-14-3" href="#fn-14">14</a>] proved in 2004 that
128-bit precision was sufficient to remove the bignum fallback
during parsing.
While that report inspired Giulietti’s development of the Schubfach printer
and Nadhezin’s proof,
it does not seem to been used in any actual floating-point parsers
besides IBM’s.

</p><p>
In 2020, based on a suggestion and initial code by Eisel,
and apparently completely independent of Slishman and Hack,
Lemire implemented a fast floating-point parser
using a 128-bit table  [<a id="fnref-21" href="#fn-21">21</a>] [<a id="fnref-22-3" href="#fn-22">22</a>] [<a id="fnref-32" href="#fn-32">32</a>].
The Eisel-Lemire algorithm is essentially Slishman’s
except with 64 extra bits of precision instead of 16.
Lemire used a fallback just as Slishman did,
unsure that it was unreachable with 128-bit precision.
Mushtak and Lemire  [<a id="fnref-25" href="#fn-25">25</a>]
published their analog to Hack’s proof a couple years later,
allowing the fallback to be removed.

</p><p>
The unrounded scaling approach to parsing
is analogous to the approach
pioneered by Slishman, Hack, Eisel, Lemire, and Mushtak,
just framed more generally
and with the carry bit optimization.
<a href="#fast_unrounded_scaling"></a></p><h3 id="fast_unrounded_scaling"><a href="#fast_unrounded_scaling">Fast Unrounded Scaling</a></h3>


<p>
Fast unrounded scaling can be viewed as a combination,
generalization, simplification, and optimization of these critical earlier works:
</p><ul>
<li>
In 1990, Slishman  [<a id="fnref-29-4" href="#fn-29">29</a>] used the table-based algorithms
for fixed-width printing and parsing, with carry bit fallback check,
but without enough precision to be completely exact.
</li><li>
In 2004, Hack  [<a id="fnref-14-4" href="#fn-14">14</a>] improved Slishman’s algorithm
by observing that 128-bit precision allowed removing the fallback.
It is unclear why Hack considered only parsing and
did not generalize to printing.
</li><li>
In 2010, Loitsch  [<a id="fnref-23-4" href="#fn-23">23</a>] used a table-based algorithm
for shortest-width printing but, echoing Slishman,
without enough precision to be completely exact.
Loitsch used a new approach to check for exactness
and trigger a fallback.
</li><li>
In 2018, Giulietti  [<a id="fnref-12-4" href="#fn-12">12</a>] used a table-based algorithm
for shortest-width printing with enough precision to be exact,
along with the critical observation about finding
formats ending in zero.
The only arguable shortcoming was not using the carry bit
optimization to halve the cost of the scaling multiplication.
</li><li>
Also in 2018, Adams  [<a id="fnref-2-6" href="#fn-2">2</a>] used a different table-based algorithm
for shortest-width printing and popularized the fact
that 128 bits was enough precision to be exact.
</li><li>
In 2020, Eisel and Lemire  [<a id="fnref-22-4" href="#fn-22">22</a>] rederived
a 128-bit form of Slishman’s algorithm.
Then in 2023, Mushtak and Lemire  [<a id="fnref-25-2" href="#fn-25">25</a>] proved
that the fallback was unreachable
using methods similar to Hack’s.</li></ul>


<p>
Even though all the necessary pieces are in those papers waiting to be
connected, it appears that no one did until now.

</p><p>
As mentioned earlier, I derived the
unrounded scaling <a href="#fixed">fixed-width printer</a>
as an optimized version of Oudompheng’s
Ryū-inspired table-driven algorithm.
While writing up that algorithm with
a new lattice-reduction-based proof of correctness,
I re-read Loitsch’s paper and
realized that for <a href="#short">shortest-width printing</a>,
unrounded scaling
enabled replacing Grisu’s approximate
calculations of the decimal bounds
with exact calculations, eliminating the fallback entirely.
Continuing to read related papers,
I read Giulietti’s Schubfach paper for the first time
and was surprised to find how much of the
approach Giulietti had anticipated,
including apparently reinventing the IEEE754
extra bits.
When I read Lemire’s paper,
I was even more surprised to find
the carry bit fallback check;
the carry bit analysis had played an important role
in my proof of unrounded scaling,
and this was the first similar analysis I had encountered.
(At that point I had not found Slishman’s paper.)
That’s when I realized unrounded scaling also <a href="#parse">applied to parsing</a>.
I knew from my proof that Lemire didn’t need the
fallback.
When I went looking for the code that implemented it
in Lemire’s library,
I found instead a mention of Mushtak and Lemire’s followup proof.
I discovered the other references later.

</p><p>
My contribution here is primarily a
synthesis of all this prior work into a single unified framework
with a simple explanation and relatively straightforward code.
Thanks to all the authors of this critical earlier work,
whose shoulders I am grateful to be standing on.
<a href="#conclusion"></a></p><h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>


<p>
Floating-point printing and parsing of
reasonably sized decimals
can be done very quickly with very little code.
At long last, the dragons have been vanquished.

</p><p>
In this post, I have tried to give credit where credit is due
and to represent others’ work fairly and accurately.
I would be extremely grateful to receive additions, corrections,
or suggestions at <a href="mailto:rsc@swtch.com">rsc@swtch.com</a>.
<a href="#references"></a></p><h2 id="references"><a href="#references">References</a></h2>
<ol>
<li id="fn-1">
P. H. Abbott <i>et al.</i>, “<a href="https://ieeexplore.ieee.org/document/5389154">Architecture and software support in IBM S/390 Parallel Enterprise Servers for IEEE Floating-Point arithmetic</a>”, <i>IBM Journal of Research and Development</i> 43(6), September 1999. <a href="#fnref-1">↩</a>
</li><li id="fn-2">
Ulf Adams, “<a href="https://dl.acm.org/doi/10.1145/3192366.3192369">Ryū: Fast Float-to-String Conversion</a>”, Proceedings of ACM PLDI 2018. <a href="#fnref-2">↩</a>
 <a href="#fnref-2-2">↩</a>
 <a href="#fnref-2-3">↩</a>
 <a href="#fnref-2-4">↩</a>
 <a href="#fnref-2-5">↩</a>
 <a href="#fnref-2-6">↩</a>
</li><li id="fn-3">
Ulf Adams, “<a href="https://dl.acm.org/doi/10.1145/3360595">Ryū Revisited: Printf Floating Point Conversion</a>”, Proceedings of ACM OOPSLA 2019. <a href="#fnref-3">↩</a>
 <a href="#fnref-3-2">↩</a>
</li><li id="fn-4">
Marc Andrysco, Ranjit Jhala, Sorin Lerner, “<a href="https://dl.acm.org/doi/10.1145/2837614.2837654">Printing Floating-Point Numbers: An Always Correct Method</a>”, Proceedings of ACM POPL 2016. <a href="#fnref-4">↩</a>
 <a href="#fnref-4-2">↩</a>
</li><li id="fn-5">
Robert G. Burger and R. Kent Dybvig, “<a href="https://dl.acm.org/doi/10.1145/231379.231397">Printing Floating-Point Numbers Quickly and Accurately</a>”, Procedings of ACM PPLDI 1996. <a href="#fnref-5">↩</a>
</li><li id="fn-6">
William D. Clinger, “<a href="https://dl.acm.org/doi/pdf/10.1145/93548.93557">How to Read Floating Point Numbers Accurately</a>”, ACM SIGPLAN Notices 25(6), June 1990 (PLDI 1990). <a href="#fnref-6">↩</a>
 <a href="#fnref-6-2">↩</a>
 <a href="#fnref-6-3">↩</a>
</li><li id="fn-7">
Jerome T. Coonen, “<a href="https://www.computer.org/csdl/magazine/co/1980/01/01653344/13rRUxbCbof">https://www.computer.org/csdl/magazine/co/1980/01/01653344/13rRUxbCbof</a>”, <i>Computer</i>, 13, January 1980. Reprinted as Chapter 2 of  [<a id="fnref-9-3" href="#fn-9">9</a>]. <a href="#fnref-7">↩</a>
 <a href="#fnref-7-2">↩</a>
</li><li id="fn-8">
Jerome T. Coonen, “<a href="https://www.computer.org/csdl/magazine/co/1981/03/01667289/13rRUy0HYMA">Underflow and the Denormalized Numbers</a>”, <i>Computer</i> 14, March 1981. <a href="#fnref-8">↩</a>
</li><li id="fn-9">
Jerome T. Coonen, “<a href="https://ieeemilestones.ethw.org/File:JeromeCoonen_PhD_Thesis.pdf">Contributions to a Proposed Standard for Binary Floating-Point Arithmetic</a>”, University of California, Berkeley Ph.D. thesis, 1984. <a href="#fnref-9">↩</a>
 <a href="#fnref-9-2">↩</a>
 <a href="#fnref-9-3">↩</a>
</li><li id="fn-10">
T. J. Dekker, “<a href="https://csclub.uwaterloo.ca/~pbarfuss/dekker1971.pdf">A Floating-Point Technique for Extending the Available Precision</a>”, <i>Numerische Mathematik</i> 18(3), June 1971. <a href="#fnref-10">↩</a>
</li><li id="fn-11">
David M. Gay, “<a href="https://www.ampl.com/_archive/first-website/REFS/rounding.pdf">Correctly Rounded Binary-Decimal and Decimal-Binary Conversions</a>”, AT&amp;T Bell Laboratories Technical Report, 1990. <a href="#fnref-11">↩</a>
 <a href="#fnref-11-2">↩</a>
 <a href="#fnref-11-3">↩</a>
 <a href="#fnref-11-4">↩</a>
</li><li id="fn-12">
Raffaello Giulietti, “<a href="https://drive.google.com/file/d/1IEeATSVnEE6TkrHlCYNY2GjaraBjOT4f/edit">The Schubfach way to render doubles</a>”, published online, 2018, revised 2021. <a href="#fnref-12">↩</a>
 <a href="#fnref-12-2">↩</a>
 <a href="#fnref-12-3">↩</a>
 <a href="#fnref-12-4">↩</a>
</li><li id="fn-13">
Herman H. Goldstein and John von Neumann, <a href="https://www.ias.edu/sites/default/files/library/pdfs/ecp/planningcodingof0103inst.pdf"><i>Planning and Coding Problems for an Electronic Computing Instrument</i></a>, Institute for Advanced Study Report, 1947. <a href="#fnref-13">↩</a>
</li><li id="fn-14">
Michel Hack, “<a href="https://dominoweb.draco.res.ibm.com/reports/rc23203.pdf">On Intermediate Precision Required for Correctly-Rounding Decimal-to-Binary Floating-Point Conversion</a>”, IBM Technical Paper, May 2004. <a href="#fnref-14">↩</a>
 <a href="#fnref-14-2">↩</a>
 <a href="#fnref-14-3">↩</a>
 <a href="#fnref-14-4">↩</a>
</li><li id="fn-15">
Kenton Hanson, “<a href="https://web.archive.org/web/20000607192440/http://www.dnai.com/~khanson/ECRBDC.html">Economical Correctly Rounded Binary Decimal Conversions</a>”, published online 1997. <a href="#fnref-15">↩</a>
 <a href="#fnref-15-2">↩</a>
</li><li id="fn-16">
Junekey Jeon, “<a href="https://fmt.dev/papers/Grisu-Exact.pdf">Grisu-Exact: A Fast and Exact Floating-Point Printing Algorithm</a>”, published online, 2020. <a href="#fnref-16">↩</a>
</li><li id="fn-17">
Junekey Jeon, “<a href="https://raw.githubusercontent.com/jk-jeon/dragonbox/master/other_files/Dragonbox.pdf">Dragonbox: A New Floating-Point Binary-to-Decimal Conversion Algorithm</a>”, published online, 2024. <a href="#fnref-17">↩</a>
 <a href="#fnref-17-2">↩</a>
 <a href="#fnref-17-3">↩</a>
</li><li id="fn-18">
Donald E. Knuth, <i>The Art of Computer Programming, Volume 2: Seminumerical Algorithms</i>, first edition, Addison-Wesley, 1969. <a href="#fnref-18">↩</a>
 <a href="#fnref-18-2">↩</a>
</li><li id="fn-19">
Donald E. Knuth, <i>The Art of Computer Programming, Volume 2: Seminumerical Algorithms</i>, second edition, Addison-Wesley, 1981. <a href="#fnref-19">↩</a>
</li><li id="fn-20">
Donald E. Knuth, <i>The Art of Computer Programming, Volume 2: Seminumerical Algorithms</i>, third edition, Addison-Wesley, 1997. <a href="#fnref-20">↩</a>
</li><li id="fn-21">
Daniel Lemire, “<a href="https://lemire.me/blog/2020/03/10/fast-float-parsing-in-practice/">Fast float parsing in practice</a>”, published online, March 2020. <a href="#fnref-21">↩</a>
</li><li id="fn-22">
Daniel Lemire, “<a href="https://arxiv.org/abs/2101.11408">Number Parsing at a Gigabyte per Second</a>”, Software: Practice and Experience 51 (8), 2021. <a href="#fnref-22">↩</a>
 <a href="#fnref-22-2">↩</a>
 <a href="#fnref-22-3">↩</a>
 <a href="#fnref-22-4">↩</a>
</li><li id="fn-23">
Florian Loitsch, “<a href="https://dl.acm.org/doi/10.1145/1809028.1806623">Printing Floating-Point Numbers Quickly and Accurately with Integers</a>”, ACM SIGPLAN Notices 45(6), June 2010 (PLDI 2010). <a href="#fnref-23">↩</a>
 <a href="#fnref-23-2">↩</a>
 <a href="#fnref-23-3">↩</a>
 <a href="#fnref-23-4">↩</a>
</li><li id="fn-24">
O. G. Mancino, “<a href="https://dl.acm.org/doi/10.1145/355592.365635">Multiple Precision Floating-Point Conversion from Decimal-to-Binary and Vice Versa</a>”, <i>Communications of the ACM</i> 9(5), May 1966. <a href="#fnref-24">↩</a>
 <a href="#fnref-24-2">↩</a>
 <a href="#fnref-24-3">↩</a>
</li><li id="fn-25">
Noble Mushtak and Daniel Lemire, “<a href="https://arxiv.org/pdf/2212.06644">Fast Number Parsing Without Fallback</a>”, <i>Software—Pratice and Experience</i>, 2023. <a href="#fnref-25">↩</a>
 <a href="#fnref-25-2">↩</a>
</li><li id="fn-26">
Dmitry Nadhezin, <a href="https://github.com/nadezhin/verify-todec">nadezhin/verify-todec GitHub repository</a>, published online, 2018. <a href="#fnref-26">↩</a>
</li><li id="fn-27">
John F. Palmer, “<a href="https://www.arithmazium.org/library/lib/palmer_intel_standard_nov1977.pdf">The Intel Standard for Floating-Point Arithmetic</a>”, Proceedings of COMPSAC 1977. <a href="#fnref-27">↩</a>
</li><li id="fn-28">
Vern Paxson, “<a href="https://www.icir.org/vern/papers/testbase-report.pdf">A Program for Testing IEEE Decimal-Binary Conversion</a>”, class paper 1991. <a href="#fnref-28">↩</a>
</li><li id="fn-29">
Gordon Slishman, “<a href="https://mp7.watson.ibm.com/f55d084fadf9ae59852574ab0058f749.html">Fast and Perfectly Rounding Decimal/Hexadecimal Conversions</a>”, IBM Research Report, April 1990. <a href="#fnref-29">↩</a>
 <a href="#fnref-29-2">↩</a>
 <a href="#fnref-29-3">↩</a>
 <a href="#fnref-29-4">↩</a>
</li><li id="fn-30">
Guy L. Steele and Jon L. White, “<a href="https://dl.acm.org/doi/10.1145/93548.93559">How to Print Floating-Point Numbers Accurately</a>”, ACM SIGPLAN Notices 25(6), June 1990 (PLDI 1990). <a href="#fnref-30">↩</a>
 <a href="#fnref-30-2">↩</a>
 <a href="#fnref-30-3">↩</a>
</li><li id="fn-31">
Guy L. Steele and Jon L. White, “<a href="https://dl.acm.org/doi/10.1145/989393.989431">How to Print Floating-Point Numbers Accurately (Retrospective)</a>”, ACM SIGPLAN Notices 39(4), April 2004 (Best of PLDI, 1979-1999). <a href="#fnref-31">↩</a>
 <a href="#fnref-31-2">↩</a>
</li><li id="fn-32">
Nigel Tao, “<a href="https://nigeltao.github.io/blog/2020/eisel-lemire.html">The Eisel-Lemire ParseNumberF64 Algorithm</a>”, published online, October 2020. <a href="#fnref-32">↩</a>
</li><li id="fn-33">
Donald Taranto, “<a href="https://dl.acm.org/doi/10.1145/368370.368376">Binary Conversion, With Fixed Decimal Precision, Of a Decimal Fraction</a>”, <i>Communications of the ACM</i> 2(7), July 1959. <a href="#fnref-33">↩</a>
</li><li id="fn-34">
Henry S. Warren, Jr., <i>Hacker’s Delight, 2nd ed.</i>, Addison-Wesley, 2012. <a href="#fnref-34">↩</a>
</li></ol>

      </div>
    </div></div>
  </body>
</html>
