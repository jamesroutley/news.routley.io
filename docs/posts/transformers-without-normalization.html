<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jiachenzhu.github.io/DyT/">Original</a>
    <h1>Transformers Without Normalization</h1>
    
    <div id="readability-page-1" class="page"><div>
        <!-- Figure Section -->
        <section id="figure">
            <div>
                <p><img src="https://jiachenzhu.github.io/DyT/webpage_assets/before_after.svg" alt="Dynamic Tanh (DyT) as a replacement for normalization in Transformers"/></p><div>
                    <p><em>Left:</em> original Transformer block. <em>Right:</em> block with our proposed Dynamic Tanh (DyT) layer. </p></div>
            </div>
        </section>

        <!-- Abstract Section -->
        <section id="abstract">
            <h2>Abstract</h2>
                <p>
                    Normalization layers are ubiquitous in modern neural networks and have long been considered essential.
                    This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique.
                    We introduce Dynamic Tanh (DyT), an element-wise operation $$\mathrm{DyT}(\boldsymbol{x}) = \tanh(\alpha \boldsymbol{x}),$$ as a drop-in replacement for normalization layers in Transformers.
                    DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings.
                    By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning.
                    We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models.
                    These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.
                </p>
        </section>
        
        <!-- Implementation Section -->
        <section id="implementation">
            <h2>Implementation</h2>
            <p>
                DyT module can be implemented in a few lines of PyTorch code:
            </p>
            
            <div>
                <pre id="code-block-dyt"><code>class DyT(nn.Module):
    def __init__(self, num_features, alpha_init_value=0.5):
        super().__init__()
        self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
    
    def forward(self, x):
        x = torch.tanh(self.alpha * x)
        return x * self.weight + self.bias</code></pre>
            </div>
        </section>

        <!-- Key Findings Section -->
        <section id="key-findings">
            <h2>Key Findings</h2>
            
            <div>
                <div>
                    
                            <h4>Layer Normalization Behaves Like Scaled Tanh Function</h4>
                            <p>Our analysis shows that layer normalization (LN) in Transformers generates input-output mappings that closely resemble scaled tanh functions. In the earlier layers, these mappings are mostly linear. However, in deeper layers, they take on distinct S-shaped curves characteristic of tanh functions.</p>
                            
                            <div>
                                <p><img src="https://jiachenzhu.github.io/DyT/webpage_assets/figures/inout_vit.png" alt="Input-output relationships in ViT normalization layers"/>
                                </p>
                                <p><img src="https://jiachenzhu.github.io/DyT/webpage_assets/figures/inout_w2v.png" alt="Input-output relationships in wav2vec 2.0 normalization layers"/>
                                </p>
                                <p><img src="https://jiachenzhu.github.io/DyT/webpage_assets/figures/inout_dit.png" alt="Input-output relationships in DiT normalization layers"/>
                                </p>
                                <p><em>Output vs. input of selected LN layers in Vision Transformer (ViT), wav2vec 2.0 (a Transformer model for speech), and Diffusion Transformer (DiT). We plot the input/output values of four LN layers in each model. The S-shaped curves highly resemble that of a tanh function.</em>
                                </p>
                            
                            </div>
                        
                </div>
            </div>


        <!-- Evaluation Section -->
        <section id="evaluation">
            <h2>Evaluation</h2>
            <p>
                We present a comprehensive evaluation of DyT across a diverse range of architectures and tasks, highlighting its effectiveness and generalizability.  
                Our experiments cover supervised learning in vision (<b>ViT</b> and <b>ConvNeXt</b>), self-supervised learning in vision (<b>MAE</b> and <b>DINO</b>), diffusion models (<b>DiT</b>), large language models (<b>LLaMA</b>), self-supervised learning in speech (<b>wav2vec 2.0</b>), and DNA sequence modeling (<b>HyenaDNA</b> and <b>Caduceus</b>).  
                In every case, Transformers with DyT achieves similar or better performance than their normalized counterparts.  
                For detailed results and comparisons, please refer to our paper.    
            </p>
        </section>                    
                        

            

        <!-- Resources Section -->
        <section id="resources">
            <h2>Resources</h2>
            <div>
                <div>
                    <div>
                        <div>
                            <h5><i></i>Paper</h5>
                            <p>Download our paper for all the details about our research.</p>
                            <p><a href="https://arxiv.org/abs/2503.10622">Download Paper</a>
                        </p></div>
                    </div>
                </div>
                <div>
                    <div>
                        <div>
                            <h5><i></i>Code</h5>
                            <p>Check out our repository for implementation details.</p>
                            <p><a href="https://github.com/jiachenzhu/DyT">View on GitHub</a>
                        </p></div>
                    </div>
                </div>
                <div>
                    <div>
                        <div>
                            <h5><i></i>Summary</h5>
                            <p>Read a concise summary of our research findings on X.</p>
                            <p><a href="https://x.com/liuzhuang1234/status/1900370738588135805">View Summary</a>
                        </p></div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Citations Section -->
        <!-- Citations Section -->
<!-- Citations Section -->
<section id="citations">
    <h2>BibTeX</h2>
            <div>
                <pre id="code-block-citation"><code>@inproceedings{Zhu2025DyT,
  title={Transformers without Normalization},
  author={Zhu, Jiachen and Chen, Xinlei and He, Kaiming and LeCun, Yann and Liu, Zhuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}</code></pre>

    </div>
</section>

        <section>
            <h2>Correspondence</h2>
            <div>
                <p>jiachen [dot] zhu [at] nyu [dot] edu</p>
                <p>zhuangl [at] princeton [dot] edu</p>
                <!-- <p></p> -->
            </div>
        </section>
    </section></div></div>
  </body>
</html>
