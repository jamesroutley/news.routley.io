<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bernsteinbear.com/blog/simple-search/?utm_source=rss">Original</a>
    <h1>A simple search engine from scratch*</h1>
    
    <div id="readability-page-1" class="page"><div>
            <p>*if you include word2vec.</p>

<p><a href="https://www.chrisgregory.me/">Chris</a> and I spent a couple hours the other day
creating a search engine for my blog from “scratch”. Mostly he walked me
through it because I only vaguely knew what word2vec was before this experiment.</p>

<p>The search engine we made is built on <em>word embeddings</em>. This refers to some
function that takes a word and maps it onto N-dimensional space (in this case,
N=300) where each dimension vaguely corresponds to some axis of meaning.
<a href="https://jaketae.github.io/study/word2vec/">Word2vec from Scratch</a> is a nice
blog post that shows how to train your own mini word2vec and explains the
internals.</p>

<p>The idea behind the search engine is to embed each of my posts into this domain
by adding up the embeddings for the words in the post. For a given
search, we’ll embed the search the same way. Then we can rank all posts by
their <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarities</a>
to the query.</p>

<p>The equation below might look scary but it’s saying that the cosine similarity,
which is the cosine of the angle between the two vectors <code>cos(theta)</code>, is
defined as the dot product divided by the product of the magnitudes of each
vector. We’ll walk through it all in detail.</p>

<figure>
<img src="https://ohadravid.github.io/assets/img/cosine-similarity.svg"/>
<figcaption>Equation from Wikimedia&#39;s <a href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine similarity</a>
page.</figcaption>
</figure>

<p>Cosine distance is probably the simplest method for comparing a query embedding
to document embeddings to rank documents. Another intuitive choice might be
euclidean distance, which would measure how far apart two vectors are in space
(rather than the angle between them).</p>

<p>We prefer cosine distance because it preserves our intuition that two vectors
have similar meanings if they have the same proportion of each embedding
dimension. If you have two vectors that point in the same direction, but one is
very long and one very short, these should be considered the same meaning. (If
two documents are about cats, but one says the word cat much more, they’re
still just both about cats).</p>

<p>Let’s open up word2vec and embed our first words.</p>

<h2 id="embedding">Embedding</h2>

<p>We take for granted this database of the top 10,000 most popular word
embeddings, which is a 12MB pickle file that vaguely looks like this:</p>

<div><div><pre><code>couch  [0.23, 0.05, ..., 0.10]
banana [0.01, 0.80, ..., 0.20]
...
</code></pre></div></div>

<p>Chris sent it to me over the internet. If you unpickle it, it’s actually a
NumPy data structure: a dictionary mapping strings to <code>numpy.float32</code> arrays. I
wrote a script to transform this pickle file into plain old Python floats and
lists because I wanted to do this all by hand.</p>

<p>The loading code is straighforward: use the <code>pickle</code> library. The usual
security caveats apply, but I trust Chris.</p>

<div><div><pre><code><span>import</span> <span>pickle</span>

<span>def</span> <span>load_data</span><span>(</span><span>path</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>path</span><span>,</span> <span>&#34;rb&#34;</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
        <span>return</span> <span>pickle</span><span>.</span><span>load</span><span>(</span><span>f</span><span>)</span>

<span>word2vec</span> <span>=</span> <span>load_data</span><span>(</span><span>&#34;word2vec.pkl&#34;</span><span>)</span>
</code></pre></div></div>

<p>You can print out <code>word2vec</code> if you like, but it’s going to be a lot of output.
I learned that the hard way. Maybe print <code>word2vec[&#34;cat&#34;]</code> instead. That will
print out the embedding.</p>

<p>To embed a word, we need only look it up in the enormous dictionary. A nonsense
or uncommon word might not be in there, though, so we return <code>None</code> in that
case instead of raising an error.</p>

<div><div><pre><code><span>def</span> <span>embed_word</span><span>(</span><span>word2vec</span><span>,</span> <span>word</span><span>):</span>
    <span>return</span> <span>word2vec</span><span>.</span><span>get</span><span>(</span><span>word</span><span>)</span>
</code></pre></div></div>

<p>To embed multiple words, we embed each one individually and then add up the
embeddings pairwise. If a given word is not embeddable, ignore it. It’s only a
problem if we can’t understand any of the words.</p>

<div><div><pre><code><span>def</span> <span>vec_add</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
    <span>return</span> <span>[</span><span>x</span> <span>+</span> <span>y</span> <span>for</span> <span>x</span><span>,</span> <span>y</span> <span>in</span> <span>zip</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)]</span>

<span>def</span> <span>embed_words</span><span>(</span><span>word2vec</span><span>,</span> <span>words</span><span>):</span>
    <span>result</span> <span>=</span> <span>[</span><span>0.0</span><span>]</span> <span>*</span> <span>len</span><span>(</span><span>next</span><span>(</span><span>iter</span><span>(</span><span>word2vec</span><span>.</span><span>values</span><span>())))</span>
    <span>num_known</span> <span>=</span> <span>0</span>
    <span>for</span> <span>word</span> <span>in</span> <span>words</span><span>:</span>
        <span>embedding</span> <span>=</span> <span>word2vec</span><span>.</span><span>get</span><span>(</span><span>word</span><span>)</span>
        <span>if</span> <span>embedding</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>result</span> <span>=</span> <span>vec_add</span><span>(</span><span>result</span><span>,</span> <span>embedding</span><span>)</span>
            <span>num_known</span> <span>+=</span> <span>1</span>
    <span>if</span> <span>not</span> <span>num_known</span><span>:</span>
        <span>raise</span> <span>SyntaxError</span><span>(</span><span>f</span><span>&#34;I can&#39;t understand any of </span><span>{</span><span>words</span><span>}</span><span>&#34;</span><span>)</span>
    <span>return</span> <span>result</span>
</code></pre></div></div>

<p>That’s the basics of embedding: it’s a dictionary lookup and vector adds.</p>

<div><div><pre><code><span>embed_words</span><span>([</span><span>a</span><span>,</span> <span>b</span><span>])</span> <span>==</span> <span>vec_add</span><span>(</span><span>embed_word</span><span>(</span><span>a</span><span>),</span> <span>embed_word</span><span>(</span><span>b</span><span>))</span>
</code></pre></div></div>

<p>Now let’s make our “search engine index”, or the embeddings for all of my
posts.</p>

<h2 id="embedding-all-of-the-posts">Embedding all of the posts</h2>

<p>Embedding all of the posts is a recursive directory traversal where we build up
a dictionary mapping path name to embedding.</p>

<div><div><pre><code><span>import</span> <span>os</span>

<span>def</span> <span>load_post</span><span>(</span><span>pathname</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>pathname</span><span>,</span> <span>&#34;r&#34;</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
        <span>contents</span> <span>=</span> <span>f</span><span>.</span><span>read</span><span>()</span>
    <span>return</span> <span>normalize_text</span><span>(</span><span>contents</span><span>).</span><span>split</span><span>()</span>

<span>def</span> <span>load_posts</span><span>():</span>
    <span># Walk _posts looking for *.md files
</span>    <span>posts</span> <span>=</span> <span>{}</span>
    <span>for</span> <span>root</span><span>,</span> <span>dirs</span><span>,</span> <span>files</span> <span>in</span> <span>os</span><span>.</span><span>walk</span><span>(</span><span>&#34;_posts&#34;</span><span>):</span>
        <span>for</span> <span>file</span> <span>in</span> <span>files</span><span>:</span>
            <span>if</span> <span>file</span><span>.</span><span>endswith</span><span>(</span><span>&#34;.md&#34;</span><span>):</span>
                <span>pathname</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>root</span><span>,</span> <span>file</span><span>)</span>
                <span>posts</span><span>[</span><span>pathname</span><span>]</span> <span>=</span> <span>load_post</span><span>(</span><span>pathname</span><span>)</span>
    <span>return</span> <span>posts</span>

<span>post_embeddings</span> <span>=</span> <span>{</span><span>pathname</span><span>:</span> <span>embed_words</span><span>(</span><span>word2vec</span><span>,</span> <span>words</span><span>)</span>
                   <span>for</span> <span>pathname</span><span>,</span> <span>words</span> <span>in</span> <span>posts</span><span>.</span><span>items</span><span>()}</span>

<span>with</span> <span>open</span><span>(</span><span>&#34;post_embeddings.pkl&#34;</span><span>,</span> <span>&#34;wb&#34;</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>pickle</span><span>.</span><span>dump</span><span>(</span><span>post_embeddings</span><span>,</span> <span>f</span><span>)</span>
</code></pre></div></div>

<p>We do this other thing, though: <code>normalize_text</code>. This is because blog posts
are messy and contain punctuation, capital letters, and all other kinds of
nonsense. In order to get the best match, we want to put things like “CoMpIlEr”
and “compiler” in the same bucket.</p>

<div><div><pre><code><span>import</span> <span>re</span>

<span>def</span> <span>normalize_text</span><span>(</span><span>text</span><span>):</span>
    <span>return</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>&#34;[^a-zA-Z]&#34;</span><span>,</span> <span>r</span><span>&#34; &#34;</span><span>,</span> <span>text</span><span>).</span><span>lower</span><span>()</span>
</code></pre></div></div>

<p>We’ll do the same thing for each query, too. Speaking of, we should test this
out. Let’s make a little search REPL.</p>

<h2 id="a-little-search-repl">A little search REPL</h2>

<p>We’ll start off by using some Python’s built-in REPL creator library, <code>code</code>.
We can make a subclass that defines a <code>runsource</code> method. All it really needs
to do is process the <code>source</code> input and return a falsy value (otherwise it
waits for more input).</p>

<div><div><pre><code><span>import</span> <span>code</span>

<span>class</span> <span>SearchRepl</span><span>(</span><span>code</span><span>.</span><span>InteractiveConsole</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>word2vec</span><span>,</span> <span>post_embeddings</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>word2vec</span> <span>=</span> <span>word2vec</span>
        <span>self</span><span>.</span><span>post_embeddings</span> <span>=</span> <span>post_embeddings</span>

    <span>def</span> <span>runsource</span><span>(</span><span>self</span><span>,</span> <span>source</span><span>,</span> <span>filename</span><span>=</span><span>&#34;&lt;input&gt;&#34;</span><span>,</span> <span>symbol</span><span>=</span><span>&#34;single&#34;</span><span>):</span>
        <span>for</span> <span>result</span> <span>in</span> <span>self</span><span>.</span><span>search</span><span>(</span><span>source</span><span>):</span>
            <span>print</span><span>(</span><span>result</span><span>)</span>
</code></pre></div></div>

<p>Then we can define a <code>search</code> function that pulls together our existing
functions. Just like that, we have a search:</p>

<div><div><pre><code><span>class</span> <span>SearchRepl</span><span>(</span><span>code</span><span>.</span><span>InteractiveConsole</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>search</span><span>(</span><span>self</span><span>,</span> <span>query_text</span><span>,</span> <span>n</span><span>=</span><span>5</span><span>):</span>
        <span># Embed query
</span>        <span>words</span> <span>=</span> <span>normalize_text</span><span>(</span><span>query_text</span><span>).</span><span>split</span><span>()</span>
        <span>try</span><span>:</span>
            <span>query_embedding</span> <span>=</span> <span>embed_words</span><span>(</span><span>self</span><span>.</span><span>word2vec</span><span>,</span> <span>words</span><span>)</span>
        <span>except</span> <span>SyntaxError</span> <span>as</span> <span>e</span><span>:</span>
            <span>print</span><span>(</span><span>e</span><span>)</span>
            <span>return</span>
        <span># Cosine similarity
</span>        <span>post_ranks</span> <span>=</span> <span>{</span><span>pathname</span><span>:</span> <span>vec_cosine_similarity</span><span>(</span><span>query_embedding</span><span>,</span>
                                                      <span>embedding</span><span>)</span> <span>for</span> <span>pathname</span><span>,</span>
                      <span>embedding</span> <span>in</span> <span>self</span><span>.</span><span>post_embeddings</span><span>.</span><span>items</span><span>()}</span>
        <span>posts_by_rank</span> <span>=</span> <span>sorted</span><span>(</span><span>post_ranks</span><span>.</span><span>items</span><span>(),</span>
                               <span>reverse</span><span>=</span><span>True</span><span>,</span>
                               <span>key</span><span>=</span><span>lambda</span> <span>entry</span><span>:</span> <span>entry</span><span>[</span><span>1</span><span>])</span>
        <span>top_n_posts_by_rank</span> <span>=</span> <span>posts_by_rank</span><span>[:</span><span>n</span><span>]</span>
        <span>return</span> <span>[</span><span>path</span> <span>for</span> <span>path</span><span>,</span> <span>_</span> <span>in</span> <span>top_n_posts_by_rank</span><span>]</span>
</code></pre></div></div>

<p>Yes, we have to do a cosine similarity. Thankfully, the Wikipedia math snippet
translates almost 1:1 to Python code:</p>

<div><div><pre><code><span>import</span> <span>math</span>

<span>def</span> <span>vec_norm</span><span>(</span><span>v</span><span>):</span>
    <span>return</span> <span>math</span><span>.</span><span>sqrt</span><span>(</span><span>sum</span><span>([</span><span>x</span><span>*</span><span>x</span> <span>for</span> <span>x</span> <span>in</span> <span>v</span><span>]))</span>

<span>def</span> <span>vec_cosine_similarity</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
    <span>assert</span> <span>len</span><span>(</span><span>a</span><span>)</span> <span>==</span> <span>len</span><span>(</span><span>b</span><span>)</span>
    <span>a_norm</span> <span>=</span> <span>vec_norm</span><span>(</span><span>a</span><span>)</span>
    <span>b_norm</span> <span>=</span> <span>vec_norm</span><span>(</span><span>b</span><span>)</span>
    <span>dot_product</span> <span>=</span> <span>sum</span><span>([</span><span>ax</span><span>*</span><span>bx</span> <span>for</span> <span>ax</span><span>,</span> <span>bx</span> <span>in</span> <span>zip</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)])</span>
    <span>return</span> <span>dot_product</span><span>/</span><span>(</span><span>a_norm</span><span>*</span><span>b_norm</span><span>)</span>
</code></pre></div></div>

<p>Finally, we can create and run the REPL.</p>

<div><div><pre><code><span>sys</span><span>.</span><span>ps1</span> <span>=</span> <span>&#34;QUERY. &#34;</span>
<span>sys</span><span>.</span><span>ps2</span> <span>=</span> <span>&#34;...... &#34;</span>

<span>repl</span> <span>=</span> <span>SearchRepl</span><span>(</span><span>word2vec</span><span>,</span> <span>post_embeddings</span><span>)</span>
<span>repl</span><span>.</span><span>interact</span><span>(</span><span>banner</span><span>=</span><span>&#34;&#34;</span><span>,</span> <span>exitmsg</span><span>=</span><span>&#34;&#34;</span><span>)</span>
</code></pre></div></div>

<p>This is what interacting with it looks like:</p>

<div><div><pre><code><span>QUERY.</span><span> </span><span>type </span>inference
<span>_posts/2024-10-15-type-inference.md
_posts/2025-03-10-lattice-bitset.md
_posts/2025-02-24-sctp.md
_posts/2022-11-07-inline-caches-in-skybison.md
_posts/2021-01-14-inline-caching.md
</span><span>QUERY.</span><span>
</span></code></pre></div></div>

<p>This is a sample query from a very small dataset (my blog). It’s a pretty good
search result, but it’s probably not representative of the overall search
quality. Chris says that I should cherry-pick “because everyone in AI does”.</p>

<p>Okay, that’s really neat. But most people who want to look for something on
my website do not run for their terminals. Though my site is expressly designed
to work well in terminal browsers such as Lynx, most people are already in a
graphical web browser. So let’s make a search front-end.</p>

<h2 id="a-little-web-search">A little web search</h2>

<p>So far we’ve been running from my local machine where I don’t mind having a
12MB file of weights sitting around. Now that we’re moving to web, I would
rather not burden casual browsers with an unexpected big download. So we need
to get clever.</p>

<p>Fortunately, Chris and I had both seen <a href="https://phiresky.github.io/blog/2021/hosting-sqlite-databases-on-github-pages/">this really cool blog post</a>
that talks about hosting a SQLite database on GitHub Pages. The blog post
details how the author:</p>

<ul>
  <li>compiled SQLite to Wasm so it could run on the client,</li>
  <li>built a virtual filesystem so it could read database files from the web,</li>
  <li>did some smart page fetching using the existing SQLite indexes,</li>
  <li>built additional software to fetch only small chunks of the database using
HTTP Range requests</li>
</ul>

<p>That’s super cool, but again: SQLite, though small, is comparatively big for
this project. We want to build things from scratch. Fortunately, we can emulate
the main ideas.</p>

<p>We can give the word2vec dict a stable order and split it into two files. One
file can just have the embeddings, no names. Another file, the index, can map
every word to the byte start and byte length of the weights for that word (we
figure start&amp;length is probably smaller on the wire than start&amp;end).</p>

<div><div><pre><code><span>#</span><span> </span><span>vecs.jsonl</span><span>
</span><span>[</span><span>0.23</span><span>,</span><span> </span><span>0.05</span><span>,</span><span> </span><span>...</span><span>,</span><span> </span><span>0.10</span><span>]</span><span>
</span><span>[</span><span>0.01</span><span>,</span><span> </span><span>0.80</span><span>,</span><span> </span><span>...</span><span>,</span><span> </span><span>0.20</span><span>]</span><span>
</span><span>...</span><span>
</span></code></pre></div></div>

<div><div><pre><code><span>#</span><span> </span><span>index.json</span><span>
</span><span>{</span><span>&#34;couch&#34;</span><span>:</span><span> </span><span>[</span><span>0</span><span>,</span><span> </span><span>20</span><span>],</span><span> </span><span>&#34;banana&#34;</span><span>:</span><span> </span><span>[</span><span>20</span><span>,</span><span> </span><span>30</span><span>],</span><span> </span><span>...</span><span>}</span><span>
</span></code></pre></div></div>

<p>The cool thing about this is that <code>index.json</code> is <em>dramatically</em> smaller than
the word2vec blob, weighing in at 244KB. Since that won’t change very often
(how often does word2vec change?), I don’t feel so bad about users eagerly
downloading the entire index.  Similarly, the <code>post_embeddings.json</code> is only
388KB. They’re even cacheable. And automagically (de)compressed by the server
and browser (to 84KB and 140KB, respectively). Both would be smaller if we
chose a binary format, but we’re punting on that for the purposes of this post.</p>

<p>Then we can make HTTP Range requests to the server and only download the parts
of the weights that we need. It’s even possible to bundle all of the ranges
into one request (it’s called multipart range). Unfortunately, GitHub Pages
does not appear to support multipart, so instead we download each word’s range
in a separate request.</p>

<p>Here’s the pertinent JS code, with (short, very familiar) vector functions
omitted:</p>

<div><div><pre><code><span>(</span><span>async</span> <span>function</span><span>()</span> <span>{</span>
  <span>// Download stuff</span>
  <span>async</span> <span>function</span> <span>get_index</span><span>()</span> <span>{</span>
    <span>const</span> <span>req</span> <span>=</span> <span>await</span> <span>fetch</span><span>(</span><span>&#34;</span><span>index.json</span><span>&#34;</span><span>);</span>
    <span>return</span> <span>req</span><span>.</span><span>json</span><span>();</span>
  <span>}</span>
  <span>async</span> <span>function</span> <span>get_post_embeddings</span><span>()</span> <span>{</span>
    <span>const</span> <span>req</span> <span>=</span> <span>await</span> <span>fetch</span><span>(</span><span>&#34;</span><span>post_embeddings.json</span><span>&#34;</span><span>);</span>
    <span>return</span> <span>req</span><span>.</span><span>json</span><span>();</span>
  <span>}</span>
  <span>const</span> <span>index</span> <span>=</span> <span>new</span> <span>Map</span><span>(</span><span>Object</span><span>.</span><span>entries</span><span>(</span><span>await</span> <span>get_index</span><span>()));</span>
  <span>const</span> <span>post_embeddings</span> <span>=</span> <span>new</span> <span>Map</span><span>(</span><span>Object</span><span>.</span><span>entries</span><span>(</span><span>await</span> <span>get_post_embeddings</span><span>()));</span>
  <span>// Add search handler</span>
  <span>search</span><span>.</span><span>addEventListener</span><span>(</span><span>&#34;</span><span>input</span><span>&#34;</span><span>,</span> <span>debounce</span><span>(</span><span>async</span> <span>function</span><span>(</span><span>value</span><span>)</span> <span>{</span>
    <span>const</span> <span>query</span> <span>=</span> <span>search</span><span>.</span><span>value</span><span>;</span>
    <span>// TODO(max): Normalize query</span>
    <span>const</span> <span>words</span> <span>=</span> <span>query</span><span>.</span><span>split</span><span>(</span><span>/</span><span>\s</span><span>+/</span><span>);</span>
    <span>if</span> <span>(</span><span>words</span><span>.</span><span>length</span> <span>===</span> <span>0</span><span>)</span> <span>{</span>
      <span>// No words</span>
      <span>return</span><span>;</span>
    <span>}</span>
    <span>const</span> <span>requests</span> <span>=</span> <span>words</span><span>.</span><span>reduce</span><span>((</span><span>acc</span><span>,</span> <span>word</span><span>)</span> <span>=&gt;</span> <span>{</span>
      <span>const</span> <span>entry</span> <span>=</span> <span>index</span><span>.</span><span>get</span><span>(</span><span>word</span><span>);</span>
      <span>if</span> <span>(</span><span>entry</span> <span>===</span> <span>undefined</span><span>)</span> <span>{</span>
        <span>// Word is not valid; skip it</span>
        <span>return</span> <span>acc</span><span>;</span>
      <span>}</span>
      <span>const</span> <span>[</span><span>start</span><span>,</span> <span>length</span><span>]</span> <span>=</span> <span>entry</span><span>;</span>
      <span>const</span> <span>end</span> <span>=</span> <span>start</span><span>+</span><span>length</span><span>-</span><span>1</span><span>;</span>
      <span>acc</span><span>.</span><span>push</span><span>(</span><span>fetch</span><span>(</span><span>&#34;</span><span>vecs.jsonl</span><span>&#34;</span><span>,</span> <span>{</span>
        <span>headers</span><span>:</span> <span>new</span> <span>Headers</span><span>({</span>
          <span>&#34;</span><span>Range</span><span>&#34;</span><span>:</span> <span>`bytes=</span><span>${</span><span>start</span><span>}</span><span>-</span><span>${</span><span>end</span><span>}</span><span>`</span><span>,</span>
        <span>}),</span>
      <span>}));</span>
      <span>return</span> <span>acc</span><span>;</span>
    <span>},</span> <span>[]);</span>
    <span>if</span> <span>(</span><span>requests</span><span>.</span><span>length</span> <span>===</span> <span>0</span><span>)</span> <span>{</span>
      <span>// None are valid words :(</span>
      <span>search_results</span><span>.</span><span>innerHTML</span> <span>=</span> <span>&#34;</span><span>No results :(</span><span>&#34;</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>
    <span>const</span> <span>responses</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span><span>requests</span><span>);</span>
    <span>const</span> <span>embeddings</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span><span>responses</span><span>.</span><span>map</span><span>(</span><span>r</span> <span>=&gt;</span> <span>r</span><span>.</span><span>json</span><span>()));</span>
    <span>const</span> <span>query_embedding</span> <span>=</span> <span>embeddings</span><span>.</span><span>reduce</span><span>((</span><span>acc</span><span>,</span> <span>e</span><span>)</span> <span>=&gt;</span> <span>vec_add</span><span>(</span><span>acc</span><span>,</span> <span>e</span><span>));</span>
    <span>const</span> <span>post_ranks</span> <span>=</span> <span>{};</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span><span>path</span><span>,</span> <span>embedding</span><span>]</span> <span>of</span> <span>post_embeddings</span><span>)</span> <span>{</span>
      <span>post_ranks</span><span>[</span><span>path</span><span>]</span> <span>=</span> <span>vec_cosine_similarity</span><span>(</span><span>embedding</span><span>,</span> <span>query_embedding</span><span>);</span>
    <span>}</span>
    <span>const</span> <span>sorted_ranks</span> <span>=</span> <span>Object</span><span>.</span><span>entries</span><span>(</span><span>post_ranks</span><span>).</span><span>sort</span><span>(</span><span>function</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>{</span>
      <span>// Decreasing</span>
      <span>return</span> <span>b</span><span>[</span><span>1</span><span>]</span><span>-</span><span>a</span><span>[</span><span>1</span><span>];</span>
    <span>});</span>
    <span>// Fun fact: HTML elements with an `id` attribute are accessible as JS</span>
    <span>// globals by that same name.</span>
    <span>search_results</span><span>.</span><span>innerHTML</span> <span>=</span> <span>&#34;&#34;</span><span>;</span>
    <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>5</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
      <span>search_results</span><span>.</span><span>innerHTML</span> <span>+=</span> <span>`&lt;li&gt;</span><span>${</span><span>sorted_ranks</span><span>[</span><span>i</span><span>][</span><span>0</span><span>]}</span><span>&lt;/li&gt;`</span><span>;</span>
    <span>}</span>
  <span>}));</span>
<span>})();</span>
</code></pre></div></div>

<p>You can take a look at the live <a href="https://bernsteinbear.com/websearch/">search
page</a>. In particular, open up the network
requests tab of your browser’s console. Marvel as it only downloads a couple
4KB chunks of embeddings.</p>

<p>So how well does our search technology work? Let’s try to build an
objective-ish evaluation.</p>

<h2 id="evaluation">Evaluation</h2>
<p>We’ll design a metric that roughly tells us when our search engine is better or worse than a naive approach without word embeddings.</p>

<p>We start by collecting an evaluation dataset of <code>(document, query)</code> pairs. Right from the start we’re going to bias this evaluation by collecting this dataset ourselves, but hopefully it’ll still help us get an intuition about the quality of the search. A query in this case is just a few search terms that we think should retrieve a document successfully.</p>

<div><div><pre><code><span>sample_documents</span> <span>=</span> <span>{</span>
  <span>&#34;_posts/2024-10-27-on-the-universal-relation.md&#34;</span><span>:</span> <span>&#34;database relation universal tuple function&#34;</span><span>,</span>
  <span>&#34;_posts/2024-08-25-precedence-printing.md&#34;</span><span>:</span> <span>&#34;operator precedence pretty print parenthesis&#34;</span><span>,</span>
  <span>&#34;_posts/2019-03-11-understanding-the-100-prisoners-problem.md&#34;</span><span>:</span> <span>&#34;probability strategy game visualization simulation&#34;</span><span>,</span>
  <span># ...
</span><span>}</span>
</code></pre></div></div>

<p>Now that we’ve collected our dataset, let’s implement a top-k accuracy metric. This metric measures the percentage of the time a document appears in the top k search results given its corresponding query.</p>

<div><div><pre><code><span>def</span> <span>compute_top_k_accuracy</span><span>(</span>
    <span># Mapping of post to sample search query (already normalized)
</span>    <span># See sample_documents above
</span>    <span>eval_set</span><span>:</span> <span>dict</span><span>[</span><span>str</span><span>,</span> <span>str</span><span>],</span>
    <span>max_n_keywords</span><span>:</span> <span>int</span><span>,</span>
    <span>max_top_k</span><span>:</span> <span>int</span><span>,</span>
    <span>n_query_samples</span><span>:</span> <span>int</span><span>,</span>
<span>)</span> <span>-&gt;</span> <span>list</span><span>[</span><span>list</span><span>[</span><span>float</span><span>]]:</span>
    <span>counts</span> <span>=</span> <span>[[</span><span>0</span><span>]</span> <span>*</span> <span>max_top_k</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>max_n_keywords</span><span>)]</span>
    <span>for</span> <span>n_keywords</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>max_n_keywords</span> <span>+</span> <span>1</span><span>):</span>
        <span>for</span> <span>post_id</span><span>,</span> <span>keywords_str</span> <span>in</span> <span>eval_set</span><span>.</span><span>items</span><span>():</span>
            <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>n_query_samples</span><span>):</span>
                <span># Construct a search query by sampling keywords
</span>                <span>keywords</span> <span>=</span> <span>keywords_str</span><span>.</span><span>split</span><span>(</span><span>&#34; &#34;</span><span>)</span>
                <span>sampled_keywords</span> <span>=</span> <span>random</span><span>.</span><span>choices</span><span>(</span><span>keywords</span><span>,</span> <span>k</span><span>=</span><span>n_keywords</span><span>)</span>
                <span>query</span> <span>=</span> <span>&#34; &#34;</span><span>.</span><span>join</span><span>(</span><span>sampled_keywords</span><span>)</span>

                <span># Determine the rank of the target post in the search results
</span>                <span>ids</span> <span>=</span> <span>search</span><span>(</span><span>query</span><span>,</span> <span>n</span><span>=</span><span>max_top_k</span><span>)</span>
                <span>rank</span> <span>=</span> <span>safe_index</span><span>(</span><span>ids</span><span>,</span> <span>post_id</span><span>)</span>

                <span># Increment the count of the rank
</span>                <span>if</span> <span>rank</span> <span>is</span> <span>not</span> <span>None</span> <span>and</span> <span>rank</span> <span>&lt;</span> <span>max_top_k</span><span>:</span>
                    <span>counts</span><span>[</span><span>n_keywords</span> <span>-</span> <span>1</span><span>][</span><span>rank</span><span>]</span> <span>+=</span> <span>1</span>

    <span>accuracies</span> <span>=</span> <span>[[</span><span>0.0</span><span>]</span> <span>*</span> <span>max_top_k</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>max_n_keywords</span><span>)]</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>max_n_keywords</span><span>):</span>
        <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>max_top_k</span><span>):</span>
            <span># Divide by the number of samples to get the average across samples and
</span>            <span># divide by the size of the eval set to get accuracy over all posts.
</span>            <span>accuracies</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>=</span> <span>counts</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>/</span> <span>n_query_samples</span> <span>/</span> <span>len</span><span>(</span><span>eval_set</span><span>)</span>

            <span># Accumulate accuracies because if a post is retrieved at rank i,
</span>            <span># it was also successfully retrieved at all ranks j &gt; i.
</span>            <span>if</span> <span>j</span> <span>&gt;</span> <span>0</span><span>:</span>
                <span>accuracies</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>+=</span> <span>accuracies</span><span>[</span><span>i</span><span>][</span><span>j</span> <span>-</span> <span>1</span><span>]</span>

    <span>return</span> <span>accuracies</span>
</code></pre></div></div>

<p>Let’s start by evaluating a baseline search engine. This implementation doesn’t use word embeddings at all. We just normalize the text, and count the number of times each query word occur in the document, then rank the documents by number of query word occurrences. Plotting top-k accuracy for various values of k gives us the following chart. Note that we get higher accuracy as we increase k – in the limit, as k approaches our number of documents we approach 100% accuracy.</p>

<p>You also might notice that the accuracy increases as we increase the number of keywords. We can see also the lines getting closer together as the number of keywords increases, which indicates there are diminishing marginal returns for each new keyword.</p>

<figure>
  <img src="https://ohadravid.github.io/assets/img/search-top-k.png"/>
</figure>

<p>Do these megabytes of word embeddings actually do anything to improve our search? We would have to compare to a baseline. Maybe that baseline is adding up the counts of all keywords in each document to rank them. We leave this as an exercise to the reader because we ran out of time :)</p>

<p>It would also be interesting to see how a bigger word2vec helps accuracy. While
sampling for top-k, there is a lot of error output (<code>I can&#39;t understand any of
[&#39;prank&#39;, ...]</code>). These unknown words get dropped from the search. A bigger
word2vec (more than 10,000 words) might contain these less-common words and
therefore search better.</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>You can build a small search engine from “scratch” with only a hundred or so
lines of code. See <a href="https://github.com/tekknolagi/tekknolagi.github.com/blob/25d0f5bbe04db7a907409dd5a48648dc8bbd3307/search.py">the full
search.py</a>,
which includes some of the extras for evaluation and plotting.</p>

<h2 id="future-ideas">Future ideas</h2>

<p>We can get fancier than simple cosine similarity. Let’s imagine that all of our
documents talk about computers, but only one of them talks about compilers
(wouldn’t that be sad). If one of our search terms is “computer” that doesn’t
really help narrow down the search and is noise in our embeddings. To reduce
noise we can employ a technique called <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> (term frequency inverse
document frequency) where we factor out common words across documents and pay
closer attention to words that are more unique to each document.</p>


        </div></div>
  </body>
</html>
