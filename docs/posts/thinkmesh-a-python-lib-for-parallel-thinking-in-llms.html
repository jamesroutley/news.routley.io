<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/martianlantern/ThinkMesh">Original</a>
    <h1>ThinkMesh: A Python lib for parallel thinking in LLMs</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">ThinkMesh is a python library for running  diverse reasoning paths in parallel, scoring them with internal confidence signals, reallocates compute to promising branches, and fuses outcomes with verifiers and reducers. It works with offline Hugging Face Transformers and vLLM/TGI, and with hosted APIs.</p>
<blockquote>
<p dir="auto">Note: This is still in it&#39;s early development phase and breaking changes can sometimes occur</p>
</blockquote>

<ul dir="auto">
<li>Parallel reasoning with DeepConf‑style confidence gating and budget reallocation</li>
<li>Offline‑first with Transformers; optional vLLM/TGI for server‑side batching</li>
<li>Hosted adapters for OpenAI and Anthropic</li>
<li>Async execution with dynamic micro‑batches</li>
<li>Reducers (majority/judge) and pluggable verifiers (regex/numeric/custom)</li>
<li>Caching, metrics, and JSON traces</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/martianlantern/thinkmesh.git
cd thinkmesh
pip install -e &#34;.[dev,transformers]&#34;"><pre>git clone https://github.com/martianlantern/thinkmesh.git
<span>cd</span> thinkmesh
pip install -e <span><span>&#34;</span>.[dev,transformers]<span>&#34;</span></span></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Quickstart: Offline DeepConf</h2><a id="user-content-quickstart-offline-deepconf" aria-label="Permalink: Quickstart: Offline DeepConf" href="#quickstart-offline-deepconf"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="from thinkmesh import think, ThinkConfig, ModelSpec, StrategySpec

cfg = ThinkConfig(
  model=ModelSpec(backend=&#34;transformers&#34;, model_name=&#34;Qwen2.5-7B-Instruct&#34;,
                  max_tokens=256, temperature=0.7, seed=42, extra={&#34;device&#34;:&#34;cuda:0&#34;}),
  strategy=StrategySpec(name=&#34;deepconf&#34;, parallel=8, max_steps=2,
                        deepconf={&#34;k&#34;:5,&#34;tau_low&#34;:-1.25,&#34;tau_ent&#34;:2.2,&#34;realloc_top_p&#34;:0.4}),
  reducer={&#34;name&#34;:&#34;majority&#34;},
  budgets={&#34;wall_clock_s&#34;:20,&#34;tokens&#34;:4000},
)
ans = think(&#34;Show that the product of any three consecutive integers is divisible by 3.&#34;, cfg)
print(ans.content, ans.confidence)"><pre><span>from</span> <span>thinkmesh</span> <span>import</span> <span>think</span>, <span>ThinkConfig</span>, <span>ModelSpec</span>, <span>StrategySpec</span>

<span>cfg</span> <span>=</span> <span>ThinkConfig</span>(
  <span>model</span><span>=</span><span>ModelSpec</span>(<span>backend</span><span>=</span><span>&#34;transformers&#34;</span>, <span>model_name</span><span>=</span><span>&#34;Qwen2.5-7B-Instruct&#34;</span>,
                  <span>max_tokens</span><span>=</span><span>256</span>, <span>temperature</span><span>=</span><span>0.7</span>, <span>seed</span><span>=</span><span>42</span>, <span>extra</span><span>=</span>{<span>&#34;device&#34;</span>:<span>&#34;cuda:0&#34;</span>}),
  <span>strategy</span><span>=</span><span>StrategySpec</span>(<span>name</span><span>=</span><span>&#34;deepconf&#34;</span>, <span>parallel</span><span>=</span><span>8</span>, <span>max_steps</span><span>=</span><span>2</span>,
                        <span>deepconf</span><span>=</span>{<span>&#34;k&#34;</span>:<span>5</span>,<span>&#34;tau_low&#34;</span>:<span>-</span><span>1.25</span>,<span>&#34;tau_ent&#34;</span>:<span>2.2</span>,<span>&#34;realloc_top_p&#34;</span>:<span>0.4</span>}),
  <span>reducer</span><span>=</span>{<span>&#34;name&#34;</span>:<span>&#34;majority&#34;</span>},
  <span>budgets</span><span>=</span>{<span>&#34;wall_clock_s&#34;</span>:<span>20</span>,<span>&#34;tokens&#34;</span>:<span>4000</span>},
)
<span>ans</span> <span>=</span> <span>think</span>(<span>&#34;Show that the product of any three consecutive integers is divisible by 3.&#34;</span>, <span>cfg</span>)
<span>print</span>(<span>ans</span>.<span>content</span>, <span>ans</span>.<span>confidence</span>)</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Quickstart: OpenAI Self‑Consistency</h2><a id="user-content-quickstart-openai-selfconsistency" aria-label="Permalink: Quickstart: OpenAI Self‑Consistency" href="#quickstart-openai-selfconsistency"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="import os
os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;sk-...&#34;
from thinkmesh import think, ThinkConfig, ModelSpec, StrategySpec
cfg = ThinkConfig(
  model=ModelSpec(backend=&#34;openai&#34;, model_name=&#34;gpt-4o-mini&#34;, max_tokens=256, temperature=0.6),
  strategy=StrategySpec(name=&#34;self_consistency&#34;, parallel=6, max_steps=1),
  reducer={&#34;name&#34;:&#34;majority&#34;},
  budgets={&#34;wall_clock_s&#34;:15,&#34;tokens&#34;:3000},
)
print(think(&#34;List three creative uses for a paperclip.&#34;, cfg).content)"><pre><span>import</span> <span>os</span>
<span>os</span>.<span>environ</span>[<span>&#34;OPENAI_API_KEY&#34;</span>] <span>=</span> <span>&#34;sk-...&#34;</span>
<span>from</span> <span>thinkmesh</span> <span>import</span> <span>think</span>, <span>ThinkConfig</span>, <span>ModelSpec</span>, <span>StrategySpec</span>
<span>cfg</span> <span>=</span> <span>ThinkConfig</span>(
  <span>model</span><span>=</span><span>ModelSpec</span>(<span>backend</span><span>=</span><span>&#34;openai&#34;</span>, <span>model_name</span><span>=</span><span>&#34;gpt-4o-mini&#34;</span>, <span>max_tokens</span><span>=</span><span>256</span>, <span>temperature</span><span>=</span><span>0.6</span>),
  <span>strategy</span><span>=</span><span>StrategySpec</span>(<span>name</span><span>=</span><span>&#34;self_consistency&#34;</span>, <span>parallel</span><span>=</span><span>6</span>, <span>max_steps</span><span>=</span><span>1</span>),
  <span>reducer</span><span>=</span>{<span>&#34;name&#34;</span>:<span>&#34;majority&#34;</span>},
  <span>budgets</span><span>=</span>{<span>&#34;wall_clock_s&#34;</span>:<span>15</span>,<span>&#34;tokens&#34;</span>:<span>3000</span>},
)
<span>print</span>(<span>think</span>(<span>&#34;List three creative uses for a paperclip.&#34;</span>, <span>cfg</span>).<span>content</span>)</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="thinkmesh think -m Qwen2.5-7B-Instruct --backend transformers --strategy deepconf &#34;What is 37*43?&#34;"><pre>thinkmesh think -m Qwen2.5-7B-Instruct --backend transformers --strategy deepconf <span><span>&#34;</span>What is 37*43?<span>&#34;</span></span></pre></div>


<div dir="auto" data-snippet-clipboard-copy-content="from thinkmesh import think, ThinkConfig, ModelSpec, StrategySpec
cfg = ThinkConfig(
  model=ModelSpec(backend=&#34;openai&#34;, model_name=&#34;gpt-4o-mini&#34;, max_tokens=256, temperature=0.7),
  strategy=StrategySpec(name=&#34;debate&#34;, parallel=4, max_steps=2, debate={&#34;rounds&#34;:2}),
  reducer={&#34;name&#34;:&#34;judge&#34;},
  budgets={&#34;wall_clock_s&#34;:25,&#34;tokens&#34;:5000},
)
print(think(&#34;Argue whether every even integer &gt; 2 is the sum of two primes.&#34;, cfg).content)"><pre><span>from</span> <span>thinkmesh</span> <span>import</span> <span>think</span>, <span>ThinkConfig</span>, <span>ModelSpec</span>, <span>StrategySpec</span>
<span>cfg</span> <span>=</span> <span>ThinkConfig</span>(
  <span>model</span><span>=</span><span>ModelSpec</span>(<span>backend</span><span>=</span><span>&#34;openai&#34;</span>, <span>model_name</span><span>=</span><span>&#34;gpt-4o-mini&#34;</span>, <span>max_tokens</span><span>=</span><span>256</span>, <span>temperature</span><span>=</span><span>0.7</span>),
  <span>strategy</span><span>=</span><span>StrategySpec</span>(<span>name</span><span>=</span><span>&#34;debate&#34;</span>, <span>parallel</span><span>=</span><span>4</span>, <span>max_steps</span><span>=</span><span>2</span>, <span>debate</span><span>=</span>{<span>&#34;rounds&#34;</span>:<span>2</span>}),
  <span>reducer</span><span>=</span>{<span>&#34;name&#34;</span>:<span>&#34;judge&#34;</span>},
  <span>budgets</span><span>=</span>{<span>&#34;wall_clock_s&#34;</span>:<span>25</span>,<span>&#34;tokens&#34;</span>:<span>5000</span>},
)
<span>print</span>(<span>think</span>(<span>&#34;Argue whether every even integer &gt; 2 is the sum of two primes.&#34;</span>, <span>cfg</span>).<span>content</span>)</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="from thinkmesh import think, ThinkConfig, ModelSpec, StrategySpec
cfg = ThinkConfig(
  model=ModelSpec(backend=&#34;vllm&#34;, model_name=&#34;Qwen2.5-7B-Instruct&#34;,
                  max_tokens=256, temperature=0.7, extra={&#34;base_url&#34;:&#34;http://localhost:8000/v1&#34;,&#34;api_key&#34;:&#34;sk-&#34;}),
  strategy=StrategySpec(name=&#34;deepconf&#34;, parallel=8, max_steps=2, deepconf={&#34;k&#34;:5}),
  reducer={&#34;name&#34;:&#34;majority&#34;},
  budgets={&#34;wall_clock_s&#34;:20,&#34;tokens&#34;:4000},
)
print(think(&#34;Give a constructive proof for the Pigeonhole Principle on a simple case.&#34;, cfg).content)"><pre><span>from</span> <span>thinkmesh</span> <span>import</span> <span>think</span>, <span>ThinkConfig</span>, <span>ModelSpec</span>, <span>StrategySpec</span>
<span>cfg</span> <span>=</span> <span>ThinkConfig</span>(
  <span>model</span><span>=</span><span>ModelSpec</span>(<span>backend</span><span>=</span><span>&#34;vllm&#34;</span>, <span>model_name</span><span>=</span><span>&#34;Qwen2.5-7B-Instruct&#34;</span>,
                  <span>max_tokens</span><span>=</span><span>256</span>, <span>temperature</span><span>=</span><span>0.7</span>, <span>extra</span><span>=</span>{<span>&#34;base_url&#34;</span>:<span>&#34;http://localhost:8000/v1&#34;</span>,<span>&#34;api_key&#34;</span>:<span>&#34;sk-&#34;</span>}),
  <span>strategy</span><span>=</span><span>StrategySpec</span>(<span>name</span><span>=</span><span>&#34;deepconf&#34;</span>, <span>parallel</span><span>=</span><span>8</span>, <span>max_steps</span><span>=</span><span>2</span>, <span>deepconf</span><span>=</span>{<span>&#34;k&#34;</span>:<span>5</span>}),
  <span>reducer</span><span>=</span>{<span>&#34;name&#34;</span>:<span>&#34;majority&#34;</span>},
  <span>budgets</span><span>=</span>{<span>&#34;wall_clock_s&#34;</span>:<span>20</span>,<span>&#34;tokens&#34;</span>:<span>4000</span>},
)
<span>print</span>(<span>think</span>(<span>&#34;Give a constructive proof for the Pigeonhole Principle on a simple case.&#34;</span>, <span>cfg</span>).<span>content</span>)</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="from thinkmesh import think, ThinkConfig, ModelSpec, StrategySpec
cfg = ThinkConfig(
  model=ModelSpec(backend=&#34;transformers&#34;, model_name=&#34;Qwen2.5-7B-Instruct&#34;, max_tokens=128),
  strategy=StrategySpec(name=&#34;self_consistency&#34;, parallel=5, max_steps=1),
  reducer={&#34;name&#34;:&#34;majority&#34;},
  verifier={&#34;type&#34;:&#34;regex&#34;,&#34;pattern&#34;:r&#34;Final Answer\s*:\s*.+$&#34;},
  budgets={&#34;wall_clock_s&#34;:10,&#34;tokens&#34;:1500},
)
print(think(&#34;Answer with &#39;Final Answer: &lt;value&gt;&#39; for 19*21.&#34;, cfg).content)"><pre><span>from</span> <span>thinkmesh</span> <span>import</span> <span>think</span>, <span>ThinkConfig</span>, <span>ModelSpec</span>, <span>StrategySpec</span>
<span>cfg</span> <span>=</span> <span>ThinkConfig</span>(
  <span>model</span><span>=</span><span>ModelSpec</span>(<span>backend</span><span>=</span><span>&#34;transformers&#34;</span>, <span>model_name</span><span>=</span><span>&#34;Qwen2.5-7B-Instruct&#34;</span>, <span>max_tokens</span><span>=</span><span>128</span>),
  <span>strategy</span><span>=</span><span>StrategySpec</span>(<span>name</span><span>=</span><span>&#34;self_consistency&#34;</span>, <span>parallel</span><span>=</span><span>5</span>, <span>max_steps</span><span>=</span><span>1</span>),
  <span>reducer</span><span>=</span>{<span>&#34;name&#34;</span>:<span>&#34;majority&#34;</span>},
  <span>verifier</span><span>=</span>{<span>&#34;type&#34;</span>:<span>&#34;regex&#34;</span>,<span>&#34;pattern&#34;</span>:<span>r&#34;Final Answer\s*:\s*.+$&#34;</span>},
  <span>budgets</span><span>=</span>{<span>&#34;wall_clock_s&#34;</span>:<span>10</span>,<span>&#34;tokens&#34;</span>:<span>1500</span>},
)
<span>print</span>(<span>think</span>(<span>&#34;Answer with &#39;Final Answer: &lt;value&gt;&#39; for 19*21.&#34;</span>, <span>cfg</span>).<span>content</span>)</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Tree Of Thought (offline)</h3><a id="user-content-tree-of-thought-offline" aria-label="Permalink: Tree Of Thought (offline)" href="#tree-of-thought-offline"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="from thinkmesh import think, ThinkConfig, ModelSpec, StrategySpec
cfg = ThinkConfig(
  model=ModelSpec(backend=&#34;transformers&#34;, model_name=&#34;Qwen2.5-7B-Instruct&#34;, max_tokens=192),
  strategy=StrategySpec(name=&#34;tree&#34;, parallel=6, max_steps=2, tree={&#34;branches&#34;:3,&#34;depth&#34;:2}),
  reducer={&#34;name&#34;:&#34;majority&#34;},
  budgets={&#34;wall_clock_s&#34;:20,&#34;tokens&#34;:3500},
)
print(think(&#34;Sketch a plan to prove that sqrt(2) is irrational.&#34;, cfg).content)"><pre><span>from</span> <span>thinkmesh</span> <span>import</span> <span>think</span>, <span>ThinkConfig</span>, <span>ModelSpec</span>, <span>StrategySpec</span>
<span>cfg</span> <span>=</span> <span>ThinkConfig</span>(
  <span>model</span><span>=</span><span>ModelSpec</span>(<span>backend</span><span>=</span><span>&#34;transformers&#34;</span>, <span>model_name</span><span>=</span><span>&#34;Qwen2.5-7B-Instruct&#34;</span>, <span>max_tokens</span><span>=</span><span>192</span>),
  <span>strategy</span><span>=</span><span>StrategySpec</span>(<span>name</span><span>=</span><span>&#34;tree&#34;</span>, <span>parallel</span><span>=</span><span>6</span>, <span>max_steps</span><span>=</span><span>2</span>, <span>tree</span><span>=</span>{<span>&#34;branches&#34;</span>:<span>3</span>,<span>&#34;depth&#34;</span>:<span>2</span>}),
  <span>reducer</span><span>=</span>{<span>&#34;name&#34;</span>:<span>&#34;majority&#34;</span>},
  <span>budgets</span><span>=</span>{<span>&#34;wall_clock_s&#34;</span>:<span>20</span>,<span>&#34;tokens&#34;</span>:<span>3500</span>},
)
<span>print</span>(<span>think</span>(<span>&#34;Sketch a plan to prove that sqrt(2) is irrational.&#34;</span>, <span>cfg</span>).<span>content</span>)</pre></div>

<p dir="auto">Traces are emitted as JSON graphs inside the returned structure. Prometheus metrics and OpenTelemetry spans can be enabled via config extras. A local disk cache deduplicates repeated generations by hashing adapter, model, prompt, and params.</p>

<ul dir="auto">
<li>Implement a new backend by providing a <code>Thinker.generate</code> method that returns token text and optional token logprobs</li>
<li>Add a new strategy by wiring a function in <code>thinkmesh/strategies</code> and registering by name</li>
<li>Add reducers/verifiers under <code>thinkmesh/reduce</code></li>
</ul>

<p dir="auto">MIT</p>

<div data-snippet-clipboard-copy-content="@misc{deepconf2025,
  title         = {DeepConf: Deep Think with Confidence},
  year          = {2025},
  howpublished  = {\url{https://jiaweizzhao.github.io/deepconf/}}
}

@misc{wang2022selfconsistency,
  title         = {Self-Consistency Improves Chain-of-Thought Reasoning in Language Models},
  author        = {Wang, Xuezhi and Wei, Jason and others},
  year          = {2022},
  eprint        = {2203.11171},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{yao2023tree,
  title         = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author        = {Yao, Shunyu and others},
  year          = {2023},
  eprint        = {2305.10601},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI}
}"><pre lang="bibex"><code>@misc{deepconf2025,
  title         = {DeepConf: Deep Think with Confidence},
  year          = {2025},
  howpublished  = {\url{https://jiaweizzhao.github.io/deepconf/}}
}

@misc{wang2022selfconsistency,
  title         = {Self-Consistency Improves Chain-of-Thought Reasoning in Language Models},
  author        = {Wang, Xuezhi and Wei, Jason and others},
  year          = {2022},
  eprint        = {2203.11171},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{yao2023tree,
  title         = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author        = {Yao, Shunyu and others},
  year          = {2023},
  eprint        = {2305.10601},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI}
}
</code></pre></div>

<p dir="auto">If you use this library in your work, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@software{thinkmesh2025,
  title        = {ThinkMesh: Parallel Thinking for LLMs},
  author       = {martianlantern},
  year         = {2025},
  note         = {Version 0.1.1},
}"><pre><span>@software</span>{<span>thinkmesh2025</span>,
  <span>title</span>        = <span><span>{</span>ThinkMesh: Parallel Thinking for LLMs<span>}</span></span>,
  <span>author</span>       = <span><span>{</span>martianlantern<span>}</span></span>,
  <span>year</span>         = <span><span>{</span>2025<span>}</span></span>,
  <span>note</span>         = <span><span>{</span>Version 0.1.1<span>}</span></span>,
}</pre></div>
</article></div></div>
  </body>
</html>
