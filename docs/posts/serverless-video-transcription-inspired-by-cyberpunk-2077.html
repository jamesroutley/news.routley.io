<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/elanmart/cbp-translate">Original</a>
    <h1>Serverless Video Transcription inspired by Cyberpunk 2077</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">

<p dir="auto">I finally got around to playing <a href="https://www.gog.com/en/game/cyberpunk_2077" rel="nofollow">Cyberpunk 2077</a> the other day, and I noticed that the game has one interesting feature:</p>
<p dir="auto">when a character speaks a foreign language, the text first appears above them in the original form, and then gets sort of live-translated into English.</p>
<p dir="auto">I&#39;ve then asked myself: how much work would it take to build something like that with modern DL stack? Is it possible to do it over a weekend?</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/994e88fdb381b50dc8b58182114b6345c50c1454edbf542c9a49f1b4e6ca9601/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f62554d616843576e384f704575736d4a56382f67697068792e676966"><img src="https://camo.githubusercontent.com/994e88fdb381b50dc8b58182114b6345c50c1454edbf542c9a49f1b4e6ca9601/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f62554d616843576e384f704575736d4a56382f67697068792e676966" alt="cyberpunk-example-gif" width="600" data-animated-image="" data-canonical-src="https://media.giphy.com/media/bUMahCWn8OpEusmJV8/giphy.gif"/></a>
</p>
<h2 dir="auto"><a id="user-content-12-the-rough-requirements" aria-hidden="true" href="#12-the-rough-requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>1.2 The rough requirements</h2>
<p dir="auto">I wanted to have a system which would</p>
<ul dir="auto">
<li>Process short video clips (e.g. a single scene)</li>
<li>Work with multiple characters / speakers</li>
<li>Detect and transcribe speech in both English and Polish</li>
<li>Translate the speech to any language</li>
<li>Assign each phrase to a speaker</li>
<li>Show the speaker on the screen</li>
<li>Add subtitles to the original video in a way mimicking the Cyberpunk example</li>
<li>Have a nice frontend</li>
<li>Run remotely in the cloud</li>
</ul>
<h2 dir="auto"><a id="user-content-13-the-tldr" aria-hidden="true" href="#13-the-tldr"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>1.3 The TL;DR</h2>
<p dir="auto">With the amazing ML ecosystem we have today, it&#39;s definatelly possible to build a PoC of a system like that in a couple of evenings.</p>
<p dir="auto">The off-the-shelf tools are quite roboust, and mostly extremely easy to integrate. What&#39;s more, the abundance of pre-trained models meant that I could build the whole app without running a single gradient update, or hand-labeling a single example.</p>
<p dir="auto">As for the timelines -- it definatelly took me more time than I anticipated, but actually most of the time was spent on non-ML issues (like figuring out how to add Unicode characters to a video frame).</p>
<p dir="auto">Here&#39;s a 60s clip of an interview conducted in Polish, translated to English. You can see that we a very clean setup like this, the results actually look quite OK!</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path fill-rule="evenodd" d="M16 3.75a.75.75 0 00-1.136-.643L11 5.425V4.75A1.75 1.75 0 009.25 3h-7.5A1.75 1.75 0 000 4.75v6.5C0 12.216.784 13 1.75 13h7.5A1.75 1.75 0 0011 11.25v-.675l3.864 2.318A.75.75 0 0016 12.25v-8.5zm-5 5.075l3.5 2.1v-5.85l-3.5 2.1v1.65zM9.5 6.75v-2a.25.25 0 00-.25-.25h-7.5a.25.25 0 00-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-4.5z"></path>
</svg>
    <span aria-label="Video description political-interview-RMF-translated.mp4">political-interview-RMF-translated.mp4</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/10772830/208771745-37e64474-438b-418d-a99b-58c11657d5f2.mp4" data-canonical-src="https://user-images.githubusercontent.com/10772830/208771745-37e64474-438b-418d-a99b-58c11657d5f2.mp4" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">And here&#39;s a part of an interview with Keanu Reeves (who plays a major character in Cyberpunk 2077) talking to Steven Colbert, translated to Polish.</p>
<p dir="auto">Note that in this case the speaker diarization is not perfect, and the speaker IDs get mixed up for a moment mid-video:</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path fill-rule="evenodd" d="M16 3.75a.75.75 0 00-1.136-.643L11 5.425V4.75A1.75 1.75 0 009.25 3h-7.5A1.75 1.75 0 000 4.75v6.5C0 12.216.784 13 1.75 13h7.5A1.75 1.75 0 0011 11.25v-.675l3.864 2.318A.75.75 0 0016 12.25v-8.5zm-5 5.075l3.5 2.1v-5.85l-3.5 2.1v1.65zM9.5 6.75v-2a.25.25 0 00-.25-.25h-7.5a.25.25 0 00-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-4.5z"></path>
</svg>
    <span aria-label="Video description keanu-reeves-interview-translated.mp4">keanu-reeves-interview-translated.mp4</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/10772830/208771178-632b180a-231e-4a77-b578-f18cd23c3697.mp4" data-canonical-src="https://user-images.githubusercontent.com/10772830/208771178-632b180a-231e-4a77-b578-f18cd23c3697.mp4" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto">I glued together a couple of tools to make this thing fly:</p>
<ul dir="auto">
<li><a href="https://github.com/kkroening/ffmpeg-python">ffmpeg-python</a> for processing the video files (e.g. extracting audio, streaming raw frames)</li>
<li><a href="https://github.com/openai/whisper">Whisper</a> for speech recognition</li>
<li><a href="https://github.com/NVIDIA/NeMo">NVIDIA NeMo</a> for speaker diarization (note: I also tested <a href="https://github.com/pyannote">PyAnnote</a>, but the results were not satisfactory)</li>
<li><a href="https://github.com/DeepLcom/deepl-python">DeepL</a> for translation</li>
<li><a href="https://github.com/serengil/retinaface">RetinaFace</a> for face detection</li>
<li><a href="https://github.com/serengil/deepface">DeepFace</a> for face embedding</li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html" rel="nofollow">scikit-learn</a> for detecting unique faces (via clustering)</li>
<li><a href="https://github.com/gradio-app/gradio">Gradio</a> for a nice demo frontend</li>
<li><a href="https://modal.com/" rel="nofollow">Modal</a> for serverless deployment</li>
</ul>
<p dir="auto">There&#39;s also <a href="https://github.com/python-pillow/Pillow">PIL</a> &amp; <a href="https://github.com/opencv/opencv-python">OpenCV</a> used to annotate the video frames, and <a href="https://github.com/yt-dlp/yt-dlp">yt-dlp</a> to download samples from YT.</p>
<p dir="auto">Here&#39;s a sketch of how these things work together to produce the final output:</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/elanmart/cbp-translate/blob/main/assets/resources/pipeline-diagram.png"><img src="https://github.com/elanmart/cbp-translate/raw/main/assets/resources/pipeline-diagram.png" alt="data-flow-diagram" width="75%"/></a>
</p>
<h2 dir="auto"><a id="user-content-21-handling-the-speech" aria-hidden="true" href="#21-handling-the-speech"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.1 Handling the speech</h2>
<p dir="auto">Extracting audio from a <code>webm</code> / <code>mp4</code> is trivial with <code>ffmpeg</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="def extract_audio(path: str, path_out: Optional[str] = None):
    &#34;&#34;&#34;Extract audio from a video file using ffmpeg&#34;&#34;&#34;

    audio = ffmpeg.input(path).audio
    output = ffmpeg.output(audio, path_out)
    output = ffmpeg.overwrite_output(output)
    ffmpeg.run(output, quiet=True)

    return path_out"><pre><span>def</span> <span>extract_audio</span>(<span>path</span>: <span>str</span>, <span>path_out</span>: <span>Optional</span>[<span>str</span>] <span>=</span> <span>None</span>):
    <span>&#34;&#34;&#34;Extract audio from a video file using ffmpeg&#34;&#34;&#34;</span>

    <span>audio</span> <span>=</span> <span>ffmpeg</span>.<span>input</span>(<span>path</span>).<span>audio</span>
    <span>output</span> <span>=</span> <span>ffmpeg</span>.<span>output</span>(<span>audio</span>, <span>path_out</span>)
    <span>output</span> <span>=</span> <span>ffmpeg</span>.<span>overwrite_output</span>(<span>output</span>)
    <span>ffmpeg</span>.<span>run</span>(<span>output</span>, <span>quiet</span><span>=</span><span>True</span>)

    <span>return</span> <span>path_out</span></pre></div>
<p dir="auto">Once we have the sound extracted, we can process it with:</p>
<h3 dir="auto"><a id="user-content-211-whisper" aria-hidden="true" href="#211-whisper"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.1.1 Whisper</h3>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/openai/whisper/blob/main/approach.png?raw=true"><img src="https://github.com/openai/whisper/raw/main/approach.png?raw=true" alt="Whisper Approach" width="50%"/></a>
</p>
<p dir="auto">There isn&#39;t much to say about <a href="https://github.com/openai/whisper">Whisper</a>, really.</p>
<p dir="auto">It&#39;s a fantastic tool, which recognizes english speach better than me.</p>
<p dir="auto">It handles mutliple languages, and works okay even with overlapping speech.</p>
<p dir="auto">I&#39;ve decided to feed the whole audio stream to <code>whisper</code> as a single input, but if you wanted to improve this part of the code, you could experiment with partitioning the audio for each speaker, but my bet is that this will not give any better results.</p>
<h3 dir="auto"><a id="user-content-212-deepl" aria-hidden="true" href="#212-deepl"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.1.2 DeepL</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/74b37e5da677aefcd0e40afa7b73e1fb373bc897448f58ad501d25edc60b2892/68747470733a2f2f7374617469632e646565706c2e636f6d2f696d672f6c6f676f2f446565704c5f4c6f676f5f6461726b426c75655f76322e737667"><img src="https://camo.githubusercontent.com/74b37e5da677aefcd0e40afa7b73e1fb373bc897448f58ad501d25edc60b2892/68747470733a2f2f7374617469632e646565706c2e636f6d2f696d672f6c6f676f2f446565704c5f4c6f676f5f6461726b426c75655f76322e737667" alt="DeepL Logo" width="5%" data-canonical-src="https://static.deepl.com/img/logo/DeepL_Logo_darkBlue_v2.svg"/></a></p>
<p dir="auto">I could use a pre-trained Neural Machine Translation model here (or use <code>Whisper</code>, since it also does translation), but I wanted to get the highest quality possible.</p>
<p dir="auto">In my experience, <a href="https://www.deepl.com/" rel="nofollow">DeepL</a> works better than Google Translate, and their API gives you 500k characters / month for free.</p>
<p dir="auto">They also provide a convenient python interface.</p>
<p dir="auto">To improve this part of the code one could try to translate the text from each speaker separately, maybe then the translation would be even more coherent? But this would strongly depend on our ability to accurately assign the phrases to speakers.</p>
<h3 dir="auto"><a id="user-content-213-speaker-diarization----nemo-and-pyannote" aria-hidden="true" href="#213-speaker-diarization----nemo-and-pyannote"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.1.3 Speaker Diarization -- NeMo and PyAnnote</h3>
<p dir="auto">Speaker diarization is a process assigning speaker IDs to each time point in the audio signal.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/elanmart/cbp-translate/blob/main/assets/resources/diarization.png"><img src="https://github.com/elanmart/cbp-translate/raw/main/assets/resources/diarization.png" alt="Speaker Diarization example"/></a>
</p><h4 dir="auto"><a id="user-content-2131-pyannote" aria-hidden="true" href="#2131-pyannote"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.1.3.1 PyAnnote</h4>
<p dir="auto">I initially used <a href="https://github.com/pyannote">PyAnnote</a> for this purpose, since it&#39;s available on <code>HuggingFace</code> and extremely straightforward to integrate into your codebase:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipeline = Pipeline.from_pretrained(
    &#34;pyannote/speaker-diarization@2.1.1&#34;,
    use_auth_token=auth_token,
    cache_dir=cache_dir,
)

dia = pipeline(path_audio)"><pre><span>pipeline</span> <span>=</span> <span>Pipeline</span>.<span>from_pretrained</span>(
    <span>&#34;pyannote/speaker-diarization@2.1.1&#34;</span>,
    <span>use_auth_token</span><span>=</span><span>auth_token</span>,
    <span>cache_dir</span><span>=</span><span>cache_dir</span>,
)

<span>dia</span> <span>=</span> <span>pipeline</span>(<span>path_audio</span>)</pre></div>
<p dir="auto">Unfortunately the quality was not really satisfactory, and errors in this part of the pipeline were hurting all of the downstream steps.</p>
<h4 dir="auto"><a id="user-content-2132-nemo" aria-hidden="true" href="#2132-nemo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.1.3.2 NeMo</h4>
<p dir="auto">I then turned to <a href="https://github.com/NVIDIA/NeMo">NeMo</a>, from good folks at <code>NVIDIA</code>.</p>
<p dir="auto">In their words: &#34;NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP).&#34;</p>
<p dir="auto">I found it to be quite reliable, especially for english. It still struggles with short segments of overlapping speech, but it&#39;s definatelly good enough for the demo.</p>
<p dir="auto">The biggest downside is that <code>NeMo</code> is a research toolkit. Therefore simple tasks like &#34;give me unique IDs for this audio file&#34; result in a code that is much more messy than the <code>PyAnnote</code> version.</p>
<p dir="auto">Note that I mainly tested it on rather high-quality, interview-type audio. I do not know how this would translate to other scenarios or very different languages (e.g. Japanese).</p>
<h3 dir="auto"><a id="user-content-214-matching-speaker-ids-to-spoken-phrases" aria-hidden="true" href="#214-matching-speaker-ids-to-spoken-phrases"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.1.4 Matching speaker IDs to spoken phrases</h3>
<p dir="auto">I used a simple heuristic here, where for every section of speech (output from <code>NeMo</code>) we find the phrase detected by <code>Whisper</code> with the largest overlap.</p>
<p dir="auto">This part of the code could definatelly be improved with a more sofisticated approach. It would also be good to look more into the timestamps returned by the two systems, since for some reason I had an impression that an offset</p>
<h2 dir="auto"><a id="user-content-22-handling-video-streams" aria-hidden="true" href="#22-handling-video-streams"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.2 Handling video streams</h2>
<p dir="auto">This is pretty straightforward with <code>cv2</code> and <code>ffmpeg</code>. The main tip is that for video processing, generators are the way to go -- you probably don&#39;t want to load 1 minute video into a numpy array (<code>1920 * 1080 * 3 * 24 * 60</code> entries will take <code>~35GB</code> of RAM).</p>
<h3 dir="auto"><a id="user-content-221-detecting-faces-in-video" aria-hidden="true" href="#221-detecting-faces-in-video"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.2.1 Detecting faces in video</h3>
<p dir="auto">Detecting faces is luckily super straight-forward with modern tools like <code>RetinaFace</code> or <code>MTCNN</code>.</p>
<p dir="auto">In this first step we run a pre-trained model to detect all faces visible in each frame.</p>
<p dir="auto">We then crop, align, and re-size them as required by the downstream embedding model, as in this example from the <a href="https://github.com/serengil/retinaface">original repo</a></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/serengil/retinaface/master/tests/outputs/alignment-procedure.png"><img src="https://raw.githubusercontent.com/serengil/retinaface/master/tests/outputs/alignment-procedure.png" alt="Speaker Diarization example" width="75%"/></a>
</p><p dir="auto">This step is quite roboust and reliable, the only downside is that it relies on <code>Tensorflow</code>, and the code can only handle single frame at a time.</p>
<p dir="auto">It&#39;s quite time-consuming to run this detection for every frame in a video, so this part of the code could definatelly use some optimizations.</p>
<p dir="auto">With a modern GPU it takes several minutes to process ~60s of video.</p>
<p dir="auto">Luckily, with <code>Modal</code> we can use massive parallelization, so the runtime is shorter, even if processing happens on single-CPU machines.</p>
<h3 dir="auto"><a id="user-content-222-embedding-faces-and-assigning-them-unique-ids" aria-hidden="true" href="#222-embedding-faces-and-assigning-them-unique-ids"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.2.2 Embedding faces and assigning them unique IDs</h3>
<p dir="auto">Once we&#39;ve located faces in each frame, we can use a pre-trained model to extract embeddings for each of them.</p>
<p dir="auto">For this I&#39;ve grabbed the <code>FaceNet512</code> model from <a href="https://github.com/serengil/deepface">DeepFace</a> library.</p>
<p dir="auto">Once embeddings are extracted, we still need to assign them unique IDs.
To do this, I went with a simple hierarchical clustering algorithm (or specifically, <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" rel="nofollow">Agglomerative Clustering</a> from <code>scikit-learn</code>)</p>
<p dir="auto">Agglomerative Clustering will recursively merge clusters as long as the distance between them is below a certain threshold. That threshold is model- and metric-specific. Here I used same value which is used by <code>DeepFace</code> when performing &#34;face verification&#34;.</p>
<p dir="auto">This part of the code could be improved in many ways:</p>
<ul dir="auto">
<li>Improve the clustering algorithm by either
<ul dir="auto">
<li>Using a different algorithm (e.g. DBSCAN)</li>
<li>Using more domain knowledge (e.g. the fact that faces with similar locations in consecutive frames are likely to be the same person, no two faces in a single frame can be a single person etc.)</li>
</ul>
</li>
<li>Investigate if it would be a good idea to identify a couple of &#34;best&#34; frames where the face is in the best position, and use them as a tempalte.</li>
<li>Enforce temporal consistency -- predictions should not be made for each frame in isolation.</li>
<li>Improve the embeddings themselves, e.g. by using a combination of models, or different distance metrics?</li>
</ul>
<p dir="auto">Here&#39;s a visual representation of how the Agglomerative Clustering works -- by changing the threshold (cutoff on the Y-axis) you will end up with different number of clusters.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/eb78c64558c124b19c4b4f341d9a44c82e7096f771ef7e964ec6e1c0efb0b4ee/68747470733a2f2f7363696b69742d6c6561726e2e6f72672f737461626c652f5f696d616765732f737068785f676c725f706c6f745f6167676c6f6d657261746976655f64656e64726f6772616d5f3030312e706e67"><img src="https://camo.githubusercontent.com/eb78c64558c124b19c4b4f341d9a44c82e7096f771ef7e964ec6e1c0efb0b4ee/68747470733a2f2f7363696b69742d6c6561726e2e6f72672f737461626c652f5f696d616765732f737068785f676c725f706c6f745f6167676c6f6d657261746976655f64656e64726f6772616d5f3030312e706e67" alt="Agglomerative Clustering Dendrogram" data-canonical-src="https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_dendrogram_001.png"/></a>
</p><h3 dir="auto"><a id="user-content-223-matching-face-ids-to-speaker-ids" aria-hidden="true" href="#223-matching-face-ids-to-speaker-ids"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.2.3 Matching Face IDs to Speaker IDs</h3>
<p dir="auto">For this we employ another simple heuristic: for each face, we create a set of frames where that face was detected.</p>
<p dir="auto">We then do the same for speakers -- create a set of frames where a given speaker can be heard.</p>
<p dir="auto">Now, for each face ID we find the speaker ID for which Jaccard index between the two sets is minimized.</p>
<h3 dir="auto"><a id="user-content-224-generating-the-frames" aria-hidden="true" href="#224-generating-the-frames"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.2.4 Generating the frames</h3>
<p dir="auto">Once we have annotated every frame with a speaker ID, face ID, phrase in original language, and phrase in the translated language -- we can finally add subtitles.</p>
<p dir="auto">Even though our system does not work in real time, I wanted to give it a similar look to the Cyberpunk example -- so as a last processing step I calculate how many characters from a recognized phrase should be displayed on the frame.</p>
<p dir="auto">What remains now is to figure out how to place the subtitles such that they fit on the screen etc.</p>
<p dir="auto">This part of the code could be improved to handle more languages. To place UTF-8 characters on the screen, I need to explicitly pass a path to a font file to <code>PIL</code>. The problem is that different languages require different fonts, so the current solution won&#39;t work e.g. for Korean.</p>
<h2 dir="auto"><a id="user-content-23-deployment" aria-hidden="true" href="#23-deployment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.3 Deployment</h2>
<h3 dir="auto"><a id="user-content-231-gradio-frontend" aria-hidden="true" href="#231-gradio-frontend"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.3.1 Gradio frontend</h3>
<p dir="auto">Last thing I wanted to check is how easy it is to deploy this system on a cloud platform.</p>
<p dir="auto">Getting the frontend ready can be trivially done with <a href="https://gradio.app/docs/" rel="nofollow">Gradio</a>.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/elanmart/cbp-translate/blob/main/assets/resources/gradio-screenshot.png"><img src="https://github.com/elanmart/cbp-translate/raw/main/assets/resources/gradio-screenshot.png" alt="Gradio Screenshot" width="70%"/></a>
</p>
<h3 dir="auto"><a id="user-content-232-serverless-backend-with-modal-and-fastapi" aria-hidden="true" href="#232-serverless-backend-with-modal-and-fastapi"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2.3.2 Serverless backend with Modal and FastAPI</h3>
<p dir="auto">We could try to deploy the model with <a href="https://huggingface.co/docs/hub/spaces-sdks-gradio" rel="nofollow">Huggingface Spaces</a>, but I wanted to try something a bit more &#34;production-ready&#34;.</p>
<p dir="auto">I wen&#39;t ahead with <a href="https://modal.com/" rel="nofollow">Modal</a> -- a serverless platform built by <a href="https://erikbern.com/" rel="nofollow">Erik Bernhardsson</a> and his team. You can read more about it <a href="https://erikbern.com/2022/12/07/what-ive-been-working-on-modal.html" rel="nofollow">in his blogpost</a></p>
<p dir="auto"><code>Modal</code> is really appealing since it allows me to write the code exactly how I imagined the programming for the cloud should look like. What locally you&#39;d write as:</p>
<div dir="auto" data-snippet-clipboard-copy-content="def run_asr(audio_path: str):
    return whisper.transcribe(audio_path)


def process_single_frame(frame: np.ndarray, text: str):
    frame = add_subtitles(frame, text)
    return frame"><pre><span>def</span> <span>run_asr</span>(<span>audio_path</span>: <span>str</span>):
    <span>return</span> <span>whisper</span>.<span>transcribe</span>(<span>audio_path</span>)


<span>def</span> <span>process_single_frame</span>(<span>frame</span>: <span>np</span>.<span>ndarray</span>, <span>text</span>: <span>str</span>):
    <span>frame</span> <span>=</span> <span>add_subtitles</span>(<span>frame</span>, <span>text</span>)
    <span>return</span> <span>frame</span></pre></div>
<p dir="auto">With <code>Modal</code> becomes</p>
<div dir="auto" data-snippet-clipboard-copy-content="@stub.function(image=gpu_image, gpu=True)
def run_asr(audio_path: str):
    return whisper.transcribe(audio_path)


@stub.function(image=cpu_image)
def process_single_frame(frame: np.ndarray, text: str):
    frame = add_subtitles(frame, text)
    return frame"><pre><span>@<span>stub</span>.<span>function</span>(<span>image</span><span>=</span><span>gpu_image</span>, <span>gpu</span><span>=</span><span>True</span>)</span>
<span>def</span> <span>run_asr</span>(<span>audio_path</span>: <span>str</span>):
    <span>return</span> <span>whisper</span>.<span>transcribe</span>(<span>audio_path</span>)


<span>@<span>stub</span>.<span>function</span>(<span>image</span><span>=</span><span>cpu_image</span>)</span>
<span>def</span> <span>process_single_frame</span>(<span>frame</span>: <span>np</span>.<span>ndarray</span>, <span>text</span>: <span>str</span>):
    <span>frame</span> <span>=</span> <span>add_subtitles</span>(<span>frame</span>, <span>text</span>)
    <span>return</span> <span>frame</span></pre></div>
<p dir="auto">So with minimal boilerplate we now have a code that can run remotely <strong>within seconds</strong>. Pretty wild.</p>
<p dir="auto">There are obviously still some rough edges (<code>Modal</code> is still in beta), and I had to work around one last issue: when running a <code>FastAPI</code> app, there is a 45 second limit for each request. And since processing a video takes a bit longer, I used a not-so-nice workaround, where pressing <code>Submit</code> for the first time gives you the job id, and you can use that id to fetch the final result:</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/elanmart/cbp-translate/blob/main/assets/resources/modal-step-1.png"><img src="https://github.com/elanmart/cbp-translate/raw/main/assets/resources/modal-step-1.png" alt="Gradio Screenshot" width="70%"/></a>
</p>

<p dir="auto">This is very obviously just a demo / proof-of-concept!</p>
<p dir="auto">The main limitations are:</p>
<ul dir="auto">
<li>Processing 30s of video takes several minutes on a modern PC</li>
<li>The approach used here will not work well for clips with multiple scenes</li>
<li>Matching faces to voices relies on simple co-occurence heuristic, and will not work in certain scenarios (e.g. if the whole conversation between two people is recorded from a single angle)</li>
<li>All the steps of the pipeline rely on imperfect tools (e.g. diarization) or simplistic heuristics (e.g. finding unique faces with agglomerative clustering)</li>
<li>The pipeline was only tested on a handful of examples</li>
</ul>

<h2 dir="auto"><a id="user-content-with-modal" aria-hidden="true" href="#with-modal"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>With Modal</h2>
<p dir="auto">First of all, you&#39;ll need a <code>Modal</code> account, see <a href="https://modal.com/" rel="nofollow">https://modal.com/</a></p>
<p dir="auto">You&#39;ll then need to add your <code>HuggingFace</code> (<code>HUGGINGFACE_TOKEN</code>) and <code>DeepL</code> (<code>DEEPL_KEY</code>) authentication tokens as <code>Secret</code>s in <code>Modal</code> dashboard.</p>
<p dir="auto">Once this is set up, you should be able to simply run:</p>
<div data-snippet-clipboard-copy-content="python -m venv ./venv
source ./venv/bin/activate
python -m pip install -r requirements-modal.txt"><pre><code>python -m venv ./venv
source ./venv/bin/activate
python -m pip install -r requirements-modal.txt
</code></pre></div>
<p dir="auto">You can then run it with</p>
<div data-snippet-clipboard-copy-content="python cbp_translate/app.py"><pre><code>python cbp_translate/app.py
</code></pre></div>
<h2 dir="auto"><a id="user-content-locally" aria-hidden="true" href="#locally"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Locally</h2>
<p dir="auto">First, export the necessary env variables:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export DEEPL_KEY=&#34;...&#34;
export HUGGINGFACE_TOKEN=&#34;...&#34;
export MODAL_RUN_LOCALLY=1"><pre><span>export</span> DEEPL_KEY=<span><span>&#34;</span>...<span>&#34;</span></span>
<span>export</span> HUGGINGFACE_TOKEN=<span><span>&#34;</span>...<span>&#34;</span></span>
<span>export</span> MODAL_RUN_LOCALLY=1</pre></div>
<p dir="auto">Then, it&#39;s best if you look at the steps defined in <code>cbp_translate/modal_/remote.py</code></p>
<p dir="auto">Roughly, it goes something like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Apt
sudo apt install ffmpeg libsndfile1 git build-essential

# We skip conda steps, assuming you have cuda and cudnn installed 
echo &#34;Skipping CUDA installation&#34;

# pip
python -m venv ./venv
source ./venv/bin/activate
python -m pip install --upgrade pip setuptools
python -m pip install -r requirements-local.txt

# Install the package for development
python setup.py develop"><pre><span><span>#</span> Apt</span>
sudo apt install ffmpeg libsndfile1 git build-essential

<span><span>#</span> We skip conda steps, assuming you have cuda and cudnn installed </span>
<span>echo</span> <span><span>&#34;</span>Skipping CUDA installation<span>&#34;</span></span>

<span><span>#</span> pip</span>
python -m venv ./venv
<span>source</span> ./venv/bin/activate
python -m pip install --upgrade pip setuptools
python -m pip install -r requirements-local.txt

<span><span>#</span> Install the package for development</span>
python setup.py develop</pre></div>
<p dir="auto">Run the CLI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python 

python cbp_translate/cli.py \
    --path-in ./assets/videos/keanu-reeves-interview.mp4 \
    --path-out ./translated.mp4 \
    --language PL"><pre>python 

python cbp_translate/cli.py \
    --path-in ./assets/videos/keanu-reeves-interview.mp4 \
    --path-out ./translated.mp4 \
    --language PL</pre></div>

<p dir="auto">There are several large files included in this repo, which are stored using <code>git-lfs</code>.</p>
<p dir="auto">To clone the repo without downloading the large files, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="GIT_LFS_SKIP_SMUDGE=1 git clone ..."><pre>GIT_LFS_SKIP_SMUDGE=1 git clone ...</pre></div>
</article>
          </div></div>
  </body>
</html>
