<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://venki.dev/notes/replicate-vs-fly">Original</a>
    <h1>Replicate vs. Fly GPU cold-start latency</h1>
    
    <div id="readability-page-1" class="page"><div><main><p><a target="_blank" rel="noopener noreferrer" href="https://replicate.com/">Replicate</a> has been my default serverless GPU choice in the past, and I’ve been trying to use it to set up some embedding models, like SPLADE and a Q&amp;A-optimized bi-encoder. On the other hand, I’m a huge fan of Fly for hosting, and they’ve <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/flydotio/status/1757104640947016142">just announced GPUs</a>.</p><p>How does cold &amp; warm latency compare between these providers?</p><h3 data-id="ef30184146324cd6a2ff749cee34de6b"><span><a href="#ef30184146324cd6a2ff749cee34de6b" title="Problem Context"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span>Problem Context</span></span></h3><p>I’m building a semantic search engine, and I’m most interested in minimizing query-time latency, and trying out multiple different models.</p><p>One constraint: I don’t want to pay the $2k/mo to keep a GPU permanently warm, so I’m generally looking for serverless providers with great cold start latency.</p><h3 data-id="10be8f743009449fb28364d559e95480"><span><a href="#10be8f743009449fb28364d559e95480" title="Test #1: Cold-start latency profile on Replicate"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span>Test #1: Cold-start latency profile on Replicate</span></span></h3><p>I’m testing a MSMARCO trained bi-encoder from <code>sentence-transformers</code> . I’m not doing anything particularly intelligent re: model loading - the 100 MB model is downloaded from Huggingface the first time the container is run.</p><p>I instrumented the Cog container to report a) time since the <code>setup</code>  function was first called, representing the end of machine startup, and b) time taken for model inference. Then, I timed it from a Python script calling the Replicate API.</p><figure><div><p><img src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fd3658b9a-e072-4310-bfa2-ac54e0c811ae%2Ffbe0efd3-4b3a-4bd1-99f3-232c9bca7c14%2FUntitled.png?table=block&amp;id=b3110136-d0f0-4d9c-97fd-85607e9ab180&amp;cache=v2" alt="Cold-start latency on Replicate for a 14 GB Cog Docker image, with 100 MB of runtime download." loading="lazy" decoding="async"/></p><figcaption>Cold-start latency on Replicate for a 14 GB Cog Docker image, with 100 MB of runtime download.</figcaption></div></figure><p>Machine startup takes around 60 seconds, downloading the model takes about 10, and embedding a single query string takes just around 5 ms.</p><p>70s is too long to wait at a search box for.</p><h3 data-id="37584dd440eb46d895fb0f160e7a5fca"><span><a href="#37584dd440eb46d895fb0f160e7a5fca" title="Test #2: Cold &amp; warm latency tests on Replicate vs Fly"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span>Test #2: Cold &amp; warm latency tests on Replicate vs Fly</span></span></h3><p>I uploaded the same Cog container to Fly, and updated my instrumentation script to call that endpoint too. With Fly, I haven’t figured out configuring the automatic scale-to-zero timer yet, so I manually used <code>fly machines kill</code>  to profile cold-start times.</p><p>To ensure that Fly didn’t have an additional layer of short-lived caching, I spaced out my cold-start tests by up to 12 hours.</p><figure><div><p><img src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fd3658b9a-e072-4310-bfa2-ac54e0c811ae%2F2a7ecdd4-639f-463e-994f-480aa282cec6%2FUntitled.png?table=block&amp;id=f9f41cae-7075-4e98-8f60-3bdfac5ff41f&amp;cache=v2" alt="Warm latency figures, and machine start time figures for Fly and Replicate, for the same 14 GB Cog Docker image. Warm latency figures start at the HTTP request to a warm model, and end at receiving a result. Machine start time figures start at the HTTP request to a cold model, and end at the setup() function invocation" loading="lazy" decoding="async"/></p><figcaption>Warm latency figures, and machine start time figures for Fly and Replicate, for the same 14 GB Cog Docker image. Warm latency figures start at the HTTP request to a warm model, and end at receiving a result. Machine start time figures start at the HTTP request to a cold model, and end at the <code>setup()</code> function invocation</figcaption></div></figure><p>Fly far dominates Replicate’s numbers, for both warm and cold inferences. After getting my initial numbers on a Replicate A40, I chatted with a friend who works at Replicate, who mentioned that T4s are hosted differently vs A40s and I might get better results. I then tried T4s, but I got much worse cold latencies, and warm latencies were still not much improved.</p><p>To ensure there wasn’t something surprisingly wrong happening in my inference code, I switched to an effectively no-op <code>predict</code> function of <code>return json.dumps({success: &#34;True&#34;})</code> , and I generally saw my results unchanged.</p><h3 data-id="ef474b91af4646e1bed2e9a79496486d"><span><a href="#ef474b91af4646e1bed2e9a79496486d" title="What now?"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span>What now?</span></span></h3><p>Replicate seems to have incredibly poor latency, regardless of cold/warm status. 800ms is a lot to spend on networking for an already warm model. If you’re thinking of deploying a model to production, and you really want to use the Cog architecture for some reason, you might still consider deploying to Fly.</p><p>Fly continues to be excellent. The min(machine_boot) I experienced with cold boots was an astounding 2s - and I’d guess that think their cold boot time is probably the best available for GPU providers.</p><p>I’m likely to still use Replicate for their open source / social aspect. I’d like to containerize and test out a bunch of embedding models for which code but no API currently exists (mostly SPLADE, more models from <code>sentence-transformers</code> ) and Replicate has a great story for transforming that effort into a public good. I’d love to build a universal embeddings API, where it’s trivial to switch the model you’re using in a product. With Replicate’s ability to let other users pay for their own usage, it’s the only provider that could support this. </p><p>But if you ask me which provider you should deploy to prod on - it’s probably not Replicate.</p></main></div></div>
  </body>
</html>
