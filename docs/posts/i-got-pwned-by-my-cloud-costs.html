<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.troyhunt.com/how-i-got-pwned-by-my-cloud-costs/">Original</a>
    <h1>I got pwned by my cloud costs</h1>
    
    <div id="readability-page-1" class="page"><section>
<p>I have been, and still remain, a massive proponent of &#34;the cloud&#34;. I built <a href="https://haveibeenpwned.com/">Have I Been Pwned</a> (HIBP) as a cloud-first service that took advantage of modern cloud paradigms such as Azure Table Storage to massively drive down costs at crazy levels of performance I never could have achieved before. I wrote many blog posts about <a href="https://www.troyhunt.com/serverless-to-the-max-doing-big-things-for-small-dollars-with-cloudflare-workers-and-azure-functions/">doing big things for small dollars</a> and did talks all over the world about the great success I&#39;d had with these approaches. One such talk was <a href="https://www.youtube.com/watch?v=k00D2pkbkUo">How I Pwned My Cloud Costs</a> so it seems apt that today, I write about the exact opposite: how my cloud costs pwned me.</p><p>It all started with my monthly Azure bill for December which was <em>way</em> over what it would normally be. It only took a moment to find the problem:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image.png" alt="" loading="lazy" width="1555" height="202" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image.png 600w, https://www.troyhunt.com/content/images/size/w1000/2022/01/image.png 1000w, https://www.troyhunt.com/content/images/2022/01/image.png 1555w"/></figure><p>That invoice came through on the 10th of Jan but due to everyone in my household other than me getting struck down with COVID (thankfully all asymptomatic to very mild), it was another 10 days before I looked at the bill. Ouch! It&#39;s much worse than that too, but we&#39;ll get to that.</p><p>Investigation time and the first thing I look at is Azure&#39;s cost analysis which breaks down a line item like the one above into all the individual services using it. HIBP is made up of many different components including a website, relationship database, serverless &#34;Functions&#34; and storage. Right away, one service floated right to the top:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-1.png" alt="" loading="lazy" width="1062" height="252" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-1.png 600w, https://www.troyhunt.com/content/images/size/w1000/2022/01/image-1.png 1000w, https://www.troyhunt.com/content/images/2022/01/image-1.png 1062w"/></figure><p>That first line item is 98% of my bandwidth costs across <em>all</em> services. Not just all HIBP services, but everything else I run in Azure from <a href="https://hack-yourself-first.com/">Hack Yourself First</a> to <a href="https://whynohttps.com/">Why No HTTPS</a>. What we&#39;re talking about here is <a href="https://azure.microsoft.com/en-au/pricing/details/bandwidth/">egress bandwidth for data being sent out of Microsoft&#39;s Azure infrastructure</a> (priced at AU$0.014 per GB) so normally things like traffic to websites. But this is a <em>storage</em> account - why? Let&#39;s start with when the usage started skyrocketing:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-4.png" alt="" loading="lazy" width="2000" height="744" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-4.png 600w, https://www.troyhunt.com/content/images/size/w1000/2022/01/image-4.png 1000w, https://www.troyhunt.com/content/images/size/w1600/2022/01/image-4.png 1600w, https://www.troyhunt.com/content/images/size/w2400/2022/01/image-4.png 2400w"/></figure><p>December 20. Immediately, I knew what this correlated to - <a href="https://www.troyhunt.com/open-source-pwned-passwords-with-fbi-feed-and-225m-new-nca-passwords-is-now-live/">the launch of the Pwned Passwords ingestion pipeline for the FBI along with hundreds of millions of new passwords provided by the NCA</a>. <em>Something</em> changed then; was it the first production release of the open source codebase? Something else? I had to dig deeper, starting with a finer-grained look at the bandwidth usage. Here&#39;s 4 hours&#39; worth:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-5.png" alt="" loading="lazy" width="2000" height="718" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-5.png 600w, https://www.troyhunt.com/content/images/size/w1000/2022/01/image-5.png 1000w, https://www.troyhunt.com/content/images/size/w1600/2022/01/image-5.png 1600w, https://www.troyhunt.com/content/images/size/w2400/2022/01/image-5.png 2400w"/></figure><p>Consistently, each one of those spikes was 17.3GB. Not a completely linear distribution, but pretty regular spikes. By now, I was starting to get a pretty good idea of what was chewing up the bandwidth: the downloadable hashes in Pwned Passwords. But these would always cache at the Cloudflare edge node, that&#39;s why I could provide the service for free, and I&#39;d done a bunch of work with the folks there to make sure the bandwidth from the origin service was negligible. Was that <em>actually</em> the problem? Let&#39;s go deeper again, right down to the individual request level by enabling diagnostics on the storage account:</p><pre><code>{
   &#34;time&#34;:&#34;2022-01-20T06:06:24.8409590Z&#34;,
   &#34;resourceId&#34;:&#34;/subscriptions/[subscription id]/resourceGroups/default-storage-westus/providers/Microsoft.Storage/storageAccounts/pwnedpasswords/blobServices/default&#34;,
   &#34;category&#34;:&#34;StorageRead&#34;,
   &#34;operationName&#34;:&#34;GetBlob&#34;,
   &#34;operationVersion&#34;:&#34;2009-09-19&#34;,
   &#34;schemaVersion&#34;:&#34;1.0&#34;,
   &#34;statusCode&#34;:200,
   &#34;statusText&#34;:&#34;Success&#34;,
   &#34;durationMs&#34;:690285,
   &#34;callerIpAddress&#34;:&#34;172.68.132.54:13300&#34;,
   &#34;correlationId&#34;:&#34;c0f0a4c6-601e-010f-80c2-0d2a1c000000&#34;,
   &#34;identity&#34;:{
      &#34;type&#34;:&#34;Anonymous&#34;
   },
   &#34;location&#34;:&#34;West US&#34;,
   &#34;properties&#34;:{
      &#34;accountName&#34;:&#34;pwnedpasswords&#34;,
      &#34;userAgentHeader&#34;:&#34;Mozilla/5.0 (Windows NT; Windows NT 10.0; de-DE) WindowsPowerShell/5.1.14393.4583&#34;,
      &#34;etag&#34;:&#34;0x8D9C1082643C213&#34;,
      &#34;serviceType&#34;:&#34;blob&#34;,
      &#34;objectKey&#34;:&#34;/pwnedpasswords/passwords/pwned-passwords-sha1-ordered-by-count-v8.7z&#34;,
      &#34;lastModifiedTime&#34;:&#34;12/17/2021 2:51:39 AM&#34;,
      &#34;serverLatencyMs&#34;:33424,
      &#34;requestHeaderSize&#34;:426,
      &#34;responseHeaderSize&#34;:308,
      &#34;responseBodySize&#34;:18555441195,
      &#34;tlsVersion&#34;:&#34;TLS 1.2&#34;
   },
   &#34;uri&#34;:&#34;https://downloads.pwnedpasswords.com/passwords/pwned-passwords-sha1-ordered-by-count-v8.7z&#34;,
   &#34;protocol&#34;:&#34;HTTPS&#34;,
   &#34;resourceType&#34;:&#34;Microsoft.Storage/storageAccounts/blobServices&#34;
}
</code></pre><p>Well, there&#39;s the problem. These requests appeared regularly in the logs, each time burning a 17.3GB hole in my wallet. <a href="https://db-ip.com/162.158.255.113">That IP address is Cloudflare&#39;s too</a> so traffic was definitely routing through their infrastructure and therefore should have been cached. Let&#39;s see what the Cloudflare dashboard has to say about it:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-6.png" alt="" loading="lazy" width="917" height="1052" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-6.png 600w, https://www.troyhunt.com/content/images/2022/01/image-6.png 917w"/></figure><p>That&#39;s <em>a lot</em> of data served by the origin in only 24 hours, let&#39;s drill down even further:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-7.png" alt="" loading="lazy" width="911" height="697" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-7.png 600w, https://www.troyhunt.com/content/images/2022/01/image-7.png 911w"/></figure><p>And there&#39;s those same zipped hashes again. Damn. At this stage, I had no idea why this was happening, I just knew it was hitting my wallet <em>hard </em>so I dropped in a firewall rule at Cloudflare:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-9.png" alt="" loading="lazy" width="1044" height="361" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-9.png 600w, https://www.troyhunt.com/content/images/size/w1000/2022/01/image-9.png 1000w, https://www.troyhunt.com/content/images/2022/01/image-9.png 1044w"/></figure><p>And immediately, the origin bandwidth hit dived:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-18.png" alt="" loading="lazy" width="882" height="536" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-18.png 600w, https://www.troyhunt.com/content/images/2022/01/image-18.png 882w"/></figure><p>The symptom was clear - Cloudflare wasn&#39;t caching things it should have been - but the root cause was anything <em>but</em> clear. I started going back through all my settings, for example the page rule that defined caching policies on the &#34;downloads&#34; subdomain:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-10.png" alt="" loading="lazy" width="766" height="320" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-10.png 600w, https://www.troyhunt.com/content/images/2022/01/image-10.png 766w"/></figure><p>All good, nothing had changed, and it looked fine anyway. So, I looked at the properties of the file itself in Azure&#39;s blob storage:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-11.png" alt="" loading="lazy" width="506" height="593"/></figure><p>Huh, no &#34;CacheControl&#34; value. But there wasn&#39;t one on any of the previous zip files either and the Cloudflare page rule above should be overriding anything here by virtue of the edge cache TTL setting anyway. In desperation, I reached out to a friend at Cloudflare and shortly thereafter, the penny dropped:</p><blockquote>So I had a quick look and I can certainly confirm that CF isn&#39;t caching those zip files.. Now I did find a setting on your plan that set the max cacheable file size to 15GB and it looks like your zipfile is 18GB big.. would it be possible that your file just grew to be beyond 15GB around that time?</blockquote><p><em>Of course!</em> I recalled a discussion years earlier where Cloudflare had upped the cacheable size, but I hadn&#39;t thought about it since. I jumped over to the Azure Storage Explorer and immediately saw the problem and why it had only just begun:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-12.png" alt="" loading="lazy" width="981" height="246" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-12.png 600w, https://www.troyhunt.com/content/images/2022/01/image-12.png 981w"/></figure><p>And there we have it - both SHA-1 archives are over 15GB. Dammit. Now knowing precisely what the root cause was, I tweaked the Cloudflare rules:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-17.png" alt="" loading="lazy" width="1047" height="501" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-17.png 600w, https://www.troyhunt.com/content/images/size/w1000/2022/01/image-17.png 1000w, https://www.troyhunt.com/content/images/2022/01/image-17.png 1047w"/></figure><p>I removed the direct download links from the HIBP website and just left the torrents which had plenty of seeds so it was still easy to get the data. Since then, Cloudflare upped that 15GB limit and I&#39;ve restored the links for folks that aren&#39;t in a position to pull down a torrent. Crisis over.</p><p>So, what was the total damage? Uh... not good:</p><p><img src="https://www.troyhunt.com/content/images/2022/01/Troys-Mega-Azure-Bandwidth-Bill.png" alt="" loading="lazy"/></p>
<p><strong>Over and above normal usage for that period, it cost me over AU$11k.</strong> Ouch! For folks in other parts of the world, that&#39;s about US$8k, GB£6k or EU€7k. This was about AU$350 a day for a month. It really hurt, and it shouldn&#39;t have happened. I should have picked up on it earlier and had safeguards in place to ensure it didn&#39;t happen. It&#39;s on me. However, just as I told earlier stories of how cost-effective the cloud can be, this one about how badly it can bite you deserved to be told. But rather than just telling a tale of woe, let&#39;s also talk about what I&#39;ve now done to stop this from happening again:</p><p>Firstly, I always knew bandwidth on Azure was expensive and I should have been monitoring it better, particularly on the storage account serving the most data. If you look back at the first graph in this post before the traffic went nuts, egress bandwidth never exceeded 50GB in a day during normal usage which is AU$0.70 worth of outbound data. Let&#39;s set up an alert on the storage account for when that threshold is exceeded:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-13.png" alt="" loading="lazy" width="833" height="1108" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-13.png 600w, https://www.troyhunt.com/content/images/2022/01/image-13.png 833w" sizes="(min-width: 720px) 720px"/></figure><p>The graph at the top of that image shows a dashed black line right towards the bottom of the y-axis which is where my bandwidth <em>should</em> be (at the most), but we&#39;re still seeing the remnants of my mistake reflected to the left of the graph where bandwidth usage was nuts. After setting up the above, it was just a matter of defining an action to fire me off an email and that&#39;s it - job done. As soon as I configured the alert, it triggered, and I received an email:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-14.png" alt="" loading="lazy" width="618" height="987" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-14.png 600w, https://www.troyhunt.com/content/images/2022/01/image-14.png 618w"/></figure><p>If I&#39;d had this in place a month earlier, this whole shambles could have been avoided.</p><p>Secondly, there&#39;s <a href="https://docs.microsoft.com/en-us/azure/cost-management-billing/costs/cost-mgt-alerts-monitor-usage-spending">cost alerts</a>. I <em>really</em> should have had this in place much earlier as it helps guard against <em>any</em> resource in Azure suddenly driving up the cost. This involves an initial step of creating a budget for my subscription:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-15.png" alt="" loading="lazy" width="622" height="353" srcset="https://www.troyhunt.com/content/images/size/w600/2022/01/image-15.png 600w, https://www.troyhunt.com/content/images/2022/01/image-15.png 622w"/></figure><p>Next, it requires conditions and I decided to alert both when the forecasted cost hits the budget, or when the <em>actual</em> cost gets halfway to the budget:</p><figure><img src="https://www.troyhunt.com/content/images/2022/01/image-16.png" alt="" loading="lazy" width="597" height="135"/></figure><p>I figure that knowing when I get halfway there is a good thing, and I can always tweak this in the future. Cost is something that&#39;s easy to gradually creep up without you really noticing, for example, I knew even before this incident that I was paying way too much for log ingestion due to App Insights storing way too much data for services that are hit frequently, namely the HIBP API. I already needed to do better at monitoring this and I should have set up cost alerts - <em>and acted on them</em> - way earlier.</p><p>I guess I&#39;m looking at this a bit like the last time I lost data due to a hard disk failure. I always knew there was a risk but until it actually happened, I didn&#39;t take the necessary steps to protect against that risk doing actual damage. But hey, it could have been so much worse; that number could have been 10x higher and I wouldn&#39;t have known any earlier.</p><p>Lastly, I still have <a href="https://haveibeenpwned.com/Donate">the donations page up on HIBP</a> so if you use the service and find it useful, your support is always appreciated. I, uh, have a bill I need to pay 😭</p>
<section>
<a href="https://www.troyhunt.com/tag/azure/">Azure</a>
<a href="https://www.troyhunt.com/tag/cloudflare/">Cloudflare</a>
<a href="https://www.troyhunt.com/tag/have-i-been-pwned-3f/">Have I Been Pwned</a>
<a href="https://www.troyhunt.com/tag/cloud/">Cloud</a>
</section>
</section></div>
  </body>
</html>
