<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.gilesthomas.com/2025/09/maths-for-llms">Original</a>
    <h1>The maths you need to start understanding LLMs</h1>
    
    <div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest(&#39;.dropdown&#39;)) {
                        let targetId = event.target.closest(&#39;.dropdown&#39;).dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? &#39;&#39; : targetId;
                        event.stopPropagation();
                    }">

                

                <div>
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

                
                
                
            </div>

            

    

    

    <blockquote>
  <p>This article is the second of three &#34;state of play&#34; posts that explain how Large Language
  Models work, aimed at readers with the level of understanding I had in mid-2022: techies
  with no deep AI knowledge. It grows out
  of <a href="https://www.gilesthomas.com/2025/08/llm-from-scratch-19-wrapping-up-chapter-4">part 19</a> in my series
  working through <a href="https://sebastianraschka.com/">Sebastian Raschka</a>&#39;s book
  &#34;<a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Build a Large Language Model (from Scratch)</a>&#34;.
  You can <a href="https://www.gilesthomas.com/2025/08/what-ai-chatbots-are-doing-under-the-hood">read the first post in the series here</a>.</p>
</blockquote>

<p>Actually coming up with ideas like GPT-based LLMs and doing serious AI research requires
serious maths.  But the good news is that if you just want to understand how they
work, while it does require some maths, if you studied it at high-school at any time since the 1960s, you did all of the groundwork
then: vectors, matrices, and so on.</p>

<p>One thing to note -- what I&#39;m covering here is what you need to know to understand <em>inference</em> -- that
is, using an existing AI, rather than the <em>training</em> process used to create them.  That&#39;s also not
much beyond high-school maths, but I&#39;ll be writing about that later on.</p>

<p>So, with that caveat, let&#39;s dig in!</p>


    
        <h3 id="vectors-and-high-dimensional-spaces">Vectors and high-dimensional spaces</h3>

<p>In <a href="https://www.gilesthomas.com/2025/08/what-ai-chatbots-are-doing-under-the-hood">the last post</a> I used the word &#34;vector&#34; in the way it&#39;s normally used by software
engineers -- pretty much as a synonym of &#34;an array of numbers&#34;.  But a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> is more
than that; it&#39;s a distance and direction in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>-dimensional space, or (equivalently)
it can be taken as a point -- you start at the origin, and then follow the vector from
there to the point in question.</p>

<p>In 2-d space, the vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>2</mn><mo>,</mo><mo>−</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math>
means &#34;two units to the right, and three down&#34;, or the point that is located if you move
that way from the origin.  In 3-d, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>5</mn><mo>,</mo><mn>1</mn><mo>,</mo><mo>−</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math> means
&#34;five right, one up, and seven away from the viewer&#34; (or in some schools of thought, seven toward the viewer),
or the point there.
With more dimensions, it becomes pretty much impossible to visualise, but conceptually it&#39;s the same.</p>

<p>We use vectors to mean things in LLMs.  For example, the vectors of logits that come
out of the LLM (see the last post) represent the likelihood of different next tokens
for an input sequence.  And when we do that, it&#39;s often useful to think of that in terms of
defining a high-dimensional space that the meaning is represented in.</p>

<h3 id="vocab-space">Vocab space</h3>

<p>The logits that come out of the LLM for each token are a set of numbers, one per possible
token, where the value in each &#34;slot&#34; is the LLM&#39;s prediction of how likely the associated token is to be the
next one.</p>

<p>The GPT-2 LLM that the book is covering uses a tokeniser with 50,257 tokens -- its
vocabulary size is 50,257 -- so
each logits vector is 50,257 items long.  Token 464 is &#34;The&#34;, so the number at position 464
in a logits vector is how likely the next token is to be &#34;The&#34;, relative to the others.</p>

<p>We can see each logits vector as being a vector in a 50,257-dimensional space ; every
point in that space is a different combination of possibilities for the next token
to choose from our tokeniser&#39;s vocabulary to continue the sequence.
I&#39;ll call this a <em>vocab space</em>.</p>

<p>That&#39;s a kind of &#34;messy&#34; vocab space, though -- let&#39;s consider two logits vectors, both points in that space, for an imaginary
LLM that has a vocabulary of just three tokens.  The first is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math>, and the second <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mo>−</mo><mn>9</mn><mo>,</mo><mo>−</mo><mn>8</mn><mo>,</mo><mo>−</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math>.  Those both mean that the first token ID
(with the smallest number) is least likely, the second is more likely than that, and the
last, with the largest number, is most likely.</p>

<p>Having two points in the space that mean the same thing seems redundant.  To tidy things up, we can run a vector
in this messy vocab space through the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> --
that will give us a list of probabilities.  I&#39;m personally treating softmax as a kind of magic, but
the important thing about it from this perspective is that it takes these messy &#34;likelihood&#34;
vectors and returns a set of numbers, all between zero and one, that represent
probabilities -- which means that the numbers in the result set sum up to one.  Importantly, all different
vectors that represent the same set of probabilities when expressed as logits will
map to the same vector in the post-softmax space.  For example, both <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mo>−</mo><mn>9</mn><mo>,</mo><mo>−</mo><mn>8</mn><mo>,</mo><mo>−</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math>
map to the same probability distribution, about <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>0.09</mn><mo>,</mo><mn>0.24</mn><mo>,</mo><mn>0.66</mn><mo stretchy="false">)</mo></mrow></math> .</p>

<blockquote>
  <p>Note: the two specific &#34;messy&#34; vectors I used were chosen because they work out to the same
  probabilities.  There are other vectors that express the same &#34;ranking&#34;, with the
  first being least likely, the second more, and the third most likely, that have different
  probability distributions.  For example, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math> has the same ranking, but
  it&#39;s hopefully obvious that we&#39;re saying that the third token is much more likely compared
  to the others than it was in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math> -- and that would be reflected in the softmax, which
  would be something like <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>0.02</mn><mo>,</mo><mn>0.05</mn><mo>,</mo><mn>0.94</mn><mo stretchy="false">)</mo></mrow></math>.</p>
</blockquote>

<p>So, we have two kinds of vocab space.  A vector in either of them represents likelihoods
for a token; there&#39;s a &#34;messy&#34; unnormalised space, where the same probability distribution can
be expressed in different ways, and a neat, tidy normalised one, where we just use real
probability distributions.</p>

<p>One extra thing before we move on; an obvious minimal case in the normalised vocab space
is a vector where all of the numbers are zero apart from one of them, which is set to
one -- that is, it&#39;s saying that the probability of one particular token is 100% and
it&#39;s definitely not any of the others.  This is an example of a one-hot vector (not
super-inventive naming) and will become important in the next post.</p>

<p>So: that&#39;s one use of a high-dimensional space; let&#39;s look at another one.</p>

<h3 id="embeddings">Embeddings</h3>

<p>An embedding space is a high-dimensional space where vectors represent meanings.  If you look at them
as points rather than directions/distances, similar concepts are
clustered together in the space.</p>

<p>Now, &#34;meaning&#34; is of course very dependent on
what you&#39;re using the meaning for.
For example, you can imagine an embedding space
where the points representing &#34;domestic cat&#34;, &#34;lion&#34; and &#34;tiger&#34; were all quite close
together in one cluster, and &#34;dog&#34;, &#34;wolf&#34; and &#34;coyote&#34; made another cluster some distance
away (both clusters being within an area that meant something like &#34;animal&#34;).  That would be a useful representation
space for a zoologist, grouping felines and canines together.</p>

<p>But for more day-to-day
use, a different space that grouped domestic animals like &#34;cat&#34; and &#34;dog&#34; closely, in
a separate cluster from wild-and-possibly-dangerous animals might be more useful.</p>

<p>So there are vast numbers of possible embedding spaces, representing different kinds
of meanings for different purposes.  You can go all the way from rich spaces representing complex concepts
to &#34;dumb&#34; spaces where you just want to cluster together concepts by the parts of speech
that they represent -- verbs, nouns, adjectives, and so on.</p>

<p>The one counterintuitive thing about embedding spaces, at least for me, is that quite
often, we don&#39;t care much about the lengths of the vectors we use.  We
might treat <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>8</mn><mo>,</mo><mn>16</mn><mo stretchy="false">)</mo></mrow></math> as being essentially the same embedding vector in a 2-d space because they
point in exactly the same direction. </p>

<p>Let&#39;s move on to what we can <em>do</em> with these high-dimensional spaces</p>

<h3 id="projections-by-matrix-multiplication">Projections by matrix multiplication</h3>

<p>A quick refresher: matrices are just vectors stacked together; if you write the vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>2</mn><mo>,</mo><mo>−</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math> like this:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>(</mo><mtable><mtr><mtd><mn>2</mn></mtd></mtr><mtr><mtd><mo>−</mo><mn>3</mn></mtd></mtr></mtable><mo>)</mo></mrow></math></p><p>...then you can stack it &#34;sideways&#34; with another vector, say <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>5</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math>, to make a matrix like this:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><mn>2</mn></mtd><mtd><mn>5</mn></mtd></mtr><mtr><mtd><mo>−</mo><mn>3</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></p><p>Or, if you write it horizontally like this:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>(</mo><mtable><mtr><mtd><mn>2</mn></mtd><mtd><mo>−</mo><mn>3</mn></mtd></mtr></mtable><mo>)</mo></mrow></math></p><p>...then it can be stacked vertically with the same other vector like this:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><mn>2</mn></mtd><mtd><mo>−</mo><mn>3</mn></mtd></mtr><mtr><mtd><mn>5</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></p><p>The size of a matrix is written in the form <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>r</mi><mi>×</mi><mi>c</mi></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>r</mi></mrow></math> is the number
of rows, and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> is the number of columns.  So both of the above are <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mn>2</mn></mrow></math> matrices;
here&#39;s a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mn>3</mn></mrow></math> one:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><mn>2</mn></mtd><mtd><mo>−</mo><mn>3</mn></mtd><mtd><mn>7</mn></mtd></mtr><mtr><mtd><mn>5</mn></mtd><mtd><mn>1</mn></mtd><mtd><mo>−</mo><mn>8</mn></mtd></mtr></mtable><mo>]</mo></mrow></math></p><p>Matrices can be multiplied together; hopefully you remember that from your schooldays,
but I wrote <a href="https://www.gilesthomas.com/2025/02/basic-neural-network-matrix-maths-part-1">a refresher</a>
back in February if you&#39;d like to remind yourself.  It also covers some useful neural net stuff :-)</p>

<p>You hopefully also remember that matrix multiplications can be used to do geometric transformations.
For example, this <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mn>2</mn></mrow></math> (two rows, two columns) matrix:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><mi>cos</mi><mi>θ</mi></mtd><mtd><mo>−</mo><mi>sin</mi><mi>θ</mi></mtd></mtr><mtr><mtd><mi>sin</mi><mi>θ</mi></mtd><mtd><mi>cos</mi><mi>θ</mi></mtd></mtr></mtable><mo>]</mo></mrow></math></p><p>Let&#39;s call it <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>R</mi></mrow></math>.  It can be used to rotate points in a 2-d space around the origin anticlockwise by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>θ</mi></mrow></math> degrees.
To do that, you put all of the points into a matrix, one point per column (like the first example above), giving a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mi>n</mi></mrow></math> matrix,
-- let&#39;s call it <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi></mrow></math>.  We multiply that one by the rotation matrix:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Y</mi><mo>=</mo><mi>R</mi><mi>·</mi><mi>X</mi></mrow></math></p><p>...and you have a new matrix with the rotated points.
That will have the shape <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mi>n</mi></mrow></math> as well, of course, because a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mn>2</mn></mrow></math> matrix
times a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mi>n</mi></mrow></math> one takes its number of rows from the first one and its number of
columns from the second.</p>

<blockquote>
  <p>NOTE: just to confuse things a bit: the way we&#39;re taught to do this kind of thing
  at school is the standard mathematical practice, and that&#39;s how I showed it above.
  The &#34;points&#34; that we&#39;re starting with are written as column vectors
  &#34;stacked side-by-side&#34; to make up a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mi>n</mi></mrow></math> matrix and then we multiply our
  rotation matrix by it, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>R</mi><mi>·</mi><mi>X</mi></mrow></math>.  However, in machine learning, people tend
  to &#34;stack up vertically&#34; a bunch of row vectors, eg. <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mn>2</mn></mrow></math>, so the
  multiplication is the other way around: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi><mi>·</mi><mi>R</mi></mrow></math>.
  In computing terms, we are storing points in row-major rather than column-major
  format.  <a href="https://www.gilesthomas.com/2025/02/basic-neural-network-matrix-maths-part-2">This post</a> explains why,
  and I&#39;ll switch to using that from now on.</p>
</blockquote>

<p>One way of thinking about that rotation matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>R</mi></mrow></math> is that it&#39;s a bit like a function, taking
a set of points in a matrix and returning another set of points that are the original
ones rotated.</p>

<p>An alternative way is to think of it projecting between two different 2-d spaces,
the second space being rotated around the origin by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>θ</mi></mrow></math> degree from the first.  That&#39;s a relatively
philosophical point in this case -- both models work well enough.</p>

<p>But when working with 3-d graphics, people use larger matrices -- simplifying a bit,
you might use a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>×</mi><mn>2</mn></mrow></math> matrix to take a collection of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> points in 3-d space, expressed
as a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><mn>3</mn></mrow></math> matrix (remember that we&#39;re using row-major matrices now).  We would multiply them as <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi><mi>·</mi><mi>R</mi></mrow></math>, and wind up with those original points
projected into 2 dimensions so that they can be displayed on a screen. </p>

<p>And that leads us to a more general statement: matrices can project between different
multidimensional spaces.  More specifically, when using row-major values, a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>d</mi><mn>1</mn></msub><mi>×</mi><msub><mi>d</mi><mn>2</mn></msub></mrow></math> matrix
projects from a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>d</mi><mn>1</mn></msub></mrow></math>-dimensional space to a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>d</mi><mn>2</mn></msub></mrow></math> dimensional space.  The numbers
in the matrix determine what kind of projection it is.</p>

<p>So, a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>×</mi><mn>2</mn></mrow></math> matrix projects points between different 2-d spaces,
likewise a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>×</mi><mn>3</mn></mrow></math> one will project points between 3-d spaces,
but a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>×</mi><mn>2</mn></mrow></math> matrix can project from a 3-d space to a 2-d one.</p>

<p>And we can make it even more extreme!
A <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>50257</mn><mi>×</mi><mn>768</mn></mrow></math> matrix can be seen as a projection from a 50,257-dimensional space to a 768-dimensional
one, and a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>768</mn><mi>×</mi><mn>50257</mn></mrow></math> one would project from a 768-dimensional space to a
50,257-dimensional space. (You&#39;ll see why I chose those specific numbers in the next post,
though you&#39;ve probably spotted the relevance of the 50,257.)</p>

<p>It&#39;s important to note that the projections can be &#34;lossy&#34;, though.  If you did the two
projections above, one after the other, you&#39;d lose information when you reduced
the number of dimensions that you could never get back, no matter what matrices you used.</p>

<p>A nice mental model for that
is the 3-d to 2-d projection for computer graphics -- if you did a perspective projection
of, say, two squares -- one large and distant, one smaller and closer -- to a 2-d
plane, then they might wind up the same size.  If you then projected back to 3-d,
you just wouldn&#39;t have the information needed to work out what their respective sizes
and distances were in the original.</p>

<p>So: matrix multiplications are projections between different spaces, with potentially different numbers of dimensions.  But they&#39;re also something else.</p>

<h3 id="neural-networks">Neural networks</h3>

<p>A single layer in a neural network is calculated like this (again, see my
<a href="https://www.gilesthomas.com/2025/02/basic-neural-network-matrix-maths-part-1">post on matrices and neural networks</a>,
and perhaps the <a href="https://www.gilesthomas.com/2025/02/basic-neural-network-matrix-maths-part-2">follow-up</a>):</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Z</mi><mo>=</mo><mi>ϕ</mi><mo stretchy="false">(</mo><mi>X</mi><msup><mi>W</mi><mi>T</mi></msup><mo>+</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></math></p><p>If we ignore the activation function <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>ϕ</mi></mrow></math> and the bias term <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>B</mi></mrow></math>, we get this:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mover><mrow><mi>Z</mi></mrow><mo stretchy="false">^</mo></mover><mo>=</mo><mi>X</mi><msup><mi>W</mi><mi>T</mi></msup></mrow></math></p><p>(The &#34;hat&#34; over the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Z</mi></mrow></math> is just to express the fact that it&#39;s not the full calculation.)</p>

<p>Now, for a neural network, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi></mrow></math> is our input batch, so it&#39;s <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><msub><mi>d</mi><mtext>in</mtext></msub></mrow></math> --
one row for each item in the batch, and one column for each input value in that item.</p>

<p>Our weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>W</mi></mrow></math> is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>d</mi><mtext>out</mtext></msub><mi>×</mi><msub><mi>d</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></math> -- <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>d</mi><mtext>out</mtext></msub></mrow></math> being the number of outputs.
We transpose it (that&#39;s what the superscript &#34;T&#34; is there to say in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>W</mi><mi>T</mi></msup></mrow></math>), which means that
we swap around rows and columns, making it a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>d</mi><mtext>in</mtext></msub><mi>×</mi><msub><mi>d</mi><mtext>out</mtext></msub></mrow></math> matrix.</p>

<p>So our result <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mover><mrow><mi>Z</mi></mrow><mo stretchy="false">^</mo></mover></mrow></math> from the unbiased neural network with no activation function is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>×</mi><msub><mi>d</mi><mtext>out</mtext></msub></mrow></math>.</p>

<p>And that takes us to the final core idea I&#39;ve found useful while working through this:
a single layer of a neural network (often abbreviated to <em>linear layer</em>) is not much more than
a matrix multiplication -- so it is, effectively, a projection from a space with as
many dimensions as it has inputs to a space with the same number of dimensions as it has
outputs.  The bias just adds on a linear &#34;shift&#34; after that, and as the activation
function is optional, we can just not do it.</p>

<h3 id="wrapping-up">Wrapping up</h3>

<p>So, those are the basic mathematical concepts that I&#39;ve needed so far to understand
LLMs.  As I said at the start, there really isn&#39;t much there beyond high-school maths.
The matrices are larger than the ones we&#39;re taught, and the high-dimensional spaces
are a bit weird, but the actual mathematics is pretty simple.</p>

<p>Up next: how do we put all of that together, along with the high-level stuff I
described about LLMs in <a href="https://www.gilesthomas.com/2025/08/what-ai-chatbots-are-doing-under-the-hood">my last post</a>,
to understand how an LLM works?</p>



    

    
        
    

    



            
        </div></div>
  </body>
</html>
