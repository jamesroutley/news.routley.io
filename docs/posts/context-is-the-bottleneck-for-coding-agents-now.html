<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://runnercode.com/blog/context-is-the-bottleneck-for-coding-agents-now">Original</a>
    <h1>Context is the bottleneck for coding agents now</h1>
    
    <div id="readability-page-1" class="page"><div><p>Intelligence is rapidly improving with each model release. Just last week it was announced that OpenAI got a perfect score on the 2025 ICPC programming contest, beating every single human contestant. They achieved this using a version (presumably a very high compute version, but still) of their publicly available GPT-5 model.</p><p>And yet, coding agents are nowhere near capable of replacing software developers. Why is that?</p><p>I’m going to argue that the limiting factor is no longer raw intelligence, but rather context. Existing coding agents simply do not have enough context about the problems they’re being asked to solve. This severely limits how long they can work effectively without human guidance.</p><h2>Intelligence and context</h2><p>How autonomous are existing coding agents? Let’s think about autonomy as a spectrum and see how far along that spectrum we are.</p><div><p><strong>Level 1 - A few lines of code</strong></p><p><strong>Level 2 - One commit</strong></p><p><strong>Level 3 - One PR</strong></p><p><strong>Level 4 - Major feature or refactor</strong></p><p><strong>Level 5 - Entire codebase</strong></p></div><p>I’d say Level 2 is all we can reliably do on production codebases right now. And even that requires substantial human guidance and review. What will it take to move further along the autonomy spectrum, without sacrificing quality?</p><p>When an agent fails at a task, the cause is usually one of two things. It’s either an intelligence failure, or it’s a context failure. Either the model didn’t have the information it needed, or it didn’t have the mental horsepower to process that information properly. There are other aspects that can affect performance, such as taste, but if we’re just talking about whether the agent succeeds or fails at a task, it’s sufficient to just consider intelligence and context. Also note that I’m including general world knowledge as part of intelligence, both for simplicity and because I think it’s hard to fully separate those two out.</p><p>Programming competitions are competitions of intelligence. The entire context needed to solve a problem is provided in the problem statement itself. There’s no existing codebase to understand, no business requirements to consider, and no unwritten development processes you need to follow.</p><p>The superhuman ICPC performance we saw this week, as well as the IOI gold medal-level performances from last month, strongly suggest that the raw intelligence and general programming knowledge of frontier models is sufficient to automate most software engineering work.</p><p>Now these performances were achieved using models that are quite a bit stronger than the models used on a daily basis by developers, like Claude 4 and GPT-5. So we can’t quite say that lack of intelligence is never a cause of failure in current coding agents. They still do some pretty dumb stuff sometimes. But as models improve, more and more of the failures in agentic coding are failures of context, not failures of intelligence.</p><h2>What context does a coding agent need?</h2><p>Context isn’t just code. It’s also specs, dev practices, conversations, etc. When human developers write code, they’re drawing from a reservoir of implicit knowledge that goes far beyond what’s visible in the codebase itself. Current coding agents are operating with maybe 20% of this context, at best.</p><p>What context does an agent need to reliably operate autonomously and ship code that’s as good or better than human developers? It’s the same things a human developer needs.</p><p>There are the basics:</p><div><p><strong>It needs to be able to access all code files</strong></p><p><strong>It needs to be able to access documentation</strong></p><p><strong>It needs to be able to run code and see the output</strong></p></div><p>And then there are the more subtle forms of context:</p><div><p><strong>It needs to have a high-level understanding of how the codebase is organized and where different code lives</strong></p><p><strong>It needs to understand all of the existing architectural patterns and conventions in the codebase</strong></p><p>Current agents struggle here because many of these patterns are emergent properties of the codebase that aren’t documented in any single place. They’re distributed across thousands of commits, pull requests, and code reviews.</p><p><strong>It needs to understand why things were done the way they were</strong></p><p>This tribal knowledge lives in Slack threads, meeting notes, incident post-mortems, and developers’ heads.</p><p><strong>It needs to understand development and deployment practices</strong></p><p>Every team has unwritten rules about how code ships. Maybe you deploy to staging-east first because of a subtle dependency. Perhaps certain tests look weird because they’re working around a known race condition. The CI/CD pipeline has manual approval steps that seem redundant but prevent real disasters that happened in the past.</p><p>Current agents can read your test configs and deployment scripts, but they don’t understand the “why” behind them. They might remove a “redundant” check that’s actually preventing a production issue, or follow official docs that everyone knows are outdated.</p><p><strong>It needs to understand product and business requirements</strong></p><p>I don’t know of any coding agents that are plugged into this kind of data right now.</p></div><p>Notice how all the basic forms of context use the word “access” while the more subtle forms of context use the word “understand.” This is important. Most of this context is not written down in a single document that the agent can just read. To the extent that it’s written down at all, it’s often scattered across many different files and apps. Some of that information will be conflicting and out of date. Giving the agent this context is not as simple as just giving it an MCP connector to your Google Drive and Linear accounts. The information needs to be processed and synthesized by the agent.</p><p>What does this mean for coding agents?</p><p>First, we need to give them access to way more context. Much of this new context will require sophisticated preprocessing to make it usable, so this is not an easy problem.</p><p>Second, not everything is written down. That means experienced human developers will still need to fill in the gaps for a very long time to come.</p><p>Third, agents need to learn to identify when they’re missing context so they can ask for human guidance. Right now they seem to be trained to just plow forward with what they have.</p></div></div>
  </body>
</html>
