<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/microsoft/JARVIS">Original</a>
    <h1>Microsoft / JARVIS: a system to connect LLMs with ML community</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><strong>This project is under construction and we will have all the code ready soon.</strong></p>
<a href="https://huggingface.co/spaces/microsoft/HuggingGPT" rel="nofollow">
    <img src="https://camo.githubusercontent.com/8a5c10d0c1b94191e0fbeae3c5a355327e5377e7141aa2aa1a451da7a346c0a7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d4f70656e253230696e2532305370616365732d626c7565" alt="Open in Spaces" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue"/>
</a>
<h2 tabindex="-1" dir="auto"><a id="user-content-updates" aria-hidden="true" href="#updates"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Updates</h2>
<ul dir="auto">
<li>[2023.04.06] We added the Gradio demo and built the web API for <code>/tasks</code> and <code>/results</code> in <code>server</code> mode.
<ul dir="auto">
<li>The Gradio demo is now hosted on Hugging Face Space. (Build with <code>inference_mode=hibrid</code> and <code>local_deployment=standard</code>)</li>
<li>The Web API <code>/tasks</code> and <code>/results</code> access intermediate results for <code>Stage #1</code>: task planning and <code>Stage #1-3</code>: model selection with execution results. See <a href="#Server">here</a>.</li>
</ul>
</li>
<li>[2023.04.03] We added the CLI mode and provided parameters for configuring the scale of local endpoints.
<ul dir="auto">
<li>You can enjoy a lightweight experience with Jarvis without deploying the models locally. See <a href="#Configuration">here</a>.</li>
<li>Just run <code>python awesome_chat.py --config lite.yaml</code> to experience it.</li>
</ul>
</li>
<li>[2023.04.01] We updated a version of code for building.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-overview" aria-hidden="true" href="#overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Overview</h2>
<p dir="auto">Language serves as an interface for LLMs to connect numerous AI models for solving complicated AI tasks!</p>
<p dir="auto">See our paper: <a href="http://arxiv.org/abs/2303.17580" rel="nofollow">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace</a>, Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu and Yueting Zhuang (the first two authors contribute equally)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/JARVIS/blob/main/assets/overview.jpg"><img src="https://github.com/microsoft/JARVIS/raw/main/assets/overview.jpg"/></a></p>
<p dir="auto">We introduce a collaborative system that consists of <strong>an LLM as the controller</strong> and <strong>numerous expert models as collaborative executors</strong> (from HuggingFace Hub). The workflow of our system consists of four stages:</p>
<ul dir="auto">
<li><strong>Task Planning</strong>: Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks.</li>
<li><strong>Model Selection</strong>: To solve the planned tasks, ChatGPT selects expert models hosted on Hugging Face based on their descriptions.</li>
<li><strong>Task Execution</strong>: Invokes and executes each selected model, and return the results to ChatGPT.</li>
<li><strong>Response Generation</strong>: Finally, using ChatGPT to integrate the prediction of all models, and generate responses.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-system-requirements" aria-hidden="true" href="#system-requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>System Requirements</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-default" aria-hidden="true" href="#default"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Default</h3>
<ul dir="auto">
<li>Ubuntu 16.04 LTS</li>
<li>VRAM &gt;= 12GB</li>
<li>RAM &gt; 12GB (minimal), 16GB (standard), 42GB (full)</li>
<li>Disk &gt; 78G (with 42G for <code>damo-vilab/text-to-video-ms-1.7b</code>)</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-minimum" aria-hidden="true" href="#minimum"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Minimum</h3>
<ul dir="auto">
<li>Ubuntu 16.04 LTS</li>
<li>Nothing else</li>
</ul>
<p dir="auto">The configuration <code>lite.yaml</code> does not require any expert models to be downloaded and deployed locally. However, it means that Jarvis is restricted to models running stably on HuggingFace Inference Endpoints.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-quick-start" aria-hidden="true" href="#quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<p dir="auto">First replace <code>openai.key</code> and <code>huggingface.token</code> in <code>server/config.yaml</code> with <strong>your personal OpenAI Key</strong> and <strong>your Hugging Face Token</strong>. Then run the following commands:</p>

<h3 tabindex="-1" dir="auto"><a id="user-content-for-server" aria-hidden="true" href="#for-server"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>For Server:</h3>
<div dir="auto" data-snippet-clipboard-copy-content="# setup env
cd server
conda create -n jarvis python=3.8
conda activate jarvis
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
pip install -r requirements.txt

# download models. Make sure that `git-lfs` is installed.
cd models
bash download.sh # required when `inference_mode` is `local` or `hybrid`. 

# run server
cd ..
python models_server.py --config config.yaml # required when `inference_mode` is `local` or `hybrid`
python awesome_chat.py --config config.yaml --mode server # for text-davinci-003"><pre><span><span>#</span> setup env</span>
<span>cd</span> server
conda create -n jarvis python=3.8
conda activate jarvis
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
pip install -r requirements.txt

<span><span>#</span> download models. Make sure that `git-lfs` is installed.</span>
<span>cd</span> models
bash download.sh <span><span>#</span> required when `inference_mode` is `local` or `hybrid`. </span>

<span><span>#</span> run server</span>
<span>cd</span> ..
python models_server.py --config config.yaml <span><span>#</span> required when `inference_mode` is `local` or `hybrid`</span>
python awesome_chat.py --config config.yaml --mode server <span><span>#</span> for text-davinci-003</span></pre></div>
<p dir="auto">Now you can access Jarvis&#39; services by the Web API.</p>
<ul dir="auto">
<li><code>/hugginggpt</code> --method <code>POST</code>, access the full service.</li>
<li><code>/tasks</code> --method <code>POST</code>, access intermediate results for Stage #1.</li>
<li><code>/results</code> --method <code>POST</code>, access intermediate results for Stage #1-3.</li>
</ul>
<p dir="auto">For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# request
curl --location &#39;http://localhost:8004/tasks&#39; \
--header &#39;Content-Type: application/json&#39; \
--data &#39;{
    &#34;messages&#34;: [
        {
            &#34;role&#34;: &#34;user&#34;,
            &#34;content&#34;: &#34;based on pose of /examples/d.jpg and content of /examples/e.jpg, please show me a new image&#34;
        }
    ]
}&#39;

# response
[{&#34;args&#34;:{&#34;image&#34;:&#34;/examples/d.jpg&#34;},&#34;dep&#34;:[-1],&#34;id&#34;:0,&#34;task&#34;:&#34;openpose-control&#34;},{&#34;args&#34;:{&#34;image&#34;:&#34;/examples/e.jpg&#34;},&#34;dep&#34;:[-1],&#34;id&#34;:1,&#34;task&#34;:&#34;image-to-text&#34;},{&#34;args&#34;:{&#34;image&#34;:&#34;&lt;GENERATED&gt;-0&#34;,&#34;text&#34;:&#34;&lt;GENERATED&gt;-1&#34;},&#34;dep&#34;:[1,0],&#34;id&#34;:2,&#34;task&#34;:&#34;openpose-text-to-image&#34;}]"><pre><span><span>#</span> request</span>
curl --location <span><span>&#39;</span>http://localhost:8004/tasks<span>&#39;</span></span> \
--header <span><span>&#39;</span>Content-Type: application/json<span>&#39;</span></span> \
--data <span><span>&#39;</span>{</span>
<span>    &#34;messages&#34;: [</span>
<span>        {</span>
<span>            &#34;role&#34;: &#34;user&#34;,</span>
<span>            &#34;content&#34;: &#34;based on pose of /examples/d.jpg and content of /examples/e.jpg, please show me a new image&#34;</span>
<span>        }</span>
<span>    ]</span>
<span>}<span>&#39;</span></span>

<span><span>#</span> response</span>
[{<span><span>&#34;</span>args<span>&#34;</span></span>:{<span><span>&#34;</span>image<span>&#34;</span></span>:<span><span>&#34;</span>/examples/d.jpg<span>&#34;</span></span>},<span><span>&#34;</span>dep<span>&#34;</span></span>:[-1],<span><span>&#34;</span>id<span>&#34;</span></span>:0,<span><span>&#34;</span>task<span>&#34;</span></span>:<span><span>&#34;</span>openpose-control<span>&#34;</span></span>},{<span><span>&#34;</span>args<span>&#34;</span></span>:{<span><span>&#34;</span>image<span>&#34;</span></span>:<span><span>&#34;</span>/examples/e.jpg<span>&#34;</span></span>},<span><span>&#34;</span>dep<span>&#34;</span></span>:[-1],<span><span>&#34;</span>id<span>&#34;</span></span>:1,<span><span>&#34;</span>task<span>&#34;</span></span>:<span><span>&#34;</span>image-to-text<span>&#34;</span></span>},{<span><span>&#34;</span>args<span>&#34;</span></span>:{<span><span>&#34;</span>image<span>&#34;</span></span>:<span><span>&#34;</span>&lt;GENERATED&gt;-0<span>&#34;</span></span>,<span><span>&#34;</span>text<span>&#34;</span></span>:<span><span>&#34;</span>&lt;GENERATED&gt;-1<span>&#34;</span></span>},<span><span>&#34;</span>dep<span>&#34;</span></span>:[1,0],<span><span>&#34;</span>id<span>&#34;</span></span>:2,<span><span>&#34;</span>task<span>&#34;</span></span>:<span><span>&#34;</span>openpose-text-to-image<span>&#34;</span></span>}]</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-for-web" aria-hidden="true" href="#for-web"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>For Web:</h3>
<p dir="auto">We provide a user-friendly web page. After starting <code>awesome_chat.py</code> in a server mode, you can run the commands to communicate with Jarvis in your browser:</p>
<ul dir="auto">
<li>you need to install <code>nodejs</code> and <code>npm</code> first.</li>
<li>if you are running the web client on another machine, you need set <code>http://{LAN_ip_of_the_server}:{port}/</code> to <code>web/src/api/hugginggpt.ts@Line=9</code>.</li>
<li>if you want to use the video generation feature, you need to compile <code>ffmpeg</code> manually with H.264.</li>
<li>you can switch to ChatGPT by <code>double click</code> on the setting icon!</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="cd web
npm install
npm run dev"><pre><span>cd</span> web
npm install
npm run dev</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Optional: Install ffmpeg
# This command need be executed without errors.
LD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/ffmpeg -i input.mp4 -vcodec libx264 output.mp4"><pre><span><span>#</span> Optional: Install ffmpeg</span>
<span><span>#</span> This command need be executed without errors.</span>
LD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/ffmpeg -i input.mp4 -vcodec libx264 output.mp4</pre></div>

<h3 tabindex="-1" dir="auto"><a id="user-content-for-gradio" aria-hidden="true" href="#for-gradio"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>For Gradio</h3>
<p dir="auto">The Gradio demo is now hosted on Hugging Face Space. You can also run the following commands to start the demo locally:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python models_server.py --config config.gradio.yaml
python run_gradio_demo.py --config config.gradio.yaml

# or run the HF Space as a Docker image
docker run -it -p 7860:7860 --platform=linux/amd64 registry.hf.space/microsoft-hugginggpt:latest python app.py"><pre>python models_server.py --config config.gradio.yaml
python run_gradio_demo.py --config config.gradio.yaml

<span><span>#</span> or run the HF Space as a Docker image</span>
docker run -it -p 7860:7860 --platform=linux/amd64 registry.hf.space/microsoft-hugginggpt:latest python app.py</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-for-cli" aria-hidden="true" href="#for-cli"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>For CLI:</h3>
<p dir="auto">You can also run Jarvis more easily in CLI mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd server
python awesome_chat.py --config config.yaml --mode cli"><pre><span>cd</span> server
python awesome_chat.py --config config.yaml --mode cli</pre></div>
<p dir="auto">Examples of CLI mode:</p>
<div data-snippet-clipboard-copy-content="Welcome to Jarvis! A collaborative system that consists of an LLM as the controller and numerous expert models as collaborative executors. Jarvis can plan tasks, schedule Hugging Face models, generate friendly responses based on your requests, and help you with many things. Please enter your request (`exit` to exit).
[ User ]: Given a collection of image A: /examples/a.jpg, B: /examples/b.jpg, C: /examples/c.jpg, please tell me how many zebras in these picture?
[ Jarvis ]: Based on your request, I have used several models to process the images you provided. For the image A: /examples/a.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text &#34;a cat sitting on a window sill looking out&#34;. Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box &#39;/images/a3dc.jpg&#39;, which contains two objects with labels &#39;potted plant&#39; and &#39;cat&#39;. For the image B: /examples/b.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text &#34;two zebras standing in a field of dry grass&#34;. Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box &#39;/images/f5df.jpg&#39;, which contains three objects with labels &#39;zebra&#39;. For the image C: /examples/c.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text &#34;a zebra standing in a field of tall grass&#34;. Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box &#39;/images/69c7.jpg&#39;, which contains one object with label &#39;zebra&#39;. Finally, I used the canny-control model canny-control to generate the image &#39;/images/7e18.png&#39; based on the image /examples/f.jpg. Therefore, there are four zebras in these pictures. Is there anything else I can help you with?
[ User ]: Please answer all the named entities in the sentence: Iron Man is a superhero appearing in American comic books published by Marvel Comics. The character was co-created by writer and editor Stan Lee, developed by scripter Larry Lieber, and designed by artists Don Heck and Jack Kirby.
[ Jarvis ]: Sure, I understand your request. Based on the inference results of the models, I will answer your request step by step. Firstly, I used the token-classification model dslim/bert-base-NER to recognize the named entities in the sentence. The inference result is that the named entities are Iron Man, Stan Lee, Larry Lieber, Don Heck and Jack Kirby. Then, I used the question-answering model deepset/roberta-base-squad2 to answer your request. The inference result is that there is no output since the context cannot be empty. Therefore, I cannot make it. I hope this answer your request. Is there anything else I can help you with?
[ User ]: Please generate a canny image based on /examples/f.jpg
[ Jarvis ]: Sure. I have generated a canny image based on /examples/f.jpg. To do this, I first used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text description of the image, which is &#34;a herd of giraffes and zebras grazing in a field&#34;. Then I used the canny-control model to generate the canny image of the original image, which is stored in the path &#34;/images/1ea2.png&#34;. Finally, I used the canny-text-to-image model lllyasviel/sd-controlnet-canny to generate the canny image based on the text description and the canny image, which is stored in the path &#34;/images/ba66.png&#34;. Do you need anything else?"><pre><code>Welcome to Jarvis! A collaborative system that consists of an LLM as the controller and numerous expert models as collaborative executors. Jarvis can plan tasks, schedule Hugging Face models, generate friendly responses based on your requests, and help you with many things. Please enter your request (`exit` to exit).
[ User ]: Given a collection of image A: /examples/a.jpg, B: /examples/b.jpg, C: /examples/c.jpg, please tell me how many zebras in these picture?
[ Jarvis ]: Based on your request, I have used several models to process the images you provided. For the image A: /examples/a.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text &#34;a cat sitting on a window sill looking out&#34;. Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box &#39;/images/a3dc.jpg&#39;, which contains two objects with labels &#39;potted plant&#39; and &#39;cat&#39;. For the image B: /examples/b.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text &#34;two zebras standing in a field of dry grass&#34;. Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box &#39;/images/f5df.jpg&#39;, which contains three objects with labels &#39;zebra&#39;. For the image C: /examples/c.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text &#34;a zebra standing in a field of tall grass&#34;. Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box &#39;/images/69c7.jpg&#39;, which contains one object with label &#39;zebra&#39;. Finally, I used the canny-control model canny-control to generate the image &#39;/images/7e18.png&#39; based on the image /examples/f.jpg. Therefore, there are four zebras in these pictures. Is there anything else I can help you with?
[ User ]: Please answer all the named entities in the sentence: Iron Man is a superhero appearing in American comic books published by Marvel Comics. The character was co-created by writer and editor Stan Lee, developed by scripter Larry Lieber, and designed by artists Don Heck and Jack Kirby.
[ Jarvis ]: Sure, I understand your request. Based on the inference results of the models, I will answer your request step by step. Firstly, I used the token-classification model dslim/bert-base-NER to recognize the named entities in the sentence. The inference result is that the named entities are Iron Man, Stan Lee, Larry Lieber, Don Heck and Jack Kirby. Then, I used the question-answering model deepset/roberta-base-squad2 to answer your request. The inference result is that there is no output since the context cannot be empty. Therefore, I cannot make it. I hope this answer your request. Is there anything else I can help you with?
[ User ]: Please generate a canny image based on /examples/f.jpg
[ Jarvis ]: Sure. I have generated a canny image based on /examples/f.jpg. To do this, I first used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text description of the image, which is &#34;a herd of giraffes and zebras grazing in a field&#34;. Then I used the canny-control model to generate the canny image of the original image, which is stored in the path &#34;/images/1ea2.png&#34;. Finally, I used the canny-text-to-image model lllyasviel/sd-controlnet-canny to generate the canny image based on the text description and the canny image, which is stored in the path &#34;/images/ba66.png&#34;. Do you need anything else?
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-configuration" aria-hidden="true" href="#configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Configuration</h2>
<p dir="auto">The server-side configuration file is <code>server/config.yaml</code>, and some parameters are presented as follows:</p>
<ul dir="auto">
<li><code>model</code>: LLM, currently supports <code>text-davinci-003</code></li>
<li><code>inference_mode</code>: mode of inference endpoints
<ul dir="auto">
<li><code>local</code>: only use the local inference endpoints</li>
<li><code>huggingface</code>: only use the Hugging Face Inference Endpoints <strong>(free of local inference endpoints)</strong></li>
<li><code>hybrid</code>: both of <code>local</code> and <code>huggingface</code></li>
</ul>
</li>
<li><code>local_deployment</code>: scale of locally deployed models, works under <code>local</code> or <code>hybrid</code> inference mode:
<ul dir="auto">
<li><code>minimal</code> (RAM&gt;12GB, ControlNet only)</li>
<li><code>standard</code> (RAM&gt;16GB, ControlNet + Standard Pipelines)</li>
<li><code>full</code> (RAM&gt;42GB, All registered models)</li>
</ul>
</li>
</ul>
<p dir="auto">On a personal laptop, we recommend the configuration of <code>inference_mode: hybrid </code>and <code>local_deployment: minimal</code>. But the available models under this setting may be limited due to the instability of remote Hugging Face Inference Endpoints.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-screenshots" aria-hidden="true" href="#screenshots"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Screenshots</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/JARVIS/blob/main/assets/screenshot_q.jpg"><img src="https://github.com/microsoft/JARVIS/raw/main/assets/screenshot_q.jpg"/></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/JARVIS/blob/main/assets/screenshot_a.jpg"><img src="https://github.com/microsoft/JARVIS/raw/main/assets/screenshot_a.jpg"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-citation" aria-hidden="true" href="#citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h2>
<p dir="auto">If you find this work useful in your method, you can cite the paper as below:</p>
<div data-snippet-clipboard-copy-content="@article{shen2023hugginggpt,
    title   = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},
    author  = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
    journal = {arXiv preprint arXiv:2303.17580},
    year    = {2023}
}"><pre><code>@article{shen2023hugginggpt,
    title   = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},
    author  = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
    journal = {arXiv preprint arXiv:2303.17580},
    year    = {2023}
}
</code></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgement" aria-hidden="true" href="#acknowledgement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgement</h2>
<ul dir="auto">
<li><a href="https://platform.openai.com/" rel="nofollow">ChatGPT</a></li>
<li><a href="https://huggingface.co/" rel="nofollow">Hugging Face</a></li>
<li><a href="https://github.com/lllyasviel/ControlNet">ControlNet</a></li>
<li><a href="https://github.com/lianginx/chatgpt-vue">ChatGPT-vue</a></li>
</ul>
</article>
          </div></div>
  </body>
</html>
