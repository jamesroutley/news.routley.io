<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://webrtchacks.com/real-time-video-processing-with-webcodecs-and-streams-processing-pipelines-part-1/">Original</a>
    <h1>Real-Time Video Processing with WebCodecs and Streams</h1>
    
    <div id="readability-page-1" class="page"><div><p>WebRTC used to be about capturing some media and sending it from Point A to Point B. Machine Learning has changed this. Now it is common to use ML to analyze and manipulate media in real time for things like virtual backgrounds, augmented reality, noise suppression, intelligent cropping, and much more. To better accommodate this growing trend, the web platform has been exposing its underlying platform to give developers more access. The result is not only more control within existing APIs, but also a bunch of new APIs like Insertable Streams, WebCodecs, Streams, WebGPU, and WebNN.</p>
<p>So how do all these new APIs work together? That is exactly what W3C specialists, <a href="https://www.w3.org/People#fd">François Daoust</a> and <a href="https://www.w3.org/People#dom">Dominique Hazaël-Massieux</a> (Dom) decided to find out. In case you forgot, W3C is the World Wide Web Consortium that standardizes the Web. François and Dom are long-time standards guys with a deep history of helping to make the web what it is today.</p>
<p>This is the first of a two-part series of articles that explores the future of real-time video processing with WebCodecs and Streams. This first section provides a review of the steps and pitfalls in a multi-step video processing pipeline using existing and the newest web APIs. Part two will explore the actual processing of video frames.</p>
<p>I am thrilled about the depth and insights these guides provide on these cutting-edge approaches – enjoy!</p>
<ul>
<li><a href="#toc_1">About Processing Pipelines</a></li>
<li><a href="#post-3953-_nc7eudoxhzf6">The demo</a>
<ul>
<li><a href="#post-3953-_twzsugow9o7d">Note: these APIs are new and may not work in your browser</a></li>
<li><a href="#post-3953-_kic3ab3akhme">Timing Stats</a></li>
</ul>
</li>
<li><a href="#post-3953-_piy5rkfm7vap">The role of WebCodecs in client-side processing</a>
<ul>
<li><a href="#post-3953-_hyjatcye1dc9">What about the Canvas? Do we need WebCodecs?</a></li>
<li><a href="#post-3953-_65zh61jfzn5k">WebCodecs advantages</a></li>
<li><a href="#post-3953-_gk15qhdru321">Note on containerized media</a></li>
</ul>
</li>
<li><a href="#post-3953-_tqoh5xxa50rh">Processing streams using… Streams</a></li>
<li><a href="#post-3953-_mlsoxxpyj6qj">Pipe chains</a>
<ul>
<li><a href="#post-3953-_vlbyeb2qqeyl">Backpressure</a></li>
<li><a href="#post-3953-_bvpkdqbuta47">Creating a pipeline</a></li>
</ul>
</li>
<li><a href="#post-3953-_9uk6sp1lrne9">Pipelining is like a game of dominoes</a>
<ul>
<li><a href="#post-3953-_eylc9lqr9026">Generating a stream</a>
<ul>
<li><a href="#post-3953-_ffuh4es7crws">From scratch</a></li>
<li><a href="#post-3953-_f9gywx64ztkl">From a camera or a WebRTC track</a></li>
<li><a href="#post-3953-_usek8unlekut">From a WebTransport stream</a></li>
<li><a href="#post-3953-_1x8j6ehglv5l">What about WebTransport?</a></li>
<li><a href="#post-3953-_l2xvd2ia9xvj">What about Data Channels?</a></li>
</ul>
</li>
<li><a href="#post-3953-_po5r16gpb8k9">Transforming a stream</a></li>
<li><a href="#post-3953-_440kg5x1c7nr">Sending/Rendering a stream</a>
<ul>
<li><a href="#post-3953-_tp56pjon6p4b">To a &lt;canvas&gt; element</a></li>
<li><a href="#post-3953-_ag2qq6wf69s4">To a &lt;video&gt; element</a></li>
<li><a href="#post-3953-_ogeyr0dsz4ww">Notes and Caveats</a></li>
<li><a href="#post-3953-_40r3vaip6gma">To the cloud with WebTransport</a></li>
</ul>
</li>
<li><a href="#post-3953-_5mtszjbqasf">Handling backpressure</a></li>
<li><a href="#post-3953-_4j83pxmfdgaz">Workers, TransformStream and VideoFrame</a>
<ul>
<li><a href="#post-3953-_8meoqecfwv5s">Multiple Workers?</a></li>
<li><a href="#post-3953-_hicxjr49vpl2">Learning: use a single worker for now</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#post-3953-_kw0obeiiac8w">Measuring performance</a></li>
<li><a href="#post-3953-_amh3sm1ce2b0">How to Process VideoFrames</a></li>
</ul>
<p>{“editor”, “<a href="https://webrtchacks.com/about/chad/">chad hart</a>“}</p>
<p><img decoding="async" src="https://webrtchacks.com/wp-content/uploads/2023/03/The-future-of-real-time-video-processing-md.png" alt="" width="800" height="582" srcset="https://webrtchacks.com/wp-content/uploads/2023/03/The-future-of-real-time-video-processing-md.png 800w, https://webrtchacks.com/wp-content/uploads/2023/03/The-future-of-real-time-video-processing-md-768x559.png 768w" sizes="(max-width: 800px) 100vw, 800px"/></p>
<hr/>

<p>In simple WebRTC video conferencing scenarios, audio and video streams captured on one device are sent to another device, possibly going through some intermediary server. The capture of raw audio and video streams from microphones and cameras relies on <code>getUserMedia</code>. Raw media streams then need to be encoded for transport and sent over to the receiving side. Received streams must be decoded before they can be rendered. The resulting video pipeline is illustrated below. Web applications do not see these separate encode/send and receive/decode steps in practice – they are entangled in the core WebRTC API and under the control of the browser.</p>
<p><img decoding="async" loading="lazy" width="801" height="446" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-1-1.png" srcset="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-1-1.png 801w, https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-1-1-768x428.png 768w" sizes="(max-width: 801px) 100vw, 801px"/></p>
<p>If you want to add the ability to do something like remove users’ backgrounds, the most scalable and privacy respective option is to do it client-side before the video stream is sent to the network. This operation needs access to the raw pixels of the video stream. Said differently, it needs to take place between the capture step and encode steps. Similarly, on the receiving side, you may want to give users options like adjusting colors and contrast, which also require raw pixel access between the decode and render steps. As illustrated below, this adds an extra process steps to the resulting video pipeline.</p>
<p><img decoding="async" loading="lazy" width="800" height="585" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-2-1.png" srcset="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-2-1.png 800w, https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-2-1-768x562.png 768w" sizes="(max-width: 800px) 100vw, 800px"/></p>
<p>This made <a href="https://www.w3.org/People/Dom/">Dominique Hazaël-Massieux</a> and me wonder how web applications can build such media processing pipelines.</p>
<p>The main problem is raw frames from a video stream cannot casually be exposed to web applications. Raw frames are:</p>
<ul>
<li>large – several MB per frame,</li>
<li>plentiful – 25 frames per second or more,</li>
<li>not easily exposable – GPU to CPU read-back often needed, and</li>
<li>browsers need to deal with a variety of pixel formats (RGBA, YUV, etc.) and color spaces under the hoods.</li>
</ul>
<p>As such, whenever possible, web technologies that manipulate video streams on the web (<a href="https://html.spec.whatwg.org/multipage/media.html">HTMLMediaElement</a>, <a href="https://www.w3.org/TR/webrtc/">WebRTC</a>, <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia">getUserMedia</a>, <a href="https://www.w3.org/TR/media-source-2/">Media Source Extensions</a>) treat them as opaque objects and hide the underlying pixels from applications. This makes it difficult for web applications to create a media processing pipeline in practice.</p>
<p>Fortunately, the <code>VideoFrame</code> interface in <a href="https://www.w3.org/TR/webcodecs/">WebCodecs</a> may help, especially if you couple this with the <code>MediaStreamTrackProcessor</code> object defined in <a href="https://www.w3.org/TR/mediacapture-transform/">MediaStreamTrack Insertable Media Processing using Streams</a> that creates a bridge between WebRTC and WebCodecs. WebCodecs lets you access and process raw pixels of media frames. Actual processing can use one of many technologies, starting with good ol’ JavaScript and including <a href="https://www.w3.org/TR/wasm-core/">WebAssembly</a>, <a href="https://www.w3.org/TR/webgpu/">WebGPU</a>, or the <a href="https://www.w3.org/TR/webnn/">Web Neural Network API</a> (WebNN).</p>
<p>After processing, you could get back to WebRTC land through the same bridge. That said, WebCodecs can also put you in control of the encode/decode steps in the pipeline through its <code>VideoEncoder</code> and <code>VideoDecoder</code> interfaces. These can give you full control over all individual steps in the pipeline:</p>
<ul>
<li>For transporting the processed image somewhere while keeping latency low, you could consider <a href="https://www.w3.org/TR/webtransport/">WebTransport</a> or WebRTC’s <code>RTCDataChannel</code>.</li>
<li>For rendering, you could render directly to a canvas through <code>drawImage</code>, using WebGPU, or via an <code>&lt;video&gt;</code> element through <code>VideoTrackGenerator</code> (also defined in <a href="https://www.w3.org/TR/mediacapture-transform/">MediaStreamTrack Insertable Media Processing using Streams</a>).</li>
</ul>
<p>Inspired by <a href="https://github.com/w3c/webcodecs/tree/main/samples/encode-decode-worker">sample code</a> created by <a href="https://webrtchacks.com/tag/bernard-aboba/">Bernard Aboba</a> – co-editor of the WebCodecs and WebTransport specifications and co-chair of the WebRTC Working Group in W3C – Dominique and I decided to spend a bit of time exploring the creation of processing media pipelines. First, we wanted to better grasp media concepts such as video pixel formats and color spaces – we probably qualify as web experts, but we are not media experts and we tend to view media streams as opaque beasts as well. Second, we wanted to assess whether technical gaps remain. Finally, we wanted to understand where and when copies get made and gather some performance metrics along the way.</p>
<p>This article describes our approach, provides highlights of our <a href="https://github.com/tidoust/media-tests/">resulting demo code</a>, and shares our learnings. The code should not be seen as authoritative or even correct (though we hope it is), it is just the result of a short journey in the world of media processing. Also, note the technologies under discussion are still nascent and do not yet support interoperability across browsers. Hopefully, this will change soon!</p>
<p><em>Note: We did not touch on audio for lack of time. Audio frames take less memory, but there are many more of them per second and they are more sensitive to timing hiccups. Audio frames are processed with the Web Audio API. It would be very interesting to add audio to the mix, be it only to explore audio/video synchronization needs.</em></p>

<p>Our <a href="https://tidoust.github.io/media-tests/">demo</a> explores the creation of video processing pipelines, captures performance metrics, evaluates the impacts of choosing a specific technology to process frames, and provides insights about where operations get done and when copies are made. The processing operations loop through all pixels in the frame and “do something with them” (what they actually do is of little interest here). Different processing technologies are used for testing purposes, not because they would necessarily be a good choice for the problem at hand.</p>
<p>The demo lets the user:</p>
<ol>
<li>Choose a source of input to create an initial stream of VideoFrame: either a <a href="https://en.wikipedia.org/wiki/Nyan_Cat">Nyan-cat</a>-like animation created from scratch using <code>OffscreenCanvas</code>, or a live stream generated from a camera. The user may also choose the resolution and framerate of the video stream.</li>
<li>Process video frames to replace green with blue using WebAssembly.</li>
<li>Process video frames to turn them into black and white using pure JavaScript.</li>
<li>Add an H.264 encoding/decoding transformation stage using WebCodecs.</li>
<li>Introduce slight delays in the stream using regular JavaScript.</li>
<li>Add an overlay to the bottom right part of the video that encodes the frame’s timestamp. The overlay is added using WebGPU and WGSL.</li>
<li>Add intermediary steps to force copies of the frame to CPU memory or GPU memory, to evaluate the impact of the frame’s location in memory on transformations.</li>
</ol>
<p><img decoding="async" loading="lazy" width="588" height="659" src="https://webrtchacks.com/wp-content/uploads/2023/03/the-start-page-of-the-demo-lets-the-use-select-run-1.png" alt="The start page of the demo lets the use select running settings before the demo gets started."/></p>
<p>Once you hit the “Start” button, the pipeline runs and the resulting stream is displayed on the screen in a <code>&lt;video&gt;</code> element. And… that’s it, really! What mattered to us was the code needed to achieve that and the insights we gained from gathering performance metrics and playing with parameters. Let’s dive into that!</p>


<h3><a id="post-3953-_twzsugow9o7d"></a>Note: these APIs are new and may not work in your browser</h3>
<p>Technologies discussed in this article and used in the demo are still “emerging” (at least as of March 2023). The demo currently only runs in <a href="https://www.google.com/chrome/canary/">Google Chrome Canary</a> with WebGPU enabled (“Unsafe WebGPU” flag set in <a href="chrome://flags/">chrome://flags/</a>). Hopefully, the demo can soon run in other browsers too. Video processing with WebCodecs is available in the <a href="https://developer.apple.com/safari/technology-preview/release-notes/#r158">technical preview of Safari</a> (16.4) and is under development in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1746557">Firefox</a>. WebGPU is also under development in <a href="https://webkit.org/status/#?search=webgpu">Safari</a> and <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1602129">Firefox</a>. A greater unknown is support for <em>MediaStreamTrack Insertable Media Processing using Streams</em> in other browsers. For example, see this <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1749540">tracking bug in Firefox</a>.</p>
<h3><a id="post-3953-_kic3ab3akhme"></a>Timing Stats</h3>
<p>Timing statistics are reported to the end of the page at the end and as objects to the console (this requires opening the dev tools panel). Provided the overlay was present, display times for each frame are reported too.</p>
<p><img decoding="async" loading="lazy" width="765" height="529" src="https://webrtchacks.com/wp-content/uploads/2023/03/stats-reported-include-a-table-with-the-average-m-1.png" alt="Stats reported include a table with the average, median, min and max of each processing step."/></p>
<p>We’ll discuss more on this in the <a href="#post-3953-_kw0obeiiac8w">Measuring Performance</a> section.</p>

<p>WebCodecs is the core of the demo and the key technology we’re using to build a media pipeline. Before we dive more into this, it may be useful to reflect on the value of using WebCodecs in this context. Other approaches could work just as well.</p>
<h3><a id="post-3953-_hyjatcye1dc9"></a>What about the Canvas? Do we need WebCodecs?</h3>
<p>In fact, client-side processing of raw video frames has been possible on the web ever since the <code>&lt;video&gt;</code> and <code>&lt;canvas&gt;</code> elements were added to HTML, with the following recipe:</p>
<ol>
<li>Render the video onto a <code>&lt;video&gt;</code> element.</li>
<li>Draw the contents of the <code>&lt;video&gt;</code> element onto a <code>&lt;canvas&gt;</code> with <a href="https://html.spec.whatwg.org/multipage/canvas.html#dom-context-2d-drawimage">drawImage</a> on a recurring basis, e.g. using <a href="https://html.spec.whatwg.org/multipage/imagebitmap-and-animations.html#dom-animationframeprovider-requestanimationframe">requestAnimationFrame</a> or the more recent <a href="https://wicg.github.io/video-rvfc/">requestVideoFrameCallback</a> that notifies applications when a video frame has been presented for composition and provides them with metadata about the frame.</li>
<li>Process the contents of the &lt;canvas&gt; whenever it gets updated.</li>
</ol>
<p>We did not integrate this approach in our demo. Among other things, the performance here would depend on having the processing happen out of the main thread. We would need to use an <a href="https://html.spec.whatwg.org/multipage/canvas.html#the-offscreencanvas-interface">OffscreenCanvas</a> to process contents in a worker, possibly coupled with a call to <a href="https://w3c.github.io/mediacapture-image/#dom-imagecapture-grabframe">grabFrame</a> to send the video frame to the worker.</p>
<h3><a id="post-3953-_65zh61jfzn5k"></a>WebCodecs advantages</h3>
<p>One drawback to the Canvas approach is that there is no guarantee that all video frames get processed. Applications can tell how many frames they missed if they hook onto <code>requestVideoFrameCallback</code> by looking at the <a href="https://wicg.github.io/video-rvfc/#dom-videoframecallbackmetadata-presentedframes">presentedFrames</a> counter, but missed frames were, by definition, missed. Another drawback is that some of the code (<code>drawImage</code> or <code>grabFrame</code>) needs to run on the main thread to access the <code>&lt;video&gt;</code> element.</p>
<p>WebGL and WebGPU also provide mechanisms to import video frames as textures directly from a <code>&lt;video&gt;</code> element, e.g. through the <a href="https://gpuweb.github.io/gpuweb/#external-texture-creation">importExternalTexture method</a> in WebGPU. This approach works well if the processing logic can fully run on the GPU.</p>
<p>WebCodecs gives applications a direct handle to a video frame and mechanisms to encode/decode them. This allows applications to create frames from scratch, or from an incoming stream, provided that the stream is in non-containerized form.</p>
<h3><a id="post-3953-_gk15qhdru321"></a>Note on containerized media</h3>
<p>One important note – media streams are usually encapsulated in a media container. The container may include other streams along with timing and other metadata. While media streams in WebRTC scenarios do not use containers, most stored media files and media streamed on the web use adaptive streaming technologies (e.g. DASH, HLS) that are in a containerized form (e.g. MP4, ISOBMFF). WebCodecs can only be used on non-containerized streams. Applications that want to use WebCodecs with containerized media need to ship additional logic on their own to extract the media streams from their container (and/or to add streams to a container). For more information about media container formats, we recommend <a href="https://bitmovin.com/container-formats-fun-1/">The Definitive Guide to Container File Formats</a> by <a href="https://www.linkedin.com/in/armin-trattnig-505b6b90/">Armin Trattnig</a>.</p>

<p>So, having a direct handle on a video frame seems useful to create a media processing pipeline. It gives a handle to the atomic chunk of data that will be processed at each step.</p>
<h3><a id="post-3953-_mlsoxxpyj6qj"></a>Pipe chains</h3>
<p>WHATWG <a href="https://streams.spec.whatwg.org/">Streams</a> are specifically designed to create <a href="https://streams.spec.whatwg.org/#pipe-chain">pipe chains</a> to process such atomic chunks. This is illustrated in the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Streams_API/Concepts">Streams API concepts</a> MDN page:</p>
<figure id="attachment_3990" aria-describedby="caption-attachment-3990"><img decoding="async" loading="lazy" src="https://webrtchacks.com/wp-content/uploads/2023/03/diagram-of-a-pipe-chain-that-illustrates-whatwg-st-1.png" alt="Diagram of a pipe chain that illustrates WHATWG Streams concepts" width="1000" height="382" srcset="https://webrtchacks.com/wp-content/uploads/2023/03/diagram-of-a-pipe-chain-that-illustrates-whatwg-st-1.png 1000w, https://webrtchacks.com/wp-content/uploads/2023/03/diagram-of-a-pipe-chain-that-illustrates-whatwg-st-1-800x306.png 800w, https://webrtchacks.com/wp-content/uploads/2023/03/diagram-of-a-pipe-chain-that-illustrates-whatwg-st-1-768x293.png 768w" sizes="(max-width: 1000px) 100vw, 1000px"/><figcaption id="caption-attachment-3990">Streams API concepts diagram by Mozilla Contributors is licensed under CC-BY-SA 2.5.</figcaption></figure>
<p>WHATWG Streams are also used as the underlying structure by some of the technologies under consideration, such as <code>WebTransport</code>, <code>VideoTrackGenerator</code>, and <code>MediaStreamTrackProcessor</code>.</p>
<h3><a id="post-3953-_vlbyeb2qqeyl"></a>Backpressure</h3>
<p>Finally, Streams provide backpressure and queuing mechanisms out of the box. As defined in the <a href="https://streams.spec.whatwg.org/#pipe-chains">WHATWG Streams standard</a>, backpressure is the process of</p>
<blockquote><p>normalizing flow from the original source according to how fast the chain can process chunks.</p></blockquote>
<p>When a step in a chain is unable to accept more chunks in its queue, it sends a signal that propagates backward through the pipe chain and up to the source to tell it to adjust its rate of production of new chunks. With backpressure, no need to worry about overflowing queues, the flow will naturally adapt to the maximum speed at which processing can run.</p>
<h3><a id="post-3953-_bvpkdqbuta47"></a>Creating a pipeline</h3>
<p>Broadly speaking, creating a media processing pipeline using streams translates to:</p>
<ol>
<li>Create a stream of <code>VideoFrame</code> objects – <em>somehow</em></li>
<li>Use <code>TransformStream</code> to create processing steps – compose them as needed</li>
<li>Send/Render the resulting stream or <code>VideoFrame</code> objects – <em>somehow</em></li>
</ol>
<p>The Devil is of course in the <em>somehow</em>. Some technologies can ingest or digest a stream of <code>VideoFrame</code> objects directly – not all of them can. Connectors are needed.</p>
<h2><a id="post-3953-_9uk6sp1lrne9"></a>Pipelining is like a game of dominoes</h2>
<p>We found it useful to visualize possibilities through a game of dominoes:</p>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-7-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-8-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-9-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-10-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-11-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-12-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-13-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-14-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-15-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-16-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-17-1.png"/></p>
<p>The left side of each domino is a type of input. The right side of the diagram shows the type of output. There are three main types of dominoes:</p>
<ol>
<li>generators,</li>
<li>transformers, and</li>
<li>consumers.</li>
</ol>
<p>As long as you match the input of a domino with the output of the preceding one, you may assemble them any way you like to create pipelines. Let’s look at them in more detail:</p>
<h3><a id="post-3953-_eylc9lqr9026"></a>Generating a stream</h3>
<h4><a id="post-3953-_ffuh4es7crws"></a>From scratch</h4>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-18-1.png"/></p>
<p>You may create a <code>VideoFrame</code> from the contents of a canvas (or a buffer of bytes for that matter). Then, to generate a stream, just write the frame to a <code>WritableStream</code> at a given rate. In our code, this is implemented in the <a href="https://github.com/tidoust/media-tests/blob/main/worker-getinputstream.js">worker-getinputstream.js</a> file. The logic creates a Nyan-cat-like animation with the W3C logo. As we will describe later, we make use of the WHATWG Streams backpressure mechanism by waiting for the writer to be ready:</p>
<div id="urvanov-syntax-highlighter-641639fe27ebd530646477" data-settings=" minimize scroll-mouseover">



<div>
<table>
<tbody><tr>
<td data-settings="show">

</td>
<td><div><p><span>await </span><span>writer</span><span>.</span><span>ready</span><span>;</span></p><p><span>const</span><span> </span><span>frame</span><span> </span><span>=</span><span> </span><span>new</span><span> </span><span>VideoFrame</span><span>(</span><span>canvas</span><span>,</span><span> </span><span>.</span><span>.</span><span>.</span><span>)</span><span>;</span></p><p><span>writer</span><span>.</span><span>write</span><span>(</span><span>frame</span><span>)</span><span>;</span></p></div></td>
</tr>
</tbody></table>
</div>
</div>


<h4><a id="post-3953-_f9gywx64ztkl"></a>From a camera or a WebRTC track</h4>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-19-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-20-1.png"/></p>
<p>In WebRTC contexts, the source of a video stream is usually a <code>MediaStreamTrack</code> obtained from the camera through a call to <code>getUserMedia</code>, or received from a peer. The <code>MediaStreamTrackProcessor</code> object (MSTP) can be used to convert the <code>MediaStreamTrack</code> to a stream of <code>VideoFrame</code> objects.</p>
<p><em>Note: <code>MediaStreamTrackProcessor</code> is only exposed in worker contexts… in theory, but Chrome currently exposes it on the main thread and only there.</em></p>
<h4><a id="post-3953-_usek8unlekut"></a>From a WebTransport stream</h4>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-21-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-22-1.png"/></p>
<p>WebTransport creates WHATWG streams, so there is no need to run any stream conversion. That said, it is fairly inefficient to transport raw decoded frames given their size. Thus why all media streams travel encoded through the cloud! As such, the <code>WebTransportReceiveStream</code> will typically contain encoded chunks, to be interpreted as <a href="https://www.w3.org/TR/webcodecs/#encodedvideochunk-interface">EncodedVideoChunk</a>. To get back to a stream of <code>VideoFrame</code> objects, each chunk needs to go through a <code>VideoDecoder</code>. Simple chunk encoding/decoding logic (without <code>WebTransport</code>) can be found in the <a href="https://github.com/tidoust/media-tests/blob/main/worker-transform.js">worker-transform.js</a> file.</p>
<h4><a id="post-3953-_1x8j6ehglv5l"></a>What about WebTransport?</h4>
<p>The demo does not integrate <code>WebTransport</code>yet. We encourage you to check <a href="https://github.com/w3c/webtransport/pull/430">Bernard Aboba’s WebCodecs/WebTransport sample</a>. Both the sample and approach presented here are limited in that only one stream is used to send/receive encoded frames. Real-life applications would likely be more complex to avoid head-of-line blocking issues. They would likely use multiple transport streams in parallel, up to one per frame. On the receiving end, frames received on individual streams then need to be reordered and merged to re-create a unique stream of encoded frames. The IETF <a href="https://datatracker.ietf.org/group/moq/about/">Media over QUIC</a> (moq) Working Group develops such a low-latency media delivery solution (over raw QUIC or WebTransport).</p>
<h4><a id="post-3953-_l2xvd2ia9xvj"></a>What about Data Channels?</h4>
<p><a href="https://webrtc.org/getting-started/data-channels">RTCDataChannel</a> could also be used to transport encoded frames, with the caveat that <a href="https://github.com/w3c/webrtc-pc/issues/1732">some adaptation logic would be needed to connect RTCDataChannel with Streams</a>.</p>
<h3><a id="post-3953-_po5r16gpb8k9"></a>Transforming a stream</h3>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-23-1.png"/></p>
<p>Once you have a Stream of <code>VideoFrame</code> objects, video processing can be structured as a <code>TransformStream</code> that takes a <code>VideoFrame</code> as input and produces an updated <code>VideoFrame</code> as output. Transform streams can be chained as needed, although it is always a good idea to keep the number of steps that need to access pixels to a minimum, since accessing pixels in a video frame typically means looping through millions of them (ie <code>1920 * 1080 = 2 074 600</code> pixels for a video frame in full HD).</p>
<p><em>Note: Part 2 explores technologies that can be used under the hood to process the pixels. We also review performance considerations.</em></p>
<h3><a id="post-3953-_440kg5x1c7nr"></a>Sending/Rendering a stream</h3>
<p>Some apps only need to extract information from the stream – like in the case of gesture detection. However, in most cases, the final stream needs to be rendered or sent somewhere.</p>
<h4><a id="post-3953-_tp56pjon6p4b"></a>To a &lt;canvas&gt; element</h4>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-24-1.png"/></p>
<p>A <code>VideoFrame</code> can be directly drawn onto a canvas. Simple!</p>
<div id="urvanov-syntax-highlighter-641639fe27ec4125596304" data-settings=" minimize scroll-mouseover">



<div>
<table>
<tbody><tr>
<td data-settings="show">

</td>
<td><div><p><span>canvasContext</span><span>.</span><span>drawImage</span><span>(</span><span>frame</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>0</span><span>)</span><span>;</span></p></div></td>
</tr>
</tbody></table>
</div>
</div>

<p>Rendering frames to a canvas gives the applications full control over when to display those frames. This seems particularly useful when a video stream needs to be synchronized with something else, e.g. overlays and/or audio. One drawback is that, if the goal is to end up with a media player, you will have to re-implement that media player from scratch. That means adding controls, support for tracks, accessibility, etc. This is no easy task…</p>
<h4><a id="post-3953-_ag2qq6wf69s4"></a>To a &lt;video&gt; element</h4>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-25-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-26-1.png"/></p>
<p>A stream of <code>VideoFrame</code> objects cannot be injected into a <code>&lt;video&gt;</code> element. Fortunately, a <code>VideoTrackGenerator</code> (VTG) can be used to convert the stream into a <code>MediaStreamTrack</code> that can then be injected into a <code>&lt;video&gt;</code> element.</p>
<h4><a id="post-3953-_ogeyr0dsz4ww"></a>Notes and Caveats</h4>
<h5><a id="post-3953-_jdo4c9j8syxi"></a>Only for Workers</h5>
<p>Note <code>VideoTrackGenerator</code> is only exposed in worker contexts… in theory, but as for <code>MediaStreamTrackProcessor</code>, Chrome currently exposes it on the main thread and only there.</p>
<h5><a id="post-3953-_8b0qetv6i9ub"></a><code>VideoTrackGenerator</code> is the new <code>MediaStreamTrackGenerator</code></h5>
<p>Also note: <code>VideoTrackGenerator</code> used to be called <code>MediaStreamTrackGenerator</code>. Implementation in Chrome has not yet caught up with the new name, so our code still uses the old name!</p>
<h4><a id="post-3953-_40r3vaip6gma"></a>To the cloud with WebTransport</h4>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-27-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-28-1.png"/></p>
<p>WebTransport can be used to send the resulting stream to the cloud. As noted before, it would require too much bandwidth to send unencoded video frames in a <code>WebTransportSendStream</code>. They need to be encoded first, using the <code>VideoEncoder</code> interface defined in WebCodecs. Simple frame encoding/decoding logic (without WebTransport) can be found in the <a href="https://github.com/tidoust/media-tests/blob/main/worker-transform.js">worker-transform.js</a> file.</p>
<h2><a id="post-3953-_5mtszjbqasf"></a>Handling backpressure</h2>
<p>Streams come geared with a backpressure mechanism. Signals propagate through the pipe chain and up to the source when the queue is building up to indicate it might be time to slow down or drop one or more frames. This mechanism is very convenient to avoid accumulating large decoded video frames in the pipeline that could exhaust memory. 1 second of full HD video at 25 frames per second happily takes 200MB of memory once decoded.</p>
<p>The API also makes it possible for web applications to implement their own buffering strategy. If you need to process a live feed in real-time, you may want to drop frames that cannot be processed in time. Alternatively, if you need to transform recorded media then you can slow down and process all frames, no matter how long it takes.</p>
<p>One structural limitation is that backpressure signals only propagate through the pipeline in parts where WHATWG streams are used. They stop whenever the signals bump into something else. For instance, <code>MediaStreamTrack</code> does not expose a WHATWG streams interface. As a result, if a <code>MediaStreamTrackProcessor</code> is used in a pipeline, it receives backpressure signals but signals do not propagate beyond it. The buffering strategy is imposed: the oldest frame will be removed from the queue when room is needed for a new frame.</p>
<p>In other words, if you ever end up with a <code>VideoTrackGenerator</code> followed by a <code>MediaStreamTrackProcessor</code> in a pipeline, backpressure signals will be handled by the <code>MediaStreamTrackProcessor</code> and will not propagate to the source before the <code>VideoTrackGenerator</code>. You should not need to create such a pipeline, but we accidentally ended up with that configuration while writing the demo. Keep in mind that this is not equivalent to an identity transform.</p>
<p><img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-29-1.png"/> <img decoding="async" loading="lazy" width="320" height="72" src="https://webrtchacks.com/wp-content/uploads/2023/03/word-image-3953-30-1.png"/></p>
<h2><a id="post-3953-_4j83pxmfdgaz"></a>Workers, TransformStream and VideoFrame</h2>
<p>So far, we have assembled dominoes without being explicit about where the underlying code is going to run. With the notable exception of <code>getUserMedia</code>, all the components that we have discussed can run in workers. Running them outside of the main thread is either good practice or mandated as in the case of <code>VideoTrackGenerator</code> and <code>MediaStreamTrackProcessor</code> – though note these interfaces are actually only available on the main thread in Chrome’s current implementation.</p>
<h3><a id="post-3953-_8meoqecfwv5s"></a>Multiple Workers?</h3>
<p>Now if we are going to have threads, why restrict yourself to one worker when you can create more? Even though a media pipeline describes a sequence of steps, it seems useful at first sight to try and run different steps in parallel.</p>
<p>To run a processing step in a worker, the worker needs to gain access to the initial stream of <code>VideoFrame</code>objects which may have been created in another worker. Workers typically do not share memory but the <code>postMessage</code> API may be used for cross-worker communication. A <code>VideoFrame</code> is not a simple object but it is defined as a <a href="https://html.spec.whatwg.org/multipage/structured-data.html#transferable-objects"><em>transferable object</em></a>, which essentially means that it can be sent from one worker to another efficiently, without requiring a copy of the underlying frame data.</p>
<p><em>Note:</em><em> Transfer detaches the object being transferred, which means that the transferred object can no longer be used by the worker that issued the call to <code>postMessage</code>.</em></p>
<p>One approach to run processing steps in separate workers would be to issue a call to <code>postMessage</code> for each and every <code>VideoFrame</code> at the end of a processing step to pass it over to the next step. From a performance perspective, while <a href="https://surma.dev/things/is-postmessage-slow/">postMessage is not necessarily slow</a>, the API is event-based and events still introduce delays. A better approach would be to pass the stream of <code>VideoFrame</code> objects once and for all when the pipeline is created. This is possible because <code>ReadableStream</code>, <code>WritableStream</code> and <code>TransformStream</code> are all <em>transferable objects</em> as well. Code to connect an input and output stream to another worker could then become:</p>
<div id="urvanov-syntax-highlighter-641639fe27ec6117263614" data-settings=" minimize scroll-mouseover">



<div>
<table>
<tbody><tr>
<td data-settings="show">

</td>
<td><div><p><span>worker</span><span>.</span><span>postMessage</span><span>(</span><span>{</span></p><p><span>    </span><span>type</span><span>:</span><span> </span><span>&#39;start&#39;</span><span>,</span></p><p><span>    </span><span>inputStream</span><span>:</span><span> </span><span>readableStream</span><span>,</span></p><p><span>    </span><span>outputStream</span><span>:</span><span> </span><span>writableStream</span></p><p><span>}</span><span>,</span><span> </span><span>[</span><span>readableStream</span><span>,</span><span> </span><span>writableStream</span><span>]</span><span>)</span><span>;</span></p></div></td>
</tr>
</tbody></table>
</div>
</div>

<p>Now, the fact that streams get transferred does not mean that the chunks that get read from or written to these streams are themselves <em>transferred</em>. Chunks are rather <em>serialized</em>. The nuance is thin (and should have a very minimal impact on performance) but particularly important for <code>VideoFrame</code> objects. Why? Because a <code>VideoFrame</code> needs to be explicitly closed through a call to its <code>close</code> method to free the underlying media resource that the <code>VideoFrame</code> points to.</p>
<p>When a <code>VideoFrame</code> is transferred, its <code>close</code> method is automatically called on the sender side. When a <code>VideoFrame</code> is serialized, even though the underlying media resource is not cloned, the <code>VideoFrame</code> object itself is cloned, and the <code>close</code> method now needs to be called twice: once on the sender side and once on the receiver side. The receiver side is not an issue: calling <code>close()</code> there is to be expected. However, there is a problem on the sender’s side: a call like <code>controller.enqueue(frame)</code> in a <code>TransformStream</code> attached to a readable stream transferred to another worker will trigger the serialization process, but that process happens asynchronously and there is no way to tell when it is done. In other words, on the sender side, code cannot simply be:</p>
<div id="urvanov-syntax-highlighter-641639fe27ec8233760721" data-settings=" minimize scroll-mouseover">



<div>
<table>
<tbody><tr>
<td data-settings="show">

</td>
<td><div><p><span>controller</span><span>.</span><span>enqueue</span><span>(</span><span>frame</span><span>)</span><span>;</span></p><p><span>frame</span><span>.</span><span>close</span><span>(</span><span>)</span><span>;</span><span> </span><span>// Too early!</span></p></div></td>
</tr>
</tbody></table>
</div>
</div>

<p>If you do that, the browser will rightfully complain when it effectively serializes the frame that it cannot clone it because the frame has already been closed. And yet the sender needs to close the frame at some point. If you don’t, one of two things could happen:</p>
<ol>
<li>the browser will either report a warning that it bumped into dangling <code>VideoFrame</code> instances (which suggests a memory leak) or</li>
<li>the pipeline simply freezes after a couple of frames are processed.</li>
</ol>
<p>The pipeline freeze happens, for example, when the <code>VideoFrame</code> is tied to hardware-decoded data. Hardware decoders use a very limited memory buffer, so pause until the memory of already decoded frames gets freed. This is a known issue. There are ongoing discussions to extend WHATWG streams with a new mechanism that would allow it to explicitly transfer ownership of the frame so that the sender side does not need to worry about the frame anymore. See for example the <a href="https://github.com/whatwg/streams/blob/main/streams-for-raw-video-explainer.md#transferring-ownership-streams-explained">Transferring Ownership Streams Explained</a> proposal.</p>
<p><em>Note:</em> <em>Closing the frame synchronously as in the code above sometimes works in practice in Chrome depending on the underlying processing pipeline. We found it hard to reproduce the exact conditions that make the browser decide to clone the frame right away or delay it. As far as we can tell, the code should not work in any case.</em></p>
<h3><a id="post-3953-_hicxjr49vpl2"></a>Learning: use a single worker for now</h3>
<p>For now, it is probably best to stick to touching streams of <code>VideoFrame</code> objects from one and only one worker. The demo does use more than one worker. It keeps track of frame instances to close at the end of the processing pipeline. However, we did that simply because we did not know initially that creating multiple workers would be problematic and require such a hack.</p>

<p>The timestamp property of a <code>VideoFrame</code> instance provides a good identifier for individual frames, and allows applications to track them throughout the pipeline. The timestamp even survives encoding (and respectively decoding) with a <code>VideoEncoder</code> (and respectively with <code>VideoDecoder</code>).</p>
<p>In the suggested pipeline model, a transformation step is a <code>TransformStream</code> that operates on encoded or decoded frames. The time taken to run the transformation step is thus simply the time taken by the transform function, or more precisely the time taken until the function calls <code>controller.enqueue(transformedChunk)</code> to send the updated frame down the pipe. The demo features a generic <a href="https://github.com/tidoust/media-tests/blob/main/InstrumentedTransformStream.js">InstrumentedTransformStream</a> class that extends <code>TransformStream</code> to record start and end times for each frame in a static cache. The class is a drop-in replacement for <code>TransformStream</code>:</p>
<div id="urvanov-syntax-highlighter-641639fe27ec9342582295" data-settings=" minimize scroll-mouseover">



<div>
<table>
<tbody><tr>
<td data-settings="show">

</td>
<td><div><p><span>const</span><span> </span><span>transformStream</span><span> </span><span>=</span><span> </span><span>new</span><span> </span><span>InstrumentedTransformStream</span><span>(</span><span>{</span></p><p><span>    </span><span>name</span><span>:</span><span> </span><span>&#39;super-duper&#39;</span><span>,</span></p><p><span>    </span><span>transform</span><span>(</span><span>chunk</span><span>,</span><span> </span><span>controller</span><span>)</span><span> </span><span>{</span></p><p><span>        </span><span>const</span><span> </span><span>transformedChunk</span><span> </span><span>=</span><span> </span><span>doSomethingWith</span><span>(</span><span>chunk</span><span>)</span><span>;</span></p><p><span>        </span><span>controller</span><span>.</span><span>enqueue</span><span>(</span><span>transformedChunk</span><span>)</span><span>;</span></p><p><span>    </span><span>}</span></p><p><span>}</span><span>)</span><span>;</span></p></div></td>
</tr>
</tbody></table>
</div>
</div>

<p>Recorded times then get entered in an instance of a generic <a href="https://github.com/tidoust/media-tests/blob/main/StepTimesDB.js">StepTimesDB</a> class to compute statistics such as minimum, maximum, average, and median times taken by each step, as well as time spent waiting in queues.</p>
<p>This works well for the part of the pipeline that uses WHATWG Streams, but as soon as the pipeline uses opaque streams, such as when frames are fed into a <code>VideoTrackGenerator</code>, we lose the ability to track individual frames. In particular, there is no easy way to tell when a video frame is actually displayed to a <code>&lt;video&gt;</code> element. The <a href="https://wicg.github.io/video-rvfc/">requestVideoFrameCallbac</a>k function reports many interesting timestamps, but not the timestamp of the frame that has been presented for composition.</p>
<p>The workaround implemented in the demo encodes the frame’s timestamp in an overlay in the bottom-right corner of the frame and then copies the relevant part of frames rendered to the <code>&lt;video&gt;</code> element to a <code>&lt;canvas&gt;</code> element whenever the <code>requestVideoFrameCallback</code> callback is called to decode the timestamp. This does not work perfectly – frames can be missed in between calls to the callback function, but it is better than nothing.</p>
<p><em>Note: <code>requestVideoFrameCallback</code> is supported in Chrome and Safari but not in Firefox for now.</em></p>
<p>It is useful for statistical purposes to track the time when the frame is rendered. For example, one could evaluate jitter effects.  Or you could use this data for synchronization purposes, like if video needs to be synchronized with an audio stream and/or other non-video overlays. Frames can of course be rendered to a canvas instead. The application can then keep control over when a frame gets displayed to the user (ignoring the challenges of reimplementing a media player).</p>
<p>A typical example of statistics returned to the console at the end of a demo run is provided below:</p>
<figure id="attachment_4015" aria-describedby="caption-attachment-4015"><img decoding="async" loading="lazy" src="https://webrtchacks.com/wp-content/uploads/2023/03/example-of-final-statistics-reported-to-the-consol-1.png" alt="Example of final statistics reported to the console. Time are per processing step and per frame. The stats include average times taken by each step per frame: background removal took 22ms, adding the overlay 1ms, encoding 8 ms, decoding only 1ms, and frames stayed on display for 38ms." width="533" height="214"/><figcaption id="caption-attachment-4015">A look at the console output of the <a href="https://tidoust.github.io/media-tests/">video frame processing tests demo</a></figcaption></figure>
<p>The times are per processing step and per frame. The statistics include the average times taken by each step per frame. For this example: background removal took <code>22ms</code>, adding the overlay <code>1ms</code>, encoding <code>8 ms</code>, decoding <code>1ms</code>, and frames stayed on display during <code>38ms</code>.</p>

<p>This article explored the creation of a real-time video processing pipeline using WebCodecs and Streams, along with considerations on handling backpressure, managing the <code>VideoFrame</code> lifecycle, and measuring performance. The next step is to actually start processing the <code>VideoFrame</code> objects that such a pipeline would expose. Please stay tuned, this is the topic of part 2!</p>
<p>{“author”: “<a href="https://twitter.com/tidoust">François Daoust</a>“}</p>

<h5>Attributions</h5>
<p>– WHATWG Stream logo: <a href="https://resources.whatwg.org/logo-streams.svg" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://resources.whatwg.org/logo-streams.svg&amp;source=gmail&amp;ust=1678463354905000&amp;usg=AOvVaw13yOnxlsJlxJRHjFslV5mx">https://resources.whatwg.<span>org</span>/l<wbr/>ogo-streams.svg</a></p>
<p>– Film strip: <a href="https://www.flaticon.com/free-icons/film" target="_blank" rel="noopener noreferrer" data-saferedirecturl="https://www.google.com/url?q=https://www.flaticon.com/free-icons/film&amp;source=gmail&amp;ust=1678463354905000&amp;usg=AOvVaw3geT0IBTiPQh8yESyjIkgK">https://www.flaticon.com/free-<wbr/>icons/film</a></p>

</div></div>
  </body>
</html>
