<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/deepseek-ai/DeepGEMM">Original</a>
    <h1>DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">DeepGEMM is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine-grained scaling, as proposed in <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a>. It supports both normal and Mix-of-Experts (MoE) grouped GEMMs. Written in CUDA, the library has no compilation need during installation, by compiling all kernels at runtime using a lightweight Just-In-Time (JIT) module.</p>
<p dir="auto">Currently, DeepGEMM exclusively supports NVIDIA Hopper tensor cores. To address the imprecise FP8 tensor core accumulation, it employs CUDA-core two-level accumulation (promotion). While it leverages some concepts from <a href="https://github.com/nvidia/cutlass">CUTLASS</a> and <a href="https://github.com/NVIDIA/cutlass/tree/main/include/cute">CuTe</a>, it avoids heavy reliance on their templates or algebras. Instead, the library is designed for simplicity, with only one core kernel function comprising around <strong>~300 lines of code</strong>. This makes it a clean and accessible resource for learning Hopper FP8 matrix multiplication and optimization techniques.</p>
<p dir="auto">Despite its lightweight design, DeepGEMM&#39;s performance matches or exceeds expert-tuned libraries across various matrix shapes.</p>

<p dir="auto">We test all shapes potentially used in DeepSeek-V3/R1 inference (including both prefilling and decoding, but without tensor parallelism) on H800 with NVCC 12.8. All speedup metrics are calculated in comparison to our internally and carefully optimized implementation based on CUTLASS 3.6.</p>
<p dir="auto">DeepGEMM does not behave very well on some shapes, optimization PRs are welcomed if you are interested.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Normal GEMMs for dense models</h3><a id="user-content-normal-gemms-for-dense-models" aria-label="Permalink: Normal GEMMs for dense models" href="#normal-gemms-for-dense-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>M</th>
<th>N</th>
<th>K</th>
<th>Computation</th>
<th>Memory bandwidth</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>64</td>
<td>2112</td>
<td>7168</td>
<td>206 TFLOPS</td>
<td>1688 GB/s</td>
<td>2.7x</td>
</tr>
<tr>
<td>64</td>
<td>24576</td>
<td>1536</td>
<td>289 TFLOPS</td>
<td>2455 GB/s</td>
<td>1.7x</td>
</tr>
<tr>
<td>64</td>
<td>32768</td>
<td>512</td>
<td>219 TFLOPS</td>
<td>2143 GB/s</td>
<td>1.8x</td>
</tr>
<tr>
<td>64</td>
<td>7168</td>
<td>16384</td>
<td>336 TFLOPS</td>
<td>2668 GB/s</td>
<td>1.4x</td>
</tr>
<tr>
<td>64</td>
<td>4096</td>
<td>7168</td>
<td>287 TFLOPS</td>
<td>2320 GB/s</td>
<td>1.4x</td>
</tr>
<tr>
<td>64</td>
<td>7168</td>
<td>2048</td>
<td>295 TFLOPS</td>
<td>2470 GB/s</td>
<td>1.7x</td>
</tr>
<tr>
<td>128</td>
<td>2112</td>
<td>7168</td>
<td>352 TFLOPS</td>
<td>1509 GB/s</td>
<td>2.4x</td>
</tr>
<tr>
<td>128</td>
<td>24576</td>
<td>1536</td>
<td>535 TFLOPS</td>
<td>2448 GB/s</td>
<td>1.6x</td>
</tr>
<tr>
<td>128</td>
<td>32768</td>
<td>512</td>
<td>358 TFLOPS</td>
<td>2103 GB/s</td>
<td>1.5x</td>
</tr>
<tr>
<td>128</td>
<td>7168</td>
<td>16384</td>
<td>645 TFLOPS</td>
<td>2604 GB/s</td>
<td>1.4x</td>
</tr>
<tr>
<td>128</td>
<td>4096</td>
<td>7168</td>
<td>533 TFLOPS</td>
<td>2221 GB/s</td>
<td>2.0x</td>
</tr>
<tr>
<td>128</td>
<td>7168</td>
<td>2048</td>
<td>510 TFLOPS</td>
<td>2277 GB/s</td>
<td>1.7x</td>
</tr>
<tr>
<td>4096</td>
<td>2112</td>
<td>7168</td>
<td>1058 TFLOPS</td>
<td>527 GB/s</td>
<td>1.1x</td>
</tr>
<tr>
<td>4096</td>
<td>24576</td>
<td>1536</td>
<td>990 TFLOPS</td>
<td>786 GB/s</td>
<td>1.0x</td>
</tr>
<tr>
<td>4096</td>
<td>32768</td>
<td>512</td>
<td>590 TFLOPS</td>
<td>1232 GB/s</td>
<td>1.0x</td>
</tr>
<tr>
<td>4096</td>
<td>7168</td>
<td>16384</td>
<td>1358 TFLOPS</td>
<td>343 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>4096</td>
<td>4096</td>
<td>7168</td>
<td>1304 TFLOPS</td>
<td>500 GB/s</td>
<td>1.1x</td>
</tr>
<tr>
<td>4096</td>
<td>7168</td>
<td>2048</td>
<td>1025 TFLOPS</td>
<td>697 GB/s</td>
<td>1.1x</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h3 tabindex="-1" dir="auto">Grouped GEMMs for MoE models (contiguous layout)</h3><a id="user-content-grouped-gemms-for-moe-models-contiguous-layout" aria-label="Permalink: Grouped GEMMs for MoE models (contiguous layout)" href="#grouped-gemms-for-moe-models-contiguous-layout"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>#Groups</th>
<th>M per group</th>
<th>N</th>
<th>K</th>
<th>Computation</th>
<th>Memory bandwidth</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>4</td>
<td>8192</td>
<td>4096</td>
<td>7168</td>
<td>1297 TFLOPS</td>
<td>418 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>4</td>
<td>8192</td>
<td>7168</td>
<td>2048</td>
<td>1099 TFLOPS</td>
<td>681 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>8</td>
<td>4096</td>
<td>4096</td>
<td>7168</td>
<td>1288 TFLOPS</td>
<td>494 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>8</td>
<td>4096</td>
<td>7168</td>
<td>2048</td>
<td>1093 TFLOPS</td>
<td>743 GB/s</td>
<td>1.1x</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h3 tabindex="-1" dir="auto">Grouped GEMMs for MoE models (masked layout)</h3><a id="user-content-grouped-gemms-for-moe-models-masked-layout" aria-label="Permalink: Grouped GEMMs for MoE models (masked layout)" href="#grouped-gemms-for-moe-models-masked-layout"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>#Groups</th>
<th>M per group</th>
<th>N</th>
<th>K</th>
<th>Computation</th>
<th>Memory bandwidth</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1024</td>
<td>4096</td>
<td>7168</td>
<td>1233 TFLOPS</td>
<td>924 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>1</td>
<td>1024</td>
<td>7168</td>
<td>2048</td>
<td>925 TFLOPS</td>
<td>968 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>2</td>
<td>512</td>
<td>4096</td>
<td>7168</td>
<td>1040 TFLOPS</td>
<td>1288 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>2</td>
<td>512</td>
<td>7168</td>
<td>2048</td>
<td>916 TFLOPS</td>
<td>1405 GB/s</td>
<td>1.2x</td>
</tr>
<tr>
<td>4</td>
<td>256</td>
<td>4096</td>
<td>7168</td>
<td>932 TFLOPS</td>
<td>2064 GB/s</td>
<td>1.1x</td>
</tr>
<tr>
<td>4</td>
<td>256</td>
<td>7168</td>
<td>2048</td>
<td>815 TFLOPS</td>
<td>2047 GB/s</td>
<td>1.2x</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>


<ul dir="auto">
<li>Hopper architecture GPUs, <code>sm_90a</code> must be supported</li>
<li>Python 3.8 or above</li>
<li>CUDA 12.3 or above
<ul dir="auto">
<li><strong>But we highly recommend 12.8 or above for the best performance</strong></li>
</ul>
</li>
<li>PyTorch 2.1 or above</li>
<li>CUTLASS 3.6 or above (could be cloned by Git submodule)</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="# Submodule must be cloned
git clone --recursive git@github.com:deepseek-ai/DeepGEMM.git

# Make symbolic links for third-party (CUTLASS and CuTe) include directories
python setup.py develop

# Test JIT compilation
python tests/test_jit.py

# Test all GEMM implements (normal, contiguous-grouped and masked-grouped)
python tests/test_core.py"><pre><span><span>#</span> Submodule must be cloned</span>
git clone --recursive git@github.com:deepseek-ai/DeepGEMM.git

<span><span>#</span> Make symbolic links for third-party (CUTLASS and CuTe) include directories</span>
python setup.py develop

<span><span>#</span> Test JIT compilation</span>
python tests/test_jit.py

<span><span>#</span> Test all GEMM implements (normal, contiguous-grouped and masked-grouped)</span>
python tests/test_core.py</pre></div>


<p dir="auto">Then, import <code>deep_gemm</code> in your Python project, and enjoy!</p>


<p dir="auto">This library exclusively contains GEMM kernels. It requires the LHS scaling factor to be TMA-aligned and transposed, and it only supports the NT format (non-transposed LHS and transposed RHS). For transposition or other FP8 casting operations, please implement or fuse them into prior kernels independently. While the library provides some simple PyTorch utility functions, these may result in slower performance, but our primary focus is on optimizing the GEMM kernels themselves.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Normal dense GEMMs (non-grouped)</h4><a id="user-content-normal-dense-gemms-non-grouped" aria-label="Permalink: Normal dense GEMMs (non-grouped)" href="#normal-dense-gemms-non-grouped"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To perform a basic non-grouped FP8 GEMM, call the <code>deep_gemm.gemm_fp8_fp8_bf16_nt</code> function. For more details, please refer to the function documentation.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Grouped GEMMs (contiguous layout)</h4><a id="user-content-grouped-gemms-contiguous-layout" aria-label="Permalink: Grouped GEMMs (contiguous layout)" href="#grouped-gemms-contiguous-layout"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Unlike traditional grouped GEMMs in CUTLASS, DeepGEMM groups only the M-axis, while N and K must remain fixed. This design is tailored for scenarios where experts in an MoE model share the same shape.</p>
<p dir="auto">For training forward passes or inference prefilling, where each expert may process a varying number of tokens, we concatenate these tokens into a single tensor, referred to as the &#34;contiguous&#34; layout. Note that each expert segment must be aligned to the GEMM M block size (<code>get_m_alignment_for_contiguous_layout()</code>).</p>
<p dir="auto">For more information, please refer to the <code>m_grouped_gemm_fp8_fp8_bf16_nt_contiguous</code> function documentation.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Grouped GEMMs (masked layout)</h4><a id="user-content-grouped-gemms-masked-layout" aria-label="Permalink: Grouped GEMMs (masked layout)" href="#grouped-gemms-masked-layout"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">During the inference decoding phase, when CUDA graph is enabled and the CPU is unaware of the number of tokens each expert receives, we support masked grouped GEMMs. By providing a mask tensor, the kernel computes only the valid portions.</p>
<p dir="auto">Use <code>m_grouped_gemm_fp8_fp8_bf16_nt_masked</code> for this purpose and consult the relevant documentation. An example usage is to use the output of low-latency kernels from <a href="https://github.com/deepseek-ai/DeepEP">DeepEP</a> as input.</p>

<p dir="auto">The library provides some utility functions besides the above kernels:</p>
<ul dir="auto">
<li><code>deep_gemm.set_num_sms</code>: set the maximum SM count to use</li>
<li><code>deep_gemm.get_num_sms</code>: get the current SM maximum count</li>
<li><code>deep_gemm.get_m_alignment_for_contiguous_layout</code>: get the group-level alignment requirement for grouped contiguous layout</li>
<li><code>deep_gemm.get_tma_aligned_size</code>: get the required TMA alignment size</li>
<li><code>deep_gemm.get_col_major_tma_aligned_tensor</code>: get a column-major TMA-aligned tensor</li>
</ul>
<p dir="auto">The library also provides some environment variables, which may be useful:</p>
<ul dir="auto">
<li><code>DG_CACHE_DIR</code>: string, the cache directory to store compiled kernels, <code>$HOME/.deep_gemm</code> by default</li>
<li><code>DG_NVCC_COMPILER</code>: string, specified NVCC compiler path; will find in <code>from torch.utils.cpp_extension.CUDA_HOME</code> by default</li>
<li><code>DG_DISABLE_FFMA_INTERLEAVE</code>: 0 or 1, disable FFMA-interleaving optimization</li>
<li><code>DG_PTXAS_VERBOSE</code>: 0 or 1, show detailed PTXAS compiler output</li>
<li><code>DG_PRINT_REG_REUSE</code>: 0 or 1, print FFMA-interleaving details</li>
<li><code>DG_JIT_PRINT_NVCC_COMMAND</code>: 0 or 1, print NVCC compilation command</li>
<li><code>DG_JIT_DEBUG</code>: 0 or 1, print more debugging information</li>
</ul>
<p dir="auto">For additional examples and details, please refer to <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/tests/test_core.py">the test code</a> or review the corresponding Python documentation.</p>

<p dir="auto">We indicate the techniques excluded from CUTLASS with 🐳.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Persistent warp-specialization</h4><a id="user-content-persistent-warp-specialization" aria-label="Permalink: Persistent warp-specialization" href="#persistent-warp-specialization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Following the CUTLASS design, the kernels in DeepGEMM are warp-specialized, enabling overlapping data movement, tensor-core MMA instructions, and CUDA-core promotion. A simplified figure illustrating this process is shown below:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepGEMM/blob/main/figures/design.png"><img src="https://github.com/deepseek-ai/DeepGEMM/raw/main/figures/design.png" alt="design"/></a></p>

<p dir="auto">The <a href="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html#tensor-memory-accelerator" rel="nofollow">Tensor Memory Accelerator</a> (TMA) is a new hardware feature introduced by the Hopper architecture, designed for faster and asynchronous data movement. Specifically, we utilize TMA for:</p>
<ul dir="auto">
<li>TMA load for LHS, LHS scaling factors, and RHS matrices</li>
<li>TMA store for the output matrix</li>
<li>TMA multicast (exclusive to the LHS matrix)</li>
<li>TMA descriptor prefetching</li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto">Common detail optimizations</h4><a id="user-content-common-detail-optimizations" aria-label="Permalink: Common detail optimizations" href="#common-detail-optimizations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>Utilization of the <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-store-instruction-stmatrix" rel="nofollow"><code>stmatrix</code></a> PTX instruction</li>
<li><a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#miscellaneous-instructions-setmaxnreg" rel="nofollow">Register count control</a> tailored for different warpgroups</li>
<li>Overlapping as much as possible, e.g. overlapping TMA store and non-TMA RHS scaling factor load 🐳</li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto">A unified and optimized block scheduler</h4><a id="user-content-a-unified-and-optimized-block-scheduler" aria-label="Permalink: A unified and optimized block scheduler" href="#a-unified-and-optimized-block-scheduler"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/include/deep_gemm/scheduler.cuh">One scheduler</a> for all non-grouped and grouped kernels</li>
<li><a href="https://github.com/NVIDIA/cutlass/blob/eefa171318b79cbe2e78514d4cce5cd0fe919d0c/media/docs/efficient_gemm.md#threadblock-rasterization">Rasterization</a> to enhance L2 cache reuse</li>
</ul>

<p dir="auto">DeepGEMM employs a fully <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/jit">Just-In-Time</a> (JIT) design, with no compilation required at installation. All kernels are compiled at runtime using a lightweight JIT implementation. This approach offers several advantages:</p>
<ul dir="auto">
<li>GEMM shapes, block sizes, and the number of pipeline stages are treated as compile-time constants
<ul dir="auto">
<li>Saving registers</li>
<li>Compilers may do more optimizations</li>
</ul>
</li>
<li>Automatic selection of block sizes, number of warpgroups, optimal pipeline stages, and TMA cluster size
<ul dir="auto">
<li>But without auto-tuning, the optimal one is deterministically selected</li>
</ul>
</li>
<li>Full unrolling of the MMA pipelines, providing compilers with more optimization opportunities
<ul dir="auto">
<li>Very important for small shapes</li>
<li>Refer to <code>launch_k_iterations</code> in <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/include/deep_gemm/fp8_gemm.cuh">the kernel file</a> for details</li>
</ul>
</li>
</ul>
<p dir="auto">Overall, JIT significantly improves performance for small shapes, similar to the approach of the <a href="https://github.com/triton-lang/triton/">Triton</a> compiler.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Unaligned block sizes 🐳</h4><a id="user-content-unaligned-block-sizes-" aria-label="Permalink: Unaligned block sizes 🐳" href="#unaligned-block-sizes-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For certain shapes, block sizes aligned to powers of 2 can lead to underutilized SMs. For instance, with <code>M=256, N=7168</code>, a typical block size assignment of <code>BLOCK_M=128, BLOCK_N=128</code> results in only <code>(256 / 128) * (7168 / 128) = 112</code> out of 132 SMs being utilized. To address this, we support unaligned block sizes like 112, enabling <code>(256 / 128) * (7168 / 112) = 128</code> SMs to work in such scenarios. Implementing this technique alongside fine-grained scaling requires careful optimization but ultimately delivers performance gains.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">FFMA SASS interleaving 🐳</h4><a id="user-content-ffma-sass-interleaving-" aria-label="Permalink: FFMA SASS interleaving 🐳" href="#ffma-sass-interleaving-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We observe a performance improvement in <a href="https://github.com/NVIDIA/cutlass/tree/main/examples/54_hopper_fp8_warp_specialized_gemm">the CUTLASS FP8 kernel</a> between NVCC 12.2 and 12.3. By comparing the compiled SASS, we discover that one bit in <a href="https://github.com/NVIDIA/cutlass/blob/eefa171318b79cbe2e78514d4cce5cd0fe919d0c/include/cutlass/gemm/collective/fp8_accumulation.hpp#L73">a series of <code>FADD</code> instructions</a> is flipped in an interleaving pattern.
After referencing some open-source <a href="https://github.com/cloudcores/CuAssembler/blob/96a9f72baf00f40b9b299653fcef8d3e2b4a3d49/CuAsm/CuControlCode.py#L46">CUDA assembler</a> implementations, we identified that this bit controls <code>yield</code>, which may enhance warp-level parallelism (just a guess, yielding the current warp and let other warps work).</p>
<p dir="auto">To leverage this, we develop <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/deep_gemm/jit/interleave_ffma.py">a similar script</a> to modify the <code>FFMA</code> instructions in the compiled binary. Besides simply modifying the <code>yield</code> bit, we also flip the <code>reuse</code> bit (registers cannot be reused if the warp is yielded). This adjustment improves performance (10%+ in some cases) for fine-grained scaling FP8 GEMMs by creating more opportunities to overlap MMA instructions with promotion <code>FFMA</code> instructions.</p>

<p dir="auto">DeepGEMM is inspired by the <a href="https://github.com/nvidia/cutlass">CUTLASS</a> project. Thanks and respect to the developers!</p>

<p dir="auto">This code repository is released under <a href="https://github.com/deepseek-ai/DeepGEMM/blob/main/LICENSE">the MIT License</a>.</p>

<div dir="auto" data-snippet-clipboard-copy-content="@misc{deepgemm2025,
      title={DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling}, 
      author={Chenggang Zhao and Liang Zhao and Jiashi Li and Zhean Xu},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/DeepGEMM}},
}"><pre><span>@misc</span>{<span>deepgemm2025</span>,
      <span>title</span>=<span><span>{</span>DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Chenggang Zhao and Liang Zhao and Jiashi Li and Zhean Xu<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
      <span>howpublished</span> = <span><span>{</span>\url{https://github.com/deepseek-ai/DeepGEMM}<span>}</span></span>,
}</pre></div>
</article></div></div>
  </body>
</html>
