<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://binarygecko.com/race-conditions-in-linux-kernel-perf-events/">Original</a>
    <h1>Race conditions in Linux Kernel perf events</h1>
    
    <div id="readability-page-1" class="page"><div data-id="6bf9706" data-element_type="container" data-settings="{&#34;_ob_use_container_extras&#34;:&#34;no&#34;,&#34;_ob_column_hoveranimator&#34;:&#34;no&#34;,&#34;_ob_glider_is_slider&#34;:&#34;no&#34;,&#34;_ob_column_has_pseudo&#34;:&#34;no&#34;}"><div><div data-id="612f2b0" data-element_type="widget" data-settings="{&#34;_ob_postman_use&#34;:&#34;no&#34;,&#34;_ob_perspektive_use&#34;:&#34;no&#34;,&#34;_ob_poopart_use&#34;:&#34;yes&#34;,&#34;_ob_shadough_use&#34;:&#34;no&#34;,&#34;_ob_allow_hoveranimator&#34;:&#34;no&#34;,&#34;_ob_widget_stalker_use&#34;:&#34;no&#34;}" data-widget_type="theme-post-content.default"><div><div data-elementor-type="wp-post" data-elementor-id="813" data-elementor-post-type="post"><div data-id="337a5bd5" data-element_type="container" data-settings="{&#34;_ob_use_container_extras&#34;:&#34;no&#34;,&#34;_ob_column_hoveranimator&#34;:&#34;no&#34;,&#34;_ob_glider_is_slider&#34;:&#34;no&#34;,&#34;_ob_column_has_pseudo&#34;:&#34;no&#34;}"><div><div data-id="6c12509c" data-element_type="widget" data-settings="{&#34;_ob_use_harakiri&#34;:&#34;yes&#34;,&#34;_ob_harakiri_writing_mode&#34;:&#34;inherit&#34;,&#34;_ob_postman_use&#34;:&#34;no&#34;,&#34;_ob_perspektive_use&#34;:&#34;no&#34;,&#34;_ob_poopart_use&#34;:&#34;yes&#34;,&#34;_ob_shadough_use&#34;:&#34;no&#34;,&#34;_ob_allow_hoveranimator&#34;:&#34;no&#34;,&#34;_ob_widget_stalker_use&#34;:&#34;no&#34;}" data-widget_type="text-editor.default"><div><h3><span>Foreword;</span></h3><p><span>We disclosed this vulnerability to the kernel security team through responsible disclosure.</span></p><p><span>We are publishing the vulnerability to demonstrate that it is fully exploitable and to ensure that the technical details are available.</span></p><p><span>No CVE number has been assigned yet, as per the kernel teams policy CVEs are only issued once a fix is available and rolled out.</span></p><p><span>The vulnerability seems to have been <strong>introduced</strong> in the <strong>4.1 kernel</strong>, when <code>aux</code> buffers were added to <code>perf</code></span><em> </em><span><code>event</code>s. This makes it roughly 9 years old.</span></p><p><span>We will demonstrate that the vulnerability is exploitable on a pre-patch vanilla kernel.</span></p><p><span>In particular as long as <code>check_pages_enabled</code> is true, the exploit strategy laid out in this blog post will not work.</span></p><p><span>The vulnerability itself <strong>does affect major distributions</strong>, but we are not publishing a blueprint for how to perform that exploit.</span></p><p><span><strong>Debian-based distributions</strong>, as well as <strong>Android</strong> and any <strong>virtual machines</strong> are <strong>not affected</strong>.</span></p><p><span>The rest of this blog post concerns the technical details of the bug and how to exploit it.</span></p><h3><span>What are perf events?</span></h3><p><span><code>perf_events</code> is a kernel subsystem intended for performance measurement of various aspects of a system.</span></p><p><span>The <code>struct perf_event</code> is returned to the user mode process as a file descriptor.</span></p><p><span>Each <code>event</code> can have an associated <code>struct perf_buffer</code> ringbuffer, which is called <code>rb</code> on the <code>event</code> struct. This ringbuffer can either be created by <code>mmap</code>ing an <code>event</code> that doesn’t currently have an <code>rb</code>, or a different <code>event</code>s ringbuffer can be assigned to <code>rb</code> through the <code>PERF_EVENT_IOC_SET_OUTPUT</code> <code>ioctl</code>.</span></p><h3><span>perf_buffer struct</span></h3><p><span>The <code>struct perf_buffer</code> data structure is used to keep track of the memory of a buffer as well as multiple counters.</span></p><p><span><a href="https://elixir.bootlin.com/linux/v6.10.6/source/kernel/events/internal.h#L13">Source</a></span></p><pre><span><code>struct perf_buffer {
    refcount_t            refcount;
    /*[...] Snip for length*/
    int                nr_pages;    /* nr of data pages  */
    /*[...] Snip for length*/
    /* poll crap */
    spinlock_t            event_lock;
    struct list_head        event_list;

    atomic_t            mmap_count;
    unsigned long            mmap_locked;
    struct user_struct        *mmap_user;

    /* AUX area */
    /*[...] Snip for length*/
    unsigned long            aux_pgoff;
    int                aux_nr_pages;
    int                aux_overwrite;
    atomic_t            aux_mmap_count;
    /*[...] Snip for length*/
    refcount_t            aux_refcount;
    int                aux_in_sampling;
    void                **aux_pages;
    void                *aux_priv;

    struct perf_event_mmap_page    *user_page;
    void                *data_pages[];
};
</code></span></pre><p><span>Most of the fields are self explanatory, so we will just give brief additional information on the ones that are relevant for this post.</span></p><ul><li><ul></ul></li></ul><ul><li><ul><li><span><code>mmap_count</code> keeps track of how many <code>vma</code>s map an event that has this buffer.</span></li></ul></li></ul><ul><li><ul><li><span><code>aux_nr_pages</code> is also used to check if <code>perf_buffer</code> has an <code>aux</code> buffer.</span></li></ul></li></ul><ul><li><ul><li><span><code>aux_mmap_count</code> keeps track of how many <code>vma</code>s map an event that has this aux buffer.</span></li></ul></li></ul><ul><li><ul><li><span><code>user_page</code> points to a special page that the user space can write to, to communicate ring buffer state and configuration.</span></li></ul></li></ul><h3><span>Creating a ring buffer</span></h3><p><span>To create an <code>rb</code> on an <code>event</code>, the <code>event</code> must be <code>mmap</code>ed.</span></p><p><span>Specifically we need that <code>pg_off==0</code> and that the number of pages we are asking for is one page larger than a power of two.</span></p><p><span><em>It should be noted that the <code>vma</code> is a mapping of the <code>struct perf_event</code> and not of the <code>struct perf_buffer</code></em></span></p><p><span>The mapped memory starts with the mapping of the <code>user_page</code> and is followed by the data pages.</span></p><p><span>If an <code>rb</code> already exists, a new <code>struct perf_buffer</code> is <strong>not</strong> created and the <code>rb-&gt;mmap_count</code> is incremented instead.</span></p><h3><span>Aux buffers</span></h3><p><span>For some <code>PMU</code>s, ring buffers can have an additional <code>aux</code> auxilliary buffer. The <code>aux buffer</code> information is embedded in the <code>perf_buffer</code> struct.</span></p><p><span>To create an <code>aux</code> buffer, its parameters must first be configured using the <code>user_page</code> (namely <code>aux_offset</code> and <code>aux_size</code>) and can then be <code>mmap</code>ed with the configured offset and size.</span></p><p><span>There are additional checks in place to make sure the <code>aux buffer</code> and the original buffer don’t overlap.</span></p><h3><span>What can you do with a <code>perf_event</code> and ringbuffers</span></h3><p><span>A <code>perf_event</code> fd has the following interactions with ringbuffers.</span></p><ul><li><ul></ul></li></ul><ul><li><ul><li><span>mmap</span><ul><li><ul></ul></li></ul><ul><li><ul><li><span>Create a <code>perf_buffer</code> <code>rb</code> if the event doesn’t have an <code>rb</code> already and <code>pgoff==0</code></span></li></ul></li></ul><ul><li><ul><li><span>Map an existing <code>perf_buffer</code> <code>rb</code> if the event already has an rb and the mmap request matches it.</span></li></ul></li></ul><ul><li><ul><li><span>Create an <code>aux</code> buffer for the <code>rb</code> if the <code>user_page</code> has set it up and <code>pgoff</code> matches the configured aux_offset.</span></li></ul></li></ul><ul><li><ul><li><span>Map an existing <code>aux</code> buffer, if the <code>rb</code> already has an <code>aux</code> buffer and the mmap request matches it.</span></li></ul></li></ul></li></ul></li></ul><ul><li><ul><li><span>ioctl</span><ul><li><ul></ul></li></ul><ul><li><ul><li><span>Assign another events <code>perf_buffer</code> to self, if own <code>event-&gt;mmap_count</code> is 0.</span></li></ul></li></ul></li></ul></li></ul><h3><span>event-&gt;mmap_mutex</span></h3><p><span>The <code>vma</code> stores a reference to a <code>struct perf_event</code>, which in turn points to a <code>struct perf_buffer</code> which is called <code>rb</code> in the code below.</span></p><p><span>In it the code keeps track of what has been closed and adjusts the appropriate counters.</span></p><p><span><a href="https://elixir.bootlin.com/linux/v6.10.6/source/kernel/events/core.c#L6373">Source</a></span></p><pre><span><code>    if (rb_has_aux(rb) &amp;&amp; vma-&gt;vm_pgoff == rb-&gt;aux_pgoff &amp;&amp;
        atomic_dec_and_mutex_lock(&amp;rb-&gt;aux_mmap_count, &amp;event-&gt;mmap_mutex)) {
        /*[...] Snip for length */

        /* this has to be the last one */
        rb_free_aux(rb);
        WARN_ON_ONCE(refcount_read(&amp;rb-&gt;aux_refcount));

        mutex_unlock(&amp;event-&gt;mmap_mutex);
    }
</code></span></pre><p><span><code>rb_has_aux(rb)</code> simply checks if <code>rb-&gt;aux_nr_pages != 0</code>.</span></p><p><span>While the lock is taken, some counters are adjusted and eventually <code>rb_free_aux</code> is called, which will free all of the memory associated with the aux buffer.</span></p><h3><span>The issue</span></h3><p><span>The problem is that the lock that is taken is on <code>event</code> rather than <code>rb</code>.</span></p><p><span>Since multiple <code>events</code> can point to the same <code>struct perf_buffer</code> the <code>event-&gt;mmap_mutex</code> does not actually prevent concurrent accesses to the <code>rb</code>.</span></p><p><span>In the case of the <code>rb</code> itself this turns out not to be an issue because you can only forward to a <code>buffer</code> that already exists. But this is not the case for the <code>aux buffer</code>.</span></p><h3><span>What can we do with it</span></h3><p><span>To get an idea of how we can use this issue to our advantage, let’s take a closer look at <code>perf_mmap</code>. I’ll provide multiple code snippets to walk you through the logic.</span></p><p><span><a href="https://elixir.bootlin.com/linux/v6.10.6/source/kernel/events/core.c#L6491">Source</a></span></p><pre><span><code>static int perf_mmap(struct file *file, struct vm_area_struct *vma)
{
    struct perf_event *event = file-&gt;private_data;
    unsigned long user_locked, user_lock_limit;
    struct user_struct *user = current_user();
    struct perf_buffer *rb = NULL;
    /* [...] Snip for length*/

    vma_size = vma-&gt;vm_end - vma-&gt;vm_start;

    if (vma-&gt;vm_pgoff == 0) {
        nr_pages = (vma_size / PAGE_SIZE) - 1;
    } else {
        /*
         * AUX area mapping: if rb-&gt;aux_nr_pages != 0, it&#39;s already
         * mapped, all subsequent mappings should have the same size
         * and offset. Must be above the normal perf buffer.
         */
        u64 aux_offset, aux_size;

        if (!event-&gt;rb)
            return -EINVAL;
</code></span></pre><p><span>If the requested <code>vm_pgoff</code> isn’t 0 we assume that it’s a request for an <code>aux</code> buffer. We can only create an aux buffer if an <code>rb</code> already exists.</span></p><pre><span><code>        nr_pages = vma_size / PAGE_SIZE;

        mutex_lock(&amp;event-&gt;mmap_mutex);
        ret = -EINVAL;

        rb = event-&gt;rb;
        if (!rb)
            goto aux_unlock;
</code></span></pre><p><span>We calculate the number of pages and then take the <strong><code>event-&gt;mmap_mutex</code></strong> lock. Afterwards we check if the event still has an <code>rb</code>. We then assign to the <strong>local <code>rb</code> variable</strong>, this is important for a case distinction later in the code.</span></p><pre><span><code>        aux_offset = READ_ONCE(rb-&gt;user_page-&gt;aux_offset);
        aux_size = READ_ONCE(rb-&gt;user_page-&gt;aux_size);

        if (aux_offset &lt; perf_data_size(rb) + PAGE_SIZE)
            goto aux_unlock;
</code></span></pre><p><span>We get the configured parameters from the user page and we check that the configured offset is actually behind the already existing buffer.</span></p><pre><span><code>    /*[...] Snipped a bunch of mundance checks on the pg_off and size*/

        if (!atomic_inc_not_zero(&amp;rb-&gt;mmap_count))
            goto aux_unlock;

        if (rb_has_aux(rb)) {
            atomic_inc(&amp;rb-&gt;aux_mmap_count);
            ret = 0;
            goto unlock;
        }
</code></span></pre><p><span>We increment the <code>rb-&gt;mmap_count</code> and if there already is an <code>aux buffer</code> we increment <code>rb-&gt;aux_mmap_count</code> and return.</span></p><pre><span><code>        atomic_set(&amp;rb-&gt;aux_mmap_count, 1);
        user_extra = nr_pages;

        goto accounting;
    }
</code></span></pre><p><span>If there isn’t an <code>aux buffer</code> already we set <code>rb-&gt;aux_mmap_count</code> to 1 and jump forward.</span></p><pre><span><code>    /*[...]Snip rb stuff before accounting label*/
accounting:
    /*[...]Snip a bunch of rlimit checking to make sure we are allowed to reserve that amount of memory*/

    if (!rb) {
    /*[...] Snip rb setup code*/
    } else {
        ret = rb_alloc_aux(rb, event, vma-&gt;vm_pgoff, nr_pages,
                   event-&gt;attr.aux_watermark, flags);
        if (!ret)
            rb-&gt;aux_mmap_locked = extra;
    }
</code></span></pre><p><span>This is the case distinction mentioned earlier.</span></p><p><span>The rest of the function just increments <code>event-&gt;mmap_count</code> and sets up the <code>vma</code> flags and <code>vm_ops</code>.</span></p><p><span>Because all of this is protected by the <code>event-&gt;mmap_lock</code> we can execute it concurrently.</span></p><h3><span>Getting an “orphaned” aux vma</span></h3><p><span>Our first goal is to get a <code>vma</code> that has the correct <code>size</code> and <code>pgoff</code> to be an aux buffer, while <code>aux_nr_pages</code> is zero.</span></p><p><span>Note that we can’t do the race in the same process, since the <code>mm-&gt;lock</code> is held during <code>mmap</code>.</span></p><p><span>If we <code>mmap</code> an existing <code>aux buffer</code> it will just increment <code>rb-&gt;aux_mmap_count</code> and succeed. The way that <code>perf_mmap</code> checks if there already is an <code>aux buffer</code> is by checking if <code>aux_nr_pages</code> is not zero.</span></p><p><span>If we take a quick look at <code>__rb_free_aux</code> it goes through all pages and frees them and only sets <code>aux_nr_pages</code> to zero at the very end.</span></p><p><span><a href="https://elixir.bootlin.com/linux/v6.10.6/source/kernel/events/ring_buffer.c#L648">Source</a></span></p><pre><span><code>static void __rb_free_aux(struct perf_buffer *rb)
{
    /*[...] Snip for length*/

    if (rb-&gt;aux_nr_pages) {
        for (pg = 0; pg &lt; rb-&gt;aux_nr_pages; pg++)
            rb_free_aux_page(rb, pg);

        kfree(rb-&gt;aux_pages);
        rb-&gt;aux_nr_pages = 0;
    }
}
</code></span></pre><p><span>Because it goes through all of the pages first, we can make the time that <code>__rb_free_aux</code> is in flight almost arbitrarily large.</span></p><p><span>That means that we can race <code>perf_mmap_close</code> of the <code>aux buffer</code> against <code>perf_mmap</code>.</span></p><p><span>If <code>perf_mmap_close</code> reduces <code>rb-&gt;aux_mmap_count</code> to 0 it will call <code>__rb_free_aux</code>. But <code>perf_mmap</code> doesn’t check if <code>rb-&gt;aux_mmap_count</code> is zero and will just increment the refcount as long as <code>rb-&gt;aux_nr_pages</code> is not zero.</span></p><p><span>If we win the race we will have a <code>vma</code> with an <code>aux buffer</code> <code>pgoff</code>, even though <code>rb-&gt;aux_nr_pages</code> has been set to 0.</span></p><h4><span>Race Oracle</span></h4><p><span>With a little bit of trickery we can determine if we were too early or too late or if we just won the race.</span></p><p><span>To do this we need the following setup:</span></p><ul><li><ul></ul></li></ul><ul><li><ul><li><span><code>event1</code></span><ul><li><ul></ul></li></ul><ul><li><ul><li><span>supports aux</span></li></ul></li></ul><ul><li><ul><li><span>already has an <code>rb</code></span></li></ul></li></ul><ul><li><ul><li><span>already has an aux buffer</span></li></ul></li></ul></li></ul></li></ul><ul><li><ul><li><span><code>event2</code></span><ul><li><ul></ul></li></ul><ul><li><ul><li><span>does <strong>not</strong> support aux</span></li></ul></li></ul><ul><li><ul><li><span>forwarded to <code>event1</code>s <code>rb</code></span></li></ul></li></ul></li></ul></li></ul><p><span>We will race the mmap of <code>event2</code> against the <code>munmap</code> of the <code>aux buffer</code> of <code>event1</code>.</span></p><p><span>If we are <strong>too early</strong>, then <code>event2</code> increments <code>rb-&gt;aux_mmap_count</code> before <code>perf_mmap_close</code> decrements it, so the <code>aux buffer</code> is never freed. So our <code>mmap</code> succeeds. If we are <strong>too late</strong>, then <code>rb-&gt;aux_nr_pages</code> was already 0 and <code>event2</code> will try to create an <code>aux buffer</code> and <code>mmap</code> will fail with <code>-EOPNOTSUPP</code>. If we <strong>win</strong> the race, then our <code>mmap</code> succeeds just like in the <em>too early</em> case, but if we try to access any of the memory in the <code>vma</code> we will get a <code>SIGBUS</code> since the <code>aux buffer</code> isn’t backed by memory.</span></p><p><span>So our oracle works like this:</span></p><p><span>Because we have an oracle we can very reliably find the correct timing for the race condition.</span></p><h3><span>What do we gain from the “orphaned” vma?</span></h3><p><span>Let’s first take a quick look at <code>perf_mmap_fault</code>. This is the fault handler that is responsible for handling page faults in the <code>perf_event</code> <code>vma</code>s.</span></p><p><span><a href="https://elixir.bootlin.com/linux/v6.10.6/source/kernel/events/core.c#L6209">Source</a></span></p><pre><span><code>static vm_fault_t perf_mmap_fault(struct vm_fault *vmf)
{
    struct perf_event *event = vmf-&gt;vma-&gt;vm_file-&gt;private_data;
    struct perf_buffer *rb;
    vm_fault_t ret = VM_FAULT_SIGBUS;

    /*[...] Snip some code related to write protecting vma pages*/

    vmf-&gt;page = perf_mmap_to_page(rb, vmf-&gt;pgoff);
    if (!vmf-&gt;page)
        goto unlock;

    get_page(vmf-&gt;page);
    vmf-&gt;page-&gt;mapping = vmf-&gt;vma-&gt;vm_file-&gt;f_mapping;
    vmf-&gt;page-&gt;index   = vmf-&gt;pgoff;

    ret = 0;
    /*[...]Snip rcu_unlock*/
    return ret;
}
</code></span></pre><p><span>The page fault handler basically looks up the associated page of the buffer and then inserts it into the vma. You may have noticed that it doesn’t take any locks associated with the <code>rb</code>.</span></p><p><span>Here is the <code>perf_mmap_to_page</code> function.</span></p><p><span><a href="https://elixir.bootlin.com/linux/v6.10.6/source/kernel/events/ring_buffer.c#L960">Source</a></span></p><pre><span><code>struct page *
perf_mmap_to_page(struct perf_buffer *rb, unsigned long pgoff)
{
    if (rb-&gt;aux_nr_pages) {
        /* above AUX space */
        if (pgoff &gt; rb-&gt;aux_pgoff + rb-&gt;aux_nr_pages)
            return NULL;

        /* AUX space */
        if (pgoff &gt;= rb-&gt;aux_pgoff) {
            int aux_pgoff = array_index_nospec(pgoff - rb-&gt;aux_pgoff, rb-&gt;aux_nr_pages);
            return virt_to_page(rb-&gt;aux_pages[aux_pgoff]);
        }
    }

    return __perf_mmap_to_page(rb, pgoff);
}
</code></span></pre><p><span>You can see that as long as <code>rb-&gt;aux_nr_pages</code> isn’t zero, we can still access the <code>aux buffer</code> pages.</span></p><p><span>This wouldn’t normally be a problem, since we can’t fault in <code>aux buffer</code> pages if we don’t have a <code>vma</code> that corresponds to the <code>aux buffer</code>. And the <code>aux buffer</code> should only be freed if <code>rb-&gt;aux_mmap_count</code> becomes zero.</span></p><p><span>But because of our orphaned <code>aux buffer</code> <code>vma</code> we now have a <code>vma</code> that has the correct <code>pgoff</code> to access the <code>aux buffer</code> but isn’t accounted for by <code>rb-&gt;aux_mmap_count</code>.</span></p><p><span>(Well, it technically <em>is</em> accounted for at this stage. But as soon as we create a new <code>aux buffer</code> – which we can do because <code>aux_nr_pages</code> is zero – it will set <code>aux_mmap_count</code> back to <code>1</code>, making the <em>orphaned</em> vma unaccounted for.)</span></p><h3><span>Stealing pages</span></h3><p><span>By racing against the freeing of the pages we are able to get <strong>several pages</strong> mapped in our process that have already been returned to the <code>page allocator</code>. This means we already have a <code>page reuse</code> primitive at this stage. The problem is that we aren’t allowed to write to any of the pages in an <code>aux buffer</code>.</span></p><p><span>But since we <strong>are allowed to write</strong> to the <code>user page</code>, we can spray <code>struct event_buffer</code> with just one page – the <code>user page</code>.</span></p><p><span>The spray is very nice, because we can simply fill each freshly allocated <code>user page</code> with data and then scan all of the <em>“stolen”</em> pages to work out if we found a match and which <em>stolen</em> page it corresponds to.</span></p><p><span>Once we have found enough pages, we then remove them from the readonly <em>orphaned</em> <code>aux buffer</code> mapping and can unmap the <code>user page</code> in our spray with <code>madvise</code> <code>MADV_DONTNEED</code>.</span></p><p><span>Because the <code>refcount</code> dropped to zero, the page is returned to the <code>page allocator</code> once again – even though we still have a reference to it – but this time it is writable.</span></p><p><span>At this stage we have a <strong>writable</strong> page reuse primitive.</span></p><h3><span>Why it doesn’t work on most distros</span></h3><p><span><strong>On most (or even all) distributions</strong> this <strong>strategy doesn’t work</strong>. If <code>check_pages_enabled</code> is true the <code>page allocator</code> will perform several sanity checks for every page it returns. And the pages that we <em>“stole”</em> have a non-zero <code>refcount</code>.</span></p><p><span>This is what that looks like:</span></p><pre><span><code>Aug 05 11:59:33 archlinux kernel: BUG: Bad page map in process exploit  pte:8000000378b4f025 pmd:3bb7ff067
Aug 05 11:59:33 archlinux kernel: page: refcount:1 mapcount:-1 mapping:00000000dbe5efca index:0x751 pfn:0x378b4f
Aug 05 11:59:33 archlinux kernel: aops:anon_aops.0 ino:836 dentry name:&#34;inotify&#34;
Aug 05 11:59:33 archlinux kernel: flags: 0x2ffff8000000004(referenced|node=0|zone=2|lastcpupid=0x1ffff)
Aug 05 11:59:33 archlinux kernel: raw: 02ffff8000000004 dead000000000100 dead000000000122 ffff975cc0585d50
Aug 05 11:59:33 archlinux kernel: raw: 0000000000000751 0000000000000000 00000001fffffffe 0000000000000000
Aug 05 11:59:33 archlinux kernel: page dumped because: bad pte
</code></span></pre><p><span>In that case the <code>page allocator</code> will <strong>skip over</strong> any pages that don’t pass the sanity checks, so the <em>stolen</em> pages will never be returned by the <code>page allocator</code> for any allocation.</span></p><p><span><em>It is possible to get around this in a scenario where another process allocates the page in the time window between the page being returned by <code>__rb_free_aux</code> and us stealing the page. But in that case the <code>refcount</code> never becomes zero, so we can not perform the pivot to a writable mapping. Making it most likely only a KASLR bypass.</em></span></p><h3><span>Demo</span></h3><p><span>But on a fresh <strong>vanilla kernel</strong> compiled with a default config these checks are <strong>not enabled</strong>.</span></p><p><span>Below you can see the output of our proof of concept.</span></p><pre><span><code>/ # /mnt/host/exploit
[+] Opening event fd
[+] Found aux mappable pmu: t=1, c=2
[+] Opened event fds
[+] redirected event2-&gt;event1
[P] Started
[C] Started
[C] Won stage 1 race
[C] Unmap Fault Started
[P] Unmap Started
[C] Won stage 2 race
[C] Caught sigbus, correcting offset
[P] Starting the spray
[C] Scanning 2339 pages, range 0x7fcb1d263000-0x7fcb1db86000
[C] Unmapping readable pages with writable counterpart
[P] Found 33 writable page(s) in 49 iterations
[C] Spinning
[P] Doing refcount manipulation
[P] Checking for change and spraying
[P] Writable page changed
dump@0x7fcb1db46000
dump: +0000: 0000000000000000000000000000000010FFFF42FFFFFFFF10FFFF42FFFFFFFF000000000000000028FFFF42FFFFFFFF28FFFF42FFFFFFFF0000000000000000
dump: +0040: 40FFFF42FFFFFFFF40FFFF42FFFFFFFF000000000000000010000000100000001000000001000000010000000200000001000000010000000000000000000000
dump: +0080: 00000000000000000000000000000000000000000000000000FFFF40FFFFFFFF601305FFFFFFFFFF000000000000000000000000000000000000000000000000
dump: +00c0: 00000000000000000000000000000000FFFFFF42FFFFFFFFFFFFFF42FFFFFFFF0000000000000000FFFFFF42FFFFFFFFFFFFFF42FFFFFFFF0000000000000000
dump: +0100: 00FFFF42FFFFFFFF00FFFF42FFFFFFFF000000000000000010000000100000001000000001000000010000000200000001000000010000000000000000000000
dump: +0140: 00000000000000000000000000000000000000000000000000FFFF40FFFFFFFF601305FFFFFFFFFF000000000000000000000000000000000000000000000000
dump: +0180: 00000000000000000000000000000000FFFFFF42FFFFFFFFFFFFFF42FFFFFFFF0000000000000000FFFFFF42FFFFFFFFFFFFFF42FFFFFFFF0000000000000000
dump: +01c0: FFFFFF42FFFFFFFFFFFFFF42FFFFFFFF000000000000000010000000100000001000000001000000010000000200000001000000010000000000000000000000
dump: +0200: 00000000000000000000000000000000000000000000000000FFFF40FFFFFFFF601305FFFFFFFFFF000000000000000000000000000000000000000000000000
dump: +0240: 0000000000000000000000000000000050FFFF42FFFFFFFF50FFFF42FFFFFFFF000000000000000068FFFF42FFFFFFFF68FFFF42FFFFFFFF0000000000000000
dump: +0280: FFFFFF42FFFFFFFFFFFFFF42FFFFFFFF000000000000000010000000100000001000000001000000010000000200000001000000010000000000000000000000
dump: +02c0: 00000000000000000000000000000000000000000000000000FFFF40FFFFFFFF601305FFFFFFFFFF000000000000000000000000000000000000000000000000
dump: +0300: 0000000000000000000000000000000010FFFF42FFFFFFFF10FFFF42FFFFFFFF000000000000000028FFFF42FFFFFFFF28FFFF42FFFFFFFF0000000000000000
dump: +0340: 40FFFF42FFFFFFFF40FFFF42FFFFFFFF000000000000000010000000100000001000000001000000010000000200000001000000010000000000000000000000
dump: +0380: 00000000000000000000000000000000000000000000000000FFFF40FFFFFFFF601305FFFFFFFFFF000000000000000000000000000000000000000000000000
dump: +03c0: 00000000000000000000000000000000FFFFFF42FFFFFFFFFFFFFF42FFFFFFFF0000000000000000FFFFFF42FFFFFFFFFFFFFF42FFFFFFFF0000000000000000
[P] Main thread done
[P] Spinning
</code></span></pre><h3><span>Try it yourself</span></h3><p><span>The <strong>PoC</strong> described in this blog post can be found <span><a href="https://github.com/Binary-Gecko/perf_PoC">here</a>.</span></span></p><p><span>If you plan to experiment with this bug it is <strong>HIGHLY</strong> recommended to patch the kernel so that you can do so in a <code>virtual machine</code>. You will most likely completely freeze the kernel if anything goes wrong with your exploit.</span></p><p><span><strong>Written by Nils Ole Timm, @Firzen14</strong></span></p></div></div></div></div></div></div></div></div></div></div>
  </body>
</html>
