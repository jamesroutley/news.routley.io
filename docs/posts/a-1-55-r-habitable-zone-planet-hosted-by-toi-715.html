<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://academic.oup.com/mnras/article/527/1/35/7172075">Original</a>
    <h1>A 1.55 RâŠ• habitable-zone planet hosted by TOI-715</h1>
    
    <div id="readability-page-1" class="page"><div id="post_body_2083775">
    
      <div><p>I&#39;ve always been interested in <a href="https://ezzeriesa.notion.site/Paradigm-shifts-1547eaa9a46f4b2da04a4dfbb204f0c3">paradigm shifts</a>. Some reflect major shifts, some others are smaller scale.</p><p>I came across the <a href="https://github.com/jxnl/instructor">instructor</a> library while looking around at tooling around LLMs. The premise is simple - if we now send prompts to language models and get a text blob back, how do we figure out if the text blob has the right structure? For example, how do we make sure the JSON parses correctly or has the right keys?<br/></p><p>With instructor, pydantic is <a href="https://www.youtube.com/watch?v=yj-wSRJwrrc">all you need</a>. You get data model validation, and outputs JSONSchema which is what OpenAI function calling uses. It&#39;s a nice intersection of LLMs and type theory.</p></div>
    
  </div></div>
  </body>
</html>
