<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://z.ai/blog/glm-4.7">Original</a>
    <h1>GLM-4.7: Advancing the Coding Capability</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><strong>GLM-4.7</strong>, your new coding partner, is coming with the following features:</p><ul><li><strong>Core Coding:</strong> GLM-4.7 brings clear gains, compared to its predecessor GLM-4.6, in multilingual agentic coding and terminal-based tasks, including (73.8%, +5.8%) on SWE-bench, (66.7%, +12.9%) on SWE-bench Multilingual, and (41%, +16.5%) on Terminal Bench 2.0. GLM-4.7 also supports thinking before acting, with significant improvements on complex tasks in mainstream agent frameworks such as Claude Code, Kilo Code, Cline, and Roo Code.</li><li><strong>Vibe Coding:</strong> GLM-4.7 takes a major step forward in UI quality. It produces cleaner, more modern webpages and generates better-looking slides with more accurate layout and sizing.</li><li><strong>Tool Using:</strong> GLM-4.7 achieves significantly improvements in Tool using. Significant better performances can be seen on benchmarks such as τ^2-Bench and on web browsing via BrowseComp.</li><li><strong>Complex Reasoning:</strong> GLM-4.7 delivers a substantial boost in mathematical and reasoning capabilities, achieving (42.8%, +12.4%) on the HLE (Humanity’s Last Exam) benchmark compared to GLM-4.6.</li></ul><p>You can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.</p></div><p><img src="https://z-cdn-media.chatglm.cn/prompts-rich-media-resources/4.7-blog/20251223-004432.png"/></p><p><strong>Benchmark Performance.</strong> More detailed comparisons of GLM-4.7 with other models GPT-5, GPT-5.1-High, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2, Kimi K2 Thinking, on 17 benchmarks (including 8 reasoning, 5 coding, and 3 agents benchmarks) can be seen in the below table.</p><div><div><table><thead><tr><th>Benchmark</th><th>GLM-4.7</th><th>GLM-4.6</th><th>Kimi K2 Thinking</th><th>DeepSeek-V3.2</th><th>Gemini 3.0 Pro</th><th>Claude Sonnet 4.5</th><th>GPT-5 High</th><th>GPT-5.1 High</th></tr></thead><tbody><tr><td colspan="9">Reasoning</td></tr><tr><td>MMLU-Pro</td><th>84.3</th><th>83.2</th><th>84.6</th><th>85.0</th><th>90.1</th><th>88.2</th><th>87.5</th><th>87.0</th></tr><tr><td>GPQA-Diamond</td><th>85.7</th><th>81.0</th><th>84.5</th><th>82.4</th><th>91.9</th><th>83.4</th><th>85.7</th><th>88.1</th></tr><tr><td>HLE</td><th>24.8</th><th>17.2</th><th>23.9</th><th>25.1</th><th>37.5</th><th>13.7</th><th>26.3</th><th>25.7</th></tr><tr><td>HLE (w/ Tools)</td><th>42.8</th><th>30.4</th><th>44.9</th><th>40.8</th><th>45.8</th><th>32.0</th><th>35.2</th><th>42.7</th></tr><tr><td>AIME 2025</td><th>95.7</th><th>93.9</th><th>94.5</th><th>93.1</th><th>95.0</th><th>87.0</th><th>94.6</th><th>94.0</th></tr><tr><td>HMMT Feb. 2025</td><th>97.1</th><th>89.2</th><th>89.4</th><th>92.5</th><th>97.5</th><th>79.2</th><th>88.3</th><th>96.3</th></tr><tr><td>HMMT Nov. 2025</td><th>93.5</th><th>87.7</th><th>89.2</th><th>90.2</th><th>93.3</th><th>81.7</th><th>89.2</th><th>-</th></tr><tr><td>IMOAnswerBench</td><th>82.0</th><th>73.5</th><th>78.6</th><th>78.3</th><th>83.3</th><th>65.8</th><th>76.0</th><th>-</th></tr><tr><td>LiveCodeBench-v6</td><th>84.9</th><th>82.8</th><th>83.1</th><th>83.3</th><th>90.7</th><th>64.0</th><th>87.0</th><th>87.0</th></tr><tr><td colspan="9">Code Agent</td></tr><tr><td>SWE-bench Verified</td><th>73.8</th><th>68.0</th><th>71.3</th><th>73.1</th><th>76.2</th><th>77.2</th><th>74.9</th><th>76.3</th></tr><tr><td>SWE-bench Multilingual</td><th>66.7</th><th>53.8</th><th>61.1</th><th>70.2</th><th>-</th><th>68.0</th><th>55.3</th><th>-</th></tr><tr><td>Terminal Bench Hard</td><th>33.3</th><th>23.6</th><th>30.6</th><th>35.4</th><th>39.0</th><th>33.3</th><th>30.5</th><th>43.0</th></tr><tr><td>Terminal Bench 2.0</td><th>41.0</th><th>24.5</th><th>35.7</th><th>46.4</th><th>54.2</th><th>42.8</th><th>35.2</th><th>47.6</th></tr><tr><td colspan="9">General Agent</td></tr><tr><td>BrowseComp</td><th>52.0</th><th>45.1</th><th>-</th><th>51.4</th><th>-</th><th>24.1</th><th>54.9</th><th>50.8</th></tr><tr><td>BrowseComp (w/ Context Manage)</td><th>67.5</th><th>57.5</th><th>60.2</th><th>67.6</th><th>59.2</th><th>-</th><th>-</th><th>-</th></tr><tr><td>BrowseComp-ZH</td><th>66.6</th><th>49.5</th><th>62.3</th><th>65.0</th><th>-</th><th>42.4</th><th>63.0</th><th>-</th></tr><tr><td>τ²-Bench</td><th>87.4</th><th>75.2</th><th>74.3</th><th>85.3</th><th>90.7</th><th>87.2</th><th>82.4</th><th>82.7</th></tr></tbody></table></div></div><div><p><strong>Coding:</strong> AGI is a long journey, and benchmarks are only one way to evaluate performance. While the metrics provide necessary checkpoints, the most important thing is still how it *feels*. True intelligence isn&#39;t just about acing a test or processing data faster; ultimately, the success of AGI will be measured by how seamlessly it integrates into our lives-<strong>&#34;coding&#34;</strong> this time.</p></div><div><div><p>Frontend Development Showcases</p><div><div><p><span>Prompt</span><span><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M6 9l6 6 6-6"></path></svg></span><span>build a html website, High-contrast dark mode + bold condensed headings + animated ticker + chunky category chips + magnetic CTA.<!-- --> <a href="https://chat.z.ai/s/23c15468-405d-4993-9110-cffa99f79acb" target="_blank">View full trajectory at Z.ai<img alt="" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" src="https://z-cdn.chatglm.cn/z-blog/z-icon.svg"/></a></span></p></div></div></div><div><p>Artifacts Showcases</p><div><div><p><span>Prompt</span><span><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M6 9l6 6 6-6"></path></svg></span><span>Design a richly crafted voxel-art environment featuring an ornate pagoda set within a vibrant garden.
Include diverse vegetation—especially cherry blossom trees—and ensure the composition feels lively, colorful, and visually striking.
Use any voxel or WebGL libraries you prefer, but deliver the entire project as a single, self-contained HTML file that I can paste and open directly in Chrome<!-- --> <a href="https://chat.z.ai/s/aa6d8ace-1aec-45a1-bf0c-bb9dc9104072" target="_blank">View full trajectory at Z.ai<img alt="" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" src="https://z-cdn.chatglm.cn/z-blog/z-icon.svg"/></a></span></p></div></div></div><div><p>Poster Showcases</p><div><div><p><span>Prompt</span><span><svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M6 9l6 6 6-6"></path></svg></span><span>Design a poster introducing Paris, with a romantic and fashionable aesthetic. The overall style should feel elegant, visually refined, and design-driven.<!-- --> <a href="https://chat.z.ai/s/83640645-5436-4069-a27b-f8094189d669" target="_blank">View full trajectory at Z.ai<img alt="" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" src="https://z-cdn.chatglm.cn/z-blog/z-icon.svg"/></a></span></p></div></div></div><div><p>Slides Creation Showcases</p></div></div><div><h2>Interleaved Thinking &amp; Preserved Thinking</h2><p>GLM-4.7 enhances <strong>Interleaved Thinking</strong>, a feature introduced since GLM-4.5, and further introduces <strong>Preserved Thinking</strong> and <strong>Turn-level Thinking</strong>. By thinking between actions and staying consistent across turns, it makes complex tasks more stable and more controllable:</p><ul><li><strong>Interleaved Thinking:</strong> GLM-4.7 thinks before every response and tool calling, improving instruction following and the quality of generation.</li><li><strong>Preserved Thinking:</strong> In coding agent scenarios, GLM-4.7 automatically retains all thinking blocks across multi-turn conversations, reusing the existing reasoning instead of re-deriving from scratch. This reduces information loss and inconsistencies, and is well-suited for long-horizon, complex tasks.</li><li><strong>Turn-level Thinking:</strong> GLM-4.7 supports per-turn control over reasoning within a session—disable thinking for lightweight requests to reduce latency/cost, enable it for complex tasks to improve accuracy and stability.</li></ul><p>More details:<!-- --> <a href="https://docs.z.ai/guides/capabilities/thinking-mode" target="_blank">https://docs.z.ai/guides/capabilities/thinking-mode</a></p></div><p><img src="https://z-cdn-media.chatglm.cn/prompts-rich-media-resources/4.7-blog/upload_058e166eb117f1c394d0505429b6248c.png"/></p><div><h2>Call GLM-4.7 API via<!-- --> <a href="https://z.ai/" target="_blank">Z.ai</a> <!-- -->API platform</h2><p>The<!-- --> <a href="https://z.ai/" target="_blank">Z.ai</a> <!-- -->API platform offers the GLM-4.7 model. For comprehensive API documentation and integration guidelines, please refer to<!-- --> <a href="https://docs.z.ai/guides/llm/glm-4.7" target="_blank">https://docs.z.ai/guides/llm/glm-4.7</a>. At the same time, the model is also available worldwide through OpenRouter (<a href="https://openrouter.ai/" target="_blank">https://openrouter.ai/</a>).</p><h2>Use GLM-4.7 with Coding Agents</h2><p>GLM-4.7 is now available to use within coding agents (Claude Code, Kilo Code, Roo Code, Cline and more).</p><p>For <strong>GLM Coding Plan subscribers</strong>: You&#39;ll be automatically upgraded to GLM-4.7. If you&#39;ve previously customized the app configs (like <code>~/.claude/settings.json</code> in Claude Code), simply update the model name to &#34;glm-4.7&#34; to complete the upgrade.</p><p>For <strong>New users</strong>: Subscribing GLM Coding Plan means having access to a Claude-level coding model at a fraction of the cost — just 1/7th the price with 3x the usage quota. Start building today:<!-- --> <a href="https://z.ai/subscribe" target="_blank">https://z.ai/subscribe</a>.</p><h2>Chat with GLM-4.7 on Z.ai</h2><p>GLM-4.7 is accessible through<!-- --> <a href="https://chat.z.ai" target="_blank">Z.ai</a>. Try to change the model option to <strong>GLM-4.7</strong>, if the system does not automatically do that (not like an AGI in that case :))</p><h2>Serve GLM-4.7 Locally</h2><p>Model weights for GLM-4.7 are publicly available on<!-- --> <a href="https://huggingface.co/zai-org/GLM-4.7" target="_blank">HuggingFace</a> <!-- -->and<!-- --> <a href="https://modelscope.cn/models/ZhipuAI/GLM-4.7" target="_blank">ModelScope</a>. For local deployment, GLM-4.7 supports inference frameworks including vLLM and SGLang. Comprehensive deployment instructions are available in the official GitHub repository.</p></div><div><h2>Footnotes</h2><p><em>1: Default settings (most tasks): temperature 1.0, top-p 0.95, max new tokens 131072. For multi-turn agentic tasks (τ²-Bench and Terminal Bench 2), enable Preserved Thinking mode.</em></p><p><em>2: Terminal Bench and SWE-bench Verified settings: temperature 0.7, top-p 1.0, max new tokens 16384.</em></p><p><em>3: τ²-Bench settings: temperature 0, max new tokens 16384. For τ²-Bench, we added an extra prompt in the Retail and Telecom interactions to avoid failures caused by users ending the interaction incorrectly; for the Airline domain, we applied the domain fixes proposed in the Claude Opus 4.5 release report.</em></p></div></div></div>
  </body>
</html>
