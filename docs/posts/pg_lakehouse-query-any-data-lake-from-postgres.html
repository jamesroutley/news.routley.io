<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/paradedb/paradedb/tree/dev/pg_lakehouse">Original</a>
    <h1>Pg_lakehouse: Query Any Data Lake from Postgres</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto"><code>pg_lakehouse</code> is an extension that transforms Postgres into an analytical query engine over object stores like S3 and table formats like Delta Lake. Queries are pushed down to <a href="https://github.com/apache/datafusion">Apache DataFusion</a>, which delivers excellent analytical performance. Combinations of the following object stores, table formats, and file formats are supported.</p>

<ul>
<li> Amazon S3</li>
<li> S3-compatible object stores (e.g. MinIO)</li>
<li> Local file system</li>
<li> Google Cloud Storage (coming soon)</li>
<li> Azure Blob Storage (coming soon)</li>
</ul>
<p dir="auto">...and potentially any service supported by <a href="https://opendal.apache.org/docs/category/services" rel="nofollow">Apache OpenDAL</a>. See the Development section for instructions on how to <a href="#adding-a-service">add a service</a>.</p>

<ul>
<li> Parquet</li>
<li> CSV</li>
<li> JSON</li>
<li> Avro</li>
<li> ORC (coming soon)</li>
</ul>

<ul>
<li> Delta Lake</li>
<li> Apache Iceberg (coming soon)</li>
</ul>
<p dir="auto"><code>pg_lakehouse</code> is supported on Postgres 14, 15, and 16. Support for Postgres 12 and 13 is coming soon.</p>

<p dir="auto">Today, a vast amount of non-operational data — events, metrics, historical snapshots, vendor data, etc. — is ingested into data lakes like S3. Querying this data by moving it into a cloud data warehouse or operating a new query engine is expensive and time consuming. The goal of <code>pg_lakehouse</code> is to enable this data to be queried directly from Postgres. This eliminates the need for new infrastructure, loss of data freshness, data movement, and non-Postgres dialects of other query engines.</p>
<p dir="auto"><code>pg_lakehouse</code> uses the foreign data wrapper (FDW) API to connect to any object store or table format and the executor hook API to push queries to DataFusion. While other FDWs like <code>aws_s3</code> have existed in the Postgres extension ecosystem, these FDWs suffer from two limitations:</p>
<ol dir="auto">
<li>Lack of support for most object stores, file, and table formats</li>
<li>Too slow over large datasets to be a viable analytical engine</li>
</ol>
<p dir="auto"><code>pg_lakehouse</code> differentiates itself by supporting a wide breadth of stores and formats (thanks to <a href="https://github.com/apache/opendal">OpenDAL</a>) and by being very fast (thanks to <a href="https://github.com/apache/datafusion">DataFusion</a>).</p>

<p dir="auto">The following example uses <code>pg_lakehouse</code> to query an example dataset of 3 million NYC taxi trips from January 2024, hosted in a public S3 bucket provided by ParadeDB.</p>
<div dir="auto" data-snippet-clipboard-copy-content="CREATE EXTENSION pg_lakehouse;
CREATE FOREIGN DATA WRAPPER s3_wrapper HANDLER s3_fdw_handler VALIDATOR s3_fdw_validator;

-- Provide S3 credentials
CREATE SERVER s3_server FOREIGN DATA WRAPPER s3_wrapper
OPTIONS (bucket &#39;paradedb-benchmarks&#39;, region &#39;us-east-1&#39;, allow_anonymous &#39;true&#39;);

-- Create foreign table
CREATE FOREIGN TABLE trips (
    &#34;VendorID&#34;              INT,
    &#34;tpep_pickup_datetime&#34;  TIMESTAMP,
    &#34;tpep_dropoff_datetime&#34; TIMESTAMP,
    &#34;passenger_count&#34;       BIGINT,
    &#34;trip_distance&#34;         DOUBLE PRECISION,
    &#34;RatecodeID&#34;            DOUBLE PRECISION,
    &#34;store_and_fwd_flag&#34;    TEXT,
    &#34;PULocationID&#34;          REAL,
    &#34;DOLocationID&#34;          REAL,
    &#34;payment_type&#34;          DOUBLE PRECISION,
    &#34;fare_amount&#34;           DOUBLE PRECISION,
    &#34;extra&#34;                 DOUBLE PRECISION,
    &#34;mta_tax&#34;               DOUBLE PRECISION,
    &#34;tip_amount&#34;            DOUBLE PRECISION,
    &#34;tolls_amount&#34;          DOUBLE PRECISION,
    &#34;improvement_surcharge&#34; DOUBLE PRECISION,
    &#34;total_amount&#34;          DOUBLE PRECISION
)
SERVER s3_server
OPTIONS (path &#39;s3://paradedb-benchmarks/yellow_tripdata_2024-01.parquet&#39;, extension &#39;parquet&#39;);

-- Success! Now you can query the remote Parquet file like a regular Postgres table
SELECT COUNT(*) FROM trips;
  count
---------
 2964624
(1 row)"><pre>CREATE EXTENSION pg_lakehouse;
CREATE FOREIGN DATA WRAPPER s3_wrapper HANDLER s3_fdw_handler VALIDATOR s3_fdw_validator;

<span><span>--</span> Provide S3 credentials</span>
CREATE SERVER s3_server FOREIGN DATA WRAPPER s3_wrapper
OPTIONS (bucket <span><span>&#39;</span>paradedb-benchmarks<span>&#39;</span></span>, region <span><span>&#39;</span>us-east-1<span>&#39;</span></span>, allow_anonymous <span><span>&#39;</span>true<span>&#39;</span></span>);

<span><span>--</span> Create foreign table</span>
CREATE FOREIGN TABLE trips (
    <span><span>&#34;</span>VendorID<span>&#34;</span></span>              <span>INT</span>,
    <span><span>&#34;</span>tpep_pickup_datetime<span>&#34;</span></span>  <span>TIMESTAMP</span>,
    <span><span>&#34;</span>tpep_dropoff_datetime<span>&#34;</span></span> <span>TIMESTAMP</span>,
    <span><span>&#34;</span>passenger_count<span>&#34;</span></span>       <span>BIGINT</span>,
    <span><span>&#34;</span>trip_distance<span>&#34;</span></span>         <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>RatecodeID<span>&#34;</span></span>            <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>store_and_fwd_flag<span>&#34;</span></span>    <span>TEXT</span>,
    <span><span>&#34;</span>PULocationID<span>&#34;</span></span>          <span>REAL</span>,
    <span><span>&#34;</span>DOLocationID<span>&#34;</span></span>          <span>REAL</span>,
    <span><span>&#34;</span>payment_type<span>&#34;</span></span>          <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>fare_amount<span>&#34;</span></span>           <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>extra<span>&#34;</span></span>                 <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>mta_tax<span>&#34;</span></span>               <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>tip_amount<span>&#34;</span></span>            <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>tolls_amount<span>&#34;</span></span>          <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>improvement_surcharge<span>&#34;</span></span> <span>DOUBLE PRECISION</span>,
    <span><span>&#34;</span>total_amount<span>&#34;</span></span>          <span>DOUBLE PRECISION</span>
)
SERVER s3_server
OPTIONS (<span>path</span> <span><span>&#39;</span>s3://paradedb-benchmarks/yellow_tripdata_2024-01.parquet<span>&#39;</span></span>, extension <span><span>&#39;</span>parquet<span>&#39;</span></span>);

<span><span>--</span> Success! Now you can query the remote Parquet file like a regular Postgres table</span>
<span>SELECT</span> <span>COUNT</span>(<span>*</span>) <span>FROM</span> trips;
  count
<span><span>--</span>-------</span>
 <span>2964624</span>
(<span>1</span> row)</pre></div>
<p dir="auto">Note that column names must be wrapped in double quotes to preserve uppercase letters. This is because DataFusion is case-sensitive and Postgres&#39; foreign table column names must match the foreign table&#39;s column names exactly.</p>

<p dir="auto">This extension uses Postgres hooks to intercept and push queries down to DataFusion. In order to enable these hooks, the extension must be added to <code>shared_preload_libraries</code> inside <code>postgresql.conf</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Inside postgresql.conf
shared_preload_libraries = &#39;pg_lakehouse&#39;"><pre><span><span>#</span> Inside postgresql.conf</span>
shared_preload_libraries = <span><span>&#39;</span>pg_lakehouse<span>&#39;</span></span></pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Inspecting the Foreign Schema</h2><a id="user-content-inspecting-the-foreign-schema" aria-label="Permalink: Inspecting the Foreign Schema" href="#inspecting-the-foreign-schema"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">The <code>arrow_schema</code> function displays the schema of a foreign table. This function is useful for verifying that the server and table credentials you&#39;ve provided are valid. If the connection is successful and <code>pg_lakehouse</code> is able to read the foreign data, a table will be returned with the <a href="https://docs.rs/datafusion/latest/datafusion/common/arrow/datatypes/enum.DataType.html" rel="nofollow">Arrow schema</a> of the foreign table. Otherwise, an empty table will be returned or an error will be thrown.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT * FROM arrow_schema(
  server =&gt; &#39;s3_server&#39;,
  path =&gt; &#39;s3://paradedb-benchmarks/yellow_tripdata_2024-01.parquet&#39;,
  extension =&gt; &#39;parquet&#39;
);"><pre><span>SELECT</span> <span>*</span> <span>FROM</span> arrow_schema(
  server <span>=&gt;</span> <span><span>&#39;</span>s3_server<span>&#39;</span></span>,
  <span>path</span> <span>=&gt;</span> <span><span>&#39;</span>s3://paradedb-benchmarks/yellow_tripdata_2024-01.parquet<span>&#39;</span></span>,
  extension <span>=&gt;</span> <span><span>&#39;</span>parquet<span>&#39;</span></span>
);</pre></div>
<p dir="auto">You can also use this function to decide what Postgres types to assign to each column of the foreign table. For instance, an Arrow <code>Utf8</code> datatype should map to a Postgres <code>TEXT</code>, <code>VARCHAR</code>, or <code>BPCHAR</code> column. If an incompatible Postgres type is chosen, querying the table will fail.</p>

<p dir="auto">To connect your own object store, please refer to the <a href="https://docs.paradedb.com/analytics/object_stores" rel="nofollow">documentation</a>.</p>

<p dir="auto">Some types like <code>date</code>, <code>timestamp</code>, and <code>timestamptz</code> must be handled carefully. Please refer to the <a href="https://docs.paradedb.com/analytics/schema#datetime-types" rel="nofollow">documentation</a>.</p>


<p dir="auto">To develop the extension, first install Rust via <code>rustup</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup install &lt;version&gt;

rustup default &lt;version&gt;"><pre>curl --proto <span><span>&#39;</span>=https<span>&#39;</span></span> --tlsv1.2 -sSf https://sh.rustup.rs <span>|</span> sh
rustup install <span>&lt;</span>version<span>&gt;</span>

rustup default <span>&lt;</span>version<span>&gt;</span></pre></div>
<p dir="auto">Note: While it is possible to install Rust via your package manager, we recommend using <code>rustup</code> as we&#39;ve observed inconcistencies with Homebrew&#39;s Rust installation on macOS.</p>
<p dir="auto">Then, install the PostgreSQL version of your choice using your system package manager. Here we provide the commands for the default PostgreSQL version used by this project:</p>

<div dir="auto" data-snippet-clipboard-copy-content="# macOS
brew install postgresql@16

# Ubuntu
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c &#39;echo &#34;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main&#34; &gt; /etc/apt/sources.list.d/pgdg.list&#39;
sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-16 postgresql-server-dev-16"><pre><span><span>#</span> macOS</span>
brew install postgresql@16

<span><span>#</span> Ubuntu</span>
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc <span>|</span> sudo apt-key add -
sudo sh -c <span><span>&#39;</span>echo &#34;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main&#34; &gt; /etc/apt/sources.list.d/pgdg.list<span>&#39;</span></span>
sudo apt-get update <span>&amp;&amp;</span> sudo apt-get install -y postgresql-16 postgresql-server-dev-16</pre></div>
<p dir="auto">If you are using Postgres.app to manage your macOS PostgreSQL, you&#39;ll need to add the <code>pg_config</code> binary to your path before continuing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export PATH=&#34;$PATH:/Applications/Postgres.app/Contents/Versions/latest/bin&#34;"><pre><span>export</span> PATH=<span><span>&#34;</span><span>$PATH</span>:/Applications/Postgres.app/Contents/Versions/latest/bin<span>&#34;</span></span></pre></div>

<p dir="auto">Then, install and initialize <code>pgrx</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Note: Replace --pg16 with your version of Postgres, if different (i.e. --pg15, --pg14, etc.)
cargo install --locked cargo-pgrx --version 0.11.3

# macOS arm64
cargo pgrx init --pg16=/opt/homebrew/opt/postgresql@16/bin/pg_config

# macOS amd64
cargo pgrx init --pg16=/usr/local/opt/postgresql@16/bin/pg_config

# Ubuntu
cargo pgrx init --pg16=/usr/lib/postgresql/16/bin/pg_config"><pre><span><span>#</span> Note: Replace --pg16 with your version of Postgres, if different (i.e. --pg15, --pg14, etc.)</span>
cargo install --locked cargo-pgrx --version 0.11.3

<span><span>#</span> macOS arm64</span>
cargo pgrx init --pg16=/opt/homebrew/opt/postgresql@16/bin/pg_config

<span><span>#</span> macOS amd64</span>
cargo pgrx init --pg16=/usr/local/opt/postgresql@16/bin/pg_config

<span><span>#</span> Ubuntu</span>
cargo pgrx init --pg16=/usr/lib/postgresql/16/bin/pg_config</pre></div>
<p dir="auto">If you prefer to use a different version of Postgres, update the <code>--pg</code> flag accordingly.</p>
<p dir="auto">Note: While it is possible to develop using pgrx&#39;s own Postgres installation(s), via <code>cargo pgrx init</code> without specifying a <code>pg_config</code> path, we recommend using your system package manager&#39;s Postgres as we&#39;ve observed inconsistent behaviours when using pgrx&#39;s.</p>

<p dir="auto"><code>pg_lakehouse</code> uses OpenDAL to integrate with various object stores. As of the time of writing, some — but not all — of the object stores supported by OpenDAL have been integrated.</p>
<p dir="auto">Adding support for a new object store is as straightforward as</p>
<ol dir="auto">
<li>Adding the service feature to <code>opendal</code> in <code>Cargo.toml</code>. For instance, S3 requires <code>services-s3</code>.</li>
<li>Creating a file in the <code>fdw/</code> folder that implements the <code>BaseFdw</code> trait. For instance, <code>fdw/s3.rs</code> implements the S3 FDW.</li>
<li>Registering the FDW in <code>fdw/handler.rs</code>.</li>
</ol>
</article></div></div>
  </body>
</html>
