<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/huggingface/smollm">Original</a>
    <h1>Full LLM training and evaluation toolkit</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device.</p>
<p dir="auto">You can find our most capable model <strong>ü§è SmolLM2-1.7B-Instruct</strong> <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct" rel="nofollow">here</a>.</p>
<p dir="auto"><strong>New: Introducing <a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk" rel="nofollow">SmolTalk</a>, the SFT dataset of SmolLM2 üöÄ</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/919b15524c95e8c27a14584c356023d8a4e234a595ebbc841abdae2e111c9549/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f79343568494d4e52455737775f58704859425f30712e706e67"><img src="https://camo.githubusercontent.com/919b15524c95e8c27a14584c356023d8a4e234a595ebbc841abdae2e111c9549/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f79343568494d4e52455737775f58704859425f30712e706e67" alt="Evaluation Results" width="600" data-canonical-src="https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/y45hIMNREW7w_XpHYB_0q.png"/></a></p>

<ol dir="auto">
<li><a href="#usage">Usage</a>
<ul dir="auto">
<li><a href="#transformers">Transformers</a></li>
<li><a href="#chat-in-trl">Chat in TRL</a></li>
<li><a href="#local-applications">Local applications</a></li>
</ul>
</li>
<li><a href="#smol-tools">Smol-tools</a></li>
<li><a href="#pre-training">Pre-training</a></li>
<li><a href="#fine-tuning">Fine-tuning</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#synthetic-data-pipelines">Synthetic data pipelines</a></li>
</ol>

<p dir="auto">Our most powerful model is <code>SmolLM2-1.7B-Instruct</code>, which you can use as an assistant with <code>transformers</code>, <code>trl</code>, or using quantized versions with tools like <code>llama.cpp</code>, <code>MLX</code>, and <code>transformers.js</code>. For lighter applications, you can also use the smaller models <code>SmolLM2-360M</code> and<code>SmolLM2-135M</code>, which are suitable for on-device usage and can be integrated similarly.
All available in this <a href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9" rel="nofollow">collection</a>.</p>


<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer
checkpoint = &#34;HuggingFaceTB/SmolLM2-1.7B-Instruct&#34;

device = &#34;cuda&#34; # for GPU usage or &#34;cpu&#34; for CPU usage
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=&#34;auto&#34;)`
model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)

messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Write a 100-word article on &#39;Benefits of Open-Source in AI research&#34;}]
input_text=tokenizer.apply_chat_template(messages, tokenize=False)
inputs = tokenizer.encode(input_text, return_tensors=&#34;pt&#34;).to(device)
outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)
print(tokenizer.decode(outputs[0]))"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>
<span>checkpoint</span> <span>=</span> <span>&#34;HuggingFaceTB/SmolLM2-1.7B-Instruct&#34;</span>

<span>device</span> <span>=</span> <span>&#34;cuda&#34;</span> <span># for GPU usage or &#34;cpu&#34; for CPU usage</span>
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>checkpoint</span>)
<span># for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=&#34;auto&#34;)`</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>checkpoint</span>).<span>to</span>(<span>device</span>)

<span>messages</span> <span>=</span> [{<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;Write a 100-word article on &#39;Benefits of Open-Source in AI research&#34;</span>}]
<span>input_text</span><span>=</span><span>tokenizer</span>.<span>apply_chat_template</span>(<span>messages</span>, <span>tokenize</span><span>=</span><span>False</span>)
<span>inputs</span> <span>=</span> <span>tokenizer</span>.<span>encode</span>(<span>input_text</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>).<span>to</span>(<span>device</span>)
<span>outputs</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>inputs</span>, <span>max_new_tokens</span><span>=</span><span>50</span>, <span>temperature</span><span>=</span><span>0.2</span>, <span>top_p</span><span>=</span><span>0.9</span>, <span>do_sample</span><span>=</span><span>True</span>)
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>outputs</span>[<span>0</span>]))</pre></div>

<p dir="auto">You can also use the TRL CLI to chat with the model from the terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install trl
trl chat --model_name_or_path HuggingFaceTB/SmolLM2-1.7B-Instruct --device cpu"><pre>pip install trl
trl chat --model_name_or_path HuggingFaceTB/SmolLM2-1.7B-Instruct --device cpu</pre></div>
<p dir="auto">You can find more details on how to leverage the model for use cases such as text summarization, text rewriting and function calling in the model card: <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct" rel="nofollow">https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct</a></p>

<p dir="auto">You can use the models locally with frameworks like <code>llama.cpp</code>, <code>MLX</code>, and <code>transformers.js</code>, which support SmolLM2.
All models are available in this <a href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9" rel="nofollow">collection</a>.</p>

<p dir="auto">A collection of lightweight AI-powered tools built with LLaMA.cpp and small language models. These tools are designed to run locally on your machine without requiring expensive GPU resources.
Further instructions on how to use the tools can be found in the <a href="https://github.com/huggingface/smollm/blob/main/smol_tools/README.md">smol-tools README</a>.</p>

<p dir="auto">You can find scripts for launching pre-training with <a href="https://github.com/huggingface/nanotron/">nanotron</a> under <a href="https://github.com/huggingface/smollm/blob/main/pre-training/README.md">pre-training</a>, we share the exact configs for training SmolLM1 and will upload SmolLM2&#39;s configs soon.</p>

<p dir="auto">You can find an example script to finetune SmolLM2 using <code>TRL</code> and <code>PEFT</code> in the <code>finetuning</code> folder. We also link to our post-training scripts for SmolLM2 using the alignment handbook.</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9390426af7c12fcab4f192c7f8d3414e3d22451cfe9b8241dc2e6d7331d0b493/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f542d63484a564137464261493063674441707a456a2e706e67"><img src="https://camo.githubusercontent.com/9390426af7c12fcab4f192c7f8d3414e3d22451cfe9b8241dc2e6d7331d0b493/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f542d63484a564137464261493063674441707a456a2e706e67" alt="image/png" data-canonical-src="https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/T-cHJVA7FBaI0cgDApzEj.png"/></a></p>
<p dir="auto">You can find more detailed evaluation of each model size in the model cards in this <a href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9" rel="nofollow">collection</a>.
We use <a href="https://github.com/huggingface/lighteval">lighteval</a> for all our evaluations, for more details refer to the <a href="https://github.com/huggingface/smollm/blob/main/evaluation/README.md">evaluation README</a>.</p>

<p dir="auto">We released <a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk" rel="nofollow">SmolTalk</a> the SFT dataset used for building SmolLM2 instruct models. It was created with <a href="https://github.com/argilla-io/distilabel">distilabel</a> and you can check and execute the synthetic data pipelines in <a href="https://github.com/huggingface/smollm/blob/main/distilabel_pipelines/README.md">distilabel_pipelines README</a></p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f7469fe83fb142b50df98d220431dc4c487f70ad946d2defeafa09be9e67b36d/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f4a4c5445626e7342515f71593033326d78467a67432e706e67"><img src="https://camo.githubusercontent.com/f7469fe83fb142b50df98d220431dc4c487f70ad946d2defeafa09be9e67b36d/68747470733a2f2f63646e2d75706c6f6164732e68756767696e67666163652e636f2f70726f64756374696f6e2f75706c6f6164732f3631633134313334326161633736346365313635346534332f4a4c5445626e7342515f71593033326d78467a67432e706e67" width="800" data-canonical-src="https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/JLTEbnsBQ_qY032mxFzgC.png"/></a></p><p dir="auto"><em>Comparison of models finetuned on SmolTalk and Orca AgentInstruct 1M. For more details, refer to the <a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk" rel="nofollow">dataset card</a>.</em></p>
</div>
</article></div></div>
  </body>
</html>
