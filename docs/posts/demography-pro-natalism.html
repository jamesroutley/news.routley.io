<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://archive.ph/4rjIt">Original</a>
    <h1>|Demography| Pro-natalism.</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h3 tabindex="-1" dir="auto"><a id="user-content-teach-gpts-to-read-api-documentation-using-llama-lora-and-langchain" aria-hidden="true" href="#teach-gpts-to-read-api-documentation-using-llama-lora-and-langchain"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Teach GPTs to read API documentation using LLaMA, LoRA, and Langchain.</h3>
<p dir="auto">Wouldn&#39;t it be great if GPTs could learn about new APIs? With LlamaAcademy you can teach GPTs to call Stripe, Notion, or even your own product&#39;s API. Instead of hosting API <em>documentation</em>, you can host an API <em>implementation</em>! Just point LlamaAcademy at your API docs, run the script, and -- <em>shazam</em>! -- a new LLaMA model will be created for you. You can host that model on your server, and users can call your bespoke mini-GPT to write their API glue.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-seriously" aria-hidden="true" href="#seriously"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Seriously?</h4>
<p dir="auto">Well, sort of. LlamaAcademy is experimental -- we haven&#39;t gotten it to consistently generate great code (yet). We&#39;d love help with that, if you&#39;re into that sort of thing.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-demo-a-llama-that-learned-notions-api" aria-hidden="true" href="#demo-a-llama-that-learned-notions-api"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Demo: A Llama That Learned Notion&#39;s API</h4>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description demo_api.mp4">demo_api.mp4</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/51882888/232329429-c7aadc40-8251-41f3-b4bb-9ac41ac2c6f8.mp4" data-canonical-src="https://user-images.githubusercontent.com/51882888/232329429-c7aadc40-8251-41f3-b4bb-9ac41ac2c6f8.mp4" controls="controls" muted="muted">

  </video>
</details>

<h2 tabindex="-1" dir="auto"><a id="user-content-how-it-works" aria-hidden="true" href="#how-it-works"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How it works</h2>
<p dir="auto">LlamaAcademy is a pipeline that combines the following steps: crawling, data generation using GPT3.5 and GPT4 and fine-tuning Vicuna-13B on synthetic data.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/danielgross/LlamaAcademy/blob/main/assets/data_generation.jpg"><img src="https://github.com/danielgross/LlamaAcademy/raw/main/assets/data_generation.jpg" alt="LlamaAcademy Data Generation"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-installation" aria-hidden="true" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<p dir="auto">You need to install firefox and Elinks, then install all necessary pythonic dependencies. You also need to input an OPENAI_KEY.</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get install firefox elinks
conda env create --file=environment.yaml
conda env config vars set OPENAI_API_KEY=YOUR_API_KEY"><pre>sudo apt-get install firefox elinks
conda env create --file=environment.yaml
conda env config vars <span>set</span> OPENAI_API_KEY=YOUR_API_KEY</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<p dir="auto">LlamaAcademy uses simple interface by abstracting every user hyper-parameters with configuration file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="GENERATE: True # Turn off if you don&#39;t want to generate the data
API_DOCS: https://developers.notion.com/reference 
DEPTH_CRAWLING: 1 # 0 if your API website is long and not hierarchical for example polygon.io. Otherwise, feel free to set, it might take much longer if your webiste has many children.
SUMMARIZE_DOCS: True
MICRO_BATCH_SIZE: 3  
BATCH_SIZE: 12
EPOCHS: 4  
LEARNING_RATE: 3e-4  
WARMUP_STEPS: 5
CUTOFF_LEN: 2048 
LORA_R: 8
LORA_ALPHA: 16
LORA_DROPOUT: 0.05
OPENAI_ENGINE: &#34;gpt-4&#34;
NUM_PROMPT_INSTRUCTIONS: 3
NUM_TASKS_TO_GENERATE: 200 # Recommended number of examples
DATA_PATH: &#34;assets/&#34;
OUTPUT_DIR: &#34;output/lora-vicuna-api-notion&#34;"><pre><span>GENERATE</span>: <span>True </span><span><span>#</span> Turn off if you don&#39;t want to generate the data</span>
<span>API_DOCS</span>: <span>https://developers.notion.com/reference </span>
<span>DEPTH_CRAWLING</span>: <span>1</span> <span><span>#</span> 0 if your API website is long and not hierarchical for example polygon.io. Otherwise, feel free to set, it might take much longer if your webiste has many children.</span>
<span>SUMMARIZE_DOCS</span>: <span>True</span>
<span>MICRO_BATCH_SIZE</span>: <span>3</span>  
<span>BATCH_SIZE</span>: <span>12</span>
<span>EPOCHS</span>: <span>4</span>  
<span>LEARNING_RATE</span>: <span>3e-4</span>  
<span>WARMUP_STEPS</span>: <span>5</span>
<span>CUTOFF_LEN</span>: <span>2048</span> 
<span>LORA_R</span>: <span>8</span>
<span>LORA_ALPHA</span>: <span>16</span>
<span>LORA_DROPOUT</span>: <span>0.05</span>
<span>OPENAI_ENGINE</span>: <span><span>&#34;</span>gpt-4<span>&#34;</span></span>
<span>NUM_PROMPT_INSTRUCTIONS</span>: <span>3</span>
<span>NUM_TASKS_TO_GENERATE</span>: <span>200</span> <span><span>#</span> Recommended number of examples</span>
<span>DATA_PATH</span>: <span><span>&#34;</span>assets/<span>&#34;</span></span>
<span>OUTPUT_DIR</span>: <span><span>&#34;</span>output/lora-vicuna-api-notion<span>&#34;</span></span></pre></div>
<p dir="auto">To run the fine-tuning process, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="CUDA_VISIBLE_DEVICES=0 python3 main.py --config configs/vicuna_13b.yaml"><pre>CUDA_VISIBLE_DEVICES=0 python3 main.py --config configs/vicuna_13b.yaml</pre></div>
<p dir="auto">After the training, run export LoRA model to HuggingFace weights by doing this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 export_hf.py --base_model jeffwan/vicuna-13b --model_folder output/lora-vicuna-api-notion"><pre>python3 export_hf.py --base_model jeffwan/vicuna-13b --model_folder output/lora-vicuna-api-notion</pre></div>
<p dir="auto">To run inference with LangChain:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 inference.py --model_folder output/lora-vicuna-api-notion"><pre>python3 inference.py --model_folder output/lora-vicuna-api-notion</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-hardware-requirements" aria-hidden="true" href="#hardware-requirements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Hardware requirements</h3>
<p dir="auto">This code is tested with 1 RTX A6000 instance in vast.ai (approximated 0.6$/1h). The peak VRAM is 27.8 GB, therefore, any GPU with VRAM &gt; 30GB will safe for fine-tuning.
The fine-tuning is done after 20 minutes with 100 examples, the data generation is completed after 1 hour (most of the time spent in GPT-4 instances generation and crawling process due to screen scraping is quite expensive).</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-plan" aria-hidden="true" href="#plan"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Plan</h2>
<ul>
<li> Implement (IA)^3 for few-shot fine-tuning.</li>
<li> Implement flash_attention.</li>
<li> Implement scratch-pad based GPT-4 agent to generate multi-turn planning and generating code.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-code-files" aria-hidden="true" href="#code-files"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Code Files</h2>
<p dir="auto">This repository provides the following Folders and Files</p>
<ul dir="auto">
<li><code>assets/</code>: The folder contains seed tasks + training URLs to generate the data (see self-instruct for more information).
<ul dir="auto">
<li><code>data.json</code>: generated data will be saved here for training.</li>
<li><code>generated_instructions.jsonl</code>: generated instructions for instruction tuning will be saved here.</li>
<li><code>training_urls.json</code>: common API for crawling and generating the training data (other direction).</li>
<li><code>seed_tasks.json</code>: human written seed tasks for self-instruct process (4-10 examples are recommended).</li>
<li><code>prompt_summary.txt</code>: prompt for GPT3.5-turbo extract and summarize the crawled API documents.</li>
<li><code>prompt_input_code.txt</code>: prompt for GPT4 generate code with references queried from the vector score.</li>
</ul>
</li>
<li><code>configs/</code>: The folder for the configuration files.</li>
<li><code>chain.py</code>: The file for custom Langchain pipeline and agents.</li>
<li><code>data_gen.py</code>: The file implementing data generation using GPT3.5, GPT4, Bing with different strategies.</li>
<li><code>main.py</code>: The main inteference file for user to customize their Alpaca to API references (scraping API references website, generating instruction-code pairs and fine-tuning Vicuna).</li>
<li><code>inference.py</code>: Allow user to inference with a trained model with a query related to the API (using Langchain + LlamaAcademy).</li>
<li><code>environment.yaml</code>: The file for the dependencies.</li>
<li><code>utils.py</code>: The file for the helper functions.</li>
<li><code>memorizing.py</code>: (Still under construction) Using <a href="https://arxiv.org/pdf/2203.08913.pdf" rel="nofollow">memory fine-tuning method</a> to force Vicuna to memorize API references without pre-training.</li>
<li><code>ingest_docs.py</code>: Implementing API references crawling using Elinks and Selenium.</li>
</ul>
</article>
          </div></div>
  </body>
</html>
