<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/punnerud/Local_Knowledge_Graph">Original</a>
    <h1>Knowledge graphs using Ollama and Embeddings to answer and visualizing queries</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/punnerud/Local_Knowledge_Graph/blob/main/example.png"><img src="https://github.com/punnerud/Local_Knowledge_Graph/raw/main/example.png" alt="Example"/></a></p>
<p dir="auto">This application uses a local Llama model to answer queries, build embeddings, and create a knowledge graph for exploring related questions and answers.</p>

<p dir="auto">The Local Knowledge Graph is a Flask-based web application that leverages a local Llama language model to process user queries, generate step-by-step reasoning, and visualize the thought process as an interactive knowledge graph. It also finds and displays related questions and answers based on semantic similarity.</p>

<ul dir="auto">
<li>Interactive web interface for submitting queries</li>
<li>Step-by-step reasoning process displayed in real-time</li>
<li>Dynamic knowledge graph visualization of the reasoning steps</li>
<li>Calculation and display of the strongest reasoning path</li>
<li>Related questions and answers based on semantic similarity</li>
<li>Local processing using a Llama language model</li>
</ul>

<ol dir="auto">
<li>Ensure you have all the required dependencies installed.</li>
<li>Start the Flask application by running <code>app.py</code>.</li>
<li>Open a web browser and navigate to <code>http://localhost:5100</code> (or the appropriate port if modified).</li>
<li>Enter your query in the input field and click &#34;Submit&#34;.</li>
<li>Watch as the application generates a step-by-step reasoning process, updating the knowledge graph in real-time.</li>
<li>Review the final answer and the strongest reasoning path.</li>
<li>Explore related questions and answers displayed below the main response.</li>
</ol>

<ul dir="auto">
<li>Python 3.7+</li>
<li>Flask</li>
<li>NumPy</li>
<li>scikit-learn</li>
<li>Annoy</li>
<li>NetworkX</li>
<li>A local Llama language model (e.g., llama3.1:8b) running on <code>http://localhost:11434</code></li>
</ul>

<ol dir="auto">
<li>Clone this repository.</li>
<li>Install the required Python packages using the requirements.txt file:
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
</li>
<li>Ensure you have a local Llama model running and accessible.</li>
<li>Run the Flask application:

</li>
</ol>

<p dir="auto">This application requires a local Llama language model to be running and accessible. Make sure you have the appropriate model set up and running before using this application.</p>
</article></div></div>
  </body>
</html>
