<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/Articles/949960/">Original</a>
    <h1>The BPF-programmable network device</h1>
    
    <div id="readability-page-1" class="page"><div>
<center>
           <div><b>Did you know...?</b><p>LWN.net is a subscriber-supported publication; we rely on subscribers
       to keep the entire operation going.  Please help out by <a href="https://lwn.net/subscribe/">buying a subscription</a> and keeping LWN on the
       net.</p></div>
           </center>
           <p>
Containers and virtual machines on Linux communicate with the world via
virtual network devices.  This arrangement makes the full power of the
Linux networking stack available, but it imposes the full overhead of that
stack as well.  Often, the routing of this networking traffic can be
handled with relatively simple logic; the BPF-programmable network device,
which was merged for the 6.7 kernel release, makes it possible to avoid
expensive network processing, in at least some cases.
</p><p>
When a guest (either a container or a virtual machine) sends data over the
network in current systems, that data first enters the network stack within
that guest, where it is formed into packets and sent out through the
virtual interface.  On the host side, that packet is received and handled,
once again within the network stack.  If the packet is destined for a peer
outside of the host, the packet will be routed to a (real) network
interface for retransmission.  The guest&#39;s data has made it into the world,
but only after having passed through two network stacks.
</p><p>
The new device, named &#34;netkit&#34;, aims to short out some of that overhead.
It is, in some sense, a typical virtual device in that a packet transmitted
at one end will only pass through the host system&#39;s memory before being
received at the other.  The difference is in how transmission works.  Every
network-interface driver provides a <a href="https://elixir.bootlin.com/linux/v6.6/source/include/linux/netdevice.h#L1057"><tt>net_device_ops</tt>
structure</a> containing a large number of function pointers — as many as
90 in the 6.6 kernel.  One of those is <tt>ndo_start_xmit()</tt>:
</p><pre>    netdev_tx_t	(*ndo_start_xmit)(struct sk_buff *skb, struct net_device *dev);
</pre>
<p>
This function&#39;s job is to initiate the transmission of the packet found in
<tt>skb</tt> by way of the indicated device <tt>dev</tt>.  In a typical
virtual device, this function will immediately &#34;receive&#34; the packet into
the network stack on the peer side with a call to a function like <a href="https://elixir.bootlin.com/linux/v6.6/source/net/core/dev.c#L5108"><tt>netif_rx()</tt></a>.
The netkit device, though, behaves a bit differently.
</p><p>
When this virtual interface is set up, it is possible to load one or more
BPF programs into each side of the interface.  Since netkit BPF programs
can affect traffic routing on the host side, only the host is allowed to
load these programs for either the host or the guest.  The
<tt>ndo_start_xmit()</tt> callback provided by netkit will, rather than
just passing the packet back into the network stack, invoke each of the
attached programs in sequence, passing the packet to each.  The BPF
programs are able to modify the packet (to change the destination device,
for example), and are expected to return a value saying what should be done
next:
</p><ul>
<li> <tt>NETKIT_NEXT</tt>: continue processing with the next BPF program
     in the series (if any).  If there are no more programs to invoke, this
     return is treated like <tt>NETKIT_PASS</tt>.
</li><li> <tt>NETKIT_PASS</tt>: immediately pass the packet into the receiving
     side&#39;s network stack without calling any other BPF programs.
</li><li> <tt>NETKIT_DROP</tt>: immediately drop the packet.
</li><li> <tt>NETKIT_REDIRECT</tt>: immediately redirect the packet to a new
     network device, queuing it for transmission without the need to pass
     through the host&#39;s network stack.
</li></ul>
<p>
Each interface can be configured with a default policy (either
<tt>NETKIT_PASS</tt> or <tt>NETKIT_DROP</tt>) that applies if there is no
BPF program loaded to make the decision.  Most of the time, the right
policy is probably to drop the packet, ensuring that no traffic leaks out
of the guest until the interface is fully configured to handle it.
</p><p>
There are performance gains to be had if the decision to drop a packet can
be made as soon as possible.  Unwanted network traffic can often come in
great quantities, so the less time spent on it, the better.  But, as <a href="https://git.kernel.org/linus/35dfaad7188c">the changelog</a> states,
the best performance gains may come from the ability to redirect packets
without re-entering the network stack:
</p><blockquote>
	For example, if the BPF program determines that the skb must be
	sent out of the node, then a redirect to the physical device can
	take place directly without going through per-CPU backlog
	queue. This helps to shift processing for such traffic from softirq
	to process context, leading to better scheduling
	decisions/performance.
</blockquote>
<p>
According to the <a href="http://vger.kernel.org/bpfconf2023_material/tcx_meta_netdev_borkmann.pdf">slides</a>
from a 2023 Linux Storage, Filesystem, Memory-Management and BPF Summit
talk, guests operating through the netkit device (which was called &#34;meta&#34;
at that time) are able to attain TCP data-transmission rates that are just
as high as can be had by running directly on the host. The performance
penalty for running within a guest has, in other words, been entirely
removed.
</p><p>
Given the potential performance gains for some users, it&#39;s not surprising
that this patch series, posted by Daniel Borkmann but also containing work
by Nikolay Aleksandrov, was merged quickly.  It was first <a href="https://lwn.net/ml/bpf/20230926055913.9859-1-daniel@iogearbox.net/">posted</a> to
the BPF mailing list on September 26, went through four revisions
there, then applied for the 6.7 merge window one month later.  This feature
will not be for all users but, for those who are deploying
network-intensive applications within containers or virtual machines, it
could be appealing indeed.<br clear="all"/></p><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Kernel/Index">Kernel</a></td><td><a href="https://lwn.net/Kernel/Index#BPF-Device_drivers">BPF/Device drivers</a></td></tr>
            <tr><td><a href="https://lwn.net/Kernel/Index">Kernel</a></td><td><a href="https://lwn.net/Kernel/Index#Networking-Performance">Networking/Performance</a></td></tr>
            </tbody></table></div></div>
  </body>
</html>
