<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.boundaryml.com/blog/structured-output-from-llms">Original</a>
    <h1>Every Way to Get Structured Output from LLMs</h1>
    
    <div id="readability-page-1" class="page"><div><p><em>By Sam Lijin - <a href="https://twitter.com/sxlijin">@sxlijin</a></em></p><p>This post will be interesting to you if:</p><ul>
<li>you&#39;re trying to get structured output from an LLM,</li>
<li>you&#39;ve tried <code>response_format: &#34;json&#34;</code> and function calling and been disappointed by the results,</li>
<li>you&#39;re tired of stacking regex on regex on regex to extract JSON from an LLM,</li>
<li>you&#39;re trying to figure out what your options are.</li>
</ul><p>Everyone using LLMs in production runs into this problem sooner or later: what
we really want is a magical black box that returns JSON in exactly the format we
want. Unfortunately, LLMs return English, not JSON, and it turns out that
converting English to JSON is kinda hard.</p><p>Here&#39;s every framework we could find that solves this problem, and how they compare.</p><p>(Disclaimer: as a player in this space, we&#39;re a little biased!)</p><h2 id="comparison"><a aria-hidden="true" tabindex="-1" href="#comparison"><span></span></a><a id="comparison">Comparison</a></h2><p>Note: we&#39;ve omitted LangChain from this list because we haven&#39;t heard of anyone
using it in production - look no further than <a href="https://www.reddit.com/r/LangChain/top/?t=all">the top posts of all time on
/r/LangChain</a>.</p><p><sup>*</sup>: Honorable mention to Microsoft&#39;s <a href="https://github.com/microsoft/aici">AICI</a>, which is
working on creating a shim for cooperative constraints implemented in
Python/JS using a WASM runtime. Haven&#39;t included it in the list because it
seems more low-level than the others, and setup is very involved.</p><p><sup>1</sup>: Applying constraints to OpenAI models can be very error-prone,
because the OpenAI API does not expose sufficient information about the
underlying model operations for the framework to actually apply constraints
effectively. See <a href="https://lmql.ai/docs/models/openai.html#openai-api-limitations">this discussion about
limitations</a>
from the LMQL documentation.</p><p><sup>2</sup>: Transformers refers to &#34;HuggingFace Transformers&#34;</p><p><sup>3</sup>: Constrained streaming generation produces partial objects, but
no good ways of interacting with the partial objects, since they are not yet
parse-able. We only consider a framework to support streaming if it allows
interacting with the partial objects (e.g. if streaming back an object with
properties <code>foo</code> and <code>bar</code>, you can access <code>obj.foo</code> before <code>bar</code> has been
streamed to the client).</p><h2 id="criteria"><a aria-hidden="true" tabindex="-1" href="#criteria"><span></span></a><a id="criteria">Criteria</a></h2><p>Most of our criteria are pretty self-explanatory, but there are two that we
want to call out:</p><h3 id="does-it-handleprevent-malformed-json-if-so-how"><a aria-hidden="true" tabindex="-1" href="#does-it-handleprevent-malformed-json-if-so-how"><span></span></a>Does it handle/prevent malformed JSON? If so, how?</h3><p>LLMs make a lot of the same mistakes that humans do when producing JSON
(e.g. a } in the wrong place or a missing comma), so
it&#39;s important that the framework can help you handle these errors.</p><p>A lot of frameworks &#34;solve&#34; this by feeding the malformed JSON back into the LLM
and asking it to repair the JSON. This kinda works, but it&#39;s also slow and
expensive. If your LLM calls individually take multiple seconds already, you
don&#39;t really want to make that even slower!</p><p>There are two techniques that exist for handling or preventing this: actually
parse the malformed JSON (BAML takes this approach) or constrain the LLM&#39;s token
generation to guarantee that valid JSON is produced (this is what Outlines,
Guidance, and a few others do).</p><p><strong>Parsing the malformed JSON is our preferred approach</strong>: it most closely aligns
with what the LLM was designed to do (emit tokens), is fast (takes microseconds),
and flexible (allows working with any LLM). It does have limitations: it can&#39;t
magically make sense of completely nonsensical JSON, after all.</p><p>Applying constraints to LLM token generation, by contrast, can be robust, but
has its own issues: doing this efficiently requires applying runtime transforms
to the model itself, so this only works with self-hosted models (e.g. Llama,
Transformers) and does not work with models like OpenAI&#39;s ChatGPT or Anthropic&#39;s
Claude.</p><h3 id="can-you-see-the-actual-prompt-do-you-have-full-control-over-the-prompt"><a aria-hidden="true" tabindex="-1" href="#can-you-see-the-actual-prompt-do-you-have-full-control-over-the-prompt"><span></span></a>Can you see the actual prompt? Do you have full control over the prompt?</h3><figure><img src="https://www.boundaryml.com/batman-show-me-the-prompt.jpeg"/><figcaption>You might remember this from <a href="https://hamel.dev/blog/posts/prompt/">&#34;Fuck You, Show Me The Prompt&#34;</a>.</figcaption></figure><p>Prompts are how we &#34;program&#34; LLMs to give us output.</p><p>The best way to get an LLM to return structured data is to craft a prompt
designed to return data matching your specific schema. To do that, you need to</p><ol>
<li>see the prompt actually getting sent to ChatGPT, and</li>
<li>try different prompts.</li>
</ol><p>Most frameworks, unfortunately, have hardcoded templates baked in which
prevent doing this.</p><h2 id="example-code"><a aria-hidden="true" tabindex="-1" href="#example-code"><span></span></a>Example code</h2><p>For each framework listed above, we&#39;ve included example code, from the framework&#39;s documentation,
provides for how you would use it.</p><hr/><h3 id="baml-python"><a aria-hidden="true" tabindex="-1" href="#baml-python"><span></span></a><a id="example-baml-python">BAML (Python)</a></h3><p>From <a href="https://github.com/BoundaryML/baml-examples/blob/main/fastapi-starter/fast_api_starter/app.py"><code>baml-examples/fastapi-starter/fast_api_starter/app.py</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark"><span data-line=""><span>from</span><span> baml_client </span><span>import</span><span> b</span></span>
<span data-line=""> </span>
<span data-line=""><span>resume </span><span>=</span><span> &#34;&#34;&#34;John Doe [...] Experience: Software Engineer Intern [...]&#34;&#34;&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> def</span><span> async_call</span><span>():</span></span>
<span data-line=""><span>  parsed </span><span>=</span><span> await</span><span> b.ExtractResume(resume)</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> def</span><span> streamed_call</span><span>():</span></span>
<span data-line=""><span>  stream </span><span>=</span><span> b.stream.ExtractResume(resume)</span></span>
<span data-line=""><span>  async</span><span> for</span><span> partial </span><span>in</span><span> stream:</span></span>
<span data-line=""><span>    print</span><span>(partial) </span><span># This is an object with auto complete for the partial Resume type</span></span>
<span data-line=""><span>  response </span><span>=</span><span> await</span><span> stream.get_final_result() </span><span># auto complete here to the full Resume type</span></span>
<span data-line=""> </span></code></pre></figure><p>From <a href="https://github.com/BoundaryML/baml-examples/blob/main/fastapi-starter/baml_src/extract_resume.baml"><code>baml-examples/fastapi-starter/baml_src/extract_resume.baml</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-dark"><code data-language="rust" data-theme="github-dark"><span data-line=""><span>class </span><span>Resume</span><span> {</span></span>
<span data-line=""><span>  name string</span></span>
<span data-line=""><span>  education </span><span>Education</span><span>[]</span></span>
<span data-line=""><span>  skills string[]</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>class </span><span>Education</span><span> {</span></span>
<span data-line=""><span>  school string</span></span>
<span data-line=""><span>  degree string</span></span>
<span data-line=""><span>  year int</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>function </span><span>ExtractResume</span><span>(raw_text</span><span>:</span><span> string) </span><span>-&gt;</span><span> Resume</span><span> {</span></span>
<span data-line=""><span>  client </span><span>GPT4</span></span>
<span data-line=""><span>  prompt #</span><span>&#34;</span></span>
<span data-line=""><span>    Parse the following resume and return a structured representation of the data in the schema below.</span></span>
<span data-line=""> </span>
<span data-line=""><span>    Resume:</span></span>
<span data-line=""><span>    ---</span></span>
<span data-line=""><span>    {{raw_text}}</span></span>
<span data-line=""><span>    ---</span></span>
<span data-line=""> </span>
<span data-line=""><span>    Output JSON format (only include these fields, and no others):</span></span>
<span data-line=""><span>    {{ ctx.output_format(prefix=null) }}</span></span>
<span data-line=""> </span>
<span data-line=""><span>    Output JSON:</span></span>
<span data-line=""><span>  &#34;</span><span>#</span></span>
<span data-line=""><span>}</span></span></code></pre></figure><hr/><h3 id="baml-ts"><a aria-hidden="true" tabindex="-1" href="#baml-ts"><span></span></a><a id="example-baml-ts">BAML (TS)</a></h3><p>From <a href="https://github.com/BoundaryML/baml-examples/blob/main/nextjs-starter/app/api/example_baml/route.ts"><code>baml-examples/nextjs-starter/app/api/example_baml/route.ts</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-dark"><code data-language="ts" data-theme="github-dark"><span data-line=""><span>import</span><span> b </span><span>from</span><span> &#39;./baml_client&#39;</span></span>
<span data-line=""><span>import</span><span> { Role } </span><span>from</span><span> &#39;./baml_client/types&#39;</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>// Async call</span></span>
<span data-line=""><span>const</span><span> result</span><span> =</span><span> await</span><span> b.</span><span>ClassifyMessage</span><span>({</span></span>
<span data-line=""><span>    convo: [</span></span>
<span data-line=""><span>        {</span></span>
<span data-line=""><span>            role: Role.Customer,</span></span>
<span data-line=""><span>            content: </span><span>&#34;I want to cancel my subscription&#34;</span></span>
<span data-line=""><span>        }</span></span>
<span data-line=""><span>    ]</span></span>
<span data-line=""><span>});</span></span>
<span data-line=""> </span>
<span data-line=""><span>// Streamed call</span></span>
<span data-line=""><span>const</span><span> stream</span><span> =</span><span> b.stream.</span><span>ClassifyMessage</span><span>({</span></span>
<span data-line=""><span>    convo: [</span></span>
<span data-line=""><span>        {</span></span>
<span data-line=""><span>            role: Role.Customer,</span></span>
<span data-line=""><span>            content: </span><span>&#34;I want to cancel my subscription&#34;</span></span>
<span data-line=""><span>        }</span></span>
<span data-line=""><span>    ]</span></span>
<span data-line=""><span>});</span></span>
<span data-line=""> </span>
<span data-line=""><span>for</span><span> await</span><span> (</span><span>const</span><span> partial</span><span> of</span><span> stream) {</span></span>
<span data-line=""><span>    console.</span><span>log</span><span>(partial); </span><span>// Autocompletes to a Category[]</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""><span>const</span><span> final</span><span> =</span><span> await</span><span> stream.</span><span>get_final_result</span><span>(); </span><span>// Autocompletes to a Category[]</span></span></code></pre></figure><p>From <a href="https://github.com/BoundaryML/baml-examples/blob/main/fastapi-starter/baml_src/classify_message.baml"><code>baml-examples/nextjs-starter/baml_src/classify_message.baml</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-dark"><code data-language="rust" data-theme="github-dark"><span data-line=""><span>enum</span><span> Category</span><span> {</span></span>
<span data-line=""><span>    Refund</span></span>
<span data-line=""><span>    CancelOrder</span></span>
<span data-line=""><span>    TechnicalSupport</span></span>
<span data-line=""><span>    AccountIssue</span></span>
<span data-line=""><span>    Question</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>class </span><span>Message</span><span> {</span></span>
<span data-line=""><span>  role </span><span>Role</span></span>
<span data-line=""><span>  content string</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>enum</span><span> Role</span><span> {</span></span>
<span data-line=""><span>  Customer</span></span>
<span data-line=""><span>  Assistant</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>template_string </span><span>PrintMessage</span><span>(msg</span><span>:</span><span> Message</span><span>, prefix</span><span>:</span><span> string</span><span>?</span><span>) #</span><span>&#34;</span></span>
<span data-line=""><span>  {{ _.role(&#39;user&#39; if msg.role == &#34;</span><span>Customer</span><span>&#34; else &#39;assistant&#39;) }}</span></span>
<span data-line=""><span>  {% if prefix %}</span></span>
<span data-line=""><span>  {{ prefix }}</span></span>
<span data-line=""><span>  {% endif %}</span></span>
<span data-line=""><span>  {{ msg.content }}</span></span>
<span data-line=""><span>&#34;</span><span>#</span></span>
<span data-line=""> </span>
<span data-line=""><span>function </span><span>ClassifyMessage</span><span>(convo</span><span>:</span><span> Message</span><span>[]) </span><span>-&gt;</span><span> Category</span><span>[] {</span></span>
<span data-line=""><span>  client </span><span>GPT4</span></span>
<span data-line=""><span>  prompt #</span><span>&#34;</span></span>
<span data-line=""><span>    {# </span></span>
<span data-line=""><span>      Prompts are auto-dedented and trimmed.</span></span>
<span data-line=""><span>      We use JINJA for our prompt syntax</span></span>
<span data-line=""><span>      (but we added some static analysis to make sure it&#39;s valid!)</span></span>
<span data-line=""><span>    #}</span></span>
<span data-line=""> </span>
<span data-line=""><span>    {{ ctx.output_format(prefix=&#34;</span><span>Classify</span><span> with the following json</span><span>:</span><span>&#34;) }}</span></span>
<span data-line=""> </span>
<span data-line=""><span>    {% for c in convo %}</span></span>
<span data-line=""><span>    {{ PrintMessage(c, </span></span>
<span data-line=""><span>      &#39;This is the message to classify:&#39; if loop.last and convo|length &gt; 1 else null</span></span>
<span data-line=""><span>    ) }}</span></span>
<span data-line=""><span>    {% endfor %}</span></span>
<span data-line=""> </span>
<span data-line=""><span>    {{ _.role(&#39;assistant&#39;) }}</span></span>
<span data-line=""><span>    JSON array of categories that match:</span></span>
<span data-line=""><span>  &#34;</span><span>#</span></span>
<span data-line=""><span>}</span></span></code></pre></figure><hr/><h3 id="baml-ruby"><a aria-hidden="true" tabindex="-1" href="#baml-ruby"><span></span></a><a id="example-baml-ruby">BAML (Ruby)</a></h3><p>From <a href="https://github.com/BoundaryML/baml-ruby-starter/blob/main/examples.rb"><code>baml-ruby-starter/examples.rb</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ruby" data-theme="github-dark"><code data-language="ruby" data-theme="github-dark"><span data-line=""><span>require_relative</span><span> &#34;baml_client/client&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>b </span><span>=</span><span> Baml</span><span>::</span><span>BamlClient</span><span>.from_directory(</span><span>&#34;baml_src&#34;</span><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>input </span><span>=</span><span> &#34;Can&#39;t access my account using my usual login credentials&#34;</span></span>
<span data-line=""><span>classified </span><span>=</span><span> b.</span><span>ClassifyMessage</span><span>(</span><span>input:</span><span> input)</span></span>
<span data-line=""> </span>
<span data-line=""><span>puts</span><span> classified.categories</span></span></code></pre></figure><p>From <a href="https://github.com/BoundaryML/baml-ruby-starter/blob/main/baml_src/classify_message.baml"><code>baml-ruby-starter/baml_src/classify_message.baml</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-dark"><code data-language="rust" data-theme="github-dark"><span data-line=""><span>enum</span><span> Category</span><span> {</span></span>
<span data-line=""><span>    Refund</span></span>
<span data-line=""><span>    CancelOrder</span></span>
<span data-line=""><span>    TechnicalSupport</span></span>
<span data-line=""><span>    AccountIssue</span></span>
<span data-line=""><span>    Question</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>class </span><span>MessageFeatures</span><span> {</span></span>
<span data-line=""><span>    categories </span><span>Category</span><span>[]</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>function </span><span>ClassifyMessage</span><span>(input</span><span>:</span><span> string) </span><span>-&gt;</span><span> MessageFeatures</span><span> {</span></span>
<span data-line=""><span>  client </span><span>GPT4Turbo</span></span>
<span data-line=""> </span>
<span data-line=""><span>  prompt #</span><span>&#34;</span></span>
<span data-line=""><span>    {# _.role(&#34;</span><span>system</span><span>&#34;) starts a system message #}</span></span>
<span data-line=""><span>    {{ _.role(&#34;</span><span>system</span><span>&#34;) }}</span></span>
<span data-line=""> </span>
<span data-line=""><span>    Classify the following INPUT.</span></span>
<span data-line=""> </span>
<span data-line=""><span>    {{ ctx.output_format }}</span></span>
<span data-line=""> </span>
<span data-line=""><span>    {# This starts a user message #}</span></span>
<span data-line=""><span>    {{ _.role(&#34;</span><span>user</span><span>&#34;) }}</span></span>
<span data-line=""> </span>
<span data-line=""><span>    INPUT: {{ input }}</span></span>
<span data-line=""> </span>
<span data-line=""><span>    Response:</span></span>
<span data-line=""><span>  &#34;</span><span>#</span></span>
<span data-line=""><span>}</span></span></code></pre></figure><hr/><h3 id="instructor-python"><a aria-hidden="true" tabindex="-1" href="#instructor-python"><span></span></a><a id="example-instructor">Instructor (Python)</a></h3><p>From <a href="https://github.com/jxnl/instructor/blob/81b6a7607ccb701e00a35a5f90dda72ceb75f995/examples/classification/simple_prediction.py#L23-L37"><code>simple_prediction.py</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark"><span data-line=""><span>class</span><span> Labels</span><span>(</span><span>str</span><span>, </span><span>enum</span><span>.</span><span>Enum</span><span>):</span></span>
<span data-line=""><span>    SPAM</span><span> =</span><span> &#34;spam&#34;</span></span>
<span data-line=""><span>    NOT_SPAM</span><span> =</span><span> &#34;not_spam&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>class</span><span> SinglePrediction</span><span>(</span><span>BaseModel</span><span>):</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""><span>    Correct class label for the given text</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>    class_label: Labels</span></span>
<span data-line=""> </span>
<span data-line=""><span>def</span><span> classify</span><span>(data: </span><span>str</span><span>) -&gt; SinglePrediction:</span></span>
<span data-line=""><span>    return</span><span> client.chat.completions.create(</span></span>
<span data-line=""><span>        model</span><span>=</span><span>&#34;gpt-3.5-turbo-0613&#34;</span><span>,</span></span>
<span data-line=""><span>        response_model</span><span>=</span><span>SinglePrediction,</span></span>
<span data-line=""><span>        messages</span><span>=</span><span>[</span></span>
<span data-line=""><span>            {</span></span>
<span data-line=""><span>                &#34;role&#34;</span><span>: </span><span>&#34;user&#34;</span><span>,</span></span>
<span data-line=""><span>                &#34;content&#34;</span><span>: </span><span>f</span><span>&#34;Classify the following text: </span><span>{</span><span>data</span><span>}</span><span>&#34;</span><span>,</span></span>
<span data-line=""><span>            },</span></span>
<span data-line=""><span>        ],</span></span>
<span data-line=""><span>    )  </span><span># type: ignore</span></span>
<span data-line=""> </span>
<span data-line=""><span>prediction </span><span>=</span><span> classify(</span><span>&#34;Hello there I&#39;m a nigerian prince and I want to give you money&#34;</span><span>)</span></span>
<span data-line=""><span>assert</span><span> prediction.class_label </span><span>==</span><span> Labels.</span><span>SPAM</span></span></code></pre></figure><hr/><h3 id="instructor-js"><a aria-hidden="true" tabindex="-1" href="#instructor-js"><span></span></a><a id="example-instructor-js">instructor-js</a></h3><p>From <a href="https://github.com/instructor-ai/instructor-js/blob/f386ad71a48bc4b39c454bad1e4302e171e2dc78/examples/classification/simple_prediction/index.ts#L25-L47"><code>simple_prediction/index.ts</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-dark"><code data-language="ts" data-theme="github-dark"><span data-line=""><span>import</span><span> { z } </span><span>from</span><span> &#34;zod&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>enum</span><span> CLASSIFICATION_LABELS</span><span> {</span></span>
<span data-line=""><span>  &#34;SPAM&#34;</span><span> =</span><span> &#34;SPAM&#34;</span><span>,</span></span>
<span data-line=""><span>  &#34;NOT_SPAM&#34;</span><span> =</span><span> &#34;NOT_SPAM&#34;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>const</span><span> SimpleClassificationSchema</span><span> =</span><span> z.</span><span>object</span><span>({</span></span>
<span data-line=""><span>  class_label: z.</span><span>nativeEnum</span><span>(</span><span>CLASSIFICATION_LABELS</span><span>)</span></span>
<span data-line=""><span>})</span></span>
<span data-line=""> </span>
<span data-line=""><span>const</span><span> createClassification</span><span> =</span><span> async</span><span> (</span><span>data</span><span>:</span><span> string</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span data-line=""><span>  const</span><span> classification</span><span> =</span><span> await</span><span> client.chat.completions.</span><span>create</span><span>({</span></span>
<span data-line=""><span>    messages: [{ role: </span><span>&#34;user&#34;</span><span>, content: </span><span>`&#34;Classify the following text: ${</span><span>data</span><span>}`</span><span> }],</span></span>
<span data-line=""><span>    model: </span><span>&#34;gpt-3.5-turbo&#34;</span><span>,</span></span>
<span data-line=""><span>    response_model: { schema: SimpleClassificationSchema, name: </span><span>&#34;SimpleClassification&#34;</span><span> },</span></span>
<span data-line=""><span>    max_retries: </span><span>3</span><span>,</span></span>
<span data-line=""><span>    seed: </span><span>1</span></span>
<span data-line=""><span>  })</span></span>
<span data-line=""> </span>
<span data-line=""><span>  return</span><span> classification</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>const</span><span> classification</span><span> =</span><span> await</span><span> createClassification</span><span>(</span></span>
<span data-line=""><span>  &#34;Hello there I&#39;m a nigerian prince and I want to give you money&#34;</span></span>
<span data-line=""><span>)</span></span>
<span data-line=""><span>// OUTPUT: { class_label: &#39;SPAM&#39; }</span></span>
<span data-line=""> </span>
<span data-line=""><span>console.</span><span>log</span><span>({ classification })</span></span>
<span data-line=""> </span>
<span data-line=""><span>assert</span><span>(</span></span>
<span data-line=""><span>  classification?.class_label </span><span>===</span><span> CLASSIFICATION_LABELS</span><span>.</span><span>SPAM</span><span>,</span></span>
<span data-line=""><span>  `Expected ${</span><span>classification</span><span>?.</span><span>class_label</span><span>} to be ${</span><span>CLASSIFICATION_LABELS</span><span>.</span><span>SPAM</span><span>}`</span></span>
<span data-line=""><span>)</span></span></code></pre></figure><hr/><h3 id="typechat-python"><a aria-hidden="true" tabindex="-1" href="#typechat-python"><span></span></a><a id="example-typechat-python">TypeChat (Python)</a></h3><p>From <a href="https://github.com/microsoft/TypeChat/blob/main/python/examples/sentiment/demo.py"><code>examples/sentiment/demo.py</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark"><span data-line=""><span>import</span><span> asyncio</span></span>
<span data-line=""> </span>
<span data-line=""><span>import</span><span> sys</span></span>
<span data-line=""><span>from</span><span> dotenv </span><span>import</span><span> dotenv_values</span></span>
<span data-line=""><span>import</span><span> schema </span><span>as</span><span> sentiment</span></span>
<span data-line=""><span>from</span><span> typechat </span><span>import</span><span> Failure, TypeChatJsonTranslator, TypeChatValidator, create_language_model, process_requests</span></span>
<span data-line=""> </span>
<span data-line=""><span>async</span><span> def</span><span> main</span><span>():    </span></span>
<span data-line=""><span>    env_vals </span><span>=</span><span> dotenv_values()</span></span>
<span data-line=""><span>    model </span><span>=</span><span> create_language_model(env_vals)</span></span>
<span data-line=""><span>    validator </span><span>=</span><span> TypeChatValidator(sentiment.Sentiment)</span></span>
<span data-line=""><span>    translator </span><span>=</span><span> TypeChatJsonTranslator(model, validator, sentiment.Sentiment)</span></span>
<span data-line=""> </span>
<span data-line=""><span>    async</span><span> def</span><span> request_handler</span><span>(message: </span><span>str</span><span>):</span></span>
<span data-line=""><span>        result </span><span>=</span><span> await</span><span> translator.translate(message)</span></span>
<span data-line=""><span>        if</span><span> isinstance</span><span>(result, Failure):</span></span>
<span data-line=""><span>            print</span><span>(result.message)</span></span>
<span data-line=""><span>        else</span><span>:</span></span>
<span data-line=""><span>            result </span><span>=</span><span> result.value</span></span>
<span data-line=""><span>            print</span><span>(</span><span>f</span><span>&#34;The sentiment is </span><span>{</span><span>result.sentiment</span><span>}</span><span>&#34;</span><span>)</span></span>
<span data-line=""> </span>
<span data-line=""><span>    file_path </span><span>=</span><span> sys.argv[</span><span>1</span><span>] </span><span>if</span><span> len</span><span>(sys.argv) </span><span>==</span><span> 2</span><span> else</span><span> None</span></span>
<span data-line=""><span>    await</span><span> process_requests(</span><span>&#34;😀&gt; &#34;</span><span>, file_path, request_handler)</span></span>
<span data-line=""> </span>
<span data-line=""> </span>
<span data-line=""><span>if</span><span> __name__</span><span> ==</span><span> &#34;__main__&#34;</span><span>:</span></span>
<span data-line=""><span>    asyncio.run(main())</span></span></code></pre></figure><p>From <a href="https://github.com/microsoft/TypeChat/blob/main/python/examples/sentiment/schema.py"><code>examples/sentiment/schema.py</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark"><span data-line=""><span>from</span><span> dataclasses </span><span>import</span><span> dataclass</span></span>
<span data-line=""><span>from</span><span> typing_extensions </span><span>import</span><span> Literal, Annotated, Doc</span></span>
<span data-line=""> </span>
<span data-line=""><span>@dataclass</span></span>
<span data-line=""><span>class</span><span> Sentiment</span><span>:</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""><span>    The following is a schema definition for determining the sentiment of a some user input.</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>    sentiment: Annotated[Literal[</span><span>&#34;negative&#34;</span><span>, </span><span>&#34;neutral&#34;</span><span>, </span><span>&#34;positive&#34;</span><span>],</span></span>
<span data-line=""><span>                         Doc(</span><span>&#34;The sentiment for the text&#34;</span><span>)]</span></span></code></pre></figure><hr/><h3 id="typechat-typescript"><a aria-hidden="true" tabindex="-1" href="#typechat-typescript"><span></span></a><a id="example-typechat-ts">TypeChat (TypeScript)</a></h3><p>From <a href="https://github.com/microsoft/TypeChat/blob/main/typescript/examples/sentiment/src/main.ts"><code>examples/sentiment/src/main.ts</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-dark"><code data-language="ts" data-theme="github-dark"><span data-line=""><span>import</span><span> { createJsonTranslator, createLanguageModel } </span><span>from</span><span> &#34;typechat&#34;</span><span>;</span></span>
<span data-line=""><span>import</span><span> { processRequests } </span><span>from</span><span> &#34;typechat/interactive&#34;</span><span>;</span></span>
<span data-line=""><span>import</span><span> { createTypeScriptJsonValidator } </span><span>from</span><span> &#34;typechat/ts&#34;</span><span>;</span></span>
<span data-line=""><span>import</span><span> { SentimentResponse } </span><span>from</span><span> &#34;./sentimentSchema&#34;</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>const</span><span> dotEnvPath</span><span> =</span><span> findConfig</span><span>(</span><span>&#34;.env&#34;</span><span>);</span></span>
<span data-line=""><span>assert</span><span>(dotEnvPath, </span><span>&#34;.env file not found!&#34;</span><span>);</span></span>
<span data-line=""><span>dotenv.</span><span>config</span><span>({ path: dotEnvPath });</span></span>
<span data-line=""> </span>
<span data-line=""><span>const</span><span> model</span><span> =</span><span> createLanguageModel</span><span>(process.env);</span></span>
<span data-line=""><span>const</span><span> schema</span><span> =</span><span> fs.</span><span>readFileSync</span><span>(path.</span><span>join</span><span>(__dirname, </span><span>&#34;sentimentSchema.ts&#34;</span><span>), </span><span>&#34;utf8&#34;</span><span>);</span></span>
<span data-line=""><span>const</span><span> validator</span><span> =</span><span> createTypeScriptJsonValidator</span><span>&lt;</span><span>SentimentResponse</span><span>&gt;(schema, </span><span>&#34;SentimentResponse&#34;</span><span>);</span></span>
<span data-line=""><span>const</span><span> translator</span><span> =</span><span> createJsonTranslator</span><span>(model, validator);</span></span>
<span data-line=""> </span>
<span data-line=""><span>// Process requests interactively or from the input file specified on the command line</span></span>
<span data-line=""><span>processRequests</span><span>(</span><span>&#34;😀&gt; &#34;</span><span>, process.argv[</span><span>2</span><span>], </span><span>async</span><span> (</span><span>request</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span data-line=""><span>    const</span><span> response</span><span> =</span><span> await</span><span> translator.</span><span>translate</span><span>(request);</span></span>
<span data-line=""><span>    if</span><span> (</span><span>!</span><span>response.success) {</span></span>
<span data-line=""><span>        console.</span><span>log</span><span>(response.message);</span></span>
<span data-line=""><span>        return</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>    console.</span><span>log</span><span>(</span><span>`The sentiment is ${</span><span>response</span><span>.</span><span>data</span><span>.</span><span>sentiment</span><span>}`</span><span>);</span></span>
<span data-line=""><span>});</span></span></code></pre></figure><p>From <a href="https://github.com/microsoft/TypeChat/blob/main/typescript/examples/sentiment/src/sentimentSchema.ts"><code>examples/sentiment/src/sentimentSchema.ts</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="ts" data-theme="github-dark"><code data-language="ts" data-theme="github-dark"><span data-line=""><span>export</span><span> interface</span><span> SentimentResponse</span><span> {</span></span>
<span data-line=""><span>    sentiment</span><span>:</span><span> &#34;negative&#34;</span><span> |</span><span> &#34;neutral&#34;</span><span> |</span><span> &#34;positive&#34;</span><span>;  </span><span>// The sentiment of the text</span></span>
<span data-line=""><span>}</span></span></code></pre></figure><hr/><h3 id="typechat-cnet"><a aria-hidden="true" tabindex="-1" href="#typechat-cnet"><span></span></a><a id="example-typechat-dotnet">TypeChat (C#/.NET)</a></h3><p>From <a href="https://github.com/microsoft/typechat.net/blob/main/examples/Sentiment/Program.cs"><code>examples/Sentiment/Program.cs</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="csharp" data-theme="github-dark"><code data-language="csharp" data-theme="github-dark"><span data-line=""><span>using</span><span> Microsoft</span><span>.</span><span>TypeChat</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>namespace</span><span> Sentiment</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>public</span><span> class</span><span> SentimentApp</span><span> : </span><span>ConsoleApp</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>    JsonTranslator</span><span>&lt;</span><span>SentimentResponse</span><span>&gt; </span><span>_translator</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>    public</span><span> SentimentApp</span><span>()</span></span>
<span data-line=""><span>    {</span></span>
<span data-line=""><span>        OpenAIConfig</span><span> config</span><span> =</span><span> Config.</span><span>LoadOpenAI</span><span>();</span></span>
<span data-line=""><span>        // Although this sample uses config files, you can also load config from environment variables</span></span>
<span data-line=""><span>        // OpenAIConfig config = OpenAIConfig.LoadFromJsonFile(&#34;your path&#34;);</span></span>
<span data-line=""><span>        // OpenAIConfig config = OpenAIConfig.FromEnvironment();</span></span>
<span data-line=""><span>        _translator </span><span>=</span><span> new</span><span> JsonTranslator</span><span>&lt;</span><span>SentimentResponse</span><span>&gt;(</span><span>new</span><span> LanguageModel</span><span>(config));</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    public</span><span> override</span><span> async</span><span> Task</span><span> ProcessInputAsync</span><span>(</span><span>string</span><span> input</span><span>, </span><span>CancellationToken</span><span> cancelToken</span><span>)</span></span>
<span data-line=""><span>    {</span></span>
<span data-line=""><span>        SentimentResponse</span><span> response</span><span> =</span><span> await</span><span> _translator.</span><span>TranslateAsync</span><span>(input, cancelToken);</span></span>
<span data-line=""><span>        Console.</span><span>WriteLine</span><span>(</span><span>$&#34;The sentiment is {</span><span>response</span><span>.</span><span>Sentiment</span><span>}&#34;</span><span>);</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span></code></pre></figure><p>From <a href="https://github.com/microsoft/typechat.net/blob/main/examples/Sentiment/SentimentSchema.cs"><code>examples/Sentiment/SentimentSchema.cs</code></a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="csharp" data-theme="github-dark"><code data-language="csharp" data-theme="github-dark"><span data-line=""><span>using</span><span> System</span><span>.</span><span>Text</span><span>.</span><span>Json</span><span>.</span><span>Serialization</span><span>;</span></span>
<span data-line=""><span>using</span><span> Microsoft</span><span>.</span><span>TypeChat</span><span>.</span><span>Schema</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>namespace</span><span> Sentiment</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>public</span><span> class</span><span> SentimentResponse</span></span>
<span data-line=""><span>{</span></span>
<span data-line=""><span>    [</span><span>JsonPropertyName</span><span>(</span><span>&#34;sentiment&#34;</span><span>)]</span></span>
<span data-line=""><span>    [</span><span>JsonVocab</span><span>(</span><span>&#34;negative | neutral | positive&#34;</span><span>)]</span></span>
<span data-line=""><span>    public</span><span> string</span><span> Sentiment</span><span> { </span><span>get</span><span>; </span><span>set</span><span>; }</span></span>
<span data-line=""><span>}</span></span></code></pre></figure><hr/><h3 id="marvin"><a aria-hidden="true" tabindex="-1" href="#marvin"><span></span></a><a id="example-marvin">Marvin</a></h3><p>From the <a href="https://www.askmarvin.ai/docs/text/functions/#parameters">Marvin docs</a>:</p><figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="github-dark"><code data-language="python" data-theme="github-dark"><span data-line=""><span>import</span><span> marvin</span></span>
<span data-line=""><span>from</span><span> pydantic </span><span>import</span><span> BaseModel</span></span>
<span data-line=""> </span>
<span data-line=""><span>class</span><span> Recipe</span><span>(</span><span>BaseModel</span><span>):</span></span>
<span data-line=""><span>    name: </span><span>str</span></span>
<span data-line=""><span>    cook_time_minutes: </span><span>int</span></span>
<span data-line=""><span>    ingredients: list[</span><span>str</span><span>]</span></span>
<span data-line=""><span>    steps: list[</span><span>str</span><span>]</span></span>
<span data-line=""> </span>
<span data-line=""><span>@marvin.fn</span></span>
<span data-line=""><span>def</span><span> recipe</span><span>(</span></span>
<span data-line=""><span>    ingredients: list[</span><span>str</span><span>], </span></span>
<span data-line=""><span>    max_cook_time: </span><span>int</span><span> =</span><span> 15</span><span>, </span></span>
<span data-line=""><span>    cuisine: </span><span>str</span><span> =</span><span> &#34;North Italy&#34;</span><span>, </span></span>
<span data-line=""><span>    experience_level:</span><span>str</span><span> =</span><span> &#34;beginner&#34;</span></span>
<span data-line=""><span>) -&gt; Recipe:</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span>
<span data-line=""><span>    Returns a complete recipe that uses all the `ingredients` and </span></span>
<span data-line=""><span>    takes less than `max_cook_time`  minutes to prepare. Takes </span></span>
<span data-line=""><span>    `cuisine` style and the chef&#39;s `experience_level` into account </span></span>
<span data-line=""><span>    as well.</span></span>
<span data-line=""><span>    &#34;&#34;&#34;</span></span></code></pre></figure></div><p>This is a living document, and we&#39;ll be updating it as we learn more about other frameworks.</p><p>If you have any questions, comments, or suggestions, feel free to reach out to us on <a href="https://discord.gg/BTNBeXGuaS">Discord</a> or Twitter at <a href="https://x.com/boundaryML">@boundaryml</a>. We&#39;re happy to also meet and help with any prompting / AI engineering questions you might have.</p></div>
  </body>
</html>
