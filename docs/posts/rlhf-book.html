<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rlhfbook.com/">Original</a>
    <h1>RLHF Book</h1>
    
    <div id="readability-page-1" class="page">
<header id="title-block-header">

<p>A short introduction to RLHF and post-training
focused on language models.</p>

<navigation-dropdown expanded="true"></navigation-dropdown>

<div>
  <h2>Abstract</h2>
  <p>Reinforcement learning from human feedback (RLHF) has become an
  important technical and storytelling tool to deploy the latest machine
  learning systems. In this book, we hope to give a gentle introduction
  to the core methods for people with some level of quantitative
  background. The book starts with the origins of RLHF â€“ both in recent
  literature and in a convergence of disparate fields of science in
  economics, philosophy, and optimal control. We then set the stage with
  definitions, problem formulation, data collection, and other common
  math used in the literature. We detail the popular algorithms and
  future frontiers of RLHF.</p>
</div>
</header>

  <section id="acknowledgements">
    <h2>Acknowledgements</h2>
    <p>I would like to thank the following people who helped me directly with this project: Costa Huang, (and of course Claude). Indirect shout-outs go to Ross Taylor, Hamish Ivison, John Schulman, and others in my RL sphere.</p>
    <p>Additionally, thank you to the <a href="https://github.com/natolambert/rlhf-book/graphs/contributors">contributors on GitHub</a> who helped improve this project.</p>
  </section>
    


</div>
  </body>
</html>
