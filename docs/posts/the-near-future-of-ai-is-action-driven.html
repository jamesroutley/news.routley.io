<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jmcdonnell.substack.com/p/the-near-future-of-ai-is-action-driven">Original</a>
    <h1>The near future of AI is action-driven</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png" width="1277" height="782" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:782,&#34;width&#34;:1277,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:123207,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 1456w" sizes="100vw"/></picture></div></a></figure></div><p><span>In 2022, large language models (LLMs) finally got good. Specifically, Google and OpenAI have led the way in creating </span><a href="https://en.wikipedia.org/wiki/Foundation_models" rel="">foundation models</a><span> that respond to instructions more usefully. For OpenAI, this came in the form of Instruct-GPT  (</span><a href="https://openai.com/blog/instruction-following/" rel="">OpenAI blogpost</a><span>), while for Google this was reflected in their FLAN training method (Wei et al. 2022, </span><a href="https://arxiv.org/abs/2109.01652" rel="">arxiv</a><span>). Flan’s which beat the Hypermind forecast for MMLU performance two years early: </span></p><p><span>But the best is yet to come. The really exciting applications will be </span><em>action-driven</em><span>, where the model acts like an agent choosing actions. And although academics can argue all day about the true definition of AGI, an action-driven LLM is going to look a lot like AGI.</span></p><p><span>Famously, LLMs often perform better at question-answering tasks when prompted to “think step by step.” (Kojima et al. 2022, </span><a href="https://arxiv.org/abs/2205.11916" rel="">arxiv</a><span>). But they can do even better if they’re given external resources, or what I call </span><em>external cognitive assets</em><span>. The ReAct model puts these pieces together (Yao et al. 2022, </span><a href="https://arxiv.org/abs/2210.03629" rel="">arxiv</a><span>). ReAct takes three steps iteratively: Thought (about what is needed), Act (choice of action), and Observation (see the outcome of the action). The actions often make use of cognitive assets like search. The authors give an example below:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png" width="1456" height="634" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:634,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:418865,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>Here, the main actions the model uses above are </span><code>Search</code><span> and </span><code>Finish</code><span>. The authors also provided  </span><code>Lookup</code><span> action that can search within the text of the article. In 1a/1b we can see the LLM doesn’t do very well answering the question on its own, because it doesn’t have enough information. In 1c we see that simply giving the algorithm a search function is not sufficient because the model seems to forget what it is doing by the time it hits the end. The authors’ approach of including both actions and specific thought steps is able to search deeply and find the answer.</span></p><p><span>Answering factual questions is useful, but for real-world tasks, providing LLMs with an array of tools can enable a much broader array of powers. Imagine a customer service LLM able to do React-style chaining. We might give it actions </span><code>SearchDocs</code><span> for information about the service and policies, </span><code>SearchClientFacts</code><span> to look up particular facts about a client, </span><code>PythonCode</code><span> to run python programs, </span><code>ManualEscalation</code><span> to submit cases to a human for approval, </span><code>MessageClient</code><span> to send the client messages, and </span><code>Finish</code><span> to indicate completion. I’ve fabricated a possible client assistance transcript below with React-style formatting:</span></p><pre><code><code>Question: Client is asking if they can cancel their meal kit order.

Thought 1: I need to know our policy for cancelling orders.
Act 1: SearchDocs[Meal kit cancellation policy]
Obs 1: Meal kits can be cancelled up until 3 days before their ship date, after which manual escalation is necessary.

Thought 2: I need to know what the customer&#39;s ship date is.
Act 2: SearchClientFacts[NextShipDate]
Obs 2: &#39;2022-12-03&#39;

Thought 3: I need to know if that is 3 days from now.
Act 3: PythonCode[`from datetime import datetime; (datetime.strptime(&#39;2022-12-03&#39;, &#39;%Y-%m-%d&#39;) - datetime.now()).days`]
Obs 3: 2

Thought 4: This means the cancellation will require a manual escalation. 
Act 4: ManualEscalation[Client needs shipment cancelled.]
Obs 4: Case has been escalated to a human service provider.

Thought 5: I should tell the client what I have done.
Act 4: MessageClient[&#34;This order is happening in 2 days. That means we need to escalate to a manual cancellation. Please wait for a customer support person to reach out to you.&#34;]
Act 4: Message sent successfully

Thought 5: This means I am done.
Act 5: Finish[]</code></code></pre><p>We can now make agents that, on the basis of natural language questions or instructions, can perform actions in a wide action space. The potential is not limited to CX or sales any particular business workflow. Any set of behaviors where an agent manipulates symbols in a computer can in principle be performed with this schema. And it looks an awful lot like AGI.</p><p>This is not 10 year tech. It may be possible right now with off-the-shelf tools. But to make it work we need to set up the right feedback loops.</p><p>If you were to attempt the above CX example in the latest version of GPT-3, it’s unlikely that you would get consistently good behavior. The LLM needs to understand the power of its own tools and it needs to know what kinds of outcomes we the user desire.</p><p><span>There are many tools for doing this. Google Brain recently achieved SOTA question-answering performance via instruction tuning (</span><a href="https://arxiv.org/abs/2210.11416" rel="">arxiv</a><span>). The secret to OpenAI’s 002-text-davinci model seems to be attributable to a combination of instruction tuning and Reinforcement learning from Human Feedback (RLHF, </span><a href="https://openai.com/blog/instruction-following/" rel="">blogpost</a><span>), wherein humans rate the success of a given prompt. The ReAct paper takes advantage of a method developed by  Zelikman et al. (2022, </span><a href="https://arxiv.org/abs/2203.14465" rel="">arxiv</a><span>) to bootstrap a large number of “valid” chains for training by using LLMs to generate them. I suspect that the very best results will come from actual reinforcement learning where a system can actually be trained to produce better results as measured via a metric of interest. For example, consider a model generating marketing copy that could be trained on conversion rate data to produce copy with consistently higher conversion.</span></p><p>Schematically, the stack for a successful action-driven deployment will look something like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png" width="1008" height="991" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/eeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:991,&#34;width&#34;:1008,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:122427,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>In the middle we have the “out of the box” foundation model. Prompts are engineered to ask for outcomes, passed into the LLM, and an output is received. Many of the recent “LLM” startups take advantage roughly of this toolset. Instruct GPT has made OpenAI’s version of this stack particularly easy to get started with.</p><p>On the left we have the External Cognitive Assets that can supercharge a model’s power. These can be any function that takes text as an input and provides text as an output, including searches, code interpreters, and chats with humans.</p><p><span>Finally on the right we have the task oriented training that’s needed to make this work well. This is the hard part. Some techniques, like instruction tuning, seem fairly straightforward to implement. RLHF is harder and involves tuning a PPO algorithm (</span><a href="https://openai.com/blog/openai-baselines-ppo/" rel="">OpenAI post</a><span>). RL will particularly benefit from proprietary datasets, especially usage logs.</span></p><p>Some startups will become very successful creating powerful feedback loops: Solving a customer pain point (maybe bootstrapping by starting with something very simple), collecting data about how to solve that better, training their models to be more consistent, and iterating. This is roughly what a moat will look like in AI, at least for now. But as the agents get more domain-general, the spaces that can be automated and offerings that are possible will expand.</p><p>We’ve already seen a few demos of giving models superpowers like this. For example Sergey Kereyev showed what can already happen when you give an LLM a Python interpreter: </p><div data-attrs="{&#34;url&#34;:&#34;https://twitter.com/sergeykarayev/status/1569377881440276481&#34;,&#34;full_text&#34;:&#34;Here&#39;s a brief glimpse of our INCREDIBLE near future.\n\nGPT-3 armed with a Python interpreter can\n· do exact math\n· make API requests\n· answer in unprecedented ways\n\nThanks to &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@goodside&lt;/span&gt; and &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@amasad&lt;/span&gt; for the idea and repl!\n\nPlay with it: &lt;a class=\&#34;tweet-url\&#34; href=\&#34;https://replit.com/@SergeyKarayev/gptpy\&#34;&gt;replit.com/@SergeyKarayev…&lt;/a&gt; &#34;,&#34;username&#34;:&#34;sergeykarayev&#34;,&#34;name&#34;:&#34;Sergey Karayev&#34;,&#34;date&#34;:&#34;Mon Sep 12 17:30:19 +0000 2022&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/FceL32AaIAApyZj.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/JnkiUyTQx1&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{},&#34;retweet_count&#34;:666,&#34;like_count&#34;:3992,&#34;expanded_url&#34;:{},&#34;video_url&#34;:null,&#34;belowTheFold&#34;:true}"><a href="https://twitter.com/sergeykarayev/status/1569377881440276481" target="_blank" rel=""></a><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_600,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFceL32AaIAApyZj.jpg"/><img src="https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFceL32AaIAApyZj.jpg" alt="Image" loading="lazy"/></picture></div></div><a href="https://twitter.com/sergeykarayev/status/1569377881440276481" target="_blank" rel=""></a></div><p>And the team at Adept built a model that can surf the web and perform basic tasks.</p><p>In terms of implementation, LangChain makes it easy to chain prompts together and provide LLMs with tools:</p><p><a href="http://Dust.tt" rel="">Dust.tt</a><span>  provides a web app and alternative model for chaining prompt components:</span></p><div data-attrs="{&#34;url&#34;:&#34;https://mobile.twitter.com/goodside/status/1588262930553786368&#34;,&#34;full_text&#34;:&#34;&lt;span class=\&#34;tweet-fake-link\&#34;&gt;@hwchase17&lt;/span&gt; &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@dust4ai&lt;/span&gt; takes some work to wrap your head around, but it&#39;s very powerful — gives a collapsible tree UI for representing k-shot example datasets, prompt templates, and prompt chaining with intermediate JS code. Replaces a lot of code around prompt APIs. &lt;a class=\&#34;tweet-url\&#34; href=\&#34;http://dust.tt\&#34;&gt;dust.tt&lt;/a&gt; &#34;,&#34;username&#34;:&#34;goodside&#34;,&#34;name&#34;:&#34;Riley Goodside&#34;,&#34;date&#34;:&#34;Thu Nov 03 20:12:45 +0000 2022&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/FgqjoKnVsAA8ADA.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/ScwqIRyU4j&#34;,&#34;alt_text&#34;:null},{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/FgqjpHDVUAAi9wI.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/ScwqIRyU4j&#34;,&#34;alt_text&#34;:null},{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/Fgqjv9fVQAAdPVM.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/ScwqIRyU4j&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{},&#34;retweet_count&#34;:4,&#34;like_count&#34;:33,&#34;expanded_url&#34;:{},&#34;video_url&#34;:null,&#34;belowTheFold&#34;:true}"><a href="https://mobile.twitter.com/goodside/status/1588262930553786368" target="_blank" rel=""></a><div><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_600,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFgqjoKnVsAA8ADA.jpg"/><img src="https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFgqjoKnVsAA8ADA.jpg" alt="Image" loading="lazy"/></picture></div></div></div><a href="https://mobile.twitter.com/goodside/status/1588262930553786368" target="_blank" rel=""></a></div><p>These systems are not a theoretical academic demo. You can hack on them starting now. Over the next few months we’re going to see increasingly surprising and useful applications rolling out. If you can think of a job that involves manipulating symbols on a computer: Action-driven AI is coming. I’m looking forward to covering this as it unfolds, but a few things I’m specifically planning to cover:</p><ul><li><p>How do you actually build the stack in the diagram above, incorporating both external cognitive assets and training?</p></li><li><p>What’s the deal with instruction tuning? Is it actually so great? How does Instruction Tuning and RLHF actually work? Why is this hard? Can we make it easy?</p></li><li><p>What will the market look like? What sort of products does this space need to succeed? What sort of startups will thrive?</p></li><li><p><span>What does the rise of agentic algorithms mean for media and society? My hope is that there will be a rebalance of power of algorithms in favor of the consumer, but much remains in the air. I learned a lot building </span><a href="https://vibecheck.network" rel="">vibecheck.network</a><span>.</span></p></li></ul><p>Stay tuned! …and email me or DM me if you want to chat or collaborate.</p><p><span>I’m not an expert on alignment but if this sort of stack really does work, it is the </span><a href="https://intelligence.org/2017/10/13/fire-alarm/" rel="">AGI Fire alarm</a><span>. If we can build a system that can take real world actions to achieve useful goals, all the pieces are in place for a very powerful AI to be created. If you are in this space and building machines like this it is incumbent on you to learn about AI alignment risks and consider how you plan to maintain alignment with your system early on. If you are a bit skeptical or want to learn more, you might be interested in Stuart Russell’s book </span><em>Human Compatible</em><span> [</span><a href="https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558632/ref=asc_df_0525558632/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=459440273404&amp;hvpos=&amp;hvnetw=g&amp;hvrand=7165565356548324582&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9032066&amp;hvtargid=pla-951654604793&amp;psc=1" rel="">amazon</a><span>]. Russell is a lifelong AI researcher and speaks as a domain expert, not a wild-eyed doomer. For the lazy, Scott Alexander reviews it </span><a href="https://slatestarcodex.com/2020/01/30/book-review-human-compatible/" rel="">here</a><span>.</span></p></div></div></div></article></div></div></div>
  </body>
</html>
