<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jmcdonnell.substack.com/p/the-near-future-of-ai-is-action-driven">Original</a>
    <h1>The near future of AI is action-driven</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png" width="1277" height="782" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:782,&#34;width&#34;:1277,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:123207,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F16e21f07-14b3-48fd-a99d-048e45c38ebf_1277x782.png 1456w" sizes="100vw"/></picture></div></a></figure></div><p><span>In 2022, large language models (LLMs) finally got good. Specifically, Google and OpenAI have led the way in creating </span><a href="https://en.wikipedia.org/wiki/Foundation_models" rel="">foundation models</a><span> that respond to instructions more usefully. For OpenAI, this came in the form of Instruct-GPT  (</span><a href="https://openai.com/blog/instruction-following/" rel="">OpenAI blogpost</a><span>), while for Google this was reflected in their FLAN training method (Wei et al. 2022, </span><a href="https://arxiv.org/abs/2109.01652" rel="">arxiv</a><span>). Flan‚Äôs which beat the Hypermind forecast for MMLU performance two years early: </span></p><div data-attrs="{&#34;url&#34;:&#34;https://twitter.com/hwchung27/status/1583529350015565827&#34;,&#34;full_text&#34;:&#34;New paper + models!\n\nWe extend instruction finetuning by\n1. scaling to 540B model\n2. scaling to 1.8K finetuning tasks\n3. finetuning on chain-of-thought (CoT) data\n\nWith these, our Flan-PaLM model achieves a new SoTA of 75.2% on MMLU.  &#34;,&#34;username&#34;:&#34;hwchung27&#34;,&#34;name&#34;:&#34;Hyung Won Chung&#34;,&#34;date&#34;:&#34;Fri Oct 21 18:43:12 +0000 2022&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/FfnS9bRUYAEkX1z.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/5dsLFxHRUe&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{&#34;full_text&#34;:&#34;New open-source language model from Google AI: Flan-T5 üçÆ\n\nFlan-T5 is instruction-finetuned on 1,800+ language tasks, leading to dramatically improved prompting and multi-step reasoning abilities.\n\nPublic models: https://t.co/bnYVnocJW2\nPaper: https://t.co/3KPGJ3tgMw https://t.co/PQpUQM0net&#34;,&#34;username&#34;:&#34;quocleix&#34;,&#34;name&#34;:&#34;Quoc Le&#34;},&#34;retweet_count&#34;:54,&#34;like_count&#34;:276,&#34;expanded_url&#34;:{},&#34;video_url&#34;:null,&#34;belowTheFold&#34;:true}"><a href="https://twitter.com/hwchung27/status/1583529350015565827" target="_blank" rel=""><div><p><img src="https://substackcdn.com/image/twitter_name/w_96/hwchung27.jpg" alt="Twitter avatar for @hwchung27" loading="lazy"/></p><p><span>Hyung Won Chung </span><span>@hwchung27</span></p></div><p>New paper + models!

We extend instruction finetuning by
1. scaling to 540B model
2. scaling to 1.8K finetuning tasks
3. finetuning on chain-of-thought (CoT) data

With these, our Flan-PaLM model achieves a new SoTA of 75.2% on MMLU.  </p><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_600,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFfnS9bRUYAEkX1z.jpg"/><img src="https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFfnS9bRUYAEkX1z.jpg" alt="Image" loading="lazy"/></picture></div></div><div><div><p><img src="https://substackcdn.com/image/twitter_name/w_40/quocleix.jpg" alt="Twitter avatar for @quocleix" loading="lazy"/></p><p><span>Quoc Le </span><span>@quocleix</span></p></div><p><span>New open-source language model from Google AI: Flan-T5 üçÆ

Flan-T5 is instruction-finetuned on 1,800+ language tasks, leading to dramatically improved prompting and multi-step reasoning abilities.

Public models: https://t.co/bnYVnocJW2
Paper: https://t.co/3KPGJ3tgMw https://t.co/PQpUQM0net</span></p></div></a><a href="https://twitter.com/hwchung27/status/1583529350015565827" target="_blank" rel=""><div><p><span>6:43 PM ‚àô Oct 21, 2022</span></p><hr/><p><span href="https://twitter.com/hwchung27/status/1583529350015565827/likes"><span>276</span><span>Likes</span></span><span href="https://twitter.com/hwchung27/status/1583529350015565827/retweets"><span>54</span><span>Retweets</span></span></p></div></a></div><p><span>But the best is yet to come. The really exciting applications will be </span><em>action-driven</em><span>, where the model acts like an agent choosing actions. And although academics can argue all day about the true definition of AGI, an action-driven LLM is going to look a lot like AGI.</span></p><div><div><p>Thanks for reading Causal Deference! Subscribe for free to receive new posts and support my work.</p><div><form action="/api/v1/free?nojs=true" method="post" novalidate=""></form></div></div></div><h2>Let‚Äôs act step by step<div id="¬ßlets-act-step-by-step"><div></div></div></h2><p><span>Famously, LLMs often perform better at question-answering tasks when prompted to ‚Äúthink step by step.‚Äù (Kojima et al. 2022, </span><a href="https://arxiv.org/abs/2205.11916" rel="">arxiv</a><span>). But they can do even better if they‚Äôre given external resources, or what I call </span><em>external cognitive assets</em><span>. The ReAct model puts these pieces together (Yao et al. 2022, </span><a href="https://arxiv.org/abs/2210.03629" rel="">arxiv</a><span>). ReAct takes three steps iteratively: Thought (about what is needed), Act (choice of action), and Observation (see the outcome of the action). The actions often make use of cognitive assets like search. The authors give an example below:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png" width="1456" height="634" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:634,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:418865,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0bb921d4-9ce2-4e36-a4c5-bdfd00d8f07a_1860x810.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>Here, the main actions the model uses above are </span><code>Search</code><span> and </span><code>Finish</code><span>. The authors also provided  </span><code>Lookup</code><span> action that can search within the text of the article. In 1a/1b we can see the LLM doesn‚Äôt do very well answering the question on its own, because it doesn‚Äôt have enough information. In 1c we see that simply giving the algorithm a search function is not sufficient because the model seems to forget what it is doing by the time it hits the end. The authors‚Äô approach of including both actions and specific thought steps is able to search deeply and find the answer.</span></p><p><span>Answering factual questions is useful, but for real-world tasks, providing LLMs with an array of tools can enable a much broader array of powers. Imagine a customer service LLM able to do React-style chaining. We might give it actions </span><code>SearchDocs</code><span> for information about the service and policies, </span><code>SearchClientFacts</code><span> to look up particular facts about a client, </span><code>PythonCode</code><span> to run python programs, </span><code>ManualEscalation</code><span> to submit cases to a human for approval, </span><code>MessageClient</code><span> to send the client messages, and </span><code>Finish</code><span> to indicate completion. I‚Äôve fabricated a possible client assistance transcript below with React-style formatting:</span></p><pre><code><code>Question: Client is asking if they can cancel their meal kit order.

Thought 1: I need to know our policy for cancelling orders.
Act 1: SearchDocs[Meal kit cancellation policy]
Obs 1: Meal kits can be cancelled up until 3 days before their ship date, after which manual escalation is necessary.

Thought 2: I need to know what the customer&#39;s ship date is.
Act 2: SearchClientFacts[NextShipDate]
Obs 2: &#39;2022-12-03&#39;

Thought 3: I need to know if that is 3 days from now.
Act 3: PythonCode[`from datetime import datetime; (datetime.strptime(&#39;2022-12-03&#39;, &#39;%Y-%m-%d&#39;) - datetime.now()).days`]
Obs 3: 2

Thought 4: This means the cancellation will require a manual escalation. 
Act 4: ManualEscalation[Client needs shipment cancelled.]
Obs 4: Case has been escalated to a human service provider.

Thought 5: I should tell the client what I have done.
Act 4: MessageClient[&#34;This order is happening in 2 days. That means we need to escalate to a manual cancellation. Please wait for a customer support person to reach out to you.&#34;]
Act 4: Message sent successfully

Thought 5: This means I am done.
Act 5: Finish[]</code></code></pre><p>We can now make agents that, on the basis of natural language questions or instructions, can perform actions in a wide action space. The potential is not limited to CX or sales any particular business workflow. Any set of behaviors where an agent manipulates symbols in a computer can in principle be performed with this schema. And it looks an awful lot like AGI.</p><p>This is not 10 year tech. It may be possible right now with off-the-shelf tools. But to make it work we need to set up the right feedback loops.</p><h2>The importance of training<div id="¬ßthe-importance-of-training"><div></div></div></h2><p>If you were to attempt the above CX example in the latest version of GPT-3, it‚Äôs unlikely that you would get consistently good behavior. The LLM needs to understand the power of its own tools and it needs to know what kinds of outcomes we the user desire.</p><p><span>There are many tools for doing this. Google Brain recently achieved SOTA question-answering performance via instruction tuning (</span><a href="https://arxiv.org/abs/2210.11416" rel="">arxiv</a><span>). The secret to OpenAI‚Äôs 002-text-davinci model seems to be attributable to a combination of instruction tuning and Reinforcement learning from Human Feedback (RLHF, </span><a href="https://openai.com/blog/instruction-following/" rel="">blogpost</a><span>), wherein humans rate the success of a given prompt. The ReAct paper takes advantage of a method developed by  Zelikman et al. (2022, </span><a href="https://arxiv.org/abs/2203.14465" rel="">arxiv</a><span>) to bootstrap a large number of ‚Äúvalid‚Äù chains for training by using LLMs to generate them. I suspect that the very best results will come from actual reinforcement learning where a system can actually be trained to produce better results as measured via a metric of interest. For example, consider a model generating marketing copy that could be trained on conversion rate data to produce copy with consistently higher conversion.</span></p><h2>Implementing a powerful AI feedback loop.<div id="¬ßimplementing-a-powerful-ai-feedback-loop"><div></div></div></h2><p>Schematically, the stack for a successful action-driven deployment will look something like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png" width="1008" height="991" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/eeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:991,&#34;width&#34;:1008,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:122427,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Feeceb678-28cd-4245-a405-f3fae98f8228_1008x991.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>In the middle we have the ‚Äúout of the box‚Äù foundation model. Prompts are engineered to ask for outcomes, passed into the LLM, and an output is received. Many of the recent ‚ÄúLLM‚Äù startups take advantage roughly of this toolset. Instruct GPT has made OpenAI‚Äôs version of this stack particularly easy to get started with.</p><p>On the left we have the External Cognitive Assets that can supercharge a model‚Äôs power. These can be any function that takes text as an input and provides text as an output, including searches, code interpreters, and chats with humans.</p><p><span>Finally on the right we have the task oriented training that‚Äôs needed to make this work well. This is the hard part. Some techniques, like instruction tuning, seem fairly straightforward to implement. RLHF is harder and involves tuning a PPO algorithm (</span><a href="https://openai.com/blog/openai-baselines-ppo/" rel="">OpenAI post</a><span>). RL will particularly benefit from proprietary datasets, especially usage logs.</span></p><p>Some startups will become very successful creating powerful feedback loops: Solving a customer pain point (maybe bootstrapping by starting with something very simple), collecting data about how to solve that better, training their models to be more consistent, and iterating. This is roughly what a moat will look like in AI, at least for now. But as the agents get more domain-general, the spaces that can be automated and offerings that are possible will expand.</p><h2>Early steps<div id="¬ßearly-steps"><div></div></div></h2><p>We‚Äôve already seen a few demos of giving models superpowers like this. For example Sergey Kereyev showed what can already happen when you give an LLM a Python interpreter: </p><div data-attrs="{&#34;url&#34;:&#34;https://twitter.com/sergeykarayev/status/1569377881440276481&#34;,&#34;full_text&#34;:&#34;Here&#39;s a brief glimpse of our INCREDIBLE near future.\n\nGPT-3 armed with a Python interpreter can\n¬∑ do exact math\n¬∑ make API requests\n¬∑ answer in unprecedented ways\n\nThanks to &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@goodside&lt;/span&gt; and &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@amasad&lt;/span&gt; for the idea and repl!\n\nPlay with it: &lt;a class=\&#34;tweet-url\&#34; href=\&#34;https://replit.com/@SergeyKarayev/gptpy\&#34;&gt;replit.com/@SergeyKarayev‚Ä¶&lt;/a&gt; &#34;,&#34;username&#34;:&#34;sergeykarayev&#34;,&#34;name&#34;:&#34;Sergey Karayev&#34;,&#34;date&#34;:&#34;Mon Sep 12 17:30:19 +0000 2022&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/FceL32AaIAApyZj.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/JnkiUyTQx1&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{},&#34;retweet_count&#34;:666,&#34;like_count&#34;:3992,&#34;expanded_url&#34;:{},&#34;video_url&#34;:null,&#34;belowTheFold&#34;:true}"><a href="https://twitter.com/sergeykarayev/status/1569377881440276481" target="_blank" rel=""><div><p><img src="https://substackcdn.com/image/twitter_name/w_96/sergeykarayev.jpg" alt="Twitter avatar for @sergeykarayev" loading="lazy"/></p><p><span>Sergey Karayev </span><span>@sergeykarayev</span></p></div></a><div><p><a href="https://twitter.com/sergeykarayev/status/1569377881440276481" target="_blank" rel=""><span>Here&#39;s a brief glimpse of our INCREDIBLE near future.

GPT-3 armed with a Python interpreter can
¬∑ do exact math
¬∑ make API requests
¬∑ answer in unprecedented ways

Thanks to </span><span>@goodside</span><span> and </span><span>@amasad</span><span> for the idea and repl!

Play with it: </span></a><a href="https://replit.com/@SergeyKarayev/gptpy" target="_blank" rel="">replit.com/@SergeyKarayev‚Ä¶</a><span> </span></p></div><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_600,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFceL32AaIAApyZj.jpg"/><img src="https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFceL32AaIAApyZj.jpg" alt="Image" loading="lazy"/></picture></div></div><a href="https://twitter.com/sergeykarayev/status/1569377881440276481" target="_blank" rel=""><div><p><span>5:30 PM ‚àô Sep 12, 2022</span></p><hr/><p><span href="https://twitter.com/sergeykarayev/status/1569377881440276481/likes"><span>3,992</span><span>Likes</span></span><span href="https://twitter.com/sergeykarayev/status/1569377881440276481/retweets"><span>666</span><span>Retweets</span></span></p></div></a></div><p>And the team at Adept built a model that can surf the web and perform basic tasks.</p><div data-attrs="{&#34;url&#34;:&#34;https://twitter.com/adeptailabs/status/1570144499187453952&#34;,&#34;full_text&#34;:&#34;1/7 We built a new model! It‚Äôs called Action Transformer (ACT-1) and we taught it to use a bunch of software tools. In this first video, the user simply types a high-level request and ACT-1 does the rest. Read on to see more examples ‚¨áÔ∏è &#34;,&#34;username&#34;:&#34;AdeptAILabs&#34;,&#34;name&#34;:&#34;Adept&#34;,&#34;date&#34;:&#34;Wed Sep 14 20:16:35 +0000 2022&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://substackcdn.com/image/upload/w_1028,c_limit,q_auto:best/l_twitter_play_button_rvaygk,w_88/p2n2dkeaagwffcjrscf3&#34;,&#34;link_url&#34;:&#34;https://t.co/mq7c0Vyd7N&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{},&#34;retweet_count&#34;:758,&#34;like_count&#34;:3747,&#34;expanded_url&#34;:{},&#34;video_url&#34;:&#34;https://video.twimg.com/ext_tw_video/1570144426772791296/pu/vid/720x810/7AfqTutfY-IywVgK.mp4?tag=12&#34;,&#34;belowTheFold&#34;:true}"><a href="https://twitter.com/adeptailabs/status/1570144499187453952" target="_blank" rel=""><div><p><img src="https://substackcdn.com/image/twitter_name/w_96/AdeptAILabs.jpg" alt="Twitter avatar for @AdeptAILabs" loading="lazy"/></p><p><span>Adept </span><span>@AdeptAILabs</span></p></div><p>1/7 We built a new model! It‚Äôs called Action Transformer (ACT-1) and we taught it to use a bunch of software tools. In this first video, the user simply types a high-level request and ACT-1 does the rest. Read on to see more examples ‚¨áÔ∏è </p></a><a href="https://twitter.com/adeptailabs/status/1570144499187453952" target="_blank" rel=""><div><p><span>8:16 PM ‚àô Sep 14, 2022</span></p><hr/><p><span href="https://twitter.com/adeptailabs/status/1570144499187453952/likes"><span>3,747</span><span>Likes</span></span><span href="https://twitter.com/adeptailabs/status/1570144499187453952/retweets"><span>758</span><span>Retweets</span></span></p></div></a></div><p>In terms of implementation, LangChain makes it easy to chain prompts together and provide LLMs with tools:</p><div data-attrs="{&#34;url&#34;:&#34;https://twitter.com/hwchase17/status/1589648221269557248&#34;,&#34;full_text&#34;:&#34;üö®Biggest &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@LangChainAI&lt;/span&gt; drop yetüö®\n\n‚ö°Ô∏èDynamic, zero-shot composition of multiple chains\n\nEasily plug in different subchains (google, REPL, wikipedia, DBs) just by telling the router LLM (in English!) when and how to use them\n\n`pip install langchain==0.0.8`\n\n&lt;a class=\&#34;tweet-url\&#34; href=\&#34;https://github.com/hwchase17/langchain/\&#34;&gt;github.com/hwchase17/lang‚Ä¶&lt;/a&gt; &#34;,&#34;username&#34;:&#34;hwchase17&#34;,&#34;name&#34;:&#34;Harrison Chase&#34;,&#34;date&#34;:&#34;Mon Nov 07 15:57:24 +0000 2022&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://substackcdn.com/image/upload/w_1028,c_limit,q_auto:best/l_twitter_play_button_rvaygk,w_88/hidtdzxmec0uyfhf0hjr&#34;,&#34;link_url&#34;:&#34;https://t.co/nItA7T0gKl&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{},&#34;retweet_count&#34;:33,&#34;like_count&#34;:170,&#34;expanded_url&#34;:{},&#34;video_url&#34;:&#34;https://video.twimg.com/ext_tw_video/1589648129925578752/pu/vid/1280x720/LvJycv3G76BJTY5w.mp4?tag=12&#34;,&#34;belowTheFold&#34;:true}"><a href="https://twitter.com/hwchase17/status/1589648221269557248" target="_blank" rel=""><div><p><img src="https://substackcdn.com/image/twitter_name/w_96/hwchase17.jpg" alt="Twitter avatar for @hwchase17" loading="lazy"/></p><p><span>Harrison Chase </span><span>@hwchase17</span></p></div></a><div><p><a href="https://twitter.com/hwchase17/status/1589648221269557248" target="_blank" rel=""><span>üö®Biggest </span><span>@LangChainAI</span><span> drop yetüö®

‚ö°Ô∏èDynamic, zero-shot composition of multiple chains

Easily plug in different subchains (google, REPL, wikipedia, DBs) just by telling the router LLM (in English!) when and how to use them

`pip install langchain==0.0.8`

</span></a><a href="https://github.com/hwchase17/langchain/" target="_blank" rel="">github.com/hwchase17/lang‚Ä¶</a><span> </span></p></div><a href="https://twitter.com/hwchase17/status/1589648221269557248" target="_blank" rel=""><div><p><span>3:57 PM ‚àô Nov 7, 2022</span></p><hr/><p><span href="https://twitter.com/hwchase17/status/1589648221269557248/likes"><span>170</span><span>Likes</span></span><span href="https://twitter.com/hwchase17/status/1589648221269557248/retweets"><span>33</span><span>Retweets</span></span></p></div></a></div><p><a href="http://Dust.tt" rel="">Dust.tt</a><span>  provides a web app and alternative model for chaining prompt components:</span></p><div data-attrs="{&#34;url&#34;:&#34;https://mobile.twitter.com/goodside/status/1588262930553786368&#34;,&#34;full_text&#34;:&#34;&lt;span class=\&#34;tweet-fake-link\&#34;&gt;@hwchase17&lt;/span&gt; &lt;span class=\&#34;tweet-fake-link\&#34;&gt;@dust4ai&lt;/span&gt; takes some work to wrap your head around, but it&#39;s very powerful ‚Äî gives a collapsible tree UI for representing k-shot example datasets, prompt templates, and prompt chaining with intermediate JS code. Replaces a lot of code around prompt APIs. &lt;a class=\&#34;tweet-url\&#34; href=\&#34;http://dust.tt\&#34;&gt;dust.tt&lt;/a&gt; &#34;,&#34;username&#34;:&#34;goodside&#34;,&#34;name&#34;:&#34;Riley Goodside&#34;,&#34;date&#34;:&#34;Thu Nov 03 20:12:45 +0000 2022&#34;,&#34;photos&#34;:[{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/FgqjoKnVsAA8ADA.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/ScwqIRyU4j&#34;,&#34;alt_text&#34;:null},{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/FgqjpHDVUAAi9wI.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/ScwqIRyU4j&#34;,&#34;alt_text&#34;:null},{&#34;img_url&#34;:&#34;https://pbs.substack.com/media/Fgqjv9fVQAAdPVM.jpg&#34;,&#34;link_url&#34;:&#34;https://t.co/ScwqIRyU4j&#34;,&#34;alt_text&#34;:null}],&#34;quoted_tweet&#34;:{},&#34;retweet_count&#34;:4,&#34;like_count&#34;:33,&#34;expanded_url&#34;:{},&#34;video_url&#34;:null,&#34;belowTheFold&#34;:true}"><a href="https://mobile.twitter.com/goodside/status/1588262930553786368" target="_blank" rel=""><div><p><img src="https://substackcdn.com/image/twitter_name/w_96/goodside.jpg" alt="Twitter avatar for @goodside" loading="lazy"/></p><p><span>Riley Goodside </span><span>@goodside</span></p></div></a><div><p><a href="https://mobile.twitter.com/goodside/status/1588262930553786368" target="_blank" rel=""><span>@hwchase17</span><span> </span><span>@dust4ai</span><span> takes some work to wrap your head around, but it&#39;s very powerful ‚Äî gives a collapsible tree UI for representing k-shot example datasets, prompt templates, and prompt chaining with intermediate JS code. Replaces a lot of code around prompt APIs. </span></a><a href="http://dust.tt" target="_blank" rel="">dust.tt</a><span> </span></p></div><div><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_600,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFgqjoKnVsAA8ADA.jpg"/><img src="https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFgqjoKnVsAA8ADA.jpg" alt="Image" loading="lazy"/></picture></div></div><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_600,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFgqjpHDVUAAi9wI.jpg"/><img src="https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFgqjpHDVUAAi9wI.jpg" alt="Image" loading="lazy"/></picture></div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_600,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFgqjv9fVQAAdPVM.jpg"/><img src="https://substackcdn.com/image/fetch/w_600,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fpbs.substack.com%2Fmedia%2FFgqjv9fVQAAdPVM.jpg" alt="Image" loading="lazy"/></picture></div></div></div><a href="https://mobile.twitter.com/goodside/status/1588262930553786368" target="_blank" rel=""><div><p><span>8:12 PM ‚àô Nov 3, 2022</span></p><hr/><p><span href="https://mobile.twitter.com/goodside/status/1588262930553786368/likes"><span>33</span><span>Likes</span></span><span href="https://mobile.twitter.com/goodside/status/1588262930553786368/retweets"><span>4</span><span>Retweets</span></span></p></div></a></div><p>These systems are not a theoretical academic demo. You can hack on them starting now. Over the next few months we‚Äôre going to see increasingly surprising and useful applications rolling out. If you can think of a job that involves manipulating symbols on a computer: Action-driven AI is coming. I‚Äôm looking forward to covering this as it unfolds, but a few things I‚Äôm specifically planning to cover:</p><ul><li><p>How do you actually build the stack in the diagram above, incorporating both external cognitive assets and training?</p></li><li><p>What‚Äôs the deal with instruction tuning? Is it actually so great? How does Instruction Tuning and RLHF actually work? Why is this hard? Can we make it easy?</p></li><li><p>What will the market look like? What sort of products does this space need to succeed? What sort of startups will thrive?</p></li><li><p><span>What does the rise of agentic algorithms mean for media and society? My hope is that there will be a rebalance of power of algorithms in favor of the consumer, but much remains in the air. I learned a lot building </span><a href="https://vibecheck.network" rel="">vibecheck.network</a><span>.</span></p></li></ul><p>Stay tuned! ‚Ä¶and email me or DM me if you want to chat or collaborate.</p><h2>PS: A note on alignment<div id="¬ßps-a-note-on-alignment"><div></div></div></h2><p><span>I‚Äôm not an expert on alignment but if this sort of stack really does work, it is the </span><a href="https://intelligence.org/2017/10/13/fire-alarm/" rel="">AGI Fire alarm</a><span>. If we can build a system that can take real world actions to achieve useful goals, all the pieces are in place for a very powerful AI to be created. If you are in this space and building machines like this it is incumbent on you to learn about AI alignment risks and consider how you plan to maintain alignment with your system early on. If you are a bit skeptical or want to learn more, you might be interested in Stuart Russell‚Äôs book </span><em>Human Compatible</em><span> [</span><a href="https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558632/ref=asc_df_0525558632/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=459440273404&amp;hvpos=&amp;hvnetw=g&amp;hvrand=7165565356548324582&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9032066&amp;hvtargid=pla-951654604793&amp;psc=1" rel="">amazon</a><span>]. Russell is a lifelong AI researcher and speaks as a domain expert, not a wild-eyed doomer. For the lazy, Scott Alexander reviews it </span><a href="https://slatestarcodex.com/2020/01/30/book-review-human-compatible/" rel="">here</a><span>.</span></p><div><div><p>Thanks for reading Causal Deference! Subscribe for free to receive new posts and support my work.</p><div><form action="/api/v1/free?nojs=true" method="post" novalidate=""></form></div></div></div></div></div></div>
  </body>
</html>
