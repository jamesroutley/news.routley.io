<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://152334H.github.io/blog/non-determinism-in-gpt-4/">Original</a>
    <h1>Non-determinism in GPT-4 is caused by Sparse MoE</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>It’s <a href="https://twitter.com/BorisMPower/status/1608522707372740609" target="_blank" rel="noopener noreffer">well-known</a> at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at <code>temperature=0.0</code>. This is an odd behavior if you’re used to dense decoder-only models, where temp=0 should imply <a href="https://nn.labml.ai/sampling/greedy.html" target="_blank" rel="noopener noreffer">greedy sampling</a> which should imply full determinism, because the logits for the next token should be a pure function of the input sequence &amp; the model weights.</p>
<p>When asked about this behaviour at the <a href="https://humanloop.com/blog/openai-plans" target="_blank" rel="noopener noreffer">developer roundtables</a> during OpenAI’s World Tour, the responses of the members of technical staff were something along the lines of,</p><blockquote><p>Honestly, we’re confused as well. We think there might be some bug in our systems, or some <a href="https://twitter.com/taliaringer/status/1511411984398487564" target="_blank" rel="noopener noreffer">non-determinism in optimized floating point calculations</a>…</p></blockquote><p>And internally, I was thinking – okay, I know the latter point is true sometimes, and maybe OpenAI doesn’t have enough engineers to look into a problem as small as this. I felt a little bit <a href="https://www.lesswrong.com/s/zpCiuR4T343j9WkcK" target="_blank" rel="noopener noreffer">confused</a> when I noticed a reference to this behavior <a href="https://community.openai.com/t/a-question-on-determinism/8185/2" target="_blank" rel="noopener noreffer">over 3 years ago</a> – 3 years, and this couldn’t be fixed?</p><p>But I didn’t have a meaningful alternative explanation for the phenomenon. After all, why would you <em>want</em> to keep things random? Ilya’s always going on about <a href="https://www.youtube.com/watch?v=Yf1o0TQzry8" target="_blank" rel="noopener noreffer">reliability</a>, right? There was no way OpenAI <em>wanted</em> to keep determinism bugged, so an unresolvable hardware limitation was the best explanation.</p><hr/><p>3 months later, reading a paper while on board a <a href="https://www.flightradar24.com/data/flights/ca969#316fde52" target="_blank" rel="noopener noreffer">boring flight</a> home, I have my answer.</p><p>In the recent <a href="https://arxiv.org/abs/2308.00951" target="_blank" rel="noopener noreffer">Soft MoE</a> paper, there was an interesting blurb in Section 2.2 that sparked a connection:</p><figure><img src="https://152334H.github.io/blog/non-determinism-in-gpt-4/Pasted%20image%2020230804191515.png"/></figure><blockquote><p>Under capacity constraints, all Sparse MoE approaches route tokens in groups of a fixed size and enforce (or encourage) balance within the group. When groups contain tokens from different sequences or inputs, these tokens often compete against each other for available spots in expert buffers. <strong>As a consequence, the model is no longer deterministic at the sequence-level, but only at the batch-level</strong>, as some input sequences may affect the final prediction for other inputs</p></blockquote><p>It is currently <a href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure" target="_blank" rel="noopener noreffer">public knowledge</a> that GPT-4 is a Mixture of Experts model. Given that GPT-4 was <a href="https://cdn.openai.com/papers/gpt-4.pdf" target="_blank" rel="noopener noreffer">trained before Q2 2022</a>, and that Sparse Mixture-of-Experts have existed <a href="https://arxiv.org/abs/2101.03961" target="_blank" rel="noopener noreffer">long before that</a>, I think the following hypothesis is justified:</p><blockquote><p>The GPT-4 API is hosted with a backend that does <a href="https://www.anyscale.com/blog/continuous-batching-llm-inference" target="_blank" rel="noopener noreffer">batched inference</a>. Although some of the randomness may be explained by other factors, <strong>the vast majority</strong> of non-determinism in the API is explainable by its Sparse MoE architecture failing to enforce per-sequence determinism.</p></blockquote><p>This is either completely wrong, or something that was already obvious and well-known to people developing MoE models. How can we verify this?</p><h2 id="are-you-really-sure-it-isnt-hardware">Are you really sure it isn’t hardware?</h2><p>Not yet. Let’s <a href="https://chat.openai.com/share/4a60bf97-0058-41d1-8a93-983230b02169" target="_blank" rel="noopener noreffer">ask GPT-4</a> to write a script to test our hypothesis:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span><span>42
</span><span>43
</span><span>44
</span><span>45
</span><span>46
</span><span>47
</span><span>48
</span><span>49
</span><span>50
</span><span>51
</span><span>52
</span><span>53
</span><span>54
</span><span>55
</span><span>56
</span><span>57
</span><span>58
</span><span>59
</span><span>60
</span><span>61
</span><span>62
</span><span>63
</span><span>64
</span><span>65
</span><span>66
</span><span>67
</span><span>68
</span><span>69
</span><span>70
</span><span>71
</span><span>72
</span><span>73
</span><span>74
</span><span>75
</span><span>76
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>os</span>
</span></span><span><span><span>import</span> <span>json</span>
</span></span><span><span><span>import</span> <span>tqdm</span>
</span></span><span><span><span>import</span> <span>openai</span>
</span></span><span><span><span>from</span> <span>time</span> <span>import</span> <span>sleep</span>
</span></span><span><span><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>
</span></span><span><span>
</span></span><span><span><span>chat_models</span> <span>=</span> <span>[</span><span>&#34;gpt-4&#34;</span><span>,</span> <span>&#34;gpt-3.5-turbo&#34;</span><span>]</span>
</span></span><span><span><span>message_history</span> <span>=</span> <span>[</span>
</span></span><span><span>    <span>{</span><span>&#34;role&#34;</span><span>:</span> <span>&#34;system&#34;</span><span>,</span> <span>&#34;content&#34;</span><span>:</span> <span>&#34;You are a helpful assistant.&#34;</span><span>},</span>
</span></span><span><span>    <span>{</span><span>&#34;role&#34;</span><span>:</span> <span>&#34;user&#34;</span><span>,</span> <span>&#34;content&#34;</span><span>:</span> <span>&#34;Write a unique, surprising, extremely randomized story with highly unpredictable changes of events.&#34;</span><span>}</span>
</span></span><span><span><span>]</span>
</span></span><span><span>
</span></span><span><span><span>completion_models</span> <span>=</span> <span>[</span><span>&#34;text-davinci-003&#34;</span><span>,</span> <span>&#34;text-davinci-001&#34;</span><span>,</span> <span>&#34;davinci-instruct-beta&#34;</span><span>,</span> <span>&#34;davinci&#34;</span><span>]</span>
</span></span><span><span><span>prompt</span> <span>=</span> <span>&#34;[System: You are a helpful assistant]</span><span>\n\n</span><span>User: Write a unique, surprising, extremely randomized story with highly unpredictable changes of events.</span><span>\n\n</span><span>AI:&#34;</span>
</span></span><span><span>
</span></span><span><span><span>results</span> <span>=</span> <span>[]</span>
</span></span><span><span>
</span></span><span><span><span>import</span> <span>time</span>
</span></span><span><span><span>class</span> <span>TimeIt</span><span>:</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>name</span><span>):</span> <span>self</span><span>.</span><span>name</span> <span>=</span> <span>name</span>
</span></span><span><span>    <span>def</span> <span>__enter__</span><span>(</span><span>self</span><span>):</span> <span>self</span><span>.</span><span>start</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>
</span></span><span><span>    <span>def</span> <span>__exit__</span><span>(</span><span>self</span><span>,</span> <span>*</span><span>args</span><span>):</span> <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>self</span><span>.</span><span>name</span><span>}</span><span> took </span><span>{</span><span>time</span><span>.</span><span>time</span><span>()</span> <span>-</span> <span>self</span><span>.</span><span>start</span><span>}</span><span> seconds&#34;</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>C</span> <span>=</span> <span>30</span>  <span># number of completions to make per model</span>
</span></span><span><span><span>N</span> <span>=</span> <span>128</span> <span># max_tokens</span>
</span></span><span><span><span># Testing chat models</span>
</span></span><span><span><span>for</span> <span>model</span> <span>in</span> <span>chat_models</span><span>:</span>
</span></span><span><span>    <span>sequences</span> <span>=</span> <span>set</span><span>()</span>
</span></span><span><span>    <span>errors</span> <span>=</span> <span>0</span> <span># although I track errors, at no point were any errors ever emitted</span>
</span></span><span><span>    <span>with</span> <span>TimeIt</span><span>(</span><span>model</span><span>):</span>
</span></span><span><span>        <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>C</span><span>):</span>
</span></span><span><span>            <span>try</span><span>:</span>
</span></span><span><span>                <span>completion</span> <span>=</span> <span>openai</span><span>.</span><span>ChatCompletion</span><span>.</span><span>create</span><span>(</span>
</span></span><span><span>                    <span>model</span><span>=</span><span>model</span><span>,</span>
</span></span><span><span>                    <span>messages</span><span>=</span><span>message_history</span><span>,</span>
</span></span><span><span>                    <span>max_tokens</span><span>=</span><span>N</span><span>,</span>
</span></span><span><span>                    <span>temperature</span><span>=</span><span>0</span><span>,</span>
</span></span><span><span>                    <span>logit_bias</span><span>=</span><span>{</span><span>&#34;100257&#34;</span><span>:</span> <span>-</span><span>100.0</span><span>},</span> <span># this doesn&#39;t really do anything, because chat models don&#39;t do &lt;|endoftext|&gt; much</span>
</span></span><span><span>                <span>)</span>
</span></span><span><span>                <span>sequences</span><span>.</span><span>add</span><span>(</span><span>completion</span><span>.</span><span>choices</span><span>[</span><span>0</span><span>]</span><span>.</span><span>message</span><span>[</span><span>&#39;content&#39;</span><span>])</span>
</span></span><span><span>                <span>sleep</span><span>(</span><span>1</span><span>)</span> <span># cheaply avoid rate limiting</span>
</span></span><span><span>            <span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span>
</span></span><span><span>                <span>print</span><span>(</span><span>&#39;something went wrong for&#39;</span><span>,</span> <span>model</span><span>,</span> <span>e</span><span>)</span>
</span></span><span><span>                <span>errors</span> <span>+=</span> <span>1</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Model </span><span>{</span><span>model</span><span>}</span><span> created </span><span>{</span><span>len</span><span>(</span><span>sequences</span><span>)</span><span>}</span><span> (</span><span>{</span><span>errors</span><span>=}</span><span>) unique sequences:&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>json</span><span>.</span><span>dumps</span><span>(</span><span>list</span><span>(</span><span>sequences</span><span>)))</span>
</span></span><span><span>    <span>results</span><span>.</span><span>append</span><span>((</span><span>len</span><span>(</span><span>sequences</span><span>),</span> <span>model</span><span>))</span>
</span></span><span><span><span># Testing completion models</span>
</span></span><span><span><span>for</span> <span>model</span> <span>in</span> <span>completion_models</span><span>:</span>
</span></span><span><span>    <span>sequences</span> <span>=</span> <span>set</span><span>()</span>
</span></span><span><span>    <span>errors</span> <span>=</span> <span>0</span>
</span></span><span><span>    <span>with</span> <span>TimeIt</span><span>(</span><span>model</span><span>):</span>
</span></span><span><span>        <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>C</span><span>):</span>
</span></span><span><span>            <span>try</span><span>:</span>
</span></span><span><span>                <span>completion</span> <span>=</span> <span>openai</span><span>.</span><span>Completion</span><span>.</span><span>create</span><span>(</span>
</span></span><span><span>                    <span>model</span><span>=</span><span>model</span><span>,</span>
</span></span><span><span>                    <span>prompt</span><span>=</span><span>prompt</span><span>,</span>
</span></span><span><span>                    <span>max_tokens</span><span>=</span><span>N</span><span>,</span>
</span></span><span><span>                    <span>temperature</span><span>=</span><span>0</span><span>,</span>
</span></span><span><span>                    <span>logit_bias</span> <span>=</span> <span>{</span><span>&#34;50256&#34;</span><span>:</span> <span>-</span><span>100.0</span><span>},</span> <span># prevent EOS</span>
</span></span><span><span>                <span>)</span>
</span></span><span><span>                <span>sequences</span><span>.</span><span>add</span><span>(</span><span>completion</span><span>.</span><span>choices</span><span>[</span><span>0</span><span>]</span><span>.</span><span>text</span><span>)</span>
</span></span><span><span>                <span>sleep</span><span>(</span><span>1</span><span>)</span>
</span></span><span><span>            <span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span>
</span></span><span><span>                <span>print</span><span>(</span><span>&#39;something went wrong for&#39;</span><span>,</span> <span>model</span><span>,</span> <span>e</span><span>)</span>
</span></span><span><span>                <span>errors</span> <span>+=</span> <span>1</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>\n</span><span>Model </span><span>{</span><span>model</span><span>}</span><span> created </span><span>{</span><span>len</span><span>(</span><span>sequences</span><span>)</span><span>}</span><span> (</span><span>{</span><span>errors</span><span>=}</span><span>) unique sequences:&#34;</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>json</span><span>.</span><span>dumps</span><span>(</span><span>list</span><span>(</span><span>sequences</span><span>)))</span>
</span></span><span><span>    <span>results</span><span>.</span><span>append</span><span>((</span><span>len</span><span>(</span><span>sequences</span><span>),</span> <span>model</span><span>))</span>
</span></span><span><span>
</span></span><span><span><span># Printing table of results</span>
</span></span><span><span><span>print</span><span>(</span><span>&#34;</span><span>\n</span><span>Table of Results:&#34;</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>&#34;Num_Sequences</span><span>\t</span><span>Model_Name&#34;</span><span>)</span>
</span></span><span><span><span>for</span> <span>num_sequences</span><span>,</span> <span>model_name</span> <span>in</span> <span>results</span><span>:</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>num_sequences</span><span>}</span><span>\t</span><span>{</span><span>model_name</span><span>}</span><span>&#34;</span><span>)</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>This final script is a little different from what you’d see if you clicked on the share link. I had to redo the script a few times along the way, because of a few problems:</p><ul><li><p>the OpenAI API was taking very long to respond. I had to add timestamp logging to check I wasn’t doing something wrong – I wasn’t, the API was simply <em>really</em> slow, with nearly 10 seconds of delay to call even 3.5 turbo. I wonder why?</p></li><li><p>some completion models were truncating their responses very early. I added a logit bias against EOS to try to fix this.</p></li><li><p>Related: there is no equivalent bias against the <code>&lt;|im_end|&gt;</code> token; the API returns, <code>Invalid key in &#39;logit_bias&#39;: 100265. Maximum value is 100257.</code> 100265 is the accurate value for <code>&lt;|im_end|&gt;</code>:</p><figure><img src="https://152334H.github.io/blog/non-determinism-in-gpt-4/Pasted%20image%2020230805031539.png"/></figure><p>I figured this lack-of-logit-bias problem for the chat models was a non-issue – most completions reached the max token length, and they were absurdly more non-deterministic anyway (adding the logit bias would realistically only increase the number of unique sequences)</p></li></ul><p>An hour of waiting and scripting later, and I got <a href="https://gist.github.com/152334H/047827ad3740627f4d37826c867a196e#comment---this-is-an-example-output-from-the-script" target="_blank" rel="noopener noreffer">confirmation</a>:</p><h3 id="empirical-results">Empirical Results</h3><p>Here are the results (3 attempts, <code>N=30</code>, <code>max_tokens=128</code>):</p><table><thead><tr><th>Model Name</th><th>Unique Completions (/30)</th><th>Average (/30)</th><th>Notes</th></tr></thead><tbody><tr><td>gpt-4</td><td>12,11,12</td><td>11.67</td><td></td></tr><tr><td>gpt-3.5-turbo</td><td>4,4,3</td><td>3.67</td><td></td></tr><tr><td>text-davinci-003</td><td>3,2,4</td><td>3.00</td><td></td></tr><tr><td>text-davinci-001</td><td>2,2,2</td><td>2.00</td><td></td></tr><tr><td>davinci-instruct-beta</td><td>1,1,1</td><td>deterministic</td><td>Outputs deteriorated into repeated loop</td></tr><tr><td>davinci</td><td>1,1,1</td><td>deterministic</td><td>Outputs deteriorated into repeated loop</td></tr></tbody></table><p>Before I noticed the <code>logit_bias</code> problem, I also obtained the following results (<code>max_tokens=256</code>):</p><table><thead><tr><th>Model Name</th><th>Unique Completions (/30)</th><th>Notes</th></tr></thead><tbody><tr><td>gpt-4</td><td>30</td><td></td></tr><tr><td>gpt-3.5-turbo</td><td>9</td><td></td></tr><tr><td>text-davinci-003</td><td>5</td><td></td></tr><tr><td>text-davinci-001</td><td>2</td><td>Noticed the logit bias problem at this point</td></tr></tbody></table><h3 id="yes-im-sure">Yes, I’m sure</h3><p>The number of unique completions from GPT-4 is <strong>ridiculously</strong> high – practically <em>always</em> non-deterministic with longer outputs. This almost certainly confirms that something is up with GPT-4.</p><p>Additionally, all other models that do not collapse into a repetitive useless loop experience some degree of non-determinism as well. This lines up with the public claim that unreliable GPU calculations are responsible for some degree of randomness. However,</p><ol><li>I’m still partially confused by the gradual increase in randomness from text-davinci-001 up to gpt-3.5-turbo. I don’t have a neat explanation for why 003 is reliably more random than 001, or turbo more so than 003. Although I expect only the chat models to be MoE models, and not any of the 3.5 completion models, I don’t feel confident based on the current evidence available.</li><li>This is only evidence that <em>something</em> is causing GPT-4 to be much, much more non-deterministic than other models. Maybe I’m still completely wrong about the MoE part. Maybe it’s just because of parameter count. (but then – why would Turbo be more unpredictable than davinci? Turbo’s faster; if you assumed the same architecture, Turbo ought to be smaller)</li></ol><h2 id="implications">Implications</h2><p>It’s actually pretty crazy to me, that this looks true. For a few reasons:</p><h3 id="were-so-far-behind">We’re so far behind</h3><p><em>If</em> the non-determinism is an inherent feature of batched inference with Sparse MoE, then this fact should be visibly obvious to anyone that works with models in that vein.</p><p>Given that the vast majority of GPT-4 users still have no idea what is causing their API calls to be unreliable, it should be concluded that (I am completely wrong, OR) too few people know anything about MoE models to launch this explanation into the public consciousness.</p><p>It implies that Google Deepmind knew, and found it trivial enough to write as a throwaway sentence in a paper. It implies that I should be a lot more bullish on them, and a lot more bearish against every other wannabe foundation model org that’s still working on dense models only.</p><h3 id="gpt-35-turbo-may-be-moe-too">GPT-3.5-Turbo may be MoE too</h3><p>I heard a <a href="https://sorry-not.leaking.sources.of.info" target="_blank" rel="noopener noreffer">rumour, once</a>, about 3.5-turbo sharing the <em>same architecture</em> as GPT-4; just with much much less parameters than it, or even GPT-3.</p><p>And, when I heard it, I was thinking: <em>Nah, that sounds too complicated for a small public model. Why wouldn’t they just use a dense one? Fits on one GPU, no complexity overhead, really simple to optimise…</em></p><p>Fast forward to now, and we’re still suffering a regime where it takes <a href="https://arxiv.org/abs/2307.09288" target="_blank" rel="noopener noreffer">70B parameters to meet Turbo’s performance</a> – a number which just <em>doesn’t make sense</em> for how much traffic OpenAI’s handling, and how much speed they get.</p><p>It’s also easy to notice that Turbo is the only other model in the API that has its <a href="https://platform.openai.com/docs/api-reference/completions/create#completions/create-logprobs" target="_blank" rel="noopener noreffer">logprobs</a> restricted from public view. The common explanation was that they were restricted to <a href="https://arxiv.org/abs/2203.10163" target="_blank" rel="noopener noreffer">prevent increased accuracy in distillation</a>, something which sounds a little bit naive today given <a href="https://arxiv.org/abs/2306.02707" target="_blank" rel="noopener noreffer">Orca</a> and <a href="https://huggingface.co/datasets/Open-Orca/OpenOrca" target="_blank" rel="noopener noreffer">others</a>. OpenAI has also <a href="https://openai.com/blog/gpt-4-api-general-availability" target="_blank" rel="noopener noreffer">publicly stated</a> that they’re working on getting the logprobs integrated into ChatCompletions, making “prevent distillation” less likely than “this is hard to engineer reliably because they’re inherently too random right now”:</p><figure><img src="https://152334H.github.io/blog/non-determinism-in-gpt-4/Pasted%20image%2020230805035805.png"/></figure><p>But, still, as I said earlier – not fully confident on this one. Maybe someone should open a <a href="https://manifold.markets/" target="_blank" rel="noopener noreffer">prediction market</a>?</p><h2 id="conclusion">Conclusion</h2><ul><li>Everyone knows that OpenAI’s GPT models are non-deterministic at temperature=0</li><li>It is typically attributed to non-deterministic CUDA optimised floating point op inaccuracies</li><li>I present a different hypothesis: <strong>batched inference in sparse MoE models are the root cause of most non-determinism in the GPT-4 API</strong>. I explain why this is a neater hypothesis than the previous one.</li><li>I empirically demonstrate that API calls to GPT-4 (and potentially some 3.5 models) are substantially more non-deterministic than other OpenAI models.</li><li>I speculate that GPT-3.5-turbo may be MoE as well, due to speed + non-det + logprobs removal.</li></ul></div></div>
  </body>
</html>
