<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2405.09673">Original</a>
    <h1>LoRA Learns Less and Forgets Less</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Biderman,+D">Dan Biderman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ortiz,+J+G">Jose Gonzalez Ortiz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Portes,+J">Jacob Portes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Paul,+M">Mansheej Paul</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Greengard,+P">Philip Greengard</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jennings,+C">Connor Jennings</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=King,+D">Daniel King</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Havens,+S">Sam Havens</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chiley,+V">Vitaliy Chiley</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Frankle,+J">Jonathan Frankle</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Blakeney,+C">Cody Blakeney</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cunningham,+J+P">John P. Cunningham</a></p></div>            
    <p><a href="https://bytes.zone/pdf/2405.09673">View PDF</a>
    <a href="https://arxiv.org/html/2405.09673v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\approx$100K prompt-response pairs) and continued pretraining ($\approx$10B unstructured tokens) data regimes. Our results show that, in most settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model&#39;s performance on tasks outside the target domain. We show that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations. We show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Dan Biderman [<a href="https://bytes.zone/show-email/7e018ac3/2405.09673">view email</a>]      </p></div></div>
  </body>
</html>
