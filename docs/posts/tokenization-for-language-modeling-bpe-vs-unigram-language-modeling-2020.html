<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ndingwall.github.io/blog/tokenization">Original</a>
    <h1>Tokenization for language modeling: BPE vs. Unigram Language Modeling (2020)</h1>
    
    <div id="readability-page-1" class="page"><div><div>
<div>
<p><em>My thanks to <a href="https://web.stanford.edu/~cgpotts/">Chris Potts</a> and to <a href="https://www.linkedin.com/in/ericjeske/">Eric Jeske</a> for valuable discussion of the ideas here and for their comments on early drafts of this post.</em></p>
<p>Imagine that you&#39;re learning English and you come across the word <code>destabilizing</code>. You&#39;ve never seen it before, but you have seen <code>de-</code> as a prefix in lots of other words and know it&#39;s something to do with negating what follows, and you&#39;ve seen <code>stabilizing</code> too. With that information, you can come to a more-or-less perfect understanding of <code>destabilizing</code>.</p>
<p>Now imagine that you mis-parsed the word and read it as <code>dest-abilizing</code>. Well, you&#39;ve seen <code>dest</code> as the beginning of <code>destination</code>, <code>destiny</code> and <code>destroy</code>—perhaps <code>dest</code> is something to do with a future state? You&#39;ve seen <code>ize</code> and <code>ing</code> suffixes, guess that <code>abilizing</code> means something like <code>enabling</code>, and conclude that <code>destabilizing</code> means something like <em>becoming able to do something</em>.</p>
<p>Unfortunately, common language tokenizers (including those used in Bert and GPT-2) misread words in just this way <em>all the time</em>:</p>

</div>
</div>
</div><div><div>
<p>There&#39;s no overlap between Bert&#39;s tokenizations of <code>stabilizing</code> and <code>destabilizing</code>: the model must learn about these words completely independently. GPT-2&#39;s tokenizer at least shares <code>izing</code> so the model has an indication that the words have something in common (albeit with <code>capsizing</code> too):</p>
</div>
</div><div><div>
<p>Now consider <code>stigmatize</code> and <code>destigmatize</code>: not only is the significance of <code>de</code> missed, but both tokenizers give the model the misleading information that there is a relationship between <code>destigmatize</code> and <code>destinies</code>, even though the <code>de</code> is operating as a negator in the first, and part of the morpheme <code>des</code> in the second:</p>
</div>
</div><div><div>
<div>
<p>A human reading <code>dest-igmat-ize</code> would have trouble understanding it until they recognized that the <code>st</code> belongs in the next group. Indeed, hyphenation dictionaries for e-readers disallow hyphens that break syllables for exactly this reason.</p>
<h2 id="Byte-pair-encoding-(BPE)">Byte pair encoding (BPE)<a href="#Byte-pair-encoding-(BPE)"> </a></h2><p>The tokenizer used by GPT-2 (and most variants of Bert) is built using <a href="https://leimao.github.io/blog/Byte-Pair-Encoding/"><em>byte pair encoding</em></a> (BPE). Bert itself uses some proprietary heuristics to learn its vocabulary but uses the same greedy algorithm as BPE to tokenize. BPE comes from information theory: the objective is to maximally compress a dataset by replacing common substrings with tokens, morphology be damned.</p>
<p>Are we disadvantaging language models by partitioning words in ways that obscure the relationships between them? Perhaps this doesn&#39;t matter. Traditional NLP models used whole words as tokens which by definition have to be learned independently, so we&#39;re better off than we used to be! Bert and GPT-2 are able to achieve astounding results with these &#39;misleading&#39; tokens. The models are large enough that (presumably) the <code>dest</code> subword representation gets a little bit of <code>de-</code> plus a little bit of <code>destiny/destroy/destination/etc</code>, and they train on so much data that they see variants of <code>stabilize</code> often enough to learn the same aspects of &#39;meaning&#39; redundantly.</p>
<p>On the other hand, the job of machine learning algorithms is to find and learn regularities in the training data. Providing them with the tokens that best capture these regularities (such as prefixes and suffixes) seems likely to help. Perhaps such a model could be smaller or could train more quickly.</p>
<p>Oddly, though, BPE has been more-or-less universally accepted as the standard preprocessing step for language modelling. Even Google&#39;s <a href="https://arxiv.org/abs/1910.10683">T5 paper</a> which, in 30 meticulous pages, adjusted and evaluated every hyperparameter imaginable⁠, treated the tokenizer as fixed. The <a href="https://arxiv.org/abs/1907.11692">RoBERTa paper</a> does consider tokenization in the <em>Text Encoding</em> section, but only compares variants of BPE (Bert&#39;s tokenizer and vanilla BPE).</p>

</div>
</div>
</div><div><div>
<div>
<h2 id="Unigram-language-modeling">Unigram language modeling<a href="#Unigram-language-modeling"> </a></h2><p><a href="https://arxiv.org/abs/2004.03720">Recent work by Kaj Bostrom and Greg Durrett</a> showed that by simply replacing BPE with a different method, morphology is better preserved and a language model trained on the resulting tokens shows improvements when fine tuned on downstream tasks.</p>
<p>Surprisingly, the replacement—unigram language modeling—has been under our noses all along. (Don&#39;t be confused with the <em>language model</em> in the name: this is still the preprocessing step.) Unigram LM tokenizers are already implemented in Google&#39;s <a href="https://github.com/google/sentencepiece">sentencepiece library</a>, but it was introduced in <a href="https://arxiv.org/abs/1804.10959">a paper ostensibly about data augmentation for translation models</a> and so the wider significance seems to have been missed.</p>
<p>Bostrom and Durrett&#39;s paper show some examples illustrating the difference:</p>
<figure>
  
    <img src="https://ndingwall.github.io/blog/images/copied_from_nb/images/tokenization/bostrom_examples.png" alt=""/>
    
    
</figure>

<p>They explain that unigram LMs recover common suffixes like &#39;ly&#39;, &#39;s&#39; and &#39;ing&#39; much more often than BPE. This is exactly what we wanted! I trained my own unigram LM and BPE tokenizers on 10 million randomly-selected sentences from Wikipedia to see if I could replicate these differences and to compare how they treat my <code>destabilizing</code> example. Here are the results with vocabularies limited to 8k and 32k words:</p>

</div>
</div>
</div><div><div>
<p>This seems to have worked well! Both Unigram LM tokenizers treat <code>destabilizing</code> as <code>de+stabilizing</code>, but BPE doesn&#39;t. (It&#39;s not quite this simple: I&#39;ve hidden the fact that the first token in a word is necessarily distinct from any subsequent token to keep track of word boundaries; more on that later.)</p>
</div>
</div><div><div>
<div>
<h2 id="Evaluating-tokenizers">Evaluating tokenizers<a href="#Evaluating-tokenizers"> </a></h2><p>Bostrom and Durrett&#39;s paper includes comparisons of the length distribution of tokens and the frequency with which they are used, but that doesn&#39;t reflect how a tokenizer captures morphology. To do that, we need gold standard tokenizations: the ones that best capture pronunciation while preserving spelling. Luckily, that&#39;s exactly what Merriam Webster provides, at least for headwords (i.e. the base form of a word from which others derive: <code>stabilize</code> is the headword of <code>stabilizing</code>). For example, here&#39;s its entry for <code>destabilize</code> with the pronunciation guide highlighted:</p>
<figure>
  
    <img src="https://ndingwall.github.io/blog/images/copied_from_nb/images/tokenization/mw_destabilize.png" alt=""/>
    
    
</figure>


</div>
</div>
</div><div><div>
<p>I&#39;ll call a subword <em>morphologically sound</em> if it consists of one or more entire Merriam Webster subwords. For example, <code>de</code>, <code>lize</code>, <code>stabilize</code> and <code>destabilize</code> are all morphologically sound, but <code>dest</code> isn&#39;t. We can&#39;t expect a tokenizer to choose &#39;morphologically sound&#39; subwords every time since that would require an enormous vocabulary; nevertheless, if tokenizer A produces more morphologically sound subwords than tokenizer B, it seems reasonable to conclude that tokenizer A better captures morphology than B. The table below shows a few examples color-coded to indicate which subwords are morphologically sound:</p>
</div>
</div><div><div>
<div>
<p>I score how well each tokenizer does on this task with the fraction of the subwords produced by a tokenizer that are morphologically sound (i.e. green). Bert gets a score of 6/22 = 27% for the sample above because 6 of its 22 subwords are green (<code>pen</code>, <code>ther</code>, <code>inter</code>, <code>mo</code>, <code>nod</code>, and <code>ular</code>), while GPT-2 gets 10/17 = 59% because 10 of its 17 subwords are green (including whole-word subwords like <code>spacing</code> and <code>penchant</code>).</p>
<p>We can&#39;t draw conclusions from this tiny sample, so I repeated the evaluation with a much larger sample of 8,000 words. Since having access to more subwords allows a tokenizer to make fewer splits and thus provides an advantage, I built tokenizers with different vocabulary sizes and plotted their scores against the vocabulary size (not counting tokens that include any non-ASCII characters to control for different numbers of characters represented).</p>

</div>
</div>
</div><div><div>
<p>It&#39;s clear that Unigram LM-based tokenizers beat the BPE-based ones by a substantial margin. This supports Bostrom and Durrett&#39;s claim that &#34;the  unigram LM method tends to produce more tokens that are morphologically interpretable.&#34;</p>
</div>
</div><div><div>
<div>
<h2 id="Speed">Speed<a href="#Speed"> </a></h2><p>We might worry that this comes at the cost of slower performance. But we needn&#39;t be concerned: while <em>learning</em> a Unigram LM tokenizer takes longer than BPE, it still took less than an hour for 10 million sentences on a 4-core, 32GB RAM EC2 machine (<code>r5ad.xlarge</code>):</p>

</div>
</div>
</div><div><div>
<div>
<p>Interestingly, the Unigram LM gets faster to learn as the vocab size increases. That&#39;s because the algorithm starts with a larger set and prunes vocab items until the desired vocab size is reached, while BPE initializes with an empty vocabulary and adds tokens.</p>
<p>For <em>inference</em>, the difference is insignificant (the shaded area and vertical bars show 95% confidence intervals estimated by repeating the experiments 10 times):</p>

</div>
</div>
</div><div><div>
<p>Bert and GPT-2&#39;s tokenizers are evaluated here using the <code>transformers</code> library which probably explains the slower performance compared to <code>sentencepiece</code>.</p>
</div>
</div><div><div>
<div>
<h2 id="What&#39;s-next?">What&#39;s next?<a href="#What&#39;s-next?"> </a></h2><p>In the short term, I share Bostrom and Durret&#39;s hope &#34;that developers of future pretrained language models will consider  adopting the unigram LM method over the more common BPE.&#34; But I suspect that there remain opportunities for further improvements.</p>
<p>First, as mentioned above, all tokenizers in common use treat subwords at the start of a word differently from those inside words. This is to permit unambiguous reconstruction of strings from token sequences (originally discussed in <a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a>). There may be other ways to achieve that aim without doubling the vocabulary, for instance by adding a <span>new_word</span> mask as an additional input dimension (which would be predicted by a seq-to-seq model as a secondary output).</p>
<p>Second, it&#39;s not clear to me using compression algorithms to preprocess inputs is a good idea at all. The renaissance of deep learning came after it was discovered that, with an appropriate architecture and sufficient compute, deep learning methods could learn better feature representations for images than humans could invent. That architecture—the convolutional neural network—embeds the nature of two-dimensional space and an invariance that objects can appear in different parts of an image.</p>
<p>The equivalent approach for natural language processing would have us treat raw characters (or bytes) as input. Character-level language modeling isn&#39;t novel: Andrej Karpathy&#39;s influential blog post <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> showed—in 2015—that character-level LSTMs could generate realistic text. Google&#39;s 2018 paper <a href="https://arxiv.org/abs/1808.04444">Character-Level Language Modeling with Deeper Self-Attention</a> showed an impressive ability to memorize and reproduce random character sequences appearing in natural text, but it did it with sequences of at most 512 characters: less than two tweets, and much shorter than sequences used in the leading language models.</p>
<p>This length limitation comes from the attention mechanism used in transformers which scales quadratically in the length of the sequence. Replacing a token sequence with an equivalent character sequence would be computationally intractable. Linguists often represent text sequences as trees, and this might provide a clue as to how we can reduce computation while embedding more of language&#39;s structure in model architectures.</p>
<figure>
  
    <img src="https://ndingwall.github.io/blog/images/copied_from_nb/images/tokenization/parse_tree.jpg" alt=""/>
    
    
</figure>

<p>_Example of a parse tree from <a href="https://en.wikipedia.org/wiki/Parse_tree">Wikipedia</a>._</p>
<p>Attention can model these trees since every token can directly attend to any other, unlike RNNs which model distances linearly. That is, attention gives <em>ball</em> a direct connection to <em>John</em>. But it&#39;s rare that a token should need to attend to another 500 tokens away, and so most of this computation is wasted. To best reflect a tree structure, we&#39;d want attention to be applied to windowed subsequences at the bottom layers of a network and for those layers to produce a shorter representation that can be combined via attention with the outputs from neighboring windows. That would allow the bottom layer to operate on characters while remaining computationally tractable. I have vague ideas about how this might be realized, but I&#39;m poorly-equipped to test them out! I&#39;d love to hear about attempts to do anything like this, or about why it wouldn&#39;t work. You can tweet me <a href="https://twitter.com/nick_dingwall">@ndingwall</a> or comment below.</p>

</div>
</div>
</div></div>
  </body>
</html>
