<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">Original</a>
    <h1>OpenAI O3 breakthrough high score on ARC-AGI-PUB</h1>
    
    <div id="readability-page-1" class="page"><div>


<p>OpenAI&#39;s new o3 system - trained on the ARC-AGI-1 Public Training set - has scored a breakthrough <strong>75.7%</strong> on the Semi-Private Evaluation set at our stated public leaderboard $10k compute limit. A high-compute (172x) o3 configuration scored <strong>87.5%</strong>.</p>

<p><img src="https://arcprize.org/media/images/blog/o-series-performance.jpg" alt="o Series Performance"/></p>

<p>This is a surprising and important step-function increase in AI capabilities, showing novel task adaptation ability never seen before in the GPT-family models. For context, ARC-AGI-1 took 4 years to go from 0% with GPT-3 in 2020 to 5% in 2024 with GPT-4o. All intuition about AI capabilities will need to get updated for o3.</p>

<p>The mission of ARC Prize goes beyond our first benchmark: to be a North Star towards AGI. And we&#39;re excited to be working with the OpenAI team and others next year to continue to design next-gen, enduring AGI benchmarks.</p>

<p>ARC-AGI-2 (same format - verified easy for humans, harder for AI) will launch alongside ARC Prize 2025. We&#39;re committed to running the Grand Prize competition until a high-efficiency, open-source solution scoring 85% is created.</p>

<p>Read on for the full testing report.</p>

<hr/>

<h2 id="openai-o3-arc-agi-results">OpenAI o3 ARC-AGI Results</h2>

<p>We tested o3 against two ARC-AGI datasets:</p>

<ul>
  <li><strong>Semi-Private Eval</strong>: 100 private tasks used to assess overfitting</li>
  <li><strong>Public Eval</strong>: 400 public tasks</li>
</ul>

<p>At OpenAI&#39;s direction, we tested at two levels of compute with variable sample sizes: 6 (high-efficiency) and 1024 (low-efficiency, 172x compute).</p>

<p>Here are the results.</p>

<div>
    <table>
    <thead>
        <tr>
        <th>Set</th>
        <th>Tasks</th>
        <th>Efficiency</th>
        <th>Score</th>
        <th>Retail Cost</th>
        <th>Samples</th>
        <th>Tokens</th>
        <th>Cost/Task</th>
        <th>Time/Task (mins)</th>
        </tr>
    </thead>
    <tbody>
        <tr>
        <td>Semi-Private</td>
        <td>100</td>
        <td>High</td>
        <td>75.7%</td>
        <td>$2,012</td>
        <td>6</td>
        <td>33M</td>
        <td>$20</td>
        <td>1.3</td>
        </tr>
        <tr>
        <td>Semi-Private</td>
        <td>100</td>
        <td>Low</td>
        <td>87.5%</td>
        <td>-</td>
        <td>1024</td>
        <td>5.7B</td>
        <td>-</td>
        <td>13.8</td>
        </tr>
        <tr>
        <td>Public</td>
        <td>400</td>
        <td>High</td>
        <td>82.8%</td>
        <td>$6,677</td>
        <td>6</td>
        <td>111M</td>
        <td>$17</td>
        <td>N/A</td>
        </tr>
        <tr>
        <td>Public</td>
        <td>400</td>
        <td>Low</td>
        <td>91.5%</td>
        <td>-</td>
        <td>1024</td>
        <td>9.5B</td>
        <td>-</td>
        <td>N/A</td>
        </tr>
    </tbody>
    </table>
</div>

<p>Note: o3 high-compute costs not available as pricing and feature availability is still TBD. The amount of compute was roughly 172x the low-compute configuration.</p>

<p>Due to variable inference budget, efficiency (e.g., compute cost) is now a required metric when reporting performance. We&#39;ve documented both the total costs and the cost per task as an initial proxy for efficiency. As an industry, we&#39;ll need to figure out <a href="https://x.com/mikeknoop/status/1868760635716386864" target="_blank">what metric best tracks efficiency</a>, but directionally, cost is a solid starting point.</p>

<p>The high-efficiency score of 75.7% is within the budget rules of ARC-AGI-Pub (costs &lt;$10k) and therefore qualifies as 1st place on the public leaderboard!</p>

<p>The low-efficiency score of 87.5% is quite expensive, but still shows that performance on novel tasks does improve with increased compute (at least up to this level.)</p>

<p>Despite the significant cost per task, these numbers aren&#39;t just the result of applying brute force compute to the benchmark. OpenAI&#39;s new o3 model represents a significant leap forward in AI&#39;s ability to adapt to novel tasks. This is not merely incremental improvement, but a genuine breakthrough, marking a qualitative shift in AI capabilities compared to the prior limitations of LLMs. o3 is a system capable of adapting to tasks it has never encountered before, arguably approaching human-level performance in the ARC-AGI domain.</p>

<p>Of course, such generality comes at a steep cost, and wouldn&#39;t quite be economical yet: you could pay a human to solve ARC-AGI tasks for roughly $5 per task (we know, we did that), while consuming mere cents in energy. Meanwhile o3 requires $17-20 per task in the low-compute mode. But cost-performance will likely improve quite dramatically over the next few months and years, so you should plan for these capabilities to become competitive with human work within a fairly short timeline.</p>

<p>o3&#39;s improvement over the GPT series proves that architecture is everything. You couldn&#39;t throw more compute at GPT-4 and get these results. Simply scaling up the things we were doing from 2019 to 2023 – take the same architecture, train a bigger version on more data – is not enough. Further progress is about new ideas.</p>

<hr/>

<h3 id="so-is-it-agi">So is it AGI?</h3>

<p>ARC-AGI serves as a critical benchmark for detecting such breakthroughs, highlighting generalization power in a way that saturated or less demanding benchmarks cannot. However, it is important to note that ARC-AGI is not an acid test for AGI – as we&#39;ve repeated dozens of times this year. It&#39;s a research tool designed to focus attention on the most challenging unsolved problems in AI, a role it has fulfilled well over the past five years.</p>

<p>Passing ARC-AGI does not equate to achieving AGI, and, as a matter of fact, I don&#39;t think o3 is AGI yet. o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence.</p>

<p>Furthermore, early data points suggest that the upcoming ARC-AGI-2 benchmark will still pose a significant challenge to o3, potentially reducing its score to under 30% even at high compute (while a smart human would still be able to score over 95% with no training). This demonstrates the continued possibility of creating challenging, unsaturated benchmarks without having to rely on expert domain knowledge. You&#39;ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.</p>

<h3 id="whats-different-about-o3-compared-to-older-models">What&#39;s different about o3 compared to older models?</h3>

<p>Why does o3 score so much higher than o1? And why did o1 score so much higher than GPT-4o in the first place? I think this series of results provides invaluable data points for the ongoing pursuit of AGI.</p>

<p>My mental model for LLMs is that they work as <a href="https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering" target="_blank">a repository of vector programs</a>. When prompted, they will fetch the program that your prompt maps to and &#34;execute&#34; it on the input at hand. LLMs are a way to store and operationalize millions of useful mini-programs via passive exposure to human-generated content.</p>

<p>This &#34;memorize, fetch, apply&#34; paradigm can achieve arbitrary levels of skills at arbitrary tasks given appropriate training data, but it cannot adapt to novelty or pick up new skills on the fly (which is to say that there is no fluid intelligence at play here.) This has been exemplified by the low performance of LLMs on ARC-AGI, the only benchmark specifically designed to measure adaptability to novelty – GPT-3 scored 0, GPT-4 scored near 0, GPT-4o got to 5%. Scaling up these models to the limits of what&#39;s possible wasn&#39;t getting ARC-AGI numbers anywhere near what basic brute enumeration could achieve years ago (up to 50%).</p>

<p>To adapt to novelty, you need two things. First, you need knowledge – a set of reusable functions or programs to draw upon. LLMs have more than enough of that. Second, you need the ability to recombine these functions into a brand new program when facing a new task – a program that models the task at hand. Program synthesis. LLMs have long lacked this feature. The o series of models fixes that.</p>

<p>For now, we can only speculate about the exact specifics of how o3 works. But o3&#39;s core mechanism appears to be natural language program search and execution within token space – at test time, the model searches over the space of possible Chains of Thought (CoTs) describing the steps required to solve the task, in a fashion perhaps not too dissimilar to AlphaZero-style Monte-Carlo tree search. In the case of o3, the search is presumably guided by some kind of evaluator model. To note, Demis Hassabis hinted back in <a href="https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/" target="_blank">a June 2023 interview</a> that DeepMind had been researching this very idea – this line of work has been a long time coming.</p>

<p>So while single-generation LLMs struggle with novelty, o3 overcomes this by generating and executing its own programs, where the program itself (the CoT) becomes the artifact of knowledge recombination. Although this is not the only viable approach to test-time knowledge recombination (you could also do test-time training, or search in latent space), it represents the current state-of-the-art as per these new ARC-AGI numbers.</p>

<p>Effectively, o3 represents a form of <em>deep learning-guided program search</em>. The model does test-time search over a space of &#34;programs&#34; (in this case, natural language programs – the space of CoTs that describe the steps to solve the task at hand), guided by a deep learning prior (the base LLM). The reason why solving a single ARC-AGI task can end up taking up tens of millions of tokens and cost thousands of dollars is because this search process has to explore an enormous number of paths through program space – including backtracking.</p>

<p>There are however two significant differences between what&#39;s happening here and what I meant when I previously described &#34;deep learning-guided program search&#34; as the best path to get to AGI. Crucially, the programs generated by o3 are <em>natural language instructions</em> (to be &#34;executed&#34; by a LLM) rather than <em>executable symbolic programs</em>. This means two things. First, that they cannot make contact with reality via execution and direct evaluation on the task – instead, they must be evaluated for fitness via another model, and the evaluation, lacking such grounding, might go wrong when operating out of distribution. Second, the system cannot autonomously acquire the ability to generate and evaluate these programs (the way a system like AlphaZero can learn to play a board game on its own.) Instead, it is reliant on expert-labeled, human-generated CoT data.</p>

<p>It&#39;s not yet clear what the exact limitations of the new system are and how far it might scale. We&#39;ll need further testing to find out. Regardless, the current performance represents a remarkable achievement, and a clear confirmation that intuition-guided test-time search over program space is a powerful paradigm to build AI systems that can adapt to arbitrary tasks.</p>

<h3 id="what-comes-next">What comes next?</h3>

<p>First of all, open-source replication of o3, facilitated by the ARC Prize competition in 2025, will be crucial to move the research community forward. A thorough analysis of o3&#39;s strengths and limitations is necessary to understand its scaling behavior, the nature of its potential bottlenecks, and anticipate what abilities further developments might unlock.</p>

<p>Moreover, ARC-AGI-1 is now saturating – besides o3&#39;s new score, the fact is that a large ensemble of low-compute Kaggle solutions can now score 81% on the private eval.</p>

<p>We&#39;re going to be raising the bar with a new version – ARC-AGI-2 - which has been in the works since 2022. It promises a major reset of the state-of-the-art. We want it to push the boundaries of AGI research with hard, high-signal evals that highlight current AI limitations.</p>

<p>Our early ARC-AGI-2 testing suggests it will be useful and extremely challenging, even for o3. And, of course, ARC Prize&#39;s objective is to produce a <em>high-efficiency</em> and <em>open-source</em> solution in order to win the Grand Prize. We currently intend to launch ARC-AGI-2 alongside ARC Prize 2025 (estimated launch: late Q1).</p>

<p>Going forward, the ARC Prize Foundation will continue to create new benchmarks to focus the attention of researchers on the hardest unsolved problems on the way to AGI. We&#39;ve started work on a third-generation benchmark which departs completely from the 2019 ARC-AGI format and incorporates some exciting new ideas.</p>

<hr/>

<h2 id="get-involved-open-source-analysis">Get Involved: Open-Source Analysis</h2>

<p>Today, we&#39;re also releasing data (results, attempts, and prompt) from our high-compute o3 testing and would like your help to analyze the results. In particular, we are very curious about the ~9% set of Public Eval tasks o3 was unable to solve, even with lots of compute, yet are straightforward for humans.</p>

<p>We invite the community to help us assess the characteristics of both solved and unsolved tasks.</p>

<p>To get your ideas flowing, here are 3 examples of tasks unsolved by high-compute o3.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-task-c6e1b8da.png" alt="ARC-AGI Task Id: c6e1b8da"/>
  <figcaption>ARC-AGI Task ID: c6e1b8da</figcaption>
</figure>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-task-0d87d2a6.png" alt="ARC-AGI Task Id: 0d87d2a6"/>
  <figcaption>ARC-AGI Task ID: 0d87d2a6</figcaption>
</figure>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-task-b457fec5.png" alt="ARC-AGI Task Id: b457fec5"/>
  <figcaption>ARC-AGI Task ID: b457fec5</figcaption>
</figure>

<p><a href="https://github.com/arcprizeorg/model_baseline/tree/main/results" target="_blank">See our full set of o3 testing data.</a></p>

<p><a href="https://github.com/arcprizeorg/model_baseline/blob/main/prompt_example_o3.md" target="_blank">Here&#39;s the prompt that was used in testing.</a></p>

<p>We&#39;ve also created a new channel in our Discord named <code>oai-analysis</code> and we&#39;d love to hear your analysis and insights there. Or tag us on X/Twitter <a href="https://x.com/arcprize" target="_blank">@arcprize</a>.</p>

<hr/>

<h2 id="conclusions">Conclusions</h2>

<p>To sum up – o3 represents a significant leap forward. Its performance on ARC-AGI highlights a genuine breakthrough in adaptability and generalization, in a way that no other benchmark could have made as explicit.</p>

<p>o3 fixes the fundamental limitation of the LLM paradigm – the inability to recombine knowledge at test time – and it does so via a form of LLM-guided natural language program search. This is not just incremental progress; it is new territory, and it demands serious scientific attention.</p>

<p><span></span> <a href="#" data-modal-id="newsletter">Sign up to get updates</a></p>

		</div></div>
  </body>
</html>
