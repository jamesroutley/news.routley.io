<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/eth-sri/language-model-arithmetic">Original</a>
    <h1>Show HN: Fine-grained stylistic control of LLMs using model arithmetic</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This repo contains the code for <a href="https://arxiv.org/abs/2311.14479" rel="nofollow">model arithmetic</a>, a comprehensive framework where arithmetic formulas express combinations of LMs and classifiers, thereby biasing the generated text towards or away from desired attributes.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/eth-sri/language-model-arithmetic/blob/main/overview.png"><img src="https://github.com/eth-sri/language-model-arithmetic/raw/main/overview.png" alt="Overview"/></a></p>
<p dir="auto">In order to install model arithmetic with Python 3, run</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m pip install -e ."><pre>python -m pip install -e <span>.</span></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" tabindex="-1" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h2>
<p dir="auto">Model arithmetic allows you to combine prompts, models, and classifiers to create new, precisely controlled LLMs that combine aspects of each component.</p>
<p dir="auto">For instance, you can easily interpolate between two differently-prompted models as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from model_arithmetic import ModelArithmetic, PromptedLLM

# define model prompt template
prompt_template = lambda formula_string, input_string: f&#34;&lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;\n{formula_string}\n&lt;&lt;/SYS&gt;&gt;\n\n{input_string} [/INST]&#34;

# define two differently-prompted models
M_child = PromptedLLM(&#34;You are a child.&#34;, prompt_template=prompt_template)
M_adult = PromptedLLM(&#34;You are an adult.&#34;, prompt_template=prompt_template)

# model arithmetic expression
formula1 = M_child - 0.6 * M_adult

# generate text as usual
ma0 = ModelArithmetic(formula1, default_model=&#34;meta-llama/Llama-2-13b-chat-hf&#34;)
print(ma0.generate_text(&#34;Write a one-sentence fairy tale.&#34;))
# -&gt; [&#34;  Oh my gumdrops! Let me tell you a super special fairy tale &#39;bout a teeny tiny princess who lived in a fluffy white castle with her sparkly unicorn and they had the most amazing adventures together!&lt;/s&gt;&#34;]"><pre><span>from</span> <span>model_arithmetic</span> <span>import</span> <span>ModelArithmetic</span>, <span>PromptedLLM</span>

<span># define model prompt template</span>
<span>prompt_template</span> <span>=</span> <span>lambda</span> <span>formula_string</span>, <span>input_string</span>: <span>f&#34;&lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;<span>\n</span><span><span>{</span><span>formula_string</span><span>}</span></span><span>\n</span>&lt;&lt;/SYS&gt;&gt;<span>\n</span><span>\n</span><span><span>{</span><span>input_string</span><span>}</span></span> [/INST]&#34;</span>

<span># define two differently-prompted models</span>
<span>M_child</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are a child.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>)
<span>M_adult</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are an adult.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>)

<span># model arithmetic expression</span>
<span>formula1</span> <span>=</span> <span>M_child</span> <span>-</span> <span>0.6</span> <span>*</span> <span>M_adult</span>

<span># generate text as usual</span>
<span>ma0</span> <span>=</span> <span>ModelArithmetic</span>(<span>formula1</span>, <span>default_model</span><span>=</span><span>&#34;meta-llama/Llama-2-13b-chat-hf&#34;</span>)
<span>print</span>(<span>ma0</span>.<span>generate_text</span>(<span>&#34;Write a one-sentence fairy tale.&#34;</span>))
<span># -&gt; [&#34;  Oh my gumdrops! Let me tell you a super special fairy tale &#39;bout a teeny tiny princess who lived in a fluffy white castle with her sparkly unicorn and they had the most amazing adventures together!&lt;/s&gt;&#34;]</span></pre></div>
<p dir="auto">Note that the <code>generate_text</code> function can also take a list of input sentences and works with standard arguments such as <code>temperature</code>, <code>top_p</code>, <code>top_k</code>, <code>batch_size</code>, <code>num_return_sequences</code> and <code>stop_texts</code> (a list of strings at which the generation should be stopped). You can also save and load a <code>ModelArithmetic</code> object:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ma0.to_pretrained(&#39;model&#39;)
ma0 = ModelArithmetic.from_pretrained(&#39;model&#39;)"><pre><span>ma0</span>.<span>to_pretrained</span>(<span>&#39;model&#39;</span>)
<span>ma0</span> <span>=</span> <span>ModelArithmetic</span>.<span>from_pretrained</span>(<span>&#39;model&#39;</span>)</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-integrating-classifiers" aria-hidden="true" tabindex="-1" href="#integrating-classifiers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Integrating Classifiers</h3>
<p dir="auto">You can integrate classifiers into your model arithmetic expressions. For instance, you can use a classifier to control the formality of your output:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from model_arithmetic import ModelArithmetic, PromptedLLM, Classifier

# define model prompt template
prompt_template = lambda formula_string, input_string: f&#34;&lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;\n{formula_string}\n&lt;&lt;/SYS&gt;&gt;\n\n{input_string} [/INST]&#34;

# define two differently-prompted models
M_child = PromptedLLM(&#34;You are a child.&#34;, prompt_template=prompt_template)
M_adult = PromptedLLM(&#34;You are an adult.&#34;, prompt_template=prompt_template)

# construct model arithmetic expression
formula1 = M_child - 0.6 * M_adult

# Initialize the classifier, the first and third arguments are used to determine on which completion tokens the classifier should be run (on the 50 most likely tokens of formula1 here). The prompt template shown here ensures that the input sentence is ignored for the classifier guidance
C_formal = Classifier(formula1, &#34;s-nlp/roberta-base-formality-ranker&#34;, n_runs_per_sample=50, prompt_template=lambda e, f: &#34;&#34;)

# integrate classifier into model arithmetic expression
formula2 = formula1 + C_formal

# generate text as usual
ma = ModelArithmetic(formula2, default_model=&#34;meta-llama/Llama-2-13b-chat-hf&#34;)
print(ma.generate_text(&#34;Write a one-sentence fairy tale.&#34;, max_length=128))
# -&gt; [&#39;  &#34;Once upon a time, in a magical land filled with fluffy clouds and sparkly rainbows, there was a little girl named me who went on a fun adventure with my stuffed unicorn named Mr. Snuggles!&#34;&lt;/s&gt;&#39;]"><pre><span>from</span> <span>model_arithmetic</span> <span>import</span> <span>ModelArithmetic</span>, <span>PromptedLLM</span>, <span>Classifier</span>

<span># define model prompt template</span>
<span>prompt_template</span> <span>=</span> <span>lambda</span> <span>formula_string</span>, <span>input_string</span>: <span>f&#34;&lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;<span>\n</span><span><span>{</span><span>formula_string</span><span>}</span></span><span>\n</span>&lt;&lt;/SYS&gt;&gt;<span>\n</span><span>\n</span><span><span>{</span><span>input_string</span><span>}</span></span> [/INST]&#34;</span>

<span># define two differently-prompted models</span>
<span>M_child</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are a child.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>)
<span>M_adult</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are an adult.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>)

<span># construct model arithmetic expression</span>
<span>formula1</span> <span>=</span> <span>M_child</span> <span>-</span> <span>0.6</span> <span>*</span> <span>M_adult</span>

<span># Initialize the classifier, the first and third arguments are used to determine on which completion tokens the classifier should be run (on the 50 most likely tokens of formula1 here). The prompt template shown here ensures that the input sentence is ignored for the classifier guidance</span>
<span>C_formal</span> <span>=</span> <span>Classifier</span>(<span>formula1</span>, <span>&#34;s-nlp/roberta-base-formality-ranker&#34;</span>, <span>n_runs_per_sample</span><span>=</span><span>50</span>, <span>prompt_template</span><span>=</span><span>lambda</span> <span>e</span>, <span>f</span>: <span>&#34;&#34;</span>)

<span># integrate classifier into model arithmetic expression</span>
<span>formula2</span> <span>=</span> <span>formula1</span> <span>+</span> <span>C_formal</span>

<span># generate text as usual</span>
<span>ma</span> <span>=</span> <span>ModelArithmetic</span>(<span>formula2</span>, <span>default_model</span><span>=</span><span>&#34;meta-llama/Llama-2-13b-chat-hf&#34;</span>)
<span>print</span>(<span>ma</span>.<span>generate_text</span>(<span>&#34;Write a one-sentence fairy tale.&#34;</span>, <span>max_length</span><span>=</span><span>128</span>))
<span># -&gt; [&#39;  &#34;Once upon a time, in a magical land filled with fluffy clouds and sparkly rainbows, there was a little girl named me who went on a fun adventure with my stuffed unicorn named Mr. Snuggles!&#34;&lt;/s&gt;&#39;]</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-union-and-intersection" aria-hidden="true" tabindex="-1" href="#union-and-intersection"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Union and intersection</h3>
<p dir="auto">You can also use our custom operators to generate text. For instance, you can use the Union operator to add some magic touch to the fairy tale:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from model_arithmetic import ModelArithmetic, PromptedLLM, Union, Classifier

# define model prompt template
prompt_template = lambda formula_string, input_string: f&#34;&lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;\n{formula_string}\n&lt;&lt;/SYS&gt;&gt;\n\n{input_string} [/INST]&#34;

# define three differently-prompted models
M_child = PromptedLLM(&#34;You are a child.&#34;, prompt_template=prompt_template)
M_adult = PromptedLLM(&#34;You are an adult.&#34;, prompt_template=prompt_template)
M_magic = PromptedLLM(&#34;You are a person who is always talking about magic.&#34;, prompt_template=prompt_template)

# construct model arithmetic expression
formula_part1 = M_child - 0.6 * M_adult + 2 * Union(M_child, M_magic)

# integrate classifier in the expression
C_formal = Classifier(formula_part1, &#34;s-nlp/roberta-base-formality-ranker&#34;, n_runs_per_sample=50, 
                      prompt_template=lambda e, f: &#34;&#34;)

formula = formula_part1 + C_formal

# generate text as usual
ma = ModelArithmetic(formula, default_model=&#34;meta-llama/Llama-2-13b-chat-hf&#34;)
print(ma.generate_text(&#34;Write a one-sentence fairy tale.&#34;))
# -&gt; [&#39;  &#34;Once upon a time, in a magical forest filled with sparkling flowers and talking animals, there lived a little girl named Lily who had a special gift for conjuring delicious rainbow-colored cupcakes that made everyone who ate them feel happy and dance with joy!&#34;&lt;/s&gt;&#39;]"><pre><span>from</span> <span>model_arithmetic</span> <span>import</span> <span>ModelArithmetic</span>, <span>PromptedLLM</span>, <span>Union</span>, <span>Classifier</span>

<span># define model prompt template</span>
<span>prompt_template</span> <span>=</span> <span>lambda</span> <span>formula_string</span>, <span>input_string</span>: <span>f&#34;&lt;s&gt;[INST]&lt;&lt;SYS&gt;&gt;<span>\n</span><span><span>{</span><span>formula_string</span><span>}</span></span><span>\n</span>&lt;&lt;/SYS&gt;&gt;<span>\n</span><span>\n</span><span><span>{</span><span>input_string</span><span>}</span></span> [/INST]&#34;</span>

<span># define three differently-prompted models</span>
<span>M_child</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are a child.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>)
<span>M_adult</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are an adult.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>)
<span>M_magic</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are a person who is always talking about magic.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>)

<span># construct model arithmetic expression</span>
<span>formula_part1</span> <span>=</span> <span>M_child</span> <span>-</span> <span>0.6</span> <span>*</span> <span>M_adult</span> <span>+</span> <span>2</span> <span>*</span> <span>Union</span>(<span>M_child</span>, <span>M_magic</span>)

<span># integrate classifier in the expression</span>
<span>C_formal</span> <span>=</span> <span>Classifier</span>(<span>formula_part1</span>, <span>&#34;s-nlp/roberta-base-formality-ranker&#34;</span>, <span>n_runs_per_sample</span><span>=</span><span>50</span>, 
                      <span>prompt_template</span><span>=</span><span>lambda</span> <span>e</span>, <span>f</span>: <span>&#34;&#34;</span>)

<span>formula</span> <span>=</span> <span>formula_part1</span> <span>+</span> <span>C_formal</span>

<span># generate text as usual</span>
<span>ma</span> <span>=</span> <span>ModelArithmetic</span>(<span>formula</span>, <span>default_model</span><span>=</span><span>&#34;meta-llama/Llama-2-13b-chat-hf&#34;</span>)
<span>print</span>(<span>ma</span>.<span>generate_text</span>(<span>&#34;Write a one-sentence fairy tale.&#34;</span>))
<span># -&gt; [&#39;  &#34;Once upon a time, in a magical forest filled with sparkling flowers and talking animals, there lived a little girl named Lily who had a special gift for conjuring delicious rainbow-colored cupcakes that made everyone who ate them feel happy and dance with joy!&#34;&lt;/s&gt;&#39;]</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-about-models" aria-hidden="true" tabindex="-1" href="#about-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>About models</h3>
<p dir="auto">A formula can have terms using different models, as long as all models have the same tokenizer. One can specify a specific model for a certain term by setting the <code>model</code> parameter:</p>
<div dir="auto" data-snippet-clipboard-copy-content="M_child = PromptedLLM(&#34;You are a child.&#34;, prompt_template=prompt_template, model=&#34;meta-llama/Llama-2-7b-chat-hf&#34;)"><pre><span>M_child</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are a child.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>, <span>model</span><span>=</span><span>&#34;meta-llama/Llama-2-7b-chat-hf&#34;</span>)</pre></div>
<p dir="auto">The selected model can also be a <code>PreTrainedModel</code> instead of a <code>string</code>.</p>
<p dir="auto">Models are by default loaded in bfloat16 format. You can change this by specifying the <code>dtype</code> parameter in the <code>ModelArithmetic</code> constructor:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ma = ModelArithmetic(formula, default_model=&#34;meta-llama/Llama-2-13b-chat-hf&#34;, dtype=torch.float32)"><pre><span>ma</span> <span>=</span> <span>ModelArithmetic</span>(<span>formula</span>, <span>default_model</span><span>=</span><span>&#34;meta-llama/Llama-2-13b-chat-hf&#34;</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>float32</span>)</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-speculative-sampling" aria-hidden="true" tabindex="-1" href="#speculative-sampling"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Speculative sampling</h3>
<p dir="auto">Speculative sampling can be performed by initializing the prompted models with the extra <code>speculative_factor</code> parameter and setting the <code>do_speculation</code> parameter in the generation function to <code>True</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="...
M_child = PromptedLLM(&#34;You are a child.&#34;, prompt_template=prompt_template)
M_adult = PromptedLLM(&#34;You are an adult.&#34;, prompt_template=prompt_template, speculative_factor=4)
...
print(ma0.generate_text(&#34;Write a one-sentence fairy tale.&#34;, do_speculation=True))"><pre>...
<span>M_child</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are a child.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>)
<span>M_adult</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are an adult.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>, <span>speculative_factor</span><span>=</span><span>4</span>)
...
<span>print</span>(<span>ma0</span>.<span>generate_text</span>(<span>&#34;Write a one-sentence fairy tale.&#34;</span>, <span>do_speculation</span><span>=</span><span>True</span>))</pre></div>
<p dir="auto">Note that one prompted model should always have <code>speculative_factor=1</code> (the default value).</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-eager-mode" aria-hidden="true" tabindex="-1" href="#eager-mode"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Eager mode</h3>
<p dir="auto">By default, we process the key-value cache stored by models since this is required for speculative sampling. Since different models use key-value caching differently, this can result in errors. We therefore included the <code>run_eager</code> parameter in the initialization of the prompted model to disable all speculative sampling which should fix this issue if it occurs:</p>
<div dir="auto" data-snippet-clipboard-copy-content="M_child = PromptedLLM(&#34;You are a child.&#34;, prompt_template=prompt_template, run_eager=True)"><pre><span>M_child</span> <span>=</span> <span>PromptedLLM</span>(<span>&#34;You are a child.&#34;</span>, <span>prompt_template</span><span>=</span><span>prompt_template</span>, <span>run_eager</span><span>=</span><span>True</span>)</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-other-operators" aria-hidden="true" tabindex="-1" href="#other-operators"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Other Operators</h3>
<p dir="auto">Finally, the library provides some other operators that can be used in formulas, of which we present a few here. The <code>TopPTopK</code> operator allows the use of nucleus and top-k sampling within a formula. The following ensures that the output token is always in the top 10 words of <code>model1</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="formula = TopPTopK(model1, top_k=10) + model2"><pre><span>formula</span> <span>=</span> <span>TopPTopK</span>(<span>model1</span>, <span>top_k</span><span>=</span><span>10</span>) <span>+</span> <span>model2</span></pre></div>
<p dir="auto">The <code>Superseded</code> operator implements <a href="https://arxiv.org/abs/2302.01318" rel="nofollow">speculative sampling</a> directly:</p>
<div dir="auto" data-snippet-clipboard-copy-content="formula = Superseded(small_model, large_model)"><pre><span>formula</span> <span>=</span> <span>Superseded</span>(<span>small_model</span>, <span>large_model</span>)</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-lm-evaluation-harness" aria-hidden="true" tabindex="-1" href="#lm-evaluation-harness"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LM Evaluation Harness</h2>
<p dir="auto">Model arithmetic is compatible with the <a href="https://github.com/EleutherAI/lm-evaluation-harness">LM Evaluation harness</a>. In order to run benchmarks from the harness, you need to install the package as described <a href="https://github.com/EleutherAI/lm-evaluation-harness">on their GitHub page</a>. An example of how to use our tool with the lm evaluation harness is shown in <code>scripts/evaluate_lm_eval.py</code>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-reproducing-results" aria-hidden="true" tabindex="-1" href="#reproducing-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Reproducing results</h2>
<p dir="auto">For the reproduction of the results presented in our paper, <em>Controlled Text Generation via Language Model Arithmetic</em>, we advice to run the code with the exact environment we used (Nvidia H100 80GB GPU on a Linux machine). To do so install <a href="https://docs.conda.io/projects/miniconda/en/latest/" rel="nofollow">Conda</a> and run</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n model_arithmetic python=3.10
conda activate model_arithmetic
python -m pip install -r requirements.txt
python -m pip install -e ."><pre>conda create -n model_arithmetic python=3.10
conda activate model_arithmetic
python -m pip install -r requirements.txt
python -m pip install -e <span>.</span></pre></div>
<p dir="auto">API Keys for both the <a href="https://perspectiveapi.com/" rel="nofollow">PERSPECTIVE API</a> and <a href="https://openai.com/" rel="nofollow">OpenAI</a> need to be available in the environment variables. Alternatively, they can be placed in the file <code>src/.env</code> as</p>
<div dir="auto" data-snippet-clipboard-copy-content="PERSPECTIVE_API_KEY=&#34;[YOUR API KEY]&#34;
OPENAI_API_KEY=&#34;[YOUR API KEY]&#34;"><pre>PERSPECTIVE_API_KEY=<span><span>&#34;</span>[YOUR API KEY]<span>&#34;</span></span>
OPENAI_API_KEY=<span><span>&#34;</span>[YOUR API KEY]<span>&#34;</span></span></pre></div>
<p dir="auto">The processed datasets are in the <code>data/datasets</code> folder. You can reproduce our results using these datasets by running</p>

<p dir="auto">This will finetune a classifier for the toxicity and sentiment control tasks, and reproduce the results from all sections of our paper. Results in CSV-format can afterwards be found in <code>eval/processed</code> and our figures in <code>eval/plots</code>.</p>
<p dir="auto">Alternatively, you can download the raw datasets and put them in the <code>data/datasets</code> folder:</p>
<ul dir="auto">
<li><a href="https://github.com/tloen/alpaca-lora/blob/main/alpaca_data.json">Alpaca Data</a></li>
<li><a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data" rel="nofollow">Jigsaw Toxicity Dataset</a> (the <code>all_data.csv</code> file should be downloaded and extracted in the <code>data/datasets</code> folder)</li>
<li><a href="https://zenodo.org/record/3606810" rel="nofollow">Politically Incorrect 4chan Messages</a> (file should be unzipped and placed in the top level of the <code>data/datasets</code> folder)</li>
<li><a href="https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="nofollow">IMDB Dataset</a></li>
</ul>
<p dir="auto">You can then reproduce the results using</p>
<div dir="auto" data-snippet-clipboard-copy-content="bash scripts/main_preprocess.sh"><pre>bash scripts/main_preprocess.sh</pre></div>
<p dir="auto">We note that part of our preprocessing code got lost, specifically for preparing the dataset that is used for finetuning the toxicity classifier. Running the code without using the preprocessed datasets might therefore result in slightly different numbers when they involve the finetuned classifier.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-cite-this-work" aria-hidden="true" tabindex="-1" href="#cite-this-work"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Cite this work</h3>
<div data-snippet-clipboard-copy-content="@article{dekoninck-2023-controlled,
  author       = {Jasper Dekoninck and
                  Marc Fischer and
                  Luca Beurer{-}Kellner and
                  Martin T. Vechev},
  title        = {Controlled Text Generation via Language Model Arithmetic},
  journal      = {CoRR},
  volume       = {abs/2311.14479},
  year         = {2023},
}"><pre><code>@article{dekoninck-2023-controlled,
  author       = {Jasper Dekoninck and
                  Marc Fischer and
                  Luca Beurer{-}Kellner and
                  Martin T. Vechev},
  title        = {Controlled Text Generation via Language Model Arithmetic},
  journal      = {CoRR},
  volume       = {abs/2311.14479},
  year         = {2023},
}
</code></pre></div>
</article>
          </div></div>
  </body>
</html>
