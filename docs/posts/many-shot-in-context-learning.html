<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2404.11018">Original</a>
    <h1>Many-Shot In-Context Learning</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+R">Rishabh Agarwal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+A">Avi Singh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L+M">Lei M. Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bohnet,+B">Bernd Bohnet</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chan,+S">Stephanie Chan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anand,+A">Ankesh Anand</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abbas,+Z">Zaheer Abbas</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nova,+A">Azade Nova</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Co-Reyes,+J+D">John D. Co-Reyes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chu,+E">Eric Chu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Behbahani,+F">Feryal Behbahani</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Faust,+A">Aleksandra Faust</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Larochelle,+H">Hugo Larochelle</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2404.11018">View PDF</a>
    <a href="https://arxiv.org/html/2404.11018v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Rishabh Agarwal [<a href="https://arxiv.org/show-email/2787173c/2404.11018">view email</a>]      </p></div></div>
  </body>
</html>
