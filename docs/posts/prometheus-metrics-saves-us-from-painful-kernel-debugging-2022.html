<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/PrometheusHostMetricsSaveUs">Original</a>
    <h1>Prometheus metrics saves us from painful kernel debugging (2022)</h1>
    
    <div id="readability-page-1" class="page"><div><h2>Our Prometheus host metrics saved us from some painful experiences</h2>

	<p><small>September  3, 2022</small></p>
</div><div><p>A couple of weeks ago, a few days after a kernel upgrade on our
servers, we had an Ubuntu 22.04 server basically die with a constant
series of out-of-memory kills from the kernel of both vital demons
and random bystander processes. There was no obvious clue as to
why, with no program or cgroup consuming an unusual amount of memory.
In the constant spew of text from the kernel as it repeatedly OOM
killed things, we did notice something:</p>

<blockquote><pre>kernel: [361299.864757] Unreclaimable slab info:
kernel: [361299.864757] Name Used Total
[...]
kernel: [361299.864924] kmalloc-2k 6676584KB 6676596KB
[...]
</pre>
</blockquote>

<p>Among our Grafana dashboards is one that provides a relatively
detailed look into the state of a particular server, including
various bits of memory usage. After we rebooted the failing server
I took a look at its dashboard, and immediately noticed that the
&#39;Slab&#39; memory usage was basically a diagonal line going up over
time from when it had its kernel update a few days ago and been
rebooted.</p>

<p>This caused me to immediately go look at the Slab memory usage for
other servers, and all of our 22.04 servers had the same behavior.
All of them had a constantly increasing amount of slab memory usage
(and in some digging, &#39;unreclaimable&#39; slab memory usage); it was
just that this particular server had a combination of usage and
low(er) RAM that caused it to run out of memory sooner than anything
else. It was clear we had a systemic issue that would take down
every one of our 22.04 servers sooner or later, with a number of
them already being alarmingly close to also running out of memory
(including our Prometheus metrics server).</p>

<p>At first this looked very much like an issue with the new kernel.
But it occurred to us that we&#39;d effectively made another kernel
change at the same time. Back at the start of August, <a href="https://utcc.utoronto.ca/~cks/space/blog/linux/UbuntuAppArmorPersistence">after
discovering that AppArmor profiles had started activating themselves</a>, we&#39;d set a kernel command line
option to turn off AppArmor in the kernel. However, activating that
option requires a reboot (to use the new command line), and on most
machines we hadn&#39;t rebooted them until our kernel update. However,
a few 22.04 machines had been rebooted earlier with the command
line update in place, and some of those machines were even running
older kernels. Inspection of Prometheus host metrics showed that
their Slab usage had started going up in the same pattern from the
moment they were rebooted, including the machines that had older
kernels.</p>

<p>We immediately reverted this kernel command line change on a few
machines that we could readily reboot without affecting people
(including the Prometheus metrics server), while leaving them using
the current kernel. Within a few hours it was fairly clear that
disabling AppArmor on the kernel command line was the trigger for
this kernel memory leak, and by the next morning it was basically
certain. We reverted the kernel command line change everywhere and
started scheduling server reboots for all of our 22.04 machines.</p>

<p>(We also filed <a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1987430">Ubuntu bug 1987430</a>.)</p>

<p>Without <a href="https://utcc.utoronto.ca/~cks/space/blog/sysadmin/PrometheusGrafanaSetup-2019">our Prometheus and Grafana setup</a>,
this most likely would have been a rather different and more painful
experience. We probably would have written off the first server
going out of memory as a one time piece of weirdness and only started
reacting when a second server had the same thing happen to it a day
or two later (and there probably would have been a succession of
servers hitting limits at that time). Then it might have taken
longer to realize that we had a steady slab leak over time, and
we&#39;d probably have blamed the recent kernel update and spent a bunch
of time and effort reverting to a previous 22.04 kernel without
actually fixing the problem. As it was, our Grafana dashboards
surfaced a big indicator of the problem right away and then our
historical data let us see that it wasn&#39;t actually the recent kernel
update at fault.</p>

<p>Most of the time our metrics system just seems nice and useful, not
a critical thing (alerts are critical, but those don&#39;t necessarily
require metrics and metric history). This was not one of those
times; it&#39;s one of the few times where having metrics, both current
and historical, clearly saved our bacon. A part of me feels that
this incident justifies our metrics systems all by itself.</p>

<p>(This elaborates on <a href="https://mastodon.social/@cks/108873927699425648">a Fediverse post of mine</a>, and also <a href="https://twitter.com/thatcks/status/1562126351175499776">a
tweet</a>.)</p>
</div></div>
  </body>
</html>
