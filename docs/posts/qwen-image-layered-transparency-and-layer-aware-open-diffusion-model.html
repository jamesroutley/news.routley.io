<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/papers/2512.15603">Original</a>
    <h1>Qwen-Image-Layered: transparency and layer aware open diffusion model</h1>
    
    <div id="readability-page-1" class="page"><div>
<div><section><div data-target="PaperContent" data-props="{&#34;comments&#34;:[{&#34;id&#34;:&#34;694373dfc2caeb482b2a4c62&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;6039478ab3ecf716b1a5fd4d&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&#34;,&#34;fullname&#34;:&#34;taesiri&#34;,&#34;name&#34;:&#34;taesiri&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:true,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false,&#34;followerCount&#34;:187},&#34;createdAt&#34;:&#34;2025-12-18T03:24:15.000Z&#34;,&#34;type&#34;:&#34;comment&#34;,&#34;data&#34;:{&#34;edited&#34;:false,&#34;hidden&#34;:false,&#34;latest&#34;:{&#34;raw&#34;:&#34;Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \\textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \\textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing.&#34;,&#34;html&#34;:&#34;&lt;p&gt;Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \\textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \\textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing.&lt;/p&gt;\n&#34;,&#34;updatedAt&#34;:&#34;2025-12-18T03:24:15.242Z&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;6039478ab3ecf716b1a5fd4d&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&#34;,&#34;fullname&#34;:&#34;taesiri&#34;,&#34;name&#34;:&#34;taesiri&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:true,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false,&#34;followerCount&#34;:187}},&#34;numEdits&#34;:0,&#34;identifiedLanguage&#34;:{&#34;language&#34;:&#34;en&#34;,&#34;probability&#34;:0.8544212579727173},&#34;editors&#34;:[&#34;taesiri&#34;],&#34;editorAvatarUrls&#34;:[&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&#34;],&#34;reactions&#34;:[{&#34;reaction&#34;:&#34;ðŸ”¥&#34;,&#34;users&#34;:[&#34;search-facility&#34;],&#34;count&#34;:1}],&#34;isReport&#34;:false}},{&#34;id&#34;:&#34;694444f99973220907e49199&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;65243980050781c16f234f1f&#34;,&#34;avatarUrl&#34;:&#34;/avatars/743a009681d5d554c27e04300db9f267.svg&#34;,&#34;fullname&#34;:&#34;Avi Basa&#34;,&#34;name&#34;:&#34;avahal&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false},&#34;createdAt&#34;:&#34;2025-12-18T18:16:25.000Z&#34;,&#34;type&#34;:&#34;comment&#34;,&#34;data&#34;:{&#34;edited&#34;:false,&#34;hidden&#34;:false,&#34;latest&#34;:{&#34;raw&#34;:&#34;arXiv lens breakdown of this paper ðŸ‘‰ https://arxivlens.com/PaperView/Details/qwen-image-layered-towards-inherent-editability-via-layer-decomposition-9194-7a40c6da\n- Executive Summary \n- Detailed Breakdown\n- Practical Applications&#34;,&#34;html&#34;:&#34;&lt;p&gt;arXiv lens breakdown of this paper ðŸ‘‰ &lt;a rel=\&#34;nofollow\&#34; href=\&#34;https://arxivlens.com/PaperView/Details/qwen-image-layered-towards-inherent-editability-via-layer-decomposition-9194-7a40c6da\&#34;&gt;https://arxivlens.com/PaperView/Details/qwen-image-layered-towards-inherent-editability-via-layer-decomposition-9194-7a40c6da&lt;/a&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Executive Summary &lt;/li&gt;\n&lt;li&gt;Detailed Breakdown&lt;/li&gt;\n&lt;li&gt;Practical Applications&lt;/li&gt;\n&lt;/ul&gt;\n&#34;,&#34;updatedAt&#34;:&#34;2025-12-18T18:16:25.241Z&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;65243980050781c16f234f1f&#34;,&#34;avatarUrl&#34;:&#34;/avatars/743a009681d5d554c27e04300db9f267.svg&#34;,&#34;fullname&#34;:&#34;Avi Basa&#34;,&#34;name&#34;:&#34;avahal&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false}},&#34;numEdits&#34;:0,&#34;identifiedLanguage&#34;:{&#34;language&#34;:&#34;en&#34;,&#34;probability&#34;:0.69692462682724},&#34;editors&#34;:[&#34;avahal&#34;],&#34;editorAvatarUrls&#34;:[&#34;/avatars/743a009681d5d554c27e04300db9f267.svg&#34;],&#34;reactions&#34;:[{&#34;reaction&#34;:&#34;ðŸ”¥&#34;,&#34;users&#34;:[&#34;taesiri&#34;],&#34;count&#34;:1}],&#34;isReport&#34;:false}},{&#34;id&#34;:&#34;6944c51c5f44ccf243c60540&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;67e013180e343983eee807bb&#34;,&#34;avatarUrl&#34;:&#34;/avatars/44f1b87f5555e0f75aea4b9f136b4839.svg&#34;,&#34;fullname&#34;:&#34;suixmeng&#34;,&#34;name&#34;:&#34;suixmeng&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false},&#34;createdAt&#34;:&#34;2025-12-19T03:23:08.000Z&#34;,&#34;type&#34;:&#34;comment&#34;,&#34;data&#34;:{&#34;edited&#34;:false,&#34;hidden&#34;:false,&#34;latest&#34;:{&#34;raw&#34;:&#34;When will the code and model be released?\n&#34;,&#34;html&#34;:&#34;&lt;p&gt;When will the code and model be released?&lt;/p&gt;\n&#34;,&#34;updatedAt&#34;:&#34;2025-12-19T03:23:08.279Z&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;67e013180e343983eee807bb&#34;,&#34;avatarUrl&#34;:&#34;/avatars/44f1b87f5555e0f75aea4b9f136b4839.svg&#34;,&#34;fullname&#34;:&#34;suixmeng&#34;,&#34;name&#34;:&#34;suixmeng&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false}},&#34;numEdits&#34;:0,&#34;identifiedLanguage&#34;:{&#34;language&#34;:&#34;en&#34;,&#34;probability&#34;:0.9410170316696167},&#34;editors&#34;:[&#34;suixmeng&#34;],&#34;editorAvatarUrls&#34;:[&#34;/avatars/44f1b87f5555e0f75aea4b9f136b4839.svg&#34;],&#34;reactions&#34;:[{&#34;reaction&#34;:&#34;ðŸ‘&#34;,&#34;users&#34;:[&#34;xa3gagh&#34;,&#34;linxaa&#34;],&#34;count&#34;:2}],&#34;isReport&#34;:false}},{&#34;id&#34;:&#34;69450f3ffdaf94348a97a6b2&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;6183d058b6128397263e0188&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/6183d058b6128397263e0188/TEnKYn0mHwb39O7Ix-Esb.jpeg&#34;,&#34;fullname&#34;:&#34;Juan Martinez&#34;,&#34;name&#34;:&#34;jjmcarrascosa&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false,&#34;followerCount&#34;:3},&#34;createdAt&#34;:&#34;2025-12-19T08:39:27.000Z&#34;,&#34;type&#34;:&#34;comment&#34;,&#34;data&#34;:{&#34;edited&#34;:false,&#34;hidden&#34;:false,&#34;latest&#34;:{&#34;raw&#34;:&#34;In the paper they point to a repo that does not exist ðŸ˜”   https://github.com/QwenLM/QwenImage-Layered&#34;,&#34;html&#34;:&#34;&lt;p&gt;In the paper they point to a repo that does not exist ðŸ˜”   &lt;a rel=\&#34;nofollow\&#34; href=\&#34;https://github.com/QwenLM/QwenImage-Layered\&#34;&gt;https://github.com/QwenLM/QwenImage-Layered&lt;/a&gt;&lt;/p&gt;\n&#34;,&#34;updatedAt&#34;:&#34;2025-12-19T08:39:27.111Z&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;6183d058b6128397263e0188&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/6183d058b6128397263e0188/TEnKYn0mHwb39O7Ix-Esb.jpeg&#34;,&#34;fullname&#34;:&#34;Juan Martinez&#34;,&#34;name&#34;:&#34;jjmcarrascosa&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:false,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false,&#34;followerCount&#34;:3}},&#34;numEdits&#34;:0,&#34;identifiedLanguage&#34;:{&#34;language&#34;:&#34;en&#34;,&#34;probability&#34;:0.9282187819480896},&#34;editors&#34;:[&#34;jjmcarrascosa&#34;],&#34;editorAvatarUrls&#34;:[&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/6183d058b6128397263e0188/TEnKYn0mHwb39O7Ix-Esb.jpeg&#34;],&#34;reactions&#34;:[],&#34;isReport&#34;:false}},{&#34;id&#34;:&#34;6945889ce22aa0be5618eeb2&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;640d3eaa3623f6a56dde856d&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg&#34;,&#34;fullname&#34;:&#34;vansin&#34;,&#34;name&#34;:&#34;vansin&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:true,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false,&#34;followerCount&#34;:34},&#34;createdAt&#34;:&#34;2025-12-19T17:17:16.000Z&#34;,&#34;type&#34;:&#34;comment&#34;,&#34;data&#34;:{&#34;edited&#34;:false,&#34;hidden&#34;:false,&#34;latest&#34;:{&#34;raw&#34;:&#34;![db080ecbcb927d73ff9cc6276ab4e765](https://cdn-uploads.huggingface.co/production/uploads/640d3eaa3623f6a56dde856d/Z2kqHF1HsSOx25BvOUzS7.jpeg)\n&#34;,&#34;html&#34;:&#34;&lt;p&gt;&lt;a rel=\&#34;nofollow\&#34; href=\&#34;https://cdn-uploads.huggingface.co/production/uploads/640d3eaa3623f6a56dde856d/Z2kqHF1HsSOx25BvOUzS7.jpeg\&#34;&gt;&lt;img alt=\&#34;db080ecbcb927d73ff9cc6276ab4e765\&#34; src=\&#34;https://cdn-uploads.huggingface.co/production/uploads/640d3eaa3623f6a56dde856d/Z2kqHF1HsSOx25BvOUzS7.jpeg\&#34;&gt;&lt;/a&gt;&lt;/p&gt;\n&#34;,&#34;updatedAt&#34;:&#34;2025-12-19T17:17:16.384Z&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;640d3eaa3623f6a56dde856d&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg&#34;,&#34;fullname&#34;:&#34;vansin&#34;,&#34;name&#34;:&#34;vansin&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:true,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false,&#34;followerCount&#34;:34}},&#34;numEdits&#34;:0,&#34;identifiedLanguage&#34;:{&#34;language&#34;:&#34;en&#34;,&#34;probability&#34;:0.17680896818637848},&#34;editors&#34;:[&#34;vansin&#34;],&#34;editorAvatarUrls&#34;:[&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg&#34;],&#34;reactions&#34;:[],&#34;isReport&#34;:false}},{&#34;id&#34;:&#34;694588b0e22aa0be5618f0b1&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;640d3eaa3623f6a56dde856d&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg&#34;,&#34;fullname&#34;:&#34;vansin&#34;,&#34;name&#34;:&#34;vansin&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:true,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false,&#34;followerCount&#34;:34},&#34;createdAt&#34;:&#34;2025-12-19T17:17:36.000Z&#34;,&#34;type&#34;:&#34;comment&#34;,&#34;data&#34;:{&#34;edited&#34;:false,&#34;hidden&#34;:false,&#34;latest&#34;:{&#34;raw&#34;:&#34;how to use it in figma or photoshop &#34;,&#34;html&#34;:&#34;&lt;p&gt;how to use it in figma or photoshop &lt;/p&gt;\n&#34;,&#34;updatedAt&#34;:&#34;2025-12-19T17:17:36.478Z&#34;,&#34;author&#34;:{&#34;_id&#34;:&#34;640d3eaa3623f6a56dde856d&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg&#34;,&#34;fullname&#34;:&#34;vansin&#34;,&#34;name&#34;:&#34;vansin&#34;,&#34;type&#34;:&#34;user&#34;,&#34;isPro&#34;:true,&#34;isHf&#34;:false,&#34;isHfAdmin&#34;:false,&#34;isMod&#34;:false,&#34;followerCount&#34;:34}},&#34;numEdits&#34;:0,&#34;identifiedLanguage&#34;:{&#34;language&#34;:&#34;en&#34;,&#34;probability&#34;:0.9442254900932312},&#34;editors&#34;:[&#34;vansin&#34;],&#34;editorAvatarUrls&#34;:[&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/1678589663024-640d3eaa3623f6a56dde856d.jpeg&#34;],&#34;reactions&#34;:[],&#34;isReport&#34;:false}}],&#34;primaryEmailConfirmed&#34;:false,&#34;paper&#34;:{&#34;id&#34;:&#34;2512.15603&#34;,&#34;authors&#34;:[{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6b4&#34;,&#34;name&#34;:&#34;Shengming Yin&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6b5&#34;,&#34;name&#34;:&#34;Zekai Zhang&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6b6&#34;,&#34;name&#34;:&#34;Zecheng Tang&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6b7&#34;,&#34;name&#34;:&#34;Kaiyuan Gao&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6b8&#34;,&#34;name&#34;:&#34;Xiao Xu&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6b9&#34;,&#34;name&#34;:&#34;Kun Yan&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6ba&#34;,&#34;name&#34;:&#34;Jiahao Li&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6bb&#34;,&#34;name&#34;:&#34;Yilei Chen&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6bc&#34;,&#34;name&#34;:&#34;Yuxiang Chen&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6bd&#34;,&#34;name&#34;:&#34;Heung-Yeung Shum&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6be&#34;,&#34;name&#34;:&#34;Lionel M. Ni&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6bf&#34;,&#34;name&#34;:&#34;Jingren Zhou&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6c0&#34;,&#34;name&#34;:&#34;Junyang Lin&#34;,&#34;hidden&#34;:false},{&#34;_id&#34;:&#34;694373cc542d62d58a7bf6c1&#34;,&#34;name&#34;:&#34;Chenfei Wu&#34;,&#34;hidden&#34;:false}],&#34;publishedAt&#34;:&#34;2025-12-17T17:12:42.000Z&#34;,&#34;submittedOnDailyAt&#34;:&#34;2025-12-18T00:54:15.231Z&#34;,&#34;title&#34;:&#34;Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition&#34;,&#34;submittedOnDailyBy&#34;:{&#34;_id&#34;:&#34;6039478ab3ecf716b1a5fd4d&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&#34;,&#34;isPro&#34;:true,&#34;fullname&#34;:&#34;taesiri&#34;,&#34;user&#34;:&#34;taesiri&#34;,&#34;type&#34;:&#34;user&#34;},&#34;summary&#34;:&#34;Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}&#34;,&#34;upvotes&#34;:36,&#34;discussionId&#34;:&#34;694373cc542d62d58a7bf6c2&#34;,&#34;ai_summary&#34;:&#34;Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.&#34;,&#34;ai_keywords&#34;:[&#34;diffusion model&#34;,&#34;RGBA layers&#34;,&#34;inherent editability&#34;,&#34;RGBA-VAE&#34;,&#34;VLD-MMDiT&#34;,&#34;Multi-stage Training&#34;,&#34;PSD&#34;,&#34;decomposition quality&#34;,&#34;consistent image editing&#34;]},&#34;canReadDatabase&#34;:false,&#34;canManagePapers&#34;:false,&#34;canSubmit&#34;:false,&#34;hasHfLevelAccess&#34;:false,&#34;upvoted&#34;:false,&#34;upvoters&#34;:[{&#34;_id&#34;:&#34;6039478ab3ecf716b1a5fd4d&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&#34;,&#34;isPro&#34;:true,&#34;fullname&#34;:&#34;taesiri&#34;,&#34;user&#34;:&#34;taesiri&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;652ce0d4c543a08aa92e010f&#34;,&#34;avatarUrl&#34;:&#34;/avatars/7978304e3fe99b0d4d0712441c6a24f3.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Haoyu Guo&#34;,&#34;user&#34;:&#34;ghy0324&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;67f4fc326576467f43cc20bf&#34;,&#34;avatarUrl&#34;:&#34;/avatars/d1ecd68111dbdb25168ebc7edfd02895.svg&#34;,&#34;isPro&#34;:true,&#34;fullname&#34;:&#34;OmniSVG&#34;,&#34;user&#34;:&#34;OmniSVG&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;67bbade8a8c89b98ec377944&#34;,&#34;avatarUrl&#34;:&#34;/avatars/640803ef641decd5c30894155bf09b6a.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;Urodoc Oncall&#34;,&#34;user&#34;:&#34;UDCAI&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;620783f24e28382272337ba4&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;GuoLiangTang&#34;,&#34;user&#34;:&#34;Tommy930&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;6438bb291efe72ba48085039&#34;,&#34;avatarUrl&#34;:&#34;/avatars/94cf959a86c0ce58263db710d693fb9e.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;boomcheng&#34;,&#34;user&#34;:&#34;boomcheng&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;68c9449d14ec7482edec505b&#34;,&#34;avatarUrl&#34;:&#34;/avatars/b05958b28a8964ee36bd8a3994b2aaec.svg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;kkmh&#34;,&#34;user&#34;:&#34;kkmh-op&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;6173262d654f639abb2aecbf&#34;,&#34;avatarUrl&#34;:&#34;/avatars/79df524de88db4bd50aaa5b00c92a0f4.svg&#34;,&#34;isPro&#34;:true,&#34;fullname&#34;:&#34;JD&#34;,&#34;user&#34;:&#34;jide&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;61e52be53d6dbb1da842316a&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/61e52be53d6dbb1da842316a/gx0WGPcOCClXPymoKglc4.jpeg&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;BÃ¶rje Karlsson&#34;,&#34;user&#34;:&#34;tellarin&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;6311ebe13a773344e06a7016&#34;,&#34;avatarUrl&#34;:&#34;/avatars/415129de698ff869ab048e392479ce91.svg&#34;,&#34;isPro&#34;:true,&#34;fullname&#34;:&#34;Emmanuel Nyanguila&#34;,&#34;user&#34;:&#34;Mitano&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;63c5d43ae2804cb2407e4d43&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/1673909278097-noauth.png&#34;,&#34;isPro&#34;:false,&#34;fullname&#34;:&#34;xziayro&#34;,&#34;user&#34;:&#34;xziayro&#34;,&#34;type&#34;:&#34;user&#34;},{&#34;_id&#34;:&#34;6437cf2fe282b4a48eaf47fd&#34;,&#34;avatarUrl&#34;:&#34;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Qz1WRFYfs6ZSnRz88wEZt.jpeg&#34;,&#34;isPro&#34;:true,&#34;fullname&#34;:&#34;Deniz Aybey&#34;,&#34;user&#34;:&#34;denizaybey&#34;,&#34;type&#34;:&#34;user&#34;}],&#34;acceptLanguages&#34;:[&#34;*&#34;],&#34;dailyPaperRank&#34;:0}"><div>
	
	
	
	
	<div><p><span>Authors:</span></p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p><p><span>

<span><span></span>
	</span>
					,</span>
			</p>
		</div></div>

<div><h2>Abstract</h2>
	<div><div><p>Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.</p>
				</div>
		<p>Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end <a href="https://huggingface.co/papers?q=diffusion%20model">diffusion model</a> that decomposes a single RGB image into multiple semantically disentangled <a href="https://huggingface.co/papers?q=RGBA%20layers">RGBA layers</a>, enabling <a href="https://huggingface.co/papers?q=inherent%20editability">inherent editability</a>, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an <a href="https://huggingface.co/papers?q=RGBA-VAE">RGBA-VAE</a> to unify the latent representations of RGB and RGBA images; (2) a <a href="https://huggingface.co/papers?q=VLD-MMDiT">VLD-MMDiT</a> (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a <a href="https://huggingface.co/papers?q=Multi-stage%20Training">Multi-stage Training</a> strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (<a href="https://huggingface.co/papers?q=PSD">PSD</a>). Experiments demonstrate that our method significantly surpasses existing approaches in <a href="https://huggingface.co/papers?q=decomposition%20quality">decomposition quality</a> and establishes a new paradigm for <a href="https://huggingface.co/papers?q=consistent%20image%20editing">consistent image editing</a>. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}</p></div></div>



<dialog>
	</dialog>



</div></section>
		<section>

			<h2><svg style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><path d="M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z" opacity=".25" fill="currentColor"></path><path d="M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z" opacity=".5" fill="currentColor"></path><path d="M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z" fill="currentColor"></path></svg>
				Models citing this paper
				<span>2</span></h2>
			<nav><article><a href="https://huggingface.co/Qwen/Qwen-Image-Layered"><div><header title="Qwen/Qwen-Image-Layered"><p><img alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png" crossorigin="anonymous"/>
	</p>
				<h4>Qwen/Qwen-Image-Layered</h4>
				
				</header>
			<div><svg width="1em" height="1em" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.095 6.788a.7.7 0 0 1 .7.7V9.99a.7.7 0 0 1-.7.7h-3.52a.7.7 0 0 1-.7-.7v-2.5a.7.7 0 0 1 .7-.7h3.52Zm-3.52 2.608v.593h3.248l-.972-.786-.255.256-.228.227-1.073-1.019-.72.73Zm0-.9.52-.48a.3.3 0 0 1 .41.004l.781.74a.1.1 0 0 0 .139-.001l.2-.198a.3.3 0 0 1 .402-.019l1.068.882V7.488h-3.52v1.008ZM9.061 1.2a.7.7 0 0 1 .7.7v4.377h-.7V1.9H2.43v3.55l.9-.9a.7.7 0 0 1 .99 0l1.74 1.74a.791.791 0 0 0-.51.47L3.82 5.04l-1.4 1.4v2.09H5.5v.7H2.43a.7.7 0 0 1-.7-.7V1.9a.7.7 0 0 1 .7-.7h6.63Zm.324 6.542a.35.35 0 1 1 0 .701.35.35 0 0 1 0-.7Zm-2.07-2.007a.7.7 0 0 1 .496.205H7.8l.32.32H6.5l.32-.32a.703.703 0 0 1 .495-.205Zm.55-3.044v.75h-.95v1.892h-.75V3.441h-.911v-.75h2.611Z" fill="currentColor"></path></svg><p>
			Image-Text-to-Image
			<span>â€¢ </span>
		
	
				<span>Updated
					<time datetime="2025-12-19T09:11:18" title="Fri, 19 Dec 2025 09:11:18 GMT">about 18 hours ago</time></span>
				
				<span>â€¢ </span></p><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" viewBox="0 0 32 32"><path fill="currentColor" d="M26 24v4H6v-4H4v4a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2v-4zm0-10l-1.41-1.41L17 20.17V2h-2v18.17l-7.59-7.58L6 14l10 10l10-10z"></path></svg><p>
					42
				
	
				<span>â€¢ </span></p><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32" fill="currentColor"><path d="M22.45,6a5.47,5.47,0,0,1,3.91,1.64,5.7,5.7,0,0,1,0,8L16,26.13,5.64,15.64a5.7,5.7,0,0,1,0-8,5.48,5.48,0,0,1,7.82,0L16,10.24l2.53-2.58A5.44,5.44,0,0,1,22.45,6m0-2a7.47,7.47,0,0,0-5.34,2.24L16,7.36,14.89,6.24a7.49,7.49,0,0,0-10.68,0,7.72,7.72,0,0,0,0,10.82L16,29,27.79,17.06a7.72,7.72,0,0,0,0-10.82A7.49,7.49,0,0,0,22.45,4Z"></path></svg><p>
					154

				</p></div></div>
		
	</a></article><article><a href="https://huggingface.co/Runware/Qwen-Image-Layered"><div><header title="Runware/Qwen-Image-Layered"><p><img alt="" src="https://cdn-avatars.huggingface.co/v1/production/uploads/641caf6c043963b1c0a27256/-w4w2t9r-T3U2LqzF2jqe.png" crossorigin="anonymous"/>
	</p>
				<h4>Runware/Qwen-Image-Layered</h4>
				
				</header>
			<div><svg width="1em" height="1em" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.095 6.788a.7.7 0 0 1 .7.7V9.99a.7.7 0 0 1-.7.7h-3.52a.7.7 0 0 1-.7-.7v-2.5a.7.7 0 0 1 .7-.7h3.52Zm-3.52 2.608v.593h3.248l-.972-.786-.255.256-.228.227-1.073-1.019-.72.73Zm0-.9.52-.48a.3.3 0 0 1 .41.004l.781.74a.1.1 0 0 0 .139-.001l.2-.198a.3.3 0 0 1 .402-.019l1.068.882V7.488h-3.52v1.008ZM9.061 1.2a.7.7 0 0 1 .7.7v4.377h-.7V1.9H2.43v3.55l.9-.9a.7.7 0 0 1 .99 0l1.74 1.74a.791.791 0 0 0-.51.47L3.82 5.04l-1.4 1.4v2.09H5.5v.7H2.43a.7.7 0 0 1-.7-.7V1.9a.7.7 0 0 1 .7-.7h6.63Zm.324 6.542a.35.35 0 1 1 0 .701.35.35 0 0 1 0-.7Zm-2.07-2.007a.7.7 0 0 1 .496.205H7.8l.32.32H6.5l.32-.32a.703.703 0 0 1 .495-.205Zm.55-3.044v.75h-.95v1.892h-.75V3.441h-.911v-.75h2.611Z" fill="currentColor"></path></svg><p>
			Image-Text-to-Image
			<span>â€¢ </span>
		
	
				<span>Updated
					<time datetime="2025-12-19T19:55:34" title="Fri, 19 Dec 2025 19:55:34 GMT">about 7 hours ago</time></span>
				
				
				
	
				

				</p></div></div>
		
	</a></article></nav>
				

			<h2><svg style="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 25 25"><ellipse cx="12.5" cy="5" fill="currentColor" fill-opacity="0.25" rx="7.5" ry="2"></ellipse><path d="M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z" fill="currentColor" opacity="0.5"></path><path d="M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z" fill="currentColor" opacity="0.5"></path><path d="M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z" fill="currentColor"></path></svg>
				Datasets citing this paper
				<span>0</span></h2>
			<p>No dataset linking this paper</p>
				<p>Cite arxiv.org/abs/2512.15603 in a dataset README.md to link it from this page.
				</p>

			<h3><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M7.80914 18.7462V24.1907H13.2536V18.7462H7.80914Z" fill="#FF3270"></path><path d="M18.7458 18.7462V24.1907H24.1903V18.7462H18.7458Z" fill="#861FFF"></path><path d="M7.80914 7.80982V13.2543H13.2536V7.80982H7.80914Z" fill="#097EFF"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M4 6.41775C4 5.08246 5.08246 4 6.41775 4H14.6457C15.7626 4 16.7026 4.75724 16.9802 5.78629C18.1505 4.67902 19.7302 4 21.4685 4C25.0758 4 28.0003 6.92436 28.0003 10.5317C28.0003 12.27 27.3212 13.8497 26.2139 15.02C27.243 15.2977 28.0003 16.2376 28.0003 17.3545V25.5824C28.0003 26.9177 26.9177 28.0003 25.5824 28.0003H17.0635H14.9367H6.41775C5.08246 28.0003 4 26.9177 4 25.5824V15.1587V14.9367V6.41775ZM7.80952 7.80952V13.254H13.254V7.80952H7.80952ZM7.80952 24.1907V18.7462H13.254V24.1907H7.80952ZM18.7462 24.1907V18.7462H24.1907V24.1907H18.7462ZM18.7462 10.5317C18.7462 9.0283 19.9651 7.80952 21.4685 7.80952C22.9719 7.80952 24.1907 9.0283 24.1907 10.5317C24.1907 12.0352 22.9719 13.254 21.4685 13.254C19.9651 13.254 18.7462 12.0352 18.7462 10.5317Z" fill="black"></path><path d="M21.4681 7.80982C19.9647 7.80982 18.7458 9.02861 18.7458 10.5321C18.7458 12.0355 19.9647 13.2543 21.4681 13.2543C22.9715 13.2543 24.1903 12.0355 24.1903 10.5321C24.1903 9.02861 22.9715 7.80982 21.4681 7.80982Z" fill="#FFD702"></path></svg>
				Spaces citing this paper
				<span>3</span></h3>

			

			<h2><svg width="1em" height="1em" aria-hidden="true" focusable="false" role="img" viewBox="0 0 12 13" fill="none" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><rect x="2" y="2.49902" width="8" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.4"></rect><rect x="6.21875" y="6.7334" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.7"></rect><rect x="2" y="6.73438" width="3.78055" height="3.76425" rx="1.16774" fill="currentColor" fill-opacity="0.5"></rect></svg>
				Collections including this paper
				<span>3</span></h2>
			<nav>






					</nav></section></div></div></div>
  </body>
</html>
