<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://localforge.dev/blog/running-qwen3-macbook-mlx">Original</a>
    <h1>How to vibe code for free: Running Qwen3 on your Mac, using MLX</h1>
    
    <div id="readability-page-1" class="page"><section>
    <div>
        
        <blockquote>
            <p>(or how to Vibe code for free!)</p>
        </blockquote>
        
        <div>
            <p>Published: 1 May 2025</p>

            <p>
                Today I wanted to test running Qwen3 latest models locally on my mac, and putting that in an agentic loop using localforge.
            </p>

            <p>
                (or how to Vibe code for free!)
            </p>

            <p>
                Qwen3 turns out to be a quite capable model available on ollama:
            </p>

            <p>
                <a href="https://ollama.com/library/qwen3" target="_blank">https://ollama.com/library/qwen3</a>
            </p>

            <p>
                And also on mlx community: <a href="https://huggingface.co/collections/mlx-community/qwen3-680ff3bcb446bdba2c45c7c4" target="_blank">https://huggingface.co/collections/mlx-community/qwen3-680ff3bcb446bdba2c45c7c4</a>
            </p>

            <p>
                Feel free to grab a model of your choice depending on mac hardware and let&#39;s dive in.
            </p>

            <h2>Here is what I did step by step:</h2>

            <div>
                <h3>Step 1: Install the core MLX library</h3>
                <p><span>pip install mlx</span>
                    
                </p>
            </div>

            <div>
                <h3>Step 2: Install the LLM helper library</h3>
                <p><span>pip install mlx-lm</span>
                    
                </p>
            </div>

            <div>
                <h3>Step 3: Run the model server</h3>
                <p><span>mlx_lm.server --model mlx-community/Qwen3-30B-A3B-8bit --trust-remote-code --port 8082</span>
                    
                </p>
            </div>

            <p>
                This command will both download and serve it (change port to whatever you want, and be ready to download tens of gigabytes of stuff)
            </p>

            <p><img src="http://gracekwak.me/images/mlx-screenshot-download.png" alt="MLX Qwen3 model download and server startup screenshot"/>
            </p>

            <p>
                After download is done you should see something like:
            </p>

            <pre>2025-05-01 13:56:26,964 - INFO - Starting httpd at 127.0.0.1 on port 8082...</pre>

            <p>
                Meaning your model is ready to receive requests. Time to configure it in localforge!
            </p>

            <h2>Configure Localforge</h2>

            <p>
                Get your latest localforge copy at <a href="https://localforge.dev" target="_blank">https://localforge.dev</a> 
                (either npm install for any platform or if you want there are DMG and ZIP files available for OSX and Windows)
            </p>

            <p>
                Once running open settings and set it up like this:
            </p>

            <h3>1) In provider list add provider</h3>
            <p>
                I have added two providers: one is ollama for a weak model, and another is for mlx qwen3
            </p>

            <div>
                <h4>a) Ollama provider settings:</h4>
                <ul>
                    <li>Choose name: <strong>LocalOllama</strong></li>
                    <li>Choose <strong>ollama</strong> from provider types</li>
                    <li>No settings required</li>
                    <li><strong>Important prerequisite:</strong> You need to have ollama installed on your machine with some sort of model serving, preferably gemma3:latest</li>
                    <li>Install instructions for this are here: <a href="https://ollama.com/library/gemma3" target="_blank">https://ollama.com/library/gemma3</a></li>
                    <li>This model is needed for simple gerund and aux interactions such as for agent to figure out what is going on, but not serious stuff.</li>
                </ul>
            </div>

            <div>
                <h4>b) Qwen provider settings:</h4>
                <ul>
                    <li>Choose any provider name such as <strong>qwen3:mlx:30b</strong></li>
                    <li>Choose <strong>openai</strong> as provider type, because we are going to be using openai api v1</li>
                    <li>For API key put something like <strong>&#34;not-needed&#34;</strong></li>
                    <li>For API url put: <strong>http://127.0.0.1:8082/v1/</strong> (note the port you used in previous steps)</li>
                </ul>
            </div>

            <h3>2) Create a custom agent</h3>
            <p>
                After you made your provider, make custom agent!
                Go to agents tab in settings and click +Add Agent, 
                type in some name, like <strong>qwen3-agent</strong>
            </p>

            <p>
                And then click pencil icon to edit your agent.
                This will open a huge window, in it
                you care about Main and Auxiliary cards at top (ignore the Expert card, can be anything or empty)
            </p>

            <div>
                <ul>
                    <li>For <strong>Main</strong> put in your qwen provider, and as model name type in:
                    <strong>mlx-community/Qwen3-30B-A3B-8bit</strong> (or whatever you downloaded from the mlx community)</li>
                    <li>For <strong>Auxiliary</strong>, choose your LocalOllama provider, and for model put in <strong>gemma3:latest</strong></li>
                </ul>
                <p>
                    You can leave agent prompt same for now, although it may make sense to simplify it for qwen.
                    In the tool sections you can unselect browser tools to make it more simple, although this is optional.
                </p>
            </div>

            <p><img src="http://gracekwak.me/images/mlx-localforge-screens.png" alt="Localforge settings screens showing Qwen3 configuration"/>
            </p>

            <h2>Using Your New Agent</h2>

            <p>
                Now that this is done, press command+s, and close the agent editor, and then close settings.
            </p>

            <p>
                You should appear in the main chat window, in it on very top 
                there is select box saying - select agent. 
                Choose your new agent (qwen3-agent)
            </p>

            <p>
                Your agent is ready to use tools!
            </p>

            <p>
                I typed in something simple like:
            </p>

            <pre>&#34;use LS tool to show me files in this folder&#34;</pre>

            <p>
                And it did!
            </p>

            <div>
                <p><img src="http://gracekwak.me/images/mlx-result.png" alt="Localforge with Qwen3 agent successfully using tools"/></p><p>Qwen3 successfully running the LS tool through Localforge</p>
            </div>

            <p>And here&#39;s a website created by Qwen3:</p>

            <div>
                <p><img src="http://gracekwak.me/images/mlx-result-2.png" alt="Website created by Qwen3 through Localforge"/></p><p>A website created by Qwen3 using Localforge</p>
            </div>

            <p>It even made a snake game that plays itself!</p>

            <div>
                <p><img src="http://gracekwak.me/images/mlx-snake-game.png" alt="Self-playing snake game created by Qwen3"/></p><p>A self-playing snake game created by Qwen3</p>
            </div>

            <div>
                <h4>Conclusion</h4>
                <p>
                    This may require a bit more experimenting such as simplifying system prompt, or tinkering with mlx settings and model choices, 
                    but I think this is definitely possible to use to get some autonomous code generation on YOUR MAC, totally free of charge!
                </p>
                <p>
                    Happy tinkering!
                </p>
            </div>

            <p>Published 1 May 2025</p>
        </div>
    </div>
</section></div>
  </body>
</html>
