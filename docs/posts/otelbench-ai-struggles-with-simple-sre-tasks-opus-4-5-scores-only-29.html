<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://quesma.com/blog/introducing-otel-bench/">Original</a>
    <h1>OTelBench: AI struggles with simple SRE tasks (Opus 4.5 scores only 29%)</h1>
    
    <div id="readability-page-1" class="page"><section data-astro-cid-xj2uyz6m="">  </section><section data-astro-cid-xj2uyz6m=""> <div data-astro-cid-xj2uyz6m=""> <div data-astro-cid-xj2uyz6m="">  <p><strong>Now on the front page of Hacker News — <a href="https://news.ycombinator.com/item?id=46811588">see the discussion</a>.</strong></p>
<p>Frontier AI models have become excellent at writing functions, but can they actually debug production systems?</p>
<p>To fix outages, you first need to see what’s happening. In a microservices world, this means producing structured events that track a single request as it hops from service to service.</p>
<p>We asked 14 models to add distributed traces to existing codebases, using the standard method: OpenTelemetry instrumentation. We picked tasks that would be easy for a Site Reliability Engineer (SRE).</p>
<figure> <a href="https://quesma.com/benchmarks/otel/#models"> <img src="https://quesma.com/_astro/overall_ranking.CkMhOjSp_14CMuY.webp" alt="OTelBench Model Rankings showing Claude Opus 4.5 leading at 29% pass rate" loading="lazy" decoding="async" fetchpriority="auto" width="1400" height="778"/>  </a> <figcaption> <p>Go to <a href="https://quesma.com/benchmarks/otel/">OTelBench website</a> for complete charts. All models struggle with
OpenTelemetry. Even the best model, Claude 4.5 Opus, succeeded only 29% of the time, and GPT 5.2 was similar at 26%.
Surprisingly, Gemini 3 Pro has no edge over Gemini 3 Flash, which scored 19%.</p> </figcaption> </figure>
<p>We are releasing <a href="https://quesma.com/benchmarks/otel/">OTelBench</a> as an open-source benchmark, with all tasks in <a href="https://github.com/QuesmaOrg/otel-bench">QuesmaOrg/otel-bench</a>. We use the <a href="https://harborframework.com/">Harbor framework</a> (by the creators of TerminalBench), so you can easily run it yourself to reproduce results, test new models, or create benchmarks for your own use cases (we welcome contributions!).</p>
<h2 id="background-what-is-distributed-tracing">Background: What is distributed tracing?</h2>
<p>When an app runs on a single machine, you can often trace an error by scrolling through a log file. But when it runs across 50 microservices, that single request gets scattered into a chaotic firehose of disconnected events. <strong>Distributed tracing</strong> solves this by linking them back together, allowing you to follow a user action, like clicking <em>Login</em>, as it jumps from the API Gateway, to the Auth Service, to the Database, and back.</p>
<figure>  <figcaption>
Distributed tracing links a user action, like a <em>Login</em> button click, to every underlying microservice call.
</figcaption> </figure>
<p>To make this work, you need <strong>instrumentation</strong>. This is code that you add to your app to:</p>
<ol>
<li><strong>Start a trace</strong> when a request comes in.</li>
<li><strong>Pass the TraceID</strong> (context) when your app calls another service.</li>
<li><strong>Send the data</strong> to a backend so you can see the graph.</li>
</ol>
<p><a href="https://opentelemetry.io/">OpenTelemetry</a> (OTel) is the industry standard for telemetry data. Its ecosystem includes:</p>
<ul>
<li><strong>Semantic conventions:</strong> A unified schema replaces chaotic naming (e.g., <code>ip_address</code> vs <code>host.ip</code>).</li>
<li><strong>Universal SDKs:</strong> Official libraries support every major programming language.</li>
<li><strong>The Collector:</strong> A centralized agent processes and enriches data (e.g., adding Kubernetes tags) before export.</li>
<li><strong>Auto-instrumentation:</strong> Runtime agents <a href="https://newsletter.signoz.io/p/bts-of-opentelemetry-auto-instrumentation">inject code to wrap calls</a>, though this often results in noisy data.</li>
</ul>
<p>However, <strong>standard</strong> doesn’t mean <strong>easy</strong>. We know this firsthand from our contributions to the ecosystem, such as <a href="https://opentelemetry.io/blog/2025/go-compile-time-instrumentation/">Go compile-time instrumentation</a>. The process may be difficult, especially due to complexity, <a href="https://grafana.com/observability-survey/2025/">as 39% of respondents complained in the 2025 Observability Survey</a>.</p>
<h2 id="benchmarking-opentelemetry-instrumentation">Benchmarking OpenTelemetry instrumentation</h2>
<p>We tested 14 frontier LLMs on 23 realistic OpenTelemetry instrumentation tasks across 11 programming languages: Go, Java, C++, Python, JavaScript, PHP, Ruby, Rust, Erlang, .NET, and Swift.</p>
<p>It is essential to benchmark various technologies since realistic distributed systems are polyglot.
To make OpenTelemetry work, the system needs to work for all of these services - if we lose track at only one service, the chain of logs gets broken.</p>
<p>The final benchmark run cost $522 in LLM tokens across 966 runs (23 tasks × 3 attempts × 14 models).</p>
<h3 id="task">Task</h3>
<p>We start with basic tasks such as adding instrumentation to a single microservice, in a single language.
The AI agents get a small microservice with around 300 lines of code from a realistic application, and work in a Linux terminal, editing it, and running any commands if needed.</p>
<p>For example, here is the prompt for <a href="https://quesma.com/benchmarks/otel/tasks/go-microservices-traces/">go-microservices-traces</a>:</p>
<blockquote>
<p><strong>Your task is:</strong> Add OTEL tracing to all microservices.</p>
<p><strong>Requirements:</strong></p>
<ol>
<li>Instrumentation should match conventions and well-known good practices.</li>
<li>Instrumentation must match the business domain of the microservices.</li>
<li>Traces must be sent to the endpoint defined by a standard OTEL environment variable.</li>
<li>Use the recent version of the OTEL SDK.</li>
</ol>
</blockquote>
<p>We tested if it satisfies the basic criteria of OpenTelemetry instrumentation.</p>
<figure> <a href="https://quesma.com/benchmarks/otel/#tasks"> <img src="https://quesma.com/_astro/simple_tasks.DRjEgaqU_ZmHRE2.webp" alt="Pass rates for simple instrumentation tasks across different programming languages" loading="lazy" decoding="async" fetchpriority="auto" width="1776" height="422"/>  </a> <figcaption> <p>See <a href="https://quesma.com/benchmarks/otel/#tasks">full task breakdown</a>. All tasks were simple for humans and involved
short services of around 300 lines of code. Yet, many of them were hard or unsolvable by all AI models.</p> </figcaption> </figure>
<h3 id="example">Example</h3>
<p>How do LLMs fail? Let’s analyze a common failure mode.</p>
<p>Consider a web service from our benchmark where a user searches and retrieves results. The test simulates <strong>two distinct user actions</strong>:</p>
<ol>
<li><strong>Happy path</strong>: User searches, gets a token, retrieves results successfully</li>
<li><strong>Error test</strong>: User tries to retrieve results with an invalid token (gets 404)</li>
</ol>
<p>A human engineer would immediately distinguish these as <strong>two independent events</strong>, resulting in <strong>two separate traces</strong>: one for the successful search and one for the failed request.</p>
<p>The code structure makes this clear – two separate blocks, each representing a user action:</p>
<pre tabindex="0" data-language="go"><code><span><span>// User Action 1: Search and get results (happy path)</span></span>
<span><span>{</span></span>
<span><span>    response </span><span>:=</span><span> client.</span><span>Post</span><span>(</span><span>&#34;/search&#34;</span><span>, query)</span></span>
<span><span>    result </span><span>:=</span><span> client.</span><span>Get</span><span>(</span><span>&#34;/result?token=&#34;</span><span> +</span><span> response.Token)</span></span>
<span><span>}</span></span>
<span></span>
<span><span>// User Action 2: Error test (invalid token)</span></span>
<span><span>{</span></span>
<span><span>    result </span><span>:=</span><span> client.</span><span>Get</span><span>(</span><span>&#34;/result?token=invalid&#34;</span><span>)  </span><span>// Should return 404</span></span>
<span><span>}</span></span></code></pre>
<p>We would expect:</p>
<figure>   <figcaption>
Expected behavior: Two distinct user actions produce two separate traces with unique TraceIDs.
</figcaption> </figure>
<p>Yet, sometimes models failed to recognize these as separate user actions. Instead of two traces, they produced:</p>
<figure>  <figcaption>
Actual result: The model failed to clear context, causing separate user actions to be conflated into a single trace.
</figcaption> </figure>
<p><strong>The core issue</strong>: Models apply instrumentation mechanically to every HTTP call without understanding the business context. They see “HTTP requests” and link them all together, rather than recognizing “these are two separate user journeys.”</p>
<p>The models successfully instrumented the HTTP calls, but failed to propagate the Context correctly. They treated the timeline as a single flat list of events rather than two distinct hierarchical trees.</p>
<p>Our tests don’t just check compilation. We verify correct span names, parent-child relationships, valid trace IDs, and context propagation. Many models produced compiling code that generated malformed traces – proving that “it builds” is not enough for SRE work.</p>
<h2 id="observations">Observations</h2>
<h3 id="models">Models</h3>
<p>We were surprised that even the top models (as of Jan 2026) struggle.
The tasks we proposed were trivial compared to real-world scenarios. In a typical SRE job, services are massive, legacy-ridden, and poorly documented. If models fail on 300 lines of clean Go code, they cannot handle production.</p>
<p>We were surprised that:</p>
<ul>
<li><a href="https://quesma.com/benchmarks/otel/models/claude-opus-4.5/">Claude Opus 4.5</a>, the best model, got just 29% of these relatively simple tasks.</li>
<li><a href="https://quesma.com/benchmarks/otel/models/gemini-3-pro-preview/">Gemini 3 Pro</a> (which aces at general intelligence) didn’t have an edge over the much cheaper <a href="https://quesma.com/benchmarks/otel/models/gemini-3-flash-preview/">Gemini 3 Flash</a>.</li>
<li><a href="https://quesma.com/benchmarks/otel/models/gpt-5.2-codex/">GPT 5.2 Codex</a> was substantially worse than <a href="https://quesma.com/benchmarks/otel/models/gpt-5.2/">GPT 5.2</a>.</li>
</ul>
<h3 id="languages">Languages</h3>
<p>Each language has a different toolset, so it is not an apples-to-apples comparison. Our benchmark is too small to perform a comprehensive per-language comparison, yet even preliminary trends are striking.</p>
<figure> <a href="https://quesma.com/benchmarks/otel/#languages"> <img src="https://quesma.com/_astro/language_chart.BXXM9bH5_Z1tu9Lc.webp" alt="Pass rates by programming language showing C++ at 37%, Go at 20%, while Java, Ruby and Swift had 0% success" loading="lazy" decoding="async" fetchpriority="auto" width="1800" height="792"/>  </a> <figcaption> <p>C++ achieved the highest pass rate (37%), though this is partly due to having a simpler task (<a href="https://quesma.com/benchmarks/otel/tasks/cpp-simple">cpp-simple</a>) in its set.
Go, with the most tasks tested (7), reached 20% — notable for a language central to distributed systems.
JavaScript, Python, PHP, and .NET saw moderate success. Just one model solved a single Rust task.
None solved any tasks in Swift, Ruby, or (surprisingly, due to build issues) Java.</p> </figcaption> </figure>
<h3 id="cost-and-time-efficiency">Cost and time efficiency</h3>
<p>In every practical application, cost and speed matter.
As of Jan 2026, the <a href="https://en.wikipedia.org/wiki/Pareto_front">Pareto frontier</a> consists of only four models, given model performance:</p>
<ul>
<li><code>19%</code> Gemini 3 Flash (cost and speed) - the cheapest and fastest model in this benchmark (11x cheaper and 2x faster than Claude Opus 4.5)</li>
<li><code>22%</code> Claude Sonnet 4.5 (speed)</li>
<li><code>26%</code> GPT 5.2 (cost)</li>
<li><code>29%</code> Claude Opus 4.5 (cost and speed) — the best model in this benchmark, the most expensive but reasonably fast</li>
</ul>
<figure> <a href="https://quesma.com/benchmarks/otel/#cost-vs-performance"> <img src="https://quesma.com/_astro/cost_vs_performance.BoPEo92o_ZSVRKt.webp" alt="Cost efficiency scatter plot showing pass rate vs cost per run, with Gemini 3 Flash highlighted as best value" loading="lazy" decoding="async" fetchpriority="auto" width="1792" height="980"/>  </a> <figcaption> <p>See <a href="https://quesma.com/benchmarks/otel/#cost-vs-performance">cost vs performance chart</a>.</p> </figcaption> </figure>
<figure> <a href="https://quesma.com/benchmarks/otel/#speed-vs-performance"> <img src="https://quesma.com/_astro/speed_vs_performance.BB4_TZ_f_Z2j9BKV.webp" alt="Speed vs performance scatter plot showing pass rate vs average time per run" loading="lazy" decoding="async" fetchpriority="auto" width="1792" height="980"/>  </a> <figcaption> <p>See <a href="https://quesma.com/benchmarks/otel/#speed-vs-performance">speed vs performance chart</a>.</p> </figcaption> </figure>
<h2 id="why-opentelemetry-instrumentation-is-hard-for-ai">Why OpenTelemetry instrumentation is hard for AI</h2>
<p>OpenTelemetry has all the potential to be a perfect task for AI agents — it is long and tedious work, requiring a lot of scrutiny, but ultimately one that has clear specifications and can be easily tested.</p>
<p>Yet, even the frontier models fail miserably.</p>
<h3 id="it-is-a-job-not-a-puzzle">It is a job, not a puzzle</h3>
<p>Instrumentation of even a small service involves <strong>long-horizon tasks</strong>, which remain at the <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">frontier of the current AI model progress</a>.
It requires diligently connecting all pieces of code and testing them correctly.</p>
<h3 id="requires-polyglot-backend-development-skills">Requires polyglot backend development skills</h3>
<p>Realistic services use multiple languages and technologies.
It is not enough to know the concept of distributed tracing, the OpenTelemetry standard, or even the APIs of SDKs. The agent must know CMake for C++, module systems for Go, or dependency management for Java - things we tested in our previous benchmark, <a href="https://quesma.com/blog/introducing-compilebench/">CompileBench</a>.</p>
<p>Usually, cloud environments are mixtures of the newest versions of technologies (sometimes past the training cut-off dates of AI models) and legacy systems. We cannot cherry-pick or rewrite everything, since a possible outage would be too costly. We need to support all languages and frameworks used in the cloud.</p>
<p>A lot of current AI progress focuses on the most popular languages (Python and TypeScript) and reasonably modern frameworks and build systems.</p>
<h3 id="less-training-data">Less training data</h3>
<p>Although adding instrumentation is a standard engineering task, it is not common practice in open-source. The most popular applications, where reliability matters the most, are in private repositories of big tech companies such as Apple, Airbnb, or Netflix.</p>
<h2 id="conclusion">Conclusion</h2>
<h3 id="key-takeaways">Key takeaways</h3>
<ul>
<li><strong>Best models struggle</strong>: The state-of-the-art Claude Opus 4.5 solved only 29% of tasks.</li>
<li><strong>Language gaps</strong>: Models failed completely on Java, Ruby, and Swift. C++ led at 37% (boosted by an easier task), Go reached 20%.</li>
<li><strong>Silent failures</strong>: Many solutions compiled correctly but produced malformed traces or conflated distinct user journeys.</li>
<li><strong>Cost efficiency</strong>: Gemini 3 Flash exceeds Gemini 3 Pro’s performance (18%) at a fraction of the cost.</li>
</ul>
<h3 id="ai-sre-is-still-mostly-hype-but-there-is-hope">AI SRE is still mostly hype, but there is hope</h3>
<p>AI SRE in 2026 is what <a href="https://quesma.com/blog/aiops-observability/">DevOps Anomaly Detection was in 2015</a> — bold claims backed by huge marketing budgets, but lacking independent verification. There are stories of <a href="https://www.deductive.ai/blogs/datadog-thank-you-for-blocking-us">SaaS vendors abruptly killing the observability stack</a>. Our results mirror <a href="https://clickhouse.com/blog/llm-observability-challenge">ClickHouse’s findings</a>: while LLMs can assist, they lack the capabilities of a skilled SRE.</p>
<p>Claude Opus 4.5, GPT-5.2, and Gemini 3 models show promising signals. Some hard tasks like <a href="https://quesma.com/benchmarks/otel/tasks/go-microservices-traces/">go-microservices-traces</a> reached <strong>55% pass rate</strong>. With more environments for Reinforcement Learning with Verified Rewards, this looks like a solvable problem.</p>
<h3 id="looking-forward">Looking forward</h3>
<p>Reliable software is incredibly economically valuable, but today it requires too much toil. No one wants to be woken up at 2 AM to troubleshoot.</p>
<p>We need a North Star to navigate the current AI boom. Just as SWE-Bench and TerminalBench2.0 became standards for software engineering, we need an SRE-style benchmark for distributed systems. Does the industry need newer models, or perhaps multi-agent systems? A good benchmark will tell us.</p>
<p>We invite you to explore the full results on <a href="https://quesma.com/benchmarks/otel/">OTelBench</a> and help us expand the test suite on <a href="https://github.com/QuesmaOrg/otel-bench">QuesmaOrg/otel-bench</a>. Have you tried using LLMs for observability? We are curious to hear if your experience matches our findings—or if you’ve found a workflow that actually works.</p>
<p>Join the discussion on <a href="https://news.ycombinator.com/item?id=46811588">Hacker News</a>, <a href="https://www.reddit.com/r/sre/comments/1qk0rug/built_otelbench_to_test_fundamental_sre_tasks/">Reddit</a> or <a href="https://www.linkedin.com/posts/przemyslaw-delewski_recently-we-built-otelbench-a-benchmark-activity-7420034952718827520-2gND">LinkedIn</a>.</p>
<p>But for now, the verdict is clear: if you need distributed tracing across services, expect to write that code yourself.</p>  </div> </div> </section></div>
  </body>
</html>
