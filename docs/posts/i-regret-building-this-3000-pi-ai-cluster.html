<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.jeffgeerling.com/blog/2025/i-regret-building-3000-pi-ai-cluster">Original</a>
    <h1>I regret building this $3000 Pi AI cluster</h1>
    
    <div id="readability-page-1" class="page"><div><p><img src="https://www.jeffgeerling.com/sites/default/files/images/raspberry-pi-compute-blade-10-node-ai-server.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-1c81ed77-5e91-4b90-aed5-0b13581d214a" data-insert-attach="{&#34;id&#34;:&#34;1c81ed77-5e91-4b90-aed5-0b13581d214a&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Raspberry Pi AI 10 node compute blade server"/></p>

<p>I ordered a set of 10 Compute Blades in April 2023 (two years ago), and they just arrived a few weeks ago. In that time Raspberry Pi upgraded the CM4 to a CM5, so I ordered a set of 10 16GB CM5 Lite modules for my blade cluster. That should give me 160 GB of total RAM to play with.</p>

<p>This was the biggest Pi cluster I&#39;ve built, and it set me back around $3,000, shipping included:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-pricing.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-abd02693-5640-4684-8157-6fca4c656ff7" data-insert-attach="{&#34;id&#34;:&#34;abd02693-5640-4684-8157-6fca4c656ff7&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="10 Node Pi Cluster Pricing"/></p>

<p>There&#39;s another Pi-powered blade computer, the <a href="https://www.kickstarter.com/projects/1907647187/small-board-big-possibilities-xerxes-pi">Xerxes Pi</a>. It&#39;s smaller and cheaper, but it just wrapped up <em>its own</em> Kickstarter. Will it ship in less than two years? Who knows, but I&#39;m a sucker for crowdfunded blade computers, so of course I backed it!</p>

<p>But my main question, after sinking in a substantial amount of money: are Pi clusters even <em>worth</em> it anymore? I There&#39;s no way this cluster could beat the $8,000, 4-node Framework Desktop cluster in performance. But what about in <em>price per gigaflop</em>, or in efficiency or compute density?</p>

<p>There&#39;s only one way to find out.</p>

<h2>Compute Blade Cluster Build</h2>

<p>I made a video going over everything in this blog post—and the entire cluster build (and rebuild, and rebuild again) process. You can watch it here, or on YouTube:</p>

<div>
<p><iframe src="https://www.youtube.com/embed/8SiB-bNyP5E" frameborder="0" allowfullscreen=""></iframe></p>
</div>

<p>But if you&#39;re on the blog, you&#39;re probably not the type to sit through a video anyway. So moving on...</p>

<h2>Clustering means doing everything over <em>n</em> times</h2>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/compute-blade-cluster-nvme-install.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-4408b312-3e1d-4729-988d-cbae7f7bfac8" data-insert-attach="{&#34;id&#34;:&#34;4408b312-3e1d-4729-988d-cbae7f7bfac8&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Installing M.2 NVMe SSDs in Compute Blades"/></p>

<p>In the course of going from &#39;everything&#39;s in the box&#39; to &#39;running AI and HPC benchmarks reliably&#39;, I rebuilt the cluster basically three times:</p>

<ol>
<li>First, my hodgepodge of random NVMe SSDs laying around the office was unreliable. Some drives wouldn&#39;t work with the Pi 5&#39;s PCIe bus, it seems, other ones were a little flaky (there&#39;s a reason these were spares sitting around the place, and not in use!)</li>
<li>After replacing all the SSDs with <a href="https://amzn.to/4lPJ0JC">Patriot P300s</a>, they were more reliable, but the CM5s would throttle under load</li>
<li>I put <a href="https://amzn.to/45YACBV">these CM heatsinks</a> on without screwing them in... then realized they would pop off sometimes, so I took all the blades out <em>again</em> and screwed them into the CM5s/Blades so they were more secure for the long term.</li>
</ol>

<h2>Compute Blade Cluster HPL Top500 Test</h2>

<p>The first benchmark I ran was my <a href="https://github.com/geerlingguy/top500-benchmark">top500 High Performance Linpack cluster benchmark</a>. This is my favorite cluster benchmark, because it&#39;s the traditional benchmark they&#39;d run on massive supercomputers to get on the <a href="https://www.top500.org">top500 supercomputer list</a>.</p>

<p>Before I installed heatsinks, the cluster got 275 Gflops, which is an 8.5x speedup over a single 8 GB CM5. Not bad, but I noticed the cluster was only using 105 Watts of power during the run. Definitely more headroom available.</p>

<p>After fixing the thermals, the cluster did not throttle, and used around 130W. At full power, I got 325 Gflops, which is a 10x performance improvement (for 10x 16GB CM5s) over a single 8 GB CM5.</p>

<p>Compared to the $8,000 <a href="https://www.jeffgeerling.com/blog/2025/i-clustered-four-framework-mainboards-test-huge-llms">Framework Cluster I benchmarked last month</a>, this cluster is about 4 times slower:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-hpl-cluster.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-4282fbe8-fd0e-4ea3-892d-548bb448a726" data-insert-attach="{&#34;id&#34;:&#34;4282fbe8-fd0e-4ea3-892d-548bb448a726&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Pi vs Framework Cluster HPL performance"/></p>

<p>But the Pi cluster is <em>slightly</em> more energy efficient, on a Gflops/W basis:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-hpl-efficiency.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-0d2634dd-ec84-4232-b303-9cbf3f1e26bf" data-insert-attach="{&#34;id&#34;:&#34;0d2634dd-ec84-4232-b303-9cbf3f1e26bf&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Pi vs Framework Cluster HPL Efficiency"/></p>

<p>But what about price?</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-hpl-cost.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-ec9135e8-e6c6-4709-83ea-751dfb72a3d6" data-insert-attach="{&#34;id&#34;:&#34;ec9135e8-e6c6-4709-83ea-751dfb72a3d6&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Pi vs Framework Cluster Cost"/></p>

<p>The Pi is a little less cost-effective for HPC applications than a Framework Desktop running a AMD&#39;s fastest APU. So discounting the fact we&#39;re only talking CPUs, I don&#39;t think any hyperscalers are looking to swap out a few thousand AMD EPYC systems for 10,000+ Raspberry Pis :)</p>

<p>But what about AI use cases?</p>

<h2>Compute Blade Cluster AI Test</h2>

<p>With 160 GB of total RAM, shared by the CPU and iGPU, this could be a small, efficient AI Cluster, right? Well, you&#39;d think.</p>

<p>But no: <a href="https://github.com/ggml-org/llama.cpp/issues/9801">currently llama.cpp can&#39;t speed up AI using Vulkan on the Pi 5 iGPU</a>. That means we have 160 GB of RAM, but only CPU-powered inference. On pokey Arm Cortex A76 CPU cores with 10 GB/sec or so of memory bandwidth.</p>

<p>A small model (Llama 3.2:3B), running on a single Pi, isn&#39;t horrible; you get about 6 tokens per second. But that is pretty weak compared to even an Intel N100 (much less a single Framework Desktop):</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-ai-llama-32-3b.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-f74c7f5d-5974-4d10-b651-95c9e9f78222" data-insert-attach="{&#34;id&#34;:&#34;f74c7f5d-5974-4d10-b651-95c9e9f78222&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="AI model Llama 3.2 3B on Pi vs Framework vs N150"/></p>

<p>You could have 10 nodes running 10 models, and that might be a very niche use case, but the real test would be running a <em>larger</em> AI model across all nodes. So I switched tracks to Llama 3.3:70B, which is a 40 GB model. It <em>has</em> to run across multiple Pis, since no single Pi has more than 16 GB of RAM.</p>

<p>Just as with the Framework cluster, llama.cpp RPC was <em>very</em> slow, since it splits up the model layers on all the cluster members, then goes round-robin style asking each node to perform its prompt processing, then token generation.</p>

<p>The Pi cluster couldn&#39;t even make it to token generation (tg) on my default settings, so I had to dial things back and only generate 16 tokens at a time to allow it to complete.</p>

<p>And after all that? Only 0.28 tokens per second, which is <em>25x slower</em> than the Framework Cluster, running the same model (except on AI Max iGPUs with Vulkan).</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-ai-llama-33-70b-tg.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-2006e890-bec6-4c83-bfad-723353ab509b" data-insert-attach="{&#34;id&#34;:&#34;2006e890-bec6-4c83-bfad-723353ab509b&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Pi vs Framework Cluster AI token generation"/></p>

<p>I also tried <a href="https://github.com/exo-explore/exo">Exo</a> and <a href="https://github.com/b4rtaz/distributed-llama">distributed-llama</a>. Exo was having trouble even running a small 3B model on even a 2 or 3 node Pi cluster configuration, so I stopped trying to get that working.</p>

<p>Distributed llama worked, but only with up to 8 nodes for the 70B model. Doing that, I got a more useful 0.85 tokens/s, but that&#39;s still 5x slower than the Framework cluster (and it was a bit more fragile than llama.cpp RPC—the tokens were sometimes gibberish):</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/framework-vs-pi-cluster-ai-llama-33-70b-tg-dllama.png" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-8f513004-09fa-4815-a2e5-676fce72077b" data-insert-attach="{&#34;id&#34;:&#34;8f513004-09fa-4815-a2e5-676fce72077b&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Pi vs Framework Cluster AI token generation - dllama"/></p>

<p>You can find <em>all</em> my AI cluster benchmarking results in the issue <a href="https://github.com/geerlingguy/beowulf-ai-cluster/issues/6">Test various AI clustering setups on 10 node Pi 5 cluster</a> over on GitHub.</p>

<h2>Gatesworks and Conclusion</h2>

<p>Bottom line: this cluster&#39;s not a powerhouse. And dollar for dollar, if you&#39;re spending over $3k on a compute cluster, it&#39;s not the best value.</p>

<p>It <em>is</em> efficient, quiet, and compact. So if <em>density</em> is important, and if you need lots of small, physically separate nodes, this could actually make sense.</p>

<p>Like the only real world use case besides learning is for CI jobs or high security edge deployments, where you&#39;re not allowed to run multiple things on one server.</p>

<p>That&#39;s what <a href="https://unredacted.org/blog/2025/05/unredacted-labs/">Unredacted Labs</a> is building Pi clusters for: they&#39;re building Tor exit relays on blades, after they found the Pi was the most efficient way to run massive amounts of nodes. If your goal is efficiency and node density, this does win, ever so slightly.</p>

<p>But for 99% of you reading this: this is not the cluster you&#39;re looking for.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/gateworks-gblade-server.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-57779fa3-1245-41f2-a134-8301fc41a298" data-insert-attach="{&#34;id&#34;:&#34;57779fa3-1245-41f2-a134-8301fc41a298&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Gateworks Gblade server"/></p>

<p>Two years ago, when I originally ordered the Blades, <a href="https://www.gateworks.com">Gateworks</a> reached out. They were selling a souped up version of the Compute Blade, made to an industrial spec. The <a href="https://www.gateworks.com/products/arm-server-blades/gblade-arm-server-blade/">GBlade</a> is around Pi 4 levels of performance, but with <em>10</em> gig networking, along with a 1 gig management interface.</p>

<p>But... it&#39;s discontinued. It doesn&#39;t look like any type of compute blade really lit the world on fire, and like the Blade movie series, the Compute Blade is more of a cult classic than a mainstream hit.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/compute-blade-cluster-front.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-3d1869ee-6403-40af-8e7d-e1521044c0b5" data-insert-attach="{&#34;id&#34;:&#34;3d1869ee-6403-40af-8e7d-e1521044c0b5&#34;,&#34;attributes&#34;:{&#34;alt&#34;:[&#34;alt&#34;,&#34;description&#34;],&#34;title&#34;:[&#34;title&#34;]}}" alt="Compute Blade 10 node Raspberry Pi Cluster mini rack 1"/></p>

<p>This is a <em>bad</em> cluster. Except for maybe blade 9, which dies every time I run a benchmark. But I will keep it going, knowing it&#39;s <em>definitely</em> easier to maintain than the <a href="https://www.independent.com/2025/04/29/worlds-biggest-raspberry-pi-cluster-is-now-at-uc-santa-barbara/">1,050 node Pi cluster at UC Santa Barbera</a>, which to my knowledge is still the world&#39;s largest!</p>

<p>Before I go, I just wanted to give a special thanks to everyone who supports my on <a href="https://www.patreon.com/geerlingguy">Patreon</a>, <a href="https://github.com/sponsors/geerlingguy">GitHub</a>, <a href="https://www.youtube.com/c/JeffGeerling">YouTube Memberships</a>, and <a href="https://www.floatplane.com/channel/JeffGeerling">Floatplane</a>. It really helps when I take on these months- (or years!) long projects.</p>

<h2>Parts Used</h2>

<p>You might not want to replicate my cluster setup — but I always get asked what parts I used (especially the slim Ethernet cables... everyone asks about those!), so here&#39;s the parts list:</p>

<ul>
<li><a href="https://www.pishop.us/product/compute-blade-dev/">Compute Blade DEV</a></li>
<li><a href="https://www.pishop.us/product/fansta-1v1-0/">Compute Blade Standard Fan Unit</a></li>
<li><a href="https://github.com/Uptime-Lab/compute-blade/tree/main/models/bladerunner">Compute Blade 10&#34; 3D Print Rackmount</a></li>
<li><a href="https://www.pishop.us/product/raspberry-pi-compute-module-5-16gb-ram-lite-cm5016000/">Raspberry Pi CM5 16GB (CM5016000)</a></li>
<li><a href="https://amzn.to/45YACBV">GLOTRENDS Aluminum CM5 Heatsink</a></li>
<li><a href="https://amzn.to/4lPJ0JC">Patriot P300 256GB NVMe SSD 10-pack</a></li>
<li><a href="https://amzn.to/3UNOwSd">GigaPlus 2.5 Gbps 10 port PoE+ switch</a></li>
<li><a href="https://www.printables.com/model/1215585-unified-10-rack-gigaplus-switch-mounting-ears">GigaPlus 10&#34; Rack Mount 3D Print ears</a></li>
<li><a href="https://amzn.to/4fX1gzr">Monoprice Cat6A SlimRun 6&#34; Cat6 patch cables (10 pack)</a></li>
<li><a href="https://amzn.to/47UlPKX">ioplex SFP+ Twinax DAC patch cable</a></li>
<li><a href="https://amzn.to/3UUjCHP">DeskPi RackMate TT</a></li>
</ul></div></div>
  </body>
</html>
