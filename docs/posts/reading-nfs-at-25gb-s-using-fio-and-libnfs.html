<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://taras.glek.net/post/nfs-for-fio/">Original</a>
    <h1>Reading NFS at 25GB/s using FIO and libnfs</h1>
    
    <div id="readability-page-1" class="page"><div role="main">
  <div>
    <div>
      <article role="main">
        <p>My current employer does a lot of really cool systems work that’s covered by NDAs. I recently did some work to integrate a cool open source tool into our workflow. Felt it deserved a blog post.</p>
<h3 id="nfs-testing-requires-parallelism">NFS Testing Requires Parallelism.</h3>
<p>I work for <a href="https://www.purestorage.com/">Pure Storage</a>. One of the products we make is a scale-out NFS<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> (and S3-compatible) server called <a href="https://www.purestorage.com/products/file-and-object/flashblade.html">FlashBlade</a>.</p>
<p>I was asked to test FlashBlade<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> performance scaling. I needed to generate NFS read workloads of 15-300 <em>Gigabytes</em>/second. Given that NICs in our lab max out at 100Gbit, (~10GB/s), this required a lab with fast servers and multiple NICs per server. I wanted to make maximum use of hardware to minimize lab space + cost. To make use of multiple NICs I needed multiple NFS connections per NIC. Linux does not make this easy<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p>
<p>The project came down to:</p>
<blockquote>
<p>What is an easy way to drive and measure a high-throughput NFS workload involving multiple servers and multiple connections per server?</p>
<p><em>This didn’t exist. Was time to write something.</em></p>
</blockquote>
<h3 id="fio">FIO</h3>
<p><a href="https://github.com/axboe/fio">Fio</a> is a gem of a benchmark utility. It has a plugin architecture capable of testing anything from phones to hadoop clusters. It records decent statistics and has a nice configuration language to define workloads. Crucially, it also can work in a client/server configuration to drive clustered workloads.</p>
<p>Incredibly, <a href="https://twitter.com/axboe">Jens</a> has been maintaining fio since at-least 2005. Changes<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> in storage technology made other utilities obsolete in meantime. Maybe fio has peaked too as it looks like his most recent io_ring IOPS records are not being tested with fio<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.</p>
<p>Two years ago when I first evaluated fio, it worked great for testing NFS via usual linux filesystem drivers, you <em>just</em> had to mount the relevant filesystems<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p>
<p><img src="https://taras.glek.net/images/NFS-for-FIO/fio-libnfs.jpg" alt="libnfs + FIO =&gt; performant"/></p>
<p><a href="https://github.com/sahlberg/libnfs">libnfs</a> is a lovely userspace implementation of NFS client protocol. I opted to integrate that into fio as an NFS plugin.</p>
<p>Now one can use fio <code>ioengine=nfs</code> to drive NFS workloads from userspace without worrying about kernel NFS-client bottlenecks/workarounds.</p>
<p>A simple write + read workload with 10 parallel connections is in the fio <a href="https://github.com/axboe/fio/blob/master/examples/nfs.fio">source code</a>.</p>
<h3 id="results-we-can-test-purestorage-flashblade-with-reasonably-small-labs">Results: We can test PureStorage FlashBlade with reasonably small labs</h3>
<p>We now use fio with a client/server workload similar to example above. Fio can drive around 25 gigabytes<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> of NFS read throughput per 1U AMD EPYC3 server. Thus we could reach 300 gigabytes with 12 servers.</p>
<p>It took about a month to write/debug the fio/libnfs integration and another week to get it landed in fio. The maintainers were super-friendly, helpful and responsive. It was my best experience contributing to an open-source project.</p>
<h3 id="next-steps">Next steps</h3>
<p>Selecting and learning to tune hardware took many more months of work. If there is further interest on tuning Linux to read from multiple 100G nics, I can do a followup post on that.</p>
<p><em>Disclaimer</em>: Fio with nfs plugin is fantastic for achieving hero bandwidth numbers, but it needs more work before it’s useful for any sort of failure-testing.</p>
<p><a href="https://news.ycombinator.com/item?id=29392402#29393195">Comments</a></p>



        

        

        
      </article>

      
        
      


      

    </div>
  </div>
</div></div>
  </body>
</html>
