<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://diff.wikimedia.org/2025/04/01/how-crawlers-impact-the-operations-of-the-wikimedia-projects/">Original</a>
    <h1>Crawlers impact the operations of the Wikimedia projects</h1>
    
    <div id="readability-page-1" class="page"><article id="post-155112">
	<div>
		<!-- .entry-header -->

		
			<!-- .post-thumbnail -->

		
		<div>
			
<figure><a href="https://diff.wikimedia.org/?attachment_id=155118"><img data-recalc-dims="1" decoding="async" width="1024" height="683" src="https://diff.wikimedia.org/wp-content/uploads/2025/03/1024px-A_millipede_insect.jpg?w=1024&amp;resize=1024%2C683" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2025/03/1024px-A_millipede_insect.jpg?w=1024 1024w, https://diff.wikimedia.org/wp-content/uploads/2025/03/1024px-A_millipede_insect.jpg?w=300 300w, https://diff.wikimedia.org/wp-content/uploads/2025/03/1024px-A_millipede_insect.jpg?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"/></a></figure>



<p>Since the beginning of 2024, the demand for the content created by the Wikimedia volunteer community – especially for the 144 million images, videos, and other files on <a href="https://commons.wikimedia.org/wiki/Commons:Welcome">Wikimedia Commons</a> – has grown significantly. In this post, we’ll discuss the reasons for this trend and its impact.</p>



<p>The <a href="https://wikimediafoundation.org/our-work/wikimedia-projects/">Wikimedia projects</a> are the largest collection of open knowledge in the world. Our sites are an invaluable destination for humans searching for information, and for all kinds of businesses that access our content automatically as a core input to their products. Most notably, the content has been a critical component of search engine results, which in turn has brought users back to our sites. But with the rise of AI, the dynamic is changing: We are observing a significant increase in request volume, with most of this traffic being driven by <a href="https://en.wikipedia.org/wiki/Web_scraping">scraping</a> bots collecting training data for large language models (LLMs) and other use cases. Automated requests for our content have grown exponentially, alongside the broader technology economy, via mechanisms including scraping, APIs, and bulk downloads. This expansion happened largely without sufficient attribution, which is key to drive new users to participate in the movement, and is causing a significant load on the underlying infrastructure that keeps our sites available for everyone. </p>



<h3><strong>A view behind the scenes: The Jimmy Carter case</strong></h3>



<p>When <a href="https://en.wikipedia.org/wiki/Jimmy_Carter">Jimmy Carter</a> died in December 2024, his page on English Wikipedia saw more than 2.8 million views over the course of a day. This was relatively high, but manageable. At the same time, quite a few users played a <a href="https://commons.wikimedia.org/wiki/File:Presidential_Debate_with_Ronald_Reagan_and_President_Carter,_October_28,_1980.webm">1.5 hour long video</a> of Carter’s 1980 presidential debate with Ronald Reagan. This caused a surge in the network traffic, doubling its normal rate. As a consequence, for about one hour a small number of Wikimedia’s connections to the Internet filled up entirely, causing slow page load times for some users. The sudden traffic surge alerted our <a href="https://www.mediawiki.org/wiki/Wikimedia_Site_Reliability_Engineering">Site Reliability team</a>, who were swiftly able to address this by changing the paths our internet connections go through to reduce the congestion. But still, this should not have caused any issues, as the Foundation is well equipped to handle high traffic spikes during exceptional events. So what happened?</p>



<p>Since January 2024, we have seen the bandwidth used for downloading multimedia content grow by 50%. This increase is not coming from human readers, but largely from automated programs that scrape the Wikimedia Commons image catalog of openly licensed images to feed images to AI models. Our infrastructure is built to sustain sudden traffic spikes from humans during high-interest events, but the amount of traffic generated by scraper bots is unprecedented and presents growing risks and costs. </p>



<p>The graph below shows that the base bandwidth demand for multimedia content has been growing steadily since early 2024 – and there’s no sign of this slowing down. This increase in baseline usage means that we have less room to accommodate exceptional events when a traffic surge might occur: a significant amount of our time and resources go into responding to non-human traffic.</p>



<figure><a href="https://diff.wikimedia.org/?attachment_id=155114"><img data-recalc-dims="1" decoding="async" width="512" height="380" src="https://diff.wikimedia.org/wp-content/uploads/2025/03/Multimedia_bandwith_demand_for_the_Wikimedia_Projects.png?resize=512%2C380" alt="" srcset="https://diff.wikimedia.org/wp-content/uploads/2025/03/Multimedia_bandwith_demand_for_the_Wikimedia_Projects.png?w=512 512w, https://diff.wikimedia.org/wp-content/uploads/2025/03/Multimedia_bandwith_demand_for_the_Wikimedia_Projects.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"/></a><figcaption><em>Multimedia bandwidth demand for the Wikimedia Projects.</em></figcaption></figure>



<h3></h3>



<p>The Wikimedia Foundation serves content to its users through a <a href="https://diff.wikimedia.org/2024/07/26/the-journey-to-open-our-first-data-center-in-south-america/">global network of datacenters</a>. This enables us to provide a faster, more seamless experience for readers around the world. When an article is requested multiple times, we memorize – or cache – its content in the datacenter closest to the user. If an article hasn’t been requested in a while, its content needs to be served from the core data center. The request then “travels” all the way from the user’s location to the core datacenter, looks up the requested page and serves it back to the user, while also caching it in the regional datacenter for any subsequent user. </p>



<p>While human readers tend to focus on specific – often similar – topics, crawler bots tend to “bulk read” larger numbers of pages and visit also the less popular pages. This means these types of requests are more likely to get forwarded to the core datacenter, which makes it much more expensive in terms of consumption of our resources. </p>



<p>While undergoing a migration of our systems, we noticed that only a fraction of the expensive traffic hitting our core datacenters was behaving how web browsers would usually do, interpreting javascript code. When we took a closer look, we found out that at least 65% of this resource-consuming traffic we get for the website is coming from bots, a disproportionate amount given the overall pageviews from bots are about 35% of the total. This high usage is also causing constant disruption for our Site Reliability team, who has to block overwhelming traffic from such crawlers before it causes issues for our readers.</p>



<p>Wikimedia is not alone with this challenge. As noted in our 2025 <a href="https://diff.wikimedia.org/2025/03/04/global-trends-2025/">global trends report</a>, technology companies are racing to scrape websites for human-created and verified information. <a href="https://www.technologyreview.com/2025/02/11/1111518/ai-crawler-wars-closed-web/">Content publishers</a>, <a href="https://thelibre.news/foss-infrastructure-is-under-attack-by-ai-companies/">open source projects</a>, and <a href="https://www.theregister.com/2025/03/18/ai_crawlers_sourcehut/">websites</a> of all kinds report similar issues. Moreover, crawlers tend to access any URL. Within the Wikimedia infrastructure, we are observing scraping not only of the Wikimedia projects, but also of key systems in our developer infrastructure, such as our code review platform or our bug tracker. All of that consumes time and resources that we need to support the Wikimedia projects, contributors, and readers. </p>



<h3><strong>Our content is free, our infrastructure is not: Establishing responsible use of infrastructure</strong></h3>



<p><a href="https://meta.wikimedia.org/wiki/Strategy/multigenerational">Delivering trustworthy content</a> also means supporting a “<a href="https://meta.wikimedia.org/wiki/Strategy/Wikimedia_movement/2017/Direction#Our_strategic_direction:_Service_and_Equity">knowledge as a service</a>” model, where we acknowledge that the whole internet draws on Wikimedia content. But this has to happen in ways that are sustainable for us: How can we continue to enable our community, while also putting boundaries around automatic content consumption? How might we funnel developers and reusers into preferred, supported channels of access? What guidance do we need to incentivise responsible content reuse? </p>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt=""/>
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2025%2F04%2F01%2Fhow-crawlers-impact-the-operations-of-the-wikimedia-projects%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div><!-- .entry-content -->
	</div>

	<!-- .entry-footer -->

</article></div>
  </body>
</html>
