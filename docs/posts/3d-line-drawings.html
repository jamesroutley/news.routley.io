<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://amritkwatra.com/experiments/3d-line-drawings">Original</a>
    <h1>3D Line Drawings</h1>
    
    <div id="readability-page-1" class="page"><article>
                <section id="title-main">
                    
                    
                    <p>
                        June 30th, 2025
                    </p>
                    
                </section>
                <section>
                    <p>
                    This is an experiment examing how to create a 3D line drawing of a scene. In this post, I will describe how this can be done by augmenting the process of generating <span>3D Gaussian Splats</span>
                    <label for="mn-35be8a"></label>
                    
                    <span><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" target="_blank" rel="noopener noreferrer">3D Gaussian Splatting for Real-Time Radiance Field Rendering</a>, by Kerbl et al.</span> and leveraging a process to transform photographs into <span>Informative Line Drawings</span>
                    <label for="mn-32f931"></label>
                    
                    <span><a href="https://carolineec.github.io/informative_drawings/" target="_blank" rel="noopener noreferrer">Learning to Generate Line Drawings that Convey Geometry and Semantics</a>, by Chan, Isola &amp; Durand</span>.
                </p>
                
                <figure>
                    
                </figure>
                <p>
                The majority of scenes shown above are generated using a <em>contour</em> style. <strong>You can switch the active scene using the menu in the top-right corner of the iframe</strong>. Each scene is trained for 21,000 iterations on an Nvidia RTX 4080S, using <a href="https://github.com/MrNeRF/gaussian-splatting-cuda" target="_blank" rel="noopener noreferrer">
<code>
gaussian-splatting-cuda
                </code>
            </a> from MrNerf with default settings. Examples here were generated using scenes from the <a href="https://www.tanksandtemples.org/" target="_blank" rel="noopener noreferrer">Tanks &amp; Temples Benchmark</a>. The scene is interactive, and rendered using <a href="https://github.com/mkkellogg/GaussianSplats3D" target="_blank" rel="noopener noreferrer">Mark Kellogg&#39;s web-based renderer</a>.
            To explore these scenes in fullscreen, <a href="https://splat-serve.pages.dev/?id=caterpillar-contour" target="_blank" rel="noopener noreferrer">click here.</a>
        </p>
        
        <h3>
            Creating Line Drawings from Images
        </h3>
        <figure>
            <label for="mn-figure-60d5a5">⊕</label>
            
            <span>
                <em>Figure 1.</em> <em>(a)</em> A source image from the Caterpillar scene in <a href="https://www.tanksandtemples.org/">Tanks &amp; Temples</a>. <em>(b)</em> A generated line drawing in the <em>contour</em> style. <em>(c)</em> A generated line drawing in the <em>anime</em> style.
            </span>
            <img src="https://amritkwatra.com/assets/3d_line_drawing__fig1-ShImBJCe.png" alt="Figure 1"/>
        </figure>
        <p>
        Images are transformed into line drawings using the approach introduced by Chan et al. in <a href="https://carolineec.github.io/informative_drawings/" target="_blank" rel="noopener noreferrer">Learning to Generate Line Drawings that Convey Geometry and Semantics</a>. They describe a process for transforming photographs into line drawings that preserve the semantics and geometry captured in the photograph while rendering the image in an artistic style. They do this by training a generative adversarial network (GAN) that minimizes <span>geometry</span>
        <label for="mn-5aa174"></label>
        
        <span>The geometry loss is computed using monocular depth estimation.</span>, <span>semantics</span>
        <label for="mn-7dcc1a"></label>
        
        <span>The semantics loss is computed using CLIP embeddings.</span>, and <span>appearance</span>
        <label for="mn-16f242"></label>
        
        <span>The appearance loss is based on a collection of unpaired style references.</span> losses. Their work is fantastic and I recommend reading their paper if you&#39;re interested in the details. Figure 1 depicts the input photograph and output line drawing in two styles, generated using Chan et al.&#39;s <a href="https://github.com/carolineec/informative-drawings" target="_blank" rel="noopener noreferrer">code and model weights</a>.
    </p>
    <h3>
        3D Gaussian Splatting
    </h3>
    <p>
    3D Gaussian Splatting is a technique that transforms collections of <span>posed images</span>
    <label for="mn-5402d3"></label>
    
    <span>Images for which we estimate the relative position and rotation of the camera in the scene. Usually, this is computed using Structure-from-Motion (SfM).</span> into a volumetric representation called a radiance field. Generally, scenes can be created from collections of images captured from multiple overlapping viewpoints, either by taking multiple photographs from different angles or by sampling an input video while moving through the scene. For more complete details on 3D Gaussian Splatting, I encourage you to read <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" target="_blank" rel="noopener noreferrer">the paper</a> or watch this <a href="https://www.youtube.com/watch?v=VkIJbpdTujE" target="_blank" rel="noopener noreferrer">explanation video</a> from Computerphile.
</p>
<p>
The scenes that are produced using 3D Gaussian Splatting are photorealistic and can be rendered at real-time rates using existing tools such as WebGL. However, I noticed that if the images used to train the 3D Gaussian Splat were swapped out with the <span>line-drawing counterparts</span>
<label for="mn-459d32"></label>

<span>While I transformed the images into line drawings, this transformation could be carried out using a number of other stylistic effects.</span> then the resulting scene would depict a kind of 3D line drawing. Similar to <span>regular sketching</span>
<label for="mn-ea08a3"></label>

<span>Illustrating a 3D scene as a 2D drawing from a specific perspective.</span>, the lines that are rendered are view-dependant and change based on your perspective in the scene.
</p>
<h3>
    Swapping Source Images
</h3>
<figure>
    <label for="mn-figure-5dc885">⊕</label>
    
    <span>
        <em>Figure 2.</em> The transformed images can be used to create 3D line drawings by swapping them for the original images in one of two places, shown here in green. Either before using SfM to generate the sparse model and camera poses or prior to training the 3D Gaussian Splat. The former computes SfM points based on the transformed images, while the latter keeps the points from the original images and computes the rendering loss using the transformed images. Figure adapted from the <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">original 3D Gaussian Splatting paper</a>.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__swap-xknqvGhJ.png" alt="Figure 2"/>
</figure>
<p>
Generating these 3D line drawings requires only a slight modification to the conventional process for generating 3D Gaussian Splats. At a high level, we simply swap the original images with the ones generated using Chan et al.&#39;s method for transforming images into informative line drawings. This swap can happen in two places: prior to training the 3D Gaussian Splat or prior to estimating camera poses and sparse points using structure-from-motion (SfM).
</p>
<p>
When images are transformed prior to training the 3D Gaussian Splat, the camera poses and initial 3D Gaussians are derived from the original images but the loss is computed by comparing the rasterization output to the informative line drawing. As a result, the final scene also captures slight amounts of color that is not present in the transformed images. I believe this is a side-effect of intializing the 3D Gaussians using the original images. If you transform the images prior to applying SfM, the initialization and camera poses are based on the transformed images and the color artifacts are removed.
</p>
<figure>
    <label for="mn-figure-35efdb">⊕</label>
    
    <span>
        <em>Figure 3.</em> <em>(a)</em> A 3D line drawing created by swapping the transformed images for the original ones prior to using SfM. <em>(b)</em> A 3D line drawing generated by swapping the transformed images for the original ones prior to training the 3D Gaussian Splat.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__pre-sfm-pre-train-DdbSCa4M.png" alt="Figure 3"/>
</figure>
<p>
If you replace the images prior to applying SfM, there is less information to use when generating a sparse model and estimating camera poses. As a result, the process is more reliable when images are replaced prior to training the 3D Gaussian Splat. Additionally, if you&#39;re iterating on different styles and tweaking the transformation process, swapping prior to training the 3D Gaussian Splat lets you avoid re-running COLMAP (or other SfM tools) for each iteration. For the majority of my examples, I swapped the images prior to training the 3D Gaussian Splat. Figure 3 shows a comparison between scenes generated using these two methods.
</p>

<figure>
    <label for="mn-figure-2b9fff">⊕</label>
    
    <span>
        <em>Figure 4.</em> <em>(a)</em> A 3D line drawing created by swapping the original images with informative line drawings prior to training the 3D Gaussian Splat. Note that the slight amount of color is an artifact that is likely the result of initializing the 3D Gaussians from the original set of images. <em>(b)</em> A 3D line drawing that blends low-frequency color information from the original scene to produce a watercolor-like effect.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__color_comp-Ck96zCOw.png" alt="Figure 4"/>
</figure>
<p>
I wanted to see how we could add some color information back into the generated line drawings and ultimately into the generated scenes. To do this, I generated a slightly modified <span>hybrid image</span>
<label for="mn-a23a0a"></label>

<span>&#34;A hybrid image is a picture that combines the low-spatial frequencies of one picture with the high spatial frequencies of another picture&#34; - <a href="https://web.stanford.edu/class/ee367/reading/OlivaTorralb_Hybrid_Siggraph06.pdf" target="_blank" rel="noopener noreferrer">Oliva, Torralba &amp; Schyns</a> </span> which blends color information from the original image into the line drawing to create a watercolor effect in the final image. Figure 4 shows the contour image and the blended hybrid image. There are a few scenes in the interactive example above that use this method of adding color, such as the <a href="https://splat-serve.pages.dev/?id=caterpillar-color" target="_blank" rel="noopener noreferrer">Blended Caterpillar Scene</a>, and <a href="https://splat-serve.pages.dev/?id=lighthouse-color" target="_blank" rel="noopener noreferrer">Blended Lighthouse Scene</a>.
</p>
<figure>
    <label for="mn-figure-03a07b">⊕</label>
    
    <span>
        <em>Figure 5.</em> A video orbiting around the spliced Caterpillar scene. In this scene, viewpoints from one hemisphere of the scene render the Blended Color Contour Caterpillar Scene, and viewpoints from the other hemisphere render the source photorealistic Caterpillar Scene. Orbitting around the scene demonstrates how style can gradually change as you move through the scene.
    </span>
    <video width="100%" controls="" autoplay="" loop="" muted="" playsinline="">
        <source src="https://splats.amritkwatra.com/real-hybrid-blend-orbit-web-2x.mp4" type="video/mp4"/>
            Your browser does not support the video tag.
        </video>
    </figure>
    <p>
    A subtle, but interesting effect can be achieved by splicing together the source scene and color scene. By training a 3D Gaussian Splat to reconstruct photorealistic images from one set of perspectives and colored, contour images from another distinct set of perspectives, you can gradually transition styles within the same scene based on the viewing perspective. Figure 5 demonstrates this by orbiting around a scene in the <a href="https://splat-serve.pages.dev/?id=caterpillar-real-hybrid-blend" target="_blank" rel="noopener noreferrer">Spliced Caterpillar Scene</a>.
</p>

<figure>
    <label for="mn-figure-ce6432">⊕</label>
    
    <span>
        <em>Figure 6.</em> A <em>collage</em> image that segments out an object (in this example the tractor) in a scene and replaces it with the corresponding object from the stylized scene. This replacement, or collage, happens in image space prior to training the Gaussian Splat. <em>(a)</em> Inserts the <em>contour</em> style into the original scene. <em>(b)</em> Inserts the original scene into the stylized <em>contour</em> scene.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__collage-D6cuP6A0.png" alt="Figure 6"/>
</figure>
<p>
Another experiment I tried was to create a <em>collage</em> scene that renders only the subject of the scene as a line drawing and leaves the background unaltered. To do this I used a Meta&#39;s <a href="https://github.com/facebookresearch/segment-anything/tree/main?tab=readme-ov-file#model-checkpoints" target="_blank" rel="noopener noreferrer">Segment-Anything Model</a> <span>(SAM)</span>
<label for="mn-f88186"></label>

<span>To avoid manually specifying areas in the image to segment using SAM, I used <a href="https://github.com/luca-medeiros/lang-segment-anything" target="_blank" rel="noopener noreferrer">LangSAM</a> to automatically segment images based on a text prompt.</span>  to mask out the subject in each input photograph and replace it with the corresponding area from the generated line drawing image. The resulting scene is depicted in Figure 6 and can be explored here: <a href="https://splat-serve.pages.dev/?id=caterpillar-collage" target="_blank" rel="noopener noreferrer">Caterpillar Collage Scene</a>.
</p>
<!--
<figure>
    <label for="mn-figure-9039c0" class="margin-toggle">⊕</label>
    <input type="checkbox" id="mn-figure-9039c0" class="margin-toggle"/>
    <span class="marginnote">
        <em>Figure 6.</em> A <em>collage</em> image that segments out multiple objects (in this example, benches) in a scene and replaces it with the corresponding object from the stylized scene.
    </span>
    <img src="../imgs/3d_line_drawing__collage_multiple.png" alt="Figure 6" />
</figure>
This method can also render multiple subjects in the scene in different styles. Figure 6 shows multiple objects (benches) being rendered as line drawings within a scene. View the full scene here: [Family Collage Scene](https://splat-serve.pages.dev/?id=family-collage). It is also possible to mix and match different styles applied to different objects in the same scene. -->

<figure>
    <label for="mn-figure-fb84bd">⊕</label>
    
    <span>
        <em>Figure 7.</em> The same scene rendered by line drawings at two output resolutions. In each scene the inset image is one of the transformed images used to generate the scene. <em>(a)</em> <a href="https://splat-serve.pages.dev/?id=caterpillar-contour-256p">460x256 pixels</a>. <em>(b)</em> <a href="https://splat-serve.pages.dev/?id=caterpillar-contour">1940x1080 pixels</a>.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__resolution__inset-BSGYWx8Y.png" alt="Figure 7"/>
</figure>
<p>
The scenes so far have been trained on high resolution images (~1080p). This not only effects how long it takes to train the 3D Gaussian Splat, but also the fidelity of details captured in the line drawing. The lower resolution outputs capture the major lines that define the shape of the subject in the scene, while higher resolution outputs can pick up on more minor details in the scene. Figure 7 shows the same scene rendered at a variety of resolutions.
</p>
<p>
The following table describes the training time and number of splats generated for line drawings generated in a range of resolutions. Each scene is trained using the same parameters for 21,000 iterations. The final row describes the original scene using images from the Caterpillar scene in the Tanks and Temples Benchmark.
</p>
<div>
    <table>
        <thead>
            <tr>
                <th>Resolution</th>
                <th>Training Time</th>
                <th>Number of Splats</th>
                <th>Uncompressed File Size (.ply)</th>
                <th>Compressed File Size (.ksplat)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-contour-128p">232x128</a></td>
                <td>3m 38s</td>
                <td>662,428</td>
                <td>164 MB</td>
                <td>14.1 MB</td>
            </tr>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-contour-256p">460x256</a></td>
                <td>4m 31s</td>
                <td>1,105,383</td>
                <td>274 MB</td>
                <td>24.5 MB</td>
            </tr>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-contour-512p">920x512</a></td>
                <td>6m 48s</td>
                <td>1,611,563</td>
                <td>400 MB</td>
                <td>36.4 MB</td>
            </tr>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-contour">1940x1080</a></td>
                <td>15m 27s</td>
                <td>2,046,676</td>
                <td>507 MB</td>
                <td>46 MB</td>
            </tr>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-original">Original (1957x1090)</a></td>
                <td>15m 9s</td>
                <td>900,798</td>
                <td>223 MB</td>
                <td>21 MB</td>
            </tr>
        </tbody></table>
    </div>
    <p>
    It is notable that a line drawing scene is roughly double the size of it&#39;s source scene in both number of splats and file size. I hypothesize that this is because splats are better suited at modelling large areas and textures than they are at strokes. As a result, a scene of a 3D line drawing must use more individual gaussians to render long, thin lines in the scene.
</p>

<p>
The code to generate these is a mashup of scripts to orchestrate between the different libraraies referenced in this post. Contact me if you&#39;re curious about running this yourself.
</p>

<p>
If you have thoughts about this work or would like to collaborate, I would love to hear from you. You can contact me at:
<code>
tansh at amritkwatra dot com
</code>
</p>

<p>
Special thanks to Ritik Batra, Ilan Mandel &amp; Thijs Roumen for their feedback and suggestions. This page was created using the open-source <a href="https://github.com/tansh-kwa/tufte-project-template" target="_blank" rel="noopener noreferrer">Tufte Project Pages</a> template.
</p>
</section>
</article></div>
  </body>
</html>
