<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://replicate.com/blog/flux-is-fast-and-open-source">Original</a>
    <h1>FLUX is fast and it&#39;s open source</h1>
    
    <div id="readability-page-1" class="page"><div>
  <p>FLUX is now much faster on Replicate, and we’ve made our optimizations open-source so you can see exactly how they work and build upon them.</p>
<p>Here are the end-to-end speeds:</p>
<ul>
<li><a href="https://replicate.com/black-forest-labs/flux-schnell">FLUX.1 [schnell]</a> at 512x512 and 4 steps: 0.29 seconds (P90: 0.49 seconds)</li>
<li><a href="https://replicate.com/black-forest-labs/flux-schnell">FLUX.1 [schnell]</a> at 1024x1024 and 4 steps: 0.72 seconds (P90: 0.95 seconds)</li>
<li><a href="https://replicate.com/black-forest-labs/flux-dev">FLUX.1 [dev]</a> at 1024x1024 and 28 steps: 3.03 seconds (P90: 3.90 seconds)</li>
</ul>
<p>This is from the west coast of the US using the Python client.</p>
<p>Here’s a demo of FLUX.1 [schnell]. (It’s live, just start typing!)</p>


<p>Here’s <a href="https://fast-flux-demo.replicate.workers.dev/">the full app, and source code</a>, if you’d like to check it out.</p>
<h2 id="how-did-we-do-it">How did we do it?</h2>
<p>Most of the models on Replicate are contributed by our community, but we maintain the FLUX models in collaboration with <a href="https://blackforestlabs.ai/">Black Forest Labs</a>.</p>
<p>We’ve done two main things to make FLUX faster:</p>
<ul>
<li>We optimized the model. We used Alex Redden’s <a href="https://github.com/aredden/flux-fp8-api">flux-fp8-api</a> as a starting point, then optimized it with <code>torch.compile</code> and used fast CuDNN attention kernels in the nightly Torch builds.</li>
<li>We added a new <a href="https://replicate.com/changelog/2024-10-09-synchronous-api">synchronous HTTP API</a> that makes all image models much faster on Replicate.</li>
</ul>
<p>The quantization in flux-fp8-api slightly changes the output of the model, but we have found it has little impact on the quality.</p>
<p><a href="https://flux-quality-comparison.vercel.app/">
<img src="https://d31rfu1d3w8e4q.cloudfront.net/static/blog/flux-is-fast/slow.webp" title="Guess which is which!"/>
</a>
<a href="https://flux-quality-comparison.vercel.app/">
<img src="https://d31rfu1d3w8e4q.cloudfront.net/static/blog/flux-is-fast/fast.webp" title="Guess which is which!"/>
</a>
</p>

<p>We’ve created a tool that compares the output of thousands of prompts on FLUX.1 [schnell] and FLUX.1 [dev]. We’re not cherry picking. <a href="https://flux-quality-comparison.vercel.app/">Take a look for yourself.</a></p>
<p>You can disable this by setting the <code>go_fast</code> input on the model to <code>false</code>.</p>
<p>We want to be open with you about how we’re optimizing the models. It’s notoriously hard to compare output between models and providers, and it’s often unclear whether providers are doing things that impact the quality of the model.</p>
<p>We’re just going to tell you how we did it and let you disable any optimizations. That means you’re not wondering whether the output you’re getting is the best quality it can be.</p>
<p>Most importantly, the code is open-source, so you can see exactly how it works: <a href="https://github.com/replicate/cog-flux">github.com/replicate/cog-flux</a></p>
<h2 id="open-source-should-be-fast-too">Open-source should be fast too</h2>
<p>Open-source models are often slow out of the box. Model providers then optimize these models to make them fast and release them behind proprietary APIs, without contributing the improvements back to the community.</p>
<p>We want to change that. We think open-source should be fast too.</p>
<p><a href="https://github.com/replicate/cog-flux">We’re open-sourcing all the improvements we make to FLUX.</a> We’re also collaborating with the <a href="https://github.com/ai-compiler-study">AI Compiler Study Group</a> and other AI researchers to make an open-source fast version of FLUX. </p>
<p>Making the FLUX optimizations open-source is not just the right thing to do, it also means all the experts in the world can collaborate together to make it the fastest. Pull requests welcome.</p>
<h2 id="its-going-to-get-faster">It’s going to get faster</h2>
<p>New techniques are coming out all the time to make models faster, and by collaborating with the community, you can be sure that they’re going to be on Replicate as fast as possible. Stay tuned.</p>
<h2 id="do-more-with-flux">Do more with FLUX</h2>
<p>You can do more than just run FLUX on Replicate. You can:</p>
<ul>
<li><a href="https://replicate.com/blog/fine-tune-flux">Fine-tune FLUX on your own data</a> (training and running trained models is going to much faster soon too!)</li>
<li><a href="https://github.com/replicate/cog-flux">Edit the code and deploy a custom version</a>, if you’re doing something advanced</li>
<li><a href="https://replicate.com/playground/">Try out the models and compare outputs on our new playground</a></li>
</ul>
<p><a href="https://x.com/replicate">Follow us on X</a> to keep up to speed.</p>
</div></div>
  </body>
</html>
