<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://entropicthoughts.com/feynman-vs-computer">Original</a>
    <h1>Feynman vs. Computer</h1>
    
    <div id="readability-page-1" class="page"><div>
                <p>
I read <a href="https://zackyzz.github.io/feynman.html">Burghelea’s article on <i>the Feynman trick</i> for integration</a>. Well, I’m not
good enough at analysis to follow along, but I tried reading it anyway because
it’s fascinating.
</p>

<p>
For people who do not have experience with analysis, integration is counting the
total size of very many, very small piles of things. Analytical integration,
i.e. the process by which we can get an exact result, can be very difficult. It
often takes knowledge of special tricks, strong pattern recognition, and plenty
of trial and error. Fortunately, in all cases in my career when I’ve needed the
value of an integral, an approximate answer has been good enough.
</p>

<p>
In practical terms, this means we <i>could</i> spend a lot of time learning
integration tricks, practice using them, and then take half an hour out of our
day to apply them to an integral in front of us … or, hear me out, <i>or</i>, we
could write four lines of JavaScript that arrive at a relatively accurate answer
in less than a second.
</p>
<section id="outline-container-the-approximating-power-of-random-numbers">

<div id="text-orgd323138">
<p>
If integration is summing many small piles, we have to figure out how big the
piles are. Their height is usually given by a mathematical function, and our
first example will be the same as in the Feynman trick article.
</p>

<p>
\[f(x) = \frac{x - 1}{\ln{x}}\]
</p>

<p>
This is to be integrated from zero to one, i.e. we want to know the size of the
shaded area in the plot below. You can think of each column of shaded pixels as
one pile, and we sum the size of all of them to get the total area.<span><sup>1</sup> Of
course, this is an <abbr>svg</abbr> image so there are no columns of pixels. Alternatively,
the more we zoom in, the thinner the columns become – but the more of them there
are. This is why we need integration: it’s dealing with the limit case of
infinitely many, infinitely thin columns.</span>
</p>


<p><img src="https://entropicthoughts.com/image/feynman-vs-computer-01.svg" alt="feynman-vs-computer-01.svg"/>
</p>

<p>
We could imagine drawing six random numbers between zero and one, and plotting
piles of the corresponding height at those locations. Since there are six piles,
their width is one sixth of the width of the area we are integrating.
</p>


<p><img src="https://entropicthoughts.com/image/feynman-vs-computer-02.svg" alt="feynman-vs-computer-02.svg"/>
</p>

<p>
Even though some of these piles overlap by chance, and even though there are
some random gaps between them, the sum of their areas (0.66) comes very close to
the actual shaded area determined analytically (0.69). If we draw more piles, we
have to make them correspondingly thinner, but the agreement between their sum
and the total size of the area improves.
</p>


<p><img src="https://entropicthoughts.com/image/feynman-vs-computer-03.svg" alt="feynman-vs-computer-03.svg"/>
</p>

<p>
These are 100× as many piles, and they’re 1/100th as thick to compensate. Their
total area is 0.70 – very close to 0.69. If we draw even more piles, we’ll get
even closer.
</p>

<p>
This illustrates a neat correspondence between integrals and expected values. In
the simple case, we can frame it mathematically as
</p>

<p>
\[\int_a^b f(x) \mathrm{d}x = E(f(x))\]
</p>

<p>
In words, this says that integrating the function \(f\) between \(a\) and \(b\) is the
same as taking the expected value of \(f(x)\) at uniformly distributed random
points between \(a\) and \(b\).
</p>
</div>
</section>
<section id="outline-container-teaching-the-computer-to-do-it">

<div id="text-org0127589">
<p>
Here’s a JavaScript function that estimates the value of an integral in the
most primitive way possible.
</p>

<p><label>In[1]:</label></p><div>
<pre>I = (B, lo, hi, f) =&gt; {
    <span>// </span><span>Generate B random values uniformly between lo and hi.</span>
    <span>let</span> <span>xs</span> = Array.from({length: B}, _ =&gt; lo + (hi - lo) * Math.random());
    <span>// </span><span>Compute the value of f at each location.</span>
    <span>let</span> <span>ys</span> = xs.map(f);
    <span>// </span><span>Return the total area of each corresponding pile.</span>
    <span>return</span> (hi-lo)*ys.reduce((r, y) =&gt; r + y, 0)/ys.length;
}
</pre>
</div>

<p>
To compute an approximation to the value of the integral we’ve seen, we run
</p>

<p><label>In[2]:</label></p><div>
<pre>I(10_000,
  0, 1,
  x =&gt; (x-1)/Math.log(x)
);
</pre>
</div>

<p><label>Out[1]:</label></p><pre>0.6916867623261724
</pre>


<p>
This is fairly close to 0.69. And we got there in four lines of JavaScript, as
promised.
</p>
</div>
</section>
<section id="outline-container-improved-approximation-through-splittage">

<div id="text-org822d3f6">
<p>
We can try this on the next example too. Now we’re asking about the integral
</p>

<p>
\[\int_0^{\frac{\pi}{2}} \frac{\ln{(1 - \sin{x})}}{\sin{x}} \mathrm{d}x\]
</p>

<p>
which, translated to JavaScript, becomes
</p>

<p><label>In[3]:</label></p><div>
<pre>I(10_000,
  0, Math.PI,
  x =&gt; Math.log(1 - Math.sin(x))/Math.sin(x)
);
</pre>
</div>

<p><label>Out[2]:</label></p><pre>-3.67
</pre>


<p>
This is again fairly close to the desired −3.7, but not quite there yet. The
tricky shape of the function is the reason we aren’t getting as close as we want.
</p>


<p><img src="https://entropicthoughts.com/image/feynman-vs-computer-04.svg" alt="feynman-vs-computer-04.svg"/>
</p>

<p>
At the upper endpoint of the integration interval, this function goes to
negative infinity. The random piles we draw come primarily from the well behaved
region of the function, and thus don’t help the computer realise this behaviour.
</p>


<p><img src="https://entropicthoughts.com/image/feynman-vs-computer-05.svg" alt="feynman-vs-computer-05.svg"/>
</p>

<p>
There are clever ways to sample adaptively from the trickier parts of the
function, but an easy solution is to just visually find a breakpoint, split the
interval on that, and then estimate the sensible part separately from the
crazy-looking part. Since the total area must be the sum of both areas, we can
add their results together for a final estimation.
</p>

<p>
In this case, we might want to pick e.g. 1.5 as the breakpoint, so we combine
the area estimations from 0–1.5 and then 1.5–\(\frac{\pi}{2}\). The result is
</p>

<p><label>In[4]:</label></p><div>
<pre>I(2_000, 0, 1.5, x =&gt; Math.log(1 - Math.sin(x))/Math.sin(x))
+ I(8_000, 1.5, Math.PI/2, x =&gt; Math.log(1 - Math.sin(x))/Math.sin(x));
</pre>
</div>

<p><label>Out[3]:</label></p><pre>-3.70
</pre>


<p>
which is indeed much closer to the actual value of −3.7.
</p>

<p>
Note that we aren’t taking more samples, we’re just sprinkling them more wisely
over the number line. We spend 2,000 samples in the relatively well-behaved
region where the function takes values from −1 to −6, and then we spend the
other 8,000 samples in the small region that goes from −6 to negative infinity.
Here it is graphically:
</p>


<p><img src="https://entropicthoughts.com/image/feynman-vs-computer-06.svg" alt="feynman-vs-computer-06.svg"/>
</p>

<p>
The reason this helps us is that this latter region contributes a lot to the
value of the integral, but it is so small on the number line that we benefit
from oversampling it compared to the other region. This is a form of <a href="https://entropicthoughts.com/sample-unit-engineering.html">sample unit
engineering</a>, which we have seen before in different contexts.
</p>
</div>
</section>
<section id="outline-container-more-evidence-of-sufficiency">

<div id="text-orgae3026f">
<p>
We can continue with some more examples from the Feynman trick article. That
gets us the following table.
</p>

<div id="orga57efd3">
<table>


<colgroup>
<col/>

<col/>

<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Integral</th>
<th scope="col">Value</th>
<th scope="col">Estimation</th>
<th scope="col">Difference</th>
<th scope="col">Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>\(\int_0^1 \frac{x-1}{\ln{x}} \mathrm{d}x\)</td>
<td>\(\ln{2}\)</td>
<td>0.6943</td>
<td>0.2 %</td>
<td> </td>
</tr>

<tr>
<td>\(\int_0^{\frac{\pi}{2}} \frac{\ln{(1 - \sin{x})}}{\sin{x}} \mathrm{d}x\)</td>
<td>\(\frac{-3 \pi^2}{8}\)</td>
<td>-3.702</td>
<td>&lt; 0.1 %</td>
<td> </td>
</tr>

<tr>
<td>\(\int_0^1 \frac{\ln{(1 - x + x^2)}}{x - x^2} \mathrm{d}x\)</td>
<td>\(\frac{-\pi^2}{9}\)</td>
<td>-1.097</td>
<td>&lt; 0.1 %</td>
<td> </td>
</tr>

<tr>
<td>\(\int_0^{\frac{\pi}{2}} \frac{\arctan{(\sin{x})}}{\sin{x}} \mathrm{d}x\)</td>
<td>\(\frac{\pi}{2}\log{(1 + \sqrt{2})}\)</td>
<td>1.385</td>
<td>&lt; 0.1 %</td>
<td> </td>
</tr>

<tr>
<td>\(\int_0^\infty x^2 e^{-\left(4x^2 + \frac{9}{x^2}\right)} \mathrm{d}x\)</td>
<td>\(\frac{13 \sqrt{\pi}}{32 e^{12}}\)</td>
<td>0.000004414</td>
<td>0.2 %</td>
<td>(1)</td>
</tr>

<tr>
<td>\(\int_0^1 \frac{\ln{x}}{1 - x^2} \mathrm{d}x\)</td>
<td>\(\frac{-\pi^2}{8}\)</td>
<td>-1.227</td>
<td>0.5 %</td>
<td>(2)</td>
</tr>

<tr>
<td>\(\int_0^\infty \frac{e^{-x^2}}{1 + x^2} \mathrm{d}x\)</td>
<td>\(\frac{\pi e}{2}\mathrm{erfc}(1)\)</td>
<td>0.6696</td>
<td>0.3 %</td>
<td>(3)</td>
</tr>
</tbody>
</table>

</div>

<p>
Notes:
</p>

<ol>
<li>The integration is from zero to infinity, but the function practically only
has a value between zero and three, so that’s the region we estimate over.</li>
<li>This is another case where the function goes to infinity near zero, so we
split up the estimation into one for the range 0–0.1, and the other for
0.1–1.0. We have not increased the sample count, only reallocated the 10,000
samples.</li>
<li>Again, the integration is from zero to infinity, but the function practically
only has a value between zero and three, so that’s the region we estimate
over.</li>
</ol>
</div>
</section>
<section id="outline-container-finding-the-error-without-a-ground-truth">

<div id="text-orga100a87">
<p>
“Now,” the clever reader says, “this is all well and good when we have the
actual value to compare to so we know the size of the error. What will we do
if we’re evluating a brand new integral? What is the size of the error then, huh?”
</p>

<p>
This is why we sampled the function randomly. That means our approximation is a
statistical average over samples, and for that we can compute the standard error
of the mean. In the JavaScript implementation below, we use <a href="https://entropicthoughts.com/quick-variance-computation.html">the quick variance
computation</a>, but we could perhaps more intuitively have used <a href="https://entropicthoughts.com/estimating-standard-deviation-from-timeseries-plot.html">the <abbr>spc</abbr> inspired
method</a>.
</p>

<p><label>In[5]:</label></p><div>
<pre>Ic = (B, lo, hi, f) =&gt; {
    <span>let</span> <span>xs</span> = Array.from(
      {length: B}, _ =&gt;
      lo + (hi - lo) * Math.random()
    );
    <span>let</span> <span>ys</span> = xs.map(f);
    <span>// </span><span>Compute the variance of the ys from the sum and</span>
    <span>// </span><span>the sum of squared ys.</span>
    <span>let</span> <span>s</span> = ys.reduce((r, y) =&gt; r + y, 0);
    <span>let</span> <span>ssq</span> = ys.reduce((r, y) =&gt; r + y**2, 0);
    <span>let</span> <span>v</span> = (ssq - s**2/B)/(B-1);
    <span>// </span><span>Compute the mean and the standard error of the mean.</span>
    <span>let</span> <span>m</span> = (hi-lo)*s/B;
    <span>let</span> <span>se</span> = (hi-lo)*Math.sqrt(v/B);
    <span>// </span><span>Compute the 90 % confidence interval of the value of</span>
    <span>// </span><span>the integral.</span>
    <span>return</span> {
        p05: m - 1.645*se,
        p95: m + 1.645*se,
    }
}
</pre>
</div>

<p>
If we run this with the first integral as an example, we’ll learn that
</p>

<p><label>In[6]:</label></p><div>
<pre>Ic(10_000,
  0, 1,
  x =&gt; (x-1)/Math.log(x)
)
</pre>
</div>

<p><label>Out[4]:</label></p><pre>Object {
  p05: 0.6896
  p95: 0.6963
}
</pre>


<p>
Not only is this range an illustration of the approximation error (small!), it is also
very likely to capture the actual value of the integral. Here are some more
examples from the same integrals as above:
</p>

<table>


<colgroup>
<col/>

<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">5 %</th>
<th scope="col">95 %</th>
<th scope="col">Actual</th>
<th scope="col">Contained?</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.6904</td>
<td>0.6972</td>
<td>0.6931</td>
<td>✅</td>
</tr>

<tr>
<td>-3.7673</td>
<td>-3.6787</td>
<td>-3.7011</td>
<td>✅</td>
</tr>

<tr>
<td>-1.0975</td>
<td>-1.0960</td>
<td>-1.0966</td>
<td>✅</td>
</tr>

<tr>
<td>1.3832</td>
<td>1.3871</td>
<td>1.3845</td>
<td>✅</td>
</tr>

<tr>
<td>0.4372</td>
<td>0.4651</td>
<td>0.4424</td>
<td>✅</td>
</tr>

<tr>
<td>-1.2545</td>
<td>-1.2254</td>
<td>-1.2337</td>
<td>✅</td>
</tr>

<tr>
<td>0.6619</td>
<td>0.6937</td>
<td>0.6716</td>
<td>✅</td>
</tr>
</tbody>
</table>

<p>
These are all built naïvely from 10,000 uniform samples. In other words, in none
of the cases have the computation been split up to allocate samples more
cleverly.
</p>

<p>
Again, we could spend a lot of time learning to integrate by hand … or we ask
the computer for less than a second of its time first, and see if the accuracy
it can do it with is appropriate for our use case. In my experience, it
generally is.
</p>
</div>
</section>
<section id="outline-container-seeing-the-effect-of-sample-unit-engineering">

<div id="text-orgad1e395">
<p>
What’s neat is we can still split up the computation like we did before, if we
believe it will make the error smaller and the confidence interval narrower.
Let’s use the following integral as an example.
</p>

<p>
\[\int_0^\infty \frac{\sin{x}}{x} \mathrm{d}x\]
</p>

<p>
This oscillates up and down quite a bit for small \(x\), and then decays but still
provides significant contributions for larger \(x\). A naive evaluation would have
a confidence interval of
</p>

<p><label>In[7]:</label></p><div>
<pre>Ic(10_000, 0, 100, x =&gt; Math.sin(x)/x)
</pre>
</div>

<p><label>Out[5]:</label></p><pre>Object {
  p05: 1.461
  p95: 1.884
}
</pre>


<p>
and while this is certainly correct<span><sup>2</sup> The actual value of the integral is half
\(\pi\) or approximatey 1.571.</span>, we can do better. We’ll estimate the region of
0–6 separately from 6–100, using half the samples for each<span><sup>3</sup> Why put the break
point at 6? The period of sin is a full turn, which is roughly 6 radians. This
ensures we get roughly symmetric contributions from both integrals. That’s not
necessary for the technique to work, but it makes the illustration a little
cleaner.</span>:
</p>

<p><label>In[8]:</label></p><div>
<pre>Ic(5_000, 0, 6, x =&gt; Math.sin(x)/x)
</pre>
</div>

<p><label>Out[6]:</label></p><pre>Object {
  p05: 1.236
  p95: 1.468
}
</pre>


<p>
This contains the bulk of the value of the integral, it seems. Let’s see what
remains in the rest of it.
</p>

<p><label>In[9]:</label></p><div>
<pre>Ic(5_000, 6, 100, x =&gt; Math.sin(x)/x)
</pre>
</div>

<p><label>Out[7]:</label></p><pre>Object {
  p05: 0.080
  p95: 0.198
}
</pre>


<p>
We can work backwards to what the standard errors must have been to produce
these confidence intervals.<span><sup>4</sup> The midpoint is the point estimation for each
region, and the standard error is 1/1.645 times the distance between the 5 %
point and the midpoint.</span>
</p>

<table>


<colgroup>
<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Region</th>
<th scope="col">Value</th>
<th scope="col">Standard error</th>
</tr>
</thead>
<tbody>
<tr>
<td>0–6</td>
<td>1.4067</td>
<td>0.0372</td>
</tr>

<tr>
<td>6–100</td>
<td>0.1390</td>
<td>0.0359</td>
</tr>
</tbody>
</table>

<p>
The estimation of the total area would be the values summed, i.e. 1.5457. The
estimation of the standard error of this we get <a href="https://entropicthoughts.com/pythagorean-addition.html">through Pythagorean addition</a> and
it is approximately 0.05143. We convert it back to a confidence interval and
compare with when we did not break it up into multiple components.
</p>

<table>


<colgroup>
<col/>

<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Method</th>
<th scope="col">5 %</th>
<th scope="col">95 %</th>
<th scope="col">Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>Single operation (10,000 samples)</td>
<td>1.461</td>
<td>1.884</td>
<td>0.423</td>
</tr>

<tr>
<td>Two operations (5,000 samples × 2)</td>
<td>1.461</td>
<td>1.630</td>
<td>0.169</td>
</tr>
</tbody>
</table>

<p>
Although in this case the two methods happen to share a lower bound, the upper
bound has been dramatically reduced. The total range of the confidence interval
is more than halved! This was because we allocated the samples more cleverly –
concentrated them in the early parts of the function – rather than increased the
number of samples.
</p>

<p>
That said, we’re at a computer, so we could try increasing the sample count. Or
maybe both?
</p>

<table>


<colgroup>
<col/>

<col/>

<col/>

<col/>
</colgroup>
<thead>
<tr>
<th scope="col">Method</th>
<th scope="col">5 %</th>
<th scope="col">95 %</th>
<th scope="col">Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>Single operation (10,000 samples)</td>
<td>1.461</td>
<td>1.884</td>
<td>0.423</td>
</tr>

<tr>
<td>Two operations (5,000 samples × 2)</td>
<td>1.461</td>
<td>1.630</td>
<td>0.169</td>
</tr>

<tr>
<td>Single operation (100,000 samples)</td>
<td>1.549</td>
<td>1.680</td>
<td>0.131</td>
</tr>

<tr>
<td>Two operations (50,000 samples × 2)</td>
<td>1.524</td>
<td>1.578</td>
<td>0.054</td>
</tr>
</tbody>
</table>

<p>
It seems like sampling more cleverly has almost the same effect as taking ten
times as many samples.
</p>

<p>
We could play around with where to put the breakpoint, and how many samples to
allocate to each side of it, and see which combination yields the lowest error.
Then we can run that combination with a lot of samples to get the most accurate
final result. That would take maybe 15 minutes of tooting about and exploring
sensible-seeming alternatives, so it’s probably still quicker than integrating
by hand.
</p>
</div>
</section>
<section id="outline-container-when-the-computer-is-not-enough">

<div id="text-org8237302">
<p>
It should be said that there are times when numeric solutions aren’t great. I
hear that in electronics and quantum dynamics, there are sometimes integrals
whose value is not a number, but a function, and knowing that function is
important in order to know how the thing it’s modeling behaves in interactions
with other things.
</p>

<p>
Those are not my domains, though. And when that’s not the case, the computer
beats Feynman any day of the week.
</p>
</div>
</section>

            </div></div>
  </body>
</html>
