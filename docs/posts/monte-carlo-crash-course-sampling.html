<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thenumb.at/Sampling/">Original</a>
    <h1>Monte Carlo Crash Course: Sampling</h1>
    
    <div id="readability-page-1" class="page"><div><main><ul><li><a href="https://thenumb.at/Probability">Continuous Probability</a></li><li><a href="https://thenumb.at/Monte-Carlo">Exponentially Better Integration</a></li><li><strong><a href="https://thenumb.at/Sampling">Sampling</a></strong></li><li><em>Coming Soon…</em></li></ul><hr/><p>In the previous chapter, we assumed that we can uniformly randomly sample our domain.
However, it’s not obvious how to actually do so—in fact, how can a deterministic computer even generate random numbers?<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><ul><li><a href="#pseudo-random-numbers">Pseudo-Random Numbers</a></li><li><a href="#uniform-rejection-sampling">Uniform Rejection Sampling</a></li><li><a href="#non-uniform-rejection-sampling">Non-Uniform Rejection Sampling</a></li><li><a href="#inversion-sampling">Inversion Sampling</a></li><li><a href="#changes-of-coordinates">Changes of Coordinates</a></li></ul><h2 id="pseudo-random-numbers">Pseudo-Random Numbers</h2><p>Fortunately, Monte Carlo methods don’t need truly random numbers.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>
Instead, we can use a <em>pseudo-random number generator</em> (PRNG).
A PRNG produces a deterministic stream of numbers that look uniformly random:</p><p>By “look uniformly random,” we mean the sequence exhibits certain statistical properties:</p><ul><li>Uniformity: samples are evenly distributed.</li><li>Independence: previous samples cannot be used to predict future samples.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></li><li>Aperiodicity: the sequence of samples does not repeat.</li></ul><p>Deterministic generators cannot fully achieve these properties, but can get pretty close, in a precise sense.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>
Here, we will use the <a href="https://www.pcg-random.org/">PCG</a> family of generators, which are performant, small, and statistically robust.</p><p>PRNGs give us uniformly random scalars, but we ultimately want to sample complex, high-dimensional domains.
Fortunately, we can build up samplers for interesting distributions using a few simple algorithms.</p><h2 id="uniform-rejection-sampling">Uniform Rejection Sampling</h2><p><em>Rejection sampling</em> transforms a sampler for a simple domain $$D$$ into a sampler for a complex domain $$\Omega$$, where $$\Omega \subseteq D$$.
All we need is a function $$\text{accept}$$ that indicates whether a point $$\mathbf{x} \in D$$ is also contained in $$\Omega$$.</p><p>Let’s build a rejection sampler for the two-dimensional unit disk.
First, we’ll choose $$D = [-1,1]\times[-1,1]$$, which clearly encloses $$\Omega$$.
We may use a PRNG to produce a sequence of uniform samples of $$[-1,1]$$, denoted as $$\xi_i$$.
Taking each pair $$D_i = (\xi_{2i},\xi_{2i+1})$$ then provides samples of $$D$$.</p><img src="https://thenumb.at/Sampling/rej.svg"/><p>Second, we’ll define $$\text{accept}(\mathbf{x})$$—for the unit disk, we may check that $$||\mathbf{x}|| \le 1$$.
Now, the rejection sampler:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>Ω</span><span>():</span>
</span></span><span><span>    <span>x</span> <span>=</span> <span>D</span><span>()</span>
</span></span><span><span>    <span>if</span> <span>accept</span><span>(</span><span>x</span><span>):</span>
</span></span><span><span>        <span>return</span> <span>x</span>
</span></span><span><span>    <span>return</span> <span>Ω</span><span>()</span>
</span></span></code></pre></div><p>In other words, sample $$D$$, and if the result is not in $$\Omega$$, just try again!</p><p>Intuitively, rejection sampling filters out samples that aren’t in $$\Omega$$.
Hence, if we start with uniform samples of $$D$$, we should be left with uniform samples of $$\Omega$$.</p><p>To formalize our reasoning, let’s derive our sampler’s PDF, denoted $$f_\text{rej}$$.
To produce a sample $$\mathbf{x}$$, we must first sample it from $$f_D$$, then accept it.
Therefore $$f_\text{rej}(\mathbf{x})$$ is equivalent to $$f_D(\mathbf{x})$$ condition on $$\mathbf{x}$$ being accepted.</p><p>\[
\begin{align*}
f_\text{rej}(\mathbf{x}) &amp;= f_{D\ |\ \text{accept}}(\mathbf{x}) \\
&amp;= \frac{\mathbb{P}\left\{\text{accept}\ |\ \mathbf{x}\right\}f_{D}(\mathbf{x})}{\mathbb{P\left\{\text{accept}\right\}}} \tag{Bayes&#39; rule}\\
&amp;= \frac{1\cdot \frac{1}{\text{Vol}(D)}}{\frac{\text{Vol}(\Omega)}{\text{Vol}(D)}} = \frac{1}{\text{Vol}(\Omega)} \tag{$f_D$ is uniform}
\end{align*}
\]</p><p>\[\begin{align*}
f_\text{rej}(\mathbf{x}) &amp;= f_{D\ |\ \text{accept}}(\mathbf{x}) \\
&amp;= \frac{\mathbb{P}\left\{\text{accept}\ |\ \mathbf{x}\right\}f_{D}(\mathbf{x})}{\mathbb{P\left\{\text{accept}\right\}}} \\&amp;\tag{Bayes&#39; rule}\\
&amp;= \frac{1\cdot \frac{1}{\text{Vol}(D)}}{\frac{\text{Vol}(\Omega)}{\text{Vol}(D)}} = \frac{1}{\text{Vol}(\Omega)} \\&amp;\tag{$f_D$ is uniform}
\end{align*}\]</p><p>$$\text{Vol}$$ indicates the volume (in two dimensions, area) of a domain.
Therefore $$f_\text{rej}$$ is indeed uniform on $$\Omega$$.<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p><h2 id="non-uniform-rejection-sampling">Non-Uniform Rejection Sampling</h2><p>Like we saw with <a href="https://thenumb.at/Monte-Carlo/#non-uniform-sampling">Monte Carlo integration</a>, rejection sampling can be straightforwardly extended to work with non-uniform distributions.</p><p>Let’s say the PDF of our distribution on $$D$$ is $$f_D(\mathbf{x})$$, and we want to use it to sample from $$\Omega$$ with PDF $$f_\Omega(\mathbf{x})$$.
We already know that $$\Omega \subseteq D$$, but we’ll also need to check a slightly stricter condition—that the ratio between our PDFs has a finite upper bound, denoted $$c$$.<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup></p><p>\[
\begin{align*}
c = \sup_{\mathbf{x}\in\Omega}\frac{f_\Omega(\mathbf{x})}{f_D(\mathbf{x})}
\end{align*}
\]</p><p>Above, we required $$\Omega \subseteq D$$ because it would otherwise be impossible to sample all parts of $$\Omega$$.
Here, we need a finite $$c$$ for essentially the same reason—we’re checking that there is no part of $$\Omega$$ that we sample infinitely infrequently.</p><p>Once we have $$c$$, we just need to update $$\text{accept}$$.
Now, we will accept a sample $$\mathbf{x}$$ with probability $$\frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})}$$, which is always at most $$1$$.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>accept</span><span>(</span><span>x</span><span>):</span>
</span></span><span><span>    <span>return</span> <span>random</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>)</span> <span>&lt;</span> <span>f_Ω</span><span>(</span><span>x</span><span>)</span> <span>/</span> <span>(</span><span>c</span> <span>*</span> <span>f_D</span><span>(</span><span>x</span><span>))</span>
</span></span></code></pre></div><p>Intuitively, we’re transforming the probability density at $$\mathbf{x}$$ from $$f_D(\mathbf{x})$$ to $$f_\Omega(\mathbf{x})$$ by accepting $$\mathbf{x}$$ with probability proportional to $$\frac{f_\Omega(\mathbf{x})}{f_D(\mathbf{x})}$$.
Note that if $$f_D$$ is uniform, we directly accept $$\mathbf{x}$$ with probability proportional to $$f_\Omega(\mathbf{x})$$.</p><p>For example, given uniform $$f_D$$ and $$f_\Omega(\mathbf{x}) \propto \frac{1}{1+||\mathbf{x}||^2}$$:</p><p>As you’d expect, we see a greater proportion of accepted samples towards the center, where $$f_\Omega(\mathbf{x})$$ is largest.</p><p>Finally, let’s check that our sampler’s PDF is actually $$f_\Omega(\mathbf{x})$$.
Like above, the PDF is equivalent to $$f_D(\mathbf{x})$$ condition on $$\mathbf{x}$$ being accepted.</p><p>\[
\begin{align*}
f_\text{rej}(\mathbf{x}) &amp;= f_{D\ |\ \text{accept}}(\mathbf{x}) \\
&amp;= \frac{\mathbb{P}\left\{\text{accept}\ |\ \mathbf{x}\right\}f_{D}(\mathbf{x})}{\mathbb{P}\left\{\text{accept}\right\}} \tag{Bayes&#39; rule}\\
&amp;= \frac{\frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})} f_{D}(\mathbf{x})}{\int_D \frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})} f_D(\mathbf{x}) \, d\mathbf{x}} \tag{$\mathbb{P}\left\{\text{accept}\right\} = \mathbb{E}\left[\frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})}\right]$}\\
&amp;= \frac{f_\Omega(\mathbf{x})}{\int_D f_\Omega(\mathbf{x}) \, d\mathbf{x}} \tag{Algebra}\\
&amp;= f_\Omega(\mathbf{x}) \tag{$\int_{D\supseteq\Omega} f_\Omega = 1$}
\end{align*}
\]</p><p>\[\begin{align*}
f_\text{rej}(\mathbf{x}) &amp;= f_{D\ |\ \text{accept}}(\mathbf{x}) \\
&amp;= \frac{\mathbb{P}\left\{\text{accept}\ |\ \mathbf{x}\right\}f_{D}(\mathbf{x})}{\mathbb{P}\left\{\text{accept}\right\}} \\&amp;\tag{Bayes&#39; rule}\\
&amp;= \frac{\frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})} f_{D}(\mathbf{x})}{\int_D \frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})} f_D(\mathbf{x}) \, d\mathbf{x}} \\&amp;\tag{$\mathbb{P}\left\{\text{accept}\right\} = \mathbb{E}\left[\frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})}\right]$}\\
&amp;= \frac{f_\Omega(\mathbf{x})}{\int_D f_\Omega(\mathbf{x}) \, d\mathbf{x}} \\&amp;\tag{Algebra}\\
&amp;= f_\Omega(\mathbf{x}) \\&amp;\tag{$\int_{D\supseteq\Omega} f_\Omega = 1$}
\end{align*}\]</p><p>In the second step, we obtain the probability of accepting an arbitrary sample by computing the expected probability of accepting $$\mathbf{x}$$ over all $$\mathbf{x} \in D$$. In the fourth, note that we define $$f_\Omega = 0$$ outside of $$\Omega$$.</p><h3 id="sample-efficiency">Sample Efficiency</h3><p>Many practical problems can be solved using only rejection sampling and uniform Monte Carlo integration.
Choosing $$D$$ to be a box enclosing $$\Omega$$ works in any number of dimensions—boxes are always easy to sample, as every dimension is independent.<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup></p><p>However, rejection sampling is only efficient when $$f_\Omega$$ can make use of a significant proportion of the probability density in $$f_D$$.
Each sample of $$f_\Omega$$ requires a geometric number of samples of $$f_D$$, distributed according to $$\mathbb{P}\{\text{accept}\}$$:</p><p>\[\begin{align*}
\mathbb{P}\left\{\text{accept}\right\} &amp;= \mathbb{E}\left[\frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})}\right] \\
&amp;= \int_D \frac{f_\Omega(\mathbf{x})}{cf_D(\mathbf{x})} f_D(\mathbf{x}) \, d\mathbf{x} \\
&amp;= \frac{1}{c}\int_D f_\Omega(\mathbf{x})\, d\mathbf{x} = \frac{1}{c}
\end{align*}\]</p><p>Since we have $$\frac{1}{c}$$ chance of accepting each sample, we should expect to generate $$c$$ samples of $$f_D$$ for each sample of $$f_\Omega$$.
Intuitively, when $$c$$ is large, it means $$f_D$$ rarely samples regions that $$f_\Omega$$ samples frequently.</p><p>For example, you may not want to use rejection sampling when $$\Omega$$ doesn’t cover much of $$D$$:</p><img src="https://thenumb.at/Sampling/badrej.svg"/><p>So, we’ll need to devise a more efficient sampling algorithm.</p><h2 id="inversion-sampling">Inversion Sampling</h2><p>Inversion sampling is a method for sampling any one-dimensional distribution with an invertible cumulative distribution function (<a href="https://thenumb.at/Probability/#cumulative-distributions">CDF</a>).
The CDF of a random variable $$X$$, denoted as $$F_X(x)$$, measures the probability that a sample is less than $$x$$.</p><p>\[\begin{align*}
F_X(x) &amp;= \mathbb{P}\{X &lt; x\}\\
&amp;= \int_{-\infty}^x f_X(x^\prime)\, dx^\prime
\end{align*}\]</p><p>Intuitively, the CDF maps $$x$$ to the percentage of probability mass lying below $$x$$:</p><p>Hence, the <em>inverse</em> CDF $$F^{-1}_X(p)$$ maps a percentage of probability mass to the corresponding $$x$$.<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup></p><p>We may define an inversion sampler $$\text{Inv}$$ by uniformly sampling $$p$$ from $$[0,1]$$ and computing $$F_X^{-1}(p)$$.
To characterize our sampler’s behavior, we can find its PDF.
The probability that $$\text{Inv} = F_X^{-1}(p)$$ falls within a range of outcomes $$dx$$ is equivalent to the probability that $$p$$ falls within the corresponding range $$dp$$:</p><p>We sampled $$p$$ uniformly, so the probability that $$p$$ falls in $$dp$$ is the length of $$dp$$.
The average probability density on $$dx$$ is then $$\frac{dp}{dx}$$.
In the limit, the length of $$dp$$ is proportional to slope of $$F_X$$—this ratio is its derivative!</p><p>\[\begin{align*}
f_\text{Inv}(x) &amp;= \frac{dp}{dx}\\
&amp;= \frac{dF_X(x)}{dx} \tag{$p = F_X(x)$}\\
&amp;= f_X(x)
\end{align*}\]</p><p>\[\begin{align*}
f_\text{Inv}(x) &amp;= \frac{dp}{dx}\\
&amp;= \frac{dF_X(x)}{dx}\\&amp; \tag{$p = F_X(x)$}\\
&amp;= f_X(x)
\end{align*}\]</p><p>Therefore, we have $$f_\text{Inv} = f_X$$.<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>
That implies inversion sampling works, but more rigorously, we can check that the CDF of our sampler matches $$F_X$$.</p><p>\[
\begin{align*}
F_\text{Inv}(x) &amp;= \mathbb{P}\{\text{Inv} &lt; x\}\\
&amp;= \mathbb{P}\{F_X^{-1}(\text{Uniform}(0,1)) &lt; x\}\\
&amp;= \mathbb{P}\{F_X(F_X^{-1}(\text{Uniform}(0,1))) &lt; F_X(x)\} \tag{$F_X$ non-decreasing}\\
&amp;= \mathbb{P}\{\text{Uniform}(0,1) &lt; F_X(x)\} \tag{Def. $F_X^{-1}$}\\
&amp;= F_X(x) \tag{$F_X(x) \in [0,1]$}
\end{align*}
\]</p><p>\[\begin{align*}
F_\text{Inv}(x) &amp;= \mathbb{P}\{\text{Inv} &lt; x\}\\
&amp;= \mathbb{P}\{F_X^{-1}(\text{Uniform}(0,1)) &lt; x\}\\
&amp;= \mathbb{P}\{F_X(F_X^{-1}(\text{Uniform}(0,1))) \\&amp; \phantom{=\mathbb{P}\{} &lt; F_X(x)\} \\&amp;\tag{$F_X$ non-decreasing}\\
&amp;= \mathbb{P}\{\text{Uniform}(0,1) &lt; F_X(x)\} \\&amp;\tag{Def. $F_X^{-1}$}\\
&amp;= F_X(x) \\&amp;\tag{$F_X(x) \in [0,1]$}
\end{align*}\]</p><p>Since their CDFs are equivalent, we indeed have $$\text{Inv} \sim X$$.</p><h3 id="marginal-inversion-sampling">Marginal Inversion Sampling</h3><p>As stated, inversion sampling only applies to one-dimensional distributions.
Fortunately, we can extend inversion to higher dimensions by iteratively sampling each dimension’s <a href="https://thenumb.at/Probability/#joint-distributions"><em>marginal distribution</em></a>.</p><p>Let’s derive an inversion sampler for the two-dimensional distribution $$f_{XY}(x,y)$$.
First, we’ll define the marginal distribution $$f_X(x)$$, which computes the total probability density at $$x$$ across all choices for $$y$$.</p><p>This distribution is one-dimensional, so we can use inversion sampling to choose a sample $$X$$.
Second, we’ll compute the marginal distribution $$f_Y(y)$$ condition on $$X$$, which must be proportional to $$f_{XY}(X,y)$$.</p><div><div><p>\[\begin{align*}
f_{Y\ |\ X}(y) &amp;= \frac{f(X,y)}{\int_{-\infty}^\infty f(X,y)\, dy}\\
&amp;= \frac{f(X,y)}{f_X(X)}
\end{align*}\]</p></div></div><p>Finally, we can apply inversion again to sample $$Y$$.
Intuitively, $$f_{Y\ |\ X}$$ selects the correct distribution for $$y$$ given our choice of $$X$$.
We will more rigorously explore why inversion sampling works in the next section.</p><h2 id="changes-of-coordinates">Changes of Coordinates</h2><p>While marginal inversion sampling can build up arbitrarily high-dimensional distributions, it’s often not necessary in practice.
That’s because inversion sampling is a special case of a more general technique for transforming random variables.</p><p>To illustrate, let’s attempt to define a uniform sampler for the unit disk.
Unlike rejection sampling, we first need to choose a <em>parameterization</em> of our domain.
A natural choice is polar coordinates, where $$\theta$$ is angle with respect to the the x-axis and $$r$$ is distance from the origin.</p><div><p><img src="https://thenumb.at/Sampling/polar.svg"/></p><p>\[\begin{align*}
\Phi(r,\theta) &amp;= (r\cos\theta, r\sin\theta)\\
\Phi^{-1}(x,y) &amp;= (\sqrt{x^2+y^2}, \text{atan2}(y,x))
\end{align*}\]</p></div><p>The unit disk is hence described by $$\mathcal{S} = \Phi(\mathcal{R})$$, where $$\mathcal{R} = [0,1]\times[0,2\pi]$$.
To produce a sampler for $$\mathcal{S}$$, we could try mapping uniform samples of $$\mathcal{R}$$ onto $$\mathcal{S}$$:</p><svg id="sample_polar_bad_slider" style="margin-top:15px"></svg><p>But uniform samples of $$\mathcal{R}$$ don’t become uniform samples of $$\mathcal{S}$$.
That’s because transforming from polar to rectangular coordinates didn’t preserve area—smaller radii contain less area, yet we weighted all $$r$$ equally.</p><p>To determine what went wrong, let’s find the PDF of this sampler.
The key observation is that a sample $$\mathbf{s}$$ falls within a circular patch $$d\mathcal{S}$$ if and only if $$\mathbf{r} = \Phi^{-1}(\mathbf{s})$$ falls within the corresponding rectangle $$d\mathcal{R}$$.<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup></p><div><p><img src="https://thenumb.at/Sampling/polarT.svg"/></p><p><img src="https://thenumb.at/Sampling/polarS.svg"/></p></div><p>Hence, the probabilities of sampling either region must be equivalent.</p><p>\[
\int_{d\mathcal{S}} f_\mathcal{S}(\mathbf{s})\, d\mathbf{s} = \int_{d\mathcal{R}} f_\mathcal{R}(\mathbf{r})\, d\mathbf{r}
\]</p><p>In the limit, these integrals reduce to the respective PDF times the area of the patch.</p><p>\[\begin{align*}
&amp;&amp; f_\mathcal{S}(\mathbf{s})\cdot|d\mathcal{S}| &amp;= f_\mathcal{R}(\mathbf{r})\cdot|d\mathcal{R}|\\
&amp;\implies&amp; f_\mathcal{S}(\mathbf{s}) &amp;= f_\mathcal{R}(\mathbf{r})\cdot\frac{|d\mathcal{R}|}{|d\mathcal{S}|}
\end{align*}\]</p><p>Intuitively, the ratio of areas $$\frac{|d\mathcal{R}|}{|d\mathcal{S}|}$$ tells us how much $$d\mathcal{S}$$ is squashed or stretched when mapped onto $$d\mathcal{R}$$.
For example, if $$d\mathcal{S}$$ is scaled down by a factor of two, $$d\mathcal{R}$$ must contain twice the probability density.</p><p>Finally, since $$\Phi^{-1}$$ maps $$\mathcal{S}$$ to $$\mathcal{R}$$, the area scaling factor is given by its derivative:<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup></p><p>\[\begin{align*}
f_\mathcal{S}(\mathbf{s}) &amp;= f_\mathcal{R}(\mathbf{r})\cdot\frac{|d\mathcal{R}|}{|d\mathcal{S}|} \\
&amp;= f_\mathcal{R}(\Phi^{-1}(\mathbf{s}))\cdot\left|\frac{d\Phi^{-1}(\mathcal{S})}{d\mathcal{S}}\right| \tag{$\mathcal{R} = \Phi^{-1}(\mathcal{S})$}\\
&amp;=f_\mathcal{R}(\Phi^{-1}(\mathbf{s}))\cdot |D\Phi^{-1}|
\end{align*}\]</p><p>\[\begin{align*}
f_\mathcal{S}(\mathbf{s}) &amp;= f_\mathcal{R}(\mathbf{r})\cdot\frac{|d\mathcal{R}|}{|d\mathcal{S}|} \\
&amp;= f_\mathcal{R}(\Phi^{-1}(\mathbf{s}))\cdot\left|\frac{d\Phi^{-1}(\mathcal{S})}{d\mathcal{S}}\right|\\&amp; \tag{$\mathcal{R} = \Phi^{-1}(\mathcal{S})$}\\
&amp;=f_\mathcal{R}(\Phi^{-1}(\mathbf{s}))\cdot |D\Phi^{-1}|
\end{align*}\]</p><p>Where $$|D\Phi^{-1}|$$ denotes the <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant#Jacobian_determinant">determinant of the Jacobian</a> of $$\Phi^{-1}$$.<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup></p><h3 id="sampling-via-change-of-coordinates">Sampling via Change of Coordinates</h3><p>Now that we know the relationship between $$f_\mathcal{S}$$ and $$f_\mathcal{R}$$, we can choose a different, non-uniform $$f_\mathcal{R}$$ that will produce a uniform $$f_\mathcal{S}$$.
Our new PDF will need to cancel out the factor of $$|D\Phi^{-1}|$$:</p><p>\[\begin{align*}
|D\Phi^{-1}| &amp;= \left|\begin{bmatrix}\frac{\delta r}{\delta x} &amp; \frac{\delta \theta}{\delta x}\\
\frac{\delta r}{\delta y} &amp; \frac{\delta \theta}{\delta y}\end{bmatrix}\right|\\
&amp;= \frac{\delta r}{\delta x}\cdot\frac{\delta \theta}{\delta y} - \frac{\delta \theta}{\delta x}\cdot\frac{\delta r}{\delta y} \\
&amp;= \frac{x}{\sqrt{x^2+y^2}} \cdot \frac{x}{x^2+y^2} + \frac{y}{x^2+y^2} \cdot \frac{y}{\sqrt{x^2+y^2}}\\
&amp;= \frac{1}{\sqrt{x^2+y^2}} = \frac{1}{r}
\end{align*}\]</p><p>Proportionality with $$\frac{1}{r}$$ makes sense—our misbehaving sampler produced too many samples near the origin.
If we instead sample $$\mathcal{R}$$ according to $$f_\mathcal{R}(r,\theta) = \frac{r}{2\pi}$$, we’ll end up with a uniform $$f_\mathcal{S}$$.<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup></p><p>\[\begin{align*}
f_\mathcal{S}(x,y) &amp;= f_\mathcal{R}(\Phi^{-1}(x,y))\cdot|D\Phi^{-1}| \\
&amp;= \frac{r}{2\pi}\cdot \frac{1}{r} = \frac{1}{2\pi}
\end{align*}\]</p><svg id="sample_polar_uni_slider" style="margin-top:15px"></svg><p>In the previous section, we applied a change of coordinates in one dimension.
That is, by taking $$\Phi = F_X^{-1}$$, we transformed the uniform unit distribution to have our desired PDF.</p><p>\[\begin{align*}
f_{\text{Inv}}(x) &amp;= f_U(F_X^{-1}(x)) \cdot \left|D\left(F_X^{-1}\right)^{-1}\right|\\
&amp;= 1 \cdot |DF_X(x)|\\
&amp;= f_X(x)
\end{align*}\]</p><p>In practice, many useful distributions can be efficiently sampled via the proper change of coordinates.
However, doing so requires a parameterization of the domain, which is sometimes infeasible to construct.
In such cases, we may turn to methods like <em>Markov Chain Monte Carlo</em>, to be discussed in a future chapter.</p><hr/><p>Written on <time datetime="2025-04-12T00:00:00+00:00">April 12, 2025</time></p></main></div></div>
  </body>
</html>
