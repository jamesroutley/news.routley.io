<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://proxiesapi.com/articles/web-scraping-in-python-the-complete-guide">Original</a>
    <h1>Web Scraping in Python – The Complete Guide</h1>
    
    <div id="readability-page-1" class="page"><div>

      <p><span>I</span>n this tutorial you&#39;ll build robust web crawlers using libraries like <a href="https://proxiesapi.com/articles/introduction-to-web-scraping-with-beautifulsoup">BeautifulSoup</a>, learn techniques to overcome real-world scraping challenges and best practices for large scale scraping.</p><p><span>You&#39;ll gain the skills to scrape complex sites, handle issues like rate limits, blocks, and javascript pages.</span></p><h2><span><strong>Why Python for Web Scraping?</strong></span></h2><p><span>Python is a popular language for web scraping due to its advantages:</span></p><li><span><strong>Simple Syntax</strong></span><span>: Python&#39;s intuitive syntax allows quick coding for scraping.</span></li><li><span><strong>Built-in Libraries</strong></span><span>: Python comes with built-in libraries and modules, like urllib and lxml, that aid in scraping.</span></li><li><span><strong>Mature Scraping Libraries</strong></span><span>: Libraries like Beautiful Soup and Scrapy simplify scraping at any scale.</span></li><li><span><strong>General Purpose</strong></span><span>: Python can be used to build complete data pipelines around scraping.</span></li><li><span><strong>Interoperability</strong></span><span>: Python integrates well with other languages and performs well when performance is crucial.</span></li><p><span>In contrast, languages like C++ require more effort for basic scraping tasks. JavaScript platforms like Node.js can be complex for beginners when building scraping scripts.</span></p><p><span>Python&#39;s simplicity, power, and interoperability makes it ideal for scraping needs. Its high-quality libraries allow quick start to scraping at scale.</span></p><h2><span><strong>Best Python Web Scraping Libraries</strong></span></h2><p><span>Some of the most popular and robust Python libraries for web scraping are:</span></p><p><span><strong>BeautifulSoup</strong></span></p><li><span>Features: Excellent HTML/XML parser, easy web scraping interface, flexible navigation and search. We will be using this library in our example scraper below.</span></li><li><span>Use Case: Small to medium scale web scraping.</span></li><li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">Link to BeautifulSoup docs</a></li><p><span><strong>Scrapy</strong></span></p><li><span>Features: Fast and scalable, middlewares, distributed crawling capability.</span></li><li><span>Use Case: Large scale advanced web scraping projects.</span></li><li><a href="https://scrapy.org/">Link to Scrapy docs</a></li><p><span><strong>Selenium</strong></span></p><li><span>Features: Full browser automation, handles javascript heavy sites.</span></li><li><span>Use Case: Sites with highly dynamic content loaded by JS.</span></li><li><a href="https://www.selenium.dev/documentation/overview/">Link to Selenium docs</a></li><p><span><strong>lxml</strong></span></p><li><span>Features: Very fast XML and HTML parser.</span></li><li><span>Use Case: Lightning fast parsing of XML/HTML data.</span></li><li><a href="https://lxml.de/">Link to lxml docs</a></li><p><span><strong>pyquery</strong></span></p><li><span>Features: jQuery-style syntax for accessing HTML elements.</span></li><li><span>Use Case: Makes scrape code look cleaner and more readable.</span></li><li><a href="https://pythonhosted.org/pyquery/">Link to pyquery docs</a></li><h2><span><strong>Prerequisites</strong></span></h2><p><span>To follow along with the code examples in this article, you will need:</span></p><p><span><strong>Virtual Environment (Recommended)</strong></span></p><p><span>While optional, we highly recommended creating a virtual env for the project:</span></p><div><pre><code>python -m venv my_web_scraping_env
</code></pre></div><p><span><strong>The Libraries</strong></span></p><p><span>We will be using the </span><span><strong>Requests</strong></span><span>, </span><span><strong>BeautifulSoup</strong></span><span> and </span><span><strong>OS</strong></span><span> libraries primarily:</span></p><div><pre><code>pip install requests beautifulsoup4
</code></pre></div><p><span>This will fetch the libraries from PyPI and install them locally.</span></p><p><span>With the prerequisites installed, you are all setup! Let&#39;s start scraping.</span></p><h2><span><strong>Lets pick a target website</strong></span></h2><p><span>For demonstration purposes, we will be scraping the Wikipedia page </span><a href="https://commons.wikimedia.org/wiki/List_of_dog_breeds">List of dog breeds</a><span> to extract information about various dog breeds.</span></p><p><span>The rationale behind choosing this page is:</span></p><li><span>Well structured HTML layout that makes scraping easy</span></li><li><span>Nice table layout with one breed per row</span></li><li><span>Contains mulitple data fields per breed including names, breed group, alternate names and images</span></li><li><span>Images can allow us to showcase scraping binary files as well</span></li><p><span>This is the page we are talking about…</span></p><img src="https://proxiesapi.com/articles/images/65d4bf9ea24a9.jpg" alt=""/><p><span>Other great pages to practice web scraping include:</span></p><li><span>Wikipedia category pages like </span><a href="https://en.wikipedia.org/wiki/Lists_of_films">Lists of films</a></li><li><span>Ecommerce product listings like </span><a href="https://www.amazon.com/books-used-books-textbooks/b?ie=UTF8&amp;node=283155">Amazon books</a></li><li><span>Real estate listings like </span><a href="https://www.zillow.com/homes/for_rent/">Zillow rentals</a></li><p><span>The concepts covered will be applicable across any site.</span></p><h2><span><strong>Write the scraping code</strong></span></h2><p><span>Let&#39;s now closely examine the full code to understand how to systematically scrape data from the dogs breed webpage.</span></p><div><pre><code>
# Full code

import os
import requests
from bs4 import BeautifulSoup

url = &#39;&lt;https://commons.wikimedia.org/wiki/List_of_dog_breeds&gt;&#39;

# Headers to masquerade as a browser
headers = {
    &#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#34;
}

# Download page HTML using requests
response = requests.get(url, headers=headers)

# Check valid response received
if response.status_code == 200:

    # Parse HTML using Beautiful Soup
    soup = BeautifulSoup(response.text, &#39;html.parser&#39;)

    # CSS selector for the main tables
    table = soup.find(&#39;table&#39;, {&#39;class&#39;: &#39;wikitable sortable&#39;})

    # Initialize data lists to store scraped info
    names = []
    groups = []
    local_names = []
    photographs = []

    # Create directory to store images
    os.makedirs(&#39;dog_images&#39;, exist_ok=True)

    # Loop through rows omitting header
    for row in table.find_all(&#39;tr&#39;)[1:]:

        # Extract each column data using CSS selectors
        columns = row.find_all([&#39;td&#39;, &#39;th&#39;])

        name = columns[0].find(&#39;a&#39;).text.strip()
        group = columns[1].text.strip()

        # Extract local name if exists
        span_tag = columns[2].find(&#39;span&#39;)
        local_name = span_tag.text.strip() if span_tag else &#39;&#39;

        # Extract photo url if exists
        img_tag = columns[3].find(&#39;img&#39;)
        photograph = img_tag[&#39;src&#39;] if img_tag else &#39;&#39;

        # Download + Save image if url exists
        if photograph:

            response = requests.get(photograph)

            if response.status_code == 200:

                image_filename = os.path.join(&#39;dog_images&#39;, f&#39;{name}.jpg&#39;)

                with open(image_filename, &#39;wb&#39;) as img_file:

                    img_file.write(response.content)

        names.append(name)
        groups.append(group)
        local_names.append(local_name)
        photographs.append(photograph)

print(names)
print(groups)
print(local_names)
print(photographs)
</code></pre></div><p><span>The imports include standard Python libraries that provide HTTP requests functionality (</span><span><ccode>requests</ccode></span><span>), parsing capability (</span><span><ccode>BeautifulSoup</ccode></span><span>), and file system access (</span><span><ccode>os</ccode></span><span>) which we will leverage.</span></p><p><span>The </span><span><ccode>requests</ccode></span><span> library allows us to make HTTP requests to the web page and check if the response is valid before parsing. </span><span><ccode>BeautifulSoup</ccode></span><span> then enables us to parse the full HTML content and isolate the main data table using CSS selectors. Finally, </span><span><ccode>os</ccode></span><span> provides file system access to save images locally.</span></p><p><span>Together they form a very handy toolkit for scraping!</span></p><h2><span><strong>Downloading the page</strong></span></h2><p><span>We first construct the target URL and initialize a requests </span><span><ccode>Session</ccode></span><span> which allows connection reuse and efficiencies when making multiple HTTP requests to the same domain:</span></p><div><pre><code>url = &#39;&lt;https://commons.wikimedia.org/wiki/List_of_dog_breeds&gt;&#39;

headers = {
    &#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#34;
}

response = requests.get(url, headers=headers)
</code></pre></div><p><span>We also setup a custom </span><span><ccode>User-Agent</ccode></span><span> HTTP header to masquerade as a Chrome browser. This helps avoid blocks from servers trying to prevent scraping.</span></p><p><span>After getting the response, we can check the status code to ensure we received a proper HTML document:</span></p><div><pre><code>if response.status_code == 200:
   # Success!
   print(response.text)
</code></pre></div><p><span>In case of errors (e.g. 404 or 500), we do not proceed with scraping and handle the failure.</span></p><h2><span><strong>Parsing the html</strong></span></h2><p><span>Since we received a valid HTML response, we can parse the text content using Beautiful Soup:</span></p><div><pre><code>soup = BeautifulSoup(response.text, &#39;html.parser&#39;)
</code></pre></div><p><span><ccode>BeautifulSoup</ccode></span><span> accepts the raw HTML text and an optional parser like </span><span><ccode>lxml</ccode></span><span> or the built-in </span><span><ccode>html.parser</ccode></span><span>, and provides simple methods and Pythonic idioms for navigating, searching, and modifying the parse tree.</span></p><p><span>Beautiful Soup transforms the messy HTML into a parse tree that mirrors the DOM structure of tags, attributes and text. We can use CSS selectors and traversal methods to quickly isolate the data we need from this tree.</span></p><h2><span><strong>The Magic of Selectors for Data Extraction</strong></span></h2><p><span>One of the most magical parts of web scraping with Python&#39;s BeautifulSoup library is using CSS selectors to extract specific content from HTML pages.</span></p><p><span>Selectors allow us to visually target the tags enclosing the data we want scraped. BeautifulSoup makes selecting elements a breeze.</span></p><p><span>For example, consider extracting book titles from this snippet:</span></p><div><pre><code>&lt;div class=&#34;book-listing&#34;&gt;
  &lt;img src=&#34;/covers/harry-potter.jpg&#34;&gt;
  &lt;span class=&#34;title&#34;&gt;Harry Potter and the Goblet of Fire&lt;/span&gt;
  &lt;span class=&#34;rating&#34;&gt;9.1&lt;/span&gt;
&lt;/div&gt;

&lt;div class=&#34;book-listing&#34;&gt;
  &lt;img src=&#34;/covers/lord-of-the-rings.jpg&#34;&gt;
  &lt;span class=&#34;title&#34;&gt;The Fellowship of the Ring&lt;/span&gt;
  &lt;span class=&#34;rating&#34;&gt;9.3&lt;/span&gt;
&lt;/div&gt;
</code></pre></div><p><span>We can directly target the </span><span><ccode>span</ccode></span><span> with class </span><span><ccode>title</ccode></span><span> through the CSS selector:</span></p><div><pre><code>soup.select(&#34;div.book-listing &gt; span.title&#34;)
</code></pre></div><p><span>This says - find all </span><span><ccode>span</ccode></span><span> tags having class </span><span><ccode>title</ccode></span><span> which are direct children of any </span><span><ccode>div</ccode></span><span> tag having </span><span><ccode>book-listing</ccode></span><span> as the CSS class.</span></p><p><span>And voila, we have exactly the titles isolated:</span></p><div><pre><code>[&lt;span class=&#34;title&#34;&gt;Harry Potter and the Goblet of Fire&lt;/span&gt;,
 &lt;span class=&#34;title&#34;&gt;The Fellowship of the Ring&lt;/span&gt;]
</code></pre></div><p><span>We can chain </span><span><ccode>.text</ccode></span><span> to extract just the readable text within the tags:</span></p><div><pre><code>[Harry Potter and the Goblet of Fire, The Fellowship of the Ring]
</code></pre></div><p><span>Selectors provide incredible precision during data extraction by leveraging the innate hierarchy of structured HTML tags surrounding it.</span></p><p><span>Some other examples of selectors:</span></p><div><pre><code># Select id attribute
soup.select(&#34;#book-title&#34;)

# Attribute equality match
soup.select(&#39;a[href=&#34;/login&#34;]&#39;)

# Partial attribute match
soup.select(&#39;span[class^=&#34;title&#34;]&#39;)

# Select direct descendant
soup.select(&#34;ul &gt; li&#34;)
</code></pre></div><p><span>As you can see, by mastering different selector types and combining multiple selectors where needed - you gain immense power to zone in on and extract the exact data you need from any HTML document, eliminating nearly all guesswork. Lets get back to the task at hand now…</span></p><h2><span><strong>Finding the table</strong></span></h2><img src="https://proxiesapi.com/articles/images/65d4bf9f4fbdb.jpg" alt=""/><p><span>Looking at the Raw HTML, we notice a </span><span><ccode>table</ccode></span><span> tag with CSS class </span><span><ccode>wikitable sortable</ccode></span><span> contains the main breed data.</span></p><p><span>We can simply select this using:</span></p><div><pre><code>table = soup.find(&#39;table&#39;, {&#39;class&#39;: &#39;wikitable sortable&#39;})
</code></pre></div><p><span>This searches the parse tree for any </span><span><ccode>table</ccode></span><span> tag having a </span><span><ccode>class</ccode></span><span> attribute matching </span><span><ccode>wikitable sortable</ccode></span><span>. Beautiful soup makes Selection using CSS selectors super easy!</span></p><h2><span><strong>Extracting all the fields</strong></span></h2><p><span>With the table isolated, we loop through every </span><span><ccode>tr</ccode></span><span> row after the header row to extract the data from each breed:</span></p><div><pre><code>for row in table.find_all(&#39;tr&#39;)[1:]:

    columns = row.find_all([&#39;td&#39;, &#39;th&#39;])

    name = columns[0].find(&#39;a&#39;).text.strip()
    group = columns[1].text.strip()
</code></pre></div><p><span>Here, </span><span><ccode>.find_all()</ccode></span><span> helps search all the row children tags for any </span><span><ccode>td</ccode></span><span> or </span><span><ccode>th</ccode></span><span> elements, which represent table cells. We select these into a list </span><span><ccode>columns</ccode></span><span>.</span></p><p><span>Using positional indexes in this columns list, we can extract the data within each cell cleanly:</span></p><div><pre><code>    name = columns[0].find(&#39;a&#39;).text.strip()
</code></pre></div><p><span>This grabs the anchor </span><span><ccode>a</ccode></span><span> tag inside the first table cell, gets </span><span><ccode>.text</ccode></span><span> property to extract raw string content and chains </span><span><ccode>.strip()</ccode></span><span> to remove whitespace. Beautiful Soup chains such operations elegantly.</span></p><p><span>Similarly for cells containing just text:</span></p><div><pre><code>    group = columns[1].text.strip()
</code></pre></div><p><span>We fetch </span><span><ccode>.text</ccode></span><span> property directly on table cell element.</span></p><p><span>The power of CSS selectors in quickly isolating specific tags, ids, classes or attributes makes data extraction very precise and straightforward in Beautiful Soup.</span></p><h2><span><strong>Downloading and saving the images</strong></span></h2><p><span>After scraping textual data like names, groups etc in each row, we check the last cell for an image link:</span></p><div><pre><code>    img_tag = columns[3].find(&#39;img&#39;)
    photograph = img_tag[&#39;src&#39;] if img_tag else &#39;&#39;
</code></pre></div><p><span>This tries detecting and fetching src attribute on any image tag if exists.</span></p><p><span>We can then download and save images using this url if present:</span></p><div><pre><code>    if photograph:

        response = requests.get(photograph)

        image_filename = os.path.join(&#39;dog_images&#39;, f&#39;{name}.jpg&#39;)

        with open(file_path, &#39;wb&#39;) as img_file:
           img_file.write(response.content)
</code></pre></div><p><span>We reuse the </span><span><ccode>requests</ccode></span><span> library to make another GET request - this time to download the image binary content and save it locally using built-in file handling capability. Pretty nifty!</span></p><p><span>And that&#39;s it! By using </span><span><ccode>requests</ccode></span><span> and BeautifulSoup together with Python&#39;s intuitive standard library, we were able to build a complete web scraper to extract complex data!</span></p><h2><span><strong>Alternative libraries and tools for web scraping</strong></span></h2><p><span>While requests and BeautifulSoup form the most popular combination, here are some alternatives worth considering:</span></p><p><span><strong>Scrapy</strong></span></p><p><span>An open source modular scraping framework meant for large scale crawling that handles throttling, cookies, proxy rotation automatically. Recommended for complex needs.</span></p><p><span><strong>Selenium</strong></span></p><p><span>Performs actual browser automation by controlling Chrome, Firefox etc. Enables scraping dynamic content that renders via JavaScript. More complex setup.</span></p><p><span><strong>pyppeteer</strong></span></p><p><span>Headless browser automation like Selenium driven through Python code. Good for javascript rendered websites.</span></p><p><span><strong>pyquery</strong></span></p><p><span>Offers jQuery style element selection. Scrape code looks very clean due to chaining syntax similar to jQuery.</span></p><p><span><strong>lxml</strong></span></p><p><span>A very fast XML/HTML parser. Great when raw parsing performance is critical.</span></p><h2><span><strong>Challenges of Web Scraping in the real world: Some tips &amp; best practices</strong></span></h2><p><span>While basic web scraping is easy, building robust production-grade scalable crawlers brings its own challenges:</span></p><h3><span><strong>Handling Dynamic Content</strong></span></h3><p><span>Many websites rely heavily on JavaScript to render content dynamically. Static scraping then fails. </span><span><strong>Solutions:</strong></span><span> Use browser automation tools like Selenium or scraper specific solutions like Scrapy&#39;s splash integration.</span></p><p><span>Here is a simple Hello World example to handle dynamic content using Selenium browser automation:</span></p><div><pre><code>from selenium import webdriver
from selenium.webdriver.common.by import By

# Initialize chrome webdriver
driver = webdriver.Chrome()

# Load page
driver.get(&#34;&lt;https://example.com&gt;&#34;)

# Wait for title to load from dynamic JS execution
driver.implicitly_wait(10)

# Selenium can extract dynamically loaded elements
print(driver.title)

# Selenium allows clicking buttons triggering JS events
driver.find_element(By.ID, &#34;dynamicBtn&#34;).click()

# Inputs can be handled as well
search = driver.find_element(By.NAME, &#39;search&#39;)
search.send_keys(&#39;Automate using Selenium&#39;)
search.submit()

# Teardown browser after done
driver.quit()
</code></pre></div><p><span>The key capabilities offered by Selenium here are:</span></p><ol><li><span>Launches a real Chrome browser to load JavaScript</span></li><li><span>Finds elements only available after execution of JS</span></li><li><span>Can interact with page by clicking, entering text etc thereby triggering JavaScript events</span></li><li><span>Experience mimics an actual user browsing dynamically generated content</span></li></ol><p><span>Together this allows handling complex sites primarily driven by JavaScript for dynamic content. Selenium provides full programmatic control to automate browsers directly thereby scraping correctly.</span></p><h3><span><strong>Getting Blocked</strong></span></h3><p><span>Websites often block scrapers via blocked IP ranges or blocking characteristic bot activity through heuristics. </span><span><strong>Solutions:</strong></span><span> Slow down requests, properly mimic browsers, rotate user agents and proxies.</span></p><p><span><strong>Rate Limiting</strong></span></p><p><span>Servers fight overload by restricting number of requests served per time. Hitting these limits lead to temporary bans or denied requests. </span><span><strong>Solutions:</strong></span><span> Honor crawl delays, use proxies and ration requests appropriately.</span></p><p><span>Here is sample code to handle rate limiting while scraping:</span></p><p><span>Many websites have protection mechanisms that temporarily block scrapers when they detect too many frequent requests coming from a single IP address.</span></p><p><span>We can counter getting blocked by rate limits by adding throttling, proxies and random delays in our code.</span></p><div><pre><code>import time
import random
import requests
from urllib.request import ProxyHandler, build_opener

# List of free public proxies
PROXIES = [&#34;104.236.141.243:8080&#34;, &#34;104.131.178.157:8085&#34;]

# Pause 5-15 seconds between requests randomly
def get_request():
    time.sleep(random.randint(5, 15))
    proxy = random.choice(PROXIES)
    opener = build_opener(ProxyHandler({&#39;https&#39;: proxy}))

    resp = opener.open(&#34;&lt;https://example.com&gt;&#34;)
    return resp

for i in range(50):
   response = get_request()
   print(&#34;Request Success&#34;)
</code></pre></div><p><span>Here each request first waits for a random interval before executing. This prevents continuous rapid requests.</span></p><p><span>We also route every alternate request through randomly chosen proxy servers via rotated IP addresses.</span></p><p><span>Together, throttling down overall crawl pace and distributing requests over different proxy IPs prevents hitting site-imposed rate limits.</span></p><p><span>Additional improvements like automatically detecting rate limit warnings in responses and reacting accordingly can enhance the scraper&#39;s resilience further.</span></p><h3><span><strong>Rotating User Agents</strong></span></h3><p><span>Websites often try to detect and block scraping bots by tracking characteristic user agent strings.</span></p><p><span>To prevent blocks, it is good practice to rotate multiple well-disguised user agents randomly to mimic a real browser flow.</span></p><p><span>Here is sample code to pick a random desktop user agent from a predefined list using Python&#39;s random library before making each request:</span></p><div><pre><code>import requests
import random

# List of desktop user agents
user_agents = [
    &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#34;,
    &#34;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 OPR/43.0.2442.991&#34;
    &#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/604.4.7 (KHTML, like Gecko) Version/11.0.2 Safari/604.4.7&#34;
]

# Pick a random user agent string
user_agent = random.choice(user_agents)

# Set request headers with user agent before making request
headers = {&#34;User-Agent&#34;: user_agent}

response = requests.get(url, headers=headers)
</code></pre></div><p><span>By varying the user agent across requests in code runs, websites have a tougher time profiling traffic as coming from an automated bot using a static user agent. This allows the scraper to fly under the radar without getting blocked.</span></p><p><span>Some additional enhancements include:</span></p><li><span>Having separate user agent lists for mobile, tablets, desktop browsers</span></li><li><span>Updating the lists with latest user agents periodically</span></li><li><span>Dynamically generating user agents to match genuine browser attributes</span></li><p><span>With effective user agent rotation and an ever expanding list of strings, scrapers enjoy better longevity undetected before site administrators can profile and actively block them.</span></p><h3><span><strong>Browser Fingerprinting</strong></span></h3><p><span>Beyond simplistic user agent checks, websites have adopted advanced browser fingerprinting techniques to identify bots.</span></p><p><span>This involves browser attribute profiling - collecting information regarding device screen size, installed fonts, browser plugins etc. together called browser fingerprints.</span></p><p><span>These fingerprints tend to remain largely consistent, stable and unique for standard tool-based bots and automation software.</span></p><p><span>Dynamic websites track fingerprints of scrapers accessing them. By detecting known crawler fingerprints they can block them even if the user agents are rotated constantly.</span></p><p><span><strong>Minimizing detection risks</strong></span></p><p><span>Some ways to minimize exposing scraper fingerprints:</span></p><li><span>Use Selenium to automate a standard desktop browser like Chrome or Firefox instead of custom bot agents</span></li><li><span>Dynamically generate randomized attributes like viewport size, screen resolution, font lists within ranges of variety exhibited by human browsers</span></li><li><span>Utilize proxy rotation and residential IP proxies to prevent tracking of IP specific attributes</span></li><li><span>Limit number of parallel requests from a single proxy to site to make traffic volume seem manual</span></li><p><span>Essentially by mimicking the natural randomness and variability across genuine user browsers, scraper fingerprints can avoid easy profiling by sites simply as another standard browser.</span></p><p><span>Here is a code example to dynamically modify browser attributes to avoid fingerprinting:</span></p><div><pre><code>from selenium import webdriver
import random

# List of common screen resolutions
screen_res = [(1366, 768), (1920, 1080), (1024, 768)]

# List of common font families
font_families = [&#34;Arial&#34;, &#34;Times New Roman&#34;, &#34;Verdana&#34;]

#Pick random resolution
width, height = random.choice(screen_res)

#Create chrome options
opts = webdriver.ChromeOptions()

# Set random screen res
opts.add_argument(f&#34;--window-size={width},{height}&#34;)

# Set random user agent
opts.add_argument(&#34;--user-agent=Mozilla/5.0...&#34;)

# Set random font list
random_fonts = random.choices(font_families, k=2)
opts.add_argument(f&#39;--font-list=&#34;{random_fonts[0]};{random_fonts[1]}&#34;&#39;)

# Initialize driver with options
driver = webdriver.Chrome(options=opts)

# Access webpage
driver.get(target_url)

# Webpage sees every scraper request originating
# from distinct unpredictable browser profiles

</code></pre></div><p><span>Here we randomly configure our Selenium controlled Chrome instance with different screen sizes, user agents and font sets per request.</span></p><p><span>and here is how you do it using <a href="https://proxiesapi.com/articles/using-proxies-with-python-requests">Python Requests</a>…</span></p><div><pre><code>import requests
import random

# Device profiles
desktop_config = {
    &#39;user-agent&#39;: &#39;Mozilla/5.0...&#39;,
    &#39;accept-language&#39;: [&#39;en-US,en&#39;, &#39;en-GB,en&#39;],
    &#39;accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#39;,
    &#39;accept-encoding&#39;: &#39;gzip, deflate, br&#39;,
    &#39;upgrade-insecure-requests&#39;: &#39;1&#39;,
    &#39;sec-fetch-site&#39;: &#39;none&#39;,
    &#39;sec-fetch-mode&#39;: &#39;navigate&#39;,
    &#39;sec-fetch-user&#39;: &#39;?1&#39;,
    &#39;sec-fetch-dest&#39;: &#39;document&#39;,
    &#39;cache-control&#39;: &#39;max-age=0&#39;
}

mobile_config = {
    &#39;user-agent&#39;: &#39;Mozilla/5.0... Mobile&#39;,
    &#39;accept-language&#39;: [&#39;en-US,en&#39;, &#39;en-GB,en&#39;],
    &#39;accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&#39;,
    &#39;x-requested-with&#39;: &#39;mark.via.gp&#39;,
    &#39;sec-fetch-site&#39;: &#39;same-origin&#39;,
    &#39;sec-fetch-mode&#39;: &#39;navigate&#39;,
    &#39;sec-fetch-user&#39;: &#39;?1&#39;,
    &#39;sec-fetch-dest&#39;: &#39;document&#39;,
    &#39;referer&#39;: &#39;&lt;https://www.example.com/&gt;&#39;,
    &#39;accept-encoding&#39;: &#39;gzip, deflate, br&#39;,
    &#39;cache-control&#39;: &#39;max-age=0&#39;
}

device_profiles = [desktop_config, mobile_config]

def build_headers():

    profile = random.choice(device_profiles)

    headers = {
         &#39;User-Agent&#39;: random.choice(profile[&#39;user-agent&#39;]),
         &#39;Accept-Language&#39;: random.choice(profile[&#39;accept-language&#39;]),
         # Other headers
         ...
    }

    return headers
</code></pre></div><p><span>Now instead of hard coding, the scraper randomly selects from plausible configuration profiles including several identifying request headers - providing realistic and human-like mutations necessary to avoid fingerprint tracking.</span></p><h3><span><strong>Parsing Complex HTML</strong></span></h3><p><span>Scrape targets often have complex HTML structures, obfuscated tags and advanced client side code packing logic which break parsers. </span><span><strong>Solutions</strong></span><span>: Careful inspection of rendered source, using robust parsers like lxml and enhancing selectors.</span></p><p><span>Here are some common types of bad HTML scrape targets exhibit and techniques to handle them:</span></p><p><span><strong>Improper Nesting</strong></span></p><p><span>HTML can often have incorrectly nested tags:</span></p><div><pre><code>&lt;b&gt;Latest News &lt;p&gt;Impact of oil prices fall...&lt;/b&gt;&lt;/p&gt;
</code></pre></div><p><span><strong>Solution</strong></span><span>: Use a parser like </span><span><em>lxml</em></span><span> that handles bad nesting and uneven tags more robustly.</span></p><p><span><strong>Broken Markup</strong></span></p><p><span>Tags could be unclosed:</span></p><div><pre><code>&lt;div&gt;
  &lt;span class=&#34;title&#34;&gt;Python Web Scraping &lt;span&gt;
  Lorem ipsum...
&lt;/div&gt;
</code></pre></div><p><span><strong>Solution</strong></span><span>: Specify tag close explicitly while parsing:</span></p><div><pre><code>title = soup.find(&#34;span&#34;, class_=&#34;title&#34;).text
</code></pre></div><p><span><strong>Non-standard Elements</strong></span></p><p><span>Vendor specific unrecognized custom tags may exist:</span></p><div><pre><code>&lt;album&gt;
  &lt;cisco:song&gt;Believer&lt;/cisco:song&gt;
&lt;/album&gt;
</code></pre></div><p><span><strong>Solution</strong></span><span>: Search for standard tags in namespace:</span></p><div><pre><code>song = soup.find(&#34;cisco:song&#34;).text
</code></pre></div><p><span><strong>Non-text Content</strong></span></p><p><span>Tables, images embedded between text tags:</span></p><div><pre><code>&lt;p&gt;
  Trending Now
  &lt;table&gt;...&lt;/table&gt;
&lt;/p&gt;
</code></pre></div><p><span><strong>Solution</strong></span><span>: Select child tags specifically:</span></p><div><pre><code>paras = soup.select(&#34;p &gt; text()&#34;)
</code></pre></div><p><span>This picks only text nodes as children ignoring other elements present under </span><span><ccode></ccode></span></p><p><span> tag.</span></p><p><span>As you can see, liberal use of selectors along with robust parsers provides the tools to handle even badly designed HTML and extract the required data reliably.</span></p><h3><span><strong>Other guidelines worth following:</strong></span></h3><li><span>Respect </span><span><ccode>robots.txt</ccode></span><span> rules</span></li><li><span>Check if API access is available before scraping sites without permission</span></li><li><span>Scrape data responsibly in moderate volumes</span></li><p><span>Adopting these practices ensures reliable, resilient and responsible scraping operations.</span></p><p><span><strong>Conclusion</strong></span><span>
In this comprehensive guide, we took an in-depth look into web scraping using Python. We covered:</span></p><li><span>Why Python and libraries like BeautifulSoup are ideal for scraping most targets</span></li><li><span>Common scraping patterns like making requests, parsing responses, handling dynamic content using Selenium</span></li><li><span>Best practices around mimicry, circumventing blocks, respecting crawl delays and auto-throttling</span></li><li><span>How to build resilient, production-grade scalable scrapers</span></li><p><span>By learning core scraping paradigms, structuring code properly and applying optimization techniques, extracting accurate web data in Python at scale has become an achievable skill!</span></p><p><span>While these examples are great for learning, scraping production-level sites can pose challenges like CAPTCHAs, IP blocks, and bot detection. Rotating proxies and automated CAPTCHA solving can help.</span></p><p><span>Proxies API offers a simple API for rendering pages with built-in proxy rotation, CAPTCHA solving, and evasion of IP blocks. You can fetch rendered pages in any language without configuring browsers or proxies yourself.</span></p><p><span>This allows scraping at scale without headaches of IP blocks. Proxies API has a free tier to get started. Check out the API and</span><a href="https://proxiesapi.com/"> sign up for an API key</a><span> to supercharge your web scraping.</span></p><p><span>With the power of Proxies API combined with Python libraries like Beautiful Soup, you can scrape data at scale without getting blocked.</span></p>      

    
    
    </div></div>
  </body>
</html>
