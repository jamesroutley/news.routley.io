<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://turbopuffer.com/blog/ann-v3">Original</a>
    <h1>ANN v3: 200ms p99 query latency over 100B vectors</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><span>January 21, 2026</span><span>•</span><span>Nathan VanBenschoten (Chief Architect)</span></p></div><p>The pursuit of scale is not vanity. When you take existing systems and optimize
them from first principles to achieve a step change in scalability, you can
create something entirely new.</p>
<p>Nothing has demonstrated that more clearly than the explosion in deep learning
over the past decade. The ML community took decades-old ideas and combined them
with advancements in hardware, new algorithms, and hyper-specialization to forge
something remarkable.</p>
<p>Both inspired by the ML community and in service of it, we recently rebuilt
vector search in turbopuffer to support scales of up to <strong>100 billion vectors</strong>
in a <strong>single search index</strong>. We call this technology Approximate Nearest
Neighbor (ANN) Search v3, <a href="#use-now">and it is available now</a>.</p>
<p>In this post, I&#39;ll dive into the technical details behind how we built for 100
billion vectors. Along the way, we’ll examine turbopuffer’s architecture, travel
up the modern memory hierarchy, zoom into a single CPU core, and then back out
to the scale of a distributed cluster.</p>

<h2 id="billion-scale-ann-search"><a aria-hidden="true" tabindex="-1" href="#billion-scale-ann-search"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Billion-scale ANN search</h2>
<p>Let’s look at the numbers to get a sense of the challenge: 100 billion vectors,
1024 dimensions per vector, 2 bytes per dimension (<code>f16</code>). <strong>This is vector
search over 200TiB of dense vector data</strong>. We want to serve a high rate (&gt; 1k
QPS) of ANN queries over this entire dataset, each with a latency target of
200ms or less.</p>
<p>With a healthy dose of mechanical sympathy, let’s consider how our hardware will
run this workload and where it will encounter bottlenecks. If one part of the
system bottlenecks (disk, network, memory, or CPU), other parts of the system
will go underutilized. The key to making the most of the available hardware is
to push down bottlenecks and balance resource utilization.</p>
<p>turbopuffer’s <a target="_blank" href="https://turbopuffer.com/docs/architecture">architecture</a> is
simple and opinionated. This simplicity makes the exercise tractable.
turbopuffer’s query tier is a stateless layer on top of object storage,
consisting of a caching hierarchy and compute. That’s it.</p>
<div><pre><code>                        ╔═ turbopuffer ════════════════════════════╗
╔════════════╗          ║                                          ║░
║            ║░         ║  ┏━━━━━━━━━━━━━━━┓     ┏━━━━━━━━━━━━━━┓  ║░
║   client   ║░───API──▶║  ┃    Memory/    ┃────▶┃    Object    ┃  ║░
║            ║░         ║  ┃   SSD Cache   ┃     ┃ Storage (S3) ┃  ║░
╚════════════╝░         ║  ┗━━━━━━━━━━━━━━━┛     ┗━━━━━━━━━━━━━━┛  ║░
 ░░░░░░░░░░░░░░         ║                                          ║░
                        ╚══════════════════════════════════════════╝░
                         ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░</code></pre><pre><code>      ╔════════════╗
      ║   client   ║░
      ╚════════════╝░
       ░░░░░║░░░░░░░░
            ▼
╔═ turbopuffer ════════════╗
║  ┏━━━━━━━━━━━━━━━━━━━━┓  ║░
║  ┃    Memory/SSD      ┃  ║░
║  ┃      Cache         ┃  ║░
║  ┗━━━━━━━━┳━━━━━━━━━━━┛  ║░
║           ▼              ║░
║  ┏━━━━━━━━━━━━━━━━━━━━┓  ║░
║  ┃    Object Storage  ┃  ║░
║  ┃      (S3)          ┃  ║░
║  ┗━━━━━━━━━━━━━━━━━━━━┛  ║░
╚══════════════════════════╝░
 ░░░░░░░░░░░░░░░░░░░░░░░░░░░░</code></pre></div>
<h3 id="identifying-the-bottleneck"><a aria-hidden="true" tabindex="-1" href="#identifying-the-bottleneck"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Identifying the bottleneck</h3>
<p>When trying to process large amounts of data at high throughput, this system
architecture could bottleneck in one of two ways. First, it could bottleneck on
the CPU instructions needed to process the data (”compute-bound”). Second, it
could bottleneck on the data path up the memory hierarchy feeding the CPU
(”bandwidth-bound”).</p>
<p>We can borrow a strategy from the GPU community in order to estimate where it
will bottleneck by classifying our workload’s
<a target="_blank" href="https://en.wikipedia.org/wiki/Roofline_model#Arithmetic_intensity">arithmetic intensity</a>.
Arithmetic intensity is the ratio of arithmetic operations to memory operations.
It is often defined using GPU FLOPs and bytes transferred through GPU memory,
but it is generalizable to other domains.</p>
<p>Different algorithms have different intensities. For example, a matrix-matrix
multiplication is more intensive than a vector dot product. This is because in a
matrix multiplication (<code>SGEMM</code>), each element in one matrix is multiplied
against N elements (a full row or column) in the other matrix. In a vector dot
product (<code>SDOT</code>), each element is multiplied only against one element from the
other vector.</p>
<div><pre><code>

               ▲
               │                                    ╱
               │                  memory bandwidth
               │                   (bytes/second) ╱
               │
               │                                ╱              arithmetic bandwidth
               ├ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─██───────────────────────┬────────────
               │                              ╱ ridge point
               │                             ╱                         │
               │                            ╱
               │                           ╱                           │
               │                          ╱
               │                         ╱                             │
  performance  │                        ╱
(FLOPS/second) │                       ╱ │                             │
               │                      ╱
               │                     ╱   │ arithmetic                  │ arithmetic
               │                    ╱      intensity 1                   intensity 2
               │                   ╱     │ (memory-bound)              │ (compute-bound)
               │                  ╱
               │                 ╱       │                             │
               │                ╱
               │               ╱         │                             │
               │              ╱
               │             ╱           │                             │
               │            ╱
               │           ╱             │                             │
               │          ╱
               │         ╱               │                             │
               │        ╱
               └───────╳─────────────────┴─────────────────────────────┴────────────▶
                                       arithmetic intensity
                                           (FLOPs/byte)

</code></pre><pre><code>performance
(FLOPS/s)
▲
│                       ╱
│    memory bandwidth   
│     (bytes/second)  ╱
│                     arithmetic
│                   ╱ bandwidth
├ ─ ─ ─ ─ ─ ─ ─ ─ ██───┬──────
│          ridge  ╱    │
│          point ╱     │
│               ╱      │
│              ╱       │
│             ╱        │
│            ╱         │
│           ╱          │
│          ╱│          │
│         ╱            │ 
│        ╱  │memory    │compute
│       ╱    bound     │bound
│      ╱    │          │
│     ╱                │
│    ╱      │          │
│   ╱                  │
│  ╱        │          │
│ ╱                    │
└╳──────────┴──────────┴──▶
       arithmetic intensity
               (FLOPs/byte)</code></pre></div>
<p><em>adapted from
<a target="_blank" href="https://modal.com/gpu-glossary/perf/arithmetic-intensity">https://modal.com/gpu-glossary/perf/arithmetic-intensity</a></em></p>
<p>As a rule of thumb, a workload that has a small constant arithmetic intensity
(e.g., SDOT) will be memory-bound. A workload that has a large constant or
linear arithmetic intensity (e.g., SGEMM) will be compute-bound. The intuition
is simple enough — if a byte pulled into a compute register is only used once or
twice, more work goes into fetching the byte than is needed to operate on it.
Meanwhile, if that byte is used many times, the memory fetch is amortized and
the computation over it dominates.</p>
<p>If we imagine the kernel of a vector search, the system fetches each data vector
and performs a distance calculation between it and a query vector. This distance
function is essentially a vector dot product, multiplying the data and query
value in each of the corresponding vector dimensions.</p>
<div><pre><code>   
╭  ╮   ╭  ╮    
│d1│   │q1│ 
│d2│   │q2│ 
│d3│ • │q3│ = ∑ di • qi
│••│   │••│
│di│   │qi│
╰  ╯   ╰  ╯</code></pre><pre><code>   
╭  ╮   ╭  ╮    
│d1│   │q1│ 
│d2│   │q2│ 
│d3│ • │q3│ = ∑ di • qi
│••│   │••│
│di│   │qi│
╰  ╯   ╰  ╯</code></pre></div>
<p>Since each element in a data vector is used only once by the distance function,
the arithmetic intensity of vector search is low. Most of the work goes into
pulling many large data vectors into CPU registers. Recognizing this, we can
predict that vector search will be <strong>bandwidth-bound</strong>, as is the case for many
analytics and search systems.</p>
<p>Consequently, it doesn’t really matter how efficient the CPU instructions of our
distance kernel are (within reason). If we are trying to maximize throughput
(queries per second) on a machine, we are going to be limited by the number of
data vectors we can run the kernel over each second.</p>
<p>With this insight in hand, our objective with ANN v3 is to <strong>utilize cache space
efficiently</strong> and <strong>balance bandwidth demands</strong> to prevent the network, disk, or
main memory from being a dominant bottleneck that limits the system’s ability to
scale.</p>
<p>Let’s take a look at all of the places in turbopuffer’s memory hierarchy that
might prevent a bandwidth-bound workload from scaling.</p>
<div><pre><code>
                    ╱ ╲
                   ╱   ╲
                  ╱ CPU ╲_______________________│ Size: &lt; 1 KB
                 ╱  Reg  ╲                      │ Bandwidth: &gt;10 TB/s
                ╱—————————╲
               ╱  L1/L2/L3 ╲____________________│ Size: KBs - MBs
              ╱    Cache    ╲                   │ Bandwidth: 1 TB/s - 10s TB/s
             ╱———————————————╲
            ╱   Main Memory   ╲_________________│ Size: GBs - TBs
           ╱      (DRAM)       ╲                │ Bandwidth: 100 GB/s - 500 GB/s
          ╱—————————————————————╲
         ╱       NVMe SSD        ╲______________│ Size: TBs - 10s TBs
        ╱   (direct I/O cache)    ╲             │ Bandwidth: 1 GB/s - 30 GB/s
       ╱———————————————————————————╲
      ╱    Cloud Object Storage     ╲___________│ Size: PBs - EBs
     ╱  (e.g., S3, GCS, Azure Blob)  ╲          │ Bandwidth: 1 GB/s - 10 GB/s
    ╱—————————————————————————————————╲
    </code></pre><pre><code>
┌────────────┐
│CPU         │ Size: &lt;1KB
│Registers   │ BW: &gt;10 TB/s
├────────────┤
│L1/L2/L3    │ Size: KB-MB
│Cache       │ BW: 1-10s TB/s
├────────────┤
│Main Memory │ Size: GB-TB
│(DRAM)      │ BW: 100-500 GB/s
├────────────┤
│NVMe SSD    │ Size: 1-10s TBs
│(direct I/O)│ BW: 1-30 GB/s
├────────────┤
│Object      │ Size: PB-EB
│Storage     │ BW: 1-10 GB/s
└────────────┘
    </code></pre></div>
<p>We observe four distinct boundaries where bandwidth may become the limiting
factor.</p>
<ul>
<li>Object Storage (where data is durably stored) ↔ NVMe</li>
<li>NVMe ↔ DRAM</li>
<li>DRAM ↔ L3/L2/L1</li>
<li>L3/L2/L1 ↔ CPU registers (where the compute actually happens)</li>
</ul>
<p>Take note of the difference in bandwidth between levels of the hierarchy, but
also differences in size. Higher tiers are orders of magnitude smaller, but can
service many orders of magnitude higher rates of data loads.</p>
<p>To balance bandwidth across this hierarchy, ANN v3 combines two complementary
techniques: <strong>hierarchical clustering</strong> and <strong>binary quantization</strong>.</p>
<p>Each exploits the same general strategy of “approximation and refinement”. ANN
v3 works by first quickly answering: <em>roughly</em> where is the answer? and only
then answering: out of that set, <em>exactly</em> what is the answer?</p>
<h3 id="hierarchical-clustering-to-narrow-the-search-space"><a aria-hidden="true" tabindex="-1" href="#hierarchical-clustering-to-narrow-the-search-space"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Hierarchical clustering to narrow the search space</h3>
<p>The first technique is <em>hierarchical clustering</em> in the index structure. Vector
indexes in turbopuffer are based on
<a target="_blank" href="https://dl.acm.org/doi/10.1145/3600006.3613166">SPFresh</a>, a centroid-based
approximate nearest neighbor index that supports incremental updates. In a
centroid-based index, vectors are grouped into clusters, each represented by a
single &#34;centroid&#34; vector (typically the mean of all vectors in that cluster). At
query time, we first compare the query to centroids to identify promising
clusters, then search only within those clusters. We extended the SPTAG
graph-based index described in the original SPFresh paper, nesting clusters
hierarchically in a multi-dimensional tree structure.</p>
<p>While hierarchical clustering is not new to v3, it is a very important aspect of
cold query performance in turbopuffer. When a namespace is <em>cold</em> (not cached on
SSD), turbopuffer must fetch some or all of it from object storage. Instead of
traversing a graph with sequential object storage round-trips to locate the
relevant data clusters, the hierarchy bounds the number of round-trips to object
storage to the height of the SPFresh tree. This places a bound on tail latency,
even for the coldest query.</p>
<div><pre><code>
                      ┌───────────────────┐
                      │ root centroid c0  │
                      └───────────────────┘
                         ╱      │      ╲
                        ╱       │       ╲
┌───────────────────┐ ┌───────────────────┐ ┌───────────────────┐
│    centroid c1    │ │    centroid c2    │ │    centroid c3    │
└───────────────────┘ └───────────────────┘ └───────────────────┘
        ╱    │    ╲        ╱    │    ╲        ╱    │    ╲
       ╱     │     ╲      ╱     │     ╲      ╱     │     ╲
┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌──────────
│ data vec v1 |  │ data vec v2 |  │ data vec v3 │  │ •••
└─────────────┘  └─────────────┘  └─────────────┘  └──────────

</code></pre><pre><code>      ┌───────────────┐
      │ root centroid │
      │       c0      │
      └───────────────┘
        ╱     │     ╲
       ╱      │      ╲
┌────────┐┌────────┐┌────────┐
│centroid││centroid││centroid│
|   c1   ││   c2   ││   c3   │
└────────┘└────────┘└────────┘
      ╱│╲       ╱│╲       ╱│╲
     ╱ │ ╲     ╱ │ ╲     ╱ │ ╲
    ╱  │  ╲   ╱  │  ╲   ╱  │  ╲
┌──────┐ ┌──────┐ ┌──────┐ ┌────
│ data │ │ data │ │ data │ │     
|  v1  │ │  v2  │ │  v3  │ │ •••
└──────┘ └──────┘ └──────┘ └────
</code></pre></div>
<p>In the case of 100 billion vector search, we can’t afford to contend with the
low bandwidth of object storage (&lt;5 GB/s) for even a fraction of data vector
reads, so we size deployments to store the entire tree on SSD. Yet even when
cached, the tree structure complements the hardware.</p>
<p>Clustering interacts well with the memory hierarchy because it provides
<strong>spatial locality</strong>. Vectors closer in space - those likely to be accessed
together - are stored contiguously. This makes memory and disk accesses
efficient. Specifically, it means that there is very little amplification when
reading from lower levels of the hierarchy, even when those levels enforce a
minimum read granularity (e.g., 4KiB from disk). Every byte fetched will be put
to good use.</p>
<p><em>Hierarchical</em> clustering, specifically, interacts well with the memory
hierarchy because it provides <strong>temporal locality</strong>. Vector clusters in the
upper levels of the tree are accessed frequently, so they will naturally remain
resident in main memory. We use a 100x branching factor between levels of our
tree to balance tree depth with cluster size. Each node in the tree has
approximately 100 children, creating a wide, shallow tree structure. This
branching factor roughly matches the size ratio between DRAM and SSD (10x -
50x), meaning that if we can fit all data vectors on SSD, we can fit all
centroid vectors in DRAM.</p>
<p>All of this gets us back to the original purpose of the approximate nearest
neighbor index: reducing the search space for each query. Approximate indexes
are a compromise between performance and recall. For centroid-based indexes, we
navigate this compromise by controlling how many clusters are scanned at each
level of the tree (often called the &#34;probes&#34; or &#34;beam width&#34; parameter).</p>
<p>For vector search at this scale, we found experimentally that with good
clustering, we needed to search about 500 data vector clusters (each 100 vectors
large) on each machine to achieve our recall target. This equates to a bandwidth
requirement of <strong>100MB per level of the tree</strong>:</p>
<div><pre><code>
                100 vectors     1024 dimensions      2 bytes
500 clusters x ───────────── x ───────────────── x ─────────── = 100MB per level
                  cluster           vector          dimension
    </code></pre><pre><code>      100 vec   1024D   2 bytes
500 • ─────── • ───── • ─────── 
      cluster   vector  dim  

      
    = 100MB per level</code></pre></div>
<p>We can use this to estimate throughput limits:</p>

<p>The derivation shows that with hierarchical clustering alone, we will end up
disk-bound fetching data vector clusters and maxing out at around 100 queries
per second. 100 qps over 100 billion vectors is admirable, but we can do better.</p>
<h3 id="binary-quantization-to-compress-vector-sizes"><a aria-hidden="true" tabindex="-1" href="#binary-quantization-to-compress-vector-sizes"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Binary quantization to compress vector sizes</h3>
<p>The second technique is <em>binary quantization</em> of data vectors. When vectors are
inserted into turbopuffer, the system stores both the full-precision vector
(<code>f32</code> or <code>f16</code> per dimension) and a transparently computed, quantized form of
the vector (1-bit per dimension).</p>
<div><pre><code>    
    [ 0.94, -0.01,  0.39, -0.72,  0.02, -0.85, -0.18,  0.99,  0.45 ]
                                    |
                                    │ binary quantization
                                    │ 
                                    ▼
                      [ 1, 0, 1, 0, 1, 0, 0, 1, 1 ]
    </code></pre><pre><code>╭       ╮               ╭   ╮
│  0.94 │               │ 1 │
│ -0.01 │               │ 0 │
│  0.39 │               │ 1 │
│ -0.72 │ ────────────▶ │ 0 │
│  0.02 │    binary     │ 1 │
│ -0.85 │ quantization  │ 0 │
│ -0.18 │               │ 0 │
│  0.99 │               │ 1 │
│  0.45 │               │ 1 │
╰       ╯               ╰   ╯</code></pre></div>
<p>The math works out as expected; <strong>binary quantization provides a 16-32x
compression for data vectors</strong>. This allows these quantized vectors to be stored
higher in the memory hierarchy and minimizes their memory bandwidth demands.</p>
<p>To avoid this compression leading to a loss of search quality, ANN v3 employs
the <a target="_blank" href="https://dl.acm.org/doi/pdf/10.1145/3654970">RaBitQ</a> quantization method.
RaBitQ exploits the mathematical properties of high-dimensional space
(<a target="_blank" href="https://en.wikipedia.org/wiki/Concentration_of_measure">concentration of measure</a>)
to compress aggressively while preserving high recall. Specifically, in high
dimensions, vector components naturally become more uniformly distributed, which
means quantization errors are spread evenly across all dimensions rather than
concentrated in problematic directions. This uniform error distribution enables
RaBitQ to provide tight theoretical error bounds alongside distance estimations
made on quantized vectors.</p>
<p>For example, consider a data vector V<sub>d</sub> and query vector
V<sub>q</sub>. A full-precision distance computation might compute the cosine
distance between V<sub>d</sub> and V<sub>q</sub> as 0.75. Now consider their
binary quantized forms, V<sub>d</sub>&#39; and V<sub>q</sub>&#39;. Due to the loss of
information from quantizing, RaBitQ cannot perfectly compute the true cosine
distance by looking just at the quantized vectors. Instead, it will compute an
estimated range like [0.69, 0.83].</p>
<p>Despite being imprecise, this confidence interval can be used to conclude that
V<sub>d</sub> is closer to V<sub>q</sub> than some other vector V<sub>d2</sub>
whose quantized distance estimate has a range [0.87, 0.91]. For other data
vectors with overlapping distance estimate ranges (e.g., [0.51, 0.77]), RaBitQ
makes no promises. Such vectors must be recompared using their full precision
vectors.</p>
<p>During a vector search, turbopuffer first evaluates the search on quantized
vectors, uses the error bounds to determine all vectors that could be in the
<em>true</em> top-k, then fetches their corresponding full-precision vectors, and
reranks over those to compute the final result. In practice, we find that less
than 1% of data vectors in the narrowed search space need to be reranked to
avoid an impact on recall.</p>
<h2 id="putting-it-all-together"><a aria-hidden="true" tabindex="-1" href="#putting-it-all-together"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Putting it all together...</h2>
<p>These two techniques compose, and their benefits multiply.</p>
<p>Upper levels of a quantized ANN tree are small but frequently accessed. As a
result, they naturally remain resident all the way up in the L3 CPU cache. We
can write out the math to demonstrate this to ourselves. With a branching factor
of 100, each level L in the tree contains <code>100^L * 1024/8</code> bytes of quantized
vector data (1 bit per dimension). With a 504MiB shared L3 cache, we can fit all
three upper levels of the tree in L3 cache, the largest requiring
<code>100^3 * 128 = 128 MiB</code> of cache space.</p>
<p>The lowest level of the quantized ANN tree is stored in DRAM, as was the case
before. However, because these vectors are compressed, they require less memory
bandwidth to access.</p>
<p>Meanwhile, the full-precision vectors remain on local SSD. However, only a small
fraction is fetched during the reranking phase, through a highly concurrent
<a target="_blank" href="https://en.wikipedia.org/wiki/Gather/scatter_(vector_addressing)">scatter-gather</a>.
This access pattern is ideal for modern NVMe drives, which have random read
latencies around 50-100 microseconds and excel at handling many parallel I/O
operations simultaneously, allowing us to access the full bandwidth of the
disks.</p>
<p>We can again estimate throughput limits. Remember that quantized vectors are 16x
smaller than unquantized <code>f16</code> vectors, equating to a bandwidth requirement of
<code>500 x 100 x 1024 x 2 / 16 = 6MB</code> per level of the tree.</p>

<p>The changes here demonstrate a remarkable dynamic in the memory hierarchy. Data
compression both reduces bandwidth requirements and allows data to remain
resident in higher bandwidth tiers at the same time, leading to a multiplicative
effect. Whereas before the theoretical throughput limit was 100 qps,
quantization unlocks a theoretical limit of 10,000 qps!</p>
<h2 id="to-end-up-compute-bound"><a aria-hidden="true" tabindex="-1" href="#to-end-up-compute-bound"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>...to end up compute-bound...</h2>
<p>By combining hierarchical clustering with binary quantization, ANN v3 makes
efficient use of cache space and balances bandwidth demands across the tiers of
the memory hierarchy.</p>
<p>However, if we run a production load against it, we notice something
interesting. Instead of hitting the theoretical 10,000 qps, the system ends up
saturated around 1,000 qps, 10% of where we’d like to be.</p>
<p>What is happening here? Why are we no longer limited by disk bandwidth, and
instead limited somewhere else?</p>
<p>Returning to our framework for arithmetic intensity, we discover what changed.
When we added in binary quantization, intensity shot up. With <code>f16</code> vector
elements, every two bytes fetched from memory were used for a single logical
operation. With binary quantized elements, however, every two bytes fetched are
used for <strong>sixteen</strong> logical operations (one per bit). In fact, the RaBitQ
algorithm reuses each bit four times when computing distance estimates (section
3.3.2 of the <a target="_blank" href="https://arxiv.org/pdf/2405.12497">paper</a>), leading to 64x higher
arithmetic intensity.</p>
<p>This large constant arithmetic intensity is enough to tip the scale towards the
system being compute-bound. Each CPU core in our system cannot keep up with the
rate of highly compressed vector data being fed to it, bottlenecking throughput.</p>
<p>Optimizing a compute-bound system is a different game, requiring an obsessive
focus on:</p>
<ol>
<li>doing the same work with fewer instructions</li>
<li>keeping the CPU pipelines fed by avoiding stalls and branch mispredictions</li>
</ol>
<p>As an example, we were recently surprised to find that
<a target="_blank" href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX2</a> (256-bit x86
SIMD) does not have an accelerated
<a target="_blank" href="https://en.wikichip.org/wiki/population_count">popcount instruction</a>. Counting
the number of bits set in a series of bytes is an important operation for
RaBitQ, so making this operation fast was essential. We switched a core distance
estimation kernel over to <a target="_blank" href="https://en.wikipedia.org/wiki/AVX-512">AVX-512</a> to
gain access to its <code>VPOPCNTDQ</code> instruction. This instruction is capable of
counting the bits set to one across a 512-bit register in only 3 cpu cycles of
latency. Better yet, it can be pipelined for a throughput of 1 instruction per
cycle.</p>
<p>Making the switch improved the performance of the kernel in microbenchmarks by
30% and improved end-to-end production throughput by about 5%. When
microbenchmarks translate to end-to-end performance gains, you have correctly
identified the system’s bottleneck.</p>
<p>A more general discussion of optimizing a compute-bound system is a deeper topic
than we have room for in this post, but it remains a focus of our work to this
day as we continue to optimize ANN v3 for higher throughput.</p>
<h2 id="in-a-distributed-cluster"><a aria-hidden="true" tabindex="-1" href="#in-a-distributed-cluster"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>...in a distributed cluster</h2>
<p>Careful readers may have noticed one inconsistency in the description of ANN v3
so far. I mentioned up top that our goal was to handle vector search over 200
TiB of dense vector data. We also discussed that for this workload shape, all of
this data should be cached on SSD to avoid the limited bandwidth and high
latency of object storage. And yet, NVMe SSDs only get so large...</p>
<p>This is where distribution comes in. Modern clouds provide storage-dense VMs
with fast locally attached NVMe drives (e.g., GCP&#39;s <code>z3</code>, AWS’s <code>I7i</code>, Azure’s
<code>Lsv4</code>). These drives are large (10-40TiB) but not 200TiB large. To achieve the
desired aggregate SSD capacity, we use a cluster of storage-optimized machines,
each storing a subset of the index.</p>
<p>At this scale, simple random sharding works well. During ingestion, each vector
is randomly assigned to one of the shards of the index. An ANN search query is
broadcast to all shards, and the global top-k is stitched together from the
sub-results.</p>
<p>This technique can scale to arbitrarily large indexes, but its cost scales
linearly with the number of machines in the cluster. This is why it is crucial
to maximize the efficiency of a single machine before turning to distribution.
Moving in the other direction leads to an unnecessarily expensive system.</p>
<h2 id="conclusion"><a aria-hidden="true" tabindex="-1" href="#conclusion"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 24 24"><path d="M9.199 13.599a5.99 5.99 0 0 0 3.949 2.345 5.987 5.987 0 0 0 5.105-1.702l2.995-2.994a5.992 5.992 0 0 0 1.695-4.285 5.976 5.976 0 0 0-1.831-4.211 5.99 5.99 0 0 0-6.431-1.242 6.003 6.003 0 0 0-1.905 1.24l-1.731 1.721a.999.999 0 1 0 1.41 1.418l1.709-1.699a3.985 3.985 0 0 1 2.761-1.123 3.975 3.975 0 0 1 2.799 1.122 3.997 3.997 0 0 1 .111 5.644l-3.005 3.006a3.982 3.982 0 0 1-3.395 1.126 3.987 3.987 0 0 1-2.632-1.563A1 1 0 0 0 9.201 13.6zm5.602-3.198a5.99 5.99 0 0 0-3.949-2.345 5.987 5.987 0 0 0-5.105 1.702l-2.995 2.994a5.992 5.992 0 0 0-1.695 4.285 5.976 5.976 0 0 0 1.831 4.211 5.99 5.99 0 0 0 6.431 1.242 6.003 6.003 0 0 0 1.905-1.24l1.723-1.723a.999.999 0 1 0-1.414-1.414L9.836 19.81a3.985 3.985 0 0 1-2.761 1.123 3.975 3.975 0 0 1-2.799-1.122 3.997 3.997 0 0 1-.111-5.644l3.005-3.006a3.982 3.982 0 0 1 3.395-1.126 3.987 3.987 0 0 1 2.632 1.563 1 1 0 0 0 1.602-1.198z"></path></svg></a>Conclusion</h2>
<p>When performance at scale becomes cost-efficient, it stops being a benchmarking
exercise and becomes a building block.</p>
<p>The ANN v3 architecture pushes turbopuffer to 100 billion-vector scale at
thousands of QPS, while holding p99 latency at 200ms. More importantly, it does
this while keeping costs low enough to run continuously in production. It
achieves this through hierarchical clustering that matches the memory hierarchy,
binary quantization that compresses vectors by 16-32x with reranking to maintain
recall, and distribution across storage-dense machines - all working together to
maximize hardware utilization and minimize bottlenecks.</p>
<p>turbopuffer customers can now use ANN v3 with the sharding and pinning
technique by splitting their largest indexes into 4 TiB namespaces and
randomly assigning vectors across them. Then, <a target="_blank" href="https://turbopuffer.com/contact/support">contact
us</a> to pin each shard to its own SSD
(this option will soon be available directly in the dashboard). We&#39;re also
working on making the sharding fully transparent in the future.</p>
<div><h2>turbopuffer</h2><p>We host <!-- -->2.5T+<!-- --> documents, handle writes at<!-- --> <!-- -->10M+ writes/s<!-- -->, and serve <!-- -->10k+ queries/s<!-- -->. We are ready for far more. We hope you&#39;ll trust us with your queries.</p><p><a href="https://skelios.com/join">Get started</a></p></div></div></div>
  </body>
</html>
