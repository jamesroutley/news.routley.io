<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.geoffreylitt.com/2025/07/27/enough-ai-copilots-we-need-ai-huds.html">Original</a>
    <h1>Enough AI copilots! We need AI HUDs</h1>
    
    <div id="readability-page-1" class="page"><div><p>In my opinion, one of the best critiques of modern AI design comes from <a href="https://cgi.csc.liv.ac.uk/~coopes/comp319/2016/papers/UbiquitousComputingAndInterfaceAgents-Weiser.pdf">a 1992 talk</a> by the researcher <a href="https://en.wikipedia.org/wiki/Mark_Weiser">Mark Weiser</a> where he ranted against “copilot” as a metaphor for AI.</p>

<p>This was 33 years ago, but it’s still incredibly relevant for anyone designing with AI.</p>

<h2 id="weisers-rant">Weiser’s rant</h2>

<p>Weiser was speaking at an <a href="https://www.dropbox.com/scl/fo/axpzd925tcsnkc9x5nd51/AJMdLqxafEYFun4Ns6fqMHo?dl=0&amp;e=1&amp;preview=frames_1992_014_Nov.pdf&amp;rlkey=znit21hyth8w24m6gm02rq2y7">MIT Media Lab event</a> on “interface agents”. They were grappling with many of the same issues we’re discussing in 2025: how to make a personal assistant that automates tasks for you and knows your full context. They even had a human “butler” on stage representing an AI agent.</p>

<p>Everyone was super excited about this… except Weiser. He was opposed to the whole idea of agents! He gave this example: how should a computer help you fly a plane and avoid collisions?</p>

<p><strong>The agentic option is a “copilot” — a virtual human who you talk with to get help flying the plane.</strong> If you’re about to run into another plane it might yell at you “collision, go right and down!”</p>

<p>Weiser offered a different option: <strong>design the cockpit so that the human pilot is naturally aware of their surroundings.</strong> In his words: “You’ll no more run into another airplane than you would try to walk through a wall.”</p>

<p>Weiser’s goal was an “invisible computer&#34;—not an assistant that grabs your attention, but a computer that fades into the background and becomes &#34;an extension of [your] body”.</p>

<figure>
  <img src="https://www.ponylang.io/images/article_images/weiser-slide.png?1753652074" alt=""/>
  <figcaption>Weiser’s 1992 slide on airplane interfaces</figcaption>
</figure>

<h2 id="huds">HUDs</h2>

<p>There’s a tool in modern planes that I think nicely illustrates Weiser’s philosophy: <strong>the Head-Up Display (HUD), which overlays flight info like the horizon and altitude on a transparent display directly in the pilot’s field of view.</strong></p>

<p>A HUD feels completely different from a copilot! You don’t talk to it. It’s literally part invisible—you just become naturally aware of more things, as if you had magic eyes.</p>

<p><img src="https://www.ponylang.io/images/article_images/copilot-hud.png?1753652074" alt=""/></p>

<h2 id="designing-huds">Designing HUDs</h2>

<p>OK enough analogies. What might a HUD feel like in modern software design?</p>

<p>One familiar example is spellcheck. Think about it: <strong>spellcheck isn’t designed as a “virtual collaborator” talking to you about your spelling.</strong> It just instantly adds red squigglies when you misspell something! You now have a new sense you didn’t have before. It’s a HUD.</p>

<p>(This example comes from Jeffrey Heer’s excellent <a href="https://idl.cs.washington.edu/files/2019-AgencyPlusAutomation-PNAS.pdf">Agency plus Automation</a> paper. We may not consider spellcheck an AI feature today, but it’s still a fuzzy algorithm under the hood.)</p>

<figure>
  <img src="https://www.ponylang.io/images/article_images/spellcheck.png?1753652074" alt=""/>
  <figcaption>Spellcheck makes you aware of misspelled words without an “assistant” interface.</figcaption>
</figure>

<p>Here’s another personal example from AI coding. Let’s say you want to fix a bug. The obvious “copilot” way is to open an agent chat and ask it to do the fix.</p>

<p>But there’s another approach I’ve found more powerful at times: <strong>use AI to build a custom debugger UI which visualizes the behavior of my program!</strong> In one example, I <a href="https://www.ponylang.io/2024/12/22/making-programming-more-fun-with-an-ai-generated-debugger">built a hacker-themed debug view of a Prolog interpreter</a>.</p>

<p>With the debugger, I have a HUD! I have new senses, I can see how my program runs. The HUD extends beyond the narrow task of fixing the bug. I can ambiently build up my own understanding, spotting new problems and opportunities.</p>

<video autoplay="" loop="" controls="controls" preload="auto" muted="muted" data-video="0" type="video/mp4" src="/images/article_images/debugger/demo.mp4" width="100%"></video>

<p>Both the spellchecker and custom debuggers show that automation / “virtual assistant” isn’t the only possible UI. We can instead use tech to build better HUDs that enhance our human senses.</p>

<h2 id="tradeoffs">Tradeoffs</h2>

<p>I don’t believe HUDs are universally better than copilots! But I do believe <strong>anyone serious about designing for AI should consider non-copilot form factors that more directly extend the human mind.</strong></p>

<p>So when should we use one or the other? I think it’s quite tricky to answer that, but we can try to use the airplane analogy for some intuition:</p>

<p>When pilots just want the plane to fly straight and level, they fully delegate that task to an autopilot, which is close to a “virtual copilot”. But if the plane just hit a flock of birds and needs to land in the Hudson, the pilot is going to take manual control, and we better hope they have great instruments that help them understand the situation.</p>

<p>In other words: routine predictable work might make sense to delegate to a virtual copilot / assistant. But when you’re shooting for extraordinary outcomes, perhaps the best bet is to equip human experts with new superpowers.</p>

<hr/>

<h2 id="further-reading">Further reading</h2>

<ul>
<li>A nice discussion of one approach to this idea can be found in <a href="https://distill.pub/2017/aia/">Using Artificial Intelligence to Augment Human Intelligence</a> by Michael Nielsen and Shan Carter.</li>
<li>A more cryptic take on the same topic: <a href="https://www.ponylang.io/2025/06/29/chat-ai-dialogue">Is chat a good UI for AI? A Socratic dialogue</a></li>
<li>A discussion of how the the HUD philosophy intersects with on-demand software creation: <a href="https://www.ponylang.io/2023/03/25/llm-end-user-programming">Malleable software in the age of LLMs</a></li>
</ul>
</div></div>
  </body>
</html>
