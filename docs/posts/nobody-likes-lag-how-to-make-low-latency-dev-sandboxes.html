<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.compyle.ai/blog/nobody-likes-lag/">Original</a>
    <h1>Nobody likes lag: How to make low-latency dev sandboxes</h1>
    
    <div id="readability-page-1" class="page"><div>  <p>Nobody likes lag, especially not developers. When you type a character in a terminal or editor, you expect it to appear instantly. At Compyle, we spin up ephemeral cloud dev environments where an agent and a user share an IDE + terminal. So how can we make a remote sandbox feel local?</p>
<p><strong>TL;DR:</strong> If you want low latency sandboxes, cut out the middlemen and put your servers next to your users.</p> 
<h2 id="the-naive-approach">The Naive Approach</h2>
<p>Here is what our initial architecture looked like: <img src="https://www.compyle.ai/blog/nobody_likes_lag/initial_architecture_diagram.png" alt="Naive architecture diagram"/></p>
<p>The flow would be something like:</p>
<ol>
<li>User starts task</li>
<li>We provision a new sandbox in our primary region</li>
<li>We communicate with the agent through a socket server that handles authorization, routing requests to the correct sandbox, and persisting any messages the agent sends (we don’t want to give the sandbox any credentials).</li>
</ol>
<p>There are three main concerns with running a coding agent sandbox.</p>
<ul>
<li>Startup time</li>
<li>Latency</li>
<li>Security</li>
</ul>
<p>I can confidently say that this architecture does pretty poorly on the first two categories. Here’s the scorecard:</p>
<h3 id="startup-time-bad-10-30-seconds">Startup time: bad (10-30 seconds)</h3>
<p>While many sandbox companies advertise sub-second cold starts, this simply wasn’t the case for us. Our dockerfile is a few hundred megabytes and we attach an encrypted volume to each machine. In practice, machines took <strong>10 seconds</strong> to start in the best case, and <strong>30 seconds</strong> in the worst. That said, now is a good time to mention that we use fly.io for our sandboxes, and love it. More on this later.</p>
<p>With this approach, users are staring at a loading screen for <strong>30 seconds</strong> before the agent’s first message would appear. Yikes.</p>
<h3 id="latency-also-bad-200ms">Latency: also bad (&gt;200ms)</h3>
<p>We had two glaring issues on the latency front:</p>
<ul>
<li>Every request makes an extra network hop. Worse yet, for websocket connections we were stitching the client connection to the agent connection in the socket server, which had some overhead.</li>
<li>Persistence was on the hot path between the agent and the client, so that extra network hop caused additional latency from any database queries done.</li>
</ul>
<p>For the agent messages, an extra <strong>150ms</strong> isn’t the end of the world because LLM calls make up the lion’s share of the gap between messages. The real issue is in the terminal and the IDE. It was about a <strong>200ms lag</strong> between hitting a key and seeing the character in the terminal. Genuinely unbearable. Same for opening a file in the IDE.</p>
<h3 id="security-its-fine">Security: it’s fine</h3>
<p>The agent wasn’t exposed to the public internet and doesn’t have any secrets. I’ll save security details for another time, though.</p>
<p>The point here is that this architecture does poorly on two of our three concerns; it’s painfully slow.</p>
<h2 id="fixing-startup-time-the-warm-pool">Fixing startup time: the warm pool</h2>
<p>The startup time is pretty easy to fix. Rather than provision a new machine every time someone requests one, we can keep a warm pool. Now, a pool of machines sit at the ready, so every time a user starts a task, we simply send an http request to start the task.</p>
<p>Great! No more cold starts, so startup time goes from <strong>30s → 50ms</strong></p>
<p>This does not solve the latency issue, though.</p>
<h2 id="cutting-out-the-middleman-and-why-i-love-flyio">Cutting out the middleman (and why I love fly.io)</h2>
<p>To remove that extra network hop, here’s the new architecture we went with: <img src="https://www.compyle.ai/blog/nobody_likes_lag/improved_architecture_diagram.png" alt="Improved architecture diagram"/></p>
<p>Spoiler, this change was the biggest win performance wise, but also the biggest lift technically because our socket server had so many responsibilities:</p>
<ol>
<li>Authorization</li>
<li>Billing and persistence for agent messages</li>
<li>Routing requests to the correct machine</li>
</ol>
<p>Each of these had to be addressed separately.</p>
<h3 id="authorization">Authorization</h3>
<p>Originally, we were authenticating users on the socket server, so each request made a database query. Since we are connecting directly to the machines now, a better approach is to create a JWT when the task is created, send it to the machine and the client, let the client connect directly to the machine. If the machine’s token doesn’t match, it’s an unauthorized request.</p>
<h3 id="billing-and-persistence">Billing and persistence</h3>
<p>Originally, we would bill users on each request going through the socket server, and send an interrupt back to the agent if the user was out of credits. We have an LLM router that receives all of the same messages, so we moved the responsibility there, and let it return a 402 for the agent to handle when the user is out of credits. For message persistence, by making the agent the source of truth for messages, we could also accommodate the database’s message records only being eventually consistent. We batch process them in a separate queue worker so we don’t hammer the database.</p>
<h3 id="routing-requests">Routing requests</h3>
<p>Originally, our sandbox urls were formatted like <code>&lt;task_id&gt;.machine.compyle.ai</code>. We would do another database lookup to find the correct machine and use fly.io’s private network addresses to route the request by machine id from the socket server.</p>
<p>We switched the domain to use <code>&lt;machine_id&gt;.sandbox.compyle.ai</code> and adopted fly.io’s clever proxying mechanism, fly replay. Fly replay is a cool trick that allows servers to respond to http requests with a 307 and <code>fly-replay</code> header to signal fly’s network to bounce the request to the machine specified in the header. Crucially, Fly can cache that behavior so that the bounce only happens once. You can read more about it <a href="https://fly.io/docs/networking/dynamic-request-routing/">here</a>.</p>
<p>At this point, the terminals and IDE were super fast for me, but I live in San Francisco, right by the data center in San Jose. For a user on the East coast (or elsewhere in the world), it was a different story. This is what the latency of a user typing <code>ls -lsa</code> into their terminal from Virginia would look like:</p>
<figure>
  <img src="https://www.compyle.ai/blog/nobody_likes_lag/initial_timestamps.png" alt="Initial timestamps"/>
  <figcaption>min 73 ms, max 85 ms, median 80 ms</figcaption>
</figure>
<p>That’s an average <strong>79ms</strong> roundtrip, and we can do better.</p>
<h2 id="multi-regional-pool">Multi-regional pool</h2>
<p>If I’m in San Francisco, and I get the benefit of working on machines in San Jose, then why shouldn’t our users? The last unlock was moving our sandboxes to the closest region to our users. We still have to keep a warm pool, so instead of keeping one warm pool in San Jose, we now keep separate pools across the US, Europe, and Asia.</p>
<p>Here’s our architecture now: <img src="https://www.compyle.ai/blog/nobody_likes_lag/final_architecture_diagram.png" alt="Final architecture diagram"/></p>
<p>And here’s what the latency looks like for a user typing <code>ls -lsa</code> into the terminal of a nearby sandbox:</p>
<figure>
  <img src="https://www.compyle.ai/blog/nobody_likes_lag/final_timestamps.png" alt="Final timestamps"/>
  <figcaption>min 13 ms, max 16 ms, median 13 ms</figcaption>
</figure>
<p>That’s an average <strong>14ms</strong> and that’s pretty good.</p>
<h2 id="final-results">Final Results</h2>
<p>Reducing terminal roundtrip time from <strong>&gt;200ms to 14ms</strong> feels great, but real lesson here is a familiar one: simple solutions are best. Our final architecture has less infrastructure than our initial architecture. The biggest speedup came from deleting the socket servers and the code that lived on them. If you want to improve performance it’s often best to look for what you can remove.</p>  </div></div>
  </body>
</html>
