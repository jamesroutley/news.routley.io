<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/huggingface/candle">Original</a>
    <h1>Candle: Torch Replacement in Rust</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a href="https://crates.io/crates/candle-core" rel="nofollow"><img src="https://camo.githubusercontent.com/cd16dc5317b00d5ec802ae4e14252a601a6160512020ecb043dbf3ee3adcdb36/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f63616e646c652d636f72652e737667" alt="Latest version" data-canonical-src="https://img.shields.io/crates/v/candle-core.svg"/></a>
<a href="https://docs.rs/candle-core" rel="nofollow"><img src="https://camo.githubusercontent.com/05cfa8e3fff22e13fb8c9f9adbc4b291dbbca36b4c0aaa02780910713258fd64/68747470733a2f2f646f63732e72732f63616e646c652d636f72652f62616467652e737667" alt="Documentation" data-canonical-src="https://docs.rs/candle-core/badge.svg"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d2b08729d49028074f6b73fc9e9f1a39306271e184ebd44386b66e70315af000/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f6c2f63616e646c652d636f72652e737667"><img src="https://camo.githubusercontent.com/d2b08729d49028074f6b73fc9e9f1a39306271e184ebd44386b66e70315af000/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f6c2f63616e646c652d636f72652e737667" alt="License" data-canonical-src="https://img.shields.io/crates/l/candle-core.svg"/></a></p>
<p dir="auto">Candle is a minimalist ML framework for Rust with a focus on easiness of use and
on performance (including GPU support). Try our online demos:
<a href="https://huggingface.co/spaces/lmz/candle-whisper" rel="nofollow">whisper</a>,
<a href="https://huggingface.co/spaces/lmz/candle-llama2" rel="nofollow">llama2</a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="let a = Tensor::randn(0f32, 1., (2, 3), &amp;Device::Cpu)?;
let b = Tensor::randn(0f32, 1., (3, 4), &amp;Device::Cpu)?;

let c = a.matmul(&amp;b)?;
println!(&#34;{c}&#34;);"><pre><span>let</span> a = <span>Tensor</span><span>::</span><span>randn</span><span>(</span><span>0f32</span><span>,</span> <span>1.</span><span>,</span> <span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>,</span> <span>&amp;</span><span>Device</span><span>::</span><span>Cpu</span><span>)</span>?<span>;</span>
<span>let</span> b = <span>Tensor</span><span>::</span><span>randn</span><span>(</span><span>0f32</span><span>,</span> <span>1.</span><span>,</span> <span>(</span><span>3</span><span>,</span> <span>4</span><span>)</span><span>,</span> <span>&amp;</span><span>Device</span><span>::</span><span>Cpu</span><span>)</span>?<span>;</span>

<span>let</span> c = a<span>.</span><span>matmul</span><span>(</span><span>&amp;</span>b<span>)</span>?<span>;</span>
<span>println</span><span>!</span><span>(</span><span>&#34;{c}&#34;</span><span>)</span><span>;</span></pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-check-out-our-examples" aria-hidden="true" href="#check-out-our-examples"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Check out our examples</h2>
<p dir="auto">Check out our <a href="https://ntietz.com/huggingface/candle/blob/main/candle-examples/examples">examples</a>:</p>
<ul dir="auto">
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-examples/examples/whisper">Whisper</a>: speech recognition model.</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-examples/examples/llama">Llama and Llama-v2</a>: general LLM.</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-examples/examples/falcon">Falcon</a>: general LLM.</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-examples/examples/bert">Bert</a>: useful for sentence embeddings.</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-examples/examples/bigcode">StarCoder</a>: LLM specialized to code
generation.</li>
</ul>
<p dir="auto">Run them using the following commands:</p>
<div data-snippet-clipboard-copy-content="cargo run --example whisper --release
cargo run --example llama --release
cargo run --example falcon --release
cargo run --example bert --release
cargo run --example bigcode --release"><pre><code>cargo run --example whisper --release
cargo run --example llama --release
cargo run --example falcon --release
cargo run --example bert --release
cargo run --example bigcode --release
</code></pre></div>
<p dir="auto">In order to use <strong>CUDA</strong> add <code>--features cuda</code> to the example command line.</p>
<p dir="auto">There are also some wasm examples for whisper and
<a href="https://github.com/karpathy/llama2.c">llama2.c</a>. You can either build them with
<code>trunk</code> or try them online:
<a href="https://huggingface.co/spaces/lmz/candle-whisper" rel="nofollow">whisper</a>,
<a href="https://huggingface.co/spaces/lmz/candle-llama2" rel="nofollow">llama2</a>.</p>
<p dir="auto">For llama2, run the following command to retrieve the weight files and start a
test server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd candle-wasm-examples/llama2-c
wget https://karpathy.ai/llama2c/model.bin
wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.bin
trunk serve --release --public-url /candle-llama2/ --port 8081"><pre><span>cd</span> candle-wasm-examples/llama2-c
wget https://karpathy.ai/llama2c/model.bin
wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.bin
trunk serve --release --public-url /candle-llama2/ --port 8081</pre></div>
<p dir="auto">And then browse to
<a href="http://localhost:8081/candle-llama2" rel="nofollow">http://localhost:8081/candle-llama2</a>.</p>

<h2 tabindex="-1" dir="auto"><a id="user-content-features" aria-hidden="true" href="#features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Features</h2>
<ul dir="auto">
<li>Simple syntax, looks and like PyTorch.</li>
<li>CPU and Cuda backends, m1, f16, bf16.</li>
<li>Enable serverless (CPU), small and fast deployments</li>
<li>WASM support, run your models in a browser.</li>
<li>Model training.</li>
<li>Distributed computing using NCCL.</li>
<li>Models out of the box: Llama, Whisper, Falcon, StarCoder...</li>
<li>Embed user-defined ops/kernels, such as <a href="https://github.com/LaurentMazare/candle/blob/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152">flash-attention
v2</a>.</li>
</ul>

<h2 tabindex="-1" dir="auto"><a id="user-content-how-to-use-" aria-hidden="true" href="#how-to-use-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to use ?</h2>

<p dir="auto">Cheatsheet:</p>
<table>
<thead>
<tr>
<th></th>
<th>Using PyTorch</th>
<th>Using Candle</th>
</tr>
</thead>
<tbody>
<tr>
<td>Creation</td>
<td><code>torch.Tensor([[1, 2], [3, 4]])</code></td>
<td><code>Tensor::new(&amp;[[1f32, 2.]], [3., 4.]], &amp;Device::Cpu)?</code></td>
</tr>
<tr>
<td>Creation</td>
<td><code>torch.zeros((2, 2))</code></td>
<td><code>Tensor::zeros((2, 2), DType::F32, &amp;Device::Cpu)?</code></td>
</tr>
<tr>
<td>Indexing</td>
<td><code>tensor[:, :4]</code></td>
<td><code>tensor.i((.., ..4))?</code></td>
</tr>
<tr>
<td>Operations</td>
<td><code>tensor.view((2, 2))</code></td>
<td><code>tensor.reshape((2, 2))?</code></td>
</tr>
<tr>
<td>Operations</td>
<td><code>a.matmul(b)</code></td>
<td><code>a.matmul(&amp;b)?</code></td>
</tr>
<tr>
<td>Arithmetic</td>
<td><code>a + b</code></td>
<td><code>&amp;a + &amp;b</code></td>
</tr>
<tr>
<td>Device</td>
<td><code>tensor.to(device=&#34;cuda&#34;)</code></td>
<td><code>tensor.to_device(&amp;Device::Cuda(0))?</code></td>
</tr>
<tr>
<td>Dtype</td>
<td><code>tensor.to(dtype=torch.float16)</code></td>
<td><code>tensor.to_dtype(&amp;DType::F16)?</code></td>
</tr>
<tr>
<td>Saving</td>
<td><code>torch.save({&#34;A&#34;: A}, &#34;model.bin&#34;)</code></td>
<td><code>candle::safetensors::save(&amp;HashMap::from([(&#34;A&#34;, A)]), &#34;model.safetensors&#34;)?</code></td>
</tr>
<tr>
<td>Loading</td>
<td><code>weights = torch.load(&#34;model.bin&#34;)</code></td>
<td><code>candle::safetensors::load(&#34;model.safetensors&#34;, &amp;device)</code></td>
</tr>
</tbody>
</table>

<h2 tabindex="-1" dir="auto"><a id="user-content-structure" aria-hidden="true" href="#structure"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Structure</h2>
<ul dir="auto">
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-core">candle-core</a>: Core ops, devices, and <code>Tensor</code> struct definition</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-nn">candle-nn</a>: Facilities to build real models</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-examples">candle-examples</a>: Real-world like examples on how to use the library in real settings</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-kernels">candle-kernels</a>: CUDA custom kernels</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-datasets">candle-datasets</a>: Datasets and data loaders.</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-transformers">candle-transformers</a>: Transformer related utilities.</li>
<li><a href="https://ntietz.com/huggingface/candle/blob/main/candle-flash-attn">candle-flash-attn</a>: Flash attention v2 layer.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-faq" aria-hidden="true" href="#faq"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FAQ</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-why-candle" aria-hidden="true" href="#why-candle"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Why Candle?</h3>
<p dir="auto">Candle stems from the need to reduce binary size in order to <em>enable serverless</em>
possible by making the whole engine smaller than PyTorch very large library volume.
This enables creating runtimes on a cluster much faster.</p>
<p dir="auto">And simply <em>removing Python</em> from production workloads.
Python can really add overhead in more complex workflows and the <a href="https://www.backblaze.com/blog/the-python-gil-past-present-and-future/" rel="nofollow">GIL</a> is a notorious source of headaches.</p>
<p dir="auto">Rust is cool, and a lot of the HF ecosystem already has Rust crates <a href="https://github.com/huggingface/safetensors">safetensors</a> and <a href="https://github.com/huggingface/tokenizers">tokenizers</a>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-other-ml-frameworks" aria-hidden="true" href="#other-ml-frameworks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Other ML frameworks</h3>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/coreylowman/dfdx">dfdx</a> is a formidable crate, with shapes being included
in types preventing a lot of headaches by getting compiler to complain about shape mismatch right off the bat
However we found that some features still require nightly and writing code can be a bit dauting for non rust experts.</p>
<p dir="auto">We&#39;re leveraging and contributing to other core crates for the runtime so hopefully both crates can benefit from each
other</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/burn-rs/burn">burn</a> is a general crate that can leverage multiple backends so you can choose the best
engine for your workload</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/LaurentMazare/tch-rs.git">tch-rs</a> Bindings to the torch library in Rust. Extremely versatile, but they
do bring in the entire torch library into the runtime. The main contributor of <code>tch-rs</code> is also involved in the development
of <code>candle</code>.</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-missing-symbols-when-compiling-with-the-mkl-feature" aria-hidden="true" href="#missing-symbols-when-compiling-with-the-mkl-feature"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Missing symbols when compiling with the mkl feature.</h3>
<p dir="auto">If you get some missing symbols when compiling binaries/tests using the mkl
features, e.g.:</p>
<div data-snippet-clipboard-copy-content="  = note: /usr/bin/ld: (....o): in function `blas::sgemm&#39;:
          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_&#39; collect2: error: ld returned 1 exit status

  = note: some `extern` functions couldn&#39;t be found; some native libraries may need to be installed or have their path specified
  = note: use the `-l` flag to specify native libraries to link
  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo (see https://doc.rust-lang.org/cargo/reference/build-scripts.html#cargorustc-link-libkindname)"><pre><code>  = note: /usr/bin/ld: (....o): in function `blas::sgemm&#39;:
          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_&#39; collect2: error: ld returned 1 exit status

  = note: some `extern` functions couldn&#39;t be found; some native libraries may need to be installed or have their path specified
  = note: use the `-l` flag to specify native libraries to link
  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo (see https://doc.rust-lang.org/cargo/reference/build-scripts.html#cargorustc-link-libkindname)
</code></pre></div>
<p dir="auto">This is likely due to some missing linker flag that enable the mkl library. You
can try adding the following at the top of your binary:</p>
<div data-snippet-clipboard-copy-content="extern crate intel_mkl_src;"><pre><code>extern crate intel_mkl_src;
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-how-to-know-where-an-error-comes-from" aria-hidden="true" href="#how-to-know-where-an-error-comes-from"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>How to know where an error comes from.</h3>
<p dir="auto">You can set <code>RUST_BACKTRACE=1</code> to be provided with backtraces when a candle
error is generated.</p>
</article>
          </div></div>
  </body>
</html>
