<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blogs.nvidia.com/blog/runai/">Original</a>
    <h1>Nvidia to Acquire Run:AI</h1>
    
    <div id="readability-page-1" class="page"><div>
		<p>To help customers make more efficient use of their AI computing resources, NVIDIA today announced it has entered into a definitive agreement to acquire Run:ai, a Kubernetes-based workload management and orchestration software provider.</p>
<p>Customer AI deployments are becoming increasingly complex, with workloads distributed across cloud, edge and on-premises data center infrastructure.</p>
<p>Managing and orchestrating generative AI, recommender systems, search engines and other workloads requires sophisticated scheduling to optimize performance at the system level and on the underlying infrastructure.</p>
<p>Run:ai enables enterprise customers to manage and optimize their compute infrastructure, whether on premises, in the cloud or in hybrid environments.</p>
<p>The company has built an open platform on <a href="https://www.nvidia.com/en-us/glossary/kubernetes/">Kubernetes</a>, the orchestration layer for modern AI and cloud infrastructure. It supports all popular Kubernetes variants and integrates with third-party AI tools and frameworks.</p>
<p>Run:ai customers include some of the world’s largest enterprises across multiple industries, which use the Run:ai platform to manage data-center-scale GPU clusters.</p>
<p>“Run:ai has been a close collaborator with NVIDIA since 2020 and we share a passion for helping our customers make the most of their infrastructure,” said Omri Geller, Run:ai cofounder and CEO. “We’re thrilled to join NVIDIA and look forward to continuing our journey together.”</p>
<p>The Run:ai platform provides AI developers and their teams:</p>
<ul>
<li aria-level="1">A centralized interface to manage shared compute infrastructure, enabling easier and faster access for complex AI workloads.</li>
<li aria-level="1">Functionality to add users, curate them under teams, provide access to cluster resources, control over quotas, priorities and pools, and monitor and report on resource use.</li>
<li aria-level="1">The ability to pool GPUs and share computing power — from <a href="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/#:~:text=Multi%2DInstance%20GPU%20(MIG)%20expands%20the%20performance%20and%20value,%2C%20cache%2C%20and%20compute%20cores.">fractions of GPUs</a> to multiple GPUs or multiple nodes of GPUs running on different clusters — for separate tasks.</li>
<li aria-level="1">Efficient GPU cluster resource utilization, enabling customers to gain more from their compute investments.</li>
</ul>
<p>NVIDIA will continue to offer Run:ai’s products under the same business model for the immediate future. And NVIDIA will continue to invest in the Run:ai product roadmap as part of <a href="http://www.nvidia.com/dgx-cloud">NVIDIA DGX Cloud</a>, an AI platform co-engineered with leading clouds for enterprise developers, offering an integrated, full-stack service optimized for generative AI.</p>
<p>NVIDIA DGX and DGX Cloud customers will gain access to Run:ai’s capabilities for their AI workloads, particularly for large language model deployments. Run:ai’s solutions are already integrated with <a href="https://www.nvidia.com/en-us/data-center/dgx-platform/">NVIDIA DGX</a>, <a href="https://www.nvidia.com/en-us/data-center/dgx-superpod/">NVIDIA DGX SuperPOD</a>, <a href="https://www.nvidia.com/en-us/data-center/base-command/">NVIDIA Base Command</a>, <a href="https://www.nvidia.com/en-us/gpu-cloud/">NGC</a> containers, and <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software, among other products.</p>
<p>NVIDIA’s accelerated computing platform and Run:ai’s platform will continue to support a broad ecosystem of third-party solutions, giving customers choice and flexibility.</p>
<p>Together with Run:ai, NVIDIA will enable customers to have a single fabric that accesses GPU solutions anywhere. Customers can expect to benefit from better GPU utilization, improved management of GPU infrastructure and greater flexibility from the open architecture.</p>

		<!-- .entry-footer -->

	</div></div>
  </body>
</html>
