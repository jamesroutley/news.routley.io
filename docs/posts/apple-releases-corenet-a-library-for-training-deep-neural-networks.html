<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/apple/corenet">Original</a>
    <h1>Apple releases CoreNet, a library for training deep neural networks</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><a id="user-content-corenet-a-library-for-training-deep-neural-networks" aria-label="Permalink: CoreNet: A library for training deep neural networks" href="#corenet-a-library-for-training-deep-neural-networks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">CoreNet is a deep neural network toolkit that allows researchers and engineers to train standard and novel small and large-scale models for variety of tasks, including foundation models (e.g., CLIP and LLM), object classification, object detection, and semantic segmentation.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><a href="#whats-new">What&#39;s new?</a></li>
<li><a href="#research-efforts-at-apple-using-corenet">Research efforts at Apple using CoreNet</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#directory-structure">Directory Structure</a></li>
<li><a href="#maintainers">Maintainers</a></li>
<li><a href="#contributing-to-corenet">Contributing to CoreNet</a></li>
<li><a href="#license">License</a></li>
<li><a href="#relationship-with-cvnets">Relationship with CVNets</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">What&#39;s new?</h2><a id="user-content-whats-new" aria-label="Permalink: What&#39;s new?" href="#whats-new"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><em><strong>April 2024</strong></em>: Version 0.1.0 of the CoreNet library includes
<ul dir="auto">
<li>OpenELM</li>
<li>CatLIP</li>
<li>MLX examples</li>
</ul>
</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Research efforts at Apple using CoreNet</h2><a id="user-content-research-efforts-at-apple-using-corenet" aria-label="Permalink: Research efforts at Apple using CoreNet" href="#research-efforts-at-apple-using-corenet"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Below is the list of publications from Apple that uses CoreNet:</p>
<ul dir="auto">
<li><a href="https://arxiv.org/abs/2404.14619" rel="nofollow">OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework</a></li>
</ul>

<ul dir="auto">
<li><a href="https://blog.plover.com/apple/corenet/blob/main">CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data</a></li>
<li><a href="https://arxiv.org/abs/2303.08983" rel="nofollow">Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement</a></li>
<li><a href="https://arxiv.org/abs/2310.14108" rel="nofollow">CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement</a></li>
<li><a href="https://arxiv.org/abs/2303.14189" rel="nofollow">FastVit: A Fast Hybrid Vision Transformer using Structural Reparameterization</a></li>
<li><a href="https://arxiv.org/abs/2306.00238" rel="nofollow">Bytes Are All You Need: Transformers Operating Directly on File Bytes</a></li>
<li><a href="https://arxiv.org/abs/2206.04040" rel="nofollow">MobileOne: An Improved One millisecond Mobile Backbone</a></li>
<li><a href="https://arxiv.org/abs/2212.10553" rel="nofollow">RangeAugment: Efficient Online Augmentation with Range Learning</a></li>
<li><a href="https://arxiv.org/abs/2206.02680" rel="nofollow">Separable Self-attention for Mobile Vision Transformers (MobileViTv2)</a></li>
<li><a href="https://arxiv.org/abs/2206.02002" rel="nofollow">CVNets: High performance library for Computer Vision, ACM MM&#39;22</a></li>
<li><a href="https://arxiv.org/abs/2110.02178" rel="nofollow">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer, ICLR&#39;22</a></li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You will need Git LFS (instructions below) to run tests and Jupyter notebooks
(<a href="https://jupyter.org/install" rel="nofollow">instructions</a>) in this repository,
and to contribute to it so we recommend that you install and activate it first.</p>
<p dir="auto">On Linux we recommend to use Python 3.10+ and PyTorch (version &gt;= v2.1.0), on
macOS system Python 3.9+ should be sufficient.</p>
<p dir="auto">Note that the optional dependencies listed below are required if you&#39;d like to
make contributions and/or run tests.</p>
<p dir="auto">For Linux (substitute <code>apt</code> for your package manager):</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install git-lfs

git clone git@github.com:apple/corenet.git
cd corenet
git lfs install
git lfs pull
# The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.
python3 -m venv venv &amp;&amp; source venv/bin/activate
python3 -m pip install --editable ."><pre>sudo apt install git-lfs

git clone git@github.com:apple/corenet.git
<span>cd</span> corenet
git lfs install
git lfs pull
<span><span>#</span> The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.</span>
python3 -m venv venv <span>&amp;&amp;</span> <span>source</span> venv/bin/activate
python3 -m pip install --editable <span>.</span></pre></div>
<p dir="auto">To install optional dependencies for audio and video processing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install libsox-dev ffmpeg"><pre>sudo apt install libsox-dev ffmpeg</pre></div>
<p dir="auto">For macOS, assuming you use Homebrew:</p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install git-lfs

git clone git@github.com:apple/corenet.git
cd corenet
cd \$(pwd -P)  # See the note below.
git lfs install
git lfs pull
# The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.
python3 -m venv venv &amp;&amp; source venv/bin/activate
python3 -m pip install --editable ."><pre>brew install git-lfs

git clone git@github.com:apple/corenet.git
<span>cd</span> corenet
<span>cd</span> <span>\$</span>(pwd -P)  <span><span>#</span> See the note below.</span>
git lfs install
git lfs pull
<span><span>#</span> The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.</span>
python3 -m venv venv <span>&amp;&amp;</span> <span>source</span> venv/bin/activate
python3 -m pip install --editable <span>.</span></pre></div>
<p dir="auto">To install optional dependencies for audio and video processing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install sox ffmpeg"><pre>brew install sox ffmpeg</pre></div>
<p dir="auto">Note that on macOS the file system is case insensitive, and case sensitivity
can cause issues with Git. You should access the repository on disk as if the
path were case sensitive, i.e. with the same capitalization as you see when you
list the directories <code>ls</code>. You can switch to such a path with the <code>cd $(pwd -P)</code>
command.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Directory Structure</h2><a id="user-content-directory-structure" aria-label="Permalink: Directory Structure" href="#directory-structure"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This section provides quick access and a brief description for important CoreNet directories.</p>
<table>
<thead>
<tr>
<th> Description </th>
<th> Quick Access </th>
</tr>
</thead>
<tbody>

<tr> <td> <div dir="auto"><h3 tabindex="-1" dir="auto"> Getting Started </h3><a id="user-content--getting-started-" aria-label="Permalink:  Getting Started " href="#-getting-started-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div> 
Working with the examples is an easy way to get started with CoreNet. 
</td> <td> <pre>└── tutorials
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/tutorials/train_a_new_model_on_a_new_dataset_from_scratch.ipynb">train_a_new_model_on_a_new_dataset_from_scratch.ipynb</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/tutorials/guide_slurm_and_multi_node_training.md">guide_slurm_and_multi_node_training.md</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/tutorials/clip.ipynb">clip.ipynb</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/tutorials/semantic_segmentation.ipynb">semantic_segmentation.ipynb</a>
    └── <a href="https://blog.plover.com/apple/corenet/blob/main/tutorials/object_detection.ipynb">object_detection.ipynb</a>
</pre> </td> </tr>
<tr> <td> <div dir="auto"><h3 tabindex="-1" dir="auto"> Training Recipes </h3><a id="user-content--training-recipes-" aria-label="Permalink:  Training Recipes " href="#-training-recipes-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
CoreNet provides reproducible training recipes, in addition to the pretrained model 
weights and checkpoints for the publications that are listed in <code>projects/</code> directory.
<p dir="auto">Publication project directories generally contain the following contents:</p>
<ul dir="auto">
<li><code>README.md</code> provides documentation, links to the pretrained weights, and citations.</li>
<li><code>&lt;task_name&gt;/&lt;model_name&gt;.yaml</code> provides configuration for reproducing the trainings and evaluations.</li>
</ul>
</td> <td> <pre>└── projects
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/byteformer">byteformer</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/catlip">catlip</a> (*)
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/clip">clip</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/fastvit">fastvit</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/mobilenet_v1">mobilenet_v1</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/mobilenet_v2">mobilenet_v2</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/mobilenet_v3">mobilenet_v3</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/mobileone">mobileone</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/mobilevit">mobilevit</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/mobilevit_v2">mobilevit_v2</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/openelm">openelm</a> (*)
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/range_augment">range_augment</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/resnet">resnet</a>
    └── <a href="https://blog.plover.com/apple/corenet/blob/main/projects/vit">vit</a>
</pre> </td> </tr>
<tr> <td> <div dir="auto"><h3 tabindex="-1" dir="auto"> MLX Examples </h3><a id="user-content--mlx-examples-" aria-label="Permalink:  MLX Examples " href="#-mlx-examples-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
MLX examples demonstrate how to run CoreNet models efficiently on Apple Silicon.
Please find further information in the <code>README.md</code> file within the corresponding example directory.
</td> <td> <pre>└──mlx_example
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/mlx_examples/clip">clip</a>
    └── <a href="https://blog.plover.com/apple/corenet/blob/main/mlx_examples/open_elm">open_elm</a>
</pre> </td> </tr>
<tr> <td> <div dir="auto"><h3 tabindex="-1" dir="auto"> Model Implementations </h3><a id="user-content--model-implementations-" aria-label="Permalink:  Model Implementations " href="#-model-implementations-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div> 
Models are organized by tasks (e.g. &#34;classification&#34;). You can find all model implementations for each
task in the corresponding task folder. 
<p dir="auto">Each model class is decorated by a
<code>@MODEL_REGISTRY.register(name=&#34;&lt;model_name&gt;&#34;, type=&#34;&lt;task_name&gt;&#34;)</code> decorator.
To use a model class in CoreNet training or evaluation,
assign <code>moels.&lt;task_name&gt;.name = &lt;model_name&gt;</code> in the YAML configuration.</p>
</td> <td> <pre>└── corenet
    └── modeling
        └── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/models">models</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/models/audio_classification">audio_classification</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/models/classification">classification</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/models/detection">detection</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/models/language_modeling">language_modeling</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/models/multi_modal_img_text">multi_modal_img_text</a>
            └── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/models/segmentation">segmentation</a>
</pre> </td> </tr>
<tr> <td> <div dir="auto"><h3 tabindex="-1" dir="auto"> Datasets </h3><a id="user-content--datasets-" aria-label="Permalink:  Datasets " href="#-datasets-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div> 
Similarly to the models, datasets are also categorized by tasks.
</td> <td> <pre>└── corenet
    └── data
        └── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/datasets">datasets</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/datasets/audio_classification">audio_classification</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/datasets/classification">classification</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/datasets/detection">detection</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/datasets/language_modeling">language_modeling</a>
            ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/datasets/multi_modal_img_text">multi_modal_img_text</a>
            └── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/datasets/segmentation">segmentation</a>
</pre> </td> </tr>
<tr> <td> <div dir="auto"><h3 tabindex="-1" dir="auto"> Other key directories </h3><a id="user-content--other-key-directories-" aria-label="Permalink:  Other key directories " href="#-other-key-directories-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div> 
In this section, we have highlighted the rest of the key directories that implement 
classes corresponding to the names that are referenced in the YAML configurations.
</td> <td> <pre>└── corenet
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/loss_fn">loss_fn</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/metrics">metrics</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/optims">optims</a>
    │   └── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/optims/scheduler">scheduler</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/train_eval_pipelines">train_eval_pipelines</a>
    ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data">data</a>
    │   ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/collate_fns">collate_fns</a>
    │   ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/sampler">sampler</a>
    │   ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/text_tokenizer">text_tokenizer</a>
    │   ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/transforms">transforms</a>
    │   └── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/data/video_reader">video_reader</a>
    └── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling">modeling</a>
        ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/layers">layers</a>
        ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/modules">modules</a>
        ├── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/neural_augmentor">neural_augmentor</a>
        └── <a href="https://blog.plover.com/apple/corenet/blob/main/corenet/modeling/text_encoders">text_encoders</a>
</pre> </td> </tr>
</tbody>
</table>
<div dir="auto"><h2 tabindex="-1" dir="auto">Maintainers</h2><a id="user-content-maintainers" aria-label="Permalink: Maintainers" href="#maintainers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This code is developed by <a href="https://sacmehta.github.io" rel="nofollow">Sachin</a>, and is now maintained by Sachin, <a href="https://mchorton.com" rel="nofollow">Maxwell Horton</a>, <a href="https://www.mohammad.pro" rel="nofollow">Mohammad Sekhavat</a>, and Yanzi Jin.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Previous Maintainers</h3><a id="user-content-previous-maintainers" aria-label="Permalink: Previous Maintainers" href="#previous-maintainers"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><a href="https://farzadab.github.io" rel="nofollow">Farzad</a></li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Contributing to CoreNet</h2><a id="user-content-contributing-to-corenet" aria-label="Permalink: Contributing to CoreNet" href="#contributing-to-corenet"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We welcome PRs from the community! You can find information about contributing to CoreNet in our <a href="https://blog.plover.com/apple/corenet/blob/main/CONTRIBUTING.md">contributing</a> document.</p>
<p dir="auto">Please remember to follow our <a href="https://blog.plover.com/apple/corenet/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">For license details, see <a href="https://blog.plover.com/apple/corenet/blob/main/LICENSE">LICENSE</a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Relationship with CVNets</h2><a id="user-content-relationship-with-cvnets" aria-label="Permalink: Relationship with CVNets" href="#relationship-with-cvnets"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">CoreNet evolved from CVNets, to encompass a broader range of applications beyond computer vision. Its expansion facilitated the training of foundational models, including LLMs.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you find our work useful, please cite the following paper:</p>
<div data-snippet-clipboard-copy-content="@inproceedings{mehta2022cvnets, 
     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, 
     title = {CVNets: High Performance Library for Computer Vision}, 
     year = {2022}, 
     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, 
     series = {MM &#39;22} 
}"><pre><code>@inproceedings{mehta2022cvnets, 
     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, 
     title = {CVNets: High Performance Library for Computer Vision}, 
     year = {2022}, 
     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, 
     series = {MM &#39;22} 
}
</code></pre></div>
</article></div></div>
  </body>
</html>
