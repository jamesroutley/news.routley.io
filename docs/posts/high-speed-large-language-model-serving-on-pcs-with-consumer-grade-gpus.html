<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/SJTU-IPADS/PowerInfer">Original</a>
    <h1>High-Speed Large Language Model Serving on PCs with Consumer-Grade GPUs</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h2 tabindex="-1" dir="auto"><a id="user-content-tldr" aria-hidden="true" tabindex="-1" href="#tldr"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TL;DR</h2>
<p dir="auto">PowerInfer is a CPU/GPU LLM inference engine leveraging <strong>activation locality</strong> for your device.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-demo-" aria-hidden="true" tabindex="-1" href="#demo-"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Demo ðŸ”¥</h2>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description powerinfer-live-demo.mp4">powerinfer-live-demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/34213478/291527562-fe441a42-5fce-448b-a3e5-ea4abb43ba23.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDMxMjYwNTQsIm5iZiI6MTcwMzEyNTc1NCwicGF0aCI6Ii8zNDIxMzQ3OC8yOTE1Mjc1NjItZmU0NDFhNDItNWZjZS00NDhiLWEzZTUtZWE0YWJiNDNiYTIzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjIxVDAyMjkxNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdiMzJkYjZhNWZkYWIxNjYzMzg5YjE1NmM4YzllZDc4MGZmNWM2Mzk0MzBkZDU2MWM2N2UxZWMxYTU1ZTc5MTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.J0kWL8RpLAmm3wmqhHmBbr54Dj4WYEhS6_IPpZjH3fc" data-canonical-src="https://private-user-images.githubusercontent.com/34213478/291527562-fe441a42-5fce-448b-a3e5-ea4abb43ba23.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDMxMjYwNTQsIm5iZiI6MTcwMzEyNTc1NCwicGF0aCI6Ii8zNDIxMzQ3OC8yOTE1Mjc1NjItZmU0NDFhNDItNWZjZS00NDhiLWEzZTUtZWE0YWJiNDNiYTIzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjIxVDAyMjkxNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdiMzJkYjZhNWZkYWIxNjYzMzg5YjE1NmM4YzllZDc4MGZmNWM2Mzk0MzBkZDU2MWM2N2UxZWMxYTU1ZTc5MTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.J0kWL8RpLAmm3wmqhHmBbr54Dj4WYEhS6_IPpZjH3fc" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">PowerInfer v.s. llama.cpp on a single RTX 4090(24G) running Falcon(ReLU)-40B-FP16 with a 11x speedup!</p>
<p dir="auto"><sub>Both PowerInfer and llama.cpp were running on the same hardware and fully utilized VRAM on RTX 4090.</sub></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-abstract" aria-hidden="true" tabindex="-1" href="#abstract"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Abstract</h2>
<p dir="auto">We introduce PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC)
equipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high <strong>locality</strong>
inherent in LLM inference, characterized by a power-law distribution in neuron activation.</p>
<p dir="auto">This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated
across inputs, while the majority, cold neurons, vary based on specific inputs.
PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine:
hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed
on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers.
PowerInfer further integrates adaptive predictors and neuron-aware sparse operators,
optimizing the efficiency of neuron activation and computational sparsity.</p>
<p dir="auto">Evaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU,
only 18% lower than that achieved by a top-tier server-grade A100 GPU.
This significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-features" aria-hidden="true" tabindex="-1" href="#features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Features</h2>
<p dir="auto">PowerInfer is a high-speed and easy-to-use inference engine for deploying LLMs locally.</p>
<p dir="auto">PowerInfer is fast with:</p>
<ul dir="auto">
<li><strong>Locality-centric design</strong>: Utilizes sparse activation and &#39;hot&#39;/&#39;cold&#39; neuron concept for efficient LLM inference, ensuring high speed with lower resource demands.</li>
<li><strong>Hybrid CPU/GPU Utilization</strong>: Seamlessly integrates memory/computation capabilities of CPU and GPU for a balanced workload and faster processing.</li>
</ul>
<p dir="auto">PowerInfer is flexible and easy to use with:</p>
<ul dir="auto">
<li><strong>Easy Integration</strong>: Compatible with popular <a href="https://huggingface.co/SparseLLM" rel="nofollow">ReLU-sparse models</a>.</li>
<li><strong>Local Deployment Ease</strong>: Designed and deeply optimized for local deployment on consumer-grade hardware, enabling low-latency LLM inference and serving on a single GPU.</li>
<li><strong>Backward Compatibility</strong>: While distinct from llama.cpp, you can make use of most of <code>examples/</code> the same way as llama.cpp such as server and batched generation. PowerInfer also supports inference with llama.cpp&#39;s model weights for compatibility purposes, but there will be no performance gain.</li>
</ul>
<p dir="auto">You can use these models with PowerInfer today:</p>
<ul dir="auto">
<li>Falcon-40B</li>
<li>Llama2 family</li>
</ul>
<p dir="auto">We have tested PowerInfer on the following platforms:</p>
<ul dir="auto">
<li>x86-64 CPU (with AVX2 instructions) on Linux</li>
<li>x86-64 CPU and NVIDIA GPU on Linux</li>
<li>Apple M Chips on macOS (As we do not optimize for Mac, the performance improvement is not significant now.)</li>
</ul>
<p dir="auto">And new features coming soon:</p>
<ul dir="auto">
<li>Mistral-7B model</li>
<li>Metal backend for sparse inference on macOS</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" tabindex="-1" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h2>
<ul dir="auto">
<li><a href="#setup-and-installation">Installation</a></li>
<li><a href="#model-weights">Model Weights</a></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-setup-and-installation" aria-hidden="true" tabindex="-1" href="#setup-and-installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Setup and Installation</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-get-the-code" aria-hidden="true" tabindex="-1" href="#get-the-code"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Get the Code</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/SJTU-IPADS/PowerInfer
cd PowerInfer
pip install -r requirements.txt # install Python helpers&#39; dependencies"><pre>git clone https://github.com/SJTU-IPADS/PowerInfer
<span>cd</span> PowerInfer
pip install -r requirements.txt <span><span>#</span> install Python helpers&#39; dependencies</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-build" aria-hidden="true" tabindex="-1" href="#build"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Build</h3>
<p dir="auto">In order to build PowerInfer you have two different options. These commands are supposed to be run from the root directory of the project.</p>
<p dir="auto">Using <code>CMake</code>(3.13+) on Linux or macOS:</p>
<ul dir="auto">
<li>If you have an NVIDIA GPU:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -S . -B build -DLLAMA_CUBLAS=ON
cmake --build build --config Release"><pre>cmake -S <span>.</span> -B build -DLLAMA_CUBLAS=ON
cmake --build build --config Release</pre></div>
<ul dir="auto">
<li>If you just CPU:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="cmake -S . -B build
cmake --build build --config Release"><pre>cmake -S <span>.</span> -B build
cmake --build build --config Release</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-model-weights" aria-hidden="true" tabindex="-1" href="#model-weights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Model Weights</h2>
<p dir="auto">PowerInfer models are stored in a special format called <em>PowerInfer GGUF</em> based on GGUF format, consisting of both LLM weights and predictor weights.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-download-powerinfer-gguf-via-hugging-face" aria-hidden="true" tabindex="-1" href="#download-powerinfer-gguf-via-hugging-face"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Download PowerInfer GGUF via Hugging Face</h3>
<p dir="auto">You can obtain PowerInfer GGUF weights at <code>*.powerinfer.gguf</code> as well as profiled model activation statistics for &#39;hot&#39;-neuron offloading from each Hugging Face repo below.</p>
<table>
<thead>
<tr>
<th>Base Model</th>
<th>PowerInfer GGUF</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA(ReLU)-2-7B</td>
<td><a href="https://huggingface.co/PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF" rel="nofollow">PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF</a></td>
</tr>
<tr>
<td>LLaMA(ReLU)-2-13B</td>
<td><a href="https://huggingface.co/PowerInfer/ReluLLaMA-13B-PowerInfer-GGUF" rel="nofollow">PowerInfer/ReluLLaMA-13B-PowerInfer-GGUF</a></td>
</tr>
<tr>
<td>Falcon(ReLU)-40B</td>
<td><a href="https://huggingface.co/PowerInfer/ReluFalcon-40B-PowerInfer-GGUF" rel="nofollow">PowerInfer/ReluFalcon-40B-PowerInfer-GGUF</a></td>
</tr>
<tr>
<td>LLaMA(ReLU)-2-70B</td>
<td><a href="https://huggingface.co/PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF" rel="nofollow">PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF</a></td>
</tr>
</tbody>
</table>
<p dir="auto">We suggest downloading/cloning the whole repo so PowerInfer can automatically make use of such directory structure for feature-complete model offloading:</p>
<div data-snippet-clipboard-copy-content=".
â”œâ”€â”€ *.powerinfer.gguf (Unquantized PowerInfer model)
â”œâ”€â”€ *.q4.powerinfer.gguf (INT4 quantized PowerInfer model, if available)
â”œâ”€â”€ activation (Profiled activation statistics for fine-grained FFN offloading)
â”‚   â”œâ”€â”€ activation_x.pt (Profiled activation statistics for layer x)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ *.[q4].powerinfer.gguf.generated.gpuidx (Generated GPU index at runtime for corresponding model)"><pre><code>.
â”œâ”€â”€ *.powerinfer.gguf (Unquantized PowerInfer model)
â”œâ”€â”€ *.q4.powerinfer.gguf (INT4 quantized PowerInfer model, if available)
â”œâ”€â”€ activation (Profiled activation statistics for fine-grained FFN offloading)
â”‚   â”œâ”€â”€ activation_x.pt (Profiled activation statistics for layer x)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ *.[q4].powerinfer.gguf.generated.gpuidx (Generated GPU index at runtime for corresponding model)
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-convert-from-original-model-weights--predictor-weights" aria-hidden="true" tabindex="-1" href="#convert-from-original-model-weights--predictor-weights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Convert from Original Model Weights + Predictor Weights</h3>
<p dir="auto">Hugging Face limits single model weight to 50GiB. For unquantized models &gt;= 40B, you can convert PowerInfer GGUF from the original model weights and predictor weights obtained from Hugging Face.</p>
<table>
<thead>
<tr>
<th>Base Model</th>
<th>Original Model</th>
<th>Predictor</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA(ReLU)-2-7B</td>
<td><a href="https://huggingface.co/SparseLLM/ReluLLaMA-7B" rel="nofollow">SparseLLM/ReluLLaMA-7B</a></td>
<td><a href="https://huggingface.co/PowerInfer/ReluLLaMA-7B-Predictor" rel="nofollow">PowerInfer/ReluLLaMA-7B-Predictor</a></td>
</tr>
<tr>
<td>LLaMA(ReLU)-2-13B</td>
<td><a href="https://huggingface.co/SparseLLM/ReluLLaMA-13B" rel="nofollow">SparseLLM/ReluLLaMA-13B</a></td>
<td><a href="https://huggingface.co/PowerInfer/ReluLLaMA-13B-Predictor" rel="nofollow">PowerInfer/ReluLLaMA-13B-Predictor</a></td>
</tr>
<tr>
<td>Falcon(ReLU)-40B</td>
<td><a href="https://huggingface.co/SparseLLM/ReluFalcon-40B" rel="nofollow">SparseLLM/ReluFalcon-40B</a></td>
<td><a href="https://huggingface.co/PowerInfer/ReluFalcon-40B-Predictor" rel="nofollow">PowerInfer/ReluFalcon-40B-Predictor</a></td>
</tr>
<tr>
<td>LLaMA(ReLU)-2-70B</td>
<td><a href="https://huggingface.co/SparseLLM/ReluLLaMA-70B" rel="nofollow">SparseLLM/ReluLLaMA-70B</a></td>
<td><a href="https://huggingface.co/PowerInfer/ReluLLaMA-70B-Predictor" rel="nofollow">PowerInfer/ReluLLaMA-70B-Predictor</a></td>
</tr>
</tbody>
</table>
<p dir="auto">You can use the following command to convert the original model weights and predictor weights to PowerInfer GGUF:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# make sure that you have done `pip install -r requirements.txt`
python convert.py --outfile /PATH/TO/POWERINFER/GGUF/REPO/MODELNAME.powerinfer.gguf /PATH/TO/ORIGINAL/MODEL /PATH/TO/PREDICTOR
# python convert.py --outfile ./ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.powerinfer.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor"><pre><span><span>#</span> make sure that you have done `pip install -r requirements.txt`</span>
python convert.py --outfile /PATH/TO/POWERINFER/GGUF/REPO/MODELNAME.powerinfer.gguf /PATH/TO/ORIGINAL/MODEL /PATH/TO/PREDICTOR
<span><span>#</span> python convert.py --outfile ./ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.powerinfer.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor</span></pre></div>
<p dir="auto">For the same reason, we suggest keeping the same directory structure as PowerInfer GGUF repos after conversion.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-inference" aria-hidden="true" tabindex="-1" href="#inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference</h2>
<p dir="auto">For CPU-only and CPU-GPU hybrid inference with all available VRAM, you can use the following instructions to run PowerInfer:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt
# ./build/bin/main -m ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf -n 128 -t 8 -p &#34;Once upon a time&#34;"><pre>./build/bin/main -m /PATH/TO/MODEL -n <span>$output_token_count</span> -t <span>$thread_num</span> -p <span>$prompt</span>
<span><span>#</span> ./build/bin/main -m ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf -n 128 -t 8 -p &#34;Once upon a time&#34;</span></pre></div>
<p dir="auto">If you want to limit the VRAM usage of GPU:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt --vram-budget $vram_gb
# ./build/bin/main -m ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf -n 128 -t 8 -p &#34;Once upon a time&#34; --vram-budget 8"><pre>./build/bin/main -m /PATH/TO/MODEL -n <span>$output_token_count</span> -t <span>$thread_num</span> -p <span>$prompt</span> --vram-budget <span>$vram_gb</span>
<span><span>#</span> ./build/bin/main -m ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf -n 128 -t 8 -p &#34;Once upon a time&#34; --vram-budget 8</span></pre></div>
<p dir="auto">Under CPU-GPU hybrid inference, PowerInfer will automatically offload all dense activation blocks to GPU and split FFN on GPU if possible.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-quantization" aria-hidden="true" tabindex="-1" href="#quantization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quantization</h2>
<p dir="auto">PowerInfer has optimized quantization support for INT4(<code>Q4_0</code>) models. You can use the following instructions to quantize PowerInfer GGUF model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./build/bin/quantize /PATH/TO/MODEL /PATH/TO/OUTPUT/QUANTIZED/MODEL Q4_0
# ./build/bin/quantize ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.powerinfer.gguf ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf Q4_0"><pre>./build/bin/quantize /PATH/TO/MODEL /PATH/TO/OUTPUT/QUANTIZED/MODEL Q4_0
<span><span>#</span> ./build/bin/quantize ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.powerinfer.gguf ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf Q4_0</span></pre></div>
<p dir="auto">Then you can use the quantized model for inference with PowerInfer with the same instructions as above.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-evaluation" aria-hidden="true" tabindex="-1" href="#evaluation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Evaluation</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/34213478/290874852-d700fa6c-77ba-462f-a2fc-3fd21c898f33.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDMxMjYwNTQsIm5iZiI6MTcwMzEyNTc1NCwicGF0aCI6Ii8zNDIxMzQ3OC8yOTA4NzQ4NTItZDcwMGZhNmMtNzdiYS00NjJmLWEyZmMtM2ZkMjFjODk4ZjMzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjIxVDAyMjkxNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY4OGZlNWI4ZjFlNzZiMzI1YmFkNjA0MmM0NmY0OWRjNWQzOWQwN2Y4YmU1YzdmZWY1MzgzZTM2Nzk5OWJmNDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.vQAF4EFwrA0MPWIzIXRga-4kam9m6gVF018tZ1Hai2w"><img src="https://private-user-images.githubusercontent.com/34213478/290874852-d700fa6c-77ba-462f-a2fc-3fd21c898f33.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDMxMjYwNTQsIm5iZiI6MTcwMzEyNTc1NCwicGF0aCI6Ii8zNDIxMzQ3OC8yOTA4NzQ4NTItZDcwMGZhNmMtNzdiYS00NjJmLWEyZmMtM2ZkMjFjODk4ZjMzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjIxVDAyMjkxNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY4OGZlNWI4ZjFlNzZiMzI1YmFkNjA0MmM0NmY0OWRjNWQzOWQwN2Y4YmU1YzdmZWY1MzgzZTM2Nzk5OWJmNDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.vQAF4EFwrA0MPWIzIXRga-4kam9m6gVF018tZ1Hai2w" alt="github-eval-4090"/></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/34213478/290875021-0fc1bfc4-aafc-4e82-a865-bec0143aff1a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDMxMjYwNTQsIm5iZiI6MTcwMzEyNTc1NCwicGF0aCI6Ii8zNDIxMzQ3OC8yOTA4NzUwMjEtMGZjMWJmYzQtYWFmYy00ZTgyLWE4NjUtYmVjMDE0M2FmZjFhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjIxVDAyMjkxNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQxMzA5YTcwNzNhYTQ1MmM3N2Q5ZDllZTlmMjM5MjcwY2UwMGExZGRhNzkzOGU3OGFhMWRlOTY1MzhjMGQ1NDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.w5BXA2H0-j-VMV6PGCeyaBZUvELIBkxgAS09xUFDWgc"><img src="https://private-user-images.githubusercontent.com/34213478/290875021-0fc1bfc4-aafc-4e82-a865-bec0143aff1a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDMxMjYwNTQsIm5iZiI6MTcwMzEyNTc1NCwicGF0aCI6Ii8zNDIxMzQ3OC8yOTA4NzUwMjEtMGZjMWJmYzQtYWFmYy00ZTgyLWE4NjUtYmVjMDE0M2FmZjFhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjIxVDAyMjkxNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQxMzA5YTcwNzNhYTQ1MmM3N2Q5ZDllZTlmMjM5MjcwY2UwMGExZGRhNzkzOGU3OGFhMWRlOTY1MzhjMGQ1NDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.w5BXA2H0-j-VMV6PGCeyaBZUvELIBkxgAS09xUFDWgc" alt="github-eval-2080ti-q4"/></a></p>
<p dir="auto">PowerInfer achieves up to 11x and 8x speedup for FP16 and INT4 models!</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-faqs" aria-hidden="true" tabindex="-1" href="#faqs"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>FAQs</h2>
<ol dir="auto">
<li>What if I encountered <code>CUDA_ERROR_OUT_OF_MEMORY</code>?
<ul dir="auto">
<li>You can try to run with <code>--reset-gpu-index</code> argument to rebuild the GPU index for this model to avoid any stale cache.</li>
<li>Due to our current implementation, model offloading might not be as accurate as expected. You can try with <code>--vram-budget</code> with a slightly lower value or <code>--disable-gpu-index</code> to disable FFN offloading.</li>
</ul>
</li>
<li>What if...
<ul dir="auto">
<li>Issues are welcomed! Please feel free to open an issue and attach your running environment and running parameters. We will try our best to help you.</li>
</ul>
</li>
</ol>
<h2 tabindex="-1" dir="auto"><a id="user-content-todos" aria-hidden="true" tabindex="-1" href="#todos"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TODOs</h2>
<p dir="auto">We will release the code and data in the following order, please stay tuned!</p>
<ul>
<li> Release core code of PowerInfer, supporting Llama-2, Falcon-40B.</li>
<li> Support Mistral-7B</li>
<li> Support Windows</li>
<li> Support text-generation-webui</li>
<li> Release perplexity evaluation code</li>
<li> Support Metal for Mac</li>
<li> Release code for OPT models</li>
<li> Release predictor training code</li>
<li> Support online split for FFN network</li>
<li> Support Multi-GPU</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-paper-and-citation" aria-hidden="true" tabindex="-1" href="#paper-and-citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Paper and Citation</h2>
<p dir="auto">More technical details can be found in our <a href="https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf" rel="nofollow">paper</a>.</p>
<p dir="auto">If you find PowerInfer useful or relevant to your project and research, please kindly cite our paper:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@techreport{song2023powerinfer,
  author      = {Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},
  title       = {PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU},
  institution = {Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University},
  year        = {2023}
}"><pre><span>@techreport</span>{<span>song2023powerinfer</span>,
  <span>author</span>      = <span><span>{</span>Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen<span>}</span></span>,
  <span>title</span>       = <span><span>{</span>PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU<span>}</span></span>,
  <span>institution</span> = <span><span>{</span>Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University<span>}</span></span>,
  <span>year</span>        = <span><span>{</span>2023<span>}</span></span>
}</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgement" aria-hidden="true" tabindex="-1" href="#acknowledgement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgement</h2>
<p dir="auto">We are thankful for the easily modifiable operator library <a href="https://github.com/ggerganov/ggml">ggml</a> and execution runtime provided by <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. We also extend our gratitude to <a href="https://nlp.csai.tsinghua.edu.cn/" rel="nofollow">THUNLP</a> for their support of ReLU-based sparse models. We also appreciate the research of <a href="https://proceedings.mlr.press/v202/liu23am.html" rel="nofollow">Deja Vu</a>, which inspires PowerInfer.</p>
</article>
          </div></div>
  </body>
</html>
