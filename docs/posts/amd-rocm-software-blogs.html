<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rocm.blogs.amd.com/">Original</a>
    <h1>AMD ROCm Software Blogs</h1>
    
    <div id="readability-page-1" class="page"><article role="main">
                  
  
  <meta charset="UTF-8"/>
  <meta name="description" content="AMD ROCmâ„¢ software blogs"/>
  <meta name="keywords" content="AMD GPU, MI300, MI250, ROCm, blog"/>

<section id="amd-rocm-software-blogs">

<div>
<div>
<div>
<div>
<div>
<p>
Stable diffusion (AITemplate)</p>
<p>Efficient image generation with Stable Diffusion models and AITemplate using AMD GPUs</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
Fine-tune Llama 2 with LoRA</p>
<p>Fine-tune Llama 2 with LoRA: Customizing a large language model for question-answering</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
LLMs and Text Generation Inference</p>
<p>Efficient deployment of LLMs with Text Generation Inference</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
LoRA &amp; efficient fine-tuning</p>
<p>Using LoRA for efficient fine-tuning: fundamental principles</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
LLM fine-tuning with JAX</p>
<p>LLM distributed supervised fine-tuning with JAX</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
Accelerating XGBoost</p>
<p>Accelerating XGBoost using Multiple AMD GPUs</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
PyTorch Lightning</p>
<p>PyTorch Lightning on AMD GPUs</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
LLM training with Megatron-DeepSpeed</p>
<p>Pre-training a large language model with Megatron-DeepSpeed on multiple AMD GPUs</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
Pre-training BERT (PyTorch)</p>
<p>Pre-training BERT using Hugging Face &amp; PyTorch on multiple AMD GPUs</p>
</div>
</div>
</div>
<div>
<div>
<div>
<p>
Pre-training BERT (TensorFlow)</p>
<p>Pre-training BERT using Hugging Face &amp; TensorFlow on a single AMD GPU</p>
</div>
</div>
</div>
</div>
</div>


</section>


                </article></div>
  </body>
</html>
