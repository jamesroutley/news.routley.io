<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://styledrop.github.io/">Original</a>
    <h1>StyleDrop: Text-to-Image Generation in Any Style</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
      <p>
        We present <i>StyleDrop</i> that enables the generation of images that faithfully follow a specific style,
        powered by <a href="https://muse-model.github.io/">Muse</a>, a text-to-image generative
        vision transformer.
        <i>StyleDrop</i> is extremely versatile and captures nuances and details of a user-provided style, such as color
        schemes, shading, design patterns, and local and global effects. <i>StyleDrop</i> works by efficiently learning
        a new style by fine-tuning very few trainable parameters (less than 1% of total model parameters), and improving
        the quality via iterative training with either human or automated feedback. Better yet, <i>StyleDrop</i> is able
        to deliver impressive results even when the user supplies only a <i>single</i> image specifying the desired
        style. An extensive study shows that, for the task of style tuning text-to-image models, Styledrop on <a href="https://muse-model.github.io/">Muse</a> convincingly outperforms other methods,
        including <a href="https://dreambooth.github.io/">DreamBooth</a> and <a href="https://textual-inversion.github.io/">Textual
          Inversion</a> on <a href="https://imagen.research.google/">Imagen</a> or <a href="https://stability.ai/stable-diffusion">Stable Diffusion</a>.
      </p>
      <!-- anon-on -->
      <!-- anon-off -->
    </div>
  </div></div>
  </body>
</html>
