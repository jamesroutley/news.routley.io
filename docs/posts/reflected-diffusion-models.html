<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aaronlou.com/blog/2023/reflected-diffusion/">Original</a>
    <h1>Reflected Diffusion Models</h1>
    
    <div id="readability-page-1" class="page"><div> <d-contents> <nav> <h3>Contents</h3>    <ul> <li><a href="#score-matching-on-bounded-domains">Score Matching on Bounded Domains</a></li> <li><a href="#how-to-solve-reverse-reflected-sdes">How to Solve Reverse Reflected SDEs</a></li> <li><a href="#probability-flow-ode">Probability Flow ODE</a></li> <li><a href="#diffusion-guidance">Diffusion Guidance</a></li> <li><a href="#generalizing-to-different-geometries">Generalizing to Different Geometries</a></li> </ul>  </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Diffusion models <d-cite key="SohlDickstein2015DeepUL, Ho2020DenoisingDP, Song2020ScoreBasedGM"></d-cite> are a new class of generative models which have quickly supplanted traditional models like GANs, VAEs, and Normalizing Flows in many domains such as image, language, and molecule generation. They have also been the driving force behind several famous text-to-image generation systems like DALLE-2, Imagen, and Stable Diffusion <d-cite key="Ramesh2022HierarchicalTI, Saharia2022PhotorealisticTD, Rombach2021HighResolutionIS"></d-cite>.</p> <p>At their core, diffusion models start by perturbing data on $\mathbb{R}^d$ with a hand-designed “forward” stochastic differential equation (SDE) with fixed coefficients $\mathbf{f}$ and $g$ <d-cite key="Song2020ScoreBasedGM"></d-cite>:</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/perturb_vp.gif-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/perturb_vp.gif-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/perturb_vp.gif-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/perturb_vp.gif" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> Forward process. Image courtesy of <a href="https://yang-song.net/" rel="external nofollow noopener" target="_blank">Yang Song</a>. </p> <p>\(\begin{equation} \mathrm{d} \mathbf{x}_t = \mathbf{f}(\mathbf{x}_t, t) \mathrm{d}t + g(t) \mathrm{d} \mathbf{B}_t \end{equation}\)</p> <p>taking our initial <strong>data distribution</strong> $p_0$ to a <strong>stationary distribution</strong> $p_T$ (normally a simple distribution like a Gaussian). Using time reversed Brownian motion $\overline{\mathbf{B}}_t$ and the <strong>score function</strong> $\nabla_x \log p_t$, one can construct a corresponding “reverse” SDE which takes $p_T$ to $p_0$:</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/denoise_vp.gif-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/denoise_vp.gif-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/denoise_vp.gif-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/denoise_vp.gif" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> Reverse process. Image courtesy of <a href="https://yang-song.net/" rel="external nofollow noopener" target="_blank">Yang Song</a>. </p> <p>\(\begin{equation} \mathrm{d} \mathbf{x}_t = \left[\mathbf{f}(\mathbf{x}_t, t) - g(t)^2 \nabla_x \log p_t(\mathbf{x}_t) \right]\mathrm{d}t + g(t) \mathrm{d} \overline{\mathbf{B}}_t \end{equation}\)</p> <p>This defines a stochastic transport from our simple distribution $p_T$ to our data distribution $p_0$, so we can build a generative model by approximating this process.</p> <p>The only unknown component is the score function $\nabla_x \log p_t$, which can be learned with a <strong>time-dependent score neural network</strong> $\mathbf{s}_\theta(\mathbf{x}, t)$. To train, we optimize what is known as the <strong>score matching loss</strong> <d-cite key="Hyvrinen2005EstimationON, Vincent2011ACB, Song2019GenerativeMB"></d-cite>:</p> \[\begin{equation} \mathbb{E}_{t \in [0, T]} \mathbb{E}_{\mathbf{x} \sim p_t} \ \lambda_t \| \mathbf{s}_\theta(\mathbf{x}_t, t) - \nabla_x \log p_t(\mathbf{x}_t)\|^2 \end{equation}\] <p>which has several equivalent (but tractable) alternative forms (see Yang’s excellent <a href="https://yang-song.net/blog/2021/score/" rel="external nofollow noopener" target="_blank">blog post</a> for an overview). Here, $\lambda_t$ is a weighting function that alters the final models behavior. For example, setting $\lambda_t = g(t)^2$ maximizes the log-likelihood of the generated data <d-cite key="Song2021MaximumLT"></d-cite>, and setting $\lambda_t = 1 /\mathbb{E}|\nabla_x \log p_t(x_t \vert x_0)|^2$ improves image quality. Once we learn $\mathbf{s}_\theta$, we can generate samples from our diffusion model by first sampling $\mathbf{x}_T \sim p_T$ and solving the reverse SDE from $T$ to $0$:</p> \[\begin{equation} \mathrm{d} \mathbf{x}_t = \left[\mathbf{f}(\mathbf{x}_t, t) - g(t)^2 s_\theta(\mathbf{x}_t, t) \right]\mathrm{d}t + g(t) \mathrm{d} \overline{\mathbf{B}}_t \end{equation}\] <h2 id="thresholding">Thresholding</h2> <p>Unfortunately, the devil always lies in the details. We need to discretize time to simulate the generative SDE, resulting in an <strong>Euler-Maruyama scheme</strong> with i.i.d. Brownian increments $\mathbf{B}_{\Delta t}^t \sim \mathcal{N}(0, \Delta t)$:</p> \[\begin{equation} \mathbf{x}_{t - \Delta t} = \mathbf{x}_{t} - \left[\mathbf{f}(\mathbf{x}_t, t) - g(t)^2 s_\theta(\mathbf{x}_t, t) \right] \Delta t + g(t) \mathbf{B}_{\Delta t}^t \end{equation}\] <p>Numerical error can arise from the discretization, the learned score function, or just plain bad luck when sampling the increments, causing our trajectory $\mathbf{x}_t$ to enter <strong>low probability regions</strong>. Since we train with the Monte Carlo version of our loss</p> \[\begin{equation} \frac{1}{nm} \sum_{i = 1}^{n} \sum_{j = 1}^m \ \lambda_{t_i} \| \mathbf{s}_\theta(\mathbf{x}_{t_i}^j, t_i) - \nabla_x \log p_{t_i}(\mathbf{x}_{t_i}^j)\|^2 \end{equation}\] <p>it is even possible to never optimize in low probability regions, so the score network behavior there tends to be <strong>undefined</strong>. Previously, this problem appeared for vanilla score matching, degrading the performance of naive Langevin dynamics<d-cite key="Song2019GenerativeMB"></d-cite>. For diffusion models, this problem again resurfaces, this time as a <strong>compounding error</strong> in the sampling process. For a lot of cases, this causes samples to <strong>drastically diverge</strong>, leading to obviously wrong blank images:</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/guided_diffusion_no_clip-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/guided_diffusion_no_clip-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/guided_diffusion_no_clip-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/guided_diffusion_no_clip.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div>  <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/imagen_no_clip-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/imagen_no_clip-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/imagen_no_clip-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/imagen_no_clip.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <div><p> Divergent samples originally reported in <a href="https://arxiv.org/abs/2205.11487" rel="external nofollow noopener" target="_blank">Imagen</a></p><d-cite key="Saharia2022PhotorealisticTD"></d-cite><p>. &#34;An astronaut riding a horse&#34;. </p></div> <p>Diffusion models were initially motivated as a stack of VAEs which gradually denoised the input<d-cite key="SohlDickstein2015DeepUL"></d-cite>. The Euler-Maruyama step can be decomposed based on this perspective:</p> \[\begin{equation} \mathbf{x}_{t - \Delta t} = \underbrace{\mathbf{x}_{t} - \left[\mathbf{f}(\mathbf{x}_t, t) - g(t)^2 s_\theta(\mathbf{x}_t, t) \right] \Delta t}_{\text{VAE Predicted Mean } \overline{\mathbf{x}}_{t - \Delta t}} + \underbrace{g(t) \mathbf{B}_{\Delta t}^t}_{\text{VAE Noise}} \end{equation}\] <p>inspiring a natural fix. We seek to generate images, so the predicted mean $\overline{\mathbf{x}}_{t - \Delta t}$ should be a “valid” image. This can be accomplished by clipping each pixel to the fixed $[0, 255]$ range (which can be rescaled to $[0, 1]$ or $[-1, 1]$ depending on the context), and this trick is known as <strong>thresholding</strong>. You can find many examples of it in the repositories of <strong>famous papers</strong>, although it is almost never mentioned:</p>     <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/guided_diffusion_clip-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/guided_diffusion_clip-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/guided_diffusion_clip-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/guided_diffusion_clip.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div>  <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/imagen_clip-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/imagen_clip-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/imagen_clip-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/imagen_clip.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <div><p> Thresholded samples originally reported in <a href="https://arxiv.org/abs/2205.11487" rel="external nofollow noopener" target="_blank">Imagen</a></p><d-cite key="Saharia2022PhotorealisticTD"></d-cite><p>. &#34;An astronaut riding a horse&#34;. These are quite saturated. </p></div> <p>Unfortunately, diffusion models are supposed to reverse the forward corrupting process. Changing the generative process like this breaks the fundamental assumption, resulting in a mismatch. This has been linked with phenomena like oversaturation when using a large amount of diffusion guidance (such as in the Imagen generated example), necessitating task-specific techniques that don’t generalize, such as <strong>dynamic thresholding</strong></p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/imagen_dyn_clip-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/imagen_dyn_clip-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/imagen_dyn_clip-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/imagen_dyn_clip.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <div><p> Dynamically thresholded samples originally reported in <a href="https://arxiv.org/abs/2205.11487" rel="external nofollow noopener" target="_blank">Imagen</a></p><d-cite key="Saharia2022PhotorealisticTD"></d-cite><p>. &#34;An astronaut riding a horse&#34;. </p></div> <h2 id="reflected-diffusion-models">Reflected Diffusion Models</h2> <p>As we take $\Delta t \to 0$, the thresholded Euler Maruyama scheme</p> \[\begin{equation} \mathbf{x}_{t + \Delta t} = \mathrm{proj}\left(\mathbf{x}_{t} + \mathbf{f}(\mathbf{x}_t, t) \Delta t\right) + g(t) \mathbf{B}_{\Delta t}^t \end{equation}\] <p>converges to what’s known as a <strong>reflected stochastic differential equation</strong><d-cite key="Pilipenko2014AnIT"></d-cite>:</p> \[\begin{equation} \mathrm{d} \mathbf{x}_t = \mathbf{f}(\mathbf{x}_t, t) \mathrm{d}t + g(t) \mathrm{d} \mathbf{B}_t + \mathrm{d} \mathbf{L}_t \end{equation}\] <p>The behavior is exactly the same on the interior of our domain, but, at the boundary, the new term $\mathbf{L}_t$ “zeroes out” all outward pointing force to ensure the particle stays within the constraints.</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/rbm.gif-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/rbm.gif-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/rbm.gif-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/rbm.gif" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> Reflected Brownian Motion, the canonical example of a reflected SDE. The process will never go below 0. </p> <p>Similar to standard stochastic differential equations, it is possible to reverse this with a <strong>reverse reflected stochastic differential equation</strong></p> \[\begin{equation} \mathrm{d} \mathbf{x}_t = \left[\mathbf{f}(\mathbf{x}_t, t) - g(t)^2 \nabla_x \log p_t \right]\mathrm{d}t + g(t) \mathrm{d} \overline{\mathbf{B}}_t + \mathrm{d} \overline{\mathbf{L}}_t \end{equation}\] <p>This forms the same forward/reverse coupling that undergirds standard diffusion models, so we can use this principle to define <strong>Reflected Diffusion Models</strong>.</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/reflected_diffusion_main-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/reflected_diffusion_main-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/reflected_diffusion_main-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/reflected_diffusion_main.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> An overview of reflected diffusion models. We learn to reverse a reflected stochastic differential equation. </p> <h3 id="score-matching-on-bounded-domains">Score Matching on Bounded Domains</h3> <p>To construct our Reflected Diffusion Models, we need to learn $\nabla_x \log p_t$. These are the marginal scores recovered from the forward reflected SDE, <strong>not</strong> the standard forward SDE. The score matching loss can be made more tractable by employing the <strong>denoising trick</strong> from standard diffusion models, reducing our loss to a (weighted combination) of denoising score matching losses for each $p_t$</p> \[\begin{equation} \mathbb{E}_{\mathbf{x} \sim p_0} \mathbb{E}_{\mathbf{x}_t \sim p_t(\cdot \vert x_0)} \| \mathbf{s}_\theta(\mathbf{x}_t, t) - \nabla_x \log p_t(\mathbf{x}_t | \mathbf{x}_0)\|^2 \end{equation}\] <p>One still needs to accurately compute the <strong>transition density</strong> $p_t(\mathbf{x}_t \vert \mathbf{x}_0)$ quickly, which is not available in closed form. In particular, $p_t(\mathbf{x}_t \vert \mathbf{x}_0)$ is actually a <strong>reflected Gaussian variable</strong>, and this leads to two natural computation strategies:</p> <div> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/method_reflection-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/method_reflection-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/method_reflection-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/method_reflection.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> <p> Strategy 1: sum up all of the reflected components of the Gaussian. The two Gaussian distributions (grey) sum up to the reflected probability (blue). </p> </div> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/method_eigenfunction-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/method_eigenfunction-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/method_eigenfunction-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/method_eigenfunction.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> <p> Strategy 2: decompose the distribution using harmonic analysis. The harmonic components (red) sum up to the reflected probability (blue). </p> </div> </div> <p>Strategy 1 is accurate for small times since we don’t need to compute as many reflections, while strategy 2 is accurate for large times since the distribution goes closer to uniform, requiring fewer harmonic components. These strategies shore up the other’s weaknesses, so we <strong>combine</strong> them to efficiently compute the transition density.</p> <h3 id="how-to-solve-reverse-reflected-sdes">How to Solve Reverse Reflected SDEs</h3> <p>We have already seen that thresholding provides a Euler-Maruyama type discretization. The core idea is that it approximates $\mathbf{L}_t$ in discrete time. However, we are by no means limited to just thresholding. We found that approximating the process with a reflection term produced better samples:</p> \[\begin{equation} \mathbf{x}_{t - \Delta t} = \mathrm{refl}\left(\mathbf{x}_{t} - \left[\mathbf{f}(\mathbf{x}_t, t) - g(t)^2 \mathbf{s}_\theta(\mathbf{x}_t, t) \right] \Delta t + g(t) \mathbf{B}_{\Delta t}^t\right) \end{equation}\] <p>This produces reasonable samples, but we can actually further augment the sampling procedure. Since $\mathbf{x}_t \sim p_t$, we can use our score function to define a <strong>predictor-corrector</strong> update scheme based on <strong>Constrained Langevin Dynamics</strong> to “correct” our sample $\mathbf{x}_t$:</p> \[\begin{equation} \mathrm{d} \mathbf{x}_t = \frac{1}{2} s_\theta(\mathbf{x}_t, t) \mathrm{d} t + \mathrm{d} \mathbf{B}_t + \mathbf{L}_t \end{equation}\] <p>With this component, we match all of the constructs from standard diffusion models. We can achieve <strong>state-of-the-art</strong> perceptual quality (as measured by Inception score) without modifying the architecture or any other components. Unfortunately, the FID score, another common metric, tends to lag behind because our generated samples have noise (at the scale of 1-2 pixels) that FID is notoriously sentitive to.</p> <table> <thead> <tr> <th>Method</th> <th>Inception score (↑)</th> </tr> </thead> <tbody> <tr> <td>NCSN++<d-cite key="Song2020ScoreBasedGM"></d-cite> </td> <td>9.89</td> </tr> <tr> <td>Subspace Diffusion<d-cite key="Jing2022SubspaceDG"></d-cite> </td> <td>9.99</td> </tr> <tr> <td><strong>Ours</strong></td> <td><strong>10.42</strong></td> </tr> </tbody> </table> <h3 id="probability-flow-ode">Probability Flow ODE</h3> <p>Analogous to the widely used DDIM scheme<d-cite key="Song2020DenoisingDI"></d-cite>, we can <strong>anneal</strong> our reflected SDE to a new noise level $\overline{g}(t) &gt; 0$.</p> \[\begin{equation} \mathrm{d} \mathbf{x}_t = \left[\mathbf{f}(\mathbf{x}_t, t) - \frac{g(t)^2 - \overline{g}(t)^2}{2} \nabla_x \log p_t(\mathbf{x}_t) \right] \mathrm{d}t + \overline{g}(t) \mathrm{d} \mathbf{B}_t + \mathrm{d} \mathbf{L}_t \end{equation}\] <p>This results in a reflected diffusion process that has the same marginal probabilities as our original SDE, and allows us to sample with a lower variance. Amazingly, as we take $\overline{g}(t) \to 0$, the boundary reflection term $\mathrm{d} \mathbf{L}_t$ disappears since $\nabla_x \log p_t$ satisfies Neumann boundary conditions:</p> \[\begin{equation} \mathrm{d} \mathbf{x}_t = \left[\mathbf{f}(\mathbf{x}_t, t) - \frac{g(t)^2}{2} \nabla_x \log p_t(\mathbf{x}_t)\right]\mathrm{d}t \end{equation}\] <p>We can replace $\nabla_x \log p_t$ with our score function approximation $\mathbf{s}_\theta$ to recover a <strong>Probability Flow ODE</strong><d-cite key="Song2020ScoreBasedGM"></d-cite>. This enables fast sampling with efficient ODE samplers, an interpretable latent space, and exact log-liklihoods through the Hutchinson trace estimator trick.</p> <p>Interestingly, learning with the $\lambda_t$ weighting function that we used for image generation results in an <strong>ELBO</strong>, so we can use the same noise schedule to generate good images and maximize likelihoods. When compared with other likelihood-based diffusion methods, our optimization has much lower variance, so we can achieve likelihood results that are <strong>close to the state of the art without requiring either importance sampling or a learned noise schedule</strong>.</p> <table> <thead> <tr> <th>Method</th> <th>CIFAR-10 BPD (↓)</th> <th>ImageNet-32 BPD (↓)</th> <th> </th> </tr> </thead> <tbody> <tr> <td>ScoreFlow<d-cite key="Song2021MaximumLT"></d-cite> </td> <td>2.86</td> <td>3.83</td> <td> </td> </tr> <tr> <td>    <em>(with importance sampling)</em> </td> <td><em>2.83</em></td> <td><em>3.76</em></td> <td> </td> </tr> <tr> <td>VDM<d-cite key="Kingma2021VariationalDM"></d-cite> </td> <td>2.70</td> <td>——</td> <td> </td> </tr> <tr> <td>    <em>(with learned noise)</em> </td> <td><em>2.65</em></td> <td><em>3.72</em></td> <td> </td> </tr> <tr> <td><strong>Ours</strong></td> <td><strong>2.68</strong></td> <td><strong>3.74</strong></td> <td> </td> </tr> </tbody> </table> <h3 id="diffusion-guidance">Diffusion Guidance</h3> <p>One of the major perks of diffusion models is their <strong>controllability</strong>. Using some conditional information $\mathbf{c}$, which could be the class or a piece of description text, we can guide samples to satisfy $c$ through a classifier $p(\mathbf{c} \vert \mathbf{x})$:</p> \[\begin{equation} \nabla \log p_t(\mathbf{x} \vert c) = \nabla \log p_t(\mathbf{x}) + \nabla \log p_t(c \vert \mathbf{x}) \end{equation}\] <p>Currently, this notion of controllable diffusion normally appears as <strong>classifier-free diffusion guidance</strong><d-cite key="Ho2022ClassifierFreeDG"></d-cite>, which instead uses an additional conditional score network and a guidance weight $w$ to approximate the synthetic distribution $p_t^w(\mathbf{x} \vert c) \propto p_t(\mathbf{x}) p_t(c \vert \mathbf{x})^w$:</p> \[\begin{equation} \nabla \log p_t^w(\mathbf{x} \vert c) = (w + 1) \nabla \log p_t(\mathbf{x} \vert c) - w \nabla \log p_t(\mathbf{x}) \end{equation}\] <p>In the literature, increasing $w$ generates more fidelitous images, which is crucial for text-to-image guided diffusion. From our experiments, we found that thresholding is <strong>critical</strong> for classifier-free guidance to work. Without it, sampling with even small weights $w$ causes images to diverge:</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/no_clip-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/no_clip-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/no_clip-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/no_clip.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> Baseline non-thresholded images for $w=1$. </p> <p>Additionally, we can’t use quick deterministic ODE sampling methods since we can’t mimic the effect of thresholding. In fact, this seems to cause samples to diverge even more:</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/no_clip_ddim-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/no_clip_ddim-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/no_clip_ddim-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/no_clip_ddim.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> Baseline non-thresholded images for $w=1$. Sampled with an ODE. </p> <p>Furthermore, although a large guidance weight $w$ is preferred in applications such as text-to-image diffusion, it is well known that this can cause samples to suffer artifacts such as oversaturation even when thresholding.</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/clip-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/clip-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/clip-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/clip.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> Baseline thresholded images for $w=15$. They suffer from oversaturation. </p> <p>We hypothesize that this these artifacts are due to the mismatch between training and sampling. In particular, the trained behavior seeks to push samples out-of-bounds, and the sampling procedure clips these out-of-bounds pixels to $0$ or $255$, resulting in oversaturation. Because Reflected Diffusion Models are trained to avoid this behavior, our high guidance weight samples are significantly less saturated and do not contain any artifacts:</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/ours-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/ours-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/ours-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/ours.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> Our images for $w=15$. These do not suffer from oversaturation. </p> <p>Lastly, when we combine score networks for classifier-free guidance, the Neumann boundary condition is maintained. As such, it is <strong>possible to sample</strong> from classifier-free guided diffusion models using our Probability Flow ODE, requiring far fewer evaluations.</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/ode_cf_guided-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/ode_cf_guided-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/ode_cf_guided-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/ode_cf_guided.png" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p> Our ODE samples ($w=1.5$). We can sample with ~100 evaluations, as opposed to 1000. </p> <h3 id="generalizing-to-different-geometries">Generalizing to Different Geometries</h3> <p>We have constructed our framework to be <strong>completely general</strong> with respect to the underlying domain $\Omega$. As such, we can apply our model to a wider variety of domains beyond the hypercube (which we used to model images):</p> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/blog/refldiff/geometry.gif-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/blog/refldiff/geometry.gif-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/blog/refldiff/geometry.gif-1400.webp"/> <img src="https://conversationswith.rocks/assets/img/blog/refldiff/geometry.gif" width="auto" height="auto" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p>For instance, applying Reflected Diffusion Models to simplices results in a simplex diffusion method that <strong>learn in high dimensions</strong>. We did not explore this further, but in principle, this opens up potential applications in fields such as language modeling.</p> <h2 id="conclusion">Conclusion</h2> <p>This blog post presented a detailed overview of our recent work “Reflected Diffusion Models”. Our full paper can be found on <a href="https://arxiv.org/abs/2304.04740" rel="external nofollow noopener" target="_blank">arxiv</a> and contains many more mathematical details, additional results, and deeper explanations. We have also released preliminary <a href="https://github.com/louaaron/Reflected-Diffusion" rel="external nofollow noopener" target="_blank">code</a>, but this is currently bare-bones and under construction.</p> <ol></ol> </div></div>
  </body>
</html>
