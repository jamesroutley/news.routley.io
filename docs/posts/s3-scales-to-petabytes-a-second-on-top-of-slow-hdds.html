<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives">Original</a>
    <h1>S3 scales to petabytes a second on top of slow HDDs</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p>Everyone knows what AWS S3 is, but few comprehend the massive scale it operates at, nor what it took to get there.</p><p><span>In essence - it’s a scalable </span><strong>multi-tenant</strong><span> storage service with APIs to store and retrieve objects, offering extremely </span><strong><span>high availability</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-1-174142024" target="_self" rel="">1</a></span></strong><span> and </span><strong>durability</strong><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-2-174142024" target="_self" rel="">2</a></span><span> at a relatively low cost</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-3-174142024" target="_self" rel="">3</a></span><span>.</span></p><ul><li><p><span>400+ </span><strong>trillion</strong><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-4-174142024" target="_self" rel="">4</a></span><span> objects</span></p></li><li><p><strong>150 million</strong><span> requests a second (150,000,000/s)</span></p></li><li><p><strong>&gt; 1 PB/s</strong><span> of peak traffic</span></p></li><li><p><strong>tens of millions</strong><span> of disks</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!DjlS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!DjlS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 424w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 848w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 1272w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!DjlS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png" width="728" height="524" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d9099051-91be-419e-9296-98944f2a4788_1456x1048.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:1048,&#34;width&#34;:1456,&#34;resizeWidth&#34;:728,&#34;bytes&#34;:1690766,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:&#34;center&#34;,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!DjlS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 424w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 848w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 1272w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div><div></div></div></div></a></figure></div><p>Hard drives.</p><p><span>How S3 achieves this scale is an engineering marvel. To understand and appreciate the system, we first must appreciate its core building block - </span><strong>the</strong><span> </span><strong>hard drive</strong><span>.</span></p><p>Hard Disk Drives (HDDs) are an old, somewhat out-of-favor technology largely superseded by SSDs. They are physically fragile, constrained for IOPS and high in latency.</p><p><span>But they nailed something flash still hasn’t: </span><strong>dirt cheap commodity economics</strong><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!B3nU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!B3nU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png 424w, https://substackcdn.com/image/fetch/$s_!B3nU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png 848w, https://substackcdn.com/image/fetch/$s_!B3nU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png 1272w, https://substackcdn.com/image/fetch/$s_!B3nU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!B3nU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png" width="1456" height="1324" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1324,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:376230,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!B3nU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png 424w, https://substackcdn.com/image/fetch/$s_!B3nU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png 848w, https://substackcdn.com/image/fetch/$s_!B3nU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png 1272w, https://substackcdn.com/image/fetch/$s_!B3nU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02d002a9-093e-45e3-9b2d-89832a28ea99_1898x1726.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption><em><span>how HDD prices have cratered in the last decades; src: </span><a href="https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage?time=earliest..2023" rel="">https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage?time=earliest..2023</a></em></figcaption></figure></div><p>Over their lifetime, HDDs have seen exponential improvement:</p><ul><li><p><strong>price</strong><span>: 6,000,000,000x cheaper per byte (inflation-adjusted)</span></p></li><li><p><strong>capacity</strong><span>: increased 7,200,000x</span></p></li><li><p><strong>size</strong><span>: decreased 5,000x</span></p></li><li><p><strong>weight</strong><span>: decreased 1,235x</span></p></li></ul><p><span>But one issue has consistently persisted - they’re constrained for IOPS. They have been stuck at 120 IOPS for the last 30 years.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!0SdG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!0SdG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!0SdG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png" width="725.234375" height="407.9443359375" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:819,&#34;width&#34;:1456,&#34;resizeWidth&#34;:725.234375,&#34;bytes&#34;:91259,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:&#34;center&#34;,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!0SdG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>This means that per byte, </span><strong>HDDs are becoming slower</strong><span>.</span></p><h2>Why are HDDs slow?<div><div></div></div></h2><p><span>HDDs are slow because of </span><strong>physics</strong><span>.</span></p><p>They require real-world mechanical movement to read data. (unlike SSDs, which use electricity travelling at ~50% the speed of light). Here is a good visualization:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MRdm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MRdm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif 424w, https://substackcdn.com/image/fetch/$s_!MRdm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif 848w, https://substackcdn.com/image/fetch/$s_!MRdm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif 1272w, https://substackcdn.com/image/fetch/$s_!MRdm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!MRdm!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif" width="816" height="559.6132930513595" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:454,&#34;width&#34;:662,&#34;resizeWidth&#34;:816,&#34;bytes&#34;:1200147,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/gif&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:&#34;center&#34;,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MRdm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif 424w, https://substackcdn.com/image/fetch/$s_!MRdm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif 848w, https://substackcdn.com/image/fetch/$s_!MRdm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif 1272w, https://substackcdn.com/image/fetch/$s_!MRdm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dcb28d1-c6fe-4b5f-ad87-2750b58c12e7_662x454.gif 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption><span>src: </span><a href="https://animagraffs.com/hard-disk-drive/" rel="">https://animagraffs.com/hard-disk-drive/</a><span> (altho it seems broken as of writing)</span></figcaption></figure></div><p><span>The platter spins around the spindle at about 7200 rounds per minute (RPM)</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-5-174142024" target="_self" rel="">5</a></span><span>.</span></p><p><span>The mechanical arm (actuator) with its read/write head physically moves across the platter and waits for it to rotate until it gets to the precise </span><a href="https://en.wikipedia.org/wiki/Logical_block_addressing" rel="">LBA</a><span> address where the data resides.</span></p><p>Accessing data from the disk therefore involves two mechanical operations and one electrical. </p><p>That physical movements are:</p><ul><li><p><strong>seek</strong><span> - the act of the actuator moving left or right to the correct track on the platter</span></p><ul><li><p><span>full-platter seek time: </span><strong>~8ms</strong></p></li><li><p><span>half-platter seek time (avg): </span><strong>~4ms</strong></p></li></ul></li><li><p><strong>rotation</strong><span> - waiting for the spindle to spin the disk until it matches the precise address on the platter’s track</span></p><ul><li><p><span>full rotational latency: </span><strong>~8.3ms</strong><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-6-174142024" target="_self" rel="">6</a></span></p></li><li><p><span>half rotational latency (avg): </span><strong>~4ms</strong></p></li></ul></li></ul><p>And then the electrical one:</p><ul><li><p><strong>transfer rate</strong><span> - the act of the head shoving bits off the platter across the bus into memory (the drive’s internal cache)</span></p><ul><li><p><span>reading 0.5MB</span><strong>:</strong><span> </span><strong>~2.5ms</strong><span> on average</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-7-174142024" target="_self" rel="">7</a></span></p></li></ul></li></ul><h3>Sequential I/O<div><div></div></div></h3><p>Hard Drives are optimized for sequential access patterns.</p><p>Reading/writing bytes that are laid out consecutively on the disk is fast. The natural rotation of the platter cycles through the block of bytes and no excessive seeks need to be performed (the actuator stays still).</p><p><span>The easiest and most popular data structure with sequential access patterns is </span><a href="https://topicpartition.io/definitions/the-log" rel="">the Log</a><span>. Popular distributed systems like </span><a href="https://www.google.com/search?q=what+is+apache+kafa+2+minte+stremaing&amp;oq=what+is+apache+kafa+2+minte+stremaing&amp;gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIJCAEQIRgKGKABMgkIAhAhGAoYoAEyCQgDECEYChigATIHCAQQIRiPAjIHCAUQIRiPAtIBCDMwNTlqMGo0qAIAsAIB&amp;sourceid=chrome&amp;ie=UTF-8" rel="">Apache Kafka</a><span> are built on top of it and through sequential access patterns squeeze out great performance off cheap hardware.</span></p><p><span>It is no surprise that S3’s storage backend - ShardStore - is based on a </span><strong>log-structured</strong><span> merge tree (LSM) itself.</span></p><p><span>In essence, writes for S3 is easy. Because they write sequentially to the disk, they take advantage of the HDD’s performance. (</span><em><span>similar to Kafka, I bet they batch pending PUTs so as to squeeze out more sequential throughput on disk via appends to the log)</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-8-174142024" target="_self" rel="">8</a></span></em></p><p>Reads, however, are trickier. AWS can’t control what files the user requests - so they have to jump around the drive when serving them.</p><h3>Random I/O<div><div></div></div></h3><p>In the average case, a read on a random part of the drive would involve half of the full physical movement.</p><p><span>The average read latency is the sum of both average physical movements plus the transfer rate. Overall, you’re looking at </span><strong>~11ms</strong><span> on average to read </span><strong>0.5 MB</strong><span> of random I/O from a drive. That’s very slow.</span></p><p><span>Since a second has 1000 milliseconds, you’d only achieve </span><strong>~45MB/s</strong><span> of random I/O from a single drive.</span></p><p><span>Because physical movements are a bottleneck - </span><strong>disks have been stuck at this same random I/O latency for the better part of 30 years</strong><span>.</span></p><p><span>They are simply not efficient under random access patterns. That’s when you’d opt for SSDs. But if you have to store massive amounts of data - SSDs become unaffordable.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-9-174142024" target="_self" rel="">9</a></span></p><p><span>This becomes a pickle when you are S3 - a random access system</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-10-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-10-174142024" target="_self" rel="">10</a></span><span> that also stores massive amounts of data.</span></p><p><span>Yet, S3 found a way to do it - it delivers tolerable latency</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-11-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-11-174142024" target="_self" rel="">11</a></span><span> and outstanding</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-12-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-12-174142024" target="_self" rel="">12</a></span><span> throughput while working around the physical limitations.</span></p><p><span>S3 solves this problem through </span><strong>massive</strong><span> </span><strong>parallelism</strong><span>.</span></p><p><span>They spread the data out in many (many!</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-13-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-13-174142024" target="_self" rel="">13</a></span><span>) hard drives so they can achieve massive read throughput by utilizing each drive in parallel.</span></p><ul><li><p><span>Storing a 1 TB file in a single HDD means limits your reading rate by that single drive’s max throughput (~300 MB/s</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-14-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-14-174142024" target="_self" rel="">14</a></span><span>).</span></p></li><li><p><span>Splitting that same 1 TB file across 20,000 different HDDs means you can read it in parallel </span><strong>at the sum of all HDDs’ throughput </strong><span>(TB/s).</span></p></li></ul><p>They do this via Erasure Coding.</p><h2>Erasure Coding<div><div></div></div></h2><p>Redundancy schemes are common practice in storage systems.</p><p><span>They are most often associated with </span><em>data durability</em><span> - protecting against data loss when hardware fails.</span></p><p><span>S3 uses </span><a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction?ref=highscalability.com" rel="">Erasure Coding</a><span> (EC). It breaks data into K shards with M redundant “parity” shards. EC allows you to reconstruct the data from any K shards out of the total K+M shards.</span></p><blockquote><p><span>The S3 team shares they use a 5-of-9 scheme. They shard each object into </span><strong>9 pieces</strong><span> - 5 Regular Shards (K) and 4 Parity Shards (M)</span></p><p>This approach tolerates up to 4 losses. To access the object, they need 5/9 shards.</p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_3mc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_3mc!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!_3mc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png" width="1456" height="1001" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/bc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1001,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:244375,&#34;alt&#34;:&#34;&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!_3mc!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>This scheme helps S3 find a middle balance - it doesn’t take much extra disk capacity yet still provides flexible I/O.</p><ul><li><p>EC makes them store 1.8x the original data.</p><ul><li><p>A naive alternative like 3-way replication would result in 3x the data. That extra 1.2x starts to matter when we’re talking hundreds of exabytes.</p></li></ul></li><li><p>EC gives them 9 possible read sources - an ample hedge against bottlenecks</p><ul><li><p>3-way replication would only give them 3 sources. If all 3 nodes are hot, performance would suffer.</p></li><li><p>9 read sources also offer much more burst demand I/O due to parallelism</p></li></ul></li></ul><p><span>An under-appreciated aspect of EC is precisely its ability to </span><strong>distribute load</strong><span>. Such schemes spread the hot spots of a system out and give it the flexibility to steer read traffic in a balanced way. And since shards are small, firing off hedge requests</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-15-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-15-174142024" target="_self" rel="">15</a></span><span> to dodge stragglers is far cheaper than with full replicas.</span></p><h2>Parallelism in Action<div><div></div></div></h2><p>S3 leverages parallelism in three main ways:</p><ol><li><p>From the user’s perspective - upload/download the file in chunks.</p></li><li><p>From the client’s perspective - send requests to multiple different front-end servers.</p></li><li><p>From the server’s perspective - store an object in multiple storage servers.</p></li></ol><p>Any part of the end-to-end path can become a bottleneck, so it’s important to optimize everything.</p><h3><strong>1. Across Front-end Servers</strong><div><div></div></div></h3><p>Instead of requesting all the files through one connection to one S3 endpoint, users are encouraged to open as many connections as necessary. This happens behind the scenes in the library code through an internal HTTP connection pool.</p><p>This approach utilizes many different endpoints of the distributed system, ensuring no single point in the infrastructure becomes too hot (e.g. front-end proxies, caches, etc)</p><h3><strong>2. Across Hard Drives</strong><div><div></div></div></h3><p>Instead of storing the data in a single hard-drive, the system breaks it into shards via EC and spreads it out across multiple storage back ends.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!h2Yl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!h2Yl!,w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif 424w, https://substackcdn.com/image/fetch/$s_!h2Yl!,w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif 848w, https://substackcdn.com/image/fetch/$s_!h2Yl!,w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif 1272w, https://substackcdn.com/image/fetch/$s_!h2Yl!,w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!h2Yl!,w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif" width="728" height="409.5" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:450,&#34;width&#34;:800,&#34;resizeWidth&#34;:728,&#34;bytes&#34;:5322543,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/gif&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:&#34;center&#34;,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!h2Yl!,w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif 424w, https://substackcdn.com/image/fetch/$s_!h2Yl!,w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif 848w, https://substackcdn.com/image/fetch/$s_!h2Yl!,w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif 1272w, https://substackcdn.com/image/fetch/$s_!h2Yl!,w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0770e38c-c1cd-4680-ac8e-2a07911c1c76_800x450.gif 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption><span>copyright: AWS; from </span><a href="https://www.youtube.com/watch?v=v3HfUNQ0JOE" rel="">this re:Invent presentation</a><span>.</span></figcaption></figure></div><h3><strong>3. Across PUT/GET Operations</strong><div><div></div></div></h3><p><span>Instead of sending one request through a single thread and HTTP connection, the client chunks it into 10 parts and uploads each in parallel.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-16-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-16-174142024" target="_self" rel="">16</a></span></p><ul><li><p>PUT requests support multipart upload, which AWS recommends in order to maximize throughput by leveraging multiple threads.</p></li><li><p><span>GET requests similarly support an HTTP header denoting you read only a particular range of the object (called </span><strong>byte-ranged GET</strong><span>). AWS again recommends this for achieving higher aggregate throughput instead of the single object read request.</span></p></li></ul><p>Uploading 1 GB/s to a single server may be difficult, but uploading 100 chunks each at 10 MB/s chunks to 100 different servers is very practical.</p><p>This simple idea goes a long way.</p><p>S3 now finds itself with a difficult problem. They have tens of millions of drives, hundreds of millions of parallel requests per second and hundreds of millions of EC shards to persist per second. </p><p>How do they spread this load around effectively so as to avoid certain nodes/disks overheating?</p><p>As we said earlier - a single disk can do around ~45 MB/s of random IOs. It seems trivial to hit that bottleneck. Not to mention any additional system maintenance work like rebalancing data around for more efficient spreading would also take valuable IOs off the disk.</p><p><span>Forming hot spots in a distributed system is dangerous, because it can easily cause a domino-like spiral into system-wide degradation</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-17-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-17-174142024" target="_self" rel="">17</a></span><span>.</span></p><p>Needless to say, S3 is very careful in trying to spread data around. Their solution is again deceptively simple:</p><ol><li><p>randomize where you place data on ingestion</p></li><li><p>continuously rebalance it</p></li><li><p>scale &amp; chill</p></li></ol><h2>Shuffle Sharding &amp; Power of Two<div><div></div></div></h2><p>Where you place data initially is key to performance. Moving it later is more expensive.</p><p>Unfortunately, at write time you have no good way of knowing whether the data you’re about to persist is going to be accessed frequently or not.</p><p>Knowing the perfect the least-loaded HDD to place new data in is also impossible at this scale. You can’t keep a synchronous globally-consistent view when you are serving hundreds of millions of requests per second across tens of millions of drives. This approach would also risk load correlation - placing similar workloads together and having them burst together at once.</p><p>A key realization is that picking at random works better in this scenario. 💡</p><p><span>It’s how AWS </span><strong>intentionally engineers decorrelation</strong><span> into their system:</span></p><ol><li><p>A given PUT picks a random set of drives</p></li><li><p>The next PUT, even if it’s targetting the same key/bucket, picks a different set of near-random drives.</p></li></ol><p><span>The way they do it is through the so-called </span><strong><a href="https://dl.acm.org/doi/10.1109/71.963420" rel="">Power of Two Random Choices</a></strong><span>:</span></p><blockquote><p><em><strong>Power of Two Random Choices</strong><span>:</span></em><span> a well-studied phenomenon in load balancing that says choosing between the least-loaded of </span><strong>two completely random nodes</strong><span> yields much </span><em>better results</em><span> than choosing just one node at random.</span></p></blockquote><h2>Rebalancing<div><div></div></div></h2><p><span>Another key realization is that </span><strong>newer data chunks are hotter than older ones</strong><span>. 💡</span></p><p><span>Fresh data is accessed more frequently. As it grows older, it gets accessed less.</span></p><p>AWS has to proactively rebalance the cold data out (so as to free up space) and rebalance cold data in (so as to make use of the free I/O).</p><p><span>Data rebalances are also needed when new racks of disks are added to S3. Each rack contains 20 PB of capacity</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-18-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-18-174142024" target="_self" rel="">18</a></span><span>, and every disk in there is completely empty. The system needs to proactively spread the load around the new capacity.</span></p><p>Suffice to say - S3 constantly rebalances data around.</p><h2>Chill@Scale<div><div></div></div></h2><p><span>The last realization is perhaps the least intuitive: </span><strong>the larger the system becomes, the more predictable it is</strong><span>. 💡</span></p><p><span>AWS experienced so-called </span><strong>workload decorrelation </strong><span>as S3 grew. That is the phenomenon of seeing a smoothening of load once it’s aggregated on a large enough scale. While their peak demand is growing in size, their peak-to-mean delta is collapsing.</span></p><p>This is because storage workloads are inherently very bursty - they demand a lot at once, and then may remain idle for a long time (months).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!RbsI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!RbsI!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!RbsI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png" width="1456" height="819" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:819,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:103067,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!RbsI!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>Because </span><strong>independent workloads do not burst together</strong><span>, the more workloads you cram together - the more those idle spots get filled up and the more predictable the system becomes in aggregate. 💡</span></p><div data-component-name="VideoEmbedPlayer" id="media-b8b8e36f-2a45-47ae-b1a7-a6c5c6a935cb"></div><p><em><span>copyright: AWS; from this </span><a href="https://www.youtube.com/watch?v=v3HfUNQ0JOE" rel="">re:Invent presentation</a><span>.</span></em></p><p>AWS S3 is a massively multi-tenant storage service. It’s a gigantic distributed system consisting of many individually slow nodes that on aggregate allow you to access data faster than any single node can provide. S3 achieves this through:</p><ul><li><p>massive parallelization across the end-to-end path (user, client, server)</p></li><li><p>neat load-balancing tricks like the power of two random</p></li><li><p>spreading out data via erasure coding</p></li><li><p>lowering tail latency via hedge requests</p></li><li><p>the economies of multi-tenancy at world scale</p></li></ul><p>It started as a service optimized for backups, video and image storage for e-commerce websites - but eventually grew support being the main storage system used for analytics and machine learning on massive data lakes.</p><p><span>Nowadays, the growing trend is for entire data infrastructure projects to be based on top of S3. This gives them the benefits of stateless nodes (easy scaling, less management) while outsourcing difficult durability, replication and load-balancing problems to S3. And get this - it also reduces cloud costs.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-19-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-19-174142024" target="_self" rel="">19</a></span></p><div data-attrs="{&#34;url&#34;:&#34;https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;}" data-component-name="CaptionedButtonToDOM"><p>Learned something? Share with a colleague on Slack</p><p data-attrs="{&#34;url&#34;:&#34;https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;}" data-component-name="ButtonCreateButton"><a href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><p>Subscribe for more interesting dives in big data distributed systems (or the occassional small data gem).</p><div><div><p>I only send e-mails when I have something interesting to share. 👌</p><div data-component-name="SubscribeWidget"><div><div><form action="/api/v1/free?nojs=true" method="post" novalidate=""><div><div></div></div></form></div></div></div></div></div><p>S3 has a lot of other goodies up its bag, including:</p><ul><li><p>shuffle sharding at the DNS level</p></li><li><p>client library hedging requests by cancelling slow requests that pass the p95 threshold and sending new ones to a different host</p></li><li><p>software updates done erasure-coding-style, including rolling out their brand-new ShardStore storage system without any impact to their fleet</p></li><li><p>conway’s law and how it shapes S3’s architecture (consisting of 300+ microservices)</p></li><li><p>their durability culture, including continuous detection, durable chain of custody, a design process that includes durability threat modelling and formal verification</p></li></ul><p>These are generally shared in their annual S3 Deep Dive at re:Invent:</p><ul><li><p><a href="https://www.youtube.com/watch?v=v3HfUNQ0JOE&amp;t=583s&amp;pp=ygUQYXdzIHMzIHJlIGludmVudNIHCQngCQGHKiGM7w%3D%3D" rel="">2022</a><span> (video)</span></p></li><li><p><a href="https://www.youtube.com/watch?v=sYDJYqvNeXU" rel="">2023</a><span> (video)</span></p></li><li><p><a href="https://www.youtube.com/watch?v=NXehLy7IiPM&amp;t=398s&amp;pp=ygUQYXdzIHMzIHJlIGludmVudA%3D%3D" rel="">2024</a><span> (video)</span></p></li><li><p><a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html" rel="">Building and operating a pretty big storage system called S3</a><span> (article)</span></p></li></ul><p>Thank you to the S3 team for sharing what they’ve built, and thank you for reading!</p><div data-component-name="DigestPostEmbed"><div><a href="https://bigdata.2minutestreaming.com/p/why-was-apache-kafka-created" rel="noopener" target="_blank"><h2>Why Was Apache Kafka Created?</h2></a><div><div><p><a href="https://substack.com/profile/1057029-stanislav-kozlovski">Stanislav Kozlovski</a></p></div><p>·</p><p>Aug 22</p></div><div><div><a href="https://bigdata.2minutestreaming.com/p/why-was-apache-kafka-created" rel="noopener" target="_blank"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!EmZj!,w_280,h_280,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84290a14-c8e4-4bd7-b10c-03dfe7793d37_1200x630.png"/><img src="https://substackcdn.com/image/fetch/$s_!EmZj!,w_280,h_280,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84290a14-c8e4-4bd7-b10c-03dfe7793d37_1200x630.png" sizes="100vw" alt="Why Was Apache Kafka Created?" width="280" height="280"/></picture></a></div><div><p>The best explanation for why Apache Kafka was created - coming directly from the horse&#39;s mouth (a LinkedIn paper)</p><div><a href="https://bigdata.2minutestreaming.com/p/why-was-apache-kafka-created"><div><p><span>Read full story</span></p><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></div></a></div></div></div></div></div><div data-component-name="DigestPostEmbed"><div><a href="https://bigdata.2minutestreaming.com/p/the-brutal-truth-about-apache-kafka-cost-calculators" rel="noopener" target="_blank"><h2>The Brutal Truth about Kafka Cost Calculators</h2></a><div><div><p><a href="https://substack.com/profile/1057029-stanislav-kozlovski">Stanislav Kozlovski</a></p></div><p>·</p><p>December 13, 2024</p></div><div><div><a href="https://bigdata.2minutestreaming.com/p/the-brutal-truth-about-apache-kafka-cost-calculators" rel="noopener" target="_blank"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!sI4t!,w_280,h_280,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47f8ff92-86dd-4e9b-9b34-3dcb019c131b_1024x1088.png"/><img src="https://substackcdn.com/image/fetch/$s_!sI4t!,w_280,h_280,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47f8ff92-86dd-4e9b-9b34-3dcb019c131b_1024x1088.png" sizes="100vw" alt="The Brutal Truth about Kafka Cost Calculators" width="280" height="280"/></picture></a></div><div><p>Common ways vendor misrepresent actual infrastructure cost in their dev-targeted marketing calculators. (with a real example)</p><div><a href="https://bigdata.2minutestreaming.com/p/the-brutal-truth-about-apache-kafka-cost-calculators"><div><p><span>Read full story</span></p><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></div></a></div></div></div></div></div><div data-component-name="DigestPostEmbed"><div><a href="https://bigdata.2minutestreaming.com/p/meet-your-new-data-lakehouse-s3-iceberg" rel="noopener" target="_blank"><h2>meet your new data lakehouse: S3 Iceberg Tables</h2></a><div><div><p><a href="https://substack.com/profile/1057029-stanislav-kozlovski">Stanislav Kozlovski</a></p></div><p>·</p><p>December 5, 2024</p></div><div><div><a href="https://bigdata.2minutestreaming.com/p/meet-your-new-data-lakehouse-s3-iceberg" rel="noopener" target="_blank"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Qdid!,w_280,h_280,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F322ba469-75c8-4314-a7ae-d82ba1e64146_1456x1048.png"/><img src="https://substackcdn.com/image/fetch/$s_!Qdid!,w_280,h_280,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F322ba469-75c8-4314-a7ae-d82ba1e64146_1456x1048.png" sizes="100vw" alt="meet your new data lakehouse: S3 Iceberg Tables" width="280" height="280"/></picture></a></div><div><p>The rise of open table formats (Iceberg, Delta) and the release of S3 Tables.</p><div><a href="https://bigdata.2minutestreaming.com/p/meet-your-new-data-lakehouse-s3-iceberg"><div><p><span>Read full story</span></p><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12h14"></path><path d="m12 5 7 7-7 7"></path></svg></div></a></div></div></div></div></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-1-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-1-174142024" contenteditable="false" target="_self" rel="">1</a></p><p><span>S3 has never been down for more than 5 hours in </span><strong>its entire existence</strong><span>. And that incident was 8 years ago, in just one region (out of 38) in AWS. It was considered one of AWS&#39; most </span><a href="https://www.gremlin.com/blog/the-2017-amazon-s-3-outage" rel="">impactful outages</a><span> of all time.</span></p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-2-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-2-174142024" contenteditable="false" target="_self" rel="">2</a></p><p><span>S3 markets itself as being </span><em><strong>designed </strong></em><span>for 11 nines of durability. Careful with the wording - they don’t legally promise 99.999999999% durability. In fact, Amazon does not legally provide any SLA for durability.</span></p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-3-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-3-174142024" contenteditable="false" target="_self" rel="">3</a></p><p><span>By relative low cost, I mean relative to the other storage you can buy on AWS. S3 is still ~$21.5-$23.5 per TB of storage. In fact, </span><a href="https://www.datawrapper.de/_/hyabD/" rel="">S3 hasn’t lowered its prices in 8 years despire HDD prices falling 60% since</a><span>. When I ran back-of-the-napkin maths for what it’d cost for me to build my own S3 bare metal, the cost came out to $0.875 per TB of storage (25x cheaper). Alternatively, hosting it on Hetzner would be around $5.73 per TB.</span></p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-4-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-4-174142024" contenteditable="false" target="_self" rel="">4</a></p><p>400,000,000,000,000</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-5-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-5-174142024" contenteditable="false" target="_self" rel="">5</a></p><p><span>The first 7200 rpm drive was the </span><a href="https://en.wikipedia.org/wiki/Seagate_Barracuda" rel="">Seagate Barracuda</a><span> released in 1992. Today, rpm has largely remained unchanged. Larger 15k-ish rpm drives exist, but aren’t super common.</span></p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-6-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-6-174142024" contenteditable="false" target="_self" rel="">6</a></p><p>7200 rotations per minute == 7200 rotations per 60000 milliseconds == 8.33ms per rotation</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-7-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-7-174142024" contenteditable="false" target="_self" rel="">7</a></p><p>HDDs on average have ~170-200 MB/s transfer rate; 200mb / 1000ms == 0.2 mb/ms; 0.5 mb == ~2.5ms; They’re simply not optimized for random access like this.</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-8-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-8-174142024" contenteditable="false" target="_self" rel="">8</a></p><p>Kafka loves batching entries. It batches on the client (by waiting), it batches in the protocol (by merging entries) and it batches on the server (by storing in page cache and utilizing the OS’ async flush). It’s such an obvious perf gain that S3 must do something similar in their back-end and storage system.</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-9-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-9-174142024" contenteditable="false" target="_self" rel="">9</a></p><p>although this is slowly but surely beginning to change for certain data thresholds. SSDs have massively deflated in price in just the last 15 years.</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-10-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-10-174142024" contenteditable="false" target="_self" rel="">10</a></p><p>In aggregate, S3 exhibits random access. You as a tenant can PUT/GET any blobs of any size. An average S3 disk would therefore consist of blobs from thousands of tenants. If they all attempt to access their data simultaneously, the drive simply cannot serve every request at once.</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-11-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-11-174142024" contenteditable="false" target="_self" rel="">11</a></p><p><span>Not a lot of benchmark actually exist here. </span><a href="https://topicpartition.io/misc/AWS-S3-PUT-latency-benchmark" rel="">Testing 0.5MB files</a><span>, I got writes at ~140ms p99 and 26ms p50, reads at 86ms p99. Larger files </span><a href="https://x.com/richardartoul/status/1918753134999949821" rel="">allegedly get larger p99</a><span>, and they vary throughout the week.</span></p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-12-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-12-174142024" contenteditable="false" target="_self" rel="">12</a></p><p><span>At least one public number is Anthropic </span><a href="https://www.allthingsdistributed.com/2025/03/in-s3-simplicity-is-table-stakes.html#:~:text=we%20have%20machine%20learning%20customers%20like%20Anthropic%20driving%20tens%20of%20terabytes%20per%20second" rel="">driving tens of terabytes per second</a><span>. There are likely much larger single customer workloads out there - S3 does more than a petabyte a second!</span></p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-13-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-13-174142024" contenteditable="false" target="_self" rel="">13</a></p><p>AWS shares that tens of thousands of their customers have their data spread over 1,000,000 disks. This is a great example of how multi-tenancy at scale can convert the financially impossible into the affordable. It would be prohibitively expensive for any single tenant to deploy a million HDDs themselves, but when shared behind a multi-tenant system - it becomes surprisingly cheap.</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-14-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-14-174142024" contenteditable="false" target="_self" rel="">14</a></p><p>e.g a modern cheap 20TB HDD maxes out at around 291 MB/s of data transfer: https://www.westerndigital.com/products/internal-drives/wd-gold-sata-hdd?sku=WD203KRYZ; note this is marketing numbers too</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-15-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-15-174142024" contenteditable="false" target="_self" rel="">15</a></p><p><span>The concept of a hedge request was popularized by this Google paper “</span><a href="https://cacm.acm.org/research/the-tail-at-scale/" rel="">The Tail at Scale</a><span>”. It essentially talks about how </span><em><strong>fanout requests</strong></em><span> (where a root request results in many sub-requests, e.g like S3’s GETs requesting multiple shards) can significantly reduce their tail latency by speculatively sending extra requests (i.e if you need 5 sub-requests to build an object - send 6). This extra request is sent only once one of the sub-requests surpasses the usual p95 latency. S3’s client libraries also utilize this concept.</span></p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-16-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-16-174142024" contenteditable="false" target="_self" rel="">16</a></p><p>An interesting detail is that each part of the multi-part upload must be getting Erasure Coded 5-of-9 too. So a single object uploaded through multipart upload can consist of hundreds of shards.</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-17-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-17-174142024" contenteditable="false" target="_self" rel="">17</a></p><p>If too many requests hit the same disk at the same point in time, the disk starts to stall because its limited I/O is exhausted. This accumulates tail latency to requests that depend on the drive. This delay impacts other operations like writes. It also gets amplified up the stack in other components beyond the drive. If left unchecked, it can cause a cascade that significantly slows down the whole system.</p></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-18-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-18-174142024" contenteditable="false" target="_self" rel="">18</a></p><div><p>As someone with no data center experience, I find it super cool when Amazon shares pictures of what the physical disks look like. Here is an example of one such rack of disks. It consists of 1000 drives - 20TB each. It’s said this rack weighs more than a car, and Amazon had to reinforce the flooring in their data centers to support it.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8v4h!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8v4h!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png 424w, https://substackcdn.com/image/fetch/$s_!8v4h!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png 848w, https://substackcdn.com/image/fetch/$s_!8v4h!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png 1272w, https://substackcdn.com/image/fetch/$s_!8v4h!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!8v4h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png" width="1196.3125" height="559.5390195741758" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:681,&#34;width&#34;:1456,&#34;resizeWidth&#34;:1196.3125,&#34;bytes&#34;:3386577,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:&#34;center&#34;,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8v4h!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png 424w, https://substackcdn.com/image/fetch/$s_!8v4h!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png 848w, https://substackcdn.com/image/fetch/$s_!8v4h!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png 1272w, https://substackcdn.com/image/fetch/$s_!8v4h!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F972230ab-0c56-4c19-b5f8-fa29b27a682b_2306x1078.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div></div></div><div data-component-name="FootnoteToDOM"><p><a id="footnote-19-174142024" href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives#footnote-anchor-19-174142024" contenteditable="false" target="_self" rel="">19</a></p><p><span>Apache Kafka (what I am most familiar with) has been seeing the so-called “</span><a href="https://aiven.io/blog/diskless-apache-kafka-kip-1150" rel="">Diskless</a><span>” trend where the write path uses S3 instead of local disks. This </span><a href="https://materializedview.io/p/cloud-storage-triad-latency-cost-durability" rel="">trades off</a><span> higher latency for lower costs (</span><a href="https://topicpartition.io/blog/kip-1150-diskless-topics-in-apache-kafka" rel="">by 90%</a><span> [!]). Similar projects exist - </span><a href="https://turbopuffer.com/" rel="">Turbopuffer</a><span> (Vector DB built on S3), </span><a href="https://slatedb.io/" rel="">SlateDB</a><span> (embedded LSM on S3), </span><a href="https://github.com/nixiesearch/nixiesearch" rel="">Nixiesearch</a><span> (Lucene on S3). In general, every data infra project seems to be offloading as much as possible to object storage. (</span><a href="https://clickhouse.com/blog/building-a-distributed-cache-for-s3" rel="">Clickhouse</a><span>, </span><a href="https://docs.opensearch.org/latest/tuning-your-cluster/availability-and-recovery/remote-store/index/" rel="">OpenSearch</a><span>, </span><a href="https://www.elastic.co/search-labs/blog/stateless-your-new-state-of-find-with-elasticsearch" rel="">Elastic</a><span>). Before Diskless, Kafka similarly used a two-tier approach where cold data was offloaded to S3 (for a </span><a href="https://getkafkanated.substack.com/p/how-to-size-your-kafka-tiered-storage-cluster" rel="">10x storage cost</a><span> saving)</span></p></div></div></div></div>
  </body>
</html>
