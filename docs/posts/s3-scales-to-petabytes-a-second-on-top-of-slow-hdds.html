<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives">Original</a>
    <h1>S3 scales to petabytes a second on top of slow HDDs</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p>Everyone knows what AWS S3 is, but few comprehend the massive scale it operates at, nor what it took to get there.</p><p><span>In essence - it‚Äôs a scalable </span><strong>multi-tenant</strong><span> storage service with APIs to store and retrieve objects, offering extremely </span><strong><span>high availability</span></strong><span> and </span><strong>durability</strong><span> at a relatively low cost</span><span>.</span></p><ul><li><p><span>400+ </span><strong>trillion</strong><span> objects</span></p></li><li><p><strong>150 million</strong><span> requests a second (150,000,000/s)</span></p></li><li><p><strong>&gt; 1 PB/s</strong><span> of peak traffic</span></p></li><li><p><strong>tens of millions</strong><span> of disks</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!DjlS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!DjlS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 424w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 848w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 1272w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!DjlS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png" width="728" height="524" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d9099051-91be-419e-9296-98944f2a4788_1456x1048.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:1048,&#34;width&#34;:1456,&#34;resizeWidth&#34;:728,&#34;bytes&#34;:1690766,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:&#34;center&#34;,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!DjlS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 424w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 848w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 1272w, https://substackcdn.com/image/fetch/$s_!DjlS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9099051-91be-419e-9296-98944f2a4788_1456x1048.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div><div></div></div></div></a></figure></div><p>Hard drives.</p><p><span>How S3 achieves this scale is an engineering marvel. To understand and appreciate the system, we first must appreciate its core building block - </span><strong>the</strong><span> </span><strong>hard drive</strong><span>.</span></p><p>Hard Disk Drives (HDDs) are an old, somewhat out-of-favor technology largely superseded by SSDs. They are physically fragile, constrained for IOPS and high in latency.</p><p><span>But they nailed something flash still hasn‚Äôt: </span><strong>dirt cheap commodity economics</strong><span>:</span></p><p>Over their lifetime, HDDs have seen exponential improvement:</p><ul><li><p><strong>price</strong><span>: 6,000,000,000x cheaper per byte (inflation-adjusted)</span></p></li><li><p><strong>capacity</strong><span>: increased 7,200,000x</span></p></li><li><p><strong>size</strong><span>: decreased 5,000x</span></p></li><li><p><strong>weight</strong><span>: decreased 1,235x</span></p></li></ul><p><span>But one issue has consistently persisted - they‚Äôre constrained for IOPS. They have been stuck at 120 IOPS for the last 30 years.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!0SdG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!0SdG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!0SdG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png" width="725.234375" height="407.9443359375" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:819,&#34;width&#34;:1456,&#34;resizeWidth&#34;:725.234375,&#34;bytes&#34;:91259,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:&#34;center&#34;,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!0SdG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!0SdG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12995ab5-6763-46b7-8116-0a7deb28aeff_1600x900.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>This means that per byte, </span><strong>HDDs are becoming slower</strong><span>.</span></p><p><span>HDDs are slow because of </span><strong>physics</strong><span>.</span></p><p>They require real-world mechanical movement to read data. (unlike SSDs, which use electricity travelling at ~50% the speed of light). Here is a good visualization:</p><p><span>The platter spins around the spindle at about 7200 rounds per minute (RPM)</span><span>.</span></p><p><span>The mechanical arm (actuator) with its read/write head physically moves across the platter and waits for it to rotate until it gets to the precise </span><a href="https://en.wikipedia.org/wiki/Logical_block_addressing" rel="">LBA</a><span> address where the data resides.</span></p><p>Accessing data from the disk therefore involves two mechanical operations and one electrical. </p><p>That physical movements are:</p><ul><li><p><strong>seek</strong><span> - the act of the actuator moving left or right to the correct track on the platter</span></p><ul><li><p><span>full-platter seek time: </span><strong>~8ms</strong></p></li><li><p><span>half-platter seek time (avg): </span><strong>~4ms</strong></p></li></ul></li><li><p><strong>rotation</strong><span> - waiting for the spindle to spin the disk until it matches the precise address on the platter‚Äôs track</span></p><ul><li><p><span>full rotational latency: </span><strong>~8.3ms</strong></p></li><li><p><span>half rotational latency (avg): </span><strong>~4ms</strong></p></li></ul></li></ul><p>And then the electrical one:</p><ul><li><p><strong>transfer rate</strong><span> - the act of the head shoving bits off the platter across the bus into memory (the drive‚Äôs internal cache)</span></p><ul><li><p><span>reading 0.5MB</span><strong>:</strong><span> </span><strong>~2.5ms</strong><span> on average</span></p></li></ul></li></ul><p>Hard Drives are optimized for sequential access patterns.</p><p>Reading/writing bytes that are laid out consecutively on the disk is fast. The natural rotation of the platter cycles through the block of bytes and no excessive seeks need to be performed (the actuator stays still).</p><p><span>The easiest and most popular data structure with sequential access patterns is </span><a href="https://topicpartition.io/definitions/the-log" rel="">the Log</a><span>. Popular distributed systems like </span><a href="https://www.google.com/search?q=what+is+apache+kafa+2+minte+stremaing&amp;oq=what+is+apache+kafa+2+minte+stremaing&amp;gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIJCAEQIRgKGKABMgkIAhAhGAoYoAEyCQgDECEYChigATIHCAQQIRiPAjIHCAUQIRiPAtIBCDMwNTlqMGo0qAIAsAIB&amp;sourceid=chrome&amp;ie=UTF-8" rel="">Apache Kafka</a><span> are built on top of it and through sequential access patterns squeeze out great performance off cheap hardware.</span></p><p><span>It is no surprise that S3‚Äôs storage backend - ShardStore - is based on a </span><strong>log-structured</strong><span> merge tree (LSM) itself.</span></p><p><span>In essence, writes for S3 is easy. Because they write sequentially to the disk, they take advantage of the HDD‚Äôs performance. (</span><em><span>similar to Kafka, I bet they batch pending PUTs so as to squeeze out more sequential throughput on disk via appends to the log)</span></em></p><p>Reads, however, are trickier. AWS can‚Äôt control what files the user requests - so they have to jump around the drive when serving them.</p><p>In the average case, a read on a random part of the drive would involve half of the full physical movement.</p><p><span>The average read latency is the sum of both average physical movements plus the transfer rate. Overall, you‚Äôre looking at </span><strong>~11ms</strong><span> on average to read </span><strong>0.5 MB</strong><span> of random I/O from a drive. That‚Äôs very slow.</span></p><p><span>Since a second has 1000 milliseconds, you‚Äôd only achieve </span><strong>~45MB/s</strong><span> of random I/O from a single drive.</span></p><p><span>Because physical movements are a bottleneck - </span><strong>disks have been stuck at this same random I/O latency for the better part of 30 years</strong><span>.</span></p><p><span>They are simply not efficient under random access patterns. That‚Äôs when you‚Äôd opt for SSDs. But if you have to store massive amounts of data - SSDs become unaffordable.</span></p><p><span>This becomes a pickle when you are S3 - a random access system</span><span> that also stores massive amounts of data.</span></p><p><span>Yet, S3 found a way to do it - it delivers tolerable latency</span><span> and outstanding</span><span> throughput while working around the physical limitations.</span></p><p><span>S3 solves this problem through </span><strong>massive</strong><span> </span><strong>parallelism</strong><span>.</span></p><p><span>They spread the data out in many (many!</span><span>) hard drives so they can achieve massive read throughput by utilizing each drive in parallel.</span></p><ul><li><p><span>Storing a 1 TB file in a single HDD means limits your reading rate by that single drive‚Äôs max throughput (~300 MB/s</span><span>).</span></p></li><li><p><span>Splitting that same 1 TB file across 20,000 different HDDs means you can read it in parallel </span><strong>at the sum of all HDDs‚Äô throughput </strong><span>(TB/s).</span></p></li></ul><p>They do this via Erasure Coding.</p><p>Redundancy schemes are common practice in storage systems.</p><p><span>They are most often associated with </span><em>data durability</em><span> - protecting against data loss when hardware fails.</span></p><p><span>S3 uses </span><a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction?ref=highscalability.com" rel="">Erasure Coding</a><span> (EC). It breaks data into K shards with M redundant ‚Äúparity‚Äù shards. EC allows you to reconstruct the data from any K shards out of the total K+M shards.</span></p><blockquote><p><span>The S3 team shares they use a 5-of-9 scheme. They shard each object into </span><strong>9 pieces</strong><span> - 5 Regular Shards (K) and 4 Parity Shards (M)</span></p><p>This approach tolerates up to 4 losses. To access the object, they need 5/9 shards.</p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!_3mc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!_3mc!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!_3mc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png" width="1456" height="1001" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/bc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1001,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:244375,&#34;alt&#34;:&#34;&#34;,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!_3mc!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 424w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 848w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 1272w, https://substackcdn.com/image/fetch/$s_!_3mc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc4c1972-7acd-447e-af5b-582ac99172f4_1600x1100.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>This scheme helps S3 find a middle balance - it doesn‚Äôt take much extra disk capacity yet still provides flexible I/O.</p><ul><li><p>EC makes them store 1.8x the original data.</p><ul><li><p>A naive alternative like 3-way replication would result in 3x the data. That extra 1.2x starts to matter when we‚Äôre talking hundreds of exabytes.</p></li></ul></li><li><p>EC gives them 9 possible read sources - an ample hedge against bottlenecks</p><ul><li><p>3-way replication would only give them 3 sources. If all 3 nodes are hot, performance would suffer.</p></li><li><p>9 read sources also offer much more burst demand I/O due to parallelism</p></li></ul></li></ul><p><span>An under-appreciated aspect of EC is precisely its ability to </span><strong>distribute load</strong><span>. Such schemes spread the hot spots of a system out and give it the flexibility to steer read traffic in a balanced way. And since shards are small, firing off hedge requests</span><span> to dodge stragglers is far cheaper than with full replicas.</span></p><p>S3 leverages parallelism in three main ways:</p><ol><li><p>From the user‚Äôs perspective - upload/download the file in chunks.</p></li><li><p>From the client‚Äôs perspective - send requests to multiple different front-end servers.</p></li><li><p>From the server‚Äôs perspective - store an object in multiple storage servers.</p></li></ol><p>Any part of the end-to-end path can become a bottleneck, so it‚Äôs important to optimize everything.</p><p>Instead of requesting all the files through one connection to one S3 endpoint, users are encouraged to open as many connections as necessary. This happens behind the scenes in the library code through an internal HTTP connection pool.</p><p>This approach utilizes many different endpoints of the distributed system, ensuring no single point in the infrastructure becomes too hot (e.g. front-end proxies, caches, etc)</p><p>Instead of storing the data in a single hard-drive, the system breaks it into shards via EC and spreads it out across multiple storage back ends.</p><p><span>Instead of sending one request through a single thread and HTTP connection, the client chunks it into 10 parts and uploads each in parallel.</span></p><ul><li><p>PUT requests support multipart upload, which AWS recommends in order to maximize throughput by leveraging multiple threads.</p></li><li><p><span>GET requests similarly support an HTTP header denoting you read only a particular range of the object (called </span><strong>byte-ranged GET</strong><span>). AWS again recommends this for achieving higher aggregate throughput instead of the single object read request.</span></p></li></ul><p>Uploading 1 GB/s to a single server may be difficult, but uploading 100 chunks each at 10 MB/s chunks to 100 different servers is very practical.</p><p>This simple idea goes a long way.</p><p>S3 now finds itself with a difficult problem. They have tens of millions of drives, hundreds of millions of parallel requests per second and hundreds of millions of EC shards to persist per second. </p><p>How do they spread this load around effectively so as to avoid certain nodes/disks overheating?</p><p>As we said earlier - a single disk can do around ~45 MB/s of random IOs. It seems trivial to hit that bottleneck. Not to mention any additional system maintenance work like rebalancing data around for more efficient spreading would also take valuable IOs off the disk.</p><p><span>Forming hot spots in a distributed system is dangerous, because it can easily cause a domino-like spiral into system-wide degradation</span><span>.</span></p><p>Needless to say, S3 is very careful in trying to spread data around. Their solution is again deceptively simple:</p><ol><li><p>randomize where you place data on ingestion</p></li><li><p>continuously rebalance it</p></li><li><p>scale &amp; chill</p></li></ol><p>Where you place data initially is key to performance. Moving it later is more expensive.</p><p>Unfortunately, at write time you have no good way of knowing whether the data you‚Äôre about to persist is going to be accessed frequently or not.</p><p>Knowing the perfect the least-loaded HDD to place new data in is also impossible at this scale. You can‚Äôt keep a synchronous globally-consistent view when you are serving hundreds of millions of requests per second across tens of millions of drives. This approach would also risk load correlation - placing similar workloads together and having them burst together at once.</p><p>A key realization is that picking at random works better in this scenario. üí°</p><p><span>It‚Äôs how AWS </span><strong>intentionally engineers decorrelation</strong><span> into their system:</span></p><ol><li><p>A given PUT picks a random set of drives</p></li><li><p>The next PUT, even if it‚Äôs targetting the same key/bucket, picks a different set of near-random drives.</p></li></ol><p><span>The way they do it is through the so-called </span><strong><a href="https://dl.acm.org/doi/10.1109/71.963420" rel="">Power of Two Random Choices</a></strong><span>:</span></p><blockquote><p><em><strong>Power of Two Random Choices</strong><span>:</span></em><span> a well-studied phenomenon in load balancing that says choosing between the least-loaded of </span><strong>two completely random nodes</strong><span> yields much </span><em>better results</em><span> than choosing just one node at random.</span></p></blockquote><p><span>Another key realization is that </span><strong>newer data chunks are hotter than older ones</strong><span>. üí°</span></p><p><span>Fresh data is accessed more frequently. As it grows older, it gets accessed less.</span></p><p>AWS has to proactively rebalance the cold data out (so as to free up space) and rebalance cold data in (so as to make use of the free I/O).</p><p><span>Data rebalances are also needed when new racks of disks are added to S3. Each rack contains 20 PB of capacity</span><span>, and every disk in there is completely empty. The system needs to proactively spread the load around the new capacity.</span></p><p>Suffice to say - S3 constantly rebalances data around.</p><p><span>The last realization is perhaps the least intuitive: </span><strong>the larger the system becomes, the more predictable it is</strong><span>. üí°</span></p><p><span>AWS experienced so-called </span><strong>workload decorrelation </strong><span>as S3 grew. That is the phenomenon of seeing a smoothening of load once it‚Äôs aggregated on a large enough scale. While their peak demand is growing in size, their peak-to-mean delta is collapsing.</span></p><p>This is because storage workloads are inherently very bursty - they demand a lot at once, and then may remain idle for a long time (months).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!RbsI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!RbsI!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!RbsI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png" width="1456" height="819" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:819,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:103067,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://bigdata.2minutestreaming.com/i/174142024?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!RbsI!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!RbsI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6d68bdf-2509-4961-832b-386cdbfdbb36_1600x900.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>Because </span><strong>independent workloads do not burst together</strong><span>, the more workloads you cram together - the more those idle spots get filled up and the more predictable the system becomes in aggregate. üí°</span></p><p><em><span>copyright: AWS; from this </span><a href="https://www.youtube.com/watch?v=v3HfUNQ0JOE" rel="">re:Invent presentation</a><span>.</span></em></p><p>AWS S3 is a massively multi-tenant storage service. It‚Äôs a gigantic distributed system consisting of many individually slow nodes that on aggregate allow you to access data faster than any single node can provide. S3 achieves this through:</p><ul><li><p>massive parallelization across the end-to-end path (user, client, server)</p></li><li><p>neat load-balancing tricks like the power of two random</p></li><li><p>spreading out data via erasure coding</p></li><li><p>lowering tail latency via hedge requests</p></li><li><p>the economies of multi-tenancy at world scale</p></li></ul><p>It started as a service optimized for backups, video and image storage for e-commerce websites - but eventually grew support being the main storage system used for analytics and machine learning on massive data lakes.</p><p><span>Nowadays, the growing trend is for entire data infrastructure projects to be based on top of S3. This gives them the benefits of stateless nodes (easy scaling, less management) while outsourcing difficult durability, replication and load-balancing problems to S3. And get this - it also reduces cloud costs.</span></p><div data-attrs="{&#34;url&#34;:&#34;https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;}" data-component-name="CaptionedButtonToDOM"><p>Learned something? Share with a colleague on Slack</p><p data-attrs="{&#34;url&#34;:&#34;https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;}" data-component-name="ButtonCreateButton"><a href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><p>Subscribe for more interesting dives in big data distributed systems (or the occassional small data gem).</p><p>S3 has a lot of other goodies up its bag, including:</p><ul><li><p>shuffle sharding at the DNS level</p></li><li><p>client library hedging requests by cancelling slow requests that pass the p95 threshold and sending new ones to a different host</p></li><li><p>software updates done erasure-coding-style, including rolling out their brand-new ShardStore storage system without any impact to their fleet</p></li><li><p>conway‚Äôs law and how it shapes S3‚Äôs architecture (consisting of 300+ microservices)</p></li><li><p>their durability culture, including continuous detection, durable chain of custody, a design process that includes durability threat modelling and formal verification</p></li></ul><p>These are generally shared in their annual S3 Deep Dive at re:Invent:</p><ul><li><p><a href="https://www.youtube.com/watch?v=v3HfUNQ0JOE&amp;t=583s&amp;pp=ygUQYXdzIHMzIHJlIGludmVudNIHCQngCQGHKiGM7w%3D%3D" rel="">2022</a><span> (video)</span></p></li><li><p><a href="https://www.youtube.com/watch?v=sYDJYqvNeXU" rel="">2023</a><span> (video)</span></p></li><li><p><a href="https://www.youtube.com/watch?v=NXehLy7IiPM&amp;t=398s&amp;pp=ygUQYXdzIHMzIHJlIGludmVudA%3D%3D" rel="">2024</a><span> (video)</span></p></li><li><p><a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html" rel="">Building and operating a pretty big storage system called S3</a><span> (article)</span></p></li></ul><p>Thank you to the S3 team for sharing what they‚Äôve built, and thank you for reading!</p></div></div></div>
  </body>
</html>
