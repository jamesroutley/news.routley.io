<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1938081">Original</a>
    <h1>What are the most important statistical ideas of the past 50 years?</h1>
    
    <div id="readability-page-1" class="page"><div id="S0001"><h2 id="_i3">1 The Most Important Statistical Ideas of the Past 50 Years</h2><p>A lot has happened in the past half century! The eight ideas reviewed below represent a categorization based on our experiences and reading of the literature and are not listed in a chronological order or in order of importance. They are separate concepts capturing different useful and general developments in statistics. The present review is intended to cover the territory and is influenced not just by our own experiences but also by discussions with others; nonetheless we recognize that any short overview will be incomplete, and we welcome further discussions from other perspectives.</p><p>Each of these ideas has pre-1970 antecedents, both in the theoretical statistics literature and in the practice of various applied fields. But each has developed enough in the past 50 years to have become something new.</p><div id="S0001-S2001"><h3 id="_i4">1.1 Counterfactual Causal Inference</h3><p>We begin with a cluster of different ideas that have appeared in statistics, econometrics, psychometrics, epidemiology, and computer science, all revolving around the challenges of causal inference, and all in some way bridging the gap between, on one hand, naive causal interpretation of observational inferences and, on the other, the recognition that correlation does not imply causation. The key idea is that causal identification is possible, under assumptions, and that one can state these assumptions rigorously and address them, in various ways, through design and analysis. Debate continues on the specifics of how to apply causal models to real data, but the work in this area over the past 50 years has allowed much more precision on the assumptions required for causal inference, and this in turn has stimulated work in statistical methods for these problems.</p><p>Different methods for causal inference have developed in different fields. In econometrics, the focus has been on the structural models and their implications for average treatment effects (Imbens and Angrist <span><a data-rid="CIT0077" data-reflink="_i22" href="#">1994</a></span>), in epidemiology the focus has been on inference with observational data (Greenland and Robins <span><a data-rid="CIT0064" data-reflink="_i22" href="#">1986</a></span>), psychologists have been aware of the importance of interactions and varying treatment effects (Cronbach <span><a data-rid="CIT0030" data-reflink="_i22" href="#">1975</a></span>), in statistics there has been work on matching and other approaches to adjust for and measure differences between treatment and control groups (Rosenbaum and Rubin <span><a data-rid="CIT0130" data-reflink="_i22" href="#">1983</a></span>). In all this work, there has been a common thread of modeling causal questions in terms of counterfactuals or potential outcomes, which is a big step beyond the earlier standard approach which did not clearly distinguish between descriptive and causal inferences. Key developments include Neyman (<span><a data-rid="CIT0116" data-reflink="_i22" href="#">1923</a></span>), Welch (<span><a data-rid="CIT0170" data-reflink="_i22" href="#">1937</a></span>), Rubin (<span><a data-rid="CIT0131" data-reflink="_i22" href="#">1974</a></span>), and Haavelmo (1943); see Heckman and Pinto (<span><a data-rid="CIT0069" data-reflink="_i22" href="#">2015</a></span>) for some background and VanderWeele (<span><a data-rid="CIT0161" data-reflink="_i22" href="#">2015</a></span>) for a recent review.</p><p>The purpose of the aforementioned methods is to define and estimate the effect of some specified treatment or exposure, adjusting for biases arising from imbalance, selection, and measurement errors. Another important area of research has been in causal discovery, where the goal is not to estimate a particular treatment effect but rather to learn something about the causal relations among several variables. There is a long history of such ideas using methods of path analysis, from researchers in various fields of application such as genetics (Wright <span><a data-rid="CIT0179" data-reflink="_i22" href="#">1923</a></span>), economics (Wold <span><a data-rid="CIT0177" data-reflink="_i22" href="#">1954</a></span>), and sociology (Duncan <span><a data-rid="CIT0039" data-reflink="_i22" href="#">1975</a></span>); as discussed by Wermouth (<span><a data-rid="CIT0172" data-reflink="_i22" href="#">1980</a></span>), these can be framed in terms of simultaneous equation models. Influential recent work in this area has linked to probabilistic ideas of graphical models (Spirtes, Glymour and Scheines <span><a data-rid="CIT0143" data-reflink="_i22" href="#">1993</a></span>; Heckerman, Geiger, and Chickering <span><a data-rid="CIT0068" data-reflink="_i22" href="#">1995</a></span>; Peters, Janzing, and Schölkopf <span><a data-rid="CIT0122" data-reflink="_i22" href="#">2017</a></span>). An important connection to psychology and computer science has arisen based on the idea that causal identification is a central task of cognition and thus should be a computable problem that can be formalized mathematically (Pearl <span><a data-rid="CIT0121" data-reflink="_i22" href="#">2009</a></span>). Path analysis and causal discovery can be framed in terms of potential outcomes, and vice versa (Morgan and Winship <span><a data-rid="CIT0110" data-reflink="_i22" href="#">2014</a></span>). However formulated, ideas and methods of counterfactual reasoning and causal structure have been influential within statistics and computer science and also in applied research and policy analysis.</p></div><div id="S0001-S2002"><h3 id="_i5">1.2 Bootstrapping and Simulation-Based Inference</h3><p>A trend of statistics in the past 50 years has been the substitution of computing for mathematical analysis, a move that began even before the onset of “big data” analysis. Perhaps, the purest example of a computationally defined statistical method is the bootstrap, in which some estimator is defined and applied to a set of randomly resampled datasets (Efron <span><a data-rid="CIT0040" data-reflink="_i22" href="#">1979</a></span>; Efron and Tibshirani <span><a data-rid="CIT0044" data-reflink="_i22" href="#">1993</a></span>). The idea is to consider the estimate as an approximate sufficient statistic of the data and to consider the bootstrap distribution as an approximation to the sampling distribution of the data. At a conceptual level, there is an appeal to thinking of prediction and resampling as fundamental principles from which one can derive statistical operations such as bias correction and shrinkage (Geisser <span><a data-rid="CIT0051" data-reflink="_i22" href="#">1975</a></span>).</p><p>Antecedents include the jackknife and cross-validation (Quenouille <span><a data-rid="CIT0126" data-reflink="_i22" href="#">1949</a></span>; Tukey <span><a data-rid="CIT0156" data-reflink="_i22" href="#">1958</a></span>; Stone <span><a data-rid="CIT0149" data-reflink="_i22" href="#">1974</a></span>; Geisser <span><a data-rid="CIT0051" data-reflink="_i22" href="#">1975</a></span>), but there was something particularly influential about the bootstrap idea in that its generality and simple computational implementation allowed it to be immediately applied to a wide variety of applications where conventional analytic approximations failed; see, for example, Felsenstein (<span><a data-rid="CIT0046" data-reflink="_i22" href="#">1985</a></span>). Availability of sufficient computational resources also helped as it became trivial to repeat inferences for many resampled datasets.</p><p>The increase in computational resources has made other related resampling and simulation-based approaches popular as well. In permutation testing, resampled datasets are generated by breaking the (possible) dependency between the predictors and target by randomly shuffling the target values. Parametric bootstrapping, prior and posterior predictive checking (Box <span><a data-rid="CIT0016" data-reflink="_i22" href="#">1980</a></span>; Rubin <span><a data-rid="CIT0132" data-reflink="_i22" href="#">1984</a></span>), and simulation-based calibration all create replicated datasets from a model instead of directly resampling from the data. Sampling from a known data-generating mechanism is commonly used to create simulation experiments to complement or replace mathematical theory when analyzing complex models or algorithms.</p></div><div id="S0001-S2003"><h3 id="_i6">1.3 Overparameterized Models and Regularization</h3><p>A major change in statistics since the 1970s, coming from many different directions, is the idea of fitting a model with a large number of parameters—sometimes more parameters than data points—using some regularization procedure to get stable estimates and good predictions. The idea is to get the flexibility of a nonparametric or highly parameterized approach, while avoiding the overfitting problem. Regularization can be implemented as a penalty function on the parameters or on the predicted curve (Good and Gaskins <span><a data-rid="CIT0059" data-reflink="_i22" href="#">1971</a></span>).</p><p>Early examples of richly parameterized models include Markov random fields (Besag <span><a data-rid="CIT0012" data-reflink="_i22" href="#">1974</a></span>), splines (Wahba and Wold <span><a data-rid="CIT0167" data-reflink="_i22" href="#">1975</a></span>; Wahba <span><a data-rid="CIT0165" data-reflink="_i22" href="#">1978</a></span>), and Gaussian processes (O’Hagan <span><a data-rid="CIT0118" data-reflink="_i22" href="#">1978</a></span>), followed by classification and regression trees (Breiman et al. <span><a data-rid="CIT0019" data-reflink="_i22" href="#">1984</a></span>), neural networks (Werbos <span><a data-rid="CIT0171" data-reflink="_i22" href="#">1981</a></span>; Rumelhart, Hinton, and Williams <span><a data-rid="CIT0133" data-reflink="_i22" href="#">1987</a></span>; Buntine and Weigend <span><a data-rid="CIT0021" data-reflink="_i22" href="#">1991</a></span>; MacKay <span><a data-rid="CIT0096" data-reflink="_i22" href="#">1992</a></span>; Neal <span><a data-rid="CIT0114" data-reflink="_i22" href="#">1996</a></span>), wavelet shrinkage (Donoho and Johnstone <span><a data-rid="CIT0037" data-reflink="_i22" href="#">1994</a></span>), lasso, horseshoe, and other alternatives to least squares (Dempster, Schatzoff, and Wermuth <span><a data-rid="CIT0033" data-reflink="_i22" href="#">1977</a></span>; Tibshirani <span><a data-rid="CIT0153" data-reflink="_i22" href="#">1996</a></span>; Carvalho, Polson, and Scott <span><a data-rid="CIT0023" data-reflink="_i22" href="#">2010</a></span>), and support vector machines (Cortes and Vapnik <span><a data-rid="CIT0028" data-reflink="_i22" href="#">1995</a></span>) and related theory (Vapnik <span><a data-rid="CIT0163" data-reflink="_i22" href="#">1998</a></span>).</p><p>The 1970s also saw the start of the development of Bayesian nonparametric priors on infinite dimensional families of probability models (Müller and Mitra <span><a data-rid="CIT0111" data-reflink="_i22" href="#">2013</a></span>), such as Dirichlet processes (Ferguson <span><a data-rid="CIT0047" data-reflink="_i22" href="#">1973</a></span>), Chinese restaurant processes (Aldous <span><a data-rid="CIT0002" data-reflink="_i22" href="#">1985</a></span>), Polya trees (Lavine <span><a data-rid="CIT0087" data-reflink="_i22" href="#">1992</a></span>; Mauldin, Sudderth, and Williams <span><a data-rid="CIT0101" data-reflink="_i22" href="#">1992</a></span>) and Pitman and Yor (1997) processed, and many other examples since then All these models have the feature of expanding with sample size, and with parameters that did not always have a direct interpretation but rather were part of a larger predictive system. In the Bayesian approach, the prior could be first considered in a function space, with the corresponding prior for the model parameters then derived indirectly.</p><p>Many of these models had limited usage until enough computational resources became easily available. Overparameterized models have continued to be developed in image recognition (Wu, Guo, and Zhu <span><a data-rid="CIT0180" data-reflink="_i22" href="#">2004</a></span>) and deep neural nets (Bengio, LeCun, and Hinton <span><a data-rid="CIT0008" data-reflink="_i22" href="#">2015</a></span>; Schmidhuber <span><a data-rid="CIT0136" data-reflink="_i22" href="#">2015</a></span>). Hastie, Tibshirani, and Wainwright (<span><a data-rid="CIT0067" data-reflink="_i22" href="#">2015</a></span>) had framed much of this work as the estimation of sparse structure, but we view regularization as being more general in that it also allows for dense models to be fit to the extent supported by data.</p><p>Along with a proliferation of statistical methods and their application to larger datasets, researchers have developed methods for tuning, adapting, and combining inferences from multiple fits, including stacking (Wolpert <span><a data-rid="CIT0178" data-reflink="_i22" href="#">1992</a></span>), Bayesian model averaging (Hoeting et al. <span><a data-rid="CIT0074" data-reflink="_i22" href="#">1999</a></span>), boosting (Freund and Schapire <span><a data-rid="CIT0048" data-reflink="_i22" href="#">1997</a></span>), and gradient boosting (Friedman <span><a data-rid="CIT0049" data-reflink="_i22" href="#">2001</a></span>). These advances have been accompanied by an alternative view of the foundations of statistics based on prediction rather than modeling (Breiman <span><a data-rid="CIT0018" data-reflink="_i22" href="#">2001</a></span>).</p></div><div id="S0001-S2004"><h3 id="_i7">1.4 Bayesian Multilevel Models</h3><p>Multilevel or hierarchical models have parameters that vary by group, allowing models to adapt to cluster sampling, longitudinal studies, time-series cross-sectional data, meta-analysis, and other structured settings. In a regression context, a multilevel model can be viewed as a particular parameterized covariance structure or as a probability distribution where the number of parameters increases in proportion to the data.</p><p>Multilevel models can be seen as Bayesian in that they include probability distributions for unknown latent characteristics or varying parameters. Conversely, Bayesian models have a multilevel structure with distributions for data given parameters and for parameters given hyperparameters.</p><p>The idea of partial pooling of local and general information is inherent in the mathematics of prediction from noisy data and, as such, dates back to Laplace and Gauss and is implicit in the ideas of Galton. Partial pooling was used in specific application areas such as animal breeding (Henderson et al. <span><a data-rid="CIT0070" data-reflink="_i22" href="#">1959</a></span>), and its general relevance to multiplicity in statistical estimation problems was given a theoretical boost by the work of Stein (<span><a data-rid="CIT0145" data-reflink="_i22" href="#">1955</a></span>) and James and Stein (<span><a data-rid="CIT0079" data-reflink="_i22" href="#">1960</a></span>), ultimately inspiring work in areas ranging from psychology (Novick et al. <span><a data-rid="CIT0117" data-reflink="_i22" href="#">1972</a></span>) to pharmacology (Sheiner, Rosenberg, and Melmon <span><a data-rid="CIT0138" data-reflink="_i22" href="#">1972</a></span>) to survey sampling (Fay and Herriot <span><a data-rid="CIT0045" data-reflink="_i22" href="#">1979</a></span>). Lindley and Smith (<span><a data-rid="CIT0092" data-reflink="_i22" href="#">1972</a></span>) and Lindley and Novick (<span><a data-rid="CIT0091" data-reflink="_i22" href="#">1981</a></span>) supplied a mathematical structure based on estimating hyperparameters of the multivariate normal distribution, with Efron and Morris (<span><a data-rid="CIT0042" data-reflink="_i22" href="#">1971</a></span>, <span><a data-rid="CIT0043" data-reflink="_i22" href="#">1972</a></span>) provided a corresponding decision-theoretic justification, and then these ideas were folded into regression modeling and applied to a wide range of problems with structured data (e.g., Liang and Zeger <span><a data-rid="CIT0090" data-reflink="_i22" href="#">1986</a></span>; Lax and Phillips <span><a data-rid="CIT0088" data-reflink="_i22" href="#">2012</a></span>). From a different direction, shrinkage of multivariate parameters has been given an information-theoretic justification (Donoho <span><a data-rid="CIT0034" data-reflink="_i22" href="#">1995</a></span>). Rather than considering multilevel modeling as a specific statistical model or computational procedure, we prefer to think of it as a framework for combining different sources of information, and as such it arises whenever we wish to make inferences from a subset of data (small-area estimation) or to generalize data to new problems (meta-analysis). Similarly, Bayesian inference has been valuable not just as a way of combining prior information with data but also as a way of accounting for uncertainty for inference and decision making.</p></div><div id="S0001-S2005"><h3 id="_i8">1.5 Generic Computation Algorithms</h3><p>The advances in modeling we have discussed have only become possible due to modern computing. But this is not just larger memory, faster CPUs, efficient matrix computations, user-friendly languages, and other innovations in computing. A key component has been advances in statistical algorithms for efficient computing.</p><p>The innovative statistical algorithms of the past 50 years are statistical in the sense of being motivated and developed in the context of the structure of a statistical problem. The EM algorithm (Dempster, Laird, and Rubin <span><a data-rid="CIT0032" data-reflink="_i22" href="#">1977</a></span>; Meng and van Dyk <span><a data-rid="CIT0104" data-reflink="_i22" href="#">1997</a></span>), Gibbs sampler (Geman and Geman <span><a data-rid="CIT0056" data-reflink="_i22" href="#">1984</a></span>; Gelfand and Smith <span><a data-rid="CIT0052" data-reflink="_i22" href="#">1990</a></span>), particle filters (Kitagawa <span><a data-rid="CIT0083" data-reflink="_i22" href="#">1993</a></span>; Gordon, Salmond, and Smith <span><a data-rid="CIT0062" data-reflink="_i22" href="#">1993</a></span>; Del Moral <span><a data-rid="CIT0031" data-reflink="_i22" href="#">1996</a></span>), variational inference (Jordan et al. <span><a data-rid="CIT0080" data-reflink="_i22" href="#">1999</a></span>), and expectation propagation (Minka <span><a data-rid="CIT0106" data-reflink="_i22" href="#">2001</a></span>, Heskes et al. <span><a data-rid="CIT0071" data-reflink="_i22" href="#">2005</a></span>) in different ways make use of the conditional independence structures of statistical models. The Metropolis algorithm (Hastings, 1970) and hybrid or Hamiltonian Monte Carlo (Duane et al. <span><a data-rid="CIT0038" data-reflink="_i22" href="#">1987</a></span>) were less directly motivated by statistical concerns—these were methods that were originally developed to compute high-dimensional probability distributions in physics—but they have become adapted to statistical computing in the same way that optimization algorithms were adopted in an earlier era to compute least squares and maximum likelihood estimates. The method called approximate Bayesian computation, in which posterior inferences are obtained by simulating from the generative model instead of evaluating the likelihood function, can be useful if the analytic form of the likelihood is intractable or very costly to compute (Rubin <span><a data-rid="CIT0132" data-reflink="_i22" href="#">1984</a></span>; Tavaré et al. <span><a data-rid="CIT0151" data-reflink="_i22" href="#">1997</a></span>; Marin et al. <span><a data-rid="CIT0099" data-reflink="_i22" href="#">2012</a></span>). Martin, Frazier, and Robert’s (<span><a data-rid="CIT0100" data-reflink="_i22" href="#">2020</a></span>) review the history of computational methods in Bayesian statistics.</p><p>Throughout the history of statistics, advances in data analysis, probability modeling, and computing have gone together, with new models motivating innovative computational algorithms and new computing techniques opening the door to more complex models and new inferential ideas, as we have already noted in the context of high-dimensional regularization, multilevel modeling, and the bootstrap. The generic automatic inference algorithms allowed decoupling the development of the models so that changing the model did not require changes to the algorithm implementation.</p></div><div id="S0001-S2006"><h3 id="_i9">1.6 Adaptive Decision Analysis</h3><p>From the 1940s through the 1960s, decision theory was often framed as foundational to statistics, via utility maximization (Wald <span><a data-rid="CIT0168" data-reflink="_i22" href="#">1949</a></span>; Savage <span><a data-rid="CIT0134" data-reflink="_i22" href="#">1954</a></span>), error-rate control (Tukey <span><a data-rid="CIT0155" data-reflink="_i22" href="#">1953</a></span>; Scheffé <span><a data-rid="CIT0135" data-reflink="_i22" href="#">1959</a></span>), and empirical Bayes analysis (Robbins <span><a data-rid="CIT0128" data-reflink="_i22" href="#">1956</a></span>,<span><a data-rid="CIT0129" data-reflink="_i22" href="#">1964</a></span>), and recent decades have seen developments following up this work, in the Bayesian decision theory (Berger <span><a data-rid="CIT0010" data-reflink="_i22" href="#">1985</a></span>) and false discovery rate analysis (Benjamini and Hochberg <span><a data-rid="CIT0009" data-reflink="_i22" href="#">1995</a></span>). Decision theory has also been influenced from the outside by psychology research on heuristics and biases in human decision making (Kahneman, Slovic, and Tversky <span><a data-rid="CIT0081" data-reflink="_i22" href="#">1982</a></span>; Gigerenzer and Todd <span><a data-rid="CIT0057" data-reflink="_i22" href="#">1999</a></span>).</p><p>One can also view decision making as an area of statistical application. Some important developments in statistical decision analysis involve Bayesian optimization (Mockus <span><a data-rid="CIT0107" data-reflink="_i22" href="#">1974</a></span>; Mockus <span><a data-rid="CIT0108" data-reflink="_i22" href="#">2012</a></span>; Shahriari et al. <span><a data-rid="CIT0137" data-reflink="_i22" href="#">2015</a></span>) and reinforcement learning (Sutton and Barto <span><a data-rid="CIT0150" data-reflink="_i22" href="#">2018</a></span>), which are related to a renaissance in experimental design for A/B testing in industry and online learning in many engineering applications. Recent advances in computation have made it possible to use richly parameterized models such as Gaussian process and neural networks as priors for functions in adaptive decision analysis, and to perform large-scale reinforcement learning, for example to create artificial intelligence to control robots, generate text, and play games such as Go (Silver et al. <span><a data-rid="CIT0140" data-reflink="_i22" href="#">2017</a></span>).</p><p>Much of this work has been done outside of statistics, with methods such as nonnegative matrix factorization (Paatero and Tapper <span><a data-rid="CIT0120" data-reflink="_i22" href="#">1994</a></span>), nonlinear dimension reduction (Lee and Verleysen <span><a data-rid="CIT0089" data-reflink="_i22" href="#">2007</a></span>), generative adversarial networks (Goodfellow et al. <span><a data-rid="CIT0061" data-reflink="_i22" href="#">2014</a></span>), and autoencoders (Goodfellow, Bengio, and Courville <span><a data-rid="CIT0060" data-reflink="_i22" href="#">2016</a></span>): these are all unsupervised learning methods for finding structures and decompositions.</p></div><div id="S0001-S2007"><h3 id="_i10">1.7 Robust Inference</h3><p>The idea of robustness is central to modern statistics, and it is all about the idea that we can use models even when they have assumptions that are not true. An important part of statistical theory is to develop models that work well, under realistic violations of these assumptions. Early work in this area was synthesized by Tukey (<span><a data-rid="CIT0157" data-reflink="_i22" href="#">1960</a></span>), see Stigler (<span><a data-rid="CIT0147" data-reflink="_i22" href="#">2010</a></span>) for a historical review. Following the theoretical work of Huber (<span><a data-rid="CIT0075" data-reflink="_i22" href="#">1972</a></span>) and others, researchers have developed robust methods that have been influential in practice, especially in economics, where there is acute awareness of the imperfections of statistical models. In economic theory there is the idea of the “as if” analysis and the reduced-form model, so it makes sense that econometricians are interested in statistical procedures that work well under a range of assumptions. For example, applied researchers in economics and other social sciences make extensive use of robust standard errors (White <span><a data-rid="CIT0173" data-reflink="_i22" href="#">1980</a></span>) and partial identification (Manski <span><a data-rid="CIT0098" data-reflink="_i22" href="#">1990</a></span>).</p><p>In general, though, the main impact of robustness in statistical research is not in the development of particular methods, so much as in the idea of evaluating statistical procedures under what Bernardo and Smith (<span><a data-rid="CIT0011" data-reflink="_i22" href="#">1994</a></span>) call the <span><img src="https://:0" alt="" data-formula-source="{&#34;type&#34; : &#34;image&#34;, &#34;src&#34; : &#34;/na101/home/literatum/publisher/tandf/journals/content/uasa20/2021/uasa20.v116.i536/01621459.2021.1938081/20211217/images/uasa_a_1938081_ilm0001.gif&#34;}"/><span></span></span><span><img src="https://:0" alt="" data-formula-source="{&#34;type&#34; : &#34;mathjax&#34;}"/><math display="inline"><mi mathvariant="script">M</mi></math></span>-open world in which the data-generating process does not fall within the class of fitted probability models. Greenland (<span><a data-rid="CIT0063" data-reflink="_i22" href="#">2005</a></span>) argued that researchers should explicitly account for sources of error that are not traditionally included in statistical models. Concerns of robustness are relevant for the densely parameterized models that are characteristic of much of modern statistics, and this has implications for model evaluation more generally (Navarro <span><a data-rid="CIT0113" data-reflink="_i22" href="#">2019</a></span>). There is a connection between robustness of a statistical method to model misspecification, and a workflow involving model checking and model improvement (Box <span><a data-rid="CIT0016" data-reflink="_i22" href="#">1980</a></span>).</p></div><div id="S0001-S2008"><h3 id="_i11">1.8 Exploratory Data Analysis</h3><p>The statistical ideas discussed above all involve some mixture of intense theory and intense computation. From a completely different direction, there has been an influential back-to-basics movement, eschewing probability models and focusing on graphical visualization of data. The virtues of statistical graphics were convincingly argued in influential books by Tukey (<span><a data-rid="CIT0159" data-reflink="_i22" href="#">1977</a></span>) and Tufte (<span><a data-rid="CIT0154" data-reflink="_i22" href="#">1983</a></span>), and many of these ideas entered statistical practice through their implementation in the data analysis environment S (Chambers et al. <span><a data-rid="CIT0025" data-reflink="_i22" href="#">1983</a></span>), a precursor to R, which is currently the dominant statistics software in many areas of statistics and its application.</p><p>Following Tukey (<span><a data-rid="CIT0158" data-reflink="_i22" href="#">1962</a></span>), the proponents of exploratory data analysis have emphasized the limitations of asymptotic theory and the corresponding benefits of open-ended exploration and communication (Cleveland <span><a data-rid="CIT0027" data-reflink="_i22" href="#">1985</a></span>) along with a general view of data science as going beyond statistical theory (Chambers <span><a data-rid="CIT0024" data-reflink="_i22" href="#">1993</a></span>; Donoho <span><a data-rid="CIT0036" data-reflink="_i22" href="#">2017</a></span>). This fits into a view of statistical modeling that is focused more on discovery than on the testing of fixed hypotheses, and as such has been influential not just in the development of specific graphical methods but also in moving the field of statistics away from theorem-proving and toward a more open and, we would say, healthier perspective on the role of learning from data in science. An example in medical statistics is the much-cited article by Bland and Altman (<span><a data-rid="CIT0014" data-reflink="_i22" href="#">1986</a></span>) that recommended graphical methods for data comparison in place of correlations and regressions.</p><p>In addition, attempts have been made to formalize exploratory data analysis: Gelman (<span><a data-rid="CIT0053" data-reflink="_i22" href="#">2003</a></span>) connected data display and visualization to Bayesian predictive checks, and Wilkinson (<span><a data-rid="CIT0176" data-reflink="_i22" href="#">2005</a></span>) formalized the comparisons and data structures inherent in statistical graphics, in a way that Wickham (<span><a data-rid="CIT0175" data-reflink="_i22" href="#">2016</a></span>) was able to implement into a highly influential set of R packages that has transformed statistical practice in many fields.</p><p>Advances in computation have allowed practitioners to build large complicated models quickly, leading to a process in which ideas of statistical graphics are useful in understanding the relation between data, fitted model, and predictions. The term “exploratory model analysis” (Unwin, Volinsky, and Winkler <span><a data-rid="CIT0160" data-reflink="_i22" href="#">2003</a></span>, Wickham <span><a data-rid="CIT0174" data-reflink="_i22" href="#">2006</a></span>) has sometimes been used to capture the experimental nature of the data analysis process, and efforts have been made to include visualization within the workflow of model building and data analysis (Gabry et al. <span><a data-rid="CIT0050" data-reflink="_i22" href="#">2019</a></span>, Gelman et al. <span><a data-rid="CIT0054" data-reflink="_i22" href="#">2020</a></span>).</p></div></div><div id="S0002"><h2 id="_i12">2 What These Ideas Have in Common and How They Differ</h2><p>It would be tempting to say that a common feature of all these methods is catchy names and good marketing. But we suspect that the names of these methods are catchy only in retrospect. Terms such as “counterfactual,” “bootstrap,” “stacking,” and “boosting” could well sound jargony rather than impressive, and we suspect it is the value of the methods that has made the names sound appealing, rather than the reverse.</p><div id="S0002-S2001"><h3 id="_i13">2.1 Ideas Lead to Methods and Workflows</h3><p>The benefit of application to statistical theory is clear. What about the benefits the other way? Most directly, one can view theory as a shortcut to computation. Such shortcuts will always be needed: demands for modeling inevitably grow with computing power; hence, the value of analytic summaries and approximations. In addition, theory can help us understand how a statistical method works, and the logic of mathematics can inspire new models and approaches to data analysis.</p><p>We consider the ideas listed above to be particularly important in that each of them was not so much a method for solving an existing problem, as an opening to new ways of thinking about statistics and new ways of data analysis.</p><p>To put it another way, each of these ideas was a codification, bringing inside the tent an approach that had been considered more a matter of taste or philosophy than statistics: </p><ul><li><p>The counterfactual framework placed causal inference within a statistical or predictive framework in which causal estimands could be precisely defined and expressed in terms of unobserved data within a statistical model, connecting to ideas in survey sampling and missing-data imputation (Little <span><a data-rid="CIT0093" data-reflink="_i22" href="#">1993</a></span>; Little and Rubin <span><a data-rid="CIT0094" data-reflink="_i22" href="#">2002</a></span>).</p></li><li><p>The bootstrap opened the door to a form of implicit nonparametric modeling.</p></li><li><p>Overparameterized models and regularization formalized and generalized the existing practice of restricting a model’s size based on the ability to estimate its parameters from the data, which is related to cross-validation and information criteria (Akaike <span><a data-rid="CIT0001" data-reflink="_i22" href="#">1973</a></span>; Mallows <span><a data-rid="CIT0097" data-reflink="_i22" href="#">1973</a></span>; Watanabe <span><a data-rid="CIT0169" data-reflink="_i22" href="#">2010</a></span>).</p></li><li><p>Multilevel models formalized “empirical Bayes” techniques of estimating a prior distribution from data, leading to the use of such methods with more computational and inferential stability in a much wider class of problems.</p></li><li><p>Generic computation algorithms make it possible for applied practitioners to quickly fit advanced models for causal inference, multilevel analysis, reinforcement learning, and many other areas, leading to a broader impact of core ideas in statistics and machine learning.</p></li><li><p>Adaptive decision analysis connects engineering problems of optimal control to the field of statistical learning, going far beyond classical experimental design.</p></li><li><p>Robust inference formalized intuitions about inferential stability, framing these questions in a way that allowed formal evaluation and modeling of different procedures to handle otherwise nebulous concerns about outliers and model misspecification, and ideas of robust inference have informed ideas of nonparametric estimation (Owen <span><a data-rid="CIT0119" data-reflink="_i22" href="#">1988</a></span>).</p></li><li><p>Exploratory data analysis moved graphical techniques and discovery into the mainstream of statistical practice, just in time for the use of these tools to better understand and diagnose problems of new complex classes of probability models that are being fit to data.</p></li></ul></div><div id="S0002-S2002"><h3 id="_i14">2.2 Advances in Computing</h3><p>Meta-algorithms—workflows that make use of existing models and inferential procedures—have always been with us in statistics: consider least squares, the method of moments, maximum likelihood, and so forth. One characteristic aspect of many of the machine learning meta-algorithms that have been developed in the past 50 years is that they involve splitting the data or model in some way. The learning meta-algorithms are associated with divide-and-conquer computational methods, most notably variational Bayes and expectation propagation, which can be viewed as generalizations of algorithms that iterate over parameters or that combine inference from subsets of the data.</p><p>Meta-algorithms and iterative computations are an important development in statistics for two reasons. First, the general idea of combining information from multiple sources, or creating a strong learner by combining weak learners, can be applied broadly, beyond the examples where such meta-algorithms were originally developed. Second, adaptive algorithms play well with online learning and ultimately can be viewed as representing a modern view of statistics in which data and computation are dispersed, a view in which information exchange and computational architecture are part of the meta-model or inferential procedure (Efron and Hastie <span><a data-rid="CIT0041" data-reflink="_i22" href="#">2016</a></span>).</p><p>It is no surprise that new methods take advantage of new technical tools: as computing improves in speed and scope, statisticians are no longer limited to simple models with analytic solutions and simple closed-form algorithms such as least squares. We can outline how the above-listed ideas make use of modern computation: </p><ul><li><p>Several of the ideas—bootstrapping, overparameterized models, and machine learning meta-analysis—directly take advantage of computing speed and could not easily be imagined in a pre-computer world. For example, the popularity of neural networks increased substantially only after the introduction of efficient GPU cards and cloud computing.</p></li><li><p>Also important, beyond computing power, is the dispersion of computing resources: desktop computers allowed statisticians and computer scientists to experiment with new methods and then allowed practitioners to use them.</p></li><li><p>Exploratory data analysis began with pencil-and-paper graphs but has completely changed with developments in computer graphics.</p></li><li><p>In the past, Bayesian inference was constrained to simple models that could be solved analytically. With the increase in computing power, variational and Markov chain simulation methods have allowed separation of model building and development of inference algorithms, leading to probabilistic programming that has freed domain experts in different fields to focus on model building and get inference done automatically. This resulted in an increase in popularity of Bayesian methods in many applied fields starting in the 1990s.</p></li><li><p>Adaptive decision analysis, Bayesian optimization, and online learning are used in computationally and data-intensive problems such as optimizing big machine learning and neural network models, real-time image processing, and natural language processing.</p></li><li><p>Robust statistics are not necessarily computationally intensive, but their use was associated with a computation-fueled move away from closed-form estimates such as least squares. The development and understanding of robust methods was facilitated by a simulation study that used extensive computation for its time (Andrews et al. <span><a data-rid="CIT0005" data-reflink="_i22" href="#">1972</a></span>).</p></li><li><p>Shrinkage for multivariate inference can be justified not just by statistical efficiency but also on computational grounds, motivating a new kind of asymptotic theory (Donoho <span><a data-rid="CIT0035" data-reflink="_i22" href="#">2006</a></span>; Candès, Romberg, and Tao <span><a data-rid="CIT0022" data-reflink="_i22" href="#">2008</a></span>).</p></li><li><p>The key ideas of counterfactual causal inference are theoretical, not computational, but in recent years causal inference has advanced by the use of computationally intensive nonparametric methods, leading to a unification of causal and predictive modeling in statistics, economics, and machine learning (Hill <span><a data-rid="CIT0072" data-reflink="_i22" href="#">2011</a></span>; Chernozhukov et al. <span><a data-rid="CIT0026" data-reflink="_i22" href="#">2018</a></span>; Wager and Athey <span><a data-rid="CIT0164" data-reflink="_i22" href="#">2018</a></span>).</p></li></ul></div><div id="S0002-S2003"><h3 id="_i15">2.3 Big Data</h3><p>In addition to the opportunities opened up for statistical analysis, modern computing has also yielded big data in ways that have inspired the application and development of new statistical methods: examples include gene arrays, streaming image and text data, and online control problems such as self-driving cars. Indeed, one reason for the popularity of the term “data science” is because, in such problems, data processing and efficient computing can be as important as the statistical methods used to fit the data.</p><p>A common feature of all the ideas discussed in this article and they facilitate the use of more data, compared to previously existing approaches: </p><ul><li><p>The counterfactual framework allows causal inference from observational data using the same structure used to model controlled experiments.</p></li><li><p>Bootstrapping can be used for bias correction and variance estimation for complex surveys, experimental designs, and other data structures where analytical calculations are not possible.</p></li><li><p>Regularization allows users to include more predictors in a model without so much concern about overfitting.</p></li><li><p>Multilevel models use partial pooling to incorporate information from different sources, applying the principle of meta-analysis more generally.</p></li><li><p>Generic computation algorithms allow users to fit larger models, which can be necessary to connect available data to underlying questions of interest.</p></li><li><p>Adaptive decision analysis makes use of stochastic optimization methods developed in numerical analysis.</p></li><li><p>Robust inference allows more routine use of data with outliers, correlations, and other aspects that could get in the way of conventional statistical modeling.</p></li><li><p>Exploratory data analysis opens the door to visualization of complex datasets and has motivated the development of tidy data analysis and the integration of statistical analysis, computation, and communication.</p></li></ul><p>The past 50 years have also seen the development of statistical programming environments, most notably S (Becker, Chambers, and Wilks <span><a data-rid="CIT0007" data-reflink="_i22" href="#">1988</a></span>) and then R (Ihaka and Gentleman <span><a data-rid="CIT0076" data-reflink="_i22" href="#">1996</a></span>), and general-purpose inference engines beginning with BUGS (Spiegelhalter <span><a data-rid="CIT0142" data-reflink="_i22" href="#">1994</a></span>) and its successors (Lunn et al. <span><a data-rid="CIT0095" data-reflink="_i22" href="#">2009</a></span>). More recently, ideas of numerical analysis, automated inference, and statistical computing have started to mix, in the form of reproducible research environments such as Jupyter notebooks and probabilistic programming environments such as Stan, Tensorflow, and Pyro (Stan Development Team <span><a data-rid="CIT0144" data-reflink="_i22" href="#">2020</a></span>; Tensorflow <span><a data-rid="CIT0152" data-reflink="_i22" href="#">2000</a></span>; Pyro <span><a data-rid="CIT0125" data-reflink="_i22" href="#">2020</a></span>). So we can expect at least some partial unification of inferential and computing methods, as demonstrated for example by the use of automatic differentiation for optimization, sampling, and sensitivity analysis.</p></div><div id="S0002-S2004"><h3 id="_i16">2.4 Connections and Interactions Among These Ideas</h3><p>Stigler (<span><a data-rid="CIT0148" data-reflink="_i22" href="#">2016</a></span>) has argued for the relevance of certain common themes underlying apparently disparate areas of statistics. This idea of interconnection can be seen to apply to recent developments as well. For example, what is the connection between robust statistics (which focuses on departures from particular model assumptions) and exploratory data analysis (which is traditionally presented as being not interested in models at all)? Exploratory methods such as residual plots and hanging rootograms can be derived from specific model classes (additive regression and the Poisson distribution, respectively) but their value comes in large part from their interpretability without reference to the models that inspired them. One can similarly consider a method such as least squares on its own terms, as an operation on data, then study the class of data-generating processes for which it will perform well, and then use the results of such a theoretical analysis to propose more robust procedures that extend the range of useful applicability, whether defined based on breakdown point, minimax risk, or otherwise. Conversely, purely computational methods such as Monte Carlo evaluation of integrals can fruitfully be interpreted as solutions to statistical inference problems (Kong et al. <span><a data-rid="CIT0085" data-reflink="_i22" href="#">2003</a></span>).</p><p>For another connection, the potential outcome framework for causal inference, which allows a different treatment effect for each unit in the population, lends itself naturally to a meta-analytic approach in which effects can vary, and this can be modeled using multilevel regression in the analyses of experiments or observational studies. Work on the bootstrap can, in retrospect, give us a new perspective on empirical Bayes (multilevel) inference as a nonparametric approach in which a normal distribution or other parametric model is used for partial pooling but final estimates are not restricted to any parametric form. And research on regularizing wavelets and other richly parameterized models has an unexpected connection to the stable inferential procedures developed in the context of robustness.</p><p>Other methodological connections are more obvious. Regularized overparameterized models are optimized using machine-learning meta-algorithms, which in turn can yield inferences that are robust to contamination. To draw these connections another way, robust regression models correspond to mixture distributions which can be viewed as multilevel models, and these can be fitted using Bayesian inference. Deep learning models are related to a form of multilevel logistic regression and relates to reproducing kernel Hilbert spaces, which are used in splines and support vector machines (Kimeldorf and Wahba <span><a data-rid="CIT0082" data-reflink="_i22" href="#">1971</a></span>; Wahba <span><a data-rid="CIT0166" data-reflink="_i22" href="#">2002</a></span>).</p><p>Highly parameterized machine learning methods can be framed as Bayesian hierarchical models, with regularizing penalty functions corresponding to hyperpriors, and unsupervised learning models can be framed as mixture models with unknown group memberships. In many cases the choice of whether to use a Bayesian generative framework depends on computation, and this can go in both ways: Bayesian computational methods can help capture uncertainty in inference and prediction, and efficient optimization algorithms can be used to approximate model-based inference.</p><p>Many of the ideas we have been discussing involve rich parameterizations followed by some statistical or computational tools for regularization. As such, they can be considered as more general implementations of the idea of sieves—models that get larger as more data become available (Grenander <span><a data-rid="CIT0065" data-reflink="_i22" href="#">1981</a></span>; Geman and Hwang <span><a data-rid="CIT0055" data-reflink="_i22" href="#">1982</a></span>; Shen and Wong <span><a data-rid="CIT0139" data-reflink="_i22" href="#">1994</a></span>).</p></div><div id="S0002-S2005"><h3 id="_i17">2.5 Links to Other New and Useful Developments in Statistics</h3><p>Where do particular statistical models fit into our story? Here, we are thinking of influential work such as hazard regression (Cox <span><a data-rid="CIT0029" data-reflink="_i22" href="#">1972</a></span>), generalized linear models (Nelder <span><a data-rid="CIT0115" data-reflink="_i22" href="#">1977</a></span>; McCullagh and Nelder <span><a data-rid="CIT0102" data-reflink="_i22" href="#">1989</a></span>), structural equation models (Baron and Kenny <span><a data-rid="CIT0006" data-reflink="_i22" href="#">1986</a></span>), latent classification (Blei, Ng, and Jordan <span><a data-rid="CIT0015" data-reflink="_i22" href="#">2003</a></span>), Gaussian processes (O’Hagan <span><a data-rid="CIT0118" data-reflink="_i22" href="#">1978</a></span>; Rasmussen and Williams <span><a data-rid="CIT0127" data-reflink="_i22" href="#">2006</a></span>), and deep learning (Hinton, Osindero, and Teh <span><a data-rid="CIT0073" data-reflink="_i22" href="#">2006</a></span>; Bengio, LeCun, and Hinton <span><a data-rid="CIT0008" data-reflink="_i22" href="#">2015</a></span>; Schmidhuber <span><a data-rid="CIT0136" data-reflink="_i22" href="#">2015</a></span>), and models for structured data such as time series (Box and Jenkins <span><a data-rid="CIT0017" data-reflink="_i22" href="#">1976</a></span>; Brillinger <span><a data-rid="CIT0020" data-reflink="_i22" href="#">1981</a></span>), spatial processes (Besag <span><a data-rid="CIT0012" data-reflink="_i22" href="#">1974</a></span>; Besag <span><a data-rid="CIT0013" data-reflink="_i22" href="#">1986</a></span>), network data (Kolaczyk <span><a data-rid="CIT0084" data-reflink="_i22" href="#">2009</a></span>), and self-similar processes (Künsch <span><a data-rid="CIT0086" data-reflink="_i22" href="#">1987</a></span>). These models and their associated applied successes can be thought of demonstrations of the ideas developed in the first section of this article, or as challenges that motivated many of these developments (e.g., generalized linear models with many predictors motivating regularization methods, or Gaussian process models motivating advances in approximate computation and a shift toward predictive evaluation), or as bridges between different statistical ideas (e.g., structural equation models connecting graphical models and causal inference, or deep learning connecting Bayesian multilevel models and generic computation algorithms). It is not possible to disentangle models, methods, applications, and principles, and alternative histories of statistics take any of these as an organizing principle.</p><p>To discuss the connections among different conceptual advances is not to deny that debates remain regarding appropriate use and interpretation of statistical methods. For example, there is a duality between false discovery rate and multilevel modeling, but procedures based on these different principles can give different results. Multilevel models are typically fit using Bayesian methods, and nothing is pooled all the way to zero in the posterior distribution. In contrast, false discovery rate methods are typically applied using <i>p</i>-value thresholds, with the goal of identifying some small number of statistically significantly nonzero results. For another example, in causal inference, there is increasing interest in densely parameterized machine learning predictions followed by poststratification to obtain population causal estimates of specified exposures or treatments, but in more open-ended settings there is the goal of discovering nonzero causal relationships. Again, different methods are used, depending on whether the aim is dense prediction or sparse discovery.</p><p>Finally, we can connect research in statistical methods to trends in the application of statistics within science and engineering. An entire series of articles could be written just on this topic. Here, we mention one such area, the replication crisis or reproducibility revolution in biology, psychology, economics, and other sciences. Landmark papers in the reproducibility revolution include Meehl (<span><a data-rid="CIT0103" data-reflink="_i22" href="#">1978</a></span>) outlining the philosophical flaws in the standard use of null hypothesis significance testing to make scientific claims, Ioannidis (<span><a data-rid="CIT0078" data-reflink="_i22" href="#">2005</a></span>) arguing that most published studies in medicine were making claims unsupported by their statistical data, and Simmons, Nelson, and Simonsohn (<span><a data-rid="CIT0141" data-reflink="_i22" href="#">2011</a></span>) explaining how “researcher degrees of freedom” can enable researchers to routinely obtain statistical significance even from data that are pure noise. Some of the proposed remedies are procedural (e.g., Amrhein, Greenland, and McShane <span><a data-rid="CIT0003" data-reflink="_i22" href="#">2019</a></span>), but there have also been suggestions that some of the problems with nonreplicable research can be resolved using multilevel models, partially pooling estimates toward zero to better reflect the population of effect sizes under study (van Zwet, Schwab, and Senn <span><a data-rid="CIT0162" data-reflink="_i22" href="#">2020</a></span>). Questions of reproducibility and stability also relate directly to bootstrapping and robust statistics (Yu <span><a data-rid="CIT0181" data-reflink="_i22" href="#">2013</a></span>).</p></div></div><div id="S0003"><h2 id="_i18">3 What Will be the Important Statistical Ideas of the Next Few Decades?</h2><div id="S0003-S2001"><h3 id="_i19">3.1 Looking Backward</h3><p>In considering the most important developments since 1970, it could also make sense to reflect upon the most important statistical ideas of 1920–1970 (these could include quality control, latent-variable modeling, sampling theory, experimental design, classical and Bayesian decision analysis, confidence intervals and hypothesis testing, maximum likelihood, the analysis of variance, and objective Bayesian inference—quite a list!), 1870–1920 (classification of probability distributions, regression to the mean, phenomenological modeling of data), and previous centuries, as studied by Stigler (<span><a data-rid="CIT0146" data-reflink="_i22" href="#">1986</a></span>) and others.</p><p>In this article, we have attempted to offer a broad perspective, reflecting the different perspectives of the authors. But others will have their own takes on what are the most important statistical ideas of the past 50 years, and another view is gained by looking at the topics of articles published in statistics journals (Anderlucci, Montanari, and Viroli <span><a data-rid="CIT0004" data-reflink="_i22" href="#">2019</a></span>). Indeed, the point of asking what are the most important ideas is not so much to answer the question, as to stimulate discussion of what it means for a statistical idea to be important. In the present article, we have avoided ranking papers by citation counts or other numerical measure, but implicitly we are measuring intellectual influence in a page-rank-like way, in that we are trying to focus on the ideas that have influenced the development of methods that have influenced statistical practice.</p><p>We are interested in the others’ views on what are the most influential statistical ideas of the last half century and how these ideas have combined to affect the practice of statistics and scientific learning.</p></div><div id="S0003-S2002"><h3 id="_i20">3.2 Looking Forward</h3><p>What will come next? We agree with Popper (<span><a data-rid="CIT0124" data-reflink="_i22" href="#">1957</a></span>) that one cannot anticipate all future scientific developments, but we might have some ideas about how current trends will continue, beyond the general observation that important work will be driven by applications.</p><p>The safest bet is that there will be continuing progress on existing combinations of methods: causal inference with rich models for potential outcomes, estimated using regularization; complex models for structured data such as networks evolving over time, robust inference for multilevel models; exploratory data analysis for overparameterized models (Mimno, Blei, and Engelhardt <span><a data-rid="CIT0105" data-reflink="_i22" href="#">2015</a></span>); subsetting and machine-learning meta-algorithms for different computational problems; and so forth. In addition we expect progress on experimental design and sampling for structured data.</p><p>We can also be sure to see advances in computation. From one direction, large and complex applied problems are being fit on faster computers, and we do not seem to have yet reached theoretical limits on efficiency of computational algorithms. From the other direction, the availability of fast computation allows applied researchers to routinely do big computations, and this has direct impact on statistics research. We have already seen this with hierarchical regressions, topic models, random forests, and deep nets, which have revolutionized many fields of application through their general availability.</p><p>Another general area that is ripe for development is model understanding, sometimes called interpretable machine learning (Murdoch et al. <span><a data-rid="CIT0112" data-reflink="_i22" href="#">2019</a></span>; Molnar <span><a data-rid="CIT0109" data-reflink="_i22" href="#">2020</a></span>). The paradox here is that the best way to understand a complicated model is often to approximate it with a simpler model, but then the question is, what is really being communicated here? One potentially useful approach is to compute sensitivities of inferences to perturbations of data and model parameters (Giordano, Broderick, and Jordan <span><a data-rid="CIT0058" data-reflink="_i22" href="#">2018</a></span>), combining ideas of robustness and regularization with gradient-based computational methods that are used in many different statistical algorithms.</p><p>What are the biggest challenges and opportunities facing statisticians? Three related trends in applications are big data, messy data, and complicated questions. In some ways, these trends go together: when using data from more sources, it should be possible to make more finely grained inferences and decisions in problems ranging from personalized medicine to recommender systems to robot cars.</p><p>Does this mean that, as sample sizes get bigger and bigger, statistical inference will become less and less important than in the past, to the point that the machine-learning approach of purely predictive inference will replace the role of statistics except in some specialized “small data” applications? We anticipate that no, there will always be a “last mile problem” by which researchers and decision makers will always be concerned with statistical issues of uncertainty and variation. For example, machine learning methods can be used in drug discovery, and hierarchical differential equation models can be used in dosing models, but when estimating efficacy in the population, we think there is no way to avoid statistical issues of generalizing from sample to population, generalizing from treatment to control group, and generalizing from observed data to underlying constructs of interest. This suggests to us that some of the most important statistical research of the next 50 years will lie at the interface of high-dimensional and nonparametric modeling and computation on one hand, and causal inference and decision making on the other.</p><p>A related question is what statistical ideas will be developed outside the area of statistics. In the past 20 years, deep learning has had huge success, with traditional statistical theory often seeming to struggle to catch up. Can we anticipate what new areas might arise, about which statisticians should become aware? Much of the history of statistics can be viewed as the incorporation of ideas from outside. Indeed, as a field we can count ourselves lucky that many of the new ideas of the past 50 years in topics as varied as causal inference, robustness, and exploratory data analysis were developed by researchers within statistics. One strength of our field is its connection to applications, and to the extent that applied statistics or data science is now often done within applied fields of science and engineering, we can expect many of the new developments to come from there too, in the same way that earlier developments in statistics came from within applied fields such as psychology and genetics. Statistics should continue to be open to ideas—general theoretical frameworks as well as specific models and methods—coming from other fields.</p><p>Finally, given that just about all new statistical and data science ideas are computationally expensive, we envision future research on validation of inferential methods, taking ideas such as unit testing from software engineering and applying them to problems of learning from noisy data. As our statistical methods become more advanced, there will be continuing need to understand the links between data, models, and substantive theory.</p></div></div></div>
  </body>
</html>
