<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://specbranch.com/posts/fp-rand/">Original</a>
    <h1>Perfect Random Floating-Point Numbers</h1>
    
    <div id="readability-page-1" class="page"><div><p>When I recently looked at the state of the art in floating point random number generation,
I was surprised to see a common procedure in many programming languages and libraries that
is not really a floating-point algorithm:</p>
<ol>
<li>Generate a random integer with bits chosen based on the precision of the format.</li>
<li>Convert to floating point.</li>
<li>Divide to produce an output between 0 and 1.</li>
</ol>
<p>In code, this looks like:</p>
<div><pre tabindex="0"><code data-lang="go"><span><span>1</span><span><span>func</span> <span>(</span><span>r</span> <span>*</span><span>Rand</span><span>)</span> <span>Float64</span><span>()</span> <span>float64</span> <span>{</span>
</span></span><span><span>2</span><span>    <span>int64</span> <span>rand_int</span> <span>=</span> <span>r</span><span>.</span><span>Int63n</span><span>(</span><span>1</span><span>&lt;&lt;</span><span>53</span><span>)</span>
</span></span><span><span>3</span><span>    <span>return</span> <span>float64</span><span>(</span><span>rand_int</span><span>)</span> <span>/</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>53</span><span>)</span>
</span></span><span><span>4</span><span><span>}</span>
</span></span></code></pre></div><p>This function is supposed to produce floating-point numbers drawn from a uniform distribution
in the interval $[0, 1)$. Zero is a possible output, but one is not, and the distribution is
uniform. The number &#34;53&#34; in the algorithm above is chosen in a way that is floating-point aware:
the double-precision floating-point numbers have 53 bits of precision, so this algorithm only
creates bits equal to the precision of the number system. It seems to fit the bill.</p>
<p>However, this algorithm has a few basic flaws. First, it cannot access most of the floating-point
numbers between 0 and 1. There are $2^{62}$ <code>float64</code> numbers in this range, and this
algorithm can only access $2^{53}$ of them: $\frac{1}{512}$ of the space. Even if we had used a
larger integer range, like $2^{64}$, many random integers would alias to the same floating-point
result and some integers would round up to 1 during division. Second, the least significant bits
that come from this generator are biased.
This is really a <a href="https://specbranch.com/posts/fixed-point/">fixed-point</a> random number
generator followed by a conversion to floating point.</p>
<p>There have been some past attempts to fix these flaws, but none that avoid a huge
performance penalty while doing so. I recently finished a paper on a new method
for generating perfect random floating point numbers, and I have put a preprint on
<a href="https://github.com/specbranch/fp-rand">Github</a> with a reference implementation in go. Thanks
to branch predictors, the perfect algorithm is not much slower than the fixed-point algorithm
above.</p>
<h2 id="the-floating-point-numbers-between-0-and-1">The Floating-Point Numbers Between 0 and 1</h2>
<p>A floating-point number is a tuple of three components, packed into a 32- or 64-bit word:</p>
<table>
  <thead>
      <tr>
          <th>Field</th>
          <th>Meaning</th>
          <th>Bits (<code>float32</code>)</th>
          <th>Bits (<code>float64</code>)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Sign ($s$)</td>
          <td>$+$ or $-$</td>
          <td>1</td>
          <td>1</td>
      </tr>
      <tr>
          <td>Exponent ($e$)</td>
          <td>Order of magnitude of the leading bit</td>
          <td>8</td>
          <td>11</td>
      </tr>
      <tr>
          <td>Manitssa ($m$)</td>
          <td>All significant bits after the leading bit</td>
          <td>23</td>
          <td>52</td>
      </tr>
  </tbody>
</table>
<p>The storage of each number is efficient: the leading bit of the number is not stored since it can
be implied from the exponent as either being a zero (when $e = 0$, allowing $0$ to be represented)
or a one bit (all other cases). Floating point also has infinities and non-number values (NaN means
&#34;not a number&#34;), which are encoded with an exponent of all ones. Otherwise, the exponent is an
unsigned number that is stored so that subtraction of a bias can create a negative exponent. A
normal floating-point number has its value given by:</p>
<p>$$ F = (-1)^s 2^{e - eBias} (1.m) $$</p>
<p>Typically, $eBias$ is half of the representable range of the exponent field. The 32-bit floats
have an 8-bit exponent, and use $eBias = 127$, so the range of possible exponents is $-126$ to $127$
($e = 0$ and $e = 255$ have special meanings).</p>
<p>Half of all floating-point numbers lie between -1 and 1: every number with an exponent from $0$
up to $eBias$ is less than 1. This seems unintuitive, but it is an
artifact of how the floating-point numbers work: they have equal precision across the range of
magnitudes. The number $0.000001$ and the number $10^{20}$ both have 53 bits of precision (or 24
if we were using 32-bit floats). This is akin to the concept of &#34;significant digits&#34;: every
floating-point number has 53 significant bits regardless of magnitude. The floating-point numbers
are power-law distributed, and many more of them have extreme magnitudes than it seems.</p>
<h4 id="floating-point-rounding">Floating-Point Rounding</h4>
<p>When an operation produces too many bits, we need to round them to produce a floating-point result.
There are four floating-point rounding modes, but three are relatively specialized:</p>
<ul>
<li>
<p><strong>Round-to-nearest-ties-to-even</strong>: Rounds to the nearest floating-point number, with ties going to
the even number. This is close to &#34;rounding&#34; as we learned in school except schoolbook rounding
involves ties going away from zero. Ties to even is better for numerical stability.</p>
</li>
<li>
<p><strong>Round toward zero (truncation)</strong>: Always round to the floating-point number closer to zero.
Equivalent to truncating trailing bits.</p>
</li>
<li>
<p><strong>Round down</strong>: Always round to the lower number.</p>
</li>
<li>
<p><strong>Round up</strong>: Always round to the higher number.</p>
</li>
</ul>
<p>Nearest-ties-to-even is the default for arithmetic because it is the most accurate.
When we consider random number generation that is floating-point native, we may prefer to follow
the chosen rounding mode rather than using the strict idea of operating in $[0, 1)$. Number lines
showing some examples of rounding in a hypothetical floating-point format are below, comparing the
round down and round to nearest modes.</p>
<p>When rounding to nearest, the regions shown in this number line round to 0.5, 2, and 5 in a
hypothetical floating-point format with $p = 2$:</p>
<figure><img src="https://specbranch.com/fp-rand/rtnExample.svg" width="100%"/>
</figure>

<p>Rounding regions center on the result, but at boundaries between exponents, the rounding region
is asymmetric. In the same hypothetical format, compare these regions that round to the same numbers:</p>
<figure><img src="https://specbranch.com/fp-rand/rzExample.svg" width="100%"/>
</figure>

<p>In this case, the region that rounds to each number is the region contained between two numbers.</p>
<h4 id="integer-arithmetic-on-floating-point-numbers">Integer Arithmetic on Floating-Point Numbers</h4>
<p>Floating-point numbers are packed into machine words that can be manipulated with bitwise logical
operations and integer arithmetic. This lets us do a few slightly dangerous things:</p>
<ul>
<li>
<p>We can construct floating-point numbers by OR-ing integers baring multiple parts of the number
together (<em>eg</em> attaching a mantissa to an exponent).</p>
</li>
<li>
<p>Adding small integers to floating-point numbers is a perterbation by a few units in the last place
(ULPs), although this gets weird when you cross exponent boundaries.</p>
</li>
<li>
<p>Adding and subtracting large integers that reach the exponent field is a fast way to multiply and
divide floating-point numbers, provided you handle the boundary conditions.</p>
</li>
</ul>
<p>These are tricks that allow you to build your own fast operations. Crazier tricks, like the
<a href="https://en.wikipedia.org/wiki/Fast_inverse_square_root"><em>Quake III</em> fast inverse square root</a>
essentially rely on these basic building blocks.</p>
<h2 id="uniform-randomness-in-floating-point">Uniform Randomness in Floating Point</h2>
<p>With this in mind, we can construct a notion for perfect floating-point random number generation:</p>
<p><em><strong>For unbiased random floating-point numbers, generate floating-point numbers with probabilities
given by drawing a real number and then rounding to floating point.</strong></em></p>
<p>Integer and fixed-point random number generation both follow this rule, but they have the
advantage of splitting space into uniform pieces, so each number is equally probable. Floating
point does not have this advantage. Instead, the probability depends on the distance between
floating-point numbers.</p>
<p>In general, the numbers between 0.5 and 1 are each twice as likely as the numbers between 0.25
and 0.5, which are twice as likely as the numbers between 0.125 and 0.25, and so on. The edges of
each exponent range have probability dicated by rounding modes: When rounding down (or toward
zero), the bottom of the range is the same probability as numbers in the range. When rounding up,
the top of the range shares a probability with the number range. When rounding to nearest, we
split the difference. A probability table for the first two exponents is below for uniform
generation in $(0, 1)$, using $p$ as the precision in bits ($p = 53$ for 64-bit floating point,
and $p = 24$ for 32-bit).</p>
<table>
  <thead>
      <tr>
          <th>Output Range</th>
          <th>Rounding Mode</th>
          <th>Lowest Number</th>
          <th>Interior Numbers</th>
          <th>Highest Number</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$[0.5, 1]$</td>
          <td>Down</td>
          <td>$2^{-p}$</td>
          <td>$2^{-p}$</td>
          <td>$0$</td>
      </tr>
      <tr>
          <td>$[0.5, 1]$</td>
          <td>Up</td>
          <td>$2^{-p - 1}$</td>
          <td>$2^{-p}$</td>
          <td>$2^{-p}$</td>
      </tr>
      <tr>
          <td>$[0.5, 1]$</td>
          <td>Nearest</td>
          <td>$2^{-p - 1} + 2^{-p - 2}$</td>
          <td>$2^{-p}$</td>
          <td>$2^{-p - 1}$</td>
      </tr>
      <tr>
          <td>$[0.25, 0.5]$</td>
          <td>Down</td>
          <td>$2^{-p - 1}$</td>
          <td>$2^{-p - 1}$</td>
          <td>$2^{-p}$</td>
      </tr>
      <tr>
          <td>$[0.25, 0.5]$</td>
          <td>Up</td>
          <td>$2^{-p - 2}$</td>
          <td>$2^{-p - 1}$</td>
          <td>$2^{-p - 1}$</td>
      </tr>
      <tr>
          <td>$[0.25, 0.5]$</td>
          <td>Nearest</td>
          <td>$2^{-p - 2} + 2^{-p - 3}$</td>
          <td>$2^{-p - 1}$</td>
          <td>$2^{-p - 2} + 2^{-p - 1}$</td>
      </tr>
  </tbody>
</table>
<p>The table continues down toward an exponent of zero. For all three modes, interior numbers in
these ranges share a probability: each of these has equal mass of real numbers above and below,
so nothing changes if we round up, round down, or split the difference. The boundaries of each
range do change, however. When rounding down, the powers of two are the same probability as the
numbers above, and when rounding up, the powers of two are the same probability as the numbers
below them. When rounding to nearest, we average the probability weight of the numbers above
and below, meaning that they are $1.5$ times as likely as a number in the lower range. The first
three rows of the table show a boundary condition, since there is no chance of rounding down
toward 1 when we are drawing our real numbers.</p>
<p>The typical form of random float generation maps best onto the &#34;round down&#34; mode, which cannot
generate 1. The other rounding modes give some probability mass to 1. Another interesting fact
we can observe is that the least significant bits of these numbers should be uniformly
distributed, even though the magnitude of the least significant bit varies.</p>
<p>Since we are treating this as a real number range, the openness or closedness of the range
doesn&#39;t actually matter. Probability masses are the same when you round real numbers in $(0, 1)$
to floating point as when you round numbers in $[0, 1]$.</p>
<h2 id="an-algorithm-for-floating-point-randomness">An Algorithm for Floating-Point Randomness</h2>
<p>We can match these probabilities by using an algorithm that works in two steps:</p>
<ol>
<li>
<p>Generate a fixed-point random number with granularity of $p$, with some special handling if
we generate 0, to find the range of real numbers from which we draw the number.</p>
</li>
<li>
<p>Finish drawing a number by backfilling the extra bits of precision that are available.</p>
</li>
</ol>
<p>The fixed-point phase finds a range of floating-point numbers that we will draw from, and the
finalization phase picks from that range. In the fixed-point phase, we note that ony the lowest
number range, from $0$ to $2^{-p}$, contains multiple exponents. Thus, to handle this range, we
simply work recursively: we run the algorithm as before, but with a scale factor of $2^{-p}$.
Once we reach the subnormal numbers, we hit the base case where all possible values from the
fixed-point phase map to a specific exponent. This allows us to identify the exponent of the
output by doing repeated fixed-point draws.</p>
<p>The fixed-point result here will have a known number of trailing zeros based on the order of
magnitude of the result. The finalization phase fills these bits.</p>
<p>In the finalization phase, we add an integer with a number of bits given by the distance between
the exponent of $1$ and the exponent of the fixed-point number we generated. This distance is the
number of bits of precision we add when we convert the fixed-point number to floating-point, so
we just backfill them. This phase varies slightly by rounding mode.</p>
<p>The whole algorithm is given below (for <code>float64</code> in Go). We use a slightly different form of
fixed-point random number generator: instead of generating a large integer and dividing, we
generate a random mantissa, place it between 1 and 2 (all of which share an exponent), and then
subtract 1. This has exactly the same output distribution as the method of division and may be
slower on some architectures.</p>
<div><pre tabindex="0"><code data-lang="go"><span><span> 1</span><span><span>// Generate float64 with a given rounding mode</span>
</span></span><span><span> 2</span><span><span>func</span> <span>(</span><span>r</span> <span>*</span><span>FPRand</span><span>)</span> <span>Float64Rounding</span><span>(</span><span>mode</span> <span>RoundingMode</span><span>)</span> <span>float64</span> <span>{</span>
</span></span><span><span> 3</span><span>    <span>const</span> <span>PRECISION</span> <span>=</span> <span>53</span><span>;</span>
</span></span><span><span> 4</span><span>    <span>const</span> <span>FP64_MANTISSA_SIZE</span> <span>=</span> <span>PRECISION</span> <span>-</span> <span>1</span><span>;</span>
</span></span><span><span> 5</span><span>    <span>const</span> <span>FP64_MANTISSA_MASK</span> <span>=</span> <span>(</span><span>1</span> <span>&lt;&lt;</span> <span>FP64_MANTISSA_SIZE</span><span>)</span> <span>-</span> <span>1</span>
</span></span><span><span> 6</span><span>
</span></span><span><span> 7</span><span>    <span>// PHASE 1: Zooming fixed-point phase</span>
</span></span><span><span> 8</span><span>
</span></span><span><span> 9</span><span>    <span>// Variables for zooming down the exponent range</span>
</span></span><span><span>10</span><span>    <span>exp_range</span> <span>:=</span> <span>uint64</span><span>(</span><span>FP64_MANTISSA_SIZE</span><span>)</span> <span>&lt;&lt;</span> <span>FP64_MANTISSA_SIZE</span>
</span></span><span><span>11</span><span>    <span>var</span> <span>one</span> <span>=</span> <span>math</span><span>.</span><span>Float64bits</span><span>(</span><span>1</span><span>)</span>
</span></span><span><span>12</span><span>
</span></span><span><span>13</span><span>    <span>// Track the number of tail bits if we get to underflow</span>
</span></span><span><span>14</span><span>    <span>var</span> <span>tail_bits</span> <span>uint64</span> <span>=</span> <span>0</span>
</span></span><span><span>15</span><span>
</span></span><span><span>16</span><span>    <span>// Generate mantissas until we get nonzero, expected to run only once</span>
</span></span><span><span>17</span><span>    <span>var</span> <span>mantissa</span> <span>=</span> <span>r</span><span>.</span><span>src</span><span>.</span><span>Int63n</span><span>(</span><span>1</span> <span>&lt;&lt;</span> <span>FP64_MANTISSA_SIZE</span><span>)</span>
</span></span><span><span>18</span><span>    <span>for</span> <span>mantissa</span> <span>==</span> <span>0</span> <span>{</span>
</span></span><span><span>19</span><span>        <span>// Zoom down so each step is between mantissa = 0 and 1 from the previous step</span>
</span></span><span><span>20</span><span>        <span>one</span> <span>-=</span> <span>exp_range</span>
</span></span><span><span>21</span><span>
</span></span><span><span>22</span><span>        <span>if</span> <span>one</span> <span>&lt;</span> <span>exp_range</span> <span>{</span>
</span></span><span><span>23</span><span>            <span>// Low-probability underflow base case after 19 rounds</span>
</span></span><span><span>24</span><span>            <span>// Exponents go: 1023, 971, 867, ..., 35</span>
</span></span><span><span>25</span><span>            <span>const</span> <span>UF_TAIL_SIZE</span> <span>=</span> <span>35</span>
</span></span><span><span>26</span><span>            <span>const</span> <span>UF_SHIFT</span> <span>=</span> <span>FP64_MANTISSA_SIZE</span> <span>-</span> <span>UF_TAIL_SIZE</span>
</span></span><span><span>27</span><span>
</span></span><span><span>28</span><span>            <span>mantissa</span> <span>=</span> <span>r</span><span>.</span><span>src</span><span>.</span><span>Int63n</span><span>(</span><span>1</span> <span>&lt;&lt;</span> <span>UF_TAIL_SIZE</span><span>)</span> <span>&lt;&lt;</span> <span>UF_SHIFT</span>
</span></span><span><span>29</span><span>            <span>tail_bits</span> <span>=</span> <span>UF_SHIFT</span>
</span></span><span><span>30</span><span>            <span>break</span>
</span></span><span><span>31</span><span>        <span>}</span>
</span></span><span><span>32</span><span>
</span></span><span><span>33</span><span>        <span>mantissa</span> <span>=</span> <span>r</span><span>.</span><span>src</span><span>.</span><span>Int63n</span><span>(</span><span>1</span> <span>&lt;&lt;</span> <span>FP64_MANTISSA_SIZE</span><span>)</span>
</span></span><span><span>34</span><span>    <span>}</span>
</span></span><span><span>35</span><span>
</span></span><span><span>36</span><span>    <span>// Generate a number between 1 and 2 (scaled based on loops) and subtract 1</span>
</span></span><span><span>37</span><span>    <span>// This is the fixed-point random result</span>
</span></span><span><span>38</span><span>    <span>num</span> <span>:=</span> <span>math</span><span>.</span><span>Float64frombits</span><span>(</span><span>one</span> <span>|</span> <span>mantissa</span><span>)</span> <span>-</span> <span>math</span><span>.</span><span>Float64frombits</span><span>(</span><span>one</span><span>)</span>
</span></span><span><span>39</span><span>    <span>num_as_int</span> <span>:=</span> <span>math</span><span>.</span><span>Float64bits</span><span>(</span><span>num</span><span>)</span>
</span></span><span><span>40</span><span>
</span></span><span><span>41</span><span>    <span>// PHASE 2: Finalization</span>
</span></span><span><span>42</span><span>
</span></span><span><span>43</span><span>    <span>// Find out how many tail bits we need to fill is by subtracting exponents</span>
</span></span><span><span>44</span><span>    <span>tail_bits</span> <span>+=</span> <span>(</span><span>one</span> <span>&gt;&gt;</span> <span>FP64_MANTISSA_SIZE</span><span>)</span> <span>-</span> <span>(</span><span>num_as_int</span> <span>&gt;&gt;</span> <span>FP64_MANTISSA_SIZE</span><span>)</span>
</span></span><span><span>45</span><span>
</span></span><span><span>46</span><span>    <span>// Handle exponent of 0 - this code path is unlikely to be hit before the universe ends</span>
</span></span><span><span>47</span><span>    <span>if</span> <span>tail_bits</span> <span>&gt;</span> <span>FP64_MANTISSA_SIZE</span> <span>{</span>
</span></span><span><span>48</span><span>        <span>tail_bits</span> <span>=</span> <span>FP64_MANTISSA_SIZE</span>
</span></span><span><span>49</span><span>    <span>}</span>
</span></span><span><span>50</span><span>
</span></span><span><span>51</span><span>    <span>// Generate the tail based on rounding mode</span>
</span></span><span><span>52</span><span>    <span>var</span> <span>tail</span> <span>uint64</span> <span>=</span> <span>0</span>
</span></span><span><span>53</span><span>    <span>if</span> <span>mode</span> <span>==</span> <span>RoundDown</span> <span>{</span>
</span></span><span><span>54</span><span>        <span>tail</span> <span>=</span> <span>r</span><span>.</span><span>src</span><span>.</span><span>Int63n</span><span>(</span><span>1</span> <span>&lt;&lt;</span> <span>tail_bits</span><span>)</span>
</span></span><span><span>55</span><span>    <span>}</span> <span>else</span> <span>if</span> <span>mode</span> <span>==</span> <span>RoundUp</span> <span>{</span>
</span></span><span><span>56</span><span>        <span>tail</span> <span>=</span> <span>r</span><span>.</span><span>src</span><span>.</span><span>Int63n</span><span>(</span><span>1</span> <span>&lt;&lt;</span> <span>tail_bits</span><span>)</span> <span>+</span> <span>1</span>
</span></span><span><span>57</span><span>    <span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>58</span><span>        <span>tail</span> <span>=</span> <span>(</span><span>r</span><span>.</span><span>src</span><span>.</span><span>Int63n</span><span>(</span><span>1</span> <span>&lt;&lt;</span> <span>tail_bits</span><span>)</span> <span>+</span> <span>1</span><span>)</span> <span>&gt;&gt;</span> <span>1</span>
</span></span><span><span>59</span><span>    <span>}</span>
</span></span><span><span>60</span><span>
</span></span><span><span>61</span><span>    <span>return</span> <span>math</span><span>.</span><span>Float64frombits</span><span>(</span><span>num_as_int</span> <span>+</span> <span>tail</span><span>)</span>
</span></span><span><span>62</span><span><span>}</span>
</span></span><span><span>63</span><span>
</span></span><span><span>64</span><span><span>// Functional equivalent to Float64 - generates numbers in [0, 1)</span>
</span></span><span><span>65</span><span><span>func</span> <span>(</span><span>r</span> <span>*</span><span>FPRand</span><span>)</span> <span>Float64</span><span>()</span> <span>float64</span> <span>{</span>
</span></span><span><span>66</span><span>    <span>return</span> <span>Float64Rounding</span><span>(</span><span>RoundDown</span><span>)</span>
</span></span><span><span>67</span><span><span>}</span>
</span></span></code></pre></div><p>The code really starts at line 10, where we set a constant that we will use to subtract
(in integer space) from &#34;one&#34; (line 11) if we have a mantissa of zero. This is just a floating
point division by a power of 2. We track tail bits at line 14, although this will be 0 unless
we hit the bottom of the exponent range.</p>
<p>Lines 17 and 18 start the action: we generate a mantissa and loop while that mantissa is 0.
This is very unlikely with 64-bit floating point! At each loop iteration, we reduce our &#34;one&#34;
variable so that it is the size of one tick mark on the previous iteration: the numbers we
generate the next time are smaller than what a mantissa of 1 would have given before. The next
mantissa is generated at line 32, and we also handle the base case here at line 22: we only
generate a partial mantissa in this case because this keeps the subtraction exact when we are
in such a ludicrously low exponent range.</p>
<p>Lines 37 and 38 complete fixed-point random number generation: we construct a floating-point
number as the OR of the exponent and sign of the <code>one</code> variable (which usually is equal to 1)
and the generated mantissa. This gives a result between 1 and 2, scaled based on the number of
times we drew a mantissa of 0. The subtraction on line 37 is thus always exact.</p>
<p>We can calculate how many trailing zeros come out of that subtraction based on the delta between
the exponents of the input and the output. This happens at line 55, with a special case for an
exponent of zero at line 46. This happens once every $2^{-126}$ random generations.</p>
<p>We now have the bottom of the range of <em>real numbers</em> we are drawing from, represented in
floating point, and we know the top of the range based on our count of tail bits. We then
backfill the missing mantissa bits randomly to complete the draw. We also know that between the
top and the bottom of the range, the space of floating point numbers is divided uniformly, so
we can do a fixed-point generation here.</p>
<p>Since we are drawing from the real numbers, rounding down means that we simply generate a number
with <code>tail_bits</code> bits. Rounding up means generating a random number with that many bits and then
incrementing everything by one. To round to nearest, we have to subdivide the range further, into
slices that are half of a ULP in size. The lowest slice gives a tail of 0, the next two slices
give a tail of 1, and so on. This allows us to exactly match the expected distribution we saw
in the table above.</p>
<h2 id="benchmarks-and-tests">Benchmarks and Tests</h2>
<p>Despite the number of loops and conditionals in this piece of code, most of them are free (on a CPU)
because of how unlikely they are to be taken. The loop at line 30 has probability $2^{-53}$, and
the remaining conditionals aside from the rounding mode switch are even more unlikely. This means
that our average random generation takes about the same amount of time as a typical random generation.</p>
<p>Representative benchmark results using the published reference code are below, using Go&#39;s two random
bitstream generators (PCG and ChaCha8) as their entropy source are below:</p>
<figure><img src="https://specbranch.com/fp-rand/benchmarks.svg#center" width="50%"/>
</figure>

<p>The speed here is indiscernable from the fixed-point random generation algorithm with the PCG bit
generator, and we incur a penalty for using the integer side of the CPU when we use the ChaCha8
bitstream generator, which runs on the vector side. The irony of a random integer generator
using the floating point side of the processor and the floating-point generator using the integer
side is not lost on me.</p>
<p>The main failing here is that this code does not auto-vectorize the way the fixed-point generator
does, but it can be done with vector operations since the loops are so rare.</p>
<p>We also can see that this is a better random number generator than the fixed-point version with
some uniformity testing. The LSBs of the division-based fixed-point generator are clearly
iscernable from random after a relatively small number of samples, with the number of
non-uniform LSBs growing as more random floating point values are generated. The figure below
comes from <a href="https://specbranch.com/posts/kolmogorov-smirnov/">K-S tests</a> of the uniformity of LSBs of each generator.
&#34;Round Down,&#34; &#34;Round Up,&#34; and &#34;Round to Nearest&#34; are this algorithm in different modes, while
&#34;Division&#34; is the fixed-point algorithm using division:</p>
<figure><img src="https://specbranch.com/fp-rand/lsbFailChart.svg" width="100%"/>
</figure>

<p>By 100,000,000 random numbers, the fixed-point
method has 16 LSBs that are clearly discernable from random, while this algorithm generates
random floating-point numbers with fully random bits. This is not as big of a deal with 64-bit
floating point, but with 32-bit floating point you only have 24 bits of precision to begin with,
so having only 8 bits of information that is indistinguishable from random at this point is
a significant loss.</p>
<h2 id="conclusions">Conclusions</h2>
<p>This is the first efficient algorithm for generating random uniform floating-point numbers that
can access the entire range of floating point outputs with correct probabilities. It adds two
extra steps: one to backfill bits and an unlikely recursion if we get a zero result. This
algorithm solves the problem of inaccuracies that arise from floating-point random number
generation, and should help make your simulations and computations more accurate.</p>
<p>Finally, I recently wrote a <a href="https://www.routledge.com/9781032933559">book</a> about floating point and how
to use it, with particular applications to games, graphics, and simulators. If you liked this
algorithm and want to learn more about the floating point numbers, please peruse the book and
consider buying a copy.</p>

    </div></div>
  </body>
</html>
