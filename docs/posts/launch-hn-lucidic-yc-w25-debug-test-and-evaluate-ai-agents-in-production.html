<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=44735843">Original</a>
    <h1>Launch HN: Lucidic (YC W25) – Debug, test, and evaluate AI agents in production</h1>
    
    <div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hi HN, we’re Abhinav, Andy, and Jeremy, and we’re building Lucidic AI (<a href="https://dashboard.lucidic.ai">https://dashboard.lucidic.ai</a>), an AI agent interpretability tool to help observe/debug AI agents.</p><p>Here is a demo: <a href="https://youtu.be/Zvoh1QUMhXQ" rel="nofollow">https://youtu.be/Zvoh1QUMhXQ</a>.</p><p>Getting started is easy with just one line of code. You just call lai.init() in your agent code and log into the dashboard. You can see traces of each run, cumulative trends across sessions, built-in or custom evals, and grouped failure modes. Call lai.create_step() with any metadata you want, memory snapshots, tool outputs, stateful info, and we&#39;ll index it for debugging.</p><p>We did NLP research at Stanford AI Lab (SAIL), where we worked on creating an AI agent (w/ fine-tuned models and DSPy) to solve math olympiad problems (focusing on AIME/USAMO); and we realized debugging these agents was hard. But the last straw was when we built an e-commerce agent that could buy items online. It kept failing at checkout, and every one-line change, tweaking a prompt, switching to Llama, adjusting tool logic, meant another 10-minute rerun just to see if we hit the same checkout page.</p><p>At this point, we were all like, this sucks, so we improved agent interpretability with better debugging, monitoring, and evals.</p><p>We started by listening to users who told us traditional LLM observability platforms don&#39;t capture the complexity of agents. Agents have tools, memories, events, not just input/output pairs. So we automatically transform OTel (and/or regular) agent logs into interactive graph visualizations that cluster similar states based on memory and action patterns.
We heard that people wanted to test small changes even with the graphs, so we created “time traveling,” where you can modify any state (memory contents, tool outputs, context), then re-simulate 30–40 times to see outcome distributions. We embed the responses, cluster by similarity, and show which modifications lead to stable vs. divergent behaviors.</p><p>Then we saw people running their agent 10 times on the same task, watching each run individually, and wasting hours looking at mostly repeated states. So we built trajectory clustering on similar state embeddings (like similar tools or memories) to surface behavioral patterns across mass simulations.</p><p>We then use that to create a force-directed layout that automatically groups similar paths your agent took, which displays states as nodes, actions as edges, and failure probability as color intensity. The clusters make failure patterns obvious; you see trends across hundreds of runs, not individual traces.</p><p>Finally, when people saw our observability features, they naturally wanted evaluation capabilities. So we developed a concept for people to make their own evals called &#34;rubrics,&#34; which lets you define specific criteria, assign weights to each criterion, and set score definitions,  giving you a structured way to measure agent performance against your exact requirements.</p><p>To evaluate these criteria, we used our own platform to build an investigator agent that reviews your criteria and evaluates performance much more effectively than traditional LLM-as-a-judge approaches.</p><p>To get started visit dashboard.lucidic.ai and <a href="https://docs.lucidic.ai/getting-started/quickstart">https://docs.lucidic.ai/getting-started/quickstart</a>. You can use it for free for 1,000 event and step creations.</p><p>Look forward to your thoughts! And don’t hesitate to reach out at team@lucidic.ai</p></div></td></div></div>
  </body>
</html>
