<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.gilesthomas.com/2025/03/llm-from-scratch-8-trainable-self-attention">Original</a>
    <h1>Writing an LLM from scratch, part 8 â€“ trainable self-attention</h1>
    
    <div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest(&#39;.dropdown&#39;)) {
                        let targetId = event.target.closest(&#39;.dropdown&#39;).dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? &#39;&#39; : targetId;
                        event.stopPropagation();
                    }">

                

                <div>
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

                
                
                
            </div>

            

    

    

    <p>This is the eighth post in my trek through <a href="https://sebastianraschka.com/">Sebastian Raschka</a>&#39;s book
&#34;<a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Build a Large Language Model (from Scratch)</a>&#34;.
I&#39;m blogging about bits that grab my interest, and things I had to rack my
brains over, as a way
to get things straight in my own head -- and perhaps to help anyone else that
is working through it too.  It&#39;s been almost a month since my
<a href="https://maryrosecook.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">last update</a> -- and
if you were suspecting that I was
<a href="https://maryrosecook.com/2025/02/blogging-in-the-age-of-ai">blogging about blogging</a> and spending time
getting <a href="https://maryrosecook.com/2025/02/adding-maths-to-the-blog">LaTeX working on this site</a> as
procrastination because this next section was always going to be a hard one, then you
were 100% right!  The good news is that -- as so often happens with these things --
it turned out to not be all that tough when I really got down to it.  Momentum
regained.</p>

<blockquote>
  <p>If you found this blog through the blogging-about-blogging, welcome!  Those
  posts were not all that typical, though, and I hope
  you&#39;ll enjoy this return to my normal form.</p>
</blockquote>

<p>This time I&#39;m covering section 3.4, &#34;Implementing self-attention
with trainable weights&#34;.  How do we create a system that can learn how to interpret
how much attention to pay to words in a sentence, when looking at other words -- for
example, that learns that in &#34;the fat cat sat on the mat&#34;, when you&#39;re looking at &#34;cat&#34;,
the word &#34;fat&#34; is important, but when you&#39;re looking at &#34;mat&#34;, &#34;fat&#34; doesn&#39;t matter
as much?</p>


    
        <p>Before diving into that, especially given the amount of time since the last post,
let&#39;s start with the 1,000-foot view of how the GPT-type
decoder-only transformer-based LLMs (hereafter &#34;LLMs&#34; to save me from RSI) work.
For each step I&#39;ve linked to the posts where I went throught the details.</p>

<ul>
<li>You start off with a string, presumably of words. (<a href="https://maryrosecook.com/2024/12/llm-from-scratch-2">Part 2</a>)</li>
<li>You split it up into tokens (words like &#34;the&#34;, or chunks like &#34;semi&#34;). (<a href="https://maryrosecook.com/2024/12/llm-from-scratch-2">Part 2</a>)</li>
<li>The job of the LLM is to predict the next token, given all of the tokens in the
string so far. (<a href="https://maryrosecook.com/2024/12/llm-from-scratch-1">Part 1</a>)</li>
<li>Step 1: map the tokens to a sequence of
vectors called <em>token embeddings</em>.  A particular token,
say, &#34;the&#34;, will have a specific embedding -- these start out random but the LLM
works out useful embeddings as it&#39;s trained. (<a href="https://maryrosecook.com/2024/12/llm-from-scratch-3">Part 3</a>)</li>
<li>Step 2: generate another sequence of <em>position embeddings</em> -- vectors of the
same size as the token embeddings, also starting random but trained, that represent
&#34;this is the first token&#34;, &#34;this is
the second token&#34;, and so on.  (<a href="https://maryrosecook.com/2024/12/llm-from-scratch-3">Part 3</a>. )</li>
<li>Step 3: add the two sequences to generate a new sequence of <em>input embeddings</em>.
The first input embedding is the first token embedding plus the first position
embedding (added element-wise), the second is the second token embedding plus the second
position embedding, and so on. (<a href="https://maryrosecook.com/2024/12/llm-from-scratch-3">Part 3</a>)</li>
<li>Step 4: self-attention.  Take the input embeddings
and for each one, generate a list of <em>attention scores</em>.  These
are numbers that represent how much attention to pay to each other token when considering the token
in question.  So (assuming one token per word) in &#34;the fat cat sat on the mat&#34;,
the token &#34;cat&#34; would need a list of 7 attention scores -- how much attention to
pay to the first &#34;the&#34;, how much to pay to &#34;fat&#34;, how much to pay to itself, &#34;cat&#34;,
how much to pay to &#34;sat&#34;, and so on.  Exactly how it does that is what this section
of the book covers -- up until now we&#39;ve been using a &#34;toy&#34; example calculation.
(<a href="https://maryrosecook.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://maryrosecook.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://maryrosecook.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://maryrosecook.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
<li>Step 5: normalise the attention scores to <em>attention weights</em>.  We
want each token&#39;s list of attention weights to add up to one -- we do this by running each list through
the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function.
(<a href="https://maryrosecook.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://maryrosecook.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://maryrosecook.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://maryrosecook.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
<li>Step 6: generate a new sequence of <em>context vectors</em>.
In the system that we&#39;ve built so far, this contains, for each token, the sum of multiplying all of the input embeddings
by their respective attention weights and adding the results together.
So in that example above, the context vector for &#34;cat&#34;
would be the input embedding for the first &#34;the&#34; times &#34;cat&#34;&#39;s attention score for
that &#34;the&#34;, plus the input embedding for &#34;fat&#34; times &#34;cat&#34;&#39;s attention score for
&#34;fat&#34;, and so on for every other token in the sequence.
(<a href="https://maryrosecook.com/2024/12/llm-from-scratch-4">Part 4</a>,
<a href="https://maryrosecook.com/2025/01/llm-from-scratch-5-self-attention">Part 5</a>, <a href="https://maryrosecook.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">Part 6</a>,
<a href="https://maryrosecook.com/2025/02/llm-from-scratch-7-coding-self-attention-part-2">Part 7</a>).</li>
</ul>

<p>After all of this is done, we have a sequence of context vectors,
each of which should in some way represent the meaning of its respective token in
the input, including those bits of meaning it gets from all of the other tokens.
So the context vector for &#34;cat&#34; will include some hint of its fatness, for example.</p>

<p>What happens with those context vectors that allows the LLM to use them to predict
what the next token might be?  That bit is still to be explained, so
we&#39;ll have to wait and see.  But the first thing to learn is how we create a trainable
attention mechanism that can take the input vectors and generate the attention
scores so that we can work out those context vectors in the first place.</p>

<p>The answer Raschka gives in this section is called <em>scaled dot product attention</em>.
He gives a crystal-clear runthrough of the code to do it, but I had to bang my head
against it for a weekend to get to a solid mental model.
So, instead of going through the
section bit-by-bit, I&#39;ll present my own explanation of how it works -- to save me
from future head-banging when trying to remember it, and perhaps to save other people&#39;s
foreheads from the same fate.</p>

<h3 id="the-summary-ahead-of-time">The summary, ahead of time</h3>

<p>I&#39;m a <a href="https://maryrosecook.com/2011/10/teaching-programming">long-time fan</a> of the Pimsleur
style of language course, where they start each tutorial with minute or so of conversation
in the language you&#39;re trying to learn, then say &#34;in 30 minutes, you&#39;ll hear that again
and you&#39;ll understand it&#34;.  You go through the lession, they play the conversation again, and you
do indeed understand it.</p>

<p>So here is a compressed summary of how self-attention works,
in my own words, based on Raschka&#39;s explanation.  It might look like a wall of jargon now, but
(hopefully) by the time
you&#39;ve finished reading this blog post, you&#39;ll be able to re-read it and it will all make sense.</p>

<p>We have an input sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>, of tokens.  We have converted it to a
sequence of input embeddings,
each of which is a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> -- each of these can be treated as a
point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional
space.  Let&#39;s represent that sequence of embeddings with values like this: <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><msub><mi>x</mi><mn>3</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math>.  Our goal is to produce a
sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> made up of context vectors, each of which represents the
the meaning of the respective input token in the context of the input as a whole.  These
context vectors will each be of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> (which in practice is often equal to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>,
but could in theory be of any length).</p>

<p>We define three matrices, the <em>query weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, the <em>key weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>,
and the <em>value weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math>. These are made up of trainable
weights; each one of them is sized <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mi>Ã—</mi><mi>c</mi></mrow></math>.  Because of those dimensions, we
can treat them as operations that project a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> -- a point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensonal
space -- to a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> -- a point in
a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional space.  We will call these projected spaces <em>key space</em>,
<em>query space</em> and
<em>value space</em>.  To convert an input vector <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, for example, we just
multiply it by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, like this <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub><mo>=</mo><msub><mi>x</mi><mi>m</mi></msub><msub><mi>W</mi><mi>q</mi></msub></mrow></math>.</p>

<p>When we are considering input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, we want to work out its <em>attention weights</em> for
every input in the sequence (including itself).  The first step is to work out the <em>attention score</em>,
which, when considering another input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math>, is calculated by taking the dot
product of the projection of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, and the projection of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> into
key space.  Doing this across all inputs provides us with an attention score
for every other token for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  We then divide these by the square root of the
dimensionality of the spaces we are projecting into, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, and run the resulting
list through the softmax function to make them all add up to one.  This list is the
attention weights for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  This process is called <em>scaled dot product attention</em>.</p>

<p>The next step is to generate a context vector for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  This is simply the
sum of the projections of all of the inputs into the value space, each one multiplied
by its associated attention weight.</p>

<p>By performing these operations for each of the input vectors, we can generate a list
of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> made up of context vectors of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, each of which represents the meaning of a input token in the context of
the input as a whole.</p>

<p>Importantly, with clever use of matrix multiplication, all of this can be done for
all inputs in the sequence, producing a context vector for every one of them,
with just five matrix multiplications and a transpose.</p>

<h3 id="now-lets-explain-it">Now let&#39;s explain it</h3>

<p>First things first, if there&#39;s anyone there that understood all of that without
already knowing how attention mechanisms work, then I salute you!  It was pretty
dense, and I hope it didn&#39;t read like my friend Jonathan&#39;s
<a href="https://www.tartley.com/posts/a-guide-to-git-using-spatial-analogies/">parody of incomprehensible guides to using git</a>.
For me, it took eight re-reads of Raschka&#39;s (emininently clear and readable)
explanation to get to a level where I felt I understood it.  I think it&#39;s also worth noting
that it&#39;s very much a &#34;mechanistic&#34; explanation -- it says how we do these calculations
without saying why.  I think that the &#34;why&#34; is actually out of scope for this book,
but it&#39;s something that fascinates me, and I&#39;ll blog about it soon.  But,
in order to understand the &#34;why&#34;, I think we need to have a solid grounding in the
&#34;how&#34;, so let&#39;s dig into that for this post.</p>

<p>Up until this section of the book, we have been working out the attention scores by taking the dot product
of the input embeddings against each other -- that is, when you&#39;re looking
at <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, the attention score for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> is just <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub><mi>Â·</mi><msub><mi>x</mi><mi>p</mi></msub></mrow></math>.  I suspected
earlier that the reason that Raschka was using that specific operation for his
&#34;toy&#34; self-attention was that the real implementation is similar, and that has turned
out right, as we&#39;re doing scaled dot products here.  But what we do is adjust them first -- <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>, the one that we&#39;re considering,
is multiplied by the query weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math> first, and the other one <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> is
multiplied by the key weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>.  Raschka refers to this as a projection,
which for me is a really nice way to look at it.  But his reference is just in passing,
and for me it needed a bit more digging in.</p>

<h3 id="matrices-as-projections-between-spaces">Matrices as projections between spaces</h3>

<blockquote>
  <p>If your matrix maths is a bit rusty -- like mine was -- and you haven&#39;t read the
  <a href="https://maryrosecook.com/2025/02/basic-neural-network-matrix-maths-part-1">primer I posted the other week</a>, then
  you might want to check it out now.</p>
</blockquote>

<p>From your schooldays, you might remember that matrices can be used to apply geometric
transformations.  For example, if you take a vector representing a point, you can multiply
it by a matrix to rotate that point about the origin.
You can use a matrix like this to rotate things anti-clockwise by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Î¸</mi></mrow></math> degrees:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><mi>x</mi></mtd><mtd><mi>y</mi></mtd></mtr></mtable><mo>]</mo><mo>[</mo><mtable><mtr><mtd><mi>cos</mi><mi>Î¸</mi></mtd><mtd><mo>âˆ’</mo><mi>sin</mi><mi>Î¸</mi></mtd></mtr><mtr><mtd><mi>sin</mi><mi>Î¸</mi></mtd><mtd><mi>cos</mi><mi>Î¸</mi></mtd></mtr></mtable><mo>]</mo><mo>=</mo><mo>[</mo><mtable><mtr><mtd><mi>x</mi><mo>.</mo><mi>cos</mi><mi>Î¸</mi><mo>+</mo><mi>y</mi><mo>.</mo><mi>sin</mi><mi>Î¸</mi></mtd><mtd><mi>x</mi><mo>.</mo><mo>âˆ’</mo><mi>sin</mi><mi>Î¸</mi><mo>+</mo><mi>y</mi><mo>.</mo><mi>cos</mi><mi>Î¸</mi></mtd></mtr></mtable><mo>]</mo></mrow></math></p><p>This being matrix multiplication, you could add on more points -- that is, if the
first matrix had more rows, each of which was a point you wanted to rotate, the same
multiplication would rotate them all by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Î¸</mi></mrow></math>.  So you can see that matrix as
being a function that maps sets of points to their rotated equivalents.  This
works in higher dimensions, too -- a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>Ã—</mi><mn>2</mn></mrow></math> matrix like this can represent
transformations in 2 dimensions, but, for example, in 3d graphics, people
use <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>Ã—</mi><mn>3</mn></mrow></math> matrices to do similar transformations to the points that make up
3d objects. </p>

<p>An alternative way of looking at this <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>2</mn><mi>Ã—</mi><mn>2</mn></mrow></math> matrix is that it&#39;s a function that
projects points from
one 2-dimensional space to another, the target space being the first space rotated
by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Î¸</mi></mrow></math> degrees anti-clockwise.  For a simple 2d example like this, or even the
3d ones, that&#39;s not
necessarily a better way of seeing it.  It&#39;s a philosophical difference rather
than a practical one.</p>

<p>But imagine if the matrix wasn&#39;t square --
that is, it had a different number of rows to the number of columns.
If you had a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>Ã—</mi><mn>2</mn></mrow></math> matrix, it could be used to multiply a matrix of vectors
in 3d space and produce a matrix in 2d space.  Remember the rule for matrix multiplication:
a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mn>3</mn></mrow></math> matrix times a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>3</mn><mi>Ã—</mi><mn>2</mn></mrow></math> matrix will give you a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mn>2</mn></mrow></math> one.</p>

<p>That is actually super-useful;
if you&#39;ve done any 3d graphics, you might remember the
<a href="https://en.wikipedia.org/wiki/Viewing_frustum">frustum</a> matrix which is used
to convert the 3d points you&#39;re working with to 2d points on a screen.  Without
going into too much detail, it allows you to project those 3d points into a 2d
space with a single matrix multiplication.</p>

<p>So: a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mi>Ã—</mi><mi>c</mi></mrow></math> matrix can be seen as a way to project a vector that represents a
point in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional space into one that represents one in a different <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional
space.</p>

<p>What we&#39;re doing in self-attention is taking our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math>-dimensional vectors that make
up the input embedding sequence, then projecting them into three different <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>-dimensional
spaces, and working with the projected versions.  Why do we do this?  That&#39;s the
question I want to look into in my future post on the &#34;why&#34;, but for now, I think one thing that is fairly
clear is that because these projections are learned as part of the training (remember,
the three matrices we&#39;re using for the projections are made up of trainable weights),
it&#39;s putting some kind of indirection into the mix that the simple dot product attention
that we were using before didn&#39;t have.</p>

<h3 id="how-to-do-the-dot-products-of-the-projected-input-embeddings">How to do the dot products of the projected input embeddings</h3>

<p>Sticking with this mechanistic view -- &#34;how&#34; rather than &#34;why&#34; -- for now,
let&#39;s look at the calculations and how matrix multiplication makes them efficient.
I&#39;m going to loosely follow Raschka&#39;s
explanation, but using mathematical notation rather than code, as (unusually for me as a career
techie) I found it a bit easier to grasp what&#39;s going on that way.</p>

<p>We&#39;ll stick with the case where we&#39;re
considering token <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> and trying to work out its attention score for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math>.
The first thing we do is project <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into query space, which we do by
multiplying it by the query weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><mo>=</mo><msub><mi>x</mi><mi>m</mi></msub><msub><mi>W</mi><mi>q</mi></msub></mrow></math></p><p>Now, let&#39;s project <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> into key space by multiplying it by the key weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>k</mi><mi>p</mi></msub><mo>=</mo><msub><mi>x</mi><mi>p</mi></msub><msub><mi>W</mi><mi>k</mi></msub></mrow></math></p><p>Our attention score is defined as being the dot product of these two vectors:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>Ï‰</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>=</mo><msub><mi>q</mi><mi>m</mi></msub><mo>.</mo><msub><mi>k</mi><mi>p</mi></msub></mrow></math></p><p>So we could write a simple loop that iterated over all of the inputs <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math> once,
generating the projections into query space for each one, and then inside that
loop iterated over <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></math> a second time, projecting them into key space, doing
the dot products, and storing those as attention scores.</p>

<p>But that would be wasteful!  We&#39;re doing matrix multiplications, so we can batch
things up.  Let&#39;s consider the projections of the inputs into the key space first;
those will always be the same, each time around our hypothetical loop.  So we can
do them in one shot.  Let&#39;s treat our input sequence as a matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi></mrow></math> like this:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><mo>.</mo><mo>.</mo><mo>.</mo></mtd></mtr><mtr><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>x</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></math></p><p>We have a row for every input embedding in our input
sequence <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow></math>, and so on, with the row being made up of the elements in that embedding.  So it
has <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math> rows, one per element in the input sequence, and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> columns, one for each
dimension in the input embeddings, so it&#39;s <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mi>d</mi></mrow></math>.  (I&#39;m using <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>3</mn></mrow></math> as
an example here, like Raschka does in the book.)</p>

<p>That&#39;s just like our matrix of points in the rotation matrix example above, so
we can project it into key space in one go, just by multiplying it by <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>.  Let&#39;s
call the result of that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math>:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math></p><p>It will look like this (again, like Raschka, I&#39;m using a 2-dimensional
key space -- that is, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mo>=</mo><mn>2</mn></mrow></math> -- so that it&#39;s easy to see whether a matrix is in the
original 3d input embedding space or a 2d projected one):</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mo>[</mo><mtable><mtr><mtd><msubsup><mi>k</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr><mtr><mtd><mo>.</mo><mo>.</mo><mo>.</mo></mtd></mtr><mtr><mtd><msubsup><mi>k</mi><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd><mtd><msubsup><mi>k</mi><mn>2</mn><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mtd></mtr></mtable><mo>]</mo></mrow></math></p><p>...where each of those rows is the projection of the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>n</mi></msub></mrow></math> to key space.
It&#39;s just all of the projections stacked on top of each other.</p>

<p>Now, let&#39;s think about that dot product -- this bit from earlier:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>Ï‰</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>=</mo><msub><mi>q</mi><mi>m</mi></msub><mo>.</mo><msub><mi>k</mi><mi>p</mi></msub></mrow></math></p><p>We now have a matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> containing all of our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>k</mi><mi>n</mi></msub></mrow></math> values.  When you&#39;re doing
a matrix multiplication, the value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math> -- that is, the element at
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math>, column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the output matrix -- is the dot product of
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> in the first matrix, taken as a vector, with column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the second
matrix, also considered as a vector.</p>

<p>It sounds like we can make use of that to do all of our dot products in a batch.
Let&#39;s treat <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math>, our projection of the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>th input token into query space, as
a single-row matrix.  Can we multiply the key matrix by it, like this</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><mi>K</mi></mrow></math></p><p>...?</p>

<p>Unfortunately not.  <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math> is a one-row matrix (size <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn><mi>Ã—</mi><mi>c</mi></mrow></math>)
and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> is our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mi>c</mi></mrow></math> key matrix.  With matrix multiplication,
the number of columns in the first matrix -- <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> in this case -- needs to match
the number of rows in the second, which is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>.  But, if we transpose K, essentially
swapping rows for columns:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><msub><mi>q</mi><mi>m</mi></msub><msup><mi>K</mi><mi>T</mi></msup></mrow></math></p><p>...then we have a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mn>1</mn><mi>Ã—</mi><mi>c</mi></mrow></math> matrix times a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mi>Ã—</mi><mi>n</mi></mrow></math> one, which does make sense --
and, even better, it&#39;s every dot product for every pair of (<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>q</mi><mi>m</mi></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>k</mi><mi>p</mi></msub></mrow></math>) for all values
of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>p</mi></mrow></math> -- that is, with two matrix multiplications -- the one to work out <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> and this one,
and a transpose, we&#39;ve worked out all of the attention scores for element <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> in our input
sequence.</p>

<p>But it gets better!</p>

<p>First, let&#39;s do the same thing as we did to project the input sequence
into key space to project it all into query space as well.  We
calculated <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math> to work out the key matrix, so we can work out the query matrix the
same way, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>.  Just like <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> was all of the input vectors projected into
key space, &#34;stacked&#34; on top of each other, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> is all of the input vectors projected
into query space.</p>

<p>Now, what happens if we multiply that by the transposed key matrix?</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math></p><p>Well, our <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> matrix is one row per input, one column per dimension in our projected
space, so it&#39;s <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mi>c</mi></mrow></math>.  And, as we know, the transposed <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>K</mi></mrow></math> matrix
is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi><mi>Ã—</mi><mi>n</mi></mrow></math>.  So our result is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mi>n</mi></mrow></math> -- and because matrix multiplication
is defined in terms of dot products, what it contains is the dot product of every
row in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Q</mi></mrow></math> -- the inputs transformed into query space -- against every column
in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mi>K</mi><mi>T</mi></msup></mrow></math> -- the inputs transformed into key space.</p>

<p>The plan was to generate attention scores by working out exactly those dot products!</p>

<p>So with three matrix multiplications, we&#39;ve done that:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Î©</mi><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math></p><p>...where I&#39;m using the capital <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Î©</mi></mrow></math> to represent a matrix where each row
represents an input in the sequence, and each column within the row represents
an attention weight for that input.  The element <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>Î©</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub></mrow></math> represents how much
attention to pay to the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> when you are trying to work out the context
vector for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.  And it has done that by working out the dot product of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> projected
into query space and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>p</mi></msub></mrow></math> projected into key space.</p>

<p>That&#39;s the &#34;dot product&#34; part of &#34;scaled dot product attention&#34; done and dusted :-)</p>

<h3 id="normalising-it">Normalising it</h3>

<p>So we&#39;ve worked out our attention scores.  The next thing we need to do is normalise
them; in the past we used the softmax function.  This function takes a list and adjusts
the values in it so that they all sum up to 1, but gives a boost to higher numbers and
a deboost to smaller ones.  I imagine it&#39;s named &#34;soft&#34; &#34;max&#34; because it&#39;s like finding
the maximum, but in a sense softer because it&#39;s leaving the other smaller numbers
in there deboosted.</p>

<p>Raschka explains that when we&#39;re working with large numbers of dimensions -- in
real-world LLMs, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math> can easily be in the thousands -- using pure softmax
can lead to small gradients -- he says that it can start acting &#34;like a step function&#34;,
which I read as meaning that you wind up with all but the largest number in the list
being scaled to really tiny numbers and the largest one dominating.  So, as a workaround,
we divide the numbers by the square root of the number of dimensions in our projected
space <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>, and then only then do we run the result through softmax. </p>

<p>Remember that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>Î©</mi></mrow></math> is a
matrix of attention scores, with one row for each input token, so we need to apply
the softmax function to each row separately.  Here&#39;s what we wind up with:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><mi>Î©</mi></mrow><mrow><msqrt><mrow><mi>c</mi></mrow></msqrt></mrow></mfrac><mo>,</mo><mtext>Â axis</mtext><mo>=</mo><mn>1</mn><mo stretchy="true" fence="true" form="postfix">)</mo></mrow></mrow></math></p><p>(The <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>a</mi><mi>x</mi><mi>i</mi><mi>s</mi><mo>=</mo><mn>1</mn></mrow></math> isn&#39;t really proper mathematical notation, it&#39;s just something
I&#39;ve borrowed from PyTorch to say that we&#39;re applying softmax to a matrix on a
per-row basis.)</p>

<p>Once we&#39;ve done that, we have our normalised attention scores -- that is, the
attention weights.  The next, and final, step, is to use those to work out the context
vectors.</p>

<h3 id="creating-the-context-vectors">Creating the context vectors</h3>

<p>Let&#39;s reiterate how we&#39;re working out the context vectors.  In the previous toy
example, for each token, we took the input embeddings, multiplied each one by
its attention weight, summed the results element-wise, and that was the result.
Now we&#39;re doing the same thing, but projecting the input embeddings into another
space first -- the value space.  So let&#39;s start off by doing that projection as
a simple matrix multiplication, just like we did for the other spaces:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>v</mi></msub></mrow></math></p><p>Now, from above we have our attention weights matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi></mrow></math>, which has in row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math> the attention
weights for every token in the input sequence for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> -- that is,
at <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>A</mi><mrow><mi>m</mi><mo>,</mo><mi>p</mi></mrow></msub></mrow></math> we have the
attention weight for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>p</mi></mrow></math> when we&#39;re working out the context vector for
input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>.  That means that for our input sequence of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>, it&#39;s
an <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mi>n</mi></mrow></math> matrix.</p>

<p>In our value matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>V</mi></mrow></math>, we also have one row per input.  The values in row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math>, treated
as a vector, are the projection of the input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math> into value space.  So it&#39;s
an <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mi>c</mi></mrow></math> matrix.</p>

<p>What happens if we do the matrix multiplication</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mi>V</mi></mrow></math></p><p>...?  We&#39;ll get a <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi><mi>Ã—</mi><mi>c</mi></mrow></math> matrix of some kind, by the rules of matrix multiplication,
but what will it mean?</p>

<p>To reiterate, the rule for matrix multiplication is that the value of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>M</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math> -- that is, the element at
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math>, column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the output matrix -- is the dot product of
row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>i</mi></mrow></math> in the first matrix, taken as a vector, with column <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>j</mi></mrow></math> in the second
matrix, also considered as a vector.</p>

<p>So, at position <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math> -- first row, first column, we have the dot product of the first row in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>A</mi></mrow></math> -- the
attention weight for every token in the input sequence when we&#39;re considering the
first token -- and the first column in <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>V</mi></mrow></math>, which is the first element of each
input embedding, projected into the value space.  So, that is the first element
of each input embedding times the attention weights for the first token.  Or,
in other words, it&#39;s the first element of the context vector for the first token!</p>

<p>At position <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math> -- first row,
second column -- we&#39;ll have the same calculation, but for the second element of each
input embedding.  That is the second element of the context vector for the first
token.</p>

<p>...and so on for the rest of the columns.  By the end of the first row,
we&#39;ll have something that (treated as a vector) is the sum of all of the input
embeddings, multiplied by the weights for the first input.  It&#39;s our context vector
for that input!</p>

<p>The same, of course, repeats for each row.  The result of that single matrix multiplication
is a matrix where the row <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>m</mi></mrow></math> is the context vector for input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>x</mi><mi>m</mi></msub></mrow></math>.</p>

<p>We&#39;re done!</p>

<h3 id="bringing-it-all-together">Bringing it all together</h3>

<p>Let&#39;s put together those steps.  We start with our input matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>X</mi></mrow></math>, which is the
input embeddings we generated earlier for our sequence of tokens of length <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>n</mi></mrow></math>.  Each
row is an embedding, and there are <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> columns, where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>d</mi></mrow></math> is the dimensionality of
our embeddings.</p>

<p>We also have our weight matrices to map input embeddings into different
spaces: the <em>query weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow></math>, the <em>key weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow></math>,
and the <em>value weights matrix</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow></math>.</p>

<p>So, we project our input matrix into those spaces with three matrix multiplications:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>q</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>k</mi></msub></mrow></math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>v</mi></msub></mrow></math></p><p>...to get our query matrix, our key matrix, and our value matrix.</p>

<p>We then calculate
our attention scores with one further matrix multiplication and a transpose to work out the dot
products:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>Î©</mi><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow></math></p><p>We normalise those to attention weights by scaling them by the square root of <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>c</mi></mrow></math>
and then applying softmax:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>A</mi><mo>=</mo><mtext>softmax</mtext><mrow><mo stretchy="true" fence="true" form="prefix">(</mo><mfrac><mrow><mi>Î©</mi></mrow><mrow><msqrt><mrow><mi>c</mi></mrow></msqrt></mrow></mfrac><mo>,</mo><mtext>Â axis</mtext><mo>=</mo><mn>1</mn><mo stretchy="true" fence="true" form="postfix">)</mo></mrow></mrow></math></p><p>...and then we use one final matrix multiplication to use that to work out the
context vectors:</p>

<p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mi>C</mi><mo>=</mo><mi>A</mi><mi>V</mi></mrow></math></p><p>And that&#39;s our self-attention mechanism :-)</p>

<p>Now, if you
<a href="#the-summary-ahead-of-time">go back to the explanation at the start</a>, then
hopefully it will make sense.</p>

<h3 id="back-to-the-book">Back to the book</h3>

<p>Section 3.4 in the book works through the above with PyTorch code, and comes out
with a nice simple <code>nn.Module</code> subclass that does exactly those matrix operations.
This is then improved -- the first version uses generic <code>nn.Parameter</code> objects for
the three weight matrices, and the second uses <code>nn.Linear</code> for more effective training.
That side of it was reasonably easy to understand.  And so, we&#39;ve wrapped up what I
think is the hardest part of &#34;Build a Large Language Model (from scratch)&#34;:
implementing self-attention
with trainable weights.</p>

<h3 id="next-steps">Next steps</h3>

<p>The remainder of chapter 3 is much easier now that we&#39;re over
this hump.  We&#39;ll be going through two things:</p>

<ul>
<li>Causal self-attention (which means that when we are looking at a given token, we don&#39;t pay any attention to later
ones, just like we humans do when reading -- our language is structured so that you
don&#39;t normally need to read forward to understand what a word means [except <a href="https://faculty.georgetown.edu/jod/texts/twain.german.html">in German</a> ;-]).</li>
<li>Multi-head attention (which isn&#39;t as complex an issue as I thought it was when I first read about it).</li>
</ul>

<p>So I think
I&#39;ll probably blog about those first, and then circle back to the &#34;why&#34; of this
form of self-attention.  It&#39;s pretty amazing that we can do all of this
-- projecting into differently-dimensioned spaces, taking dot products between
every token&#39;s input embeddings in those spaces, and weighting the projected input
tokens by the weights we generate -- with just five matrix multiplications.  But
why do we do that specifically?</p>

<p>The names of the matrices used -- query, key and value -- hint at
the roles they play in a metaphorical way; Raschka says in a sidebar that
it&#39;s a nod to information retrieval systems like databases.  However, it&#39;s different
enough to how DBs actually work that I can&#39;t quite make the connection.  I&#39;m sure
it will come with time, though.</p>

<p>I also want to, probably in a separate post, consider what batches do to all of this.
With <a href="https://maryrosecook.com/2025/02/basic-neural-network-matrix-maths-part-1">normal neural networks</a>,
all of our activations when considering a given input are single-row or -column
matrices (depending on the ordering of our equations).  Extending to batches
just means moving to normal multi-row, multi-column matrices.</p>

<p>But ever since
we introduced the matrix of attention scores <a href="https://maryrosecook.com/2025/01/llm-from-scratch-6-coding-self-attention-part-1">for the first time</a>,
it&#39;s been clear that even with a single input sequence going through our LLM, we&#39;re
already using full matrices.  How do we handle batches where we&#39;re processing
multiple input sequences in parallel?  It seems that we&#39;re going to need to use
some kind of higher-order tensors -- if scalars are order zero tensors, vectors are
order one tensors, and matrices are order two tensors, we&#39;re going to need to start
considering order three tensors at least.  That will require a bit of thought!</p>

<p>But for now, that&#39;s all -- see you next time!  And please do comment below --
any thoughts, questions or suggestions would be very welcome, of course, but even
if you just found this post useful it would be great to know :-)</p>



    

    
        
    

    



            
        </div></div>
  </body>
</html>
