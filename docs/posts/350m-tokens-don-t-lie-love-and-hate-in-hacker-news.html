<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://outerbounds.com/blog/hacker-news-sentiment/">Original</a>
    <h1>350M Tokens Don&#39;t Lie: Love and Hate in Hacker News</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content" itemprop="articleBody"><hr/><p><strong>tl;dr</strong>
We analyzed all Hacker News posts with more than five comments between January 2020 and June 2023.
Leveraging LLama3 70B LLM, we examined both the posts and their associated comments to gain insights
into how the Hacker News community engages with different topics. You can download the datasets
we produced at the bottom of the article.</p><p>Use the tool below to explore various topics and the sentiments they evoke. The sentiment column
reports the median sentiment score, 0 being the most negative sentiment and 9 the most positive.
Click on the column headers to discover topics that inspire strong reactions, whether positive or
negative, and to identify divisive subjects that tend to generate polarized commentary.</p><h2 id="motivation">Motivation<a href="#motivation" aria-label="Direct link to Motivation" title="Direct link to Motivation">‚Äã</a></h2><p>If you have been following Hacker News for a while, you have likely developed an intuition
for the topics that the community loves - and topics the community loves to hate.
If you port <strong>Factorio</strong> to run on <strong>ZX Spectrum</strong> using <strong>Rust</strong>, you will be overwhelmed
with love and karma. On the other hand, you better don an asbestos suit if, instead
of raising a <strong>funding round</strong>, you sell your startup to a <strong>private equity</strong> company
so they can add <strong>telemetry</strong> in the codebase to power <strong>targeted ads</strong>.</p><p>But that&#39;s just a hazy intuition! Since the community is a big believer in <strong>rationality</strong>
and <strong>data science</strong> (the phrase is divisive though), we would be in a stronger position
if we could back our hunch with proper <strong>data analysis</strong>.</p><p>Also, it would give us an excuse to play with <strong>large language models</strong> which, all the
<strong>AI hype</strong> aside, are mind-blowingly effective at practical tasks like this. And,
we happen to be developers of an <strong>open source</strong> tool,
<a href="https://github.com/netflix/metaflow" target="_blank" rel="noopener noreferrer">Metaflow</a>, written in <strong>Python</strong>,
which makes it <strong>fun</strong> and <strong>educational</strong> to <strong>hack</strong> projects like this.</p><p>Type the bolded phrases in the textbox above to see if we are optimizing for an
appropriate <strong>emotional response</strong>. </p><h2 id="what-topics-trend-on-hacker-news">What topics trend on Hacker News?<a href="#what-topics-trend-on-hacker-news" aria-label="Direct link to What topics trend on Hacker News?" title="Direct link to What topics trend on Hacker News?">‚Äã</a></h2><p>As described in the implementation section below, we downloaded about 100,000 pages posted
on Hacker News to understand what content resonates with the community. We focus on posts
that gained at least 20 upvotes and 5 comments, prompting a large language model to come
up with ten phrases that best describe each page.</p><p>Here are the top 20 topics, aggregated from the 100,000 pages, along with the number of
posts covering each topic:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/top_topics-f47d2ee20906766a39ed8fdb2908c264.png" width="1295" height="1033"/></p><p>The top topics are hardly surprising. However, the community is known for having
diverse intellectual interests. The top 20 topics represent only 10% of the topics
covered across posts. You can see this by yourself by using the tool above that
cover all the 14,000 topics that are associated with at least five posts.</p><p>Have the top topics evolved over time? You bet. Here are the top-10 trending
topics:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/growing_topics-955fdd56864b7346df6f39888123e4a4.png" width="1410" height="1019"/></p><p>Even though the dataset only extends to June 2023, we can see a tsunami
of AI, natural language processing, and related topics. Sadly, layoffs started
trending mid-2022 as well. The rise of AI articles likely explains the growth
in the Technology topic as well.</p><h2 id="whats-declining">What&#39;s declining<a href="#whats-declining" aria-label="Direct link to What&#39;s declining" title="Direct link to What&#39;s declining">‚Äã</a></h2><p>In 2020-2022, an overwhelming macro-trend was everything related
to the COVID pandemic which fortunately is not a pressing issue
anymore. Fascinatingly, August 2021 was a tumultuous month with especially
<a href="https://www.wired.com/story/apple-photo-scanning-csam-communication-safety-messages/" target="_blank" rel="noopener noreferrer">Apple&#39;s proposed CSAM
scanning</a>
causing an uproar in posts related to Privacy and Apple.</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/declining_topics-fded256187c75900d3f7871a9acae077.png" width="1471" height="1048"/></p><p>If you want to take a trip down the memory lane, consider the past
topics on the left which haven&#39;t been covered once since January 2022:</p><table><thead><tr><th>Before Jan 2022 üìâ</th><th>After Jan 2022 üìà</th></tr></thead><tbody><tr><td>George Floyd</td><td>GPT-4</td></tr><tr><td>Herd immunity</td><td>Stable Diffusion</td></tr><tr><td>Antibodies</td><td>Russia-Ukraine War</td></tr><tr><td>IOS 14</td><td>Ventura</td></tr><tr><td>Freenode</td><td>Bank Failure</td></tr><tr><td>Suez Canal</td><td>Midjourney</td></tr><tr><td>Wallstreetbets</td><td>Hiring Freeze</td></tr><tr><td>Hydroxychloroquine</td><td>FIDO Alliance</td></tr><tr><td>Infection rates</td><td>Cost of living crisis</td></tr></tbody></table><p>Correspondingly, the topics on the right didn&#39;t exist before
January 2022. It doesn&#39;t take a PhD in Economics to understand how
<em>Wallstreetbets</em> got replaced by <em>Cost of living crisis</em>,
<em>Hiring freeze</em>, and <em>Bank failure</em>.</p><h2 id="but-how-do-people-feel-about-these-topics">But how do people <em>feel</em> about these topics<a href="#but-how-do-people-feel-about-these-topics" aria-label="Direct link to but-how-do-people-feel-about-these-topics" title="Direct link to but-how-do-people-feel-about-these-topics">‚Äã</a></h2><p>Importantly, sharing or upvoting a post doesn&#39;t imply endorsement - <a href="https://xkcd.com/386/" target="_blank" rel="noopener noreferrer">often the opposite</a>. Hence
to truly understand the dynamics of a community, we need to analyze how people react to posts, as expressed
through their comments.</p><p>To do this, we reconstructed comment threads associated with the posts in the dataset and asked
an LLM to classify the sentiment of the discourse between 0 and 9, zero being an all-out flamewar
and nine indicating a perfect harmony and positivity.</p><p>Our dutiful LLM took the job as a virtual community moderator and went through 100k comment threads in
about 9 hours, reading through 230M words consisting of the full emotional gamut of bitterness,
passion, wisdom, humor, and love.</p><p>Here&#39;s what the LLM came back with in terms of the distribution of sentiments:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/sentiment-distribution-da1bd4c602de66b6f1ef68cb3434a4f8.png" width="721" height="399"/></p><p>Firstly, the LLM is utterly befuddled about what a neutral discussion looks like - there are no 5 in the results.
Or, maybe this is a snarky note by the LLM noting that humans are incapable of unemotional, unbiased discourse.</p><p>Secondly, the sentiments are clearly skewed
towards the positive side, which aligns with our personal experience with the site. The bias to
positivity and optimism is a key reason why we open Hacker News daily. There&#39;s enough vitriol elsewhere.</p><p>It is also worth noting that at least some of the few ones assigned by the machine seem to be bogus.
For instance, <a href="https://news.ycombinator.com/item?id=32088537" target="_blank" rel="noopener noreferrer">this post by BentoML</a> was given a score of one,
although the sentiment seems majorly positive and supportive. All the usual caveats about LLMs apply.</p><h2 id="love-and-hate-in-hacker-news">Love and hate in Hacker News<a href="#love-and-hate-in-hacker-news" aria-label="Direct link to Love and hate in Hacker News" title="Direct link to Love and hate in Hacker News">‚Äã</a></h2><p>Now with the topics and the sentiment scores at hand, we can finally give a scientific answer to the question of
what the community loves and loves to hate:</p><table><thead><tr><th>Love üòç</th><th>Hate üò†</th></tr></thead><tbody><tr><td>Programming</td><td>FTX</td></tr><tr><td>Computer Science</td><td>Police Misconduct</td></tr><tr><td>Open Source</td><td>Sam Bankman-Fried</td></tr><tr><td>Python</td><td>Xinjiang</td></tr><tr><td>Game Development</td><td>Torture</td></tr><tr><td>Rust</td><td>Employee Monitoring</td></tr><tr><td>Electronics</td><td>Cost Cutting</td></tr><tr><td>Mathematics</td><td>Racial Profiling</td></tr><tr><td>Functional Programming</td><td>Online Safety Bill</td></tr><tr><td>Programming Language</td><td>War on Terror</td></tr><tr><td>Physics</td><td>Atlassian</td></tr><tr><td>Embedded Systems</td><td>CSAM</td></tr><tr><td>Self Improvement</td><td>NYPD</td></tr><tr><td>Database</td><td>Alameda Research</td></tr><tr><td>Unix</td><td>International Students</td></tr><tr><td>Astronomy</td><td>TSA</td></tr><tr><td>Retro Computing</td><td>Earn It Act</td></tr><tr><td>Nostalgia</td><td>Car Features</td></tr><tr><td>Debugging</td><td>Bloatware</td></tr></tbody></table><p>Geeks, nerds, and hackers should find themselves right at home! Interestingly, while the community tends to be
visionary and forward-looking (with technical matters at least), it definitely has a soft spot for (technical) nostalgia.
While the modern world is amazing, we miss ZX Spectrum, Z80 assembly, and 8086 dearly. At least we find some
comfort in PICO-8.</p><p>On the anger-inducing side, most topics need no explanation. It is worth clarifying though that Hacker News does not
hate International Students, but the posts related to them tend to be overwhelmingly negative,
reflecting the community‚Äôs sympathy for the challenges faced by those studying abroad.</p><p>Comically, Hacker News is not a community of car lovers. When we talk about cars, it is because there&#39;s
something wrong with them. For more insights like this, use the tool at the top of the page to explore
the diverse landscape of HN topics in detail.</p><h2 id="some-topics-are-just-divisive">Some topics are just divisive<a href="#some-topics-are-just-divisive" aria-label="Direct link to Some topics are just divisive" title="Direct link to Some topics are just divisive">‚Äã</a></h2><p>Besides topics being unimodally love or hate-inducing, some topics are bimodal: Sometimes
a post about the topic generates a highly positive response, other times a flamewar. Examples include</p><ul><li><strong>GNOME</strong> - <a href="https://www.mayrhofer.eu.org/post/kde-vs-gnome/" target="_blank" rel="noopener noreferrer">KDE vs. GNOME</a> - the war has been raging for 25 years.</li><li><strong>Google</strong> - a dominant force in the Internet, both in good and bad.</li><li><strong>Government regulations</strong> - damned if you do and damned if you don&#39;t.</li><li><strong>Venture capital</strong> - the lifeblood of Silicon Valley and a source of endless gossip.</li></ul><p>See more by sorting by the <code>divisive</code> column in the tool above. To rank highly by the divisiveness
score, the topic must be associated with both negative and positive posts equally
and not many neutral ones.</p><h2 id="is-the-mood-improving">Is the mood improving?<a href="#is-the-mood-improving" aria-label="Direct link to Is the mood improving?" title="Direct link to Is the mood improving?">‚Äã</a></h2><p><em>Cue <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines" target="_blank" rel="noopener noreferrer">Betteridge&#39;s Law</a></em>.</p><p>We can plot the average sentiment over time, as expressed in daily comment threads:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/sentiment-over-time-97569616155ec98e8d15c008b62591bc.png" width="1147" height="818"/></p><p>The data proves that things sucked in August 2021 (or maybe it was just the Apple debacle highlighted
in the chart above). Overall, there&#39;s a clear but modest downward trend in the average sentiment.</p><p>It
would require a deeper analysis to understand why this is the case. Putting aside an obvious hypothesis
that life is just getting worse (which <a href="https://en.wikipedia.org/wiki/Factfulness" target="_blank" rel="noopener noreferrer">might not be true</a>),
an alternative hypothesis may be a variant of
<a href="https://en.wikipedia.org/wiki/Eternal_September" target="_blank" rel="noopener noreferrer">Eternal September</a>. It takes conscious and tireless
effort to maintain a positive mood in a growing community. Kudos to HN moderators for keeping the
community thriving and positive over the years!</p><h2 id="implementation">Implementation<a href="#implementation" aria-label="Direct link to Implementation" title="Direct link to Implementation">‚Äã</a></h2><p>A reason to be excited and optimistic about the future is the very existence of this article.
While natural language processing and sentiment analysis have been around for decades,
the quality, versatility, and the ease of use afforded by LLMs is absolutely unprecedented.</p><p>Attaining the quality
of topics and sentiment scores with a messy dataset like the one here would have required a PhD-thesis
level of effort just a few years ago - and very likely the results would have been worse. In contrast,
we developed all the code for this article in about seven hours. Processing 350M tokens with just
a decent-sized model would have required a supercomputer a decade ago, whereas in our case it took about 16 hours
using widely available hardware.</p><p>Most amazingly, all the building blocks, LLMs included, are available in open source! Let&#39;s do a
quick overview (with code and data), showing how you can repeat the experiment at home.</p><p>Here&#39;s what we did at the high level:</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/hacker-news-architecture-cec4efca1150350d4bb35ac06dfb7761.png" width="1920" height="1080"/></p><p>Each white box in the picture is <a href="https://docs.metaflow.org" target="_blank" rel="noopener noreferrer">a Metaflow flow</a>, linked below:</p><ol><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hninit.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentInit</code></a> creates a list of posts to analyze.</li><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hncrawl.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentCrawl</code></a> downloads the posts.</li><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnposts.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentAnalyzePosts</code></a> parses the posts and runs them through an LLM.</li><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hncomments.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentCommentData</code></a> reconstructs comment threads based on <a href="https://console.cloud.google.com/marketplace/product/y-combinator/hacker-news" target="_blank" rel="noopener noreferrer">a Hacker News dataset in Google BigQuery</a>. We should/could
have used this in the step (1) too. Next time!</li><li><a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnsentiment.py" target="_blank" rel="noopener noreferrer"><code>HNSentimentAnalyzeComments</code></a> runs the comment threads through an LLM.</li><li>Data analyses and the charts shown above are produced in notebooks (the gray box in the picture).</li></ol><p>Here&#39;s what the flows do:</p><h3 id="donwloading-posts">Donwloading posts<a href="#donwloading-posts" aria-label="Direct link to Donwloading posts" title="Direct link to Donwloading posts">‚Äã</a></h3><p>First, we wanted to analyze topics covered by Hacker News posts that have
generated some discussion. Using <a href="https://huggingface.co/datasets/julien040/hacker-news-posts" target="_blank" rel="noopener noreferrer">a publicly
available dataset of HN posts</a>
(<a href="https://julienc.me/" target="_blank" rel="noopener noreferrer">thanks Julien!</a>), we queried all posts between
January 2020 and June 2023 (the latest date available in this dataset) which had at
least 20 upvotes and more than five comments, which resulted in about 100,000 posts.
<a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hninit.py" target="_blank" rel="noopener noreferrer">Here&#39;s the simple Metaflow flow</a>
that did the job, much thanks to DuckDB.</p><p>Since the 100,000 posts are mostly on different domains, we can safely download
them in parallel without DDOS&#39;ing the servers. It took only around 25 minutes to
download the pages with 100 parallel workers
(<a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hncrawl.py" target="_blank" rel="noopener noreferrer">see here how</a>).</p><h3 id="large-scale-document-understanding-with-llms">Large-scale document understanding with LLMs<a href="#large-scale-document-understanding-with-llms" aria-label="Direct link to Large-scale document understanding with LLMs" title="Direct link to Large-scale document understanding with LLMs">‚Äã</a></h3><p>Parsing the text content from random HTML pages used to be a massive PITA, but
<a href="https://beautiful-soup-4.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">BeautifulSoup</a> makes it <a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnposts.py#L94" target="_blank" rel="noopener noreferrer">beautifully
straightforward</a>.</p><p>At this point, we have a relatively clean dataset of about 100,000 text documents with
more than 500M tokens in total. Processing the dataset once with a state-of-the-art LLM,
say, GPT-4o, or Llama3 70b on AWS Bedrock would cost around $1,300 (using ChatGPT batch API),
not counting the output tokens. Besides the cost, time is a concern - we want to process
the dataset as fast as possible, so we can evaluate the results quickly, and rinse-and-repeat
if needed. Hence we don&#39;t want to get rate-limited or otherwise bottlenecked by the APIs.</p><p>We have <a href="https://outerbounds.com/blog/document-understanding/" target="_blank" rel="noopener noreferrer">posted previously</a> about our
success with <a href="https://outerbounds.com/blog/nim-announcement/" target="_blank" rel="noopener noreferrer">NVIDIA NIM microservices</a> that
provide a quickly growing library of LLMs and other GenAI models as prepackaged images,
fully optimized to take advantage of vLLM, TensorRT-LLM and the Triton Inference Server,
so you don&#39;t have to spend time <a href="https://outerbounds.com/blog/the-many-ways-to-deploy-a-model/#measuring-performance" target="_blank" rel="noopener noreferrer">chasing the latest tricks with LLM
inference</a>.</p><p>Since we had NIMs included in <a href="https://outerbounds.com/platform" target="_blank" rel="noopener noreferrer">our Outerbounds deployment</a>,
we just <a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnposts.py#L40" target="_blank" rel="noopener noreferrer">added <code>@nim</code> to our flow</a>
to get  access to a high-throughput LLM endpoint that costs only as much as the auto-scaling GPUs
it runs on, in this case, four H100 GPUs. Naturally you can hit an LLM endpoint of your
choosing - the options are many these days.</p><h3 id="prompting-an-llm-to-produce-a-list-of-topics-for-each-post">Prompting an LLM to produce a list of topics for each post<a href="#prompting-an-llm-to-produce-a-list-of-topics-for-each-post" aria-label="Direct link to Prompting an LLM to produce a list of topics for each post" title="Direct link to Prompting an LLM to produce a list of topics for each post">‚Äã</a></h3><p>Our prompt is straightforward:</p><div open=""><div><pre tabindex="0"><code><span><span>Assign 10 tags that best describe the following article.</span><br/></span><span><span>Reply only the tags in the following format:</span><br/></span><span><span>1. first tag</span><br/></span><span><span>2. second tag</span><br/></span><span><span>N. Nth tag</span><br/></span><span><span>---</span><br/></span><span><span>[First 5000 tokens from a web page]</span><br/></span></code></pre></div></div><p>The llama3 70b model we used has a 8,000 token context window, but we decided to
limit the number of tokens to 5,000 to account for differences in the tokenizer behavior,
making sure that we don&#39;t go past the limit. </p><p>Processing about 140M inputs tokens in this manner took about 9 hours. We were able to
increase throughput to around 4,300 input tokens per second by hitting the model concurrently
with five workers, as neatly shown in our UI below, to take advantage of dynamic batching
and other optimizations.</p><p><img loading="lazy" src="https://outerbounds.com/assets/images/sentiment-mfgui-582a32fc57b12b9a0835e61997598237.png" width="1869" height="950"/></p><p>Instead of trying to download 100,000 comment pages directly from Hacker News, we
leveraged <a href="https://console.cloud.google.com/marketplace/product/y-combinator/hacker-news" target="_blank" rel="noopener noreferrer">a Hacker News dataset in Google
BigQuery</a>.
Annoyingly, comments in the database are not directly associated with their parent
post, so we had to implement <a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hncomments.py#L152" target="_blank" rel="noopener noreferrer">a small function to reconstruct the comment
threads</a>.</p><p>We can&#39;t run the function with BigQuery directly but luckily we can export the data easily
in Parquet files. Loading the resulting 16M rows in DuckDB and scanning through them
was a breeze, aided by the fact that Metaflow knows how to <a href="https://outerbounds.com/blog/metaflow-fast-data/" target="_blank" rel="noopener noreferrer">load data
fast</a>. We just
added <code>@resources(disk=10000, cpu=8, memory=32000)</code>  to run the function on a large enough instance.</p><h3 id="prompting-an-llm-to-analyze-sentiment">Prompting an LLM to analyze sentiment<a href="#prompting-an-llm-to-analyze-sentiment" aria-label="Direct link to Prompting an LLM to analyze sentiment" title="Direct link to Prompting an LLM to analyze sentiment">‚Äã</a></h3><p>With the comment threads at hand, we were able to <a href="https://github.com/outerbounds/hacker-news-sentiment/blob/main/hnsentiment.py" target="_blank" rel="noopener noreferrer">run them through our
LLM</a> with this prompt:</p><div open=""><div><pre tabindex="0"><code><span><span>In the scale between 0-10 where 0 is the most negative sentiment</span><br/></span><span><span>and 10 is the most positive sentiment, rank the following discussion.</span><br/></span><span><span>Reply in this format:</span><br/></span><span><span></span><br/></span><span><span>SENTIMENT X</span><br/></span><span><span></span><br/></span><span><span>where X is the sentiment rating</span><br/></span><span><span>---</span><br/></span><span><span>[First 3000 tokens from a comment thread]</span><br/></span></code></pre></div></div><p>Getting a simple structured output like this seems to work without issues. We processed
through some 230M input tokens in this manner, which took only about 7 hours as we
needed only two output tokens.</p><p>You can reproduce all the steps above using your favorite toolchain. Here are
key reasons why we used Metaflow and why you might want to consider it too:</p><ul><li><p><strong>Staying organized without effort</strong> - a big benefit compared to random Python
scripts or notebooks is that Metaflow persists all artifacts automatically, tracks all
executions, and keeps everything organized. We relied on <a href="https://docs.metaflow.org/scaling/tagging" target="_blank" rel="noopener noreferrer">Metaflow&#39;s
namespaces</a> to execute large and expensive
runs alongside prototypes, knowing that the two can&#39;t interfere with each other. We
used <a href="https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags/" target="_blank" rel="noopener noreferrer">Metaflow tags</a>
to organize data sharing between flows while they were being developed independently.</p></li><li><p><strong>Easy cloud scaling</strong> - crawling required <a href="https://docs.metaflow.org/scaling/remote-tasks/introduction#running-many-tasks-in-parallel-with-foreach" target="_blank" rel="noopener noreferrer">horizontal
scaling</a>,
DuckDB required <a href="https://docs.metaflow.org/scaling/remote-tasks/introduction#requesting-compute-resources" target="_blank" rel="noopener noreferrer">vertical scaling</a>,
and LLMs required <a href="https://docs.metaflow.org/scaling/remote-tasks/gpu-compute" target="_blank" rel="noopener noreferrer">a GPU backend</a>.
Metaflow handled all the cases out of the box.</p></li><li><p><strong>Highly available orchestrator</strong> - running a large dataset through an LLM can cost
thousands of dollars. You don&#39;t want the run to fail because of random issues.
We relied on <a href="https://docs.metaflow.org/production/introduction" target="_blank" rel="noopener noreferrer">a highly available Argo Workflows
orchestrator</a> that Metaflow
supports out of the box to keep the run running for hours.</p></li></ul><p>You can do all of the above using <a href="https://metaflow.org" target="_blank" rel="noopener noreferrer">open-source Metaflow</a>, but we had a few
additional benefits by running the flows on <a href="https://outerbounds.com/platform" target="_blank" rel="noopener noreferrer">Outerbounds Platform</a>:
It is simply fun to develop code like this, including notebooks, with VSCode running on
<a href="https://outerbounds.com/features/cloud-workstations/" target="_blank" rel="noopener noreferrer">cloud workstations</a>,
<a href="https://outerbounds.com/features/compute-at-scale/" target="_blank" rel="noopener noreferrer">scaling to the cloud</a> is smooth sailing,
and <a href="https://outerbounds.com/blog/nim-announcement/" target="_blank" rel="noopener noreferrer"><code>@nim</code></a> allowed us to hit LLMs without
worrying about cost or rate limiting.</p><p>If features like this sound relevant to your interests, we are happy
to <a href="https://outerbounds.com/get-started/" target="_blank" rel="noopener noreferrer">get you started for free</a>.</p><h2 id="dive-deeper-at-home">Dive deeper at home<a href="#dive-deeper-at-home" aria-label="Direct link to Dive deeper at home" title="Direct link to Dive deeper at home">‚Äã</a></h2><p>There&#39;s much more that can be analyzed and visualized with this dataset. Instead of spending
a few thousand dollars hitting OpenAI APIs, you can
<a href="https://github.com/outerbounds/hacker-news-sentiment/tree/main/data" target="_blank" rel="noopener noreferrer">download the topics and sentiments we created</a>:</p><ul><li><p><code>post-sentiment.json</code> contains a mapping <code>post_id -&gt; sentiment_score</code></p></li><li><p><code>post-topics.json</code> contains a mapping <code>post_id -&gt; [topics]</code></p></li><li><p><code>topics-data.json</code> contains a cleaned and joined dataset based on the above JSONs,
powering the tool at the top of this article.</p></li></ul><p>You can find metadata related to post IDs in <a href="https://huggingface.co/datasets/julien040/hacker-news-posts" target="_blank" rel="noopener noreferrer">this HuggingFace
dataset</a> and in the
<a href="https://console.cloud.google.com/marketplace/product/y-combinator/hacker-news" target="_blank" rel="noopener noreferrer">Google BigQuery Hacker News
dataset</a>.
To view posts, simply open
<a href="https://news.ycombinator.com/item?id=41157595" target="_blank" rel="noopener noreferrer">https://news.ycombinator.com/item?id=[post_id]</a>.</p><p>For instance, it would be interesting to look into the correlation between post domains and
sentiments and topics. Our hunch is that certain domains produce predominantly positive
sentiments and vice versa. Or, do divisive topics garner more points?</p><p>If you create something fun with this data, please link back to this blog article and let us
know! Join <a href="http://slack.outerbounds.co" target="_blank" rel="noopener noreferrer">Metaflow Slack</a> and drop a note on <code>#ask-metaflow</code>.</p><p>To support our open-source efforts, please give <a href="http://github.com/netflix/metaflow" target="_blank" rel="noopener noreferrer">Metaflow a star</a>! ü§ó</p></div></div>
  </body>
</html>
