<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bitbashing.io/async-rust.html">Original</a>
    <h1>Maybe Rust isn’t a good tool for massively concurrent, userspace software</h1>
    
    <div id="readability-page-1" class="page"><article>
    <p>But to get at whatever the hell I mean by that,
we need to talk about why <code>async</code> Rust exists in the first place.
Let’s talk about:</p>

<h2 id="modern-concurrency-theyre-green-theyre-mean--they-ate-my-machine">Modern Concurrency: They’re Green, They’re Mean, &amp; They Ate My Machine</h2>

<p><img src="https://assets.bitbashing.io/images/gofast.png" alt="gotta go fast"/></p>

<!-- ## Why are we here? -->

<p>Suppose we want our code to go fast. We have two big problems to solve:</p>

<ol>
  <li>
    <p>We want to use the whole computer. Code runs on CPUs, and in 2023,
even my phone has eight of the damn things. If I want to use more than
12% of the machine, I need several cores.</p>
  </li>
  <li>
    <p>We want to keep working while we wait for slow things to complete
instead of just twiddling our thumbs.
Sending a message over the Internet, or even opening a file<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>
takes eternities in computer time—we could literally do
<em>millions</em> of other things meanwhile.</p>
  </li>
</ol>

<p>And so, we turn to our friends <em>parallelism and concurrency</em>.
It’s a favorite hobby of CS nerds to quibble over distinctions between the two,
so to oversimplify:</p>

<p><strong>Parallelism</strong> is about running code <em>in parallel</em> on several CPUs.</p>

<p><strong>Concurrency</strong> is about breaking a problem into separate, independent parts.</p>

<p>These <a href="https://www.youtube.com/watch?v=oV9rvDllKEg">are not the same thing</a>—single-core
machines have been running code concurrently for half a century now—but they are related.
So much online <em>well akshually</em>-ing ignores how we often break programs
into concurrent pieces <em>so that</em> those pieces can run in parallel,
and interleave in ways that keep our cores crunching!
(If we didn’t care about performance, why would we bother?)</p>

<h3 id="how-do-i-concurrency">How do I concurrency?</h3>

<p>One of the simplest ways to build a concurrent system is to split code into multiple processes.
After all, the operating system is a lean, mean, concurrency machine,
conspiring with your hardware to make each process think it has the
whole box to itself.
And the OS’s scheduler gives us parallelism for free, running <em>time slices</em> of
any process that’s ready on an available CPU core.
Once upon a time this was
<a href="https://en.wikipedia.org/wiki/Common_Gateway_Interface"><em>the</em> way</a>,
and we still employ it today whenever we pipe shell commands together.</p>

<figure>
<img src="https://cube-drone.com/103.gif"/>
<figcaption>
All hail <a href="https://cube-drone.com">CubeDrone</a>
</figcaption>
</figure>

<p>But this approach has its limitations. Inter-process communication is not cheap,
since most implementations copy data to OS memory and back.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup></p>

<h3 id="mutex-based-concurrency-considered-harmful-or-hoare-was-right">Mutex-Based Concurrency Considered Harmful, or, <em>Hoare Was Right</em></h3>

<blockquote>
  <p>Some people, when confronted with a problem, think, “I know, I’ll use threads,”
and then two they hav erpoblesms.</p>
</blockquote>

<p>We can avoid these overheads using <em>threads</em>—processes that share the same memory.
Common wisdom <!-- ---ones you'll learn in any operating systems
course---tell --> teaches us to connect them with mysterious beasts,
like <em>mutexes</em>, <em>condition variables</em>, and <em>semaphores</em>. This is a dangerous game!
Simple mistakes will plague you with <em>race conditions</em>
and <em>deadlocks</em> and other terrible diseases that fill your code with bugs,
but only on Tuesdays when it’s raining and the temperature is is a multiple of three.
And god help you if you want to learn how this stuff actually works on modern
hardware.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<p>There is Another Way.
In his 1978 paper, <em>Communicating Sequential Processes</em>, Tony Hoare
suggested connecting threads with queues, or <em>channels</em>,
which they can use to send each other messages.
This has many advantages:</p>

<ul>
  <li>
    <p>Threads enjoy process-like isolation from the rest of the program,
since they don’t share memory.
(Bonus points for memory-safe languages that make it hard to
accidentally scramble another thread!)</p>
  </li>
  <li>
    <p>Each thread has a very obvious set of inputs (the channels it receives from)
and outputs (the channels it sends to).
This is easy to reason about, and easy to debug!
Instrument the channels for powerful visibility into your system,
measuring each thread’s throughput.</p>
  </li>
  <li>
    <p>Channels <em>are the synchronization</em>.
If a channel is empty, the receiver waits until it’s not.
If a channel is full, the sender waits.
Threads never sleep while they have work to do,
and gracefully pause if they outpace the rest of the system.</p>
  </li>
</ul>

<p>After decades of mutex madness,
many modern languages heed Hoare’s advice and
provide channels in their standard library.
In Rust, we call them
<a href="https://doc.rust-lang.org/std/sync/mpsc/fn.sync_channel.html"><code>std::sync::mpsc::sync_channel</code></a>.</p>

<p>Most software can stop here, building concurrent systems with threads and channels.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>
Combine them with tools to parallelize CPU-intensive loops
(like Rust’s <a href="https://crates.io/crates/rayon">Rayon</a>
or Haskell’s <a href="https://hackage.haskell.org/package/parallel-3.2.2.0/docs/Control-Parallel-Strategies.html"><code>par</code></a>),
and you’ve got a powerful cocktail.</p>

<p>But…</p>

<h3 id="ludicrous-speed-go">Ludicrous Speed, go!</h3>

<p><img src="https://assets.bitbashing.io/images/ludicrous-speed.jpg" alt="going to plaid"/></p>

<p>Some problems demand a <em>lot</em> of concurrency.
The canonical example, described by Dan Kagel as the
<a href="https://web.archive.org/web/19990508164301/http://www.kegel.com/c10k.html"><em>C10K problem</em></a>
back in 1999, is a web server connected to tens of thousands of concurrent users.
At this scale, threads won’t cut it—while they’re <em>pretty</em> cheap,<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" rel="footnote">5</a></sup>
fire up a thread per connection and your computer will grind to a halt.</p>

<p>To solve this, some languages provide a concurrency model where:</p>

<ol>
  <li>
    <p>Tasks are created and managed in <em>userspace</em>,
i.e., without the operating system’s help.</p>
  </li>
  <li>
    <p>A <em>runtime</em> schedules these tasks onto a pool of OS threads,
usually sized so that each CPU core gets a thread, to maximize parallelism.</p>
  </li>
</ol>

<p>This scheme goes by many names—<em>green threads, lightweight threads,
lightweight processes, fibers, coroutines</em>, and more—complete with pedantic
nerds endlessly debating the subtle differences between them.</p>

<p>Rust comes at this problem with an “async/await” model,
seen previously in places like C# and Node.js.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" rel="footnote">6</a></sup>
Here, functions marked <code>async</code> don’t block, but immediately return
a <em>future</em> or <em>promise</em> that can be awaited to produce the result.</p>
<div><div><pre><code><span>fn</span> <span>foo</span><span>()</span> <span>-&gt;</span> <span>i32</span> <span>{</span> <span>/* returns an int when called */</span> <span>}</span>

<span>async</span> <span>fn</span> <span>bar</span><span>()</span> <span>-&gt;</span> <span>i32</span> <span>{</span> <span>/* returns a future we can .await to get an int */</span> <span>}</span>
</code></pre></div></div>

<h2 id="painawait"><code>pain.await</code></h2>

<p>On one hand, futures in Rust are exceedingly small and fast,
thanks to their <em>cooperatively scheduled, stackless</em> design.
But unlike other languages with userspace concurrency,
Rust tries to offer this abstraction while <em>also</em> promising the programmer
total low-level control.</p>

<p>There’s a fundamental tension between the two, and the poor <code>async</code> Rust programmer
is perpetually caught in the middle, torn between the language’s design goals
and the massively-concurrent world they’re trying to build.
Rust attempts to statically verify the lifetime of every object and reference
in your program, all at compile time.
Futures promise the opposite: that we can break code
<em>and the data it references</em> into thousands of little pieces,
runnable at any time, on any thread,
based on conditions we can only know once we’ve started!
A future that reads data from a client should only run when that client’s socket
has data to read, and no lifetime annotation will tells us when that might be.</p>

<h2 id="send-help">Send help</h2>

<p>Assuring the compiler that everything will be okay runs into the same challenges
you see when working with raw threads.
Data must either be marked <code>Send</code> and moved,
or passed through references with <code>&#39;static</code> lifetimes.
Both are easier said than done.
Moving (at least without cloning)
is often a non-starter, since it’s common in <code>async</code> code to spawn many
tasks that share common state.
And references are a pain too—there’s no
<a href="https://doc.rust-lang.org/std/thread/fn.scope.html"><code>thread::scope</code></a> equivalent to help us
bound futures’ lifetimes to anything short of “forever”.</p>
<div><div><pre><code><span>fn</span> <span>foo</span><span>(</span><span>&amp;</span><span>big</span><span>,</span> <span>&amp;</span><span>chungus</span><span>)</span>
</code></pre></div></div>
<p>is out,</p>
<div><div><pre><code><span>async</span> <span>fn</span> <span>foo</span><span>(</span><span>&amp;</span><span>BIG_GLOBAL_STATIC_REF_OR_SIMILAR_HORROR</span><span>,</span> <span>sendable_chungus</span><span>.clone</span><span>())</span>
</code></pre></div></div>
<p>is in.</p>

<p>And unlike launching raw threads, where you might have to deal with these annoyances
in a handful of functions,
this happens <em>constantly</em> due to
<a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/"><code>async</code>’s viral nature</a>.
Since any function that calls an <code>async</code> function must itself be <code>async</code>,<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" rel="footnote">7</a></sup>
you need to solve this problem everywhere, all the time.</p>

<h2 id="just-arc-my-shit-up">Just Arc my shit up</h2>

<p>A seasoned Rust developer will respond by saying that Rust gives us simple tools
for dynamic lifetimes spanning multiple threads.
We call them “atomic reference counts”,
or <a href="https://doc.rust-lang.org/std/sync/struct.Arc.html"><code>Arc</code></a>.
While it’s true that they solve the immediate problem—borrows check and our
code compiles—they’re far from a silver bullet.
Used pervasively, <code>Arc</code> gives you the world’s worst garbage collector.
Like a GC, the lifetime of objects and the resources they represent
(memory, files, sockets) is unknowable.
But you take this loss without the wins you’d get from an actual GC!</p>

<p>Don’t buy the “GC is slow” FUD—the claim is a misunderstanding of
latency vs. throughput at best and a bizarre psyop at worst.
A modern, moving garbage collector gets you more allocation throughput,
less fragmentation, and means you don’t have to play Mickey Mouse games with
weak pointers to avoid cycle leaks.
You can even trick systems programmers into leveraging GC in one of the world’s
most important software projects by calling it
<a href="https://www.kernel.org/doc/html/next/RCU/whatisRCU.html">“deferred destruction”</a>.
More on that another day.</p>

<h2 id="other-random-nonsense">Other random nonsense</h2>

<ul>
  <li>
    <p>Because Rust coroutines are stackless, the compiler turns each one into
a state machine that advances to the next <code>.await</code>.<sup id="fnref:42" role="doc-noteref"><a href="#fn:42" rel="footnote">8</a></sup>
But this makes any recursive <code>async</code> function a recursively-defined type!
A user just trying to call a function from itself is met with
inscrutable errors until they manually box it or use a
<a href="https://crates.io/crates/async-recursion">crate</a> that does the same.</p>
  </li>
  <li>
    <p>There’s an important distinction between a <em>future</em>—which does nothing
until awaited—and a <a href="https://docs.rs/tokio/latest/tokio/task/fn.spawn.html"><code>task</code></a>,
which spawns work in the runtime’s thread pool… returning a future that
marks its completion.</p>
  </li>
  <li>
    <p>There’s nothing keeping you from calling blocking code inside a future,
and there’s nothing keeping that call from blocking the runtime thread it’s on.
You know, the entire thing we’re trying to avoid with all this <code>async</code> business.</p>
  </li>
</ul>

<h2 id="running-away">Running away</h2>

<p><img src="https://assets.bitbashing.io/images/run-away.jpg"/></p>

<p>Mixed together, this all gives <code>async</code> Rust a much different flavor than
“normal” Rust. One with many more gotchas,
that is harder to understand and teach,
and that pushes users to either:</p>

<ul>
  <li>
    <p>Develop a deep understanding of how these abstractions actually work,<sup id="fnref:69" role="doc-noteref"><a href="#fn:69" rel="footnote">9</a></sup>
writing complicated code to handle them, or</p>
  </li>
  <li>
    <p>Sprinkle <code>Arc</code>, <code>Pin</code>, <code>&#39;static</code>, and other sacred runes throughout their
code and hope for the best.</p>
  </li>
</ul>

<p>Rust proponents (I’d consider myself one!) might call these criticisms overblown.
But I’ve seen whole teams of experienced developers,
trying to use Rust for some new project, mired in this minutia.
To whatever challenges teaching Rust has, <code>async</code> adds a whole new set.</p>

<p>The degree to which these problems <em>just aren’t a thing</em> in other languages
can’t be overstated either.
In Haskell or Go, “async code” is just normal code.
You might say this isn’t a fair comparison—after all,
those languages hide the difference between blocking and non-blocking
code behind fat runtimes, and lifetimes are handwaved with garbage collection.
But that’s exactly the point!
These are pure wins when we’re doing this sort of programming.</p>

<p>Maybe Rust isn’t a good tool for massively concurrent, userspace software.
We can save it for the 99% of our projects that
<a href="#mutex-based-concurrency-considered-harmful-or-hoare-was-right">don’t have to be</a>.</p>

<hr/>

<div role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>…a file which could also be on the other side of the Internet!
  Thanks, NFS! <a href="#fnref:1" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>We could cut down on IPC overhead by sharing memory between the processes,
  but this gives away one of the main advantages of multiple
  processes: that the OS isolates them from each other. <a href="#fnref:2" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Mara Bos recently put out <a href="https://marabos.nl/atomics/">a fantastic book</a>
  that despite targeting Rust specifically, does a wonderful job of
  explaining the fundamentals of low-level concurrency in any language.
  If you don’t have time for a whole book, I’ve done my best to sum it up
  <a href="https://assets.bitbashing.io/papers/concurrency-primer.pdf">in a few pages</a>. <a href="#fnref:3" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Of course I’m simplifying here. Not every program can be expressed as a
  <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a>,
  and you’ll still find good occasions for other primitives—say,
  atomic flags to indicates changes in global state.
  Still, Hoare’s model is a great default, and I’ve always found it helpful
  to think about how data flows through my system. <a href="#fnref:4" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Each thread has a 4kB control block in Linux,
  and switching between threads requires a trip to the operating system
  scheduler. This <em>context switch</em> to the OS’s memory is much more expensive
  than a normal function call. <a href="#fnref:5" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Uniquely, Rust doesn’t provide a runtime for its futures in the language,
  delegating instead to libraries like <a href="https://tokio.rs/">Tokio</a>.
  This is great for users—Rust’s build tooling (<code>cargo</code>) and
  <a href="https://crates.io/">ecosystem</a> gives developers the freedom to choose
  alternatives that better suit unique environments they find themselves in.
  But it’s a detail that’s largely immaterial to our discussion;
  one can imagine a world where Tokio is built into the language and all the
  same rules apply. <a href="#fnref:6" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>You can break the chain by commanding the entire runtime to
  <a href="https://docs.rs/tokio/1.32.0/tokio/runtime/struct.Runtime.html#method.block_on">block on</a>
  the completion of a future, but you probably shouldn’t do this pervasively
  since it isn’t composable. If a function blocks on a future,
  and that future calls a function that blocks on a future, congrats!
  The runtime panics! <a href="#fnref:7" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:42" role="doc-endnote">
      <p>Learn more in Without Boats’s
   <a href="https://without.boats/blog/futures-and-segmented-stacks/"><em>Futures and Segmented Stacks</em></a>
   or the C++ paper <a href="https://open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1364r0.pdf">P1364: <em>Fibers under the magnifying glass</em></a>. <a href="#fnref:42" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:69" role="doc-endnote">
      <p>Amos Wenger AKA fasterthanlime’s
  <a href="https://fasterthanli.me/articles/pin-and-suffering"><em>Pin and Suffering</em></a>
  is a fantastic and snarky intro. <a href="#fnref:69" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </article></div>
  </body>
</html>
