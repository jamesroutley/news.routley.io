<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rachelbythebay.com/w/2024/05/27/feed/">Original</a>
    <h1>So many feed readers, so many bizarre behaviors</h1>
    
    
<p>
It&#39;s been well over a year since I 
<a href="https://rachelbythebay.com/w/2023/01/18/http/">started serving 429s</a>
to clients which are hitting the feed too often.  Since then, much has 
happened, and most of it is generally good news.
</p>
<p>
I&#39;ve heard from users and authors alike of feed software.  Sometimes the 
users have filed bug reports and/or feature requests and have gotten 
positive results from the project (or vendor).  Other times, the authors 
of such software have gotten in touch, did some digging, found a few 
nuances of how their libraries work, and improved the situation.
</p>
<p>
Some of them are trying but are still not quite making it right.
</p>
<p>
Here&#39;s some of what&#39;s been going on.
</p>
<p>
...
</p>
<p>
At least one reader improved to not send a date from 1800.  
Unfortunately, it&#39;s now
<a href="https://rachelbythebay.com/w/2023/03/31/html/">sending the wrong value</a>
in its If-Modified-Since headers.  Instead of sending the value it   
obtained from the Last-Modified header on the past fetch, it&#39;s using the 
value from the &#34;updated&#34; header in the feed itself.
</p>
<p>
These are different layers of the system, and you can&#39;t mix their values 
together.  They&#39;re close, but not exactly the same.  This is how it 
works.
</p>
<p>
I get the impression from stalking their issues that they don&#39;t really 
control their HTTP requests very well because it&#39;s done by some other 
library.  That&#39;s where the 1800 thing came from in the first place.  It 
sounds like yet another
<a href="https://rachelbythebay.com/w/2020/08/09/lib/">case</a>
of using libraries that really don&#39;t do their jobs properly.
</p>
<p>
...
</p>
<p>
There&#39;s another one which hits every 2 minutes without fail, and 
there&#39;s no way to change it.  I&#39;ve even installed that app on a test 
account and verified this myself - it&#39;s hard-coded.  As a result, it&#39;s 
the one feed user-agent I&#39;ve had to block outright.  It doesn&#39;t stop 
requesting things even when it hits a brick wall of 403s.  Clearly, I 
need to use a bigger hammer.
</p>
<p>
...
</p>
<p>
A fair number of people are sending conditional requests, but are doing 
it every 5 or 10 minutes.  This is ridiculous.  I don&#39;t write that 
often, and never have.  Polling more is not going to get you anywhere, 
and indeed, will now get you delayed so you get your updates much later 
than the well-behaved people.  Knock it off.
</p>
<p>
It seems like most of these come from things which appear to just be 
jammed into web browsers as some kind of extension.  From hearing from 
at least one developer, it seems like they don&#39;t do conditional requests 
as a matter of course.  This, despite being part of a web browser 
ecosystem which has understood the notion of a conditional request and 
caching things locally for nearly three decades.  Amazing.
</p>
<p>
...
</p>
<p>
A while back, I added a &#34;Retry-After: &#34; header to the feed.  Anyone who 
gets a 429 will also get intel on when they should try back.  It&#39;s in 
seconds, so it&#39;ll be something like 3600 or 86400 depending on which 
kind of request was sent in the first place.
</p>
<p>
There are feed services which will actually reset their countdowns every 
time someone trips a 429.  I&#39;m not doing that.  Yet.
</p>
<p>
This is why noticing and honoring that header matters.
</p>
<p>
...
</p>
<p>
Oh, here&#39;s a new thing: goofy programs that try to &#34;guess&#34; the feed URL.  
I see all kinds of stupid requests to paths that might have a feed on 
it.  This is a new level of density on the part of the authors of those 
programs.
</p>
<p>
Here&#39;s the thing.  I&#39;ve had metadata in the top of every single /w/ post 
*and* its index since some time in 2012.  It looks like this:
</p>
<pre class="terminal">&lt;link rel=&#34;alternate&#34; type=&#34;application/atom+xml&#34; href=&#34;/w/atom.xml&#34;&gt;
</pre>
<p>
If you view source on this post or any other on the web, you&#39;ll see it 
up there, just hanging out.
</p>
<p>
I did that way back then because browsers used to care about RSS and 
Atom, and they&#39;d put that little yellow feed icon somewhere in the top 
bar when they spotted this sort of thing in a page.  At least in the 
case of Firefox, you could click on it, and it would throw the target 
URL to a helper of your choice.
</p>
<p>
I wrote a feed reader system at the time (remember fred?), and indeed, I 
could click on that icon and it would flip the feed URL over to my 
&#34;subscribe to new feed&#34; handler.  It was easy.
</p>
<p>
Then, something happened, and browsers gave up on feeds, and the icon 
disappeared.  I kept it there anyway, figuring people would make use of 
it.  It&#39;s still the right way to programmatically find out where to get 
an Atom feed for the content you&#39;re looking at.
</p>
<p>
So what&#39;s with all of the groping around in the dark with made-up URLs?
</p>
<p>
...
</p>
<p>
This one blows my mind.  I put together a page which has the feed URL on 
it as just plain text, not a link.  I&#39;ve seen people paste it into their 
feed reader and include spaces and even newlines.  Seriously!  
</p>
<p>
I know this because I get requests for things like &#34;/w/atom.xml%20&#34; over 
and over from feed readers which obviously don&#39;t notice they get a 404 
every time.
</p>
<p>
...
</p>
<p>
Now we get to the part where I pitch a way forward, and nobody takes me 
up on the offer.  The idea is basically this: I get some kind of 
commitment and support from the people who do feed reader stuff, and in 
turn, I build a new kind of web site which amounts to a &#34;feed reader 
correctness score&#34;.
</p>
<p>
It would probably work like this: you load up a page and it hands you a 
special (fake) feed URL that is keyed to you and you alone.  You plug it 
into your feed reader program through whatever flow and it will keep 
track of every single request to that keyed URL.
</p>
<p>
Then, after it had collected data for a while, a report would eventually 
become available.  Just off the top of my head, the kinds of things it 
might say could look like this:
</p>
<p>
* Poll history: 46 checks in the past 48 hours (average 62 minutes)
</p>
<p>
* Request types: (1) unconditional (45) conditional
</p>
<p>
* If-Modified-Since timestamps: (45) matches (0) made up from whole cloth
</p>
<p>
* ETag hashes: (45) matches (0) made up from whole cloth
</p>
<p>
* Useless cookies sent: none!
</p>
<p>
* Useless referrers sent: none!
</p>
<p>
* Useless CGI arguments sent: none!
</p>
<p>
* User-agents: (40) FooGronk/1.0 +http://fg.example.org/ (6) 
FooGronk/1.01 +http://fg.example.org/
</p>
<p>
That&#39;s the kind of stuff I&#39;d expect to see from a nigh-perfect reader.  
It connects at a reasonable pace, it sends headers with correct values, 
and it doesn&#39;t send along stuff like cookies that I never set in the 
first place.
</p>
<p>
But, okay, this is nothing but vaporware unless someone actually wants 
it, is willing to support it, and will commit to take actions based what 
it says.
</p>
<p>
There&#39;s a bigger lesson here: don&#39;t measure stuff if nobody&#39;s going to 
take actions based on the results.  It only ever ends in misery.  I 
wanted to write a separate post about this very topic, but figured I&#39;d 
give a preview of it right here.
</p>
<p>
Okay world, surprise me.  Do the right thing.
</p>

  </body>
</html>
