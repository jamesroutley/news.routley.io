<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rjp.io/blog/2025-06-17-unreasonable-effectiveness-of-fuzzing">Original</a>
    <h1>The unreasonable effectiveness of fuzzing for porting programs</h1>
    
    <div id="readability-page-1" class="page"><article>
        

<section>
<p><em>A simple strategy of having LLMs write fuzz tests and build up a port in topological order seems effective at automating porting from C to Rust.</em></p>
<h2>Agents are starting to produce more and more code</h2>
<p>A week or 2 back, I was reflecting on some code Claude had generated for me and
I had a sort of moment of clarity. &#34;Clarity&#34; might be overstating it; more
like the type of thought you have in the shower or after a few beers. Anyway.</p>
<p>The thought was: LLMs produce more and more code, and they&#39;ll eventually be
producing more code than people. We can imagine at some point we&#39;re going to
fork from mostly people writing code to mostly computers. What does this mean
for how we&#39;re going to treat our libraries and code maintenance in the future?</p>
<p>LLMs make it easier to deal with API issues or inconsistencies, they&#39;re dealing
with it, not us. Will this continue, leaving us with a <a href="https://en.wikipedia.org/wiki/The_Princess_and_the_Pea">The
Princess and the Pea</a> situation where we deal with piles of leaking
abstractions? Or will we use LLMs to radically change our APIs when needed - will
things get cleaner and better and faster as we go?</p>
<p>LLMs open up the door to performing radical updates that we&#39;d
never really consider in the past. We can port our libraries from one language
to another. We can change our APIs to fix issues, and give downstream users an
LLM prompt to migrate over to the new version automatically, instead of
rewriting their code themselves. We can make massive internal refactorings.
These are types of tasks that in the past, <em>rightly</em>, a senior engineer would
reject in a project until its the last possibly option. Breaking customers
almost never pays off, and its hard to justify refactoring on a &#34;maintenance
mode&#34; project.</p>
<p>But if its more about finding the right prompt and letting an
LLM do the work, maybe that changes our decision process.</p>
<h2>Maintaining big important libraries is no fun</h2>
<p>I used to work on
<a href="https://www.tensorflow.org">TensorFlow</a>. (TensorFlow is a system to develop
gradient based models, like PyTorch. It&#39;s not used so much anymore outside of
Google.) Now TensorFlow had some design flaws in the core language, and a
botched version 1 -&gt; version 2 migration didn&#39;t help matters too much. But as a
maintainer, the biggest issue was the enormous technical debt. It suffered from
a sort of &#34;career advancement syndrome&#34; <label for="sn-sad-481a60bc"></label><span>I sadly missed this period and only was around for the aftermath where we had to deal with the cruft. </span>: TensorFlow was popular and you got credit for
contributing to it, so there was a huge incentive to add some feature as quickly
as you could and then get away.</p>
<p>As a result of a few years of this style of development, a huge surface of
Python code had been cobbled together on top of the C++ core. The complexity
only spiraled over time: engineers came into the project and needed to get
something done. Increasingly the easiest thing to do was to add some Python global or
context manager, shove something in it, then grab it later. Do this over and
over and eventually it becomes impossibly to figure out what&#39;s happening. It
also ended up being incredibly slow. It would take 10s of minutes to build some
graphs (TensorFlow&#39;s equivalent of a program).</p>
<p>As a result of this over time TensorFlow became harder and harder to maintain,
much less evolve <label for="sn-drag-b9936c28"></label><span>It was also hard to keep good engineers working on the project. If
you&#39;re a good engineer, other teams want you and you could choose to work on
less crufty, more exciting projects like <a href="https://docs.jax.dev">Jax</a> or
<a href="https://thelabyrinth.substack.com/p/openxla.org">XLA</a>, or you know, Ads. </span>. One idea we discussed internally to try to fix this was
porting the Python code, bit by bit, into a more comprehensible C++ layer, and
then re-exporting it via PyBind. At first you&#39;d still call everything through
the Python interface, but as you ported code over, you could start to use the
internal C++ interfaces more and more, getting faster and more stable as you
went. This would incrementally improve the performance and consistency of the
system, or that was the hope.</p>
<p>But it would have been an enormous effort to do this all. Each time you tried to
port a module you&#39;d reveal subtle errors or assumptions that you&#39;d need to debug
and revisit, update users etc. Given our staffing, we couldn&#39;t really justify
the time versus the real work of making sure Ads didn&#39;t break.</p>
<p>Even though TensorFlow was moving to maintenance mode, if we could have made
this type of change it would have improved the maintainer and users&#39; experience
for the years we need to keep the lights on. But because the cleanup was thorny
and slow, we just couldn&#39;t justify the cost.</p>
<p>This is by no means limited to TensorFlow. If you&#39;re working on something like
<a href="https://github.com/libexpat/libexpat">libexpat</a> its hard to justify
refactoring your code or rewriting it in Rust, no matter how potentially useful
it would be, when you&#39;ve got this at the top of your page:</p>
<p><img alt="libexpat staffing warning" src="https://thelabyrinth.substack.com/p/libexpat-warning.png"/></p>
<p>But what if we could make this type of refactoring much, much cheaper?</p>
<h2>Infinite patience versus really hard problems</h2>
<p>My experience with coding agents like <a href="https://github.com/anthropics/claude-code">Claude
Code</a> and <a href="https://aider.chat">Aider</a>
makes me think this could be different today. I&#39;ve found that agents are really
good when you set them up against a concrete obstacle: &#34;I changed this API, now
fix everything until the tests pass&#34;. They can show real ingenuity in finding
solutions to subtle problems. I&#39;ve had the LLM diagnose subtle bugs in a
program, write test cases, write minimizers, and probe for solutions, where my
contribution to this conversation largely consisted of &#34;keep going until the
test passes&#34;. Not always, of course. Good luck getting an agent to produce
anything other than React &amp; Tailwind on your new project, for instance.</p>
<p>My hunch is that the stronger the test case, the more concrete the objective the
LLM has to fulfill, the less likely it is to try to sabotage the problem. e.g. &#34;I made
an empty test and it passed&#34;, or to misinterpret the request &#34;you asked for a
Ruby on Rails project, so here&#39;s NextJS, like you asked for&#34;.</p>
<p>You&#39;re putting the agent between a rock and a hard place, as it were, and this
reduces the easy solutions it could use, and thus it works harder to find a real
solution.</p>
<p>In general its really hard to define test cases, a priori, that are specific
enough to constrain the LLM to do exactly what you want. But what about when
we&#39;re changing an existing project? What if we could constrain the output before
and after the changes to be the same? This could be as simple as running all of
our tests for our project, if we&#39;re confident they&#39;re very thorough. For most
complex changes, it might be hard to keep both versions aligned.</p>
<p>What happens if we take a very specific and important type of refactoring:
moving from an unsafe language like C to a safe language like Rust? &#34;I did something with memory I shouldn&#39;t have&#34; are
still in the <a href="https://www.cvedetails.com/vulnerabilities-by-types.php">top 3 for CVEs</a><label for="sn-cve-f1b8d569"></label><span>No longer #1, not because the number of memory CVEs is going down, but because XSS vulnerabilities have grown so quickly ðŸ¤¦. </span>. This is exactly the
type of thing we might <em>like</em> to do, but hard to justify.  Can we make porting more of a prompt engineering problem instead of a coding
one? Let&#39;s find out...</p>
<h2>Going from C to Rust</h2>
<p>Porting C to Rust with LLMs isn&#39;t a brand new idea:
<a href="https://github.com/immunant/c2rust">c2rust</a> can produce a mechanical
translation of C code to Rust, though the result is intentionally &#34;C in Rust
syntax&#34;, with the idea that a person uses it as a starting point to then Rustify
the project.</p>
<p>More recently researchers have started throwing LLMs at the problem.
<a href="https://arxiv.org/pdf/2412.14234">Syzygy</a> is the only paper I found which produced a complete working
implementation of a library, the other papers were more like &#34;we threw this at
the wall and it compiled&#34;. Syzygy leverages a bunch of machinery to mechanize
parts of the porting process:</p>
<p><img alt="syzygy" src="https://thelabyrinth.substack.com/p/syzygy.png"/></p>
<p>What&#39;s cool is their approach <em>works</em>: they get a compression program out of it, and its <a href="https://github.com/syzygy-project/Syzygy_Zopfli/blob/main/rust_code/src/main.rs">all safe Rust</a>. That&#39;s a non-trivial achievement! That said, it runs 3x slower than the C version, and its not
<em>identical</em>: how much this detail matters depends on the users of your API and how much
you care about <a href="https://en.wikipedia.org/wiki/Hyrum%27s_Law">Hyrum&#39;s Law</a>.</p>
<p>Syzygy puts a lot of work into developing test cases for symbols as they port,
but because the tests are against inferred properties of the C program, instead
of directly comparing against the C behavior, you can end up with subtle changes
in behavior. This is hard to avoid with their approach: if the interfaces
differ, it becomes hard to perform a direct comparison between your C and Rust
programs.</p>
<p>I wanted to test if we could do something radically simpler and more direct: <em>what if we just
randomly compared the C and Rust output as we ported</em>? Would that be sufficient
to port an entire library? Or would our tests be ineffectual and we&#39;d get stuck
halfway through. In effect we&#39;d be performing a specific type of fuzz or
<a href="https://en.wikipedia.org/wiki/Property_testing">property testing</a> to our
program as we ported it.</p>
<h2>Property testing</h2>
<p>Property testing has an interesting supposition: we can avoid the drudge work of
writing tests by having the computer build out the test cases for us, and just
check if a property holds for our system.  Trivia item: it is a fundamental law
of all property testing frameworks that they start with one of 2 examples:</p>
<ul>
<li>reverse is its own inverse, so <code>x = reverse(reverse(x))</code>.</li>
<li>the elements of a sorted list are in order</li>
</ul>
<p>Don&#39;t believe me? Check it out:</p>
<ul>
<li><a href="https://github.com/BurntSushi/quickcheck">https://github.com/BurntSushi/quickcheck</a></li>
<li><a href="https://github.com/HypothesisWorks/hypothesis">https://github.com/HypothesisWorks/hypothesis</a></li>
<li><a href="https://hackage-content.haskell.org/package/QuickCheck-2.16.0.0/docs/Test-QuickCheck.html">Haskell Quickcheck</a></li>
<li><a href="https://github.com/emil-e/rapidcheck">https://github.com/emil-e/rapidcheck</a></li>
</ul>
<p>These examples are common for a reason. They&#39;re the perfect fit for property
testing: you just feed random lists in and check your condition holds. A common
critique of property testing is that it doesn&#39;t really extend beyond these
examples. Either its too hard to generate meaningful inputs, or
properties to test, and so we&#39;re better off writing individual test
cases. My experience is that there&#39;s some narrow cases where property testing is
great, but I still end up writing plenty of individual tests.</p>
<p>Sometimes determining the property that holds is hard: I can
test individual cases and validate them, but I don&#39;t know how to generalize.  Or
generating interesting inputs might be hard. If I want to test a C parser,
sampling random bit strings isn&#39;t going to help me very much.  Entire projects
like <a href="https://github.com/csmith-project/csmith">Csmith</a> are dedicated to fuzzing C compilers.</p>
<p>But in our case we&#39;ve solved at least the property test is solved for us: we
have the <em>perfect</em> output property to test: for a given input X, does our Rust
library produce the exact same output as our C library? If we can test this over
an interesting part of our input space, then we can be confident we&#39;ve preserved
our behavior.</p>
<p>Let&#39;s see how this works out.</p>
<h2>Porting C to Rust, attempt 0</h2>
<p>The logs of my first attempt are fortunately lost to the howling void. I spent
an hour or 2 with Claude Code, following this policy:</p>
<ul>
<li>Port symbol X</li>
<li>Write a test for symbol X to make sure it works</li>
</ul>
<p>I wasn&#39;t comparing directly against the C version, instead relying on the unit tests
to validate the behavior was preserved. This attempt did not go well. As modules
were ported over, and we started to use symbols from previous ports, we exposed
more and more latent bugs. Eventually, you&#39;re in a state where the top-level
<code>ZopfliCompress</code> doesn&#39;t work, and you&#39;re stuck debugging the whole program to
figure out what happened.</p>
<p>I realized this wasn&#39;t going to scale. Even if I got something working, I was
looking at a lot of work and I couldn&#39;t explain how to replicate what I did with
another project.</p>
<h2>Porting C to Rust, attempt 1</h2>
<p>My <del>first</del>second attempt was <del>truly</del> slightly less crude:</p>
<ul>
<li>For each C module, ask Claude Code to write a Rust version.</li>
<li>Then write a fuzz test for that module</li>
</ul>
<p>The results of this experiment are on
<a href="https://github.com/rjpower/zopfli/tree/master/zopfli-rs/">Github</a>.</p>
<p>This time, it worked: other than writing the <a href="https://github.com/rjpower/zopfli/blob/master/port/RUST_PORTING.md">porting guidelines</a> and cajoling the model, I didn&#39;t do much work.  I didn&#39;t do any debugging by
hand or have to intercede to understand what was happening. In the case a test found a discrepancy, I would have the model write a <a href="https://github.com/rjpower/zopfli/blob/master/port/bugs/20241219_deflate_tree_encoding_discrepancy.md">bug report</a>
and then iterate until it solved the problem.</p>
<p>But again this wasn&#39;t automated and it was hardly reproducible. I had to babysit
the process quite a bit<label for="sn-babysit-b94bff4f"></label><span>Mostly telling it to keep going. I probably could have written <code>while true; echo &#39;keep going until the test passes&#39; | claude</code> and gotten most of the way there. </span>, and while the library seemed to work, the
results were <em>subtly different</em> than the original Zopfli C implementation. And
because of the ad-hoc approach, I didn&#39;t have a good idea of what had changed.</p>
<p>Still this was promising. So I was incentivized to try again, with a bit more rigor and automation.</p>
<h2>Porting C to Rust, &#34;for real&#34;</h2>
<p>Our final process would be more rigorous. We want to port a small portion of our
program at a time, and automate as much of the porting as possibly. Instead of
calling into an agent and babysitting it, we&#39;d need to hand-roll our own
machinery to call into an LLM API and perform the necessary edits in a fully
automated fashion. So our overall process would be:</p>
<ul>
<li>
<p>Sort symbols in topological order based on the static call graph. If function F calls function G, then first port function G, then function F, etc. (Cycles need to be ported together, but we didn&#39;t have any for this library). This ensures that when we are porting a new symbol into Rust, we can call into our child symbols immediately; we don&#39;t have to implement anything new or call into C. &#34;Symbol&#34; here means a struct, enum or function.</p>
</li>
<li>
<p>For a given public C declaration (in a header file), create an FFI entry for it and a Rust implementation with the exact same signature. We&#39;ll do this by giving the LLM the C header &amp; source and asking for a translation.</p>
</li>
</ul>
<pre><code>// ffi.rs
pub fn AddDynamicTree(
    ll_lengths: *const ::std::os::raw::c_uint,
    d_lengths: *const ::std::os::raw::c_uint,
    bp: *mut ::std::os::raw::c_uchar,
    out: *mut *mut ::std::os::raw::c_uchar,
    outsize: *mut usize,
);

// deflate.rs
pub unsafe fn AddDynamicTree(
    ll_lengths: *const c_uint,
    d_lengths: *const c_uint,
    bp: *mut c_uchar,
    out: *mut *mut c_uchar,
    outsize: *mut usize,
) {
    ...
}
</code></pre>
<ul>
<li>Have the LLM write a fuzz test which samples over possibly inputs and compares the C and Rust implementations, asserting if there is a discrepancy <a href="https://github.com/rjpower/portkit/blob/1d5a2c21adbfc71cb0906fd16a0cf7252be5dcd5/zopfli-port/rust/fuzz/fuzz_targets/fuzz_AddDynamicTree.rs">example</a>. No attempt was made to guide the fuzz test inputs other than instructing the LLM to avoid generating invalid structures (e.g. null pointers or numbers out of an expected range).</li>
</ul>
<pre><code>#[derive(Debug, arbitrary::Arbitrary)]
struct FuzzInput {
    ll_lengths: Vec&lt;u32&gt;,
    d_lengths: Vec&lt;u32&gt;,
}

fuzz_target!(|input: FuzzInput| {
    ...
    // setup inputs
    let c_result = ffi::AddDynamicTree(...)
    let rust_result = rust::AddDynamicTree(...)

    assert_eq!(c_result, rust_result, &#34;C and Rust diverged.&#34;);
</code></pre>
<ul>
<li>Run the fuzz test and fix until everything compiles and the fuzz test passes.</li>
</ul>
<p>We repeat this for each symbol in the program until we hit the top-level main().</p>
<p>Prior to porting I modified the C source slightly to make a few static functions
extern so that we could create an FFI to them. This would allow the LLM to port
smaller chunks at a time (otherwise there were some modules where the LLM would
have to port 1000 lines of code at once because only the main symbol was
visible). I also copied some <code>#define</code> constants by hand because my C traversal
hadn&#39;t detected them.</p>
<p>Originally I tried to break up the task of porting a symbol. We&#39;d separately
define the FFI to C, a stub implementation of the Rust function, define a fuzz
test, then build the implementation. The idea was that this would let us verify
each of these via a separate call and we&#39;d be more robust. Ultimately this
proved more confusing to the agents than helpful.</p>
<p>In the end, I simply prompted the LLM to do all the steps for porting a symbol
at once, reprompting it if the fuzz test didn&#39;t exist or pass.</p>
<p>And in the end... it worked? The result is a <a href="https://github.com/rjpower/portkit/tree/main/zopfli-port">Rust implementation of
Zopfli</a> that gives
<em>identical</em> results on every input I&#39;ve tried to the C version. This is
different from the Syzygy results, where they ended up with a program that
compresses correctly, but not identically <label for="sn-better-65fea515"></label><span>This isn&#39;t a claim my approach is <em>better</em> than Syzygy, just different. </span> to the C version.  Because
we locked the Rust and C versions to use the same API at each step, the
resulting program isn&#39;t very &#34;rusty&#34;, but its a complete translation.</p>
<p>About 90% of symbols were auto-ported using gemini-2.5-pro-06-05 and a crappy
&#34;agent&#34; library I wrote up for this task. The remaining 10% I switched over to
running in a Claude Code, as Gemini seemed to struggle with patch formats and
imports. The only work I did manually was to adjust a few places where the LLM
was calling the FFI code from Rust and clean up some warning messages.</p>
<h2>But why does this work at all?</h2>
<p>To clarify, the surprising result is not that fuzzing would detect discrepancies
at the top-level of our library. Target specific fuzzers work great for this task:
<a href="https://github.com/csmith-project/csmith">CSmith</a> and
<a href="https://jepsen.io/">Jepsen</a> find all sorts of weird interesting bugs.
What&#39;s surprising is that with only one exception<label for="sn-claude-4e0b395b"></label><span>Gemini introduces a weird typo into a program, which was detected a few symbols downstream by Claude and repaired. You can see <a href="https://thelabyrinth.substack.com/p/claude-rust-port-conversation.md">the session log here</a>. </span>, the LLM translation + naive
fuzz test correctly validated the symbol behavior <em>for every symbol</em>. This meant
that we didn&#39;t run into any issues where after porting A, B ... Q, suddenly we
detect a subtle bug in R.</p>
<p>This meant we didn&#39;t have to &#34;backtrack&#34;, or inspect the rest of the code base
as we ported symbols: each symbol ported in isolation, we tested it (implicitly
testing the dependent symbols as well), and we moved on. If this didn&#39;t work,
we&#39;d have to inspect the whole program and debug it, over and over, as we built
it up.</p>
<p>If we think about it, this shouldn&#39;t have worked as well as it did. Fuzzing
shouldn&#39;t hold for all functions. Imagine I&#39;m converting something like a manual
floating-point multiplier:</p>
<pre><code>mul(a_bits: &amp;[byte], b_bits: &amp;[byte], c_bits: mut &amp;[byte]):
</code></pre>
<p>If I fuzz this by providing random bytes, am I likely to detect a subtle issue
with underflow, or detect the <a href="https://en.wikipedia.org/wiki/Pentium_F00F_bug">Pentium FOOF bug</a>?  Or imagine a C parser:
fuzzing random bytes wouldn&#39;t trigger much of the parser. Without a lot of
probes, it seems hard to test these function spaces!</p>
<p>My hunch is that this works due to a combination of factors:</p>
<ul>
<li>I chose a simple library to work with. Zopfli isn&#39;t trivial, but you aren&#39;t juggling a lot of state, for example.</li>
<li>Fuzzers are good at probing the input space. If you <a href="https://llvm.org/docs/LibFuzzer.html#tracing-cmp-instructions">compile with the right arguments</a> the fuzzer will try to choose inputs which trigger different branches. You can even give good examples (a corpus) to start the fuzzer off. I didn&#39;t do this for my experiments, but its easy to imagine an LLM generating a decent starting corpus.</li>
<li>Most code doesn&#39;t express subtle logic paths. If I test if a million inputs are correctly sorted, I&#39;ve probably implemented the sorter correctly.</li>
<li>Complex logic, when it exists, is broken up across functions. Our individual tests make it easier to ensure the combined calls work.</li>
<li>The LLM produces correct code most of the time! The fuzz test is just there to validate we did the right thing.</li>
</ul>
<h2>Caveats, or don&#39;t try this at home</h2>
<p>While the overall system seems to work, its far from ready to drop in to a new project.</p>
<p><em>The resulting Rust code is very &#34;C-like&#34;</em></p>
<p>By construction, we use the same unsafe C interface for each symbol we port.
This is different from Syzygy, which generates a safe Rust version of the
program in one shot.  This was convenient but not strictly required: I could
have tweaked the prompts and clearly indicate public/private interfaces, letting
the LLM use more idiomatic Rust for internal functions. Retaining the same
interfaces simplified writing the fuzz tests and let the LLM call into the
original C code when debugging.</p>
<p>That said, because our end result has end-to-end fuzz tests and tests for every
symbol, its now much easier to &#34;rustify&#34; the code with confidence. This is a
great task for future work.</p>
<p><em>The automation isn&#39;t complete</em></p>
<p>I tweaked the agent framework and prompts as I went to get better results out of Gemini. I needed to have Claude come in at the end to finish the last few symbols, and I fixed a few typos here and there. With better prompting and a better agent framework, this intervention would be reduced, but likely not go away entirely.</p>
<p><em>Zopfli is easy</em></p>
<p>Zopfli is a simple library with many functions which are input/output oriented.
It&#39;s not clear how this would work if you introduced more stateful interfaces.</p>
<h2>Conclusions and future work</h2>
<p>Whew. This ended up taking a few days and being more than the initial trivial
experiment I intended. That said, I think there&#39;s some interesting insights:</p>
<ul>
<li><em>Porting via LLMs is surprisingly cost effective and only getting cheaper</em>. Even with my multiple rounds of experimentation and tweaking agents etc, the total cost for this experiment was ~$50, or about $0.01 a line. (For comparison, Syzygy cost ~$1500 using the O3 model). I suspect this could be brought down another 10x by walking up a &#34;complexity tree&#34;: first try porting symbols mechanically (constants, structs, enums), then trying out a cheap model before falling back to more expensive models. For instance, Gemini Flash 2.5 is capable of producing correct Rust code for some non-trivial problems.</li>
<li><em>Full automation is hard</em>. The chat based interface of LLMs/agents can lead us to believe that they are more capable than they really are. We don&#39;t often realize how much we&#39;re guiding the direction of the LLMs until we try to go completely hands off.</li>
<li><em>Full automation isn&#39;t necessary</em>. Imagine a system where you have agents port as many symbols as they possibly can, in parallel. If a symbol fails to port, you mark it &#34;tainted&#34; and move on. You then continue until you can&#39;t port anything else. In such a system, a human could be brought in to fix issues as they emerge, but you could still rely on the agents to handle &gt;90% of the work of porting in an asynchronous manner.</li>
</ul>
<p>If you&#39;re interested in the (truly terrible) code I used for the porting, you
can find it on <a href="https://github.com/rjpower/portkit/">Github</a>. I&#39;m not sure if I&#39;ll
continue down this route further myself, but let me know if you&#39;re interested in
this space or trying to port a project and I&#39;m happy to chat more!</p>


</section>

    </article></div>
  </body>
</html>
