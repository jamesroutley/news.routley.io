<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://developers.googleblog.com/en/introducing-gemma-3n/">Original</a>
    <h1>Gemma 3n preview: Mobile-first AI</h1>
    
    <div id="readability-page-1" class="page"><div>

    
      <section>
        
      </section>
    

    <section>
      
    </section>

    <section>
      
    </section>

    <section>

      <section>
      
        
          
        
          
        

      
      </section>
      
    </section>

    
    <section>
      <div>
          

<div>
    <p data-block-key="ghtsi">Following the exciting launches of <a href="https://blog.google/technology/developers/gemma-3/">Gemma 3</a> and <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">Gemma 3 QAT</a>, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we&#39;re pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we&#39;re now extending that vision to highly capable, real-time AI operating directly on the devices you use every day – your phones, tablets, and laptops.</p><p data-block-key="bietk">To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung System LSI, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.</p><p data-block-key="74nf9"><a href="https://deepmind.google/models/gemma/gemma-3n/">Gemma 3n</a> is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of <a href="https://deepmind.google/technologies/gemini/nano/">Gemini Nano</a>, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.</p>
</div>   


    
    <div>
        <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_OjwrVp1.original.png" alt="Chatbot Arena Elo scores"/></p><p>
                        This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Gemma 3n ranks highly amongst both popular proprietary and open models.
                    </p>
                
            
        </div>
    </div>
  <div>
    <p data-block-key="ghtsi">Gemma 3n leverages a Google DeepMind innovation called Per-Layer Embeddings (PLE) that delivers a significant reduction in RAM usage. While the raw parameter count is 5B and 8B, this innovation allows you to run larger models on mobile devices or live-stream from the cloud, with a memory overhead comparable to a 2B and 4B model, meaning the models can operate with a dynamic memory footprint of just 2GB and 3GB. Learn more in our <a href="https://ai.google.dev/gemma/docs/gemma-3n#parameters">documentation</a>.</p><p data-block-key="q3ib">By exploring Gemma 3n, developers can get an early preview of the open model’s core capabilities and mobile-first architectural innovations that will be available on Android and Chrome with Gemini Nano.</p><p data-block-key="64a2c">In this post, we&#39;ll explore Gemma 3n&#39;s new capabilities, our approach to responsible development, and how you can access the preview today.</p><h3 data-block-key="chmin" id="key-capabilities-of-gemma-3n"><b></b></h3><p data-block-key="54i3c">Engineered for fast, low-footprint AI experiences running locally, Gemma 3n delivers:</p><ul><li data-block-key="binsd"><b>Optimized On-Device Performance &amp; Efficiency:</b> Gemma 3n starts responding approximately 1.5x faster on mobile with significantly better quality (compared to Gemma 3 4B) and a reduced memory footprint achieved through innovations like Per Layer Embeddings, KVC sharing, and advanced activation quantization.</li></ul><ul><li data-block-key="cgst0"><b>Many-in-1 Flexibility:</b> A model with a 4B active memory footprint that natively includes a nested state-of-the-art 2B active memory footprint submodel (thanks to <a href="https://arxiv.org/abs/2310.07707">MatFormer</a> training). This provides flexibility to dynamically trade off performance and quality on the fly without hosting separate models. We further introduce mix’n’match capability in Gemma 3n to dynamically create submodels from the 4B model that can optimally fit your specific use case -- and associated quality/latency tradeoff. Stay tuned for more on this research in our upcoming technical report.</li></ul><ul><li data-block-key="1cvli"><b>Privacy-First &amp; Offline Ready:</b> Local execution enables features that respect user privacy and function reliably, even without an internet connection.</li></ul><ul><li data-block-key="dofaq"><b>Expanded Multimodal Understanding with Audio:</b> Gemma 3n can understand and process audio, text, and images, and offers significantly enhanced video understanding. Its audio capabilities enable the model to perform high-quality Automatic Speech Recognition (transcription) and Translation (speech to translated text). Additionally, the model accepts interleaved inputs across modalities, enabling understanding of complex multimodal interactions. (Public implementation coming soon)</li></ul><ul><li data-block-key="1mkna"><b>Improved Multilingual Capabilities:</b> Improved multilingual performance, particularly in Japanese, German, Korean, Spanish, and French. Strong performance reflected on multilingual benchmarks such as 50.1% on WMT24++ (ChrF).</li></ul>
</div>   


    
    <div>
        <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Artboard_1.original.png" alt="MMLU performance"/></p><p>
                        This chart show’s MMLU performance vs model size of Gemma 3n’s mix-n-match (pretrained) capability.
                    </p>
                
            
        </div>
    </div>
  <div>
    <h3 data-block-key="2rib8" id="unlocking-new-on-the-go-experiences"><b>Unlocking New On-the-go Experiences</b></h3><p data-block-key="1hssr">Gemma 3n will empower a new wave of intelligent, on-the-go applications by enabling developers to:</p><ol><li data-block-key="9jafp"><b>Build live, interactive experiences</b> that understand and respond to real-time visual and auditory cues from the user&#39;s environment.</li></ol><p data-block-key="c7kgq"><b><br/></b>2. <b>Power deeper understanding</b> and contextual text generation using combined audio, image, video, and text inputs—all processed privately on-device.</p><p data-block-key="t3pn"><b><br/></b>3. <b>Develop advanced audio-centric applications</b>, including real-time speech transcription, translation, and rich voice-driven interactions.</p>
</div>    <div>
    <h3 data-block-key="q2vk1" id="building-responsibly-together"><b>Building Responsibly, Together</b></h3><p data-block-key="4evp8">Our commitment to responsible AI development is paramount. Gemma 3n, like all Gemma models, underwent rigorous safety evaluations, data governance, and fine-tuning alignment with our safety policies. We approach open models with careful risk assessment, continually refining our practices as the AI landscape evolves.</p><h3 data-block-key="eyvht" id="get-started:-preview-gemma-3n-today"><b></b></h3><p data-block-key="a8cmr">We&#39;re excited to get Gemma 3n into your hands through a preview starting today:</p><ul><li data-block-key="oaqh"><b>Cloud-based Exploration with Google AI Studio:</b> Try Gemma 3n directly in your browser on <a href="https://aistudio.google.com/app/prompts/new_chat?model=gemma-3n-e4b-it">Google AI Studio</a> – no setup needed. Explore its text input capabilities instantly.</li></ul><ul><li data-block-key="e7c6o"><b>On-Device Development with Google AI Edge:</b> For developers looking to integrate Gemma 3n locally, <a href="https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling">Google AI Edge</a> provides tools and libraries. You can get started with text and image understanding/generation capabilities today.</li></ul><p data-block-key="46ck9">Explore this announcement and all Google I/O 2025 updates on <a href="https://io.google/2025/?utm_source=blogpost&amp;utm_medium=pr&amp;utm_campaign=event&amp;utm_content=">io.google</a> starting May 22.</p>
</div> 
      </div>
    </section>
    

    <section>
      
      
    </section>

    
    
    
  </div></div>
  </body>
</html>
