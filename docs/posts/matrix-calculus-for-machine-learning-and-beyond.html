<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2501.14787">Original</a>
    <h1>Matrix Calculus (For Machine Learning and Beyond)</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://www.bryanbraun.com/pdf/2501.14787">View PDF</a>
    <a href="https://arxiv.org/html/2501.14787v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>  This course, intended for undergraduates familiar with elementary calculus and linear algebra, introduces the extension of differential calculus to functions on more general vector spaces, such as functions that take as input a matrix and return a matrix inverse or factorization, derivatives of ODE solutions, and even stochastic derivatives of random functions. It emphasizes practical computational applications, such as large-scale optimization and machine learning, where derivatives must be re-imagined in order to be propagated through complicated calculations. The class also discusses efficiency concerns leading to &#34;adjoint&#34; or &#34;reverse-mode&#34; differentiation (a.k.a. &#34;backpropagation&#34;), and gives a gentle introduction to modern automatic differentiation (AD) techniques.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Steven G. Johnson [<a href="https://www.bryanbraun.com/show-email/182cbc52/2501.14787" rel="nofollow">view email</a>]      </p></div></div>
  </body>
</html>
