<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://aider.chat/docs/benchmarks-1106.html">Original</a>
    <h1>GPT-4-turbo preliminary benchmark results on code-editing</h1>
    
    <div id="readability-page-1" class="page"><div id="content" role="main">
      

<p><a href="https://aider.chat/assets/benchmarks-1106.svg"><img src="https://aider.chat/assets/benchmarks-1106.svg" alt="benchmark results"/></a></p>

<p><a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">OpenAI just released new versions of GPT-3.5 and GPT-4</a>,
and there’s a lot
of interest about their ability to code compared to the previous versions.
With that in mind, I’ve been benchmarking the new models.</p>

<p>Aider is an open source command line chat tool that lets you work with GPT to edit
code in your local git repo.
To do this, aider needs to be able to reliably recognize when GPT wants to edit
your source code,
determine which files it wants to modify
and accurately apply the changes it’s trying to make.
Doing a good job on this “code editing” task requires a good LLM, good prompting and
a good tool driving the interactions with the LLM.</p>

<p>Aider relies on a
<a href="https://aider.chat/docs/benchmarks.html">code editing benchmark</a>
to quantitatively evaluate
performance
whenever one of these things changes.
For example,
whenever I change aider’s prompting or the backend which drives LLM conversations,
I run the benchmark to make sure these changes produce improvements (not regressions).</p>

<p>The benchmark uses aider to try and complete
<a href="https://github.com/exercism/python">133 Exercism Python coding exercises</a>.
For each exercise, Exercism provides a starting python file with stubs for the needed functions,
a natural language description of the problem to solve
and a test suite to evaluate whether the coder has correctly solved the problem.</p>

<p>The benchmark gives aider two tries to complete the task:</p>

<ol>
  <li>On the first try, aider gives GPT the stub code file to edit and the natural language instructions that describe the problem. This reflects how you code with aider. You add your source code files to the chat and ask for changes, which are automatically applied.</li>
  <li>If the test suite fails after the first try, aider gives GPT the test error output and asks it to fix the code. Aider supports this sort of interaction using a command like <code>/run pytest</code> to run and share pytest results in the chat with GPT. You can <code>/run</code> whatever tests/linters/etc make sense for your language/framework/situation.</li>
</ol>

<h2 id="benchmark-results">Benchmark results</h2>

<h3 id="gpt-4-1106-preview">gpt-4-1106-preview</h3>

<p>For now, I have only benchmarked the GPT-4 models using the <code>diff</code> edit method.
This is the edit format that aider uses by default with gpt-4.</p>

<ul>
  <li>The new <code>gpt-4-1106-preview</code> model seems <strong>much faster</strong> than the earlier GPT-4 models. I won’t be able to properly quantify this until the rate limits loosen up.</li>
  <li><strong>It seems better at producing correct code on the first try</strong>. It gets
~57% of the coding exercises correct, without needing to see errors from the test suite. Previous models only get 46-47% of the exercises correct on the first try.</li>
  <li>The new model seems to perform similar
(~66%) to the old models (63-64%) after being given a second chance to correct bugs by reviewing test suite error output.</li>
</ul>

<p><strong>These are preliminary results.</strong>
OpenAI is enforcing very low
rate limits on the new GPT-4 model. The limits are so low, that
I have only been able to attempt
113
out of the 133 Exercism problems.
The problems are selected in random order, so results should be <em>roughly</em>
indicative of the full benchmark.</p>

<h3 id="gpt-35-turbo-1106">gpt-3.5-turbo-1106</h3>

<p>I benchmarked the GPT-3.5 models with both the <code>whole</code> and <code>diff</code> edit format.
None of the gpt-3.5 models seem able to effectively use the <code>diff</code> edit format, including the newest November (1106) model.</p>

<p>The comments below only focus on comparing the <code>whole</code> edit format results:</p>

<ul>
  <li>The new <code>gpt-3.5-turbo-1106</code> model is completing the benchmark <strong>3-4X faster</strong> than the earlier GPT-3.5 models.</li>
  <li>The success rate after the first try of 42% is comparable to the previous June (0613) model. The new November and previous June models are both worse than the original March (0301) model’s 50% result on the first try.</li>
  <li>The new model’s 56% success rate after the second try seems comparable to the original March model, and somewhat better than the June model’s 50% score.</li>
</ul>

<h3 id="updates">Updates</h3>

<p>I will update the results on this page as quickly as my rate limit allows.</p>


      
    </div></div>
  </body>
</html>
