<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/apple/ml-depth-pro">Original</a>
    <h1>Depth Pro: Sharp monocular metric depth in less than a second</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</h2><a id="user-content-depth-pro-sharp-monocular-metric-depth-in-less-than-a-second" aria-label="Permalink: Depth Pro: Sharp Monocular Metric Depth in Less Than a Second" href="#depth-pro-sharp-monocular-metric-depth-in-less-than-a-second"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">This software project accompanies the research paper:
<strong><a href="https://arxiv.org/abs/2410.02073" rel="nofollow">Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</a></strong>,
<em>Aleksei Bochkovskii, AmaÃ«l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun</em>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://clarionsong.substack.com/apple/ml-depth-pro/blob/main/data/depth-pro-teaser.jpg"><img src="https://clarionsong.substack.com/apple/ml-depth-pro/raw/main/data/depth-pro-teaser.jpg" alt=""/></a></p>
<p dir="auto">We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image.</p>
<p dir="auto">The model in this repository is a reference implementation, which has been re-trained. Its performance is close to the model reported in the paper but does not match it exactly.</p>

<p dir="auto">We recommend setting up a virtual environment. Using e.g. miniconda, the <code>depth_pro</code> package can be installed via:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n depth-pro -y python=3.9
conda activate depth-pro

pip install -e ."><pre>conda create -n depth-pro -y python=3.9
conda activate depth-pro

pip install -e <span>.</span></pre></div>
<p dir="auto">To download pretrained checkpoints follow the code snippet below:</p>
<div dir="auto" data-snippet-clipboard-copy-content="source get_pretrained_models.sh   # Files will be downloaded to `checkpoints` directory."><pre><span>source</span> get_pretrained_models.sh   <span><span>#</span> Files will be downloaded to `checkpoints` directory.</span></pre></div>

<p dir="auto">We provide a helper script to directly run the model on a single image:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run prediction on a single image:
depth-pro-run -i ./data/example.jpg
# Run `depth-pro-run -h` for available options."><pre><span><span>#</span> Run prediction on a single image:</span>
depth-pro-run -i ./data/example.jpg
<span><span>#</span> Run `depth-pro-run -h` for available options.</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="from PIL import Image
import depth_pro

# Load model and preprocessing transform
model, transform = depth_pro.create_model_and_transforms()
model.eval()

# Load and preprocess an image.
image, _, f_px = depth_pro.load_rgb(image_path)
image = transform(image)

# Run inference.
prediction = model.infer(image, f_px=f_px)
depth = prediction[&#34;depth&#34;]  # Depth in [m].
focallength_px = prediction[&#34;focallength_px&#34;]  # Focal length in pixels."><pre><span>from</span> <span>PIL</span> <span>import</span> <span>Image</span>
<span>import</span> <span>depth_pro</span>

<span># Load model and preprocessing transform</span>
<span>model</span>, <span>transform</span> <span>=</span> <span>depth_pro</span>.<span>create_model_and_transforms</span>()
<span>model</span>.<span>eval</span>()

<span># Load and preprocess an image.</span>
<span>image</span>, <span>_</span>, <span>f_px</span> <span>=</span> <span>depth_pro</span>.<span>load_rgb</span>(<span>image_path</span>)
<span>image</span> <span>=</span> <span>transform</span>(<span>image</span>)

<span># Run inference.</span>
<span>prediction</span> <span>=</span> <span>model</span>.<span>infer</span>(<span>image</span>, <span>f_px</span><span>=</span><span>f_px</span>)
<span>depth</span> <span>=</span> <span>prediction</span>[<span>&#34;depth&#34;</span>]  <span># Depth in [m].</span>
<span>focallength_px</span> <span>=</span> <span>prediction</span>[<span>&#34;focallength_px&#34;</span>]  <span># Focal length in pixels.</span></pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Evaluation (boundary metrics)</h3><a id="user-content-evaluation-boundary-metrics" aria-label="Permalink: Evaluation (boundary metrics)" href="#evaluation-boundary-metrics"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Our boundary metrics can be found under <code>eval/boundary_metrics.py</code> and used as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# for a depth-based dataset
boundary_f1 = SI_boundary_F1(predicted_depth, target_depth)

# for a mask-based dataset (image matting / segmentation) 
boundary_recall = SI_boundary_Recall(predicted_depth, target_mask)"><pre><span># for a depth-based dataset</span>
<span>boundary_f1</span> <span>=</span> <span>SI_boundary_F1</span>(<span>predicted_depth</span>, <span>target_depth</span>)

<span># for a mask-based dataset (image matting / segmentation) </span>
<span>boundary_recall</span> <span>=</span> <span>SI_boundary_Recall</span>(<span>predicted_depth</span>, <span>target_mask</span>)</pre></div>

<p dir="auto">If you find our work useful, please cite the following paper:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{Bochkovskii2024:arxiv,
  author     = {Aleksei Bochkovskii and Ama\&#34;{e}l Delaunoy and Hugo Germain and Marcel Santos and
               Yichao Zhou and Stephan R. Richter and Vladlen Koltun}
  title      = {Depth Pro: Sharp Monocular Metric Depth in Less Than a Second},
  journal    = {arXiv},
  year       = {2024},
  url        = {https://arxiv.org/abs/2410.02073},
}"><pre><span>@article</span>{<span>Bochkovskii2024:arxiv</span>,
  <span>author</span>     = <span><span>{</span>Aleksei Bochkovskii and Ama\&#34;{e}l Delaunoy and Hugo Germain and Marcel Santos and</span>
<span>               Yichao Zhou and Stephan R. Richter and Vladlen Koltun<span>}</span></span>
  title      = <span><span>{</span>Depth Pro: Sharp Monocular Metric Depth in Less Than a Second<span>}</span></span>,
  <span>journal</span>    = <span><span>{</span>arXiv<span>}</span></span>,
  <span>year</span>       = <span><span>{</span>2024<span>}</span></span>,
  <span>url</span>        = <span><span>{</span>https://arxiv.org/abs/2410.02073<span>}</span></span>,
}</pre></div>

<p dir="auto">This sample code is released under the <a href="https://clarionsong.substack.com/apple/ml-depth-pro/blob/main/LICENSE">LICENSE</a> terms.</p>
<p dir="auto">The model weights are released under the <a href="https://clarionsong.substack.com/apple/ml-depth-pro/blob/main/LICENSE">LICENSE</a> terms.</p>

<p dir="auto">Our codebase is built using multiple opensource contributions, please see <a href="https://clarionsong.substack.com/apple/ml-depth-pro/blob/main/ACKNOWLEDGEMENTS.md">Acknowledgements</a> for more details.</p>
<p dir="auto">Please check the paper for a complete list of references and datasets used in this work.</p>
</article></div></div>
  </body>
</html>
