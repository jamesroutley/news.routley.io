<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/blog/TimothyAlexisVass/explaining-the-sdxl-latent-space">Original</a>
    <h1>Explaining the SDXL Latent Space</h1>
    
    <div id="readability-page-1" class="page"><div><div>
		
		<!-- HTML_TAG_START --><div><div>
		<p><span>Published
				November 20, 2023</span></p></div>
	</div>
<p><a rel="nofollow" href="#a-complete-demonstration">TL;DR</a></p>

<p><a rel="nofollow" href="#a-short-background-story">A Short background story</a></p>
<h2>
	<a rel="nofollow" href="#a-short-background-story" id="a-short-background-story">
		
	</a>
	<span>
		A short background story
	</span>
</h2>
<p><em>Special thanks to: <a rel="nofollow" href="https://github.com/madebyollin/">Ollin Boer Bohan</a> <a rel="nofollow" href="https://github.com/Haoming02">Haoming</a>, <a rel="nofollow" href="https://github.com/segalinc">Cristina Segalin</a> and <a rel="nofollow" href="https://github.com/Birch-san">Birchlabs</a> for helping with information, discussion and knowledge!</em></p>
<p>I was creating <a rel="nofollow" href="#udi-sdxl-correction-filters">correction filters for the SDXL inference process</a> to an UI I&#39;m creating for diffusion models.</p>
<p>After having many years of experience with image correction, I wanted the fundamental capability to improve the actual output from SDXL.
There were many techniques which I wanted available in the UX, which I set out to fix myself.
I noticed that SDXL output is almost always either noisy in regular patterns or overly smooth.
The color space always needed white balancing, with a biased and restricted color range, simply because of how SD models work.</p>
<p>Making corrections in a post process after the image is generated and converted to 8-bit RGB made very little sense, if it was possible to improve the information and color range before the actual output.</p>
<p>The most important thing to know in order to create filters and correction tools is to understand the data you are working with.</p>
<p>This led me to an experimental exploration of the SDXL latents with the intention of understanding them.
The tensor, which the diffusion models based on the SDXL architecture work with, looks like this:<br/></p>
<pre><code>[batch_size, 4 channels, height (y), width (x)]
</code></pre>
<p>My first question was simply &#34;<strong>What exactly are these 4 channels?</strong>&#34;.
To which most answers I received were along the lines of &#34;It&#39;s not something that a human can understand.&#34;</p>
<p>But it is most definitely understandable. It&#39;s even very easy to understand and useful to know. </p>
<h2>
	<a rel="nofollow" href="#the-4-channels-of-the-sdxl-latents" id="the-4-channels-of-the-sdxl-latents">
		
	</a>
	<span>
		The 4 channels of the SDXL latents
	</span>
</h2>
<p>For a 1024×1024px image generated by SDXL, the latents tensor is 128×128px, where every pixel in the latent space represents 64 (8×8) pixels in the pixel space. If we generate and decode the latents into a standard 8-bit jpg image, then... </p>
<h3>
	<a rel="nofollow" href="#the-8-bit-pixel-space-has-3-channels" id="the-8-bit-pixel-space-has-3-channels">
		
	</a>
	<span>
		The 8-bit pixel space has 3 channels
	</span>
</h3>
<p>Red (R), Green (G) and Blue (B), each with 256 possible values ranging between 0-255.
So, to store the full information of 64 pixels, we need to be able to store 64×256 = 16,384 values, per channel, in every latent pixel.</p>
<h3>
	<a rel="nofollow" href="#the-sdxl-latent-representation-of-an-image-has-4-channels" id="the-sdxl-latent-representation-of-an-image-has-4-channels">
		
	</a>
	<span>
		<a rel="nofollow" href="https://timothyalexisvass.github.io/sdxl-colorspace">The SDXL latent representation of an image has 4 channels</a>
	</span>
</h3>
<p><em>Click the heading for an interactive demo!</em></p>
<p><strong>0:</strong> Luminance</p>
<p>If each value can range between -4 and 4 at the point of decoding, then in a 16-bit floating point format with half precision, each latent pixel can contain 16,384 distinct values for each of the 4 channels.</p>
<h3>
	<a rel="nofollow" href="#direct-conversion-of-sdxl-latents-to-rgb-with-a-linear-approximation" id="direct-conversion-of-sdxl-latents-to-rgb-with-a-linear-approximation">
		
	</a>
	<span>
		Direct conversion of SDXL latents to RGB with a linear approximation
	</span>
</h3>
<p>With this understanding, we can create an approximation function which directly converts the latents to RGB:</p>
<pre><code><span>def</span> <span>latents_to_rgb</span>(<span>latents</span>):
    weights = (
        (<span>60</span>, -<span>60</span>, <span>25</span>, -<span>70</span>),
        (<span>60</span>,  -<span>5</span>, <span>15</span>, -<span>50</span>),
        (<span>60</span>,  <span>10</span>, -<span>5</span>, -<span>35</span>)
    )

    weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))
    biases_tensor = torch.tensor((<span>150</span>, <span>140</span>, <span>130</span>), dtype=latents.dtype).to(latents.device)
    rgb_tensor = torch.einsum(<span>&#34;...lxy,lr -&gt; ...rxy&#34;</span>, latents, weights_tensor) + biases_tensor.unsqueeze(-<span>1</span>).unsqueeze(-<span>1</span>)
    image_array = rgb_tensor.clamp(<span>0</span>, <span>255</span>)[<span>0</span>].byte().cpu().numpy()
    image_array = image_array.transpose(<span>1</span>, <span>2</span>, <span>0</span>)  

    <span>return</span> Image.fromarray(image_array)
</code></pre>
<p>Here we have the latents_to_rgb result and a regular decoded output, resized for comparison:</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/YuycEEf9Wb6qON7m1Z2sX.png"/> <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/tcJ3JsG8rrQkOjkyxQIPE.png"/>
</p>

<h3>
	<a rel="nofollow" href="#a-probable-reason-why-the-sdxl-color-range-is-biased-towards-yellow" id="a-probable-reason-why-the-sdxl-color-range-is-biased-towards-yellow">
		
	</a>
	<span>
		A probable reason why the SDXL color range is biased towards yellow
	</span>
</h3>
<p>Relatively few things in nature are blue, or white. These colors are most prominent in the sky, during enjoyable conditions.
So, the model, knowing reality through images, thinks in luminance (channel 0) cyan/red (channel 1) and lime/medium purple (channel 2), where Red and Green are primary and blue is secondary. This is why very often, SDXL generations are biased towards yellow (red + green).</p>
<p>During inference, the values in the tensor will begin at <code>min &lt; -30</code> and <code>max &gt; 30</code> and the min/max boundary at time of decoding is around <code>-4</code> to <code>4</code>. At higher <code>guidance_scale</code> the values will have a higher difference between <code>min</code> and <code>max</code>.</p>
<p>One key in understanding the boundary is to look at what happens in the decoding process: </p>
<pre><code>decoded = vae.decode(latents / vae.scaling_factor).sample 
decoded = decoded.div(<span>2</span>).add(<span>0.5</span>).clamp(<span>0</span>, <span>1</span>) 
</code></pre>
<p>If the values at this point are outside of the range 0 to 1, some information will be lost in the clamp.
So if we can make corrections during denoising to serve the VAE what it expects, we may get better results. </p>
<h2>
	<a rel="nofollow" href="#what-needs-correcting" id="what-needs-correcting">
		
	</a>
	<span>
		What needs correcting?
	</span>
</h2>
<p>How do you sharpen a blurry image, white balance, improve detail, increase contrast or increase the color range?
The best way is to begin with a sharp image, which is correctly white balanced with great contrast, crisp details and a high range.</p>
<p>It&#39;s far easier to blur a sharp image, shift the color balance, reduce contrast, get nonsensical details and limit the color range than to improve it.</p>
<p>SDXL has a very prominent tendency to color bias and put values outside of the actual boundaries (left image). Which is easily solved by centering the values and getting them within the boundaries (right image): </p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/PIDEjtCUDjeA-vqTpHSE4.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/3Y0omWaPai6c-zU_rNNpC.jpeg"/>
</p>
<div>
  <p>Original output outside boundaries</p>
  <p>Exaggerated correction for illustrative purposes</p>
</div>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/XoXGAfW3Lh_6X_HyXT-mN.png"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/GArQv4GqdcRaAmEh85faE.png"/>
</p>

<pre><code><span>def</span> <span>center_tensor</span>(<span>input_tensor, per_channel_shift=<span>1</span>, full_tensor_shift=<span>1</span>, channels=[<span>0</span>, <span>1</span>, <span>2</span>, <span>3</span>]</span>):
    <span>for</span> channel <span>in</span> channels:
        input_tensor[<span>0</span>, channel] -= input_tensor[<span>0</span>, channel].mean() * per_channel_shift
    <span>return</span> input_tensor - input_tensor.mean() * full_tensor_shift
</code></pre>
<h2>
	<a rel="nofollow" href="#lets-take-an-example-output-from-sdxl" id="lets-take-an-example-output-from-sdxl">
		
	</a>
	<span>
		Let&#39;s take an example output from SDXL
	</span>
</h2>
<pre><code>seed: 77777777
guidance_scale: 20 # A high guidance scale can be fixed too
steps with base: 23
steps with refiner: 10

prompt: Cinematic.Beautiful smile action woman in detailed white mecha gundam armor with red details,green details,blue details,colorful,star wars universe,lush garden,flowers,volumetric lighting,perfect eyes,perfect teeth,blue sky,bright,intricate details,extreme detail of environment,infinite focus,well lit,interesting clothes,radial gradient fade,directional particle lighting,wow

negative_prompt: helmet, bokeh, painting, artwork, blocky, blur, ugly, old, boring, photoshopped, tired, wrinkles, scar, gray hair, big forehead, crosseyed, dumb, stupid, cockeyed, disfigured, crooked, blurry, unrealistic, grayscale, bad anatomy, unnatural irises, no pupils, blurry eyes, dark eyes, extra limbs, deformed, disfigured eyes, out of frame, no irises, assymetrical face, broken fingers, extra fingers, disfigured hands
</code></pre>
<p><strong>Notice</strong> that I&#39;ve purposely chosen a high guidance scale.</p>
<p>How can we fix this image? It&#39;s half painting, half photograph. The colors range is biased towards yellow. To the right is a fixed generation with the exact same settings. </p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/jBMIbZgnxebjU1eQ2jZo_.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/lcWkMaCh4Cl3UwHNJMe_k.jpeg"/>
</p>

<p>But also with a sensible <code>guidance_scale</code> set to 7.5, we can still conclude that the fixed output is better, without nonsensical details and correct white balance.</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/3uoRRsCPP2uPh1vEUkFAO.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/2OP4jslTd653TPSF0l8Yr.jpeg"/>
</p>

<p>There are many things we can do in the latent space to generally improve a generation and there are some very simple things which we can do to target specific errors in a generation:</p>
<h3>
	<a rel="nofollow" href="#outlier-removal" id="outlier-removal">
		
	</a>
	<span>
		Outlier removal
	</span>
</h3>
<p>This will control the amount of nonsensical details, by pruning values that are the farthest from the mean of the distribution. It also helps in generating at higher guidance_scale.</p>
<pre><code>
<span>def</span> <span>soft_clamp_tensor</span>(<span>input_tensor, threshold=<span>3.5</span>, boundary=<span>4</span></span>):
    <span>if</span> <span>max</span>(<span>abs</span>(input_tensor.<span>max</span>()), <span>abs</span>(input_tensor.<span>min</span>())) &lt; <span>4</span>:
        <span>return</span> input_tensor
    channel_dim = <span>1</span>

    max_vals = input_tensor.<span>max</span>(channel_dim, keepdim=<span>True</span>)[<span>0</span>]
    max_replace = ((input_tensor - threshold) / (max_vals - threshold)) * (boundary - threshold) + threshold
    over_mask = (input_tensor &gt; threshold)

    min_vals = input_tensor.<span>min</span>(channel_dim, keepdim=<span>True</span>)[<span>0</span>]
    min_replace = ((input_tensor + threshold) / (min_vals + threshold)) * (-boundary + threshold) - threshold
    under_mask = (input_tensor &lt; -threshold)

    <span>return</span> torch.where(over_mask, max_replace, torch.where(under_mask, min_replace, input_tensor))
</code></pre>
<h3>
	<a rel="nofollow" href="#color-balancing-and-increased-range" id="color-balancing-and-increased-range">
		
	</a>
	<span>
		Color balancing and increased range
	</span>
</h3>
<p>I have two main methods of achieving this. The first one is to shrink towards the mean while normalizing the values (Which will also remove outliers) and the second is to fix when the values get biased towards some color. This also helps in generating at higher guidance_scale.</p>
<pre><code>
<span>def</span> <span>center_tensor</span>(<span>input_tensor, channel_shift=<span>1</span>, full_shift=<span>1</span>, channels=[<span>0</span>, <span>1</span>, <span>2</span>, <span>3</span>]</span>):
    <span>for</span> channel <span>in</span> channels:
        input_tensor[<span>0</span>, channel] -= input_tensor[<span>0</span>, channel].mean() * channel_shift
    <span>return</span> input_tensor - input_tensor.mean() * full_shift
</code></pre>
<h3>
	<a rel="nofollow" href="#tensor-maximizing" id="tensor-maximizing">
		
	</a>
	<span>
		Tensor maximizing
	</span>
</h3>
<p>This is basically done by multiplying the tensors by a very small amount like <code>1e-5</code> for a few steps and to make sure that the final tensor is using the full possible range ( closer to -4/4) before converting to RGB. Remember, in the pixel space, it&#39;s easier to reduce contrast, saturation and sharpness with intact dynamics than to increase it.</p>
<pre><code>
<span>def</span> <span>maximize_tensor</span>(<span>input_tensor, boundary=<span>4</span>, channels=[<span>0</span>, <span>1</span>, <span>2</span>]</span>):
    min_val = input_tensor.<span>min</span>()
    max_val = input_tensor.<span>max</span>()

    normalization_factor = boundary / <span>max</span>(<span>abs</span>(min_val), <span>abs</span>(max_val))
    input_tensor[<span>0</span>, channels] *= normalization_factor

    <span>return</span> input_tensor
</code></pre>
<h3>
	<a rel="nofollow" href="#callback-implementation-example" id="callback-implementation-example">
		
	</a>
	<span>
		Callback implementation example
	</span>
</h3>
<pre><code><span>def</span> <span>callback</span>(<span>pipe, step_index, timestep, cbk</span>):
      <span>if</span> timestep &gt; <span>950</span>:
          threshold = <span>max</span>(cbk[<span>&#34;latents&#34;</span>].<span>max</span>(), <span>abs</span>(cbk[<span>&#34;latents&#34;</span>].<span>min</span>())) * <span>0.998</span>
          cbk[<span>&#34;latents&#34;</span>] = soft_clamp_tensor(cbk[<span>&#34;latents&#34;</span>], threshold*<span>0.998</span>, threshold)
      <span>if</span> timestep &gt; <span>700</span>:
          cbk[<span>&#34;latents&#34;</span>] = center_tensor(cbk[<span>&#34;latents&#34;</span>], <span>0.8</span>, <span>0.8</span>)
      <span>if</span> timestep &gt; <span>1</span> <span>and</span> timestep &lt; <span>100</span>:
          cbk[<span>&#34;latents&#34;</span>] = center_tensor(cbk[<span>&#34;latents&#34;</span>], <span>0.6</span>, <span>1.0</span>)
          cbk[<span>&#34;latents&#34;</span>] = maximize_tensor(cbk[<span>&#34;latents&#34;</span>])
      <span>return</span> cbk

  image = base(
      prompt,
      guidance_scale = guidance_scale,
      callback_on_step_end=callback,
      callback_on_step_end_inputs=[<span>&#34;latents&#34;</span>]
  ).images[<span>0</span>]
</code></pre>
<p>This simple implementation of the three methods are used in the last set of images, with the <a rel="nofollow" href="#long-prompts-at-high-guidance-scales-becoming-possible">women in the garden</a>.</p>
<h2>
	<a rel="nofollow" href="#a-complete-demonstration" id="a-complete-demonstration">
		
	</a>
	<span>
		<a rel="nofollow" href="https://timothyalexisvass.github.io/sdxl-correction/">A complete demonstration</a>
	</span>
</h2>
<p><a rel="nofollow" href="https://timothyalexisvass.github.io/sdxl-correction/"><em>Click the heading or this link for an interactive demo!</em></a></p>
<p>This demonstration uses a more advanced implementation of the techniques by detecting outliers using Z-score, by shifting towards mean dynamically and by applying strength to each technique.</p>
<h4>
	<a rel="nofollow" href="#original-sdxl-too-yellow-and-slight-modification-white-balanced" id="original-sdxl-too-yellow-and-slight-modification-white-balanced">
		
	</a>
	<span>
		Original SDXL (too yellow) and slight modification (white balanced)
	</span>
</h4>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/nAOqJuCFBCnvhuDWAdt3q.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/B2DxT2vaiXwTEkLqtmo6l.jpeg"/>
</p>

<h4>
	<a rel="nofollow" href="#medium-modification-and-hard-modification-both-with-all-3-techniques-applied" id="medium-modification-and-hard-modification-both-with-all-3-techniques-applied">
		
	</a>
	<span>
		Medium modification and hard modification (both with all 3 techniques applied)
	</span>
</h4>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/v2vN8bL4H_xjmSEa-YNbV.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/EdOI9msLOV29WLwMpxqsr.jpeg"/>
</p>

<h2>
	<a rel="nofollow" href="#increasing-color-range--removing-color-bias" id="increasing-color-range--removing-color-bias">
		
	</a>
	<span>
		Increasing color range / removing color bias
	</span>
</h2>
<p>For the below, SDXL has limited the color range to red and green in the regular output. Because there is nothing in the prompt suggesting that there is such a thing as blue. This is a rather good generation, but the color range has become restricted.</p>
<p>If you give someone a palette of black, red, green and yellow and then tell them to paint a clear blue sky, the natural response is to ask you to supply blue and white.</p>
<p>To include blue in the generation, we can simply realign the color space when it gets restricted and SDXL will appropriately include the full color spectrum in the generation.</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/TESaZbRO9Cs5iarFYEKyI.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/LQnAxgTi1okXVD530p9Of.jpeg"/>
</p>

<h2>
	<a rel="nofollow" href="#long-prompts-at-high-guidance-scales-becoming-possible" id="long-prompts-at-high-guidance-scales-becoming-possible">
		
	</a>
	<span>
		Long prompts at high guidance scales becoming possible
	</span>
</h2>
<p>Here is a typical scenario, where the increased color range makes the whole prompt possible.</p>
<p><strong>prompt:</strong> Photograph of woman in red dress in a luxury garden surrounded with <strong>blue</strong>, yellow, purple and flowers in <strong>many colors</strong>, high class, award-winning photography, Portra 400, full format. <strong>blue sky</strong>, intricate details even to the smallest particle, extreme detail of the environment, sharp portrait, <strong>well lit</strong>, <strong>interesting</strong> outfit, beautiful shadows, <strong>bright</strong>, photoquality, ultra realistic, masterpiece</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/KTjI5LkGBaR1GarQpzTLq.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/5w4_PvycQsDNw2OaUelOD.jpeg"/>
</p>

<h4>
	<a rel="nofollow" href="#here-are-some-more-comparisons-on-the-same-concept" id="here-are-some-more-comparisons-on-the-same-concept">
		
	</a>
	<span>
		Here are some more comparisons on the same concept
	</span>
</h4>
<p><em>Keep in mind that these all just use the <a rel="nofollow" href="#callback-implementation-example">same static modifications</a>.</em></p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/RtP73Piqn31OODhjlFL5R.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/P3Dx7lbdsX3wV_T8Tz1hZ.jpeg"/>
</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/pGySTL_HWJiTDlMPHeqW_.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/R640f7f2tEZ2Bi0PMeifP.jpeg"/>
</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/8pkhzEfyhXICIQagm9Y2E.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/vr_End8uCGXEIl_qgjgLn.jpeg"/>
</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/lAiHqYaLjwNNDUWBoy4ng.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/Rp89eYXUCf9TcGGIovxx_.jpeg"/>
</p>
<p><img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/eHKGxPJwBg7vaxWLFkSKq.jpeg"/>
  <img src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/tKcpDdVWjrb4os6qgSk7U.jpeg"/>
</p>

<h2>
	<a rel="nofollow" href="#udi-sdxl-correction-filters" id="udi-sdxl-correction-filters">
		
	</a>
	<span>
		UDI SDXL Correction filters
	</span>
</h2>
<h3>
	<a rel="nofollow" href="#back-to-top" id="back-to-top">
		
	</a>
	<span>
		<a rel="nofollow" href="#table-of-contents">Back to top</a>
	</span>
</h3>
<p><a rel="nofollow" href="https://youtu.be/jjdrhIgLDvQ"><img alt="image/jpeg" src="https://cdn-uploads.huggingface.co/production/uploads/64e0903c4c78e1eba50b47fa/XjA0h119T9pQCLnSXSTb1.jpeg"/></a></p>
<!-- HTML_TAG_END --></div>
	</div></div>
  </body>
</html>
