<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.gwern.net/Scaling-hypothesis">Original</a>
    <h1>The Scaling Hypothesis (2021)</h1>
    
    <div id="readability-page-1" class="page"><div id="page-metadata">
        <p>On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data &amp; compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.</p>
        
        
      </div><div id="markdownBody">
        <div>
          <blockquote>
            <p>GPT-3, announced by <a href="https://en.wikipedia.org/wiki/OpenAI" data-link-icon="wikipedia" data-link-icon-type="svg" title="OpenAI">OpenAI</a> in May 2020, is the largest neural network ever trained, by over an order of magnitude. Trained on Internet text data, it is the successor to <a href="https://www.moderndescartes.com/docs/ai/gpt/2019-radford.pdf#openai" id="gpt-2-paper" data-link-icon="openai" data-link-icon-type="svg" title="&#39;Language Models are Unsupervised Multitask Learners&#39;, Radford et al 2019">GPT-2</a>⁠, which had surprised everyone by its natural language understanding &amp; generation ability. To the surprise of most (including myself), this vast increase in size did not run into diminishing or negative returns, as many expected, but the benefits of scale continued to happen as forecasted by OpenAI. These benefits were not merely learning more facts &amp; text than GPT-2, but qualitatively distinct &amp; even more surprising in showing <a href="#meta-learning"><em>meta-learning</em></a>: while GPT-2 learned how to do common natural language tasks like text summarization, GPT-3 instead learned how to follow directions and learn new tasks from a few examples. (As a result, GPT-3 outputs &amp; interaction are more fascinating &amp; human-like than GPT-2.)</p>
            <p>While the immediate applications of GPT-3, like my poetry or humor writings, are nice, the short-term implications of GPT-3 are much more important.</p>
            <p>First, while GPT-3 is expensive by conventional DL standards, it is cheap by scientific/​commercial/​military/​government budget standards, and the results indicate that models could be made much larger. Second, models can also be made much more powerful, as GPT is an old approach known to be flawed in both minor &amp; major ways, and far from an ‘ideal’ <a href="https://www.moderndescartes.com/docs/www/arxiv.org/2f90212754aa5c9487dcc3552e5d807f87063eca.pdf#google" id="vaswani-et-al-2017" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1706.03762?fallback=original#google" title="&#39;Attention Is All You Need&#39;, Vaswani et al 2017 (Original URL: https://arxiv.org/abs/1706.03762#google )">Transformer</a>⁠. Third, GPT-3’s capabilities come from learning on raw (unsupervised) data; that has long been one of the weakest areas of DL, holding back progress in other areas like <a href="https://en.wikipedia.org/wiki/Reinforcement_learning" data-link-icon="wikipedia" data-link-icon-type="svg" title="Reinforcement learning">reinforcement learning</a> or robotics. Models like GPT-3 suggest that large unsupervised models will be vital components of future DL systems, as they can be ‘plugged into’ systems to immediately provide understanding of the world, humans, natural language, and reasoning.</p>
            <p>The meta-learning has a longer-term implication: it is a demonstration of the <a href="#blessings-of-scale"><em>blessings of scale</em></a>⁠, where problems with simple neural networks vanish, and they become more powerful, more generalizable, more human-like when simply made very large &amp; trained on very large datasets with very large compute—even though those properties are believed to require complicated architectures &amp; fancy algorithms (and this perceived need drives much research). Unsupervised models benefit from this, as training on large corpuses like Internet-scale text present a myriad of difficult problems to solve; this is enough to drive meta-learning despite GPT not being designed for meta-learning in any way. (This family of phenomena is perhaps driven by neural networks functioning as ensembles of many sub-networks with them all averaging out to an Occam’s razor, which for small data &amp; models, learn superficial or memorized parts of the data, but can be forced into true learning by making the problems hard &amp; rich enough; as <a href="https://www.moderndescartes.com/Backstop#deep-bayes" id="gwern-backstop-deep-bayes">meta-learners learn amortized Bayesian inference</a>⁠, they build in informative <a href="https://en.wikipedia.org/wiki/Prior_probability" data-link-icon="wikipedia" data-link-icon-type="svg" title="Prior probability">priors</a> when trained over many tasks, and become dramatically more sample-efficient and better at generalization.)</p>
            <p>The blessings of scale in turn support a radical theory: an old AI paradigm held by a few pioneers in connectionism (early artificial neural network research) and by more recent deep learning researchers, the <a href="#scaling-hypothesis"><em>scaling hypothesis</em></a>⁠. The scaling hypothesis regards the blessings of scale as the secret of AGI: intelligence is ‘just’ simple neural units &amp; learning algorithms applied to diverse experiences at a (currently) unreachable scale. As increasing computational resources permit running such algorithms at the necessary scale, the neural networks will get ever more intelligent.</p>
            <p>When? Estimates of Moore’s law-like progress curves decades ago by pioneers like <a href="https://en.wikipedia.org/wiki/Hans_Moravec" data-link-icon="wikipedia" data-link-icon-type="svg" title="Hans Moravec">Hans Moravec</a> indicated that it would take until the 2010s for the sufficiently-cheap compute for tiny insect-level prototype systems to be available, and the 2020s for the first sub-human systems to become feasible, and these forecasts are holding up. (Despite this vindication, the scaling hypothesis is so unpopular an idea, and difficult to prove in advance rather than as a <em>fait accompli</em>, that while the GPT-3 results finally drew some public notice after OpenAI enabled limited public access &amp; people could experiment with it live, it is unlikely that many entities will modify their research philosophies, much less kick off an ‘arms race’.)</p>
            <p>More concerningly, GPT-3’s scaling curves, unpredicted meta-learning, and success on various anti-AI challenges suggests that in terms of futurology, AI researchers’ forecasts are an emperor sans garments: they have no coherent model of how AI progress happens or why GPT-3 was possible or what specific achievements should cause alarm, where intelligence comes from, and do not learn from any falsified predictions. Their primary concerns appear to be supporting the status quo, placating public concern, and remaining respectable. As such, their comments on AI risk are meaningless: they would make the same public statements if the scaling hypothesis were true or not.</p>
            <p>Depending on what investments are made into scaling DL, and how fast compute grows, the 2020s should be quite interesting—sigmoid or singularity?</p>
            <p>For more ML scaling research, follow the <a href="https://old.reddit.com/r/mlscaling/" id="gwern-old-reddit-com-r-mlscaling" data-link-icon="reddit" data-link-icon-type="svg" title="&#39;ML Scaling subreddit&#39;, Branwen 2020"> /   r /   MLScaling</a> subreddit. For a fiction treatment as SF short story, see <a href="https://www.moderndescartes.com/fiction/Clippy" id="gwern-fiction-clippy" title="&#39;It Looks Like You’re Trying To Take Over The World&#39;, Branwen 2022">“It Looks Like You’re Trying To Take Over The World”</a>⁠.</p>
          </blockquote>
        </div>
        
        <section id="meta-learning">
          
          <p><span>Learning to learn.</span> In May 2020, OA released—to remarkably little interest from researchers, no blog post, no media blitz, and little public discussion beyond the snidely dismissive—the long-awaited followup to <a href="https://openai.com/blog/better-language-models/" id="gpt-2-blog" data-link-icon="openai" data-link-icon-type="svg" title="Better Language Models and Their Implications">GPT-2</a>⁠, one model to rule them all: a 117× larger 175b-parameter model with far more powerful language generation, which lets it solve a wide variety of problems from arithmetic<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> to English translation to unscrambling anagrams to SAT analogies—purely from being prompted with text examples, without any specialized training or finetuning whatsoever, merely next-word prediction training on a big Internet text corpus. This implies GPT-3’s attention mechanisms serve as <a href="https://www.moderndescartes.com/docs/www/arxiv.org/5f3ef8ce619f3af4f40dc1e235b3492d3c2685e3.pdf#deepmind" id="ba-et-al-2016" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1610.06258?fallback=original#deepmind" title="&#39;Using Fast Weights to Attend to the Recent Past&#39;, Ba et al 2016 (Original URL: https://arxiv.org/abs/1610.06258#deepmind )">“fast weights”</a> that have “learned to learn” by training on sufficiently varied data<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>⁠, forcing it to do more than just learn ordinary textual relationships. Like OpenAI’s <a href="https://openai.com/blog/jukebox/" id="jukebox-blog" data-link-icon="openai" data-link-icon-type="svg" title="&#39;Jukebox: We&#39;re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We&#39;re releasing the model weights and code, along with a tool to explore the generated samples.&#39;, Dhariwal et al 2020">Jukebox</a> just weeks ago (itself a remarkable demonstration of scaling in synthesizing <em>raw audio</em> music complete with remarkably realistic voices/​instruments), the announcement of GPT-3 appears to have sunk almost without a trace, so I will go into more depth than usual.</p>
        </section>
        <section id="flexing-gpt">
          
          <div>
            <blockquote>
              <p>‘“They are absolutely reasonable. I think that is their distinguishing characteristic. Yes, Mr. Erskine, an absolutely reasonable people. I assure you there is no nonsense about the Americans.” “How dreadful!” cried Lord Henry. “I can stand brute force, but brute reason is quite unbearable. There is something unfair about its use. It is hitting below the intellect.”’</p>
              <p><em>The Picture of Dorian Gray</em>, Oscar Wilde</p>
            </blockquote>
          </div>
          <p><span>“Attacks only get better.”</span> 2 years ago, <a href="https://openai.com/blog/language-unsupervised/" id="openai-2018" data-link-icon="openai" data-link-icon-type="svg" title="Improving Language Understanding with Unsupervised Learning: We&#39;ve obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we&#39;re also releasing. Our approach is a combination of two existing ideas: transformers and unsupervised pre-training. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse datasets.">GPT-1</a> was interestingly useful pretraining and adorable with its “sentiment neuron”. 1 year ago, GPT-2 was impressive with its excellent text generation &amp; finetuning capabilities. This year, GPT-3 is scary because it’s a magnificently obsolete architecture from early 2018 (used mostly for software engineering convenience as the infrastructure has been debugged), which is small &amp; shallow compared to what’s possible<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a><a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>⁠, with a simple uniform architecture<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> trained in the dumbest way possible (unidirectional prediction of next text token) on a single impoverished modality (random Internet HTML text dumps<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>) on tiny data (fits on a laptop), sampled in a dumb way<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a>⁠, its benchmark performance sabotaged by bad prompts &amp; <a href="https://www.moderndescartes.com/GPT-3#bpes" id="gwern-gpt-3-bpes" title="&#39;GPT-3 Creative Fiction § BPEs&#39;, Branwen 2020">data encoding problems</a> (especially arithmetic &amp; commonsense reasoning), and yet, the first version already manifests crazy runtime meta-learning—and the scaling curves <em>still</em> are not bending! The samples are also better than ever, whether it’s GPT-3 inventing new penis jokes<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> or writing (mostly working) <a href="https://www.moderndescartes.com/docs/www/justpaste.it/b5a07c7305ca81b0de2d324f09445f9ef407c17e.html#javascript" id="gpt-3-2020" rel="archived alternate nofollow" data-url-original="https://justpaste.it/7eovk#javascript" title="&#39;GPT-3 random sample dump: JavaScript tutorial&#39;, GPT- 2020 (Original URL: https://justpaste.it/7eovk#javascript )">JavaScript tutorials</a> about rotating arrays.</p>
          <p>It’s odd that this qualitative leap appears to be largely missed by the standard NLP benchmarks. Nothing in the raw metrics reported on, say, Penn Tree Bank or LAMBADA or WinoGrande would lead you to expect all of this hilarious and creative output; the meta-learning results might, but only if you already thought meta-learning was important. This suggests to me that a useful post-GPT-3 contribution would be figuring out how to benchmark these sorts of flexible text generation capabilities (possibly something along the lines of Chollet’s image-based <a href="https://www.moderndescartes.com/docs/www/arxiv.org/4c7ad5971c05b5e8ee09d941b50ca32026399bf0.pdf#google" id="chollet-2019" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1911.01547?fallback=original#google" title="&#39;On the Measure of Intelligence&#39;, Chollet 2019 (Original URL: https://arxiv.org/abs/1911.01547#google )">Abstraction and Reasoning Corpus (ARC)</a>).</p>
        </section>
        <section id="baking-the-cake">
          
          <figure>
            <img alt="Is GPT actually part of AGI—or is the cake a lie? (LeCun 2019)" decoding="async" height="789" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="https://www.moderndescartes.com/images/ai/2019-lecun-isscctalk-cake.png" srcset="/images/ai/2019-lecun-isscctalk-cake.png-768px.png 768w, /images/ai/2019-lecun-isscctalk-cake.png 1400w" width="1400"/>
            <figcaption aria-hidden="true">
              Is GPT actually part of AGI—or is the cake a lie? (<a href="https://www.moderndescartes.com/docs/ai/scaling/2019-02-18-lecun-isscc-talk-deeplearninghardwarepastpresentandfuture.pdf#page=60" id="lecun-2019-page-60" data-link-icon="pdf" data-link-icon-type="svg" title="Deep Learning Hardware: Past, Present, &amp; Future: slide 60: &#39;How Much Information is the Machine Given during Learning?&#39;">LeCun 2019</a>)
            </figcaption>
          </figure>
          <p><span>Not the whole picture, but a big part.</span> Does it set SOTA on every task? No, of course not. But the question is not whether we can lawyerly find any way in which it might not work, but <a href="https://www.moderndescartes.com/Forking-Paths" id="gwern-forking-paths" title="&#39;Technology Forecasting: The Garden of Forking Paths&#39;, Branwen 2014">whether there is any way which it might work</a>⁠. And there are many ways it might work better (see the <a href="https://www.moderndescartes.com/docs/www/arxiv.org/662ef6c17690db4b6703318f0e1d6829a8ff47be.pdf#page=34" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2005.14165.pdf&amp;org=openai#page=34" title="GPT-3: Language Models are Few-Shot Learners: 5. Limitations (Original URL: https://arxiv.org/pdf/2005.14165.pdf&amp;org=openai#page=34 )">“Limitations” section</a> for just a few). Does GPT-3 <em>do</em> anything like steer a robot around SF shooting lasers and rockets at humans⸮ No, of course not. It is ‘just’ a text prediction model, an idiot savant of text; but an idiot savant, we should remember, is only a genetic mutation or bit of brain damage away from a normal human. If RL is the cherry on the top of the supervised learning frosting, and supervised learning is the frosting on top of the unsupervised learning cake, well, it looks like the cake layers are finally rising.</p>
          <figure>
            <img alt="A better GPT-3 lesson." decoding="async" height="649" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="https://www.moderndescartes.com/images/ai/gpt/2020-07-24-meme-moneyprinter-bitterlesson-gpt3.png" srcset="/images/ai/gpt/2020-07-24-meme-moneyprinter-bitterlesson-gpt3.png-768px.png 768w, /images/ai/gpt/2020-07-24-meme-moneyprinter-bitterlesson-gpt3.png 1400w" title="GPT-3&#39;s implications in the &#39;money printer go brr&#39; meme format: the head of Rich Sutton says &#39;GPUs go bitter&#39;, referencing his &#39;bitter lesson&#39; that most clever AI innovations are ultimately useless as they hamstring AI performance and are surpassed by methods that make fewer assumptions &amp; use more compute/data, while the personification of AI academia, where cleverness is rewarded and heavy use of compute is considered cheating and ugly, sheds tears and complains about approaches like GPT-3 beating decades of clever academic systems." width="1400"/>
            <figcaption aria-hidden="true">
              A better GPT-3 lesson.
            </figcaption>
          </figure>
          <p><span>Scaling still working.</span> I was surprised, as I had expected closer to 100b parameters, and I thought that the performance of <a href="https://www.moderndescartes.com/docs/www/arxiv.org/0b9e7be08a4baf0b4fc120364ea36172ecb3c9f0.pdf#salesforce" id="keskar-et-al-2019" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1909.05858?fallback=original#salesforce" title="&#39;CTRL: A Conditional Transformer Language Model for Controllable Generation&#39;, Keskar et al 2019 (Original URL: https://arxiv.org/abs/1909.05858#salesforce )">CTRL</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/904a1f22bf4ef78890a43c99ae058eb1d1c8e9d9.pdf#google" id="adiwardana-et-al-2020" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2001.09977?fallback=original#google" title="&#39;Towards a Human-like Open-Domain Chatbot&#39;, Adiwardana et al 2020 (Original URL: https://arxiv.org/abs/2001.09977#google )">Meena</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/nv-adlr.github.io/498e2f976d807cbfe54cd891bb64d9fe0f0f9f6a.html" id="adlr-2019" data-link-icon="n" data-link-icon-type="text,sans,italic" rel="archived alternate nofollow" data-url-original="https://nv-adlr.github.io/MegatronLM" title="MegatronLM: Training Billion+ Parameter Language Models Using GPU Model Parallelism (Original URL: https://nv-adlr.github.io/MegatronLM )">MegatronLM</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/6c6da30801f7053b5392a4582eaff2b665d5df34.pdf#google" id="raffel-et-al-2019" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1910.10683?fallback=original#google" title="&#39;T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&#39;, Raffel et al 2019 (Original URL: https://arxiv.org/abs/1910.10683#google )">T5</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/www.microsoft.com/879d00b0771726c7a08e6951f1aa29a2f16aa062.html" id="rosset-2020" data-link-icon="MS" data-link-icon-type="text,sans,italic" rel="archived alternate nofollow" data-url-original="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" title="&#39;Turing-NLG: A 17-billion-parameter language model by Microsoft&#39;, Rosset 2020 (Original URL: https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/ )">Turing-NLG</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/a8efcc8272af6f434119f87a00c2edaf84241597.pdf#google" id="huang-et-al-2018" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1811.06965?fallback=original#google" title="&#39;GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism&#39;, Huang et al 2018 (Original URL: https://arxiv.org/abs/1811.06965#google )">GPipe</a> suggested that, <a href="https://www.moderndescartes.com/notes/Scaling" id="gwern-notes-scaling" title="&#39;Machine Learning Scaling&#39;, Branwen 2021">the scaling papers</a><a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> notwithstanding, the scaling curves had started to bend and by 100b, it might be hard to justify further scaling. However, in the latest version of <a href="https://www.moderndescartes.com/docs/ai/gpt/2009-halevy.pdf" id="halevy-et-al-2009" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;The Unreasonable Effectiveness of Data&#39;, Halevy et al 2009">“the unreasonable effectiveness of data”</a> where “the curves cross”/​“scissor effect” and the neural method eventually wins (eg. <a href="https://www.moderndescartes.com/docs/ai/scaling/2001-banko.pdf#microsoft" id="banko-brill-2001" data-link-icon="MS" data-link-icon-type="text,sans,italic" title="Scaling to Very Very Large Corpora for Natural Language Disambiguation">Banko &amp; Brill 2001</a>⁠, <a href="https://www.moderndescartes.com/docs/ai/scaling/2007-brants.pdf#google" id="brants-et-al-2007" data-link-icon="google" data-link-icon-type="svg" title="Large Language Models in Machine Translation">Brants et al 2007</a>⁠, <a href="https://www.moderndescartes.com/images/ai/2017-koehn-figure3-bleuscoreswithvaryingamountsoftrainingdata.png" data-link-icon="image" data-link-icon-type="svg" data-image-height="1520" data-image-width="1027" title="Six Challenges for Neural Machine Translation: Challenges: 3.2. Amount of Training Data: Figure 3: BLEU scores for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data. Quality for NMT starts much lower, outperforms SMT at about 15 million words, and even beats a SMT system with a big 2 billion word in-domain language model under high-resource conditions.">Koehn &amp; Knowles 2017</a>), GPT-3 hits twice that without noticeable change in scaling factors: its scaling continues to be roughly logarithmic/​<a href="https://en.wikipedia.org/wiki/Power_law" data-link-icon="wikipedia" data-link-icon-type="svg" title="Power law">power-law</a>⁠, as it was for much smaller models &amp; as forecast, and it has not hit a regime where gains effectively halt or start to require increases vastly beyond feasibility. That suggests that it would be both possible and useful to head to trillions of parameters (which are still well within available compute &amp; budgets, requiring merely thousands of GPUs &amp; perhaps $10–$100m budgets assuming no improvements which of course there will be, see Hernandez &amp; Brown 2020 etc in this issue), and eyeballing the graphs, many benchmarks like the <a href="https://en.wikipedia.org/wiki/Winograd_Schema_Challenge" data-link-icon="wikipedia" data-link-icon-type="svg" title="Winograd Schema Challenge">Winograd schema</a> <a href="https://www.moderndescartes.com/docs/www/arxiv.org/bc2fa7f431d54c3530f7870942d979b9853a4b84.pdf#allen" id="sakaguchi-et-al-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1907.10641?fallback=original#allen" title="&#39;WinoGrande: An Adversarial Winograd Schema Challenge at Scale&#39;, Sakaguchi et al 2019 (Original URL: https://arxiv.org/abs/1907.10641#allen )">WinoGrande</a> would fall by 10t parameters. The predictability of scaling is striking, and makes scaling models more like statistics than AI. (AI is statistics which does what we want it to but doesn’t work; and statistics is AI which works but doesn’t do what we want.)</p>
          <figure>
            <img alt="GPT-3: not even that much compute—3640 petaflop / s-day, only 2× their estimate for AlphaGo Zero, 1860. (Historical graph modified by myself from “AI and Compute”, Amodei et al 2018.)" decoding="async" height="1021" loading="lazy" sizes="(max-width: 768px) 100vw, 1471px" src="https://www.moderndescartes.com/images/ai/gpt/2019-11-07-amodei-aiandcompute-twodistincteras-gpt3modified.png" srcset="/images/ai/gpt/2019-11-07-amodei-aiandcompute-twodistincteras-gpt3modified.png-768px.png 768w, /images/ai/gpt/2019-11-07-amodei-aiandcompute-twodistincteras-gpt3modified.png 1471w" width="1400"/>
            <figcaption aria-hidden="true">
              GPT-3: not even that much compute—<a href="https://www.moderndescartes.com/docs/www/arxiv.org/90cd91e98db4f7b0b1cd57da7c3713dbe34c2146.pdf#org=openai&amp;page=46" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2005.14165.pdf#org=openai&amp;page=46" title="Total Compute Used to Train Language Model: Table D.1 (Original URL: https://arxiv.org/pdf/2005.14165.pdf#org=openai&amp;page=46 )">3640 petaflop / s-day</a>⁠, only 2× their estimate for AlphaGo Zero, 1860. (Historical graph modified by myself from <a href="https://openai.com/blog/ai-and-compute/" id="amodei-et-al-2018" data-link-icon="openai" data-link-icon-type="svg" title="We&#39;re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore&#39;s Law had a 2-year doubling period). Since 2012, this metric has grown by more than 300,000× (a 2-year doubling period would yield only a 7× increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it&#39;s worth preparing for the implications of systems far outside today&#39;s capabilities.">“AI and Compute”, Amodei et al 2018</a>⁠.)
            </figcaption>
          </figure>
          <p><span>Anti-scaling: penny-wise, pound-foolish.</span> GPT-3 is an extraordinarily expensive model by the standards of machine learning: it is estimated that training it may require the annual cost of more machine learning researchers than you can count on one hand (~$5m<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a>), up to $30 of hard drive space to store the model (500–800GB), and multiple pennies of electricity per 100 pages of output (0.4 kWH). Researchers are concerned about the prospects for scaling: can ML afford to run projects which cost more than 0.1 milli-Manhattan-Projects⸮<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a> Surely it would be too expensive, even if it represented another large leap in AI capabilities, to spend up to 10 milli-Manhattan-Projects to scale GPT-3 100× to a trivial thing like human-like performance in many domains⸮ Many researchers feel that such a suggestion is absurd and refutes the entire idea of scaling machine learning research further, and that the field would be more productive if it instead focused on research which can be conducted by an impoverished goatherder on an old laptop running off solar panels.<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a> Nonetheless, I think we can expect further scaling. (10×? No, 10× isn’t cool. You know what’s cool? <a href="https://old.reddit.com/r/slatestarcodex/comments/hys565/are_we_in_an_ai_overhang/fzezi7d/" data-link-icon="reddit" data-link-icon-type="svg" title="People I know at OpenAI say v4 is around the corner and easily doable, and...will be here soon (not months but year or so). And they are confident it will scale and be around 100--1000×.">100–1000×</a>⁠, trained on a <a href="https://www.moderndescartes.com/docs/www/blogs.microsoft.com/ca0c69af68578f40db2cf9037e7789a4f90fa06a.html" id="langston-2020" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://blogs.microsoft.com/ai/openai-azure-supercomputer/" title="Microsoft announces new supercomputer, lays out vision for future AI work (Original URL: https://blogs.microsoft.com/ai/openai-azure-supercomputer/ )">fancy new supercomputer</a>⁠.)</p>
        </section>
        <section id="scaling">
          
          <p><span>How far will scaling go?</span> The scaling papers suggest that the leaps we have seen over the past few years are not even half way there in terms of absolute likelihood loss, never mind what real-world capabilities each additional decrement translates into. The scaling curves are clean; from <a href="https://www.moderndescartes.com/docs/www/arxiv.org/20d126b9c3baf640f8d1d5dff3e253faac2e8242.pdf#openai" id="kaplan-et-al-2020" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2001.08361?fallback=original#openai" title="&#39;Scaling Laws for Neural Language Models&#39;, Kaplan et al 2020 (Original URL: https://arxiv.org/abs/2001.08361#openai )">“Scaling Laws for Neural Language Models”, Kaplan et al 2020</a>:</p>
          <figure>
            <img alt="DL scaling laws: compute, data, model parameters. (Figure 1)" decoding="async" height="584" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="https://www.moderndescartes.com/images/ai/gpt/2020-kaplan-figure1-dlscaling.png" srcset="/images/ai/gpt/2020-kaplan-figure1-dlscaling.png-768px.png 768w, /images/ai/gpt/2020-kaplan-figure1-dlscaling.png 1400w" title="Figure 1: Language modeling performance improves smoothly as we increase the model size, dataset size, and amount of compute used for training. For optimal performance all three factors must be scaled up in tandem. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two. (Kaplan et al 2020)" width="1400"/>
            <figcaption aria-hidden="true">
              DL scaling laws: compute, data, model parameters. (<a href="https://www.moderndescartes.com/docs/www/arxiv.org/20d126b9c3baf640f8d1d5dff3e253faac2e8242.pdf#page=3&amp;org=openai" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2001.08361.pdf#page=3&amp;org=openai" title="Scaling Laws for Neural Language Models: Figure 1: Language modeling performance improves smoothly as we increase the model size, dataset size, and amount of compute used for training. (Original URL: https://arxiv.org/pdf/2001.08361.pdf#page=3&amp;org=openai )">Figure 1</a>)
            </figcaption>
          </figure>
          <p>GPT-3 represents ~10<sup>3</sup> on this chart, leaving plenty of room for further loss decreases—especially given the <a href="https://www.moderndescartes.com/docs/www/arxiv.org/20d126b9c3baf640f8d1d5dff3e253faac2e8242.pdf#page=17&amp;org=openai" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2001.08361.pdf#page=17&amp;org=openai" title="&#39;Scaling Laws for Neural Language Models: Figure 15: Far beyond the model sizes we study empirically, we find a contradiction between our equations&#39;, Kaplan et al 2020 (Original URL: https://arxiv.org/pdf/2001.08361.pdf#page=17&amp;org=openai )">uncertainty in extrapolation</a>:</p>
          <figure>
            <img alt="Projecting DL power laws: still room beyond GPT-3." decoding="async" height="606" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="https://www.moderndescartes.com/images/ai/gpt/2020-kaplan-figure15-projectingscaling.png" srcset="/images/ai/gpt/2020-kaplan-figure15-projectingscaling.png-768px.png 768w, /images/ai/gpt/2020-kaplan-figure15-projectingscaling.png 1400w" title="Figure 15: Far beyond the model sizes we study empirically, we find a contradiction between our equations for _L(C~min~)_ and _L(D)_ due to the slow growth of data needed for compute-efficient training. The intersection marks the point before which we expect our predictions to break down. The location of this point is highly sensitive to the precise exponents from our power-law fits. (Kaplan et al 2020)" width="1400"/>
            <figcaption aria-hidden="true">
              Projecting DL power laws: still room beyond GPT-3.
            </figcaption>
          </figure>
          <p>Lo and behold, the scaling laws continue for GPT-3 models for several orders past <a href="#kaplan-et-al-2020">Kaplan et al 2020</a>⁠; from <a href="https://www.moderndescartes.com/docs/www/arxiv.org/90cd91e98db4f7b0b1cd57da7c3713dbe34c2146.pdf#page=11&amp;org=openai" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2005.14165.pdf#page=11&amp;org=openai" title="GPT-3: Language Models are Few-Shot Learners: Figure 3.1: Smooth scaling of performance with compute (Original URL: https://arxiv.org/pdf/2005.14165.pdf#page=11&amp;org=openai )">Brown et al 2020</a>:</p>
          <figure>
            <img alt="GPT-3 continues to scale as predicted. (Note GPT-3’s curve has not ‘bounced’, and it trained only ~0.5 epoches, see Table 2.2)" decoding="async" height="845" loading="lazy" sizes="(max-width: 768px) 100vw, 1400px" src="https://www.moderndescartes.com/images/ai/gpt/2020-brown-figure31-gpt3scaling.png" srcset="/images/ai/gpt/2020-brown-figure31-gpt3scaling.png-768px.png 768w, /images/ai/gpt/2020-brown-figure31-gpt3scaling.png 1400w" title="Brown et al 2020: Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in Kaplan et al 2020 continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts. (Brown et al 2020). Cross-validation loss extrapolation: $L(oss) = 2.57 · C(ompute in petaflop-s/days) ^ −0.048$" width="1400"/>
            <figcaption aria-hidden="true">
              GPT-3 continues to scale as predicted. (Note GPT-3’s curve has not ‘bounced’, and it trained only ~0.5 epoches, see <a href="https://www.moderndescartes.com/docs/www/arxiv.org/90cd91e98db4f7b0b1cd57da7c3713dbe34c2146.pdf#org=openai&amp;page=9" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2005.14165.pdf#org=openai&amp;page=9" title="Table 2.2: Datasets used to train GPT-3. &#39;Weight in training mix&#39; refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once. (Original URL: https://arxiv.org/pdf/2005.14165.pdf#org=openai&amp;page=9 )">Table 2.2</a>)
            </figcaption>
          </figure>
          <p>If we see such striking gains in halving the validation loss but with so far left to go, what is left to emerge as we third or halve again? How far does this go, exactly? How do we predict what emerges when? Bueller? Bueller? (See also <a href="https://www.moderndescartes.com/images/ai/2020-adiwardana-meena-figure1-humanratingsvslikelihood.png" data-link-icon="image" data-link-icon-type="svg" data-image-height="1343" data-image-width="926" title="Towards a Human-like Open-Domain Chatbot, Adiwardana et al 2020: Figure 1: Interactive SSA vs Perplexity [exp(cross-entropy loss)]. Each point is a different version of the Meena model. A regression line is plotted, for which the coefficient of determination (R^2) is 0.93, an indication of strong correlation between perplexity and the human evaluation metric (SSA). The dotted lines show the SSA performance of other chatbots, humans (86%), the best end-to-end trained Meena model (72%), and the full version of Meena which incorporates a filtering mechanism and tuned decoding (Section 5) and scores 79%. Mitsuku and Cleverbot scored the same on overall SSA, but Mitsuku displayed higher sensibleness, whereas Cleverbot had higher specificity. See Sections 2.5, 2.6, and 4.3 for more details on how we performed these comparisons and how to interpret the results">Meena’s perplexity vs human-ness chatbot ratings</a>⁠, GPT-3-written news articles’ <a href="https://www.moderndescartes.com/images/ai/gpt/2020-brown-figure313-humanabilitytodetectmodelgeneratednewsstories.png" data-link-icon="image" data-link-icon-type="svg" data-image-height="1150" data-image-width="1500" title="Brown et al 2020: Figure 3.13: People&#39;s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberately-bad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best fit is a power law with 95% confidence intervals.">probability of fooling humans by parameter count</a>⁠, and <a href="https://www.moderndescartes.com/images/ai/gpt/2020-hendrycks-figure1b-gpt3-qascaling.png" data-link-icon="image" data-link-icon-type="svg" data-image-height="863" data-image-width="706" title="Figure 1b: GPT-3 Few Shot Test Performance: Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more continuous improvements with model size increases, but on our test, GPT-3 moves beyond random chance with the largest model.">GPT-3 model size vs Q&amp;A</a> from <a href="https://www.moderndescartes.com/docs/www/arxiv.org/a8c13af08875d3fd9b1303535ec6929901b3f5b8.pdf" id="hendrycks-et-al-2020-q-and-a" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2009.03300?fallback=original" title="Measuring Massive Multitask Language Understanding (Original URL: https://arxiv.org/abs/2009.03300 )">Hendrycks et al 2020</a>⁠.)</p>
          <section id="blessings-of-scale">
            <h2><a href="#blessings-of-scale" title="Link to section: § &#39;Blessings Of Scale&#39;">Blessings Of Scale</a></h2>
            <div>
              <blockquote>
                <p>“Extrapolating the spectacular performance of GPT-3 into the future suggests that the answer to life, the universe and everything is just 4.398 trillion parameters.”</p>
                <p><a href="https://nitter.hu/geoffreyhinton/status/1270814602931187715" data-link-icon="twitter" data-link-icon-type="svg">Geoff Hinton</a></p>
              </blockquote>
            </div><!-- -->
            <p><span>We don’t know how to train NNs.</span> The <em>blessings of scale</em> is the observation that for deep learning, hard problems are easier to solve than easy problems—everything gets better as it gets larger (in contrast to the usual outcome in research, where small things are hard and large things impossible). The bigger the neural net/​compute/​data/​problem, the faster it learns, the better it learns, the stabler it learns, and so on. A problem we can’t solve at all at small <em>n</em> may suddenly become straightforward with millions or billions of <em>n</em>. “NNs are lazy”: they can do far more than we make them do when we push them beyond easy answers &amp; cheap shortcuts. The <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" id="sutton-2019-2" title="&#39;The Bitter Lesson&#39;, Sutton 2019">bitter lesson</a> is the harder and bigger, the better. (Besides GPT-3, one could mention recent progress in <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning" data-link-icon="wikipedia" data-link-icon-type="svg" title="Semi-supervised learning">semi-supervised learning</a> &amp; the model-based DRL renaissance.)</p>
            <figure>
              <img alt="AlphaGo Zero: ‘just stack moar layers lol!’" decoding="async" height="943" loading="lazy" sizes="(max-width: 768px) 100vw, 787px" src="https://www.moderndescartes.com/images/rl/2017-12-24-meme-nnlayers-alphagozero.jpg" srcset="/images/rl/2017-12-24-meme-nnlayers-alphagozero.jpg-768px.jpg 768w, /images/rl/2017-12-24-meme-nnlayers-alphagozero.jpg 787w" title="Humorous description of the simplicity of the AlphaGo Zero architecture compared to AlphaGo Master" width="787"/>
              <figcaption aria-hidden="true">
                AlphaGo Zero: ‘just stack moar layers lol!’
              </figcaption>
            </figure>
            <p><span>Blessings of scale: stability → generalization → meta-learning.</span> GPT-3 is hamstrung by its training &amp; data, but DL enjoys an unreasonably effective <a href="https://en.wikipedia.org/wiki/Blessing_of_dimensionality" data-link-icon="wikipedia" data-link-icon-type="svg" title="Blessing of dimensionality">blessing of dimensionality</a>—just simply training a <em>big</em> model on a <em>lot</em> of data induces better properties like meta-learning without even the slightest bit of that architecture being built in; and in general, training on more and harder tasks creates ever more human-like performance, generalization, and robustness. The GPT natural-language &amp; programming language models, <a href="https://openai.com/blog/image-gpt/" id="chen-et-al-2020-blog" data-link-icon="openai" data-link-icon-type="svg" title="Image GPT: We find that, just as a large transformer model trained on language can generate coherent text, the same exact model trained on pixel sequences can generate coherent image completions and samples. By establishing a correlation between sample quality and image classification accuracy, we show that our best generative model also contains features competitive with top convolutional nets in the unsupervised setting.">iGPT</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/openreview.net/8d35c9933e254e48bf806bab2368f8891968d699.pdf#google" id="dosovitskiy-et-al-2020" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://openreview.net/forum?id=YicbFdNTTy#google" title="Vision Transformer (ViT): An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale (Original URL: https://openreview.net/forum?id=YicbFdNTTy#google )">Vision Transformer</a> for images (and to some degree <a href="https://www.moderndescartes.com/docs/www/arxiv.org/b2c130201eb1ea6fbb31059ef0b7351c928ac067.pdf#openai" id="polu-sutskever-2020" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2009.03393?fallback=original#openai" title="&#39;GPT-f: Generative Language Modeling for Automated Theorem Proving&#39;, Polu &amp; Sutskever 2020 (Original URL: https://arxiv.org/abs/2009.03393#openai )">GPT-f</a>), show that simply scaling up models &amp; datasets without any supervision produces results competitive with the best (and most complex) alternatives, using the same simple architecture, gradually passing from superficial surface correlations to more human-like brain activity (<a href="https://www.biorxiv.org/content/10.1101/2020.06.26.174482v3.full" id="schrimpf-et-al-2020" data-link-icon="chi-dna" data-link-icon-type="svg" title="The neural architecture of language: Integrative reverse-engineering converges on a model for predictive processing">Schrimpf et al 2020</a>) and linguistic biases as data increases (eg. <a href="https://www.moderndescartes.com/docs/www/arxiv.org/698400da2f4ae545e7be4aab3c9ae93a51dbff5c.pdf" id="warstadt-et-al-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2010.05358?fallback=original" title="Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually) (Original URL: https://arxiv.org/abs/2010.05358 )">Warstadt et al 2020</a>). In fact, one may not even need complicated attention mechanisms at scale, as fully-connected networks—hard to get much simpler than them!—<a href="https://www.moderndescartes.com/notes/FC" id="gwern-notes-fc" title="&#39;Fully-Connected Neural Nets&#39;, Branwen 2021">work surprisingly well</a> for many tasks. One typically trains such large models with simple optimizers like Adam—because the complicated ones lose their advantages as batch sizes increase and <a href="https://www.moderndescartes.com/docs/www/arxiv.org/f0ad6e381a59a9d948adfff8f554642fdef5e368.pdf" id="nado-et-al-2021" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2102.06356?fallback=original" title="&#39;A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes&#39;, Nado et al 2021 (Original URL: https://arxiv.org/abs/2102.06356 )">the simple optimizers work fine</a> and are more memory-efficient anyway. <a href="https://www.moderndescartes.com/docs/www/arxiv.org/ce54be912a7192483c027c22c5fa8ff05e01ec77.pdf#page=13" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1912.06680.pdf&amp;org=openai#page=13" title="&#39;Dota 2 with Large Scale Deep Reinforcement Learning: Section 4.3: Batch Size&#39;, Berner et al 2019 (Original URL: https://arxiv.org/pdf/1912.06680.pdf&amp;org=openai#page=13 )">OA5</a> does not just scale to, but <a href="#PPO-DoTA2">stabilizes at</a>⁠, minibatches of millions due to <a href="https://openai.com/blog/science-of-ai/" id="mccandlish-et-al-2018" data-link-icon="openai" data-link-icon-type="svg" title="How AI Training Scales: We&#39;ve discovered that the gradient noise scale, a simple statistical metric, predicts the parallelizability of neural network training on a wide range of tasks. Since complex tasks tend to have noisier gradients, increasingly large batch sizes are likely to become useful in the future, removing one potential limit to further growth of AI systems. More broadly, these results show that neural network training need not be considered a mysterious art, but can be rigorized and systematized. [&#39;An Empirical Model of Large-Batch Training&#39;, McCandlish et al 2018]">gradient noise</a>⁠. OA5-like, <a href="https://www.moderndescartes.com/docs/www/arxiv.org/89e3a06521a16cee0ceb01817edd661e744ce5ed.pdf#page=8&amp;org=deepmind" id="brock-et-al-2018-page-8-org-deepmind" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/1809.11096.pdf#page=8&amp;org=deepmind" title="&#39;BigGAN: Large Scale GAN Training For High Fidelity Natural Image Synthesis: 5.2 Additional Evaluation On JFT-300M&#39;, Brock et al 2018 (Original URL: https://arxiv.org/pdf/1809.11096.pdf#page=8&amp;org=deepmind )">BigGAN</a> stabilizes at large-scale image datasets like <a href="https://www.moderndescartes.com/docs/www/arxiv.org/df445e965df19c4dfa31f170827c675ef0042573.pdf#google" id="sun-et-al-2017" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1707.02968?fallback=original#google" title="&#39;Revisiting Unreasonable Effectiveness of Data in Deep Learning Era&#39;, Sun et al 2017 (Original URL: https://arxiv.org/abs/1707.02968#google )">JFT-300M</a> &amp; benefits from unusually large minibatches and VAEs (long an also-ran to <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" data-link-icon="wikipedia" data-link-icon-type="svg" title="Generative adversarial network">GANs</a> or autoregressive models in terms of sharp image generation) catch up if you make them very deep (<a href="https://www.moderndescartes.com/docs/www/arxiv.org/3e2eff21840379a01918be5f7ff900b06302f4bb.pdf#openai" id="child-2020" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2011.10650?fallback=original#openai" title="VDVAE: Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images (Original URL: https://arxiv.org/abs/2011.10650#openai )">Child 2020</a>⁠, <a href="https://www.moderndescartes.com/docs/www/arxiv.org/c8be434b574558518e8ed79bdd0871cbe967f5f6.pdf#nvidia" id="vahdat-kautz-2020" data-link-icon="n" data-link-icon-type="text,sans,italic" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2007.03898?fallback=original#nvidia" title="NVAE: A Deep Hierarchical Variational Autoencoder (Original URL: https://arxiv.org/abs/2007.03898#nvidia )">Vahdat &amp; Kautz 2020</a>); while classifier CNNs like <a href="https://www.moderndescartes.com/docs/www/arxiv.org/a54e22ca7c8754abebd73bc6f7c6f43d86587de4.pdf#google" id="kolesnikov-et-al-2019" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1912.11370?fallback=original#google" title="&#39;Big Transfer (BiT): Large Scale Learning of General Visual Representations for Transfer&#39;, Kolesnikov et al 2019 (Original URL: https://arxiv.org/abs/1912.11370#google )">BiT</a><a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/57646d2aac9c2ece67907255e3bfbd44b6ed8380.pdf#google" id="djolonga-et-al-2020" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2007.08558?fallback=original#google" title="On Robustness and Transferability of Convolutional Neural Networks (Original URL: https://arxiv.org/abs/2007.08558#google )">Dojolonga et al 2020</a> or <a href="https://www.moderndescartes.com/docs/www/arxiv.org/f75f5fe30e766cc805e1352792f6bdaa2f7a7999.pdf" id="orhan-2019" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1907.07640?fallback=original" title="&#39;Robustness properties of Facebook&#39;s ResNeXt WSL models&#39;, Orhan 2019 (Original URL: https://arxiv.org/abs/1907.07640 )">ResNeXt</a> or <a href="https://www.moderndescartes.com/docs/www/arxiv.org/fe0d46172204018e36e6768211691bf52b7de8f6.pdf#google" id="xie-et-al-2019" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1911.04252?fallback=original#google" title="&#39;Self-training with Noisy Student improves ImageNet classification&#39;, Xie et al 2019 (Original URL: https://arxiv.org/abs/1911.04252#google )">Noisy Student</a> transfer &amp; <a href="https://www.moderndescartes.com/docs/www/arxiv.org/cff443f62710c3854a54c260b64b451983027c72.pdf" id="taori-et-al-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2007.00644?fallback=original" title="&#39;Measuring Robustness to Natural Distribution Shifts in Image Classification&#39;, Taori et al 2020 (Original URL: https://arxiv.org/abs/2007.00644 )">robustify</a> <a href="https://www.moderndescartes.com/docs/www/arxiv.org/94add3361c6fbe713fb0e02cf8a0eb8544f4f8f9.pdf#google" id="bhojanapalli-et-al-2021" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2103.14586?fallback=original#google" title="&#39;Understanding Robustness of Transformers for Image Classification&#39;, Bhojanapalli et al 2021 (Original URL: https://arxiv.org/abs/2103.14586#google )">with</a> human-like errors<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a>⁠, multimodal learning produces better representations on fewer data (eg. <a href="https://www.moderndescartes.com/docs/www/arxiv.org/148ad98016dfd4ed2a180eda5a6e668df06d7696.pdf#facebook" id="lu-et-al-2019" data-link-icon="facebook" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1912.02315?fallback=original#facebook" title="&#39;12-in-1: Multi-Task Vision and Language Representation Learning&#39;, Lu et al 2019 (Original URL: https://arxiv.org/abs/1912.02315#facebook )">ViLBERT</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/1baba29f2a277a4c01cfced90771d03e12a33c82.pdf#google" id="sun-et-al-2019" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1904.01766?fallback=original#google" title="&#39;VideoBERT: A Joint Model for Video and Language Representation Learning&#39;, Sun et al 2019 (Original URL: https://arxiv.org/abs/1904.01766#google )">VideoBERT</a>⁠, motivating <a href="https://www.moderndescartes.com/docs/www/www.technologyreview.com/8ec24a193fd7caede72e77306220ff9434044075.html" id="hao-2020" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/" title="The messy, secretive reality behind OpenAI&#39;s bid to save the world (Original URL: https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/ )">OA’s interest in big multimodal models</a>), and RNNs can <a href="https://www.moderndescartes.com/docs/www/arxiv.org/596ca5608045bdf1d454ed0a2585539ce761c5e7.pdf#google" id="villegas-et-al-2019" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1911.01655?fallback=original#google" title="&#39;High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks&#39;, Villegas et al 2019 (Original URL: https://arxiv.org/abs/1911.01655#google )">predict videos</a>⁠. <a href="https://www.moderndescartes.com/docs/reinforcement-learning/alphastar/2019-vinyals.pdf#deepmind" id="vinyals-et-al-2019" data-link-icon="deepmind" data-link-icon-type="svg" title="&#39;Grandmaster level in StarCraft II using multi-agent reinforcement learning&#39;, Vinyals et al 2019">AlphaStar</a> reaches human-level with hundreds of competing self-players to cover possible strategies. Imitation learning DRL like <a href="https://www.moderndescartes.com/docs/www/arxiv.org/a92c5f78737a62a7d820973edf89eca967a16798.pdf#deepmind" id="paine-et-al-2018" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1810.05017?fallback=original#deepmind" title="&#39;MetaMimic: One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL&#39;, Le Paine et al 2018 (Original URL: https://arxiv.org/abs/1810.05017#deepmind )">MetaMimic</a> generalizes at hundreds of tasks to train a deep net. Disentanglement emerges in <a href="https://www.moderndescartes.com/docs/www/arxiv.org/4cb3118987e4ea896320737fe1a5bf959c722d04.pdf#nvidia" id="karras-et-al-2018" data-link-icon="n" data-link-icon-type="text,sans,italic" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1812.04948?fallback=original#nvidia" title="&#39;A Style-Based Generator Architecture for Generative Adversarial Networks&#39;, Karras et al 2018 (Original URL: https://arxiv.org/abs/1812.04948#nvidia )">StyleGAN</a> with sufficiently deep <em>w</em> embeddings, with enough parameters to train raw audio in the aforementioned Jukebox, or in <a href="https://www.moderndescartes.com/docs/www/arxiv.org/78fbc4214a090ff9f7503286e4b7b46831218c90.pdf#deepmind" id="santoro-et-al-2017" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1706.01427?fallback=original#deepmind" title="&#39;A simple neural network module for relational reasoning&#39;, Santoro et al 2017 (Original URL: https://arxiv.org/abs/1706.01427#deepmind )">relational networks</a>⁠/​<a href="https://www.moderndescartes.com/docs/reinforcement-learning/2018-eslami.pdf#deepmind" id="eslami-et-al-2018" data-link-icon="deepmind" data-link-icon-type="svg" title="&#39;Neural scene representation and rendering&#39;, Eslami et al 2018">GQN</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/84ece82d735c9be9b70a28c54c1ced8f397c81b1.pdf" id="clark-et-al-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2002.05867?fallback=original" title="&#39;Transformers as Soft Reasoners over Language&#39;, Clark et al 2020 (Original URL: https://arxiv.org/abs/2002.05867 )">Transformers</a> with enough samples to force factorization. (See also <a href="https://www.moderndescartes.com/docs/www/arxiv.org/31789cb844dc07b26ee0d475d3104a25769286a4.pdf#deepmind" id="hill-et-al-2019" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1910.00571?fallback=original#deepmind" title="Environmental drivers of systematicity and generalization in a situated agent (Original URL: https://arxiv.org/abs/1910.00571#deepmind )">Hill et al 2019</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/91d05cb4826475027d44c20888258e55f85d3a23.pdf" id="chaplot-et-al-2017" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1706.07230?fallback=original" title="Gated-Attention Architectures for Task-Oriented Language Grounding (Original URL: https://arxiv.org/abs/1706.07230 )">Chaplot et al 2017</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/8e88f06b266781da6426a0f860c19b28b7843953.pdf#baidu" id="yu-et-al-2018" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1802.01433?fallback=original#baidu" title="Interactive Grounded Language Acquisition and Generalization in a 2D World (Original URL: https://arxiv.org/abs/1802.01433#baidu )">Yu et al 2018</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/d3ff9252cbcf4fb7013fc3a5c31ad77eafd503a9.pdf" id="lake-2019" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1906.05381?fallback=original" title="Compositional generalization through meta sequence-to-sequence learning (Original URL: https://arxiv.org/abs/1906.05381 )">Lake 2019</a>⁠/​<a href="https://www.moderndescartes.com/docs/www/arxiv.org/3055f56297b0b66ccd0175272dbfadc114c47663.pdf#deepmind" id="abramson-et-al-2020" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2012.05672?fallback=original#deepmind" title="Imitating Interactive Intelligence (Original URL: https://arxiv.org/abs/2012.05672#deepmind )">Interactive Agents Group 2020</a>⁠.) Training <a href="https://www.moderndescartes.com/docs/www/arxiv.org/1422afb87785f06d672ee466e57d5c19d9f8bb85.pdf#openai" id="dactyl-paper" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1910.07113?fallback=original#openai" title="&#39;Solving Rubik&#39;s Cube With A Robot Hand&#39;, Akkaya et al 2019 (Original URL: https://arxiv.org/abs/1910.07113#openai )">Dactyl</a> on millions of domain randomizations induced similar implicit meta-learning where during each runtime invocation, the <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" data-link-icon="wikipedia" data-link-icon-type="svg" title="Recurrent neural network">RNN</a> probes its environment and encodes its understanding of robot hand control into its hidden state; and <a href="https://www.moderndescartes.com/docs/www/arxiv.org/803c87aa219812728e409a82d339fc3f89fb7f98.pdf#facebook" id="wijmans-et-al-2019" data-link-icon="facebook" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1911.00357?fallback=original#facebook" title="&#39;DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames&#39;, Wijmans et al 2019 (Original URL: https://arxiv.org/abs/1911.00357#facebook )">DD-PPO</a> outperforms classical robot planners by scaling 2 orders. Or in <a href="https://openai.com/blog/procgen-benchmark/" id="cobbe-et-al-2019" data-link-icon="openai" data-link-icon-type="svg" title="Procgen Benchmark: We&#39;re releasing Procgen Benchmark, 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills">Procgen</a> or <a href="https://distill.pub/2020/understanding-rl-vision/#diversity-hypothesis" id="hilton-et-al-2020" data-link-icon="distillpub" data-link-icon-type="svg" title="&#39;Understanding RL Vision&#39;, Hilton et al 2020">CoinRun</a>⁠, training on hundreds of levels trains agents to solve levels individually and worsens performance on other levels, but at thousands of levels, they begin to generalize to unseen levels. (Similarly, <a href="https://www.moderndescartes.com/docs/www/arxiv.org/bf1944c23d35395e5bc2bfce32024161eaa4062b.pdf#facebook" id="aghajanyan-et-al-2021" data-link-icon="facebook" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2101.11038?fallback=original#facebook" title="&#39;Muppet: Massive Multi-task Representations with Pre-Finetuning&#39;, Aghajanyan et al 2021 (Original URL: https://arxiv.org/abs/2101.11038#facebook )">language model pretraining-finetuning</a> overfits at small numbers of datasets but improves markedly with enough diversity.) <a href="https://www.moderndescartes.com/docs/reinforcement-learning/alphago/2018-silver.pdf#deepmind" id="silver-et-al-2018" data-link-icon="deepmind" data-link-icon-type="svg" title="&#39;A general reinforcement learning algorithm that masters chess, shogi and Go through self-play&#39;, Silver et al 2018">AlphaZero</a> demonstrated truly superhuman Go without ‘delusions’ just by training a bigger model on a richer signal &amp; pro-level play without any search—and <a href="https://www.moderndescartes.com/docs/www/arxiv.org/cc2fdbefb86a5a1261f586b54a869351918b3a80.pdf#deepmind" id="schrittwieser-et-al-2019" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1911.08265?fallback=original#deepmind" title="&#39;MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model&#39;, Schrittwieser et al 2019 (Original URL: https://arxiv.org/abs/1911.08265#deepmind )">MuZero</a>⁠, for that matter, demonstrated that just training an RNN end-to-end to predict a reward on enough data is enough to obsolete even AlphaZero and learn tree search implicitly (but better). And on and on. DM researcher <a href="https://www.lesswrong.com/posts/Wnqua6eQkewL3bqsF/matt-botvinick-on-the-spontaneous-emergence-of-learning" id="scholl-2020" data-link-icon="LW" data-link-icon-type="text" title="Matt Botvinick on the spontaneous emergence of learning algorithms">Matthew Botvinick</a>⁠, discussing their meta-reinforcement learning work where they were surprised to discover meta-learning emerging, and that it did so regardless of which specific architecture they used:</p>
            <blockquote>
              <p>“…it’s something that just happens. In a sense, you can’t avoid this happening. If you have a system that has memory, and the function of that memory is shaped by reinforcement learning, and this system is trained on a series of interrelated tasks, this is going to happen. You can’t stop it.”</p>
            </blockquote>
            <p>Pace <a href="https://www.moderndescartes.com/docs/ai/scaling/1995-breiman.pdf" id="breiman-1995" data-link-icon="pdf" data-link-icon-type="svg" title="Reflections After Refereeing Papers for NIPS">Breiman</a>⁠, <strong>why</strong>? Why do they transfer and generalize? Why do these blessings of scale exist? Why do we need to train large models when small models provably exist with the same performance? Why do larger models not overfit (though they <a href="https://www.moderndescartes.com/docs/www/arxiv.org/a9a93a2fa9957314b5cbb9138b38ec9a9aaee0ae.pdf#google" id="zhang-et-al-2016" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1611.03530?fallback=original#google" title="&#39;Understanding deep learning requires rethinking generalization&#39;, Zhang et al 2016 (Original URL: https://arxiv.org/abs/1611.03530#google )">can</a>) and generalize better than smaller models? What’s up with the whole <a href="https://openai.com/blog/deep-double-descent/" id="nakkiran-et-al-2019" data-link-icon="openai" data-link-icon-type="svg" title="Deep Double Descent: We show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time. This effect is often avoided through careful regularization. While this behavior appears to be fairly universal, we don&#39;t yet fully understand why it happens, and view further study of this phenomenon as an important research direction.">‘double descent’</a> anyway?</p>
            <p>These are all, ahem, deep questions about neural networks and heavily debated, but right now, I would suggest that the answer lies in some mix of the model compression/​distillation, <a href="https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks/" id="morcos-tian-2019" data-link-icon="facebook" data-link-icon-type="svg" title="Understanding the generalization of &#39;lottery tickets&#39; in neural networks">‘lottery ticket hypothesis’</a>⁠, <a href="https://www.moderndescartes.com/docs/www/arxiv.org/aea56633ef88d71aadfb45b62bb1a218cda38102.pdf" id="wilson-izmailov-2020" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2002.08791?fallback=original" title="&#39;Bayesian Deep Learning and a Probabilistic Perspective of Generalization&#39;, Wilson &amp; Izmailov 2020 (Original URL: https://arxiv.org/abs/2002.08791 )">Bayesian neural network</a>⁠, and <a href="https://www.moderndescartes.com/docs/www/arxiv.org/98ad3320bdb3cbab2681220ed6b4dc44c455f14d.pdf#google" id="roeder-et-al-2020" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2007.00810?fallback=original#google" title="&#39;On Linear Identifiability of Learned Representations&#39;, Roeder et al 2020 (Original URL: https://arxiv.org/abs/2007.00810#google )">learned representation</a> (like <a href="https://distill.pub/2020/circuits/zoom-in/#openai" id="olah-et-al-2020" data-link-icon="openai" data-link-icon-type="svg" title="&#39;Zoom In: An Introduction to Circuits: By studying the connections between neurons, we can find meaningful algorithms in the weights of neural networks&#39;, Olah et al 2020">circuits</a>) literatures.</p>
            <p>Big models work because they encode a dizzyingly vast number of sub-models in an extremely <a href="https://www.moderndescartes.com/docs/www/colah.github.io/2d2a03bce5f6de8b48799f894aee58479e2a9f48.html" id="olah-2014" rel="archived alternate nofollow" data-url-original="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" title="&#39;Neural Networks, Manifolds, and Topology&#39;, Olah 2014 (Original URL: https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ )">high-dimensional</a> abstract space, representing countless small sub-models (<a href="https://www.moderndescartes.com/docs/www/arxiv.org/30ce90addac851033c6e3521cc4ca59d43093066.pdf#deepmind" id="orseau-et-al-2020" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2006.12156?fallback=original#deepmind" title="Logarithmic Pruning is All You Need (Original URL: https://arxiv.org/abs/2006.12156#deepmind )">Orseau et al 2020</a>) <a href="https://www.moderndescartes.com/docs/ai/scaling/2020-hasson.pdf" id="hasson-et-al-2020" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks&#39;, Hasson et al 2020">interpolating over data</a>⁠, one of which is likely to solve the problem well, and so ensures the problem is soluble by the overall model. They function as an ensemble: even though there countless overfit sub-models inside the single big model, they all average out, leading to a preference for simple solutions. This Occam’s razor biases the model towards simple solutions which are flexible enough to gradually expand in complexity to match the data.</p>
            <p>However, “neural nets are lazy”: sub-models which memorize pieces of the data, or latch onto superficial features, learn quickest and are the easiest to represent internally. If the model &amp; data &amp; compute are not big or varied enough, the optimization, by the end of the cursory training, will have only led to a sub-model which achieves a low loss but missed important pieces of the desired solution.</p>
            <p>On the other hand, for a model like GPT-3, it is sufficiently powerful a model that its sub-models can do anything from poetry to arithmetic, and it is trained on so much data that those superficial models may do well early on, but gradually fall behind more abstract models; a sub-model which memorizes some of the data is indeed much simpler than a sub-model which encodes genuine arithmetic (a NN can probably memorize tens of thousands of lookup table entries storing examples of addition in the space it would take to encode an abstract algorithm like ‘addition’), but it can’t possibly memorize <em>all</em> the instances of arithmetic (implicit or explicit) in GPT-3’s Internet-scale dataset. If a memorizing sub-model tried to do so, it would become extremely large and penalized. Eventually, after enough examples and enough updates, there may be a phase transition (<a href="https://www.moderndescartes.com/docs/www/arxiv.org/658b4a13863e88e856c6cfcf686a5e9eb01776b3.pdf#page=22" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://arxiv.org/pdf/2103.10948.pdf#page=22" title="The Shape of Learning Curves: a Review: 6. Ill-behaved learning curves: 6.1. Phase transitions (Original URL: https://arxiv.org/pdf/2103.10948.pdf#page=22 )">Viering &amp; Loog 2021</a>), and the simplest ‘arithmetic’ model which accurately predicts the data just <em>is</em> arithmetic. And then the meta-learning, after seeing enough instances of algorithms which vary slightly within each sample, making it hard to learn each task separately, just <em>is</em> learning of more generic algorithms, yielding sub-models which achieve lower loss than the rival sub-models, which either fail to predict well or bloat unacceptably. (GPT-2-1.5b apparently was too small or shallow to ensemble easily over sub-models encoding meta-learning algorithms, or perhaps not trained long enough on enough data to locate the meta-learner models; GPT-3 was.)</p>
            <p>So, the larger the model, the better, if there is enough data &amp; compute to push it past the easy convenient sub-models and into the sub-models which express desirable traits like generalizing, factorizing perception into meaningful <a href="https://en.wikipedia.org/wiki/Latent_variable" data-link-icon="wikipedia" data-link-icon-type="svg" title="Latent variable">latent</a> dimensions, meta-learning tasks based on descriptions, learning causal reasoning &amp; logic, and so on. If the ingredients are there, it’s going to happen.</p>
          </section>
          <section id="scaling-hypothesis">
            <h2><a href="#scaling-hypothesis" title="Link to section: § &#39;Scaling Hypothesis&#39;">Scaling Hypothesis</a></h2>
            <p>The strong <em>scaling hypothesis</em> is that, once we find a scalable architecture like self-attention or convolutions, which like the brain can be applied fairly uniformly (eg. <a href="https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine" id="cannell-2015" data-link-icon="LW" data-link-icon-type="text" title="&#39;The Brain as a Universal Learning Machine&#39;, Cannell 2015">“The Brain as a Universal Learning Machine”</a> or Hawkins), we can simply train ever larger NNs and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks &amp; data. More powerful NNs are ‘just’ scaled-up weak NNs, in much the same way that human brains look much like <a href="https://www.moderndescartes.com/docs/psychology/neuroscience/2012-herculanohouzel.pdf" id="herculano-houzel-2012" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;The remarkable, yet not extraordinary, human brain as a scaled-up primate brain and its associated cost&#39;, Herculano-Houzel 2012">scaled-up primate brains</a>⁠. While I was highly skeptical of scaling hypothesis advocates when I first became interested in AI 2004–2010 (back when AI was stuck in the doldrums of hopelessly narrow tools and dates like 2028 seemed impossibly far away), which smacked of numerology and “if you build it they will come” logic (at the time, we certainly didn’t have general algorithms that you could just throw compute at), in 2020, I have to admit, I was wrong and they were right. We built the compute, and the algorithms <em>did</em> come, and the scaling hypothesis has only looked more and more plausible every year since 2010.</p>
          </section>
        </section>
        <section id="why-does-pretraining-work">
          
          <p>The pretraining thesis goes something like this:</p>
          <figure>
            <img alt="“Figure 1: Envisioned evolution of NLP research through three different eras or curves” (the hypothetical S-curves &amp; progress in natural language modeling; from Cambria &amp; White 2014)" decoding="async" height="775" loading="lazy" sizes="(max-width: 768px) 100vw, 1154px" src="https://www.moderndescartes.com/images/ai/2014-cambria-figure1-hypotheticalnlpprogresscurves.png" srcset="/images/ai/2014-cambria-figure1-hypotheticalnlpprogresscurves.png-768px.png 768w, /images/ai/2014-cambria-figure1-hypotheticalnlpprogresscurves.png 1154w" width="1154"/>
            <figcaption aria-hidden="true">
              “Figure 1: Envisioned evolution of NLP research through three different eras or curves” (the hypothetical S-curves &amp; progress in natural language modeling; from <a href="https://www.moderndescartes.com/docs/ai/scaling/2014-cambria.pdf" id="cambria-white-2014" data-link-icon="pdf" data-link-icon-type="svg" title="Jumping NLP Curves: A Review of Natural Language Processing Research">Cambria &amp; White 2014</a>)
            </figcaption>
          </figure>
          <p>Humans, one might say, are the <a href="https://en.wikipedia.org/wiki/Great_Oxidation_Event" data-link-icon="wikipedia" data-link-icon-type="svg" title="Great Oxidation Event">cyanobacteria of AI</a>: we constantly emit large amounts of structured data, which implicitly rely on logic, causality, object permanence, history—all of that good stuff. All of that is implicit and encoded into our writings and videos and ‘data exhaust’. A model learning to predict must learn to understand all of that to get the best performance; as it predicts the easy things which are mere statistical pattern-matching, what’s left are the hard things. AI critics often say that the long tail of scenarios for tasks like self-driving cars or natural language can only be solved by true generalization &amp; reasoning; it follows then that if models solve the long tail, they must learn to generalize &amp; reason.</p>
          <p>Early on in training, a model learns the crudest levels: that some letters like ‘e’ are more frequent than others like ‘z’, that every 5 characters or so there is a space, and so on. It goes from predicted uniformly-distributed bytes to what looks like Base-60 encoding—alphanumeric gibberish. As crude as this may be, it’s enough to make quite a bit of absolute progress: a random predictor needs 8 bits to ‘predict’ a byte/​character, but just by at least matching letter and space frequencies, it can almost halve its error to around 5 bits.<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a> Because it is learning so much from every character, and because the learned frequencies are simple, it can happen so fast that if one is not logging samples frequently, one might not even observe the improvement.</p>
          <p>As training progresses, the task becomes more difficult. Now it begins to learn what words actually exist and do not exist. It doesn’t know anything about meaning, but at least now when it’s asked to predict the second half of a word, it can actually do that to some degree, saving it a few more bits. This takes a while because any specific instance will show up only occasionally: a word may not appear in a dozen samples, and there are many thousands of words to learn. With some more work, it has learned that punctuation, pluralization, possessives are all things that exist. Put that together, and it may have progressed again, all the way down to 3–4 bits error per character! (While the progress is gratifyingly fast, it’s still all gibberish, though, makes no mistake: a sample may be spelled correctly, but it doesn’t make even a bit of sense.)</p>
          <p>But once a model has learned a good English vocabulary and correct formatting/​spelling, what’s next? There’s not much juice left in predicting within-words. The next thing is picking up associations among words. What words tend to come first? What words ‘cluster’ and are often used nearby each other? Nautical terms tend to get used a lot with each other in sea stories, and likewise Bible passages, or American history Wikipedia article, and so on. If the word “Jefferson” is the last word, then “Washington” may not be far away, and it should hedge its bets on predicting that ‘W’ is the next character, and then if it shows up, go all-in on “ashington”. Such bag-of-words approaches still predict badly, but now we’re down to perhaps &lt;3 bits per character.</p>
          <p>What next? Does it stop there? Not if there is enough data and the earlier stuff like learning English vocab doesn’t hem the model in by using up its learning ability. Gradually, other words like “President” or “general” or “after” begin to show the model subtle correlations: “Jefferson was President after…” With many such passages, the word “after” begins to serve a use in predicting the next word, and then the use can be broadened.</p>
          <p>By this point, the loss is perhaps 2 bits: every additional 0.1 bit decrease comes at a steeper cost and takes more time. However, now the sentences have started to make sense. A sentence like “Jefferson was President after Washington” does in fact mean something (and if occasionally we sample “Washington was President after Jefferson”, well, what do you expect from such an un-converged model). Jarring errors will immediately jostle us out of any illusion about the model’s understanding, and so training continues. (Around here, Markov chain &amp; <em>n</em>-gram models start to fall behind; they can memorize increasingly large chunks of the training corpus, but they can’t solve increasingly critical syntactic tasks like balancing parentheses or quotes, much less start to ascend from syntax to semantics.)</p>
          <p>Now training is hard. Even subtler aspects of language must be modeled, such as keeping pronouns consistent. This is hard in part because the model’s errors are becoming rare, and because the relevant pieces of text are increasingly distant and ‘long-range’. As it makes progress, the absolute size of errors shrinks dramatically. Consider the case of associating names with gender pronouns: the difference between “Janelle ate some ice cream, because he likes sweet things like ice cream” and “Janelle ate some ice cream, because she likes sweet things like ice cream” is one no human could fail to notice, and yet, it is a difference of a single letter. If we compared two models, one of which didn’t understand gender pronouns at all and guessed ‘he’/​‘she’ purely at random, and one which understood them perfectly and always guessed ‘she’, the second model would attain a lower average error of barely &lt;0.02 bits per character!</p>
          <p>Nevertheless, as training continues, these problems and more, like imitating genres, get solved, and eventually at a loss of 1–2 (where a small char-RNN might converge on a small corpus like Shakespeare or some Project Gutenberg ebooks), we will finally get samples that sound human—at least, for a few sentences. These final samples may convince us briefly, but, aside from issues like repetition loops, even with good samples, the errors accumulate: a sample will state that someone is “alive” and then 10 sentences later, use the word “dead”, or it will digress into an irrelevant argument instead of the expected next argument, or someone will do something physically improbable, or it may just continue for a while without seeming to <em>get</em> anywhere.</p>
          <p>All of these errors are far less than &lt;0.02 bits per character; we are now talking not hundredths of bits per characters but less than ten-thousandths.</p>
          <p>The pretraining thesis argues that this can go even further: we can compare this performance directly with humans doing the same objective task, who can achieve closer to <a href="https://www.moderndescartes.com/Differences#efficient-natural-languages" id="gwern-differences-efficient-natural-languages">0.7 bits per character</a>⁠. What is in that missing &gt;0.4?</p>
          <figure>
            <img alt="“Yeah, but there’s more to being smart than knowing compression schemes!” “No there’s not!” “Shoot—he knows the secret!!”" decoding="async" height="500" loading="lazy" src="https://www.moderndescartes.com/images/cs/2004-ryannorth-dinosaurcomics-391.png" title="https://qwantz.com/index.php?comic=354" width="735"/>
            <figcaption aria-hidden="true">
              “Yeah, but there’s more to being smart than knowing compression schemes!” “No there’s not!” “Shoot—he knows the secret!!”
            </figcaption>
          </figure>
          <p>Well—<em>everything</em>! Everything that the model misses. While just babbling random words was good enough at the beginning, at the end, it needs to be able to reason our way through the most difficult textual scenarios requiring causality or commonsense reasoning. Every error where the model predicts that ice cream put in a freezer will “melt” rather than “freeze”, every case where the model can’t keep straight whether a person is alive or dead, every time that the model chooses a word that doesn’t help build somehow towards the ultimate conclusion of an ‘essay’, every time that it lacks the theory of mind to compress novel scenes describing the Machiavellian scheming of a dozen individuals at dinner jockeying for power as they talk, every use of logic or abstraction or instructions or Q&amp;A where the model is befuddled and needs more bits to cover up for its mistake where a human would think, understand, and predict. Each of these cognitive breakthroughs allows ever so slightly better prediction of a few relevant texts; nothing less than true understanding will suffice for ideal prediction.</p>
          <p>If we trained a model which reached that loss of &lt;0.7, which could predict text indistinguishable from a human, whether in a dialogue or quizzed about ice cream or being tested on SAT analogies or tutored in mathematics, if for every string the model did just as good a job of predicting the next character as you could do, how could we say that it doesn’t <em>truly</em> understand everything? (If nothing else, we could, by definition, replace humans in any kind of text-writing job!)</p>
          <p><span>The last bits are deepest.</span> The implication here is that the final few bits are the most valuable bits, which require the most of what we think of as intelligence. A helpful analogy here might be our actions: for the most part, all humans execute actions equally well. We all pick up a tea mug without dropping, and can lift our legs to walk down thousands of steps without falling even once. For everyday actions (the sort which make up most of a corpus), anybody, of any intelligence, can get enough practice &amp; feedback to do them quite well. Where individuals differ is when they start running into the long tail of novel choices, rare choices, choices that take seconds but unfold over a lifetime, choices where we will never get any feedback (like after our death). One only has to make a single bad decision, out of a lifetime of millions of discrete decisions, to wind up in jail or dead. A small absolute average improvement in decision quality, if it is in <em>those</em> decisions, may be far more important than its quantity indicates, and give us some intutition for why those last bits are the hardest/​deepest. (Why do humans have such large brains, when animals like chimpanzees do so many ordinary activities seemingly as well with a fraction of the expense? Why is language worthwhile? Perhaps because of considerations like these. We may be at our most human while filling out the paperwork for life insurance.)</p>
          <p><span>Reasons for doubt.</span> The pretraining thesis, while logically impeccable—how is a model supposed to solve all possible trick questions without understanding, just <em>guessing</em>?—never struck me as convincing, an argument admitting neither confutation nor conviction. It feels too much like a magic trick: “here’s some information theory, here’s a human benchmark, here’s how we can encode all tasks as a sequence prediction problem, hey presto—Intelligence!” There are lots of algorithms which are Turing-complete or ‘universal’ in some sense; there are lots of algorithms like <a href="https://www.lesswrong.com/tag/aixi" data-link-icon="LW" data-link-icon-type="text">AIXI</a> which solve AI in some theoretical sense (Schmidhuber &amp; company have many of these cute algorithms such as ‘the fastest possible algorithm for all problems’, with the minor catch of some constant factors which require computers bigger than the universe).</p>
          <p>Why think pretraining or sequence modeling is not another one of them? Sure, <em>if</em> the model got a low enough loss, it’d have to be intelligent, but how could you prove that would happen in practice? (Training char-RNNs was fun, but they hadn’t exactly revolutionized deep learning.) It might require more text than exists, countless petabytes of data for all of those subtle factors like logical reasoning to represent enough training signal, amidst all the noise and distractors, to train a model. Or maybe your models are too small to do more than absorb the simple surface-level signals, and you would have to scale them 100 orders of magnitude for it to work, because the scaling curves didn’t cooperate. Or maybe your models are fundamentally broken, and stuff like abstraction require an entirely different architecture to work at all, and whatever you do, your current models will saturate at poor performance. Or it’ll train, but it’ll spend all its time trying to improve the surface-level modeling, absorbing more and more literal data and facts without ever ascending to the higher planes of cognition as planned. Or…</p>
          <div>
            <blockquote>
              <p>‘The possibilities of developing an atomic weapon and the desirability of doing it secretly were discussed at a Princeton University conference in which I participated in March 1939…<a href="https://en.wikipedia.org/wiki/Niels_Bohr" data-link-icon="wikipedia" data-link-icon-type="svg" title="Niels Bohr">Bohr</a> said this rare variety could not be separated from common uranium except by turning the country into a gigantic factory. Bohr was worried that this could be done and that an atomic bomb could be developed—but he hoped that neither could be accomplished. Years later, when Bohr came to Los Alamos, I was prepared to say, “You see . . .” But before I could open my mouth, he said: <strong>“You see, I told you it couldn’t be done without turning the whole country into a factory. You have done just that.”</strong>’</p>
              <p><a href="https://en.wikipedia.org/wiki/Edward_Teller" data-link-icon="wikipedia" data-link-icon-type="svg" title="Edward Teller">Edward Teller</a><a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
            </blockquote>
          </div>
          <p>But apparently, it would’ve worked fine. Even RNNs probably would’ve worked—Transformers are nice, but they seem mostly be about efficiency.<a href="#fn17" id="fnref17" role="doc-noteref"><sup>17</sup></a> (Training large RNNs is much more expensive, and doing BPTT over multiple nodes is much harder engineering-wise.) It just required more compute &amp; data than anyone was willing to risk on it until a few true-believers were able to get their hands on a few million dollars of compute.</p>
          <ol type="1">
            <li>
              <p><strong>Q:</strong> Did anyone predict, quantitatively, that this would happen where it did?</p>
              <p><strong>A:</strong> Not that I know of.</p>
            </li>
            <li>
              <p><strong>Q:</strong> What would future scaled-up models learn?</p>
              <p>GPT-2-1.5b had a <a href="https://en.wikipedia.org/wiki/Cross_entropy" data-link-icon="wikipedia" data-link-icon-type="svg" title="Cross entropy">cross-entropy</a> WebText validation loss of ~3.3 (based on the perplexity of ~10 in <a href="https://www.moderndescartes.com/images/ai/gpt/2019-radford-figure4-gpt2validationloss.png" data-link-icon="image" data-link-icon-type="svg" data-image-height="863" data-image-width="936" title="Figure 4: The performance of LMs trained on WebText as a function of model size (from https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf#page=9)">Figure 4</a>⁠, and log<sub>2</sub>(10) = 3.32). GPT-3 halved that loss to ~1.73 judging from <a href="https://www.moderndescartes.com/images/ai/gpt/2020-brown-figure31-gpt3scaling.png" data-link-icon="image" data-link-icon-type="svg" data-image-height="845" data-image-width="1400" title="Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in Kaplan et al 2020 continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts. (Brown et al 2020). Cross-validation loss extrapolation: $L(oss) = 2.57 · C(ompute in petaflop-s/days) ^ −0.048$">Brown et al 2020</a> and using the scaling formula (2.57 × (3.64 × 10<sup>3</sup>)<sup>−0.048</sup>). For a hypothetical GPT-4, if the scaling curve continues for another 3 orders or so of compute (100–1000×) before crossing over and hitting harder <a href="https://en.wikipedia.org/wiki/Diminishing_returns" data-link-icon="wikipedia" data-link-icon-type="svg" title="Diminishing returns">diminishing returns</a>⁠, the cross-entropy loss will drop to ~1.24 (2.57 × (3.64 × (10<sup>3</sup> × 10<sup>3</sup>))<sup>−0.048</sup>).</p>
              <p>If GPT-3 gained so much meta-learning and world knowledge by dropping its absolute loss ~50% when starting from GPT-2’s level, what capabilities would another ~30% improvement over GPT-3 gain? (Cutting the loss that much would still not reach human-level, as far as I can tell.<a href="#fn18" id="fnref18" role="doc-noteref"><sup>18</sup></a>) What would a drop to ≤1, perhaps using wider context windows or recurrency, gain?</p>
              <p><strong>A:</strong> I don’t know.</p>
            </li>
            <li>
              <p><strong>Q:</strong> Does anyone?</p>
              <p><strong>A:</strong> Not that I know of.<a href="#fn19" id="fnref19" role="doc-noteref"><sup>19</sup></a></p>
            </li>
          </ol>
        </section>
        <section id="prospects">
          
          <div>
            <blockquote>
              <p>In the problem of decoding, the most important information which we can possess is the knowledge that the message which we are reading is not gibberish…In a similar way, when we consider a problem of nature such as that of atomic reactions and atomic explosives, the largest single item of information which we can make public is that they exist. Once a scientist attacks a problem which he knows to have an answer, his entire attitude is changed. He is already some 50% of his way toward that answer…<strong>the one secret concerning the atomic bomb which might have been kept and which was given to the public and to all potential enemies without the least inhibition, was that of the possibility on its construction.</strong> Take a problem of this importance and assure the scientific world that it has an answer; then both the intellectual ability of the scientists and the existing laboratory facilities are so widely distributed that the quasi-independent realization of the task will be a matter of merely a few years anywhere in the world.</p>
              <p><a href="https://en.wikipedia.org/wiki/Norbert_Wiener" data-link-icon="wikipedia" data-link-icon-type="svg" title="Norbert Wiener">Norbert Wiener</a>⁠, pg124–125, <em><a href="https://en.wikipedia.org/wiki/The_Human_Use_of_Human_Beings" data-link-icon="wikipedia" data-link-icon-type="svg" title="The Human Use of Human Beings">The Human Use of Human Beings</a></em> (emphasis added)</p>
            </blockquote>
          </div>
          <div>
            <blockquote>
              <p>“People who work in machine learning simply didn’t think that neural networks could do much. People didn’t believe large neural networks could be trained…The ideas were all there, the thing that was missing was a lot of supervised data and a lot of compute. Once you have [those two], then there is a third thing is that is needed—and that is <em>conviction</em>. Conviction that if you take the right stuff, which already exists, and apply and mix it with a lot of data and a lot of compute, that it will in fact work. And so that was the missing piece.”</p>
              <p><a href="https://www.youtube.com/13CZPWmke6A?t=950#org=openai" data-link-icon="openai" data-link-icon-type="svg" title="Ilya Sutskever: Deep Learning | Lex Fridman Podcast #94">Ilya Sutskever</a><a href="#fn20" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
            </blockquote>
          </div>
          <p>What can we expect from future DL work? Will GPT-3 kickstart an arms race where soon we will be discussing, blase, what would seem now like ludicrously farfetched schemes like bidirectional multimodal Transformer 100× the size trained on 100× the data (video/​text/​PDFs-as-images/​photo/​robotics) with supplementary supervised learning as the backbone of a MuZero-like learning+planning DRL agent running on thousands of tasks (such as coding) simultaneously?</p>
          <p>The existence of <a href="https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang" id="jones-2020" data-link-icon="LW" data-link-icon-type="text" title="&#39;Are we in an AI overhang?&#39;, Jones 2020">the hardware overhang</a> implies that the limiting factor here is less hardware than human: will any organization treat GPT-3 as a Sputnik moment and invest aggressively in scaling programs? Is there a GPT-4-equivalent brewing away inside DeepMind or Google Brain’s <a href="https://www.moderndescartes.com/docs/ai/scaling/hardware/2020-jouppi.pdf#google" id="jouppi-et-al-2020" data-link-icon="google" data-link-icon-type="svg" title="&#39;A domain-specific supercomputer for training deep neural networks&#39;, Jouppi et al 2020">TPU</a> pods now? They aren’t stupid, they have the hardware, they have the budgets, they have the people.</p>
          <p>But I think they lack a vision. As far as I can tell: they do not have any such thing, because Google Brain &amp; DeepMind do not believe in the scaling hypothesis the way that Sutskever, Amodei and others at OA do. Just read through machine learning Twitter to see the disdain for the scaling hypothesis. (A quarter year on from GPT-3 and counting, can you name a single dense model as large as the 17b Turing-NLG—never mind larger than GPT-3?)</p>
          <p>Google Brain is entirely too practical and short-term focused to dabble in such esoteric &amp; expensive speculation, although Quoc V. Le’s group occasionally surprises you. They’ll dabble in <a href="https://www.moderndescartes.com/docs/ai/scaling/moe/index" title="&#39;AI/MoE directory&#39;, N/A 2022">mixture-of-expert models</a> like <a href="https://www.moderndescartes.com/docs/www/arxiv.org/dfa5f44358e0ba7fab6aa08795cf49faedcdf141.pdf#google" id="lepikhin-et-al-2020" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/2006.16668?fallback=original#google" title="&#39;GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding&#39;, Lepikhin et al 2020 (Original URL: https://arxiv.org/abs/2006.16668#google )">GShard</a>⁠, but mostly because they expect to be likely to be able to deploy it or something like it to production in Google Translate.</p>
          <p>DeepMind<a href="#fn21" id="fnref21" role="doc-noteref"><sup>21</sup></a> holds what we might call the “weak scaling hypothesis”: they believe that AGI will require us to “find the right algorithms” effectively replicating a mammalian brain module by module, and that while these modules will be extremely large &amp; expensive by contemporary standards (which is why compute is important, to give us “a more powerful tool with which to hunt for the right algorithms”), they still need to be invented &amp; finetuned piece by piece, with little risk or surprise until the final assembly. Each piece, however, itself can scale: there’s no magical intelligence gland or quantum woo which creates a bright line between humans and, say, chimpanzees or rodents. (As much as we humans extravagantly admire our own capabilities like language or logic, those are relatively minor flourishes on the basic brain—each organism solves the same basic problems, like exploration, long-term memory, learning world-models, associating rewards with specific actions, meta-learning, etc.) As such, once you have a rat-level AGI, a human-level AGI is just more so. (And rats are a lot easier to experiment on.) That is how you get DM contraptions like <a href="https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark" id="puigdomènech-et-al-2020" data-link-icon="deepmind" data-link-icon-type="svg" title="&#39;Agent57: Outperforming the Atari Human Benchmark&#39;, Badia et al 2020">Agent57</a> which throw the kitchen sink at the wall to see what sticks, and why they place such emphasis on neuroscience as inspiration and cross-fertilization for reverse-engineering the brain. (See also Sam Altman’s <a href="https://audio.hbr.org/exponential-view/20201006152648-S5E01_HowGPT-3IsShapingOurAIFuture.mp3?listeningSessionID=0CD_382_23__53c57875ba360c6182f72cc891a037ef695389e2#t=2205" data-link-icon="audio" data-link-icon-type="svg" title="&#39;How GPT-3 Is Shaping Our AI Future&#39; with Sam Altman/Azeem Azhar (The Exponential View), Wednesday 7 October 2020">podcast interview comments</a> on OA’s advantage vs unnamed rivals with more compute is because the lack of compute makes them stay “small and focused”—“for sure” like a startup approach.) When someone seems to have come up with a scalable architecture for cracking a hard problem, like AlphaZero or AlphaStar, they are willing to pour on the gas to make it scale, but otherwise, incremental refinement on ALE and then <a href="https://www.moderndescartes.com/docs/www/arxiv.org/a0587c147d53bd579bf2b1444ac70c450ecbe73c.pdf#deepmind" id="beattie-et-al-2016" data-link-icon="deepmind" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1612.03801?fallback=original#deepmind" title="&#39;DeepMind Lab&#39;, Beattie et al 2016 (Original URL: https://arxiv.org/abs/1612.03801#deepmind )">DMLab-30</a> is the game plan. They have been biting off and chewing pieces of the brain for a decade, and it’ll probably take another decade or two of steady chewing if all goes well. Because they have locked up so much talent and have so much proprietary code and believe all of that is a major moat to any competitor trying to replicate the complicated brain, they are fairly easygoing. You will not see DM ‘bet the company’ on any moonshot; Google’s cashflow isn’t going anywhere (and <a href="https://www.moderndescartes.com/newsletter/2020/06#deepmind-budget" id="gwern-newsletter-2020-06-deepmind-budget" data-link-icon="deepmind" data-link-icon-type="svg">DM’s budget</a>), and slow and steady wins the race.</p>
          <p>Going beyond that, most other research labs like Tesla or FAIR are irrelevant and uninterested. Chinese AI companies are a question mark: past the language barrier, I seem to discern interest in AGI &amp; little of the reflexive Western opposition, and companies like Baidu occasionally release important research (such as the early scaling paper <a href="https://www.moderndescartes.com/docs/www/arxiv.org/60f4f6d336fc296f79b51aba2bb7f47ff7acbdb9.pdf#baidu" id="hestness-et-al-2017" data-link-icon="𝛘" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1712.00409?fallback=original#baidu" title="Deep Learning Scaling is Predictable, Empirically (Original URL: https://arxiv.org/abs/1712.00409#baidu )">Hestness et al 2017</a>), but overall, Chinese AI may be overestimated, and they seem to suffer from a kind of Dutch disease—funding for surveillance technology, and for narrow e-commerce niches, is so plentiful that other areas are neglected.</p>
          <p>OA, lacking anything like DM’s long-term funding from Google or its enormous headcount, is making a startup-like bet that they know an important truth which is a secret: “the scaling hypothesis is true!” So, simple DRL algorithms like PPO on top of large simple architectures like RNNs or Transformers can emerge, exploiting the blessings of scale, and meta-learn their way to powerful capabilities, enabling further funding for still more compute &amp; scaling, in a virtuous cycle. This is why OA had to revise its corporate form: lacking any enormous endowment or extremely deep-pocketed patron like Google, where does it get the money to scale (or hire machine learning engineer/​researchers who can command salaries in the millions)? OA has to <em>earn</em> the necessary money, so in a move like Mozilla Foundation owning Mozilla Corporation (to sell Firefox search engine placement), or the Hershey orphanage owning Hershey Chocolate or the Girl Scouts licensing their cookies, OpenAI switched from a pure nonprofit funded by donations to a nonprofit which owns a for-profit subsidiary/​startup, “OpenAI LP”, which can take investments and engage in for-profit activities. OA LP, while controlled by OA, can then shoot for the moon. And if OA is wrong to trust in the <a href="https://slatestarcodex.com/2018/11/26/is-science-slowing-down-2/" id="alexander-2018" data-link-icon="SSC" data-link-icon-type="text,tri" title="Is Science Slowing Down?">God of Straight Lines On Graphs</a>⁠, well, they never could compete with DM directly using DM’s favored approach, and were always going to be an also-ran footnote, so they have no regret.</p>
          <p>While all of this hypothetically can be replicated <em>relatively</em> easily (never underestimate the amount of tweaking and special sauce it takes) by competitors if they wished (the necessary amounts of compute budgets are still trivial in terms of Big Science or other investments like AlphaGo or AlphaStar or Waymo, after all), said competitors lack the very most important thing, which no amount of money or GPUs can ever cure: the courage of their convictions. They are too hidebound and deeply philosophically wrong to ever admit fault and try to overtake OA until it’s too late. How can we talk seriously about any kind of military Manhattan Project when the US military <a href="https://www.moderndescartes.com/docs/www/warontherocks.com/fd1ea16f6c32503a68269a38b235095fad81b3e0.html" data-link-icon="WOTR" data-link-icon-type="text,quad,sans" rel="archived alternate nofollow" data-url-original="https://warontherocks.com/2020/10/trust-algorithms-the-army-doesnt-even-trust-its-own-ai-developers/" title="Trust Algorithms? The Army Doesn’t Even Trust Its Own AI Developers (Original URL: https://warontherocks.com/2020/10/trust-algorithms-the-army-doesnt-even-trust-its-own-ai-developers/ )">doesn’t even let its developers use Tensorflow or PyTorch</a>⁠, or about government projects in the shadow of coronavirus? This might seem absurd (surely the Bitter Lesson/​scaling hypothesis have now earned enough prior probability to be taken seriously and receive major research investments to test how far they can go, especially given how important the implications are), but look at the repeated criticism of OA <em>every time</em> they release a new example of the scaling hypothesis, from GPT-1 to <a href="https://openai.com/blog/learning-dexterity/" data-link-icon="openai" data-link-icon-type="svg">Dactyl</a> to OA5 to GPT-2 to iGPT to GPT-3… To paraphrase St Augustine, most peoples’ reaction to the Bitter Lesson or scaling hypothesis is “grant me scale &amp; compute—but not yet”.<a href="#fn22" id="fnref22" role="doc-noteref"><sup>22</sup></a></p>
          <p>A critical indicator will be whether organizations beyond ‘the usual suspects’ (Microsoft <a href="https://www.moderndescartes.com/docs/www/www.microsoft.com/52910c912dc887bd735617b0892267bef7fa9e40.html" id="team-2020" data-link-icon="MS" data-link-icon-type="text,sans,italic" rel="archived alternate nofollow" data-url-original="https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/" title="&#39;ZeRO-2 &amp; DeepSpeed: Shattering barriers of deep learning speed &amp; scale&#39;, Team 2020 (Original URL: https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/ )">ZeRO-2</a> team has reached <a href="https://www.moderndescartes.com/docs/www/www.microsoft.com/9c296826f3b501d40c1c3ae000a24b09890d8fc5.html" id="team-et-al-2020" data-link-icon="MS" data-link-icon-type="text,sans,italic" rel="archived alternate nofollow" data-url-original="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/" title="&#39;DeepSpeed: Extreme-scale model training for everyone&#39;, Team et al 2020 (Original URL: https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/ )">1t-scale training</a>⁠, but there is also Nvidia, Salesforce, Allen, Google DM/​GB, Connor/​EleutherAI, Facebook FAIR) start participating or if they continue to dismiss scaling. At least as of 2020-10-26, 152 days later, no model has come near GPT-3, and indeed, no model has even exceeded Turing-NLG’s 17b.<a href="#fn23" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
        </section>
        <section id="critiquing-the-critics">
          
          <p><span>Keeping track.</span> GPT-3 in 2020 makes as good a point as any to take a look back on the past decade. It’s remarkable to reflect that someone who started a PhD because they were excited by these new “ResNets” would still not have finished it by now—that is how recent even <a href="https://www.moderndescartes.com/docs/www/arxiv.org/bc867f0b0ef2f4393a37068883c7d3bd4b5dc45e.pdf#microsoft" id="he-et-al-2015" data-link-icon="MS" data-link-icon-type="text,sans,italic" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1512.03385?fallback=original#microsoft" title="&#39;Deep Residual Learning for Image Recognition&#39;, He et al 2015 (Original URL: https://arxiv.org/abs/1512.03385#microsoft )">resnets</a> are, never mind Transformers, and how rapid the pace of progress is. In 2010, one could easily fit everyone in the world who genuinely believed in deep learning into a moderate-sized conference room (assisted slightly by the fact that 3 of them were busy founding <a href="https://en.wikipedia.org/wiki/DeepMind" data-link-icon="wikipedia" data-link-icon-type="svg" title="DeepMind">DeepMind</a>). Someone interested in machine learning in 2010 <em>might</em> have read about some interesting stuff from weirdo diehard connectionists in recognizing hand-written digits using all of 1–2 million parameters, or some modest neural tweaks to standard voice-recognition <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" data-link-icon="wikipedia" data-link-icon-type="svg" title="Hidden Markov model">hidden Markov models</a>⁠. In 2010, who would have predicted that over the next 10 years, deep learning would undergo a Cambrian explosion causing a mass extinction of alternative approaches throughout machine learning, that models would scale up to 175,000 million parameters, and that these enormous models would just spontaneously develop all these capabilities?</p>
          <p>No one. That is, no one aside from a few diehard connectionists written off as willfully-deluded old-school fanatics by the rest of the AI community (never mind the world), such as <a href="https://www.moderndescartes.com/docs/www/jetpress.org/3d313da208f6eac437cf56b4dca0acc49c93da33.html" id="moravec-1998" rel="archived alternate nofollow" data-url-original="https://jetpress.org/volume1/moravec.htm" title="&#39;When will computer hardware match the human brain?&#39;, Moravec 1998 (Original URL: https://jetpress.org/volume1/moravec.htm )">Moravec</a>⁠, Schmidhuber, <a href="https://www.youtube.com/watch?v=13CZPWmke6A" data-link-icon="youtube" data-link-icon-type="svg" title="&#39;Ilya Sutskever: Deep Learning | AI Podcast #94 with Lex Fridman&#39;, 2020-05-08">Sutskever</a>⁠, Legg, &amp; Amodei? One of the more shocking things about looking back is realizing how unsurprising and easily predicted all of this was if you listened to the right people. In 1998, 22 years ago, Moravec noted that AI research could be deceptive, and hardware limits meant that “intelligent machine research did not make steady progress in its first 50 years, it marked time for 30 of them!”, predicting that as Moore’s law continued, “things will go much faster in the next 50 years than they have in the last 50.” Moravec further observed that part of the reason for rapid progress was the hardware overhang: while supercomputers of the necessary power would exist long before the connectionist revolution began, no one would be allowed to use them, as they would be devoted to ‘more important’ (prestigious) hard STEM work, like “physics simulations” (ie. climate simulations &amp; nuclear bombs)<a href="#fn24" id="fnref24" role="doc-noteref"><sup>24</sup></a>⁠, and “AI research must wait for the power to become more affordable.” Affordable meaning a workstation roughly ~<span data-originalyear="1998" data-originalamount="1000" data-currentyear="2022" data-currentamount="1,956.1" title="CPI inflation-adjusted US dollar: from nominal $1,000.0 in 1998 → real $1,956.1 in 2022">$1,956.1<span><sup>$1,000.0</sup><sub>1998</sub></span></span>; sufficiently cheap compute to rival a human would arrive sometime in the 2020s, with the 2010s seeing affordable systems in the lizard–mouse range. As it happens, the start of the DL revolution is typically dated to <a href="https://en.wikipedia.org/wiki/AlexNet" data-link-icon="wikipedia" data-link-icon-type="svg" title="AlexNet">AlexNet</a> in 2012, by a grad student<a href="#fn25" id="fnref25" role="doc-noteref"><sup>25</sup></a> using 2 GTX 580 3GB GPUs (launch list price of… <span data-originalyear="2010" data-originalamount="500" data-currentyear="2022" data-currentamount="692.9" title="CPI inflation-adjusted US dollar: from nominal $500.0 in 2010 → real $692.9 in 2022">$692.9<span><sup>$500.0</sup><sub>2010</sub></span></span>, for a system build cost of perhaps <span data-originalyear="2012" data-originalamount="1500" data-currentyear="2022" data-currentamount="2,005.7" title="CPI inflation-adjusted US dollar: from nominal $1,500.0 in 2012 → real $2,005.7 in 2022">$2,005.7<span><sup>$1,500.0</sup><sub>2012</sub></span></span>). 2020 saw GPT-3 arrive, and as discussed before, there are many reasons to expect the cost to fall, in addition to the large hardware compute gains that are being forecast for the 2020s despite the general deceleration of Moore’s law.<a href="#fn26" id="fnref26" role="doc-noteref"><sup>26</sup></a></p>
          <p>The accelerating pace of the last 10 years should wake anyone from their dogmatic slumber and make them sit upright. And there are 28 years left in Moravec’s forecast…</p>
          <p>The temptation, that many do not resist so much as revel in, is to give in to a <em>déformation professionnelle</em> and dismiss any model as “just” this or that(“just billions of IF statements” or “just a bunch of multiplications” or “just millions of memorized web pages”), missing the forest for the trees, as Moravec commented of chess engines:</p>
          <blockquote>
            <p>The event was notable for many reasons, but one especially is of interest here. Several times during both matches, Kasparov reported signs of mind in the machine. At times in the second tournament, he worried there might be humans behind the scenes, feeding Deep Blue strategic insights!…In all other chess computers, he reports a mechanical predictability stemming from their undiscriminating but limited lookahead, and absence of long-term strategy. In Deep Blue, to his consternation, he saw instead an “alien intelligence.”</p>
            <p>…Deep Blue’s creators know its <em>quantitative</em> superiority over other chess machines intimately, but lack the chess understanding to share Kasparov’s deep appreciation of the difference in the <em>quality</em> of its play. I think this dichotomy will show up increasingly in coming years. Engineers who know the mechanism of advanced robots most intimately will be the last to admit they have real minds. From the inside, robots will indisputably be machines, acting according to mechanical principles, however elaborately layered. Only on the outside, where they can be appreciated as a whole, will the impression of intelligence emerge. A human brain, too, does not exhibit the intelligence under a neurobiologist’s microscope that it does participating in a lively conversation.</p>
          </blockquote>
          <p>But of course, if we ever succeed in AI, or in reductionism in general, it <em>must be by reducing Y to ‘just X’</em>. Showing that some task requiring intelligence can be solved by a well-defined algorithm with no ‘intelligence’ is precisely what success must look like! (Otherwise, the question has been thoroughly begged &amp; the problem has only been pushed elsewhere; computer chips are made of transistors, not especially tiny homunculi.)</p>
          <div>
            <blockquote>
              <p>“As long as the AI [OA5] can explore, it will learn, given enough time…We just kept waiting for the magic to run out. We kept waiting to hit a wall, and we never seemed to hit a wall.”</p>
              <p><a href="https://www.moderndescartes.com/docs/www/qz.com/dee0d5a1bf1131a00d2beb9acdc69b55e387178a.html" data-link-icon="openai" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://qz.com/1311732/openai-built-gaming-bots-that-can-work-as-a-team-with-inhuman-precision/" title="OpenAI built gaming bots that can work as a team with inhuman precision (Original URL: https://qz.com/1311732/openai-built-gaming-bots-that-can-work-as-a-team-with-inhuman-precision/ )">Greg Brockman</a></p>
            </blockquote>
          </div>
          <div>
            <blockquote>
              <p>“Give it the compute, give it the data, and it will do amazing things. This stuff is like—it’s like <em>alchemy</em>!”</p>
              <p><a href="https://www.newyorker.com/magazine/2019/10/14/can-a-machine-learn-to-write-for-the-new-yorker" data-link-icon="thenewyorker" data-link-icon-type="svg" title="Can a Machine Learn to Write for The New Yorker? Extraordinary advances in machine learning in recent years have resulted in A.I.s that can write for you.">Ilya Sutskever</a>⁠, summer 2019</p>
            </blockquote>
          </div><!-- -->
          <p><span>Hindsight is 20⁄20.</span> Even in 2015, the scaling hypothesis seemed highly dubious: you needed something to scale, after all, and it was all too easy to look at flaws in existing systems and imagine that they would never go away and progress would sigmoid any month now, soon. Like the genomics revolution where a few far-sighted seers extrapolated that the necessary <em>n</em> for GWASes would increase exponentially &amp; deliver powerful PGSes soon, while sober experts wrung their hands over “missing heritability” &amp; the miraculous complexity of biology &amp; scoff about how such <em>n</em> requirements proved <a href="https://en.wikipedia.org/wiki/Genome-wide_association_study" data-link-icon="wikipedia" data-link-icon-type="svg" title="Genome-wide association study">GWAS</a> was a failed paradigm, the future arrived at first slowly and then quickly. Yet, here we are: all honor to the fanatics, shame and humiliation to the critics!<a href="#fn27" id="fnref27" role="doc-noteref"><sup>27</sup></a> If only one could go back 10 years, or even 5, to watch every AI researchers’ head explode reading this paper… Unfortunately, few heads appear to be exploding now, because human capacity for hindsight &amp; excuses is boundless (“I can get that much with finetuning, anyway I predicted it all along, how boring”) and, unfortunately, <a href="https://intelligence.org/2017/10/13/fire-alarm/" id="yudkowsky-2017" data-link-icon="miri" data-link-icon-type="svg" title="Yudkowsky 2017">“there is no fire alarm”</a> for AGI. (If you are still <em>certain</em> that there is near-zero probability of AGI in the next few decades, why? Did you predict—in writing—capabilities like GPT-3? Is this how you expect AI failure to look in the decades beforehand? What specific task, what specific number, would convince you otherwise? How would the world look different than it does now if these crude prototype insect-brain-sized DL systems were not on a path to success?)</p>
          <p><span>Authority without accountability.</span> What should we think about the experts? Projections of failure were made by eminent, respectable, serious people. They spoke in considered tones of why AI hype was excessive and might trigger an “AI winter”, and the fundamental flaws of fashionable approaches and why brute force could not work. These statements were made routinely in 2014, 2015, 2016… And they were wrong. I am aware of few issuing a <em>mea culpa</em> or reflecting on it.<a href="#fn28" id="fnref28" role="doc-noteref"><sup>28</sup></a> It is a puzzling failure, and I’ve <a href="https://www.moderndescartes.com/newsletter/2019/13#what-progress" id="gwern-newsletter-2019-13-what-progress">reflected on it before</a>⁠.</p>
          <p><span>Phatic, not predictive.</span> There is, however, a certain tone of voice the bien pensant all speak in, whose sound is the same whether right or wrong; a tone shared with many statements in January to March of this year; a tone we can also find in a 1940 <em>Scientific American</em> article authoritatively titled, <a href="https://www.moderndescartes.com/docs/existential-risk/1940-sciam-harrington-nuclearweapons-dontworryitcanthappen.pdf" id="harrington-1940" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;Don’t Worry—It Can’t Happen&#39;, Harrington 1940">“Don’t Worry—It Can’t Happen”</a>⁠, which advised the reader to not be concerned about it any longer “and get sleep”. (‘It’ was the atomic bomb, about which certain scientists had stopped talking, raising public concerns; not only could it happen, the British bomb project had already begun, and 5 years later it did happen.)</p>
          <p><span>The iron law of bureaucracy: Cathedral gothic.</span> This tone of voice is the voice of <a href="https://www.moderndescartes.com/docs/www/srconstantin.wordpress.com/d1568a58322e49bb2aa146e6d8e4ee68d6deb785.html" id="constantin-2016" rel="archived alternate nofollow" data-url-original="https://srconstantin.wordpress.com/2016/10/20/ra/" title="&#39;Ra&#39;, Sarah Constantin 2016 (Original URL: https://srconstantin.wordpress.com/2016/10/20/ra/ )">authority</a>⁠.
          </p>
          <p>When someone speaks about future possibilities, what is the tone of their voice?</p>
          <p><a href="https://www.moderndescartes.com/Scaling-hypothesis#blessings-of-scale" id="gwern-scaling-hypothesis-blessings-of-scale">null</a> <!-- LinkAuto override: disable self-linking --> <a href="https://www.moderndescartes.com/Scaling-hypothesis" id="gwern-scaling-hypothesis" title="&#39;The Scaling Hypothesis&#39;, Branwen 2020">null</a> <!-- LinkAuto override: disable self-linking --></p><!-- Training on diverse data:
FLAN
T0
https://arxiv.org/abs/2201.11473#microsoft
https://arxiv.org/abs/2112.10668#facebook
preference-learning like InstructGPT
https://arxiv.org/abs/2109.01652#google
https://arxiv.org/abs/2111.10952#google ExT5
https://arxiv.org/abs/2104.02133
https://arxiv.org/abs/2101.11038#facebook Muppet

I should clarify that we aren't data-limited in the sense of large natural data dumps, but we are data-limited in other kinds of data in terms of triggering interesting latent capabilities.

In terms of raw data, The Pile and CC have more data than you need for the foreseeable future. This does not apply to other kinds of data, like curated sets of prompts. If you think of the pretraining paradigm, the point of large natural real world datadumps is not to be large or because we care about them or because the updates on 99% of the data will be useful, but that by virtue of their sheer size and indiscriminateness, they happen to contain, hidden throughout like flecks of gold in a giant river of sand, implicit unlabeled 'hard tasks' which foster generalization and capabilities through the blessing of scale. One might go through a gigabyte of text before finding an example which truly stresses a model's understanding of "whether a kilogram of feathers weighs more than a kilogram of toasters" - these are simply weird things to write, and are mostly just implicit, and most examples are easily solved by shortcuts. The more easy examples you solve, the more gigabytes or terabytes you have to process in order to find a bunch of examples you haven't already solved, and the bigger your model has to be to potentially absorb the remainder. So there are diminishing returns and you rapidly run out of compute before you run out of raw data.

However, if you can write down a few examples of each of those tasks and produce a highly concentrated dose of those tasks (by distilling the dumps, collating existing challenging benchmarks' corpuses, recruiting humans to write targeted tasks, using adversarial methods to focus on weak points etc.), you can potentially bring to the surface a lot of learning and meta-learning. This is hard to do because we don't know what most of those hard tasks are: they are the water in which we swim, and we don't know what we know or how we know it (which is much of why AI is hard). But you can still try. This has been a very effective approach over the past year or so, and we have yet to see the limits of this approach: the more varied your prompts and tasks, the better models work.

Plateaus/breakthroughs:
few-shotting
Arithmetic in GPT-3 paper eg. https://www.lesswrong.com/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance
Anagram/scrambles in GPT-3 paper
text style transfer in LaMDA
BIG-bench https://www.lesswrong.com/posts/vwLxd6hhFvPbvKmBH/yudkowsky-and-christiano-discuss-takeoff-speeds?commentId=AWSj7eHKm2ELGvzSS https://www.youtube.com/watch?t=19310&v=x-9KxACAPIo `ipa_translate`, `simple_arithmetic_json_subtasks`
RL finetuning with small n: Anthropic (500 PMP pairs) https://www.lesswrong.com/posts/oBpebs5j5ngs3EXr5/a-summary-of-anthropic-s-first-paper-3 https://arxiv.org/abs/2112.00861
inner monologues in LaMDA https://arxiv.org/abs/2201.11903#google

grokking? https://arxiv.org/abs/2201.02177#openai
Nyquist learners? https://arxiv.org/abs/2108.07686
patient teachers https://arxiv.org/abs/2106.05237#google

-->
        </section>
        
      </div></div>
  </body>
</html>
