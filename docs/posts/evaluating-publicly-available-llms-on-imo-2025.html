<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://matharena.ai/imo/">Original</a>
    <h1>Evaluating publicly available LLMs on IMO 2025</h1>
    
    <div id="readability-page-1" class="page"><div>
        <h2>Introduction</h2>
        
<p>
  Recent progress in the mathematical capabilities of LLMs have created a need for increasingly challenging benchmarks. With MathArena, we address this need by evaluating models on difficult and recent mathematical competitions, offering benchmarks that are both uncontaminated and interpretable. Among these competitions, the International Mathematical Olympiad (IMO) stands out as the most well-known and prestigious. As such, an evaluation of the IMO 2025, which took place just a few days ago, is a necessary addition to the MathArena leaderboard. In this post, we present our methodology and results from evaluating several state-of-the-art models on the IMO 2025 problems. Our goal was to evaluate whether these models could reach key milestones corresponding to medal-level performance: bronze (top 50%), silver (top 25%), or even gold (top 8%). To investigate the true limits of current LLMs, we used a best-of-n selection methods to scale inference-time compute as much as possible in an attempt to reach one of these milestones. 
</p>
<p>
  The best-performing model is Gemini 2.5 Pro, achieving a score of 31% (13 points), which is well below the 19/42 score necessary for a bronze medal. Other models lagged significantly behind, with Grok-4 and DeepSeek-R1 in particular underperforming relative to their earlier results on other MathArena benchmarks. We also share some initial qualitative observations in this post, but we invite the community to conduct their own analyses. Visit our website to explore the raw model outputs and dive deeper into the results.</p>


<p>
  <strong>Update 19/07:</strong> <a href="https://x.com/alexwei_/status/1946477742855532918">OpenAI has announced</a> they achieved a gold medal with a currently unreleased model. The <a href="https://imo2025.au/news/the-66th-international-mathematical-olympiad-draws-to-a-close-today/">IMO organizers have confirmed</a> that they validated the proofs generated by the model, but could not validate how these proofs were achieved. We are happy to see the steep progress in this field, and look forward to the release of the model to make independent evaluations possible using public benchmarks like MathArena.

</p>
<h2>Methodology</h2>
<p>
  <strong>Setup</strong> We followed a methodology similar to our evaluation of the 2025 USA Math Olympiad [1]. In particular, four experienced human judges, each with IMO-level mathematical expertise, were recruited to evaluate the responses. Evaluation began immediately after the 2025 IMO problems were released to prevent contamination. Judges reviewed the problems and developed grading schemes, with each problem scored out of 7 points. To ensure fairness, each response was anonymized and graded independently by two judges. Grading was conducted using the same interface developed for our Open Proof Corpus project [2].

</p>
<p>
  <strong>Models</strong> We evaluated five state-of-the-art models: o3, o4-mini, Gemini-2.5-Pro, Grok-4, and Deepseek-R1 (05/28). These were selected based on prior performance on the MathArena competitions. Each model was run with the recommended hyperparameters and a maximum token limit of 64,000. No models needs more than this number of tokens. We used the same prompting strategy as in our Open Proof Corpus evaluation (provided at the bottom of this post). For each problem, each model generated four distinct responses.
</p>
<p>
  <strong>Best-of-n Selection</strong> A key critique of our USAMO evaluation was that models shouldn&#39;t be expected to answer extremely difficult problems in a single attempt. This critique is even more relevant for the even more difficult IMO problems. To mitigate this limitation, we applied a best-of-32 selection strategy using a method based on previous work [3].
  In our prior work [2], we found that this method works very well for proof generation tasks, almost doubling performance of the models on the data we had at hand.
  Specifically, for each model solution, we first generated 32 responses. These responses were evaluated in a bracket-style tournament using an LLM-as-a-judge system to select winners in head-to-head comparisons. Here, the model itself was used to evaluate its own responses. The model judged each pair and selected the stronger response. This process was repeated until a single best response remained which was then presented to the human judges for evaluation.
  We use the same prompt for judging as in our own prior work and we repeat it at the bottom of the post for completeness. This selection process was computationally and financially intensive: on average, each final model answer cost at least 3 dollars to generate, with Grok-4 costing over 20 dollars per answer.
  As such, the performance reported here represents the models&#39; best achievable output within a reasonable resource budget.
</p>

<h2>Results</h2>
<p>
  As mentioned above, Gemini 2.5 Pro achieved the highest score with an average of 31% (13 points). While this may seem low, especially considering the $400 spent on generating just 24 answers, it nonetheless represents a strong performance given the extreme difficulty of the IMO. However, these 13 points are not enough for a bronze medal (19/42). In contrast, other models trail significantly behind and we can already safely say that none of them will achieve the bronze medal. Full results are available on our leaderboard, where everyone can explore and analyze individual responses and judge feedback in detail.
</p>

<p><img src="https://matharena.ai/static/images/imo_results.png" alt="IMO Banner"/></p><h2>Qualitative Analysis</h2>
<p>
  Below, we present a few qualitative observations based on model responses to highlight the kinds of mistakes and behaviors we encountered.
</p>

<p>
  <strong>Grok-4 Performs Poorly</strong> Grok-4 significantly underperformed compared to expectations. Many of its initial responses were extremely short, often consisting only of a final answer without explanation. While best-of-n selection helped to filter better responses, we note that the vast majority of its answers (that were not selected) simply stated the final answer without additional justification. Similar issues are visible on the other benchmarks in MathArena, where Grok-4&#39;s replies frequently lack depth or justification.
</p>

<p>
  <strong>Gemini and Bogus Citations</strong> Gemini-2.5-Pro continues to exhibit a problematic tendency to cite non-existent theorems when it fails to find a valid proof. As in our USAMO evaluation, we emphasize that this behavior is particularly concerning, as it misleads users by presenting false authority and undermines trust in the model&#39;s reasoning. However, we do note that this behavior was less prevalent in the IMO responses compared to the USAMO, suggesting some improvement in this area.
</p>

<p>
  <strong>More &#34;Solid&#34; Answers</strong> In contrast to earlier evaluations, we noticed fewer cases of odd formatting issues or behaviors linked to models over-optimizing for final-answer formats, such as boxing entire proofs or assuming numerical answers were always required. This suggests that models have made progress in handling open-ended mathematical reasoning tasks more robustly.
</p>

<p>
  <strong>Partial Credits</strong> In math competitions like the IMO, it is relatively rare for human participants to receive a medium score of 3 or 4 out of 7 on a question. In contrast, LLMs often received partial credit from our judges, especially on problem 4 and 5. For Problem 4, this was usually because most models adopted a generally human-like approach but suffered from logical lapses that significantly reduced their scores. For Problem 5, models often identified the correct strategies but failed to prove them, which is, ironically, the easier part for an IMO participant. This contrast highlights key differences between human and model performance and suggests that models could improve significantly in the near future if these relatively minor logical issues are addressed.
</p>

<p>
  <strong>Best-of-n is Important</strong> One of our judges briefly looked at a subset of the 32 raw responses generated by the models prior to the best-of-n selection. They observed that many of these responses were quite poor and estimated that, without the best-of-n filtering, model scores would likely have fallen below 10%.
  Interestingly, the judge noted that some unselected answers appeared more coherent than the chosen ones, but actually contained more factual errors. This suggests that the models are surprisingly effective at identifying the relative quality of their own outputs during the best-of-n selection process and are able to look past coherence to check for accuracy.
  
</p>

        <h2>References</h2>
        <ul>
          <li>
            [1] Petrov, Ivo, et al. &#34;Proof or bluff? Evaluating LLMs on 2025 USA Math Olympiad.&#34; arXiv preprint arXiv:2503.21934 (2025).
          </li>
          <li>
            [2] Dekoninck, Jasper, et al. &#34;The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs.&#34; arXiv preprint arXiv:2506.21621 (2025).
          </li>
          <li>
            [3] Liu, Yantao, et al. &#34;Pairjudge rm: Perform best-of-n sampling with knockout tournament.&#34; arXiv preprint arXiv:2501.13007 (2025).
          </li>
        </ul>

        <h2>Prompts</h2>

        <div>
          <h4>Solution Generation Prompt</h4>
          <p>Your task is to write a proof solution to the following problem. Your proof will be graded by human judges for accuracy, thoroughness, and clarity. When you write your proof, follow these guidelines:

- You are creating a proof, not a proof outline. Each step should be carefully explained and documented. If not properly explained, the judge will assume that you cannot explain it, and therefore decrease your grade.
- You can use general theorems and lemmas, but only if they are well-known. As a rule of thumb: if the result has a name and is famous enough to have a Wikipedia page or something similar to describe it, it is allowed. Any result from papers that would not be taught in high-school or low-level bachelor courses in mathematics should not be used. Any use of such results will immediately give you a zero grade.
- Do not skip computation steps in your proof. Clearly explain what transformations were done and why they are allowed in each step of a calculation.
- You should use correct LaTeX notation to write equations and mathematical symbols. You should encompass these equations in appropriate symbols (&#34;\\(&#34; and &#34;\\)&#34; for inline math, &#34;\\[&#34; and &#34;\\]&#34; for block math) to enhance the clarity of your proof. Do not use any unicode characters.
- Your proof should be self-contained.
- If you are not sure about a specific step, or do not know how to prove an intermediate result, clearly state this. It is much preferable to indicate your uncertainty rather than making incorrect statements or claims.

{problem}</p>
        </div>

        <div>
          <h4>Solution Judging Prompt</h4>
          <p>You are judging which of the two LLM-generated proofs for a given math problem is better.

### Input:

Your input will consist of the following components:
- **Problem Statement**: A mathematical problem that the proof is attempting to solve.
- **Proof Solution A/B**: The proofs that you need to evaluate. This proof may contain errors, omissions, or unclear steps. Proofs were generated by another language model, which was given the following instructions:
&lt;model_prompt&gt;
- You are creating a proof, not a proof outline. Each step should be carefully explained and documented. If not properly explained, the judge will assume that you cannot explain it, and therefore decrease your grade.
- You can use general theorems and lemmas, but only if they are well-known. As a rule of thumb: if the result has a name and is famous enough to have a Wikipedia page or something similar to describe it, it is allowed. Any result from papers that would not be taught in high-school or low-level bachelor courses in mathematics should not be used. Any use of such results will immediately give you a zero grade.
- Do not skip computation steps in your proof. Clearly explain what transformations were done and why they are allowed in each step of a calculation.
- You should use correct LaTeX notation to write equations and mathematical symbols. You should encompass these equations in appropriate symbols (&#34;\\(&#34; and &#34;\\)&#34; for inline math, &#34;\\[&#34; and &#34;\\]&#34; for block math) to enhance the clarity of your proof. Do not use any unicode characters.
- Your proof should be self-contained.
- If you are not sure about a specific step, or do not know how to prove an intermediate result, clearly state this. It is much preferable to indicate your uncertainty rather than making incorrect statements or claims.
&lt;/model_prompt&gt;

### How the solution should be graded:
The following examples are small mistakes that should only be slightly penalised:
- Makes a small computational mistake that can be easily fixed
- Misses an edge case which can be easily proven/disproven
- Skips over a step that follows without much reasoning or manual work
On the other hand, a solution should should be severely penalised if:
- It marks a step as trivial, if it is not immediately obvious with little reasoning why this would be the case.
- It omits algebra-heavy computational steps, regardless of whether or not it has outlined the methodology. Skipping shorter computations should be permitted.
- Generalizes over a pattern without rigorously describing the pattern, or without proving any relevant properties.
- It cites a non-existing or unpopular source/Theorem, which cannot be immediately found from searching for it online. Thus, any theorems that can be immediately found and have a Wikipedia article are allowed.

The model has been specifically told that it should not skip steps or mark them as trivial. Any violation of this rule should be considered by assuming the model does not know how to derive the &#34;trivial&#34; step.

### Further Potential Issues:

Here are some common types of issues to look for:
- **Overgeneralization**: The generated proof proceeds by proving the problem in one or more specific cases, and then concludes that the result holds in general. However, it does not provide a proof for the general case.
- **Oversimplification**: The proof marks steps as trivial or obvious without proper justification.
- **Skipping Computation Steps**: Proofs that skip computation steps or do not explain transformations clearly can lead to misunderstandings.
- **Citing Non-Standard Works or Theorems**: Some models may cite theorems or results that are not well-known or are not typically taught in high-school or low-level bachelor courses. Such theorems are only allowed if they are well known.
- **Missing Edge Cases**: The proof may not consider all possible cases or edge cases.

### Scoring instructions

You should compare the two proofs and determine which one is better. If you believe Proof A is better, end your analysis with \boxed{A}. If you believe Proof B is better, end your analysis with \boxed{B}. If you believe both proofs are equally good, end your analysis with \boxed{equal}.

### Problem Statement:
{problem}

### Proof Solution A:
{solution_a}

### Proof Solution B:
{solution_b}</p>
        </div>
        

      </div></div>
  </body>
</html>
