<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://shlee.fedipress.au/2024/call-to-action-fediverse-media-server/">Original</a>
    <h1>Call to Action: Fediverse Media Server</h1>
    
    <div id="readability-page-1" class="page"><article id="post-408">
	
		<p>
By  on <a href="https://shlee.fedipress.au/2024/call-to-action-fediverse-media-server/" title="2:56 am" rel="bookmark"><time datetime="2024-05-11T02:56:35+10:30">11 May 2024</time></a>	• 
	</p>
	<section>
<p><span></span> <span>Post Views:</span> <span>2,652</span>
			</p>
<p><strong>Edit: So to put my money where my mouth is.. Right now storage is costing me around $150AUD a month… so if somebody can build a media server we can share/deploy and offer it for all of the Fediverse instances… I will give them a minimum of $3600AUD (2 years of Wasabi money) as a bounty once it’s in production!</strong></p>





<p>I am the “proud” owner of an overpowered Proxmox cluster hosted in colocation at an Australian data center. My cluster is two 2U AMD EPYC servers with a 1U IBM for backup (Only two of those servers have the blessing of the machine god).</p>



<figure><img fetchpriority="high" decoding="async" width="1024" height="768" src="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/0a773c1c4813f300-1-1024x768.jpeg" alt="" srcset="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/0a773c1c4813f300-1-1024x768.jpeg 1024w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/0a773c1c4813f300-1-300x225.jpeg 300w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/0a773c1c4813f300-1-768x576.jpeg 768w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/0a773c1c4813f300-1-1536x1152.jpeg 1536w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/0a773c1c4813f300-1-2048x1536.jpeg 2048w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/0a773c1c4813f300-1-676x507.jpeg 676w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<figure><table><tbody><tr><td>Unit 0 is AMD EPYC 7642 (Rome Zen2) with 512GB of DDR4-3200</td><td>Unit 1 is AMD EPYC 7713 (Milan Zen3) with 256GB of DDR4-3200</td></tr></tbody></table><figcaption>Notice the unused memory and low CPU usage.</figcaption></figure>



<p>and I’m using a Mikrotik RB5009UG+S+IN Ethernet router</p>



<hr/>





<p>I’ve been hosting Fediverse services such as <em>Aus.Social and Pixelfed.au</em> for the last 5 years, and I’ve been working hard to maintain the quality of service while lowing the total cost of ownership. During this time, I’ve seen hundreds of other administrators shutdown their instances due to high stress and high cost of hosting, and I want to offer support to these admins in my region (including taking ownership of abandoned instances).</p>



<p>After migrating all of my Fediverse services to my dedicated servers, I became hyper aware of having excess compute/memory/storage capability, and as such I started to consider if I could offer to migrate other mastodon instances to my colo at a massive discount compared to other cloud providers. (Notice 500GB of unused memory in the screenshots above).</p>



<p>Sadly I’m currently unable to offer this to anybody right now due to my primary mastodon instances remote media caching policy requirements. A.S is currently eating more than 300GB of data usage every day since go-live. The bandwidth shown below is currently hosting just <em>Aus.Social</em>, and this alone is costing me half of my bill (or $330 a month) because Mastodon is currently downloading, testing and uploading every piece of media to my S3 bucket from every single toot it sees.</p>


<div>
<figure><img decoding="async" width="685" height="314" src="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-16.png" alt="" srcset="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-16.png 685w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-16-300x138.png 300w" sizes="(max-width: 685px) 100vw, 685px"/></figure></div>


<p>If I can lower my usage using shared services, CDNs, de-duplication (or hopefully Jortage Rivot style APIs), I can start to offer VMs to other administrators VMs on my cluster without the additional bandwidth overhead of their server downloading and uploading every media attachment they see (including from my server likely hosted on the same box). This is my mission with this post!</p>


<div>
<figure><img decoding="async" width="406" height="220" src="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-11.png" alt="" srcset="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-11.png 406w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-11-300x163.png 300w" sizes="(max-width: 406px) 100vw, 406px"/></figure></div>


<p>(I will move to another colocation with cheaper bandwidth in 12 months once my contract is over but that’s besides the point.. sadly I live in Australia is a fake country and bandwidth is just expensive everywhere).</p>



<hr/>





<p>The current model is wasteful, so there is a motivating factor for administrators to lower their costs and be more environmentally friendly! Win win.</p>



<p><strong>Storage</strong>: Let’s consider an example where there are 1000 active Mastodon instances, each with at least 1 user following the user on my instance, and all of these instances are using Wasabi S3. In this scenario, the minimum cost of those 1001 Wasabi buckets is $5 USD per TB minimum, totaling $5005 USD a month to store the same file.</p>



<p>My actual instance sits at around 10TB, costing $69 USD a month… so I’d imagine those 1000 imaginary instance would be between 1TB and 10TB and that’s still a massive amount of expense just on S3 buckets.</p>



<p>If we could split the <strong>one-instance-one-bucket</strong> model up, the total cost of ownership for the entire Fediverse would drop dramatically, and considering there are almost 10,000 Mastodon instances (excluding Pixelfed and the other Fediverse servers)… I’d hate to calculate how much the Fediverse costs simply in storage (local or S3).</p>



<p><strong>Compute</strong>: If the validation/transcoding were offloaded from the instances, this would lower the CPU/memory required by Sidekiq to maintain the instances. This would allow people to use smaller instance types and lower the total cost of ownership globally once again.</p>



<hr/>





<figure><img loading="lazy" decoding="async" width="1317" height="690" src="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-9.png" alt="" srcset="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-9.png 1317w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-9-300x157.png 300w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-9-1024x536.png 1024w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-9-768x402.png 768w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-9-676x354.png 676w" sizes="(max-width: 1317px) 100vw, 1317px"/></figure>



<p><strong>Bandwidth/Storage Calculator</strong> (Based on S3 remote storage)</p>



<ul>
<li>A.S account @shlee posts a 1MB image</li>



<li>A.S transcodes and uploads the 1MB image to my S3</li>



<li>1000 instances download the 1MB file </li>



<li>1000 instances waste compute resources transcoding/validating the file.</li>



<li>1000 instances upload the 1MB file to their S3</li>



<li><strong>Total:</strong> 2002MB of bandwidth and 1001MB of storage used across 1001 instances.</li>
</ul>



<p>or as explained again by the Jortage team at <a href="https://jortage.com/">https://jortage.com/</a></p>



<figure><img loading="lazy" decoding="async" width="899" height="144" src="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-10.png" alt="" srcset="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-10.png 899w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-10-300x48.png 300w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-10-768x123.png 768w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-10-676x108.png 676w" sizes="(max-width: 899px) 100vw, 899px"/></figure>



<p>The current Mastodon Media Modal requires every single instance that has ingested a toot with media to download a copy of that media (validate it/transcode it if required) and upload to their local storage or remote storage (S3 bucket).</p>



<p>As shown in my diagram, this requirement means my instance is downloading 150GB and uploading 150GB of media every day! If I was hosting five other mastodon instances, they would all be downloading and uploading files without any coordination or awareness of each other. My media bucket is exclusively used by my instance.</p>



<p>This is bad because it wastes bandwidth, it wastes storage (no de-duplication) and it wastes compute… so I’m after a new modal which shares media between my primary instance A.S and any other instances hosted on my service. </p>



<p>but it also doesn’t provide high availability (or backup) because every instance still holds all of their media resources in one location as a single point of failure.</p>



<hr/>





<p><strong>Pushing media</strong></p>



<figure><img loading="lazy" decoding="async" width="1024" height="523" src="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-13-1024x523.png" alt="" srcset="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-13-1024x523.png 1024w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-13-300x153.png 300w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-13-768x392.png 768w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-13-676x345.png 676w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-13.png 1070w" sizes="(max-width: 1024px) 100vw, 1024px"/></figure>



<p>A possible media workflow looks something like this – with a combination of “shared media servers” or Fediverse Delivery Networks (FDNs) and original V1 style instances running their own buckets.</p>



<ul>
<li>An A.S user posts a toot with an original piece of media. (Media is validated/transcoded/signed as such by A.S)</li>



<li>A.S uploads the new toots’ media to the media server.</li>



<li>Media Server 1 confirms the file is valid/signed and stores it locally (Transcodes if required).</li>



<li>Toot is replicated to the other fediverse servers.</li>



<li>Local instances (sharing the same media server) ask “media server 1” if the file exists. It does / nothing is downloaded.</li>



<li>External instances ask their “media server 2” if the file exists. it is not in the cache and downloads it from the origin media server.</li>



<li>Servers using the V1 modal download the media as usual.</li>
</ul>



<p><strong>Pulling media</strong></p>



<figure><img loading="lazy" decoding="async" width="982" height="333" src="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-18.png" alt="" srcset="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-18.png 982w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-18-300x102.png 300w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-18-768x260.png 768w" sizes="(max-width: 982px) 100vw, 982px"/></figure>



<p>Grabbing external media is a similar flow.. with the media server downloading and validating/transcoding the media on behalf of the instance.</p>



<p>+ I would believe this would duplicate Claires “cross-instance media processing synchronization” because the processing would be completed on the media server.</p>



<p>The end user experience, As the Media Servers would likely have CDNs like bunny or fastly in front of them. Their CDNs could download a copy of the original media once and then cache it on their edge servers globally. Limiting the traffic from the FDN media-server to the global end users, but allowing for fast ingestion.<br/></p>



<hr/>


<h2 id="bandwidth-calculator-redux"><strong>Bandwidth Calculator Redux</strong> </h2>


<p>(Based on S3 remote storage with 100 media servers supporting 700 instances and 300 current style V1 instances with their own S3 buckets)</p>



<ul>
<li>A.S account @shlee posts a 1MB image</li>



<li>A.S transcodes and uploads the 1MB image to my Media server</li>



<li>100 Media Servers download the 1MB file</li>



<li>100 media servers validate the signature on the file (no transcode required)</li>



<li>300 instances download/transcode/upload the 1MB file to their S3<br/></li>



<li><strong>Total:</strong> 702MB of bandwidth and 401MB of storage used across 1001 instances</li>



<li><strong>Bonus</strong>: If all 1000 instances media was stored in 100 Media Servers… 102MB of bandwidth and 101MB of storage (compared to 2002MB bandwidth and 1001MB storage of the current design)</li>
</ul>



<p>Instances using media servers would also have less CPU/memory utilisation allowing admins to use smaller instance types due to offloading the transcoding.. this would also lower the cost of the fediverse.</p>



<p>Note: This concept should work for single user instances, standard instances and mass-hosted instances. At best, moving all of the media verification/transcoding from the instance sidekiq jobs to a dedicated service would make sense for single user instances as well by having a dedicated process/vm/container for the media processing similar to the streaming server being a dedicated task, and just makes the ecosystem a little more “microservice based”.</p>



<hr/>





<p>Jortage developer Una mentions “Rivet” in their breakdown and this is an API concept which enables my example above.</p>



<p>I don’t believe Rivet is actually “in the works atm” due to circumstances.</p>



<figure><img loading="lazy" decoding="async" width="893" height="160" src="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-14.png" alt="" srcset="https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-14.png 893w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-14-300x54.png 300w, https://shlee.fedipress.au/wp-content/uploads/sites/2/2024/05/image-14-768x138.png 768w" sizes="(max-width: 893px) 100vw, 893px"/></figure>



<hr/>





<p>Mastodon, Pixelfed and most of the other Fediverse servers operates on ActivityPub, which serves as a protocol for enabling actors to communicate with each other. This is the type of framework that should influence the media server. It could speak AP.</p>



<p>The beauty of this concept lies in the flexibility it offers. Only the API between the Mastodon Instance and the Media server (FDN) needs to be firmly established, while the design of the media server can vary greatly.</p>



<p>From regional ingestion points to globally distributed anycast FDNs, the infrastructure can be built in numerous ways. It could rely solely on local storage for hot caching and utilize S3 for warm storage, or incorporate services like Fastly/Bunny as a CDN layered atop the media server. There are no rigid requirements for the actual design; it simply needs to communicate securely with the instances it serves and provide files on demand.</p>



<p>I’d like to believe all of the media servers should be open source and deployable by anybody with the will to do so. This follows the decentralised ethos of the Fediverse. It means friendly instances can band together into groups to pool resources and save money. Win win.</p>



<p>Jortage has a GitHub, but it doesn’t have any instructions on how to deploy it… I’ve contacted the Jortage admin and told them I’m more than happy to just give them the $100USD a month I’m giving Wasabi if they build a Jortage AU ingestion node but I need the Rivet API to take full advantage of that to solve my problem completely.</p>



<hr/>



<p><strong>Finally</strong>, if the model is built to separate the instances and the media, my current hosted instances total bandwidth on my colo VMs could drop from 300GB a day every single day, to only serving the web, api and streaming traffic (which is a handful of GB a day)… with the media downloaded/uploaded/cached on my friendly neighborhood (FDN) Media server.</p>



<p>Once my monthly data usage per instance is minimal (TBs to GBs), I could host a lot of Fediverse instances (DB/Web/Streaming/Sidekiq) while halving my bill…. these cost savings would allow me to give the Fediverse developers money instead of Wasabi (and my colo provider)… and I bet I’m not the only one!</p>



<p>Remember, if we could go from 10,000 S3 buckets to 5000 S3 buckets, the cost of the entire ecosystem drops dramatically… the cost savings increase per instance on a shared media server are shared as more people join this media server.</p>



<hr/>





<p>These are some brainstorming concepts that V2 and beyond could offer:</p>



<ul>
<li><strong>Motivation for single instances:</strong> I could store the media server on a cheap linode instance (free bandwidth) and not have to worry about media bandwidth on my cluster. Right now, there is a tight connection between the instance and the media via Sidekiq and the DB/Redis… We could disconnect this relationship and turn it into an API between the mastodon instance and the media server which could tolerate higher latencies. (This would solve my problem by just moving my media ingestion/processing/caching to a cloud provider).<br/></li>
</ul>



<ul>
<li><strong>High Availability</strong>: An instance could list a primary and multiple secondary FDN media servers as their media source and this would protect against outages or failures on a single source.</li>



<li><strong>Failure recovery:</strong> If the media from the fediverse is transcoded/validated and then hashed/signed correctly. An instance or media server suffering a failure without a backup, could authoratively access other media servers and recover the instances local media as long as the media was shared with at least one known media server.</li>



<li><strong>AP Ingestion</strong>: If the V2 modal speaks AP, it can be attached to relays to download media head of time for a better end user experience by having the media before the toot reaches the instance (this is a bit of a weird use case but worth discussion).</li>



<li><strong>AP Deleting</strong>: If the V2 modal speaks AP, it can receive a signed delete from an actor and delete the media for all instances at the same time. Offering people higher level of safety.</li>



<li><strong>Toot De-duplication</strong>: Bots are known to upload the exact same picture/video over and over on schedule, and deduce would only use the space of one upload or possibly one image URL as it’s the same hash – Weekend bot for example.</li>



<li><strong>Edge Cases Accounts: </strong>There are a lot of accounts just uploading endless large videos on a schedule – @flameReactor is using 1.5% of my total storage for a single account. (if I was sharing my storage with more than one instance, this kind of thing might be less annoying).</li>
</ul>
		<p>Categories: <a href="https://shlee.fedipress.au/category/mastodon/" rel="category tag">Mastodon</a> <a href="https://shlee.fedipress.au/category/rants/" rel="category tag">Rants</a>		</p>
	<div>
		<p><img alt="" src="https://secure.gravatar.com/avatar/9527d84e2f18ffc4825c56d154f12ca6?s=100&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/9527d84e2f18ffc4825c56d154f12ca6?s=200&amp;d=identicon&amp;r=g 2x" height="100" width="100" loading="lazy" decoding="async"/></p><h3>shlee</h3>
			</div>
	</section>
</article></div>
  </body>
</html>
