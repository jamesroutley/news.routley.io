<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.kolaayonrinde.com/blog/2024/02/11/mamba.html">Original</a>
    <h1>Mamba Explained: The State Space Model Taking On Transformers</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody">
    <!-- <nav class="sidebar"> -->
    <!-- Table of Contents -->
    <!-- <h4>Table of Contents</h4> -->
    <!--  -->
    <!-- </nav> -->
    <!-- <br /> -->
    <!-- <br /> -->
    <h3 id="the-state-space-model-taking-on-transformers">The State Space Model taking on Transformers</h3>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/snake.png" width="400" alt="Mamba vs Transformer"/>
    <figcaption></figcaption>
  </figure>
</div>


<p>Right now, AI is eating the world.</p>

<p>And by AI, I mean Transformers. Practically all the big breakthroughs in AI over
the last few years are due to Transformers.</p>

<p><strong>Mamba</strong>, however, is one of an alternative class of models called <strong>State
Space Models</strong> (<strong>SSMs</strong>). Importantly, for the first time, Mamba promises
similar performance (and crucially similar
<a href="https://arxiv.org/pdf/2203.15556.pdf"><em>scaling laws</em></a>) as the Transformer
whilst being feasible at long sequence lengths (say 1 million tokens). We
achieve this long context by removing the ‚Äúquadratic bottleneck‚Äù in the
Attention Mechanism. Mamba also runs <em>fast</em> - like ‚Äúup to 5x faster than
Transformer fast‚Äù<sup id="fnref:figure" role="doc-noteref"><a href="#fn:figure" rel="footnote">1</a></sup>.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/mamba_scaling.png" width="800" alt="Scaling Laws for Mamba vs other Language Models"/>
    <figcaption>Mamba performs similarly (or slightly better than) other Language Models on The Pile</figcaption>
  </figure>
</div>


<p>Gu and Dao, the Mamba authors write:</p>

<blockquote>
  <p>Mamba enjoys fast inference and linear scaling in sequence length, and its
performance improves on real data up to million-length sequences. As a general
sequence model backbone, Mamba achieves state-of-the-art performance across
several modalities such as language, audio, and genomics. On language
modeling, our Mamba-3B model outperforms Transformers of the same size and
matches Transformers twice its size, both in pretraining and downstream
evaluation.</p>
</blockquote>



<p>Here we‚Äôll discuss:</p>

<ul>
  <li>The advantages (and disadvantages) of Mamba (üêç) vs Transformers (ü§ñ),</li>
  <li>Analogies and intuitions for thinking about Mamba, and</li>
  <li>What Mamba means for Interpretability, AI Safety and Applications.</li>
</ul>

<h2 id="problems-with-transformers---maybe-attention-isnt-all-you-need">Problems with Transformers - Maybe Attention <em>Isn‚Äôt</em> All You Need</h2>

<p>We‚Äôre very much in the Transformer-era of history. ML used to be about detecting
cats and dogs. Now, with Transformers, we‚Äôre
<a href="https://openai.com/research/gpt-4">generating human-like poetry</a>,
<a href="https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf">coding better than the median competitive programmer</a>,
and
<a href="https://www.nature.com/articles/s41586-021-03819-2">solving the protein folding problem</a>.</p>

<p>But Transformers have one core problem. In a transformer, every token can look
back at every previous token when making predictions. For this lookback, we
cache detailed information about each token in the so-called KV cache.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/attention.png" width="800" alt="attention"/>
    <figcaption>When using the Attention Mechanism, information from all previous tokens can be passed to the current token</figcaption>
  </figure>
</div>


<p>This pairwise communication means a forward pass is O(n¬≤) time complexity in
training (the dreaded <code>quadratic bottleneck</code>) and each new token generated
autoregressively takes O(n) time. That is to say, as the context gets larger,
the model gets <em>slower</em>.</p>

<p>To add insult to injury, storing this KV cache requires O(n) space. The fateful
<code>CUDA OOM</code> error looms large as the memory footprint balloons. If space were the
only issue, we might just add more GPUs but with latency growing
quadratically‚Ä¶ perhaps not.</p>

<p>On the margin, we can mitigate the quadratic bottleneck with techniques like
<a href="https://paperswithcode.com/method/sliding-window-attention">Sliding Window Attention</a>
or clever CUDA optimisations like
<a href="https://arxiv.org/pdf/2205.14135.pdf">FlashAttention</a>. But ultimately, for
super long context windows (like a chatbot which remembers every conversation
you‚Äôve shared), we need a different approach.</p>

<h3 id="foundation-model-backbones">Foundation Model Backbones</h3>

<p>Fundamentally, all good ML architecture backbones have components for two
important operations:</p>

<ol>
  <li><strong>Communication</strong> <em>between</em> tokens</li>
  <li><strong>Computation</strong> <em>within</em> a token</li>
</ol>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/transformer_block.png" width="800" alt="Transformer Block"/>
    <figcaption>The Transformer Block</figcaption>
  </figure>
</div>


<p>In transformers, this is <strong>Attention</strong> (<code>communication</code>) and <strong>MLPs</strong>
(<code>computation</code>). We improve transformers by optimising these two
operations<sup id="fnref:scale" role="doc-noteref"><a href="#fn:scale" rel="footnote">2</a></sup>.</p>

<p>We would like to replace the Attention component <sup id="fnref:attention" role="doc-noteref"><a href="#fn:attention" rel="footnote">3</a></sup> with some other
method for communicating between tokens. <strong>Mamba</strong> uses the Control Theory
inspired <strong>SSM</strong> for <code>Communication</code> and keeps MLP-style projections for
<code>Computation</code>.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/mamba_block.png" width="800" alt="Mamba Block"/>
    <figcaption>The Mamba Block</figcaption>
  </figure>
</div>


<p>Like a Transformer made up of stacked transformer blocks, Mamba is made up of
stacked Mamba blocks as above.</p>

<p>We would like to understand and motivate the choice of the SSM for sequence
transformations.</p>

<h2 id="motivating-mamba---a-throwback-to-temple-run">Motivating Mamba - A Throwback to Temple Run</h2>

<p>Imagine we‚Äôre building a Temple Run agent <sup id="fnref:temple-run" role="doc-noteref"><a href="#fn:temple-run" rel="footnote">4</a></sup>. It chooses if the
runner should move left or right at any time.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/temple_run.png" width="400" alt="Temple Run"/>
    <figcaption></figcaption>
  </figure>
</div>


<p>To successfully pick the correct direction, we need information about our
surroundings. Let‚Äôs call the collection of relevant information the <code>state</code>.
Here the state likely includes your current position and velocity, the position
of the nearest obstacle, weather conditions, etc.</p>

<blockquote>
  <p>Claim 1: if you know the current state of the world and how the world is
evolving, then you can use this to determine the direction to move.</p>
</blockquote>

<p>Note that you don‚Äôt need to look at the whole screen all the time. You can
figure out what will happen to most of the screen by noting that as you run, the
obstacles move down the screen. You only need to look at the top of the screen
to understand the new information and then simulate the rest.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/temple_run_annotated.png" width="800" alt="Temple Run"/>
    <figcaption></figcaption>
  </figure>
</div>


<p>This lends itself to a natural formulation. Let h be the hidden state, relevant
knowledge about the world. Also let x be the input, the observation that you get
each time. h‚Äô then represents the derivative of the hidden state, i.e. how the
state is evolving. We‚Äôre trying to predict y, the optimal next move (right or
left).</p>

<p>Now, Claim 1 states that
<code>from the hidden state h, h‚Äô, and the new observation x, you can figure out y</code>.</p>

<p>More concretely, h, the state, can be represented as a differential equation (Eq
1a):</p><p>

\[h‚Äô(t) = \mathbf{A}h(t) + \mathbf{B}x(t)\]

</p><p>Knowing h allows you to determine your next move y (Eq 1b):</p><p>

\[y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)\]

</p><p>The system evolves as a function of the current state and new observations. A
small new observation is enough because we can determine most of the state by
applying known state dynamics to the previous state. That is, most of the screen
isn‚Äôt new, it‚Äôs just the natural downward movement of the previous state. Fully
knowing the state would allow us to pick the best next move, y.</p>

<p>You can learn a lot about the system dynamics by observing the top of the
screen - if it‚Äôs moving faster, we can infer the whole screen is and the game is
speeding up<sup id="fnref:smooth" role="doc-noteref"><a href="#fn:smooth" rel="footnote">5</a></sup>. In this way, even if we start off knowing nothing about
the game except our limited observation, pretty soon we could understand the
whole screen.</p>

<h3 id="whats-the-state">What‚Äôs the State?</h3>

<p>Here, <strong>state</strong> refers to the variables that, when combined with the input
variables, fully determine the future system behaviour. In theory, once we have
the state, there‚Äôs nothing else we need to know about the past to predict the
future. With this choice of state, the system is converted to a <strong>Markov
Decision Process</strong>. Ideally, the state is a fairly small amount of information
which captures the essential properties of the system. That is, <strong>the state is a
compression of the past</strong> <sup id="fnref:diagonal" role="doc-noteref"><a href="#fn:diagonal" rel="footnote">6</a></sup></p>

<h2 id="discretisation---how-to-deal-with-living-in-a-quantised-world">Discretisation - How To Deal With Living in a Quantised World</h2>

<p>Okay, great! So, given some state and input observation, we have an
autoregressive-style system to determine the next action. Amazing!</p>

<p>In practice though, there‚Äôs a little snag here. We‚Äôre modelling time as
continuous. But in real life, we get new inputs and take new actions at discrete
time steps <sup id="fnref:discrete" role="doc-noteref"><a href="#fn:discrete" rel="footnote">7</a></sup>.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/quantised.png" width="600" alt="Reality is Quantised"/>
    <figcaption></figcaption>
  </figure>
</div>


<p>We would like to convert this <em>continuous-time differential equation</em> into a
<em>discrete-time difference equation</em>. This conversion process is known as
<code>discretisation</code>. Discretisation is a well-studied problem in the literature.
Mamba uses the <a href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero-Order Hold</a>
(ZOH) discretisation<sup id="fnref:zoh" role="doc-noteref"><a href="#fn:zoh" rel="footnote">8</a></sup>. To give an idea of what‚Äôs happening morally,
consider a naive first-order approximation<sup id="fnref:Euler" role="doc-noteref"><a href="#fn:Euler" rel="footnote">9</a></sup>.</p>

<p>From Equation 1a, we have</p><p>

\[h‚Äô(t) = \mathbf{A}h(t) + \mathbf{B}x(t)\]

</p><p>And for small ‚àÜ,</p><p>

\[h‚Äô(t) \approx \frac{h(t+\Delta) - h(t)}{\Delta}\]

</p><p>by the definition of the derivative.</p>

<p>We let:</p><p>

\[h_t = h(t)\]

</p><p>and</p><p>

\[h_{t+1} = h(t + \Delta)\]

</p><p>and substitute into Equation 1a giving:</p><p>

\[h_{t+1} - h_t \approx \Delta (\mathbf{A}h_t + \mathbf{B}x_t)\]

\[\Rightarrow h_{t+1} \approx (I + \Delta \mathbf{A})h_t + (\Delta
\mathbf{B})x_t\]

</p><p>Hence, after renaming the coefficients and relabelling indices, we have the
discrete representations:</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/equation_2.png" width="800" alt="Equation 2"/>
    <figcaption>The Discretised Version of the SSM Equation</figcaption>
  </figure>
</div>


<p>If you‚Äôve ever looked at an RNN before <sup id="fnref:rnn" role="doc-noteref"><a href="#fn:rnn" rel="footnote">10</a></sup> and this feels familiar - <em>trust
your instincts</em>:</p>

<blockquote>
  <p>We have some input x, which is combined with the previous hidden state by some
transform to give the new hidden state. Then we use the hidden state to
calculate the output at each time step.</p>
</blockquote>

<h2 id="understanding-the-ssm-matrices">Understanding the SSM Matrices</h2>

<p>Now, we can interpret the A, B, C, D matrices more intuitively:</p>

<ul>
  <li>A is the transition state matrix. It shows how you transition the current
state into the next state. It asks ‚ÄúHow should I forget the less relevant
parts of the state over time?‚Äù</li>
  <li>B is mapping the new input into the state, asking ‚ÄúWhat part of my new input
should I remember?‚Äù. <sup id="fnref:B" role="doc-noteref"><a href="#fn:B" rel="footnote">11</a></sup></li>
  <li>C is mapping the state to the output of the SSM. It asks, ‚ÄúHow can I use the
state to make a good next prediction?‚Äù. <sup id="fnref:C" role="doc-noteref"><a href="#fn:C" rel="footnote">12</a></sup></li>
  <li>D is how the new input passes through to the output. It‚Äôs a kind of modified
skip connection that asks ‚ÄúHow can I use the new input in my prediction?‚Äù</li>
</ul>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/graphical_matmuls.png" width="600" alt="Visual SSM Equations"/>
    <figcaption>Visual Representation of The SSM Equations</figcaption>
  </figure>
</div>


<p>Additionally, ‚àÜ has a nice interpretation - it‚Äôs the step size, or what we might
call the <code>linger time</code> or the <code>dwell time</code>. For large ‚àÜ, you focus more on that
token; for small ‚àÜ, you skip past the token immediately and don‚Äôt include it
much in the next state.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/mamba_hardware_diagram.png" width="800" alt="Hardware Diagram"/>
    <figcaption></figcaption>
  </figure>
</div>


<p>And that‚Äôs it! That‚Äôs the SSM, our ~drop-in replacement for Attention
(<code>Communication</code>) in the Mamba block. The <code>Computation</code> in the Mamba
architecture comes from regular linear projections, non-linearities, and local
convolutions - the regular ML building blocks we know and love!</p>

<p>Okay great, that‚Äôs the theory - but does this work? Well‚Ä¶</p>

<h2 id="effectiveness-vs-efficiency-attention-is-focus-selectivity-is-prioritisation">Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation</h2>

<p>At WWDC ‚Äò97, Steve Jobs famously noted that
‚Äú<a href="https://www.youtube.com/watch?v=H8eP99neOVs&amp;t=98s">focusing is about saying no</a>‚Äù.
Focus is ruthless prioritisation. It‚Äôs common to think about Attention
<em>positively</em> as choosing what to <em>notice</em>. In the Steve Jobs sense, we might
instead frame Attention <em>negatively</em> as choosing what to <em>discard</em>.</p>

<p>There‚Äôs a classic intuition pump in Machine Learning known as the
<a href="https://ieeexplore.ieee.org/document/8555495">Cocktail Party Problem</a>
<sup id="fnref:alcohol" role="doc-noteref"><a href="#fn:alcohol" rel="footnote">13</a></sup>. Imagine a party with dozens of simultaneous loud conversations:</p>

<p>Question:</p>

<blockquote>
  <p>How do we recognise what one person is saying when others are talking at the
same time? <sup id="fnref:frequency" role="doc-noteref"><a href="#fn:frequency" rel="footnote">14</a></sup></p>
</blockquote>

<p>Answer:</p>

<blockquote>
  <p>The brain solves this problem by focusing your ‚Äúattention‚Äù on a particular
stimulus <em>and hence</em> drowning out all other sounds as much as possible.</p>
</blockquote>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/cocktail_party.png" width="450" alt="Cocktail Party"/>
    <figcaption></figcaption>
  </figure>
</div>


<hr/>



<p>Transformers use Dot-Product Attention to focus in on the most relevant tokens.
A big reason Attention is so great is that you have the potential to look back
at everything that ever happened in its context. This is like photographic
memory when done right. <sup id="fnref:photo" role="doc-noteref"><a href="#fn:photo" rel="footnote">15</a></sup></p>

<p>Transformers (ü§ñ) are extremely <strong>effective</strong>. But they aren‚Äôt very
<strong>efficient</strong>. They store everything from the past so that they can look back at
tokens with theoretically perfect recall.</p>

<p>Traditional RNNs (üîÅ) are the opposite - they forget a lot, only recalling a
small amount in their hidden state and discarding the rest. They are very
<strong>efficient</strong> - their state is small. Yet they are less <strong>effective</strong> as
discarded information cannot be recovered.</p>

<p>We‚Äôd like something closer to the Pareto frontier of the
effectiveness/efficiency tradeoff. Something that‚Äôs more effective than
traditional RNNs and more efficient than transformers.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/pareto_frontier.png" width="800" alt="Pareto Frontier"/>
    <figcaption>The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency.</figcaption>
  </figure>
</div>


<p>SSMs are as <strong>efficient</strong> as RNNs, but we might wonder how <strong>effective</strong> they
are. After all, it seems like they would have a hard time discarding only
<em>unnecessary</em> information and keeping everything relevant. If each token is
being processed the same way, applying the same A and B matrices as if in a
factory assembly line for tokens, there is no context-dependence. We would like
the forgetting and remembering matrices (A and B respectively) to vary and
dynamically adapt to inputs.</p>

<h3 id="the-selection-mechanism">The Selection Mechanism</h3>

<p><strong>Selectivity</strong> allows each token to be transformed into the state in a way that
is unique to its own needs. Selectivity is what takes us from vanilla SSM models
(applying the same A (forgetting) and B (remembering) matrices to every input)
to Mamba, the <strong><em>Selective</em></strong> <em>State Space Model</em>.</p>

<p>In regular SSMs, A, B, C and D are learned matrices - that is</p>

<p>\(\mathbf{A} =
\mathbf{A}_{\theta}\) etc. (where Œ∏ represents the learned
parameters)</p>

<p>With the Selection Mechanism in Mamba, A, B, C and D are also functions of x.
That is \(\mathbf{A} = \mathbf{A}_{\theta(x)}\) etc; the matrices are context
dependent rather than static.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/ssm_algorithm.png" width="800" alt="SSM Algorithm"/>
    <figcaption>Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be <b> selective </b> i.e. context dependent </figcaption>
  </figure>
</div>


<p>Making A and B functions of x allows us to get the best of both worlds:</p>

<ul>
  <li>We‚Äôre selective about what we include in the state, which improves
<strong>effectiveness</strong> vs traditional SSMs.</li>
  <li>Yet, since the state size is bounded, we improve on <strong>efficiency</strong> relative to
the Transformer. We have O(1), not O(n) space and O(n) not O(n¬≤) time
requirements.</li>
</ul>

<p>The Mamba paper authors write:</p>

<blockquote>
  <p>The efficiency vs. effectiveness tradeoff of sequence models is characterized
by how well they compress their state: efficient models must have a small
state, while effective models must have a state that contains all necessary
information from the context. In turn, we propose that a fundamental principle
for building sequence models is selectivity: or the context-aware ability to
focus on or filter out inputs into a sequential state. In particular, a
selection mechanism controls how information propagates or interacts along the
sequence dimension.</p>
</blockquote>



<hr/>



<p>Humans (mostly) don‚Äôt have photographic memory for everything they experience
within a lifetime - or even within a day! There‚Äôs just way too much information
to retain it all. Subconsciously, we select what to remember by choosing to
forget, throwing away most information as we encounter it. Transformers (ü§ñ)
decide what to focus on at <strong>recall time</strong>. Humans (üßë) also decide what to
throw away at <strong>memory-making time</strong>. Humans filter out information early and
often.</p>

<p>If we had infinite capacity for memorisation, it‚Äôs clear the transformer
approach is better than the human approach - it truly is more effective. But
it‚Äôs less efficient - transformers have to store so much information about the
past that might not be relevant. Transformers (ü§ñ) only decide what‚Äôs relevant
at <strong>recall time</strong>. The innovation of Mamba (üêç) is allowing the model better
ways of forgetting earlier - it‚Äôs focusing by choosing what to <em>discard</em> using
<strong>Selectivity</strong>, throwing away less relevant information at <strong>memory-making
time</strong><sup id="fnref:seq_len" role="doc-noteref"><a href="#fn:seq_len" rel="footnote">16</a></sup>.</p>

<h3 id="the-problems-of-selectivity">The Problems of Selectivity</h3>

<p>Applying the Selection Mechanism does have its gotchas though. Non-selective
SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is
because the component of \(y_t\) which depends on \(x_i\) can be expressed
as a linear map, i.e. a single matrix that can be precomputed!</p>

<p>For example (ignoring the D component, the skip connection):</p><p>

\[y_2 = \mathbf{C}\mathbf{B}x_2 + \mathbf{C}\mathbf{A}\mathbf{B}x_1 +
\mathbf{C}\mathbf{A}\mathbf{A}\mathbf{B}x_0\]

</p><p>If we‚Äôre paying attention, we might spot something even better here - this
expression can be written as a convolution. Hence we can apply the Fast Fourier
Transform and the Convolution Theorem to compute this <em>very</em> efficiently on
hardware as in Equation 3 below.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/equations_2_3.png" width="800" alt="Equations 2 and 3"/>
    <figcaption>We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3. </figcaption>
  </figure>
</div>


<p>Unfortunately, with the Selection Mechanism, we lose the convolutional form.
Much attention is given to making Mamba efficient on modern GPU hardware using
similar hardware optimisation tricks to Tri Dao‚Äôs Flash Attention <sup id="fnref:cuda" role="doc-noteref"><a href="#fn:cuda" rel="footnote">17</a></sup>. With
the hardware optimisations, Mamba is able to run faster than comparably sized
Transformers.</p>

<h3 id="machine-learning-for-political-economists---how-large-should-the-state-be">Machine Learning for Political Economists - How Large Should The State Be?</h3>

<p>The Mamba authors write, ‚Äúthe efficiency vs. effectiveness tradeoff of sequence
models is characterised by how well they compress their state‚Äù. In other words,
like in political economy<sup id="fnref:oop" role="doc-noteref"><a href="#fn:oop" rel="footnote">18</a></sup>, the fundamental problem is how to manage the
state.</p>

<p>üîÅ <strong>Traditional RNNs are anarchic</strong></p>

<blockquote>
  <p>They have a small, minimal state. The size of the state is bounded. The
compression of state is poor.</p>
</blockquote>

<p>ü§ñ <strong>Transformers are communist</strong></p>

<blockquote>
  <p>They have a maximally large state. The ‚Äústate‚Äù is just a cache of the entire
history with no compression. Every context token is treated equally until
recall time.</p>
</blockquote>

<p>üêç<strong>Mamba has a compressed state</strong></p>

<blockquote>
  <p>‚Ä¶but it‚Äôs selective about what goes in. Mamba says we can get away with a
small state if the state is well focused and effective <sup id="fnref:politik" role="doc-noteref"><a href="#fn:politik" rel="footnote">19</a></sup>.</p>
</blockquote>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/political_spectrum.png" width="800" alt="Language Models and State Size"/>
    <figcaption>Language Models and State Size</figcaption>
  </figure>
</div>


<p>The upshot is <strong>state representation is critical</strong>. A smaller state is more
efficient; a larger state is more effective. The key is to <strong>selectively</strong> and
<strong>dynamically</strong> compress data into the state. Mamba‚Äôs Selection Mechanism allows
for context-dependent reasoning, focusing and ignoring. For both performance and
interpretability, understanding the state seems to be very useful.</p>

<h2 id="information-flow-in-transformer-vs-mamba">Information Flow in Transformer vs Mamba</h2>

<p>How do Transformers know anything? At initialisation, a transformer isn‚Äôt very
smart. It learns in two ways:</p>

<ol>
  <li>Training data (Pretraining, SFT, RLHF etc)</li>
  <li>In context-data</li>
</ol>

<h4 id="training-data">Training Data</h4>

<p>Models learn from their training data. This is a kind of lossy compression of
input data into the weights. We can think of the effect of pretraining data on
the transformer kinda like the effect of your ancestor‚Äôs experiences on your
genetics - you can‚Äôt recall their experiences, you just have vague instincts
about them <sup id="fnref:analogy" role="doc-noteref"><a href="#fn:analogy" rel="footnote">20</a></sup>.</p>

<h4 id="in-context-data">In Context-Data</h4>

<p>Transformers use their context as short-term memory, which they can recall with
~perfect fidelity. So we get
<a href="https://thegradient.pub/in-context-learning-in-context/">In-Context Learning</a>,
e.g. using induction heads to solve the
<a href="https://arxiv.org/pdf/2211.00593.pdf">Indirect Object Identification</a> task, or
<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf">computing Linear Regression</a>.</p>

<h4 id="retrieval">Retrieval</h4>

<p>Note that Transformers don‚Äôt filter their context at all until recall time. So
if we have a bunch of information we think <em>might</em> be useful to the Transformer,
we filter it <em>outside</em> the Transformer (using Information Retrieval strategies)
and then stuff the results into the prompt. This process is known as Retrieval
Augmented Generation (RAG). RAG determines relevant information for the context
window of a transformer. A human with the internet is kinda like a RAG system -
you still have to know what to search but whatever you retrieve is as salient as
short-term memory to you.</p>

<h4 id="information-flow-for-mamba">Information Flow for Mamba</h4>

<p>Training Data acts similarly for Mamba. However, the lines are slightly blurred
for in-context data and retrieval. In-context data for Mamba <em>is</em>
compressed/filtered similar to retrieval data for transformers. This in-context
data is also accessible for look-up like for transformers (although with
somewhat lower fidelity).</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/information_flow.png" width="800" alt="The Information Flow in Mamba"/>
    <figcaption></figcaption>
  </figure>
</div>


<p>Transformer context is to Mamba states what short-term is to long-term memory.
Mamba doesn‚Äôt just have ‚ÄúRAM‚Äù, it has a hard drive<sup id="fnref:hard-drive" role="doc-noteref"><a href="#fn:hard-drive" rel="footnote">21</a></sup> <sup id="fnref:ssd" role="doc-noteref"><a href="#fn:ssd" rel="footnote">22</a></sup>.</p>

<h3 id="swapping-states-as-a-new-prompting-paradigm">Swapping States as a New Prompting Paradigm</h3>

<p>Currently, we often use RAG to give a transformer contextual information.</p>

<p>With Mamba-like models, you could instead imagine having a library of states
created by running the model over specialised data. States could be shared kinda
like
<a href="https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language">LoRAs</a>
for image models.</p>

<p>For example, I could do inference on 20 physics textbooks and, say, 100 physics
questions and answers. Then I have a state which I can give to you. Now you
don‚Äôt need to add any few-shot examples; you just simply ask your question.
<strong>The in-context learning is in the state</strong>.</p>

<p>In other words, you can drag and drop downloaded states into your model, like
literal plug-in cartridges. And note that ‚Äútraining‚Äù a state doesn‚Äôt require any
backprop. It‚Äôs more like a highly specialised one-pass fixed-size compression
algorithm. This is unlimited in-context learning applied at inference time for
zero-compute or latency. <sup id="fnref:steering" role="doc-noteref"><a href="#fn:steering" rel="footnote">23</a></sup></p>

<p>The structure of an effective LLM call goes from‚Ä¶</p>

<ol>
  <li>System Prompt</li>
  <li>Preamble</li>
  <li>Few shot-examples</li>
  <li>Question</li>
</ol>

<p>‚Ä¶for Transformers, to simply‚Ä¶</p>

<ol>
  <li>Inputted state (with problem context, initial instructions, textbooks, and
few-shot examples)</li>
  <li>Short question</li>
</ol>

<p>‚Ä¶for Mamba.</p>

<p>This is cheaper and faster than few-shot prompting (as the state is infinitely
reusable without inference cost). It‚Äôs also MUCH cheaper than finetuning and
doesn‚Äôt require any gradient updates. We could imagine retrieving states in
addition to context.</p>

<h2 id="mamba--mechanistic-interpretability">Mamba &amp; Mechanistic Interpretability</h2>

<p>Transformer interpretability typically involves:</p>

<ol>
  <li>understanding token relationships via attention,</li>
  <li>understanding circuits, and</li>
  <li>using
<a href="https://www.kolaayonrinde.com/blog/2023/11/03/dictionary-learning.html">Dictionary Learning</a>
for unfolding MLPs.</li>
</ol>

<p>Most of the ablations that we would like to do for Mamba are still valid, but
understanding token communication (1) is now more nuanced. All information moves
between tokens via hidden states instead of the Attention Mechanism which can
‚Äúteleport‚Äù information from one sequence position to another.</p>

<p>For understanding in-context learning (ICL) tasks with Mamba, we will look to
intervene on the SSM state. A classic task in-context learning task is
<a href="https://arxiv.org/pdf/2211.00593.pdf">Indirect Object Identification</a> in which
a model has to finish a paragraph like:</p>

<blockquote>
  <p><em>Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an
apple to [BLANK]</em></p>
</blockquote>

<p>The model is expected to fill in the blank with the name that is not repeated in
the paragraph. In the chart below we can see that information is passed from the
<code>[Shelby/Emma]</code> position to the final position via the hidden state (see the two
blue lines in the top chart).</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/patching_state.png" width="800" alt="Patching State"/>
    <figcaption></figcaption>
  </figure>
</div>


<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/patching_residual_stream.png" width="800" alt="Patching Residual Stream"/>
    <figcaption></figcaption>
  </figure>
</div>


<p>Since it‚Äôs hypothesised that much of In-Context Learning in Transformers is
downstream of more primitive sequence position operations (like
<a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Induction Heads</a>),
Mamba being able to complete this task suggests a more general In-Context
Learning ability.</p>

<h2 id="whats-next-for-mamba--ssms">What‚Äôs Next for Mamba &amp; SSMs?</h2>

<p>Mamba-like models are likely to excel in scenarios requiring extremely long
context and long-term memory. Examples include:</p>

<ul>
  <li>Processing DNA</li>
  <li>Generating (or reasoning over) video</li>
  <li>Writing novels</li>
</ul>

<p>An illustrative example is agents with long-term goals.</p>

<blockquote>
  <p>Suppose you have an agent interacting with the world. Eventually, its
experiences become too much for the context window of a transformer. The agent
then has to compress or summarise its experiences into some more compact
representation.</p>

  <p>But how do you decide what information is the most useful as a summary? If the
task is language, LLMs are actually fairly good at summaries - okay, yeah,
you‚Äôll lose some information, but the most important stuff can be retained.</p>

  <p>However, for other disciplines, it might not be clear how to summarise. For
example, what‚Äôs the best way to summarise a 2 hour movie? <sup id="fnref:movie" role="doc-noteref"><a href="#fn:movie" rel="footnote">24</a></sup>. Could the
model itself learn to do this naturally rather than a hacky workaround like
trying to describe the aesthetics of the movie in text?</p>
</blockquote>

<p>This is what Mamba allows. Actual long-term memory. A real state where the model
learns to keep what‚Äôs important.
<a href="https://arxiv.org/pdf/2309.10668.pdf">Prediction is compression</a> - learning
what‚Äôs useful to predict what‚Äôs coming next inevitably leads to building a
useful compression of the previous tokens.</p>



<hr/>



<p>The implications for Assistants are clear:</p>

<p>Your chatbot co-evolves with you. It remembers.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/her.png" width="800" alt="Her"/>
    <figcaption>The film HER is looking better and better as time goes on üò≥</figcaption>
  </figure>
</div>


<h3 id="agents--ai-safety">Agents &amp; AI Safety</h3>

<p>One reason for positive updates in existential risk from AGI is Language Models.
Previously, Deep-RL agents trained via self-play looked set to be the first
AGIs. Language models are inherently much safer since they aren‚Äôt trained with
long-term goals. <sup id="fnref:safe" role="doc-noteref"><a href="#fn:safe" rel="footnote">25</a></sup></p>

<p>The potential for long-term sequence reasoning here brings back the importance
of agent-based AI safety. Few agent worries are relevant to Transformers with an
8k context window. Many are relevant to systems with impressive long-term
memories and possible instrumental goals.</p>

<h3 id="the-best-collab-since-taco-bell--kfc--x-">The Best Collab Since Taco Bell &amp; KFC: ü§ñ x üêç</h3>

<p>The Mamba authors show that there‚Äôs value in combining Mamba‚Äôs long context with
the Transformer‚Äôs high fidelity over short sequences. For example, if you‚Äôre
making long videos, you likely can‚Äôt fit a whole movie into a Transformer‚Äôs
context for attention <sup id="fnref:image" role="doc-noteref"><a href="#fn:image" rel="footnote">26</a></sup>. You could imagine having Attention look at the
most recent frames for short-term fluidity and an SSM for long-term narrative
consistency <sup id="fnref:optimisation" role="doc-noteref"><a href="#fn:optimisation" rel="footnote">27</a></sup>.</p>



<hr/>



<p>This isn‚Äôt the end for Transformers. Their high effectiveness is exactly what‚Äôs
needed for many tasks. But now Transformers aren‚Äôt the only option. Other
architectures are genuinely feasible.</p>

<p>So we‚Äôre not in the post-<code>Transformer</code> era. But for the first time, we‚Äôre living
in the post-<code>only-Transformers</code> era <sup id="fnref:other-models" role="doc-noteref"><a href="#fn:other-models" rel="footnote">28</a></sup>. And this blows the
possibilities wide open for sequence modelling with extreme context lengths and
native long-term memory.</p>

<p>Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell
Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard
Professor), currently have a bet <a href="http://www.isattentionallyouneed.com/">here</a>.</p>

<div>
  <figure>
    <img src="http://pepijndevos.nl/blog/images/mamba/attention_wager.png" width="800" alt="Attention Wager"/>
    <figcaption></figcaption>
  </figure>
</div>


<p>Currently Transformers are far and away in the lead. With 3 years left, there‚Äôs
now a research direction with a fighting chance.</p>

<p>All that remains to ask is: <code>Is Attention All We Need?</code></p>



<p><em>Thanks to Gon√ßalo for reading an early draft, Jaden for the nnsight library
used for the Interpretability analysis and Tessa for Mamba patching
visualisations.</em></p>

<p><em>Also see: <a href="https://arxiv.org/pdf/2312.00752.pdf">Mamba paper</a>, Mamba Python
code, <a href="https://srush.github.io/annotated-s4/">Annotated S4</a>,
<a href="https://www.cognitiverevolution.ai/emergency-pod-mamba-memory-and-the-ssm-moment/">Nathan Labenz podcast</a></em></p>




  </div></div>
  </body>
</html>
