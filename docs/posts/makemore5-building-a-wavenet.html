<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/makemore5-building-a-wavenet/">Original</a>
    <h1>Makemore5 Building a WaveNet</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>A look at episode #6: <a href="https://youtu.be/t3YJ5hKiMQ0?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener noreffer ">The spelled-out intro to language modeling: Building makemore Part 5: Building a WaveNet</a> from <a href="https://karpathy.ai/" target="_blank" rel="noopener noreffer ">Andrej Karpathy</a> amazing tutorial series.</p>

<p>
  <iframe src="https://www.youtube.com/embed/t3YJ5hKiMQ0" allowfullscreen="" title="YouTube Video"></iframe>
</p>

<p>Starting from the makemore3 (3-gram character-level MLP model) code as a base. It implements a deepter more structured model (while maintaining roughly the same number of parameters) to improve the loss.</p>
<h2 id="improve-the-structure-of-the-code">Improve the structure of the code</h2>
<p>The first half of the video focus on bringing more structure to the code. The forward pass get refactored into a model class heavily inspired by PyTorch <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreffer ">torch.NN</a>. The loss visualization also receive some love, and get smoothed out by taking the mean of the loss over a bunch of batches to improve legibility.</p>
<p>Going from:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>plt</span><span>.</span><span>plot</span><span>(</span><span>lossi</span><span>)</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/makemore5-building-a-wavenet/loss-before.png" title="loss-before" data-thumbnail="loss-before.png" data-sub-html="&lt;h2&gt;Loss before&lt;/h2&gt;&lt;p&gt;loss-before&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="loss-before.png" data-srcset="loss-before.png, loss-before.png 1.5x, loss-before.png 2x" data-sizes="auto" alt="loss-before.png"/>
    </a><figcaption>Loss before</figcaption>
    </figure>
<p>To:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>plt</span><span>.</span><span>plot</span><span>(</span><span>torch</span><span>.</span><span>tensor</span><span>(</span><span>lossi</span><span>)</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>,</span> <span>1000</span><span>)</span><span>.</span><span>mean</span><span>(</span><span>1</span><span>))</span>
</span></span></code></pre></div><figure><a href="https://swe-to-mle.pages.dev/posts/makemore5-building-a-wavenet/loss-after.png" title="loss-before" data-thumbnail="loss-after.png" data-sub-html="&lt;h2&gt;Loss after&lt;/h2&gt;&lt;p&gt;loss-before&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="loss-after.png" data-srcset="loss-after.png, loss-after.png 1.5x, loss-after.png 2x" data-sizes="auto" alt="loss-after.png"/>
    </a><figcaption>Loss after</figcaption>
    </figure>
<p><img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="woah.gif" data-srcset="woah.gif, woah.gif 1.5x, woah.gif 2x" data-sizes="auto" alt="woah.gif" title="woah"/></p>
<h2 id="changing-the-model-architecture">Changing the model architecture</h2>
<p>The architecture change is inspired by the <a href="https://arxiv.org/abs/1609.03499" target="_blank" rel="noopener noreffer ">WaveNet 2016 paper</a> focussing on the <em>stack of dilated causal convolutional layers</em> (aka. merging character embeddings 2 by 2 in the shape of a binary tree instead of all at once).</p>
<figure><a href="https://swe-to-mle.pages.dev/posts/makemore5-building-a-wavenet/wavenet.png" title="wavenet" data-thumbnail="wavenet.png" data-sub-html="&lt;h2&gt;WaveNet&#39;s stack of dilated causal convolutional layers&lt;/h2&gt;&lt;p&gt;wavenet&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="wavenet.png" data-srcset="wavenet.png, wavenet.png 1.5x, wavenet.png 2x" data-sizes="auto" alt="wavenet.png"/>
    </a><figcaption>WaveNet&#39;s stack of dilated causal convolutional layers</figcaption>
    </figure>
<p>This is meant to preserve more of the meaning associated with each pair of embeddings by incorporating them together more gradually. Anyone else gets reminded of a cooking recipe here?</p>
<p><em>Gently fold the <del>beaten egg whites</del> <strong>embeddings</strong> into the rich, melted <del>chocolate mixture</del> <strong>causal layers</strong>, taking care not to deflate them.</em></p>
<figure><a href="https://swe-to-mle.pages.dev/posts/makemore5-building-a-wavenet/fold.png" title="fold" data-thumbnail="fold.png" data-sub-html="&lt;h2&gt;Folding egg whites to make a chocolate mousse&lt;/h2&gt;&lt;p&gt;fold&lt;/p&gt;">
        <img src="https://swe-to-mle.pages.dev/svg/loading.min.svg" data-src="fold.png" data-srcset="fold.png, fold.png 1.5x, fold.png 2x" data-sizes="auto" alt="fold.png"/>
    </a><figcaption>Folding egg whites to make a chocolate mousse</figcaption>
    </figure>
<h3 id="why-does-wavenet-use-a-convolutional-architecture">Why does WaveNet use a convolutional architecture?</h3>
<p>TL;DR: It’s faster.</p>
<p>Convolutional NN let you do efficient sliding window operations by (1) Running all in CUDA-land instead of a python loop, and (2) by making it possible to re-use shared state instead of recomputing everything for each offset of the sliding window.</p>
<h2 id="the-code">The code</h2>
<p>Here’s my take on the tutorial with additional notes. You can get the code on <a href="https://github.com/peluche/makemore" target="_blank" rel="noopener noreffer ">GitHub</a> or bellow.</p>


</div></div>
  </body>
</html>
