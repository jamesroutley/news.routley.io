<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://community.amd.com/t5/ai/amd-unveils-its-first-small-language-model-amd-135m/ba-p/711368">Original</a>
    <h1>AMD Unveils Its First Small Language Model AMD-135M</h1>
    
    <div id="readability-page-1" class="page"><div>
		<div itemprop="text" id="bodyDisplay">
	
		<div>
			
				
					
					
						<p>In the ever-evolving landscape of artificial intelligence, large language models (LLMs) like GPT-4 and Llama have garnered significant attention for their impressive capabilities in natural language processing and generation. However, small language models (SLMs) are emerging as an essential counterpart in the AI model community offering a unique advantage for specific use cases.  AMD is excited to release its very first small language model, AMD-135M with Speculative Decoding.  This work demonstrates the commitment to an open approach to AI which will lead to more inclusive, ethical, and innovative technological progress, helping ensure that its benefits are more widely shared, and its challenges more collaboratively addressed. </p>
<p><strong>AMD-135M: First AMD Small Language Model </strong></p>
<p>AMD-135M is the first small language model for Llama family that was trained from scratch on AMD Instinct™ MI250 accelerators utilizing 670B tokens and divided into two models: AMD-Llama-135M and AMD-Llama-135M-code.</p>
<ul>
<li><strong>Pretraining</strong>: The AMD-Llama-135M model was trained from scratch with 670 billion tokens of general data over six days using four MI250 nodes. </li>
<li><strong>Code Finetuning</strong>: The AMD-Llama-135M-code variant was fine-tuned with an additional 20 billion tokens of code data, taking four days on the same hardware. </li>
</ul>
<p>The training code, dataset and weights for this model are open sourced so that developers can reproduce the model and help train other SLMs and LLMs.</p>
<p><strong>Optimization with Speculative Decoding </strong></p>
<p>Large language models typically use an autoregressive approach for inference. However, a major limitation of this approach is that each forward pass can only generate a single token, resulting in low memory access efficiency and affecting overall inference speed.</p>
<p>The advent of speculative decoding has solved this problem. The basic principle involves using a small draft model to generate a set of candidate tokens, which are then verified by the larger target model. This approach allows each forward pass to generate multiple tokens without compromising performance, thereby significantly reducing memory access consumption, and enabling several orders of magnitude speed improvements.</p>
<p><strong>Inference Performance Acceleration</strong></p>
<p>Using AMD-Llama-135M-code as a draft model for CodeLlama-7b, we tested the inference performance with and without speculative decoding on the MI250 accelerator for data center, and Ryzen™ AI processor (with NPU) for AI PC. For the particular configurations that we tested using AMD-Llama-135M-code as the draft model, we saw a<span> </span>speedup on the Instinct MI250 accelerator, Ryzen AI CPU<SPAN size="2">[2]</SPAN>, and on Ryzen AI NPU<SPAN size="2">[2]</SPAN><span> </span>versus the inference without speculative decoding.[3] The AMD-135M SLM establishes an end-to-end workflow, encompassing both training and inferencing, on select AMD platforms.</p>

<p><u><strong>Next Steps</strong></u></p>
<p><strong>Additional Resources </strong></p>
<ul>
<li>For information about the training, inferencing and insights of this model, please visit <a id="menuro5m" title="https://github.com/amd-aig-aima/amd-llm" href="https://github.com/AMD-AIG-AIMA/AMD-LLM" target="_blank" rel="noreferrer noopener nofollow" aria-label="Link AMD Github">AMD Github</a> repository to get access to the code.</li>
<li>Visit Hugging Face <a id="menuro5o" title="https://huggingface.co/amd/amd-llama-135m" href="https://huggingface.co/amd/AMD-Llama-135m" target="_blank" rel="noreferrer noopener nofollow" aria-label="Link Model Card">Model Card</a> to download the model file.</li>
<li>Apply for Instinct accelerator card access on the <a id="menuro5q" title="https://www.amd.com/en/forms/registration/developer-cloud-application.html" href="https://www.amd.com/en/forms/registration/developer-cloud-application.html" target="_blank" rel="noreferrer noopener" aria-label="Link AMD Developer Cloud">AMD Developer Cloud</a>.</li>
<li>For any questions, contact us by email <a id="menuro5s" title="mailto:amd_ai_mkt@amd.com" href="mailto:amd_ai_mkt@amd.com" target="_blank" rel="noreferrer noopener nofollow" aria-label="Link amd_ai_mkt@amd.com">amd_ai_mkt@amd.com</a>.  </li>
</ul>
<p>Explore, innovate, and together, let us push the boundaries of AI. </p>

<p><SPAN size="2">Footnotes</SPAN></p>
<p><SPAN size="2"><span>[1] The training code for AMD-135M is based on </span><span>TinyLlama</span><span>, utilizing multi-node distributed training with PyTorch FSDP. </span><span data-ccp-props="{&#34;134233117&#34;:true,&#34;201341983&#34;:0,&#34;335551550&#34;:6,&#34;335551620&#34;:6,&#34;335559739&#34;:200,&#34;335559740&#34;:360}"> </span></SPAN></p>
<p><SPAN size="2"><span>[2] Test ran on AMD Ryzen 9 PRO 7940HS with Radeon 780M Graphics. The Ryzen AI APU Architecture includes CPU and NPU kernels. </span><span data-ccp-props="{&#34;134233117&#34;:true,&#34;201341983&#34;:0,&#34;335551550&#34;:6,&#34;335551620&#34;:6,&#34;335559739&#34;:200,&#34;335559740&#34;:360}"> </span></SPAN></p>
<p><SPAN size="2"><span>[3] These are the configurations that we tested. You might get different results on other configurations. </span><span data-ccp-props="{&#34;134233117&#34;:true,&#34;201341983&#34;:0,&#34;335551550&#34;:6,&#34;335551620&#34;:6,&#34;335559739&#34;:200,&#34;335559740&#34;:360}"> </span></SPAN></p>
<p><SPAN size="2"><span>[4] The performance had been tested on AMD Instinct MI250 + </span><span>ROCm</span><span>TM</span><span> 6.0 using standardized tests with </span><a href="https://github.com/EleutherAI/lm-evaluation-harness%22%20/t%20%22_blank" target="_blank" rel="noopener nofollow noreferrer"><span>lm-evaluation-harness</span></a><span>. Additionally, the model performance tests are independent of the hardware environment. </span><span data-ccp-props="{&#34;134233117&#34;:true,&#34;201341983&#34;:0,&#34;335551550&#34;:6,&#34;335551620&#34;:6,&#34;335559739&#34;:200,&#34;335559740&#34;:360}"> </span></SPAN></p>
<p><SPAN size="2"><span>[5] </span><span>Hellaswag</span><span> is dataset and metrics that tests how well that LLMs can reason about physical </span><span>situations;</span><span data-contrast="auto">  </span><span data-ccp-props="{&#34;134233117&#34;:true,&#34;201341983&#34;:0,&#34;335551550&#34;:6,&#34;335551620&#34;:6,&#34;335559739&#34;:200,&#34;335559740&#34;:360}"> </span></SPAN></p>
<p><SPAN size="2"><span>WinoGrande</span><span> is a dataset and codebase for evaluating natural language understanding models on a challenging task of Winograd Schema;  </span></SPAN></p>
<p><SPAN size="2"><span>SciQ is a dataset of closed-domain question answering tasks with text inputs and outputs;  </span></SPAN></p>
<p><SPAN size="2"><span>MMLU is a dataset of multiple-choice questions on abstract algebra topics, such as groups, rings, fields, and polynomials; </span></SPAN></p>
<p><SPAN size="2"><span>ARC-Easy is a dataset of grade-school level science questions for testing advanced question answering systems. </span></SPAN></p>
<p><SPAN size="2"><span>SlimPajama is a deduplicated version of RedPajama and sources from Commoncrawl, C4, GitHub, Books, ArXiv, Wikpedia and StackExchange. We drop the Books data from SlimPajama due to license issues; </span></SPAN></p>
<p><SPAN size="2"><span>[6] Test ran on AMD Ryzen 9 PRO 7940HS w/ Radeon 780M Graphics. The Ryzen AI APU Architecture includes CPU and NPU kernels. </span></SPAN></p>

					
				
			
			
			
				
			
			
			
			
			
			
		</div>
		
		
	

	
	
</div>
	</div></div>
  </body>
</html>
