<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">Original</a>
    <h1>Tackling multiple tasks with a single visual language model</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><p>One key aspect of intelligence is the ability to quickly learn how to perform a new task when given a brief instruction. For instance, a child may recognise real animals at the zoo after seeing a few pictures of the animals in a book, despite any differences between the two. But for a typical visual model to learn a new task, it must be trained on tens of thousands of examples specifically labelled for that task. If the goal is to count and identify animals in an image, as in “three zebras”, one would have to collect thousands of images and annotate each image with their quantity and species. This process is inefficient, expensive, and resource-intensive, requiring large amounts of annotated data and the need to train a new model each time it’s confronted with a new task. As part of DeepMind’s mission to solve intelligence, we’ve explored whether an alternative model could make this process easier and more efficient, given only limited task-specific information.</p><p>Today, in the preprint of our <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf" target="_blank">paper</a>, we introduce <em>Flamingo, </em>a single visual language model (VLM) that sets a new state of the art in few-shot learning on a wide range of open-ended multimodal tasks. This means Flamingo can tackle a number of difficult problems with just a handful of task-specific examples (in a “few shots”), without any additional training required. Flamingo’s simple interface makes this possible, taking as input a prompt consisting of interleaved images, videos, and text and then output associated language. </p><p>Similar to the behaviour of <a href="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval">large language models</a> (LLMs), which can address a language task by processing examples of the task in their text prompt, Flamingo’s visual and text interface can steer the model towards solving a multimodal task. Given a few example pairs of visual inputs and expected text responses composed in Flamingo’s prompt, the model can be asked a question with a new image or video, and then generate an answer. </p><p><figure>
  <video autoplay="" muted="" loop="">
  	<source src="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/Fig_01.mp4" type="video/mp4"/>
  </video>
	
	<figcaption>
	  Figure 1. Given the two examples of animal pictures and a text identifying their name and a comment about where they can be found, Flamingo can mimic this style given a new image to output a relevant description: “This is a flamingo. They are found in the Caribbean.”.
	</figcaption>
</figure></p><p>Of the 16 tasks we studied, Flamingo beats all previous few-shot learning approaches when given as few as four examples per task. In several cases, the same Flamingo<em> </em>model outperforms methods that are fine-tuned and optimised for each task independently and uses multiple orders of magnitude more task-specific data. This should allow non-expert people to quickly and easily use accurate visual language models on new tasks at hand.</p><figure><p><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/626a705661073c5f801de094_Fig_02.svg" loading="lazy" alt=""/></p><figcaption>Figure 2. <strong>Left:</strong> Few-shot performance of the Flamingo across 16 different multimodal tasks against task specific state-of-the-art performance. <strong>Right:</strong> Examples of expected inputs and outputs for three of our 16 benchmarks.</figcaption></figure><p>In practice, Flamingo fuses large language models with powerful visual representations – each separately pre-trained and frozen – by adding novel architecture components in between. Then it is trained on a mixture of complementary large-scale multimodal data coming only from the web, without using any data annotated for machine learning purposes. Following this method, we start from <a href="https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training">Chinchilla</a>, our recently introduced compute-optimal 70B parameter language model, to train our final Flamingo<em> </em>model, an 80B parameter VLM. After this training is done, Flamingo can be directly adapted to vision tasks via simple few-shot learning without any additional task-specific tuning.</p><p>We also tested the model’s qualitative capabilities beyond our current benchmarks. As part of this process, we compared our model&#39;s performance when captioning images related to gender and skin colour, and ran our model&#39;s generated captions through Google&#39;s Perspective API, which evaluates toxicity of text. While the initial results are positive, more research towards evaluating ethical risks in multimodal systems is crucial and we urge people to evaluate and consider these issues carefully before thinking of deploying such systems in the real world.</p><p>Multimodal capabilities are essential for important AI applications, such as <a href="https://vizwiz.org/tasks-and-datasets/vqa/">aiding the visually impaired</a> with everyday visual challenges or <a href="https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set/">improving the identification of hateful content</a> on the web. Flamingo makes it possible to efficiently adapt to these examples and other tasks on-the-fly without modifying the model. Interestingly, the model demonstrates out-of-the-box abilities of multimodal dialogue, as seen here.</p><div><figure>
	
	
	<figcaption>
	  Figure 3 - Flamingo can engage in multimodal dialogue out of the box, seen here discussing an unlikely &#34;soup monster&#34; image generated by <a href="#https://openai.com/dall-e-2/" target="_blank">OpenAI&#39;s DALL·E 2</a> (left), and passing and identifying the famous <a href="https://en.wikipedia.org/wiki/Stroop_effect" target="_blank">Stroop test</a> (right).
	</figcaption>
</figure></div><p>Flamingo is an effective and efficient general-purpose family of models that can be applied to image and video understanding tasks with minimal task-specific examples. Models like Flamingo hold great promise to benefit society in practical ways and we’re continuing to improve their flexibility and capabilities so they can be safely deployed for the benefit of everyone. Flamingo’s abilities pave the way towards rich interactions with learned visual language models that can enable better interpretability and exciting new applications, like a visual assistant which helps people in everyday life – and we’re delighted by the results so far.</p></div></div></div><div><div><div><p>Notes</p><p>We would like to thank the whole team behind the Flamingo project: Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. We would also like to thank the blog post contributors: Aliya Ahmad, Dominic Barlow, Arielle Bier, Matt Botvinick, Jordan Hoffmann, Gaby Pearl, and Emma Yousif.</p></div></div></div></div>
  </body>
</html>
