<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Nixtla/statsforecast/tree/main/experiments/m3">Original</a>
    <h1>Statistical vs. Deep Learning forecasting methods</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">Comparison of several Deep Learning models and ensembles to classical statistical univariate models for the 3,003 series of the M3 competition.</p>
<h2 dir="auto"><a id="user-content-abstract" aria-hidden="true" href="#abstract"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Abstract</h2>
<p dir="auto">We present a reproducible experiment that shows that:</p>
<ol dir="auto">
<li>
<p dir="auto">A simple statistical ensemble outperforms most individual deep-learning models.</p>
</li>
<li>
<p dir="auto">A simple statistical ensemble is 25,000 faster and only slightly less accurate than an ensemble of deep learning models.</p>
</li>
</ol>
<p dir="auto">In other words, deep-learning ensembles outperform statistical ensembles just by 0.36 points in SMAPE. However, the DL ensemble takes more than 14 days to run and costs around USD 11,000, while the statistical ensemble takes 6 minutes to run and costs $0.5c.</p>
<h2 dir="auto"><a id="user-content-background" aria-hidden="true" href="#background"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Background</h2>
<p dir="auto">In <a href="https://www.tandfonline.com/doi/full/10.1080/01605682.2022.2118629" rel="nofollow">Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward</a>, Makridakis and other prominent participants of the forecasting science community compare several Deep Learning and Statistical models for all 3,003 series of the M3 competition.</p>
<blockquote>
<p dir="auto">The purpose of [the] paper is to test empirically the value currently added by Deep Learning (DL) approaches in time series forecasting by comparing the accuracy of some state-of-theart DL methods with that of popular Machine Learning (ML) and statistical ones.</p>
</blockquote>
<p dir="auto">The authors conclude that:</p>
<blockquote>
<p dir="auto">We find that combinations of DL models perform better than most standard models, both statistical and ML, especially for the case of monthly series and long-term forecasts.</p>
</blockquote>
<p dir="auto">We don&#39;t think that&#39;s the full picture.</p>
<p dir="auto">By including a statistical ensemble, we show that these claims are not completely warranted and that one should rather conclude that, for this setting at least, Deep Learning is rather unattractive.</p>
<h2 dir="auto"><a id="user-content-experiment" aria-hidden="true" href="#experiment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Experiment</h2>
<p dir="auto">Building upon the original design, we further included <a href="https://www.sciencedirect.com/science/article/abs/pii/S0169207019300585" rel="nofollow">A simple combination of univariate models</a> in the comparison.</p>
<p dir="auto">This ensemble is formed by averaging four statistical models: <a href="https://www.jstatsoft.org/article/view/v027i03" rel="nofollow"><code>AutoARIMA</code></a>, <a href="https://robjhyndman.com/expsmooth/" rel="nofollow"><code>ETS</code></a>, <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/nav.22074" rel="nofollow"><code>CES</code></a> and <a href="https://doi.org/10.1016/j.ijforecast.2016.02.005" rel="nofollow"><code>DynamicOptimizedTheta</code></a>. This combination won sixth place and was the simplest ensemble among the top 10 performers in the M4 competition.</p>
<p dir="auto">For the experiment, we use StatsForecast&#39;s implementation of <a href="https://nixtla.github.io/statsforecast/models.html#autoarima" rel="nofollow">Arima</a>, <a href="https://nixtla.github.io/statsforecast/models.html#autoets" rel="nofollow">ETS</a>, <a href="https://nixtla.github.io/statsforecast/models.html#autoces" rel="nofollow">CES</a> and <a href="https://nixtla.github.io/statsforecast/models.html#dynamic-optimized-theta-method" rel="nofollow">DOT</a>.</p>
<p dir="auto">For the DL models and ensembles, we reproduce the reported metrics and results from the mentioned paper.</p>
<h2 dir="auto"><a id="user-content-results" aria-hidden="true" href="#results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Results</h2>
<h3 dir="auto"><a id="user-content-accuracy-comparison-with-sota-benchmarks" aria-hidden="true" href="#accuracy-comparison-with-sota-benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Accuracy: Comparison with SOTA benchmarks</h3>
<p dir="auto">Accuracy is reported in Symmetric mean absolute percentage error (<a href="https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error" rel="nofollow">SMAPE</a>)</p>
<p dir="auto">The M3 dataset has four groups of time series. In the next graph, you can see the performance of all models and ensembles.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/10517170/204959437-6a124ad6-a1b5-47c7-ba24-91d447efb1ce.png"><img width="734" alt="image" src="https://user-images.githubusercontent.com/10517170/204959437-6a124ad6-a1b5-47c7-ba24-91d447efb1ce.png"/></a></p>
<p dir="auto">In the next table, you can see the performance of the models across all four groups and the average performance for all groups.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/10517170/204958689-38cdea5f-58d7-42f5-b825-0f2a0b63f617.png"><img width="734" alt="image" src="https://user-images.githubusercontent.com/10517170/204958689-38cdea5f-58d7-42f5-b825-0f2a0b63f617.png"/></a></p>
<h3 dir="auto"><a id="user-content-computational-complexity-comparison-with-sota-benchmarks" aria-hidden="true" href="#computational-complexity-comparison-with-sota-benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Computational Complexity: Comparison with SOTA benchmarks</h3>
<p dir="auto">Computational complexity is reported in time, lines of code and, Relative Computational Complexity (RCC).</p>
<h4 dir="auto"><a id="user-content-tine" aria-hidden="true" href="#tine"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Tine</h4>
<p dir="auto">Using <code>StatsForecast</code> and a 96 cores EC2 instance (c5d.24xlarge) it takes 5.6 mins to train, forecast and ensemble the four models for the 3,003 series of M3.</p>
<table>
<thead>
<tr>
<th>Time (mins)</th>
<th>Yearly</th>
<th>Quarterly</th>
<th>Monthly</th>
<th>Other</th>
</tr>
</thead>
<tbody>
<tr>
<td>StatsForecast ensemble</td>
<td>1.10</td>
<td>1.32</td>
<td>2.38</td>
<td>1.08</td>
</tr>
</tbody>
</table>
<p dir="auto">The authors of the paper only report computational time for the monthly group, which amounts to 20,680 mins or 14.3 days. In comparison, the StatsForecast ensemble only takes 2.38 minutes to run for that group. Furthermore, the authors don&#39;t include times for Hyperparameter optimization.</p>
<p dir="auto">For this comparison, we will take the reported 14 days of computational time. However, it must be noted that the true computational time must be significantly higher for all groups.</p>
<h4 dir="auto"><a id="user-content-engineering" aria-hidden="true" href="#engineering"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Engineering</h4>
<p dir="auto">Furthermore, running all statistical models, including data downloading, data wrangling, training, forecasting and ensembling the models, can be achieved in less than 150 lines of Python code. In comparison, <a href="https://github.com/gjmulder/m3-gluonts-ensemble">this</a> repo has more than 1,000 lines of code and needs Python, R, Mongo and Shell code.</p>
<h4 dir="auto"><a id="user-content-relative-computational-complexity" aria-hidden="true" href="#relative-computational-complexity"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Relative Computational Complexity</h4>
<p dir="auto">The mentioned paper uses Relative Computational Complexity (RCC) for comparing the models. To calculate the RCC of <code>StatsForecast</code>, we followed the same methodology and measured the time it takes to generate naive forecasts for all 3,003 series in our environment.</p>
<p dir="auto">Using a <code>c5d.24xlarge</code> instance (96 CPU, 192 GB RAM) it takes 12 seconds to train and predict 3,003 instances of a Seasonal Naive forecast. Therefore, the RCC of the simple ensemble is 28.</p>
<p dir="auto">In the next table, you can find the RCC of the deep learning models and the ensembles</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Type</th>
<th>Relative Computational Complexity (RCC)</th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepAR</td>
<td>DL</td>
<td>313,000</td>
</tr>
<tr>
<td>Feed-Forward</td>
<td>DL</td>
<td>47,300</td>
</tr>
<tr>
<td>Transformer</td>
<td>DL</td>
<td>47,500</td>
</tr>
<tr>
<td>WaveNet</td>
<td>DL</td>
<td>306,000</td>
</tr>
<tr>
<td>Ensemble-DL</td>
<td>DL</td>
<td>713,800</td>
</tr>
<tr>
<td>Ensemble - Stats</td>
<td>Statistical</td>
<td>28</td>
</tr>
<tr>
<td>SeasonalNaive</td>
<td>Benchmark</td>
<td>1</td>
</tr>
</tbody>
</table>
<h3 dir="auto"><a id="user-content-summary-comparison-with-sota-benchmarks" aria-hidden="true" href="#summary-comparison-with-sota-benchmarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Summary: Comparison with SOTA benchmarks</h3>
<p dir="auto">We present a summary comparison, including SMAPE, RCC, Cost proxy, and self-reported computational time.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://user-images.githubusercontent.com/10517170/204958747-ea9e53ce-d0fc-41d1-bb71-eac7bed4be94.png"><img width="734" alt="image" src="https://user-images.githubusercontent.com/10517170/204958747-ea9e53ce-d0fc-41d1-bb71-eac7bed4be94.png"/></a></p>
<p dir="auto">We observe that <code>StatsForecast</code> yields average SMAPE results similar to DeepAR with computational savings of 99%.</p>
<p dir="auto">Furthermore, the StatsForecast ensemble:</p>
<ul dir="auto">
<li>Has better performance than the <code>N-BEATS</code> model for the <code>Yearly</code> and <code>Other</code> groups.</li>
<li>Has a better average performance than the individual <code>Gluon-TS</code> models.</li>
<li>It performs better than all <code>Gluont-TS</code> models for the <code>Monthly</code> and <code>Other</code> groups.</li>
<li>It is consistently better than the <code>Transformer</code>, <code>Wavenet</code>, and <code>Feed-Forward</code> models.</li>
</ul>
<p dir="auto">In conclusion, the deep learning ensemble achieves 12.27 points of accuracy (sMAPE), with a relative computational cost of 713,000 and a proxy monetary cost of USD 11,4200.
The simple statistical ensemble achieves 12.63 points of accuracy, with a relative computational cost of 28 and a proxy monetary cost of USD 0.5c.</p>
<p dir="auto">Therefore, the DL Ensemble is only 0.36 points more accurate than the statistical ensemble, but 25,000 times more expensive.</p>
<p dir="auto">In plain English: a deep-learning ensemble that takes more than 14 days to run and costs around USD 11,000, outperforms a statistical ensemble that takes 6 minutes to run and costs $0.5c by only 0.36 points of SMAPE.</p>
<h2 dir="auto"><a id="user-content-conclusions" aria-hidden="true" href="#conclusions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Conclusions</h2>
<p dir="auto">For this setting: Deep Learning models are simply worse than a statistical ensemble. To outperform this statistical ensemble by 0.36 points of SMAPE a complicated deep learning ensemble is needed. The deep learning ensemble, however, takes more than two weeks to run, costs several thousands of dollars and demands several engineering hours.</p>
<p dir="auto">In conclusion: in terms of speed, costs, simplicity and interpretability, deep learning is far behind the simple statistical ensemble. In terms of accuracy, they seem to be rather close.</p>
<p dir="auto">This conclusion might or not hold in other datasets, however, given the a priori uncertainty of the benefits and the certainty of cost, statistical methods should be considered the first option in daily forecasting practice.</p>
<h2 dir="auto"><a id="user-content-unsolicited-advice" aria-hidden="true" href="#unsolicited-advice"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Unsolicited Advice</h2>
<p dir="auto">Choose your models wisely.</p>
<p dir="auto">It would be extremely expensive and borderline irresponsible to favor deep learning models in an organization before establishing solid baselines.</p>
<p dir="auto">Simpler is sometimes better. Not everything that glows is gold.</p>
<h2 dir="auto"><a id="user-content-reproducibility" aria-hidden="true" href="#reproducibility"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Reproducibility</h2>
<p dir="auto">To reproduce the main results you have to:</p>
<ol dir="auto">
<li>Create the environment using <code>conda env create -f environment.yml</code>.</li>
<li>Activate the environment using <code>conda activate m3-dl</code>.</li>
<li>Run the experiments using <code>python -m src.experiment --group [group]</code> where <code>[group]</code> can be <code>Other</code>, <code>Monthly</code>, <code>Quarterly</code>, and <code>Yearly</code>.</li>
<li>Finally, you can evaluate the forecasts using <code>python -m src.evaluation</code>.</li>
</ol>
<h2 dir="auto"><a id="user-content-references" aria-hidden="true" href="#references"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>References</h2>
<ul dir="auto">
<li><a href="https://www.jstatsoft.org/article/view/v027i03" rel="nofollow">Hyndman, Rob J. &amp; Khandakar, Yeasmin (2008). &#34;Automatic Time Series Forecasting: The forecast package for R&#34;</a></li>
<li><a href="https://robjhyndman.com/expsmooth/" rel="nofollow">Hyndman, Rob J., et al (2008). &#34;Forecasting with exponential smoothing: the state space approach&#34;</a></li>
<li><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/nav.22074" rel="nofollow">Svetunkov, Ivan &amp; Kourentzes, Nikolaos. (2015). &#34;Complex Exponential Smoothing&#34;. 10.13140/RG.2.1.3757.2562. </a></li>
<li><a href="https://doi.org/10.1016/j.ijforecast.2016.02.005" rel="nofollow">Jose A. Fiorucci, Tiago R. Pellegrini, Francisco Louzada, Fotios Petropoulos, Anne B. Koehler: Models for optimising the theta method and their relationship to state space models, International Journal of Forecasting, Volume 32, Issue 4, 2016, Pages 1151-1161, ISSN 0169-2070</a></li>
<li><a href="https://doi.org/10.1016/j.ijforecast.2019.01.006" rel="nofollow">Fotios Petropoulos, Ivan Svetunkov: A simple combination of univariate models, International Journal of Forecasting, Volume 36, Issue 1, 2020, Pages 110-115, ISSN 0169-2070.</a></li>
<li><a href="https://www.tandfonline.com/doi/pdf/10.1080/01605682.2022.2118629?needAccess=true" rel="nofollow">Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, ArtemiosAnargyros Semenoglou, Gary Mulder &amp; Konstantinos Nikolopoulos (2022): Statistical, machine
learning and deep learning forecasting methods: Comparisons and ways forward, Journal of the
Operational Research Society, DOI: 10.1080/01605682.2022.2118629</a></li>
</ul>
</article>
          </div></div>
  </body>
</html>
