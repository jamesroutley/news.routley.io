<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/">Original</a>
    <h1>Muvera: Making multi-vector retrieval as fast as single-vector search</h1>
    
    <div id="readability-page-1" class="page"><div data-gt-publish-date="20250625">
                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="0xkr2">Neural <a href="https://developers.google.com/machine-learning/crash-course/embeddings" target="_blank" rel="noopener noreferrer">embedding models</a> have become a cornerstone of modern <a href="https://en.wikipedia.org/wiki/Information_retrieval" target="_blank" rel="noopener noreferrer">information retrieval</a> (IR). Given a query from a user (e.g., “How tall is Mt Everest?”), the goal of IR is to find information relevant to the query from a very large collection of data (e.g., the billions of documents, images, or videos on the Web). Embedding models transform each datapoint into a single-vector “embedding”, such that <i>semantically</i> similar datapoints are transformed into <i>mathematically</i> similar vectors. The embeddings are generally compared via the <a href="https://en.wikipedia.org/wiki/Inner_product_space#Euclidean_vector_space" target="_blank" rel="noopener noreferrer">inner-product similarity</a>, enabling efficient retrieval through optimized <a href="https://en.wikipedia.org/wiki/Maximum_inner-product_search" target="_blank" rel="noopener noreferrer">maximum inner product search</a> (MIPS) algorithms. However, recent advances, particularly the introduction of multi-vector models like <a href="https://www.answer.ai/posts/colbert-pooling.html" target="_blank" rel="noopener noreferrer">ColBERT</a>, have demonstrated significantly improved performance in IR tasks.</p><p data-block-key="ef8b9">Unlike single-vector embeddings, multi-vector models represent each data point with a <i>set</i> of embeddings, and leverage more sophisticated similarity functions that can capture richer relationships between datapoints. For example, the popular <a href="https://www.sciencedirect.com/topics/engineering/chamfer-matching" target="_blank" rel="noopener noreferrer">Chamfer similarity measure</a> used in state-of-the-art multi-vector models captures when the information in one multi-vector embedding is contained within another multi-vector embedding. While this multi-vector approach boosts accuracy and enables retrieving more relevant documents, it introduces substantial computational challenges. In particular, the increased number of embeddings and the complexity of multi-vector similarity scoring make retrieval significantly more expensive.</p><p data-block-key="bkmgl">In “<a href="https://arxiv.org/abs/2405.19504" target="_blank" rel="noopener noreferrer">MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings</a>”, we introduce a novel multi-vector retrieval algorithm designed to bridge the efficiency gap between single- and multi-vector retrieval. We transform multi-vector retrieval into a simpler problem by constructing fixed dimensional encodings (FDEs) of queries and documents, which are single vectors whose inner product approximates multi-vector similarity, thus reducing complex multi-vector retrieval back to single-vector maximum inner product search (MIPS). This new approach allows us to leverage the highly-optimized MIPS algorithms to retrieve an initial set of candidates that can then be re-ranked with the exact multi-vector similarity, thereby enabling efficient multi-vector retrieval without sacrificing accuracy. We have provided an open-source implementation of our FDE construction algorithm on <a href="https://github.com/google/graph-mining/tree/main/sketching/point_cloud" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="0xkr2">The challenge of multi-vector retrieval</h2><p data-block-key="6ktb8">Multi-vector models generate multiple embeddings per query or document, often one embedding per token. One typically calculates the similarity between a query and a document using Chamfer matching, which measures the maximum similarity between each query embedding and the closest <a href="https://github.com/Yeema/ecommerce-review-score-classification/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md" target="_blank" rel="noopener noreferrer">document embedding</a>, and then adds these similarities up across all query vectors (the standard method of computing multi-vector similarity). The Chamfer similarity, therefore, provides a &#34;holistic&#34; measure of how each part of the query relates to some part of the document.</p><p data-block-key="5s9if">While multi-vector representations offer advantages like improved interpretability and generalization, they pose significant retrieval challenges:</p><ul><li data-block-key="1kcio"><i>Increased embedding volume</i>: Generating embeddings per token drastically increases the number of embeddings to be processed.</li><li data-block-key="50lte"><i>Complex and compute-intensive similarity scoring</i>: Chamfer matching is a non-linear operation requiring a matrix product, which is more expensive than a single vector <a href="https://en.wikipedia.org/wiki/Dot_product" target="_blank" rel="noopener noreferrer">dot-product</a>.</li><li data-block-key="5mtsc"><i>Lack of efficient sublinear search methods</i>: Single-vector retrieval benefits from highly optimized algorithms (e.g., based on <a href="https://en.wikipedia.org/wiki/Space_partitioning" target="_blank" rel="noopener noreferrer">space partitioning</a>) that simultaneously achieve high accuracy and sublinear search times, avoiding exhaustive comparisons. The complex nature of multi-vector similarity prevents the direct application of these fast geometric techniques, hindering efficient retrieval at scale.</li></ul><p data-block-key="fsa1h">Unfortunately, traditional single-vector MIPS algorithms cannot be directly applied to multi-vector retrieval — for example, a document might have a token with high similarity to a single query token, but overall, the document might not be very relevant. This problem necessitates more complex and computationally intensive retrieval methods.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="0xkr2">MUVERA: A solution with fixed dimensional encodings</h2><p data-block-key="67rm1">MUVERA offers an elegant solution by reducing multi-vector similarity search to single-vector MIPS to make retrieval over complex multi-vector data much faster. Imagine you have a large dataset of &#34;multi-vector sets&#34; (i.e., sets of vectors) where each set describes some datapoint, but searching through each of these sets is slow. MUVERA&#39;s trick is to take that whole group of multi-vectors and squeeze them into a single, easier-to-handle vector that we call a <i>fixed dimensional encoding</i> (FDE). A key part is that if you compare these simplified FDEs, their comparison closely matches what you&#39;d get if you compared the original, more complex multi-vector sets. This lets us use much quicker search methods designed for single vectors.</p><p data-block-key="919ud">Here&#39;s a simplified breakdown of how MUVERA works:</p><ol><li data-block-key="2be5p"><i>FDE generation</i>: MUVERA employs mappings to convert query and document multi-vector sets into FDEs. These mappings are designed to capture the essential similarity information in a fixed-length vector.</li><li data-block-key="2t4u9"><i>MIPS-based retrieval</i>: The FDEs of documents are indexed using a standard MIPS solver. Given a query, its FDE is computed, and the MIPS solver efficiently retrieves the most similar document FDEs.</li><li data-block-key="6taf6"><i>Re-ranking</i>: The initial candidates retrieved by MIPS are re-ranked using the original Chamfer similarity for improved accuracy.</li></ol><p data-block-key="ed9lp">A key advantage of MUVERA is that the FDE transformation is data-oblivious. This means it doesn&#39;t depend on the specific dataset, making it both robust to changes in data distribution and suitable for streaming applications. Additionally, unlike single-vectors produced by a model, FDE’s are guaranteed to approximate the true Chamfer similarity to within a specified error. Thus, after the re-ranking stage, MUVERA is guaranteed to find the most similar multi-vector representations.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="0xkr2">Theoretical foundations</h2><p data-block-key="d79eo">Our approach is inspired by techniques used in <a href="https://arxiv.org/abs/2111.03528" target="_blank" rel="noopener noreferrer">probabilistic tree embeddings</a>, a powerful tool in the theory of geometric algorithms. However, we adapt these techniques to work with inner products and Chamfer similarity.</p><p data-block-key="ebt62">The core idea behind FDE generation is to partition the embedding space into sections (illustrated in the figure above). If similar vectors from a query and a document fall into the same section, we can approximate their similarity efficiently. However, since we don&#39;t know the optimal matching between query and document vectors beforehand, we use a randomized partitioning scheme.</p><p data-block-key="4jl3a">We also provide theoretical guarantees for MUVERA, proving that FDEs offer a strong approximation of Chamfer similarity (you can read more in <a href="https://arxiv.org/abs/2405.19504" target="_blank" rel="noopener noreferrer">the paper</a>). This is a significant result, as it provides a principled way to perform multi-vector retrieval using single-vector proxies with provable accuracy.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="0xkr2">Experimental results</h2><p data-block-key="9aa8i">We evaluated MUVERA on several information retrieval datasets from the <a href="https://arxiv.org/abs/2104.08663" target="_blank" rel="noopener noreferrer">BEIR</a> benchmarks. Our experiments demonstrate that MUVERA consistently achieves high retrieval accuracy with significantly reduced latency compared to the previous state-of-the-art method known as <a href="https://arxiv.org/pdf/2205.09707" target="_blank" rel="noopener noreferrer">PLAID</a>.</p><p data-block-key="4h9g9">Our key findings include:</p><p data-block-key="ji43"><i>Improved recall:</i> MUVERA outperforms the single-vector heuristic, a common approach used in multi-vector retrieval (which PLAID also employs), achieving better recall while retrieving significantly fewer candidate documents (shown in the figure below). For instance, FDE’s retrieve 5–20x fewer candidates to achieve a fixed recall.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <p data-block-key="0xkr2"><i>Reduced latency:</i> Compared to PLAID, a highly optimized multi-vector retrieval system based on the single-vector heuristic, MUVERA achieves an average of 10% higher recall with a remarkable 90% reduction in latency across the BEIR datasets (shown in the figure below).</p>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="0xkr2">Moreover, we found that MUVERA&#39;s FDEs can be <i>effectively compressed</i> using product quantization, reducing memory footprint by 32x with minimal impact on retrieval quality.</p><p data-block-key="avedq">These results highlight MUVERA&#39;s potential to significantly accelerate multi-vector retrieval, making it more practical for real-world applications.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="0xkr2">Conclusion</h2><p data-block-key="f5c08">We have presented MUVERA, a novel and efficient multi-vector retrieval algorithm with provable guarantees on its approximation quality and good practical performance. By reducing multi-vector search to single-vector MIPS, MUVERA leverages existing optimized search techniques and achieves state-of-the-art performance with significantly improved efficiency. Interested readers can find an open-source implementation of our FDE construction algorithm on <a href="https://github.com/google/graph-mining/tree/main/sketching/point_cloud" target="_blank" rel="noopener noreferrer">GitHub</a>.</p><p data-block-key="dnb18">Our work opens up new avenues for efficient multi-vector retrieval, which is crucial for various applications, including search engines, recommendation systems, and natural language processing. We believe that further research and optimization of MUVERA will lead to even greater performance gains and broader adoption of multi-vector retrieval techniques.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="0xkr2">Acknowledgements</h2><p data-block-key="6n887"><i>The work summarized in this blog post was done in collaboration with Majid Hadian, Jason Lee, and Vahab Mirrokni. Lastly, we thank Kimberly Schwede for their valuable help with making the animation in this blog post.</i></p>
</div>

    </div>
</section>

                    
                </div></div>
  </body>
</html>
