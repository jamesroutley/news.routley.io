<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/markasoftware/llama-cpu">Original</a>
    <h1>Fork of Facebookâ€™s LLaMa model to run on CPU</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This is a fork of <a href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a> to run on CPU.</p>
<p dir="auto">Usage and setup is exactly the same:</p>
<ol dir="auto">
<li>Create a conda environment (for me I needed Python 3.10 instead of 3.11 because of some pytorch bug?)</li>
<li><code>pip install -r requirements.txt</code></li>
<li>Torrent the dataset.</li>
<li>Run <code>torchrun</code> as described in the upstream readme.</li>
</ol>
<p dir="auto">Tested on 7B model. Even with 32GiB of ram, you&#39;ll need swap space or zram enabled to load the model (maybe it&#39;s doing some conversions?), but once it actually starts doing inference it settles down at a more reasonable &lt;20GiB of ram.</p>
<p dir="auto">On a Ryzen 7900X, the 7B model is able to infer several words per second, quite a lot better than you&#39;d expect!</p>
</article>
          </div></div>
  </body>
</html>
