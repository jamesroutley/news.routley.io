<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://codspeed.io/blog/state-of-python-3-13-performance-free-threading">Original</a>
    <h1>State of Python 3.13 Performance: Free-Threading</h1>
    
    <div id="readability-page-1" class="page"><div><p>CPython 3.13 was released two weeks ago and this release is the most
performance-oriented in some time. After a quick read of the release notes, a
few things stand out for the impact they can have on the performance:</p>
<ul>
<li>CPython can now run in <strong>free-threaded mode</strong>, with the global interpreter
lock (GIL) disabled</li>
<li>a brand new <strong>just-in-time</strong> (JIT) compiler has been added</li>
<li>CPython now bundles the <code>mimalloc</code> allocator out of the box</li>
</ul>
<p>Let&#39;s focus on the free-threaded mode in this article to see how to leverage
this change and how it can impact the performance of Python applications by
measuring performance with CodSpeed.</p>
<blockquote>
<p>⏭️ The JIT and <code>mimalloc</code> performance will be covered in the next post. Stay
tuned!</p>
</blockquote>
<h2 id="free-threaded-cpython"><a href="#free-threaded-cpython">Free-threaded CPython<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<p>Free-threading is an experimental feature in Python 3.13 that allows CPython to
run without the Global Interpreter Lock (GIL). The GIL is a mutex preventing
multiple threads from executing Python bytecode simultaneously. This design
choice has simplified CPython&#39;s memory management and made the C API easier to
work with. However, it has also been one of the most significant barriers to
utilizing modern multi-core processors effectively.</p>
<h4 id="the-multiprocessing-workaround"><a href="#the-multiprocessing-workaround">The Multiprocessing Workaround<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h4>
<p>The traditional solution has been to use the <code>multiprocessing</code> module, which
spawns separate Python processes instead of threads and while this approach
works, it comes with significant limitations:</p>
<ol>
<li>
<p><strong>Memory Overhead</strong>: Each process requires its own Python interpreter
instance and memory space. For data-intensive applications, this can quickly
become a bottleneck.</p>
</li>
<li>
<p><strong>Communication Cost</strong>: Processes can&#39;t share memory directly. Data must be
serialized and deserialized when passed between processes, which adds
overhead and complexity.</p>
</li>
<li>
<p><strong>Startup Time</strong>: Creating new processes is significantly slower than
creating threads, making it impractical for tasks that require frequent
spawning of workers.</p>
</li>
</ol>

<p>To illustrate these limitations, let&#39;s consider implementing PageRank, the
algorithm that powered Google&#39;s early search engine. PageRank is an ideal
example because it:</p>
<ol>
<li>Is compute-intensive (matrix operations)</li>
<li>Works with large datasets (the web graph)</li>
<li>Can benefit significantly from parallelization</li>
</ol>
<p>A naive multithreaded implementation in Python 3.12 or earlier would be
bottlenecked by the GIL during matrix operations, while a multiprocessing
version would struggle with:</p>
<ul>
<li>The memory overhead of copying the graph to each process</li>
<li>The cost of transferring partial results between processes</li>
<li>The complexity of managing shared state</li>
</ul>
<p>Before we proceed, it&#39;s important to clarify that our focus here isn&#39;t on the
specifics of the PageRank algorithm itself but rather on the parallelization so
we won&#39;t go into the details of the algorithm itself here.</p>
<p>Let&#39;s look at how we would implement this with those different concurrency
models.</p>
<h3 id="textbook-implementation-single-threaded-"><a href="#textbook-implementation-single-threaded-">Textbook Implementation (Single-Threaded)<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<pre tabindex="0"><code><span><span>def</span><span> pagerank_single</span><span>(</span><span>matrix</span><span>: np.ndarray, </span><span>num_iterations</span><span>: </span><span>int</span><span>) -&gt; np.ndarray:</span></span>
<span><span>    &#34;&#34;&#34;Single-threaded PageRank implementation&#34;&#34;&#34;</span></span>
<span><span>    size </span><span>=</span><span> matrix.shape[</span><span>0</span><span>]</span></span>
<span><span>    # Initialize scores</span></span>
<span><span>    scores </span><span>=</span><span> np.</span><span>ones</span><span>(size) </span><span>/</span><span> size</span></span>
<span></span>
<span><span>    for</span><span> _ </span><span>in</span><span> range</span><span>(num_iterations):</span></span>
<span><span>        new_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span><span>        for</span><span> i </span><span>in</span><span> range</span><span>(size):</span></span>
<span><span>            # Get nodes that point to current node</span></span>
<span><span>            incoming </span><span>=</span><span> np.</span><span>where</span><span>(matrix[:, i])[</span><span>0</span><span>]</span></span>
<span><span>            for</span><span> j </span><span>in</span><span> incoming:</span></span>
<span><span>                # Add score contribution from incoming node</span></span>
<span><span>                new_scores[i] </span><span>+=</span><span> scores[j] </span><span>/</span><span> np.</span><span>sum</span><span>(matrix[j]) </span></span>
<span><span>        # Apply damping factor</span></span>
<span><span>        scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores </span></span>
<span></span>
<span><span>    return</span><span> scores</span></span></code></pre>
<p>The highlighted lines show the two most computationally intensive parts of the
algorithm. The first computes the score contribution from incoming nodes, while
the second applies the damping factor, incorporating the new scores into the
final result.</p>
<p>Parallelizing the first part will be the most beneficial and easy to implement
since we can divide the range and thus efficiently use multiple threads to
compute the <code>new_scores</code> array.</p>
<h3 id="multithreaded-implementation"><a href="#multithreaded-implementation">Multithreaded Implementation<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<p>For the multithreaded implementation, we&#39;ll start by dividing the matrix into
multiple chunks:</p>
<pre tabindex="0"><code><span><span>chunk_size </span><span>=</span><span> size </span><span>//</span><span> num_threads</span></span>
<span><span>chunks </span><span>=</span><span> [(i, </span><span>min</span><span>(i </span><span>+</span><span> chunk_size, size)) </span><span>for</span><span> i </span><span>in</span><span> range</span><span>(</span><span>0</span><span>, size, chunk_size)]</span></span></code></pre>
<p>Each thread will then work on a different chunk of the matrix, updating the new
scores:</p>
<pre tabindex="0"><code><span><span>def</span><span> _thread_worker</span><span>(</span></span>
<span><span>    matrix</span><span>: np.ndarray,</span></span>
<span><span>    scores</span><span>: np.ndarray,</span></span>
<span><span>    new_scores</span><span>: np.ndarray,</span></span>
<span><span>    start_idx</span><span>: </span><span>int</span><span>,</span></span>
<span><span>    end_idx</span><span>: </span><span>int</span><span>,</span></span>
<span><span>    lock</span><span>: threading.Lock,</span></span>
<span><span>):</span></span>
<span><span>    size </span><span>=</span><span> matrix.shape[</span><span>0</span><span>]</span></span>
<span><span>    local_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span></span>
<span><span>    for</span><span> i </span><span>in</span><span> range</span><span>(start_idx, end_idx):</span></span>
<span><span>        incoming </span><span>=</span><span> np.</span><span>where</span><span>(matrix[:, i])[</span><span>0</span><span>]</span></span>
<span><span>        for</span><span> j </span><span>in</span><span> incoming:</span></span>
<span><span>            local_scores[i] </span><span>+=</span><span> scores[j] </span><span>/</span><span> np.</span><span>sum</span><span>(matrix[j])</span></span>
<span></span>
<span><span>    with</span><span> lock: </span></span>
<span><span>        new_scores </span><span>+=</span><span> local_scores </span></span></code></pre>
<p>It&#39;s important to note that updating the <code>new_scores</code> array is done behind a
lock to prevent race conditions. This could potentially become a bottleneck if
the lock is held for too long, but in practice, parallelizing the first part of
the algorithm should provide a significant speedup already.</p>
<p>Finally, we&#39;ll feed the chunks to each of the threads:</p>
<pre tabindex="0"><code><span><span>new_scores </span><span>=</span><span> np.</span><span>zeros</span><span>(size)</span></span>
<span><span>lock </span><span>=</span><span> threading.</span><span>Lock</span><span>() </span></span>
<span><span>with</span><span> concurrent.futures.</span><span>ThreadPoolExecutor</span><span>(</span><span>max_workers</span><span>=</span><span>num_threads) </span><span>as</span><span> executor: </span></span>
<span><span>    # Process chunks in parallel</span></span>
<span><span>    futures </span><span>=</span><span> executor.</span><span>map</span><span>( </span></span>
<span><span>        lambda</span><span> args</span><span>: </span><span>_thread_worker</span><span>(*args), </span><span># starmap isn&#39;t available on ThreadPoolExecutor</span></span>
<span><span>        [ </span></span>
<span><span>            (matrix, scores, new_scores, start_idx, end_idx, lock) </span></span>
<span><span>            for</span><span> start_idx, end_idx </span><span>in</span><span> chunks </span></span>
<span><span>        ], </span></span>
<span><span>    ) </span></span>
<span><span>new_scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores</span></span>
<span><span>scores </span><span>=</span><span> new_scores</span></span></code></pre>
<h3 id="multiprocessing-implementation"><a href="#multiprocessing-implementation">Multiprocessing Implementation<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h3>
<p>Essentially, the <code>multiprocessing</code> implementation is extremely similar to the
<code>threading</code> one, let&#39;s focus on the differences:</p>
<ul>
<li>
<p>Each worker, will now return the <code>local_scores</code> array instead of updating a
shared <code>new_scores</code> array since processes can&#39;t share memory directly. The
local scores will then be aggregated in the main process:</p>
<pre tabindex="0"><code><span><span># Combine results</span></span>
<span><span>new_scores </span><span>=</span><span> sum</span><span>(chunk_results)</span></span></code></pre>
<p>While this should be faster than the threading version, it still incurs the
overhead of the inter-process communication which can become very significant
for large datasets.</p>
</li>
<li>
<p>Instead of using a <code>ThreadPoolExecutor</code>, it will use a <code>multiprocessing.Pool</code>.
The APIs are very similar, but <code>multiprocessing.Pool</code> will spawn a pool of
processes instead of threads:</p>
<pre tabindex="0"><code><span><span>with</span><span> multiprocessing.</span><span>Pool</span><span>(</span><span>processes</span><span>=</span><span>num_processes) </span><span>as</span><span> pool: </span></span>
<span><span>    # Process chunks in parallel</span></span>
<span><span>    chunk_results </span><span>=</span><span> pool.</span><span>starmap</span><span>(_process_chunk, chunks) </span></span>
<span><span>    # Combine results</span></span>
<span><span>    new_scores </span><span>=</span><span> sum</span><span>(chunk_results)</span></span>
<span><span>    new_scores </span><span>=</span><span> (</span><span>1</span><span> -</span><span> DAMPING</span><span>) </span><span>/</span><span> size </span><span>+</span><span> DAMPING</span><span> *</span><span> new_scores</span></span>
<span><span>    scores </span><span>=</span><span> new_scores</span></span></code></pre>
</li>
</ul>
<h2 id="measuring-performance"><a href="#measuring-performance">Measuring Performance<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<p>In order to measure the actual performance changes, let&#39;s build a performance
test. First things first, we need to generate some testing data:</p>
<pre tabindex="0"><code><span><span>def</span><span> create_test_graph</span><span>(</span><span>size</span><span>: </span><span>int</span><span>) -&gt; np.ndarray:</span></span>
<span><span>    # Fixed seed</span></span>
<span><span>    np.random.</span><span>seed</span><span>(</span><span>0</span><span>) </span></span>
<span><span>    # Create random adjacency matrix with ~5 outgoing edges per node</span></span>
<span><span>    matrix </span><span>=</span><span> np.random.</span><span>choice</span><span>([</span><span>0</span><span>, </span><span>1</span><span>], </span><span>size</span><span>=</span><span>(size, size), </span><span>p</span><span>=</span><span>[</span><span>1</span><span> -</span><span> 5</span><span>/</span><span>size, </span><span>5</span><span>/</span><span>size])</span></span>
<span><span>    # Find nodes with no outgoing edges</span></span>
<span><span>    zero_outdegree </span><span>=</span><span> ~</span><span>matrix.</span><span>any</span><span>(</span><span>axis</span><span>=</span><span>1</span><span>)</span></span>
<span><span>    zero_indices </span><span>=</span><span> np.</span><span>where</span><span>(zero_outdegree)[</span><span>0</span><span>]</span></span>
<span><span>    # For each node with no outgoing edges, add a random edge</span></span>
<span><span>    if</span><span> len</span><span>(zero_indices) </span><span>&gt;</span><span> 0</span><span>:</span></span>
<span><span>        random_targets </span><span>=</span><span> np.random.</span><span>randint</span><span>(</span><span>0</span><span>, size, </span><span>size</span><span>=len</span><span>(zero_indices))</span></span>
<span><span>        matrix[zero_indices, random_targets] </span><span>=</span><span> 1</span></span>
<span></span>
<span><span>    return</span><span> matrix</span></span></code></pre>
<p>As highlighted, we&#39;re using a fixed seed to ensure reproducibility from one run
to the next. This is important when comparing the performance of different
implementations. Here we&#39;re building some fake connections between pages to
build a realistic graph but the mathematical operations would be exactly the
same with an empty matrix as long as the size is the same.</p>
<p>Next, we&#39;ll use
<a target="blank" href="https://github.com/CodSpeedHQ/pytest-codspeed"><code>pytest-codspeed</code></a>, a <code>pytest</code>
plugin to measure the performance of the different implementations with various
parameter and with multiple builds/versions of CPython.</p>
<p>First let&#39;s define the benchmark cases:</p>
<pre tabindex="0"><code><span><span>@pytest</span><span>.</span><span>mark</span><span>.</span><span>parametrize</span><span>(</span></span>
<span><span>    &#34;pagerank&#34;</span><span>,</span></span>
<span><span>    [</span></span>
<span><span>        pagerank_single</span><span>,</span></span>
<span><span>        partial</span><span>(</span><span>pagerank_multiprocess</span><span>,</span><span> num_processes</span><span>=</span><span>8</span><span>),</span></span>
<span><span>        partial</span><span>(</span><span>pagerank_multithread</span><span>,</span><span> num_threads</span><span>=</span><span>8</span><span>),</span></span>
<span><span>    ],</span></span>
<span><span>    ids</span><span>=</span><span>[</span><span>&#34;single&#34;</span><span>,</span><span> &#34;8-processes&#34;</span><span>,</span><span> &#34;8-threads&#34;</span><span>],</span></span>
<span><span>)</span></span>
<span><span>@pytest</span><span>.</span><span>mark</span><span>.</span><span>parametrize</span><span>(</span></span>
<span><span>    &#34;graph&#34;</span><span>,</span></span>
<span><span>    [</span></span>
<span><span>        create_test_graph</span><span>(</span><span>100</span><span>),</span></span>
<span><span>        create_test_graph</span><span>(</span><span>1000</span><span>),</span></span>
<span><span>        create_test_graph</span><span>(</span><span>2000</span><span>),</span></span>
<span><span>    ],</span></span>
<span><span>    ids</span><span>=</span><span>[</span><span>&#34;XS&#34;</span><span>,</span><span> &#34;L&#34;</span><span>,</span><span> &#34;XL&#34;</span><span>],</span></span>
<span><span>)</span></span>
<span><span>def</span><span> test_pagerank</span><span>(</span></span>
<span><span>    benchmark</span><span>: BenchmarkFixture,</span></span>
<span><span>    pagerank</span><span>: PagerankFunc,</span></span>
<span><span>    graph</span><span>: np.ndarray,</span></span>
<span><span>):</span></span>
<span><span>    benchmark</span><span>(pagerank, graph, </span><span>num_iterations</span><span>=</span><span>10</span><span>)</span></span></code></pre>
<p>Here we&#39;re testing the 3 implementations with 3 different graph sizes. The
<code>benchmark</code> fixture is provided by <code>pytest-codspeed</code> and will measure the
execution time of the <code>pagerank</code> function with the given args.</p>
<p>Then, let&#39;s write a GitHub Actions workflow to measure the performance with
various builds of CPython on CodSpeed&#39;s infrastructure:</p>
<pre tabindex="0"><code><span><span>on</span><span>:</span></span>
<span><span>  push</span><span>:</span></span>
<span><span>jobs</span><span>:</span></span>
<span><span>  codspeed</span><span>:</span></span>
<span><span>    runs-on</span><span>: </span><span>codspeed-macro</span><span> # requests a CodSpeed Macro runner for the jobs</span></span>
<span><span>    strategy</span><span>:</span></span>
<span><span>      matrix</span><span>:</span></span>
<span><span>        python-version</span><span>: [</span><span>&#34;3.12&#34;</span><span>, </span><span>&#34;3.13&#34;</span><span>]</span></span>
<span><span>        include</span><span>:</span></span>
<span><span>          - { </span><span>python-version</span><span>: </span><span>&#34;3.13t&#34;</span><span>, </span><span>gil</span><span>: </span><span>&#34;1&#34;</span><span> } </span></span>
<span><span>          - { </span><span>python-version</span><span>: </span><span>&#34;3.13t&#34;</span><span>, </span><span>gil</span><span>: </span><span>&#34;0&#34;</span><span> } </span></span>
<span><span>    env</span><span>:</span></span>
<span><span>      UV_PYTHON</span><span>: </span><span>${{ matrix.python-version }}</span></span>
<span><span>    steps</span><span>:</span></span>
<span><span>      - </span><span>uses</span><span>: </span><span>actions/checkout@v4</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Install uv</span></span>
<span><span>        uses</span><span>: </span><span>astral-sh/setup-uv@v3</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Install CPython &amp; dependencies</span></span>
<span><span>        run</span><span>: </span><span>uv sync --all-extras</span></span>
<span><span>      - </span><span>name</span><span>: </span><span>Run benchmarks</span></span>
<span><span>        uses</span><span>: </span><span>CodSpeedHQ/action@v3</span></span>
<span><span>        env</span><span>:</span></span>
<span><span>          PYTHON_GIL</span><span>: </span><span>${{ matrix.gil }}</span></span>
<span><span>        with</span><span>:</span></span>
<span><span>          run</span><span>: </span><span>uv run pytest --codspeed --codspeed-max-time 10 -vs src/tests.py</span></span></code></pre>
<p>Here we&#39;re running the benchmarks with Python 3.12, 3.13, and 3.13 with free
threading support (<code>3.13t</code>), both with and without the GIL. Running 3.13 both
with and without free-threading support will allow us to see its impact on the
performance even when the GIL is enabled.</p>
<p>The python builds used are pulled by <code>uv</code> directly from
<a target="blank" href="https://github.com/indygreg/python-build-standalone">python-build-standalone</a>
built with <code>GCC 6.3.0 20170516</code></p>
<p>The jobs will run on
<a target="blank" href="https://docs.codspeed.io/instruments/walltime/">CodSpeed Macro</a> runners, which
are ARM64 bare-metal instances with 16 cores and 32GB of RAM, dedicated to each
job.</p>
<h2 id="results"><a href="#results">Results<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<figure><div><p>Measured by</p></div><figcaption><p>Time (in s) for the L graph size, <strong>less is better</strong></p></figcaption></figure>
<!-- -->
<ul>
<li>Without enabling new build options both <b>3.12</b> and
<b>3.13</b> perform very similarly.
We also clearly see the limitation of the <code>multiprocessing</code> implementation being even
slower than the single-threaded one due to the overhead of the inter-process communication.</li>
</ul>
<!-- -->
<ul>
<li>As expected the <code>threading</code> based implementation is the fastest when running
<b>3.13t with no GIL</b> and the GIL is effectively not limiting the
parallel execution of the threads anymore.</li>
</ul>
<!-- -->
<ul>
<li>Still, when running with the free threaded build both <b>with</b> and
<b>without</b> the GIL, we see a significant
slowdown for all other implementations. This is mostly because the free-threaded build
requires the specializing adaptive interpreter to be disabled, thus clearly decreasing
the performance of the other implementations. This overhead should be reduced in the
3.14 release where the specializing adaptive interpreter will be thread-safe and thus
will be re-enabled. At that point, migrating to the free-threaded build should be a
no-brainer for a lot of parallel applications and it will be interesting to measure
the performance changes.</li>
</ul>
<p>For all other graph sizes, the results are very similar and the conclusions are
the same. From this measurement, we can see that the new free-threaded build of
CPython 3.13 can have a significant impact on the performance of parallel
applications, bringing a very relevant alternative to <code>multiprocessing</code>. Still
it&#39;s experimental and not yet ready for production use because of the overall
slowdown it introduces, but it&#39;s a very promising step in the right direction!</p>
<h4 id="note"><a href="#note">Note<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h4>
<p>This benchmark doesn&#39;t include subinterpreters, which is another way to run
Python code parallelly without the GIL introduced in Python 3.12. Subintepreters
proved to be slower than other approaches in most cases, mostly because where
data sharing and inter-worker communication has not been fully solved yet. But
when it&#39;s there, it might definitely be a nice alternative to <code>multiprocessing</code>.</p>
<h2 id="resources"><a href="#resources">Resources<svg viewBox="0 0 24 24" focusable="false"><g fill="currentColor"><path d="M10.458,18.374,7.721,21.11a2.853,2.853,0,0,1-3.942,0l-.892-.891a2.787,2.787,0,0,1,0-3.941l5.8-5.8a2.789,2.789,0,0,1,3.942,0l.893.892A1,1,0,0,0,14.94,9.952l-.893-.892a4.791,4.791,0,0,0-6.771,0l-5.8,5.8a4.787,4.787,0,0,0,0,6.77l.892.891a4.785,4.785,0,0,0,6.771,0l2.736-2.735a1,1,0,1,0-1.414-1.415Z"></path><path d="M22.526,2.363l-.892-.892a4.8,4.8,0,0,0-6.77,0l-2.905,2.9a1,1,0,0,0,1.414,1.414l2.9-2.9a2.79,2.79,0,0,1,3.941,0l.893.893a2.786,2.786,0,0,1,0,3.942l-5.8,5.8a2.769,2.769,0,0,1-1.971.817h0a2.766,2.766,0,0,1-1.969-.816,1,1,0,1,0-1.415,1.412,4.751,4.751,0,0,0,3.384,1.4h0a4.752,4.752,0,0,0,3.385-1.4l5.8-5.8a4.786,4.786,0,0,0,0-6.771Z"></path></g></svg></a></h2>
<ul>
<li>
<p><a target="blank" href="https://github.com/CodSpeedHQ/python-parallel-pagerank/">Repository with the full code</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.python.org/3.13/whatsnew/3.13.html">What&#39;s New In Python 3.13?</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.python.org/3/howto/free-threading-python.html">Python experimental support for free threading</a></p>
</li>
<li>
<p><a target="blank" href="https://py-free-threading.github.io/">py-free-threading</a>: a centralized
collection of documentation and trackers around compatibility with
free-threaded CPython</p>
</li>
<li>
<p><a target="blank" href="https://peps.python.org/pep-0659/">PEP 659 - Specializing Adaptive Interpreter</a></p>
</li>
<li>
<p><a target="blank" href="https://docs.google.com/document/d/1hsV25JSwDb08c6-aHrI_YBHyl_uQKlys0ODihSx_aSw">Docs on having the Specializing Adaptive Interpreter without the GIL</a></p>
</li>
</ul></div></div>
  </body>
</html>
