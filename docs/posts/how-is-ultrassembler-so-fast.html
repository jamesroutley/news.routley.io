<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jghuff.com/articles/ultrassembler-so-fast/">Original</a>
    <h1>How is Ultrassembler so fast?</h1>
    
    <div id="readability-page-1" class="page"><div>
            
<p>
  How is Ultrassembler so fast?
</p>
<p>Published 2025-08-30</p>
<p><a href="https://github.com/Slackadays/Chata/tree/main/ultrassembler">Ultrassembler</a> is a superfast and complete RISC-V assembler library that I&#39;m writing as a component of the bigger <a href="https://github.com/Slackadays/Chata">Chata signal processing</a> project. </p>
<p>Assemblers take in a platform-dependent assembly language and output that platform&#39;s native machine code which runs directly on the processor.</p>
<p><img src="https://jghuff.com/RISC-V-assembly-safe.svg" alt="Infographic showing the RISC-V assembly process for addi"/></p>
<p><span>&#34;Why would you want to do this?&#34;</span> you might ask. First, existing RISC-V assemblers that conform the the entirety of the specification, <code>as</code> and <code>llvm-mc</code>, ship as binaries that you run as standalone programs. This is normally not an issue. However, in Chata&#39;s case, it needs to access a RISC-V assembler from within its C++ code. The alternative is to use some ugly C function like <code>system()</code> to run external software as if it were a human or script running a command in a terminal. </p>
<p>Here&#39;s an example of what I&#39;m talking about:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>#include </span><span>&lt;</span><span>iostream</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>string</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>
</span><span>int </span><span>main</span><span>() {
</span><span>    std::string command = &#34;</span><span>riscv64-linux-gnu-as code.s -o code.bin</span><span>&#34;;
</span><span>
</span><span>    </span><span>int</span><span> res = std::</span><span>system</span><span>(command.</span><span>data</span><span>());
</span><span>
</span><span>    </span><span>if </span><span>(res != </span><span>0</span><span>) {
</span><span>        std::cerr &lt;&lt; &#34;</span><span>Error executing command: </span><span>&#34; &lt;&lt; command &lt;&lt; std::endl;
</span><span>    }
</span><span>    </span><span>return</span><span> res;
</span><span>}
</span></code></pre>
<p>It gets even worse once you realize you need temporary files and possibly have to artisanally craft the command beforehand. Additionally, invoking the assembler in this manner incurs a significant performance overhead on embedded systems which lack significant processing power. There must be a better way. </p>
<p>Enter Ultrassembler.</p>
<p>With these two goals of speed and standard conformance in mind, I wrote Ultrassembler as a completely standalone library with GNU <code>as</code> as the speed and standard conformity benchmark. </p>
<p>The results are nothing short of staggering. </p>
<p>After months of peformance optimization, Ultrassembler can assemble a test file with about 16 thousand RISC-V instructions over 10 times faster than <code>as</code>, and around 20 times faster than <code>llvm-mc</code>. To put it another way, it only takes about 1000 CPU instructions (+-50% depending on platform) to assemble one RISC-V instruction, while it takes 10,000 for <code>as</code> and 20,000 for <code>llvm-mc</code>. This happens with plain old C++ code only and no platform-specific assembly code, although integrating assembly could crank up the speed even further.</p>
<p>Such performance ensures a good user experience on the platforms where Chata runs, but also as a consequence of this lack of overhead, you could also combine Ultrassembler with fantastic libraries like <a href="https://github.com/libriscv/libriscv">libriscv</a> to implement low-to-no-cost RISC-V scripting in things like games, or maybe even in your JIT programming language!</p>
<p>Let&#39;s look at some of the ways I made Ultrassembler this fast so that you can reap the benefits too.</p>
<p><span>WARNING!</span>   The code you&#39;re about to see here is only current as of this article&#39;s publishing. The actual code Ultrassembler uses could be different by the time you read this in the future!</p>

<p>Exceptions, C++&#39;s first way of handling errors, are slow. Super duper slow. Mega slow. So slow, in fact, that many Programming Furus©️®️™️ say you should never ever use them. They&#39;ll infect your code with their slowness and transform you into a slow old hunchback in no time. </p>
<p>Or so you would think.</p>
<p>C++ exceptions, despite being so derided, are in fact zero-overhead. Huh? Didn&#39;t I just say they were super duper slow? Let me explain.</p>
<p>It&#39;s not clear when exactly exceptions are slow. I had to do some research here. As it turns out, GCC&#39;s <code>libstdc++</code> uses a so-called &#34;zero-overhead&#34; exception system, meaning that in the ideal normal case where the C++ code calls zero exceptions, there is zero performance penalty. But when it does call an exception, it could become very slow depending on how the code is laid out. Most programmers, not knowing this, frequently use exceptions in their normal cases, and as a result, their programs are slow. Such mysterious behavior caught the attention of Programming Furus©️®️™️ and has made exceptions appear somewhat of a curse.</p>
<p>This tragic curse turns out to be a heavenly blessing for Ultrassembler. In the normal case, there are zero errors to report as a result of proper usage of RISC-V instructions. But if there&#39;s some error somewhere, say somebody put in the wrong register, then Ultrassembler sounds the alarm. Since such mistakes only occur as a result of human error (ex bugs in codegen and Ultrassembler itself) the timeframe in which to report the error can expand to that of a human. As a result, even if an exception triggered by a mistake took a full 1 second (about a million times slower than it does in reality), it doesn&#39;t matter because the person percepting the error message can only do so in approximately that second timeframe.</p>
<p><span>&#34;But hold on!&#34;</span> you exclaim. <span>&#34;What about std::expected?&#34;</span> In response to some programs which frequently need to handle errors not seen by humans, C++ added a system to reduce the overhead of calling errors, <code>std::expected</code>. I tried this in Ultrassembler and the results weren&#39;t pretty. It trades off exception speed for normal case speed. Since the normal case is the norm in Ultrassembler, <code>std::expected</code> incurred at least a 10% performance loss due to the way the <code>std::expected</code> object wraps two values (the payload and the error code) together. <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p2544r0.html">See this C++ standard document for the juicy details.</a></p>
<p>The end result of the use of exceptions is that there is zero performance penalty to optimize out.</p>

<p>Between all of the RISC-V instruction set extensions, there are 2000+ individual &#34;instructions&#34; (many instructions are identical to one another with a slight numerical change). There are also hundreds of CSRs and just under a hundred registers. This requires data structures large enough to store the properties of thousands of entries. How do you do that? It&#39;s tricky. So, how about I just show you what Ultrassembler uses as of this writing:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>struct </span><span>rvregister {
</span><span>    RegisterType type; </span><span>//1B
</span><span>    RegisterID id; </span><span>//1B
</span><span>    uint8_t encoding;
</span><span>    uint8_t padding;
</span><span>};
</span><span>
</span><span>const</span><span> std::array&lt;rvregister, </span><span>96</span><span>&gt; registers;
</span><span>
</span><span>struct </span><span>rvinstruction {
</span><span>    RVInstructionID id; </span><span>//2B
</span><span>    RVInstructionFormat type; </span><span>//1B
</span><span>    uint8_t opcode;
</span><span>    uint16_t funct;
</span><span>    RVInSetMinReqs setreqs; </span><span>//1B
</span><span>    rreq regreqs = reg_reqs::any_regs; </span><span>//1B
</span><span>    special_snowflake_args ssargs = </span><span>special_snowflake_args</span><span>(); </span><span>//2B
</span><span>};
</span><span>
</span><span>// We use a strong typedef to define both rreq and ssflag, but the underlying is a uint8_t in both cases
</span><span>
</span><span>namespace </span><span>ssarg {
</span><span>
</span><span>constexpr</span><span> ssflag get_imm_for_rs = </span><span>ssflag</span><span>(</span><span>0b00000001</span><span>);
</span><span>constexpr</span><span> ssflag use_frm_for_funct3 = </span><span>ssflag</span><span>(</span><span>0b00000010</span><span>);
</span><span>constexpr</span><span> ssflag special_handling = </span><span>ssflag</span><span>(</span><span>0b00000100</span><span>);
</span><span>constexpr</span><span> ssflag swap_rs1_rs2 = </span><span>ssflag</span><span>(</span><span>0b00001000</span><span>);
</span><span>constexpr</span><span> ssflag use_funct_for_imm = </span><span>ssflag</span><span>(</span><span>0b00010000</span><span>);
</span><span>constexpr</span><span> ssflag no_rs1 = </span><span>ssflag</span><span>(</span><span>0b00100000</span><span>);
</span><span>constexpr</span><span> ssflag has_custom_reg_val = </span><span>ssflag</span><span>(</span><span>0b01000000</span><span>);
</span><span>
</span><span>}
</span><span>
</span><span>struct </span><span>special_snowflake_args {
</span><span>    uint8_t custom_reg_val = </span><span>0</span><span>;
</span><span>    ssflag flags; </span><span>//1B
</span><span>};
</span><span>
</span><span>const</span><span> std::array&lt;rvinstruction, </span><span>2034</span><span>&gt; instructions;
</span></code></pre>
<p>Let&#39;s go over what each <code>struct</code> does.</p>
<h2 id="rvregister"><code>rvregister</code></h2>
<p><code>rvregister</code> is how Ultrassembler stores the data for all the RISC-V registers. What describes a register? You have its friendly name (like x0 or v20), an alias (like zero or fa1), what kind of register it is (integer, float, or vector?), and what raw encoding it looks like in instructions. You can get away with single bytes to represent the type and encoding. And, that&#39;s what we use here to keep data access simple. You could squeeze everything into one or two bytes through clever bitmasking, but after doing so, I couldn&#39;t find much of a speedup. This could be situational and so you should not dismiss such a trick.</p>
<p>Why not store the name and alias strings? Ultrassembler does not actually reference the name nor the alias anywhere in its code. Why? Strings are very expensive. This fact is not obvious if you have not made software at the level of Ultrassembler, where string comparison and manipulation grind computation to a crawl. So we just don&#39;t use strings anywhere. In spite of this, the initializers of <code>const std::array&lt;rvregister, 96&gt; registers</code> do contain both the name and alias, but the constructors silently discard these data. Such inclusion enables external scripts to look at the array and generate code around it. We&#39;ll look at that in the next section. But for now, know that we hate strings.</p>
<h2 id="rvinstruction"><code>rvinstruction</code></h2>
<p><code>rvinstruction</code> follows a similar idea, with the biggest differences being that it&#39;s much bigger, 2000+ entries versus 96, and that there is more information to store per entry. This necessitates some extra memory saving magic because there are so many different instructions. We first need an ID for each instruction to do specific checks if needed. We have almost more than 2048 instructions (subject to future expansion) but less than 4196, so we&#39;ll need 2 bytes. There are fewer than 256 &#34;types&#34; of instructions (R, I, S, B, U, J, etc.), so 1 byte is good. Same idea with all the other fields. Similarly to <code>rvregister</code>, it would be possible to use bitmasking to compress everything, but this might not result in a significant speedup.</p>
<h2 id="special-snowflake-args"><code>special_snowflake_args</code></h2>
<p>In RISC-V, many instructions require special attention because they have a special encoding, do something special, or are otherwise different from the rest of the herd. To avoid hardcoding behavior handling as much as possible, <code>special_snowflake_args</code> encodes specific properties that many of these special instructions share, such as getting an immediate value instead of a register, swapping the <code>rs1</code> and <code>rs2</code> registers (or <code>vs1</code> and <code>vs2</code>), or omitting a register entirely. We can encode all these properties in a binary way so we use a custom bitmask system to save all the properties in a single byte. <code>custom_reg_val</code>, however, is a separate 1-byte field because registers use 5 bits, and only exists in tandem with <code>has_custom_reg_val</code>.</p>
<p>All together, we are able to use only 20kB of memory to save all the instructions, not withstanding future entries. This fits nicely into many CPU data caches.</p>

<p>In C++, by default, containers that dynamically allocate memory do so through the heap. The underlying OS provides the heap through assignment of a certain section of its virtual memory to the program requesting the heap memory. Heap allocation happens transparently most of the time. Unfortunately for us, it matters where exactly that memory is. Memory located far away from everything else (often the case with heap memory) unnecessarily clogs up the CPU&#39;s memory cache. Additionally, in C++, requesting that heap memory also requires a syscall every time the container geometrically changes size (roughly speaking, 1B -&gt; 2B -&gt; 4B -&gt; 8B -&gt; ... -&gt; 1MB). Syscalls drastically slow down code execution (more so than yo mama is big) because the OS needs to save all the registers, swap in the kernel&#39;s, and run the kernel code, all while clogging up the CPU cache again. Therefore, we need a way to allocate memory close to our variables with zero syscalls. </p>
<p>The solution? </p>
<p>Preallocated memory pools.</p>
<p>C++ offers a totally neato way to use the containers you know and love with a custom crafted memory allocator of your choice. </p>
<p>Here&#39;s how Ultrassembler does it.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>constexpr </span><span>size_t memory_pool_size = </span><span>33554432</span><span>;
</span><span>
</span><span>template </span><span>&lt;</span><span>class</span><span> T&gt;
</span><span>class </span><span>MemoryBank</span><span>;
</span><span>
</span><span>typedef</span><span> std::basic_string&lt;</span><span>char</span><span>, std::char_traits&lt;</span><span>char</span><span>&gt;, MemoryBank&lt;</span><span>char</span><span>&gt;&gt; ultrastring;
</span><span>
</span><span>template </span><span>&lt;</span><span>typename</span><span> T&gt;
</span><span>using </span><span>ultravector = std::vector&lt;T, MemoryBank&lt;T&gt;&gt;;
</span><span>
</span><span>class </span><span>GlobalMemoryBank </span><span>{
</span><span>    </span><span>inline static</span><span> std::array&lt;std::byte, memory_pool_size&gt; pool;
</span><span>    </span><span>inline static </span><span>size_t used </span><span>= </span><span>0</span><span>; 
</span><span>    </span><span>inline static long</span><span> pagesize </span><span>= </span><span>sysconf</span><span>(_SC_PAGE_SIZE); </span><span>// This only happens once :)
</span><span>
</span><span>public</span><span>:
</span><span>    </span><span>void</span><span>* </span><span>grab_some_memory</span><span>(size_t </span><span>requested</span><span>);
</span><span>
</span><span>    </span><span>void </span><span>reset</span><span>();
</span><span>}</span><span>;
</span><span>
</span><span>extern</span><span> GlobalMemoryBank memory_bank;
</span><span>
</span><span>template </span><span>&lt;</span><span>class</span><span> T&gt;
</span><span>class </span><span>MemoryBank </span><span>{
</span><span>public</span><span>:
</span><span>    </span><span>using </span><span>value_type </span><span>=</span><span> T;
</span><span>
</span><span>    </span><span>MemoryBank</span><span>() </span><span>= </span><span>default</span><span>;
</span><span>
</span><span>    [[nodiscard]] T</span><span>* </span><span>allocate</span><span>(size_t requested) {
</span><span>        std::size_t bytes </span><span>=</span><span> requested </span><span>* sizeof</span><span>(T);
</span><span>        </span><span>return </span><span>reinterpret_cast</span><span>&lt;T</span><span>*</span><span>&gt;(memory_bank.</span><span>grab_some_memory</span><span>(bytes));
</span><span>    }
</span><span>
</span><span>    </span><span>void </span><span>deallocate</span><span>(T</span><span>* </span><span>ptr</span><span>, size_t </span><span>requested</span><span>) { </span><span>return</span><span>; }
</span><span>
</span><span>    </span><span>bool </span><span>operator==</span><span>(</span><span>const</span><span> MemoryBank</span><span>&amp;</span><span>) </span><span>const </span><span>{ </span><span>return </span><span>true</span><span>; }
</span><span>}</span><span>;
</span><span>
</span><span>// In another file...
</span><span>
</span><span>void</span><span>* GlobalMemoryBank::</span><span>grab_some_memory</span><span>(size_t </span><span>requested</span><span>) {
</span><span>    </span><span>if </span><span>(requested + used &gt; pool.</span><span>size</span><span>()) {
</span><span>        </span><span>throw </span><span>UASError</span><span>(OutOfMemory, &#34;</span><span>Out of memory!</span><span>&#34;);
</span><span>    }
</span><span>    </span><span>void</span><span>* ptr = reinterpret_cast&lt;</span><span>void</span><span>*&gt;(pool.</span><span>data</span><span>() + used);
</span><span>    used += requested;
</span><span>    </span><span>return</span><span> ptr;
</span><span>}
</span><span>
</span><span>void </span><span>GlobalMemoryBank::</span><span>reset</span><span>() {
</span><span>    used = </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>Let&#39;s go through this section by section.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>constexpr </span><span>size_t memory_pool_size = </span><span>33554432</span><span>;
</span><span>
</span><span>template </span><span>&lt;</span><span>class</span><span> T&gt;
</span><span>class </span><span>MemoryBank</span><span>;
</span><span>
</span><span>typedef</span><span> std::basic_string&lt;</span><span>char</span><span>, std::char_traits&lt;</span><span>char</span><span>&gt;, MemoryBank&lt;</span><span>char</span><span>&gt;&gt; ultrastring;
</span><span>
</span><span>template </span><span>&lt;</span><span>typename</span><span> T&gt;
</span><span>using </span><span>ultravector = std::vector&lt;T, MemoryBank&lt;T&gt;&gt;;
</span></code></pre>
<p>This is boilerplate defining <em>how big our memory pool is</em> (in bytes), declaring <em>the regular memory pool class</em> (annoying!), what our <em>special memory pool string</em> is an alias of (a standard string but with the regular memory pool allocator), and the same creation of <em>a vector using the regular memory pool</em>.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>class </span><span>GlobalMemoryBank </span><span>{
</span><span>    </span><span>inline static</span><span> std::array&lt;std::byte, memory_pool_size&gt; pool;
</span><span>    </span><span>inline static </span><span>size_t used </span><span>= </span><span>0</span><span>;
</span><span>    </span><span>inline static long</span><span> pagesize </span><span>= </span><span>sysconf</span><span>(_SC_PAGE_SIZE);
</span><span>
</span><span>public</span><span>:
</span><span>    </span><span>void</span><span>* </span><span>grab_some_memory</span><span>(size_t </span><span>requested</span><span>);
</span><span>
</span><span>    </span><span>void </span><span>reset</span><span>();
</span><span>}</span><span>;
</span><span>
</span><span>extern</span><span> GlobalMemoryBank memory_bank;
</span></code></pre>
<p>This class defines the <em>memory pool wrapper</em> that the actual allocator uses. Why? This has to do with how C++ uses custom allocators. When you use a container with a custom allocator, each declaration of that container creates a separate instance of that container <em>and the allocator class</em>. Therefore, if you added the memory pool array as a member of this custom allocator class, each declaration of the container would result in separate instantiations of the underlying memory pool object. This is UNACCEPTABLE for Ultrassembler. Therefore, we instead use a helper class that the allocators call to. As a consequence, it allows us to add memory pool functionality controlled independently of the containers through calls to the helper <code>GlobalMemoryBank</code> class in the future.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>template </span><span>&lt;</span><span>class</span><span> T&gt;
</span><span>class </span><span>MemoryBank </span><span>{
</span><span>public</span><span>:
</span><span>    </span><span>using </span><span>value_type </span><span>=</span><span> T;
</span><span>
</span><span>    </span><span>MemoryBank</span><span>() </span><span>= </span><span>default</span><span>;
</span><span>
</span><span>    [[nodiscard]] T</span><span>* </span><span>allocate</span><span>(size_t requested) {
</span><span>        std::size_t bytes </span><span>=</span><span> requested </span><span>* sizeof</span><span>(T);
</span><span>        </span><span>return </span><span>reinterpret_cast</span><span>&lt;T</span><span>*</span><span>&gt;(memory_bank.</span><span>grab_some_memory</span><span>(bytes));
</span><span>    }
</span><span>
</span><span>    </span><span>void </span><span>deallocate</span><span>(T</span><span>* </span><span>ptr</span><span>, size_t </span><span>requested</span><span>) { </span><span>return</span><span>; }
</span><span>
</span><span>    </span><span>bool </span><span>operator==</span><span>(</span><span>const</span><span> MemoryBank</span><span>&amp;</span><span>) </span><span>const </span><span>{ </span><span>return </span><span>true</span><span>; }
</span><span>}</span><span>;
</span></code></pre>
<p>This is the actual <em>custom allocator</em> object that we pass to C++ containers. The definition of a custom allocator in C++ is simply a class that provides the <code>allocate</code> and <code>deallocate</code> functions publicly. That&#39;s literally it. There are in fact more potential functions that you could add to handle specific uses, but <code>allocate</code> and <code>deallocate</code> are all we need for Ultrassembler. We define this class as a template because the return value of the <code>allocate</code> function must match the underlying type of the container using the allocator class. We furthermore define the <code>==</code> operator because C++ requires that two objects using allocators match their allocators. You&#39;ll normally never notice this because the default allocator for all C++ containers, <code>std::allocator</code>, provides all the allocator functions and operator comparison functions, and as a result, handles all comparisons transparently. Ultrassembler only uses equality. Finally, we provide a default constructor <code>MemoryBank() = default;</code> as this is what the C++ standard expects too from allocator classes.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void</span><span>* GlobalMemoryBank::</span><span>grab_some_memory</span><span>(size_t </span><span>requested</span><span>) {
</span><span>    </span><span>if </span><span>(requested + used &gt; pool.</span><span>size</span><span>()) {
</span><span>        </span><span>throw </span><span>UASError</span><span>(OutOfMemory, &#34;</span><span>Out of memory!</span><span>&#34;);
</span><span>    }
</span><span>    </span><span>void</span><span>* ptr = reinterpret_cast&lt;</span><span>void</span><span>*&gt;(pool.</span><span>data</span><span>() + used);
</span><span>    used += requested;
</span><span>    </span><span>return</span><span> ptr;
</span><span>}
</span><span>
</span><span>void </span><span>GlobalMemoryBank::</span><span>reset</span><span>() {
</span><span>    used = </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>These functions implement <em>allocating the memory</em> and <em>resetting the memory bank</em>. Allocating should be obvious. However, resetting might not. As it stands, the memory pool simply gives up if it runs out of memory to allocate. We don&#39;t deallocate because such an operation would add extra overhead and subjects us to the issue of memory fragementation. Memory fragmentation is when you deallocate a small object from a large area of allocated memory, leaving a small area of unallocated memory laying in the otherwise allocated area. If you want to allocate a new object, tough luck, you probably can&#39;t fit it in this small area. You need to wait for the other objects to deallocate first. This cycle continues until your memory usage looks like Swiss cheese and doesn&#39;t support allocating any more objects, leading to a system crash. Normally, the OS kernel handles this problem transparently. Linux for example uses a &#34;buddy allocator&#34; to help deal with it. Memory fragmentation is also less of an issue with huge swaths of memory on modern systems. Our memory pool unfortunately lacks those luxuries of large memory and processing power for buddy allocators. Therefore, we provide the <code>reset</code> function to start everything over if the software using Ultrassembler receives an <code>OutOfMemory</code> exception.</p>
<p>Our memory pool trick lets Ultrassembler enjoy optimal memory locality and predefined memory usage while also completely eliminating syscalls (almost) and memory leaks, notwithstanding occasional memory bank resets.</p>

<p>A while ago, I read <a href="https://mazzo.li/posts/value-speculation.html">this fascinating article on something called L1 value speculation</a>. The basic idea is to free the branch predictor by giving it extra work to do guessing the next value in the linked list. If it&#39;s right (usually it is) then you get a free speedup.</p>
<p>Ultrassembler does something similar. Instead of a linked list, we iterate through an array checking for specific combinations of characters that define the end of a sequence to copy. </p>
<pre data-lang="cpp"><code data-lang="cpp"><span>auto</span><span> ch = [&amp;]() {
</span><span>    </span><span>return</span><span> data[i];
</span><span>};
</span><span>
</span><span>volatile char</span><span> preview;
</span><span>while </span><span>(i &lt; data.</span><span>size</span><span>() &amp;&amp; </span><span>not_at_end</span><span>(</span><span>ch</span><span>()) &amp;&amp; !</span><span>is_whitespace</span><span>(</span><span>ch</span><span>())) {
</span><span>    c.</span><span>inst</span><span>.</span><span>push_back</span><span>(</span><span>ch</span><span>());
</span><span>    i++;
</span><span>    preview = </span><span>ch</span><span>();
</span><span>}
</span></code></pre>
<p>As built-in strings in C++ are super duper mega slow even with custom allocators, we spend a lot of time on <code>c.inst.push_back(ch());</code>. There&#39;s fortunately a workaround. If the CPU knows that we&#39;ll be accessing the next character in the target string, why not queue it up first? This is exactly what <code>volatile char preview;</code> and <code>preview = ch();</code> accomplish. We already have an opportunity for speculation with the <code>i++;</code> and <code>i &lt; data.size();</code>. Although I&#39;m not 100% sure, my hypothesis on why <code>preview</code> provides a speedup is that the branch predictor can only handle <code>i &lt; data.size()</code> and not additionally the character loading of <code>ch()</code>. Therefore, we should preemptively load <code>ch()</code> during <code>c.inst.push_back(ch());</code>. </p>
<p>Eagle eyed readers will notice how there is an opportunity for memory overflow if we are at the end of a string and <code>i++;</code> then <code>preview = ch();</code> loads a character past the string <code>data</code>. However, Ultrassembler accounts for this by preemptively adding an extra null character to the input string <code>data</code> earlier in the code, ensuring that such illegal memory accesses are impossible by definition. </p>
<p>This optimization sped up parsing of the instruction names enough that the overall Ultrassembler performance increased by about 10%.</p>

<p>Here&#39;s one weird trick I haven&#39;t seen anywhere else.</p>
<p>Imagine I provided you these words: apple, apricot, avocado, and banana. </p>
<p>Now, what if I told you a mystery word I was looking for among the ones I provided was 7 letters long. You would immediately discard &#34;apple&#34; and &#34;banana&#34; because they&#39;re not 7 letters long. Now, I tell you that it starts with &#34;a.&#34; You wouldn&#39;t discard any at this point because both &#34;apricot&#34; and &#34;avocado&#34; start with the letter a. Finally, I tell you that the second letter is &#34;v.&#34; Immediately we know &#34;avocado&#34; is the mystery word because no other word remaining starts with &#34;av.&#34;</p>
<p>This is the basic idea behind the instruction, register, CSR, and pseudoinstruction lookup systems in Ultrassembler. There&#39;s a rub, though. The code for these lookups looks something like this:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>const </span><span>uint16_t </span><span>fast_instr_search</span><span>(</span><span>const</span><span> ultrastring&amp; </span><span>inst</span><span>) {
</span><span>    </span><span>const auto</span><span> size = inst.</span><span>size</span><span>();
</span><span>
</span><span>    </span><span>if </span><span>(size == </span><span>2</span><span>) {
</span><span>        </span><span>if </span><span>(inst[</span><span>0</span><span>] == &#39;</span><span>s</span><span>&#39;) {
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>d</span><span>&#39;) </span><span>return </span><span>44</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>w</span><span>&#39;) </span><span>return </span><span>17</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>b</span><span>&#39;) </span><span>return </span><span>15</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>h</span><span>&#39;) </span><span>return </span><span>16</span><span>;
</span><span>        }
</span><span>        </span><span>if </span><span>(inst[</span><span>0</span><span>] == &#39;</span><span>o</span><span>&#39;) {
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>r</span><span>&#39;) </span><span>return </span><span>35</span><span>;
</span><span>        }
</span><span>        </span><span>if </span><span>(inst[</span><span>0</span><span>] == &#39;</span><span>l</span><span>&#39;) {
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>d</span><span>&#39;) </span><span>return </span><span>43</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>w</span><span>&#39;) </span><span>return </span><span>12</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>b</span><span>&#39;) </span><span>return </span><span>10</span><span>;
</span><span>            </span><span>if </span><span>(inst[</span><span>1</span><span>] == &#39;</span><span>h</span><span>&#39;) </span><span>return </span><span>11</span><span>;
</span><span>        }
</span><span>    }
</span><span>
</span><span>    </span><span>if </span><span>(size == </span><span>3</span><span>) {
</span><span>        etc...
</span></code></pre>
<p>Clearly, there&#39;s a lot of work to do if you&#39;ve got thousands of entries like the instructions array does. There&#39;s a fix for that though! </p>
<p>Enter codegen. </p>
<p>Ultrassembler uses artisan-crafted Python scripts to traverse through the listings and extract the string names for each instruction, register, CSR, and pseudoinstruction. Then, these scripts generate C++ code which performs these precomputed lookups. </p>
<p>Here&#39;s what the instruction search script looks like. <span>WARNING!</span> If this script looks ugly, it&#39;s because Python is one of the worst programming languages out there for anything more than mere supportive, throwaway software like this.</p>
<pre data-lang="python"><code data-lang="python"><span>input </span><span>= &#34;</span><span>src/instructions.cpp</span><span>&#34;
</span><span>output = &#34;</span><span>src/generated/instruction_search.cpp</span><span>&#34;
</span><span>
</span><span>import </span><span>re
</span><span>
</span><span>content = &#34;&#34;
</span><span>with </span><span>open</span><span>(</span><span>input</span><span>, &#34;</span><span>r</span><span>&#34;) </span><span>as </span><span>file:
</span><span>    content = file.</span><span>read</span><span>()
</span><span>
</span><span>regex = &#34;</span><span>(?&lt;={)</span><span>\&#34;</span><span>([\w.]+)</span><span>\&#34;</span><span>&#34;
</span><span>
</span><span>instructions = re.</span><span>findall</span><span>(regex, content)
</span><span>
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>len</span><span>(instructions)):
</span><span>    instructions[i] = (instructions[i], i, </span><span>len</span><span>(instructions[i]))
</span><span>
</span><span>instructions.</span><span>sort</span><span>()
</span><span>
</span><span>print</span><span>(instructions)
</span><span>
</span><span>min_len = </span><span>min</span><span>([i[</span><span>2</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions])
</span><span>
</span><span>max_len = </span><span>max</span><span>([i[</span><span>2</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions])
</span><span>
</span><span>depth = </span><span>0
</span><span>
</span><span>current_instr = &#34;&#34;
</span><span>
</span><span>code = &#34;</span><span>// SPDX-License-Identifier: MPL-2.0</span><span>\n</span><span>&#34;
</span><span>code += &#34;</span><span>// The generate_instruction_search.py script automatically generated this code. DO NOT MODIFY!</span><span>\n</span><span>&#34;
</span><span>code += &#34;</span><span>#include </span><span>\&#34;</span><span>../instructions.hpp</span><span>\&#34;\n</span><span>&#34;
</span><span>code += &#34;</span><span>#include </span><span>\&#34;</span><span>../ultrassembler.hpp</span><span>\&#34;\n\n</span><span>&#34;
</span><span>code += &#34;</span><span>namespace ultrassembler_internal {</span><span>\n\n</span><span>&#34;
</span><span>code += &#34;</span><span>const uint16_t fast_instr_search(const ultrastring&amp; inst) {</span><span>\n</span><span>&#34;
</span><span>code += &#34;</span><span>    const auto size = inst.size();</span><span>\n\n</span><span>&#34;
</span><span>
</span><span>def </span><span>ind</span><span>():
</span><span>    </span><span>return </span><span>&#34;    &#34; * (depth + </span><span>2</span><span>)
</span><span>
</span><span>def </span><span>instr_exists</span><span>(</span><span>instr</span><span>, </span><span>length</span><span>):
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>instructions:
</span><span>        </span><span>if </span><span>i[</span><span>0</span><span>] == instr and i[</span><span>2</span><span>] == length:
</span><span>            </span><span>return </span><span>True
</span><span>    </span><span>return </span><span>False
</span><span>    
</span><span>def </span><span>prefix_exists</span><span>(</span><span>prefix</span><span>, </span><span>length</span><span>):
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>instructions:
</span><span>        </span><span>if </span><span>i[</span><span>0</span><span>].</span><span>startswith</span><span>(prefix) and i[</span><span>2</span><span>] == length:
</span><span>            </span><span>return </span><span>True
</span><span>    </span><span>return </span><span>False
</span><span>
</span><span>potentialchars = &#34;&#34;
</span><span>
</span><span>for </span><span>instr </span><span>in </span><span>instructions:
</span><span>    </span><span>for </span><span>char </span><span>in </span><span>instr[</span><span>0</span><span>]:
</span><span>        </span><span>if </span><span>char not in potentialchars:
</span><span>            potentialchars += char
</span><span>
</span><span>def </span><span>process_depth</span><span>(</span><span>current_len</span><span>):
</span><span>    </span><span>global </span><span>code, current_instr, depth
</span><span>    </span><span>for </span><span>letter </span><span>in </span><span>potentialchars:
</span><span>        </span><span>if </span><span>instr_exists</span><span>(current_instr + letter, current_len):
</span><span>            code += </span><span>ind</span><span>() + </span><span>f</span><span>&#34;</span><span>if (inst[</span><span>{depth}</span><span>] == &#39;</span><span>{letter}</span><span>&#39;) return </span><span>{instructions[[i[</span><span>0</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions].</span><span>index</span><span>(current_instr + letter)][</span><span>1</span><span>]}</span><span>;</span><span>\n</span><span>&#34;
</span><span>        </span><span>elif </span><span>prefix_exists</span><span>(current_instr + letter, current_len):
</span><span>            code += </span><span>ind</span><span>() + </span><span>f</span><span>&#34;</span><span>if (inst[</span><span>{depth}</span><span>] == &#39;</span><span>{letter}</span><span>&#39;) </span><span>{{\n</span><span>&#34;
</span><span>            current_instr += letter
</span><span>            depth += </span><span>1
</span><span>            </span><span>process_depth</span><span>(current_len)
</span><span>            depth -= </span><span>1
</span><span>            current_instr = current_instr[:-</span><span>1</span><span>]
</span><span>            code += </span><span>ind</span><span>() + &#34;</span><span>}</span><span>\n</span><span>&#34;
</span><span>
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(min_len, max_len + </span><span>1</span><span>):
</span><span>    code += </span><span>f</span><span>&#34;</span><span>    if (size == </span><span>{i}</span><span>) </span><span>{{\n</span><span>&#34;
</span><span>    </span><span>process_depth</span><span>(i)
</span><span>    code += &#34;</span><span>    }</span><span>\n\n</span><span>&#34;
</span><span>
</span><span>code += &#34;</span><span>    return instr_search_failed;</span><span>\n</span><span>&#34;
</span><span>code += &#34;</span><span>}</span><span>\n\n</span><span>&#34;
</span><span>code += &#34;</span><span>} // namespace ultrassembler_internal</span><span>&#34;
</span><span>
</span><span>print</span><span>(code)
</span><span>
</span><span>with </span><span>open</span><span>(output, &#34;</span><span>w</span><span>&#34;) </span><span>as </span><span>file:
</span><span>    file.</span><span>write</span><span>(code)
</span></code></pre>
<p>Let&#39;s go through it section by section.</p>
<pre data-lang="python"><code data-lang="python"><span>input </span><span>= &#34;</span><span>src/instructions.cpp</span><span>&#34;
</span><span>output = &#34;</span><span>src/generated/instruction_search.cpp</span><span>&#34;
</span><span>
</span><span>import </span><span>re
</span><span>
</span><span>content = &#34;&#34;
</span><span>with </span><span>open</span><span>(</span><span>input</span><span>, &#34;</span><span>r</span><span>&#34;) </span><span>as </span><span>file:
</span><span>    content = file.</span><span>read</span><span>()
</span></code></pre>
<p>This simply tells the script what file to read and where to generate the code, imports the regex package, and reads the input file.</p>
<pre data-lang="python"><code data-lang="python"><span>regex = &#34;</span><span>(?&lt;={)</span><span>\&#34;</span><span>([\w.]+)</span><span>\&#34;</span><span>&#34;
</span><span>
</span><span>instructions = re.</span><span>findall</span><span>(regex, content)
</span><span>
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>len</span><span>(instructions)):
</span><span>    instructions[i] = (instructions[i], i, </span><span>len</span><span>(instructions[i]))
</span><span>
</span><span>instructions.</span><span>sort</span><span>()
</span><span>
</span><span>print</span><span>(instructions)
</span></code></pre>
<p>This regex searches for all instances of quotes in the instruction C++ code. That code looks like this:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>const</span><span> std::array&lt;rvinstruction, </span><span>2034</span><span>&gt; instructions = {
</span><span>        {{&#34;</span><span>lui</span><span>&#34;, LUI, U, op_LUI, </span><span>0b000</span><span>, RVI, int_reg},
</span><span>         {&#34;</span><span>auipc</span><span>&#34;, AUIPC, U, op_AUIPC, </span><span>0b000</span><span>, RVI, int_reg},
</span><span>         {&#34;</span><span>jal</span><span>&#34;, JAL, J, op_JAL, </span><span>0b000</span><span>, RVI, int_reg}, etc...
</span></code></pre>
<p>Then, it creates a new array with the instruction name, what position it is in the array, and its length. This might seem redundant at first, but it&#39;s helpful later. We then sort all the insructions alphabetically (also important!) and show all of them for debugging/status purposes.</p>
<pre data-lang="python"><code data-lang="python"><span>min_len = </span><span>min</span><span>([i[</span><span>2</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions])
</span><span>
</span><span>max_len = </span><span>max</span><span>([i[</span><span>2</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions])
</span><span>
</span><span>depth = </span><span>0
</span><span>
</span><span>current_instr = &#34;&#34;
</span><span>
</span><span>code = &#34;</span><span>// SPDX-License-Identifier: MPL-2.0</span><span>\n</span><span>&#34;
</span><span>code += &#34;</span><span>// The generate_instruction_search.py script automatically generated this code. DO NOT MODIFY!</span><span>\n</span><span>&#34;
</span><span>code += &#34;</span><span>#include </span><span>\&#34;</span><span>../instructions.hpp</span><span>\&#34;\n</span><span>&#34;
</span><span>code += &#34;</span><span>#include </span><span>\&#34;</span><span>../ultrassembler.hpp</span><span>\&#34;\n\n</span><span>&#34;
</span><span>code += &#34;</span><span>namespace ultrassembler_internal {</span><span>\n\n</span><span>&#34;
</span><span>code += &#34;</span><span>const uint16_t fast_instr_search(const ultrastring&amp; inst) {</span><span>\n</span><span>&#34;
</span><span>code += &#34;</span><span>    const auto size = inst.size();</span><span>\n\n</span><span>&#34;
</span><span>
</span><span>def </span><span>ind</span><span>():
</span><span>    </span><span>return </span><span>&#34;    &#34; * (depth + </span><span>2</span><span>)
</span><span>
</span><span>def </span><span>instr_exists</span><span>(</span><span>instr</span><span>, </span><span>length</span><span>):
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>instructions:
</span><span>        </span><span>if </span><span>i[</span><span>0</span><span>] == instr and i[</span><span>2</span><span>] == length:
</span><span>            </span><span>return </span><span>True
</span><span>    </span><span>return </span><span>False
</span><span>    
</span><span>def </span><span>prefix_exists</span><span>(</span><span>prefix</span><span>, </span><span>length</span><span>):
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>instructions:
</span><span>        </span><span>if </span><span>i[</span><span>0</span><span>].</span><span>startswith</span><span>(prefix) and i[</span><span>2</span><span>] == length:
</span><span>            </span><span>return </span><span>True
</span><span>    </span><span>return </span><span>False
</span><span>
</span><span>potentialchars = &#34;&#34;
</span><span>
</span><span>for </span><span>instr </span><span>in </span><span>instructions:
</span><span>    </span><span>for </span><span>char </span><span>in </span><span>instr[</span><span>0</span><span>]:
</span><span>        </span><span>if </span><span>char not in potentialchars:
</span><span>            potentialchars += char
</span></code></pre>
<p>This is a lot of boilerplate for the algorithm later to come. We find the shortest and longest instructions. We add the first parts of the generated file. We define an indentation helper for nice formatting. We define additional helper functions to check if a whole instruction exists with a given name and length or if there is an instruction with the provided prefix and length. Finally, we assemble an array with all the characters to search for that the instructions use to avoid unnecessary computation later.</p>
<pre data-lang="python"><code data-lang="python"><span>def </span><span>process_depth</span><span>(</span><span>current_len</span><span>):
</span><span>    </span><span>global </span><span>code, current_instr, depth
</span><span>    </span><span>for </span><span>letter </span><span>in </span><span>potentialchars:
</span><span>        </span><span>if </span><span>instr_exists</span><span>(current_instr + letter, current_len):
</span><span>            code += </span><span>ind</span><span>() + </span><span>f</span><span>&#34;</span><span>if (inst[</span><span>{depth}</span><span>] == &#39;</span><span>{letter}</span><span>&#39;) return </span><span>{instructions[[i[</span><span>0</span><span>] </span><span>for </span><span>i </span><span>in </span><span>instructions].</span><span>index</span><span>(current_instr + letter)][</span><span>1</span><span>]}</span><span>;</span><span>\n</span><span>&#34;
</span><span>        </span><span>elif </span><span>prefix_exists</span><span>(current_instr + letter, current_len):
</span><span>            code += </span><span>ind</span><span>() + </span><span>f</span><span>&#34;</span><span>if (inst[</span><span>{depth}</span><span>] == &#39;</span><span>{letter}</span><span>&#39;) </span><span>{{\n</span><span>&#34;
</span><span>            current_instr += letter
</span><span>            depth += </span><span>1
</span><span>            </span><span>process_depth</span><span>(current_len)
</span><span>            depth -= </span><span>1
</span><span>            current_instr = current_instr[:-</span><span>1</span><span>]
</span><span>            code += </span><span>ind</span><span>() + &#34;</span><span>}</span><span>\n</span><span>&#34;
</span><span>
</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(min_len, max_len + </span><span>1</span><span>):
</span><span>    code += </span><span>f</span><span>&#34;</span><span>    if (size == </span><span>{i}</span><span>) </span><span>{{\n</span><span>&#34;
</span><span>    </span><span>process_depth</span><span>(i)
</span><span>    code += &#34;</span><span>    }</span><span>\n\n</span><span>&#34;
</span></code></pre>
<p>Here&#39;s where the magic happens. We process one instruction length depth at a time. Like the algorithm we talked about at the beginning of this section, we start with the shortest possible &#34;words&#34; and work our way to the longest. Each depth step works through a search of all the possible characters and first checks if we have already found an instruction. If there is such an instruction, we add it to the code. Alternatively, if there is no such instruction but there is in fact an instruction that starts with the current sequence, we go down a depth level because we know that eventually, we will find an instruction with an exact match. Once we&#39;ve gone through all of the possible instructions and depths, we exit the <code>for</code> loop.</p>
<pre data-lang="python"><code data-lang="python"><span>code += &#34;</span><span>    return instr_search_failed;</span><span>\n</span><span>&#34;
</span><span>code += &#34;</span><span>}</span><span>\n\n</span><span>&#34;
</span><span>code += &#34;</span><span>} // namespace ultrassembler_internal</span><span>&#34;
</span><span>
</span><span>print</span><span>(code)
</span><span>
</span><span>with </span><span>open</span><span>(output, &#34;</span><span>w</span><span>&#34;) </span><span>as </span><span>file:
</span><span>    file.</span><span>write</span><span>(code)
</span></code></pre>
<p>This completes the generated search function, shows it all for debugging/status purposes, and finally writes the generated code to the output file path.</p>
<p>There are no other instances of this kind of codegen that I know of. That&#39;s surprising, because codegen allows us to perform lookup of thousands of instructions with near-zero overhead. I estimate each instruction lookup takes on the order of 10 instructions to complete.</p>
<p>Here&#39;s what the resulting compiled assembly looks like on my x86 PC:</p>
<pre><code><span>0000000000029340 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE&gt;:
</span><span>   29340:	f3 0f 1e fa          	endbr64 
</span><span>   29344:	48 8b 47 08          	mov    0x8(%rdi),%rax
</span><span>   29348:	48 83 f8 02          	cmp    $0x2,%rax
</span><span>   2934c:	0f 84 c6 00 00 00    	je     29418 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0xd8&gt;
</span><span>   29352:	48 83 f8 03          	cmp    $0x3,%rax
</span><span>   29356:	75 28                	jne    29380 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x40&gt;
</span><span>   29358:	48 8b 17             	mov    (%rdi),%rdx
</span><span>   2935b:	0f b6 0a             	movzbl (%rdx),%ecx
</span><span>   2935e:	80 f9 61             	cmp    $0x61,%cl
</span><span>   29361:	0f 84 79 2b 00 00    	je     2bee0 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x2ba0&gt;
</span><span>   29367:	80 f9 64             	cmp    $0x64,%cl
</span><span>   2936a:	0f 85 58 10 00 00    	jne    2a3c8 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x1088&gt;
</span><span>   29370:	80 7a 01 69          	cmpb   $0x69,0x1(%rdx)
</span><span>   29374:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
</span><span>   29379:	0f 84 09 2f 00 00    	je     2c288 &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x2f48&gt;
</span><span>   2937f:	c3                   	ret
</span><span>   # There are thousands more lines of this!
</span></code></pre>
<p>And RISC-V:</p>
<pre><code><span>000000000007c33c &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE&gt;:
</span><span>   7c33c:	7179                	addi	sp,sp,-48
</span><span>   7c33e:	f406                	sd	ra,40(sp)
</span><span>   7c340:	e42a                	sd	a0,8(sp)
</span><span>   7c342:	6522                	ld	a0,8(sp)
</span><span>   7c344:	00089317          	auipc	t1,0x89
</span><span>   7c348:	afc33303          	ld	t1,-1284(t1) # 104e40 &lt;_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcEN22ultrassembler_internal10MemoryBankIcEEE4sizeEv@@Base+0xad9c4&gt;
</span><span>   7c34c:	9302                	jalr	t1
</span><span>   7c34e:	ec2a                	sd	a0,24(sp)
</span><span>   7c350:	6762                	ld	a4,24(sp)
</span><span>   7c352:	4789                	li	a5,2
</span><span>   7c354:	22f71c63          	bne	a4,a5,7c58c &lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x250&gt;
</span><span>   7c358:	4581                	li	a1,0
</span><span>   7c35a:	6522                	ld	a0,8(sp)
</span><span>   7c35c:	00089317          	auipc	t1,0x89
</span><span>   7c360:	c6433303          	ld	t1,-924(t1) # 104fc0 &lt;_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcEN22ultrassembler_internal10MemoryBankIcEEEixEm@@Base+0xaaef8&gt;
</span><span>   7c364:	9302                	jalr	t1
</span><span>   # Also thousands more lines of this!
</span></code></pre>

<p>This is similar to script codegen but with native C++ only.</p>
<p>One of the verification steps in Ultrassembler involves checking that the immediate value of an instruction (for example, <code>addi t0, t1, 100</code>) fits within some known range. C++ allows us to both cleanly invoke this check for an arbitrary range and do so with little to no runtime overhead to calculate that range.</p>
<p>Here&#39;s how it works.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>template </span><span>&lt;</span><span>auto</span><span> bits&gt;
</span><span>void </span><span>verify_imm</span><span>(</span><span>const auto</span><span>&amp; </span><span>imm</span><span>) {
</span><span>    </span><span>using </span><span>T = </span><span>decltype</span><span>(bits);
</span><span>    </span><span>if constexpr </span><span>(std::is_signed_v&lt;T&gt;) {
</span><span>        </span><span>if </span><span>(imm &lt; -(</span><span>1 </span><span>&lt;&lt; (bits - </span><span>1</span><span>)) || imm &gt;= (</span><span>1 </span><span>&lt;&lt; (bits - </span><span>1</span><span>))) {
</span><span>            </span><span>throw </span><span>UASError</span><span>(ImmOutOfRange, &#34;</span><span>Immediate </span><span>&#34; + </span><span>to_ultrastring</span><span>(imm) + &#34;</span><span> is out of range [</span><span>&#34; + </span><span>to_ultrastring</span><span>(-(</span><span>1 </span><span>&lt;&lt; (bits - </span><span>1</span><span>))) + &#34;</span><span>, </span><span>&#34; + </span><span>to_ultrastring</span><span>((</span><span>1 </span><span>&lt;&lt; (bits - </span><span>1</span><span>))) + &#34;</span><span>)</span><span>&#34;, </span><span>0</span><span>, </span><span>0</span><span>);
</span><span>        }
</span><span>    } </span><span>else if constexpr </span><span>(std::is_unsigned_v&lt;T&gt;) {
</span><span>        </span><span>if </span><span>(imm &lt; </span><span>0 </span><span>|| imm &gt;= (</span><span>1</span><span>u </span><span>&lt;&lt; bits)) {
</span><span>            </span><span>throw </span><span>UASError</span><span>(ImmOutOfRange, &#34;</span><span>Immediate </span><span>&#34; + </span><span>to_ultrastring</span><span>(imm) + &#34;</span><span> is out of range [0, </span><span>&#34; + </span><span>to_ultrastring</span><span>((</span><span>1</span><span>u </span><span>&lt;&lt; bits)) + &#34;</span><span>)</span><span>&#34;, </span><span>0</span><span>, </span><span>0</span><span>);
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>Each invocation looks something like <code>verify_imm&lt;5u&gt;(imm)</code>. We supply a numeric literal and the immediate variable to check. C++&#39;s template facilities then check whether we&#39;ve supplied a signed or unsigned numeric literal, as RISC-V instruction can vary whether they expect signed or unsigned numbers only. We then calculate the lowest possible number (<code>-(1 &lt;&lt; (bits - 1))</code> for signed and <code>0</code> for unsigned) and the highest possible number (<code>(1 &lt;&lt; (bits - 1))</code> for signed and <code>(1u &lt;&lt; bits)</code> for unsigned) and check the input against that. We then throw an error if it doesn&#39;t fit these calculated constraints or return silently if it does.</p>
<p>The <code>if constexpr</code> tells the compiler to generate each signed or unsigned execution path at compile time depending on what numeric literal we&#39;ve provided, allowing us to make each function call as pretty and fast as possible.</p>

<p>For the times where we can&#39;t or don&#39;t want to use a precomputed string search, Ultrassembler uses an optimized string comparison function to minimize overhead.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>bool </span><span>fast_eq</span><span>(</span><span>const auto</span><span>&amp; </span><span>first</span><span>, </span><span>const</span><span> std::string_view&amp; </span><span>second</span><span>) {
</span><span>    </span><span>if </span><span>(first.</span><span>size</span><span>() != second.</span><span>size</span><span>()) { 
</span><span>        </span><span>return </span><span>false</span><span>;
</span><span>    }
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; first.</span><span>size</span><span>(); i++) {
</span><span>        </span><span>if </span><span>(first[i] != second[i]) {
</span><span>            [[likely]] </span><span>return </span><span>false</span><span>;
</span><span>        } </span><span>else </span><span>{
</span><span>            [[unlikely]] </span><span>continue</span><span>;
</span><span>        }
</span><span>    }
</span><span>    </span><span>return </span><span>true</span><span>;
</span><span>}
</span></code></pre>
<p>How does this work? First, we check to make sure the input strings are the same length. It&#39;s impossible by definition for them to be the same if they have different lengths. Then, we compare them character by character. Here, we use C++20&#39;s <code>[[likely]]</code> and <code>[[unlikely]]</code> tags to help the compiler optimize the positioning of each comparison. It&#39;s statistically more likely to have a comparison failure than a success because we are usually comparing one input string against many possible options but it can only match with up to one.</p>

<p>This one surprised me.</p>
<p>When you call a C++ function, you can choose to pass your arguments <em>by value</em>, or <em>by reference</em>. By default, C++ uses <em>by value</em>, which means the code internally makes a copy of the argument and provides that copy to the function. If you add a <code>&amp;</code> to make it a reference instead (there are other ways to do this too) then the code generates a pointer to that original object and passes that pointer to the function. However, unlike pointers, references handle referencing and dereferencing transparently. As an aside, this also means Ultrassembler technically doesn&#39;t use pointers... anywhere! Pointers are horrible.</p>
<p>One of the most common pieces of C++ optimization advice is to use references whenever possible to avoid the copy overhead incurred by value references. It might surprise you, then, to find out that the following code is vastly faster due to the use of a value argument:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>size_t </span><span>parse_this_line</span><span>(size_t </span><span>i</span><span>, </span><span>const</span><span> ultrastring&amp; </span><span>data</span><span>, assembly_context&amp; </span><span>c</span><span>) {
</span><span>    </span><span>// code that does &#34;i++;&#34; a lot
</span><span>}
</span><span>
</span><span>// later, in a different function:
</span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; data.</span><span>size</span><span>();) {
</span><span>    i = </span><span>parse_this_line</span><span>(i, data, c);
</span><span>    </span><span>// etc...
</span><span>}
</span></code></pre>
<p>If we had applied the Programming Furus©️®️™️&#39;s advice to pass <code>i</code> by reference, it would have looked like:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void </span><span>parse_this_line</span><span>(size_t&amp; </span><span>i</span><span>, </span><span>const</span><span> ultrastring&amp; </span><span>data</span><span>, assembly_context&amp; </span><span>c</span><span>) {
</span><span>    </span><span>// code that does &#34;i++;&#34; a lot
</span><span>}
</span><span>
</span><span>// later, in a different function:
</span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; data.</span><span>size</span><span>();) {
</span><span>    </span><span>parse_this_line</span><span>(i, data, c);
</span><span>    </span><span>// etc...
</span><span>}
</span></code></pre>
<p>So why is the first one faster? Here&#39;s why.</p>
<p>Under the hood of all programming languages, you have assembly code which translates to the CPU&#39;s machine code. There are also no variables. Instead, you&#39;ve got registers which hold raw data and raw memory. In most application processors today, the registers are 64 bits wide, and maybe wider for special vector operations which don&#39;t matter here. 64 bits happens to match the maximum width of so-called <em>fundamental types</em> in C and C++ which are integers and most common floats. Therefore, we can fit at least one fundamental type into each register.</p>
<p>Quick refresher of the registers in RISC-V:</p>
<p><img src="https://jghuff.com/RISC-V-registers-safe.svg" alt="Infographic of the integer RISC-V registers"/></p>
<p>Assembly also has little concept of a function call. Internally, all function calls do is clear out the current registers, load them with the function parameters, then jump to the function&#39;s address. This means all function calls involve at least one copy per argument, whether it&#39;s a fundamental type or a pointer to a fundamental type or a pointer to something else.</p>
<pre><code><span># Here&#39;s what this looks like in RISC-V assembly.
</span><span># Say we have a number in register t0, like 69.
</span><span>
</span><span>addi t0, x0, 69
</span><span>
</span><span># We also have a function foobar that takes a single integer argument (like &#34;void foobar(size_t arg)&#34; in C/C++)
</span><span># We can copy that register (and therefore its value) to argument register a0 before calling foobar
</span><span>
</span><span>addi a0, t0, 0
</span><span>
</span><span>jal foobar
</span><span>
</span><span># The copying of this value only took one step!
</span></code></pre>
<p>You can see where we&#39;re going. If our goal is to minimize copying, it would be better to copy a fundamental type once than to generate a pointer, copy that, then dereference that pointer to get the underlying value. That is the crux of this subtle optimization trick. The cost to copy one register is less than the cost to copy a register holding a pointer. </p>
<p>Note how I&#39;ve only talked about fundamental types. Any type which does not fit in a single register, AKA many structs, containers, or anything else that isn&#39;t a fundamental type, costs more to copy by value in multiple registers than it does to copy a single register holding a pointer. I don&#39;t know of any Programming Furu©️®️™️ that makes this distinction clear.</p>

<p>One of the steps to assemble a jump operation in RISC-V assembly is to calculate the offset of bytes to the jump target. However, this is often impossible unless all other instructions are already assembled. Ultrassembler does its best to avoid insertions or deletions through a clever trick to assemble jump instructions with a placeholder jump offset and then insert the correct offset in-place at the end.</p>
<p>Here&#39;s how it works:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void </span><span>solve_label_offsets</span><span>(assembly_context&amp; </span><span>c</span><span>) {
</span><span>    </span><span>using </span><span>enum RVInstructionFormat;
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; c.</span><span>label_locs</span><span>.</span><span>size</span><span>(); i++) {
</span><span>        </span><span>if </span><span>(!c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>is_dest</span><span>) {
</span><span>            </span><span>for </span><span>(size_t j = </span><span>0</span><span>; j &lt; c.</span><span>label_locs</span><span>.</span><span>size</span><span>(); j++) {
</span><span>                </span><span>if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(j).</span><span>is_dest </span><span>&amp;&amp; c.</span><span>label_locs</span><span>.</span><span>at</span><span>(j).</span><span>id </span><span>== c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>id</span><span>) {
</span><span>                    uint32_t inst = </span><span>0</span><span>;
</span><span>
</span><span>                    </span><span>if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>i_bytes </span><span>== </span><span>2</span><span>) {
</span><span>                        inst = reinterpret_cast&lt;uint16_t&amp;&gt;(c.</span><span>machine_code</span><span>.</span><span>at</span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>));
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>i_bytes </span><span>== </span><span>4</span><span>) {
</span><span>                        inst = reinterpret_cast&lt;uint32_t&amp;&gt;(c.</span><span>machine_code</span><span>.</span><span>at</span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>));
</span><span>                    }
</span><span>
</span><span>                    int32_t offset = c.</span><span>label_locs</span><span>.</span><span>at</span><span>(j).</span><span>loc </span><span>- c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>;
</span><span>
</span><span>                    </span><span>if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>format </span><span>== Branch) {
</span><span>                        inst &amp;= </span><span>0b00000001111111111111000001111111</span><span>;
</span><span>                        inst |= ((offset &gt;&gt; </span><span>11</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>7</span><span>;      </span><span>// Add imm[11]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>1</span><span>) &amp; </span><span>0b1111</span><span>) &lt;&lt; </span><span>8</span><span>;    </span><span>// Add imm[4:1]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>5</span><span>) &amp; </span><span>0b111111</span><span>) &lt;&lt; </span><span>25</span><span>; </span><span>// Add imm[10:5]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>12</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>31</span><span>;     </span><span>// Add imm[12]
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>format </span><span>== J) {
</span><span>                        inst &amp;= </span><span>0b00000000000000000000111111111111</span><span>;
</span><span>                        inst |= ((offset &gt;&gt; </span><span>12</span><span>) &amp; </span><span>0b11111111</span><span>) &lt;&lt; </span><span>12</span><span>;  </span><span>// Add imm[19:12]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>11</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>20</span><span>;         </span><span>// Add imm[11]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>1</span><span>) &amp; </span><span>0b1111111111</span><span>) &lt;&lt; </span><span>21</span><span>; </span><span>// Add imm[10:1]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>20</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>31</span><span>;         </span><span>// Add imm[20]
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>format </span><span>== CJ) {
</span><span>                        inst &amp;= </span><span>0b1110000000000011</span><span>;
</span><span>                        inst |= ((offset &gt;&gt; </span><span>5</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>2</span><span>;   </span><span>// Add offset[5]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>1</span><span>) &amp; </span><span>0b111</span><span>) &lt;&lt; </span><span>3</span><span>; </span><span>// Add offset[3:1]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>7</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>6</span><span>;   </span><span>// Add offset[7]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>6</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>7</span><span>;   </span><span>// Add offset[6]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>10</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>8</span><span>;  </span><span>// Add offset[10]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>8</span><span>) &amp; </span><span>0b11</span><span>) &lt;&lt; </span><span>9</span><span>;  </span><span>// Add offset[9:8]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>4</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>11</span><span>;  </span><span>// Add offset[4]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>11</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>12</span><span>; </span><span>// Add offset[11]
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>format </span><span>== CB) {
</span><span>                        inst &amp;= </span><span>0b1110001110000011</span><span>;
</span><span>                        inst |= ((offset &gt;&gt; </span><span>5</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>2</span><span>;   </span><span>// Add offset[5]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>1</span><span>) &amp; </span><span>0b11</span><span>) &lt;&lt; </span><span>3</span><span>;  </span><span>// Add offset[2:1]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>6</span><span>) &amp; </span><span>0b11</span><span>) &lt;&lt; </span><span>5</span><span>;  </span><span>// Add offset[7:6]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>3</span><span>) &amp; </span><span>0b11</span><span>) &lt;&lt; </span><span>10</span><span>; </span><span>// Add offset[4:3]
</span><span>                        inst |= ((offset &gt;&gt; </span><span>8</span><span>) &amp; </span><span>0b1</span><span>) &lt;&lt; </span><span>12</span><span>;  </span><span>// Add offset[8]
</span><span>                    }
</span><span>
</span><span>                    </span><span>if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>i_bytes </span><span>== </span><span>2</span><span>) {
</span><span>                        reinterpret_cast&lt;uint16_t&amp;&gt;(c.</span><span>machine_code</span><span>.</span><span>data</span><span>()[c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>]) = inst;
</span><span>                    } </span><span>else if </span><span>(c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>i_bytes </span><span>== </span><span>4</span><span>) {
</span><span>                        reinterpret_cast&lt;uint32_t&amp;&gt;(c.</span><span>machine_code</span><span>.</span><span>data</span><span>()[c.</span><span>label_locs</span><span>.</span><span>at</span><span>(i).</span><span>loc</span><span>]) = inst;
</span><span>                    }
</span><span>                }
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>When we find a jump instruction that needs later TLC, we save its location and some other attributes to a special array. Then, after the rest of the code is done assembling, we go back through each jump instruction and calculate the correct offset and insert that offset in-place in the correct instruction format.</p>
<p>I believe this is faster than what some other assemblers do for instructions which jump to a location reachable within the constraints of the offset&#39;s size. However, it&#39;s not useful for far jumps, which require a separate helper instruction to extend the jump. Ultrassembler doesn&#39;t support those yet.</p>

<p>Here&#39;s a few more optimization tricks that aren&#39;t quite significant enough for their own sections but deserve a mention anyway.</p>
<h2 id="memory-padding">Memory padding</h2>
<p>There are a few strings which Ultrassembler frequently reads and writes. To insure against runtime memory pool allocation overhead, we preemptively allocate a good amount of memory.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>c.</span><span>inst</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg1</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg2</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg3</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg4</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg5</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg6</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>arg_extra</span><span>.</span><span>reserve</span><span>(</span><span>32</span><span>);
</span><span>c.</span><span>machine_code</span><span>.</span><span>reserve</span><span>(</span><span>128000</span><span>);
</span></code></pre>
<p>I found that 32 bytes gave the biggest speedup for small strings, and sizes above a few kB are more appropriate for the machine code output.</p>
<h2 id="inline-some-functions">Inline some functions</h2>
<p>Sometimes, functions are faster when you mark them <code>inline</code> to suggest that the code have a copy for each invocation. This tends to work better for smaller functions.</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>inline const </span><span>uint8_t </span><span>decode_encoding_length</span><span>(</span><span>const </span><span>uint8_t </span><span>opcode</span><span>) {
</span><span>    </span><span>if </span><span>((opcode &amp; </span><span>0b11</span><span>) != </span><span>0b11</span><span>) {
</span><span>        </span><span>return </span><span>2</span><span>;
</span><span>    } </span><span>else </span><span>{
</span><span>        </span><span>return </span><span>4</span><span>;
</span><span>    }
</span><span>}
</span></code></pre>
<p>Try it and see what works best for your own code.</p>
<h2 id="minimize-string-stripping-copies">Minimize string stripping copies</h2>
<p>Here&#39;s a special case of minimizing string copying. This function removes the parentheses and optionally the number 0 from a string like &#34;0(t4)&#34;:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void </span><span>remove_extraneous_parentheses</span><span>(ultrastring&amp; </span><span>str</span><span>) {
</span><span>    </span><span>if </span><span>(str.</span><span>back</span><span>() == &#39;</span><span>)</span><span>&#39;) {
</span><span>        str.</span><span>pop_back</span><span>();
</span><span>    }
</span><span>    </span><span>if </span><span>(str.</span><span>front</span><span>() == &#39;</span><span>0</span><span>&#39;) {
</span><span>        str.</span><span>erase</span><span>(</span><span>0</span><span>, </span><span>1</span><span>);
</span><span>    }
</span><span>    </span><span>if </span><span>(str.</span><span>front</span><span>() == &#39;</span><span>(</span><span>&#39;) {
</span><span>        str.</span><span>erase</span><span>(</span><span>0</span><span>, </span><span>1</span><span>);
</span><span>    }
</span><span>}
</span></code></pre>
<p>Why do we tackle the last character first? When you erase one or more characters from a string, C++ internally copies every individual character after setting the characters to erase to blank. In other words, it looks a little like this:</p>
<pre><code><span># Erase &#34;foo&#34; from &#34;foobar&#34;
</span><span>
</span><span>foobar
</span><span>
</span><span> oobar
</span><span>
</span><span>  obar
</span><span>
</span><span>   bar
</span><span>
</span><span>b  bar
</span><span>
</span><span>ba bar
</span><span>
</span><span>barbar
</span><span>
</span><span>barba
</span><span>
</span><span>barb
</span><span>
</span><span>bar
</span></code></pre>
<p>That&#39;s a lot of copies. So it would be great if we can avoid copying more of these characters in the future. Then, we handle the case where the input string is like &#34;(t4)&#34; where there is no 0 at the beginning. Finally is the removal of the front parenthesis. </p>
<p>This optimization yielded a surprising speedup (several percent overall) due to how often the case of &#34;0(reg)&#34; shows up in RISC-V assembly.</p>
<h2 id="call-small-lambda-functions-frequently">Call small lambda functions frequently</h2>
<p>These three lambda functions both help make parsing faster and simplify the code:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>auto</span><span> is_whitespace = [](</span><span>const char</span><span>&amp; c) {
</span><span>    </span><span>return</span><span> c == &#39;</span><span>\t</span><span>&#39; || c == &#39; &#39;;
</span><span>};
</span><span>auto</span><span> ch = [&amp;]() {
</span><span>    </span><span>return</span><span> data[i];
</span><span>};
</span><span>auto</span><span> not_at_end = [](</span><span>const char</span><span>&amp; c) {
</span><span>    </span><span>return</span><span> c != &#39;</span><span>\n</span><span>&#39; &amp;&amp; c != &#39;</span><span>#</span><span>&#39;;
</span><span>};
</span></code></pre>
<p>Why do they work? The simplification part is obvious, but maybe not for speed. One reason might be because the compiler now knows how often we do the same comparisons over and over. If it knows we do the same thing many times, it can optimize with that known fact.</p>
<p>Also note how the first and last functions violate the earlier optimization trick regarding passing fundamental types by value. That trick does not entirely apply to lambda functions, which work differently, where they could be inline and incur zero function call overhead. Passing by reference enables the zero function call overhead optimization.</p>
<h2 id="strip-out-the-compilation-junk">Strip out the compilation junk</h2>
<p>By default, C++ compilers like GCC and Clang add in a lot of junk that we can safely strip out. Here&#39;s how we do it in CMake:</p>
<pre data-lang="cmake"><code data-lang="cmake"><span>target_compile_options</span><span>(objultra </span><span>PRIVATE </span><span>-fno-rtti -fno-stack-protector -fomit-frame-pointer)
</span></code></pre>
<h3 id="fno-rtti">-fno-rtti</h3>
<p>RTTI is runtime type identification. Only some software uses this feature but it adds nonzero overhead to all. Therefore, we disable it to eliminate that overhead.</p>
<h3 id="fno-stack-protector">-fno-stack-protector</h3>
<p>The stack protector is a feature that many Programming Furus©️®️™️ peddle to improve security. However, it adds considerable overhead, and does nothing for security outside of a specific attack. Therefore, we disable it to eliminate that overhead.</p>
<h3 id="fomit-frame-pointer">-fomit-frame-pointer</h3>
<p>The frame pointer is a specific feature on some CPU platforms (like x86). However, it&#39;s not actually needed anymore for modern CPUs, and it adds overhead. Therefore, we disable it to eliminate that overhead.</p>
<h2 id="link-time-optimization">Link-time optimization</h2>
<p>Link-time optimization, or LTO, is a more intelligent way for the compiler to optimize your code than regular optimization passes. It can enable some serious speedups if your code benefits from function inlining or has code across many files. It&#39;s been supported for a while now but isn&#39;t enabled by default. Here&#39;s how to enable it in CMake:</p>
<pre data-lang="cmake"><code data-lang="cmake"><span>include</span><span>(CheckIPOSupported)
</span><span>check_ipo_supported</span><span>(</span><span>RESULT </span><span>lto_supported)
</span><span>if</span><span>(lto_supported AND NOT NO_LTO)
</span><span>  </span><span>set_property</span><span>(</span><span>TARGET </span><span>${</span><span>this_target</span><span>} </span><span>PROPERTY </span><span>INTERPROCEDURAL_OPTIMIZATION TRUE)
</span><span>  </span><span>if</span><span>(CMAKE_COMPILER_IS_GNUCXX)
</span><span>    </span><span>list</span><span>(</span><span>APPEND </span><span>CMAKE_CXX_COMPILE_OPTIONS_IPO &#34;</span><span>-flto=auto</span><span>&#34;) </span><span># set the thread amount to what is available on the CPU
</span><span>  </span><span>endif</span><span>()
</span><span>endif</span><span>()
</span></code></pre>
<p>This has been nothing but a benefit for Ultrassembler.</p>
<h2 id="make-structs-memory-friendly">Make structs memory-friendly</h2>
<p>This struct holds variables that most of the Ultrassembler code uses:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>struct </span><span>assembly_context {
</span><span>    ultrastring inst;
</span><span>    ultrastring arg1;
</span><span>    ultrastring arg2;
</span><span>    ultrastring arg3;
</span><span>    ultrastring arg4;
</span><span>    ultrastring arg5;
</span><span>    ultrastring arg6;
</span><span>    ultrastring arg_extra;
</span><span>    ultravector&lt;uint8_t&gt; machine_code;
</span><span>    ultravector&lt;RVInstructionSet&gt; supported_sets;
</span><span>    ultravector&lt;std::pair&lt;ultrastring, </span><span>int</span><span>&gt;&gt; labels;
</span><span>    ultravector&lt;label_loc&gt; label_locs;
</span><span>    ultravector&lt;std::pair&lt;ultrastring, ultrastring&gt;&gt; constants;
</span><span>    ultravector&lt;directive_options&gt; options;
</span><span>    int32_t custom_inst = </span><span>0</span><span>;
</span><span>    uint32_t line = </span><span>1</span><span>;
</span><span>    uint32_t column = </span><span>0</span><span>;
</span><span>    uint16_t inst_offset = </span><span>0</span><span>;
</span><span>};
</span></code></pre>
<p>We order them in descending memory size, from 32 bytes for <code>ultrastring</code> to 2 for <code>uint16_t</code>. This packs the members the most efficient way possible for memory usage.</p>
<p>Also, these variables are not in the global scope or a namespace because holding them all in a struct enables multithreaded operation. It would be possible to add <code>thread_local</code> to each one to enable multithreading easily, but in testing, this added enormous overhead compared to a plain old struct.</p>
<h2 id="memory-locality">Memory locality</h2>
<p>Memory locality is the general idea that the most frequently accessed memory should be close together. Ultrassembler has many such cases, and we already help ensure memory locality through preallocated memory pools. We go further by ensuring sections of code which frequently work on one area of memory get their own space to work with.</p>
<p>Here&#39;s an example:</p>
<pre data-lang="cpp"><code data-lang="cpp"><span>void </span><span>make_inst</span><span>(assembly_context&amp; </span><span>c</span><span>) {
</span><span>    </span><span>// boilerplate
</span><span>
</span><span>    uint32_t inst = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>// code which modifies this inst variable
</span><span>
</span><span>    reinterpret_cast&lt;uint32_t&amp;&gt;(c.</span><span>machine_code</span><span>[c.</span><span>machine_code</span><span>.</span><span>size</span><span>() - bytes]) = inst;
</span><span>}
</span></code></pre>
<p>We work on the local <code>inst</code> variable to prevent far reaches across memory to the <code>c.machine_code</code> vector. When we&#39;re done, we write to <code>c.machine_code</code> once and invoke only one far memory access as a result.</p>

<p>Congrats if you read all the way here!</p>
<p>Hopefully you&#39;ve learned something new and/or useful. Although I&#39;ve crafted the optimizations here for Ultrassembler, there&#39;s nothing stopping you from applying the same underlying principles to your own code. </p>
<p>Check out Ultrassembler: <a href="https://github.com/Slackadays/Chata/tree/main/ultrassembler">https://github.com/Slackadays/Chata/tree/main/ultrassembler</a></p>



        </div></div>
  </body>
</html>
