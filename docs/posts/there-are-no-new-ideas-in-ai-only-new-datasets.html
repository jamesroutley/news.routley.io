<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only">Original</a>
    <h1>There are no new ideas in AI only new datasets</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><article><div><div><div dir="auto"><p><span>Most people know that AI has made unbelievable progress over the last fifteen years– especially in the last five. It might feel like that progress is </span><em>*inevitable*</em><span> – although large paradigm-shift-level breakthroughs are uncommon, we march on anyway through a stream of slow &amp; steady progress. In fact, some researchers have recently declared a </span><a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/" rel="">“Moore’s Law for AI”</a><span> where the computer’s ability to do certain things (in this case, certain types of coding tasks) increases exponentially with time:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!56cS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!56cS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png 424w, https://substackcdn.com/image/fetch/$s_!56cS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png 848w, https://substackcdn.com/image/fetch/$s_!56cS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png 1272w, https://substackcdn.com/image/fetch/$s_!56cS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!56cS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png" width="720" height="430" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/a6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:430,&#34;width&#34;:720,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;Length of asks AIs can do is doubling every 7 months&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="Length of asks AIs can do is doubling every 7 months" title="Length of asks AIs can do is doubling every 7 months" srcset="https://substackcdn.com/image/fetch/$s_!56cS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png 424w, https://substackcdn.com/image/fetch/$s_!56cS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png 848w, https://substackcdn.com/image/fetch/$s_!56cS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png 1272w, https://substackcdn.com/image/fetch/$s_!56cS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa6c8b571-bdbe-46cc-aa5c-8fd5e5555b01_720x430.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div><div></div></div></div></a><figcaption>the proposed “Moore’s Law for AI”. (by the way, anyone who thinks they can run an autonomous agent for an hour with no intervention as of April 2025 is fooling themselves)</figcaption></figure></div><p>Although I don’t really agree with this specific framing for a number of reasons, I can’t deny the trend of progress. Every year, our AIs get a little bit smarter, a little bit faster, and a little bit cheaper, with no end in sight.</p><p>Most people think that this continuous improvement comes from a steady supply of ideas from the research community across academia – mostly MIT, Stanford, CMU – and industry – mostly Meta, Google, and a handful of Chinese labs, with lots of research done at other places that we’ll never get to learn about.</p><p>And we certainly have made a lot of progress due to research, especially on the systems side of things. This is how we’ve made models cheaper in particular. Let me cherry-pick a few notable examples from the last couple years:</p><p><span>- in 2022 Stanford researchers gave us </span><a href="https://arxiv.org/abs/2205.14135)" rel="">FlashAttention</a><span>, a better way to utilize memory in language models that’s used literally everywhere;</span></p><p><span>- in 2023 Google researchers developed </span><a href="https://arxiv.org/abs/2211.17192" rel="">speculative decoding</a><span>, which all model providers use to speed up inference (also developed at </span><a href="https://arxiv.org/pdf/2302.01318" rel="">DeepMind</a><span>, I believe concurrently?)</span></p><p><span>- in 2024 a ragtag group of internet fanatics developed </span><a href="https://kellerjordan.github.io/posts/muon/" rel="">Muon</a><span>, which seems to be a better optimizer than SGD or Adam and may end up as the way we train language models in the future</span></p><p><span>- in 2025 DeepSeek released </span><a href="https://arxiv.org/abs/2501.12948" rel="">DeepSeek-R1</a><span>, an open-source model that has equivalent reasoning power to similar closed-source models from AI labs (specifically Google and OpenAI)</span></p><p><span>So we’re definitely figuring stuff out. And the reality is actually cooler than that: we’re engaged in a decentralized globalized exercise of Science, where findings are shared openly on </span><a href="https://arxiv.org/" rel="">ArXiv</a><span> and at conferences and on social media and every month we’re getting incrementally smarter.</span></p><p><span>If we’re doing so much important research, why do some argue that progress is slowing down? </span><a href="https://www.lesswrong.com/posts/4mvphwx5pdsZLMmpY/recent-ai-model-progress-feels-mostly-like-bullshit" rel="">People are still complaining</a><span>. The two most recent huge models, </span><a href="https://x.ai/news/grok-3" rel="">Grok 3</a><span> and </span><a href="https://openai.com/index/introducing-gpt-4-5/" rel="">GPT-4.5</a><span>, only obtained a marginal improvement on capabilities of their predecessors. In one particularly salient example, when </span><a href="https://arxiv.org/abs/2503.21934v1" rel="">language models were evaluated on the latest math olympiad exam</a><span>, they scored only 5%, indicating that </span><a href="https://cdn.openai.com/o1-system-card-20241205.pdf" rel="">recent announcements may have been overblown</a><span> when reporting system ability.</span></p><p><span>And if we try to chronicle the </span><em>*big*</em><span> breakthroughs, the real paradigm shifts, they seem to be happening at a different rate. Let me go through a few that come to mind:</span></p><p><span>1. Deep neural networks: Deep neural networks first took off after the </span><a href="https://www.notion.so/There-Are-No-New-Ideas-in-AI-Only-New-Data-1cf5109a45d880e6b0d5d6e3a4ba2fdc?pvs=21" rel="">AlexNet model</a><span> won an image recognition competition in 2012</span></p><p><span>2. Transformers + LLMs: in 2017 Google proposed transformers in </span><a href="https://arxiv.org/abs/1706.03762" rel="">Attention Is All You Need</a><span>, which led to </span><a href="https://arxiv.org/abs/1810.04805" rel="">BERT</a><span> (Google, 2018) and the original </span><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="">GPT</a><span> (OpenAI, 2018)</span></p><p><span>3. RLHF: first proposed (to my knowledge) in the </span><a href="https://arxiv.org/abs/2203.02155" rel="">InstructGPT paper</a><span> from OpenAI in 2022</span></p><p>4. Reasoning: in 2024 OpenAI released O1, which led to DeepSeek R1</p><p>If you squint just a little, these four things (DNNs → Transformer LMs → RLHF → Reasoning) summarize everything that’s happened in AI. We had DNNs (mostly image recognition systems), then we had text classifiers, then we had chatbots, now we have reasoning models (whatever those are).</p><p>Say we want to make a fifth such breakthrough; it could help to study the four cases we have here. What new research ideas led to these groundbreaking events?</p><p><span>It’s not crazy to argue that </span><strong>all the underlying mechanisms of these breakthroughs existed in the 1990s,</strong><span> if not before. We’re applying relatively simple neural network architectures and doing either supervised learning (1 and 2) or reinforcement learning (3 and 4).</span></p><p>Supervised learning via cross-entropy, the main way we pre-train language models, emerged from Claude Shannon’s work in the 1940s.</p><p><span>Reinforcement learning, the main way we post-train language models via RLHF and reasoning training, is slightly newer. It can be traced to the </span><a href="https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" rel="">introduction of policy-gradient methods in 1992</a><span> (and these ideas were certainly around for the first edition of the Sutton &amp; Barto “Reinforcement Learning” textbook in 1998).</span></p><p><span>Ok, let’s agree for now that these “major breakthroughs” were arguably fresh applications of things that we’d known for a while. First of all – this tells us something about the </span><em>*next*</em><span> major breakthrough (that “secret fifth thing” I mentioned above). Our breakthrough is probably not going to come from a completely new idea, rather it’ll be the resurfacing of something we’ve known for a while.</span></p><p><span>But there’s a missing piece here: each of these four breakthroughs </span><strong>enabled us to learn from a new data source:</strong></p><p><span>1. AlexNet and its follow-ups unlocked </span><a href="http://(https://www.image-net.org/" rel="">ImageNet</a><span>, a large database of class-labeled images that drove fifteen years of progress in computer vision</span></p><p><span>2. Transformers unlocked training on “The Internet” and a race to download, categorize, and parse all the text on </span><a href="https://arxiv.org/abs/2101.00027" rel="">The Web</a><span> (which </span><a href="https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications" rel="">it seems</a><span> </span><a href="https://arxiv.org/abs/2305.16264" rel="">we’ve mostly done</a><span> </span><a href="https://arxiv.org/abs/2305.13230" rel="">by now</a><span>)</span></p><p>3. RLHF allowed us to learn from human labels indicating what “good text” is (mostly a vibes thing)</p><p><span>4. Reasoning seems to let us learn from </span><a href="http://incompleteideas.net/IncIdeas/KeytoAI.html" rel="">“verifiers”</a><span>, things like calculators and compilers that can evaluate the outputs of language models</span></p><p>Remind yourself that each of these milestones marks the first time the respective data source (ImageNet, The Web, Humans, Verifiers) was used at scale. Each milestone was followed by a frenzy of activity: researchers compete to (a) siphon up the remaining useful data from any and all available sources and (b) make better use of the data we have through new tricks to make our systems more efficient and less data-hungry. (I expect we’ll see this trend in reasoning models throughout 2025 and 2026 as researchers compete to find, categorize, and verify everything that might be verified.)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!U_q8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!U_q8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg 424w, https://substackcdn.com/image/fetch/$s_!U_q8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg 848w, https://substackcdn.com/image/fetch/$s_!U_q8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!U_q8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!U_q8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg" width="750" height="300" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:300,&#34;width&#34;:750,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;How to train and validate on Imagenet&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="How to train and validate on Imagenet" title="How to train and validate on Imagenet" srcset="https://substackcdn.com/image/fetch/$s_!U_q8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg 424w, https://substackcdn.com/image/fetch/$s_!U_q8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg 848w, https://substackcdn.com/image/fetch/$s_!U_q8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!U_q8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ddd3045-3daa-4adf-8d06-c75b3ac6f436_750x300.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a><figcaption><span>Progress in AI may have been inevitable once we gathered </span><a href="https://www.image-net.org/" rel="">ImageNet</a><span>, at the time the largest public collection of images from the Web</span></figcaption></figure></div><p>There’s something to be said for the fact that our actual technical innovations may not make a huge difference in these cases. Examine the counterfactual. If we hadn’t invented AlexNet, maybe another architecture would have come along that could handle ImageNet. If we never discovered Transformers, perhaps we would’ve settled with LSTMs or SSMs or found something else entirely to learn from the mass of useful training data we have available on the Web.</p><p>This jibes with the theory some people have that nothing matters but data. Some researchers have observed that for all the training techniques, modeling tricks, and hyperparameter tweaks we make, the thing that makes the biggest difference by-and-large is changing the data.</p><p><span>As one salient example, some researchers worked on </span><a href="https://arxiv.org/abs/2212.10544" rel="">developing a new BERT-like model using an architecture other than transformers</a><span>. They spent a year or so tweaking the architecture in hundreds of different ways, and managed to produce a different type of model (this is a state-space model or “SSM”) that performed about equivalently to the original transformer when trained on the same data.</span></p><p><span>This discovered equivalence is really profound because it hints that </span><em>*there is an upper bound to what we might learn from a given dataset*</em><span>. All the training tricks and model upgrades in the world won’t get around the cold hard fact that there is only so much you can learn from a given dataset.</span></p><p><span>And maybe this apathy to new ideas is what we were supposed to take away from </span><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="">The Bitter Lesson</a><span>. If data is the only thing that matters, why are 95% of people working on new methods?</span></p><p>The obvious takeaway is that our next paradigm shift isn’t going to come from an improvement to RL or a fancy new type of neural net. It’s going to come when we unlock a source of data that we haven’t accessed before, or haven’t properly harnessed yet.</p><p><span>One obvious source of information that a lot of people are working towards harnessing is video. According to </span><a href="https://www.dexerto.com/entertainment/how-many-videos-are-there-on-youtube-2197264/" rel="">a random site on the Web</a><span>, about 500 hours of video footage are uploaded to YouTube *per minute*. This is a ridiculous amount of data, much more than is available as text on the entire internet. It’s potentially a much richer source of information too as videos contain not just words but the inflection behind them as well as rich information about physics and culture that just can’t be gleaned from text.</span></p><p>It’s safe to say that as soon as our models get efficient enough, or our computers grow beefy enough, Google is going to start training models on YouTube. They own the thing, after all; it would be silly not to use the data to their advantage.</p><p><span>A final contender for the next “big paradigm” in AI is a data-gathering systems that some way </span><em>embodied</em><span>– or, in the words of a regular person, robots. We’re currently not able to gather and process information from cameras and sensors in a way that’s amenable to training large models on GPUs. If we could build smarter sensors or scale our computers up until they can handle the massive influx of data from a robot with ease, we might be able to use this data in a beneficial way.</span></p><p>It’s hard to say whether YouTube or robots or something else will be the Next Big Thing for AI. We seem pretty deeply entrenched in the camp of language models right now, but we also seem to be running out of language data pretty quickly. But if we want to make progress in AI, maybe we should stop looking for new ideas, and start looking for new data.</p></div></div></div></article></div></div></div><div><div id="discussion"><div><h4>Discussion about this post</h4></div></div></div></div>
  </body>
</html>
