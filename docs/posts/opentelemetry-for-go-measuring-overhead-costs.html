<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://coroot.com/blog/opentelemetry-for-go-measuring-the-overhead/">Original</a>
    <h1>OpenTelemetry for Go: Measuring overhead costs</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="text"><p>Everything comes at a cost — and observability is no exception. When we add metrics, logging, or distributed tracing to our applications, it helps us understand what’s going on with performance and key UX metrics like success rate and latency. But what’s the cost?</p>
<p>I’m not talking about the price of observability tools here, I mean the instrumentation overhead. If an application logs or traces everything it does, that’s bound to slow it down or at least increase resource consumption. Of course, that doesn’t mean we should give up on observability. But it does mean we should measure the overhead so we can make informed tradeoffs.</p>
<p>These days, when people talk about instrumenting applications, in 99% of cases they mean OpenTelemetry. OpenTelemetry is a vendor-neutral open source framework for collecting telemetry data from your app such as metrics, logs, and traces. It’s quickly become the industry standard.</p>
<p>In this post, I want to measure the overhead of using OpenTelemetry in a Go application. To do that, I’ll use a super simple Go HTTP server that increments a counter in an in-memory database <a href="https://valkey.io/">Valkey</a> (a Redis fork) on every request. The idea behind the benchmark is straightforward:</p>
<ul>
<li>First, we’ll run the app under load without any instrumentation and measure its performance and resource usage.</li>
<li>Then, using the exact same workload, we’ll repeat the test with OpenTelemetry SDK for Go enabled and compare the results.</li>
</ul>
<h2>Test setup</h2>
<p>For this benchmark, I’ll use four Linux nodes, each with 4 vCPUs and 8GB of RAM. One will run the application, another will host Valkey, a third will be used for the load generator, and the fourth for observability (using Coroot Community Edition).</p>
<p><img fetchpriority="high" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-12-at-18.20.16.png" alt="" width="3164" height="220" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-12-at-18.20.16.png 3164w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-12-at-18.20.16-300x21.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-12-at-18.20.16-1024x71.png 1024w" sizes="(max-width: 3164px) 100vw, 3164px"/></p>
<p>I want to make sure the components involved in the test don’t interfere with each other, so I’m running them on separate nodes. This time, I’m not using Kubernetes, instead, I’ll run everything in plain Docker containers. I’m also using the host network mode for all containers, to avoid <em>docker-proxy</em> introducing any additional latency into the network path.</p>
<p>Now, let’s take a look at the application code:</p>
<pre><code>package main

import (
	&#34;context&#34;
	&#34;log&#34;
	&#34;net/http&#34;
	&#34;os&#34;
	&#34;strconv&#34;

	&#34;github.com/go-redis/redis/extra/redisotel&#34;
	&#34;github.com/go-redis/redis/v8&#34;

	&#34;go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp&#34;
	&#34;go.opentelemetry.io/otel&#34;
	&#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace&#34;
	&#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp&#34;
	&#34;go.opentelemetry.io/otel/propagation&#34;
	&#34;go.opentelemetry.io/otel/sdk/trace&#34;
)

var (
	rdb *redis.Client
)

func initTracing() {
	rdb.AddHook(redisotel.TracingHook{})
	client := otlptracehttp.NewClient()
	exporter, err := otlptrace.New(context.Background(), client)
	if err != nil {
		log.Fatal(err)
	}
	tracerProvider := trace.NewTracerProvider(trace.WithBatcher(exporter))
	otel.SetTracerProvider(tracerProvider)
	otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(
		propagation.TraceContext{},
		propagation.Baggage{},
	))
}

func handler(w http.ResponseWriter, r *http.Request) {
	cmd := rdb.Incr(r.Context(), &#34;counter&#34;)
	if err := cmd.Err(); err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
		return
	}
	_, _ = w.Write([]byte(strconv.FormatInt(cmd.Val(), 10)))
}

func main() {
	rdb = redis.NewClient(&amp;redis.Options{Addr: os.Getenv(&#34;REDIS_SERVER&#34;)})
	h := http.Handler(http.HandlerFunc(handler))
	if os.Getenv(&#34;ENABLE_OTEL&#34;) != &#34;&#34; {
		log.Println(&#34;enabling opentelemetry&#34;)
		initTracing()
		h = otelhttp.NewHandler(http.HandlerFunc(handler), &#34;GET /&#34;)
	}
	log.Fatal(http.ListenAndServe(&#34;:8080&#34;, h))
}
</code></pre>
<p>By default, the application runs without instrumentation. Only if the environment variable ENABLE_OTEL is set, the OpenTelemetry SDK will be initialized. So runs without this variable will serve as the baseline for comparison.</p>
<h2>Running the Benchmark</h2>
<p>Now let’s start all the components and begin testing.</p>
<p>First, we launch Valkey using the following command:</p>
<pre><code>docker run --name valkey -d --net=host valkey/valkey</code></pre>
<p>Next, we start the Go app and point it to the Valkey instance by IP:</p>
<pre><code>docker run -d --name app -e REDIS_SERVER=&#34;192.168.1.2:6379&#34; --net=host failurepedia/redis-app:0.5</code></pre>
<p>To generate load, I’ll use <a href="https://github.com/giltene/wrk2">wrk2</a>, which allows precise control over request rate. In this test, I’m setting it to 10,000 requests per second using 100 connections and 8 threads. Each run will last 20 minutes:</p>
<pre><code>docker run --rm --name load-generator -ti cylab/wrk2 \
   -t8 -c100 -d1200s -R10000 --u_latency http://192.168.1.3:8080/
</code></pre>
<h2>Results</h2>
<p>Let’s take a look at the results.</p>
<p>We started by running the app without any instrumentation. This serves as our baseline for performance and resource usage. Based on metrics gathered by Coroot using eBPF, the app successfully handled 10,000 requests per second. The majority of requests were served in under 5 milliseconds. The 95th percentile (p95) latency was around 5ms, the 99th percentile (p99) was about 10ms, with occasional spikes reaching up to 20ms.</p>
<p><img decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.24.29.png" alt="" width="1586" height="922" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.24.29.png 1586w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.24.29-300x174.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.24.29-1024x595.png 1024w" sizes="(max-width: 1586px) 100vw, 1586px"/></p>
<p>CPU usage was steady at around 2 CPU cores (or 2 CPU seconds per second), and memory consumption stayed low at roughly 10 MB.</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.09.png" alt="" width="1544" height="416" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.09.png 1544w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.09-300x81.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.09-1024x276.png 1024w" sizes="auto, (max-width: 1544px) 100vw, 1544px"/></p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.22.png" alt="" width="1544" height="404" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.22.png 1544w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.22-300x78.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.37.22-1024x268.png 1024w" sizes="auto, (max-width: 1544px) 100vw, 1544px"/></p>
<p>So that’s our baseline. Now, let’s restart the app container with the OpenTelemetry SDK enabled and see how things change:</p>
<pre><code>docker run -d --name app \
  -e REDIS_SERVER=&#34;192.168.1.2:6379&#34; \
  -e ENABLE_OTEL=1 \
  -e OTEL_SERVICE_NAME=&#34;app&#34; \
  -e OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=&#34;http://192.168.1.4:8080/v1/traces&#34; \
  --net=host failurepedia/redis-app:0.5</code></pre>
<p data-start="1069" data-end="1166">Everything else stayed the same – the infrastructure, the workload, and the duration of the test.</p>
<p data-start="1168" data-end="1202">Now let’s break down what changed.</p>
<p data-start="1168" data-end="1202"><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.29.16.png" alt="" width="1584" height="428" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.29.16.png 1584w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.29.16-300x81.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.29.16-1024x277.png 1024w" sizes="auto, (max-width: 1584px) 100vw, 1584px"/></p>
<p>Memory usage increased from around 10 megabytes to somewhere between 15 and 18 megabytes. This additional overhead comes from the SDK and its background processes for handling telemetry data. While there is a clear difference, it doesn’t look like a significant increase in absolute terms, especially for modern applications where memory budgets are typically much larger.</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.28.59.png" alt="" width="1560" height="438" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.28.59.png 1560w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.28.59-300x84.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.28.59-1024x288.png 1024w" sizes="auto, (max-width: 1560px) 100vw, 1560px"/></p>
<p>CPU usage jumped from 2 cores to roughly 2.7 cores. That’s about a 35 percent increase. This is expected since the app is now tracing every request, preparing and exporting spans, and doing more work in the background.</p>
<p>To understand exactly where this additional CPU usage was coming from, I used Coroot’s built-in eBPF-based CPU profiler to capture and compare profiles before and after enabling OpenTelemetry.</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.53.14.png" alt="" width="3156" height="1680" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.53.14.png 3156w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.53.14-300x160.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.53.14-1024x545.png 1024w" sizes="auto, (max-width: 3156px) 100vw, 3156px"/></p>
<p>The profiler showed that about 10 percent of total CPU time was spent in <em>go.opentelemetry.io/otel/sdk/trace.NewBatchSpanProcessor</em>, which handles span batching and export. Redis calls also got slightly more expensive — tracing added around 7 percent CPU overhead to go-redis operations. The rest of the increase came from instrumented HTTP handlers and middleware.</p>
<p>In short, the overhead comes from OpenTelemetry’s span processing pipeline, not from the app’s core logic.</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.41.57.png" alt="" width="1552" height="914" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.41.57.png 1552w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.41.57-300x177.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-12.41.57-1024x603.png 1024w" sizes="auto, (max-width: 1552px) 100vw, 1552px"/></p>
<p data-start="1376" data-end="1510">Latency also changed, though not dramatically. With OpenTelemetry enabled, more requests fell into the 5 to 10 millisecond range. The 99th percentile latency went from 10 to about 15 milliseconds. Throughput remained stable at around 10,000 requests per second. We didn’t see any errors or timeouts.</p>
<p data-start="1376" data-end="1510"><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.45.49.png" alt="" width="1546" height="410" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.45.49.png 1546w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.45.49-300x80.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-14.45.49-1024x272.png 1024w" sizes="auto, (max-width: 1546px) 100vw, 1546px"/></p>
<p data-start="1376" data-end="1510">Network traffic also increased. With tracing enabled, the app started exporting telemetry data to Coroot, which resulted in an outbound traffic volume of about 4 megabytes per second, or roughly 32 megabits per second. For high-throughput services or environments with strict network constraints, this is something to keep in mind when enabling full request-level tracing.</p>
<p>Overall, enabling OpenTelemetry introduced a noticeable but controlled overhead. These numbers aren’t negligible, especially at scale — but they’re also not a dealbreaker. For most teams, the visibility gained through distributed tracing and the ability to troubleshoot issues faster will justify the tradeoff.</p>
<h2>eBPF-based instrumentation</h2>
<p>I often hear from engineers, especially in ad tech and other high-throughput environments, that they simply can’t afford the overhead of distributed tracing. At the same time, observability is absolutely critical for them. This is exactly the kind of scenario where eBPF-based instrumentation fits well.</p>
<p>Instead of modifying application code or adding SDKs, an agent can observe application behavior at the kernel level using eBPF. Coroot’s agent supports this approach and is capable of collecting both metrics and traces using eBPF, without requiring any changes to the application itself.</p>
<p>However, in high-load environments like the one used in this benchmark, we generally recommend disabling eBPF-based tracing and working with metrics only. Metrics still allow us to clearly see how services interact with each other, without storing data about every single request. They’re also much more efficient in terms of storage and runtime overhead.</p>
<p>Throughout both runs of our test, Coroot’s agent was running on each node. Here’s what its CPU usage looked like:</p>
<p><img loading="lazy" decoding="async" src="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-15.12.03.png" alt="" width="1562" height="448" srcset="https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-15.12.03.png 1562w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-15.12.03-300x86.png 300w, https://coroot.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-15.12.03-1024x294.png 1024w" sizes="auto, (max-width: 1562px) 100vw, 1562px"/></p>
<p data-start="1287" data-end="1572"><em>Node201</em> was running Valkey, <em>node203</em> was running the app, and <em>node204</em> was the load generator. As the chart shows, even under consistent load, the agent’s CPU usage stayed under 0.3 cores. That makes it lightweight enough for production use, especially when working in metrics-only mode.</p>
<p data-start="1574" data-end="1650">This approach offers a practical balance: good visibility with minimal cost.</p>
<h2>Final Thoughts</h2>
<p>Observability comes at a cost, but as this experiment shows, that cost depends heavily on how you choose to implement it.</p>
<p>OpenTelemetry SDKs provide detailed traces and deep visibility, but they also introduce measurable overhead in terms of CPU, memory, and network traffic. For many teams, especially when fast incident resolution is a priority, that tradeoff is entirely justified.</p>
<p>At the same time, eBPF-based instrumentation offers a more lightweight option. It allows you to collect meaningful metrics without modifying application code and keeps resource usage minimal, especially when tracing is disabled and only metrics are collected.</p>
<p>The right choice depends on your goals. If you need full traceability and detailed diagnostics, SDK-based tracing is a strong option. If your priority is low overhead and broad system visibility, eBPF-based metrics might be the better fit.</p>
<p>Observability isn’t free, but with the right approach, it can be both effective and efficient.</p>
</div></div>
  </body>
</html>
