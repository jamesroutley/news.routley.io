<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=38730143">Original</a>
    <h1>Show HN: Emu2 â€“ A Gemini-like open-source 37B Multimodal Model</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>Hello HN,
I&#39;m excited to introduce Emu2, the latest generative multimodal model developed by the Beijing Academy of Artificial Intelligence (BAAI). Emu2 is an open-source initiative that reflects BAAI&#39;s commitment to fostering open, secure, and responsible AI research. It&#39;s designed to enhance AI&#39;s proficiency in handling tasks across various modalities with minimal examples and straightforward instructions.</p><p>Emu2 has demonstrated superior performance over other large-scale models like Flamingo-80B in few-shot multimodal understanding tasks. It serves as a versatile base model for developers, providing a flexible platform for crafting specialized multimodal applications.</p><p>Key features of Emu2 include:</p><p>- A more streamlined modeling framework than its predecessor, Emu.</p><p>- A decoder capable of reconstructing images from the encoder&#39;s semantic space.</p><p>- An expansion to 37 billion parameters, boosting both capabilities and generalization.</p><p>BAAI has also released fine-tuned versions, Emu2-Chat for visual understanding and Emu2-Gen for visual generation, which stand as some of the most powerful open-source models available today.</p><p>Here are the resources for those interested in exploring or contributing to Emu2:</p><p>- Project: https://baaivision.github.io/emu2/</p><p>- Model: https://huggingface.co/BAAI/Emu2</p><p>- Code: https://github.com/baaivision/Emu/tree/main/Emu2</p><p>- Demo: https://huggingface.co/spaces/BAAI/Emu2</p><p>- Paper: https://arxiv.org/abs/2312.13286</p><p>We&#39;re eager to see how the HN community engages with Emu2 and we welcome your feedback to help us improve. Let&#39;s collaborate to push the boundaries of multimodal AI!</p></div></div></div>
  </body>
</html>
