<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lucasb.eyer.be/articles/vit_cnn_speed.html">Original</a>
    <h1>The Speed of VITs and CNNs</h1>
    
    <div id="readability-page-1" class="page"><p id="nojs">
    You disabled JavaScript.
    Please enable it for syntax-highlighting, or don&#39;t complain about unlegible code snippets =)
    This page doesn&#39;t contain any tracking/analytics/ad code.
  </p><section id="content">

<h2 id="context">Context</h2>
<p>Computer vision is now powered by two workhorse architectures:
Convolutional Neural Networks (CNN) and Vision Transformers (ViT).
CNNs slide a feature extractor (stack of convolutions) over the image
to get the final, usually lower-resolution, feature map on which the task is performed.
ViTs on the other hand cut the image into patches from the start and perform stacks
of self-attention on all the patches, leading to the final feature map, also of lower resolution.</p>
<p>It is often stated that because of the quadratic self-attention,
ViTs aren&#39;t practical at higher resolution.
As the most prominent example, here is Yann LeCun, Godfather of CNNs, stating the following:</p>
<p><a href="https://x.com/ylecun/status/1796263750485295560"><img src="https://lucasb.eyer.be/articles/vit_cnn_speed/yann.webp" width="75%"/></a></p>
<p>However, I believe this criticism is a misguided knee-jerk reaction and, in practice,
ViTs scale perfectly fine up to at least 1024x1024px², which is enough for the vast
majority of usage scenarios for image encoders.</p>
<p>In this article, I make two points:</p>
<ul>
<li>ViTs scale just fine up to at least 1024x1024px²</li>
<li>For the vast majority of uses, that resolution is more than enough.</li>
</ul>
<h2 id="vits-scale-just-fine-with-resolution">ViTs scale just fine with resolution</h2>
<p>First, I set out to quantify the inference speed of plain ViTs and CNNs on a range of current GPUs.
To give this benchmark as wide an appeal as possible, I stray away from my usual JAX+TPU toolbox
and perform benchmarking using PyTorch on a few common GPUs.
I use models from the de-facto standard vision model repository <a href="https://github.com/huggingface/pytorch-image-models">timm</a>,
and <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#demonstrating-speedups">follow PyTorch best practices in terms of benchmarking and performance</a>
by using <code>torch.compile</code>.
I further sweep over dtype (<code>float32</code>, <code>float16</code>, <code>bfloat16</code>),
attention implementation (<code>sdpa_kernel</code>),
and matmul precision (<code>set_float32_matmul_precision</code>)
and take the best setting among all these for each measurement.
Since I am quite rusty in PyTorch, here is my <a href="https://github.com/lucasb-eyer/cnn_vit_benchmarks">full benchmarking code</a>,
and I&#39;ll be glad to take feedback from experts.
Besides just speed, I also compute <abbr title="FLoating point OPerations">FLOPs</abbr> and measure peak memory usage.
Thanks to <a href="https://runpod.io?ref=ror42ku9">RunPod</a> for providing the compute and making the benchmarking easy.</p>
<p>I benchmarked various devices:

and batch-sizes: <span id="bs"><a id="bs1">1</a> | <a id="bs8">8</a> | <a id="bs32">32</a></span>, so take your pick.
<span>(<label for="vitdet"><abbr title="Vision Transformer for Detection">ViTDet</abbr>, for later</label>)</span>
<br/></p>
<p><img id="result" src="https://lucasb.eyer.be/articles/vit_cnn_speed/bench_bs32_NVIDIA%20GeForce%20RTX%203070.svg" width="100%"/></p>


<p>Now that you&#39;ve browsed these measurements a bit, I hope we get to the same conclusions:</p>
<ul>
<li>ViT&#39;s scale just fine with resolution, at least up to 1024px².</li>
<li>Often times ViT is faster than an equivalent CNN, especially on more modern GPUs.</li>
<li><abbr title="FLoating point OPerations">FLOPs</abbr> != speed, see also my <a href="https://arxiv.org/abs/2110.12894">Efficiency Misnomer</a>
  paper with Mostafa, Yi, Anurag, and Ashish on the topic.</li>
<li>Out of the box<sup id="fnref:1"><a href="#fn:1">1</a></sup>, ViT is more memory-efficient. On the GTX3070, it is <strong>the only</strong> model that can go beyond 512px².</li>
</ul>
<p>But wait, it gets better! We already had all of this in the original ViT paper.
We&#39;ve successfully <a href="https://arxiv.org/abs/1912.11370">scaled ResNets before anyone else</a>,
and were most annoyed by memory. So we (me, specifically) included this figure in the appendix:</p>
<p><img src="https://lucasb.eyer.be/articles/vit_cnn_speed/paper_appendix.webp" width="75%"/></p>

<p>This figure was on TPUv3, i.e. a few generations ago. This blogpost is on various current GPUs.
I think it is safe to say that these are universal take-aways between ViTs and CNNs by now;
they have stood the test of time.</p>
<h2 id="you-dont-need-very-high-resolution">You don&#39;t need very high resolution</h2>
<p>My second argument is that people waste too much of their time focussing on resolution,
aspect ratio, and related things.</p>
<p>My <em>conservative</em> claim is that you can always stretch to a square, and for:</p>
<ul>
<li>natural images, meaning most photos, 224px² is enough;</li>
<li>text in photos, phone screens, diagrams and charts, 448px² is enough;</li>
<li>desktop screens and single-page documents, 896px² is enough.</li>
</ul>
<p>(Yes, you recoginized correctly, these are the <a href="https://arxiv.org/abs/2407.07726">PaliGemma</a> numbers.
That&#39;s no coincidence.)</p>
<p>Higher resolutions exist purely for human consumption: for the aesthetic beauty of very crisp lines, and to avoid eye strain.
However, computer vision models do not suffer from eye strain, and do not care about aesthetic beauty.
At least for now, while AI is not yet sentient.</p>
<p>There are a few very special exceptions, including medical and satellite images or multi-page documents.
I believe these can be split into pieces of any of the above sizes, with <em>maybe</em> a light global feature.
But I am no expert in those.</p>
<p>The most important thing is to always look at your data, the same way your model will see it.
If you can solve your task looking at it, even with effort, then so can your model.
Let&#39;s do this for a few representative images:</p>
<figure id="res_block">
  <!-- Yep, this is abuse of semantics, ul/li for what should be radios, but this was simpler. -->
  <p>Image:
  </p><ul id="dataset">
    <li id="mscoco">Natural
    </li><li id="stvqa">Natural with text
    </li><li id="phone">Smartphone
    </li><li id="chart">Chart
    </li><li id="diagram">Diagram
    </li><li id="lucas_screen">Desktop
    </li><li id="document">Document
  </li></ul>
  <p>Intrinsic resolution:
  </p><ul id="res">
    <li id="r128">128px²
    </li><li id="r224">224px²
    </li><li id="r256">256px²
    </li><li id="r384">384px²
    </li><li id="r448">448px²
    </li><li id="r512">512px²
    </li><li id="r768">768px²
    </li><li id="r896">896px²
    </li><li id="r1024">1024px²
    </li><li id="orig">Original
  </li></ul>
  <p>Resize method:
  </p><ul id="framework">
    <li id="tf">TensorFlow
    </li><li id="tv">TorchVision
  </li></ul>
  <ul id="method_tf">
    <li id="nearest_tf">Nearest
    </li><li id="area_tf">Area
    </li><li id="bilinear_tf">Bilinear
    </li><li id="bilinear_noaa_tf">Bilinear (no aa)
    </li><li id="bicubic_tf">Bicubic
    </li><li id="gaussian_tf">Gaussian
    </li><li id="lanczos3_tf">Lanczos (3px)
    </li><li id="lanczos5_tf">Lanczos (5px)
    </li><li id="mitchellcubic_tf">Mitchell
  </li></ul>
  <ul id="method_tv">
    <li id="nearest_tv">Nearest
    </li><li id="bilinear_tv">Bilinear
    </li><li id="bilinear_noaa_tv">Bilinear (no antialias)
    </li><li id="bicubic_tv">Bicubic
  </li></ul>
  <img id="resimg" src="https://lucasb.eyer.be/articles/vit_cnn_speed/imgs/lucas_screen_256_aa_bilinear_tf.webp" width="100%"/>
  <figcaption id="mscoco_cap">This is MSCOCO validation image ID 136355. Original resolution: 640x427.</figcaption>
  <figcaption id="stvqa_cap">This is ST-VQA (Scene-Text VQA), IIIT-Text subset, image 385. Original resolution: 1600x1195.</figcaption>
  <figcaption id="phone_cap">This is image 55459 from the RICO dataset. Original resolution: 1080x1920.</figcaption>
  <figcaption id="chart_cap">Note that I chose an unusually long chart to exemplify an extreme case of aspect ratio stretching.
    Still, 512px² is enough.</figcaption>
  <figcaption id="diagram_cap">This is image 3337 from the AI2 Diagrams dataset. Original resolution: 1500x968.</figcaption>
  <figcaption id="lucas_screen_cap">This is a screenshot of Lucas&#39; desktop, reading a random paper.
    Lucas really likes tiny fonts and icons to maximize space, so this is an extreme case.
    Original resolution: 3840x2400.</figcaption>
  <figcaption id="document_cap">This is image mtvg_0227_2 from the DocVQA dataset.
    I chose an especially bad document image with very small text, most are significantly more legible.</figcaption>
</figure>



<p>Hopefully this demo convinced you that I am right.</p>
<h3 id="resolution-or-compute">Resolution... or compute?</h3>
<p>One important point that the vast majority of people forget when they talk about resolution,
is that increasing resolution also significantly increases the model&#39;s <em>capacity</em>.
Now, capacity is a fuzzy concept, but it&#39;s generally agreed that it is a weird mixture of
the model&#39;s size, measured in parameters and unaffected by resolution, but also the model&#39;s compute (<abbr title="FLoating point OPerations">FLOPs</abbr>),
which, as we&#39;ve just seen, increases significantly with resolution.</p>
<p>So, while it has been a common trick to increase performance by increasing resolution since
<a href="https://arxiv.org/abs/1906.06423">FixRes</a> and <a href="https://arxiv.org/abs/1912.11370">BiT</a> in 2019,
it took a whole five years for someone (me) to clearly disentangled these two factors in <a href="https://arxiv.org/abs/2407.07726">the 2024 PaliGemma report</a>.
We ran an experiment where we compute performance at 224px² resolution and at 448px² resolution,
but also <em>at 448px² resolution by first resizing the image to 224px² and then back up to 448px²</em>.
This setting uses the compute (<abbr title="FLoating point OPerations">FLOPs</abbr>) of the 448px² setting, but with the raw information content of the 224px² setting,
and thus the improvements this setting has over the 224px² setting are purely due to model capacity.</p>
<p><img src="https://lucasb.eyer.be/articles/vit_cnn_speed/paper_paligemma.webp" width="75%"/></p>

<p>As we can clearly see, a lot (but not all) of the improved performance at 448px² comes from the increased capacity.
For example, the improved ChartQA results can almost entirely be attributed to capacity increase, not resolution increase.</p>
<h2 id="bonus-local-attention">Bonus: Local Attention</h2>
<p>Besides all this, there is a very simple and elegant mechanism to make ViTs for high resolution even faster and more memory efficient: local attention.
In local attention, the image (or feature-map) is split into non-overlapping windows, and a token only attends to other tokens within its window.
Effectively, this means the windows are moved to the batch dimension for the local attention operation.</p>
<div><div>
<p>The <a href="https://arxiv.org/abs/2112.09747"><abbr title="Universal Vision Transformer">UViT</abbr></a> and <a href="https://arxiv.org/abs/2203.16527"><abbr title="Vision Transformer for Detection">ViTDet</abbr></a> papers introduced this idea,
and suggests to use local attention in most layers of a high-resolution ViT, and global attention only in few.
Even better: <abbr title="Vision Transformer for Detection">ViTDet</abbr> suggests to <em>upcycle</em> plain ViTs that were pre-trained at low-resolution (say 224px²)
to high resolution ones by using the pre-training resolution as window size for most layers.
This <abbr title="Vision Transformer for Detection">ViTDet</abbr>-style local attention was then successfully used by the <a href="https://segment-anything.com/">Segment Anything (<abbr title="Segment Anything Model">SAM</abbr>)</a> line of work.</p>
<p>This has negligible impact on the model&#39;s quality while being very simple, elegant, and compatible.
Importantly, I am not aware of an equally simple and effective idea for CNNs.
This, and token-dropping, are examples of beautiful ideas that become possible thanks to ViT&#39;s simplicity, and would be hard and complicated to implement properly with CNNs.</p>
<p>Now, scroll back up to <a href="#vits-scale-just-fine-with-resolution">the benchmark figures</a>,
and check that <span>(<label for="vitdet2"><abbr title="Vision Transformer for Detection">ViTDet</abbr></label>)</span> checkbox that you previously ignored.
Now even at 1024px² the <abbr title="Vision Transformer for Detection">ViTDet</abbr> is faster than the ConvNeXt.</p>
</div><div>
<p id="vitdet_container">
  <svg id="vitdet_svg"></svg>
  <figcaption>ViTDet architecture schematic to visualize local attention. It&#39;s interactive.</figcaption>
</p>
</div></div>
<!--
<span style="text-align: center"><div style="position: relative; width: 90%; left: 5%">
  <img src=vit_cnn_speed/local_attn.webp width=100% />
  <figcaption>Image credit: <a href=https://medium.com/@14prakash/vitdet-the-go-to-architecture-for-image-foundation-models-3f5f44e6ac4a>Prakash Jay</a>.</figcaption>
</div></span>-->

<h2 id="final-thoughts">Final thoughts</h2>
<h3 id="training">Training</h3>
<p>This was all for inference. Doing similar measurements for training code would be interesting too.
In my experience, take-aways are the same regarding speed (with a roughly architecture-independent factor of 3x).
The memory consumption could look different, as we need to keep many buffers alive for backprop.
But in my experience training many of these models, ViTs are also more memory efficent during training.</p>
<h3 id="learning-ability">Learning ability</h3>
<p>Besides speed and scalability, one should also think about what works with which architecture.
Several ideas in recent literature are explicitly said to work with ViTs but not with CNNs:
&#34;<a href="https://arxiv.org/abs/2104.02057">MoCo v3 and SimCLR are more favorable for ViT-B than R50</a>&#34;,
&#34;<a href="https://arxiv.org/abs/2104.14294">This property emerges only when using DINO with ViT architectures, and does not appear with
other existing self-supervised methods nor with a ResNet-50</a>&#34;, and
the patch dropping idea from <a href="https://arxiv.org/abs/2111.06377">Masked AutoEncoders</a> is <em>only</em>
possible with the plain ViT architecture with non-overlapping patches.
For image-text training à la CLIP, both the <a href="https://arxiv.org/abs/2103.00020">original CLIP paper</a>
and my unpublished experiments show a clearly better performance when using a ViT encoder vs other
convolutional encoders, however none of us has a good explanation of why that would be the case.
Notably, two of these four references are from <a href="https://people.csail.mit.edu/kaiming/">Kaiming He</a>,
the inventor of ResNets.</p>
<h3 id="preference">Preference</h3>
<p>At the end of the day, use whatever works best in your scenario and constraints.
Constraints may include things like familiarity or availability of checkpoints.
I am not religious about architectures, ViT happens to fit most of my use cases well.
The only thing I am religious about, is not making unfounded claims, and calling them out when I see them =)</p>
<p><strong>Acknowledgements:</strong> I thank <a href="https://kolesnikov.ch/">Alexander Kolesnikov</a> and <a href="https://sites.google.com/view/xzhai">Xiaohua Zhai</a> for feedback on a draft of this post.</p>
<p>If this has been useful to your research, consider citing it:</p>
<pre><code>@misc{beyer2024vitspeed,
  author = {Beyer, Lucas},
  title = {{On the speed of ViTs and CNNs}},
  year = {2024},
  howpublished = {\url{http://lb.eyer.be/a/vit-cnn-speed.html}},
}
</code></pre>
<!-- Script at the end-->


<h2>Footnotes</h2>

  </section></div>
  </body>
</html>
