<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lemire.me/blog/2023/07/07/packing-a-string-of-digits-into-an-integer-quickly/">Original</a>
    <h1>Packing a string of digits into an integer quickly</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>Suppose that I give you a short string of digits, containing possibly spaces or other characters (e.g., <tt>&#34;20141103 012910&#34;</tt>). We would like to pack the digits into an integer (e.g., <tt>0x20141103012910</tt>) so that the lexicographical order over the string matches the ordering of the integers.</p>
<p>We can use the fact that in ASCII, the digits have byte values 0x30, 0x31 and so forth. Thus as a sequence of bytes, the string <tt>&#34;20141103 012910&#34;</tt> is actually 0x32, 0x30, 0x31… so we must select the least significant 4 bits of each byte, discarding the rest. Intel and AMD processors have a convenient instruction which allows us to select any bits from a word to construct a new word (<tt>pext</tt>).</p>
<p>A problem remains: Intel and AMD processors are little endian, which means that if I load the string in memory, the first byte becomes the least significant, not the most significant. Thankfully, Intel and AMD can handle this byte order during the load process.</p>
<p>In C, the desired function using Intel intrinsics might look like this:</p>
<pre><span>#</span><span>include </span><span>&lt;</span><span>x86intrin.h</span><span>&gt; // Windows: &lt;intrin.h&gt;</span>
<span>#</span><span>include </span><span>&lt;</span><span>string.h</span><span>&gt;</span>

<span>// From &#34;20141103 012910&#34;, we want to get</span>
<span>// 0x20141103012910</span>
uint64_t extract_nibbles<span>(</span><span>const</span> <span>char</span><span>*</span> c<span>)</span> <span>{</span>
  uint64_t part1<span>,</span> part2<span>;</span>
<span>  memcpy</span><span>(</span><span>&amp;</span>part1<span>,</span> c<span>,</span> <span>sizeof</span><span>(</span>uint64_t<span>)</span><span>)</span><span>;</span>
<span>  memcpy</span><span>(</span><span>&amp;</span>part2 <span>,</span> c <span>+</span> <span>7</span><span>,</span> <span>sizeof</span><span>(</span>uint64_t<span>)</span><span>)</span><span>;</span>
  part1 <span>=</span> _bswap64<span>(</span>part1<span>)</span><span>;</span>
  part2 <span>=</span> _bswap64<span>(</span>part2<span>)</span><span>;</span>
  part1 <span>=</span> _pext_u64<span>(</span>part1<span>,</span> <span>0x0f0f0f0f0f0f0f0f</span><span>)</span><span>;</span>
  part2 <span>=</span> _pext_u64<span>(</span>part2<span>,</span> <span>0x0f000f0f0f0f0f0f</span><span>)</span><span>;</span>
<span>  return</span> <span>(</span>part1<span>&lt;</span><span>&lt;</span><span>24</span><span>)</span> <span>|</span> <span>(</span>part2<span>)</span><span>;</span>
<span>}</span>
</pre>
<p>It compiles to relatively few instructions: only 4 non-load instructions. The memcpy calls in my code translate into 64-bit load instructions. The register loading instructions (<tt>movabs</tt>) are nearly free in practice.</p>
<pre>movbe rax<span>,</span> QWORD PTR <span>[</span>rdi<span>]</span>
movbe rdx<span>,</span> QWORD PTR <span>[</span>rdi<span>+</span><span>7</span><span>]</span>
movabs rcx<span>,</span> <span>1085102592571150095</span>
pext rax<span>,</span> rax<span>,</span> rcx
movabs rcx<span>,</span> <span>1080880467920490255</span>
sal rax<span>,</span> <span>24</span>
pext rdx<span>,</span> rdx<span>,</span> rcx
or rax<span>,</span> rdx

</pre>
<p>Prior to the AMD Zen 3 processors, <tt>pext</tt> had terrible performance on AMD processors. Recent AMD processors have performance on par with Intel, meaning that <tt>pext</tt> has a latency of about 3 cycles, and can run once per cycle. So it is about as expensive as a multiplication. Not counting the loads, the above function could nearly be complete in about 5 cycles, which is quite fast.</p>
<p>For ARM processors, you can do it with ARM NEON like so: mask the high nibbles, shuffle the bytes, then shift/or, then narrow (16-&gt;8), extract to general register.</p>
<pre><span>#</span><span>include </span><span>&lt;</span><span>arm_neon.h</span><span>&gt;</span>
<span>// From &#34;20141103 012910&#34;, we want to get</span>
<span>// 0x20141103012910</span>
uint64_t extract_nibbles<span>(</span><span>const</span> <span>char</span> <span>*</span>c<span>)</span> <span>{</span>
<span>  const</span> uint8_t <span>*</span>ascii <span>=</span> <span>(</span><span>const</span> uint8_t <span>*</span><span>)</span><span>(</span>c<span>)</span><span>;</span>
  uint8x16_t in <span>=</span> vld1q_u8<span>(</span>ascii<span>)</span><span>;</span>
<span>  // masking the high nibbles,</span>
  in <span>=</span> vandq_u8<span>(</span>in<span>,</span> vmovq_n_u8<span>(</span><span>0x0f</span><span>)</span><span>)</span><span>;</span>
<span>  // shuffle the bytes</span>
<span>  const</span> uint8x16_t shuf <span>=</span> <span>{</span><span>14</span><span>,</span> <span>13</span><span>,</span> <span>12</span><span>,</span> <span>11</span><span>,</span> <span>10</span><span>,</span> <span>9</span><span>,</span> <span>7</span><span>,</span> <span>6</span><span>,</span>
<span>    5</span><span>,</span> <span>4</span><span>,</span> <span>3</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>255</span><span>,</span> <span>255</span><span>}</span><span>;</span>
  in <span>=</span> vqtbl1q_u8<span>(</span>in<span>,</span> shuf<span>)</span><span>;</span>
<span>  // then shift/or</span>
  uint16x8_t ins <span>=</span>
    vsraq_n_u16<span>(</span>vreinterpretq_u16_u8<span>(</span>in<span>)</span><span>,</span>
    vreinterpretq_u16_u8<span>(</span>in<span>)</span><span>,</span> <span>4</span><span>)</span><span>;</span>
<span>  // then narrow (16-&gt;8),</span>
  int8x8_t packed <span>=</span> vmovn_u16<span>(</span>ins<span>)</span><span>;</span>
<span>  // extract to general register.</span>
<span>  return</span> vget_lane_u64<span>(</span>vreinterpret_u64_u16<span>(</span>packed<span>)</span><span>,</span> <span>0</span><span>)</span><span>;</span>
<span>}</span>

</pre>
<p>It might compile to something like this:</p>
<pre>adrp x8<span>,</span> .LCPI0_<span>0</span>
ldr q1<span>,</span> <span>[</span>x0<span>]</span>
movi v0.16b<span>,</span> #<span>15</span>
ldr q2<span>,</span> <span>[</span>x8<span>,</span> <span>:</span>lo1<span>2</span><span>:</span>.LCPI0_<span>0</span><span>]</span>
<span>and</span> v0.16b<span>,</span> v1.16b<span>,</span> v0.16b
tbl v0.16b<span>,</span> <span>{</span> v0.16b <span>}</span><span>,</span> v2.16b
usra v0.<span>8h</span><span>,</span> v0.<span>8h</span><span>,</span> #<span>4</span>
xtn v0.8b<span>,</span> v0.<span>8h</span>
fmov x0<span>,</span> d0

</pre>
<p><a href="https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/tree/master/2023/07/07">My source code is available</a>.</p>
</div></div>
  </body>
</html>
