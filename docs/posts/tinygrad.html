<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/geohot/tinygrad">Original</a>
    <h1>Tinygrad</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/geohot/tinygrad/master/docs/logo.png"><img src="https://raw.githubusercontent.com/geohot/tinygrad/master/docs/logo.png"/></a>
</p>
<hr/>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/geohot/tinygrad/workflows/Unit%20Tests/badge.svg"><img src="https://github.com/geohot/tinygrad/workflows/Unit%20Tests/badge.svg" alt="Unit Tests"/></a></p>
<p dir="auto">For something in between a <a href="https://github.com/pytorch/pytorch">pytorch</a> and a <a href="https://github.com/karpathy/micrograd">karpathy/micrograd</a></p>
<p dir="auto">This may not be the best deep learning framework, but it is a deep learning framework.</p>
<p dir="auto">The sub 1000 line core of it is in <code>tinygrad/</code></p>
<p dir="auto">Due to its extreme simplicity, it aims to be the easiest framework to add new accelerators to, with support for both inference and training. Support the simple basic ops, and you get SOTA <a href="https://arxiv.org/abs/1905.11946" rel="nofollow">vision</a> <code>models/efficientnet.py</code> and <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">language</a> <code>models/transformer.py</code> models.</p>
<p dir="auto">We are working on support for the Apple Neural Engine and the Google TPU in the <code>accel/</code> folder. Eventually, <a href="https://geohot.github.io/blog/jekyll/update/2021/06/13/a-breakdown-of-ai-chip-companies.html" rel="nofollow">we will build custom hardware</a> for tinygrad, and it will be blindingly fast. Now, it is slow.</p>
<h3 dir="auto"><a id="user-content-installation" aria-hidden="true" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Installation</h3>
<div data-snippet-clipboard-copy-content="pip3 install git+https://github.com/geohot/tinygrad.git --upgrade

# or for development
git clone https://github.com/geohot/tinygrad.git
cd tinygrad
python3 setup.py develop"><pre>pip3 install git+https://github.com/geohot/tinygrad.git --upgrade

<span><span>#</span> or for development</span>
git clone https://github.com/geohot/tinygrad.git
<span>cd</span> tinygrad
python3 setup.py develop</pre></div>
<h3 dir="auto"><a id="user-content-example" aria-hidden="true" href="#example"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Example</h3>
<div data-snippet-clipboard-copy-content="from tinygrad.tensor import Tensor

x = Tensor.eye(3)
y = Tensor([[2.0,0,-2.0]])
z = y.matmul(x).sum()
z.backward()

print(x.grad)  # dz/dx
print(y.grad)  # dz/dy"><pre><span>from</span> <span>tinygrad</span>.<span>tensor</span> <span>import</span> <span>Tensor</span>

<span>x</span> <span>=</span> <span>Tensor</span>.<span>eye</span>(<span>3</span>)
<span>y</span> <span>=</span> <span>Tensor</span>([[<span>2.0</span>,<span>0</span>,<span>-</span><span>2.0</span>]])
<span>z</span> <span>=</span> <span>y</span>.<span>matmul</span>(<span>x</span>).<span>sum</span>()
<span>z</span>.<span>backward</span>()

<span>print</span>(<span>x</span>.<span>grad</span>)  <span># dz/dx</span>
<span>print</span>(<span>y</span>.<span>grad</span>)  <span># dz/dy</span></pre></div>
<h3 dir="auto"><a id="user-content-same-example-in-torch" aria-hidden="true" href="#same-example-in-torch"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Same example in torch</h3>
<div data-snippet-clipboard-copy-content="import torch

x = torch.eye(3, requires_grad=True)
y = torch.tensor([[2.0,0,-2.0]], requires_grad=True)
z = y.matmul(x).sum()
z.backward()

print(x.grad)  # dz/dx
print(y.grad)  # dz/dy"><pre><span>import</span> <span>torch</span>

<span>x</span> <span>=</span> <span>torch</span>.<span>eye</span>(<span>3</span>, <span>requires_grad</span><span>=</span><span>True</span>)
<span>y</span> <span>=</span> <span>torch</span>.<span>tensor</span>([[<span>2.0</span>,<span>0</span>,<span>-</span><span>2.0</span>]], <span>requires_grad</span><span>=</span><span>True</span>)
<span>z</span> <span>=</span> <span>y</span>.<span>matmul</span>(<span>x</span>).<span>sum</span>()
<span>z</span>.<span>backward</span>()

<span>print</span>(<span>x</span>.<span>grad</span>)  <span># dz/dx</span>
<span>print</span>(<span>y</span>.<span>grad</span>)  <span># dz/dy</span></pre></div>
<h2 dir="auto"><a id="user-content-neural-networks" aria-hidden="true" href="#neural-networks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Neural networks?</h2>
<p dir="auto">It turns out, a decent autograd tensor library is 90% of what you need for neural networks. Add an optimizer (SGD, RMSprop, and Adam implemented) from tinygrad.optim, write some boilerplate minibatching code, and you have all you need.</p>
<h3 dir="auto"><a id="user-content-neural-network-example-from-testtest_mnistpy" aria-hidden="true" href="#neural-network-example-from-testtest_mnistpy"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Neural network example (from test/test_mnist.py)</h3>
<div data-snippet-clipboard-copy-content="from tinygrad.tensor import Tensor
import tinygrad.optim as optim

class TinyBobNet:
  def __init__(self):
    self.l1 = Tensor.uniform(784, 128)
    self.l2 = Tensor.uniform(128, 10)

  def forward(self, x):
    return x.dot(self.l1).relu().dot(self.l2).logsoftmax()

model = TinyBobNet()
optim = optim.SGD([model.l1, model.l2], lr=0.001)

# ... and complete like pytorch, with (x,y) data

out = model.forward(x)
loss = out.mul(y).mean()
optim.zero_grad()
loss.backward()
optim.step()"><pre><span>from</span> <span>tinygrad</span>.<span>tensor</span> <span>import</span> <span>Tensor</span>
<span>import</span> <span>tinygrad</span>.<span>optim</span> <span>as</span> <span>optim</span>

<span>class</span> <span>TinyBobNet</span>:
  <span>def</span> <span>__init__</span>(<span>self</span>):
    <span>self</span>.<span>l1</span> <span>=</span> <span>Tensor</span>.<span>uniform</span>(<span>784</span>, <span>128</span>)
    <span>self</span>.<span>l2</span> <span>=</span> <span>Tensor</span>.<span>uniform</span>(<span>128</span>, <span>10</span>)

  <span>def</span> <span>forward</span>(<span>self</span>, <span>x</span>):
    <span>return</span> <span>x</span>.<span>dot</span>(<span>self</span>.<span>l1</span>).<span>relu</span>().<span>dot</span>(<span>self</span>.<span>l2</span>).<span>logsoftmax</span>()

<span>model</span> <span>=</span> <span>TinyBobNet</span>()
<span>optim</span> <span>=</span> <span>optim</span>.<span>SGD</span>([<span>model</span>.<span>l1</span>, <span>model</span>.<span>l2</span>], <span>lr</span><span>=</span><span>0.001</span>)

<span># ... and complete like pytorch, with (x,y) data</span>

<span>out</span> <span>=</span> <span>model</span>.<span>forward</span>(<span>x</span>)
<span>loss</span> <span>=</span> <span>out</span>.<span>mul</span>(<span>y</span>).<span>mean</span>()
<span>optim</span>.<span>zero_grad</span>()
<span>loss</span>.<span>backward</span>()
<span>optim</span>.<span>step</span>()</pre></div>
<h2 dir="auto"><a id="user-content-gpu-and-accelerator-support" aria-hidden="true" href="#gpu-and-accelerator-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>GPU and Accelerator Support</h2>
<p dir="auto">tinygrad supports GPUs through PyOpenCL.</p>
<div data-snippet-clipboard-copy-content="from tinygrad.tensor import Tensor
(Tensor.ones(4,4).gpu() + Tensor.ones(4,4).gpu()).cpu()"><pre><span>from</span> <span>tinygrad</span>.<span>tensor</span> <span>import</span> <span>Tensor</span>
(<span>Tensor</span>.<span>ones</span>(<span>4</span>,<span>4</span>).<span>gpu</span>() <span>+</span> <span>Tensor</span>.<span>ones</span>(<span>4</span>,<span>4</span>).<span>gpu</span>()).<span>cpu</span>()</pre></div>
<h3 dir="auto"><a id="user-content-ane-support-broken" aria-hidden="true" href="#ane-support-broken"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ANE Support?! (broken)</h3>
<p dir="auto">If all you want to do is ReLU, you are in luck! You can do very fast ReLU (at least 30 MEGAReLUs/sec confirmed)</p>
<p dir="auto">Requires your Python to be signed with <code>ane/lib/sign_python.sh</code> to add the <code>com.apple.ane.iokit-user-access</code> entitlement, which also requires <code>amfi_get_out_of_my_way=0x1</code> in your <code>boot-args</code>. Build the library with <code>ane/lib/build.sh</code></p>
<div data-snippet-clipboard-copy-content="from tinygrad.tensor import Tensor

a = Tensor([-2,-1,0,1,2]).ane()
b = a.relu()
print(b.cpu())"><pre><span>from</span> <span>tinygrad</span>.<span>tensor</span> <span>import</span> <span>Tensor</span>

<span>a</span> <span>=</span> <span>Tensor</span>([<span>-</span><span>2</span>,<span>-</span><span>1</span>,<span>0</span>,<span>1</span>,<span>2</span>]).<span>ane</span>()
<span>b</span> <span>=</span> <span>a</span>.<span>relu</span>()
<span>print</span>(<span>b</span>.<span>cpu</span>())</pre></div>
<p dir="auto">Warning: do not rely on the ANE port. It segfaults sometimes. So if you were doing something important with tinygrad and wanted to use the ANE, you might have a bad time.</p>
<h3 dir="auto"><a id="user-content-adding-an-accelerator" aria-hidden="true" href="#adding-an-accelerator"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Adding an accelerator</h3>
<p dir="auto">You need to support 14 first class ops:</p>
<div data-snippet-clipboard-copy-content="Relu, Log, Exp                  # unary ops
Sum, Max                        # reduce ops (with axis argument)
Add, Sub, Mul, Pow              # binary ops (with broadcasting)
Reshape, Transpose, Slice       # movement ops
Matmul, Conv2D                  # processing ops"><pre><code>Relu, Log, Exp                  # unary ops
Sum, Max                        # reduce ops (with axis argument)
Add, Sub, Mul, Pow              # binary ops (with broadcasting)
Reshape, Transpose, Slice       # movement ops
Matmul, Conv2D                  # processing ops
</code></pre></div>
<p dir="auto">While more ops may be added, I think this base is stable.</p>
<h2 dir="auto"><a id="user-content-imagenet-inference" aria-hidden="true" href="#imagenet-inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>ImageNet inference</h2>
<p dir="auto">Despite being tiny, tinygrad supports the full EfficientNet. Pass in a picture to discover what it is.</p>
<div data-snippet-clipboard-copy-content="ipython3 examples/efficientnet.py https://media.istockphoto.com/photos/hen-picture-id831791190"><pre>ipython3 examples/efficientnet.py https://media.istockphoto.com/photos/hen-picture-id831791190</pre></div>
<p dir="auto">Or, if you have a webcam and cv2 installed</p>
<div data-snippet-clipboard-copy-content="ipython3 examples/efficientnet.py webcam"><pre>ipython3 examples/efficientnet.py webcam</pre></div>
<p dir="auto">PROTIP: Set &#34;GPU=1&#34; environment variable if you want this to go faster.</p>
<p dir="auto">PROPROTIP: Set &#34;DEBUG=1&#34; environment variable if you want to see why it&#39;s slow.</p>
<h3 dir="auto"><a id="user-content-tinygrad-supports-gans" aria-hidden="true" href="#tinygrad-supports-gans"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>tinygrad supports GANs</h3>
<p dir="auto">See <code>examples/mnist_gan.py</code></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/geohot/tinygrad/master/docs/mnist_by_tinygrad.jpg"><img src="https://raw.githubusercontent.com/geohot/tinygrad/master/docs/mnist_by_tinygrad.jpg"/></a>
</p>
<h3 dir="auto"><a id="user-content-tinygrad-supports-yolo" aria-hidden="true" href="#tinygrad-supports-yolo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>tinygrad supports yolo</h3>
<p dir="auto">See <code>examples/yolov3.py</code></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://raw.githubusercontent.com/geohot/tinygrad/master/docs/yolo_by_tinygrad.jpg"><img src="https://raw.githubusercontent.com/geohot/tinygrad/master/docs/yolo_by_tinygrad.jpg"/></a>
</p>
<h2 dir="auto"><a id="user-content-the-promise-of-small" aria-hidden="true" href="#the-promise-of-small"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>The promise of small</h2>
<p dir="auto">tinygrad will always be below 1000 lines. If it isn&#39;t, we will revert commits until tinygrad becomes smaller.</p>
<h3 dir="auto"><a id="user-content-running-tests" aria-hidden="true" href="#running-tests"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Running tests</h3>

</article>
          </div></div>
  </body>
</html>
