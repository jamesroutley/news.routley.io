<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2406.04823">Original</a>
    <h1>BERTs Are Generative In-Context Learners</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://arxiv.org/pdf/2406.04823">View PDF</a>
    <a href="https://arxiv.org/html/2406.04823v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>While in-context learning is commonly associated with causal language models, such as GPT, we demonstrate that this capability also &#39;emerges&#39; in masked language models. Through an embarrassingly simple inference technique, we enable an existing masked model, DeBERTa, to perform generative tasks without additional training or architectural changes. Our evaluation reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks. These complementary strengths suggest that the field&#39;s focus on causal models for in-context learning may be limiting - both architectures can develop these capabilities, but with distinct advantages; pointing toward promising hybrid approaches that combine the strengths of both objectives.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: David Samuel [<a href="https://arxiv.org/show-email/415602fe/2406.04823" rel="nofollow">view email</a>]      </p></div></div>
  </body>
</html>
