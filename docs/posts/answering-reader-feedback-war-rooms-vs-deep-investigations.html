<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rachelbythebay.com/w/2025/02/22/war/">Original</a>
    <h1>Answering reader feedback: war rooms vs. deep investigations</h1>
    
    
<p>
A reader asked me to follow up on something I had mentioned quite a  
while back.  Way back in 2018, I 
<a href="https://rachelbythebay.com/w/2018/12/21/env/">wrote</a>
that maybe I&#39;d write about &#34;war rooms&#34; and why they are bad for getting 
thoughtful analysis work to happen.  
</p>
<p>
Here&#39;s my best take on it, in feedback style, where things are sort of 
off-the-cuff and I don&#39;t have a prepared statement ahead of time.  
Ready?  Here we go.
</p>
<p>
My canonical example for a hellish war room is what happened on Friday, 
August 1, 2014.  For people in the know (like anyone who took my class 
for new technical employees), that&#39;s the date of &#34;Call the Cops&#34; - an 
epic outage that took down all of Facebook for several hours.  It was so 
named internally because the sheriff of LA County tweeted something like 
&#34;we know it&#39;s down, it&#39;s not a law enforcement emergency, so stop 
calling us about it&#34;.
</p>
<p>
I&#39;ve mentioned this any number of times in talks and posts, and other 
people have too.  It&#39;s no secret.  The site tanked bigtime that day.
</p>
<p>
It actually broke while I was waiting for my bus to arrive, so I ended 
up getting online from the sidewalk using my cell phone&#39;s tethering 
function, then rode in doing my very best to follow things while 
bouncing around as the bus shot up 101 to Menlo Park.
</p>
<p>
Then, since it was Friday and we were going to do the usual weekly &#34;big 
show&#34; of reviewing recent outages (SEVs), the Muppet-named room used for 
those events was instead repurposed as a &#34;war room&#34;.  It was a bunch of 
engineer type people thinking very hard and sweating a lot... in a 
relatively small room with not-great ventilation, door open or no.
</p>
<p>
It got smelly.  Okay?  I&#39;m not going to sugar-coat it.
</p>
<p>
There&#39;s more, though.  I&#39;ve said a few times that I&#39;m really not &#34;fully 
functional&#34; when I&#39;m going through the Mac UI.  For that reason, I ran 
VMWare Fusion on my company laptop and had Ubuntu running in a virtual 
machine, and used that for all of my work.
</p>
<p>
Still, actually doing work on the laptop proper (a 13 inch Macbook Air) 
was janky as hell because the key layout isn&#39;t quite right (want to 
alt-tab in Linux? better use option-tab!), and besides, the screen is 
tiny!  It&#39;s a great little machine for browsing the web and reading 
e-mail, but it&#39;s no place to try to crack open a bunch of xterms and do 
Real Work.
</p>
<p>
Could I run my terminals in there?  Yes.  Did I?  Yes, for a while.  Was 
I effective?  Not really.  I missed my desk, my normal chair, my big 
Thunderbolt monitor, my full-size (and yet entirely boring) keyboard, 
and a relatively odor-free environment.
</p>
<p>
Fortunately, by that point in my life I had gotten old enough to where I 
was willing to take steps to bolster my own sanity and effectiveness, 
and said I&#39;d get back on from my desk.  I crossed back to our building, 
posted up at my desk, and proceeded to start cranking on things from 
there.
</p>
<p>
There were any number of things going on, and I hopped around trying to 
be useful.  It took a while to finally settle down on one thing in 
particular: why had the machines effectively dropped off the network?  
Why was sshd dead?  What was going on there?
</p>
<p>
My whole goal was to find out WHY the machines had seemingly nuked 
everything when they ran out of memory during the &#34;push&#34; that morning.  
Was it some pathological kernel &#34;OOM killer&#34; thing?  Was it something 
else running amok and shutting down the wrong jobs?
</p>
<p>
We *had* to know, or we couldn&#39;t be sure it wouldn&#39;t happen again.  Lots 
of other people were hacking away, trying to figure out how to reproduce 
what had happened that morning, and others were simultaneously cranking 
away trying to reduce the memory bloat on the web servers.
</p>
<p>
But, without finding out what the hell had happened that reduced the 
machines to init and this &#34;fbagent&#34; thing, we were underneath this Sword 
of Damocles situation where it could drop on us at any point and take us 
down *yet again*.
</p>
<p>
People figured out that yes, they had run the machines out of memory, 
specifically with the push - the distribution of new bytecode to the web 
servers.  Other people started taking steps to beat back some of the 
bloat that had been creeping in that summer, so the memory situation 
wouldn&#39;t be so bad.  I suspect some others also dialed back the number 
of threads (simultaneous requests) on the smaller web servers to keep 
them from running quite as &#34;hot&#34;.
</p>
<p>
I still had my assignment to root-cause the damn &#34;nuke the world&#34; thing.  
There was no way it was going to happen in that little conference room 
without my usual fine-tuned environment, and that&#39;s why I bailed.
</p>
<p>
It took me a couple of weeks to really make any sense of it.  I mean, 
really, I&#39;m not exaggerating here.  The outage was August 1st, and I 
finally figured out the sequence required to nuke all the processes on 
the box on the afternoon of August 19th.  I know this because there was 
a screenshot floating around where I talked my way through it on IRC as 
it shook itself out... and that made it into one of my public talks.
</p>
<p>
That&#39;s over 18 days of not knowing why, and worrying what could have 
happened pretty much the whole time.
</p>
<p>
In those days, the machines were running Upstart for init (pid 1) so 
the only things that would come back up are things set to &#34;respawn&#34; in 
inittab (remember that?), so we&#39;d get a getty on the console and little 
more.  That&#39;s how I was able to use the out-of-band access to jump in 
and go &#34;yep, machine is up but everything else (including sshd) is down&#34;.
</p>
<p>
[ Side note: If you did that today, I assume that systemd would end up 
restarting most of the things on the box, and you might actually 
recover from it.  I&#39;m not about to try just for a post, though! ]
</p>
<p>
Why did fork fail?  Easy: the box ran out of memory.  But, I had to 
reproduce that to know for sure.  How did I do that?  This took much 
longer, and was after chasing many dead ends based on rumors about 
&#34;kernel OOM killers&#34; and stuff like that.  Were we deadlocking during 
the OOM kill?  There was some scary stuff going on where the hosts would 
get really squirrelly while the messages spewed into the printk ring 
buffer.  That consumed a bunch of time right there, and was also not 
what actually caused it.
</p>
<p>
Finally reproducing it involved shrinking my test system&#39;s swap size 
from what had been multiple gigabytes down to just 64 MB.  Then I also 
ran some &#34;memeater&#34; things I had coded up: they would malloc() some 
space and dirty the pages by writing to them so they actually got 
physical memory handed to them.  Then they just sat and waited around.
After putting enough memory pressure on the box, it finally borked.
</p>
<p>
Even then, I thought it was the task scheduler thing the company had 
written for its own prod environment, because, again, everyone assumed 
it was guilty, and that was the undercurrent.  But no, it wasn&#39;t.  A few 
minutes later, I found the smoking gun: fbagent had logged something 
about &#34;starting kill of child -1&#34; at exactly the time everything died.
</p>
<p>
Years of nerding out on Linux boxes had taught me what killing &#34;pid -1&#34; 
would actually do, and this finally explained why this fbagent thing 
hadn&#39;t died in the slaughter of every process on the box.  It was the 
thing doing the murdering!
</p>
<p>
This fbagent process ran as root, ran a bunch of subprocesses, called 
fork(), didn&#39;t handle a -1 return code, and then later went to kill 
that &#34;wayward child&#34;.  Sending a signal (SIGKILL in this case) to &#34;pid 
-1&#34; on Linux sends it to everything but init and yourself.  If you&#39;re 
root (yep) and not running in some kind of PID namespace (yep to 
that too), that&#39;s pretty much the whole world.
</p>
<p>
Of course, I was looking at the checked-in source code and couldn&#39;t 
figure out just where the hell this -1 was coming from.  The call to 
fork() *did* check for a -1 and handled it as an error and bailed out.  
So how was it somehow surviving all the way down to where kill() 
was called?
</p>
<p>
That was another rathole, and the answer was also a thing to behold: I 
couldn&#39;t see it in the checked-in source code because it had been fixed.  
Some other engineer on a completely unrelated project had tripped over 
it, figured it out, and sent a fix to the team which owned that program.  
They had committed it, so the source code looked fine.
</p>
<p>
[ Another side note: this person who fixed a bug in some code that 
wasn&#39;t their actual &#34;job&#34; was the kind of excellent behavior that used 
to be lionized there - &#34;nothing at FB is someone else&#39;s problem&#34;.  That 
credo died a long time ago. ]
</p>
<p>
Unfortunately, production (many many machines) was running the last 
release which had been cut WELL before that point.  It had the bug in 
it: run machine out of memory, make fork fail, kill the world, all die, 
oh the embarrassment.
</p>
<p>
I can&#39;t imagine doing that kind of multi-window parallel investigation 
stuff on a teeny little laptop screen with people right next to me on 
either side, while the whole room flop-sweats about going out of 
business or whatever if they don&#39;t get it fixed.
</p>
<p>
I guess a &#34;war room&#34; might work out if you have a bunch of stuff that 
has to happen to deal with a possible &#34;crisis&#34; and then it&#39;s just a 
matter of coordinating it.  You don&#39;t have people doing &#34;heads-down 
hack&#34; stuff nearly as much in a case like that.
</p>
<p>
I have actually seen such a gathering work out nicely, and I&#39;ll leave 
that as a tale for yet another time.
</p>

  </body>
</html>
