<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.stephendiehl.com/posts/post_transformers/">Original</a>
    <h1>Attention Wasn&#39;t All We Needed</h1>
    
    <div id="readability-page-1" class="page"><div>
            
<p>There&#39;s a lot of modern techniques that have been developed since the original <em>Attention Is All You Need</em> paper. Let&#39;s look at some of the most important ones that have been developed over the years and try to implement the basic ideas as succinctly as possible. We&#39;ll use the Pytorch framework for most of the examples. Note that most of these examples are highly simplified sketches of the core ideas, if you want the full implementation please read the original paper or the production code in frameworks like PyTorch or Jax.</p>
<ol>
<li><a href="#group-query-attention">Group Query Attention</a></li>
<li><a href="#multi-head-latent-attention">Multi-head Latent Attention</a></li>
<li><a href="#flash-attention">Flash Attention</a></li>
<li><a href="#ring-attention">Ring Attention</a></li>
<li><a href="#pre-normalization">Pre-normalization</a></li>
<li><a href="#rmsnorm">RMSNorm</a></li>
<li><a href="#swiglu">SwiGLU</a></li>
<li><a href="#rotary-positional-embedding">Rotary Positional Embedding</a></li>
<li><a href="#mixture-of-experts">Mixture of Experts</a></li>
<li><a href="#learning-rate-warmup">Learning Rate Warmup</a></li>
<li><a href="#cosine-schedule">Cosine Schedule</a></li>
<li><a href="#adamw-optimizer">AdamW Optimizer</a></li>
<li><a href="#multi-token-prediction">Multi-token Prediction</a></li>
<li><a href="#speculative-decoding">Speculative Decoding</a></li>
</ol>
<h2 id="group-query-attention" tabindex="-1">Group Query Attention</h2>
<p>Ok starting off in no particular order, <strong>Grouped Query Attention</strong> is a technique to reduce the memory usage of the KV cache during inference.  Group Query Attention is an architectural optimization for the standard multi-head attention mechanism. The core idea behind GQA is based on the observation that the computational bottleneck and memory footprint in MHA are heavily influenced by the size of the K and V projections and their corresponding caches. GQA proposes to reduce this cost by sharing a single set of K and V projections across multiple Q heads. Instead of having \(N_h\) distinct heads for Q, K, and V (as in MHA), GQA uses \(N_h\) query heads but only \(N_{kv}\) key/value heads, where \(N_{kv} &lt; N_h\) and \(N_h\) is typically a multiple of \(N_{kv}\). These \(N_h\) query heads are divided into \(N_{kv}\) groups, with each group of \(N_h / N_{kv}\) query heads attending to the <em>same</em> key and value head. This structure significantly reduces the parameter count for <code>K</code> and <code>V</code> projection matrices and, more importantly, shrinks the size of the K/V cache needed during autoregressive decoding.</p>
<p>Let the input sequence representation be \(X \in \mathbb{R}^{L \times d_{\text{model}}}\), where \(L\) is the sequence length and \(d_{\text{model}}\) is the embedding dimension. GQA first projects \(X\) into queries, keys, and values using different linear transformations: \(Q = XW_Q\), \(K = XW_K\), and \(V = XW_V\). Here, \(W_Q \in \mathbb{R}^{d_{\text{model}} \times (N_h d_k)}\), \(W_K \in \mathbb{R}^{d_{\text{model}} \times (N_{kv} d_k)}\), and \(W_V \in \mathbb{R}^{d_{\text{model}} \times (N_{kv} d_k)}\), where \(d_k\) is the dimension of each head (<code>head_dim</code>). These are reshaped into \(N_h\) query heads \(Q_i \in \mathbb{R}^{L \times d_k}\) (\(i=1...N_h\)) and \(N_{kv}\) key/value heads \(K_j, V_j \in \mathbb{R}^{L \times d_k}\) (\(j=1...N_{kv}\)). The key step in GQA is sharing: for the \(i\)-th query head, the corresponding key and value heads are \(K_{\lceil i / g \rceil}\) and \(V_{\lceil i / g \rceil}\), where \(g = N_h / N_{kv}\) is the group size (number of queries per KV head). The attention output for the \(i\)-th head is computed as:</p>
<p>$$</p>
<p>In implementation, we do this by computing the \(N_{kv}\) key/value heads and then repeating or interleaving them \(g\) times to match the \(N_h\) query heads before the batched matrix multiplication for attention scores, as shown in the <a href="https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html"><code>repeat_interleave</code></a> step in the example code. Finally, the outputs of all \(N_h\) heads are concatenated and passed through an output projection \(W_O\).</p>
<p>GQA is primarily used as a technique to accelerate inference speed and reduce memory requirements without significantly compromising model performance. During autoregressive generation, the previously computed keys and values for the context sequence are cached and reused for subsequent token predictions. The size of this K/V cache is directly proportional to the number of K/V heads (\(N_{kv}\) in GQA, \(N_h\) in MHA). By reducing \(N_{kv}\), GQA drastically cuts down the memory bandwidth needed to load the K/V cache at each decoding step, which is the main performance bottleneck.</p>
<p>While it might slightly reduce the model&#39;s representational capacity compared to MHA (as K/V projections are shared), empirical results show that GQA achieves a favorable trade-off, maintaining most of the quality of MHA while offering substantial speedups and memory savings, making it popular for deploying large models efficiently. Multi-query attention (MQA), where \(N_{kv}=1\), is an extreme form of GQA.</p>
<pre><code><span>class</span> <span>GroupQueryAttention</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> num_heads<span>,</span> num_kv_heads<span>=</span><span>None</span><span>,</span> head_dim<span>=</span><span>64</span><span>,</span> dropout<span>=</span><span>0.0</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>num_heads <span>=</span> num_heads
        self<span>.</span>num_kv_heads <span>=</span> num_kv_heads <span>if</span> num_kv_heads <span>else</span> num_heads
        self<span>.</span>head_dim <span>=</span> head_dim
        
        <span># Ensure num_heads is divisible by num_kv_heads</span>
        <span>assert</span> self<span>.</span>num_heads <span>%</span> self<span>.</span>num_kv_heads <span>==</span> <span>0</span><span>,</span> <span>&#34;num_heads must be divisible by num_kv_heads&#34;</span>
        
        <span># Number of queries per key-value head</span>
        self<span>.</span>num_queries_per_kv <span>=</span> self<span>.</span>num_heads <span>//</span> self<span>.</span>num_kv_heads
        
        <span># Projections</span>
        self<span>.</span>q_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>k_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> self<span>.</span>num_kv_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>v_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> self<span>.</span>num_kv_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>o_proj <span>=</span> nn<span>.</span>Linear<span>(</span>num_heads <span>*</span> head_dim<span>,</span> dim<span>)</span>
        self<span>.</span>dropout <span>=</span> nn<span>.</span>Dropout<span>(</span>dropout<span>)</span>
        
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Project to queries, keys, values</span>
        q <span>=</span> self<span>.</span>q_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k <span>=</span> self<span>.</span>k_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_kv_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v <span>=</span> self<span>.</span>v_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_kv_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Transpose for attention computation</span>
        q <span>=</span> q<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        k <span>=</span> k<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_kv_heads, seq_len, head_dim]</span>
        v <span>=</span> v<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_kv_heads, seq_len, head_dim]</span>
        
        <span># Repeat k,v for each query head in the group</span>
        k <span>=</span> k<span>.</span>repeat_interleave<span>(</span>self<span>.</span>num_queries_per_kv<span>,</span> dim<span>=</span><span>1</span><span>)</span>
        v <span>=</span> v<span>.</span>repeat_interleave<span>(</span>self<span>.</span>num_queries_per_kv<span>,</span> dim<span>=</span><span>1</span><span>)</span>
        
        <span># Scaled dot-product attention</span>
        scale <span>=</span> <span>1.0</span> <span>/</span> math<span>.</span>sqrt<span>(</span>self<span>.</span>head_dim<span>)</span>
        attn <span>=</span> torch<span>.</span>matmul<span>(</span>q<span>,</span> k<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
        
        <span># Apply mask if provided</span>
        <span>if</span> mask <span>is</span> <span>not</span> <span>None</span><span>:</span>
            attn <span>=</span> attn<span>.</span>masked_fill<span>(</span>mask <span>==</span> <span>0</span><span>,</span> <span>-</span><span>1e9</span><span>)</span>
        
        <span># Softmax and dropout</span>
        attn <span>=</span> torch<span>.</span>softmax<span>(</span>attn<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        attn <span>=</span> self<span>.</span>dropout<span>(</span>attn<span>)</span>
        
        <span># Apply attention to values</span>
        out <span>=</span> torch<span>.</span>matmul<span>(</span>attn<span>,</span> v<span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        out <span>=</span> out<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads <span>*</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Output projection</span>
        out <span>=</span> self<span>.</span>o_proj<span>(</span>out<span>)</span>
        
        <span>return</span> out
</code></pre>
<h2 id="multi-head-latent-attention" tabindex="-1">Multi-head Latent Attention</h2>
<p><strong>Multi-head Latent Attention</strong> introduces a set of learnable &#34;latent&#34; vectors that act as an intermediary bottleneck between the input sequence elements. The core idea is to alleviate the quadratic computational cost \(O(L^2)\), where \(L\) is the sequence length, inherent in standard self-attention mechanisms. Instead of allowing every input element to attend directly to every other element, inputs first attend to a fixed number of latent units (\(N_{\text{latents}}\)), and these latents then attend back to the inputs (or variations thereof). This effectively decouples the direct interaction within the long input sequence, replacing it with two cross-attention steps involving the much smaller set of latents. This approach assumes that the essential information from the input sequence can be effectively summarized or compressed into these latent representations, thus maintaining representational power while significantly reducing computation, especially when \(N_{\text{latents}} \ll L\).</p>
<p>The mechanism involves two main stages of attention computation, typically within a multi-head framework. Let the input sequence be \(X \in \mathbb{R}^{L \times d}\) and the learnable latent array be \(L \in \mathbb{R}^{N_{\text{latents}} \times d}\). Both \(X\) and \(L\) are projected into Q, K, and V using shared or separate projection matrices. Let&#39;s denote the input projections as \(Q_X, K_X, V_X\) and latent projections as \(Q_L, K_L, V_L\), split across multiple heads. The first cross-attention step computes how latents attend to the input: the latent queries \(Q_L\) attend to the input keys \(K_X\) and aggregate information from input values \(V_X\). The attention output for the latents is</p>
<p>$$</p>
<p>Where \(d_k\) is the head dimension. In the second cross-attention step, the input queries \(Q_X\) attend to the keys derived from the latents (e.g., \(K_L\)) and aggregate information from the values associated with the latents (which could be \(V_L\) or, as implemented in the example code, the updated latent representation \(H_L\)). The final output \(O\) is then</p>
<p>$$</p>
<p>These operations are performed independently for each head, and the results are concatenated and passed through a final linear projection.</p>
<p>Multi-head Latent Attention is primarily employed in architectures designed to handle very long sequences or high-dimensional inputs where standard self-attention is computationally infeasible. Examples include processing long documents, high-resolution images (treating patches as a sequence), audio signals, or video data. By using a fixed number of latents (\(N_{\text{latents}}\)), the computational complexity is reduced from \(O(L^2)\) to \(O(L \cdot N_{\text{latents}})\), making it scalable to much larger inputs. The learnable latent vectors \(L\) (initialized randomly and updated via backpropagation, as seen in <code>self.latents = nn.Parameter(...)</code> in the code) adapt during training to function as a compressed representation or memory bank relevant to the task. While this introduces an information bottleneck, potentially limiting fine-grained local interactions compared to full self-attention, it excels at capturing global context efficiently and has proven effective in various modalities, enabling Transformer-like architectures to be applied to previously challenging domains due to sequence length constraints.</p>
<pre><code><span>class</span> <span>MultiHeadLatentAttention</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> num_heads<span>,</span> num_latents<span>=</span><span>64</span><span>,</span> head_dim<span>=</span><span>64</span><span>,</span> dropout<span>=</span><span>0.0</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>num_heads <span>=</span> num_heads
        self<span>.</span>num_latents <span>=</span> num_latents
        self<span>.</span>head_dim <span>=</span> head_dim
        
        <span># Projections</span>
        self<span>.</span>q_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>k_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>v_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>o_proj <span>=</span> nn<span>.</span>Linear<span>(</span>num_heads <span>*</span> head_dim<span>,</span> dim<span>)</span>
        
        <span># Latent vectors (learned)</span>
        self<span>.</span>latents <span>=</span> nn<span>.</span>Parameter<span>(</span>torch<span>.</span>randn<span>(</span><span>1</span><span>,</span> num_latents<span>,</span> dim<span>)</span><span>)</span>
        
        self<span>.</span>dropout <span>=</span> nn<span>.</span>Dropout<span>(</span>dropout<span>)</span>
        
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Get latents for this batch</span>
        latents <span>=</span> self<span>.</span>latents<span>.</span>expand<span>(</span>batch_size<span>,</span> <span>-</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>)</span>
        
        <span># Project inputs to queries, keys, values</span>
        q_x <span>=</span> self<span>.</span>q_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k_x <span>=</span> self<span>.</span>k_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v_x <span>=</span> self<span>.</span>v_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Project latents to queries, keys, values</span>
        q_latents <span>=</span> self<span>.</span>q_proj<span>(</span>latents<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> self<span>.</span>num_latents<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k_latents <span>=</span> self<span>.</span>k_proj<span>(</span>latents<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> self<span>.</span>num_latents<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v_latents <span>=</span> self<span>.</span>v_proj<span>(</span>latents<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> self<span>.</span>num_latents<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Transpose for attention computation</span>
        q_x <span>=</span> q_x<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        k_x <span>=</span> k_x<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        v_x <span>=</span> v_x<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        
        q_latents <span>=</span> q_latents<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, num_latents, head_dim]</span>
        k_latents <span>=</span> k_latents<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, num_latents, head_dim]</span>
        v_latents <span>=</span> v_latents<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, num_latents, head_dim]</span>
        
        <span># Scale factor for attention</span>
        scale <span>=</span> <span>1.0</span> <span>/</span> math<span>.</span>sqrt<span>(</span>self<span>.</span>head_dim<span>)</span>
        
        <span># Compute latent-to-input attention</span>
        attn_latent_to_input <span>=</span> torch<span>.</span>matmul<span>(</span>q_latents<span>,</span> k_x<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
        
        <span># Apply mask if provided</span>
        <span>if</span> mask <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span># Expand mask for the latent queries</span>
            latent_mask <span>=</span> mask<span>.</span>unsqueeze<span>(</span><span>1</span><span>)</span><span>.</span>expand<span>(</span><span>-</span><span>1</span><span>,</span> self<span>.</span>num_heads<span>,</span> <span>-</span><span>1</span><span>,</span> <span>-</span><span>1</span><span>)</span>
            attn_latent_to_input <span>=</span> attn_latent_to_input<span>.</span>masked_fill<span>(</span>latent_mask <span>==</span> <span>0</span><span>,</span> <span>-</span><span>1e9</span><span>)</span>
        
        <span># Softmax and dropout</span>
        attn_latent_to_input <span>=</span> torch<span>.</span>softmax<span>(</span>attn_latent_to_input<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        attn_latent_to_input <span>=</span> self<span>.</span>dropout<span>(</span>attn_latent_to_input<span>)</span>
        
        <span># Apply attention weights to input values</span>
        latent_output <span>=</span> torch<span>.</span>matmul<span>(</span>attn_latent_to_input<span>,</span> v_x<span>)</span>  <span># [batch_size, num_heads, num_latents, head_dim]</span>
        
        <span># Compute input-to-latent attention</span>
        attn_input_to_latent <span>=</span> torch<span>.</span>matmul<span>(</span>q_x<span>,</span> k_latents<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
        
        <span># Softmax and dropout</span>
        attn_input_to_latent <span>=</span> torch<span>.</span>softmax<span>(</span>attn_input_to_latent<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        attn_input_to_latent <span>=</span> self<span>.</span>dropout<span>(</span>attn_input_to_latent<span>)</span>
        
        <span># Updated latent values are used as values for input-to-latent attention</span>
        output <span>=</span> torch<span>.</span>matmul<span>(</span>attn_input_to_latent<span>,</span> latent_output<span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        
        <span># Reshape and apply output projection</span>
        output <span>=</span> output<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads <span>*</span> self<span>.</span>head_dim<span>)</span>
        output <span>=</span> self<span>.</span>o_proj<span>(</span>output<span>)</span>
        
        <span>return</span> output
</code></pre>
<h2 id="flash-attention" tabindex="-1">Flash Attention</h2>
<p><strong>Flash Attention</strong> (particularly the latest implementation <a href="https://pytorch.org/blog/flashattention-3/">FlashAttention-3</a>) addresses the significant memory bottleneck inherent in standard self-attention mechanisms within Transformers, particularly for long sequences. The conventional approach computes the full attention score matrix \( S = QK^T \), where \(Q, K \in \mathbb{R}^{N \times d}\) are the query and key matrices for a sequence of length \(N\). This requires storing the \(N \times N\) matrix \(S\), leading to \(O(N^2)\) memory complexity with respect to sequence length. This becomes prohibitive for large \(N\). Flash Attention overcomes this by avoiding the materialization and storage of the full \(S\) matrix in the GPU&#39;s slow high bandwidth memory. Instead, it leverages tiling and recomputation techniques, processing the attention computation in smaller blocks that fit into the much faster on-chip SRAM.</p>
<p>The core mechanism involves breaking the Q, K, and V matrices into blocks. Flash Attention iteratively loads blocks of K and V into SRAM, and for each block of Q, it computes the attention scores against the current K block also residing in SRAM. Crucially, it employs an online softmax algorithm. Instead of computing the full softmax denominator across all keys at once, it maintains running statistics (the maximum score seen so far for numerical stability, and the cumulative sum of exponentiated scores for normalization) as it iterates through the K/V blocks. This allows it to compute the correctly scaled attention output block-by-block without ever needing the complete \(N \times N\) matrix. By keeping intermediate results primarily within the fast SRAM and minimizing data transfer to and from high bandwidth memory, Flash Attention significantly reduces the memory footprint related to sequence length from \(O(N^2)\) down to \(O(N)\) (dominated by storing Q, K, V themselves) and achieves substantial speedups due to improved memory access patterns.</p>
<p>In practice the FlashAttention-3 implementation is a family of highly-optimized CUDA kernels that are designed to be efficient for different hardware configurations. But a minimal toy implementation in PyTorch is shown below:</p>
<pre><code><span>class</span> <span>FlashAttention</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> num_heads<span>,</span> head_dim<span>=</span><span>64</span><span>,</span> dropout<span>=</span><span>0.0</span><span>,</span> block_size<span>=</span><span>1024</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>num_heads <span>=</span> num_heads
        self<span>.</span>head_dim <span>=</span> head_dim
        self<span>.</span>block_size <span>=</span> block_size
        
        <span># Projections</span>
        self<span>.</span>q_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>k_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>v_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>o_proj <span>=</span> nn<span>.</span>Linear<span>(</span>num_heads <span>*</span> head_dim<span>,</span> dim<span>)</span>
        
        self<span>.</span>dropout <span>=</span> nn<span>.</span>Dropout<span>(</span>dropout<span>)</span>
        
    <span>def</span> <span>_flash_attention_forward</span><span>(</span>self<span>,</span> q<span>,</span> k<span>,</span> v<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        <span># This is a simplified approximation of Flash Attention</span>
        <span># In practice, FlashAttention uses custom CUDA kernels for tiled attention</span>
        
        batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> head_dim <span>=</span> q<span>.</span>shape
        scale <span>=</span> <span>1.0</span> <span>/</span> math<span>.</span>sqrt<span>(</span>head_dim<span>)</span>
        
        <span># Initialize output and attention statistics</span>
        output <span>=</span> torch<span>.</span>zeros_like<span>(</span>q<span>)</span>
        normalizer <span>=</span> torch<span>.</span>zeros<span>(</span><span>(</span>batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> <span>1</span><span>)</span><span>,</span> device<span>=</span>q<span>.</span>device<span>)</span>
        
        <span># Process blocks of keys and values</span>
        <span>for</span> block_start <span>in</span> <span>range</span><span>(</span><span>0</span><span>,</span> seq_len<span>,</span> self<span>.</span>block_size<span>)</span><span>:</span>
            block_end <span>=</span> <span>min</span><span>(</span>block_start <span>+</span> self<span>.</span>block_size<span>,</span> seq_len<span>)</span>
            
            <span># Extract key and value blocks</span>
            k_block <span>=</span> k<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> block_start<span>:</span>block_end<span>]</span>
            v_block <span>=</span> v<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> block_start<span>:</span>block_end<span>]</span>
            
            <span># Compute attention scores for this block</span>
            attn_scores <span>=</span> torch<span>.</span>matmul<span>(</span>q<span>,</span> k_block<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
            
            <span># Apply mask if provided</span>
            <span>if</span> mask <span>is</span> <span>not</span> <span>None</span><span>:</span>
                block_mask <span>=</span> mask<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>:</span><span>,</span> block_start<span>:</span>block_end<span>]</span>
                attn_scores <span>=</span> attn_scores<span>.</span>masked_fill<span>(</span>block_mask <span>==</span> <span>0</span><span>,</span> <span>-</span><span>1e9</span><span>)</span>
            
            <span># Apply softmax and dropout</span>
            attn_probs <span>=</span> torch<span>.</span>softmax<span>(</span>attn_scores<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
            attn_probs <span>=</span> self<span>.</span>dropout<span>(</span>attn_probs<span>)</span>
            
            <span># Update output with the attention results for this block</span>
            output <span>+=</span> torch<span>.</span>matmul<span>(</span>attn_probs<span>,</span> v_block<span>)</span>
            normalizer <span>+=</span> attn_probs<span>.</span><span>sum</span><span>(</span>dim<span>=</span><span>-</span><span>1</span><span>,</span> keepdim<span>=</span><span>True</span><span>)</span>
        
        <span># Normalize the output</span>
        output <span>=</span> output <span>/</span> <span>(</span>normalizer <span>+</span> <span>1e-6</span><span>)</span>
        
        <span>return</span> output
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Project queries, keys, values</span>
        q <span>=</span> self<span>.</span>q_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k <span>=</span> self<span>.</span>k_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v <span>=</span> self<span>.</span>v_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Transpose for attention computation</span>
        q <span>=</span> q<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        k <span>=</span> k<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        v <span>=</span> v<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        
        <span># Compute flash attention</span>
        output <span>=</span> self<span>.</span>_flash_attention_forward<span>(</span>q<span>,</span> k<span>,</span> v<span>,</span> mask<span>)</span>
        
        <span># Reshape and apply output projection</span>
        output <span>=</span> output<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads <span>*</span> self<span>.</span>head_dim<span>)</span>
        output <span>=</span> self<span>.</span>o_proj<span>(</span>output<span>)</span>
        
        <span>return</span> output
</code></pre>
<p>In reality the repository <a href="https://github.com/Dao-AILab/flash-attention">Dao-AILab/flash-attention</a> has a number of different implementations for different hardware configurations. The <code>flash_attn_qkvpacked_func</code> function is a minimal example of how to use the FlashAttention-3 implementation. It takes a packed QKV tensor as input and returns the attention output.</p>
<pre><code><span>import</span> torch
<span>from</span> flash_attn <span>import</span> flash_attn_qkvpacked_func

<span># Minimal configuration</span>
BATCH_SIZE<span>,</span> SEQ_LEN<span>,</span> NUM_HEADS<span>,</span> HEAD_DIM <span>=</span> <span>2</span><span>,</span> <span>64</span><span>,</span> <span>4</span><span>,</span> <span>32</span>
CAUSAL <span>=</span> <span>False</span>
DTYPE <span>=</span> torch<span>.</span>float16
DEVICE <span>=</span> <span>&#34;cuda&#34;</span>

<span># Create dummy packed QKV tensor</span>
<span># Shape: (batch_size, seq_len, 3, num_heads, head_dim)</span>
qkv <span>=</span> torch<span>.</span>randn<span>(</span>
    BATCH_SIZE<span>,</span>
    SEQ_LEN<span>,</span>
    <span>3</span><span>,</span>
    NUM_HEADS<span>,</span>
    HEAD_DIM<span>,</span>
    dtype<span>=</span>DTYPE<span>,</span>
    device<span>=</span>DEVICE<span>,</span>
<span>)</span>

<span>print</span><span>(</span><span><span>f&#34;Input qkv shape: </span><span><span>{</span>qkv<span>.</span>shape<span>}</span></span><span>&#34;</span></span><span>)</span>

<span># Call FlashAttention packed QKV function</span>
output <span>=</span> flash_attn_qkvpacked_func<span>(</span>
    qkv<span>,</span>
    dropout_p<span>=</span><span>0.0</span><span>,</span> <span># Set dropout probability (0.0 for no dropout)</span>
    causal<span>=</span>CAUSAL<span>,</span>
    softmax_scale<span>=</span><span>None</span> <span># Use default scaling (1 / sqrt(head_dim))</span>
<span>)</span>

<span># Output shape: (batch_size, seq_len, num_heads, head_dim)</span>
<span>print</span><span>(</span><span><span>f&#34;Output shape: </span><span><span>{</span>output<span>.</span>shape<span>}</span></span><span>&#34;</span></span><span>)</span>
<span>print</span><span>(</span><span>&#34;FlashAttention call successful.&#34;</span><span>)</span>
</code></pre>
<h2 id="ring-attention" tabindex="-1">Ring Attention</h2>
<p><a href="https://arxiv.org/abs/2310.01889"><strong>Ring Attention</strong></a> uses blockwise computation of self-attention on multiple GPUs and enables training and inference of sequences that would be too long for a single devices. It addresses the significant memory bottleneck inherent in standard self-attention mechanisms, particularly when processing very long sequences where the quadratic memory complexity \(O(N^2)\) of the full attention score matrix becomes prohibitive.</p>
<p>The core idea is to distribute the computation across multiple processing units, like GPUs, arranged conceptually in a ring topology. This approach avoids the need for any single device to hold the entire <code>K</code> and <code>V</code> tensors. Instead, these tensors are sharded or chunked along the sequence length dimension, drastically reducing the peak memory requirement per device and enabling attention calculations over sequences that would otherwise exceed the memory capacity of individual accelerators.</p>
<p>In a practical distributed implementation, each device initially holds the <code>Q</code>, <code>K</code>, and <code>V</code> shards corresponding to its segment of the input sequence. The attention calculation unfolds in synchronized steps across this ring. During each step, a device calculates partial attention scores using its local <code>Q</code> shard and the <code>K</code> shard it currently possesses. The crucial element is the subsequent communication: the <code>K</code> and <code>V</code> shards are passed to the next device in the ring. This rotation repeats until every <code>Q</code> shard has interacted with every <code>K/V</code> shard. Throughout this process, each device accumulates partial outputs (weighted <code>V</code> vectors) and normalization factors (softmax denominators). Finalizing the attention output typically involves a collective operation across all devices to combine these partial results correctly for each segment of the sequence.</p>
<p>The Python example below offers a simulated Ring Attention logic on a single device, illustrating the underlying principles without necessitating actual multi-GPU hardware. The <code>_simulate_ring_attention</code> function mimics the distributed process by iterating through hypothetical shards. In each iteration, it selects slices of the <code>K</code> and <code>V</code> tensors (<code>k_shard</code>, <code>v_shard</code>) to represent the data one device would handle at a given step. It then computes attention scores between the full <code>Q</code> tensor (a simplification from a truly distributed setup) and the current <code>k_shard</code>. The simulation effectively captures the essence of the ring approach by accumulating the weighted values and the softmax normalizers across these iterations, mirroring how partial results would be combined in a distributed setting before a final normalization step yields the output. While demonstrating the computational flow, this simulation naturally doesn&#39;t provide the parallelism or memory savings of a true multi-device Ring Attention implementation.</p>
<pre><code><span>class</span> <span>RingAttention</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> num_heads<span>,</span> head_dim<span>=</span><span>64</span><span>,</span> dropout<span>=</span><span>0.0</span><span>,</span> num_shards<span>=</span><span>4</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>num_heads <span>=</span> num_heads
        self<span>.</span>head_dim <span>=</span> head_dim
        self<span>.</span>num_shards <span>=</span> num_shards
        
        <span># Projections</span>
        self<span>.</span>q_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>k_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>v_proj <span>=</span> nn<span>.</span>Linear<span>(</span>dim<span>,</span> num_heads <span>*</span> head_dim<span>)</span>
        self<span>.</span>o_proj <span>=</span> nn<span>.</span>Linear<span>(</span>num_heads <span>*</span> head_dim<span>,</span> dim<span>)</span>
        
        self<span>.</span>dropout <span>=</span> nn<span>.</span>Dropout<span>(</span>dropout<span>)</span>
        
    <span>def</span> <span>_simulate_ring_attention</span><span>(</span>self<span>,</span> q<span>,</span> k<span>,</span> v<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        <span># This simulates ring attention without actual multi-GPU support</span>
        batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> head_dim <span>=</span> q<span>.</span>shape
        scale <span>=</span> <span>1.0</span> <span>/</span> math<span>.</span>sqrt<span>(</span>head_dim<span>)</span>
        
        <span># Compute shard sizes</span>
        shard_size <span>=</span> <span>(</span>seq_len <span>+</span> self<span>.</span>num_shards <span>-</span> <span>1</span><span>)</span> <span>//</span> self<span>.</span>num_shards
        
        <span># Initialize outputs</span>
        output <span>=</span> torch<span>.</span>zeros_like<span>(</span>q<span>)</span>
        normalizer <span>=</span> torch<span>.</span>zeros<span>(</span><span>(</span>batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> <span>1</span><span>)</span><span>,</span> device<span>=</span>q<span>.</span>device<span>)</span>
        
        <span># Simulate sharded processing</span>
        <span>for</span> shard_idx <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_shards<span>)</span><span>:</span>
            start_idx <span>=</span> shard_idx <span>*</span> shard_size
            end_idx <span>=</span> <span>min</span><span>(</span>start_idx <span>+</span> shard_size<span>,</span> seq_len<span>)</span>
            
            <span># Process this shard&#39;s keys and values</span>
            <span>if</span> start_idx <span>&lt;</span> seq_len<span>:</span>
                k_shard <span>=</span> k<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> start_idx<span>:</span>end_idx<span>]</span>
                v_shard <span>=</span> v<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> start_idx<span>:</span>end_idx<span>]</span>
                
                <span># Compute attention scores</span>
                attn_scores <span>=</span> torch<span>.</span>matmul<span>(</span>q<span>,</span> k_shard<span>.</span>transpose<span>(</span><span>2</span><span>,</span> <span>3</span><span>)</span><span>)</span> <span>*</span> scale
                
                <span># Apply mask if provided</span>
                <span>if</span> mask <span>is</span> <span>not</span> <span>None</span><span>:</span>
                    shard_mask <span>=</span> mask<span>[</span><span>:</span><span>,</span> <span>:</span><span>,</span> <span>:</span><span>,</span> start_idx<span>:</span>end_idx<span>]</span>
                    attn_scores <span>=</span> attn_scores<span>.</span>masked_fill<span>(</span>shard_mask <span>==</span> <span>0</span><span>,</span> <span>-</span><span>1e9</span><span>)</span>
                
                <span># Apply softmax and dropout (accumulated over shards)</span>
                attn_probs <span>=</span> torch<span>.</span>softmax<span>(</span>attn_scores<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
                attn_probs <span>=</span> self<span>.</span>dropout<span>(</span>attn_probs<span>)</span>
                
                <span># Update output and normalizer</span>
                output <span>+=</span> torch<span>.</span>matmul<span>(</span>attn_probs<span>,</span> v_shard<span>)</span>
                normalizer <span>+=</span> attn_probs<span>.</span><span>sum</span><span>(</span>dim<span>=</span><span>-</span><span>1</span><span>,</span> keepdim<span>=</span><span>True</span><span>)</span>
        
        <span># Normalize the output</span>
        output <span>=</span> output <span>/</span> <span>(</span>normalizer <span>+</span> <span>1e-6</span><span>)</span>
        
        <span>return</span> output
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> mask<span>=</span><span>None</span><span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Project queries, keys, values</span>
        q <span>=</span> self<span>.</span>q_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        k <span>=</span> self<span>.</span>k_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        v <span>=</span> self<span>.</span>v_proj<span>(</span>x<span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads<span>,</span> self<span>.</span>head_dim<span>)</span>
        
        <span># Transpose for attention computation</span>
        q <span>=</span> q<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        k <span>=</span> k<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        v <span>=</span> v<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>  <span># [batch_size, num_heads, seq_len, head_dim]</span>
        
        <span># Compute ring attention</span>
        output <span>=</span> self<span>.</span>_simulate_ring_attention<span>(</span>q<span>,</span> k<span>,</span> v<span>,</span> mask<span>)</span>
        
        <span># Reshape and apply output projection</span>
        output <span>=</span> output<span>.</span>transpose<span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>.</span>reshape<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_heads <span>*</span> self<span>.</span>head_dim<span>)</span>
        output <span>=</span> self<span>.</span>o_proj<span>(</span>output<span>)</span>
        
        <span>return</span> output
</code></pre>
<h2 id="pre-normalization" tabindex="-1">Pre-normalization</h2>
<p><strong>Pre-normalization</strong> (often referred to as Pre-LN) was a shift in the architectural design of residual blocks. Instead of applying the normalization layer <em>after</em> the main operation (like self-attention or a feed-forward network) as done in traditional post-normalization schemes, pre-normalization applies it <em>before</em>. This seemingly small change has significant implications for training dynamics. By normalizing the input <em>before</em> it enters the computationally intensive sub-layer, pre-normalization helps to stabilize the activations and gradients flowing through the network. This stabilization effect is particularly pronounced in very deep networks, mitigating issues like vanishing or exploding gradients and often allowing for higher learning rates and faster, more reliable convergence.</p>
<p>The typical implementation within a residual block follows the structure \( x + f(\text{norm}(x)) \), as demonstrated in the <code>PreNorm</code> class. Here, \( x \) is the input to the block, \( \text{norm}(\cdot) \) represents a normalization function like Layer Normalization (LN) or Root Mean Square Normalization (RMSNorm), and \( f(\cdot) \) denotes the main transformation function (e.g., multi-head attention or a position-wise feed-forward network). The input \( x \) first passes through the normalization layer (e.g., <code>self.norm(x)</code>). The normalized output is then processed by the function <code>fn</code>. Crucially, the output of this function is then added back to the <em>original</em>, unnormalized input \( x \) via the residual connection. This structure ensures a clean gradient path through the identity connection (<code>+ x</code>), further enhancing training stability compared to post-normalization where the normalization layer resides on the residual path itself.</p>
<pre><code><span>class</span> <span>PreNorm</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> fn<span>,</span> norm_type<span>=</span><span>&#39;layer&#39;</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>fn <span>=</span> fn
        
        <span>if</span> norm_type <span>==</span> <span>&#39;layer&#39;</span><span>:</span>
            self<span>.</span>norm <span>=</span> nn<span>.</span>LayerNorm<span>(</span>dim<span>)</span>
        <span>elif</span> norm_type <span>==</span> <span>&#39;rms&#39;</span><span>:</span>
            self<span>.</span>norm <span>=</span> RMSNorm<span>(</span>dim<span>)</span>
        <span>else</span><span>:</span>
            <span>raise</span> ValueError<span>(</span><span><span>f&#34;Unknown normalization type: </span><span><span>{</span>norm_type<span>}</span></span><span>&#34;</span></span><span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>,</span> <span>*</span>args<span>,</span> <span>**</span>kwargs<span>)</span><span>:</span>
        <span># Apply normalization first, then the function</span>
        <span>return</span> self<span>.</span>fn<span>(</span>self<span>.</span>norm<span>(</span>x<span>)</span><span>,</span> <span>*</span>args<span>,</span> <span>**</span>kwargs<span>)</span> <span>+</span> x
</code></pre>
<h2 id="rmsnorm" tabindex="-1">RMSNorm</h2>
<p><strong>RMSNorm</strong> (or Root Mean Square Normalization) is a simplification of the widely used LayerNorm, designed to reduce computational overhead while retaining comparable performance and often improving training stability. Unlike LayerNorm, which centers the activations by subtracting the mean and then scales by the standard deviation, RMSNorm omits the mean centering step entirely. The motivation behind this simplification stems from the empirical observation that the re-centering operation in LayerNorm accounts for a noticeable portion of its computational cost and that removing it often does not significantly harm, and can sometimes even benefit, model performance. It operates solely on the basis of re-scaling the inputs according to their root mean square magnitude.</p>
<p>The core mechanism of RMSNorm involves normalizing the activations within a layer by dividing them by their root mean square value, computed across the features (or a specified dimension). For an input vector \( x = (x_1, \dots, x_n) \), the RMS value is calculated as \( \text{RMS}(x) = \sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2} \). The normalized output \( \bar{x}_i \) is then \( \bar{x}_i = \frac{x_i}{\text{RMS}(x) + \epsilon} \), where \( \epsilon \) is a small constant for numerical stability. Similar to LayerNorm, RMSNorm typically includes a learnable scaling parameter \( g \) (and sometimes a bias \( b \), although the original formulation often omits it to stick closer to the simplification principle), resulting in the final output \( y_i = g_i \bar{x}_i \). By foregoing the mean calculation, RMSNorm offers a reduction in computation (estimated to be 7-64% faster than LayerNorm on GPUs depending on the setup) and memory usage, making it an attractive alternative, especially for large models where efficiency is paramount.</p>
<pre><code><span>class</span> <span>RMSNorm</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> eps<span>=</span><span>1e-8</span><span>,</span> elementwise_affine<span>=</span><span>True</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>eps <span>=</span> eps
        self<span>.</span>elementwise_affine <span>=</span> elementwise_affine
        
        <span>if</span> elementwise_affine<span>:</span>
            self<span>.</span>weight <span>=</span> nn<span>.</span>Parameter<span>(</span>torch<span>.</span>ones<span>(</span>dim<span>)</span><span>)</span>
        <span>else</span><span>:</span>
            self<span>.</span>register_parameter<span>(</span><span>&#39;weight&#39;</span><span>,</span> <span>None</span><span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>)</span><span>:</span>
        <span># Calculate root mean square along the last dimension</span>
        rms <span>=</span> torch<span>.</span>sqrt<span>(</span>torch<span>.</span>mean<span>(</span>x <span>**</span> <span>2</span><span>,</span> dim<span>=</span><span>-</span><span>1</span><span>,</span> keepdim<span>=</span><span>True</span><span>)</span> <span>+</span> self<span>.</span>eps<span>)</span>
        
        <span># Normalize by RMS</span>
        x_normalized <span>=</span> x <span>/</span> rms
        
        <span># Apply scaling if using learnable parameters</span>
        <span>if</span> self<span>.</span>elementwise_affine<span>:</span>
            x_normalized <span>=</span> x_normalized <span>*</span> self<span>.</span>weight
        
        <span>return</span> x_normalized
</code></pre>
<h2 id="swiglu" tabindex="-1">SwiGLU</h2>
<p>SwiGLU is an activation function derived from the Gated Linear Unit (GLU) family, specifically tailored for enhancing the performance of neural networks. The core concept behind GLU variants is to introduce a gating mechanism that adaptively controls the flow of information through the network. Standard feed-forward layers typically apply a single non-linearity to a linear transformation of the input. In contrast, GLU-based activations split the output of a linear layer into two parts; one part acts as a &#34;gate&#34; after passing through a non-linearity, modulating the other part via element-wise multiplication. SwiGLU distinguishes itself by employing the Sigmoid-weighted Linear Unit (SiLU), also known as Swish (\( \text{SiLU}(x) = x \cdot \sigma(x) \), where \( \sigma \) is the sigmoid function), as the specific non-linearity applied to the gating part. This choice has been empirically shown to yield significant performance improvements in various Transformer-based models compared to other activations like ReLU or standard GLU variants using sigmoid or other functions for the gate.</p>
<p>The operational mechanism of SwiGLU within a feed-forward block typically involves projecting the input \( x \) using two separate linear transformations, yielding \( Wx + b \) and \( Vx + c \). The SwiGLU activation is then computed as \( \text{SwiGLU}(x, W, V, b, c) = \text{SiLU}(Wx + b) \odot (Vx + c) \), where \( \odot \) denotes element-wise multiplication. Effectively, the input is processed through two parallel linear paths. One path undergoes the SiLU activation to form the gate values, which then scale the output of the second linear path. This gating allows the network to dynamically control which features are passed forward based on the input context, leading to increased expressive power and better gradient flow compared to simpler activation functions. Its success in models like PaLM and LLaMA highlights its effectiveness in capturing complex patterns within language data.</p>
<pre><code><span>class</span> <span>SwiGLU</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim_in<span>,</span> dim_hidden<span>=</span><span>None</span><span>,</span> dim_out<span>=</span><span>None</span><span>,</span> bias<span>=</span><span>True</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        dim_hidden <span>=</span> dim_hidden <span>or</span> <span>4</span> <span>*</span> dim_in
        dim_out <span>=</span> dim_out <span>or</span> dim_in
        
        <span># Linear transformations for gating</span>
        self<span>.</span>w1 <span>=</span> nn<span>.</span>Linear<span>(</span>dim_in<span>,</span> dim_hidden<span>,</span> bias<span>=</span>bias<span>)</span>
        self<span>.</span>w2 <span>=</span> nn<span>.</span>Linear<span>(</span>dim_in<span>,</span> dim_hidden<span>,</span> bias<span>=</span>bias<span>)</span>
        
        <span># Output projection</span>
        self<span>.</span>w3 <span>=</span> nn<span>.</span>Linear<span>(</span>dim_hidden<span>,</span> dim_out<span>,</span> bias<span>=</span>bias<span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>)</span><span>:</span>
        <span># SwiGLU applies SiLU activation to one branch and gates it with the other</span>
        hidden1 <span>=</span> self<span>.</span>w1<span>(</span>x<span>)</span>
        hidden2 <span>=</span> self<span>.</span>w2<span>(</span>x<span>)</span>
        
        <span># SiLU (Swish) activation: x * sigmoid(x)</span>
        hidden1_act <span>=</span> hidden1 <span>*</span> torch<span>.</span>sigmoid<span>(</span>hidden1<span>)</span>
        
        <span># Element-wise product for gating</span>
        hidden <span>=</span> hidden1_act <span>*</span> hidden2
        
        <span># Output projection</span>
        <span>return</span> self<span>.</span>w3<span>(</span>hidden<span>)</span>
</code></pre>
<h2 id="rotary-positional-embedding" tabindex="-1">Rotary Positional Embedding</h2>
<p><a href="https://arxiv.org/abs/2104.09864"><strong>Rotary Positional Embedding</strong></a> (RoPE) introduces an elegant method for incorporating positional information directly into the self-attention mechanism of Transformer models, specifically designed to capture relative positional dependencies effectively. Traditional approaches often rely on adding absolute positional encodings to the input embeddings or using complex relative positional bias terms within the attention score calculation. RoPE takes a different route by viewing positional encoding as a rotation operation applied to the query and key vectors <em>before</em> their dot product is computed. The key insight is that by rotating the Q vector corresponding to position \( m \) and the K vector corresponding to position \( n \) by angles proportional to \( m \) and \( n \) respectively, the resulting dot product inherently depends only on the <em>relative</em> position \( m - n \) and the content of the vectors themselves, gracefully encoding relative distance without altering the vectors&#39; magnitudes.</p>
<p>The core mathematical idea leverages the properties of complex number multiplication or, equivalently, 2D rotation matrices. Imagine representing pairs of embedding dimensions as complex numbers. RoPE applies a rotation to the query vector \( q_m \) at position \( m \) and the key vector \( k_n \) at position \( n \) using position-dependent rotation matrices \( R_m \) and \( R_n \), respectively. These matrices effectively rotate the vectors in 2D subspaces spanned by pairs of dimensions. The angle of rotation for each 2D subspace is determined by the absolute position (\( m \) or \( n \)) multiplied by a frequency term \( \theta_i \), which decreases for higher dimensions, analogous to sinusoidal embeddings. The crucial property is that the dot product between the rotated vectors, \( (R_m q_m)^T (R_n k_n) \), simplifies such that it only depends on the original vectors \( q_m, k_n \) and a rotation matrix \( R_{m-n} \) corresponding to the relative distance, effectively embedding relative positional information directly into the attention score calculation.</p>
<p>In practice, this rotation is implemented efficiently without explicit matrix multiplication. As shown in the <code>apply_rotary_pos_emb</code> function, the embedding dimensions are typically split into pairs. For each pair \( (x_i, x_{i+1}) \), the rotation corresponding to position \( m \) and frequency \( \theta_j \) (derived from <code>inv_freq</code> in the <code>RotaryEmbedding</code> class) is applied using trigonometric functions: the new pair \( (x&#39;_i, x&#39;_{i+1}) \) becomes \( (x_i \cos(m\theta_j) - x_{i+1} \sin(m\theta_j), x_{i+1} \cos(m\theta_j) + x_i \sin(m\theta_j)) \). The <code>RotaryEmbedding</code> class pre-computes the necessary cosine and sine values (<code>cos</code>, <code>sin</code>) based on the sequence length and the inverse frequency bands (<code>inv_freq</code>), which are derived from a base value (<code>base</code>) and the embedding dimension (<code>dim</code>). These pre-computed values represent \( \cos(m\theta_j) \) and \( \sin(m\theta_j) \) for all positions \( m \) and relevant frequencies \( \theta_j \).</p>
<p>Applying RoPE involves generating these <code>cos</code> and <code>sin</code> embeddings for the given sequence length and then using them to transform the Q and K vectors pair-wise across their head dimension <em>after</em> the initial linear projections but <em>before</em> the attention score calculation. This method offers several advantages: it naturally encodes relative positions, avoids adding positional information directly to word embeddings (potentially preserving more semantic information), and has shown strong empirical performance, including good generalization to sequence lengths longer than those seen during training. By integrating position information via rotation, RoPE provides a computationally efficient and effective mechanism for context-aware sequence modeling.</p>
<pre><code><span>class</span> <span>RotaryEmbedding</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> dim<span>,</span> base<span>=</span><span>10000</span><span>,</span> interleaved<span>=</span><span>False</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>dim <span>=</span> dim
        self<span>.</span>base <span>=</span> base
        self<span>.</span>interleaved <span>=</span> interleaved
        
        <span># Generate inverse frequency bands</span>
        inv_freq <span>=</span> <span>1.0</span> <span>/</span> <span>(</span>base <span>**</span> <span>(</span>torch<span>.</span>arange<span>(</span><span>0</span><span>,</span> dim<span>,</span> <span>2</span><span>)</span><span>.</span><span>float</span><span>(</span><span>)</span> <span>/</span> dim<span>)</span><span>)</span>
        self<span>.</span>register_buffer<span>(</span><span>&#39;inv_freq&#39;</span><span>,</span> inv_freq<span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> seq_len<span>,</span> device<span>=</span><span>None</span><span>)</span><span>:</span>
        <span># Get device from buffer if not specified</span>
        <span>if</span> device <span>is</span> <span>None</span><span>:</span>
            device <span>=</span> self<span>.</span>inv_freq<span>.</span>device
            
        <span># Generate position indices</span>
        positions <span>=</span> torch<span>.</span>arange<span>(</span>seq_len<span>,</span> device<span>=</span>device<span>)</span><span>.</span><span>float</span><span>(</span><span>)</span>
        
        <span># Compute sinusoidal patterns</span>
        freqs <span>=</span> torch<span>.</span>outer<span>(</span>positions<span>,</span> self<span>.</span>inv_freq<span>)</span>
        
        <span># Get sine and cosine embeddings</span>
        emb <span>=</span> torch<span>.</span>cat<span>(</span><span>(</span>freqs<span>,</span> freqs<span>)</span><span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        cos <span>=</span> torch<span>.</span>cos<span>(</span>emb<span>)</span><span>[</span><span>:</span><span>,</span> <span>:</span>self<span>.</span>dim<span>]</span>
        sin <span>=</span> torch<span>.</span>sin<span>(</span>emb<span>)</span><span>[</span><span>:</span><span>,</span> <span>:</span>self<span>.</span>dim<span>]</span>
        
        <span>return</span> cos<span>,</span> sin

<span>def</span> <span>apply_rotary_pos_emb</span><span>(</span>q<span>,</span> k<span>,</span> cos<span>,</span> sin<span>,</span> interleaved<span>=</span><span>False</span><span>)</span><span>:</span>
    <span># Apply rotary embeddings to queries and keys</span>
    batch_size<span>,</span> num_heads<span>,</span> seq_len<span>,</span> head_dim <span>=</span> q<span>.</span>shape
    cos <span>=</span> cos<span>.</span>reshape<span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> seq_len<span>,</span> cos<span>.</span>shape<span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>  <span># [1, 1, seq_len, dim/2]</span>
    sin <span>=</span> sin<span>.</span>reshape<span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> seq_len<span>,</span> sin<span>.</span>shape<span>[</span><span>-</span><span>1</span><span>]</span><span>)</span>  <span># [1, 1, seq_len, dim/2]</span>
    
    <span># Split queries and keys for rotation</span>
    half_dim <span>=</span> head_dim <span>//</span> <span>2</span>
    q1<span>,</span> q2 <span>=</span> q<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> <span>:</span>half_dim<span>]</span><span>,</span> q<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> half_dim<span>:</span><span>]</span>
    k1<span>,</span> k2 <span>=</span> k<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> <span>:</span>half_dim<span>]</span><span>,</span> k<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> half_dim<span>:</span><span>]</span>
    
    <span># Apply rotation using half-dim rotary embeddings</span>
    q_rotated <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>
        q1 <span>*</span> cos <span>-</span> q2 <span>*</span> sin<span>,</span>
        q2 <span>*</span> cos <span>+</span> q1 <span>*</span> sin
    <span>]</span><span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
    
    k_rotated <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>
        k1 <span>*</span> cos <span>-</span> k2 <span>*</span> sin<span>,</span>
        k2 <span>*</span> cos <span>+</span> k1 <span>*</span> sin
    <span>]</span><span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
    
    <span>return</span> q_rotated<span>,</span> k_rotated
</code></pre>
<h2 id="mixture-of-experts" tabindex="-1">Mixture of Experts</h2>
<p>Mixture of Experts (shortened as MoE) is a model architecture designed to significantly increase the parameter count, and thus the potential capacity, of a neural network without incurring a proportionally massive increase in computational cost during inference or training. The core idea is to replace computationally intensive components, like the feed-forward network block in a Transformer, with multiple, smaller &#34;expert&#34; networks. Crucially, not all experts process every input token. Instead, a lightweight &#34;router&#34; or &#34;gating&#34; network dynamically selects a small subset of experts (typically just one or two, known as top-k routing) deemed most suitable for processing each specific input token based on its features. This conditional computation allows MoE models to possess potentially trillions of parameters while only activating a small fraction of them for any given input, maintaining manageable FLOPs compared to a similarly sized dense model.</p>
<p>The router network is the core idea, it acts as a learned decision-maker. It takes the representation of an input token and produces scores or logits indicating the suitability of each available expert for that token. In the example code&#39;s <code>_compute_routing_weights</code> function, these logits are often processed using a top-k function to identify the <code>k</code> experts with the highest scores. The scores for these selected experts are then typically normalized, often using a softmax function, to produce routing weights. These weights determine the contribution of each selected expert to the final output for that token. During training, noise can be added to the router logits (as seen with <code>noise_std</code>) to encourage exploration and prevent the router from collapsing to always favor only a few experts early on.</p>
<p>Once the router selects the top-k experts and calculates their respective weights for a given input token, that token is dispatched only to those chosen experts. Each selected expert network (often a standard FFN, as shown in the <code>experts</code> ModuleList) processes the token independently. The outputs produced by these active experts are then combined to form the final output for that token. This combination is typically a weighted sum, where the output of each selected expert is multiplied by its corresponding routing weight calculated by the router. For instance, if experts <code>i</code> and <code>j</code> are selected with weights <code>w_i</code> and <code>w_j</code>, the final output for token <code>x</code> would be <code>w_i * expert_i(x) + w_j * expert_j(x)</code>. This ensures that the final representation incorporates specialized knowledge from the most relevant experts.</p>
<p>A challenge in training MoE models is ensuring that all experts are utilized effectively; otherwise, the router might learn to consistently overload a few &#34;popular&#34; experts while others remain underdeveloped. To counteract this, an auxiliary load balancing loss is typically incorporated into the training objective, as demonstrated by the <code>_compute_balance_loss</code> method. This loss encourages the router to distribute the computational load (i.e., the input tokens) more evenly across all available experts, often by penalizing imbalances in either the number of tokens assigned to each expert or the sum of routing weights directed towards each expert. By successfully implementing sparse activation via routing and incorporating load balancing, MoE enables the construction of extremely large yet computationally efficient models.</p>
<pre><code><span>class</span> <span>MixtureOfExperts</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> input_dim<span>,</span> hidden_dim<span>,</span> output_dim<span>,</span> num_experts<span>=</span><span>4</span><span>,</span> top_k<span>=</span><span>2</span><span>,</span> noise_std<span>=</span><span>1.0</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>input_dim <span>=</span> input_dim
        self<span>.</span>hidden_dim <span>=</span> hidden_dim
        self<span>.</span>output_dim <span>=</span> output_dim
        self<span>.</span>num_experts <span>=</span> num_experts
        self<span>.</span>top_k <span>=</span> <span>min</span><span>(</span>top_k<span>,</span> num_experts<span>)</span>
        self<span>.</span>noise_std <span>=</span> noise_std
        
        <span># Create experts</span>
        self<span>.</span>experts <span>=</span> nn<span>.</span>ModuleList<span>(</span><span>[</span>
            nn<span>.</span>Sequential<span>(</span>
                nn<span>.</span>Linear<span>(</span>input_dim<span>,</span> hidden_dim<span>)</span><span>,</span>
                nn<span>.</span>ReLU<span>(</span><span>)</span><span>,</span>
                nn<span>.</span>Linear<span>(</span>hidden_dim<span>,</span> output_dim<span>)</span>
            <span>)</span> <span>for</span> _ <span>in</span> <span>range</span><span>(</span>num_experts<span>)</span>
        <span>]</span><span>)</span>
        
        <span># Router network</span>
        self<span>.</span>router <span>=</span> nn<span>.</span>Linear<span>(</span>input_dim<span>,</span> num_experts<span>)</span>
        
        <span># Save expert counts and loads for balancing loss</span>
        self<span>.</span>register_buffer<span>(</span><span>&#39;expert_counts&#39;</span><span>,</span> torch<span>.</span>zeros<span>(</span>num_experts<span>)</span><span>)</span>
        
    <span>def</span> <span>_compute_routing_weights</span><span>(</span>self<span>,</span> x<span>)</span><span>:</span>
        <span># Calculate routing weights</span>
        routing_logits <span>=</span> self<span>.</span>router<span>(</span>x<span>)</span>  <span># [batch_size, seq_len, num_experts]</span>
        
        <span># Add noise during training to encourage exploration</span>
        <span>if</span> self<span>.</span>training <span>and</span> self<span>.</span>noise_std <span>&gt;</span> <span>0</span><span>:</span>
            noise <span>=</span> torch<span>.</span>randn_like<span>(</span>routing_logits<span>)</span> <span>*</span> self<span>.</span>noise_std
            routing_logits <span>=</span> routing_logits <span>+</span> noise
        
        <span># Get top-k experts for each token</span>
        routing_weights<span>,</span> selected_experts <span>=</span> torch<span>.</span>topk<span>(</span>routing_logits<span>,</span> self<span>.</span>top_k<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        
        <span># Normalize the routing weights with softmax</span>
        routing_weights <span>=</span> F<span>.</span>softmax<span>(</span>routing_weights<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
        
        <span>return</span> routing_weights<span>,</span> selected_experts
    
    <span>def</span> <span>_compute_balance_loss</span><span>(</span>self<span>,</span> selected_experts<span>,</span> routing_weights<span>)</span><span>:</span>
        <span># Compute auxiliary load balancing loss</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> selected_experts<span>.</span>shape
        
        <span># Compute probability of each expert being selected across batch</span>
        expert_mask <span>=</span> torch<span>.</span>zeros<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>num_experts<span>,</span> device<span>=</span>selected_experts<span>.</span>device<span>)</span>
        
        <span># For each position in selected_experts, increment the corresponding expert index</span>
        <span>for</span> k <span>in</span> <span>range</span><span>(</span>self<span>.</span>top_k<span>)</span><span>:</span>
            expert_mask<span>.</span>scatter_<span>(</span><span>-</span><span>1</span><span>,</span> selected_experts<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> k<span>:</span>k<span>+</span><span>1</span><span>]</span><span>,</span> routing_weights<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> k<span>:</span>k<span>+</span><span>1</span><span>]</span><span>)</span>
        
        <span># Compute mean routing probability per expert</span>
        expert_routing_probs <span>=</span> expert_mask<span>.</span>mean<span>(</span>dim<span>=</span><span>[</span><span>0</span><span>,</span> <span>1</span><span>]</span><span>)</span>
        
        <span># Compute load balancing loss (all experts should receive equal probability)</span>
        target_probs <span>=</span> torch<span>.</span>ones_like<span>(</span>expert_routing_probs<span>)</span> <span>/</span> self<span>.</span>num_experts
        balance_loss <span>=</span> F<span>.</span>mse_loss<span>(</span>expert_routing_probs<span>,</span> target_probs<span>)</span> <span>*</span> self<span>.</span>num_experts
        
        <span>return</span> balance_loss
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> x<span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> x<span>.</span>shape
        
        <span># Compute routing weights and selected experts</span>
        routing_weights<span>,</span> selected_experts <span>=</span> self<span>.</span>_compute_routing_weights<span>(</span>x<span>)</span>
        
        <span># Prepare output</span>
        output <span>=</span> torch<span>.</span>zeros<span>(</span>batch_size<span>,</span> seq_len<span>,</span> self<span>.</span>output_dim<span>,</span> device<span>=</span>x<span>.</span>device<span>)</span>
        
        <span># Dispatch to selected experts</span>
        <span>for</span> k <span>in</span> <span>range</span><span>(</span>self<span>.</span>top_k<span>)</span><span>:</span>
            <span># Extract the current expert indices and weights</span>
            expert_indices <span>=</span> selected_experts<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> k<span>]</span>  <span># [batch_size, seq_len]</span>
            expert_weights <span>=</span> routing_weights<span>[</span><span>.</span><span>.</span><span>.</span><span>,</span> k<span>]</span><span>.</span>unsqueeze<span>(</span><span>-</span><span>1</span><span>)</span>  <span># [batch_size, seq_len, 1]</span>
            
            <span># Dispatch to each expert</span>
            <span>for</span> expert_idx <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_experts<span>)</span><span>:</span>
                <span># Find tokens assigned to this expert</span>
                mask <span>=</span> <span>(</span>expert_indices <span>==</span> expert_idx<span>)</span>
                
                <span>if</span> mask<span>.</span><span>any</span><span>(</span><span>)</span><span>:</span>
                    <span># Gather input tokens assigned to this expert</span>
                    expert_inputs <span>=</span> x<span>[</span>mask<span>]</span>
                    
                    <span># Process with the expert</span>
                    expert_outputs <span>=</span> self<span>.</span>experts<span>[</span>expert_idx<span>]</span><span>(</span>expert_inputs<span>)</span>
                    
                    <span># Scatter outputs back with appropriate weights</span>
                    output<span>[</span>mask<span>]</span> <span>+=</span> expert_outputs <span>*</span> expert_weights<span>[</span>mask<span>]</span>
                    
                    <span># Update expert counts (for monitoring)</span>
                    self<span>.</span>expert_counts<span>[</span>expert_idx<span>]</span> <span>+=</span> mask<span>.</span><span>sum</span><span>(</span><span>)</span><span>.</span>item<span>(</span><span>)</span>
        
        <span># Compute auxiliary load balancing loss</span>
        balance_loss <span>=</span> self<span>.</span>_compute_balance_loss<span>(</span>selected_experts<span>,</span> routing_weights<span>)</span>
        
        <span># Return output and auxiliary loss</span>
        <span>return</span> output<span>,</span> balance_loss
</code></pre>
<h2 id="learning-rate-warmup" tabindex="-1">Learning Rate Warmup</h2>
<p>Learning rate warmup is a widely adopted heuristic employed during the initial phase of training neural networks to enhance stability and prevent divergence. At the beginning of training, model parameters are typically initialized randomly, often far from an optimal configuration. If a relatively large learning rate is used immediately, the initial gradients, which can also be large and erratic, may cause drastic parameter updates, potentially pushing the model into a poor region of the loss landscape or even causing numerical instability (e.g., loss explosion). Learning rate warmup mitigates this risk by starting the training process with a very small learning rate, which is then gradually increased over a predefined number of initial training steps (the &#34;warmup steps&#34;) until it reaches its target base value, from which point a standard learning rate schedule (like decay) might commence.</p>
<p>The mechanism involves progressively scaling the base learning rate during the warmup phase. A common strategy, illustrated by the <code>LinearWarmupScheduler</code> class, is linear warmup. In this approach, the learning rate \( \eta_t \) at step \( t \) is calculated as \( \eta_t = \eta_{\text{base}} \times \frac{t}{T_{\text{warmup}}} \) for \( t &lt; T_{\text{warmup}} \), where \( \eta_{\text{base}} \) is the target base learning rate and \( T_{\text{warmup}} \) is the total number of warmup steps. As seen in the <code>get_lr</code> method, the scaling factor <code>scale</code> increases linearly from near zero to 1 over the <code>warmup_steps</code>. Once the step count <code>last_epoch</code> reaches <code>warmup_steps</code>, the learning rate becomes equal to the <code>base_lrs</code>, and the warmup phase concludes. This gentle ramp-up allows the model to adapt gradually during the critical early stages when activations and gradients might otherwise be volatile, leading to smoother convergence and often enabling the use of higher base learning rates later in training.</p>
<pre><code><span>class</span> <span>LinearWarmupScheduler</span><span>(</span>_LRScheduler<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> optimizer<span>,</span> warmup_steps<span>,</span> last_epoch<span>=</span><span>-</span><span>1</span><span>)</span><span>:</span>
        self<span>.</span>warmup_steps <span>=</span> warmup_steps
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span>optimizer<span>,</span> last_epoch<span>)</span>
    
    <span>def</span> <span>get_lr</span><span>(</span>self<span>)</span><span>:</span>
        <span>if</span> self<span>.</span>last_epoch <span>&lt;</span> self<span>.</span>warmup_steps<span>:</span>
            <span># During warmup: linearly increase from 0 to base LR</span>
            scale <span>=</span> <span>float</span><span>(</span>self<span>.</span>last_epoch <span>+</span> <span>1</span><span>)</span> <span>/</span> <span>float</span><span>(</span><span>max</span><span>(</span><span>1</span><span>,</span> self<span>.</span>warmup_steps<span>)</span><span>)</span>
            <span>return</span> <span>[</span>base_lr <span>*</span> scale <span>for</span> base_lr <span>in</span> self<span>.</span>base_lrs<span>]</span>
        <span>else</span><span>:</span>
            <span># After warmup: use base learning rate</span>
            <span>return</span> self<span>.</span>base_lrs
</code></pre>
<h2 id="cosine-schedule" tabindex="-1">Cosine Schedule</h2>
<p><strong>Cosine scheduling</strong> (sometimes called <strong>cosine annealing</strong>) is a learning rate schedule technique. Its core principle is to gradually decrease the learning rate over the course of training, following the shape of a cosine curve. Unlike step decay schedules, which reduce the learning rate abruptly at specific epochs, cosine annealing provides a smooth, continuous reduction. Typically, the learning rate starts at an initial high value and decreases following the first half-cycle of a cosine function, reaching a predefined minimum value (often close to zero) by the final training step. This smooth decay has been shown empirically to help the optimization process by allowing larger steps early in training for broad exploration of the loss landscape, and progressively smaller steps later on for fine-tuning and convergence towards a good minimum.</p>
<p>Early in training, a higher learning rate encourages faster exploration and helps escape poor local minima. As training progresses and the model parameters approach a more optimal region, reducing the learning rate becomes crucial to avoid overshooting the minimum and to allow for more precise convergence. The cosine function provides a schedule that starts with a relatively slow decay rate, allowing the optimizer to maintain momentum initially. The decay rate then accelerates towards the middle of the schedule before slowing down again as it approaches the minimum learning rate. This final slow-down phase near the end of training is particularly important, as it allows the optimizer to carefully settle into a potentially flat minimum, which empirical evidence suggests often correlates with better generalization performance compared to sharper minima.</p>
<p>Furthermore, cosine scheduling is often combined with a &#34;warmup&#34; phase, as seen in the code example. During this initial phase (e.g., <code>warmup_steps</code>), the learning rate is typically increased linearly from a very small value (or zero) up to the main initial learning rate. This warmup period helps stabilize training in the very beginning, especially for large models or datasets where large initial learning rates applied to randomly initialized weights could lead to instability or divergence. After the warmup, the cosine decay phase begins, smoothly decreasing the learning rate from its peak value down to the target minimum (<code>base_lr * min_lr_ratio</code>) over the remaining <code>total_steps - warmup_steps</code>. This combination of a gentle start (warmup) followed by a smooth, theoretically motivated decay (cosine annealing) provides a robust and effective learning rate strategy that often requires less hyperparameter tuning than step-based schedules and frequently leads to improved model accuracy.</p>
<p>There is a <a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">CosineScheduler</a> in PyTorch that implements this technique. Let&#39;s look at a simplified version of it:</p>
<pre><code><span>class</span> <span>CosineAnnealingWarmupScheduler</span><span>(</span>_LRScheduler<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> optimizer<span>,</span> warmup_steps<span>,</span> total_steps<span>,</span> min_lr_ratio<span>=</span><span>1e-4</span><span>,</span> last_epoch<span>=</span><span>-</span><span>1</span><span>)</span><span>:</span>
        self<span>.</span>warmup_steps <span>=</span> warmup_steps
        self<span>.</span>total_steps <span>=</span> total_steps
        self<span>.</span>min_lr_ratio <span>=</span> min_lr_ratio
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span>optimizer<span>,</span> last_epoch<span>)</span>
    
    <span>def</span> <span>get_lr</span><span>(</span>self<span>)</span><span>:</span>
        <span>if</span> self<span>.</span>last_epoch <span>&lt;</span> self<span>.</span>warmup_steps<span>:</span>
            <span># During warmup: linearly increase from 0 to base LR</span>
            scale <span>=</span> <span>float</span><span>(</span>self<span>.</span>last_epoch <span>+</span> <span>1</span><span>)</span> <span>/</span> <span>float</span><span>(</span><span>max</span><span>(</span><span>1</span><span>,</span> self<span>.</span>warmup_steps<span>)</span><span>)</span>
            <span>return</span> <span>[</span>base_lr <span>*</span> scale <span>for</span> base_lr <span>in</span> self<span>.</span>base_lrs<span>]</span>
        <span>else</span><span>:</span>
            <span># After warmup: cosine decay from base LR to min_lr</span>
            progress <span>=</span> <span>float</span><span>(</span>self<span>.</span>last_epoch <span>-</span> self<span>.</span>warmup_steps<span>)</span> <span>/</span> <span>float</span><span>(</span>
                <span>max</span><span>(</span><span>1</span><span>,</span> self<span>.</span>total_steps <span>-</span> self<span>.</span>warmup_steps<span>)</span>
            <span>)</span>
            <span># Cosine decay formula: min_lr + 0.5 * (base_lr - min_lr) * (1 + cos(pi * progress))</span>
            scale <span>=</span> self<span>.</span>min_lr_ratio <span>+</span> <span>0.5</span> <span>*</span> <span>(</span><span>1.0</span> <span>-</span> self<span>.</span>min_lr_ratio<span>)</span> <span>*</span> <span>(</span>
                <span>1.0</span> <span>+</span> math<span>.</span>cos<span>(</span>math<span>.</span>pi <span>*</span> progress<span>)</span>
            <span>)</span>
            <span>return</span> <span>[</span>base_lr <span>*</span> scale <span>for</span> base_lr <span>in</span> self<span>.</span>base_lrs<span>]</span>
</code></pre>
<h2 id="adamw-optimizer" tabindex="-1">AdamW Optimizer</h2>
<p><strong>AdamW</strong> (Adam with Decoupled Weight Decay) addresses a subtle issue in the standard implementation of weight decay (L2 regularization) within adaptive optimizers like Adam. This improved optimizer gives us a method where weight decay does not accumulate in the momentum nor variance. In traditional Adam, L2 regularization is often implemented by adding the decay term (\(\lambda \cdot \text{weight}\)) directly to the gradient before computing the moving averages (\(m_t\) and \(v_t\)). However, this couples the weight decay effect with the adaptive learning rate derived from the gradient moments. Consequently, parameters with historically large gradients (and thus larger \(v_t\) values) experience smaller effective weight decay, contrary to the goal of applying uniform regularization pressure. AdamW decouples these processes: it performs the standard Adam updates based purely on the gradients and separately applies the weight decay step directly to the weights, effectively restoring the original behavior of L2 regularization where weights decay proportionally to their magnitude, independent of their gradient history.</p>
<p>The core update mechanism of AdamW largely follows the standard Adam procedure but modifies how weight decay is applied. At each step \(t\) for a parameter \(\theta\), it first calculates the biased first moment estimate \(m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t\) and the biased second raw moment estimate \(v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2\), where \(g_t\) is the gradient at step \(t\), and \(\beta_1\), \(\beta_2\) are exponential decay rates. These are then bias-corrected: \(\hat{m}_t = m_t / (1 - \beta_1^t)\) and \(\hat{v}_t = v_t / (1 - \beta_2^t)\). The crucial difference lies in the update rule. Instead of incorporating weight decay into \(g_t\), AdamW first applies weight decay directly to the parameters: \(\theta_{t-1}&#39; = \theta_{t-1} \cdot (1 - \text{lr} \cdot \lambda)\), where \(\text{lr}\) is the learning rate and \(\lambda\) is the weight_decay factor (as seen in the line <code>p.data.mul_(1 - group[&#39;lr&#39;] * group[&#39;weight_decay&#39;])</code>). Then, the standard Adam update using the bias-corrected moments is applied to these decayed weights: \(\theta_t = \theta_{t-1}&#39; - \text{lr} \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)\). This corresponds to the code line <code>p.data.addcdiv_(exp_avg, denom, value=-step_size)</code>, operating on the already decayed <code>p.data</code>.</p>
<p>In deep learning practice, AdamW is employed similarly to Adam. It is instantiated by providing the model&#39;s parameters and key hyperparameters like the learning rate (\(\text{lr}\)), beta values (\(\beta_1\), \(\beta_2\)), epsilon (\(\epsilon\)), and the weight decay factor (\(\lambda\)). The <code>step()</code> method, typically called within the training loop after computing gradients (<code>loss.backward()</code>), executes the update logic described above for each parameter. Its primary advantage is improved generalization, particularly observed in training large models like Transformers where regularization is critical. By ensuring that the weight decay strength is independent of the adaptive learning rate scaling, AdamW often allows for better hyperparameter tuning (especially \(\text{lr}\) and \(\lambda\)) and can lead to models that perform better on unseen data compared to standard Adam with L2 regularization.</p>
<p>There is a highly optimized implementation of <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html">AdamW</a> in PyTorch. Let&#39;s look at a simplified version of it, the example below demonstrates a minimal PyTorch implementation, initializing state variables (like <code>exp_avg</code>, <code>exp_avg_sq</code>) and performing the decoupled weight decay and moment-based updates within the parameter loop.</p>
<pre><code><span>class</span> <span>AdamW</span><span>(</span>Optimizer<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> params<span>,</span> lr<span>=</span><span>1e-3</span><span>,</span> betas<span>=</span><span>(</span><span>0.9</span><span>,</span> <span>0.999</span><span>)</span><span>,</span> eps<span>=</span><span>1e-8</span><span>,</span>
                 weight_decay<span>=</span><span>1e-2</span><span>,</span> amsgrad<span>=</span><span>False</span><span>)</span><span>:</span>
        defaults <span>=</span> <span>dict</span><span>(</span>lr<span>=</span>lr<span>,</span> betas<span>=</span>betas<span>,</span> eps<span>=</span>eps<span>,</span>
                        weight_decay<span>=</span>weight_decay<span>,</span> amsgrad<span>=</span>amsgrad<span>)</span>
        <span>super</span><span>(</span>AdamW<span>,</span> self<span>)</span><span>.</span>__init__<span>(</span>params<span>,</span> defaults<span>)</span>
    
    <span>def</span> <span>step</span><span>(</span>self<span>,</span> closure<span>=</span><span>None</span><span>)</span><span>:</span>
        loss <span>=</span> <span>None</span>
        <span>if</span> closure <span>is</span> <span>not</span> <span>None</span><span>:</span>
            loss <span>=</span> closure<span>(</span><span>)</span>
        
        <span>for</span> group <span>in</span> self<span>.</span>param_groups<span>:</span>
            <span>for</span> p <span>in</span> group<span>[</span><span>&#39;params&#39;</span><span>]</span><span>:</span>
                <span>if</span> p<span>.</span>grad <span>is</span> <span>None</span><span>:</span>
                    <span>continue</span>
                
                <span># Get gradient</span>
                grad <span>=</span> p<span>.</span>grad<span>.</span>data
                
                <span>if</span> grad<span>.</span>is_sparse<span>:</span>
                    <span>raise</span> RuntimeError<span>(</span><span>&#39;AdamW does not support sparse gradients&#39;</span><span>)</span>
                
                amsgrad <span>=</span> group<span>[</span><span>&#39;amsgrad&#39;</span><span>]</span>
                state <span>=</span> self<span>.</span>state<span>[</span>p<span>]</span>
                
                <span># State initialization</span>
                <span>if</span> <span>len</span><span>(</span>state<span>)</span> <span>==</span> <span>0</span><span>:</span>
                    state<span>[</span><span>&#39;step&#39;</span><span>]</span> <span>=</span> <span>0</span>
                    <span># Exponential moving average of gradient values</span>
                    state<span>[</span><span>&#39;exp_avg&#39;</span><span>]</span> <span>=</span> torch<span>.</span>zeros_like<span>(</span>p<span>.</span>data<span>)</span>
                    <span># Exponential moving average of squared gradient values</span>
                    state<span>[</span><span>&#39;exp_avg_sq&#39;</span><span>]</span> <span>=</span> torch<span>.</span>zeros_like<span>(</span>p<span>.</span>data<span>)</span>
                    <span>if</span> amsgrad<span>:</span>
                        <span># Maintains max of all exp. moving avg. of sq. grad. values</span>
                        state<span>[</span><span>&#39;max_exp_avg_sq&#39;</span><span>]</span> <span>=</span> torch<span>.</span>zeros_like<span>(</span>p<span>.</span>data<span>)</span>
                
                exp_avg<span>,</span> exp_avg_sq <span>=</span> state<span>[</span><span>&#39;exp_avg&#39;</span><span>]</span><span>,</span> state<span>[</span><span>&#39;exp_avg_sq&#39;</span><span>]</span>
                <span>if</span> amsgrad<span>:</span>
                    max_exp_avg_sq <span>=</span> state<span>[</span><span>&#39;max_exp_avg_sq&#39;</span><span>]</span>
                
                beta1<span>,</span> beta2 <span>=</span> group<span>[</span><span>&#39;betas&#39;</span><span>]</span>
                
                state<span>[</span><span>&#39;step&#39;</span><span>]</span> <span>+=</span> <span>1</span>
                
                <span># Decay the first and second moment running average coefficient</span>
                exp_avg<span>.</span>mul_<span>(</span>beta1<span>)</span><span>.</span>add_<span>(</span>grad<span>,</span> alpha<span>=</span><span>1</span> <span>-</span> beta1<span>)</span>
                exp_avg_sq<span>.</span>mul_<span>(</span>beta2<span>)</span><span>.</span>addcmul_<span>(</span>grad<span>,</span> grad<span>,</span> value<span>=</span><span>1</span> <span>-</span> beta2<span>)</span>
                
                <span>if</span> amsgrad<span>:</span>
                    <span># Maintains the maximum of all 2nd moment running avg. till now</span>
                    torch<span>.</span>maximum<span>(</span>max_exp_avg_sq<span>,</span> exp_avg_sq<span>,</span> out<span>=</span>max_exp_avg_sq<span>)</span>
                    <span># Use the max. for normalizing running avg. of gradient</span>
                    denom <span>=</span> max_exp_avg_sq<span>.</span>sqrt<span>(</span><span>)</span><span>.</span>add_<span>(</span>group<span>[</span><span>&#39;eps&#39;</span><span>]</span><span>)</span>
                <span>else</span><span>:</span>
                    denom <span>=</span> exp_avg_sq<span>.</span>sqrt<span>(</span><span>)</span><span>.</span>add_<span>(</span>group<span>[</span><span>&#39;eps&#39;</span><span>]</span><span>)</span>
                
                bias_correction1 <span>=</span> <span>1</span> <span>-</span> beta1 <span>**</span> state<span>[</span><span>&#39;step&#39;</span><span>]</span>
                bias_correction2 <span>=</span> <span>1</span> <span>-</span> beta2 <span>**</span> state<span>[</span><span>&#39;step&#39;</span><span>]</span>
                step_size <span>=</span> group<span>[</span><span>&#39;lr&#39;</span><span>]</span> <span>*</span> math<span>.</span>sqrt<span>(</span>bias_correction2<span>)</span> <span>/</span> bias_correction1
                
                <span># Apply weight decay BEFORE the optimization step</span>
                <span>if</span> group<span>[</span><span>&#39;weight_decay&#39;</span><span>]</span> <span>!=</span> <span>0</span><span>:</span>
                    p<span>.</span>data<span>.</span>mul_<span>(</span><span>1</span> <span>-</span> group<span>[</span><span>&#39;lr&#39;</span><span>]</span> <span>*</span> group<span>[</span><span>&#39;weight_decay&#39;</span><span>]</span><span>)</span>
                
                <span># Update parameters</span>
                p<span>.</span>data<span>.</span>addcdiv_<span>(</span>exp_avg<span>,</span> denom<span>,</span> value<span>=</span><span>-</span>step_size<span>)</span>
        
        <span>return</span> loss
</code></pre>
<h2 id="multi-token-prediction" tabindex="-1">Multi-token Prediction</h2>
<p>Multi-token prediction is a technique developed to accelerate the inference speed of autoregressive language models. Normally, autoregressive generation predicts tokens one by one: the model takes a sequence, predicts the single most likely next token, appends it to the sequence, and repeats the process. This sequential nature, requiring one full forward pass of the model for each generated token, becomes a significant bottleneck for latency-sensitive applications. Multi-token prediction attempts to overcome this by modifying the model&#39;s prediction head to output probabilities for multiple future tokens simultaneously based on the current hidden state, thereby reducing the number of forward passes needed to generate a sequence of a given length.</p>
<p>The implementation typically involves adapting the final layer(s) of the language model. Instead of a single output layer (or &#34;language model head&#34;) mapping the final hidden state to logits over the vocabulary for the next token, a multi-token predictor might employ several strategies. One approach, as shown in the example class using separate heads, is to have multiple distinct prediction heads, each trained to predict a token at a different future offset (e.g., one head for token \(t+1\), another for \(t+2\), up to \(t+N\)). These heads usually take the same final hidden state (e.g., corresponding to token \(t\)) as input but learn different projections specialized for their respective time steps. Another approach involves using a single shared prediction head, which might require more complex mechanisms, potentially involving learned transformations of the hidden state or incorporating positional information, to generate distinct probability distributions for each of the \(N\) future tokens from essentially the same starting representation.</p>
<p>Training a multi-token predictor involves teaching the model to correctly anticipate the sequence of \(N\) subsequent tokens given the preceding context. During the training phase, as illustrated in the <code>compute_loss</code> method, the model receives input sequences and its predictions for the next \(N\) tokens are compared against the actual \(N\) target tokens in the training data. A loss function, usually cross-entropy, is calculated for each predicted position (\(t+1\) to \(t+N\)) and then aggregated (e.g., averaged) to form the final loss signal used for backpropagation.</p>
<p>While this can show speed improvements, multi-token prediction has several drawbacks: the accuracy of predicting tokens further into the future tends to decrease, and the chosen sequence of \(N\) tokens might diverge from the optimal path that would have been taken by single-token generation. Therefore, it often represents a trade-off between generation speed and potential quality degradation that is very use-case dependent.</p>
<pre><code><span>class</span> <span>MultiTokenPredictor</span><span>(</span>nn<span>.</span>Module<span>)</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> hidden_dim<span>,</span> vocab_size<span>,</span> num_predicted_tokens<span>=</span><span>2</span><span>,</span> shared_prediction_head<span>=</span><span>False</span><span>)</span><span>:</span>
        <span>super</span><span>(</span><span>)</span><span>.</span>__init__<span>(</span><span>)</span>
        self<span>.</span>hidden_dim <span>=</span> hidden_dim
        self<span>.</span>vocab_size <span>=</span> vocab_size
        self<span>.</span>num_predicted_tokens <span>=</span> num_predicted_tokens
        self<span>.</span>shared_prediction_head <span>=</span> shared_prediction_head
        
        <span>if</span> shared_prediction_head<span>:</span>
            <span># Share the same prediction head for all positions</span>
            self<span>.</span>lm_head <span>=</span> nn<span>.</span>Linear<span>(</span>hidden_dim<span>,</span> vocab_size<span>)</span>
        <span>else</span><span>:</span>
            <span># Use separate prediction heads for each position</span>
            self<span>.</span>lm_heads <span>=</span> nn<span>.</span>ModuleList<span>(</span><span>[</span>
                nn<span>.</span>Linear<span>(</span>hidden_dim<span>,</span> vocab_size<span>)</span> 
                <span>for</span> _ <span>in</span> <span>range</span><span>(</span>num_predicted_tokens<span>)</span>
            <span>]</span><span>)</span>
    
    <span>def</span> <span>forward</span><span>(</span>self<span>,</span> hidden_states<span>)</span><span>:</span>
        batch_size<span>,</span> seq_len<span>,</span> _ <span>=</span> hidden_states<span>.</span>shape
        
        <span># Get the hidden states for the last token</span>
        last_hidden <span>=</span> hidden_states<span>[</span><span>:</span><span>,</span> <span>-</span><span>1</span><span>]</span>
        
        <span>if</span> self<span>.</span>shared_prediction_head<span>:</span>
            <span># Use the shared head for all predictions</span>
            logits <span>=</span> self<span>.</span>lm_head<span>(</span>last_hidden<span>)</span>
            next_token_logits <span>=</span> logits<span>.</span>unsqueeze<span>(</span><span>1</span><span>)</span>
            
            <span># For positions beyond the first, we need to make a projection</span>
            multi_token_logits <span>=</span> <span>[</span><span>]</span>
            <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_predicted_tokens<span>)</span><span>:</span>
                <span>if</span> i <span>==</span> <span>0</span><span>:</span>
                    multi_token_logits<span>.</span>append<span>(</span>logits<span>)</span>
                <span>else</span><span>:</span>
                    <span># A simple projection for demonstration; in practice, this would be more complex</span>
                    projected_hidden <span>=</span> last_hidden <span>+</span> i <span>*</span> <span>0.1</span>  <span># Just a dummy transformation</span>
                    multi_token_logits<span>.</span>append<span>(</span>self<span>.</span>lm_head<span>(</span>projected_hidden<span>)</span><span>)</span>
            
            multi_token_logits <span>=</span> torch<span>.</span>stack<span>(</span>multi_token_logits<span>,</span> dim<span>=</span><span>1</span><span>)</span>
        <span>else</span><span>:</span>
            <span># Use separate heads for each position</span>
            multi_token_logits <span>=</span> torch<span>.</span>stack<span>(</span><span>[</span>
                head<span>(</span>last_hidden<span>)</span> <span>for</span> head <span>in</span> self<span>.</span>lm_heads
            <span>]</span><span>,</span> dim<span>=</span><span>1</span><span>)</span>
            
            <span># The first position is used for next-token prediction</span>
            next_token_logits <span>=</span> multi_token_logits<span>[</span><span>:</span><span>,</span> <span>0</span><span>:</span><span>1</span><span>]</span>
        
        <span>return</span> next_token_logits<span>,</span> multi_token_logits
    
    <span>def</span> <span>compute_loss</span><span>(</span>self<span>,</span> hidden_states<span>,</span> labels<span>,</span> ignore_index<span>=</span><span>-</span><span>100</span><span>)</span><span>:</span>
        <span># Get predictions</span>
        _<span>,</span> multi_token_logits <span>=</span> self<span>.</span>forward<span>(</span>hidden_states<span>)</span>
        
        <span># Prepare targets: shift labels to align with predictions</span>
        <span># We need the next num_predicted_tokens tokens after the input</span>
        shifted_labels <span>=</span> labels<span>[</span><span>:</span><span>,</span> <span>:</span><span>-</span>self<span>.</span>num_predicted_tokens<span>]</span>
        targets <span>=</span> <span>[</span><span>]</span>
        
        <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_predicted_tokens<span>)</span><span>:</span>
            targets<span>.</span>append<span>(</span>labels<span>[</span><span>:</span><span>,</span> <span>1</span><span>+</span>i<span>:</span><span>1</span><span>+</span>i<span>-</span>self<span>.</span>num_predicted_tokens<span>+</span><span>1</span><span>]</span><span>)</span>
        
        <span># Stack targets</span>
        stacked_targets <span>=</span> torch<span>.</span>stack<span>(</span>targets<span>,</span> dim<span>=</span><span>1</span><span>)</span>
        
        <span># Compute loss across all predicted positions</span>
        loss <span>=</span> <span>0</span>
        <span>for</span> i <span>in</span> <span>range</span><span>(</span>self<span>.</span>num_predicted_tokens<span>)</span><span>:</span>
            loss <span>+=</span> F<span>.</span>cross_entropy<span>(</span>
                multi_token_logits<span>[</span><span>:</span><span>,</span> i<span>]</span><span>.</span>view<span>(</span><span>-</span><span>1</span><span>,</span> self<span>.</span>vocab_size<span>)</span><span>,</span>
                stacked_targets<span>[</span><span>:</span><span>,</span> i<span>]</span><span>.</span>reshape<span>(</span><span>-</span><span>1</span><span>)</span><span>,</span>
                ignore_index<span>=</span>ignore_index
            <span>)</span>
        
        <span>return</span> loss <span>/</span> self<span>.</span>num_predicted_tokens
</code></pre>
<h2 id="speculative-decoding" tabindex="-1">Speculative Decoding</h2>
<p>Speculative decoding is an clever technique designed to accelerate the inference process of large autoregressive language models, significantly reducing the wall-clock time required to generate text. Standard generation is bottlenecked by its sequential nature, where the computationally expensive large model (the &#34;target&#34; model) must perform a full forward pass to predict just one token at a time. Speculative decoding introduces a secondary, much smaller and faster &#34;draft&#34; model. This draft model rapidly generates a short sequence of candidate future tokens (a &#34;draft&#34;). The core idea is then to use the large target model to verify this entire draft sequence in a single, parallel forward pass, potentially accepting multiple tokens at once and thus amortizing the cost of the target model&#39;s computation over several generated tokens.</p>
<p>The mechanism hinges on comparing the predictions of the draft model with those of the target model. After the draft model proposes a sequence of \( k \) tokens, \( d_1, d_2, \dots, d_k \), the target model is run once on the original input sequence concatenated with the draft. This single pass yields the target model&#39;s probability distributions for the <em>next</em> token at <em>each</em> position within the draft sequence. For each draft token \( d_i \), its validity is checked against the target model&#39;s prediction at that same position. If the target model strongly agrees with the draft token (e.g., it would have also predicted \( d_i \) with high probability, or based on a specific acceptance rule comparing \( P_{\text{target}}(d_i) \) and \( P_{\text{draft}}(d_i) \)), the token is accepted. This verification proceeds sequentially through the draft.</p>
<p>The process continues until either all \( k \) draft tokens are accepted, or a draft token \( d_j \) is rejected. If rejection occurs at position \( j \), all preceding accepted draft tokens (\( d_1, \dots, d_{j-1} \)) are kept. The remaining part of the draft (\( d_j, \dots, d_k \)) is discarded. Crucially, the target model&#39;s probability distribution calculated at position \( j \) can then be used to sample a <em>corrected</em> token to append after the accepted sequence, ensuring the overall generation statistically follows the distribution of the more powerful target model. This corrected token, along with the previously accepted ones, forms the input for the next round of draft generation. By accepting multiple tokens per target model inference step on average, speculative decoding can achieve substantial speedups (e.g., 2-3x) with minimal impact on the quality of the generated text.</p>
<pre><code><span>class</span> <span>SimpleSpeculativeDecoding</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span>self<span>,</span> target_model<span>,</span> draft_model<span>,</span> tokenizer<span>,</span> max_draft_tokens<span>=</span><span>5</span><span>)</span><span>:</span>
        self<span>.</span>target_model <span>=</span> target_model
        self<span>.</span>draft_model <span>=</span> draft_model
        self<span>.</span>tokenizer <span>=</span> tokenizer
        self<span>.</span>max_draft_tokens <span>=</span> max_draft_tokens
    
    <span>def</span> <span>generate</span><span>(</span>self<span>,</span> prompt<span>,</span> max_length<span>=</span><span>100</span><span>)</span><span>:</span>
        <span># Start with the prompt&#39;s token IDs</span>
        input_ids <span>=</span> self<span>.</span>tokenizer<span>.</span>encode<span>(</span>prompt<span>,</span> return_tensors<span>=</span><span>&#34;pt&#34;</span><span>)</span>
        
        <span># Keep generating until we reach max_length or an end token</span>
        <span>while</span> input_ids<span>.</span>shape<span>[</span><span>1</span><span>]</span> <span>&lt;</span> max_length<span>:</span>
            <span># Step 1: Generate multiple draft tokens</span>
            draft_input_ids <span>=</span> input_ids<span>.</span>clone<span>(</span><span>)</span>
            draft_tokens <span>=</span> <span>[</span><span>]</span>
            
            <span>with</span> torch<span>.</span>no_grad<span>(</span><span>)</span><span>:</span>
                <span>for</span> _ <span>in</span> <span>range</span><span>(</span>self<span>.</span>max_draft_tokens<span>)</span><span>:</span>
                    <span># Generate next token with draft model</span>
                    outputs <span>=</span> self<span>.</span>draft_model<span>(</span>draft_input_ids<span>)</span>
                    next_token_logits <span>=</span> outputs<span>.</span>logits<span>[</span><span>:</span><span>,</span> <span>-</span><span>1</span><span>,</span> <span>:</span><span>]</span>
                    next_token <span>=</span> torch<span>.</span>argmax<span>(</span>next_token_logits<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
                    
                    <span># Save the draft token</span>
                    draft_tokens<span>.</span>append<span>(</span>next_token<span>.</span>item<span>(</span><span>)</span><span>)</span>
                    
                    <span># Add to draft input</span>
                    draft_input_ids <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>draft_input_ids<span>,</span> next_token<span>.</span>unsqueeze<span>(</span><span>0</span><span>)</span><span>]</span><span>,</span> dim<span>=</span><span>1</span><span>)</span>
                    
                    <span># Check for end token</span>
                    <span>if</span> next_token<span>.</span>item<span>(</span><span>)</span> <span>==</span> self<span>.</span>tokenizer<span>.</span>eos_token_id<span>:</span>
                        <span>break</span>
            
            <span># Step 2: Verify with target model</span>
            <span>with</span> torch<span>.</span>no_grad<span>(</span><span>)</span><span>:</span>
                <span># Get target model probabilities for all tokens including drafts</span>
                verification_ids <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>input_ids<span>,</span> torch<span>.</span>tensor<span>(</span><span>[</span>draft_tokens<span>]</span><span>)</span><span>.</span>to<span>(</span>input_ids<span>.</span>device<span>)</span><span>]</span><span>,</span> dim<span>=</span><span>1</span><span>)</span>
                target_outputs <span>=</span> self<span>.</span>target_model<span>(</span>verification_ids<span>)</span>
                target_logits <span>=</span> target_outputs<span>.</span>logits<span>[</span><span>:</span><span>,</span> input_ids<span>.</span>shape<span>[</span><span>1</span><span>]</span><span>-</span><span>1</span><span>:</span><span>]</span>
                
                <span># Convert to probabilities</span>
                target_probs <span>=</span> F<span>.</span>softmax<span>(</span>target_logits<span>,</span> dim<span>=</span><span>-</span><span>1</span><span>)</span>
                
                <span># Accept tokens until mismatch or all drafts accepted</span>
                accepted_tokens <span>=</span> <span>[</span><span>]</span>
                <span>for</span> i<span>,</span> token_id <span>in</span> <span>enumerate</span><span>(</span>draft_tokens<span>)</span><span>:</span>
                    <span># Check if target model agrees with draft</span>
                    target_prob <span>=</span> target_probs<span>[</span><span>0</span><span>,</span> i<span>,</span> token_id<span>]</span><span>.</span>item<span>(</span><span>)</span>
                    draft_prob <span>=</span> <span>1.0</span>  <span># Draft chose this with highest probability</span>
                    
                    <span># Accept based on probability ratio (simplified)</span>
                    <span>if</span> random<span>.</span>random<span>(</span><span>)</span> <span>&lt;</span> <span>min</span><span>(</span><span>1.0</span><span>,</span> target_prob <span>/</span> draft_prob<span>)</span><span>:</span>
                        accepted_tokens<span>.</span>append<span>(</span>token_id<span>)</span>
                    <span>else</span><span>:</span>
                        <span># Rejection: get new token from target model</span>
                        new_token <span>=</span> torch<span>.</span>argmax<span>(</span>target_logits<span>[</span><span>0</span><span>,</span> i<span>]</span><span>)</span><span>.</span>item<span>(</span><span>)</span>
                        accepted_tokens<span>.</span>append<span>(</span>new_token<span>)</span>
                        <span>break</span>
            
            <span># Add accepted tokens to input_ids</span>
            input_ids <span>=</span> torch<span>.</span>cat<span>(</span><span>[</span>input_ids<span>,</span> torch<span>.</span>tensor<span>(</span><span>[</span>accepted_tokens<span>]</span><span>)</span><span>.</span>to<span>(</span>input_ids<span>.</span>device<span>)</span><span>]</span><span>,</span> dim<span>=</span><span>1</span><span>)</span>
            
            <span># Check for end token</span>
            <span>if</span> input_ids<span>[</span><span>0</span><span>,</span> <span>-</span><span>1</span><span>]</span><span>.</span>item<span>(</span><span>)</span> <span>==</span> self<span>.</span>tokenizer<span>.</span>eos_token_id<span>:</span>
                <span>break</span>
        
        <span># Decode the generated tokens</span>
        <span>return</span> self<span>.</span>tokenizer<span>.</span>decode<span>(</span>input_ids<span>[</span><span>0</span><span>]</span><span>)</span>
</code></pre>

          </div></div>
  </body>
</html>
