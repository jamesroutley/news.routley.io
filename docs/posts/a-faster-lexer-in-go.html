<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eli.thegreenplace.net/2022/a-faster-lexer-in-go/">Original</a>
    <h1>A Faster Lexer in Go</h1>
    
    <div id="readability-page-1" class="page"><div>
                
                <p>It&#39;s been a while since I&#39;ve last <a href="https://eli.thegreenplace.net/2014/03/27/rewriting-the-lexer-benchmark-in-go">rewritten my favorite lexical analyzer</a>
:-) That post is the last in a series implementing a lexer for the <a href="https://llvm.org/docs/TableGen/">TableGen
language</a> in a variety of programming
languages, using multiple techniques. The last lexer written, in Go, was very
fast indeed - processing 1 MiB of source in about 20 milliseconds.</p>
<p>The other day I started wondering whether Go compiler improvements from the last
few years made this code run any faster. Back in 2014 I measured it with Go 1.2,
and now Go 1.18 is out. So I tried (on the same machine) with some newer Go
versions; the full code for this is <a href="https://github.com/eliben/code-for-blog/tree/master/2014/tablegen-lexer-go">still here</a>,
and the benchmark is run as follows <a href="#id2" id="id1">[1]</a>:</p>
<div><pre><span></span>$ TDINPUT=input.td go test -bench=Preall -benchtime=5s
</pre></div>
<p>These are the results:</p>
<p><img alt="Benchmark results for different Go versions" src="https://eli.thegreenplace.net/images/2022/lexer-benchmark-per-go-version.png"/></p><p>Go 1.5 is comparable to 1.2, but by 1.10 there was a significant improvement
in performance, and a further improvement in later versions. The code produced
by Go 1.18 is more than twice as fast as the original lexer. Now it takes only
~9.6 ms to process the same 1 MiB of TableGen source!</p>
<div id="looking-deeper-into-the-performance-of-this-lexer">
<h2>Looking deeper into the performance of this lexer</h2>
<p>This got me curious - what <em>does</em> the lexer spend its time on? Since Go has
fantastic tooling for performance profiling, time to whip up some flags... This
will be for the most recent version of Go (1.18):</p>
<div><pre><span></span>$ TDINPUT=input.td go test -cpuprofile cpu.out -bench=Preall -benchtime=5s
...

# --nodefraction tells pprof to ignore nodes that take less than 5% of the
# total time - this significantly reduces the clutter in the produced graph

$ go tool pprof --nodefraction=0.05 ./example.com.test cpu.out
...
(pprof) web
</pre></div>
<p>Here&#39;s the important part of the profile:</p>
<p><img alt="pprof CPU profile for the lexer" src="https://eli.thegreenplace.net/images/2022/lexer-pprof-cpu.png"/></p><p>As expected, the <tt>next</tt> function is very heavy in the profile, as it should
be, since this is the main code taking characters from the input stream and
making them ready for the lexer to process:</p>
<div><pre><span></span><span>func</span> <span>(</span><span>lex</span> <span>*</span><span>Lexer</span><span>)</span> <span>next</span><span>()</span> <span>{</span>
  <span>if</span> <span>lex</span><span>.</span><span>nextpos</span> <span>&lt;</span> <span>len</span><span>(</span><span>lex</span><span>.</span><span>buf</span><span>)</span> <span>{</span>
    <span>lex</span><span>.</span><span>rpos</span> <span>=</span> <span>lex</span><span>.</span><span>nextpos</span>

    <span>// r is the current rune, w is its width. We start by assuming the</span>
    <span>// common case - that the current rune is ASCII (and thus has width=1).</span>
    <span>r</span><span>,</span> <span>w</span> <span>:=</span> <span>rune</span><span>(</span><span>lex</span><span>.</span><span>buf</span><span>[</span><span>lex</span><span>.</span><span>nextpos</span><span>]),</span> <span>1</span>

    <span>if</span> <span>r</span> <span>&gt;=</span> <span>utf8</span><span>.</span><span>RuneSelf</span> <span>{</span>
      <span>// The current rune is not actually ASCII, so we have to decode it</span>
      <span>// properly.</span>
      <span>r</span><span>,</span> <span>w</span> <span>=</span> <span>utf8</span><span>.</span><span>DecodeRune</span><span>(</span><span>lex</span><span>.</span><span>buf</span><span>[</span><span>lex</span><span>.</span><span>nextpos</span><span>:])</span>
    <span>}</span>

    <span>lex</span><span>.</span><span>nextpos</span> <span>+=</span> <span>w</span>
    <span>lex</span><span>.</span><span>r</span> <span>=</span> <span>r</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>lex</span><span>.</span><span>rpos</span> <span>=</span> <span>len</span><span>(</span><span>lex</span><span>.</span><span>buf</span><span>)</span>
    <span>lex</span><span>.</span><span>r</span> <span>=</span> <span>-</span><span>1</span> <span>// EOF</span>
  <span>}</span>
<span>}</span>
</pre></div>
<p>Assuming that most characters in the stream are within ASCII bounds, what this
function does on each call is very little. FWIW, it&#39;s very similar to how the
inner loop of Go&#39;s own <tt>text/scanner</tt> package works.</p>
<p>But notice the 16.5% spent on <tt>slicebytetostring</tt> - what is that all
about? Note that it&#39;s invoked from several <tt>scan*</tt> methods. This has to be
the <tt>string</tt> conversion of buffer slices, e.g. from <tt>scanIdentifier</tt>:</p>
<div><pre><span></span><span>return</span> <span>Token</span><span>{</span><span>IDENTIFIER</span><span>,</span> <span>string</span><span>(</span><span>lex</span><span>.</span><span>buf</span><span>[</span><span>startpos</span><span>:</span><span>lex</span><span>.</span><span>rpos</span><span>]),</span> <span>startpos</span><span>}</span>
</pre></div>
<p>In Go, strings are immutable while <tt>[]byte</tt> is mutable; the two cannot safely
alias. Therefore, when we have some <tt>b []byte</tt> and we do <tt>string(b)</tt>, Go
allocates a copy of the slice to a new location and creates a string header to
point to it.</p>
<p>We can verify this by running memory profiling and looking at the allocations:</p>
<div><pre><span></span>$ TDINPUT=input.td go test -memprofile=mem.mprof -bench=Preall -benchtime=5s
...

$ go tool pprof --alloc_objects example.com.test mem.mprof
...
(pprof) list scanIdentifier
Total: 27388406
ROUTINE ======================== example%2ecom.(*Lexer).scanIdentifier
  21501647   21501647 (flat, cum) 78.51% of Total
         .          .    238:func (lex *Lexer) scanIdentifier() Token {
         .          .    239: startpos := lex.rpos
         .          .    240: for isAlpha(lex.r) || isDigit(lex.r) {
         .          .    241:         lex.next()
         .          .    242: }
  21501647   21501647    243: return Token{IDENTIFIER, string(lex.buf[startpos:lex.rpos]), startpos}
         .          .    244:}
         .          .    245:
         .          .    246:func (lex *Lexer) scanNumber() Token {
         .          .    247: startpos := lex.rpos
         .          .    248: for isDigit(lex.r) {
</pre></div>
<p>As expected, many allocations come from the suspected line. Looking back at the
CPU profiling graph, we also see that much of the run-time of
<tt>slicebytetostirng</tt> is spent on the allocation (<tt>mallocgc</tt>). This is
actually a good clue to the stark performance improvement in Go since 1.2; while
the compiler certainly became better since then (especially with the
register-based ABI <a href="https://eli.thegreenplace.net/2022/interface-method-calls-with-the-go-register-abi/">introduced in 1.17</a>),
it didn&#39;t improve <em>that</em> much. What did improve a whole lot is the garbage
collector; many allocations mean lots of heap to scan and track, and lots of
garbage to clean up.</p>
</div>
<div id="avoiding-allocations-with-sub-strings">
<h2>Avoiding allocations with sub-strings</h2>
<p>Before we move on, let&#39;s get one thing out of the way. There are all kinds of
tricks you can play in Go using <tt>unsafe</tt> to explicitly alias byte slices and
strings and thus avoid allocations; this is even done <a href="https://pkg.go.dev/strings#Builder.String">in the standard library</a>. I&#39;ll leave this topic out of
this post and may cover it separately in the future.</p>
<p>I did wonder about the cost of these allocations, though; what if we avoid all
the copies? At the moment, the lexer&#39;s API is such that it takes a <tt>[]byte</tt> as
input:</p>
<div><pre><span></span><span>func</span> <span>NewLexer</span><span>(</span><span>buf</span> <span>[]</span><span>byte</span><span>)</span> <span>*</span><span>Lexer</span>
</pre></div>
<p>And the tokens we return have <tt>string</tt> values:</p>
<div><pre><span></span><span>type</span> <span>Token</span> <span>struct</span> <span>{</span>
  <span>Name</span> <span>TokenName</span>
  <span>Val</span>  <span>string</span>
  <span>Pos</span>  <span>int</span>
<span>}</span>
</pre></div>
<p>What if we created a lexer with a <tt>string</tt> input instead? Since Go strings
are immutable, subslices alias each other and are trivial to create and pass
around; consider this code sample:</p>
<div><pre><span></span><span>s</span> <span>:=</span> <span>&#34;hello there&#34;</span>
<span>s2</span> <span>:=</span> <span>s</span><span>[</span><span>6</span><span>:]</span>
</pre></div>
<p>When <tt>s</tt> is created, it allocates an 11-byte sequence of bytes and creates a
<a href="https://pkg.go.dev/reflect#StringHeader">2-word string header</a> in <tt>s</tt>: a
pointer to the sequence of bytes and a <tt>len</tt> field with the value 11. When the
second line is run, Go just creates a new string header that points into the
same byte buffer (with offset 6) and length 5. There are no allocations (string
headers are typically created on the stack just like integers).</p>
<p>So I went ahead and rewrote the lexer using this different API; the code changes
are pretty minor and the full code is <a href="https://github.com/eliben/code-for-blog/tree/master/2014/tablegen-lexer-go/lexer-string">available here</a>.
The <tt>scanIdentifier</tt> method now looks like this:</p>
<div><pre><span></span><span>func</span> <span>(</span><span>lex</span> <span>*</span><span>Lexer</span><span>)</span> <span>scanIdentifier</span><span>()</span> <span>Token</span> <span>{</span>
  <span>startpos</span> <span>:=</span> <span>lex</span><span>.</span><span>rpos</span>
  <span>for</span> <span>isAlpha</span><span>(</span><span>lex</span><span>.</span><span>r</span><span>)</span> <span>||</span> <span>isDigit</span><span>(</span><span>lex</span><span>.</span><span>r</span><span>)</span> <span>{</span>
    <span>lex</span><span>.</span><span>next</span><span>()</span>
  <span>}</span>
  <span>return</span> <span>Token</span><span>{</span><span>IDENTIFIER</span><span>,</span> <span>lex</span><span>.</span><span>buf</span><span>[</span><span>startpos</span><span>:</span><span>lex</span><span>.</span><span>rpos</span><span>],</span> <span>startpos</span><span>}</span>
<span>}</span>
</pre></div>
<p>Note there&#39;s no <tt><span>string(...)</span></tt> cast for the token value; since <tt>lex.buf</tt> is
already a <tt>string</tt> in this version, that wouldn&#39;t be necessary. Instead, we
just return a string slice, which creates a new 2-word header (instead of
allocating a new string and copying the data into it).</p>
<p>If we benchmark this version, it runs in 7.7 ms for our input, about 20% faster.</p>
</div>
<div id="is-this-a-good-api-though">
<h2>Is this a good API though?</h2>
<p>OK, so we found a way to further improve the performance of the lexer by taking
a <tt>string</tt> input; is this API useful though? Is it common for users of
a lexer to have a fully formed <tt>string</tt>? To be fair, the question applies
equally to the existing <tt>[]byte</tt> API. IMHO, in most cases the answer is <em>it
depends</em>. If the input to parse is already in memory (say, it was entered into
some sort of text box in your GUI), then yes, <tt>string</tt> is fine in an API.
More typically, though, this data is read from <em>somewhere</em>.</p>
<p>Go has useful I/O interfaces like <tt>io.Reader</tt>; this interface encapsulates
&#34;a place we read data from&#34;, and this data is typically read incrementally;
e.g. you don&#39;t slurp the whole input file in a single go, but read it in chunks
as needed. My old-ish SSD has read speeds in the order of 500 MiB / sec, meaning
that a 1 MiB file will take about 2 ms to read from disk. If we really care
about performance, overlapping this read with lexing makes sense. But this
brings us to a <tt>io.Reader</tt> based API, where our substring optimization won&#39;t
really work.</p>
<p>Let&#39;s see how Go itself does it; the <a href="https://pkg.go.dev/text/scanner">text/scanner package</a> is initialized thus:</p>
<div><pre><span></span><span>func</span> <span>(</span><span>s</span> <span>*</span><span>Scanner</span><span>)</span> <span>Init</span><span>(</span><span>src</span> <span>io</span><span>.</span><span>Reader</span><span>)</span> <span>*</span><span>Scanner</span>
</pre></div>
<p>And to obtain the text of a token, you call <tt>TokenText</tt>, which returns a
<tt>string</tt>. If we look under the hood, <tt>TokenText</tt> does the equivalent of
<tt>string(some []byte buffer)</tt>, which incurs an allocation and copy.</p>
<p>When the API is <tt>io.Reader</tt>, there&#39;s really no choice for this, though. It&#39;s
really hard to safely return a <tt>string</tt> that aliases part of a <tt>[]byte</tt>
buffer accumulated from reading some external source. This is what other
standard library package do as well - <tt>io.Reader</tt> input is very idiomatic.</p>
<p>The Go compiler itself has a lexical scanner in
<tt>src/cmd/compile/internal/scanner.go</tt>, and it also takes an <tt>io.Reader</tt> (in
its <tt>init</tt> method); naturally, it also has to allocate and copy literal values
with a <tt><span>string(...)</span></tt> conversion (these are stored in its <tt>lit</tt> field).</p>
</div>
<div id="garbage-collector">
<h2>Garbage Collector</h2>
<p>We&#39;re not done yet. Let&#39;s look at the profile of the string version, using
the same profiling invocation as before:</p>
<p><img alt="pprof CPU profile for the lexer string version" src="https://eli.thegreenplace.net/images/2022/lexer-pprof-cpu-2.png"/></p><p>We see that our <tt>Lexer</tt> methods no longer allocate heap data - that&#39;s good!
We can prove this further by looking at memory profiling or the compiler&#39;s
escape analysis with <tt><span>-gcflags=&#34;-m&#34;</span></tt> - I&#39;ll leave this as an exercise for
diligent readers.</p>
<p>That said, the garbage collector (GC) still clearly runs, taking non-trivial %s
of CPU, as we can see from the right side of the graph. Why does this happen?</p>
<p>This is a good example of why benchmarking is tricky, particularly in GC&#39;d
languages. The Go GC is concurrent, running &#34;silently&#34; alongside our application
whenever it sees fit. In benchmarking situations, this often depends on the
nature of the specific benchmark; specifically, in our benchmark the
top-level function <tt>tokenizeAllPrealloc</tt> is invoked in a loop. This function
pre-allocates a slice of 200k tokens with <tt>make</tt>. Each <tt>Token</tt> in our
lexer occupies 4 words, which is 32 bytes on my 64-bit machine. Overall,
the pre-allocation of the token slice takes somewhere on the order of 6.4 MiB.
Running in a benchmarking loop for 5 seconds results in over 700 cycles, for a
total of 4.5 GiB of heap data (disregarding any additional heap the benchmarking
machinery uses) - which is a lot!</p>
<p>Go lets us control GC behavior with the <tt>GOGC</tt> env var, which sets the
percentage of heap growth that triggers a collection cycle. The default is 100,
which means a cycle is triggered when the heap doubles. If we use
<tt>GODEBUG=gctrace=1</tt> to trace GC invocations, we&#39;ll see that the GC runs over
700 cycles during our benchmark; clearly, this affects performance.</p>
<p>What happens if we try to tweak <tt>GOGC</tt>? Here&#39;s a chart for different values:</p>
<p><img alt="Run-time with different values of GOGC" src="https://eli.thegreenplace.net/images/2022/lexer-runtime-gogc.png"/></p><p>The fastest run of ~6 ms is achieved with <tt>GOGC=1600</tt> and stays stable
thereafter. Using tracing again, we see that with this setting the GC only
runs ~50 times during the 5 sec benchmark, compared to 700+ previously.
FWIW, disabling GC entirely with <tt>GOGC=off</tt> produces a slightly slower
run-time at 6.5 ms (there&#39;s quite a lot of data here, so an overly large heap
may occasionally lead to swapping).</p>
<p>Does any of this matter? This depends on your application! Admittedly, we&#39;re
deep in the weeds here, trying to shave off sub-milliseconds from fully lexing
a large input file. If you care about performance at this level of granularity,
it&#39;s very likely that you&#39;ll consider the full lifecycle of your application
and will have the GC tuned already.</p>
<p>As we&#39;ve seen with the stark performance improvements from older versions of Go,
the GC has improved a lot. And it keeps improving! Right now an <a href="https://github.com/golang/proposal/blob/master/design/48409-soft-memory-limit.md">accepted
proposal</a>
aims to add more control for users with a new <tt>GOMEMLIMIT</tt> var. This is
likely to land in the next Go version (or the one after it).</p>
<p>A longer term proposal to <a href="https://github.com/golang/go/issues/51317">add arenas to Go</a> is also on the table. This
permits much finer-grained memory control, and is particularly suited for
programs like compilers or other users of lexical analysis. Such programs have a
lifecycle that&#39;s very suitable for arenas - you allocate a bunch of data per
phase, and then at the very end you release <em>everything</em> in one step.</p>
</div>
<div id="conclusion">
<h2>Conclusion</h2>
<p>This post discussed some potential optimizations to a lexical scanner written
in Go. It touched upon the relative efficiency of converting byte slices to
strings vs. taking substrings, discussed some strategies w.r.t. API design
in Go, and even got into optimizing GC behavior.</p>
<p>I&#39;m pleasantly surprised at how fast lexical scanning in Go is. Even the copying
version without GC tuning takes 9.6 ms to process 1 MiB of input source - that&#39;s
over 100 MiB /sec! Depending on where your source is stored, this could be
almost as fast as, or faster than your CPU can <em>read</em> the source into memory.
With API changes and GC tuning we were able to improve this by a further 37% to
6 ms, though the specifics here depend on your actual application. Compared to
our original version compiled with Go 1.2, the new lexer is more than 3 times
as fast!</p>
<hr/>

</div>

            </div></div>
  </body>
</html>
