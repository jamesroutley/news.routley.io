<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pieterma.es/syntopic-reading-claude/">Original</a>
    <h1>Reading across books with Claude Code</h1>
    
    <div id="readability-page-1" class="page"><div> <p>LLMs are overused to summarise and underused to help us read deeper.</p>
<p>To explore how they can enrich rather than reduce, I set Claude Code up with tools to mine a library of 100 non-fiction books.
It found sequences of excerpts connected by an interesting idea, or <em>trails</em>.</p>

<p>Here’s a part of one such trail, linking deception in the startup world to the social psychology of mass movements (I’m especially pleased by the jump from Jobs to Theranos):</p>

<h2 id="how-it-works">How it works</h2>
<p><img src="https://pieterma.es/_astro/pipeline.Bl0o8nI__2728Xv.webp" alt="Pipeline diagram" loading="lazy" decoding="async" fetchpriority="auto" width="3396" height="1194"/> </p>
<p>The books were selected from Hacker News’ favourites, which I previously <a href="https://hnbooks.pieterma.es" rel="nofollow" target="_blank">scraped and visualized</a>.</p>
<p>Claude browses the books a chunk at a time. A chunk is a segment of roughly 500 words that aligns with paragraphs when possible.
This length is a good balance between saving tokens and providing enough context for ideas to breathe.</p>
<p>Chunks are indexed by topic, and topics are themselves indexed for search. This makes it easy to look up all passages in the corpus that relate to, say, <em>deception</em>.</p>
<p>This works well when you know what to look for, but search alone can’t tell you which topics are present to begin with.
There are over 100,000 extracted topics, far too many to be browsed directly. To support exploration, they are grouped into a hierarchical tree structure.</p>
<p>This yields around 1,000 top-level topics. They emerge from combining lower-level topics, and not all of them are equally useful:</p>
<ul>
<li><em>Incidents that frustrated Ev Williams</em></li>
<li><em>Names beginning with “Da”</em></li>
<li><em>Events between 1971 &amp; 1974</em></li>
</ul>
<p>However, this <a href="https://en.wikipedia.org/wiki/The_Analytical_Language_of_John_Wilkins" rel="nofollow" target="_blank">Borgesian</a> taxonomy is good enough for Claude to piece together what the books are about.</p>
<p>Claude uses the topic tree and the search via a few CLI tools.</p>
<ul>
<li>Find all chunks associated with a topic similar to a query.</li>
<li>Find topics which occur in a window of chunks around a given topic.</li>
<li>Find topics that co-occur in multiple books.</li>
<li>Browse topics and chunks that are siblings in the topic tree.</li>
</ul>
<p>To generate the trails, the agent works in stages.</p>
<p><img src="https://pieterma.es/_astro/agent_steps.CPyESBEq_Z2rjTaW.webp" alt="Generate ideas, research a trail, connect the highlights" loading="lazy" decoding="async" fetchpriority="auto" width="1553" height="228"/></p>
<ol>
<li>First, it scans the library and the existing trails, and proposes novel trail ideas. It mainly browses the topic tree to find unexplored areas and rarely reads full chunks in depth.</li>
<li>Then, it takes a specific idea and turns it into a trail. It receives seed topics from the previous stage and browses many chunks.
It extracts excerpts, specific sequences of sentences, and decides on how best to order them to support an insight.</li>
<li>Finally, it adds highlights and edges between consecutive excerpts.</li>
</ol>
<h2 id="what-i-learned">What I learned</h2>
<h3 id="claude-code-is-great-for-non-coding-tasks">Claude Code is great for non-coding tasks</h3>
<p>Even though I’ve been using Claude Code to develop for months, my first instinct for this project was to consider it as a traditional pipeline of several discrete stages.
My initial attempt at this system consisted of multiple LLM modules with carefully hand-assembled contexts.</p>
<p>On a whim, I ran Claude with access to the debugging tools I’d been using and a minimal prompt: <em>“find something interesting.”</em>
It immediately did a better job at pulling in what it needed than the pipeline I was trying to tune by hand, while requiring much less orchestration.
It was a clear improvement to push as much of the work into the agent’s loop as possible.</p>
<p>I ended up using Claude as my main interface to the project.</p>
<p>The latter opened up options that I wouldn’t have considered before.
For example, I changed my mind on how short I wanted excerpts to be.
I communicated my new preference to Claude, which then looked through all the existing trails and edited them as necessary, balancing the way the overall meaning of the trail changed.
Previously, I would’ve likely considered all previous trails to be outdated and generated new ones, because the required edits would’ve been too nuanced to specify.</p>
<p>In general, agents have widened my ambitions.</p>
<h3 id="ask-the-agent-what-it-needs">Ask the agent what it needs</h3>
<p>My focus went from optimising prompts to implementing better tools for Claude to use, moving up a rung on the abstraction ladder.</p>
<p>My mental model of the AI component changed: from a function mapping input to output, to a coworker I was assisting.
I spent my time thinking about the affordances that would make the workflow better, as if I were designing them for myself.
That they were to be used by an agent was a mere detail.</p>
<p>This worked because the agent is now intelligent enough that the way it uses these tools overlaps with my own mental model.
It is generally easy to empathise with it and predict what it will do.</p>
<p>Initially I watched Claude’s logs closely and tried to guess where it was lacking a certain ability.
Then I realised I could simply ask it to provide feedback at the end and list the functionality it wished it had.
Claude was excellent at proposing new commands and capabilities that would make the work more efficient.</p>
<p>Claude suggested improvements, which Claude implemented, so Claude could do the work better.
At least I’m still needed to pay for the tokens — for now.</p>
<h3 id="novelty-is-a-useful-guide">Novelty is a useful guide</h3>
<p>It’s hard to quantify <em>interestingness</em> as an objective to optimise for.</p>
<p>As a sign of the times, this novelty search was implemented in two ways:</p>
<ol>
<li>By biasing the search algorithm towards under-explored topics and books.</li>
<li>By asking Claude nicely.</li>
</ol>
<p>A topic’s novelty score was calculated as the mean distance from its embedding’s <em>k</em> nearest neighbors. A book’s novelty score is the average novelty of the unique topics that it contains.
This value was used to rank search results, so that those which were both relevant and novel were more likely to be seen.</p>
<p>On a prompting level, Claude starts the ideation phase by looking at all the existing trails and is asked to avoid any conceptual overlap.
This works fairly well, though it is often distracted by any topics related to secrecy, systems theory, or tacit knowledge.</p>
<p>It’s as if the very act of finding connections in a corpus summons the spirit of Umberto Eco and amps up the conspiratorial thinking.</p>
<h2 id="how-its-implemented">How it’s implemented</h2>
<p><img src="https://pieterma.es/_astro/implementation.CNN37YWY_Z1HGHsR.webp" alt="Implementation diagram" loading="lazy" decoding="async" fetchpriority="auto" width="1591" height="1959"/></p>
<ul>
<li>EPUBs are parsed using <a href="https://github.com/rushter/selectolax" rel="nofollow" target="_blank"><code>selectolax</code></a>, which I picked over BeautifulSoup for its speed and simpler API.</li>
<li>Everything from the plain text to the topic tree is stored in SQLite. Embeddings are stored using <a href="https://github.com/asg017/sqlite-vec" rel="nofollow" target="_blank"><code>sqlite-vec</code></a>.</li>
<li>The text is split into sentences using <a href="https://github.com/superlinear-ai/wtpsplit-lite" rel="nofollow" target="_blank"><code>wtpsplit</code></a> (the <code>sat-6l-sm</code> model).
Those sentences are then grouped into chunks, trying to get up to 500 words without breaking up paragraphs.</li>
<li>I used <a href="https://dspy.ai" rel="nofollow" target="_blank"><code>DSPy</code></a> to call LLMs. It worked well for the structured data extraction and it was easy to switch out different models to experiment.
I tried its prompt optimizers before I went full agentic, and their results were very promising.</li>
<li>I settled on Gemini 2.5 Flash Lite for topic extraction.
The model gets passed a chunk and is asked to return 3-5 topics. It is also asked whether the chunk is <em>useful</em>, in order to filter out index entries, acknowledgements, orphan headers, etc.
I was surprised at how stable these extracted topics were: similar chunks often shared some of the exact same topic labels.
Processing 100 books used about 60M input tokens and ~£10 in total.</li>
<li>After a couple books got indexed, I shared the results with Claude Opus along with the original prompt and asked it to improve it.
This is a half-baked single iteration of the type of prompt optimisation DSPy implements, and it worked rather well.</li>
<li>Topic pairs with a distance below a threshold get merged together. This takes care of near-duplicates such as <em>“Startup founder”</em>, <em>“Startup founders”</em>, and <em>“Founder of startups”</em>.</li>
<li>The CLI output uses a semi-XML format. In order to stimulate navigating, most output is nested with related content. For example, when searching for a topic, chunks are shown with the other topics they contain.
This allows us to get a sense of what the chunk is about, as well as which other topics might be interesting.
There’s probably more token-efficient formats, but I never hit the limit of the context window.</li>
</ul>
<pre tabindex="0" data-language="xml"><code><span><span>&lt;</span><span>topics</span><span> query</span><span>=</span><span>&#34;</span><span>deception</span><span>&#34;</span><span> count</span><span>=</span><span>&#34;</span><span>1</span><span>&#34;</span><span>&gt;</span></span>
<span><span>  &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>47193</span><span>&#34;</span><span> books</span><span>=</span><span>&#34;</span><span>7</span><span>&#34;</span><span> score</span><span>=</span><span>&#34;</span><span>0.0173</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Deception</span><span>&#34;</span><span>&gt;</span></span>
<span><span>    &lt;</span><span>chunk</span><span> id</span><span>=</span><span>&#34;</span><span>186</span><span>&#34;</span><span> book</span><span>=</span><span>&#34;</span><span>1</span><span>&#34;</span><span>&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>47192</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Business deal</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>47108</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Internal conflict</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>46623</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Startup founders</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>    &lt;/</span><span>chunk</span><span>&gt;</span></span>
<span><span>    &lt;</span><span>chunk</span><span> id</span><span>=</span><span>&#34;</span><span>1484</span><span>&#34;</span><span> book</span><span>=</span><span>&#34;</span><span>4</span><span>&#34;</span><span>&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>51835</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Gawker Media</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>53006</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Legal Action</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>52934</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Maskirovka</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>52181</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Strategy</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>    &lt;/</span><span>chunk</span><span>&gt;</span></span>
<span><span>    &lt;</span><span>chunk</span><span> id</span><span>=</span><span>&#34;</span><span>2913</span><span>&#34;</span><span> book</span><span>=</span><span>&#34;</span><span>9</span><span>&#34;</span><span>&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>59348</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Blood testing system</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>59329</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Elizabeth Holmes</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>59352</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Investor demo</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>      &lt;</span><span>topic</span><span> id</span><span>=</span><span>&#34;</span><span>59349</span><span>&#34;</span><span> label</span><span>=</span><span>&#34;</span><span>Theranos</span><span>&#34;</span><span>/&gt;</span></span>
<span><span>    &lt;/</span><span>chunk</span><span>&gt;</span></span>
<span><span>  &lt;/</span><span>topic</span><span>&gt;</span></span>
<span><span>&lt;/</span><span>topics</span><span>&gt;</span></span></code></pre>
<ul>
<li>
<p>Topics are embedded using <code>google/embeddinggemma-300m</code> and reranked using <code>BAAI/bge-reranker-v2-m3</code>.</p>
</li>
<li>
<p>Many CLI tools require loading the embedding model and other expensive state. The first call transparently starts a separate server process which loads all these resources once and holds onto them for a while.
Subsequent CLI calls use this server through Python’s <code>multiprocessing.connection</code>.</p>
</li>
<li>
<p>The topic collection is turned into a graph (backed by <code>igraph</code>) by adding edges based on the similarity of their embeddings and the point-wise mutual information of their co-occurrences.</p>
</li>
<li>
<p>The graph is turned into a tree by applying <a href="https://leidenalg.readthedocs.io/en/stable/" rel="nofollow" target="_blank">Leiden</a> partitioning recursively until a minimum size is reached.
I tried the Surprise quality function because it had no parameters to tweak, and found it to be good enough. Each group is labelled by Gemini based on all the topics that it contains.</p>
</li>
<li>
<p>Excerpts are cleaned by Gemini to remove EPUB artifacts, parsing errors, headers, footnotes, etc.
Doing this only for excerpts that are actually shown, instead of during pre-processing, saved a lot of tokens.</p>
</li>
</ul> </div></div>
  </body>
</html>
