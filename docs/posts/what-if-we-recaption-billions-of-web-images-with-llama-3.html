<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2406.08478">Original</a>
    <h1>What If We Recaption Billions of Web Images with LLaMA-3?</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xianhang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tu,+H">Haoqin Tu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hui,+M">Mude Hui</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zeyu Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+B">Bingchen Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+J">Junfei Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren,+S">Sucheng Ren</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mei,+J">Jieru Mei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Q">Qing Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+H">Huangjie Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Y">Yuyin Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+C">Cihang Xie</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2406.08478">View PDF</a>
    <a href="https://arxiv.org/html/2406.08478v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. Our paper aims to bridge this community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users&#39; text instructions, especially in following complex queries. Our project page is <a href="https://www.haqtu.me/Recap-Datacomp-1B/" rel="external noopener nofollow">this https URL</a>
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Bingchen Zhao [<a href="https://arxiv.org/show-email/4a59523a/2406.08478">view email</a>]      </p></div></div>
  </body>
</html>
