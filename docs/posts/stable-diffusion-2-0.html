<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://stability.ai/blog/stable-diffusion-v2-release">Original</a>
    <h1>Stable Diffusion 2.0</h1>
    
    <div id="readability-page-1" class="page"><div id="page" role="main">
        
          <article data-page-sections="62f2452bc121595f4d87c71c" id="sections">
  
    

<section data-test="page-section" data-section-theme="bright" data-section-id="62f2452bc121595f4d87c71e" data-controller="SectionWrapperController" data-current-styles="{
&#34;imageOverlayOpacity&#34;: 0.15,
&#34;backgroundWidth&#34;: &#34;background-width--full-bleed&#34;,
&#34;sectionHeight&#34;: &#34;section-height--medium&#34;,
&#34;horizontalAlignment&#34;: &#34;horizontal-alignment--center&#34;,
&#34;verticalAlignment&#34;: &#34;vertical-alignment--middle&#34;,
&#34;contentWidth&#34;: &#34;content-width--wide&#34;,
&#34;sectionTheme&#34;: &#34;bright&#34;,
&#34;sectionAnimation&#34;: &#34;none&#34;,
&#34;backgroundMode&#34;: &#34;image&#34;
}" data-current-context="{
&#34;video&#34;: {
&#34;playbackSpeed&#34;: 0.5,
&#34;filter&#34;: 1,
&#34;filterStrength&#34;: 0,
&#34;zoom&#34;: 0,
&#34;videoSourceProvider&#34;: &#34;none&#34;
},
&#34;backgroundImageId&#34;: null,
&#34;backgroundMediaEffect&#34;: null,
&#34;typeName&#34;: &#34;blog-side-by-side&#34;
}" data-animation="none">
  
  <div>
    <div>
      
      
      
      
      
      
      <div data-content-field="main-content" data-item-id="">
  <article id="article-">
  
    <div>
      

      <div>
        <div><div data-layout-label="Post Body" data-type="item" id="item-637e82c37e103318bfd0ecbc"><div><div><div data-aspect-ratio="100" data-block-type="5" id="block-f5eb68b10d1194463385"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/98ea7c65-f73e-4a48-b8d4-94180ad637e2/697c70e5ffe354b9ab6cc9477f91eff253379ec54c032403ad6ae3e5.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/98ea7c65-f73e-4a48-b8d4-94180ad637e2/697c70e5ffe354b9ab6cc9477f91eff253379ec54c032403ad6ae3e5.jpeg" data-image-dimensions="1024x768" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637ec02b36ea19641d932532" data-type="image"/>
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-d1070adcde16f32a49ef"><div>

<p>It is our pleasure to announce the open-source release of <a href="https://github.com/Stability-AI/stablediffusion"><span>Stable Diffusion Version 2</span></a>.</p><p>The original <a href="https://github.com/CompVis/stable-diffusion"><span>Stable Diffusion V1</span></a> led by <a href="https://ommer-lab.com"><span>CompVis</span></a> changed the nature of open source AI models and spawned hundreds of other models and innovations all over the world. It had one of the fastest climbs to 10K Github stars of any software, rocketing through 33K stars in less than two months.</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1669236368755_4008"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/17e07286-40e5-491a-b613-54147d86f72b/SD_dev_adoption_768.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/17e07286-40e5-491a-b613-54147d86f72b/SD_dev_adoption_768.png" data-image-dimensions="768x614" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637ea7154db2523fb4acccb6" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669236368755_4350"><div>

<p>The dynamic team of Robin Rombach (<a href="https://stability.ai"><span>Stability AI</span></a>) and Patrick Esser (<a href="https://runwayml.com/"><span>Runway ML</span></a>) from the<a href="https://ommer-lab.com"> <span>CompVis Group at LMU Munich</span></a> headed by <a href="https://ommer-lab.com/people/ommer/"><span>Prof. Dr. Björn Ommer</span></a>, led the original Stable Diffusion V1 release. They built on their prior work of the lab with<a href="https://arxiv.org/abs/2112.10752"> <span>Latent Diffusion Models</span></a> and got critical support from <a href="https://laion.ai/"><span>LAION</span></a> and <a href="https://eleuther.ai"><span>Eleuther AI</span></a>. You can read more about the original Stable Diffusion V1 release in our earlier <a href="https://stability.ai/blog/stable-diffusion-announcement"><span>blog post</span></a>. Robin is now leading the effort with Katherine Crowson at Stability AI to create the next generation of media models with our broader team.</p><p>Stable Diffusion 2.0 delivers a number of big improvements and features versus the original V1 release, so let’s dive in and take a look at them.</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1669252272108_10357"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/3b5bd917-0689-44f5-a2aa-18804be83a13/astronaut_feeding_chickens.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/3b5bd917-0689-44f5-a2aa-18804be83a13/astronaut_feeding_chickens.png" data-image-dimensions="768x768" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637ec6cf5042fe5ee9679d31" data-type="image"/>
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669252272108_10700"><div>

<p><span><em>New Text-to-Image Diffusion Models</em></span></p><p>The Stable Diffusion 2.0 release includes robust text-to-image models trained using a brand new text encoder (OpenCLIP), developed by LAION with support from Stability AI, which greatly improves the quality of the generated images compared to earlier V1 releases. The text-to-image models in this release can generate images with default resolutions of both 512x512 pixels and 768x768 pixels. </p><p>These models are trained on an aesthetic subset of the<a href="https://laion.ai/blog/laion-5b/"> <span>LAION-5B</span></a> dataset created by the DeepFloyd team at Stability AI, which is then further filtered to remove adult content using LAION’s<a href="https://openreview.net/forum?id=M3Y74vmsMcY"> <span>NSFW filter</span></a>.</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1669237481819_5250"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/89c137e3-aa35-411b-b6c5-c15ba315f934/sd2.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/89c137e3-aa35-411b-b6c5-c15ba315f934/sd2.png" data-image-dimensions="1536x768" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637ea6b6b9941d0aa250fa90" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Examples of images produced using Stable Diffusion 2.0, at 768x768 image resolution.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669237481819_6896"><div>

<p><span><em>Super-resolution Upscaler Diffusion Models</em></span></p><p>Stable Diffusion 2.0 also includes an Upscaler Diffusion model that enhances the resolution of images by a factor of 4. Below is an example of our model upscaling a low-resolution generated image (128x128) into a higher resolution image (512x512). Combined with our text-to-image models, Stable Diffusion 2.0 can now generate images with resolutions of 2048x2048–or even higher.</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1669237481819_7369"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/9d913e45-aacd-45df-aa36-88f1190e9377/LowRes-SuperRes.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/9d913e45-aacd-45df-aa36-88f1190e9377/LowRes-SuperRes.png" data-image-dimensions="1080x540" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637ea6d148297548820f80cb" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Left: 128x128 low-resolution image. Right: 512x512 resolution image produced by Upscaler.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669237481819_8176"><div>

<p><span><em>Depth-to-Image Diffusion Model</em></span></p><p>Our new <em>depth-guided</em> stable diffusion model, called <em>depth2img</em>, extends the previous image-to-image feature from V1 with brand new possibilities for creative applications. <em>Depth2img</em> infers the depth of an input image (using an existing <a href="https://github.com/isl-org/MiDaS"><span>model</span></a>), and then generates new images using both the text and depth information.</p>


</div></div><div data-aspect-ratio="57.752808988764045" data-block-type="5" id="block-yui_3_17_2_1_1669237481819_8988"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/46a9ee2b-3f8e-43a9-8195-0de38f8c6b9d/StableDiffusionV2_Depth2Img.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/46a9ee2b-3f8e-43a9-8195-0de38f8c6b9d/StableDiffusionV2_Depth2Img.png" data-image-dimensions="1600x924" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637e905d4aea254bc99b6947" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>The input image on the left can produce several new images (on the right). This new model can be used for structure-preserving image-to-image and shape-conditional image synthesis.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669237481819_10369"><p>Depth-to-Image can offer all sorts of new creative applications, delivering transformations that look radically different from the original but which still preserve the coherence and depth of that image:</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1669237481819_10748"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/01b5f8a8-7568-4c43-91bf-2f1764d264b9/d2i.gif" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/01b5f8a8-7568-4c43-91bf-2f1764d264b9/d2i.gif" data-image-dimensions="768x512" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637ea68a6dfc091494fbadd7" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Depth-to-Image preserves coherence.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669237481819_11820"><div>

<p><span><em>Updated Inpainting Diffusion Model</em></span></p><p>We also include a new text-guided inpainting model, fine-tuned on the new Stable Diffusion 2.0 base text-to-image, which makes it super easy to switch out parts of an image intelligently and quickly.</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1669241401719_11932"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/86d2d10c-c67e-416b-9b24-d923586f06b2/inpainting.gif" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/86d2d10c-c67e-416b-9b24-d923586f06b2/inpainting.gif" data-image-dimensions="704x704" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637ea69c6c712a342a3c0246" data-type="image"/>
            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>The updated inpainting model fine-tuned on Stable Diffusion 2.0 text-to-image model.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669241401719_14256"><div>

<p>Just like the first iteration of Stable Diffusion, we’ve worked hard to optimize the model to run on a single GPU–we wanted to make it accessible to as many people as possible from the very start. We’ve already seen that, when millions of people get their hands on these models, they collectively create some truly amazing things. This is the power of open source: tapping the vast potential of millions of talented people who might not have the resources to train a state-of-the-art model, but who have the ability to do something incredible with one.</p><p>This new release, along with its powerful new features like <em>depth2img</em> and higher resolution upscaling capabilities, will serve as the foundation of countless applications and enable an explosion of new creative potential.</p>


</div></div><div data-block-type="5" id="block-yui_3_17_2_1_1669252272108_11989"><div>















  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-src="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/04b3e64d-4767-4e92-b214-4e8bdab45e06/Furby_768.png" data-image="https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/04b3e64d-4767-4e92-b214-4e8bdab45e06/Furby_768.png" data-image-dimensions="768x736" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="637ec7b92d73311e474f9909" data-type="image"/>
            </p>
          </div>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669252272108_12329"><div>

<p>For more details about accessing the model, please check out the release notes on our GitHub:<a href="https://github.com/Stability-AI/stablediffusion"><span> https://github.com/Stability-AI/StableDiffusion</span></a></p><p>We will offer active support to this repository as our direct contribution to open source AI and look forward to all the amazing things you all build on it. </p>


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1669252272108_13831"><p><em>We are releasing these models into the Stability AI API Platform (</em><a href="https://platform.stability.ai"><span><em>platform.stability.ai</em></span></a><em>) and </em><a href="https://beta.dreamstudio.ai"><span><em>DreamStudio</em></span><em> </em></a><em>in the next few days. We will be sending out an update on this with information for developers and partners, including pricing updates. We hope you all enjoy these updates!</em></p></div><div data-block-type="2" id="block-yui_3_17_2_1_1669241401719_15880"><p><em>We are hiring researchers and engineers who are excited to work on the next generation of open source Generative AI models! If you’re interested in joining Stability AI, please reach out to careers@stability.ai, with your CV and a short statement about yourself.</em></p></div></div></div></div></div>

        

        
        
          
        
      </div>

      
    </div>
  
</article>

</div>
    </div>
  </div>
  
</section>

  
</article>

          
            
              
                

              
            
          
        
      </div></div>
  </body>
</html>
