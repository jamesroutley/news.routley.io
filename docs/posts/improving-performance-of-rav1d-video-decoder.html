<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ohadravid.github.io/posts/2025-05-rav1d-faster/">Original</a>
    <h1>Improving performance of rav1d video decoder</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><em>*on macOS with an M3 chip</em></p><p>A while ago, <a href="https://www.memorysafety.org/blog/rav1d-perf-bounty/">memorysafety.org announced a contest</a> for improving performance of <code>rav1d</code>, a Rust port of the <code>dav1d</code> AV1 decoder.</p><p>As this literally has my name written on it, I thought it would be fun to give it a try (even though I <em>probably</em> can‚Äôt participate in the contest).</p><p>This is a write-up about two small performance improvements I found (<a href="https://github.com/memorysafety/rav1d/pull/1397">1st PR</a>, <a href="https://github.com/memorysafety/rav1d/pull/1400">2nd PR</a>) and how I found them (you can also jump to the <a href="#summary">summary in the end</a>).</p><p><img src="https://ohadravid.github.io/2025-05-rav1d-faster/drakeposting.webp" alt="drakeposting meme - working on rav1d because there‚Äôs a contest with money, working on rav1d because my last name is Ravid"/></p><h2 id="background-and-approach">Background and Approach</h2><p><a href="https://github.com/memorysafety/rav1d"><code>rav1d</code></a> is a port of <a href="https://code.videolan.org/videolan/dav1d"><code>dav1d</code></a>, created by (1) running <a href="https://github.com/immunant/c2rust"><code>c2rust</code></a> on <code>dav1d</code>, (2) incorporating <code>dav1d</code>‚Äôs asm-optimized functions, and (3) changing the code to be more Rust-y and safer.</p><p>The authors also published <a href="https://www.memorysafety.org/blog/rav1d-performance-optimization/">a detailed article</a> about the process and the performance work they did.</p><p>More recently, the contest was announced, with the baseline being:</p><blockquote><p>Our Rust-based rav1d decoder is currently about 5% slower than the C-based dav1d decoder.</p></blockquote><p>Video decoders are notoriously complex pieces of software, but because we are comparing the performance of two similar deterministic binaries we might be able to avoid a lot of that complexity - with the right tooling.</p><p>We can‚Äôt expect to find huge wins, and some regressions might be too-hard-to-tackle (for example, LLVM finding a Rust function harder to optimize than the C version),
but it‚Äôs worth a shot, especially since aarch64 (my environment) is probably less optimized than x86_64.</p><p>My approach here was to:</p><ol><li>Use a sampling profiler to capture snapshots of both runs on the same input.</li><li>Use the optimized asm calls as ‚Äúanchors‚Äù since they should match perfectly.</li><li>Compare the Rust and C versions function by function, and if there‚Äôs a big enough discrepancy, dive into that function.</li></ol><h2 id="baseline">Baseline</h2><p>First things first, we need to build and compare perf locally (using <code>hyperfine</code> and the sample files noted in the contest‚Äôs rules and <code>rav1d</code>‚Äôs <a href="https://github.com/memorysafety/rav1d/blob/main/.github/workflows/build-and-benchmark-x86.yml">CI</a>).</p><p>We‚Äôll be using the single threaded version (<code>--threads 1</code>) to keep things simple.</p><p>For <code>rav1d</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ git clone git@github.com:memorysafety/rav1d.git <span>&amp;&amp;</span> <span>cd</span> rav1d <span>&amp;&amp;</span> git log -n1
</span></span><span><span>commit a654c1e82adb2d9a33ae50d2a82a7a747102cbb6
</span></span><span><span>$ rustc --version --verbose <span># set by rust-toolchain.toml</span>
</span></span><span><span>rustc 1.88.0-nightly <span>(</span>b45dd71d1 2025-04-30<span>)</span>
</span></span><span><span>...
</span></span><span><span>LLVM version: 20.1.2
</span></span><span><span>$ cargo build --release
</span></span><span><span>    Finished <span>`</span>release<span>`</span> profile <span>[</span>optimized<span>]</span> target<span>(</span>s<span>)</span> in ..
</span></span><span><span>$ hyperfine --warmup <span>2</span> <span>&#34;target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads 1&#34;</span>
</span></span><span><span>Benchmark 1: target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>  Time <span>(</span>mean ¬± œÉ<span>)</span>:     73.914 s ¬±  0.151 s    <span>[</span>User: 73.295 s, System: 0.279 s<span>]</span>
</span></span><span><span>  Range <span>(</span>min ‚Ä¶ max<span>)</span>:   73.770 s ‚Ä¶ 74.132 s    <span>10</span> runs
</span></span></code></pre></div><p>For <code>dav1d</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ git clone https://code.videolan.org/videolan/dav1d.git <span>&amp;&amp;</span> <span>cd</span> dav1d <span>&amp;&amp;</span> git checkout 1.5.1
</span></span><span><span>$ brew install llvm@20 <span>&amp;&amp;</span> <span>export</span> CC<span>=</span>clang<span>;</span> $CC --version
</span></span><span><span>Homebrew clang version 20.1.4
</span></span><span><span>$ meson setup build <span>&#34;-Dbitdepths=[&#39;8&#39;,&#39;16&#39;]&#34;</span>
</span></span><span><span>$ bear -- ninja -C build tools/dav1d
</span></span><span><span>...
</span></span><span><span><span>[</span>88/88<span>]</span> Linking target tools/dav1d
</span></span><span><span>$ hyperfine --warmup <span>2</span> <span>&#34;build/tools/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads 1&#34;</span>
</span></span><span><span>Benchmark 1: build/tools/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>  Time <span>(</span>mean ¬± œÉ<span>)</span>:     67.912 s ¬±  0.541 s    <span>[</span>User: 67.208 s, System: 0.282 s<span>]</span>
</span></span><span><span>  Range <span>(</span>min ‚Ä¶ max<span>)</span>:   66.933 s ‚Ä¶ 68.948 s    <span>10</span> runs
</span></span></code></pre></div><p>So <code>rav1d</code> is about 9% (6 seconds) slower than <code>dav1d</code> for that sample file, at least on an M3 chip.</p><p>(Ideally, <code>clang</code> and <code>rustc</code> should use the same LLVM version, but a patch version difference is probably fine.)</p><h2 id="profiling">Profiling</h2><p>I used <a href="https://github.com/mstange/samply">samply</a> which is my current go-to sampling profiler:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>./dav1d $ sudo samply record ./build/tools/dav1d -q -i /Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>./rav1d $ sudo samply record ./target/release/dav1d -q -i /Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span></code></pre></div><p>(The Rust binary is also called <code>dav1d</code>, which is a bit confusing.)</p><p>By default, <code>samply</code> uses a rate of 1000Hz, which means that (for example) any diff of 500 samples in a function will account for about 0.5 second of runtime difference.</p><p>Usually, starting with the ‚Äúinverted stack‚Äù view helps to narrow down interesting options (which we‚Äôll explore in <a href="#profiling-again-but-inverted">the next section</a>),
but this time we want to focus on the anchors we know should match: the asm functions.</p><p>You can view the full profiler snapshots online in the Firefox Profiler (<a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Fdav1d_profile.json.gz/calltree/?assemblyView=2~11c50~174~dav1d_filter_sbrow_cdef_8bpc&amp;globalTrackOrder=0&amp;search=dav1d_cdef_brow_8bpc&amp;thread=0&amp;v=10">dav1d</a>, <a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_baseline.json.gz/calltree/?globalTrackOrder=0&amp;search=rav1d_cdef_brow&amp;thread=0&amp;v=10">rav1d</a>),
but here are the relevant, filtered, clippings (<em>Note: these are not interactive. Check out the links if you want to explore more</em>).</p><p>First, here‚Äôs the <code>dav1d</code> (C) version (total number of samples: ~69,500):</p><p>Next, here‚Äôs the <code>rav1d</code> (Rust) version (total number of samples: ~75,150):</p><p>Look at the highlighted functions, <code>dav1d_cdef_brow_8bpc</code> and <code>rav1d_cdef_brow</code>.</p><p>There is a slight divergence between <code>dav1d</code> and <code>rav1d</code>: while the <code>_neon</code> extension notes the Arm-specific assembly functions that are shared between the two binaries, we see that:</p><ol><li><code>dav1d</code> calls <code>cdef_filter_8x8_neon</code> and <code>cdef_filter_4x4_neon</code>, and each of them dispatches the relevant assembly functions (either the <code>8</code> or the <code>4</code> version, respectively).</li><li><code>rav1d</code> calls <code>cdef_filter_neon_erased</code>, which handles the dispatch of <em>all</em> the assembly functions.</li></ol><p>We can also see that <code>cdef_filter8_pri_sec_edged_8bpc_neon</code> has almost identical sample counts in both snapshots, which means we are on the right track.</p><p>Let‚Äôs ignore the <code>cdef_filter4_pri_edged_8bpc_neon</code> function which <em>doesn‚Äôt match</em>, at least for now (<em>foreshadowing a possible part 2 in the series</em>).</p><p>This means that (A) the <em>Self</em> sample count for <code>dav1d_cdef_brow_8bpc</code> should match <code>rav1d_cdef_brow</code>,
<strong>and</strong> (B) that summing both <code>cdef_filter_{8x8,4x4}_neon</code> <em>Self</em> sample counts should match <code>cdef_filter_neon_erased</code> <em>Self</em> sample count.</p><p>Now we see something interesting: focusing in the second part, the summed <em>Self</em> sample count of <code>cdef_filter_{8x8,4x4}_neon</code> is about 400 samples, while <code>rav1d</code>‚Äôs <code>cdef_filter_neon_erased</code> is almost 670 samples. We can also see that <code>dav1d_cdef_brow_8bpc</code> is 1790 samples, vs <code>rav1d_cdef_brow</code>‚Äôs 2350 samples.</p><p>Together, this difference accounts for about 1% of the total runtime of <code>rav1d</code>!</p><p>Jumping to the <code>cdef_filter_neon_erased</code> implementation, except for a bunch of pointer casting using <code>.cast()</code>,
there‚Äôs only one ‚Äúbig thing‚Äù going on that‚Äôs not part of the call-to-asm machinery:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>#[deny(unsafe_op_in_unsafe_fn)]</span>
</span></span><span><span><span>pub</span> <span>unsafe</span> <span>extern</span> <span>&#34;C&#34;</span> <span>fn</span> <span>cdef_filter_neon_erased</span><span>&lt;</span>
</span></span><span><span>    <span>BD</span>: <span>BitDepth</span><span>,</span>
</span></span><span><span>    <span>const</span> W: <span>usize</span><span>,</span>
</span></span><span><span>    <span>const</span> H: <span>usize</span><span>,</span>
</span></span><span><span>    <span>const</span> <span>TMP_STRIDE</span>: <span>usize</span><span>,</span>
</span></span><span><span>    <span>const</span> <span>TMP_LEN</span>: <span>usize</span><span>,</span>
</span></span><span><span><span>&gt;</span><span>(</span>
</span></span><span><span>    <span>// .. snip ..
</span></span></span><span><span><span></span><span>)</span> <span>{</span>
</span></span><span><span>    <span>use</span> <span>crate</span>::src::align::Align16<span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>// .. snip ..
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>let</span> <span>mut</span> tmp_buf <span>=</span> Align16<span>([</span><span>0</span><span>u16</span><span>;</span> <span>TMP_LEN</span><span>]);</span>
</span></span><span><span>    <span>let</span> tmp <span>=</span> <span>&amp;</span><span>mut</span> tmp_buf<span>.</span><span>0</span><span>[</span><span>2</span> <span>*</span> <span>TMP_STRIDE</span> <span>+</span> <span>8</span><span>..</span><span>];</span>
</span></span><span><span>    
</span></span><span><span>    padding::<span>Fn</span>::neon::<span>&lt;</span><span>BD</span><span>,</span> W<span>&gt;</span><span>().</span>call::<span>&lt;</span><span>BD</span><span>&gt;</span><span>(</span>tmp<span>,</span> dst<span>,</span> stride<span>,</span> left<span>,</span> top<span>,</span> bottom<span>,</span> H<span>,</span> edges<span>);</span>
</span></span><span><span>    filter::<span>Fn</span>::neon::<span>&lt;</span><span>BD</span><span>,</span> W<span>&gt;</span><span>().</span>call<span>(</span>dst<span>,</span> stride<span>,</span> tmp<span>,</span> pri_strength<span>,</span> sec_strength<span>,</span> dir<span>,</span> damping<span>,</span> H<span>,</span> edges<span>,</span> bd<span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>With <code>TMP_LEN</code> being <code>12 * 16 + 8 = 200</code> or <code>12 * 8 + 8 = 104</code>, so <code>tmp_buf = [u16; 200]</code> in the worst case.
That‚Äôs a lot of memory to zero for a scratch buffer!</p><p>What does <code>dav1d</code> do here?</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#define DEFINE_FILTER(w, h, tmp_stride)                                      \
</span></span></span><span><span><span>static void                                                                  \
</span></span></span><span><span><span>cdef_filter_##w##x##h##_neon(</span><span>/* .. snip .. */</span><span>)                               \
</span></span></span><span><span><span>{                                                                            \
</span></span></span><span><span><span>    ALIGN_STK_16(uint16_t, tmp_buf, 12 * tmp_stride + 8,);                   \
</span></span></span><span><span><span>    uint16_t *tmp = tmp_buf + 2 * tmp_stride + 8;                            \
</span></span></span><span><span><span>    BF(dav1d_cdef_padding##w, neon)(tmp, dst, stride,                        \
</span></span></span><span><span><span>                                    left, top, bottom, h, edges);            \
</span></span></span><span><span><span>    BF(dav1d_cdef_filter##w, neon)(dst, stride, tmp, pri_strength,           \
</span></span></span><span><span><span>                                   sec_strength, dir, damping, h, edges      \
</span></span></span><span><span><span>                                   HIGHBD_TAIL_SUFFIX);                      \
</span></span></span><span><span><span>}
</span></span></span><span><span><span></span>
</span></span><span><span><span>DEFINE_FILTER</span><span>(</span><span>8</span><span>,</span> <span>8</span><span>,</span> <span>16</span><span>)</span>
</span></span><span><span><span>DEFINE_FILTER</span><span>(</span><span>4</span><span>,</span> <span>8</span><span>,</span> <span>8</span><span>)</span>
</span></span><span><span><span>DEFINE_FILTER</span><span>(</span><span>4</span><span>,</span> <span>4</span><span>,</span> <span>8</span><span>)</span>
</span></span></code></pre></div><p>A few macro expansions later, we get <code>uint16_t tmp_buf[200] __attribute__((aligned(16)));</code></p><p>This means that <code>tmp_buf</code> isn‚Äôt initialized by the <code>cdef_filter_{8x8,4x4}_neon</code> functions:
instead, it is used as a write destination for the <code>padding</code> assembly function,
and later by the <code>filter</code> assembly function as-is.
It seems likely that the compiler doesn‚Äôt know this initialization can be eliminated,</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ RUSTFLAGS<span>=</span><span>&#34;--emit=llvm-ir&#34;</span> cargo build --release --target aarch64-apple-darwin
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="llvm"><span><span><span>; rav1d::src::cdef::neon::cdef_filter_neon_erased
</span></span></span><span><span><span>; Function Attrs: nounwind
</span></span></span><span><span><span></span><span>define</span> <span>internal</span> <span>void</span> @_ZN5rav1d3src4cdef4neon23cdef_filter_neon_erased17h7e4dbe8ecff68724E<span>(</span><span>ptr</span> <span>no</span><span>undef</span> %dst<span>,</span> <span>i64</span> <span>no</span><span>undef</span> %stride<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %left<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %top<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %bottom<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %pri_strength<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %sec_strength<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %dir<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %damping<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %edges<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %bitdepth_max<span>,</span> <span>ptr</span> <span>nocapture</span> <span>readnone</span> %_dst<span>,</span> <span>ptr</span> <span>nocapture</span> <span>readnone</span> %_top<span>,</span> <span>ptr</span> <span>nocapture</span> <span>readnone</span> %_bottom<span>)</span> <span>unnamed_addr</span> #1 <span>{</span>
</span></span><span><span><span>start:</span>
</span></span><span><span>  %tmp_buf <span>=</span> <span>alloca</span> <span>[</span><span>400</span> <span>x</span> <span>i8</span><span>],</span> <span>align</span> <span>16</span>
</span></span><span><span>  <span>call</span> <span>void</span> @llvm.lifetime.start.p0<span>(</span><span>i64</span> <span>400</span><span>,</span> <span>ptr</span> <span>nonnull</span> %tmp_buf<span>)</span>
</span></span><span><span>  <span>call</span> <span>void</span> @llvm.memset.p0.i64<span>(</span><span>ptr</span> <span>no</span><span>undef</span> <span>nonnull</span> <span>align</span> <span>16</span> <span>dereferenceable</span><span>(</span><span>400</span><span>)</span> %tmp_buf<span>,</span> <span>i8</span> <span>0</span><span>,</span> <span>i64</span> <span>400</span><span>,</span> <span>i1</span> <span>false</span><span>)</span>
</span></span><span><span>  %_37 <span>=</span> <span>getelementptr</span> <span>inbounds</span> <span>nuw</span> <span>i8</span><span>,</span> <span>ptr</span> %tmp_buf<span>,</span> <span>i64</span> <span>80</span>
</span></span><span><span>  <span>call</span> <span>void</span> @dav1d_cdef_padding8_16bpc_neon<span>(</span><span>ptr</span> <span>no</span><span>undef</span> <span>nonnull</span> %_37<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %dst<span>,</span> <span>i64</span> <span>no</span><span>undef</span> %stride<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %left<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %top<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %bottom<span>,</span> <span>i32</span> <span>no</span><span>undef</span> <span>8</span><span>,</span> <span>i32</span> <span>no</span><span>undef</span> %edges<span>)</span> #121
</span></span><span><span>  %edges2.i <span>=</span> <span>zext</span> <span>i32</span> %edges <span>to</span> <span>i64</span>
</span></span><span><span>  %_0.i.i.i.i <span>=</span> <span>and</span> <span>i32</span> %bitdepth_max<span>,</span> <span>65535</span>
</span></span><span><span>  <span>call</span> <span>void</span> @dav1d_cdef_filter8_16bpc_neon<span>(</span><span>ptr</span> <span>no</span><span>undef</span> %dst<span>,</span> <span>i64</span> <span>no</span><span>undef</span> %stride<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> <span>nonnull</span> <span>readonly</span> <span>align</span> <span>2</span> %_37<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %pri_strength<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %sec_strength<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %dir<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %damping<span>,</span> <span>i32</span> <span>no</span><span>undef</span> <span>8</span><span>,</span> <span>i64</span> <span>no</span><span>undef</span> %edges2.i<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %_0.i.i.i.i<span>)</span> #121
</span></span><span><span>  <span>call</span> <span>void</span> @llvm.lifetime.end.p0<span>(</span><span>i64</span> <span>400</span><span>,</span> <span>ptr</span> <span>nonnull</span> %tmp_buf<span>)</span>
</span></span><span><span>  <span>ret</span> <span>void</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h3 id="avoid-needlessly-zeroing-buffers-with-maybeuninit">Avoid Needlessly Zeroing Buffers with <code>MaybeUninit</code></h3><p>This should be pretty easy actually! Rust has <a href="https://doc.rust-lang.org/std/mem/union.MaybeUninit.html"><code>std::mem::MaybeUninit</code></a> for just such an occasion:</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>-let mut tmp_buf = Align16([0u16; TMP_LEN])
</span></span></span><span><span><span></span><span>+let mut tmp_buf = Align16([MaybeUninit::&lt;u16&gt;::uninit(); TMP_LEN]);
</span></span></span></code></pre></div><p>We can still take a sub-slice safely (<code>&amp;mut tmp_buf.0[2 * TMP_STRIDE + 8..]</code>), but we will need to update the signatures of the inner functions to use the new type (<code>tmp: *mut MaybeUninit&lt;u16&gt;</code>, <code>tmp: &amp;[MaybeUninit&lt;u16&gt;]</code>).</p><p>Since the code that used these was unsafe anyway, we don‚Äôt need to add any new unsafe blocks - only to verify that the existing code hasn‚Äôt changed (w.r.t <code>dav1d</code>) to rely on this buffer being zeroed.</p><p>Before, <code>cdef_filter_neon_erased</code> had 670 <em>Self</em> samples. Re-running the profiler, we get <a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_after.json.gz/calltree/?globalTrackOrder=0&amp;search=rav1d_cdef_brow&amp;thread=0&amp;v=10">a new snapshot</a>:</p><p>Just 274 samples! Slightly less than <code>dav1d</code>‚Äôs <code>cdef_filter_{8x8,4x4}_neon</code> <em>Self</em> sample count.</p><p>Maybe this isn‚Äôt the only place where time is wasted zeroing buffers? A quick search for other big <code>Align16</code> buffers resulted in this lucky find:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span>(</span><span>crate</span><span>)</span> <span>fn</span> <span>rav1d_cdef_brow</span><span>&lt;</span><span>BD</span>: <span>BitDepth</span><span>&gt;</span><span>(</span><span>/* .. snip ..*/</span><span>)</span>
</span></span><span><span><span>{</span>
</span></span><span><span>    <span>// .. snip ..
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>for</span> by <span>in</span> <span>(</span>by_start<span>..</span>by_end<span>).</span>step_by<span>(</span><span>2</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>// .. snip ..
</span></span></span><span><span><span></span>        <span>let</span> <span>mut</span> lr_bak <span>=</span>
</span></span><span><span>            Align16<span>([[[[</span><span>0.</span>into<span>();</span> <span>2</span> <span>/* x */</span><span>];</span> <span>8</span> <span>/* y */</span><span>];</span> <span>3</span> <span>/* plane */</span> <span>];</span> <span>2</span> <span>/* idx */</span><span>]);</span>
</span></span><span><span>        
</span></span><span><span>        <span>// .. snip ..
</span></span></span><span><span><span></span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Again, the matching code from <code>dav1d</code> doesn‚Äôt initialize this buffer.
Here, switching to <code>MaybeUninit</code> is more difficult, but we can still offer a modest improvement: we‚Äôll only need to do the initialization <strong>once</strong> if we hoist <code>lr_bak</code> to the top level!</p><div><pre tabindex="0"><code data-lang="diff"><span><span>pub(crate) fn rav1d_cdef_brow&lt;BD: BitDepth&gt;(/* .. snip ..*/)
</span></span><span><span>{
</span></span><span><span>    // .. snip ..
</span></span><span><span><span>+   let mut lr_bak =
</span></span></span><span><span><span>+       Align16([[[[0.into(); 2 /* x */]; 8 /* y */]; 3 /* plane */ ]; 2 /* idx */]);
</span></span></span><span><span><span></span>        
</span></span><span><span>    for by in (by_start..by_end).step_by(2) {
</span></span><span><span>        // .. snip ..
</span></span><span><span><span>-       let mut lr_bak =
</span></span></span><span><span><span>-           Align16([[[[0.into(); 2 /* x */]; 8 /* y */]; 3 /* plane */ ]; 2 /* idx */]);
</span></span></span><span><span><span></span>        
</span></span><span><span>        // .. snip ..
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>Since <code>dav1d</code> never initialized it anyway, we know that logically any data read from this buffer was written beforehand with a valid value
(which really helps to drive home the idea that <a href="https://www.ralfj.de/blog/2021/11/18/ub-good-idea.html">Undefined Behavior deserves a better reputation</a>). The savings are very small here, but every penny counts!</p><p>Running the full benchmark, we get a nice speed boost from the original <code>73.914 s ¬± 0.151 s</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ hyperfine --warmup <span>2</span> <span>&#34;target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads 1&#34;</span>
</span></span><span><span>Benchmark 1: target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>  Time <span>(</span>mean ¬± œÉ<span>)</span>:     72.644 s ¬±  0.250 s    <span>[</span>User: 72.023 s, System: 0.239 s<span>]</span>
</span></span><span><span>  Range <span>(</span>min ‚Ä¶ max<span>)</span>:   72.281 s ‚Ä¶ 73.098 s    <span>10</span> runs
</span></span></code></pre></div><p>There‚Äôs still a way to go to <code>dav1d</code>‚Äôs <code>67.912 s ¬± 0.541 s</code>, but 1.2 seconds (1.5%) improvement in total runtime is a great start, and covers about 20% of the performance diff between the two.</p><h2 id="profiling-again-but-inverted">Profiling Again, But Inverted</h2><p>Let‚Äôs reload the profiler outputs from the start, but use the ‚Äúinverted stack‚Äù view.</p><p>There are a few options we can explore for optimization, but the function that got my attention was <code>add_temporal_candidate</code>: the difference between the Rust and the C version is significant enough (~400 samples, about 0.5 seconds),
and the function itself seems innocuous: it‚Äôs about 50 lines of <code>if</code>s and <code>for</code>s, with a few calls to short utility functions.</p><p>To help us find out where we are bleeding out the missing performance, we can try to recompile <code>rav1d</code> with debug symbols.
The <code>rav1d</code> project helpfully defines a <code>[profile.release-with-debug]</code> in its <code>Cargo.toml</code>, allowing us to run:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ cargo build --profile<span>=</span>release-with-debug
</span></span><span><span>$ sudo samply record target/release-with-debug/dav1d ...
</span></span></code></pre></div><p>What we get back is slightly different than before (<a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_with_debug.json.gz/calltree/?globalTrackOrder=0&amp;invertCallstack&amp;thread=0&amp;v=10">Link</a>): the <code>release-with-debug</code> profile will not be as-optimized,
and small functions calls appear bigger than they really are, but we get a <strong>line-by-line sample breakdown of the function</strong>, and it should steer us in the right direction.</p><p>If you scroll a little, one thing that will jump out to you will be that the <code>if cand.mv.mv[0] == mv {</code> and <code>if cand.mv == mvp {</code> lines seem to cover a combined 600 samples!</p><p>Let‚Äôs pull up <code>mv: Mv</code>‚Äôs definition:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>#[derive(Clone, Copy, PartialEq, Eq, Default, FromZeroes, FromBytes, AsBytes)]</span>
</span></span><span><span><span>#[repr(C)]</span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>Mv</span> <span>{</span>
</span></span><span><span>    <span>pub</span> y: <span>i16</span><span>,</span>
</span></span><span><span>    <span>pub</span> x: <span>i16</span><span>,</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Huh. How can this be slow? It‚Äôs just <code>#[derive(PartialEq)]</code>.</p><p><img src="https://ohadravid.github.io/2025-05-rav1d-faster/futurama_fry.jpg" alt="Futurama Fry Looking Suspicious" width="40%" height="414px"/></p><p>And even more suspiciously, the <code>dav1d</code> version is slightly different, and uses <code>mvstack[n].mv.n == mvp.n</code> to do the same comparisons.
But what is <code>n</code>? Looking at <code>dav1d</code>‚Äôs definition of <code>mv</code>, we find:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>union</span> mv <span>{</span>
</span></span><span><span>    <span>struct</span> <span>{</span>
</span></span><span><span>        <span>int16_t</span> y<span>,</span> x<span>;</span>
</span></span><span><span>    <span>};</span>
</span></span><span><span>    <span>uint32_t</span> n<span>;</span>
</span></span><span><span><span>}</span> mv<span>;</span>
</span></span></code></pre></div><p>It seems like the <code>dav1d</code> authors knew that comparing two <code>i16</code>s can be slow, so when they compare two <code>mv</code>s, they treat them as <code>u32</code>s.</p><h3 id="replace-field-wise-equality-with-byte-wise-equality-that-optimizes-better">Replace Field-wise Equality with Byte-wise Equality that Optimizes Better</h3><p>Can this be the problem?</p><p>Fortunately, we have a different option: We can use <code>transmute</code> to re-interpret <code>Mv</code> as a <code>u32</code>, and use that to implement <code>PartialEq</code>.</p><p>Firing up <a href="https://godbolt.org/z/r9MfTeY8b">Godbolt</a>, we can inspect the generated code for the two ways to do the comparison:</p><p>Clearly the <code>transmute</code> version is superior, but can we avoid the <code>unsafe</code> block?<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><p>It turns out that the <code>zerocopy</code> crate can statically verify the <a href="https://docs.rs/zerocopy/latest/zerocopy/trait.IntoBytes.html#safety">safety requirements</a> for a <code>struct</code> to be represented as <code>&amp;[u8]</code>, allowing us to write:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>use</span> zerocopy::<span>{</span>AsBytes<span>,</span> FromBytes<span>,</span> FromZeroes<span>};</span>
</span></span><span><span>
</span></span><span><span><span>#[derive(Clone, Copy, Eq, Default, FromZeroes, FromBytes, AsBytes)]</span>
</span></span><span><span><span>#[repr(C)]</span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>Mv</span> <span>{</span>
</span></span><span><span>    <span>pub</span> y: <span>i16</span><span>,</span>
</span></span><span><span>    <span>pub</span> x: <span>i16</span><span>,</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>impl</span> <span>PartialEq</span> <span>for</span> Mv <span>{</span>
</span></span><span><span>    <span>#[inline(always)]</span>
</span></span><span><span>    <span>fn</span> <span>eq</span><span>(</span><span>&amp;</span>self<span>,</span> other: <span>&amp;</span><span>Self</span><span>)</span> -&gt; <span>bool</span> <span>{</span>
</span></span><span><span>        self<span>.</span>as_bytes<span>()</span> <span>==</span> other<span>.</span>as_bytes<span>()</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Which produces the same (optimized) assembly we saw when we used <code>transmute</code>.</p><p>After implementing similar optimizations for <code>RefMvs{Mv,Ref}Pair</code>, we can re-run the benchmark:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ hyperfine --warmup <span>2</span> <span>&#34;target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads 1&#34;</span>
</span></span><span><span>Benchmark 1: target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>  Time <span>(</span>mean ¬± œÉ<span>)</span>:     72.182 s ¬±  0.289 s    <span>[</span>User: 71.501 s, System: 0.242 s<span>]</span>
</span></span><span><span>  Range <span>(</span>min ‚Ä¶ max<span>)</span>:   71.850 s ‚Ä¶ 72.722 s    <span>10</span> runs
</span></span></code></pre></div><p>This is <em>another</em> 0.5 second improvement over our previous result (<code>72.644 s ¬± 0.250 s</code>), or a 2.3% improvement over the baseline (<code>73.914 s ¬± 0.151 s</code>).</p><p>We are now only 4.2 seconds from <code>dav1d</code>‚Äôs <code>67.912 s ¬± 0.541 s</code>, so we covered about 30% of the performance diff we saw at the start of this article.</p><p>You might be wondering why the default implementation of <code>PartialEq</code> results in bad code generation,
and <a href="https://github.com/memorysafety/rav1d/pull/1400#issuecomment-2891734817">a comment</a> on the PR adding these impls pointed to <a href="https://github.com/rust-lang/rust/issues/140167">Rust issue #140167</a>,
which tracks exactly this type of problem.</p><p>If you consider the C case, when using a <code>struct { int16_t y, x; }</code> it‚Äôs possible to initialize only <code>y</code> while leaving <code>x</code> uninitialized.
As long as equality is checked with <code>this.y == other.y &amp;&amp; this.x == other.x</code> and all <code>y</code>s are different, you don‚Äôt get any UB.</p><p>Therefore, it‚Äôs invalid to optimize this to a single memory load and compare <strong>unless the code can guarantee that all fields are always initialized</strong>.
However, quoting this <a href="https://github.com/rust-lang/rust/issues/140167#issuecomment-2895174679">comment</a> by @hanna-kruppe on the issue:</p><blockquote><p>That‚Äôs not simply a missed optimization opportunity. While the load of the second field can‚Äôt load poison/undef, that property is control-dependent. ..</p></blockquote><h2 id="summary">Summary</h2><p>Using a few profiler snapshots from the <code>samply</code> profiler, we compared running <code>rav1d</code> and <code>dav1d</code> on the same input file, saw a 6-second (9%) runtime difference, and found two relatively low hanging fruits we could optimize:</p><ol><li>Avoiding an expensive zero-initialization in a hot, Arm-specific code path (<a href="https://github.com/memorysafety/rav1d/pull/1397">PR</a>), improving runtime by 1.2 seconds (-1.6%).</li><li>Switching the default <code>PartialEq</code> impls of small numeric <code>struct</code>s with an optimized version that re-interpret them as bytes (<a href="https://github.com/memorysafety/rav1d/pull/1400">PR</a>), improving runtime by 0.5 seconds (-0.7%).</li></ol><p>Each of these provide a nice speedup despite being only a few dozen lines in total, and without introducing new unsafety into the codebase.</p><p>The <code>rav1d</code> project maintainers were nice and responsive, and helped make these PRs more correct and better overall (big shout out to @kkysen üöÄ).</p><p>There is still a gap of about 6% between the two implementations so there are still many more optimizations to discover,
and I suspect this approach of comparing between profiler snapshots of <a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Fdav1d_profile.json.gz/calltree/?assemblyView=2~11c50~174~dav1d_filter_sbrow_cdef_8bpc&amp;globalTrackOrder=0&amp;search=dav1d_cdef_brow_8bpc&amp;thread=0&amp;v=10"><code>dav1d</code></a> and <a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_after.json.gz/calltree/?globalTrackOrder=0&amp;search=rav1d_cdef_brow&amp;thread=0&amp;v=10"><code>rav1d</code></a> will yield at least some of them.</p><p>Go ahead and give this a try! Maybe <code>rav1d</code> can eventually become faster than <code>dav1d</code> üëÄü¶Ä.</p><p>Discuss on <a href="https://www.reddit.com/r/rust/comments/1ksnljw/making_the_rav1d_video_decoder_1_faster/">r/rust</a>, <a href="https://lobste.rs/s/j3mzif/making_rav1d_video_decoder_1_faster">lobsters</a>, <a href="https://news.ycombinator.com/item?id=44061160">HN</a>! üëã</p><p><em>If you liked this, you might also like <a href="https://ohadravid.github.io/posts/2025-01-debugging-vit-and-tensorrt/">Debugging a Vision Transformer Compilation Issue</a> and <a href="https://ohadravid.github.io/posts/2023-03-rusty-python/">Making Python 100x faster with less than 100 lines of Rust</a></em>.</p></div></div></div>
  </body>
</html>
