<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggozad/haiku.rag">Original</a>
    <h1>Retrieval Augmented Generation Based on SQLite</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">A Retrieval-Augmented Generation (RAG) library on SQLite.</p>

<ul dir="auto">
<li><strong>Local SQLite</strong>: No need to run additional servers</li>
<li><strong>Support for various embedding providers</strong>: You can use Ollama, VoyageAI, OpenAI or add your own</li>
<li><strong>Hybrid Search</strong>: Vector search using <code>sqlite-vec</code> combined with full-text search <code>FTS5</code>, using Reciprocal Rank Fusion</li>
<li><strong>File monitoring</strong> when run as a server automatically indexing your files</li>
<li><strong>Extended file format Support</strong>: Parse 40+ file formats including PDF, DOCX, HTML, Markdown, audio and more. Or add a url!</li>
<li><strong>MCP server</strong> Exposes functionality as MCP tools.</li>
<li><strong>CLI commands</strong> Access all functionality from your terminal</li>
<li><strong>Python client</strong> Call <code>haiku.rag</code> from your own python applications.</li>
</ul>


<p dir="auto">By default Ollama (with the <code>mxbai-embed-large</code> model) is used for the embeddings.
For other providers use:</p>
<ul dir="auto">
<li><strong>VoyageAI</strong>: <code>uv pip install haiku.rag --extra voyageai</code></li>
<li><strong>OpenAI</strong>: <code>uv pip install haiku.rag --extra openai</code></li>
</ul>

<p dir="auto">You can set the directories to monitor using the <code>MONITOR_DIRECTORIES</code> environment variable (as comma separated values) :</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Monitor single directory
export MONITOR_DIRECTORIES=&#34;/path/to/documents,/another_path/to/documents&#34;"><pre><span><span>#</span> Monitor single directory</span>
<span>export</span> MONITOR_DIRECTORIES=<span><span>&#34;</span>/path/to/documents,/another_path/to/documents<span>&#34;</span></span></pre></div>
<p dir="auto">If you want to use an alternative embeddings provider (Ollama being the default) you will need to set the provider details through environment variables:</p>
<p dir="auto">By default:</p>
<div dir="auto" data-snippet-clipboard-copy-content="EMBEDDINGS_PROVIDER=&#34;ollama&#34;
EMBEDDINGS_MODEL=&#34;mxbai-embed-large&#34; # or any other model
EMBEDDINGS_VECTOR_DIM=1024"><pre>EMBEDDINGS_PROVIDER=<span><span>&#34;</span>ollama<span>&#34;</span></span>
EMBEDDINGS_MODEL=<span><span>&#34;</span>mxbai-embed-large<span>&#34;</span></span> <span><span>#</span> or any other model</span>
EMBEDDINGS_VECTOR_DIM=1024</pre></div>
<p dir="auto">For VoyageAI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="EMBEDDINGS_PROVIDER=&#34;voyageai&#34;
EMBEDDINGS_MODEL=&#34;voyage-3.5&#34; # or any other model
EMBEDDINGS_VECTOR_DIM=1024
VOYAGE_API_KEY=&#34;your-api-key&#34;"><pre>EMBEDDINGS_PROVIDER=<span><span>&#34;</span>voyageai<span>&#34;</span></span>
EMBEDDINGS_MODEL=<span><span>&#34;</span>voyage-3.5<span>&#34;</span></span> <span><span>#</span> or any other model</span>
EMBEDDINGS_VECTOR_DIM=1024
VOYAGE_API_KEY=<span><span>&#34;</span>your-api-key<span>&#34;</span></span></pre></div>
<p dir="auto">For OpenAI:</p>
<div dir="auto" data-snippet-clipboard-copy-content="EMBEDDINGS_PROVIDER=&#34;openai&#34;
EMBEDDINGS_MODEL=&#34;text-embedding-3-small&#34; # or text-embedding-3-large
EMBEDDINGS_VECTOR_DIM=1536
OPENAI_API_KEY=&#34;your-api-key&#34;"><pre>EMBEDDINGS_PROVIDER=<span><span>&#34;</span>openai<span>&#34;</span></span>
EMBEDDINGS_MODEL=<span><span>&#34;</span>text-embedding-3-small<span>&#34;</span></span> <span><span>#</span> or text-embedding-3-large</span>
EMBEDDINGS_VECTOR_DIM=1536
OPENAI_API_KEY=<span><span>&#34;</span>your-api-key<span>&#34;</span></span></pre></div>

<p dir="auto"><code>haiku.rag</code> includes a CLI application for managing documents and performing searches from the command line:</p>

<div dir="auto" data-snippet-clipboard-copy-content="# List all documents
haiku-rag list

# Add document from text
haiku-rag add &#34;Your document content here&#34;

# Add document from file or URL
haiku-rag add-src /path/to/document.pdf
haiku-rag add-src https://example.com/article.html

# Get and display a specific document
haiku-rag get 1

# Delete a document by ID
haiku-rag delete 1

# Search documents
haiku-rag search &#34;machine learning&#34;

# Search with custom options
haiku-rag search &#34;python programming&#34; --limit 10 --k 100

# Start file monitoring &amp; MCP server (default HTTP transport)
haiku-rag serve # --stdio for stdio transport or --sse for SSE transport"><pre><span><span>#</span> List all documents</span>
haiku-rag list

<span><span>#</span> Add document from text</span>
haiku-rag add <span><span>&#34;</span>Your document content here<span>&#34;</span></span>

<span><span>#</span> Add document from file or URL</span>
haiku-rag add-src /path/to/document.pdf
haiku-rag add-src https://example.com/article.html

<span><span>#</span> Get and display a specific document</span>
haiku-rag get 1

<span><span>#</span> Delete a document by ID</span>
haiku-rag delete 1

<span><span>#</span> Search documents</span>
haiku-rag search <span><span>&#34;</span>machine learning<span>&#34;</span></span>

<span><span>#</span> Search with custom options</span>
haiku-rag search <span><span>&#34;</span>python programming<span>&#34;</span></span> --limit 10 --k 100

<span><span>#</span> Start file monitoring &amp; MCP server (default HTTP transport)</span>
haiku-rag serve <span><span>#</span> --stdio for stdio transport or --sse for SSE transport</span></pre></div>
<p dir="auto">All commands support the <code>--db</code> option to specify a custom database path. Run</p>

<p dir="auto">to see additional parameters for a command.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">File Monitoring &amp; MCP server</h2><a id="user-content-file-monitoring--mcp-server" aria-label="Permalink: File Monitoring &amp; MCP server" href="#file-monitoring--mcp-server"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can start the server (using Streamble HTTP, stdio or SSE transports) with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Start with default HTTP transport
haiku-rag serve # --stdio for stdio transport or --sse for SSE transport"><pre><span><span>#</span> Start with default HTTP transport</span>
haiku-rag serve <span><span>#</span> --stdio for stdio transport or --sse for SSE transport</span></pre></div>
<p dir="auto">You need to have set the <code>MONITOR_DIRECTORIES</code> environment variable for monitoring to take place.</p>

<p dir="auto"><code>haiku.rag</code> can watch directories for changes and automatically update the document store:</p>
<ul dir="auto">
<li><strong>Startup</strong>: Scan all monitored directories and add any new files</li>
<li><strong>File Added/Modified</strong>: Automatically parse and add/update the document in the database</li>
<li><strong>File Deleted</strong>: Remove the corresponding document from the database</li>
</ul>

<p dir="auto"><code>haiku.rag</code> includes a Model Context Protocol (MCP) server that exposes RAG functionality as tools for AI assistants like Claude Desktop. The MCP server provides the following tools:</p>
<ul dir="auto">
<li><code>add_document_from_file</code> - Add documents from local file paths</li>
<li><code>add_document_from_url</code> - Add documents from URLs</li>
<li><code>add_document_from_text</code> - Add documents from raw text content</li>
<li><code>search_documents</code> - Search documents using hybrid search</li>
<li><code>get_document</code> - Retrieve specific documents by ID</li>
<li><code>list_documents</code> - List all documents with pagination</li>
<li><code>delete_document</code> - Delete documents by ID</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Using <code>haiku.rag</code> from python</h2><a id="user-content-using-haikurag-from-python" aria-label="Permalink: Using haiku.rag from python" href="#using-haikurag-from-python"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<div dir="auto" data-snippet-clipboard-copy-content="from pathlib import Path
from haiku.rag.client import HaikuRAG

# Use as async context manager (recommended)
async with HaikuRAG(&#34;path/to/database.db&#34;) as client:
    # Create document from text
    doc = await client.create_document(
        content=&#34;Your document content here&#34;,
        uri=&#34;doc://example&#34;,
        metadata={&#34;source&#34;: &#34;manual&#34;, &#34;topic&#34;: &#34;example&#34;}
    )

    # Create document from file (auto-parses content)
    doc = await client.create_document_from_source(&#34;path/to/document.pdf&#34;)

    # Create document from URL
    doc = await client.create_document_from_source(&#34;https://example.com/article.html&#34;)

    # Retrieve documents
    doc = await client.get_document_by_id(1)
    doc = await client.get_document_by_uri(&#34;file:///path/to/document.pdf&#34;)

    # List all documents with pagination
    docs = await client.list_documents(limit=10, offset=0)

    # Update document content
    doc.content = &#34;Updated content&#34;
    await client.update_document(doc)

    # Delete document
    await client.delete_document(doc.id)

    # Search documents using hybrid search (vector + full-text)
    results = await client.search(&#34;machine learning algorithms&#34;, limit=5)
    for chunk, score in results:
        print(f&#34;Score: {score:.3f}&#34;)
        print(f&#34;Content: {chunk.content}&#34;)
        print(f&#34;Document ID: {chunk.document_id}&#34;)
        print(&#34;---&#34;)"><pre><span>from</span> <span>pathlib</span> <span>import</span> <span>Path</span>
<span>from</span> <span>haiku</span>.<span>rag</span>.<span>client</span> <span>import</span> <span>HaikuRAG</span>

<span># Use as async context manager (recommended)</span>
<span>async</span> <span>with</span> <span>HaikuRAG</span>(<span>&#34;path/to/database.db&#34;</span>) <span>as</span> <span>client</span>:
    <span># Create document from text</span>
    <span>doc</span> <span>=</span> <span>await</span> <span>client</span>.<span>create_document</span>(
        <span>content</span><span>=</span><span>&#34;Your document content here&#34;</span>,
        <span>uri</span><span>=</span><span>&#34;doc://example&#34;</span>,
        <span>metadata</span><span>=</span>{<span>&#34;source&#34;</span>: <span>&#34;manual&#34;</span>, <span>&#34;topic&#34;</span>: <span>&#34;example&#34;</span>}
    )

    <span># Create document from file (auto-parses content)</span>
    <span>doc</span> <span>=</span> <span>await</span> <span>client</span>.<span>create_document_from_source</span>(<span>&#34;path/to/document.pdf&#34;</span>)

    <span># Create document from URL</span>
    <span>doc</span> <span>=</span> <span>await</span> <span>client</span>.<span>create_document_from_source</span>(<span>&#34;https://example.com/article.html&#34;</span>)

    <span># Retrieve documents</span>
    <span>doc</span> <span>=</span> <span>await</span> <span>client</span>.<span>get_document_by_id</span>(<span>1</span>)
    <span>doc</span> <span>=</span> <span>await</span> <span>client</span>.<span>get_document_by_uri</span>(<span>&#34;file:///path/to/document.pdf&#34;</span>)

    <span># List all documents with pagination</span>
    <span>docs</span> <span>=</span> <span>await</span> <span>client</span>.<span>list_documents</span>(<span>limit</span><span>=</span><span>10</span>, <span>offset</span><span>=</span><span>0</span>)

    <span># Update document content</span>
    <span>doc</span>.<span>content</span> <span>=</span> <span>&#34;Updated content&#34;</span>
    <span>await</span> <span>client</span>.<span>update_document</span>(<span>doc</span>)

    <span># Delete document</span>
    <span>await</span> <span>client</span>.<span>delete_document</span>(<span>doc</span>.<span>id</span>)

    <span># Search documents using hybrid search (vector + full-text)</span>
    <span>results</span> <span>=</span> <span>await</span> <span>client</span>.<span>search</span>(<span>&#34;machine learning algorithms&#34;</span>, <span>limit</span><span>=</span><span>5</span>)
    <span>for</span> <span>chunk</span>, <span>score</span> <span>in</span> <span>results</span>:
        <span>print</span>(<span>f&#34;Score: <span><span>{</span><span>score</span>:.3f<span>}</span></span>&#34;</span>)
        <span>print</span>(<span>f&#34;Content: <span><span>{</span><span>chunk</span>.<span>content</span><span>}</span></span>&#34;</span>)
        <span>print</span>(<span>f&#34;Document ID: <span><span>{</span><span>chunk</span>.<span>document_id</span><span>}</span></span>&#34;</span>)
        <span>print</span>(<span>&#34;---&#34;</span>)</pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="async with HaikuRAG(&#34;database.db&#34;) as client:

    results = await client.search(
        query=&#34;machine learning&#34;,
        limit=5,  # Maximum results to return, defaults to 5
        k=60      # RRF parameter for reciprocal rank fusion, defaults to 60
    )

    # Process results
    for chunk, relevance_score in results:
        print(f&#34;Relevance: {relevance_score:.3f}&#34;)
        print(f&#34;Content: {chunk.content}&#34;)
        print(f&#34;From document: {chunk.document_id}&#34;)"><pre><span>async</span> <span>with</span> <span>HaikuRAG</span>(<span>&#34;database.db&#34;</span>) <span>as</span> <span>client</span>:

    <span>results</span> <span>=</span> <span>await</span> <span>client</span>.<span>search</span>(
        <span>query</span><span>=</span><span>&#34;machine learning&#34;</span>,
        <span>limit</span><span>=</span><span>5</span>,  <span># Maximum results to return, defaults to 5</span>
        <span>k</span><span>=</span><span>60</span>      <span># RRF parameter for reciprocal rank fusion, defaults to 60</span>
    )

    <span># Process results</span>
    <span>for</span> <span>chunk</span>, <span>relevance_score</span> <span>in</span> <span>results</span>:
        <span>print</span>(<span>f&#34;Relevance: <span><span>{</span><span>relevance_score</span>:.3f<span>}</span></span>&#34;</span>)
        <span>print</span>(<span>f&#34;Content: <span><span>{</span><span>chunk</span>.<span>content</span><span>}</span></span>&#34;</span>)
        <span>print</span>(<span>f&#34;From document: <span><span>{</span><span>chunk</span>.<span>document_id</span><span>}</span></span>&#34;</span>)</pre></div>
</article></div></div>
  </body>
</html>
