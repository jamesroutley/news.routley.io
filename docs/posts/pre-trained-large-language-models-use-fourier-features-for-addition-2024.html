<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2406.03445">Original</a>
    <h1>Pre-Trained Large Language Models Use Fourier Features for Addition (2024)</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    <p>
  [Submitted on 5 Jun 2024]</p>
    
                
    <p><a href="https://concurrencycorner.blogspot.com/pdf/2406.03445">View PDF</a>
    <a href="https://arxiv.org/html/2406.03445v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Deqing Fu [<a href="https://concurrencycorner.blogspot.com/show-email/20287840/2406.03445" rel="nofollow">view email</a>]      </p></div></div>
  </body>
</html>
