<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/s44271-024-00157-7">Original</a>
    <h1>Psychoacoustic and archeoacoustic nature of ancient Aztec skull whistles</h1>
    
    <div id="readability-page-1" class="page"><div>
                    <section data-title="Introduction"><div id="Sec1-section"><h2 id="Sec1">Introduction</h2><div id="Sec1-content"><p>Various ancient sound tools and musical instruments have been discovered in archeological excavation sites throughout the world. Since Paleolithic times, humans created sophisticated musical instruments to produce sounds for various purposes ranging from imitating environmental and animal sounds up to aesthetic and symbolic manifestations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="McDermott, J. The evolution of music. Nature 453, 287–288 (2008)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR1" id="ref-link-section-d62632563e422">1</a></sup>. These ancient musical instruments and tools often have specific natural and/or mythological associations in their cultural context, given their sound quality (sound iconography) as well as their construction and visual appearance (visual iconography).</p><p>We here experimentally investigated the archeological case of Aztec skull whistles, which can be dated back to 1250–1521 CE<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Both, A. A. Skull whistles of the Aztecs (Totenkopfpfeifen der Azteken). Baessler-Arch. 53, 43–54 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR2" id="ref-link-section-d62632563e429">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Velázquez Cabrera, R. Silbato de la muerte. Arqueología 42, 184–202 (2009)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR3" id="ref-link-section-d62632563e432">3</a></sup>. Skull whistles are uniquely crafted and made from clay with an approximate size of 3–5 cm. They have a particular sound production mechanism, which makes the Aztec skull whistles rather unique and unusual sound tools compared to historical and contemporary music instruments (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig1">1</a>), and they have not yet been classified according to the (revised) Hornbostel-Sachs classification of musical instruments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="von Hornbostel, E. M. &amp; Sachs, C. Systematik der Musikinstrumente. Ein Versuch. Z. Ethnol. 553, 590 (1914)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR4" id="ref-link-section-d62632563e439">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="MIMO Consortium. Revision of the Hornbostel-Sachs Classification of Musical Instruments by the MIMO Consortium. 
                  http://www.mimo-international.com/documents/hornbostel%20sachs.pdf
                  
                 (2008)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR5" id="ref-link-section-d62632563e442">5</a></sup>. Their configuration allows for the collision of various airstreams, which were exclusively developed in prehispanic Mesoamerica<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Beristain, S., Menchaca, R. &amp; Velazquez, R. Ancient noise generators. J. Acoust. Soc. Am. 112, 2368–2368 (2002)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR6" id="ref-link-section-d62632563e446">6</a></sup>. Skull whistles produce a specific non-linear and noisy sound (wind- or hiss-like sound) that can have a shrill, piercing, and scream-like sound quality when played with intensive air pressure<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Both, A. A. Skull whistles of the Aztecs (Totenkopfpfeifen der Azteken). Baessler-Arch. 53, 43–54 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR2" id="ref-link-section-d62632563e450">2</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Both, A. A. On the context of imitative and associative processes in Prehispanic music. Stud. zur. Musikarch.äologie 5, 319–332 (2006)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR7" id="ref-link-section-d62632563e453">7</a></sup>.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Original exemplars and replicas of Aztec skull whistles."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Original exemplars and replicas of Aztec skull whistles.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://mclare.blog/articles/s44271-024-00157-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="262"/></picture></a></div><p><b>a</b> Human sacrifice with original skull whistle (small red box and enlarged rotated view in lower right) discovered 1987–89 at the Ehecatl-Quetzalcoatl temple in Mexico City, Mexico (burial 20; photo by Salvador Guillien Arroyo, Proyecto Tlatelolco 1987–2006, INAH Mexico). <b>b</b> Three original skull whistle exemplars from the collection of the Ethnological Museum in Berlin (Staatliche Mussen zu Berlin, Germany; photo by Claudia Obrocki). <b>c</b> A computer-tomographically (CT) reconstructed cross-section of the right exemplar in (<b>b</b>) (IV Ca 2621m) showing the four major compartments ((<b>a</b>) tubular airduct with constricted passage, (<b>b</b>) hemispherical counterpressure chamber, (<b>c</b>) collision chamber located between a/b, (<b>d</b>) bell). <b>d</b> Replicas of original skull whistles were built as a copy of original exemplars in shape and material. Exemplars “Replica 2621z” and “Replica 2621v” (manufactured by Arnd Adje Both and Osvaldo Padrón Pérez) as replicas of the original skull whistles with inventory numbers IV Ca 2621z and IV Ca 2621 v as shown in (<b>b</b>). <b>e</b> Digitalization and 3D reconstruction of the skull whistle replicas by using CT scans of the replicas. <b>f</b> 3D models of an original skull whistle (Ethnological Museum IV Ca 2621u) and the Replica 2621z demonstrate the air flow dynamics, construction similarity, and sound generation process.</p></div></figure></div><p>Skull whistles have not received much scientific attention so far<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Both, A. A. Skull whistles of the Aztecs (Totenkopfpfeifen der Azteken). Baessler-Arch. 53, 43–54 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR2" id="ref-link-section-d62632563e517">2</a></sup> despite a very early reference to these instruments in the late 19th century<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Kollmann, J. Flöten und Pfeifen aus Alt-Mexiko. in Adolf Bastian als Fest-Gruss zu seinem 70. Geburtstage 559–574 (Berlin, 1896)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR8" id="ref-link-section-d62632563e521">8</a></sup>. However, such Aztec skull whistles are now frequently mentioned in popular contexts and have received a lot of media attention lately, given their potential spine-chilling sound similar to scary sound effects in horror movies and human screams<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Trevor, C., Arnal, L. H. &amp; Frühholz, S. Terrifying film music mimics alarming acoustic feature of human screams. J. Acoust. Soc. Am. 147, EL540–EL545 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR9" id="ref-link-section-d62632563e525">9</a></sup>. In such popular contexts, they are often labeled as “death whistles”<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Velazquez, R. &amp; Rivas Castro, F. Death whistle. J. Acoust. Soc. Am. 128, 2389–2389 (2010)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR10" id="ref-link-section-d62632563e529">10</a></sup>, given a presumed but so far unsubstantiated association with collective warfare by Aztec communities to scare enemies (which we refer to as “warfare hypothesis”). We here use the more neutral label of “skull whistles” for these Aztec instruments given their general appearance as portraying a human or mystical skull that figures as the sound body of the whistle.</p><p>Only theories concerning the visual, acoustic, and contextual iconography of skull whistles exist so far, but there seems to be a multi-layered symbolism according to the cultural and mythological codex of the Aztecs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Both, A. A. On the context of imitative and associative processes in Prehispanic music. Stud. zur. Musikarch.äologie 5, 319–332 (2006)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR7" id="ref-link-section-d62632563e536">7</a></sup>. This skull-like visual iconography might portray Mictlantecuhtli, the Aztec Lord of the Underworld, and might provide a link to Aztec sacrificial cults. The sound iconography reveals a possible association with Ehecatl, the Aztec God of the Wind, who traveled to the underworld to obtain the bones of previous world ages to create humankind (which we refer to as the “deity symbolism hypothesis”)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Both, A. A. Skull whistles of the Aztecs (Totenkopfpfeifen der Azteken). Baessler-Arch. 53, 43–54 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR2" id="ref-link-section-d62632563e540">2</a></sup>. Regarding the place and context of skull whistle discoveries, often involving ritual burial sites with human sacrifices, the skull whistles might have had a ritual and ceremonial iconography for the mythological descent into the Mictlan, the Aztec underworld, after sacrificial death. The fifth level of Mictlan is filled with deadly, razor-sharp, and piercing winds<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Both, A. A. Skull whistles of the Aztecs (Totenkopfpfeifen der Azteken). Baessler-Arch. 53, 43–54 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR2" id="ref-link-section-d62632563e544">2</a></sup>, which again points to its potential sound iconography (which we refer to as the “ritual symbolism hypothesis”). For the latter, the rich Aztec sound imitation tradition also seems relevant<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Both, A. A. On the context of imitative and associative processes in Prehispanic music. Stud. zur. Musikarch.äologie 5, 319–332 (2006)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR7" id="ref-link-section-d62632563e548">7</a></sup>, with many Aztec instruments being designed to mimic environmental (wind, rain)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Both, A. A. On the context of imitative and associative processes in Prehispanic music. Stud. zur. Musikarch.äologie 5, 319–332 (2006)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR7" id="ref-link-section-d62632563e552">7</a></sup>, animal (bird call, snake hiss)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Both, A. A. Aztec music culture. World Music 52, 14–28 (2010)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR11" id="ref-link-section-d62632563e557">11</a></sup>, or human sounds (screams)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Trevor, C., Arnal, L. H. &amp; Frühholz, S. Terrifying film music mimics alarming acoustic feature of human screams. J. Acoust. Soc. Am. 147, EL540–EL545 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR9" id="ref-link-section-d62632563e561">9</a></sup>, with many of such instruments being used in ritual contexts<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Both, A. A. Aztec music culture. World Music 52, 14–28 (2010)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR11" id="ref-link-section-d62632563e565">11</a></sup>.</p><p>Thus, there is currently a diversity of potential archeoacoustic and iconographic theories concerning the skull whistles, which mainly relate to the acoustic and symbolic sound features of skull whistles. Experimentally exploring the perceptual effects of skull whistles could help to assess the psychoacoustic effects on human listeners as well as to discuss the potential archeoacoustic and pragmatic use of skull whistles by Aztec communities for affective (creating ritual wind-like atmospheres, scaring by scream-like quality) and/or symbolic purposes (mythical piercing winds, hostile conditions of the underworld). As for any human experimental study with a historical perspective, direct experiments cannot be performed with original Aztec humans. We however performed experiments with naïve European listeners from unbiased community samples. We specifically report data from various analytical approaches to first assess the physical and acoustic nature of skull whistles on the one hand, and second, we performed seven different psychoacoustic and neuroscientific laboratory experiments to assess the perceptual nature of skull whistles during their processing by humans.</p><p>The data reported here would provide knowledge in two major perspectives. First, from a psychoacoustic perspective, we assessed how modern humans respond to sounds produced by unique archeological artifacts that represent important historical sound tools. Second, from an archeoacoustic perspective and as outlined above, three major hypotheses exist so far concerning the cultural and practical meaning of skull whistles (warfare hypothesis, deity symbolism hypothesis, and ritual symbolism hypothesis). Each hypothesis would likely predict differential effects of skull whistle sounds on human listeners, and we here took a precise experimental approach to obtain confirmatory evidence that is potentially in favor of certain hypotheses.</p></div></div></section><section data-title="Methods"><div id="Sec2-section"><h2 id="Sec2">Methods</h2><div id="Sec2-content"><h3 id="Sec3">Original skull whistles</h3><p>Most of the known original Aztec skull whistles (SW Orig) are preserved in archeological collections of museums and research laboratories around the world. In this paper, we refer to two original skull whistles stored in INAH facilities of the Tlatelolco archeological site, Mexico City, Mexico (Burial 7, Elements 2 and 4; Inventory Nrs. 10-262662, 10-263074). Further skull whistle artifacts are stored in the Ethnological Museum Berlin, Humboldt Forum, Germany (Inventory Nrs. IV Ca 2621a; 2621m; 2621z; 2621x; 2621 v) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig1">1b</a>). Some noise whistles forming part of the Aztec fire snake incense ladles are stored in the Museo Nacional de Antropología and the Museo del Templo Mayor, Mexico City, Mexico (Find Nr. V, Inventory Nrs. 10-135830; 11-2883; 11-4004).</p><p>We obtained sound recordings (16 bit, 44.1 kHz) from the two abovementioned skull whistles excavated by Salvador Guilliem Arroyo at the temple precinct of Tlatelolco, Mexico City. From these recordings, we extracted a total of 20 short sound files that were included in further analyses. We also obtained sound recordings of three noise whistles forming part of Aztec fire snake incense ladles discovered by Leopoldo Batres in Find No. V, which is an offering site of the temple precinct of Tenochtitlan. The fire snake incense ladles have the same skull whistle construction at their rear end, are played in the same way, and can produce the same sounds as skull whistles. From these recordings, we extracted a total of 15 short sound files from playing these exemplars.</p><p>To assess the outer and inner construction of all original skull whistles, we acquired computer tomography (CT) scans for some of the exemplars that are part of the collection of the Ethnological Museum in Berlin and again with the permission of the Berlin State Museums. High-resolution CT scans (Siemens CT Somaris, section resolution 0.160 mm<sup>3</sup>) were obtained for the exemplars with the inventory labels IV Ca 2621m and IV Ca 2621u (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig1">1c</a>). The CT scans enabled us to create high-quality 3D objects based on a digital surface reconstruction of the surface of the objects, and these CT scans allowed us to virtually explore the outer and inner architecture of the original skull whistles.</p><h3 id="Sec4">Replica skull whistles</h3><p>Experimental archeology has the purpose of re-creating archeologically relevant objects to be able to perform scientific experiments with such objects. The manufacturing process of the object is equally important as the experiments that can be performed afterward. Here we took two examples (IV Ca 2621u, IV Ca 2621z; smb.museum/en/museums-institutions/ethnologisches-museum/home/) from the Ethnological Museum in Berlin as a blueprint to rebuild the skull whistles as replicas (SW repl, referred to as “Replica 2621u” and “Replica 2621z”) using clay material (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig1">1d</a>). We also acquired high-resolution CT scans (Siemens CT Somatom Definition AS+ scanner, section resolution 0.128 mm<sup>3</sup>) of these two replicas and created high-quality 3D objects from the replicas. This helped to confirm the similarity between the original and the replica skull whistles.</p><p>The category of replica skull whistles that were used in this study also included four skull whistles that were artisanal creations based on the general description of skull whistles in the literature. These additional artisanal skull whistles (SW art) were also made of clay and constructed along the general principles of skull whistles including the four major compartments. We thus used a total of six replica skull whistles, and we asked <i>n</i> = 5 humans (3 male and 2 female participants, mean age 31.60y, SD 7.92) to produce the typical sounds of the skull whistles using three different levels of air pressure (SW low, SW med/medium, SW high) when playing the skull whistles. This resulted in a total of 270 sound recordings (mono 16 bit, 44.1 kHz) from these replicas and artisanal skull whistles.</p><p>Careful handling only allowed us to record sounds from original whistles as described above with a normal air pressure intensity, but sound recordings from replica skull whistles were acquired with low (SW low), medium (SW med), and high-intensity air pressure (SW high), as air pressure intensity can produce different sound qualities. Obtaining sound recordings with different air pressure levels ensured that the acoustic analysis was not biased towards a certain sound quality of the skull whistle.</p><h3 id="Sec5">Large dataset of sounds</h3><p>To compare the original skull whistles and replica skull whistles against the acoustic and perceptual profile of a large and diverse sample of human, animal, nature, and technical sounds, we collected sounds from various databases and our own recordings<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Trevor, C., Arnal, L. H. &amp; Frühholz, S. Terrifying film music mimics alarming acoustic feature of human screams. J. Acoust. Soc. Am. 147, EL540–EL545 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR9" id="ref-link-section-d62632563e632">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Frühholz, S., Dietziker, J., Staib, M. &amp; Trost, W. Neurocognitive processing efficiency for discriminating human non-alarm rather than alarm scream calls. PLoS Biol. 19, e3000751 (2021)." href="#ref-CR12" id="ref-link-section-d62632563e635">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Frühholz, S., Trost, W. &amp; Grandjean, D. Whispering–The hidden side of auditory communication. Neuroimage 142, 602–612 (2016)." href="#ref-CR13" id="ref-link-section-d62632563e635_1">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Frühholz, S., Klaas, H. S., Patel, S. &amp; Grandjean, D. Talking in fury: the cortico-subcortical network underlying angry vocalizations. Cereb. Cortex 25, 2752–2762 (2015)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR14" id="ref-link-section-d62632563e638">14</a></sup>. A broad collection of sounds was taken from the ESC-50 library<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Piczak, K. J. ESC: Dataset for environmental sound classification. in MM 2015—Proceedings of the 2015 ACM Multimedia Conference 1015–1018 (Association for Computing Machinery, Inc, 2015). 
                  https://doi.org/10.1145/2733373.2806390
                  
                ." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR15" id="ref-link-section-d62632563e642">15</a></sup>, which is a labeled collection of environmental audio recordings from many sound categories in the broad sound classes of animal sounds (ANI), natural soundscape, and water sounds (labeled as “NAT” sounds), human non-speech sounds (HUM), interior/domestic sounds (INT), and exterior urban noises (EXT). The human sounds were extended by neutral and emotional voice recordings from our own databases of nonverbal<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Frühholz, S., Dietziker, J., Staib, M. &amp; Trost, W. Neurocognitive processing efficiency for discriminating human non-alarm rather than alarm scream calls. PLoS Biol. 19, e3000751 (2021)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR12" id="ref-link-section-d62632563e646">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Frühholz, S., Klaas, H. S., Patel, S. &amp; Grandjean, D. Talking in fury: the cortico-subcortical network underlying angry vocalizations. Cereb. Cortex 25, 2752–2762 (2015)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR14" id="ref-link-section-d62632563e649">14</a></sup> and whispered vocalizations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Frühholz, S., Trost, W. &amp; Grandjean, D. Whispering–The hidden side of auditory communication. Neuroimage 142, 602–612 (2016)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR13" id="ref-link-section-d62632563e653">13</a></sup>, and with short speech and speech-like utterances from established databases<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Belin, P., Fillion-Bilodeau, S. &amp; Gosselin, F. The Montreal affective voices: a validated set of nonverbal affect bursts for research on auditory affective processing. Behav. Res Methods 40, 531–539 (2008)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR16" id="ref-link-section-d62632563e657">16</a></sup>. Modern instrument sounds (INS) were taken from the Philharmonia database<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Philharmonia Orchester. Philharmonia Database." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR17" id="ref-link-section-d62632563e662">17</a></sup>, and short neutral and emotional music sounds (MUS) were again taken from our own database<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Trevor, C., Arnal, L. H. &amp; Frühholz, S. Terrifying film music mimics alarming acoustic feature of human screams. J. Acoust. Soc. Am. 147, EL540–EL545 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR9" id="ref-link-section-d62632563e666">9</a></sup>. We also recorded sounds from ancient Aztec instruments (e.g., ancient flute, incense pipe, trumpet; labeled as “AZT” instruments) and more contemporary and popular Mexican flutes (MEX) that produce sounds to mimic animal sounds (e.g., eagle, jaguar) and human sounds (e.g., Llorona). These sounds were recorded in a steady state and in a modulated manner. Finally, we collected synthetic sounds (SYN) from freely available sound sources including electronic sounds produced by technical devices (e.g., computer system start-up sounds, pinball machines, printers). The full list of the 86 sound categories, including the skull whistle sounds and all other sounds used in this study can be found in Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S1</a>. The total number of sounds was <i>n</i> = 2567 with a mono 16 bit encoding and a 44.1 kHz sampling rate. Each sound file was cropped to 800 ms and all sounds were normalized to have the same RMS. The final sound pressure level was set to SPL 70 dBA for all sounds with a fade-in/out of 15 ms.</p><h3 id="Sec6">Acoustic analysis of sounds</h3><h4 id="Sec7">Acoustic feature extraction</h4><p>None of the studies reported in the following using experimental data collections and parametric data analysis has been preregistered. To perform acoustic analyses of the sounds, we extracted acoustic features with the openSMILE Toolbox<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Eyben, F., Weninger, F., Gross, F. &amp; Schuller, B. Recent developments in openSMILE, the Munich open-source multimedia feature extractor. in Proceedings of the 21st ACM International Conference on Multimedia—MM ’13 835–838 (ACM Press, New York, New York, USA, 2013). 
                  https://doi.org/10.1145/2502081.2502224
                  
                ." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR18" id="ref-link-section-d62632563e688">18</a></sup>. The feature configuration file applied (emobase2010.conf) extracted a total of 1582 acoustic features in the spectral and temporal domain including the arithmetic mean as well as the first and second derivative for some of the features<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Eyben, F. et al. The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Trans. Affect Comput 7, 190–202 (2016)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR19" id="ref-link-section-d62632563e692">19</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Colace, F. &amp; Casaburi, L. An approach for sentiment classification of music. in ICEIS 2016—Proceedings of the 18th International Conference on Enterprise Information Systems vol. 2 421–426 (SciTePress, 2016)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR20" id="ref-link-section-d62632563e695">20</a></sup>.</p><p>To quantify the amount of spectral and temporal information contained in each sound file<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Trevor, C., Arnal, L. H. &amp; Frühholz, S. Terrifying film music mimics alarming acoustic feature of human screams. J. Acoust. Soc. Am. 147, EL540–EL545 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR9" id="ref-link-section-d62632563e702">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Frühholz, S., Dietziker, J., Staib, M. &amp; Trost, W. Neurocognitive processing efficiency for discriminating human non-alarm rather than alarm scream calls. PLoS Biol. 19, e3000751 (2021)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR12" id="ref-link-section-d62632563e705">12</a></sup>, we also quantified the modulation power spectrum (MPS) according to a previous description<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Elliott, T. M. &amp; Theunissen, F. E. The modulation transfer function for speech intelligibility. PLoS Comput. Biol. 5, e1000302 (2009)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR21" id="ref-link-section-d62632563e709">21</a></sup> by using the MPS toolbox for MATLAB (theunissen.berkeley.edu/Software.html). To obtain an MPS for each sound, we first converted the amplitude waveform to a log amplitude of its spectrogram obtained by using Gaussian windows and a log-frequency axis. The MPS results from the amplitude squared as a function of the Fourier pairs of the time (temporal modulation in Hz) and frequency axis (spectral modulation in cycles/octave) of the spectrogram. The low-pass filter boundaries of the modulation spectrum were set to 200 Hz for the temporal modulation rate and to 12 cycles/octave for the spectral modulation rate.</p><h4 id="Sec8">Modulation power spectrum (MPS)</h4><p>A statistical difference of the MPS from the 4 skull whistle sound categories (SW orig, SW high, SW med, SW low) compared with sounds from the other 9 major sound categories (<i>mus</i> music, <i>nat</i> nature, <i>ani</i> animal, <i>hum</i> human, <i>int</i> interior, <i>ext</i> exterior, <i>syn</i> synthetic, <i>mex</i> mexican flutes, <i>ins</i> instruments) was tested using a permutation approach (<i>n</i> = 2000) by shuffling category labels, resulting in a log-transformed <i>p</i>-value map for the entire spectral and temporal range of the MPS in terms of the difference between categories.</p><h4 id="Sec9">Representational similarity analysis (RSA)</h4><p>Based on the pattern of the 1582 acoustic features, we performed a presentational similarity analysis (RSA) according to a procedure and code described previously<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Nili, H. et al. A toolbox for representational similarity analysis. PLoS Comput. Biol. 10, (2014)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR22" id="ref-link-section-d62632563e763">22</a></sup>. The RSA is based on a calculation of pairwise distances between pairs of sounds across all sounds and all 1582 acoustic features. For the RSA calculation, all acoustic features were z-transformed, and distance/similarity was calculated as Pearson correlation between acoustic patterns. This resulted in an acoustic similarity matrix with values defined as [1-correlation] and thus with a range of [0-2] for highest-to-lowest similarity, or lowest-to-highest dissimilarity, respectively. This RSA analysis was performed two times, first on all 2567 sounds for getting similarity estimation across all sounds, and a second RSA was performed on the 305 skull whistle sounds only to estimate the similarity within these sounds. The skull whistle sounds were divided in 20 categories, including one category of original Aztec skull whistle sounds (orig whis), one category of Aztec fire snake incense ladles (orig snake), two replica skull whistle categories (rep1, rep2), and four artisanal skull whistle sounds (art1, art2, art3, art4).</p><h4 id="Sec10">Hierarchical clustering analysis (HCA)</h4><p>We additionally performed a hierarchical clustering analysis (HCA) to generate dendrograms of how sound categories cluster together based on their acoustic patterns. For this, we calculated the Euclidean distance between sounds based on their z-transformed values of acoustic features, and the linkage in clusters was determined as the weighted average distance. Nodes were grouped with a linkage cluster threshold of <i>c</i> &lt; 2.0.</p><h3 id="Sec11">Perceptual assessment and ratings of sounds</h3><h4 id="Sec12">Dimensional ratings</h4><p>All 2567 sounds were subjected to a perceptual rating analysis by <i>n</i> = 70 human listeners (27 male and 43 female participants, mean age 25.01 y, SD 4.55). Across all studies reported here, the adjectives female/male refer to sex (not gender), and the participants provided the information via self-report. All participants in the studies described were recruited by public announcements and volunteered to participate. Participants were reimbursed with CHF 15 per hour or with course credits for their participation. Participants gave informed and written consent for their participation following the ethical and data security guidelines of the University of Zürich (Switzerland). All experiments were approved by the cantonal ethics committee of the Swiss canton of Zürich (#2017-01086).</p><p>Since a perpetual rating of all sounds by one listener would have been extremely time-consuming and exhausting, each listener only rated a random selection of <i>n</i> = 221–433 sounds. Listeners were asked to rate each sound along four different dimensions on a visual analog scale setup using 10-point Likert scales. Ratings were done on the arousal level elicited by the sounds (0 not arousing at all, 10 highly arousing), the urgency to respond to the sounds (0 no urgency at all, 10 high urgency), the naturalness of sounds (0 not natural at all, 10 highly natural), and the valence of sounds (−5 highly negative, 0 neutral, 5 highly positive).</p><p>Ratings were obtained and summarized for the 4 skull whistle sound categories (SW orig, SW low, SW med, SW high) and for the 9 categories of other sounds (human, animal, nature, exterior, interior, music, instrument, Mexican, synthetic). Each sound was rated ten times by a random selection of ten out of the 70 participants. The mean of these ten ratings was calculated and plotted in the 4-dimensional space of ratings. Additionally, we calculated the distribution of ratings across all participants for the 13 sound categories. These distributions were used to calculate inter-rater consistencies (intraclass correlation coefficient, ICC) and we calculated the statistical differences in ratings across the sound categories using by fitting a linear mixed model (LME) to the data:</p><div id="Equ1"><p><span>$${rating} \sim 1\,+{category}+(1{\backslash }{participant})$$</span></p><p>
                    (1)
                </p></div><p>with the “rating” being each of the 4 rating scales separately, “category” being the 13 sound categories as a fixed effect, and “participant” as a random effect factor.</p><h4 id="Sec13">Representational similarity analysis</h4><p>Based on the pattern of the 4 perceptual rating scales, we performed a presentational similarity analysis (RSA) similar to the pattern of acoustic features (see above). This RSA analysis was again performed two times, first on all 2567 sounds for getting similarity estimation across all sounds, and a second RSA was performed on the 305 skull whistle sounds only to estimate the similarity within these sounds. We again also performed a hierarchical clustering analysis (HCA) to generate dendrograms of how sound categories cluster together based on their perceptual ratings and the nodes were grouped with a linkage cluster threshold of <i>c</i> &lt; 2.0.</p><h3 id="Sec14">Free choice labeling and categorical classification of sounds</h3><h4 id="Sec15">Free choice labeling</h4><p>To obtain an unbiased estimate about how human listeners would label skull whistle sounds and sounds of the other sound categories, we performed a free choice labeling experiment with <i>n</i> = 40 human listeners (15 male and 25 female participants, mean age 24.60 y, SD 3.43; all native English speakers). We presented a balanced selection of 200 out of the total 2567 sounds to each participant; this selection was necessary to ensure that we could obtain labels from each participant within a reasonable duration of the experiment. We selected sounds from the major sound categories including 8 SW orig, 8 SW high, 8 SW med, 8 SW low sounds as well as 32 nature (8 thunder, 8 fire/wood cracking, 8 water, 8 wind sounds), 32 animal (8 bird, 8 reptile, 8 monkey, 8 other mammal sounds), 32 human (8 baby cries, 8 whispers, 8 affective voice, 8 vowel/syllable), 8 interior, 16 exterior (8 engine, 4 horn, 4 chainsaw), 8 synthetic/music, 8 Mexican flute, 24 instrument (8 brass, 8 string, 8 wood instrument sounds), and 8 white/pink noise sounds. We asked human listeners to provide two types of written labels in English for each of these sounds: first, participants were asked to provide a single substantive label (referred to as sounds “Labels”) for each sound that would best describe the object, mechanisms, or process that most likely produced the sound; second, participants were asked to provide a single adjective (referred to as “Adjectives”) for each sound that would best describe the affective nature and human reaction to the sound.</p><p>Raw labels for each participant and each sound were pre-processed before any statistical analysis. This pre-processing followed several rules to make labels more consistent and unified for proper statistical handling<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Bones, O., Cox, T. J. &amp; Davies, W. J. Sound categories: category formation and evidence-based taxonomies. Front. Psychol. 9, 1277 (2018)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR23" id="ref-link-section-d62632563e936">23</a></sup>: white space, special characters, and numbers were removed; misspelling was corrected where unambiguous (otherwise replaced with “unidentified” label); plural was converted to singular; generic names were removed (e.g., “sounds”, “emotion”) and replaced with the “unidentified” label; synonyms were unified using Microsoft’s synonym checker; and words were unified to the most common word stem. In the first step of the analysis, we quantified the relative occurrence of labels for each sound calculated as the total number of occurrences multiplied by the percentage of participants that used this label; the resulting numbers were divided by 100 for better plotting and representation of the data. We used a binomial test (<i>p</i> &lt; 0.05, FDR corrected) to determine if the relative occurrence numbers were significantly greater than would be expected by chance given the probability of labels/adjectives.</p><p>In the second step of the analysis, we aimed to reduce the diverse number of labels given during the free choice labeling task to a low and consistent number of labels that would represent more general sound and affective categories. We therefore presented all labels and adjectives to a group of 7 independent human raters (3 male and 4 female participants, mean age 33.57 y, SD 7.25; all native English speakers), and asked them to assign the original substantive labels to one of 9 general sound categories (human, animal, nature, exterior, interior, music, instrument, synthetic, unknown), and to assign the adjective to one of 10 emotional labels (anger, disgust, fear, anticipation, joy, sadness, surprise, trust, neutral, unknown) according to the emotion classification scheme proposed by Robert Plutchik<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Plutchik, R. A psychoevolutionary theory of emotions. Soc. Sci. Inf. 21, 529–553 (1982)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR24" id="ref-link-section-d62632563e946">24</a></sup>. Based on the categorical re-labeling of the original substantive labels and adjectives, we again determined the relative occurrence of these new labels the same way as was done for the original labels. We again used a binomial test (<i>p</i> &lt; 0.05, FDR corrected) to determine if the relative occurrence numbers of labels/adjectives were significantly greater than would be expected by chance given the probability of labels/adjectives.</p><p>Based on these re-labeled data, we also performed a Correspondence Analysis (CA) to estimate the principal dimension that could represent data on a more general level. First, we used Pearson’s Chi-squared test to evaluate the level of dependency of row (sounds) and column elements (labels, adjectives) of the data matrix. Second, after performing the CA analysis, we retained 3 dimensions for the substantive labels (explained variance 61.31%) and 3 dimensions for the adjectives (explained variance 69.39%) to obtain a 3-dimensional representation of all sounds in a sound category and a sound affective space, respectively.</p><h4 id="Sec16">Forced choice sound classification (3-alternative forced choice, 3AFC)</h4><p>To assess how humans classify skull whistle and other sounds into three major sound categories as a basic process of sound processing and classification, we presented a selection of 72 sounds to human listeners in two separate experiments with separate samples of human participants. In the first experiment with 76 participants (24 male and 52 female participants, mean age 24.17 y, SD 4.46; four out of the original 79 datasets were not included because of non-recorded responses due to technical response device errors), we included sounds from 6 sound categories consisting of 12 SW orig, 12 animal, 12 nature, 12 exterior/interior, 12 human, and 12 music sounds. These sounds were presented twice in random order, resulting in a total of 144 trials. Using three buttons on a keyboard, participants listened to each sound and classified it afterward as belonging to one of three categories (animated, technical, environment). The category “animated” sound had to be chosen if participants believed that the sound was produced by a living organism, the category “technical” sound had to be chosen if the sound originated from a technical device or process, and the category “environment” had to be chosen if the sound originated from a non-living environmental object or process. The next trial started 1 s after a category had been chosen for the current trials. The second experiment was the same as the first experiment, but instead of SW orig sounds we presented SW repl sounds here. A different sample of 58 participants took part in the second experiment (17 male and 41 female participants, mean age 25.60 y, SD 6.29; 3 of 61 datasets could not be included again due to technical response device errors). Data for the two experiments were analyzed separately, and classification data were entered into a repeated measures analysis of variance with the within-subject factors sounds category (6 levels) and classification category (3 levels).</p><h4 id="Sec17">Dichotic listening experiment</h4><p>Sounds can have different meanings and affective importance for human listeners. The more meaningful and effectively engaging sounds are the more they attract attention and distract from other ongoing tasks. We tested the meaningfulness and importance of skull whistle and other sounds using a dichotic listening experiment. We again performed two different experiments here including the same 72 sounds as described in the section above, but also including two additional categories of noise sounds (pink noise) and silent trials (no sounds presented on the unattended ear, see below) as baseline conditions. In the first experiment, 47 human listeners (13 male and 34 female participants, mean age 23.91 y, SD 4.21) listened to sounds from original skull whistles and other sounds that were presented on one ear, while one of two simple sine wave tones was presented on the other ear. The two sine wave tones were a 350 Hz (low tone) and a 370 Hz sine wave tone (high tone). Participants were asked to attend to the ear where these tones were presented and decide if the tone was “high” or “low” using two buttons and their right-hand index and middle finger. The skull whistle and other sounds were presented on the unattended ear, and each of these sounds was presented two times on the left (1 trial with low tone, 1 trial with high tone) and two times on the right ear (1 trial with low tone, 1 trial with high tone). There was a total of 384 trials split across two runs. The first run was preceded by 16 training trials to familiarize participants with the task and the high/low tone. Within each run, there were 8 blocks of 24 trials with the attended ear and tone discrimination task switching with every block. Button assignment and laterality of the first attended ear were counter-balanced across participants. The second experiment included an independent sample of 47 human listeners (14 male and 33 female participants, mean age 25.02 y, SD 3.91), with the only difference being that we presented SW repl sounds instead of the SW orig sounds. Data for the two experiments were analyzed separately, and reaction time and classification data were entered into a repeated measures analysis of variance with the within-subject factors sounds category (8 levels) and attended ear (2 levels).</p><h3 id="Sec18">Neural decoding of skull whistle sounds</h3><h4 id="Sec19">Experimental setup</h4><p>The experiment included 32 human participants (14 male and 18 female participants, mean age 26.00 y, SD 5.47). All participants had normal hearing and normal or corrected-to-normal vision. Exclusion criteria were hearing and visual impairments as well as psychiatric or neurological disorders in life history. Participants were invited to take part in a functional magnetic resonance imaging experiment (fMRI) to quantify brain activity during the processing of skull whistle and other sounds.</p><p>This experiment included the same 200 sounds that are described in the section for the free choice labeling experiment (see above). The experiment consisted of 4 different runs, and each run presented all 200 sounds in random order as single trials. Each run also contained a random selection of 20 trials, in which sounds of the previous trial were repeated. The participants’ task was to detect these sound repetitions and indicate the detection of a repeated sound by a button press with their right index finger. Participants were asked to listen attentively to the sounds, and the sound repetition task ensured that participants kept an attentive listening state. All repetition trials were excluded from all further analyses.</p><h4 id="Sec20">Brain data acquisition and pre-processing</h4><p>Structural and functional brain data were recorded on a 3 T Philips Ingenia MR scanner by using a standard 32-channel head coil. A high-resolution structural image was acquired by using a T1-weighted scan (301 contiguous 1.2 mm slices, repetition time [TR]/echo time [TE] = 1.96 s/3.71 ms, field of view [FOV] = 256 mm, in-plane resolution 1 × 1 mm<sup>2</sup>). Functional whole-brain images were recorded by using a T2*-weighted echo-planar pulse imaging (EPI) sequence (TR 1.6 s, TE 30 ms, flip angle [FA] 82°; in-plane resolution 220 × 114.2 mm, voxel size 2.75 × 2.75 × 3.5 mm<sup>3</sup>; slice gap 0.6 mm) covering the whole brain.</p><p>Pre-processing and statistical analyses of functional images were performed using the Statistical Parametric Mapping software (SPM12, fil.ion.ucl.ac.uk/spm). Functional data were first manually realigned to the AC-PC axis, and functional images were then motion-corrected using a 6-parameter rigid-body transformation with realignment to the mean functional image. This was followed by a slice time correction of the slices acquired within a brain volume. Each participant’s anatomical T1 image was then co-registered to the mean functional brain image, followed by a segmentation of the T1 image for estimating normalization parameters using a geodesic shooting and Gauss-Newton optimization approach<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Ashburner, J. &amp; Friston, K. J. Diffeomorphic registration using geodesic shooting and Gauss-Newton optimisation. Neuroimage 55, 954–967 (2011)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR25" id="ref-link-section-d62632563e1003">25</a></sup> for transformations into the standard space. Based on the estimated parameters, the anatomical and functional images were then normalized to the Montreal Neurological Institute (MNI) stereotactic space. Functional images were re-sampled into an isotropic 2 mm<sup>3</sup> voxel size during the normalization procedure. All functional images were spatially smoothed with an 8 mm full-width half-maximum (FWHM) isotropic Gaussian kernel.</p><h4 id="Sec21">GLM functional brain data analysis</h4><p>Functional brain data were then entered into a fixed-effects single-subject analysis, with a general linear model (GLM) design matrix containing 26 separate regressors for each of the 25 sound categories plus an additional regressor for all repetition trials. The 25 sound categories consisted of 4 skull whistle categories (SW orig, SW high, SW med, SW low), 4 human sound categories (nonverbal vocalizations, baby cries, vowel/syllable, whisper), 4 animal sound categories (monkeys, other mammals, snakes, birds), 4 categories for technical/synthetic sounds (alarm sounds, tools, vehicles/engines, synthetic), 4 categories for nature and environment sounds (wind, thunder, fire/cracking wood, water), 4 categories of instrument sounds (Mexican flutes, brass, wood, string), and one category of white/pink noise sounds. Each sound category contained 8 individual sounds. All sound trials were modeled with a stick function aligned to the onset of each stimulus, which was then convolved with a standard hemodynamic response function (HRF). The design matrix also included six motion correction parameters as regressors of no interest to account for signal artifacts due to head motion. Contrast images for each of the 25 main sound categories and from each participant were then taken to several separate random-effects factorial group-level analyses. Different directional contrasts on the group-level were performed between conditions and were thresholded at a combined voxel threshold of <i>p</i> &lt; 0.05 (FWE corrected) and a cluster extent threshold of <i>k</i> = 10.</p><h4 id="Sec22">Functional brain connectivity analysis</h4><p>We used the Connectivity Toolbox (CONN toolbox)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Whitfield-Gabrieli, S. &amp; Nieto-Castanon, A. Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks. Brain Connect 2, 125–141 (2012)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR26" id="ref-link-section-d62632563e1031">26</a></sup> to perform functional connectivity analyses following a standard procedure for functional brain data analysis. We computed seed-to-voxel analysis based on 10 regions of interest (ROI). The ROIs included three right-hemispheric frontal regions originating from the [SW &gt; other sounds] contrast (MFC, MFG, INS) and an additional parietal region in IPS. The ROIs also included six bilateral regions in the auditory cortex (PTe, PPo, ST) resulting from the [Other sounds &gt; SW] contrast. From all ten ROIs we extracted the time series of data in a 3 mm square around the peak voxel coordinate. From the time series data, spurious sources of noise were estimated and removed by using an automated denoising procedure, and the residual BOLD time series was band-pass filtered in the range 0.008–0.09 Hz to minimize the influence of physiological, head-motion, and other noise sources.</p><p>We performed a generalized psycho-physiological interaction (gPPI) analysis, computing the interaction between the seed BOLD time series and a condition-specific factor when predicting each voxel BOLD time series. In contrast to standard PPI, gPPI allows the inclusion of the interaction factor of all task conditions simultaneously in the estimation model to better account for between-condition effects and influences. We included all 24 original sound conditions in a single gPPI model based on a bivariate correlation approach between seed and target regions. For the group-level analysis, we specified seed-to-voxel analysis for the right fronto-parietal seeds regions during the task conditions including the four skull whistle sounds. Two further seed-to-voxel analyses were set up for the left and right AC seed regions separately including the task conditions of all other 20 sound categories. The significance threshold was set to a voxel-level threshold of <i>p</i> = 0.005 combined with an FWE-corrected cluster-level threshold of <i>p</i> = 0.05.</p><h4 id="Sec23">MVPA and RSA analysis</h4><p>To identify brain regions that encode the acoustic and perceptual similarity/difference of skull whistle sounds to the sounds of the other categories, we correlated the results of an RSA analysis of the sounds used in this experiment with a voxel-wise indicator of functional brain activity differences between the sound categories. This analysis procedure included several steps.</p><p>In the first step, we repeated the RSA analysis as described above for the acoustical features of sounds (1582 acoustic features) and the perceptual rating pattern (4 rating scales), but we performed the analysis here only on the 200 sounds that have been used in this experiment. This resulted in RSM patterns that encode the acoustic similarity/difference of 25 sound categories as well as the perceptual similarity/difference of the 25 sound categories. We thus obtained a 25 × 25 dissimilarity matrix based on the acoustic features and a 25 × 25 dissimilarity matrix based on the perceptual rating patterns.</p><p>In the second step, we performed a multivoxel pattern analysis (MVPA) implemented as searchlight decoding analysis using The Decoding Toolbox (TDT)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Hebart, M. N., Görgen, K. &amp; Haynes, J.-D. The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data. Front. Neuroinform. 8, 88 (2015)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR27" id="ref-link-section-d62632563e1059">27</a></sup> for the purpose of an information-based brain mapping according to multivoxel activity patterns, which are assumed to differ between the experimental conditions. The searchlight analysis was performed on normalized but unsmoothed functional data. This analysis was performed on single-trial beta images resulting from an iterative GLM analysis as recommended for better sensitivity in event-related designs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Turner, B. O., Mumford, J. A., Poldrack, R. A. &amp; Ashby, F. G. Spatiotemporal activity estimation for multivoxel pattern analysis with rapid event-related designs. Neuroimage 62, 1429–1438 (2012)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR28" id="ref-link-section-d62632563e1063">28</a></sup>. We obtained beta images for each trial by using a GLM with one regressor modeling a single trial and a second regressor modeling all remaining trials. This GLM modeling was repeated for each trial, including movement parameters as a regressor of no interest to account for false positive activity due to head movements. For each voxel, we defined a local sphere of 6 mm radius to investigate the local multivoxel pattern information in the single-trial beta images. This procedure was repeated for every pairwise comparison between the 25 sound category conditions. For each of these iterations, we trained a multivoxel support vector machine classifier using the LIBSVM package and implemented a leave-one-run-out cross-validation design. This procedure resulted in a brain map of local pairwise decoding accuracy across the 25 experimental conditions for each participant. This was represented as a voxel-wise 25 × 25 MVPA dissimilarity matrix with the assumption that the decoding accuracy indicates how well conditions can be discriminated on a brain level (i.e., higher decoding accuracy means higher neural discrimination).</p><p>In the third step, we cross-correlated (Spearmen correlation) the brain MVPA dissimilarity matrices separately with the acoustic RSM and the perceptual RSM on a voxel-by-voxel level. We restricted this correlation analysis to the matrix part that represented the similarity/dissimilarity measure for the skull whistle sounds as the target sounds of this experiment and similarity analysis. This restricted matrix was the 4 × 25 matrix including the 4 categories of skull whistle sounds and the 25 overall sound categories. This resulted in voxel-wise and Fisher z-transformed correlation maps for each participant. The correlation maps were spatially smoothed with an 8 mm full-width half-maximum (FWHM) isotropic Gaussian kernel and were entered into a group-level GLM analysis (voxel threshold of <i>p</i> &lt; 0.05, FWE corrected; cluster extent threshold of <i>k</i> = 10) separately for the acoustic RSM and the perceptual rating RSM analysis.</p><p>To test the distinctiveness of the resulting correlation maps with the specific RSM matrices for the acoustic and perceptual properties, we performed two additional permutation-based analyses. We created <i>n</i> = 30 random permutations of the acoustic and perceptual RSM matrices and entered them into the same cross-correlation approach as described above. The resulting brain correlation maps were averaged on the group level using a conjunction analysis (conjunction null hypothesis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Nichols, T., Brett, M., Andersson, J., Wager, T. &amp; Poline, J. B. Valid conjunction inference with the minimum statistic. Neuroimage 25, 653–660 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR29" id="ref-link-section-d62632563e1083">29</a></sup>) across the permutations to determine if random permutation would or would not produce similar significant correlation maps as for the main analysis (voxel threshold of <i>p</i> &lt; 0.05, FWE corrected; cluster extent threshold of <i>k</i> = 10). No significant results were found in this permutation-based analysis.</p><h3 id="Sec24">Reporting summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM3">Nature Portfolio Reporting Summary</a> linked to this article.</p></div></div></section><section data-title="Results"><div id="Sec25-section"><h2 id="Sec25">Results</h2><div id="Sec25-content"><h3 id="Sec26">Artisanal and digital reconstruction of Aztec skull whistles</h3><p>We first aimed to understand the physical structure and sound production mechanisms of original Aztec skull whistles. Skull whistles typically feature a tubular airduct with a constricted passage, a hemispherical counterpressure chamber, a collision chamber, and a bell cavity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Both, A. A. On the context of imitative and associative processes in Prehispanic music. Stud. zur. Musikarch.äologie 5, 319–332 (2006)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR7" id="ref-link-section-d62632563e1118">7</a></sup>. We acquired a high-resolution computer-tomographical (CT) image of one of the exemplars from the EMB to confirm this principle outer and inner architecture of SWs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig1">1c</a>). Given this architecture, the driving acoustic mechanism is thought to rely on the Venturi effect generating a constant air aspiration, collision, and turbulence process<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Both, A. A. Skull whistles of the Aztecs (Totenkopfpfeifen der Azteken). Baessler-Arch. 53, 43–54 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR2" id="ref-link-section-d62632563e1125">2</a></sup>. At high playing intensities and air speeds, this leads to acoustic distortions and to a rough and piercing sound character that seems uniquely produced by the skull whistles.</p><p>Given this established structure of original skull whistles (SW orig), a specialized music archeologist (Arnd Adje Both<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Both, A. A. Skull whistles of the Aztecs (Totenkopfpfeifen der Azteken). Baessler-Arch. 53, 43–54 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR2" id="ref-link-section-d62632563e1132">2</a></sup>, together with the ceramist Osvaldo Padrón Pérez) planned and manufactured two replica skull whistles (SW repl) for the purpose of this study. These replica skull whistles are referred to as exemplars “Replica 2621 u” and “Replica 2621z” and they were copies of two specific exemplars from the EMB, using the same structure and material (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig1">1d</a>). This was done to acquire knowledge about how original skull whistles might have been manufactured by the Aztecs, and to perform in-detail psychoacoustic experiments with these replica skull whistles. We also acquired high-resolution CT scans of the replicas (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig1">1e</a>). Subsequently, we produced 3D digital reconstructions from the CT scans to compare the 3D models of the replica skull whistles with the 3D models of the original skull whistles (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig1">1f</a>). Both the original and the replica skull whistles are very similar, if not nearly identical, in their architecture and the corresponding airflow dynamics.</p><h3 id="Sec27">Skull whistles produce a rough and piercing sound</h3><p>To quantify the acoustic features as well as the similarity between the original and the replica skull whistles, we recorded and analyzed sounds produced from playing two original skull whistles stored in the EMB (IV Ca 2621 u, IV Ca 2621z; kindly provided by Arnd Adje Both). We also acquired sound recordings from playing several replica skull whistles including the two copies of the original skull whistles of the EMB (IV Ca 2621 u, IV Ca 2621z) and four additional artisanal skull whistles that were manufactured according to the general SW structure but were not exact copies of the original SWs. These recordings overall confirmed the noisy, partly rough and piercing sounds produced by skull whistles. This sound quality can also be inferred from the spectral (frequency) and temporal sound profile of skull whistles, which contains broadband pink-like noise features as well as elements of high-pitched frequencies (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig2">2a</a>).</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Acoustic profiles of skull whistles and the MPS profile."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Acoustic profiles of skull whistles and the MPS profile.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://mclare.blog/articles/s44271-024-00157-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="130"/></picture></a></div><p><b>a</b> Example spectrograms from four skull whistle sound categories, an original skull whistle (SW orig; upper panel) as well as from a replica skull whistle with sounds produced at three different levels of air pressure (SW high, SW med/medium, SW low). <b>b</b> Mean modulation power spectrum (MPS; <i>pwr</i> power level) for original (<i>n</i> = 35) and replica skull whistle sounds (<i>n</i> = 90 for each level, total <i>n</i> = 270) (left upper panel). The other plots show statistical difference maps (log-transformed <i>p</i>-values, <i>n</i> = 2000 permutation statistics) between MPSs for the four skull whistle sound categories compared to sounds from 9 major sound categories (<i>mus</i> musical sound effects, <i>nat</i> nature, <i>ani</i> animal, <i>hum</i> human, <i>int</i> interior, <i>ext</i> exterior, <i>syn</i> synthetic, <i>mex</i> Mexican flutes, <i>ins</i> solo instruments; total <i>n</i> = 2262). The difference maps highlight three MPS patches (−log<sub>10</sub>(p) &gt; abs(3)) that show relative power differences in skull whistle sounds (left upper panel; patch marked as a —MPS noise patch, patch marked as b —MPS pitch patch, patch marked as c —MPS slow pattern patch For abbreviations of sound categories, see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S1</a>.</p></div></figure></div><p>A major hypothesis for the existence of skull whistles is that Aztec communities created and used skull whistles to imitate and symbolize natural and mythical sound entities. To explore this hypothesis, we compared the acoustic profile of the original and replica skull whistles with sounds from a broad soundscape. We assembled a large database of 2262 sounds from natural and artificial sources covered by 9 major sound categories and 82 subcategories. To obtain a first general indicator of the (dis)similarity of SWs to other sounds, we calculated the modulation power spectrum (MPS) that quantifies the spectral and temporal profile of sounds (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig2">2b</a>), which the human auditory system uses to differentiate sounds<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Woolley, S. M. N., Fremouw, T. E., Hsu, A. &amp; Theunissen, F. E. Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds. Nat. Neurosci. 8, 1371–1379 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR30" id="ref-link-section-d62632563e1243">30</a></sup>. This revealed three major observations: first, there were only minor differences between the original and replica skull whistles in how they differed from other sounds. Second, three distinct levels of differences appeared when comparing skull whistles to other sounds, with electronic music effects and natural sounds showing the lowest difference to skull whistles, with animal, human, interior/exterior, and synthetic sounds showing a medium level of difference, and with Mexican flutes and solo instrument sounds showing the most significant difference to skull whistle sounds.</p><p>A third important observation was that these differences concerned three different patches of the MPS that have a psychoacoustic significance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig2">2b</a>, left upper panel). One patch covered a broad range of spectral and temporal modulation rates and signifies the noisy and rough acoustic nature of the skull whistles. Such effects are often found with sounds that carry scary affective meanings like primate screams<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Frühholz, S., Dietziker, J., Staib, M. &amp; Trost, W. Neurocognitive processing efficiency for discriminating human non-alarm rather than alarm scream calls. PLoS Biol. 19, e3000751 (2021)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR12" id="ref-link-section-d62632563e1253">12</a></sup> or terrifying music<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Trevor, C., Arnal, L. H. &amp; Frühholz, S. Terrifying film music mimics alarming acoustic feature of human screams. J. Acoust. Soc. Am. 147, EL540–EL545 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR9" id="ref-link-section-d62632563e1257">9</a></sup>. A smaller patch located centrally showed a broad distribution of spectral modulation rates. Such patches typically represent the pitch patterns of sounds explaining the (piercing) pitch impression of skull whistles when compared to some other sound categories (nature, interior), while not having less pitch strength when compared with other sound categories (animal/human, Mexican flutes, instruments). The third patch was centered on low temporal and spectral modulation rates, which are typical for structured and slow-oscillating sound profiles, such as human speech<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Woolley, S. M. N., Fremouw, T. E., Hsu, A. &amp; Theunissen, F. E. Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds. Nat. Neurosci. 8, 1371–1379 (2005)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR30" id="ref-link-section-d62632563e1261">30</a></sup> and animal vocal patterns<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Fukushima, M., Doyle, A. M., Mullarkey, M. P., Mishkin, M. &amp; Averbeck, B. B. Distributed acoustic cues for caller identity in macaque vocalization. R. Soc. Open Sci. 2, 150432 (2015)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR31" id="ref-link-section-d62632563e1265">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Hechavarría, J. C., Jerome Beetz, M., García-Rosales, F. &amp; Kössl, M. Bats distress vocalizations carry fast amplitude modulations that could represent an acoustic correlate of roughness. Sci. Rep. 10, 7332 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR32" id="ref-link-section-d62632563e1268">32</a></sup>. Skull whistles showed significantly lower power in these slow-oscillating patterns, also given that the structure of skull whistles seems not to allow introduction large modulations during playing. Their overall psychoacoustic meaning of skull whistles seems to be the production of single noisy, rough, and piercing sounds.</p><p>To obtain a more detailed picture of the (dis)similarity of skull whistles sounds to sounds of the broader soundscape, we quantified 1582 acoustic features of each sound and performed two sets of acoustic similarity analyses. First, a representational similarity analysis (RSA) quantified the correlation between the acoustic feature pattern of sounds, and the RSA indicated that original and replica skull whistles were very similar in terms of acoustic patterns (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig3">3a, b</a>, red box). Replica skull whistles thus seem to closely match the original skull whistles in terms of their acoustic sound quality, and they both seem to form a unique category of sounds (Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S1a</a>), which however also share some similarities with other sound objects. Contemporary and popular Mexican flutes were most similar to skull whistles, and many such flutes visually and acoustically resemble animal species (jaguar, owl, turtle whistles) and iconize skulls (monkey/scary skull whistle, Llorona whistles). These contemporary whistles are also aerophones with a similar playing style to skull whistles. Further similar sounds were nonspeech human voices and tool/technical sounds with a rough and shrill sound quality (alarm clocks, chainsaw, emergency horn). Medium similar sounds included animals (bovidae), human (painful bursts, nails scratching), and nature sounds (water, wind), and the most dissimilar sounds were solo instruments (guitar, brass, percussion), digital sounds, tonal human (laughing) and animal sounds (Canidae), and water sounds.</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="RSA analysis on acoustic profiles and hierarchical clustering analysis."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: RSA analysis on acoustic profiles and hierarchical clustering analysis.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://mclare.blog/articles/s44271-024-00157-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="2274"/></picture></a></div><p><b>a</b> Representational similarity analysis (RSA) across all 80 sound categories (left panel) based on a pattern of 1582 acoustic features; dashed vertical lines indicate linear steps (0.2) in similarity. <b>b</b> Enlarged view of the four top lines of the RSA values for the four skull whistle sound categories and their acoustic similarity to sounds of the other 76 categories; sounds are sorted for similarity in descending order (lower values represent higher similarity). <b>c</b> Hierarchical clustering analysis on all sounds and their acoustic features (colored nodes with linkage cluster threshold of <i>c</i> &lt; 2.0). For abbreviations of sound categories, see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S1</a>.</p></div></figure></div><p>A subsequent hierarchical clustering analysis (HCA) confirmed these findings (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig3">3c</a>), showing that the original and replica skull whistles formed their own cluster of sound objects. The acoustically closest cluster consisted of contemporary Mexican flutes, tool/technical sounds (emergency horn, bell), and human body sounds (scratching nails). The second closest cluster consisted of animal sounds (insects, bovidae), human voices, and chainsaw sounds. It thus seems that skull whistles were acoustically close to other sounds with an intended acoustic and symbolic iconography, and to sounds that potentially provoke alerting, startling, and affective responses in human listeners. We have to note that these analyses of acoustic similarity and acoustic clustering are solely based on quantitative acoustic data without any bias by human perceptual impressions. But SW sounds to be similar to other sound with well-known basic psychoacoustic effects on listeners potentially across historical ages, SW thus might have been manufactured and used by Aztec communities for this hybrid purpose of serving both a symbolic and affective meaning for potential listeners.</p><h3 id="Sec28">Skull whistle sounds are perceived as aversive and startling</h3><p>To investigate the psychoacoustic effects and affective responses in listeners, we performed a perceptual assessment study with human participants. Based on dimensional arousal (how strong does the sound trigger emotional responses in listeners), valence (negative-to-positive emotional quality), urgency (how urgent would a listener respond to the sound), and naturalness ratings (how natural/unnatural is the sound), we located the SWs in this four-dimensional psychoacoustic affective space (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig4">4a</a>). Skull whistles were rated as being largely of negative emotional quality (valence) with a low-to-medium level of arousal intensity. Skull whistle sounds trigger a medium level of urgency responses in listeners and are rated as sounding largely unnatural.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Perceptual ratings on sounds."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Perceptual ratings on sounds.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://mclare.blog/articles/s44271-024-00157-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="138"/></picture></a></div><p><b>a</b> All 2567 sounds were perceptually rated by <i>n</i> = 70 human listeners along 4 dimensions (arousal, valence, urgency, naturalness) on a 10-point Likert scale. Shown are 2D plots with valence (negative −5, positive 5) on the y-axis and the other scales on the x-axis. Skull whistle sounds are color-coded in red; 9 major sound categories are color-coded in blue-to-yellow. <b>b</b> Mean, median, and distribution of ratings for each of the 13 sound categories. Rating on all four scales revealed a high inter-rater consistency (valence ICC = 0.78, F<sub>2566,23094</sub> = 4.611, <i>p</i> &lt; 0.001, CI<sub>95%</sub> [0.77 0.80]; arousal ICC = 0.71, F<sub>2566,23094</sub> = 3.450, <i>p</i> &lt; 0.001, CI<sub>95%</sub> [0.69 0.73]; urgency ICC = 0.80, F<sub>2566,23094</sub> = 5.037, <i>p</i> &lt; 0.001, CI<sub>95%</sub> [0.79 0.81]; naturalness ICC = 0.78, F<sub>2566,23094</sub> = 4.597, <i>p</i> &lt; 0.001, CI<sub>95%</sub> [0.77 0.79]). Red-colored bars at the bottom of plots indicate significant differences from the skull whistle categories (p &lt; 1e<sup>-5</sup>, FDR corrected, posthoc coefficient test; see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S2</a>–<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S5</a> for full statistics on posthoc comparisons); blue-colored bars at the top of plots indicate differences within the skull whistle categories (<i>p</i> &lt; 1e<sup>-5</sup>, FDR corrected, posthoc coefficient test; see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S2</a>–<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">5</a>). For abbreviations of sound categories, see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S1</a>.</p></div></figure></div><p>All four rating scales showed a main effect for differences between sound categories (valence F<sub>1,121.06</sub> = 221.46, <i>p</i> &lt; 0.001, eta2 = 0.10, CI<sub>95%</sub> [0.10 0.11]; arousal F<sub>1,94.96</sub> = 212.63, <i>p</i> &lt; 0.001, eta2 = 0.08, CI<sub>95%</sub> [0.07 0.09]; urgency F<sub>1,115.03</sub> = 494.64, <i>p</i> &lt; 0.001, eta2 = 0.08, CI<sub>95%</sub> [0.07 0.08]; naturalness F<sub>1,113.99</sub> = 106.16, <i>p</i> &lt; 0.001, eta2 = 0.16, CI<sub>95%</sub> [0.15 0.17]), but showed some differential effects when skull whistles were compared to other sound categories. All four skull whistle sound categories were rated similarly in terms of their high negative valence, and they revealed significantly the most negative valence compared with all other sound categories (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig4">4b</a>; for full statistics on posthoc comparisons, see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S2</a>). Similarly, skull whistles trigger significantly higher urgent tendencies than all other sound categories, with original skull whistles triggering somewhat lower urgencies than some of the replica skull whistles (Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S3</a>). Skull whistles were mostly significantly more arousing than other sounds, but no significant difference was found when compared to biological sounds (human, animal, nature), some contemporary Mexican flutes, and synthetic sounds (Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S4</a>). Only original skull whistle sounds were significantly more arousing than any other sound category. Finally, skull whistles sounded more unnatural than original biological sounds (human, animal, nature) and exterior sounds, and they largely also sounded less natural than some musical sounds (music, instrument) (Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S5</a>). No significant differences were found in naturalness rating when compared with contemporary Mexican flutes as well as interior and synthetic sounds. The sound of skull whistles thus seems to carry a negative emotional meaning of relevant arousal intensity. This seems to trigger urgent response tendencies in listeners, which is a typical psychoacoustic and affective profile of aversive, scary, and startling sounds<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Arnal, L. H., Kleinschmidt, A., Spinelli, L., Giraud, A.-L. &amp; Mégevand, P. The rough sound of salience enhances aversion through neural synchronisation. Nat. Commun. 10, 3671 (2019)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR33" id="ref-link-section-d62632563e1463">33</a></sup>.</p><p>As for the acoustic patterns of the sounds, we also performed two sets of similarity analyses based on the perceptual patterns from the four rating scales. Using a representational similarity analysis, the original and replica skull whistles again showed the closest perceptual similarity to each other (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig5">5a, b</a>, <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S1b</a>). The psychoacoustically and effectively closest sounds to skull whistles were many exteriors (firearm, siren, horn), instruments (Mexican scary flute), and human sounds (fear, pain, anger, sad voice) that have an alarming, startling, and aversive effect on listeners. The psychoacoustically and effectively most distant sounds were instrument (guitar, string, brass, percussion), nature (water, fire), and some Mexican flute sounds (animal imitating flutes), and most of these sounds usually have a pleasant and relaxing listener effect. This pattern was also confirmed by a hierarchical clustering analysis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig5">5c</a>), where skull whistles formed their own cluster, immediately neighbored by a cluster of exterior alarming and startling sounds (horn, siren, firearm). Further close sound cluster consisted of human (fear, pain, anger, sad voice) and some Mexican flute sounds (Mexican scary flute).</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="RSA analysis on perceptual ratings on sounds and hierarchical clustering analysis."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: RSA analysis on perceptual ratings on sounds and hierarchical clustering analysis.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://mclare.blog/articles/s44271-024-00157-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig5_HTML.png?as=webp"/><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="2063"/></picture></a></div><p><b>a</b> Representational similarity analysis (RSA) on the pattern of perceptual ratings (left panel); dashed vertical lines indicate linear steps (0.2) in similarity. <b>b</b> Enlarged view of the four top lines are the RSA values for the four skull whistle sound categories and their perceptual similarity to sounds of the other 76 categories; sounds are sorted for similarity in descending order (lower values represent higher similarity). <b>c</b> Hierarchical clustering analysis on all sounds and their perceptual ratings (colored nodes with linkage cluster threshold of <i>c</i> &lt; 2.0). For abbreviations of sound categories, see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S1</a>.</p></div></figure></div><p>Overall, skull whistles seem to produce a perceptually unique cluster of sounds that share and mimic affective qualities with many other scary and aversive sounds. While some of these effectively similar sounds might not have been present in prehispanic Aztec environments and thus impossible to be directly mimicked by skull whistles (firearm, siren), some other biological sounds were part of Aztec environments (human voices). We might thus speculate that skull whistles have been created by Aztec communities to mimic the acoustic nature of sounds or at least mimic the psychoacoustic and affective impact that sounds can have on listeners. This might have been done for cultural and ritual purposes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Both, A. A. Aztec music culture. World Music 52, 14–28 (2010)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR11" id="ref-link-section-d62632563e1516">11</a></sup>, but further cross-disciplinary and archeological evidence is needed here.</p><h3 id="Sec29">Skull whistle sounds have a hybrid natural-artificial nature</h3><p>If skull whistles acoustically and effectively mimics other sounds and sound qualities, naive listeners might form associations and speculations about the origin of the sound<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Guastavino, C. et al. Sound categories: category formation and evidence-based taxonomies. (2018) 
                  https://doi.org/10.3389/fpsyg.2018.01277
                  
                ." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR34" id="ref-link-section-d62632563e1528">34</a></sup>. We asked humans who were completely unfamiliar with SW sounds to listen to a balanced selection of 200 out of the total 2567 sounds to provide a substantive and adjective label for each sound in a free-choice labeling approach. The substantive label should specify the underlying acoustic object or process that potentially produced the sound, while the adjective should provide a verbal affective description of the sound (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig6">6a</a>).</p><div data-test="figure" data-container-section="figure" id="figure-6" data-title="Free categorical and emotional labeling of sounds."><figure><figcaption><b id="Fig6" data-test="figure-caption-text">Fig. 6: Free categorical and emotional labeling of sounds.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://mclare.blog/articles/s44271-024-00157-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig6_HTML.png?as=webp"/><img aria-describedby="Fig6" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="285"/></picture></a></div><p><b>a</b> Human listeners (<i>n</i> = 40) labeled 200 sounds (4 skull whistle categories, 8 other sound categories) with a substantive label (“Labels”) describing the origin or source of the sound (upper panel) as well as with an adjective label (“Adjectives”) describing their emotional response to the sound (lower panel). Plots show the sorted frequency of labels used as the relative number (relative nb = total nb of label multiplied by the percentage of listeners using the label; y-axis is relative nb/100); plots only show results for the four skull whistle categories. Asterisks indicate significance <i>p</i> &lt; 0.05 (significant difference of observed frequency above expected probability), binomial test, FDR corrected; see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S6</a>-<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">7</a>. <b>b</b> To perform a correspondence analysis (CA) on the labels, we re-labeled the original labels along the 9 major sound categories and 8 major emotional dimensions according to the model by Plutchik<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Plutchik, R. A psychoevolutionary theory of emotions. Soc. Sci. Inf. 21, 529–553 (1982)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR24" id="ref-link-section-d62632563e1565">24</a></sup>. Re-labeling was done based on the classification probabilities of <i>n</i> = 7 independent raters. *<i>p</i> &lt; 0.05, binomial test, FDR corrected; see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S8</a>-<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">9</a>. <b>c</b> Explained variance of the dimensions resulting from the CA; dimensions 1–3 explained &gt;61% of the variance in the data. <b>d</b> Dimensions 1–3 of the CA for categorical labels and emotional adjectives. <b>e</b> Probability for classifying sounds skull whistle sounds and sounds form 5 other categories as originating from an “animated”, “technical”, or environmental source. Left plot is for the experiment including original skull whistles (<i>n</i> = 76), right plot is for the experiment including replica skull whistles (<i>n</i> = 58). <b>f</b> Reaction time and accuracy data from the dichotic listening experiment, where humans discriminated low/high tone on the attended ear while presenting other sounds or silence on the unattended ear. Left plot is for the experiment including original skull whistles (<i>n</i> = 47), the right plot is for the experiment including replica skull whistles (<i>n</i> = 47).</p></div></figure></div><p>The most frequent substantive label given by human listeners was “scream” (SW orig: binomial test <i>p</i> &lt; 0.001, all FDR corrected, relative risk RR = 16.35, CI<sub>95%</sub> [0.24 0.38]; SW high: <i>p</i> &lt; 0.001, RR = 25.95, CI<sub>95%</sub> [0.65 0.82]; SW med: p &lt; 0.001, RR = 26.63, CI<sub>95%</sub> [0.46 0.67]; SW low: <i>p</i> &lt; 0.001, RR = 25.84, CI<sub>95%</sub> [0.45 0.67]), while the two most frequent adjective labels were “aversive” (SW orig: p &lt; 0.001, RR = 21.07, CI<sub>95%</sub> [0.24 0.39]; SW high: <i>p</i> &lt; 0.001, RR = 9.42, CI<sub>95%</sub> [0.13 0.32]; SW med: <i>p</i> &lt; 0.001, RR = 17.37, CI<sub>95%</sub> [0.27 0.48]; SW low: <i>p</i> &lt; 0.001, RR = 14.69, CI<sub>95%</sub> [0.22 0.42]) and “scary” (SW orig: <i>p</i> &lt; 0.001, RR = 11.97, CI<sub>95%</sub> [0.12 0.25]; SW high: <i>p</i> &lt; 0.001, RR = 17.28, CI<sub>95%</sub> [0.23 0.51]; SW med: <i>p</i> &lt; 0.001, RR = 8.17, CI<sub>95%</sub> [0.10 0.27]; SW low: <i>p</i> &lt; 0.001, RR = 11.75, CI<sub>95%</sub> [0.17 0.35]). Other frequent substantive labels were a mix of living (human) and technical sound sources (concerning vehicles, instruments, or kettles/whistles), with the most diverse labels found for original skull whistles (see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S6</a>–<a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">7</a> for a full report of statistics). Although natural sound sources were mentioned, they were not frequent enough for statistical significance (see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S6</a>). All original substantive and adjective labels were then re-labeled to the labels of the 9 major sound categories according to our major sound dataset, and according to 10 emotional labels from a common taxonomy of human affect<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Plutchik, R. A psychoevolutionary theory of emotions. Soc. Sci. Inf. 21, 529–553 (1982)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR24" id="ref-link-section-d62632563e1685">24</a></sup>. The most frequent substantive label was “human” (SW orig: <i>p</i> &lt; 0.001, RR = 3.21, CI<sub>95%</sub> [0.30 40]; SW high: <i>p</i> &lt; 0.001, RR = 6.97, CI<sub>95%</sub> [0.71 0.83]; SW med: <i>p</i> &lt; 0.001, RR = 4.19, CI<sub>95%</sub> [0.40 0.53]; SW low: <i>p</i> &lt; 0.001, RR = 5.17, CI<sub>95%</sub> [0.50 0.64]), followed by exterior/interior sound sources (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig6">6b</a>, see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S8</a> for full report of statistics). Again, skull whistle sounds were most diverse here, also including “instrument” and “nature” labels. “Fear” (SW orig: <i>p</i> &lt; 0.001, RR = 6.27, CI<sub>95%</sub> [0.57 0.68]; SW high: <i>p</i> &lt; 0.001, RR = 8.68, CI<sub>95%</sub> [0.82 0.91]; SW med: <i>p</i> &lt; 0.001, RR = 7.89, CI<sub>95%</sub> [0.74 0.85]; SW low: <i>p</i> &lt; 0.001, RR = 8.15, CI<sub>95%</sub> [0.76 0.86]) was by far the most frequent emotional label, with some original SW sounds also being labeled with “joy” (SW orig: <i>p</i> &lt; 0.001, RR = 2.51, CI<sub>95%</sub> [0.21 030]) (Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S9</a>).</p><p>Based on these re-labeled data, we performed a correspondence analysis to assess how labels provided for the SWs would correspond to labels given to the other sounds (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig6">6c, d</a>). Skull whistle sounds were located in an acoustic object space between human sounds on one end and a cluster of exterior/interior/synthetic sounds on the other end, with also a space of “unknown” labeling close by. “Unknown” labels were provided if listeners could not associate any object or process with the sound. For the affective space, skull whistle sounds were centered around the fear and anger label, as some other human, exterior/interior, and electronic music effects also did. These labeling data together point to a hybrid natural-artificial status of skull whistle sounds, such that they are primarily associated with a human origin, but also form associations with sounds produced from technical objects and processes.</p><p>This hybrid status was also confirmed in an independent experiment, where we asked participants to classify sounds into three possible categories regarding the potential origin of the sound as being either from an animated (living organism), technical (technical object/process), or environmental sound source (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig6">6e</a>). The experiment was set up as a 3-alternative forced choice (3AFC) task. Whereas most of the other sounds were classified into a specific category (except for nature sounds being both technical and environmental) (sound-by-class interaction, F<sub>10,750</sub> = 467.322, <i>p</i> &lt; 0.001, eta2 = 0.70, CI<sub>95%</sub> [0.65 0.74]; Greenhouse-Geisser (GG) corrected <i>p</i>-value based on the Mauchly test for checking sphericity violations), skull whistles received a somehow mixed classification both for the original and replica SWs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig6">6e</a> left and right panel). For original skull whistles, classifications were mixed between the animated (<i>p</i> &lt; 0.001, eta2 = 0.59, CI<sub>95%</sub> [0.40 0.56]; comparison against “environment”) and technical categories (<i>p</i> &lt; 0.001, eta2 = 0.42, CI<sub>95%</sub> [0.24 0.37]; comparison against “environment”), with classifications as “animated” being higher than classifications as “technical” (<i>p</i> = 0.003, eta2 = 0.10, CI<sub>95%</sub> [0.05 0.29]). An almost identical pattern was found in a second experiment where we used replica SWs instead of the original skull whistle sounds (sound-by-class interaction, F<sub>10,570</sub> = 374.785, <i>p</i> &lt; 0.001, eta2 = 0.68, CI<sub>95%</sub> [0.65 0.71]; GG corrected). Replica skull whistle sounds again received a mixed classification between the animated (<i>p</i> &lt; 0.001, eta2 = 0.53, CI<sub>95%</sub> [0.32 0.50]; comparison against “environment”) and technical categories (<i>p</i> &lt; 0.001, eta2 = 0.67, CI<sub>95%</sub> [0.47 0.65]; comparison against “environment”), but the comparisons between animated and technical classifications did not reveal a significant difference (<i>p</i> = 0.092, eta2 = 0.07, CI<sub>95%</sub> [−0.02 0.33]).</p><p>The pattern of emotional labeling again pointed to a rather negative quality of skull whistle sounds, and the origin might stem from socially relevant human voices and sounds produced by technical devices that have biological significance. Sounds with social and biological relevance often receive increased attention from the cognitive system to prioritize their processing, and this often interferes with other ongoing mental processes. We tested this in a dichotic listening experiment, where humans performed low-high discriminations on simple tones (high- or low-pitch tone) presented on their attended ear, while we presented skull whistle and other sounds on the unattended ear (or no sound, silent condition) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig6">6f</a>). For an experiment including original skull whistle sounds, all sounds presented on the unattended ear interfered with the tone discrimination task performed on the attended ear (F<sub>7,322</sub> = 9.075, <i>p</i> &lt; 0.001, eta2 = 0.01, CI<sub>95%</sub> [0.003 0.011]; GG corrected), especially in terms of slowing down the reaction time (posthoc tests against silent condition; all <i>p</i> &lt; 0.020; for full statistics see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S10</a>). Comparing original skull whistles with all other sound categories regarding the interference level did not reveal significant differences (<i>p</i> &gt; 0.473, Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S10</a>), but only exterior sounds showed a significantly higher interference level than skull whistle sounds (<i>p</i> = 0.005). The same pattern of results was obtained in a second experiment with replica skull whistles instead of the original skull whistle sounds (F<sub>7,308</sub> = 13.503, <i>p</i> &lt; 0.001; GG corrected; posthoc tests against the silent condition, all <i>p</i> &lt; 0.003, for full statistics see Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S11</a>), including significantly stronger interference effects of exterior sounds compared to skull whistle sounds (<i>p</i> = 0.046, Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S11</a>).</p><p>Being able to interfere with other ongoing mental processes highlights the notion that skull whistle sounds carry some relevant social and/or biological meaning and associations to human listeners. We have to highlight the notion here again that we only investigated these psychoacoustic effects in samples of naïve European listeners from modern cultures. Basic psychoacoustic distraction effects of meaningful sounds are however even present in primate species close to humans, which share basic auditory and affective sound processing dynamics<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Bodin, C. et al. Functionally homologous representation of vocalizations in the auditory cortex of humans and macaques. Curr. Biol. 31, 4839–4844.e4 (2021)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR35" id="ref-link-section-d62632563e1862">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Frühholz, S. &amp; Schweinberger, S. R. Nonverbal auditory communication—evidence for integrated neural systems for voice signal production and perception. Prog. Neurobiol. (2020) 
                  https://doi.org/10.1016/j.pneurobio.2020.101948
                  
                ." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR36" id="ref-link-section-d62632563e1865">36</a></sup>. Despite cultural differences, there might also be large psychoacoustic processing similarities between the evolutionary closer modern and Aztec humans, and psychoacoustic effects of SWs might have been capitalized on by Aztec communities.</p><h3 id="Sec30">Neural decoding of skull whistle sounds requires elaborated and associative processing</h3><p>We finally performed a neuroimaging experiment with human listeners using functional magnetic resonance imaging. We investigated the neural processes that support decoding of such associated and affective meanings from SW sounds, examining whether this decoding happens at the level of basic auditory processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Frühholz, S., Trost, W. &amp; Kotz, S. A. The sound of emotions-towards a unifying neural network perspective of affective sound processing. Neurosci. Biobehav. Rev. 68, 1–15 (2016)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR37" id="ref-link-section-d62632563e1878">37</a></sup> or rather at the level of higher-order cognition<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Frühholz, S. &amp; Grandjean, D. Processing of emotional vocalizations in bilateral inferior frontal cortex. Neurosci. Biobehav. Rev. 37, 2847–2855 (2013)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR38" id="ref-link-section-d62632563e1882">38</a></sup>. We again presented the selection of 200 sounds from the broader sound dataset, including original and replica skull whistle sounds, sounds from five major sound categories (human, animal, technical, nature, instrument) as well as additional eight white and pink noise stimuli. Common and known sounds typically elicit activity in auditory cortical regions (AC)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Bizley, J. K. &amp; Cohen, Y. E. The what, where and how of auditory-object perception. Nat. Rev. Neurosci. 14, 693–707 (2013)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR39" id="ref-link-section-d62632563e1886">39</a></sup> for basic acoustic analysis and auditory cognition<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Rauschecker, J. P. &amp; Scott, S. K. Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing. Nat. Neurosci. 12, 718–724 (2009)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR40" id="ref-link-section-d62632563e1890">40</a></sup>. Sounds from the five major sound categories elicited significantly higher activity in many low- and higher-order AC regions compared to skull whistle sounds (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig7">7a</a>, Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S12</a>).</p><div data-test="figure" data-container-section="figure" id="figure-7" data-title="Brain responses to skull whistle sounds."><figure><figcaption><b id="Fig7" data-test="figure-caption-text">Fig. 7: Brain responses to skull whistle sounds.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://mclare.blog/articles/s44271-024-00157-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig7_HTML.png?as=webp"/><img aria-describedby="Fig7" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs44271-024-00157-7/MediaObjects/44271_2024_157_Fig7_HTML.png" alt="figure 7" loading="lazy" width="685" height="498"/></picture></a></div><p><b>a</b> Functional brain activity in human listeners (<i>n</i> = 32) when comparing activity for skull whistles with activity for all other 5 general sound categories. <b>b</b> Brain activity when contrasting skull whistle with animated sounds (human, animal; left panel) and with artificial sounds (technical, instrument; right panel). <b>c</b> Functional connectivity patterns from seed regions in the right fronto-parietal network that were significant for processing skull whistle sounds (upper panel), and for the left (mid panel) and right auditory cortex regions (lower panel) that were significant for processing other sounds. Voxel-level threshold of <i>p</i> = 0.005 combined with an FWE-corrected cluster-level threshold of <i>p</i> = 0.05. <b>d</b> Representational similarity matrices (RSM) for acoustic and percental patterns of the sounds included in the neuroimaging experiment. Bottom plots show the RSM for the skull whistle sounds only (rev, reversed coding), corresponding to the black box in the upper panels. <b>e</b> Brain areas with significant cross-correlation between the acoustic and perceptual similarity patterns and measures of neural similarity between sound categories. All brain activity from contrasts includes a voxel threshold <i>p</i> &lt; 0.05 (FWE corrected), cluster threshold <i>k</i> = 10.</p></div></figure></div><p>Contrarily, neural activity for skull whistle sounds was rather located in regions for higher-order cognition. Skull whistle sounds compared to all other sounds elicited brain activity in the lateral (<i>IFC</i> inferior frontal cortex) and medial frontal cortex (MFC) as well as the insula. The IFC typically provides elaborated sound evaluation and classification processes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Frühholz, S. &amp; Grandjean, D. Processing of emotional vocalizations in bilateral inferior frontal cortex. Neurosci. Biobehav. Rev. 37, 2847–2855 (2013)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR38" id="ref-link-section-d62632563e1957">38</a></sup>, with potential support of sensory-affective integration mechanisms in the insula, and associative processing in the MFC<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Frühholz, S., Trost, W. &amp; Kotz, S. A. The sound of emotions-towards a unifying neural network perspective of affective sound processing. Neurosci. Biobehav. Rev. 68, 1–15 (2016)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR37" id="ref-link-section-d62632563e1961">37</a></sup>. Brain activity in the insula and MFC as well as additional activity in the intraparietal sulcus (IPS) for skull whistle sounds were especially found when compared to brain activity for animated sounds (human, animal) but not when compared to artificial sounds (technical, instrument) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig7">7b</a>; see also Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S2</a>). This might further highlight the notion that skull whistle sounds are of a hybrid nature, such that they appear as partly coming from an animated source but also having a wider symbolic and associative meaning. This wider meaning might warrant increased processing in the fronto-insular brain systems as well as an elaborated sound mapping and acoustic evidence integration in the IPS<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Yao, J. D., Gimoto, J., Constantinople, C. M. &amp; Sanes, D. H. Parietal cortex is required for the integration of acoustic evidence. Curr. Biol. 30, 3293–3303.e4 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR41" id="ref-link-section-d62632563e1972">41</a></sup>.</p><p>This right fronto-parietal-insular network for skull whistle sound processing was largely interconnected to itself, as revealed by a functional brain connectivity analysis, with additional connections to the left insula and the anterior cingulate cortex, which supports the MFC in the cognitive appraisal of sounds<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Etkin, A., Egner, T. &amp; Kalisch, R. Emotional processing in anterior cingulate and medial prefrontal cortex. Trends Cogn. Sci. 15, 85–93 (2011)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR42" id="ref-link-section-d62632563e1979">42</a></sup> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig7">7c</a>, Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S13</a>). Interestingly, the functional network for processing the other sounds, which was centered especially in left AC, showed links with the skull whistle processing network, especially to the right fronto-insular system and the MFC. Whereas for other sounds, this connection might point to a natural forward mapping from sound analysis (auditory cortex) to sound evaluation (fronto-insular), the neural network for skull whistle sound processing suggests a challenging level of elaborated processing and higher-order cognition.</p><p>While the previous analysis mainly quantified the neural differences in the processing of skull whistle sounds compared with other sounds, we finally also performed an analysis that focused more on the neural similarity in processing such sounds. Identical to the RSA of the acoustic and perceptual patterns for all 2567 sounds (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig3">3</a>a, b, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig5">5a, b</a>; Tab. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://mclare.blog/articles/s44271-024-00157-7#MOESM2">S14</a>), we performed an RSA for the 192 selected sounds (excluding noise sounds) of the neuroimaging experiment. Again, the skull whistles were most similar to each other, and all other sounds showed a variable degree of similarity to the skull whistle based on the acoustic and perceptual patterns (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig7">7d</a>). We combined these parametric acoustic and perceptual data with a parametric analysis of the neural similarity of activation patterns between sound categories<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Giordano, B. L., McAdams, S., Zatorre, R. J., Kriegeskorte, N. &amp; Belin, P. Abstract encoding of auditory objects in cortical activity patterns. Cereb. Cortex 23, 2025–2037 (2013)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR43" id="ref-link-section-d62632563e2004">43</a></sup>. For the latter, we performed a multivariate decoding analysis at every brain location to quantify the likelihood that each location can separate between any of two sound categories based on their activation pattern. The reverse of this separation likelihood was taken as a measure of neural similarity<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Giordano, B. L., McAdams, S., Zatorre, R. J., Kriegeskorte, N. &amp; Belin, P. Abstract encoding of auditory objects in cortical activity patterns. Cereb. Cortex 23, 2025–2037 (2013)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR43" id="ref-link-section-d62632563e2009">43</a></sup>. The resulting neural cross-correlation data as described below were quite distinctive to the observed acoustic and perceptual similarity patterns in relation to skull whistles, as no neural significance was found when randomly permuting the similarity matrixes (see “Methods”).</p><p>Cross-correlating the acoustic similarity for the sound categories with neural similarity revealed a broadly extended neural significance in the AC, IFC, and MFC, and (pre)motor cortex (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://mclare.blog/articles/s44271-024-00157-7#Fig7">7e</a>). Cross-correlating perceptual similarity with neural similarity revealed neural significance in bilateral low-order AC and right higher-order AC. This focus of neural significance for the perceptual similarity analysis on the AC might have been driven by similarity to very specific sounds, such as baby cries, screams, or alarm sounds, which have distinctive rough and piercing sound features as well as aversive affective features, which receive low-level decoding in the neural auditory system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Concina, G., Renna, A., Grosso, A. &amp; Sacchetti, B. The auditory cortex and the emotional valence of sounds. Neurosci. Biobehav. Rev. 98, 256–264 (2019)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR44" id="ref-link-section-d62632563e2020">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Staib, M., Abivardi, A. &amp; Bach, D. R. Primary auditory cortex representation of fear‐conditioned musical sounds. Hum. Brain Mapp. 41, 882–891 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR45" id="ref-link-section-d62632563e2023">45</a></sup>. The neural similarity analysis based on acoustic patterns seems to involve a more extensive brain network and various levels of processing, potentially ranging from acoustic analysis (AC), and acoustic object classifications (IFC), to elaborate evaluations (MFC). The specific role of the (pre)motor cortex might be to represent embodied acoustic recognitions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Archakov, D. et al. Auditory representation of learned sound sequences in motor regions of the macaque brain. Proc. Natl Acad. Sci. 117, 15242–15252 (2020)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR46" id="ref-link-section-d62632563e2027">46</a></sup> and to discriminate acoustic patterns that are critical for behavioral choices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Morán, I., Perez-Orive, J., Melchor, J., Figueroa, T. &amp; Lemus, L. Auditory decisions in the supplementary motor area. Prog. Neurobiol. 202, 102053 (2021)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR47" id="ref-link-section-d62632563e2031">47</a></sup>.</p><p>While the neural effort for decoding similarities of acoustic patterns between skull whistle and other sounds thus seems to involve a multi-level process, pointing to an ambiguous mixture of familiar and unfamiliar sound components as well as a multi-layered symbolism, the neural effort in terms of matching perceived affective similarities seems rather unambiguous.</p></div></div></section><section data-title="Discussion"><div id="Sec31-section"><h2 id="Sec31">Discussion</h2><div id="Sec31-content"><p>We carefully assume a certain level of comparability of how modern humans and humans in Aztec communities many centuries ago responded to skull whistle sounds on a basic affective level. This assumption might be valid given the very basic and salient psychoacoustic effects that rely on biological and neural principles of sound recognition that are even shared between humans and animals<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Frühholz, S. &amp; Schweinberger, S. R. Nonverbal auditory communication—evidence for integrated neural systems for voice signal production and perception. Prog. Neurobiol. (2020) 
                  https://doi.org/10.1016/j.pneurobio.2020.101948
                  
                ." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR36" id="ref-link-section-d62632563e2047">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="LeDoux, J. Rethinking the emotional brain. Neuron 73, 653–676 (2012)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR48" id="ref-link-section-d62632563e2050">48</a></sup>. Human listeners in our experiments rated skull whistle sounds as very negative and specifically labeled them largely as scary and aversive, which potentially also trigger urgent response tendencies and interfere with ongoing mental processes<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Frühholz, S., Trost, W. &amp; Kotz, S. A. The sound of emotions-towards a unifying neural network perspective of affective sound processing. Neurosci. Biobehav. Rev. 68, 1–15 (2016)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR37" id="ref-link-section-d62632563e2054">37</a></sup>. This immediacy effect is further highlighted by very specific brain activity in low-order AC that is correlated with the affective similarity of SWs to other sounds. In case of an aversive and alerting sound quality, low-order AC detects the aversive sound quality<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Concina, G., Renna, A., Grosso, A. &amp; Sacchetti, B. The auditory cortex and the emotional valence of sounds. Neurosci. Biobehav. Rev. 98, 256–264 (2019)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR44" id="ref-link-section-d62632563e2058">44</a></sup> and tunes the neural system for in-depth sound analysis<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Arnal, L. H., Kleinschmidt, A., Spinelli, L., Giraud, A.-L. &amp; Mégevand, P. The rough sound of salience enhances aversion through neural synchronisation. Nat. Commun. 10, 3671 (2019)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR33" id="ref-link-section-d62632563e2062">33</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Kumar, S., von Kriegstein, K., Friston, K. &amp; Griffiths, T. D. Features versus feelings: dissociable representations of the acoustic features and valence of aversive sounds. J. Neurosci. 32, 14184–14192 (2012)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR49" id="ref-link-section-d62632563e2065">49</a></sup>.</p><p>We of course have to highlight the notion that our experimental data were acquired with samples from a modern European population. Our data thus first provide basic evidence of how modern humans respond to the acoustic quality of a historically very important sound tool with a unique acoustic profile. All listeners in our experiences were naïve about the presence of skull whistle sounds on the acoustic samples presented in the experiments, and thus all data are largely unbiased and basic assessments of the psychoacoustic effects of skull whistles. On this basic level of psychoacoustic processing, we might assume some similarity to the acoustic processing in humans from previous Aztec cultures. Aztec communities might have capitalized on this aversive and scary nature of skull whistle sounds in specific contexts, which would support the introduced warfare and ritual symbolism hypothesis, but rather not the deity symbolism hypothesis. However, given also the strong associative/symbolic sound impressions requiring higher-order cognition, warfare usage seemed rather unlikely. Given both the aversive/scary and associative/symbolic sound nature as well as currently known excavation locations at ritual burial sites with human sacrifices, usage in ritual contexts seems very likely, especially in sacrificial rites and ceremonies related to the dead. Skull whistles might have been used to scare the human sacrifice or the ceremonial audience, but further cross-documentation is needed here. The symbolic and associative meaning might be specifically related to the dangerous and scary travel of the dead to the underworld. This potential double nature of a biological and symbolic affective significance might also be based on the hybrid natural-artificial status of skull whistle sounds, as SWs sounds received ambiguous associations with natural and artificial sound sources across different experiments and analyses in our study.</p><p>Listeners in our experiments were naïve to the presented sounds, especially about the fact that the broader sound set also included skull whistle sounds. Modern humans perceived skull whistle sounds as being of a hybrid natural-artificial nature and a hybrid familiar-unfamiliar nature. Unlike naïve listeners in our experiments, human listeners in Aztec communities were probably aware that the sound originated from the skull whistles as a musical sound tool, especially when used in ritual contexts and being a common part of Aztec musical culture. Being aware that the skull whistle sound was produced by a technical tool, it still seems to elicit impressions of originating or mimicking an animated sound source. This might greatly contribute to the symbolic and potentially fictional nature of skull whistle sounds<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Trevor, C. &amp; Frühholz, S. Music as an evolved tool for socio-affective fiction. Emot. Rev. 16, 180–194 (2024)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR50" id="ref-link-section-d62632563e2075">50</a></sup> and highlights the frequent intention of ancient cultures to capture and represent mythical entities/contexts in musical tools<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Trevor, C. &amp; Frühholz, S. Music as an evolved tool for socio-affective fiction. Emot. Rev. 16, 180–194 (2024)." href="#ref-CR50" id="ref-link-section-d62632563e2079">50</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Mehr, S. A., Krasnow, M. M., Bryant, G. A. &amp; Hagen, E. H. Origins of music in credible signaling. Behav. Brain Sci. 44, e60 (2021)." href="#ref-CR51" id="ref-link-section-d62632563e2079_1">51</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Barber, S. B. &amp; Sánchez, M. O. A divine wind: the arts of death and music in terminal formative Oaxaca. Anc. Mesoam. 23, 9–24 (2012)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR52" id="ref-link-section-d62632563e2082">52</a></sup>.</p><p>In terms of the hybrid, symbolic, and potentially fictional nature of skull whistle sounds mentioned above, this is also reflected in our neuroimaging brain data, which makes the ritual hypothesis more likely than the warfare hypothesis. Listening to skull whistle sounds elicited significantly higher brain activity in regions and neural networks associated with higher-order auditory cognition, sound evaluation, and associative processing. Sounds with affective significance commonly trigger neural processes to evaluate the emotional nature and the specific source of the sound because this is relevant to preparing appropriate behavioral responses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Frühholz, S., Trost, W. &amp; Kotz, S. A. The sound of emotions-towards a unifying neural network perspective of affective sound processing. Neurosci. Biobehav. Rev. 68, 1–15 (2016)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR37" id="ref-link-section-d62632563e2089">37</a></sup>. Skull whistle sounds are unambiguous in their affective nature but are rather ambiguous in the determination of their sound origin, which intensifies higher-order brain processing. The psychoacoustic ambiguity and hybridity however open many options for symbolism in an archeoacoustic perspective.</p><h3 id="Sec32">Limitations</h3><p>As an important limitation, we have first to highlight the notion that we only tested the psychoacoustic nature of skull whistles in human samples from modern European populations, which imposes some limitations in terms of the anthropological and cultural comparability to Aztec cultures many centuries ago. This unfortunately is a common limitation for all experimental historical studies. However, we are confident that some basic neurocognitive mechanisms are shared between modern humans and humans from Aztec cultures. Many affective and auditory cognitive mechanisms are shared between humans and their closest monkey species<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Bodin, C. et al. Functionally homologous representation of vocalizations in the auditory cortex of humans and macaques. Curr. Biol. 31, 4839–4844.e4 (2021)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR35" id="ref-link-section-d62632563e2101">35</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Frühholz, S. &amp; Schweinberger, S. R. Nonverbal auditory communication—evidence for integrated neural systems for voice signal production and perception. Prog. Neurobiol. (2020) 
                  https://doi.org/10.1016/j.pneurobio.2020.101948
                  
                ." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR36" id="ref-link-section-d62632563e2104">36</a></sup>. The shared neurocognitive mechanisms between modern humans and Aztec humans thus seem very reasonable, and we tested for the basic neurocognitive mechanisms of decoding skull whistle sounds in the last experiment in our study.</p><p>A related topic concerns the pattern of sound categories that were used in some experiments. These categories to classify sounds were derived from modern classifications of sounds as used in acoustic, machine-learning, and psychological studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Piczak, K. J. ESC: Dataset for environmental sound classification. in MM 2015—Proceedings of the 2015 ACM Multimedia Conference 1015–1018 (Association for Computing Machinery, Inc, 2015). 
                  https://doi.org/10.1145/2733373.2806390
                  
                ." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR15" id="ref-link-section-d62632563e2111">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Frühholz, S., Trost, W. &amp; Kotz, S. A. The sound of emotions-towards a unifying neural network perspective of affective sound processing. Neurosci. Biobehav. Rev. 68, 1–15 (2016)." href="https://mclare.blog/articles/s44271-024-00157-7#ref-CR37" id="ref-link-section-d62632563e2114">37</a></sup>. Some of these sound categories might not match the soundscape of Aztec cultures. Unfortunately, a detailed taxonomy of sound classes described by Aztec cultures is missing today, but major sound classes (human, animal, nature, music, tool sounds) might be similar to modern humans. We have to note that all participants in our study were naive and unbiased regarding the sounds used, and the sounds were not presented as belonging to a certain sound category. The taxonomy of sound classes was only used posthoc during data analysis for the purpose of data grouping.</p><p>An additional common limitation for experimental historical studies is that evidence is often not ultimate in favor of one of several competing hypotheses. New evidence might support certain hypotheses to some degree while disconfirming other hypotheses to another degree. This also concerns the data reported here, which are relatively but not absolutely in favor of one of the current hypotheses concerning skull whistles (ritual symbolism hypothesis). To obtain a relatively broad, consistent, and relatively unbiased assessment of SW sounds, we here performed several physical and perceptual experiments on skull whistle sounds using diverse approaches. This allowed a multi-level and critical weighing of evidence in support of certain hypotheses.</p></div></div></section>
                </div><div>
                <section data-title="Data availability"><div id="data-availability-section"><h2 id="data-availability">Data availability</h2><div id="data-availability-content">
              
              <p>The conditions of ethics approval and consent procedures do not permit the public archiving of the participants’ anonymized study data. These data are available upon request to the corresponding author in consultation with the cantonal ethics committee of the Swiss canton Zurich. Preprocessed numerical data are available at <a href="https://github.com/caneuro/skullwhistle">https://github.com/caneuro/skullwhistle</a>, and functional brain maps are available at <a href="https://identifiers.org/neurovault.collection:18295">https://identifiers.org/neurovault.collection:18295</a>. Examples of skull whistle sounds can be found here: <a href="https://caneuro.github.io/blog/2024/study-skullwhistle/">https://caneuro.github.io/blog/2024/study-skullwhistle/</a>.</p>
            </div></div></section><section data-title="Code availability"><div id="code-availability-section"><h2 id="code-availability">Code availability</h2><div id="code-availability-content">
              
              <p>Study-specific analysis codes and data are deposited at <a href="https://github.com/caneuro/skullwhistle">https://github.com/caneuro/skullwhistle</a>, and the code can be sued to reproduce the statistical analysis as reported in the manuscript. which can be used together with the Several codes for the data analysis are taken from existing toolboxes: MPS analysis (<a href="https://github.com/theunissenlab/soundsig">https://github.com/theunissenlab/soundsig</a>), RSA analysis (<a href="https://github.com/rsagroup/rsatoolbox_matlab">https://github.com/rsagroup/rsatoolbox_matlab</a>), CA analysis (<a href="http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/113-ca-correspondence-analysis-in-r-essentials/#r-packages">http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/113-ca-correspondence-analysis-in-r-essentials/#r-packages</a>), brain data analysis with <a href="https://www.fil.ion.ucl.ac.uk/spm/software/spm12/">SPM12</a> brain connectivity analysis with the CONN toolbox (<a href="https://web.conn-toolbox.org/">https://web.conn-toolbox.org/</a>), and MVPA analysis (<a href="https://sites.google.com/site/tdtdecodingtoolbox/">https://sites.google.com/site/tdtdecodingtoolbox/</a>).</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">McDermott, J. The evolution of music. <i>Nature</i> <b>453</b>, 287–288 (2008).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/453287a" data-track-item_id="10.1038/453287a" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2F453287a" aria-label="Article reference 1" data-doi="10.1038/453287a">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18480798" aria-label="PubMed reference 1">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20evolution%20of%20music&amp;journal=Nature&amp;doi=10.1038%2F453287a&amp;volume=453&amp;pages=287-288&amp;publication_year=2008&amp;author=McDermott%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="2."><p id="ref-CR2">Both, A. A. Skull whistles of the Aztecs (Totenkopfpfeifen der Azteken). <i>Baessler-Arch.</i> <b>53</b>, 43–54 (2005).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Skull%20whistles%20of%20the%20Aztecs%20%28Totenkopfpfeifen%20der%20Azteken%29&amp;journal=Baessler-Arch.&amp;volume=53&amp;pages=43-54&amp;publication_year=2005&amp;author=Both%2CAA">
                    Google Scholar</a> 
                </p></li><li data-counter="3."><p id="ref-CR3">Velázquez Cabrera, R. Silbato de la muerte. <i>Arqueología</i> <b>42</b>, 184–202 (2009).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Silbato%20de%20la%20muerte&amp;journal=Arqueolog%C3%ADa&amp;volume=42&amp;pages=184-202&amp;publication_year=2009&amp;author=Vel%C3%A1zquez%20Cabrera%2CR">
                    Google Scholar</a> 
                </p></li><li data-counter="4."><p id="ref-CR4">von Hornbostel, E. M. &amp; Sachs, C. Systematik der Musikinstrumente. Ein Versuch. <i>Z. Ethnol.</i> <b>553</b>, 590 (1914).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Systematik%20der%20Musikinstrumente.%20Ein%20Versuch&amp;journal=Z.%20Ethnol.&amp;volume=553&amp;publication_year=1914&amp;author=Hornbostel%2CEM&amp;author=Sachs%2CC">
                    Google Scholar</a> 
                </p></li><li data-counter="5."><p id="ref-CR5">MIMO Consortium. Revision of the Hornbostel-Sachs Classification of Musical Instruments by the MIMO Consortium. <a href="http://www.mimo-international.com/documents/hornbostel%20sachs.pdf" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://www.mimo-international.com/documents/hornbostel%20sachs.pdf">http://www.mimo-international.com/documents/hornbostel%20sachs.pdf</a> (2008).</p></li><li data-counter="6."><p id="ref-CR6">Beristain, S., Menchaca, R. &amp; Velazquez, R. Ancient noise generators. <i>J. Acoust. Soc. Am.</i> <b>112</b>, 2368–2368 (2002).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1121/1.4779600" data-track-item_id="10.1121/1.4779600" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1121%2F1.4779600" aria-label="Article reference 6" data-doi="10.1121/1.4779600">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Ancient%20noise%20generators&amp;journal=J.%20Acoust.%20Soc.%20Am.&amp;doi=10.1121%2F1.4779600&amp;volume=112&amp;pages=2368-2368&amp;publication_year=2002&amp;author=Beristain%2CS&amp;author=Menchaca%2CR&amp;author=Velazquez%2CR">
                    Google Scholar</a> 
                </p></li><li data-counter="7."><p id="ref-CR7">Both, A. A. On the context of imitative and associative processes in Prehispanic music. <i>Stud. zur. Musikarch.äologie</i> <b>5</b>, 319–332 (2006).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20context%20of%20imitative%20and%20associative%20processes%20in%20Prehispanic%20music&amp;journal=Stud.%20zur.%20Musikarch.%C3%A4ologie&amp;volume=5&amp;pages=319-332&amp;publication_year=2006&amp;author=Both%2CAA">
                    Google Scholar</a> 
                </p></li><li data-counter="8."><p id="ref-CR8">Kollmann, J. Flöten und Pfeifen aus Alt-Mexiko. in <i>Adolf Bastian als Fest-Gruss zu seinem 70</i>. <i>Geburtstage</i> 559–574 (Berlin, 1896).</p></li><li data-counter="9."><p id="ref-CR9">Trevor, C., Arnal, L. H. &amp; Frühholz, S. Terrifying film music mimics alarming acoustic feature of human screams. <i>J. Acoust. Soc. Am.</i> <b>147</b>, EL540–EL545 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1121/10.0001459" data-track-item_id="10.1121/10.0001459" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1121%2F10.0001459" aria-label="Article reference 9" data-doi="10.1121/10.0001459">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32611175" aria-label="PubMed reference 9">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Terrifying%20film%20music%20mimics%20alarming%20acoustic%20feature%20of%20human%20screams&amp;journal=J.%20Acoust.%20Soc.%20Am.&amp;doi=10.1121%2F10.0001459&amp;volume=147&amp;pages=EL540-EL545&amp;publication_year=2020&amp;author=Trevor%2CC&amp;author=Arnal%2CLH&amp;author=Fr%C3%BChholz%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="10."><p id="ref-CR10">Velazquez, R. &amp; Rivas Castro, F. Death whistle. <i>J. Acoust. Soc. Am.</i> <b>128</b>, 2389–2389 (2010).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1121/1.3508553" data-track-item_id="10.1121/1.3508553" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1121%2F1.3508553" aria-label="Article reference 10" data-doi="10.1121/1.3508553">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Death%20whistle&amp;journal=J.%20Acoust.%20Soc.%20Am.&amp;doi=10.1121%2F1.3508553&amp;volume=128&amp;pages=2389-2389&amp;publication_year=2010&amp;author=Velazquez%2CR&amp;author=Rivas%20Castro%2CF">
                    Google Scholar</a> 
                </p></li><li data-counter="11."><p id="ref-CR11">Both, A. A. Aztec music culture. <i>World Music</i> <b>52</b>, 14–28 (2010).</p><p><a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Aztec%20music%20culture&amp;journal=World%20Music&amp;volume=52&amp;pages=14-28&amp;publication_year=2010&amp;author=Both%2CAA">
                    Google Scholar</a> 
                </p></li><li data-counter="12."><p id="ref-CR12">Frühholz, S., Dietziker, J., Staib, M. &amp; Trost, W. Neurocognitive processing efficiency for discriminating human non-alarm rather than alarm scream calls. <i>PLoS Biol.</i> <b>19</b>, e3000751 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pbio.3000751" data-track-item_id="10.1371/journal.pbio.3000751" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pbio.3000751" aria-label="Article reference 12" data-doi="10.1371/journal.pbio.3000751">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33848299" aria-label="PubMed reference 12">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8043411" aria-label="PubMed Central reference 12">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Neurocognitive%20processing%20efficiency%20for%20discriminating%20human%20non-alarm%20rather%20than%20alarm%20scream%20calls&amp;journal=PLoS%20Biol.&amp;doi=10.1371%2Fjournal.pbio.3000751&amp;volume=19&amp;publication_year=2021&amp;author=Fr%C3%BChholz%2CS&amp;author=Dietziker%2CJ&amp;author=Staib%2CM&amp;author=Trost%2CW">
                    Google Scholar</a> 
                </p></li><li data-counter="13."><p id="ref-CR13">Frühholz, S., Trost, W. &amp; Grandjean, D. Whispering–The hidden side of auditory communication. <i>Neuroimage</i> <b>142</b>, 602–612 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2016.08.023" data-track-item_id="10.1016/j.neuroimage.2016.08.023" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2016.08.023" aria-label="Article reference 13" data-doi="10.1016/j.neuroimage.2016.08.023">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27530550" aria-label="PubMed reference 13">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Whispering%E2%80%93The%20hidden%20side%20of%20auditory%20communication&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2016.08.023&amp;volume=142&amp;pages=602-612&amp;publication_year=2016&amp;author=Fr%C3%BChholz%2CS&amp;author=Trost%2CW&amp;author=Grandjean%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="14."><p id="ref-CR14">Frühholz, S., Klaas, H. S., Patel, S. &amp; Grandjean, D. Talking in fury: the cortico-subcortical network underlying angry vocalizations. <i>Cereb. Cortex</i> <b>25</b>, 2752–2762 (2015).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1093/cercor/bhu074" data-track-item_id="10.1093/cercor/bhu074" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhu074" aria-label="Article reference 14" data-doi="10.1093/cercor/bhu074">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24735671" aria-label="PubMed reference 14">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Talking%20in%20fury%3A%20the%20cortico-subcortical%20network%20underlying%20angry%20vocalizations&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhu074&amp;volume=25&amp;pages=2752-2762&amp;publication_year=2015&amp;author=Fr%C3%BChholz%2CS&amp;author=Klaas%2CHS&amp;author=Patel%2CS&amp;author=Grandjean%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="15."><p id="ref-CR15">Piczak, K. J. ESC: Dataset for environmental sound classification. in <i>MM 2015</i>—<i>Proceedings of the 2015 ACM Multimedia Conference</i> 1015–1018 (Association for Computing Machinery, Inc, 2015). <a href="https://doi.org/10.1145/2733373.2806390" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1145/2733373.2806390">https://doi.org/10.1145/2733373.2806390</a>.</p></li><li data-counter="16."><p id="ref-CR16">Belin, P., Fillion-Bilodeau, S. &amp; Gosselin, F. The Montreal affective voices: a validated set of nonverbal affect bursts for research on auditory affective processing. <i>Behav. Res Methods</i> <b>40</b>, 531–539 (2008).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3758/BRM.40.2.531" data-track-item_id="10.3758/BRM.40.2.531" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3758%2FBRM.40.2.531" aria-label="Article reference 16" data-doi="10.3758/BRM.40.2.531">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18522064" aria-label="PubMed reference 16">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Montreal%20affective%20voices%3A%20a%20validated%20set%20of%20nonverbal%20affect%20bursts%20for%20research%20on%20auditory%20affective%20processing&amp;journal=Behav.%20Res%20Methods&amp;doi=10.3758%2FBRM.40.2.531&amp;volume=40&amp;pages=531-539&amp;publication_year=2008&amp;author=Belin%2CP&amp;author=Fillion-Bilodeau%2CS&amp;author=Gosselin%2CF">
                    Google Scholar</a> 
                </p></li><li data-counter="17."><p id="ref-CR17">Philharmonia Orchester. Philharmonia Database.</p></li><li data-counter="18."><p id="ref-CR18">Eyben, F., Weninger, F., Gross, F. &amp; Schuller, B. Recent developments in openSMILE, the Munich open-source multimedia feature extractor. in <i>Proceedings of the 21st ACM International Conference on Multimedia</i>—<i>MM ’13</i> 835–838 (ACM Press, New York, New York, USA, 2013). <a href="https://doi.org/10.1145/2502081.2502224" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1145/2502081.2502224">https://doi.org/10.1145/2502081.2502224</a>.</p></li><li data-counter="19."><p id="ref-CR19">Eyben, F. et al. The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. <i>IEEE Trans. Affect Comput</i> <b>7</b>, 190–202 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1109/TAFFC.2015.2457417" data-track-item_id="10.1109/TAFFC.2015.2457417" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1109%2FTAFFC.2015.2457417" aria-label="Article reference 19" data-doi="10.1109/TAFFC.2015.2457417">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Geneva%20minimalistic%20acoustic%20parameter%20set%20%28GeMAPS%29%20for%20voice%20research%20and%20affective%20computing&amp;journal=IEEE%20Trans.%20Affect%20Comput&amp;doi=10.1109%2FTAFFC.2015.2457417&amp;volume=7&amp;pages=190-202&amp;publication_year=2016&amp;author=Eyben%2CF">
                    Google Scholar</a> 
                </p></li><li data-counter="20."><p id="ref-CR20">Colace, F. &amp; Casaburi, L. An approach for sentiment classification of music. in <i>ICEIS 2016</i>—<i>Proceedings of the 18th International Conference on Enterprise Information Systems</i> vol. 2 421–426 (SciTePress, 2016).</p></li><li data-counter="21."><p id="ref-CR21">Elliott, T. M. &amp; Theunissen, F. E. The modulation transfer function for speech intelligibility. <i>PLoS Comput. Biol.</i> <b>5</b>, e1000302 (2009).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1371/journal.pcbi.1000302" data-track-item_id="10.1371/journal.pcbi.1000302" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1371%2Fjournal.pcbi.1000302" aria-label="Article reference 21" data-doi="10.1371/journal.pcbi.1000302">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19266016" aria-label="PubMed reference 21">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2639724" aria-label="PubMed Central reference 21">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20modulation%20transfer%20function%20for%20speech%20intelligibility&amp;journal=PLoS%20Comput.%20Biol.&amp;doi=10.1371%2Fjournal.pcbi.1000302&amp;volume=5&amp;publication_year=2009&amp;author=Elliott%2CTM&amp;author=Theunissen%2CFE">
                    Google Scholar</a> 
                </p></li><li data-counter="22."><p id="ref-CR22">Nili, H. et al. A toolbox for representational similarity analysis. <i>PLoS Comput. Biol.</i> <b>10</b>, (2014).</p></li><li data-counter="23."><p id="ref-CR23">Bones, O., Cox, T. J. &amp; Davies, W. J. Sound categories: category formation and evidence-based taxonomies. <i>Front. Psychol.</i> <b>9</b>, 1277 (2018).</p></li><li data-counter="24."><p id="ref-CR24">Plutchik, R. A psychoevolutionary theory of emotions. <i>Soc. Sci. Inf.</i> <b>21</b>, 529–553 (1982).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1177/053901882021004003" data-track-item_id="10.1177/053901882021004003" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F053901882021004003" aria-label="Article reference 24" data-doi="10.1177/053901882021004003">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20psychoevolutionary%20theory%20of%20emotions&amp;journal=Soc.%20Sci.%20Inf.&amp;doi=10.1177%2F053901882021004003&amp;volume=21&amp;pages=529-553&amp;publication_year=1982&amp;author=Plutchik%2CR">
                    Google Scholar</a> 
                </p></li><li data-counter="25."><p id="ref-CR25">Ashburner, J. &amp; Friston, K. J. Diffeomorphic registration using geodesic shooting and Gauss-Newton optimisation. <i>Neuroimage</i> <b>55</b>, 954–967 (2011).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2010.12.049" data-track-item_id="10.1016/j.neuroimage.2010.12.049" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2010.12.049" aria-label="Article reference 25" data-doi="10.1016/j.neuroimage.2010.12.049">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21216294" aria-label="PubMed reference 25">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Diffeomorphic%20registration%20using%20geodesic%20shooting%20and%20Gauss-Newton%20optimisation&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2010.12.049&amp;volume=55&amp;pages=954-967&amp;publication_year=2011&amp;author=Ashburner%2CJ&amp;author=Friston%2CKJ">
                    Google Scholar</a> 
                </p></li><li data-counter="26."><p id="ref-CR26">Whitfield-Gabrieli, S. &amp; Nieto-Castanon, A. Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks. <i>Brain Connect</i> <b>2</b>, 125–141 (2012).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1089/brain.2012.0073" data-track-item_id="10.1089/brain.2012.0073" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1089%2Fbrain.2012.0073" aria-label="Article reference 26" data-doi="10.1089/brain.2012.0073">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22642651" aria-label="PubMed reference 26">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Conn%3A%20a%20functional%20connectivity%20toolbox%20for%20correlated%20and%20anticorrelated%20brain%20networks&amp;journal=Brain%20Connect&amp;doi=10.1089%2Fbrain.2012.0073&amp;volume=2&amp;pages=125-141&amp;publication_year=2012&amp;author=Whitfield-Gabrieli%2CS&amp;author=Nieto-Castanon%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="27."><p id="ref-CR27">Hebart, M. N., Görgen, K. &amp; Haynes, J.-D. The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data. <i>Front. Neuroinform.</i> <b>8</b>, 88 (2015).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.3389/fninf.2014.00088" data-track-item_id="10.3389/fninf.2014.00088" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.3389%2Ffninf.2014.00088" aria-label="Article reference 27" data-doi="10.3389/fninf.2014.00088">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25610393" aria-label="PubMed reference 27">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285115" aria-label="PubMed Central reference 27">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Decoding%20Toolbox%20%28TDT%29%3A%20a%20versatile%20software%20package%20for%20multivariate%20analyses%20of%20functional%20imaging%20data&amp;journal=Front.%20Neuroinform.&amp;doi=10.3389%2Ffninf.2014.00088&amp;volume=8&amp;publication_year=2015&amp;author=Hebart%2CMN&amp;author=G%C3%B6rgen%2CK&amp;author=Haynes%2CJ-D">
                    Google Scholar</a> 
                </p></li><li data-counter="28."><p id="ref-CR28">Turner, B. O., Mumford, J. A., Poldrack, R. A. &amp; Ashby, F. G. Spatiotemporal activity estimation for multivoxel pattern analysis with rapid event-related designs. <i>Neuroimage</i> <b>62</b>, 1429–1438 (2012).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2012.05.057" data-track-item_id="10.1016/j.neuroimage.2012.05.057" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2012.05.057" aria-label="Article reference 28" data-doi="10.1016/j.neuroimage.2012.05.057">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22659443" aria-label="PubMed reference 28">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatiotemporal%20activity%20estimation%20for%20multivoxel%20pattern%20analysis%20with%20rapid%20event-related%20designs&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2012.05.057&amp;volume=62&amp;pages=1429-1438&amp;publication_year=2012&amp;author=Turner%2CBO&amp;author=Mumford%2CJA&amp;author=Poldrack%2CRA&amp;author=Ashby%2CFG">
                    Google Scholar</a> 
                </p></li><li data-counter="29."><p id="ref-CR29">Nichols, T., Brett, M., Andersson, J., Wager, T. &amp; Poline, J. B. Valid conjunction inference with the minimum statistic. <i>Neuroimage</i> <b>25</b>, 653–660 (2005).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuroimage.2004.12.005" data-track-item_id="10.1016/j.neuroimage.2004.12.005" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuroimage.2004.12.005" aria-label="Article reference 29" data-doi="10.1016/j.neuroimage.2004.12.005">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=15808966" aria-label="PubMed reference 29">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Valid%20conjunction%20inference%20with%20the%20minimum%20statistic&amp;journal=Neuroimage&amp;doi=10.1016%2Fj.neuroimage.2004.12.005&amp;volume=25&amp;pages=653-660&amp;publication_year=2005&amp;author=Nichols%2CT&amp;author=Brett%2CM&amp;author=Andersson%2CJ&amp;author=Wager%2CT&amp;author=Poline%2CJB">
                    Google Scholar</a> 
                </p></li><li data-counter="30."><p id="ref-CR30">Woolley, S. M. N., Fremouw, T. E., Hsu, A. &amp; Theunissen, F. E. Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds. <i>Nat. Neurosci.</i> <b>8</b>, 1371–1379 (2005).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nn1536" data-track-item_id="10.1038/nn1536" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn1536" aria-label="Article reference 30" data-doi="10.1038/nn1536">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16136039" aria-label="PubMed reference 30">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=Tuning%20for%20spectro-temporal%20modulations%20as%20a%20mechanism%20for%20auditory%20discrimination%20of%20natural%20sounds&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn1536&amp;volume=8&amp;pages=1371-1379&amp;publication_year=2005&amp;author=Woolley%2CSMN&amp;author=Fremouw%2CTE&amp;author=Hsu%2CA&amp;author=Theunissen%2CFE">
                    Google Scholar</a> 
                </p></li><li data-counter="31."><p id="ref-CR31">Fukushima, M., Doyle, A. M., Mullarkey, M. P., Mishkin, M. &amp; Averbeck, B. B. Distributed acoustic cues for caller identity in macaque vocalization. <i>R. Soc. Open Sci.</i> <b>2</b>, 150432 (2015).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1098/rsos.150432" data-track-item_id="10.1098/rsos.150432" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1098%2Frsos.150432" aria-label="Article reference 31" data-doi="10.1098/rsos.150432">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=27019727" aria-label="PubMed reference 31">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4806230" aria-label="PubMed Central reference 31">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Distributed%20acoustic%20cues%20for%20caller%20identity%20in%20macaque%20vocalization&amp;journal=R.%20Soc.%20Open%20Sci.&amp;doi=10.1098%2Frsos.150432&amp;volume=2&amp;publication_year=2015&amp;author=Fukushima%2CM&amp;author=Doyle%2CAM&amp;author=Mullarkey%2CMP&amp;author=Mishkin%2CM&amp;author=Averbeck%2CBB">
                    Google Scholar</a> 
                </p></li><li data-counter="32."><p id="ref-CR32">Hechavarría, J. C., Jerome Beetz, M., García-Rosales, F. &amp; Kössl, M. Bats distress vocalizations carry fast amplitude modulations that could represent an acoustic correlate of roughness. <i>Sci. Rep.</i> <b>10</b>, 7332 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41598-020-64323-7" data-track-item_id="10.1038/s41598-020-64323-7" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41598-020-64323-7" aria-label="Article reference 32" data-doi="10.1038/s41598-020-64323-7">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32355293" aria-label="PubMed reference 32">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7192923" aria-label="PubMed Central reference 32">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Bats%20distress%20vocalizations%20carry%20fast%20amplitude%20modulations%20that%20could%20represent%20an%20acoustic%20correlate%20of%20roughness&amp;journal=Sci.%20Rep.&amp;doi=10.1038%2Fs41598-020-64323-7&amp;volume=10&amp;publication_year=2020&amp;author=Hechavarr%C3%ADa%2CJC&amp;author=Jerome%20Beetz%2CM&amp;author=Garc%C3%ADa-Rosales%2CF&amp;author=K%C3%B6ssl%2CM">
                    Google Scholar</a> 
                </p></li><li data-counter="33."><p id="ref-CR33">Arnal, L. H., Kleinschmidt, A., Spinelli, L., Giraud, A.-L. &amp; Mégevand, P. The rough sound of salience enhances aversion through neural synchronisation. <i>Nat. Commun.</i> <b>10</b>, 3671 (2019).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s41467-019-11626-7" data-track-item_id="10.1038/s41467-019-11626-7" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41467-019-11626-7" aria-label="Article reference 33" data-doi="10.1038/s41467-019-11626-7">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31413319" aria-label="PubMed reference 33">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6694125" aria-label="PubMed Central reference 33">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20rough%20sound%20of%20salience%20enhances%20aversion%20through%20neural%20synchronisation&amp;journal=Nat.%20Commun.&amp;doi=10.1038%2Fs41467-019-11626-7&amp;volume=10&amp;publication_year=2019&amp;author=Arnal%2CLH&amp;author=Kleinschmidt%2CA&amp;author=Spinelli%2CL&amp;author=Giraud%2CA-L&amp;author=M%C3%A9gevand%2CP">
                    Google Scholar</a> 
                </p></li><li data-counter="34."><p id="ref-CR34">Guastavino, C. et al. Sound categories: category formation and evidence-based taxonomies. (2018) <a href="https://doi.org/10.3389/fpsyg.2018.01277" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.3389/fpsyg.2018.01277">https://doi.org/10.3389/fpsyg.2018.01277</a>.</p></li><li data-counter="35."><p id="ref-CR35">Bodin, C. et al. Functionally homologous representation of vocalizations in the auditory cortex of humans and macaques. <i>Curr. Biol.</i> <b>31</b>, 4839–4844.e4 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cub.2021.08.043" data-track-item_id="10.1016/j.cub.2021.08.043" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cub.2021.08.043" aria-label="Article reference 35" data-doi="10.1016/j.cub.2021.08.043">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34506729" aria-label="PubMed reference 35">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8585503" aria-label="PubMed Central reference 35">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=Functionally%20homologous%20representation%20of%20vocalizations%20in%20the%20auditory%20cortex%20of%20humans%20and%20macaques&amp;journal=Curr.%20Biol.&amp;doi=10.1016%2Fj.cub.2021.08.043&amp;volume=31&amp;pages=4839-4844.e4&amp;publication_year=2021&amp;author=Bodin%2CC">
                    Google Scholar</a> 
                </p></li><li data-counter="36."><p id="ref-CR36">Frühholz, S. &amp; Schweinberger, S. R. Nonverbal auditory communication—evidence for integrated neural systems for voice signal production and perception. <i>Prog. Neurobiol.</i> (2020) <a href="https://doi.org/10.1016/j.pneurobio.2020.101948" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1016/j.pneurobio.2020.101948">https://doi.org/10.1016/j.pneurobio.2020.101948</a>.</p></li><li data-counter="37."><p id="ref-CR37">Frühholz, S., Trost, W. &amp; Kotz, S. A. The sound of emotions-towards a unifying neural network perspective of affective sound processing. <i>Neurosci. Biobehav. Rev.</i> <b>68</b>, 1–15 (2016).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neubiorev.2016.05.002" data-track-item_id="10.1016/j.neubiorev.2016.05.002" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neubiorev.2016.05.002" aria-label="Article reference 37" data-doi="10.1016/j.neubiorev.2016.05.002">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20sound%20of%20emotions-towards%20a%20unifying%20neural%20network%20perspective%20of%20affective%20sound%20processing&amp;journal=Neurosci.%20Biobehav.%20Rev.&amp;doi=10.1016%2Fj.neubiorev.2016.05.002&amp;volume=68&amp;pages=1-15&amp;publication_year=2016&amp;author=Fr%C3%BChholz%2CS&amp;author=Trost%2CW&amp;author=Kotz%2CSA">
                    Google Scholar</a> 
                </p></li><li data-counter="38."><p id="ref-CR38">Frühholz, S. &amp; Grandjean, D. Processing of emotional vocalizations in bilateral inferior frontal cortex. <i>Neurosci. Biobehav. Rev.</i> <b>37</b>, 2847–2855 (2013).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neubiorev.2013.10.007" data-track-item_id="10.1016/j.neubiorev.2013.10.007" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neubiorev.2013.10.007" aria-label="Article reference 38" data-doi="10.1016/j.neubiorev.2013.10.007">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24161466" aria-label="PubMed reference 38">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Processing%20of%20emotional%20vocalizations%20in%20bilateral%20inferior%20frontal%20cortex&amp;journal=Neurosci.%20Biobehav.%20Rev.&amp;doi=10.1016%2Fj.neubiorev.2013.10.007&amp;volume=37&amp;pages=2847-2855&amp;publication_year=2013&amp;author=Fr%C3%BChholz%2CS&amp;author=Grandjean%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="39."><p id="ref-CR39">Bizley, J. K. &amp; Cohen, Y. E. The what, where and how of auditory-object perception. <i>Nat. Rev. Neurosci.</i> <b>14</b>, 693–707 (2013).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nrn3565" data-track-item_id="10.1038/nrn3565" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnrn3565" aria-label="Article reference 39" data-doi="10.1038/nrn3565">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24052177" aria-label="PubMed reference 39">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4082027" aria-label="PubMed Central reference 39">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20what%2C%20where%20and%20how%20of%20auditory-object%20perception&amp;journal=Nat.%20Rev.%20Neurosci.&amp;doi=10.1038%2Fnrn3565&amp;volume=14&amp;pages=693-707&amp;publication_year=2013&amp;author=Bizley%2CJK&amp;author=Cohen%2CYE">
                    Google Scholar</a> 
                </p></li><li data-counter="40."><p id="ref-CR40">Rauschecker, J. P. &amp; Scott, S. K. Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing. <i>Nat. Neurosci.</i> <b>12</b>, 718–724 (2009).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/nn.2331" data-track-item_id="10.1038/nn.2331" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fnn.2331" aria-label="Article reference 40" data-doi="10.1038/nn.2331">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=19471271" aria-label="PubMed reference 40">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2846110" aria-label="PubMed Central reference 40">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Maps%20and%20streams%20in%20the%20auditory%20cortex%3A%20nonhuman%20primates%20illuminate%20human%20speech%20processing&amp;journal=Nat.%20Neurosci.&amp;doi=10.1038%2Fnn.2331&amp;volume=12&amp;pages=718-724&amp;publication_year=2009&amp;author=Rauschecker%2CJP&amp;author=Scott%2CSK">
                    Google Scholar</a> 
                </p></li><li data-counter="41."><p id="ref-CR41">Yao, J. D., Gimoto, J., Constantinople, C. M. &amp; Sanes, D. H. Parietal cortex is required for the integration of acoustic evidence. <i>Curr. Biol.</i> <b>30</b>, 3293–3303.e4 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.cub.2020.06.017" data-track-item_id="10.1016/j.cub.2020.06.017" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.cub.2020.06.017" aria-label="Article reference 41" data-doi="10.1016/j.cub.2020.06.017">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32619478" aria-label="PubMed reference 41">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7483995" aria-label="PubMed Central reference 41">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=Parietal%20cortex%20is%20required%20for%20the%20integration%20of%20acoustic%20evidence&amp;journal=Curr.%20Biol.&amp;doi=10.1016%2Fj.cub.2020.06.017&amp;volume=30&amp;pages=3293-3303.e4&amp;publication_year=2020&amp;author=Yao%2CJD&amp;author=Gimoto%2CJ&amp;author=Constantinople%2CCM&amp;author=Sanes%2CDH">
                    Google Scholar</a> 
                </p></li><li data-counter="42."><p id="ref-CR42">Etkin, A., Egner, T. &amp; Kalisch, R. Emotional processing in anterior cingulate and medial prefrontal cortex. <i>Trends Cogn. Sci.</i> <b>15</b>, 85–93 (2011).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.tics.2010.11.004" data-track-item_id="10.1016/j.tics.2010.11.004" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.tics.2010.11.004" aria-label="Article reference 42" data-doi="10.1016/j.tics.2010.11.004">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=21167765" aria-label="PubMed reference 42">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20processing%20in%20anterior%20cingulate%20and%20medial%20prefrontal%20cortex&amp;journal=Trends%20Cogn.%20Sci.&amp;doi=10.1016%2Fj.tics.2010.11.004&amp;volume=15&amp;pages=85-93&amp;publication_year=2011&amp;author=Etkin%2CA&amp;author=Egner%2CT&amp;author=Kalisch%2CR">
                    Google Scholar</a> 
                </p></li><li data-counter="43."><p id="ref-CR43">Giordano, B. L., McAdams, S., Zatorre, R. J., Kriegeskorte, N. &amp; Belin, P. Abstract encoding of auditory objects in cortical activity patterns. <i>Cereb. Cortex</i> <b>23</b>, 2025–2037 (2013).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1093/cercor/bhs162" data-track-item_id="10.1093/cercor/bhs162" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1093%2Fcercor%2Fbhs162" aria-label="Article reference 43" data-doi="10.1093/cercor/bhs162">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22802575" aria-label="PubMed reference 43">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Abstract%20encoding%20of%20auditory%20objects%20in%20cortical%20activity%20patterns&amp;journal=Cereb.%20Cortex&amp;doi=10.1093%2Fcercor%2Fbhs162&amp;volume=23&amp;pages=2025-2037&amp;publication_year=2013&amp;author=Giordano%2CBL&amp;author=McAdams%2CS&amp;author=Zatorre%2CRJ&amp;author=Kriegeskorte%2CN&amp;author=Belin%2CP">
                    Google Scholar</a> 
                </p></li><li data-counter="44."><p id="ref-CR44">Concina, G., Renna, A., Grosso, A. &amp; Sacchetti, B. The auditory cortex and the emotional valence of sounds. <i>Neurosci. Biobehav. Rev.</i> <b>98</b>, 256–264 (2019).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neubiorev.2019.01.018" data-track-item_id="10.1016/j.neubiorev.2019.01.018" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neubiorev.2019.01.018" aria-label="Article reference 44" data-doi="10.1016/j.neubiorev.2019.01.018">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=30664888" aria-label="PubMed reference 44">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20auditory%20cortex%20and%20the%20emotional%20valence%20of%20sounds&amp;journal=Neurosci.%20Biobehav.%20Rev.&amp;doi=10.1016%2Fj.neubiorev.2019.01.018&amp;volume=98&amp;pages=256-264&amp;publication_year=2019&amp;author=Concina%2CG&amp;author=Renna%2CA&amp;author=Grosso%2CA&amp;author=Sacchetti%2CB">
                    Google Scholar</a> 
                </p></li><li data-counter="45."><p id="ref-CR45">Staib, M., Abivardi, A. &amp; Bach, D. R. Primary auditory cortex representation of fear‐conditioned musical sounds. <i>Hum. Brain Mapp.</i> <b>41</b>, 882–891 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1002/hbm.24846" data-track-item_id="10.1002/hbm.24846" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1002%2Fhbm.24846" aria-label="Article reference 45" data-doi="10.1002/hbm.24846">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31663229" aria-label="PubMed reference 45">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Primary%20auditory%20cortex%20representation%20of%20fear%E2%80%90conditioned%20musical%20sounds&amp;journal=Hum.%20Brain%20Mapp.&amp;doi=10.1002%2Fhbm.24846&amp;volume=41&amp;pages=882-891&amp;publication_year=2020&amp;author=Staib%2CM&amp;author=Abivardi%2CA&amp;author=Bach%2CDR">
                    Google Scholar</a> 
                </p></li><li data-counter="46."><p id="ref-CR46">Archakov, D. et al. Auditory representation of learned sound sequences in motor regions of the macaque brain. <i>Proc. Natl Acad. Sci.</i> <b>117</b>, 15242–15252 (2020).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1073/pnas.1915610117" data-track-item_id="10.1073/pnas.1915610117" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1073%2Fpnas.1915610117" aria-label="Article reference 46" data-doi="10.1073/pnas.1915610117">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32541016" aria-label="PubMed reference 46">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7334521" aria-label="PubMed Central reference 46">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 46" href="http://scholar.google.com/scholar_lookup?&amp;title=Auditory%20representation%20of%20learned%20sound%20sequences%20in%20motor%20regions%20of%20the%20macaque%20brain&amp;journal=Proc.%20Natl%20Acad.%20Sci.&amp;doi=10.1073%2Fpnas.1915610117&amp;volume=117&amp;pages=15242-15252&amp;publication_year=2020&amp;author=Archakov%2CD">
                    Google Scholar</a> 
                </p></li><li data-counter="47."><p id="ref-CR47">Morán, I., Perez-Orive, J., Melchor, J., Figueroa, T. &amp; Lemus, L. Auditory decisions in the supplementary motor area. <i>Prog. Neurobiol.</i> <b>202</b>, 102053 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.pneurobio.2021.102053" data-track-item_id="10.1016/j.pneurobio.2021.102053" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.pneurobio.2021.102053" aria-label="Article reference 47" data-doi="10.1016/j.pneurobio.2021.102053">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33957182" aria-label="PubMed reference 47">PubMed</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Auditory%20decisions%20in%20the%20supplementary%20motor%20area&amp;journal=Prog.%20Neurobiol.&amp;doi=10.1016%2Fj.pneurobio.2021.102053&amp;volume=202&amp;publication_year=2021&amp;author=Mor%C3%A1n%2CI&amp;author=Perez-Orive%2CJ&amp;author=Melchor%2CJ&amp;author=Figueroa%2CT&amp;author=Lemus%2CL">
                    Google Scholar</a> 
                </p></li><li data-counter="48."><p id="ref-CR48">LeDoux, J. Rethinking the emotional brain. <i>Neuron</i> <b>73</b>, 653–676 (2012).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.neuron.2012.02.004" data-track-item_id="10.1016/j.neuron.2012.02.004" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.neuron.2012.02.004" aria-label="Article reference 48" data-doi="10.1016/j.neuron.2012.02.004">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22365542" aria-label="PubMed reference 48">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3625946" aria-label="PubMed Central reference 48">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=Rethinking%20the%20emotional%20brain&amp;journal=Neuron&amp;doi=10.1016%2Fj.neuron.2012.02.004&amp;volume=73&amp;pages=653-676&amp;publication_year=2012&amp;author=LeDoux%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="49."><p id="ref-CR49">Kumar, S., von Kriegstein, K., Friston, K. &amp; Griffiths, T. D. Features versus feelings: dissociable representations of the acoustic features and valence of aversive sounds. <i>J. Neurosci.</i> <b>32</b>, 14184–14192 (2012).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1523/JNEUROSCI.1759-12.2012" data-track-item_id="10.1523/JNEUROSCI.1759-12.2012" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1523%2FJNEUROSCI.1759-12.2012" aria-label="Article reference 49" data-doi="10.1523/JNEUROSCI.1759-12.2012">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23055488" aria-label="PubMed reference 49">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3505833" aria-label="PubMed Central reference 49">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Features%20versus%20feelings%3A%20dissociable%20representations%20of%20the%20acoustic%20features%20and%20valence%20of%20aversive%20sounds&amp;journal=J.%20Neurosci.&amp;doi=10.1523%2FJNEUROSCI.1759-12.2012&amp;volume=32&amp;pages=14184-14192&amp;publication_year=2012&amp;author=Kumar%2CS&amp;author=Kriegstein%2CK&amp;author=Friston%2CK&amp;author=Griffiths%2CTD">
                    Google Scholar</a> 
                </p></li><li data-counter="50."><p id="ref-CR50">Trevor, C. &amp; Frühholz, S. Music as an evolved tool for socio-affective fiction. <i>Emot. Rev.</i> <b>16</b>, 180–194 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1177/17540739241259562" data-track-item_id="10.1177/17540739241259562" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F17540739241259562" aria-label="Article reference 50" data-doi="10.1177/17540739241259562">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=39101012" aria-label="PubMed reference 50">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC11294008" aria-label="PubMed Central reference 50">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Music%20as%20an%20evolved%20tool%20for%20socio-affective%20fiction&amp;journal=Emot.%20Rev.&amp;doi=10.1177%2F17540739241259562&amp;volume=16&amp;pages=180-194&amp;publication_year=2024&amp;author=Trevor%2CC&amp;author=Fr%C3%BChholz%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="51."><p id="ref-CR51">Mehr, S. A., Krasnow, M. M., Bryant, G. A. &amp; Hagen, E. H. Origins of music in credible signaling. <i>Behav. Brain Sci.</i> <b>44</b>, e60 (2021).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1017/S0140525X20000345" data-track-item_id="10.1017/S0140525X20000345" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1017%2FS0140525X20000345" aria-label="Article reference 51" data-doi="10.1017/S0140525X20000345">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Origins%20of%20music%20in%20credible%20signaling&amp;journal=Behav.%20Brain%20Sci.&amp;doi=10.1017%2FS0140525X20000345&amp;volume=44&amp;publication_year=2021&amp;author=Mehr%2CSA&amp;author=Krasnow%2CMM&amp;author=Bryant%2CGA&amp;author=Hagen%2CEH">
                    Google Scholar</a> 
                </p></li><li data-counter="52."><p id="ref-CR52">Barber, S. B. &amp; Sánchez, M. O. A divine wind: the arts of death and music in terminal formative Oaxaca. <i>Anc. Mesoam.</i> <b>23</b>, 9–24 (2012).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1017/S0956536112000016" data-track-item_id="10.1017/S0956536112000016" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1017%2FS0956536112000016" aria-label="Article reference 52" data-doi="10.1017/S0956536112000016">Article</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20divine%20wind%3A%20the%20arts%20of%20death%20and%20music%20in%20terminal%20formative%20Oaxaca&amp;journal=Anc.%20Mesoam.&amp;doi=10.1017%2FS0956536112000016&amp;volume=23&amp;pages=9-24&amp;publication_year=2012&amp;author=Barber%2CSB&amp;author=S%C3%A1nchez%2CMO">
                    Google Scholar</a> 
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s44271-024-00157-7?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>We are very thankful to <i>Arnd Adje Both</i> for helpful advice during the study from a music archeological perspective and for guiding and supervising the artisanal reproduction of original skull whistles. We thank <i>Osvaldo Padrón Pérez</i> for manufacturing two of the skull whistle replicas. We thank the <i>Ethnological Museum of Berlin (EBM)</i> for access to original skull whistle artifacts. This study was supported by the Swiss National Science Foundation (SNSF PP00P1_157409/1 and 100014_182135/1). The funders had no role in study design, data collection, and analysis, decision to publish, or preparation of the manuscript.</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Cognitive and Affective Neuroscience Unit, University of Zurich, Zurich, Switzerland</p><p>Sascha Frühholz, Pablo Rodriguez, Mathilde Bonard, Florence Steiner &amp; Marine Bobin</p></li><li id="Aff2"><p>Department of Psychology, University of Oslo, Oslo, Norway</p><p>Sascha Frühholz</p></li></ol><h3 id="contributions">Contributions</h3><p>S.F., P.R. contributed to designing the experiments, data acquisition, data analysis, and writing the manuscript; M. Bobin, M. Bonard, F.S. contributed to data acquisition and data analysis.</p><h3 id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:s.fruehholz@gmail.com">Sascha Frühholz</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar2">Competing interest</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Peer review"><div id="peer-review-section"><h2 id="peer-review">Peer review</h2><div id="peer-review-content">
              
              
                <h3 id="FPar1">Peer review information</h3>
                <p><i>Communications psychology</i> thanks Etienne Thoret and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Primary Handling Editors: Marike Schiffer. A peer review file is available.</p>
              
            </div></div></section><section data-title="Additional information"><div id="additional-information-section"><h2 id="additional-information">Additional information</h2><p><b>Publisher’s note</b> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></section><section data-title="Supplementary information"><div id="Sec33-section"><h2 id="Sec33">Supplementary information</h2></div></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" rel="license">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Psychoacoustic%20and%20Archeoacoustic%20nature%20of%20ancient%20Aztec%20skull%20whistles&amp;author=Sascha%20Fr%C3%BChholz%20et%20al&amp;contentID=10.1038%2Fs44271-024-00157-7&amp;copyright=The%20Author%28s%29&amp;publication=2731-9121&amp;publicationDate=2024-11-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY-NC-ND">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1038/s44271-024-00157-7" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s44271-024-00157-7" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Frühholz, S., Rodriguez, P., Bonard, M. <i>et al.</i> Psychoacoustic and Archeoacoustic nature of ancient Aztec skull whistles.
                    <i>Commun Psychol</i> <b>2</b>, 108 (2024). https://doi.org/10.1038/s44271-024-00157-7</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s44271-024-00157-7?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2024-05-24">24 May 2024</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2024-10-31">31 October 2024</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2024-11-11">11 November 2024</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s44271-024-00157-7</span></p></li></ul></div></div></div></div></section>
            </div></div>
  </body>
</html>
