<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://towardsdatascience.com/word2vec-explained-49c52b4ccb71">Original</a>
    <h1>Word2Vec Explained. Explaining the Intuition of Word2Vec</h1>
    
    <div id="readability-page-1" class="page"><div><p id="5a04"><strong>Table of Contents</strong></p><ul><li id="3478">Introduction</li><li id="db11">What is a Word Embedding?</li><li id="3fb6">Word2Vec Architecture</li><li id="051d">Implementation</li><li id="41e2">Concluding Remarks</li><li id="6c6b">Resources</li></ul><p id="337d">Word2Vec is a recent breakthrough in the world of NLP. <a href="https://en.wikipedia.org/wiki/Tomas_Mikolov" rel="noopener ugc nofollow" target="_blank">Tomas Mikolov</a> a Czech computer scientist and currently a researcher at CIIRC ( <a href="https://en.wikipedia.org/wiki/Czech_Institute_of_Informatics,_Robotics_and_Cybernetics" rel="noopener ugc nofollow" target="_blank">Czech Institute of Informatics, Robotics and Cybernetics</a>) was one of the leading contributors towards the research and implementation of word2vec. Word embeddings are an integral part of solving many problems in NLP. They depict how humans understand language to a machine. You can imagine them as a vectorized representation of text. Word2Vec, a common method of generating word embeddings, has a variety of applications such as text similarity, recommendation systems, sentiment analysis, etc.</p><p id="24a9">Before we get into word2vec, let’s establish an understanding of what word embeddings are. This is important to know because the overall result and output of word2vec will be embeddings associated to each unique word passed through the algorithm.</p><p id="89b0">Word embeddings is a technique where individual words are transformed into a numerical representation of the word (a vector). Where each word is mapped to one vector, this vector is then learned in a way which resembles a neural network. The vectors try to capture various characteristics of that word with regard to the overall text. These characteristics can include the semantic relationship of the word, definitions, context, etc. With these numerical representations, you can do many things like identify similarity or dissimilarity between words.</p><p id="52ee">Clearly, these are integral as inputs to various aspects of machine learning. A machine cannot process text in their raw form, thus converting the text into an embedding will allow users to feed the embedding to classic machine learning models. The simplest embedding would be a one hot encoding of text data where each vector would be mapped to a category.</p><pre><span id="888e">For example: </span><span id="0735">have = [1, 0, 0, 0, 0, 0, ... 0]</span></pre><p id="67a4">However, there are multiple limitations of simple embeddings such as this, as they do not capture characteristics of the word, and they can be quite large depending on the size of the corpus.</p><p id="d329">The effectiveness of Word2Vec comes from its ability to group together vectors of similar words. Given a large enough dataset, Word2Vec can make strong estimates about a words meaning based on their occurrences in the text. These estimates yield word associations with other words in the corpus. For example, words like “King” and “Queen” would be very similar with one another. When conducting algebraic operations on word embeddings you can find a close approximation of word similarities. For example, the 2 dimensional embedding vector of &#34;king&#34; - the 2 dimensional embedding vector of &#34;man&#34; + the 2 dimensional embedding vector of &#34;woman&#34; yielded a vector which is very close to the embedding vector of &#34;queen&#34;. Note, that the values below were chosen arbitrarily.</p><pre><span id="85f3">King    -    Man    +    Woman    =    Queen</span></pre><figure><p><img alt="" src="https://miro.medium.com/max/1128/1*hnu-NqrK3C7wmYWcKXpb-Q.png" width="564" height="442" role="presentation"/></p><figcaption>You can see that the words King and Queen are close to each other in position. (Image provided by the author)</figcaption></figure><p id="8ca7">There are two main architectures which yield the success of word2vec. The skip-gram and CBOW architectures.</p><h2 id="00ac">CBOW (Continuous Bag of Words)</h2><p id="eed9">This architecture is very similar to a feed forward neural network. This model architecture essentially tries to predict a target word from a list of context words. The intuition behind this model is quite simple, given a phrase <code>&#34;Have a great day&#34;</code> , we will chose our target word to be “a” and our context words to be [“have”, “great”, “day”]. What this model will do is take the distributed representations of the context words to try and predict the target word.</p><figure><p><img alt="" src="https://miro.medium.com/max/784/1*_8Ul4ICaCtmZWPrWqH32Ow.png" width="392" height="514" role="presentation"/></p><figcaption>CBOW Architecture. Image taken from <a href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">Efficient Estimation of Word Representation in Vector Space</a></figcaption></figure><h2 id="a16f">Continuous Skip-Gram Model</h2><p id="a0a6">The skip-gram model is a simple neural network with one hidden layer trained in order to predict the probability of a given word being present when an input word is present. Intuitively, you can imagine the skip-gram model being the opposite of the CBOW model. In this architecture, it takes the current word as an input and tries to accurately predict the words before and after this current word. This model essentially tries to learn and predict the context words around the specified input word. Based on experiments assessing the accuracy of this model it was found that the prediction quality improves given a large range of word vectors, however it also increases the computational complexity. The process can be described visually as seen below.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/1*M6UxaLSbNMeoDFWRN_kPeQ.png" width="700" height="405" role="presentation"/></p></div><figcaption>Example of generating training data for skip-gram model. Window size is 3. Image provided by author</figcaption></figure><p id="ee74">As seen above, given some corpus of text, a target word is selected over some rolling window. The training data consists of pair wise combinations of that target word and all other words in the window. This is the resulting training data for the neural network. Once the model is trained, we can essentially yield a probability of a word being a context word for a given target. The following image below represents the architecture of the neural network for the skip-gram model.</p><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/1*UYAkOS9JQwdozQjCzttuow.png" width="700" height="240" role="presentation"/></p></div><figcaption>Skip-Gram Model architecture (Image provided by author)</figcaption></figure><p id="9ce9">A corpus can be represented as a vector of size N, where each element in N corresponds to a word in the corpus. During the training process, we have a pair of target and context words, the input array will have 0 in all element except for the target word. The target word will be equal to 1. The hidden layer will learn the embedding representation of each word, yielding a d-dimensional embedding space. The output layer is a dense layer with a softmax activation function. The output layer will essentially yield a vector of the same size as the input, each element in the vector will consist of a probability. This probability indicates the similarity between the target word and the associated word in the corpus.</p><p id="8468">For a more detailed overview of both these models, I highly recommend reading the original paper which outlined these results <a href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="2bba">I’ll be showing how to use word2vec to generate word embeddings and use those embeddings for finding similar words and visualization of embeddings through PCA.</p><h2 id="9aa6"><strong>Data</strong></h2><p id="9c46">For the purposes of this tutorial we’ll be working with the Shakespeare dataset. You can find the file I used for this tutorial <a href="https://github.com/vatsal220/medium_articles/blob/main/w2v/data/shakespeare.txt" rel="noopener ugc nofollow" target="_blank">here</a>, it includes all the lines Shakespeare has written for his plays.</p><h2 id="a0dc">Requirements</h2><pre><span id="65c2">nltk==3.6.1</span></pre><p id="2c18"><strong>Note: </strong>Since we’re working with NLTK you might need to download the following corpus for the rest of the tutorial to work. This can easily be done by the following commands :</p><pre><span id="44f7">import nltk</span></pre><h2 id="8f75">Import Data</h2><figure><div></div></figure><p id="cf2c"><strong>Note:</strong> Change the <code><strong>PATH</strong></code> variable to the path of the data you’re working with.</p><h2 id="3cbe">Preprocess Data</h2><figure><div></div></figure><p id="6cf7"><strong>Stopword Filtering Note</strong></p><ul><li id="5203">Be aware that the stopwords removed from these lines are of modern vocabulary. The application &amp; data has a high importance to the type of preprocessing tactics necessary for cleaning of words.</li><li id="44fb">In our scenario, words like “you” or “yourself” would be present in the stopwords and eliminated from the lines, however since this is Shakespeare text data, these types of words would not be used. Instead “thou” or “thyself” might be useful to remove. Stay keen to these types of miniature changes because they make a drastic difference in the performance of a good model versus a poor one.</li><li id="f742">For the purposes of this example, I won’t be going into extreme details in identifying stopwords from a different century, but be aware that you should.</li></ul><h2 id="faa0">Embed</h2><figure><div></div></figure><figure><p><img alt="" src="https://miro.medium.com/max/1164/1*RSuePgt7WsS9qIrj88DuIA.png" width="582" height="440" role="presentation"/></p><figcaption>Words in the Shakespeare data which is most similar to thou (Image provided by the Author)</figcaption></figure><h2 id="534b">PCA on Embeddings</h2><figure><div></div></figure><figure><div role="button" tabindex="0"><p><img alt="" src="https://miro.medium.com/max/1400/1*E8jo0k46xpXQOSJ4Ibo0jw.png" width="700" height="517" role="presentation"/></p></div><figcaption>Words similar to each other would be placed closer together to one another. Image provided by author</figcaption></figure><p id="3b91">Tensorflow has made a very beautiful, intuitive and user friendly representation of the word2vec model. I highly recommend you to explore it as it allows you to interact with the results of word2vec. The link is below.</p><p id="bcc1">Word embeddings are an essential part of solving many problems in NLP, it depicts how humans understand language to a machine. Given a large corpus of text, word2vec produces an embedding vector associated to each word in the corpus. These embeddings are structured such that words with similar characteristics are in close proximity of one another. CBOW (continuous bag of words) and the skip-gram model are the two main architectures associated to word2vec. Given an input word, skip-gram will try to predict the words in context to the input whereas the CBOW model will take a variety of words and try to predict the missing one.</p><p id="5008">I’ve also written about node2vec which uses word2vec to generate node embeddings given a network. You can read about it here.</p><ul><li id="f048"><a href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a></li><li id="0197"><a href="https://www.kdnuggets.com/2019/02/word-embeddings-nlp-applications.html" rel="noopener ugc nofollow" target="_blank">https://www.kdnuggets.com/2019/02/word-embeddings-nlp-applications.html</a></li><li id="d5a4"><a href="https://wiki.pathmind.com/word2vec" rel="noopener ugc nofollow" target="_blank">https://wiki.pathmind.com/word2vec</a></li><li id="2403"><a href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">https://projector.tensorflow.org/</a></li></ul></div><div><p id="f2f0">If you enjoyed reading this article, please consider following me for upcoming articles explaining other data science materials and those materials (like word2vec) to solve relevant problems in different areas of data science. Here are some other articles I’ve written which I think you might enjoy.</p></div></div>
  </body>
</html>
