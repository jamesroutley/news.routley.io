<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://electricalexis.github.io/notagen-demo/">Original</a>
    <h1>NotaGen: Symbolic Music Generation</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    <div>
      <div>
        <div>
          <div>
            
            
            <p><span>Yongxin Huang<sup>4</sup>,</span>
              <span>Shuai Fan<sup>5</sup>,</span>
              <span>Xiaobing Li<sup>1</sup>,</span>
              <span>Feng Yu<sup>1</sup>,</span>
              <span><a href="https://scholar.google.com/citations?user=zIgT0HMAAAAJ" target="_blank">Maosong Sun</a><sup>1,6</sup></span>
            </p>
            </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser-->
<section>
  
</section>



<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            We introduce <b>NotaGen</b>, a symbolic music generation model aims to <b>explore the potential of producing high-quality classical sheet music</b>. Inspired by the success of Large Language Models (LLMs), NotaGen adopts <b>pre-training</b>, <b>fine-tuning</b>, and <b>reinforcement learning</b> paradigms (henceforth referred to as the LLM training paradigms). It is pre-trained on 1.6M pieces of music, and then fine-tuned on approximately 9K high-quality classical compositions conditioned on ``period-composer-instrumentation&#39;&#39;  prompts. For reinforcement learning, we propose the <b>CLaMP-DPO</b> method, which further enhances generation quality and controllability without requiring human annotations or predefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in symbolic music generation models with different architectures and encoding schemes. Furthermore, subjective A/B tests show that NotaGen outperforms baseline models against human compositions, greatly advancing musical aesthetics in symbolic music generation.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Data Representation and Model Architecture -->
 
<section>
  
</section>

<section>
    
  </section>
<!-- End Data Representation and Model Architecture -->

<!-- Training Paradigms -->
<section>
  </section>

<!-- End re-training and Fine-tuning -->



<section>

  </section>
        
      
    
  





<!--BibTex citation -->
  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@misc{wang2025notagenadvancingmusicalitysymbolic,
        title={NotaGen: Advancing Musicality in Symbolic Music Generation with Large Language Model Training Paradigms}, 
        author={Yashan Wang and Shangda Wu and Jianhuai Hu and Xingjian Du and Yueqi Peng and Yongxin Huang and Shuai Fan and Xiaobing Li and Feng Yu and Maosong Sun},
        year={2025},
        eprint={2502.18008},
        archivePrefix={arXiv},
        primaryClass={cs.SD},
        url={https://arxiv.org/abs/2502.18008}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->









<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  
  
</div>
  </body>
</html>
