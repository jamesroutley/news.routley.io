<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/segmind/SSD-1B">Original</a>
    <h1>Segmind Stable Diffusion â€“ A smaller version of Stable Diffusion XL</h1>
    
    <div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START -->
<p><a rel="noopener nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/0Iu_0f0d1ihGy0YiOd9uS.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/0Iu_0f0d1ihGy0YiOd9uS.png"/></a></p>
<h2>
	<a rel="noopener nofollow" href="#demo" id="demo">
		
	</a>
	<span>
		Demo
	</span>
</h2>
<p>Try out the model at <a rel="noopener nofollow" href="https://www.segmind.com/models/ssd-1b">Segmind SSD-1B</a> for âš¡ fastest inference. You can also try it on <a rel="noopener nofollow" href="https://huggingface.co/spaces/segmind/Segmind-Stable-Diffusion">ðŸ¤— Spaces</a></p>
<h2>
	<a rel="noopener nofollow" href="#model-description" id="model-description">
		
	</a>
	<span>
		Model Description
	</span>
</h2>
<p>The Segmind Stable Diffusion Model (SSD-1B) is a <strong>distilled 50% smaller</strong> version of the Stable Diffusion XL (SDXL), offering a <strong>60% speedup</strong> while maintaining high-quality text-to-image generation capabilities. It has been trained on diverse datasets, including Grit and Midjourney scrape data, to enhance its ability to create a wide range of visual content based on textual prompts.</p>
<p>This model employs a knowledge distillation strategy, where it leverages the teachings of several expert models in succession, including SDXL, ZavyChromaXL, and JuggernautXL, to combine their strengths and produce impressive visual outputs.</p>
<p>Special thanks to the HF team ðŸ¤— especially <a rel="noopener nofollow" href="https://huggingface.co/sayakpaul">Sayak</a>, <a rel="noopener nofollow" href="https://github.com/patrickvonplaten">Patrick</a> and <a rel="noopener nofollow" href="https://huggingface.co/multimodalart">Poli</a> for their collaboration and guidance on this work.</p>
<h2>
	<a rel="noopener nofollow" href="#image-comparision-sdxl-10-vs-ssd-1b" id="image-comparision-sdxl-10-vs-ssd-1b">
		
	</a>
	<span>
		Image Comparision (SDXL-1.0 vs SSD-1B)
	</span>
</h2>
<p><a rel="noopener nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/mOM_OMxbivVBELad1QQYj.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/mOM_OMxbivVBELad1QQYj.png"/></a></p>
<h2>
	<a rel="noopener nofollow" href="#usage" id="usage">
		
	</a>
	<span>
		Usage:
	</span>
</h2>
<p>This model can be used via the ðŸ§¨ Diffusers library. </p>
<p>Make sure to install diffusers from source by running</p>
<pre><code>pip install git+https://github.com/huggingface/diffusers
</code></pre>
<p>In addition, please install <code>transformers</code>, <code>safetensors</code> and <code>accelerate</code>:</p>
<pre><code>pip install transformers accelerate safetensors
</code></pre>
<p>To use the model, you can run the following:</p>
<pre><code><span>from</span> diffusers <span>import</span> StableDiffusionXLPipeline
<span>import</span> torch
pipe = StableDiffusionXLPipeline.from_pretrained(<span>&#34;segmind/SSD-1B&#34;</span>, torch_dtype=torch.float16, use_safetensors=<span>True</span>, variant=<span>&#34;fp16&#34;</span>)
pipe.to(<span>&#34;cuda&#34;</span>)


prompt = <span>&#34;An astronaut riding a green horse&#34;</span> 
neg_prompt = <span>&#34;ugly, blurry, poor quality&#34;</span> 
image = pipe(prompt=prompt, negative_prompt=neg_prompt).images[<span>0</span>]
</code></pre>
<h3>
	<a rel="noopener nofollow" href="#please-do-use-negative-prompting-and-a-cfg-around-90-for-the-best-quality" id="please-do-use-negative-prompting-and-a-cfg-around-90-for-the-best-quality">
		
	</a>
	<span>
		Please do use negative prompting, and a CFG around 9.0 for the best quality!
	</span>
</h3>
<h3>
	<a rel="noopener nofollow" href="#model-description-1" id="model-description-1">
		
	</a>
	<span>
		Model Description
	</span>
</h3>
<ul>
<li><strong>Developed by:</strong> <a rel="noopener nofollow" href="https://www.segmind.com/">Segmind</a></li>
<li><strong>Developers:</strong> <a rel="noopener nofollow" href="https://huggingface.co/Warlord-K">Yatharth Gupta</a> and <a rel="noopener nofollow" href="https://huggingface.co/Icar">Vishnu Jaddipal</a>.</li>
<li><strong>Model type:</strong> Diffusion-based text-to-image generative model</li>
<li><strong>License:</strong> Apache 2.0</li>
<li><strong>Distilled From</strong> <a rel="noopener nofollow" href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">stabilityai/stable-diffusion-xl-base-1.0</a></li>
</ul>
<h3>
	<a rel="noopener nofollow" href="#key-features" id="key-features">
		
	</a>
	<span>
		Key Features
	</span>
</h3>
<ul>
<li><p><strong>Text-to-Image Generation:</strong> The model excels at generating images from text prompts, enabling a wide range of creative applications.</p>
</li>
<li><p><strong>Distilled for Speed:</strong> Designed for efficiency, this model offers a 60% speedup, making it a practical choice for real-time applications and scenarios where rapid image generation is essential.</p>
</li>
<li><p><strong>Diverse Training Data:</strong> Trained on diverse datasets, the model can handle a variety of textual prompts and generate corresponding images effectively.</p>
</li>
<li><p><strong>Knowledge Distillation:</strong> By distilling knowledge from multiple expert models, the Segmind Stable Diffusion Model combines their strengths and minimizes their limitations, resulting in improved performance.</p>
</li>
</ul>
<h3>
	<a rel="noopener nofollow" href="#model-architecture" id="model-architecture">
		
	</a>
	<span>
		Model Architecture
	</span>
</h3>
<p>The SSD-1B Model is a 1.3B Parameter Model which has several layers removed from the Base SDXL Model</p>
<p><img width="400" src="https://cdn-uploads.huggingface.co/production/uploads/62f8ca074588fe31f4361dae/SfEAW1rl2mxzhur02BCjk.png"/></p><h3>
	<a rel="noopener nofollow" href="#training-info" id="training-info">
		
	</a>
	<span>
		Training info
	</span>
</h3>
<p>These are the key hyperparameters used during training:</p>
<ul>
<li>Steps: 251000</li>
<li>Learning rate: 1e-5</li>
<li>Batch size: 32</li>
<li>Gradient accumulation steps: 4</li>
<li>Image resolution: 1024</li>
<li>Mixed-precision: fp16</li>
</ul>
<h3>
	<a rel="noopener nofollow" href="#speed-comparision" id="speed-comparision">
		
	</a>
	<span>
		Speed Comparision
	</span>
</h3>
<p>We have observed that SSD-1B is upto 60% faster than the Base SDXL Model. Below is a comparision on an A100 80GB.</p>
<p><a rel="noopener nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/TyymF1OkUjXLrHUp1XF0t.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/TyymF1OkUjXLrHUp1XF0t.png"/></a></p>
<p>Below are the speed up metrics on a RTX 4090 GPU.</p>
<p><a rel="noopener nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/moMZrlDr-HTFkZlqWHUjQ.png"><img alt="image/png" src="https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/moMZrlDr-HTFkZlqWHUjQ.png"/></a></p>
<h3>
	<a rel="noopener nofollow" href="#model-sources" id="model-sources">
		
	</a>
	<span>
		Model Sources
	</span>
</h3>
<p>For research and development purposes, the SSD-1B Model can be accessed via the Segmind AI platform. For more information and access details, please visit <a rel="noopener nofollow" href="https://www.segmind.com/models/ssd-1b">Segmind</a>.</p>
<h2>
	<a rel="noopener nofollow" href="#uses" id="uses">
		
	</a>
	<span>
		Uses
	</span>
</h2>
<h3>
	<a rel="noopener nofollow" href="#direct-use" id="direct-use">
		
	</a>
	<span>
		Direct Use
	</span>
</h3>
<p>The Segmind Stable Diffusion Model is suitable for research and practical applications in various domains, including:</p>
<ul>
<li><p><strong>Art and Design:</strong> It can be used to generate artworks, designs, and other creative content, providing inspiration and enhancing the creative process.</p>
</li>
<li><p><strong>Education:</strong> The model can be applied in educational tools to create visual content for teaching and learning purposes.</p>
</li>
<li><p><strong>Research:</strong> Researchers can use the model to explore generative models, evaluate its performance, and push the boundaries of text-to-image generation.</p>
</li>
<li><p><strong>Safe Content Generation:</strong> It offers a safe and controlled way to generate content, reducing the risk of harmful or inappropriate outputs.</p>
</li>
<li><p><strong>Bias and Limitation Analysis:</strong> Researchers and developers can use the model to probe its limitations and biases, contributing to a better understanding of generative models&#39; behavior.</p>
</li>
</ul>
<h3>
	<a rel="noopener nofollow" href="#downstream-use" id="downstream-use">
		
	</a>
	<span>
		Downstream Use
	</span>
</h3>
<p>The Segmind Stable Diffusion Model can also be used directly with the ðŸ§¨ Diffusers library training scripts for further training, including:</p>
<ul>
<li><strong><a rel="noopener nofollow" href="https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py">Fine-Tune</a>:</strong></li>
</ul>
<pre><code><span>export</span> MODEL_NAME=<span>&#34;segmind/SSD-1B&#34;</span>
<span>export</span> VAE_NAME=<span>&#34;madebyollin/sdxl-vae-fp16-fix&#34;</span>
<span>export</span> DATASET_NAME=<span>&#34;lambdalabs/pokemon-blip-captions&#34;</span>

accelerate launch train_text_to_image_lora_sdxl.py \
  --pretrained_model_name_or_path=<span>$MODEL_NAME</span> \
  --pretrained_vae_model_name_or_path=<span>$VAE_NAME</span> \
  --dataset_name=<span>$DATASET_NAME</span> --caption_column=<span>&#34;text&#34;</span> \
  --resolution=1024 --random_flip \
  --train_batch_size=1 \
  --num_train_epochs=2 --checkpointing_steps=500 \
  --learning_rate=1e-04 --lr_scheduler=<span>&#34;constant&#34;</span> --lr_warmup_steps=0 \
  --mixed_precision=<span>&#34;fp16&#34;</span> \
  --seed=42 \
  --output_dir=<span>&#34;sd-pokemon-model-lora-sdxl&#34;</span> \
  --validation_prompt=<span>&#34;cute dragon creature&#34;</span> --report_to=<span>&#34;wandb&#34;</span> \
  --push_to_hub
</code></pre>
<ul>
<li><strong><a rel="noopener nofollow" href="https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py">LoRA</a>:</strong></li>
</ul>
<pre><code><span>export</span> MODEL_NAME=<span>&#34;segmind/SSD-1B&#34;</span>
<span>export</span> VAE_NAME=<span>&#34;madebyollin/sdxl-vae-fp16-fix&#34;</span>
<span>export</span> DATASET_NAME=<span>&#34;lambdalabs/pokemon-blip-captions&#34;</span>

accelerate launch train_text_to_image_sdxl.py \
  --pretrained_model_name_or_path=<span>$MODEL_NAME</span> \
  --pretrained_vae_model_name_or_path=<span>$VAE_NAME</span> \
  --dataset_name=<span>$DATASET_NAME</span> \
  --enable_xformers_memory_efficient_attention \
  --resolution=512 --center_crop --random_flip \
  --proportion_empty_prompts=0.2 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 --gradient_checkpointing \
  --max_train_steps=10000 \
  --use_8bit_adam \
  --learning_rate=1e-06 --lr_scheduler=<span>&#34;constant&#34;</span> --lr_warmup_steps=0 \
  --mixed_precision=<span>&#34;fp16&#34;</span> \
  --report_to=<span>&#34;wandb&#34;</span> \
  --validation_prompt=<span>&#34;a cute Sundar Pichai creature&#34;</span> --validation_epochs 5 \
  --checkpointing_steps=5000 \
  --output_dir=<span>&#34;sdxl-pokemon-model&#34;</span> \
  --push_to_hub
</code></pre>
<ul>
<li><strong><a rel="noopener nofollow" href="https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_sdxl.py">Dreambooth LoRA</a>:</strong></li>
</ul>
<pre><code><span>export</span> MODEL_NAME=<span>&#34;segmind/SSD-1B&#34;</span>
<span>export</span> INSTANCE_DIR=<span>&#34;dog&#34;</span>
<span>export</span> OUTPUT_DIR=<span>&#34;lora-trained-xl&#34;</span>
<span>export</span> VAE_PATH=<span>&#34;madebyollin/sdxl-vae-fp16-fix&#34;</span>

accelerate launch train_dreambooth_lora_sdxl.py \
  --pretrained_model_name_or_path=<span>$MODEL_NAME</span>  \
  --instance_data_dir=<span>$INSTANCE_DIR</span> \
  --pretrained_vae_model_name_or_path=<span>$VAE_PATH</span> \
  --output_dir=<span>$OUTPUT_DIR</span> \
  --mixed_precision=<span>&#34;fp16&#34;</span> \
  --instance_prompt=<span>&#34;a photo of sks dog&#34;</span> \
  --resolution=1024 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --learning_rate=1e-5 \
  --report_to=<span>&#34;wandb&#34;</span> \
  --lr_scheduler=<span>&#34;constant&#34;</span> \
  --lr_warmup_steps=0 \
  --max_train_steps=500 \
  --validation_prompt=<span>&#34;A photo of sks dog in a bucket&#34;</span> \
  --validation_epochs=25 \
  --seed=<span>&#34;0&#34;</span> \
  --push_to_hub
</code></pre>
<h3>
	<a rel="noopener nofollow" href="#out-of-scope-use" id="out-of-scope-use">
		
	</a>
	<span>
		Out-of-Scope Use
	</span>
</h3>
<p>The SSD-1B Model is not suitable for creating factual or accurate representations of people, events, or real-world information. It is not intended for tasks requiring high precision and accuracy.</p>
<h2>
	<a rel="noopener nofollow" href="#limitations-and-bias" id="limitations-and-bias">
		
	</a>
	<span>
		Limitations and Bias
	</span>
</h2>
<p>Limitations &amp; Bias
The SSD-1B Model has some challenges in embodying absolute photorealism, especially in human depictions. While it grapples with incorporating clear text and maintaining the fidelity of complex compositions due to its autoencoding approach, these hurdles pave the way for future enhancements. Importantly, the model&#39;s exposure to a diverse dataset, though not a panacea for ingrained societal and digital biases, represents a foundational step towards more equitable technology. Users are encouraged to interact with this pioneering tool with an understanding of its current limitations, fostering an environment of conscious engagement and anticipation for its continued evolution.</p>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
