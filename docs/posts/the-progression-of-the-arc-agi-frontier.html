<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.danielsosebee.com/p/the-progression-of-the-arc-agi-frontier">Original</a>
    <h1>The Progression of the ARC-AGI Frontier</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><article><div><div><div dir="auto"><p><em>(being weekly post 7 of 52 in the year 2026)</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!KNXC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!KNXC!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png 424w, https://substackcdn.com/image/fetch/$s_!KNXC!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png 848w, https://substackcdn.com/image/fetch/$s_!KNXC!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png 1272w, https://substackcdn.com/image/fetch/$s_!KNXC!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!KNXC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png" width="1456" height="786" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:786,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:433438,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:&#34;https://blog.danielsosebee.com/i/188063753?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!KNXC!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png 424w, https://substackcdn.com/image/fetch/$s_!KNXC!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png 848w, https://substackcdn.com/image/fetch/$s_!KNXC!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png 1272w, https://substackcdn.com/image/fetch/$s_!KNXC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19e38fd8-eb94-4585-8d2c-8bd25e3351e7_3328x1796.png 1456w" sizes="100vw" fetchpriority="high"/></picture><div></div></div></a></figure></div><p><em>Somewhere in a data center, billions of billions of floating point calculations whir away on a fleet of GPUs, spitting out hundreds of thousands of tokens. The last 100 tokens are formatted and sent to a remote server for grading. A single puzzle has been completed. 119 more to go.</em></p><p><em>Finally, the result comes back: 84.6%. A new world record for Gemini 3 Deep Think! Has artificial general intelligence been achieved?</em></p><p>ARC-AGI is a series of benchmarks aimed towards evaluating the intelligence and/or reasoning ability of AI systems via the completion of visual patterns. Since 2019, AI systems have steadily progressed towards solving these tasks, with multiple high scores being published this month, each using a different solution architecture. But what particular approaches have succeeded, and how can we interpret those results?</p><p>These questions are answered in three parts:</p><ol><li><p><strong>The history of ARC</strong><span> - starting from the seminal paper </span><em>On the Measure of Intelligence</em><span> and continuing through the evolution of the benchmark and key historical solutions</span></p></li><li><p><strong>Do we have AGI yet? (probably not)</strong><span> - discussing how we might interpret benchmark results in light of the AGI question</span></p></li><li><p><strong>Are base LLMs the best? (so far, often so)</strong><span> - discussing the performance of different solution architectures, and how we might attribute performance gains to different research advancements</span></p></li></ol><p>Finally, I touch on whether today’s commercial LLMs might grow to become generally intelligent.</p><p><span>In 2019, </span><a href="https://keras.io/" rel="">Keras</a><span> creator Francois Chollet was concerned by humanity’s lack of progress towards artificial general intelligence (AGI). Disillusioned by the lack of flexibility in contemporary AI systems, Chollet published the paper </span><a href="https://arxiv.org/abs/1911.01547" rel="">On the Measure of Intelligence</a><span>, arguing that better metrics for intelligence would aid in the development of usefully intelligent systems.</span></p><p><span>He wished to first nail down a definition of intelligence. The paper discussed many existing definitions, eventually arriving at </span><em>skill-acquisition efficiency </em><span>- the ability for a person or machine to quickly learn new techniques in new environments and thereby solve new problems.</span></p><p><span>To help researchers measure skill-acquisition efficiency, the paper presented a set of puzzles titled “Abstraction and Reasoning Corpus (ARC).” Each ARC task includes pairs of pixel grids, each pair representing an </span><em>input</em><span> and an </span><em>output</em><span>. The test taker must determine the pattern between inputs and outputs and apply that pattern to a test puzzle (a lone input grid). For example, the following puzzle requires completion of a symmetrical pattern:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!-75U!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!-75U!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png 424w, https://substackcdn.com/image/fetch/$s_!-75U!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png 848w, https://substackcdn.com/image/fetch/$s_!-75U!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png 1272w, https://substackcdn.com/image/fetch/$s_!-75U!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!-75U!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png" width="1298" height="692" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:692,&#34;width&#34;:1298,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:829463,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://blog.danielsosebee.com/i/188063753?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!-75U!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png 424w, https://substackcdn.com/image/fetch/$s_!-75U!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png 848w, https://substackcdn.com/image/fetch/$s_!-75U!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png 1272w, https://substackcdn.com/image/fetch/$s_!-75U!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34f37f97-d39e-4fa5-a64c-7d1a1bc32c29_1298x692.png 1456w" sizes="100vw" loading="lazy"/></picture><div></div></div></a><figcaption>(Figure reproduced from Chollet 2019)</figcaption></figure></div><p>Each task in the dataset is handcrafted to represent a unique human-understandable pattern. If a test-taking machine could cheaply and reliably solve these without training on similar tasks beforehand, that would be a strong signal of its skill-acquisition efficiency and therefore general intelligence.</p><p>At the time of the paper’s release, no machine scored well, but commercial LLMs were right around the corner.</p><p>In 2019 LLMs were still nascent technology, with OpenAI’s GPT-3 model to be completed one year later in 2020 and released as ChatGPT in November 2022. How did this model and its successors fare against ARC?</p><p><span>Surprisingly poorly (though unsurprising for Chollet). Even GPT-4, released March 2023, scored only 7% when presented with textual versions of the problems (source: </span><a href="https://community.openai.com/t/gpt-4-and-the-arc-challenge/168955" rel="">Jack Cole</a><span>). And GPT-4o, released mid 2024, achieved only 4.5% (source: </span><a href="https://arcprize.org/leaderboard" rel="">ARC leaderboard</a><span>).</span></p><p>Notably, GPT-4 was OpenAI’s first “vision model,” but it seemed that the model’s new image-processing capabilities did not yield substantial gains on visual reasoning tasks like ARC’s.</p><p><span>This failing stood in contrast to early LLM’s ability to mimic human conversation. Such mimicry was once proposed as a measure of intelligence in Alan Turing’s famous </span><a href="https://en.wikipedia.org/wiki/Turing_test" rel="">Turing Test</a><span>. And yet, progress in conversation mimicry soared while progress on ARC stalled.</span></p><p><strong>Program search (2020-2023): </strong><span>Chollet first hosted an “ARCathon” competition in 2020, and a pseudonymous “ice cuber” won with a 21% score. Ice cuber’s approach involved no neural networks, and instead used brute-force program-search over a domain-specific language (DSL) of pre-built image-transformation primitives, attempting to compose a sequence of transformations that would faithfully produce the example outputs from the example inputs (source: </span><a href="https://github.com/victorvikram/ARC-icecuber" rel="">ARC-icecuber repository</a><span>).</span></p><p><span>As competitions continued, brute-force program search solutions remained winning but with diminishing gains. In early 2023, Michael Hodel created an ensemble of past program-search solutions, scoring a 30.5% and capping off the era of program-search domination (</span><a href="https://github.com/ndbroadbent/arc_agi_pareto_frontiers/blob/main/historical_arc_results.py" rel="">source</a><span>).</span></p><p><strong>Transformer-based solutions take the lead (late 2023)</strong><span>: Then</span><strong> </strong><span>at the 2023 ARCathon, a team named MindsAI shared first place, and soon after recorded a new high score of 33%. MindsAI’s solution is documented in their </span><a href="https://arxiv.org/html/2506.14276" rel="">2025 paper</a><span>, and is notable as the first state of the art deep-learning based approach.</span></p><p>Their solution has three main components:</p><ol><li><p><strong>Pre-training on synthetic and augmented data</strong><span>: They used Michael Hodel’s </span><a href="https://github.com/michaelhodel/re-arc" rel="">RE-ARC</a><span> to create 400,000 ARC-like puzzles for pre-training.</span></p></li><li><p><strong>Test-time fine-tuning (TTT)</strong><span>: At test time, for each puzzle, they generate a small training dataset based on augmentations of the given input-output pairs (for example, removing one pair, rotating the grids, applying a new color mapping), then they fine-tune the model on that augmented dataset. This was their core innovation, to which they attribute a 300% accuracy boost.</span></p></li><li><p><strong>Augment, Inference, Reverse-Augment, Vote (AIRV)</strong><span>: This part is very clever in my mind. First they use the same data augmentation engine as above to transform the test input grid into a large number of similar grids (using rotations, color mappings, etc.). Then, they run inference on each transformed grid, collecting a list of results. These results can then be remapped using the inverse of the original transformations, yielding legitimate candidate results. The candidate that received the most “votes” (that was produced the most times across all transformations) is chosen as the winner.</span></p></li></ol><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!vkuZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!vkuZ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png 424w, https://substackcdn.com/image/fetch/$s_!vkuZ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png 848w, https://substackcdn.com/image/fetch/$s_!vkuZ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png 1272w, https://substackcdn.com/image/fetch/$s_!vkuZ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!vkuZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png" width="952" height="508" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/cdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:508,&#34;width&#34;:952,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:130287,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://blog.danielsosebee.com/i/188063753?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!vkuZ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png 424w, https://substackcdn.com/image/fetch/$s_!vkuZ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png 848w, https://substackcdn.com/image/fetch/$s_!vkuZ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png 1272w, https://substackcdn.com/image/fetch/$s_!vkuZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcdbaca8a-6c48-45ad-8777-932b62ec303e_952x508.png 1456w" sizes="100vw" loading="lazy"/></picture><div></div></div></a><figcaption><span>The shape of the MindsAI solution pipeline (diagram from a paper by The ARChitects, a team whose later solution was similar at a high level) (</span><a href="https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf" rel="">source</a><span>)</span></figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Zylv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Zylv!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png 424w, https://substackcdn.com/image/fetch/$s_!Zylv!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png 848w, https://substackcdn.com/image/fetch/$s_!Zylv!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png 1272w, https://substackcdn.com/image/fetch/$s_!Zylv!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!Zylv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png" width="118" height="59" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:78,&#34;width&#34;:156,&#34;resizeWidth&#34;:118,&#34;bytes&#34;:5657,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://blog.danielsosebee.com/i/188063753?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Zylv!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png 424w, https://substackcdn.com/image/fetch/$s_!Zylv!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png 848w, https://substackcdn.com/image/fetch/$s_!Zylv!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png 1272w, https://substackcdn.com/image/fetch/$s_!Zylv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72e96430-6f9d-42b4-9648-3b0fdf474054_156x78.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>In mid 2024, Chollet teamed up with </span><a href="https://zapier.com/" rel="">Zapier</a><span> founder Mike Knoop to announce the ARC Prize, setting up $1M+ in incentives including a $600K grand prize. To secure the grand prize, a team would need to publish open-source code capable of achieving an 85% success rate on the ARC puzzles, using limited computational resources within a secure internet-free sandbox.</span></p><p>By the end of 2024, MindsAI was in the lead with a 58% scoring solution (an updated version of their previous winning solution). However, they declined to publish their solution’s source code and so did not make it to the podium. First place went instead to “the ARChitects” with a 53.5% score, and the grand prize remained unclaimed.</p><p><span>The ARChitects overall pipeline was similar in form to MindsAI’s, but with different design decisions for many subcomponents (</span><a href="https://da-fr.github.io/arc-prize-2024/the_architects.pdf" rel="">paper</a><span>). Their candidate generation and selection approaches were novel:</span></p><ul><li><p><strong>Candidate generation</strong><span>: ARChitects essentially used the AIRV approach described above, but rather than sampling from a model with high temperature in order to create candidate answers, they performed depth-first search over all possible completions, extracting only those whose sampling probability exceeded a certain threshold.</span></p></li><li><p><strong>Candidate selection:</strong><span> Their voting system took into account the sampling probability of each candidate across each input transformation.</span></p></li></ul><p>In sum, they leveraged the sampling probability distribution built into their deep-learning model to a greater degree than past approaches. Still this was not enough to beat the incumbent MindsAI.</p><p><span>In parallel to the above contest, the ARC Prize Foundation released a second benchmark and leaderboard for closed-source solutions, allowing players like OpenAI to officially test their models. The public test dataset is “semi-private” - it’s not available on the open internet and therefore shouldn’t make it into any model’s training corpus (an important consideration when testing for “skill acquisition”), however the data </span><em>is</em><span> provided publicly at test time, and so could be leaked or trained on by a bad actor. That’s part of why there’s no monetary reward for ARC-AGI-Pub (though AI labs are surely incentivized to do well on this benchmark for PR reasons).</span></p><p><span>A series of results on ARC-AGI-Pub brought the high-score for unassisted LLMs into the low 30 percents by the end of 2024. Then, a preview version of OpenAI’s O3, an early chain-of-thought enabled </span><a href="https://en.wikipedia.org/wiki/Reasoning_model" rel="">reasoning model</a><span>, achieved an 88% on ARC-AGI-Pub in December 2024 (source: </span><a href="https://arcprize.org/blog/oai-o3-pub-breakthrough" rel="">ARC Prize blog</a><span>). This was a breakthrough result, but came with some important caveats:</span></p><ol><li><p><span>The preview model was trained specifically for success on ARC, unlike the later released O3 model which performed worse (source: </span><a href="https://arcprize.org/blog/analyzing-o3-with-arc-agi" rel="">ARC Prize blog</a><span>).</span></p></li><li><p>O3-preview took a whopping 14 minutes and $4,560 of GPU compute per task, far exceeding the compute allowance of the open-source ARC Prize contest, and far exceeding reasonable estimates of the true cost of solving these small puzzles.</p></li></ol><p>Still, in 2025 the ARC Prize Foundation wished to counter the recent success of reasoning models, so they released a a new dataset called “ARC-AGI-2” and the old dataset became “ARC-AGI-1”. ARC-AGI-2 looked much like its predecessor:</p><p>… but sought to test for capabilities with which reasoning models struggled, such as:</p><ol><li><p><strong>Symbolic interpretation</strong><span>: assigning significance to visual symbols (see image above)</span></p></li><li><p><strong>Compositional reasoning</strong><span>: applying multiple rules on top of each other</span></p></li><li><p><strong>Contextual rule application</strong><span>: applying rules under specific contexts</span></p></li></ol><p>In tandem, the 2025 ARC Prize was launched with similar prizes and guidelines to the prior year’s. This time around the eventual top score was 24%, indicating that ARC-AGI-2 was indeed a harder benchmark.</p><p>Despite the low scores on 2025’s ARC Prize, today’s frontier reasoning models continue to notch higher scores at greater efficiencies against the semi-private test set. Just this month, Google announced a massive 84.6% score (at $13/task) for their new Gemini 3 Deep Think model.</p><p><span>Is it over for ARC? Nope, the cycle continues: ARC Prize is already planning their 2026 benchmark, this time with an interactive game format (ARC-AGI-3), to be released March 25th, which will in all likelihood be more difficult for today’s LLMs. You can play the first three example games </span><a href="https://three.arcprize.org/" rel="">here</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Xmje!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Xmje!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png 424w, https://substackcdn.com/image/fetch/$s_!Xmje!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png 848w, https://substackcdn.com/image/fetch/$s_!Xmje!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png 1272w, https://substackcdn.com/image/fetch/$s_!Xmje!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!Xmje!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png" width="233" height="219.60919540229884" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:820,&#34;width&#34;:870,&#34;resizeWidth&#34;:233,&#34;bytes&#34;:34134,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://blog.danielsosebee.com/i/188063753?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Xmje!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png 424w, https://substackcdn.com/image/fetch/$s_!Xmje!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png 848w, https://substackcdn.com/image/fetch/$s_!Xmje!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png 1272w, https://substackcdn.com/image/fetch/$s_!Xmje!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F07924bd6-abd5-4d81-b91e-54c4242637fe_870x820.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>Is Chollet moving the goalposts with each new benchmark? And is Gemini 3 Deep Think already AGI?</p><p><span>Well, the goalposts </span><em>are</em><span> moving, but wouldn’t a general intelligence be capable of following a moving target? Isn’t that exactly what skill acquisition would enable? Concretely, </span><strong>as long as each successive ARC-AGI benchmark is achievable by humans, we can say that failures on the part of machines indicate a lack of general intelligence.</strong></p><p>In fact, Chollet’s original paper acknowledges the need for periodic refreshes:</p><blockquote><p>ARC only features 1,000 tasks in total, and there may be some amount of conceptual overlap across many tasks. This could make ARC potentially vulnerable to shortcut strategies that could solve the tasks without featuring intelligence.</p><p>[…]</p><p>…to mitigate potential vulnerability against such shortcuts, we intend to keep adding new tasks to ARC in the future, possibly by crowd-sourcing them.</p></blockquote><p>What are the specific shortcuts that Chollet might consider disqualifying? The most likely culprit would be directly training on large synthetic datasets that closely mimic the puzzles themselves. Doing so arguably counts as pre-acquiring the skills that are supposed to be acquired at test time. Such pre-training has already been a hallmark of the winning deep-learning strategies since 2023, and is especially prominent in 2025 prize winner NVARC’s solution. Also, we cannot rule out that closed-source model providers like Google might be taking similar approaches as well to improve their results on the public benchmark. Unfortunately, this makes the entire leaderboard harder to interpret.</p><p>A key pattern to observe is whether new ARC-AGI benchmarks saturate immediately. If Gemini 3 Deep Think scores well on ARC-AGI-3 in March, that would indicate the kind of fluid intelligence Chollet was looking for in his original paper, as the model probably hasn’t trained on environments similar to ARC-AGI-3’s games. If instead it takes time for Google and other labs to saturate the benchmark, we will be left wondering whether the gain is from general intelligence abilities, or from pre-trained skills that are aimed at beating the benchmark.</p><p>Which methods are most successful against ARC, and which are most promising? Firstly, to define success…</p><p><span>The ARC leaderboards show a </span><a href="https://en.wikipedia.org/wiki/Pareto_front" rel="">Pareto frontier</a><span> between cost and score, and any submission along that frontier of low costs and high scores can be considered successful. What trends do we see?</span></p><p>The ARC-AGI frontier has vacillated between different solution architectures - the early 2020s saw search-based program synthesis winning, while recent years saw chain-of-thought/reasoning LLMs pull ahead using transduction (producing the output grid directly instead of producing a program that produces the grid)… just look at how much of the frontier in the image above is dominated by Google’s green Gemini 3 Flash Preview and Gemini 3 Deep Think.</p><p><span>But even in recent months, we see a variety of architectures neck-and-neck for the frontier. Earlier this month a solution by Johan Land (</span><a href="https://www.kaggle.com/code/johanland/johan-land-solver-v7-public" rel="">code</a><span>) topped the leaderboard with 72.9% on ARC-AGI-2 for $39/task. While this was soon eclipsed by Gemini 3 Deep Think, this solution has its part to play in the overall trends.</span></p><p>Land’s solution involves a complicated pipeline, which works as follows (you can skip this numbered list if you’re satisfied by “it’s complicated”):</p><ol><li><p>In parallel, run a bunch of state-of-the-art reasoning LLMs, some of which are tasked with producing a grid output, and some of which are tasked with code generation.</p></li><li><p>If there’s sufficient consensus among results produced by the above, then finish (this saves some money)</p></li><li><p><em>(if enabled) repeat the above steps using different models</em></p></li><li><p>Then run all five of these strategies in parallel</p><ol><li><p><strong>Deep thinking:</strong><span> prompt a model to think especially deeply about the problem before producing its output</span></p></li><li><p><strong>Image-based:</strong><span> Feed rendered images to a model, ask for grid output</span></p></li><li><p><strong>Hints:</strong><span> Get an LLM to analyze images and produce hints. An LLM uses these hints to produce a grid output.</span></p></li><li><p><strong>Structured, object-based thinking:</strong><span> A 3-part pipeline that compels models to describe the objects they see, then describe the transformations between those objects, then finally, in context of all previous answers, produce a grid output.</span></p></li><li><p><strong>Codegen:</strong><span> Re-run the codegen aspects from step 1, but with more models runs.</span></p></li></ol></li></ol><p>This was truly a “throw everything at the wall and see what sticks” solution, combining lots of raw LLM power with various forms of cognitive scaffolding.</p><p>For a brief moment it was the highest-scoring solution out there, surpassing Claude Opus 4.6’s score of 69.2% by 3.7%, though at 11 times the cost. Given the 120 puzzles in the semi-private test suite, Mr. Land spent ~$5K on the test run alone, to say nothing of the costs in developing his methods.</p><p>Gemini 3 Deep Think outperformed Land on both accuracy and cost. Approaches with more scaffolding and/or symbolic reasoning, like Land’s, have occasionally reached the frontier, but they usually represent modest gains over incumbent transduction-based approaches, while incurring larger costs. Every approach at the top of the current public leaderboard heavily relies on LLMs even when combined with scaffolding, and improvements in the underlying models seem to cause improvements across the board. This suggests that a large amount of progress towards solving ARC (at least on the closed-source end) is due to advancements in LLMs.</p><p><span>Is this bad news for the ARC Prize Foundation’s mission to support novel research? Perhaps, but they have a few facts working in their favor. Firstly, LLMs </span><em>did</em><span> see meaningful algorithmic improvements over recent years - OpenAI’s O1/O3 models, for example, introduced the chain-of-thought reasoning approach that led to huge gains on ARC and many other benchmarks. Secondly, for the actual ARC Prize, which requires open-source models to run on limited hardware, each year’s winners have presented solutions with meaningful complexity outside of the LLMs that they leverage.</span></p><p>A possible synthesis of these facts: at smaller cost scales, independent researchers are incentivized to explore a variety of novel approaches, and these approaches beat out simpler LLM-based approaches and push the research frontier forward. At larger cost scales, submissions become too expensive for independent researchers, except for the occasional tech Senior Vice President (Johan Land), and the charts become dominated by big labs whose primary motive is to showcase their models as standalone products.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!N5Cl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!N5Cl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png 424w, https://substackcdn.com/image/fetch/$s_!N5Cl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png 848w, https://substackcdn.com/image/fetch/$s_!N5Cl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png 1272w, https://substackcdn.com/image/fetch/$s_!N5Cl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!N5Cl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png" width="240" height="72.32876712328768" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:132,&#34;width&#34;:438,&#34;resizeWidth&#34;:240,&#34;bytes&#34;:30312,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://blog.danielsosebee.com/i/188063753?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!N5Cl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png 424w, https://substackcdn.com/image/fetch/$s_!N5Cl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png 848w, https://substackcdn.com/image/fetch/$s_!N5Cl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png 1272w, https://substackcdn.com/image/fetch/$s_!N5Cl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F534e6869-097f-4d6f-a0b5-b2242258e453_438x132.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>A recent </span><a href="https://x.com/fchollet/status/1803096195684012371?lang=en" rel="">twitter post</a><span> from Chollet reads:</span></p><blockquote><p><span>I believe that program synthesis will solve reasoning. And I believe that deep learning will solve program synthesis (by guiding a discrete program search process).</span></p></blockquote><p><span>Chollet has long been an advocate for program synthesis, and in January 2025 he and Knoop founded </span><a href="https://ndea.com/" rel="">NDEA</a><span>, a research lab building “AI systems that blend intuitive pattern recognition and formal reasoning into a unified architecture.”</span></p><p><span>As suggested by the tweet above, the architecture he wishes to build would not be the kind of </span><a href="https://blog.redwoodresearch.org/p/getting-50-sota-on-arc-agi-with-gpt" rel="">LLM-assisted program-synthesis approach</a><span> that Ryan Greenblatt used to top the ARC public leaderboards in mid-2024, which we see repurposed as a sub-component of Johan Land’s pipeline above - though by all appearances, such architectures are close to his vision. NDEA has not published anything yet, so we’ll have to wait and see how their approach differs.</span></p><p>However, the current frontier of ARC-AGI-2 submissions all use direct transduction without program synthesis, so it would be impressive if NDEA could present a program-synthesis architecture that’s competitive on past or future ARC benchmarks.</p><p>The ARC-AGI frontier has been populated by a wide variety of cognitive architectures, but for now is dominated by commercial LLMs - the same systems that answer our questions and write our code. Seeing whether these systems succeed immediately on the novel ARC-AGI-3 tasks will be a good test of their general intelligence abilities.</p><p><span>A key open question is whether commercial LLMs will grow to succeed at ARC-AGI-3 and subsequent versions. Per the ARC Prize website, “AGI [is] still unsolved. New ideas [are] needed.” But Dario Amodei, CEO of Anthropic, would disagree. In </span><a href="https://www.dwarkesh.com/p/dario-amodei-2" rel="">a recent interview with Dwarkesh Patel</a><span> he indicated that he expects Anthropic’s training plan to scale to fully generalized intelligence (at least in domains that are operable by text). I’ll leave with his quote:</span></p><blockquote><p>…all the cleverness, all the techniques, all the “we need a new method to do something”, that doesn’t matter very much. There are only a few things that matter.</p><p>[…]</p><p><span>In fact, I would point to the history in </span><a href="https://en.wikipedia.org/wiki/Machine_learning" rel="">ML</a><span> of people coming up with things that are barriers that end up kind of dissolving within the big blob of compute. People talked about, “How do your models keep track of nouns and verbs?” “They can understand syntactically, but they can’t understand semantically? It’s only statistical correlations.” “You can’t understand a paragraph, you can’t understand a word. There’s reasoning, you can’t do reasoning.” But then suddenly it turns out you can do code and math very well.</span></p><p>[…]</p><p>I think we may get to the point in a year or two where the models can just do [software engineering] end-to-end. That’s a whole task. That’s a whole sphere of human activity that we’re just saying models can do now.</p></blockquote></div></div></div></article></div></div></div><div><div id="discussion"><div><h4>Discussion about this post</h4></div></div></div></div>
  </body>
</html>
