<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://localrf.github.io/">Original</a>
    <h1>Localrf â€“ Nerf from casual shaky videos</h1>
    
    <div id="readability-page-1" class="page">

<h3>
  <span><a href="https://ameuleman.github.io/">Andreas Meuleman</a><sup>1</sup></span>
  <span><a href="https://www.cmlab.csie.ntu.edu.tw/~yulunliu/">Yu-Lun Liu</a><sup>2</sup></span>
  <span><a href="http://chengao.vision/">Chen Gao</a><sup>3</sup></span>
<!-- </h3>
<h3 class="authors"> -->
  <span><a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a><sup>3,4</sup></span>
  <span><a href="https://changilkim.com/">Changil Kim</a><sup>3</sup></span>
  <span><a href="http://vclab.kaist.ac.kr/minhkim/">Min H. Kim</a><sup>1</sup></span>
  <span><a href="http://johanneskopf.de/">Johannes Kopf</a><sup>3</sup></span>
</h3>




<iframe width="560" height="315" src="https://www.youtube.com/embed/VizL7q9o-5Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>

<h2 id="vidcmp_expa">
  Video comparisons
</h2>
<video id="vidcmp_expa_vid0" controls="" loop="">
  <source src="videos/forest1_input.mp4" type="video/mp4"/>
</video>
<video id="vidcmp_expa_vid1" controls="" loop="">
  <source src="videos/forest1_mipnerf360.mp4" type="video/mp4"/>
</video>
<video id="vidcmp_expa_vid2" controls="" loop="">
  <source src="videos/forest1_nerfacto.mp4" type="video/mp4"/>
</video>
<video id="vidcmp_expa_vid3" controls="" loop="">
  <source src="videos/forest1_barf.mp4" type="video/mp4"/>
</video>
<video id="vidcmp_expa_vid4" controls="" loop="">
  <source src="videos/forest1_nopenerf.mp4" type="video/mp4"/>
</video>
<video id="vidcmp_expa_vid5" controls="" loop="" autoplay="" muted="">
  <source src="videos/forest1_ours.mp4" type="video/mp4"/>
</video>
<div>
  
  <div>
    <div>  
      <p>COLMAP poses</p>
      <p>Self calibrated</p>
      </div>
  </div>
  <p><b>Comparisons.</b> We show rendered smooth trajectory from each method.   
  </p>
</div>

<h2 id="prog_opt">
  Progressive local radiance fields optimization
</h2>
<video controls="" loop="" autoplay="" muted="">
  <source src="videos/prog_opt.mp4" type="video/mp4"/>
</video>
<p> We progressively add frames and estimate their poses while optimizing radiance fields. Local radiance fields are dynamically allocated throughout the process. We show the current frames in red and the radiance fields locations with the blue boxes. On the right, we show the last input frame used on top and the last rendered testing view at the bottom. We observe that our method starts estimating a coarse radiance field along with poses before a refinement stage that sharpens the renders. Then, we create a new local radiance field and process another segment of the video. </p>

<h2 id="abstract">
  Abstract
</h2>
<p>
  We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive optimization significantly improves the robustness of the reconstruction. For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the Tanks and Temples dataset and our collected outdoor dataset, Static Hikes, show that our approach compares favorably with the state-of-the-art.
</p>

<h2 id="prog_opt">
  Stabilization comparison
</h2>


<p> By smoothing the camera path, we achieve much smoother stabilization than 2D methods such as FuSta. </p>

<h2 id="vidcmp_expb">
  Ablations
</h2>
<video id="vidcmp_expb_vid0" controls="" loop="">
  <source src="videos/forest1_input.mp4" type="video/mp4" autoplay="" muted=""/>
</video>
<video id="vidcmp_expb_vid1" controls="" loop="">
  <source src="videos/forest1_no_prog.mp4" type="video/mp4" autoplay="" muted=""/>
</video>
<video id="vidcmp_expb_vid2" controls="" loop="">
  <source src="videos/forest1_no_loc.mp4" type="video/mp4" autoplay="" muted=""/>
</video>
<video id="vidcmp_expb_vid3" controls="" loop="" autoplay="" muted="">
  <source src="videos/forest1_ours.mp4" type="video/mp4"/>
</video>
<div>
  
  
  <p> <b>Ablation.</b> Progressive optimization is crucial to the pose estimation and local radiance fields grants more robustness and improves sharpness in later parts of the sequences. </p>
</div>




<h2 id="citation">
  Citation
</h2>
<pre>@inproceedings{meuleman2023localrf,
  author    = {Meuleman, Andreas and Liu, Yu-Lun and Gao, Chen and Huang, Jia-Bin and Kim, Changil and Kim, Min H. and Kopf, Johannes},
  title     = {Progressively Optimized Local Radiance Fields for Robust View Synthesis},
  booktitle = {CVPR},
  year      = {2023},
}
</pre>
</div>
  </body>
</html>
