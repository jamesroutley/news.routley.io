<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://matklad.github.io/2024/08/01/primitive-recursive-functions.html">Original</a>
    <h1>Primitive Recursive Functions for a Working Programmer</h1>
    
    <div id="readability-page-1" class="page"><div>
  <article>


<p><span>Programmers on the internet often use </span>“<span>Turing-completeness</span>”<span> terminology. Typically, not being</span>
<span>Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most</span>
<span>such discussions are misinformed </span>—<span> that not being Turing complete doesn</span>’<span>t actually mean what folks</span>
<span>want it to mean, and is instead a stand-in for a bunch of different practically useful properties,</span>
<span>which are mostly orthogonal to actual Turing completeness.</span></p>
<p><span>While I am generally descriptivist in nature and am ok with words loosing their </span><em><span>original</span></em><span> meaning</span>
<span>as long as the new meaning is sufficiently commonly understood, Turing completeness is a hill I will</span>
<span>die on. It is a term from math, it has a very specific meaning, and you are not allowed to</span>
<span>re-purpose it for anything else, sorry!</span></p>
<p><span>I understand why this happens: to really understand what Turing completeness is and is not you need</span>
<span>to know one (simple!) theoretical result about so-called primitive recursive functions. And,</span>
<span>although this result is simple, I was only made aware of it in a fairly advanced course during my</span>
<span>masters. That</span>’<span>s the CS education deficiency I want to rectify </span>—<span> you can</span>’<span>t teach students the</span>
<span>halting problem without </span><em><span>also</span></em><span> teaching them about primitive recursion!</span></p>
<p><span>The post is going to be rather meaty, and will be split in three parts:</span></p>
<p><span>In Part I, I give a TL;DR for the theoretical result and some of its consequences. Part II is going</span>
<span>to be a whirlwind tour of Turing Machines, Finite State Automata and Primitive Recursive Functions.</span>
<span>And then Part III will circle back to practical matters.</span></p>
<p><span>If math makes you slightly nauseas, you might to skip Part II. But maybe give it a try? The math</span>
<span>we</span>’<span>ll need will be baby math from first principles, without reference to any advanced results.</span></p>
<section id="Part-I-TL-DR">

    <h2>
    <a href="#Part-I-TL-DR"><span>Part I: TL;DR</span> </a>
    </h2>
<p><span>Here</span>’<span>s the key result </span>—<span> suppose you have a program in some Turing complete language, and you also</span>
<span>know that it</span>’<span>s not too slow. Suppose it runs faster than</span>
<span><span>O(2</span><sup><span>2</span><sup><span>N</span></sup></sup><span>).</span></span>
<span>That is, two to the power of two to the power of N, a very large number. In this case, you can</span>
<span>implement this algorithm in a non-Turing complete language.</span></p>
<p><span>Most practical problems fall into this </span>“<span>faster than two to the two to the power of two</span>”<span> space.</span>
<span>Hence it follows that you don</span>’<span>t need full power of a Turing Machine to tackle them. Hence, a</span>
<span>language not being Turing complete doesn</span>’<span>t in any way restrict you in practice, or gives you extra</span>
<span>powers to control the computation.</span></p>
<p><span>Or, to restate this: in practice, a program which doesn</span>’<span>t terminate, and a program that needs a</span>
<span>billion billions steps to terminate are equivalent. Making something non-Turing complete by itself</span>
<span>doesn</span>’<span>t help with the second problem in any way. And there</span>’<span>s a trivial approach that solves the</span>
<span>first problem for any existing Turing-complete language </span>—<span> in the implementation, count the steps</span>
<span>and bail with an error after a billion.</span></p>
</section>
<section id="Part-II-Weird-Machines">

    <h2>
    <a href="#Part-II-Weird-Machines"><span>Part II: Weird Machines</span> </a>
    </h2>
<p><span>The actual theoretical result is quite a bit more general than that. It is (unsurprisingly)</span>
<span>recursive:</span></p>

<figure>
<blockquote><p><span>If a function is computed by a Turing Machine, and the runtime of this machine is bounded by some</span>
<span>primitive recursive function of input, then the original function itself can be written as a</span>
<span>primitive recursive function.</span></p>
</blockquote>

</figure>
<p><span>It is expected that this sounds like gibberish at this point! So let</span>’<span>s just go and prove this thing,</span>
<span>right here in this blog post! Will work up slowly towards this result. The plan is as follows:</span></p>
<ul>
<li>
<em><span>First</span></em><span>, to brush up notation, we</span>’<span>ll define Finite State Machines.</span>
</li>
<li>
<em><span>Second</span></em><span>, we</span>’<span>ll turn our humble Finite State Machine into the all-powerful Turing Machine (spoiler</span>
—<span> a Turing Machine is an FSM with a pair of stacks), and, as is customary, wave our hands about</span>
<span>the Universal Turing Machine.</span>
</li>
<li>
<em><span>Third</span></em><span>, we leave the cozy world of imperative programming and define primitive recursive</span>
<span>functions.</span>
</li>
<li>
<em><span>Finally</span></em><span>, we</span>’<span>ll talk about the relative computational power of TMs and PRFs, including the teased</span>
<span>up result and more!</span>
</li>
</ul>
</section>
<section id="Finite-State-Machines">

    <h2>
    <a href="#Finite-State-Machines"><span>Finite State Machines</span> </a>
    </h2>
<p><dfn><span>Finite State Machines</span></dfn><span> are simple! An FSM takes a string as input, and returns a binary</span>
<span>answer, </span>“<span>yes</span>”<span> or </span>“<span>no</span>”<span>. Unsurprisingly an FSM has a finite number of states: Q0, Q1, </span>…<span>, Qn.</span>
<span>A subset of states are designated as </span>“<span>yes</span>”<span> states, the rest are </span>“<span>no</span>”<span> states. There</span>’<span>s also one</span>
<span>specific starting state.</span></p>
<p><span>The behavior of the state machine is guided by a transition (step) function, </span><code>s</code><span>. This function</span>
<span>takes the current state of FSM, the next symbol of input, and returns a new state.</span></p>
<p><span>The semantics of FSM is determined by repeatably applying the single step function for all symbols of</span>
<span>the input, and noting whether the final state is a </span>“<span>yes</span>”<span> state or a </span>“<span>no</span>”<span> state.</span></p>
<p><span>Here</span>’<span>s an FSM which accepts only strings of zeros and ones of even length:</span></p>

<figure>


<pre><code><span>States:     { Q0, Q1 }</span>
<span>Yes States: { Q0 }</span>
<span>Start State:  Q0</span>
<span></span>
<span>s :: State -&gt; Symbol -&gt; State</span>
<span>s Q0 0 = Q1</span>
<span>s Q0 1 = Q1</span>
<span>s Q1 0 = Q0</span>
<span>s Q1 1 = Q0</span></code></pre>

</figure>
<p><span>This machine ping-pongs between states Q0 and Q1 ends up in Q0 only for inputs of even length</span>
<span>(including an empty input).</span></p>
<p><span>What can FSMs do? As they give a binary answer, they are recognizers </span>—<span> they don</span>’<span>t compute</span>
<span>functions, but rather just characterize certain sets of strings. A famous result is that the</span>
<span>expressive power of FSMs is equivalent to the expressive power of regular expressions. If you can</span>
<span>write a regular expression for it, you could also do an FSM!</span></p>
<p><span>There are also certain things that state machines can</span>’<span>t do. For example they can</span>’<span>t enter an infinite</span>
<span>loop. Any FSM is linear in the input size and always terminates. But there are much more specific</span>
<span>sets of strings that couldn</span>’<span>t be recognized by an FSM. Consider this set:</span></p>

<figure>


<pre><code><span>1</span>
<span>010</span>
<span>00100</span>
<span>0001000</span>
<span>...</span></code></pre>

</figure>
<p><span>That is, an infinite set which contains </span>‘<span>1</span>’<span>s surrounded by the equal number of </span>‘<span>0</span>’<span>s on the both</span>
<span>sides. Let</span>’<span>s prove that there isn</span>’<span>t a state machine that recognizes this set!</span></p>
<p><span>As usually, suppose there </span><em><span>is</span></em><span> such a state machine. It has a certain number of states </span>—<span> maybe a</span>
<span>dozen, maybe a hundred, maybe a thousand, maybe even more. But let</span>’<span>s say fewer than a million.</span>
<span>Then, let</span>’<span>s take a string which looks like a million zeros, followed by one, followed by million</span>
<span>zeros. And let</span>’<span>s observe our FSM eating this particular string.</span></p>
<p><span>First of all, because the string is in fact a one surrounded by the equal number of zeros on both</span>
<span>sides, the FSM ends up in a </span>“<span>yes</span>”<span> state. Moreover, because the length of the string is much greater</span>
<span>than the number of states in the state machine, the state machine necessary visits some state twice.</span>
<span>There is a cycle, where the machine goes from A to B to C to D and back to A. This cycle might be</span>
<span>pretty long, but it</span>’<span>s definitely shorter than the total number of states we have.</span></p>
<p><span>And now we can fool the state machine. Let</span>’<span>s make it eat our string again, but this time, once it</span>
<span>completes the ABCDA cycle, we</span>’<span>ll force it to traverse this cycle again. That is, the original cycle</span>
<span>corresponds to some portion of our giant string:</span></p>

<figure>


<pre><code><span>0000 0000000000000000000 00 .... 1 .... 00000</span>
<span>     &lt;- cycle portion -&gt;</span></code></pre>

</figure>
<p><span>If we duplicate this portion, our string will no longer look like one surrounded by equal number of</span>
<span>twos, but the state machine will still in the </span>“<span>yes</span>”<span> state. Which is a contradiction that completes</span>
<span>the proof.</span></p>
</section>
<section id="Turing-Machine-Definition">

    <h2>
    <a href="#Turing-Machine-Definition"><span>Turing Machine: Definition</span> </a>
    </h2>
<p><span>A </span><dfn><span>Turing Machine</span></dfn><span> is only slightly more complex than an FSM. Like an FSM, a TM has a bunch of states</span>
<span>and a single-step transition function. While an FSM has an immutable input which is being feed to it</span>
<span>symbol by symbol, a TM operates with a mutable tape. The input gets written to the tape at the</span>
<span>start. At each step, a TM looks at the current symbol on the tape, changes its state according to a</span>
<span>transition function and, additionally:</span></p>
<ul>
<li>
<span>Replaces the current symbol with a new one (which might or might not be different).</span>
</li>
<li>
<span>Moves the reading head that points at the current symbol one position to the left or to the right.</span>
</li>
</ul>
<p><span>When a machine reaches a designated halt state, it stops, and whatever is written on the tape at</span>
<span>that moment is the result. That is, while FSMs are binary recognizers, TMs are functions. Keep in</span>
<span>mind that a TM does not necessary stop. It might be the case that a TM goes back and forth over the</span>
<span>tape, overwrites it, changes its internal state, but never quite gets to the final state.</span></p>
<p><span>Here</span>’<span>s an example Turing Machine:</span></p>

<figure>


<pre><code><span>States:  {A, B, C, H}</span>
<span>Start State: A</span>
<span>Final State: H</span>
<span></span>
<span>s :: State -&gt; Symbol -&gt; (State, Symbol, Left | Right)</span>
<span>s A 0 = (B, 1, Right)</span>
<span>s A 1 = (H, 1, Right)</span>
<span>s B 0 = (C, 0, Right)</span>
<span>s B 1 = (B, 1, Right)</span>
<span>s C 0 = (C, 1, Left)</span>
<span>s C 1 = (A, 1, Left)</span></code></pre>

</figure>
<p><span>If the configuration of the machine looks like this:</span></p>

<figure>


<pre><code><span>000010100000</span>
<span>     ^</span>
<span>     B</span></code></pre>

</figure>
<p><span>Then we are in the </span><code>s B 0 = (C, 0, Right)</code><span> case, so we should change the state to C, replace 0 with</span>
<span>1, and move to the right:</span></p>

<figure>


<pre><code><span>000011100000</span>
<span>      ^</span>
<span>      C</span></code></pre>

</figure>
</section>
<section id="Turing-Machine-Programming">

    <h2>
    <a href="#Turing-Machine-Programming"><span>Turing Machine: Programming</span> </a>
    </h2>
<p><span>There is a bunch of fiddly details to Turing Machines!</span></p>
<p><span>The tape is conceptually infinite, so beyond the input, everything is just zeros. This creates a</span>
<span>problem: it might be hard to say where the input (or the output) ends! There are a couple of</span>
<span>technical solutions here. One is to say that there are three different symbols on the tape </span>—
<span>zeros, ones, and blanks, and require that the tape is initialized with blanks. A different solution</span>
<span>is to invent some encoding scheme. For example, we can say that the input is a sequence of 8-bit</span>
<span>bytes, without interior null bytes. So, eight consecutive zeros at a byte boundary designate the end</span>
<span>of input/output.</span></p>
<p><span>It</span>’<span>s useful to think about how this byte-oriented TM could be implemented. We could have one large</span>
<span>state for each byte of input. So, Q142 would mean that the head is on the byte with value 142. And</span>
<span>then we</span>’<span>ll have a bunch of small states to read out the current byte. Eg, we start reading a byte in</span>
<span>state </span><code>S</code><span>. Depending on the next bit we move to S0 or S1, then to S00, or S01, etc. Once we reached</span>
<span>something like S01111001, we move back 8 positions and enter state Q121. This is one of the patterns</span>
<span>of Turing Machine programming </span>—<span> while your main memory is the tape, you can represent some</span>
<span>constant amount of memory directly in the states.</span></p>
<p><span>What we</span>’<span>ve done here is essentially lowering a byte-oriented Turing Machine to a bit-oriented</span>
<span>machine. So, we could think only in terms of big states operating on bytes, as we know the general</span>
<span>pattern for converting that to direct bit-twiddling.</span></p>
<p><span>With this encoding scheme in place, we now can feed arbitrary files to a Turing Machine! Which will</span>
<span>be handy to the next observation:</span></p>
<p><span>You can</span>’<span>t actually program a Turing Machine. What I mean is that, counter-intuitively, there isn</span>’<span>t</span>
<span>some user-supplied program that a Turing Machine executes. Rather, the program is hard-wired into</span>
<span>the machine. Transition function </span><em><span>is</span></em><span> the program.</span></p>
<p><span>But with some ingenuity we can regain our ability to write programs. Recall that we</span>’<span>ve just learned</span>
<span>to feed arbitrary files to a TM. So what we could do is to write a text file that specifies a TM and</span>
<span>its input, and then feed that entire file as an input to an </span>“<span>interpreter</span>”<span> Turing Machine which would</span>
<span>read the file, and act as if the machine specified there. Turing Machine can have an </span><code>eval</code>
<span>function.</span></p>
<p><span>Is such </span>“<span>interpreter</span>”<span> Turing Machine possible? Yes! And it is not hard: if you spend couple of hours</span>
<span>programming Turing Machines by hand, you</span>’<span>ll see that you pretty much can do anything </span>—<span> you can do</span>
<span>numbers, arithmetics, loops, control flow. It</span>’<span>s just very very tedious.</span></p>
<p><span>So let</span>’<span>s just declare that we</span>’<span>ve actually coded up this Universal Turing Machine which simulates a</span>
<span>TM given to it as an input in a particular encoding.</span></p>
<p><span>This sort of construct also gives rise to Church-Turing thesis. We have a TM which can run other</span>
<span>TMs. And you can implement a TM interpreter in something like Python. And, with a bit of legwork,</span>
<span>you could </span><em><span>also</span></em><span> implement a Python interpreter as a TM (you likely want to avoid doing that</span>
<span>directly, and instead do a simpler interpreter for WASM, and then use a Python interpreter complied</span>
<span>to WASM). This sort of bidirectional interpretation shows that Python and TM have equivalent</span>
<span>computing power. Moreover, it</span>’<span>s quite hard to come up with a reasonable computational device which</span>
<span>is more powerful than a Turing Machine.</span></p>
<p><span>There are computational devices that are strictly weaker than TMs though. Recall FSM. By this point,</span>
<span>it should be obvious that a TM can simulate an FSM. Everything a Finite State Machine can do, a</span>
<span>Turing Machine can do as well. And it should be intuitively clear that TM is more powerful than an</span>
<span>FSM. FSM gets to use only a finite number of states. A TM has these same states, but it also posses</span>
<span>a tape which serves like an infinitely sized external memory.</span></p>
<p><span>Directly proving that you </span><em><span>can</span>’<span>t</span></em><span> encode a Universal Turing Machine as an FSM sounds complicated,</span>
<span>so let</span>’<span>s prove something simpler. Recall that we have established that there</span>’<span>s no FSM that accepts</span>
<span>only ones surrounded by the equal number of zeros on both sides (because a sufficiently large word</span>
<span>of this form would necessary enter a cycle in a state machine, which could then be further pumped).</span>
<span>But it</span>’<span>s actaully easy to write a Turing Machine that does this:</span></p>
<ul>
<li>
<span>Erase zero (at the left side of the tape)</span>
</li>
<li>
<span>Go to the right end of the tape</span>
</li>
<li>
<span>Erase zero</span>
</li>
<li>
<span>Go to the left side of the tape</span>
</li>
<li>
<span>Repeat</span>
</li>
<li>
<span>If what</span>’<span>s left is a single </span><code>1</code><span> the answer is </span>“<span>yes</span>”<span>, otherwise it is a </span>“<span>no</span>”
</li>
</ul>
<p><span>We found a specific problem that can be solved by a TM, but is out of reach of any FSM. So it</span>
<span>necessary follows that there isn</span>’<span>t an FSM that can simulate an arbitrary TM.</span></p>
<p><span>It is also useful to take a closer look at the tape. It is a convenient skeuomorphic abstraction</span>
<span>which makes the behavior of the machine intuitive, but it is inconvenient to implement in a normal</span>
<span>programming language. There isn</span>’<span>t a standard data structure that behaves just like a tape.</span></p>
<p><span>One cool practical trick is to simulate the tape as a pair of stacks. Take this:</span></p>

<figure>


<pre><code><span>Tape: A B C D E F G</span>
<span>Head:     ^</span></code></pre>

</figure>
<p><span>And transform it to something like this:</span></p>

<figure>


<pre><code><span>Left Stack:  [A, B, C]</span>
<span>Right Stack: [G, F, E, D]</span></code></pre>

</figure>
<p><span>That is, everything to the left of the head is one stack, everything to the right, reversed, is the</span>
<span>other.  Here, moving the reading head left or right corresponds to popping a value off one stack and</span>
<span>pushing it onto another.</span></p>
<p><span>So, an equivalent-in-power definition would be to say that an TM is an FSM endowed with two</span>
<span>stacks.</span></p>
<p><span>This of course creates an obvious question: is an FSM with just one stack a thing? Yes! It would be</span>
<span>called a pushdown automaton, and it would correspond to context-free languages. But that</span>’<span>s beyond</span>
<span>the scope of this post!</span></p>
<p><span>There</span>’<span>s yet another way to look at the tape, or the pair of stacks, if the set of symbols is 0 and</span>
<span>1. You could say that a stack is just a number! So, something like</span>
<code>[1, 0, 1, 1]</code>
<span>will be</span>
<span><code>1 + 2 + 8 = 11</code><span>.</span></span>
<span>Looking at the top of the stack is </span><code>stack % 2</code><span>, removing item from the stack is </span><code>stack / 2</code><span> and</span>
<span>pushing x onto the stack is </span><code>stack * 2 + x</code><span>. We won</span>’<span>t need this </span><em><span>right</span></em><span> now, so just hold onto ths</span>
<span>for a brief moment.</span></p>
</section>
<section id="Turing-Machine-Limits">

    <h2>
    <a href="#Turing-Machine-Limits"><span>Turing Machine: Limits</span> </a>
    </h2>
<p><span>Ok, so we have some idea about the lower bound for a power of a Turing Machine </span>—<span> FSMs are strictly</span>
<span>less expressive. What about the opposite direction? Is there some computation that a Turing Machine</span>
<span>is incapable of doing?</span></p>
<p><span>Yes! Let</span>’<span>s construct a function which maps natural numbers to natural numbers, which can</span>’<span>t be</span>
<span>implemented by a Turing Machine. Recall that we can encode an arbitrary Turing Machine as text. That</span>
<span>means that we can actually enumerate all possible Turing Machines, and write then in a giant line,</span>
<span>from the most simple Turing Machine to more complex ones:</span></p>

<figure>


<pre><code><span>TM_0</span>
<span>TM_1</span>
<span>TM_2</span>
<span>...</span>
<span>TM_326</span>
<span>...</span></code></pre>

</figure>
<p><span>This is of course going to be an infinite list.</span></p>
<p><span>Now, let</span>’<span>s see how TM0 behaves on input </span><code>0</code><span>: it either prints something, or doesn</span>’<span>t terminate. Then,</span>
<span>note how TM1 behaves on input </span><code>1</code><span>, and generalizing, create function </span><code>f</code><span> that behaves as the nth TM</span>
<span>on input </span><code>n</code><span>. It might look something like this:</span></p>

<figure>


<pre><code><span>f(0) = 0</span>
<span>f(1) = 111011</span>
<span>f(2) = doesn&#39;t terminate</span>
<span>f(3) = 0</span>
<span>f(4) = 101</span>
<span>...</span></code></pre>

</figure>
<p><span>Now, let</span>’<span>s construct function </span><code>g</code><span> which is maximally diffed from </span><code>f</code><span>: where </span><code>f</code><span> gives </span><code>0</code><span>, </span><code>g</code><span> will</span>
<span>return </span><code>1</code><span>, and it will return </span><code>0</code><span> in all other cases:</span></p>

<figure>


<pre><code><span>g(0) = 1</span>
<span>g(1) = 0</span>
<span>g(2) = 0</span>
<span>g(3) = 1</span>
<span>g(4) = 0</span>
<span>...</span></code></pre>

</figure>
<p><span>There isn</span>’<span>t a Turing machine that computes </span><code>g</code><span>. For suppose there is. Then, it exists in our list of</span>
<span>all Turing Machines somewhere. Let</span>’<span>s say it is TM1000064. So, if we feed </span><code>0</code><span> to it, it will return</span>
<code>g(0)</code><span>, which is </span><code>1</code><span>, which is different from </span><code>f(0)</code><span>. And the same holds for </span><code>1</code><span>, and </span><code>2</code><span>, and </span><code>3</code><span>.</span>
<span>But once we get to </span><code>g(1000064)</code><span>, we are in trouble, because, by the definition of </span><code>g</code><span>, </span><code>g(1000064)</code>
<span>is different from what is computed by TM1000064! So such a machine is impossible.</span></p>
<p><span>Those math savvy might express this more succinctly </span>—<span> there</span>’<span>s a countably-infinite number of</span>
<span>Turing Machines, and an uncountably-infinite number of functions. So there </span><em><span>must</span></em><span> be some functions</span>
<span>which do not have a corresponding Turing Machine. It is the same proof </span>—<span> the diagonalization</span>
<span>argument is hiding in the claim that the set of all functions is an uncountable set.</span></p>
<p><span>But this is super weird and abstract. Let</span>’<span>s rather come up with some very specific problem which</span>
<span>isn</span>’<span>t solvable by a Turing Machine. The halting problem: given source code for a Turing Machine and</span>
<span>its input, determine if the machine halts on this input eventually.</span></p>
<p><span>As we have waved our hands sufficiently vigorously to establish that Python and Turing Machines have</span>
<span>equivalent computational power, I am going to try to solve this in Python:</span></p>

<figure>


<pre><code><span><span>def</span> <span>halts</span>(<span>program_source_code: <span>str</span>, program_input: <span>str</span></span>) -&gt; Bool:</span>
<span>    </span>
<span>    </span>
<span>    <span>return</span> the_answer</span>
<span></span>
<span>raw_input = <span>input</span>()</span>
<span>[program_source_code, program_input] = parse(raw_input)</span>
<span><span>print</span>(<span>&#34;Yes&#34;</span> <span>if</span> halts(program_source_code, program_input) <span>else</span> <span>&#34;No&#34;</span>)</span></code></pre>

</figure>
<p><span>Now, I will do a weird thing and start asking whether a program termintates, if it is fed its own</span>
<span>source code, in a reverse-quine of sorts:</span></p>

<figure>


<pre><code><span><span>def</span> <span>halts_on_self</span>(<span>program_source_code: <span>str</span></span>) -&gt; Bool:</span>
<span>    program_input = program_source_code</span>
<span>    <span>return</span> halts(program_source_code, program_input)</span></code></pre>

</figure>
<p><span>and finally I construct this weird beast of a program:</span></p>

<figure>


<pre><code><span><span>def</span> <span>halts</span>(<span>program_source_code: <span>str</span>, program_input: <span>str</span></span>) -&gt; Bool:</span>
<span>    </span>
<span>    <span>return</span> the_answer</span>
<span></span>
<span><span>def</span> <span>halts_on_self</span>(<span>program_source_code: <span>str</span></span>) -&gt; Bool:</span>
<span>    program_input = program_source_code</span>
<span>    <span>return</span> halts(program_source_code, program_input)</span>
<span></span>
<span><span>def</span> <span>weird</span>(<span>program_input</span>):</span>
<span>    <span>if</span> halts_on_self(program_input):</span>
<span>        <span>while</span> <span>True</span>:</span>
<span>            <span>pass</span></span>
<span></span>
<span>weird(<span>input</span>())</span></code></pre>

</figure>
<p><span>To make this even worse, I</span>’<span>ll feed the text of this </span><code>weird</code><span> program to itself. Does it terminate</span>
<span>with this input? Well, if it terminates, and if our </span><code>halts</code><span> function is implemented correctly, then</span>
<span>the </span><code>halts_on_self(program_input)</code><span> invocation above returns </span><code>True</code><span>. But then we enter the infinite</span>
<span>loop and don</span>’<span>t actually terminate.</span></p>
<p><span>Hence, it must be the case that </span><code>weird</code><span> does not terminate when self-applied. But then</span>
<code>halts_on_self</code><span> returns </span><code>False</code><span>, and it should terminate. So we get a contradiction both ways. Which</span>
<span>necessary means that either our </span><code>halts</code><span> sometimes returns a straight-up incorrect answer, or that it</span>
<span>sometimes does not terminate.</span></p>
<p><span>So this is the flip side of Turing Machine</span>’<span>s power </span>—<span> it is so powerful that it becomes impossible</span>
<span>to tell whether it</span>’<span>ll terminate or not!</span></p>
<p><span>It actually gets much worse, because this result can be generalized to an unreasonable degree!</span>
<span>In general, there</span>’<span>s very little we can say about arbitrary programs.</span></p>
<p><span>We can easily check syntactic properties (is the program text shorter than 4 kilobytes?), but they</span>
<span>are, in some sense, not very interesting, as they depend a lot on how exactly one writes a program.</span>
<span>It would be much more interesting to check some refactoring-invariant properties, which hold when</span>
<span>you change the text of the program, but leave the behavior intact. Indeed, </span>“<span>does this change</span>
<span>preserve behavior?</span>”<span> would be one very useful property to check!</span></p>
<p><span>So let</span>’<span>s define two TMs to be equivalent, if they have identical behavior. That is, for each</span>
<span>specific input, either both machines don</span>’<span>t terminate, or they both halt, and give identical results.</span></p>
<p><span>Then, our refactoring-invariant properties are, by definition, properties that hold (or do not hold)</span>
<span>for the entire classes of equivalence of TMs.</span></p>
<p><span>And a somewhat depressing result here is that there are no non-trivial refactoring-invariant</span>
<span>properties that you can algorithmically check.</span></p>
<p><span>Suppose we have some magic TM, called P, which checks such a property. Let</span>’<span>s show that, using P, we can</span>
<span>solve the problem we know we can not solve </span>—<span> the halting problem.</span></p>
<p><span>Consider a Turing Machine that is just an infinite loop and never terminates, M1. P might or might</span>
<span>not hold for it. But, because P is not-trivial (it holds for some machines and doesn</span>’<span>t hold for some</span>
<span>machines), there</span>’<span>s some different machine M2 which differs from M1 with respect to P. That is,</span>
<code>P(M1) xor P(M2)</code><span> holds.</span></p>
<p><span>Let</span>’<span>s use these M1 and M2 to figure out where a given machine M halts on input I. Using Universal</span>
<span>Turing Machine (interpreter), we can construct a new machine, M12 that just runs M on input I, then</span>
<span>erases the contents of the tape and runs M2. Now, if M halts on I, then the resulting machine M12 is</span>
<span>behaviorally-equivalent to M2. If M doesn</span>’<span>t halt on I, then the result is equivalent to the infinite</span>
<span>loop program, M1. Or, in pseudo-code:</span></p>

<figure>


<pre><code><span><span>def</span> <span>M1</span>(<span><span>input</span></span>):</span>
<span>    <span>while</span> <span>True</span>:</span>
<span>        <span>pass</span></span>
<span></span>
<span><span>def</span> <span>M2</span>(<span><span>input</span></span>):</span>
<span>    </span>
<span>    </span>
<span></span>
<span><span>assert</span>(P(M1) != P(M2))</span>
<span></span>
<span><span>def</span> <span>halts</span>(<span>M, I</span>):</span>
<span>    <span>def</span> <span>M12</span>(<span><span>input</span></span>):</span>
<span>        M(I) </span>
<span>        <span>return</span> M2(<span>input</span>)</span>
<span></span>
<span>    <span>return</span> P(M12) == P(M2)</span></code></pre>

</figure>
<p><span>This is pretty bad and depressing </span>—<span> can</span>’<span>t learn anything meaningful about an arbitrary Turing</span>
<span>Machine! So let</span>’<span>s finally get to the actual topic of today</span>’<span>s post:</span></p>
</section>
<section id="Primitive-Recursive-Functions">

    <h2>
    <a href="#Primitive-Recursive-Functions"><span>Primitive Recursive Functions</span> </a>
    </h2>
<p><span>This is going to be another computational device, like FSMs and TMs. Like FSM, it</span>’<span>s going to be a</span>
<span>nice, always terminating, non-Turing complete device. But it would turn out to have quite a bit of</span>
<span>power of a full Turing Machine!</span></p>
<p><span>However, unlike both TMs and FSMs, </span><dfn><span>Primitive Recursive Functions</span></dfn><span> are defined directly as</span>
<span>functions which take a tuple of natural numbers and return a natural number. The two simplest ones</span>
<span>are </span><code>zero</code><span> (that is, zero-arity function that returns </span><code>0</code><span>) and </span><code>succ</code><span> </span>—<span> an unary function that</span>
<span>just adds 1. Everything else is going to get constructed out of these two:</span></p>

<figure>


<pre><code><span>zero = 0</span>
<span>succ(x) = x + 1</span></code></pre>

</figure>
<p><span>One way we are allowed to combine these functions is by composition. So we can get all the constants</span>
<span>right of the bat:</span></p>

<figure>


<pre><code><span>succ(zero) = 1</span>
<span>succ(succ(zero)) = 2</span>
<span>succ(succ(succ(zero))) = 3</span></code></pre>

</figure>
<p><span>We aren</span>’<span>t going to get allowed to use general recursion (because it can trivially non-terminate),</span>
<span>but we do get to use a restricted form of C-style loop. It is a bit fiddly to defile formally! The</span>
<span>overall shape is </span><span><code>LOOP(init, f, n)</code><span>.</span></span></p>
<p><span>Here, </span><code>init</code><span> and </span><code>n</code><span> are numbers </span>—<span> initial value of accumulator and the total number of</span>
<span>iterations. The </span><code>f</code><span> is an unary function that specifies the loop body </span>–<span> it takes the current value</span>
<span>of the accumulator and returns the new value. So</span></p>

<figure>


<pre><code><span>LOOP(init, f, 0) = init</span>
<span>LOOP(init, f, 1) = f(init)</span>
<span>LOOP(init, f, 2) = f(f(init))</span>
<span>LOOP(init, f, 3) = f(f(f(init)))</span></code></pre>

</figure>
<p><span>While this is </span><em><span>similar</span></em><span> to a C-style loop, the crucial difference here is that the total number of</span>
<span>iterations </span><code>n</code><span> is fixed up-front. There</span>’<span>s no way to mutate the loop counter in the loop body.</span></p>
<p><span>This allows us to define addition:</span></p>

<figure>


<pre><code><span>add(x, y) = LOOP(x, succ, y)</span></code></pre>

</figure>
<p><span>Multiplication is trickier. Conceptually, to multiply </span><code>x</code><span> and </span><code>y</code><span>, we want to </span><code>LOOP</code><span> from zero, and</span>
<span>repeat </span>“<span>add </span><code>x</code>”<span> </span><code>y</code><span> times. The problem here is that we can</span>’<span>t write </span>“<span>add </span><code>x</code>”<span> function yet</span></p>

<figure>


<pre><code><span># Doesn&#39;t work, add is a binary function!</span>
<span>mul(x, y) = LOOP(0, add, y)</span></code></pre>

</figure>

<figure>


<pre><code><span># Doesn&#39;t work either, no x in scope!</span>
<span>add_x v = add(x, v)</span>
<span>mul(x, y) = LOOP(0, add_x, y)</span></code></pre>

</figure>
<p><span>One way around this is to defile </span><code>LOOP</code><span> as a family of operators, which can pass extra arguments to</span>
<span>the iteration function:</span></p>

<figure>


<pre><code><span>LOOP0(init, f, 2) = f(f(init))</span>
<span>LOOP1(c1, init, f, 2) = f(c1, f(c1, init))</span>
<span>LOOP2(c1, c2, init, f, 2) = f(c1, c2, f(c1, c2, init))</span></code></pre>

</figure>
<p><span>That is, </span><code>LOOP_N</code><span> takes extra </span><code>n</code><span> arguments, and passes them through to any invocation of the body</span>
<span>function. To express this idea a little bit more succinctly, let</span>’<span>s just allow to partially  apply</span>
<span>the second argument of </span><code>LOOP</code><span>. That is:</span></p>
<ul>
<li>
<span>All our functions are going to be first order. All arguments are numbers, the result is a number.</span>
<span>There aren</span>’<span>t high order functions, there aren</span>’<span>t closures.</span>
</li>
<li>
<span>The </span><code>LOOP</code><span> is not a function in our language </span>—<span> its a builtin operator, a keyword. So, for</span>
<span>convenience, we allow to pass partially applied functions to it. But semantically this is</span>
<span>equivalent to just passing in extra argumennts on each iteration.</span>
</li>
</ul>
<p><span>Which finally allows us to write</span></p>

<figure>


<pre><code><span>mul(x, y) = LOOP(0, add x, y)</span></code></pre>

</figure>
<p><span>Ok, so that</span>’<span>s progress </span>—<span> we made something as complicated as multiplication, and we still are in</span>
<span>the guaranteed-to-terminate land. Because each loop has a fixed number of iteration, everything</span>
<span>eventually finishes.</span></p>
<p><span>We can go on and define x</span><sup><span>y</span></sup><span>:</span></p>

<figure>


<pre><code><span>pow(x, y) = LOOP(1, mul x, y)</span></code></pre>

</figure>
<p><span>And this in turn allows to define a couple of concerning fast growing functions:</span></p>

<figure>


<pre><code><span>pow_2(n) = pow(2, n)</span>
<span>pow_2_2(n) = pow_2(pow_2(n))</span></code></pre>

</figure>
<p><span>That</span>’<span>s fun, but to do some programming, we</span>’<span>ll need an </span><code>if</code><span>. We</span>’<span>ll get to it, but first we</span>’<span>ll need</span>
<span>some boolean operations. We can encode </span><code>false</code><span> as </span><code>0</code><span> and </span><code>true</code><span> as </span><code>1</code><span>. Then</span></p>

<figure>


<pre><code><span>and(x, y) = mul(x, y)</span></code></pre>

</figure>
<p><span>But </span><code>or</code><span> creates a problem: we</span>’<span>ll need a subtraction.</span></p>

<figure>


<pre><code><span>or(x, y) = sub(</span>
<span>  add(x, y),</span>
<span>  mul(x, y),</span>
<span>)</span></code></pre>

</figure>
<p><span>Defining </span><code>sub</code><span> is tricky, due to two problems:</span></p>
<p><span>First, we only have natural numbers, no negatives. This one is easy to solve </span>—<span> we</span>’<span>ll just define</span>
<span>subtraction to saturate.</span></p>
<p><span>The second problem is more severe </span>—<span> I think we actually can</span>’<span>t express subtraction given the set of</span>
<span>allowable operations so far. That is because all our operations are monotonic </span>—<span> the result is</span>
<span>never less than the arguments. One way to solve this problem is to defile the LOOP in such a way</span>
<span>that the body function also gets passed a second argument </span>—<span> the current iteration. So, if you</span>
<span>iterate up to </span><code>n</code><span>, the last iteration will observe </span><code>n - 1</code><span>, and that would be the non-monotonic</span>
<span>operation that creates subtraction. But that seems somewhat inelegant to me, so instead I will just</span>
<span>add a </span><code>pred</code><span> function to the basis, and use that to add loop counters to our iterations.</span></p>

<figure>


<pre><code><span>pred(0) = 0 # saturate</span>
<span>pred(1) = 0</span>
<span>pred(2) = 1</span>
<span>...</span></code></pre>

</figure>
<p><span>Now we can say:</span></p>

<figure>


<pre><code><span>sub(x, y) = LOOP(x, pred, y)</span>
<span></span>
<span>and(x, y) = mul(x, y)</span>
<span>or(x, y) = sub(</span>
<span>  add(x, y),</span>
<span>  mul(x, y)</span>
<span>)</span>
<span>not(x) = sub(1, x)</span>
<span></span>
<span>if(cond, a, b) = add(</span>
<span>  mul(a, cond),</span>
<span>  mul(b, not(cond)),</span>
<span>)</span></code></pre>

</figure>
<p><span>And now we can do a bunch of comparison operators:</span></p>

<figure>


<pre><code><span>is_zero(x) = sub(1, x)</span>
<span></span>
<span># x &gt;= y</span>
<span>ge(x, y) = is_zero(sub(y, x))</span>
<span></span>
<span># x == y</span>
<span>eq(x, y) = and(ge(x, y), ge(y, x))</span>
<span></span>
<span># x &gt; y</span>
<span>gt(x, y) = and(ge(x, y), not(eq(x, y)))</span>
<span></span>
<span># x &lt; y</span>
<span>lt(x, y) = gt(y, x)</span></code></pre>

</figure>
<p><span>With that we could implement modulus. To compute </span><code>x % m</code><span> we will start with </span><code>x</code><span>, and will be</span>
<span>subtracting </span><code>m</code><span> until we get a number smaller than </span><code>m</code><span>. We</span>’<span>ll need at most </span><code>x</code><span> iterations for that.</span></p>
<p><span>In pseudo-code:</span></p>

<figure>


<pre><code><span><span>def</span> <span>mod</span>(<span>x, m</span>):</span>
<span>  current = x</span>
<span></span>
<span>  <span>for</span> _ <span>in</span> <span>0.</span>.x:</span>
<span>    <span>if</span> current &lt; m:</span>
<span>      current = current</span>
<span>    <span>else</span>:</span>
<span>      current = current - m</span>
<span></span>
<span>  <span>return</span> current</span></code></pre>

</figure>
<p><span>And as a bona fide PRF:</span></p>

<figure>


<pre><code><span>mod_iter(m, x) = if(</span>
<span>  lt(x, m),</span>
<span>  x,        # then</span>
<span>  sub(x, m) # else</span>
<span>)</span>
<span>mod(x, m) = LOOP(x, mod_iter m, x)</span></code></pre>

</figure>
<p><span>That</span>’<span>s a curious structure </span>—<span> rather than computing the modulo directly, we essentially search for</span>
<span>it using trial and error, and relying on the fact that the search has a clear upper bound.</span></p>
<p><span>Division can be done similarly: to divide x by y, start with 0, and then repeatedly add one to the</span>
<span>accumulator until the product of accumulator and y exceeds x:</span></p>

<figure>


<pre><code><span>div_iter x y acc = if(</span>
<span>  le(mul(succ(acc), y), y),</span>
<span>  succ(acc), # then</span>
<span>  acc        # else</span>
<span>)</span>
<span>div(x, y) = LOOP(0, div_iter x y, x)</span></code></pre>

</figure>
<p><span>This really starts looking like programming! One thing we are currently missing are data structures.</span>
<span>While our functions take multiple arguments, they only return one number. But it</span>’<span>s easy enough to</span>
<span>pack two numbers into one: to represent an </span><code>(a, b)</code><span> pair, we</span>’<span>ll use 2</span><sup><span>a</span></sup><span> 3</span><sup><span>b</span></sup><span> number:</span></p>

<figure>


<pre><code><span>mk_pair(a, b) = mul(pow(2, a), pow(3, b))</span></code></pre>

</figure>
<p><span>To deconstruct such a pair into its first and second components, we need to find the maximum power</span>
<span>of 2 or 3 that divides our number. Which is exactly the same shape we used to implement </span><code>div</code><span>:</span></p>

<figure>


<pre><code><span>max_factor_iter p m acc = if(</span>
<span>  is_zero(mod(p, pow(m, succ(acc)))),</span>
<span>  succ(acc), # then</span>
<span>  acc,       # else</span>
<span>)</span>
<span>max_factor(p, m) = LOOP(0, max_factor_iter p m, p)</span>
<span></span>
<span>fst(p) = max_factor(p, 2)</span>
<span>snd(p) = max_factor(p, 3)</span></code></pre>

</figure>
<p><span>Here again we use the fact that the maximal power of two that divides </span><code>p</code><span> is not larger than </span><code>p</code>
<span>itself, so we can over-estimate the number of iterations we</span>’<span>ll need as </span><code>p</code><span>.</span></p>
<p><span>Using this pair construction, we can finally add a loop counter to our </span><code>LOOP</code><span> construct. To track</span>
<span>the counter, we pack it as a pair with the accumulator:</span></p>

<figure>


<pre><code><span>LOOP(mk_pair(init, 0), f, n)</span></code></pre>

</figure>
<p><span>And then inside f, we first unpack that pair into accumulator and counter, pass them to actual loop</span>
<span>iteration, and then pack the result again, incrementing the counter:</span></p>

<figure>


<pre><code><span>f acc = mk_pair(</span>
<span>  g(fst(acc), snd(acc)),</span>
<span>  succ(snd(acc)),</span>
<span>)</span></code></pre>

</figure>
<p><span>Ok, so we have achieved something remarkable: while we are writing terminating-by-construction</span>
<span>programs, which are definitely not Turing complete, we have constructed basic programming staples,</span>
<span>like boolean logic and data structures, and we have also build some rather complicated mathematical</span>
<span>functions, like </span><span><span>2</span><sup><span>2</span><sup><span>N</span></sup></sup><span>.</span></span></p>
<p><span>We could try to further enrich our little primitive recursive kingdom by adding more and more</span>
<span>functions on the ad hoc basis, but let</span>’<span>s try to be really ambitious and go for the main prize </span>—
<span>simulating Turing Machines.</span></p>
<p><span>We know that we will fail: Turing machine can enter the infinite loop, but PRF necessary terminates.</span>
<span>That means, that, if PRF were able to simulate an arbitrary TM, it would have to say after certain</span>
<span>finite amount of steps that </span>“<span>this TM doesn</span>’<span>t terminate</span>”<span>.  And, while we didn</span>’<span>t do this, it</span>’<span>s easy to</span>
<span>see that you </span><em><span>could</span></em><span> simulate the other way around and implement PRFs in a TM. But that would give</span>
<span>us an TM algorithm to decide if an arbitrary TM halts, which we know doesn</span>’<span>t exist.</span></p>
<p><span>So, this is hopeless! But we might still be able to learn something from failing.</span></p>
<p><span>Ok! So let</span>’<span>s start with a configuration of a TM which we somehow need to encode into a single</span>
<span>number. First, we need the state variable proper (Q0, Q1, etc), which seems easy enough to represent</span>
<span>with a number. Then, we need a tape and a position of the reading head. Recall how we used a pair of</span>
<span>stacks to represent exactly the tape and the position. And recall that we can look at a stack of</span>
<span>zeros and ones as a number in binary form, where push and pop operations are implemented using </span><code>%</code><span>,</span>
<code>*</code><span>, and </span><code>/</code><span> </span>—<span> exactly the operations we already can do. So, our configuration is just three</span>
<span>numbers: </span><span><code>(S, stack1, stack2)</code><span>.</span></span></p>
<p><span>And, using 2</span><sup><span>a</span></sup><span>3</span><sup><span>b</span></sup><span>5</span><sup><span>c</span></sup><span> trick, we can pack this triple just into a single number. But that means we</span>
<span>could directly encode a single step of a Turing Machine:</span></p>

<figure>


<pre><code><span>single_step(config) = if(</span>
<span>  # if the state is Q0 ...</span>
<span>  eq(fst(config), 0)</span>
<span></span>
<span>  # and the symbol at the top of left stack is 0</span>
<span>  if(is_zero(mod(snd(config), 2))</span>
<span>    mk_triple(</span>
<span>      1,                    # move to state Q1</span>
<span>      div(snd(config), 2),  # pop value from the left stack</span>
<span>      mul(trd(config), 2),  # push zero onto the right stack</span>
<span>    ),</span>
<span>    ... # Handle symbol 1 in state Q1</span>
<span>  )</span>
<span>  # if the state is Q1 ...</span>
<span>  if(eq(fst(config), 1)</span>
<span>    ...</span>
<span>  )</span>
<span>)</span></code></pre>

</figure>
<p><span>And now we could plug that into our </span><code>LOOP</code><span> to simulate Turing Machine running for N steps:</span></p>

<figure>


<pre><code><span>n_steps initial_config n =</span>
<span>  LOOP(initial_config, single_step, n)</span></code></pre>

</figure>
<p><span>The catch of course that we can</span>’<span>t know the </span><code>N</code><span> that</span>’<span>s going to be enough. But we can have a very</span>
<span>good guess! We could do something like this:</span></p>

<figure>


<pre><code><span>hopefully_enough_steps initial_config =</span>
<span>  LOOP(initial_config, single_step, pow_2_2(initial_config))</span></code></pre>

</figure>
<p><span>That is, run for some large tower of exponents of the initial state. Which would be plenty for</span>
<span>normal algorithms, which are usually 2</span><sup><span>N</span></sup><span> at worst!</span></p>
<p><span>Or, generalizing:</span></p>

<figure>
<blockquote><p><span>If a TM has a runtime which is bounded by some primitive-recursive function, then the entire</span>
<span>TM can be replaced with a PRF. Be advised that PRFs can grow </span><em><span>really</span></em><span> fast.</span></p>
</blockquote>

</figure>
<p><span>Which is the headline result we have set out to prove!</span></p>
</section>
<section id="Primitive-Recursive-Functions-Limit">

    <h2>
    <a href="#Primitive-Recursive-Functions-Limit"><span>Primitive Recursive Functions: Limit</span> </a>
    </h2>
<p><span>It might seem that non-termination is the only principle obstacle. That anything that terminates at</span>
<span>all has to be implementable as a PRF. Alas, that</span>’<span>s not so. Let</span>’<span>s go and construct a function that is</span>
<span>surmountable by a TM, but is out of reach of PRFs.</span></p>
<p><span>We will combine the ideas of the impossibility proofs for FSMs (noting that if a function is</span>
<span>computed by some machine, that machine has a specific finite size) and TMs (diagonalization).</span></p>
<p><span>So, suppose we have some function </span><code>f</code><span> that can</span>’<span>t be computed by a PRF. How would we go about proving</span>
<span>that? Well, we</span>’<span>d start with </span>“<span>suppose that we have a PRF P that computes </span><code>f</code>”<span>. And then we could</span>
<span>notice that P would save some finite size. If you look at it abstractly, the P is its syntax tree,</span>
<span>with lots of </span><code>LOOP</code><span> construct, but it always boils down to some </span><code>succ</code><span>s and </span><code>zero</code><span>s at the leaves.</span>
<span>Let</span>’<span>s say that the depth of P is </span><code>d</code><span>.</span></p>
<p><span>And, actually, if you look at it, there are only finite number of PRFs with depth at most </span><code>d</code><span>. Some</span>
<span>of them describe pretty fast growing functions. But probably there</span>’<span>s a limit to how fast a function</span>
<span>can grow, given that it is computed by a PRF of size </span><code>d</code><span>. Or, to use a concrete example: we have</span>
<span>constructed a PRF of depth 5 that computes two to the power of two to the power of N. Probably if we</span>
<span>were smarter, we could have squeezed a couple more levels into that tower of exponents. But</span>
<span>intuitively it seems that if you build a tower of, say, 10 exponents, that would grow faster than</span>
<em><span>any</span></em><span> PRF of depth </span><code>5</code><span>. And that this generalizes </span>—<span> for any fixed depth, there</span>’<span>s a high-enough</span>
<span>tower of exponents that grows faster than any PRF with that depth.</span></p>
<p><span>So we could conceivably build an </span><code>f</code><span> that defeats our </span><code>d</code><span>-deep P. But that</span>’<span>s not quite a victory</span>
<span>yet: maybe that </span><code>f</code><span> is feasible for </span><code>d+2</code><span>-deep PRF! So here we</span>’<span>ll additionally apply</span>
<span>diagonalization: for each depth, we</span>’<span>ll build it</span>’<span>s own depth-specific nemesis </span><code>f_d</code><span>. And then we</span>’<span>ll</span>
<span>define our overall function as</span></p>

<figure>


<pre><code><span>a(n) = f_n(n)</span></code></pre>

</figure>
<p><span>So, for </span><code>n</code><span> large enough it</span>’<span>ll grow faster than a PRF with any fixed depth.</span></p>
<p><span>So that</span>’<span>s the general plan, the rest of the own is basically just calculating the upper bound on the</span>
<span>growth of a PRF of depth </span><code>d</code><span>.</span></p>
<p><span>One technical difficulty here is that PRFs tend to have different artities:</span></p>

<figure>


<pre><code><span>f(x, y)</span>
<span>g(x, y, z, t)</span>
<span>h(x)</span></code></pre>

</figure>
<p><span>Ideally, we</span>’<span>d use just one upper bound of them all. So we</span>’<span>ll be looking for an upper bound of the</span>
<span>following form:</span></p>

<figure>


<pre><code><span>f(x, y, z, t) &lt;= A_d(max(x, y, z, t))</span></code></pre>

</figure>
<p><span>That is:</span></p>
<ul>
<li>
<span>Compute the depth of </span><code>f</code><span>, </span><code>d</code><span>.</span>
</li>
<li>
<span>Compute the largest of its arguments.</span>
</li>
<li>
<span>And plug that into unary function for depth </span><code>d</code><span>.</span>
</li>
</ul>
<p><span>Let</span>’<span>s start with </span><code>d=1</code><span>. We have only primitive functions on this level, </span><code>succ</code><span>, </span><code>zero</code><span>, and </span><code>pred</code><span>,</span>
<span>so we could say that</span></p>

<figure>


<pre><code><span>A_1(x) = x + 1</span></code></pre>

</figure>
<p><span>Now, let</span>’<span>s handle arbitrary other depth </span><code>d + 1</code><span>. In that case, our function is non-primitive, so at</span>
<span>the root of the syntax tree we have either a composition or a </span><code>LOOP</code><span>.</span></p>
<p><span>Composition would look like this:</span></p>

<figure>


<pre><code><span>f(x, y, z, ...) = g(</span>
<span>  h1(x, y, z, ...),</span>
<span>  h2(x, y, z, ...),</span>
<span>  h3(x, y, z, ...),</span>
<span>)</span></code></pre>

</figure>
<p><span>where </span><code>g</code><span> and </span><code>h_n</code><span> are </span><code>d</code><span> deep and the resulting </span><code>f</code><span> is </span><code>d+1</code><span> deeep. We can immediately estimate</span>
<span>the </span><code>h_n</code><span> then:</span></p>

<figure>


<pre><code><span>f(args...) &lt;= g(</span>
<span>  A_d(maxarg),</span>
<span>  A_d(maxarg),</span>
<span>  A_d(maxarg),</span>
<span>  ...</span>
<span>)</span></code></pre>

</figure>
<p><span>In this somewhat loose notation, </span><code>args...</code><span> stands for a tuple of arguments, and </span><code>maxarg</code><span> stands for</span>
<span>the largest one.</span></p>
<p><span>And then we could use the same estimate for </span><code>g</code><span>:</span></p>

<figure>


<pre><code><span>f(args...) &lt;= A_d(A_d(maxarg))</span></code></pre>

</figure>
<p><span>This is super high-order, so let</span>’<span>s do a concrete example for depth-2 two-argument function which</span>
<span>starts with a composition:</span></p>

<figure>


<pre><code><span>f(x, y) &lt;= A_1(A_1(max(x, y)))</span>
<span>         = A_1(max(x, y) + 1)</span>
<span>         = max(x, y) + 2</span></code></pre>

</figure>
<p><span>This sounds legit: if we don</span>’<span>t use LOOP, then </span><code>f(x, y)</code><span> is either </span><code>succ(succ(x))</code><span> or </span><code>succ(succ(y))</code>
<span>so </span><code>max(x, y) + 2</code><span> indeed is the bound!</span></p>
<p><span>Ok, now the fun case! If the top-level node is a </span><code>LOOP</code><span>, then we have</span></p>

<figure>


<pre><code><span>f(args...) = LOOP(</span>
<span>  g(args...),</span>
<span>  h(args...),</span>
<span>  t(args...),</span>
<span>)</span></code></pre>

</figure>
<p><span>This sounds complicated to estimate, especially due to that last </span><code>t(args...)</code><span> argument, which is the</span>
<span>number of iterations. So we</span>’<span>ll be cowards and </span><em><span>won</span>’<span>t</span></em><span> actually try to estimate this case. Instead,</span>
<span>we will require that our PRF is written in a simplified form, where the first and the last arguments</span>
<span>to </span><code>LOOP</code><span> are simple.</span></p>
<p><span>So, if your PRF looks like</span></p>

<figure>


<pre><code><span>f(x, y) = LOOP(x + y, mul, pow2(x))</span></code></pre>

</figure>
<p><span>you are required to re-write it first as</span></p>

<figure>


<pre><code><span>helper(u, v) = LOOP(u, mul, v)</span>
<span>f(x, y) = helper(x + y, pow2(x))</span></code></pre>

</figure>
<p><span>So now we only have to deal with this:</span></p>

<figure>


<pre><code><span>f(args...) = LOOP(</span>
<span>  arg,</span>
<span>  g(args...),</span>
<span>  arg,</span>
<span>)</span></code></pre>

</figure>
<p><code>f</code><span> has depth </span><code>d+1</code><span>, </span><code>g</code><span> has depth </span><code>d</code><span>.</span></p>
<p><span>On the first iteration, we</span>’<span>ll call </span><code>g(args..., arg)</code><span>, which we can estimate as </span><code>A_d(maxarg)</code><span>. That</span>
<span>is, </span><code>g</code><span> does get an </span><em><span>extra</span></em><span> argument, but it is one of the original arguments of </span><code>f</code><span>, and we are</span>
<span>looking at the maximum argument anyway, so it doesn</span>’<span>t matter.</span></p>
<p><span>On the second iteration, we are going to call</span>
<code>g(args..., prev_iteration)</code>
<span>which we can estimate as</span>
<span><code>A_d(max(maxarg, prev_iteration))</code><span>.</span></span></p>
<p><span>Now we plug our estimation for the first iteration:</span></p>

<figure>


<pre><code><span>g(args..., prev_iteration)</span>
<span>  &lt;= A_d(max(maxarg, prev_iteration))</span>
<span>  &lt;= A_d(max(maxarg, A_d(maxarg)))</span>
<span>  =  A_d(A_d(maxarg))</span></code></pre>

</figure>
<p><span>That is, the estimate for the first iteration is </span><code>A_d(maxarg)</code><span>. The estimation for the second</span>
<span>iteration adds one more layer: </span><code>A_d(A_d(maxarg))</code><span>. For the third iteration we</span>’<span>ll get</span>
<span><code>A_d(A_d(A_d(maxarg)))</code><span>.</span></span></p>
<p><span>So the overall thing is going to be smaller than </span><code>A_d</code><span> iteratively applied to itself some number of</span>
<span>times, where </span>“<span>some number</span>”<span> is one of the </span><code>f</code><span> original arguments. But no harm</span>’<span>s done if we iterate up</span>
<span>to </span><code>maxarg</code><span>.</span></p>
<p><span>As a sanity check, the worst depth-2 function constructed with iteration is probably</span></p>

<figure>


<pre><code><span>f(x, y) = LOOP(x, succ, y)</span></code></pre>

</figure>
<p><span>which is </span><code>x + y</code><span>. And our estimate gives </span><code>x + 1</code><span> applied </span><code>maxarg</code><span> times to </span><code>maxarg</code><span>, which is </span><code>2 *
maxarg</code><span>, which is indeed the correct upper bound!</span></p>
<p><span>Combining everything together, we have:</span></p>

<figure>


<pre><code><span>A_1(x) = x + 1</span>
<span></span>
<span>f(args...) &lt;= max(</span>
<span>  A_d(A_d(maxarg)),               # composition case</span>
<span>  A_d(A_d(A_d(... A_d(maxarg)))), # LOOP case,</span>
<span>   &lt;-    maxarg A&#39;s         -&gt;</span>
<span>)</span></code></pre>

</figure>
<p><span>That </span><code>max</code><span> there is significant </span>—<span> although it seems like the second line, with </span><code>maxarg</code>
<span>applications, is, always going to be longer, </span><code>maxarg</code><span>, in fact, could be as small as zero. But we</span>
<span>can take </span><code>maxarg + 2</code><span> repetitions to fix this:</span></p>

<figure>


<pre><code><span>f(args...) &lt;=</span>
<span>  A_d(A_d(A_d(... A_d(maxarg)))),</span>
<span>  &lt;-    maxarg + 2 A&#39;s         -&gt;</span></code></pre>

</figure>
<p><span>So let</span>’<span>s just define </span><code>A_{d+1}(x)</code><span> to make that inequality work:</span></p>

<figure>


<pre><code><span>A_{d+1}(x) = A_d(A_d( .... A_d(x)))</span>
<span>            &lt;- x + 2 A_d&#39;s in total-&gt;</span></code></pre>

</figure>
<p><span>Unpacking:</span></p>
<p><span>We define a family of unary functions </span><code>A_d</code><span>, such that each </span><code>A_d</code><span> </span>“<span>grows faster</span>”<span> than any n-ary PRF</span>
<span>of depth </span><code>d</code><span>. If </span><code>f</code><span> is a ternary PRF of depth 3, then </span><span><code>f(1, 92, 10) &lt;= A_3(92)</code><span>.</span></span></p>
<p><span>To evaluate </span><code>A_d</code><span> at point </span><code>x</code><span>, we use the following recursive procedure:</span></p>
<ul>
<li>
<span>If </span><code>d</code><span> is </span><code>1</code><span>, return </span><code>x + 1</code><span>.</span>
</li>
<li>
<span>Otherwise, evaluate </span><code>A_{d-1}</code><span> at point </span><code>x</code><span> to get, say, </span><code>v</code><span>. Then evaluate </span><code>A_{d-1}</code><span> again at</span>
<span>point </span><code>v</code><span> this time, yielding </span><code>u</code><span>. Then compute </span><code>A_{d-1}(u)</code><span>. Overall, repeat this process </span><code>x+2</code>
<span>times, and return the final number.</span>
</li>
</ul>
<p><span>We can simplify this a bit if we stop treating </span><code>d</code><span> as a kind of function </span><em><span>index</span></em><span>, and instead say</span>
<span>that our </span><code>A</code><span> is just a function of two arguments. Then we have the following equations:</span></p>

<figure>


<pre><code><span>A(1, x) = x + 1</span>
<span>A(d + 1, x) = A(d, A(d, A(d, ..., A(d, x))))</span>
<span>                &lt;- x + 2 A_d&#39;s in total-&gt;</span></code></pre>

</figure>
<p><span>The last equation can re-formatted as</span></p>

<figure>


<pre><code><span>A(</span>
<span>  d,</span>
<span>  A(d, A(d, ..., A(d, x))),</span>
<span>  &lt;- x + 1 A_d&#39;s in total-&gt;</span>
<span>)</span></code></pre>

</figure>
<p><span>And for non-zero x that is just</span></p>

<figure>


<pre><code><span>A(</span>
<span>  d,</span>
<span>  A(d + 1, x - 1),</span>
<span>)</span></code></pre>

</figure>
<p><span>So we get the following recursive definition for A(d, x):</span></p>

<figure>


<pre><code><span>A(1, x) = x + 1</span>
<span>A(d + 1, 0) = A(d, A(d, 0))</span>
<span>A(d + 1, x) = A(d, A(d + 1, x - 1))</span></code></pre>

</figure>
<p><span>As a Python program:</span></p>

<figure>


<pre><code><span>def A(d, x):</span>
<span>  if d == 1: return x + 1</span>
<span>  if x == 0: return A(d-1, A(d-1, 0))</span>
<span>  return A(d-1, A(d, x - 1))</span></code></pre>

</figure>
<p><span>It</span>’<span>s easy to see that computing </span><code>A</code><span> on a Turing Machine using this definition terminates </span>—<span> this</span>
<span>is a function with two arguments, and every recursive call uses lexicographically smaller pair of</span>
<span>arguments. And we constructed A in such a way that </span><code>A(d, x)</code><span> as a function of </span><code>x</code><span> is larger than any</span>
<span>PRF with a single argument of depth d. But that means that the following function with one argument</span>
<code>a(x) = A(x, x) </code></p>
<p><span>grows faster than </span><em><span>any</span></em><span> PRF. And that</span>’<span>s an example of a function which a Turing Machine have no</span>
<span>trouble computing (given sufficient time), but which is beyond the capabilities of PRF.</span></p>
</section>
<section id="Part-III-Descent-From-the-Ivory-Tower">

    <h2>
    <a href="#Part-III-Descent-From-the-Ivory-Tower"><span>Part III, Descent From the Ivory Tower</span> </a>
    </h2>
<p><span>Remember, this is a tree-part post! And are finally at the part 3! So let</span>’<span>s circe back to the</span>
<span>practical matters. We have learned that:</span></p>
<ul>
<li>
<span>Turing machines don</span>’<span>t necessary terminate.</span>
</li>
<li>
<span>While other computational devices, like FSMs and PRFs, can be made to always terminate, there</span>’<span>s no</span>
<span>guarantee that they</span>’<span>ll terminate fast. PRFs in particular, can compute quite large functions!</span>
</li>
<li>
<span>And non-Turing complete devices can be quiet expressive. For example, any real-world algorithm</span>
<span>that works on a TM can be adapted to run as a PRF.</span>
</li>
<li>
<span>Moreover, you don</span>’<span>t even have to contort the algorithm much to make it fit. There</span>’<span>s a universal</span>
<span>recipe for how to take something Turing complete, and make it a primitive recursive function</span>
<span>instead </span>—<span> just add an iteration counter to the device, and forcibly halt it if the counter grows</span>
<span>too large.</span>
</li>
</ul>
<p><span>Or, more succinctly: there</span>’<span>s no practical difference between a program that doesn</span>’<span>t terminate, and</span>
<span>the one that terminates after a billion years. As a practitioner, if you think you need to solve the</span>
<span>first problem, you need to solve the second problem as well. And making your programming language</span>
<span>non-Turing complete doesn</span>’<span>t really help this.</span></p>
<p><span>And yet, there are a lot of configuration languages out there, that use non-Turing completeness as</span>
<span>one of the key design goal. Why is that?</span></p>
<p><span>I would say that we are never interested in Turing-completeness per-se. We usually want some </span><em><span>much</span></em>
<span>stronger properties. And yet, there</span>’<span>s no convenient, catchy name for that bag of features of a good</span>
<span>configuration language. So, </span>“<span>non-Turing-complete</span>”<span> gets used as a sort of rallying cry to signal that</span>
<span>something is a good configuration language, and maybe sometimes even to justify to others inventing</span>
<span>a new language instead of taking something like Lua. That is, the </span><em><span>real</span></em><span> reason why you want at</span>
<span>least a different implementation is all those properties you really need, but they are kinda hard to</span>
<span>explain, or at least much harder than </span>“<span>we can</span>’<span>t use Python/Lua/JavaScript because they are</span>
<span>Turing-complete</span>”<span>.</span></p>
<p><span>So what </span><em><span>are</span></em><span> the properties of a good configuration language?</span></p>
<p><em><span>First</span></em><span>, we need the language to be deterministic. If you launch Python and type </span><code>id([])</code><span>, you</span>’<span>ll</span>
<span>see some number. If you hit </span><code>^C</code><span>, and than do this again, you</span>’<span>ll see a different number. This is OK</span>
<span>for </span>“<span>normal</span>”<span> programming, but is usually anathema for configuration. Configuration is often use as a</span>
<span>key in some incremental, caching system, and letting in non-determinism there wrecks absolute chaos!</span></p>
<p><em><span>Second</span></em><span>, you need the language to be well-defined. You can compile Python with ASLR disabled, and</span>
<span>use some specific allocator, such that </span><code>id([])</code><span> always returns the same result. But that result</span>
<span>would be hard to predict! And if someone tries to do an alternative implementation, even if they</span>
<span>disable ASLR as well, they are likely to get a different deterministic number! Or the same could</span>
<span>happen if you just update the version of Python. So, the semantics of the language should be clearly</span>
<span>pinned-down by some sort of the reference, such that it is possible to guarantee not only</span>
<span>deterministic behavior, but fully identical behavior across different implementations.</span></p>
<p><em><span>Third</span></em><span>, you need the language to be pure. If your configuration can access environment variables or</span>
<span>read files on disk, than the meaning of the configuration would depend on the environment where the</span>
<span>configuration is evaluated, and you again don</span>’<span>t want that, to make caching work.</span></p>
<p><em><span>Fourth</span></em><span>, a thing that is closely related to purity is security and sandboxing. The </span><em><span>mechanism</span></em><span> to</span>
<span>achieve both purity and security is the same </span>—<span> you don</span>’<span>t expose general IO to your language. But</span>
<span>the purpose is different: purity is about not letting the results being non-deterministic, while</span>
<span>security is about not exposing access tokens to the attacker.</span></p>
<p><span>And now this gets tricky. One particular possible attack is a denial of service </span>—<span> sending some bad</span>
<span>config which makes our system to just spin there burning the CPU. Even if you control all IO, you</span>
<span>are generally still open to these kinds of attacks. It might be OK to say this is outside of the</span>
<span>threat model </span>—<span> that no one would find it valuable enough to just burn your CPU, if they can</span>’<span>t also</span>
<span>do IO, and that, even in the event this happens, there</span>’<span>s going to be some easy mitigation in the</span>
<span>form of higher-level timeout.</span></p>
<p><span>But you also might choose to provide some sort of guarantees about execution time, and that</span>’<span>s really</span>
<span>hard. The two approaches work. One is to make sure that processing is </span><em><span>obviously linear</span></em><span>. Not just</span>
<span>terminates, but is actually proportional to the size of inputs, and in a very direct way. If the</span>
<span>correspondence is not direct, than it</span>’<span>s highly likely that it is in fact non linear. The second</span>
<span>approach is to ensure </span><em><span>metered execution</span></em><span> </span>—<span> during processing, decrement a counter for every</span>
<span>simple atomic step and terminate processing when the counter reaches zero.</span></p>
<p><em><span>Finally</span></em><span> one more vague property you</span>’<span>d want from a configuration language is for it to be simple.</span>
<span>That is, to ensure that, when people use your language, they write simple programs. It seems to me</span>
<span>that this might actually be the case where banning recursion and unbounded loops could help, though</span>
<span>I am not sure. As we know from the PRF exercise, this won</span>’<span>t actually prevent people from writing</span>
<span>arbitrary recursive programs. It</span>’<span>ll just require </span><a href="https://mochiro.moe/posts/09-meson-raytracer/"><span>some roundabout</span>
<span>code</span></a><span> to do that. But maybe that</span>’<span>ll be enough of a</span>
<span>speedbump to make someone invent a simple solution, instead of brute-forcing the most obvious one?</span></p>
<p><span>That</span>’<span>s all for today! Have a great weekend, and remember:</span></p>

<figure>
<blockquote><p><span>Any algorithm that can be implemented by a Turing Machine such that its runtime is bounded by some</span>
<span>primitive recursive function of input can also be implemented by a primitive recursive function!</span></p>
</blockquote>

</figure>
</section>
</article>
  </div></div>
  </body>
</html>
