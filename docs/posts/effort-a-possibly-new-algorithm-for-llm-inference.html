<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kolinko.github.io/effort/">Original</a>
    <h1>Effort â€“ a possibly new algorithm for LLM Inference</h1>
    
    <div id="readability-page-1" class="page"><article>

      
      <p>A possibly new algorithm for LLM Inference</p>
      <section>
        <p>
        With Effort you can adjust smoothly - and in real time - how many calculations you&#39;d like to do during inference of an LLM model. 
        </p><p>
          At 50% calculations it is as fast as regular matrix multiplications on Apple Silicon chips. At 25% effort it&#39;s twice as fast and still retains most of the quality.
          </p><p>
            You can also freely choose to skip loading the least important weights.
          </p><p>
            It is implemented for Mistral now, it should work for all the other models just as well. No retraining needed, just conversion to a different format and some precomputation.

          </p><p>
          <img src="https://kolinko.github.io/effort/ryc/ryc0.1.png"/>
</p><p>
You can download the implementation here - <a href="https://github.com/kolinko/effort">from Github</a>. It should run right after fetching the converted weights.
</p><p>
The implementation is done for FP16 only for now. The multiplications are fast, but inference overall is still slightly lacking because some non-essential parts - softmax etc - need improvement.

</p><p>Mixtral and Q8 are in the works. 
</p><p>
Oh, and it also allows to <b>dynamically decide how much of the model you want to load into the memory</b>. You can skip the least important 10-20-30% weights while loading. It doesn&#39;t require any conversion steps - it just loads less data. Sort of an ad-hoc distillation, you could say.
</p><p>
  Let&#39;s see it in action now.
  </p><figure id="demo">
 
  


The actual speed is constrained by the implementation overhead. Even on 0%, it still takes 15ms on my machine (and a few seconds on M1 Air) to produce a single token. These parts are done in &lt;1ms in Llama.cpp/Ollama, and I could use help from a Swift/Metal engineer to fix this.
</figure>
<p>
  <a href="https://github.com/kolinko/effort">You can download and test it yourself from Github.</a>
</p><p>
  Getting back to benchmarks...
</p><figure>
          <img src="https://kolinko.github.io/effort/ryc/ryc0.2.png"/>

The pink line is actual speed, with a suboptimal implementation of the overhead (calculating norms, attn scores etc). There is a ~15ms overhead to every token that I didn&#39;t manage to fix, but Llama.cpp/Ollama/MLX don&#39;t seem to have. Help would be appreciated here from someone proficient in Metal/Swift.
</figure>
  <p>
  Let&#39;s talk quality now. 
  </p><p>
    First, of the multiplication approximation itself.
  </p><figure>
<img src="https://kolinko.github.io/effort/ryc/ryc0.3.png"/>
Tracked by taking a sample input state vector, multiplying it by say wq, or w1. Some Matrixes seem to be easier to approximate using this method, some are slightly more difficult. But more or less the output looks like here.
  </figure>
  <p>
And now for the model itself.
</p><figure>
  <img src="https://kolinko.github.io/effort/ryc/ryc0.4.png"/>
  Measured by generating a 500 token text first, and then comparing predictions of the tokens when this text would be given as an input. Perplexity score would be nice here, see notes at the end why it&#39;s not yet done.
</figure>
<p>
  And basic QA tests:
  </p><figure>
  <img src="https://kolinko.github.io/effort/ryc/ryc0.5.png"/>
  BasicQ is a list of a few tricky QA prepared by GPT-4. Hopefully this and the working demo are enough to show the potential. Performing HumanEval and HellaSWAG needs fixes in the implementation first - see below.
</figure>
<p>
If you&#39;re still sceptical - as I would be - please head out to the <a href="https://kolinko.github.io/effort/pesky.html">Help Needed!</a> section to see what&#39;s needed for the better tests to be performed.
</p><p>
  The initial results (and undocumented experiments with Mixtral) seem to be robust enough to warrant publication. I hope though that the above is enough to convince you to <a href="https://github.com/kolinko/effort">play with the 0.0.1B version</a>.
</p></section>
 <section>
  

<p>
  - <a href="https://kolinko.github.io/effort/equations.html">The Basics</a>
</p>
<p>
  - <a href="https://kolinko.github.io/effort/bucketmul.html">Introducing bucketMul</a>
</p>
  <p>
    - <a href="https://kolinko.github.io/effort/gpu.html">The GPU implementation</a>
  </p>
    <p>
    - <a href="https://kolinko.github.io/effort/q8.html">MoE, quantization and the others.</a>
    </p>
    <p>
      - <a href="https://kolinko.github.io/effort/pesky.html">Pesky details (or: Help Needed!)</a>
    </p>
    <h2>And of course...</h2>
    <p>
      - <a href="https://kolinko.github.io/effort/about.html">About the Author(s)</a>
    </p>
    <p>
      - <a href="https://kolinko.github.io/effort/setup.html">Download and Run</a>
</p>
    <p>
      - Citations, notes and so on
    </p>

  </section>
    </article></div>
  </body>
</html>
