<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/sail-sg/EditAnything">Original</a>
    <h1>EditAnything: Segment Anything &#43; ControlNet &#43; BLIP2 &#43; Stable Diffusion</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This is an ongoing project aims to <strong>Edit and Generate Anything</strong> in an image,
powered by <a href="https://github.com/facebookresearch/segment-anything">Segment Anything</a>, <a href="https://github.com/lllyasviel/ControlNet">ControlNet</a>,
<a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">BLIP2</a>, <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion" rel="nofollow">Stable Diffusion</a>, etc.</p>
<p dir="auto">This is a small project for fun. Any forms of contribution and suggestion
are very welcomed!</p>

<p dir="auto">2023/04/10 - We transfer the pretrained model into diffusers style, the pretrained model is auto loaded when using <code>sam2image_diffuser.py</code>. Now you can combine our pretrained model with different base models easily!</p>
<p dir="auto">2023/04/09 - We released a pretrained model of StableDiffusion based ControlNet that generate images conditioned by SAM segmentation.</p>

<p dir="auto">Highlight features:</p>
<ul dir="auto">
<li>Pretrained ControlNet with SAM mask as condition enables the image generation with fine-grained control.</li>
<li>category-unrelated SAM mask enables more forms of editing and generation.</li>
<li>BLIP2 text generation enables text guidance-free control.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-generating-anything-by-segment-anything" aria-hidden="true" href="#generating-anything-by-segment-anything"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Generating Anything by Segment-Anything</h2>
<p dir="auto">BLIP2 Prompt: &#34;a large white and red ferry&#34;
<a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/EditAnything/blob/main/images/sample1.jpg"><img src="https://github.com/sail-sg/EditAnything/raw/main/images/sample1.jpg" alt="p"/></a>
(1:input image; 2: segmentation mask; 3-8: generated images.)</p>
<p dir="auto">BLIP2 Prompt: &#34;a cloudy sky&#34;
<a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/EditAnything/blob/main/images/sample2.jpg"><img src="https://github.com/sail-sg/EditAnything/raw/main/images/sample2.jpg" alt="p"/></a></p>
<p dir="auto">BLIP2 Prompt: &#34;a black drone flying in the blue sky&#34;
<a target="_blank" rel="noopener noreferrer" href="https://github.com/sail-sg/EditAnything/blob/main/images/sample3.jpg"><img src="https://github.com/sail-sg/EditAnything/raw/main/images/sample3.jpg" alt="p"/></a></p>
<ol dir="auto">
<li>The human prompt and BLIP2 generated prompt build the text instruction.</li>
<li>The SAM model segment the input image to generate segmentation mask without category.</li>
<li>The segmentation mask and text instruction guide the image generation.</li>
</ol>

<ul>
<li>
<p dir="auto"> Conditional Generation trained with 85k samples in SAM dataset.</p>
</li>
<li>
<p dir="auto"> Training with more images from LAION and SAM.</p>
</li>
<li>
<p dir="auto"> Interactive control on different masks for image editing.</p>
</li>
<li>
<p dir="auto"> Using <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Grounding DINO</a> for category-related auto editing.</p>
</li>
<li>
<p dir="auto"> ChatGPT guided image editing.</p>
</li>
</ul>

<p dir="auto"><strong>Create a environment</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="    conda env create -f environment.yaml
    conda activate control"><pre>    conda env create -f environment.yaml
    conda activate control</pre></div>
<p dir="auto"><strong>Install BLIP2 and SAM</strong></p>
<p dir="auto">Put these models in <code>models</code> folder.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install git+https://github.com/huggingface/transformers.git

pip install git+https://github.com/facebookresearch/segment-anything.git"><pre>pip install git+https://github.com/huggingface/transformers.git

pip install git+https://github.com/facebookresearch/segment-anything.git</pre></div>
<p dir="auto"><strong>Download pretrained model</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="
# Segment-anything ViT-H SAM model. 
cd models/
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

# BLIP2 model will be auto downloaded.

# Get edit-anything-ckpt-v0-1.ckpt pretrained model from huggingface. 
# No need to download this if your are using sam2image_diffuser.py!!!
https://huggingface.co/shgao/edit-anything-v0-1
"><pre><span><span>#</span> Segment-anything ViT-H SAM model. </span>
<span>cd</span> models/
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

<span><span>#</span> BLIP2 model will be auto downloaded.</span>

<span><span>#</span> Get edit-anything-ckpt-v0-1.ckpt pretrained model from huggingface. </span>
<span><span>#</span> No need to download this if your are using sam2image_diffuser.py!!!</span>
https://huggingface.co/shgao/edit-anything-v0-1
</pre></div>
<p dir="auto"><strong>Run Demo</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python sam2image_diffuser.py
# or 
python sam2image.py"><pre>python sam2image_diffuser.py
<span><span>#</span> or </span>
python sam2image.py</pre></div>
<p dir="auto">Set &#39;use_gradio = True&#39; in sam2image.py and sam2image_diffuser.py if you
have GUI to run the gradio demo.</p>

<ol dir="auto">
<li>Generate training dataset with <code>dataset_build.py</code>.</li>
<li>Transfer stable-diffusion model with <code>tool_add_control_sd21.py</code>.</li>
<li>Train model with <code>sam_train_sd21.py</code>.</li>
</ol>

<p dir="auto">This project is based on:</p>
<p dir="auto"><a href="https://github.com/facebookresearch/segment-anything">Segment Anything</a>,
<a href="https://github.com/lllyasviel/ControlNet">ControlNet</a>,
<a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2">BLIP2</a>,
<a href="https://github.com/sail-sg/MDT">MDT</a>,
<a href="https://huggingface.co/spaces/stabilityai/stable-diffusion" rel="nofollow">Stable Diffusion</a>,
<a href="https://github.com/LUSSeg">Large-scale Unsupervised Semantic Segmentation</a></p>
<p dir="auto">Thanks for these amazing projects!</p>
</article>
          </div></div>
  </body>
</html>
