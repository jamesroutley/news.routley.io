<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kevinlynagh.com/towards-the-cutest-neural-network/">Original</a>
    <h1>Towards the Cutest Neural Network</h1>
    
    <div id="readability-page-1" class="page"><div><p><a href="https://kevinlynagh.com/">← Back to Kevin&#39;s homepage</a><span>Published: 2025 April 28</span></p><p>I recently needed to use a microcontroller to estimate the pose (translation and orientation) of an object using readings from six different sensors.
Since the readings were non-linear and coupled with each other, an explicit analytical solution was out of the question.</p>

<p>I figured I’d have a go at using a simple neural network to approximate it:</p>

<ol>
<li>generate training data (on my computer) using a forward simulation (pose to sensor readings)</li>
<li>train a lil’ neural network (a few dense layers, 100’s of parameters, tops) to approximate the inverse mapping function (sensor readings to pose)</li>
<li>deploy this network on my microcontroller (Cortex-M0, 16 kB RAM, 32 kB flash) to actually do the inference</li>
</ol>

<p>Since neural networks have been around since the 1980’s, I figured it’d be straightforward.
A quick background search uncovered lots of promising leads too, especially regarding “quantization”, which I wanted to do as my microcontroller doesn’t have hardware support for floating point operations.</p>

<p>However, this turned out to be much more difficult than I’d anticipated.
It seems like my use case — end-to-end training of a simple dense neural network with integer-only inference — is quite uncommon.</p>

<p>The vast majority of papers and software libraries I found turned out to be complex, heavyweight (in terms of inference code size), and have lots of unstated assumptions and background requirements.</p>

<p>To make a web design analogy: It felt like I kept falling into <code>npm create-react-app</code> rabbit holes rather than what I wanted: The moral equivalent of opening <code>index.html</code> with notepad.exe and typing <code>&lt;h1&gt;Welcome to my Homepage&lt;/h1&gt;</code>.</p>

<p>I’m writing up my notes to:</p>

<ol>
<li>checkpoint my understanding of the space and current best solution</li>
<li>help anyone else who simply wants a wholesome lil’ universal function approximator with low conceptual and hardware overhead</li>
<li>solicit “why don’t you just …” emails from experienced practitioners who can point me to the library/tutorial I’ve been missing =D (see the <a href="#appendix-alternatives-considered">alternatives-considered</a> down the page for what I struck out on)</li>
</ol>
<h2 id="tl-dr-how-to-do-it"><a href="#tl-dr-how-to-do-it">tl;dr, how to do it?</a></h2>
<ol>
<li>use TensorFlow to do <a href="https://www.tensorflow.org/model_optimization/guide/quantization/training">quantization-aware training</a> and save the resulting model out to a <code>.tflite</code> flatbuffer file</li>
<li>use the <a href="https://github.com/matteocarnelos/microflow-rs">microflow-rs</a> crate for inference — it’s basically a proc macro that reads tflite file and generates straightforward Rust code that uses <a href="https://nalgebra.org/">nalgebra</a> to multiply matrices</li>
</ol>

<p>Props to Matteo for being seemingly the only person in this space who can put a clear “hello world” inference example in their README.md:</p>
<div><pre><span></span><span>use</span><span> </span><span>microflow</span>::<span>model</span><span>;</span><span></span>

<span>#[model(</span><span>&#34;path/to/model.tflite&#34;</span><span>)]</span><span></span>
<span>struct</span> <span>MyModel</span><span>;</span><span></span>

<span>fn</span> <span>main</span><span>()</span><span> </span><span>{</span><span></span>
<span>    </span><span>let</span><span> </span><span>prediction</span><span> </span><span>=</span><span> </span><span>MyModel</span>::<span>predict</span><span>(</span><span>input_data</span><span>);</span><span></span>
<span>}</span><span></span>
</pre></div><h2 id="it-should-be-cuter"><a href="#it-should-be-cuter">It should be cuter.</a></h2>
<p>Unfortunately, MicroFlow (and TensorFlow’s out-of-the-box quantization routines, from what I can tell) require floating point operations for inference.</p>

<p>While I could use software floating point, I want to make the <em>cutest possible</em> neural network, which means integer arithmetic operations only.</p>

<p>(The code size and speed of software floating point depends on the routines used. <a href="https://www.quinapalus.com/qfplib-m0-full.html">qfplib-m0-full</a> has a good overview and shows that, e.g., the GCC compiler software floating point multiplication takes 166 cycles.)</p>

<p>On the MicroFlow side, removing floating point operations would require a big redesign, as:</p>

<ul>
<li>they’re baked into the API (<a href="https://github.com/matteocarnelos/microflow-rs/blob/2fb39afc20f568f08990be256f861b4aef60e5c7/microflow-macros/src/lib.rs#L163-L166">predict</a> assumes f32 inputs and outputs).</li>
<li>the internals themselves rely on floating point

<ol>
<li>the MicroFlow proc macro reads the <code>.tflite</code> flatbuffer (<a href="https://github.com/matteocarnelos/microflow-rs/blob/2fb39afc20f568f08990be256f861b4aef60e5c7/microflow-macros/src/lib.rs#L46">source</a>)</li>
<li>when <a href="https://github.com/matteocarnelos/microflow-rs/blob/2fb39afc20f568f08990be256f861b4aef60e5c7/microflow-macros/src/lib.rs#L124-L126">reading a fully-connected layer</a> it combines the various quantization scale factors and zero points <a href="https://github.com/matteocarnelos/microflow-rs/blob/2fb39afc20f568f08990be256f861b4aef60e5c7/microflow-macros/src/ops/fully_connected.rs#L96-L119">into f32 constants</a></li>
<li>these constants are then <a href="https://github.com/matteocarnelos/microflow-rs/blob/2fb39afc20f568f08990be256f861b4aef60e5c7/src/ops/fully_connected.rs#L68-L73">used during inference</a> at runtime</li>
</ol></li>
</ul>

<p>On the TensorFlow side, I can’t find any documentation/options about whether the quantization-aware training routines use floating point activation scaling or if they can model quantized multipliers.</p>

<p>But I’m getting ahead of myself — let’s first review the mathematics of neural networks and meaning of “quantization”.</p>
<h2 id="neural-network-overview"><a href="#neural-network-overview">Neural network overview</a></h2>
<p>The basic idea of neural networks you have a bunch of training data:</p>

<ul>
<li>$x$, the input data you have (in my case, readings from six sensors)</li>
<li>$y$, the output value you are trying to predict (the object pose)</li>
</ul>

<p>Since you have no idea how to go from $x$ to $y$ yourself, you throw together $n$ general equations (“layers”) like:</p>

<p>\begin{align}
a_1 &amp;= \sigma(W_1 x + b_1) \newline
a_2 &amp;= \sigma(W_2 a_1 + b_2) \newline
\vdots \newline
\hat{y} &amp;= \sigma(W_{n} a_{n-1} + b_n)
\end{align}</p>

<p>These equations take your input $x$ and make a prediction $\hat{y}$ which, hopefully, is close to $y$.</p>

<p>The equations require:</p>

<ul>
<li>free parameters $W$ and $b$ (the “weights” and “biases”, respectively); in our case these are a matrix and vector rather than scalar values, since our input data $x$ comes from multiple sensors and the output pose $y$ has six components (three spatial positions and three spatial rotations)</li>
<li>$\sigma$, some nonlinear “activation” function (traditionally <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> is used, but these days folks like <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>, $\sigma(x) = \mathrm{max}(0, x)$)</li>
</ul>

<p>You start with random values for all of your $W$ and $b$, see how well they work for all the $x$ and $y$ pairs of your training dataset, then keep adjusting $W$ and $b$ until $\hat{y}$ (the prediction) is close to $y$ (the actual value).</p>

<p>For more details on how and why this works, see Michael Nielsen’s awesome <a href="http://neuralnetworksanddeeplearning.com/">Neural networks and deep learning</a>.</p>
<h2 id="quantization-overview"><a href="#quantization-overview">Quantization overview</a></h2>
<p>The real numbers of the mathematical neural network theory are usually implemented on a computer with floating point numbers.
These are already quantized: The real numbers are infinite, but 32 bits of memory on a computer can represent, at most, $2^{32} - 1$ different things.
How you map, 32, 8, or 4 bits of memory to the real number line is up to you, the programmer.
See:</p>

<ul>
<li><a href="https://fabiensanglard.net/floating_point_visually_explained/">Floating Point Visually Explained</a></li>
<li><a href="https://web.archive.org/web/20241112154925/http://www.digitalsignallabs.com/downloads/fp.pdf">Fixed-Point Arithmetic: An Introduction</a></li>
</ul>

<p>The “quantization” of neural networks refers to replacing some (or all) of the typically 32-bit floating point numbers of the network parameters with smaller representations.</p>

<p>One method, called “fake quantization”, is to store the parameters as, e.g., 8-bit integers, then convert them back to 32-bit floating point when doing the actual math.
This reduces storage and memory bandwidth requirements, but since it still involves floating point it’s not what I’m interested in for my microcontroller application. (Floating point: Not Cute.)</p>

<p>The quantization I want is calculating $\sigma(Wx + b)$ using only integer arithmetic.
Typically this is done by storing the weights as i8 and the biases as i32.
(The reason the biases are larger is because they’re used as the initial accumulator value for the corresponding row of the matrix-vector product $Wx$, which is the sum of a bunch of i8 * i8 products, and so would likely overflow a i8 accumulator.)</p>

<p>Once you have the accumulated i32 sum and you pass it through the activation function $\sigma$, you then need to convert it back into an i8 so it can be used as $x$ in the next layer’s $\sigma(Wx + b)$</p>

<p>This is known as “activation scaling” and, as we saw earlier in MicroFlow, it is usually implemented with floating point multiplication.</p>

<p>However, activation scaling can also be done using a “quantized multiplier” instead, where the scaling factor is $2^{-n} M_0$, with $0.5 \le M_0 \le 1$ and $n$ a natural number.
This allows the scaling to be done with a fixed-point multiplication and arithmetic shift.</p>

<p>There’s one other large aspect of quantization to consider: How to find the suitable, e.g., 8-bit weights for the network.
There are two approaches:</p>

<ul>
<li><p><strong>post-training quantization</strong>: You basically just round the parameters of an already-trained neural network to the closest quantized number.
It’s up to you to select the quantization sizes (bit-widths) and how to map those bits to real numbers.
The overall accuracy of the network will go down compared to the, e.g., original f32 parameter network, but your network will be much smaller and faster to run.
You also don’t need to do any training, which is usually a hassle for, e.g.,  large vision models that require terabytes of Internet cat photos.</p></li>
<li><p><strong>quantization-aware-training</strong>: You train the network with the forward-pass modeling the quantization of the parameters.
This leads to better accuracy, since you are optimizing your actual use case (quantized inference) rather than optimizing for f32 inference and then unceremoniously rounding away those carefully trained weights later.
The downsides are that:</p>

<ul>
<li>you have to train a network (not a problem in my case, since my network is small and I have to do that anyway)</li>
<li>you have to do some fancy stuff so gradients can flow through the quantization functions (the literature calls this “the straight-through estimator”, but I grug programmer call this “pretend bad function not here”)</li>
</ul></li>
</ul>

<p>For more on the details of quantization / numerics, see:</p>

<ul>
<li>Lei Mao’s <a href="https://leimao.github.io/article/Neural-Networks-Quantization/">Quantization for Neural Networks</a></li>
<li><a href="https://stackoverflow.com/a/62541941">this stack overflow answer</a> about <code>.tflite</code> inference</li>
<li>gemmlowp’s <a href="https://github.com/google/gemmlowp/blob/16e8662c34917be0065110bfcd9cc27d30f52fdf/doc/quantization.md">quantization.md</a>, which explains in full detail the numerical computer bits</li>
<li>this <a href="https://github.com/thommaskevin/TinyML/tree/cc0b870e1e86ce2ac56ab9b0df58193c16b12c89/22_QAT">Tiny ML walkthrough/notebook</a></li>
<li>this paper: <a href="https://arxiv.org/abs/1712.05877">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></li>
<li>this <code>.tflite</code> <a href="https://netron.app/">model visualizer</a></li>
</ul>
<h2 id="recap-next-steps"><a href="#recap-next-steps">Recap + next steps</a></h2>
<p>I’m currently able to use TensorFlow to perform quantization-aware training of my two-layer dense neural network, yielding quantization parameters in a <code>.tflite</code> that I can run on my tiny microcontroller using the lovely MicroFlow crate.</p>

<p>This is <em>pretty cute</em> (and sufficient for my actual application), but it requires floating point operations (for input, activation, and output scaling), which add latency and code size (not cute).</p>

<p>A <em>maximally</em> cute neural network would efficiently use <a href="https://developer.arm.com/documentation/dui0497/a/the-cortex-m0-instruction-set/instruction-set-summary?lang=en">the instructions available on my hardware</a> to transform the incoming sensor readings (six u16 values) to outgoing estimates of the pose (six i16 values).</p>

<p>No runtime allocators, flatbuffers, or Extensible Compiler Frameworks.</p>

<p>How could this be accomplished?</p>

<p>Unless I had an expert guide who was absolutely sure it’d be straightforward (<a href="mailto:kevin@keminglabs.com">email me!</a>), I’d avoid high-level frameworks like TensorFlow and PyTorch and instead implement the quantization-aware training myself.</p>

<p>I’d use JAX, since it provides both:</p>

<ul>
<li>automatic differentiation (gradients are necessary for training)</li>
<li>just-in-time compilation (so the training goes fast on my computer)</li>
</ul>

<p>In fact, their docs show a simple a demo of <a href="https://docs.jax.dev/en/latest/notebooks/neural_network_with_tfds_data.html">training a neural network</a> using gradient descent, using no other framework or dependencies.</p>

<p>I spent an hour with my friend <a href="https://www.nikolasgoebel.com/">Niko</a> figuring out how to define a custom gradient and we managed to descend our way to solving a toy quantized problem:</p>
<div><pre><span></span><span>import</span> <span>jax</span>
<span>import</span> <span>jax.numpy</span> <span>as</span> <span>jnp</span>
<span>import</span> <span>jax.nn</span> <span>as</span> <span>nn</span>

<span># Quantization function that rounds x to one decimal point.</span>
<span># Define a custom gradient so it&#39;s ignored during automatic differentiation.</span>
<span>@jax</span><span>.</span><span>custom_vjp</span>
<span>def</span> <span>quantize</span><span>(</span><span>x</span><span>):</span>
  <span>return</span> <span>jnp</span><span>.</span><span>round</span><span>(</span><span>x</span> <span>*</span> <span>10.0</span><span>)</span> <span>/</span> <span>10.0</span>

<span>def</span> <span>quantize_fwd</span><span>(</span><span>x</span><span>):</span>
  <span>return</span> <span>quantize</span><span>(</span><span>x</span><span>),</span> <span>(</span><span>None</span><span>,</span> <span>)</span>

<span>def</span> <span>quantize_bwd</span><span>(</span><span>res</span><span>,</span> <span>g</span><span>):</span>
  <span>return</span> <span>(</span><span>1.0</span> <span>*</span> <span>g</span><span>,</span> <span>)</span>

<span>quantize</span><span>.</span><span>defvjp</span><span>(</span><span>quantize_fwd</span><span>,</span> <span>quantize_bwd</span><span>)</span>


<span>def</span> <span>predict</span><span>(</span><span>weights</span><span>,</span> <span>biases</span><span>,</span> <span>x</span><span>):</span>
    <span>a1</span> <span>=</span> <span>nn</span><span>.</span><span>relu</span><span>(</span><span>quantize</span><span>(</span><span>weights</span><span>)</span> <span>@</span> <span>x</span> <span>+</span> <span>quantize</span><span>(</span><span>biases</span><span>))</span>
    <span>return</span> <span>a1</span>

<span>def</span> <span>loss</span><span>(</span><span>weights</span><span>,</span> <span>biases</span><span>,</span> <span>x</span><span>,</span> <span>y</span><span>):</span>
    <span>yhat</span> <span>=</span> <span>predict</span><span>(</span><span>weights</span><span>,</span> <span>biases</span><span>,</span> <span>x</span><span>)</span>
    <span>return</span> <span>jnp</span><span>.</span><span>mean</span><span>((</span><span>yhat</span> <span>-</span> <span>y</span><span>)</span><span>**</span><span>2</span><span>)</span>

<span>loss_and_grad</span> <span>=</span> <span>jax</span><span>.</span><span>value_and_grad</span><span>(</span><span>loss</span><span>,</span> <span>argnums</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>))</span>

<span>weights</span> <span>=</span> <span>jnp</span><span>.</span><span>array</span><span>([</span><span>0.5</span><span>])</span>
<span>biases</span> <span>=</span> <span>jnp</span><span>.</span><span>array</span><span>([</span><span>0.5</span><span>])</span>
<span>input</span> <span>=</span> <span>jnp</span><span>.</span><span>array</span><span>([</span><span>1.</span><span>])</span>
<span>target</span> <span>=</span> <span>jnp</span><span>.</span><span>array</span><span>([</span><span>0.2</span><span>])</span>

<span>learning_rate</span> <span>=</span> <span>0.01</span>

<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>100</span><span>):</span>
    <span>loss_value</span><span>,</span> <span>(</span><span>dloss_weights</span><span>,</span> <span>dloss_biases</span><span>)</span> <span>=</span> <span>loss_and_grad</span><span>(</span><span>weights</span><span>,</span> <span>biases</span><span>,</span> <span>input</span><span>,</span> <span>target</span><span>)</span>
    <span>weights</span> <span>=</span> <span>weights</span> <span>-</span> <span>learning_rate</span> <span>*</span> <span>dloss_weights</span>
    <span>biases</span> <span>=</span> <span>biases</span> <span>-</span> <span>learning_rate</span> <span>*</span> <span>dloss_biases</span>
    <span>print</span><span>(</span><span>loss_value</span><span>,</span> <span>(</span><span>weights</span><span>,</span> <span>biases</span><span>),</span> <span>quantize</span><span>(</span><span>weights</span><span>),</span> <span>quantize</span><span>(</span><span>biases</span><span>))</span>

<span># this prints out decreasing loss values reaching 0 when quantized weights and biases sum to the target value (0.2).</span>
</pre></div>
<p>It’s all straightforward math!
I didn’t even to bring in that “<a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam">Adam</a>” guy.</p>

<p>So to do the full quantization-aware training, I’d write out:</p>

<ul>
<li>quantization of the weights, biases, and activations</li>
<li>custom gradients for the backward pass through this quantization</li>
<li>the <a href="https://arxiv.org/abs/1902.08153">Learned Step Size Quantization</a> to find the dyadic rational activation scaling</li>
</ul>

<p>then train to find network parameters.</p>

<p>To make sure I didn’t mess up the implementation, I’d compare my fully-integer-quantized task loss with the mostly-integer-quantized task loss from TensorFlow.</p>

<p>Then, for the inference, I’d manually write the Rust that does the matrix multiplication and activation scaling.
I’d have my Python training notebook write out the weights as a string of Rust that my firmware can <a href="https://doc.rust-lang.org/std/macro.include.html">include!</a> into the binary.</p>

<p>I suspect the entire training notebook and inference code would be less than 200 lines total.
Most importantly, I would actually understand what’s going on, which is aesthetically much more satisfying than messing with complex frameworks =D</p>

<p>Until then!</p>
<h2 id="thanks"><a href="#thanks">Thanks</a></h2>
<p>Thanks to <a href="https://ricklamers.io/about/">Rick Lamers</a> and <a href="https://github.com/matteocarnelos/">Matteo Carnelos</a> for reviewing these notes.</p>
<h2 id="appendix-tensorflow-notes"><a href="#appendix-tensorflow-notes">Appendix: TensorFlow notes</a></h2>
<p>“TensorFlow” is an OG deep neural network framework from Google.
“TensorFlow Lite” is when they got inference working on mobile phones.
“TensorFlow Lite Micro” is when they got inference working on microcontrollers (presumably so you can say “Hello Google” or whatever to their smart speakers or your Nest thermostat or something).</p>

<p>The best introduction and conceptual documentation I found was this <a href="https://www.youtube.com/playlist?list=PL7rtKJAz_mPe6kAbiH6Ucq02Vpa95qvBJ">2023 lecture series on TensorFlow Lite Micro</a>.</p>

<p>In a very Google move at some point they renamed the project and docs to “<a href="https://ai.google.dev/edge/litert/microcontrollers">Lite RT</a>” but didn’t finish the job, so the various code repos, python packages, and file formats are all still various permutations of the earlier names.</p>

<p>I managed to follow their <a href="https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world">tflite-micro/tensorflow/lite/micro/examples/hello_world</a>, but here’s the <code>pyproject.toml</code> I needed to cobble together to do it:</p>
<div><pre><span></span>requires-python = &#34;&gt;=3.12&#34;
dependencies = [
    &#34;absl-py&gt;=1.2&#34;,
    &#34;ai-edge-litert&gt;=1.2.0&#34;,
    &#34;jupyter&gt;=1.1.1&#34;,
    &#34;numpy&gt;=1.10&#34;,
    &#34;tensorflow&gt;=2.18.1&#34;,
    &#34;tensorflow-model-optimization&gt;=0.8.0&#34;,
    &#34;tf-keras&gt;=2.18.0&#34;,
]
</pre></div>
<p>And yeah, go learn how to use <a href="https://docs.astral.sh/uv/">uv</a> because Google does <em>not</em> keep <code>tensorflow-model-optimization</code> (required for quantization) compatible with <code>tensorflow</code> — just trying to pull the latest of everything will give you transitive dependency conflicts.</p>

<p>The Bazel scripts used in the hello_world example didn’t work with Bazel 8 and when I tried Bazel 7 it blew up with some python errors. ¯\_(ツ)_/¯
They do have makefiles, but those required me to <em>upgrade make</em> — first time I’ve ever run into that in my career.</p>

<p>They don’t have it anywhere in the docs, but to build the runtime so you can link to it in your firmware, you need to run something like:</p>
<div><pre><span></span>make -f tensorflow/lite/micro/tools/make/Makefile OPTIMIZED_KERNEL_DIR=cmsis_nn TARGET=cortex_m_generic TARGET_ARCH=cortex-m0 microlite
</pre></div>
<p>Unfortunately, this all turned out for naught because even just linking to the tensorflow lite micro runtime blew up my program size to 37 kB, which is too much for my 32 kB flash microcontroller (especially considering that I need to also include, uh, <em>my</em> program and the actual neural network model).</p>
<h2 id="appendix-alternatives-considered"><a href="#appendix-alternatives-considered">Appendix: Alternatives considered</a></h2>
<p>Here are some other libraries/frameworks I found but wasn’t able to use:</p>

<ul>
<li><p><a href="https://github.com/ARM-software/CMSIS-NN">CMSIS-NN</a> is an ARM-specific neural network kernel.
Maybe I could string together the inference code myself, but I couldn’t find any examples or “howto”-style documentation, just reference docs for (presumably) people who are writing machine learning compilers.
The one example linked in the repo requires tflite micro, which was too big for my microcontroller anyway.</p></li>
<li><p><a href="https://github.com/iree-org/iree">IREE</a> looks active and “scales […] down to satisfy the constraints and special considerations of mobile and edge deployments” but the <a href="https://github.com/iree-org/iree/blob/21072b646fe84d4ca9aef0daefbfe2b5d435cc59/docs/website/docs/assets/images/iree_architecture.svg">graphic</a> on their readme suggests the runtime size is &gt; 25kB, which is too big for my application so I didn’t even try.</p></li>
<li><p><a href="https://tvm.apache.org/docs/v0.9.0/topic/microtvm/index.html">MicroTVM</a> came up in a Google search but there weren’t any simple usage examples.
I then discovered on <a href="https://discuss.tvm.apache.org/t/questions-regarding-the-future-development-of-microtvm/17926/7">a forum post</a> that it had been phased out.</p></li>
<li><p><a href="https://github.com/uTensor/uTensor">uTensor</a> looked promising — their readme calls out their runtime size of 2kB — but I didn’t pursue it because their <a href="https://github.com/uTensor/uTensor/blob/master/src/uTensor/README.md">overview doc</a> spends a bunch of time discussing concepts I know I don’t need, so the whole thing felt a bit too heavy/complex for my “I just wanna matmul a simple fixed network architecture” situation.</p></li>
<li><p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine</a> seems to be a well-documented academic project, but all of their examples and discussion involve complex network architectures for vision/audio and on-device training; I couldn’t find an example of a simple dense network inference, nor did I see anything about integer-only inference.</p></li>
</ul>
</div></div>
  </body>
</html>
