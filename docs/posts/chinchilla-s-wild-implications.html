<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">Original</a>
    <h1>Chinchilla&#39;s Wild Implications</h1>
    
    <div id="readability-page-1" class="page"><p>Crossposted from the <a href="https://alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">AI Alignment Forum</a>. May contain more technical jargon than usual.</p><div><p>(<a href="https://colab.research.google.com/drive/1qv2-hUR5hPqw3OcfmLEhq6Tg15bf06Mj?usp=sharing">Colab notebook</a> here.)</p><p>This post is about language model scaling laws, specifically the laws derived in the DeepMind paper that introduced Chinchilla.</p><p>The paper came out a few months ago, and has been discussed a lot, but some of its implications deserve more explicit notice in my opinion.  In particular:</p><ul><li>Data, not size, is the currently active constraint on language modeling performance.  Current returns to additional data are immense, and current returns to additional model size are miniscule; indeed, most recent landmark models are wastefully big.<ul><li>If we can leverage enough data, there is no reason to train ~500B param models, much less 1T or larger models.</li><li>If we <i>have to</i> train models at these large sizes, it will mean we have encountered a barrier to exploitation of data scaling, which would be a great loss relative to what would otherwise be possible.</li></ul></li><li>The literature is extremely unclear on how much text data is actually available for training.  We may be &#34;running out&#34; of general-domain data, but the literature is too vague to know one way or the other.</li><li>The <i>entire</i> available quantity of data in highly specialized domains like code is woefully tiny, compared to the gains that would be possible if much more such data were available.</li></ul><p>Some things to note at the outset:</p><ul><li>This post assumes you have some familiarity with LM scaling laws.</li><li>As in the paper, I&#39;ll assume here that models never see repeated data in training.<ul><li>This simplifies things: we don&#39;t need to draw a distinction between data size and step count, or between train loss and test loss.</li></ul></li><li>I focus on the parametric scaling law from the paper&#39;s &#34;Approach 3,&#34; because it&#39;s provides useful intuition.<ul><li>Keep in mind, though, that Approach 3 yielded somewhat different results from Approaches 1 and 2 (which agreed with one another, and were used to determine Chinchilla&#39;s model and data size).</li><li>So you should take the exact numbers below with a grain of salt.  They may be off by a few orders of magnitude (but not <i>many</i> orders of magnitude).</li></ul></li></ul><h2 id="1__the_scaling_law"><span><span><span><span aria-label=""><span aria-hidden="true"></span></span></span></span></span>1. the scaling law</h2><p>The paper fits a scaling law for LM loss <span><span><span><span aria-label="L"><span aria-hidden="true"><span><span>L</span></span></span></span></span></span></span>, as a function of model size <span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span> and data size <span><span><span><span aria-label="D"><span aria-hidden="true"><span><span>D</span></span></span></span></span></span></span>.</p><p>Its functional form is very simple, and easier to reason about than the <span><span><span><span aria-label="L(N, D)"><span aria-hidden="true"><span><span>L</span></span><span><span>(</span></span><span><span>N</span></span><span><span>,</span></span><span><span>D</span></span><span><span>)</span></span></span></span></span></span></span> law from the earlier Kaplan et al papers.  It is a sum of three terms:</p><p><span><span><span><span aria-label="L(N, D)  = \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}} + E"><span aria-hidden="true"><span><span>L</span></span><span><span>(</span></span><span><span>N</span></span><span><span>,</span></span><span><span>D</span></span><span><span>)</span></span><span><span>=</span></span><span><span><span><span><span>A</span></span></span><span><span><span><span><span>N</span></span></span><span><span><span><span><span>α</span></span></span></span></span></span></span><span></span></span><span></span></span><span><span>+</span></span><span><span><span><span><span>B</span></span></span><span><span><span><span><span>D</span></span></span><span><span><span><span><span>β</span></span></span></span></span></span></span><span></span></span><span></span></span><span><span>+</span></span><span><span>E</span></span></span></span></span></span></span></p><p>The first term only depends on the model size.  The second term only depends on the data size.  And the third term is a constant.</p><p>You can think about this as follows.</p><p>An &#34;infinitely big&#34; model, trained on &#34;infinite data,&#34; would achieve loss <span><span><span><span aria-label="E"><span aria-hidden="true"><span><span>E</span></span></span></span></span></span></span>.  To get the loss for a real model, you add on two &#34;corrections&#34;:</p><ol><li>one for the fact that the model&#39;s only has <span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span> parameters, not infinitely many</li><li>one for the fact that the model only sees <span><span><span><span aria-label="D"><span aria-hidden="true"><span><span>D</span></span></span></span></span></span></span> training examples, not infinitely many</li></ol><p><span><span><span><span aria-label="L(N, D)  = \underbrace{\frac{A}{N^{\alpha}}}_\text{finite model} + \underbrace{\frac{B}{D^{\beta}}}_\text{finite data} + \underbrace{E}_\text{irreducible}"><span aria-hidden="true"><span><span>L</span></span><span><span>(</span></span><span><span>N</span></span><span><span>,</span></span><span><span>D</span></span><span><span>)</span></span><span><span>=</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>A</span></span></span><span><span><span><span><span>N</span></span></span><span><span><span><span><span>α</span></span></span></span></span></span></span><span></span></span><span></span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite model</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>B</span></span></span><span><span><span><span><span>D</span></span></span><span><span><span><span><span>β</span></span></span></span></span></span></span><span></span></span><span></span></span></span></span></span><span><span><span><span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite data</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>E</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>irreducible</span></span></span></span></span></span></span></span></span></span></span></p><p><span><span><span><span aria-label=""><span aria-hidden="true"></span></span></span></span></span>Here&#39;s the same thing, with the constants fitted to DeepMind&#39;s experiments on the MassiveText dataset.</p><p><span><span><span><span aria-label="L(N, D)  = \underbrace{\frac{406.4}{N^{0.34}}}_\text{finite model} + \underbrace{\frac{410.7}{D^{0.28}}}_\text{finite data} + \underbrace{1.69}_\text{irreducible}"><span aria-hidden="true"><span><span>L</span></span><span><span>(</span></span><span><span>N</span></span><span><span>,</span></span><span><span>D</span></span><span><span>)</span></span><span><span>=</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>406.4</span></span></span><span><span><span><span><span>N</span></span></span><span><span><span><span><span>0.34</span></span></span></span></span></span></span><span></span></span><span></span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite model</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>410.7</span></span></span><span><span><span><span><span>D</span></span></span><span><span><span><span><span>0.28</span></span></span></span></span></span></span><span></span></span><span></span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite data</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>1.69</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>irreducible</span></span></span></span></span></span></span></span></span></span></span></p><h3 id="plugging_in_real_models">plugging in real models</h3><p><strong>Gopher</strong> is a model with 280B parameters, trained on 300B tokens of data.  What happens if we plug in those numbers?</p><p><span><span><span><span aria-label="L(280\cdot10^9, \ 300\cdot10^9)  = \underbrace{0.052}_\text{finite model} + \underbrace{0.251}_\text{finite data} + \underbrace{1.69}_\text{irreducible} = 1.993"><span aria-hidden="true"><span><span>L</span></span><span><span>(</span></span><span><span>280</span></span><span><span>⋅</span></span><span><span><span><span>10</span></span></span><span><span><span>9</span></span></span></span><span><span>,</span></span><span><span> </span></span><span><span>300</span></span><span><span>⋅</span></span><span><span><span><span>10</span></span></span><span><span><span>9</span></span></span></span><span><span>)</span></span><span><span>=</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>0.052</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite model</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>0.251</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite data</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>1.69</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>irreducible</span></span></span></span></span></span><span><span>=</span></span><span><span>1.993</span></span></span></span></span></span></span></p><p><span><span><span><span aria-label=""><span aria-hidden="true"></span></span></span></span></span>What jumps out here is that the &#34;finite model&#34; term is <i>tiny.</i></p><p>In terms of the impact on LM loss, Gopher&#39;s parameter count might as well be infinity.  There&#39;s a <i>little</i> more to gain on that front, but not much.</p><p>Scale the model up to 500B params, or 1T params, or 100T params, or 3^^^3 params . . . and the most this can <i>ever</i> do for you is an 0.052 reduction in loss.</p><p>Meanwhile, the &#34;finite data&#34; term is <i>not</i> tiny.  Gopher&#39;s training data size is very much <i>not</i> infinity, and we can go a long way by making it bigger.</p><hr/><p><strong>Chinchilla</strong> is a model with the same training compute cost as Gopher, allocated more evenly between the two terms in the equation.</p><p>It&#39;s 70B params, trained on 1.4T tokens of data.  Let&#39;s plug that in:</p><p><span><span><span><span aria-label="L(70\cdot10^9, \ 1400\cdot10^9)  = \underbrace{0.083}_\text{finite model} + \underbrace{0.163}_\text{finite data} + \underbrace{1.69}_\text{irreducible} = 1.936"><span aria-hidden="true"><span><span>L</span></span><span><span>(</span></span><span><span>70</span></span><span><span>⋅</span></span><span><span><span><span>10</span></span></span><span><span><span>9</span></span></span></span><span><span>,</span></span><span><span> </span></span><span><span>1400</span></span><span><span>⋅</span></span><span><span><span><span>10</span></span></span><span><span><span>9</span></span></span></span><span><span>)</span></span><span><span>=</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>0.083</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite model</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>0.163</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite data</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>1.69</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>irreducible</span></span></span></span></span></span><span><span>=</span></span><span><span>1.936</span></span></span></span></span></span></span></p><p>Much better!</p><p>Without using any more compute, we&#39;ve improved the loss by 0.057.  That&#39;s bigger than Gopher&#39;s entire &#34;finite model&#34; term!</p><p>The paper demonstrates that Chinchilla roundly defeats Gopher on downstream tasks, as we&#39;d expect.</p><p>Even that understates the accomplishment, though.  At least in terms of loss, Chinchilla doesn&#39;t just beat Gopher.  It beats <i>any model</i> <i>trained on Gopher&#39;s data, no matter how big.</i></p><p>To put this in context: until this paper, it was conventional to train all large LMs on roughly 300B tokens of data.  (GPT-3 did it, and everyone else followed.)</p><p>Insofar as we trust our equation, this <i>entire</i> line of research -- which includes GPT-3, LaMDA, Gopher, Jurassic, and MT-NLG -- could <i>never</i> have beaten Chinchilla, no matter how big the models got.</p><p>People put immense effort into training models that big, and were working on even bigger ones, and yet none of this, in principle, could ever get as far Chinchilla did.</p><hr/><p>Here&#39;s where the various models lie on a contour plot of LM loss (per the equation), with <span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span> on the x-axis and <span><span><span><span aria-label="D"><span aria-hidden="true"><span><span>D</span></span></span></span></span></span></span> on the y-axis.</p><figure><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/570b8e7f8945085468d888095168f4eec14482661e20930c.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/570b8e7f8945085468d888095168f4eec14482661e20930c.png/w_118 118w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/570b8e7f8945085468d888095168f4eec14482661e20930c.png/w_198 198w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/570b8e7f8945085468d888095168f4eec14482661e20930c.png/w_278 278w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/570b8e7f8945085468d888095168f4eec14482661e20930c.png/w_358 358w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/570b8e7f8945085468d888095168f4eec14482661e20930c.png/w_438 438w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/570b8e7f8945085468d888095168f4eec14482661e20930c.png/w_518 518w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/570b8e7f8945085468d888095168f4eec14482661e20930c.png/w_598 598w"/></figure><p>Only PaLM is remotely close to Chinchilla here.  (Indeed, PaLM does slightly better.)</p><p>PaLM is a huge model.  It&#39;s the largest one considered here, though MT-NLG is a close second.  Everyone writing about PaLM mentions that it has 540B parameters, and the PaLM paper does a lot of experiments on the differences between the 540B PaLM and smaller variants of it.</p><p>According to this scaling law, though, PaLM&#39;s <i>parameter count</i> is a mere footnote relative to PaLM&#39;s <i>training data size</i>. </p><p>PaLM isn&#39;t competitive with Chinchilla because it&#39;s big.  MT-NLG is almost the same size, and yet it&#39;s trapped in the pinkish-purple zone on the bottom-left, with Gopher and the rest.</p><p>No, PaLM is competitive with Chinchilla only because it was trained on more tokens (780B) than the other non-Chinchilla models.  For example, this change in data size constitutes 85% of the loss improvement from Gopher to PaLM.</p><p>Here&#39;s the precise breakdown for PaLM:</p><p><span><span><span><span aria-label="L(540\cdot10^9, \ 780\cdot10^9)  = \underbrace{0.042}_\text{finite model} + \underbrace{0.192}_\text{finite data} + \underbrace{1.69}_\text{irreducible} = 1.924"><span aria-hidden="true"><span><span>L</span></span><span><span>(</span></span><span><span>540</span></span><span><span>⋅</span></span><span><span><span><span>10</span></span></span><span><span><span>9</span></span></span></span><span><span>,</span></span><span><span> </span></span><span><span>780</span></span><span><span>⋅</span></span><span><span><span><span>10</span></span></span><span><span><span>9</span></span></span></span><span><span>)</span></span><span><span>=</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>0.042</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite model</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>0.192</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>finite data</span></span></span></span></span></span><span><span>+</span></span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>1.69</span></span></span></span></span><span><span><span><span><span></span><span></span><span></span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span>irreducible</span></span></span></span></span></span><span><span>=</span></span><span><span>1.924</span></span></span></span></span></span></span></p><p>PaLM&#39;s gains came with a great cost, though.  It used way more training compute than any previous model, and its size means it also takes a lot of inference compute to run.</p><p>Here&#39;s a visualization of loss vs. training compute (loss on the y-axis and in color as well):</p><figure><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f160599490c166353ca0b83be235a4c66770a1bb01ac8ab.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f160599490c166353ca0b83be235a4c66770a1bb01ac8ab.png/w_133 133w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f160599490c166353ca0b83be235a4c66770a1bb01ac8ab.png/w_213 213w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f160599490c166353ca0b83be235a4c66770a1bb01ac8ab.png/w_293 293w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f160599490c166353ca0b83be235a4c66770a1bb01ac8ab.png/w_373 373w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f160599490c166353ca0b83be235a4c66770a1bb01ac8ab.png/w_453 453w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f160599490c166353ca0b83be235a4c66770a1bb01ac8ab.png/w_533 533w"/></figure><p>Man, we spent all that compute on PaLM, and all we got was the slightest edge over Chinchilla!</p><p>Could we have done better?  In the equation just above, PaLM&#39;s terms look pretty unbalanced.  Given that compute, we probably should have used more data and trained a smaller model.</p><p>The paper tells us how to pick <i>optimal</i> values for params and data, given a compute budget.  Indeed, that&#39;s its main focus.</p><p>If we use its recommendations for PaLM&#39;s compute, we get the point &#34;palm_opt&#34; on this plot:</p><figure><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c3acfae02c60b025e881ae51facfdef3911004f283d0dc1.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c3acfae02c60b025e881ae51facfdef3911004f283d0dc1.png/w_139 139w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c3acfae02c60b025e881ae51facfdef3911004f283d0dc1.png/w_219 219w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c3acfae02c60b025e881ae51facfdef3911004f283d0dc1.png/w_299 299w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c3acfae02c60b025e881ae51facfdef3911004f283d0dc1.png/w_379 379w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c3acfae02c60b025e881ae51facfdef3911004f283d0dc1.png/w_459 459w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c3acfae02c60b025e881ae51facfdef3911004f283d0dc1.png/w_539 539w"/></figure><p>Ah, now we&#39;re talking!</p><hr/><p>&#34;palm_opt&#34; sure looks good.  But how would we train it, concretely?</p><p>Let&#39;s go back to the <span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span>-vs.-<span><span><span><span aria-label="D"><span aria-hidden="true"><span><span>D</span></span></span></span></span></span></span> contour plot world.</p><figure><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d089ab263cadf93ac74b73c022c7b60d89cef121ebe89a23.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d089ab263cadf93ac74b73c022c7b60d89cef121ebe89a23.png/w_112 112w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d089ab263cadf93ac74b73c022c7b60d89cef121ebe89a23.png/w_192 192w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d089ab263cadf93ac74b73c022c7b60d89cef121ebe89a23.png/w_272 272w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d089ab263cadf93ac74b73c022c7b60d89cef121ebe89a23.png/w_352 352w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d089ab263cadf93ac74b73c022c7b60d89cef121ebe89a23.png/w_432 432w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d089ab263cadf93ac74b73c022c7b60d89cef121ebe89a23.png/w_512 512w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d089ab263cadf93ac74b73c022c7b60d89cef121ebe89a23.png/w_592 592w"/></figure><p>I&#39;ve changed the axis limits here, to accommodate the <strong>massive</strong> data set you&#39;d need to spent PaLM&#39;s compute optimally.</p><p>How much data would that require?  Around 6.7T tokens, or ~4.8 times as much as Chinchilla used.</p><p>Meanwhile, the resulting model would not be nearly as big as PaLM.  The optimal compute law actually puts it at 63B params.</p><p>Okay, so we just need to get 6.7T tokens and . . . wait, how exactly <i>are</i> we going to get 6.7T tokens?  How much text data <i>is</i> there, exactly?</p><h2 id="2__are_we_running_out_of_data_">2. are we running out of data?</h2><p>It is frustratingly hard to find an answer to this question.</p><p>The main moral I want to get across in this post is that the large LM community has not taken data scaling seriously enough.</p><p>LM papers are meticulous about <span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span> -- doing all kinds of scaling analyses on models of various sizes, etc.  There has been tons of smart discussion about the hardware and software demands of training high-<span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span> models.  The question &#34;what would it take to get to 1T params? (or 10T?)&#34; is on everyone&#39;s radar.</p><p>Yet, meanwhile:</p><ul><li>Everyone trained their big models on 300B tokens, for no particular reason, until this paper showed how hilariously wasteful this is</li><li>Papers rarely do scaling analyses that vary data size -- as if the concepts of &#34;LM scaling&#34; and &#34;adding more parameters&#34; have effectively merged in people&#39;s minds</li><li>Papers basically never talk about what it would take to scale their <i>datasets</i> up by 10x or 50x</li><li>The data collection sections of LM papers tend to be vague and slapdash, often failing to answer basic questions like &#34;where did you scrape these webpages from?&#34; or &#34;how many more could you scrape, if you wanted to?&#34;</li></ul><p>As a particularly egregious example, here is what the <a href="https://arxiv.org/abs/2201.08239">LaMDA</a> paper says about the composition of their training data:</p><blockquote><p>The pre-training data, called Infiniset, is a combination of dialog data from public dialog data and other public web documents. It consists of 2.97B documents and 1.12B dialogs with 13.39B utterances. The composition of the data is as follows: 50% dialogs data from public forums; 12.5% C4 data [11]; 12.5% code documents from sites related to programming like Q&amp;A sites, tutorials, etc; 12.5% Wikipedia (English); 6.25% English web documents; and 6.25% Non-English web documents. The total number of words in the dataset is 1.56T.</p></blockquote><p>&#34;Dialogs data from public forums&#34;?  Which forums?  Did you use all the forum data you could find, or only 0.01% of it, or something in between?  And why measure <i>words</i> instead of tokens -- unless they <i>meant </i>tokens?</p><p>If people were as casual about scaling <span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span> as this quotation is about scaling <span><span><span><span aria-label="D"><span aria-hidden="true"><span><span>D</span></span></span></span></span></span></span>, the methods sections of large LM papers would all be a few sentences long.  Instead, they tend to look like this (excerpted from ~3 pages of similar material):</p><figure><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_170 170w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_340 340w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_510 510w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_680 680w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_850 850w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_1020 1020w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_1190 1190w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_1360 1360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_1530 1530w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/de03b0c23ae15f234e97eebf5e628224bb5fb45435c1656b.png/w_1694 1694w"/><figcaption>From the PaLM paper</figcaption></figure><hr/><p>...anyway.  How much more data could we get?</p><p>This question is complicated by the fact that not all data is equally good.</p><p>(<a href="https://docs.google.com/spreadsheets/d/1zsahRIxNnXSq9z9tEHbISCkYxsivnDG9V3LbJLJTL90/edit?usp=sharing">This messy Google sheet</a> contains the calculations behind some of what I say below.)</p><h3 id="web_scrapes">web scrapes</h3><p>If you just want <i>a lot of text</i>, the easiest way to get it is from web scrapes like Common Crawl.</p><p>But these are infamously full of garbage, and if you want to train a good LM, you probably want to aggressively filter them for quality.  And the papers don&#39;t tell us how much <i>total</i> web data they have, only how much <i>filtered</i> data.</p><p><strong id="MassiveWeb">MassiveWeb</strong></p><p>The training dataset used for Gopher and Chinchilla is called MassiveText, and the web scrape portion of it is called MassiveWeb.  This data originates in a mysterious, unspecified web scrape, which is funneled through a series of filters, including quality heuristics and an attempt to only keep English text.</p><p>MassiveWeb is 506B.  Could it be made bigger, by scaling up the original web scrape?  That depends on how complete the original web scrape was -- but we know nothing about it.</p><p><strong id="The_GLaM_PaLM_web_corpus">The GLaM/PaLM web corpus</strong></p><p>PaLM used a different web scrape corpus.  It was first used in <a href="https://proceedings.mlr.press/v162/du22c.html">this paper</a> about &#34;GLaM,&#34; which again did not say anything about the original scraping process, only describing the quality filtering they did (and not in much detail).</p><p>The GLaM paper says its filtered web corpus is 143B tokens.  That&#39;s a lot smaller than MassiveWeb.  Is that because of the filtering?  Because the original scrape was smaller?  Dunno.</p><p>To further complicate matters, the PaLM authors used a <i>variant</i> of the GLaM dataset which made multilingual versions of (some of?) the English-only components.</p><p>How many tokens did this add?  They don&#39;t say.</p><p>We <i>are</i> told that 27% (211B) of PaLM&#39;s training tokens came from this web corpus, and we are separately told that they tried to avoid repeating data.  So the PaLM version of the GLaM web corpus is probably at least 211B, versus the original 143B.  (Though I am not very confident of that.)</p><p>Still, that&#39;s much smaller than MassiveWeb.  Is this because they had a higher quality bar (which would be bad news for further data scaling)?  They do attribute some of PaLM&#39;s success to quality filtering, citing the ablation on this in the GLaM paper.</p><p>It&#39;s hard to tell, but there is this ominous comment, in the section where they talk about PaLM vs. Chinchilla:</p><blockquote><p>Although there is a large amount of very high-quality textual data available on the web, there is not an infinite amount. For the corpus mixing proportions chosen for PaLM, data begins to repeat in some of our subcorpora after 780B tokens, which is why we chose that as the endpoint of training. It is unclear how the “value” of repeated data compares to unseen data for large-scale language model training.</p></blockquote><p>The subcorpora that start to repeat are probably the web and dialogue ones.</p><p>Read literally, this passage seems to suggest that even the vast web data resources available to <i>Google Research</i> (!) are starting to strain against the data demands of large LMs.  Is that plausible?  I don&#39;t know.</p><h3 id="domain_specific_corpora">domain-specific corpora</h3><p>We can speak with more confidence about text in specialized domains that&#39;s less common on the open web, since there&#39;s less of it out there, and people are more explicit about where they&#39;re getting it.</p><p><strong id="Code">Code</strong></p><p>If you want code, it&#39;s on Github.  There&#39;s some in other places too, but if you&#39;ve exhausted Github, you probably aren&#39;t going to find orders of magnitude of additional code data.  (I think?)</p><p>We&#39;ve more-or-less exhausted Github.  It&#39;s been scraped a few times with different kinds of filtering, which yielded broadly similar data sizes:</p><ul><li>The Pile&#39;s scrape had 631GB of text, and ~299B tokens</li><li>The MassiveText scrape had 3.1TB of text, and 506B tokens</li><li>The PaLM scrape had only 196GB of text (we aren&#39;t told how many tokens)</li><li>The Codex paper&#39;s scrape was python-only and had 159GB of text</li></ul><p>(The text to token ratios vary due to differences in how whitespace was tokenized.)</p><p>All of these scrapes contained a large fraction of the total code available on Github (in the Codex paper&#39;s case, just the python code).</p><p>Generously, there might be<strong> ~1T</strong> <strong>tokens </strong>of code out there, but not vastly more than that.</p><p><strong id="Arxiv">Arxiv</strong></p><p>If you want to train a model on advanced academic research in physics or mathematics, you go to Arxiv.</p><p>For example, Arxiv was about half the training data for the math-problem-solving LM <a href="https://arxiv.org/abs/2206.14858">Minerva.</a></p><p>We&#39;ve exhausted Arxiv.  Both the Minerva paper and the Pile use basically all of Arxiv, and it amounts to a measly <strong>21B tokens</strong>.</p><p><strong id="Books">Books</strong></p><p>Books?  What exactly are &#34;books&#34;?</p><p>In the Pile, &#34;books&#34; means the Books3 corpus, which means &#34;<a href="https://twitter.com/theshawwn/status/1320282149329784833?lang=en">all of Bibliotik</a>.&#34;  It contains 196,640 full-text books, amounting to only <strong>27B tokens</strong>.</p><p>In MassiveText, a mysterious subset called &#34;books&#34; has <strong>560B tokens</strong>.  That&#39;s a lot more than the Pile has!  Are these all the books?  In . . . the world?  In . . . Google books?  Who even knows?</p><p>In the GLaM/PaLM dataset, an equally mysterious subset called &#34;books&#34; has <strong>390B tokens.</strong></p><p>Why is the GLaM/PaLM number so much smaller than the MassiveText number?  Is it a tokenization thing?  Both of these datasets were made by Google, so it&#39;s not like the Gopher authors have special access to some secret trove of forbidden books (I assume??).</p><p>If we want LMs to learn the kind of stuff you learn from books, and not just from the internet, this is what we have.</p><p>As with the web, it&#39;s hard to know what to make of it, because we don&#39;t know whether this is &#34;basically all the books in the world&#34; or just some subset that an engineer pulled at one point in time.</p><h3 id="_all_the_data_we_have_">&#34;all the data we have&#34;</h3><p>In my <a href="https://docs.google.com/spreadsheets/d/1zsahRIxNnXSq9z9tEHbISCkYxsivnDG9V3LbJLJTL90/edit?usp=sharing">spreadsheet</a>, I tried to make a rough, erring-on-generous estimate of what you&#39;d get if you pooled together all the sub-corpora mentioned in the papers I&#39;ve discussed here.</p><p>I tried to make it an overestimate, and did some extreme things like adding up both MassiveWeb <i>and</i> the GLaM/PaLM web corpus as though they were disjoint.</p><p>The result was <strong>~3.2T</strong> tokens, or </p><ul><li>about 1.6x the size of MassiveText</li><li>about 35% of the data we would need to train palm_opt</li></ul><p>Recall that this already contains &#34;basically all&#34; of the open-source code in the world, and  &#34;basically all&#34; of the theoretical physics papers written in the internet era -- within an order of magnitude, anyway.  In these domains, the &#34;low-hanging fruit&#34; of data scaling are not low-hanging at all.</p><h2 id="what_is_compute___on_a_further_barrier_to_data_scaling_">what is compute? (on a further barrier to data scaling)</h2><p>Here&#39;s another important comment from the PaLM paper&#39;s Chinchilla discussion.  This is about barriers to doing a head-to-head comparison experiment:</p><blockquote><p>If the smaller model were trained using fewer TPU chips than the larger model, this would proportionally increase the wall-clock time of training, since the total training FLOP count is the same. If it were trained using the same number of TPU chips, it would be very difficult to maintain TPU compute efficiency without a drastic increase in batch size. The batch size of PaLM 540B is already 4M tokens, and it is unclear if even larger batch sizes would maintain sample efficiency.</p></blockquote><p>In LM scaling research, all &#34;compute&#34; is treated as fungible.  There&#39;s one resource, and you spend it on params and steps, where compute = params * steps.</p><p>But params can be <i>parallelized</i>, while steps cannot.</p><p>You can take a big model and spread it (and its activations, gradients, Adam buffers, etc.) across a cluster of machines in various ways.  This is how people scale up <span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span> in practice.</p><p>But to scale up <span><span><span><span aria-label="D"><span aria-hidden="true"><span><span>D</span></span></span></span></span></span></span>, you have to either:</p><ul><li>take more optimization steps -- an inherently serial process, which takes linearly more time as you add data, no matter how fancy your computers are</li><li>increase the batch size -- which tends to degrade model quality beyond a certain critical size, and current high-<span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span> models are already pushing against that limit</li></ul><p>Thus, it is unclear whether the &#34;compute&#34; you spend in high-<span><span><span><span aria-label="D"><span aria-hidden="true"><span><span>D</span></span></span></span></span></span></span> models is as readily available (and as bound to grow over time) as we typically imagine &#34;compute&#34; to be.</p><p>If LM researchers start getting serious about scaling up data, no doubt people will think hard about this question, but that work has not yet been done.</p><h2 id="appendix__to_infinity">appendix: to infinity</h2><p>Earlier, I observation that Chinchilla beats any Gopher of arbitrary size.</p><p>The graph below expands on that observation, by including two variants of each model:</p><ul><li>one with the finite-model term set to zero, i.e. the infinite-parameter limit</li><li>one with the finite-data term set to zero, i.e. the infinite-data limit</li></ul><p>(There are two x-axes, one for data and one for params.  I included the latter so I have a place to put the infinite-data models without making an infinitely big plot.  </p><p>The dotted line is Chinchilla, to emphasize that it beats infinite-params Gopher.)</p><figure><img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec01a17eb897513c896b09f496e30dc7c3636aa027da927c.png" srcset="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec01a17eb897513c896b09f496e30dc7c3636aa027da927c.png/w_112 112w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec01a17eb897513c896b09f496e30dc7c3636aa027da927c.png/w_192 192w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec01a17eb897513c896b09f496e30dc7c3636aa027da927c.png/w_272 272w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec01a17eb897513c896b09f496e30dc7c3636aa027da927c.png/w_352 352w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec01a17eb897513c896b09f496e30dc7c3636aa027da927c.png/w_432 432w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec01a17eb897513c896b09f496e30dc7c3636aa027da927c.png/w_512 512w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec01a17eb897513c896b09f496e30dc7c3636aa027da927c.png/w_592 592w"/></figure><figure><img/></figure><p>The main takeaway IMO is the size of the gap between ∞ data models and all the others.  Just another way of emphasizing how skewed these models are toward <span><span><span><span aria-label="N"><span aria-hidden="true"><span><span>N</span></span></span></span></span></span></span>, and away from <span><span><span><span aria-label="D"><span aria-hidden="true"><span><span>D</span></span></span></span></span></span></span>.</p></div></div>
  </body>
</html>
