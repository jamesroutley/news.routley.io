<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.google/technology/developers/t5gemma-2/">Original</a>
    <h1>T5Gemma 2: The next generation of encoder-decoder models</h1>
    
    <div id="readability-page-1" class="page"><article>

    
    


<section>
  
</section>


    

    
      








<div data-analytics-module="{
    &#34;module_name&#34;: &#34;Hero Menu&#34;,
    &#34;section_header&#34;: &#34;T5Gemma 2: The next generation of encoder\u002Ddecoder models&#34;
  }">
  
  <div>
    <div>
      
      
        <p>
          T5Gemma 2 is more than a re-training. It incorporates significant architectural changes while inheriting many of the powerful, next-generation features of the Gemma 3 family.
        </p>
      
    </div>
  </div>
  
  
</div>

    

    
      










<div>
  <div>
    <figure>
      <div>
        <p><img alt="T5Gemma 2 Text" data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/T5-Keyword_RD1-V01.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/T5-Keyword_RD1-V01.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/T5-Keyword_RD1-V01.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/T5-Keyword_RD1-V01.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/T5-Keyword_RD1-V01.width-2200.format-webp.webp 2200w"/>
        </p>
      </div>
      
    </figure>
  </div>
</div>






    

    
    <section>
      <div>
        
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
  
    
    
    
    

    <uni-article-speakable page-title="T5Gemma 2: The next generation of encoder\u002Ddecoder models" listen-to-article="Listen to article" data-date-modified="2025-12-18T18:34:28.666285+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js" data-highlight-mode="word-over-paragraph"></uni-article-speakable>
  





            
            
<!--article text-->

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;T5Gemma 2: The next generation of encoder\u002Ddecoder models&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="7j9y9"><a href="https://arxiv.org/abs/2512.14856">T5Gemma 2</a> is the next evolution of our encoder-decoder family based on Gemma 3, featuring the first multi-modal and long-context encoder-decoder models.</p><p data-block-key="d5qcc">Unlike T5Gemma, T5Gemma 2 adopts tied word embeddings (over encoder and decoder) and merged decoder self- and cross-attention to save model parameters. It offers compact pre-trained models at sizes of 270M-270M (~370M total, excluding vision encoder), 1B-1B (~1.7B) and 4B-4B (~7B) parameters, making them ideal for rapid experimentation and deployment in on-device applications.</p><h2 data-block-key="amrfl">Background</h2><p data-block-key="f86n5">With the original <a href="https://developers.googleblog.com/en/t5gemma/">T5Gemma</a>, we demonstrated that we could successfully adapt modern, pre-trained decoder-only models into an encoder-decoder architecture, unlocking new versatility. By initializing with weights from a powerful decoder-only model and then applying continued pre-training, we created high-quality, inference-efficient models while bypassing the computational cost of training from scratch.</p><p data-block-key="fljar">T5Gemma 2 extends this into the realm of vision-language models by incorporating key innovations from Gemma 3.</p><h2 data-block-key="dlcqj">What’s new</h2><p data-block-key="3mg06">T5Gemma 2 is more than a re-training. It incorporates significant architectural changes while inheriting many of the powerful, next-generation features of the Gemma 3 family.</p><h3 data-block-key="2sigb">Architectural innovations for efficiency</h3><p data-block-key="djbnv">To maximize efficiency at smaller scales, we have introduced key structural refinements:</p><ul><li data-block-key="8jb3m"><b>Tied embeddings:</b> We now tie the embeddings between the encoder and decoder. This significantly reduces the overall parameter count, allowing us to pack more active capabilities into the same memory footprint — crucial for our new compact 270M-270M model.</li><li data-block-key="81pdv"><b>Merged attention:</b> In the decoder, we adopt a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.</li></ul><h3 data-block-key="c6ibb">Next-generation capabilities</h3><p data-block-key="2ls8k">Drawing from Gemma 3, T5Gemma 2 also represents a significant upgrade in model capabilities:</p><ul><li data-block-key="cln6a"><b>Multimodality:</b> T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.</li><li data-block-key="7b1hr"><b>Extended long context:</b> We&#39;ve dramatically expanded the context window. Leveraging Gemma 3&#39;s alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.</li><li data-block-key="f1t8a"><b>Massively multilingual:</b> Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.</li></ul><h2 data-block-key="6i3nf">Performance</h2><p data-block-key="efjl4">T5Gemma 2 sets a new standard for what compact encoder-decoder models can achieve. Our new models demonstrate strong performance across key capability areas, inheriting the powerful multimodal and long-context features from the Gemma 3 architecture.</p></div>
      </div>
    </div>
  

  
    

















<uni-image-carousel section-header="T5Gemma 2: The next generation of encoder\u002Ddecoder models" images="[
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1a_rEj400Y.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1a_rEj400Y.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Stem and Code Bar Chart 1&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      },
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1b_NoaRso0.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1b_NoaRso0.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Reasoning and Factuality Bar Chart 1&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      },
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1c_4TfnVTY.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1c_4TfnVTY.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Multilingual Bar Chart 1&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      },
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1d_udeLl71.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1d_udeLl71.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Multimodal Bar Chart 1&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      },
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1e_FexKBEb.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/1e_FexKBEb.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Long Context Bar Chart 1&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      }
    
  ]">
  
    
  
    
  
    
  
    
  
    
  
</uni-image-carousel>

  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;T5Gemma 2: The next generation of encoder\u002Ddecoder models&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="7j9y9"><i>Pre-training performance of Gemma 3, T5Gemma and T5Gemma 2 across five unique capabilities.</i></p><p data-block-key="d6lo1">As shown in the charts above, T5Gemma 2 delivers:</p><ul><li data-block-key="ig02"><b>Strong multimodal performance</b>, outperforming Gemma 3 on several benchmarks. We adapt text-only Gemma 3 base models (270M and 1B) into effective multimodal encoder-decoder models.</li><li data-block-key="c9e34"><b>Superior long-context capability</b>, with substantial quality gains over Gemma 3 and T5Gemma. Using a separate encoder makes T5Gemma 2 better at handling long-context problems.</li><li data-block-key="8d2dk"><b>Improved general capabilities</b>. Across coding, reasoning and multilingual tasks, T5Gemma 2 generally surpasses its corresponding Gemma 3 counterpart.</li></ul></div>
      </div>
    </div>
  

  
    

















<uni-image-carousel section-header="T5Gemma 2: The next generation of encoder\u002Ddecoder models" images="[
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2a_ujNUT3c.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2a_ujNUT3c.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Stem and Code Bar Chart 2&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      },
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2b_FTIWGlT.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2b_FTIWGlT.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Reasoning and Factuality Bar Chart 2&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      },
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2c_sT6yU5w.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2c_sT6yU5w.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Multilingual Bar Chart 2&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      },
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2d_lLPQSDG.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2d_lLPQSDG.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Multimodal Bar Chart 2&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      },
    
      {
        
          
          
          &#34;src&#34;: [&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2e_noRgImg.max-1080x1080.format-webp.webp&#34;,&#34;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2e_noRgImg.max-1080x1080.format-webp.webp&#34;],
        
        &#34;alt&#34;: &#34;Long Context Bar Chart 2&#34;,
        &#34;isVideo&#34;: false,
        &#34;videoTitle&#34;: &#34;&#34;
      }
    
  ]">
  
    
  
    
  
    
  
    
  
    
  
</uni-image-carousel>

  

  
    <div role="presentation" data-analytics-module="{
           &#34;module_name&#34;: &#34;Paragraph&#34;,
           &#34;section_header&#34;: &#34;T5Gemma 2: The next generation of encoder\u002Ddecoder models&#34;
         }">
      <div data-component="uni-article-paragraph">
        <div><p data-block-key="7j9y9"><i>Post-training performance.</i> <i>Note: we are not releasing any post-trained / IT checkpoints. These results here are only for illustration, where we performed a minimal SFT without RL for T5Gemma 2. Also note pre-training and post-training benchmarks are different, so scores are not comparable across plots.</i></p><p data-block-key="8rk3r">Similar to the original T5Gemma, we find that the post-training performance of T5Gemma 2 generally yields better results than its decoder-only counterparts. This makes T5Gemma 2 suitable for both large language model research as well as downstream applications.</p><h2 data-block-key="9kp0s">Getting started</h2><p data-block-key="c0599">We’re looking forward to seeing what the community builds with T5Gemma 2. This release includes pre-trained checkpoints, designed to be post-trained by developers for specific tasks before deployment.</p><p data-block-key="1jig8">These pre-trained checkpoints are available now for broad use across several platforms:</p><ul><li data-block-key="2tb99"><a href="https://arxiv.org/abs/2512.14856"><b>Read the paper on arXiv</b></a></li><li data-block-key="6f6t2"><a href="https://www.kaggle.com/models/google/t5gemma-2"><b>Download on Kaggle</b></a></li><li data-block-key="a01iv"><a href="https://huggingface.co/collections/google/t5gemma-2"><b>Available on Hugging Face</b></a></li><li data-block-key="fqg6k"><a href="https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Research/[T5Gemma_2]Example.ipynb"><b>Explore via Colab</b></a></li><li data-block-key="4jh5a"><a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/t5gemma"><b>Run Inference via Vertex AI</b></a></li></ul></div>
      </div>
    </div>
  


            
            

            
              




            
          </div>
        
      </div>
    </section>
  </article></div>
  </body>
</html>
