<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://devcenter.upsun.com/posts/cut-aws-bandwidth-costs-95-with-dm-cache/">Original</a>
    <h1>Reduce bandwidth costs with dm-cache: fast local SSD caching for network storage</h1>
    
    <div id="readability-page-1" class="page"><div><h2>The bandwidth billing challenge<span id="the-bandwidth-billing-challenge"></span>
<a href="#the-bandwidth-billing-challenge" aria-label="Permalink for this section"></a></h2><p>When deploying infrastructure across multiple AWS availability zones (AZs), bandwidth costs can become a significant operational expense. Some of our Upsun infrastructure spans three AZs for high availability, but this architecture created an unexpected challenge with our <a href="https://devcenter.upsun.com/posts/how-do-we-do-incremental-but-really-full-backups-on-top-of-ceph/">Ceph-based storage system</a>.</p><p>Since Ceph distributes data across the cluster and AWS bills for inter-AZ traffic, approximately two-thirds of our disk I/O traffic crossed AZ boundaries—generating substantial bandwidth charges. With all disk operations flowing over the network rather than accessing local storage, we needed a solution that could reduce this costly network traffic without compromising our distributed storage benefits.</p><h2>The local SSD caching experiment<span id="the-local-ssd-caching-experiment"></span>
<a href="#the-local-ssd-caching-experiment" aria-label="Permalink for this section"></a></h2><p>Our AWS instance types included small amounts of local SSD storage that weren’t being utilized for primary storage. This presented an opportunity: what if we could use these fast, local disks as a read cache in front of our network-based Ceph storage?</p><p>We implemented a three-step caching strategy using Linux device mapper (dm-cache):</p><ol><li><strong>Volume partitioning</strong>: Used LVM to split the local SSD into small 512MB cache volumes</li><li><strong>Read-only caching</strong>: Configured dm-cache to place these volumes in front of our Ceph RBD (RADOS Block Device) volumes, caching reads while passing writes directly through to the network storage</li><li><strong>Container integration</strong>: Exposed the dm-cache devices to our containers as their primary storage interface</li></ol><h2>Understanding dm-cache architecture<span id="understanding-dm-cache-architecture"></span>
<a href="#understanding-dm-cache-architecture" aria-label="Permalink for this section"></a></h2><p><img src="https://devcenter.upsun.com/images/dm-cache.png" alt="dm-cache architecture" loading="lazy"/></p><p>The dm-cache kernel module was originally designed to address a classic storage trade-off: placing small, expensive SSDs in front of large, affordable HDDs to create hybrid storage with both capacity and performance. Our use case follows the same pattern—except instead of slow HDDs, we’re caching in front of network-attached storage.</p><p>This approach works because of how application I/O patterns typically behave. Most applications exhibit temporal locality in their data access, repeatedly reading the same files or database pages. By caching frequently accessed blocks locally, we can serve repeated reads from fast SSD without network round-trips.</p><p>For detailed setup instructions, check out the <a href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/advanced-logical-volume-management_configuring-and-managing-logical-volumes#caching-logical-volumes_advanced-logical-volume-management" target="_blank" rel="noopener">Red Hat documentation on caching logical volumes</a>.</p><h2>Implementation considerations<span id="implementation-considerations"></span>
<a href="#implementation-considerations" aria-label="Permalink for this section"></a></h2><p>When implementing dm-cache in production environments, consider these key factors:</p><p><strong>Cache sizing</strong>: While our 512MB cache proved effective, the optimal size depends on your application’s working set. Monitor cache hit rates and adjust accordingly.</p><p><strong>Write policy</strong>: We chose write-through mode exclusively, where all writes bypass the cache and go directly to network storage. This decision was crucial for Upsun because we host thousands of e-commerce applications where data integrity is non-negotiable.</p><p>In e-commerce environments, every transaction—whether it’s inventory updates, order processing, or payment confirmations—must be immediately persisted to durable storage. A lost write due to cache failure could mean:</p><ul><li>Customer orders disappearing after payment processing</li><li>Inventory miscounts leading to overselling</li><li>Financial transaction data corruption</li><li>Regulatory compliance violations</li></ul><p>While write-back mode offers better write performance by buffering writes in the local cache, it introduces risk scenarios we couldn’t accept:</p><ul><li><strong>Cache device failure</strong>: Unwritten data in the cache would be permanently lost</li><li><strong>Power failures</strong>: Cached writes might not survive unexpected shutdowns</li><li><strong>Split-brain scenarios</strong>: Cache and storage could become inconsistent during network partitions</li></ul><p>For e-commerce workloads, the performance benefit of write-back mode isn’t worth the data integrity risk. Our customers depend on transactional consistency, and write-through mode ensures every write operation is safely committed to our replicated Ceph storage before the application considers it complete.</p><p><strong>Performance monitoring</strong>: Track metrics like cache hit ratios, network bandwidth utilization, and application response times to validate the effectiveness of your caching strategy.</p><h2>Results: dramatic performance and cost improvements<span id="results-dramatic-performance-and-cost-improvements"></span>
<a href="#results-dramatic-performance-and-cost-improvements" aria-label="Permalink for this section"></a></h2><p>The impact exceeded our expectations. With cache volumes of only 512MB serving RBD volumes ranging from 5-50GB, we achieved:</p><ul><li><strong>95% reduction in read traffic</strong> over the network</li><li><strong>30x improvement in IOPS</strong> for cached operations</li><li><strong>50% reduction in read latency</strong></li><li><strong>30x increase in read bandwidth</strong> for frequently accessed data</li></ul><p>These improvements translate directly to cost savings through reduced inter-AZ bandwidth charges, while the performance gains provide a significantly better experience for applications running on our platform.</p><h2>Why this approach works at scale<span id="why-this-approach-works-at-scale"></span>
<a href="#why-this-approach-works-at-scale" aria-label="Permalink for this section"></a></h2><p>The success of this caching strategy relies on several key factors that align well with typical container workloads:</p><p><strong>Read-heavy patterns</strong>: Many applications read configuration files, static assets, and database indexes repeatedly, making them ideal for caching.</p><p><strong>Working set locality</strong>: Applications tend to access a subset of their total data frequently, allowing small caches to be highly effective.</p><p><strong>Write-through simplicity</strong>: By passing writes directly to network storage, we maintain data consistency while avoiding complex cache invalidation scenarios.</p><p>The 512MB cache size might seem small, but it’s sufficient to cache the most frequently accessed blocks for typical containerized applications, delivering outsized performance benefits.</p></div></div>
  </body>
</html>
