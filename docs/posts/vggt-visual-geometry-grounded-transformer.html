<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/vggt">Original</a>
    <h1>VGGT: Visual Geometry Grounded Transformer</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{wang2025vggt,
  title={VGGT: Visual Geometry Grounded Transformer},
  author={Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}"><pre><span>@inproceedings</span>{<span>wang2025vggt</span>,
  <span>title</span>=<span><span>{</span>VGGT: Visual Geometry Grounded Transformer<span>}</span></span>,
  <span>author</span>=<span><span>{</span>Wang, Jianyuan and Chen, Minghao and Karaev, Nikita and Vedaldi, Andrea and Rupprecht, Christian and Novotny, David<span>}</span></span>,
  <span>booktitle</span>=<span><span>{</span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2025<span>}</span></span>
}</pre></div>

<p dir="auto">Visual Geometry Grounded Transformer (VGGT, CVPR 2025) is a feed-forward neural network that directly infers all key 3D attributes of a scene, including extrinsic and intrinsic camera parameters, point maps, depth maps, and 3D point tracks, <strong>from one, a few, or hundreds of its views, within seconds</strong>.</p>

<p dir="auto">First, clone this repository to your local machine, and install the dependencies (torch, torchvision, numpy, Pillow, and huggingface_hub).</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:facebookresearch/vggt.git 
cd vggt
pip install -r requirements.txt"><pre>git clone git@github.com:facebookresearch/vggt.git 
<span>cd</span> vggt
pip install -r requirements.txt</pre></div>
<p dir="auto">Alternatively, you can install VGGT as a package (<a href="https://jamiepalatnik.com/facebookresearch/vggt/blob/main/docs/package.md">click here</a> for details).</p>
<p dir="auto">Now, try the model with just a few lines of code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images

device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;
# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) 
dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] &gt;= 8 else torch.float16

# Initialize the model and load the pretrained weights.
# This will automatically download the model weights the first time it&#39;s run, which may take a while.
model = VGGT.from_pretrained(&#34;facebook/VGGT-1B&#34;).to(device)

# Load and preprocess example images (replace with your own image paths)
image_names = [&#34;path/to/imageA.png&#34;, &#34;path/to/imageB.png&#34;, &#34;path/to/imageC.png&#34;]  
images = load_and_preprocess_images(image_names).to(device)

with torch.no_grad():
    with torch.cuda.amp.autocast(dtype=dtype):
        # Predict attributes including cameras, depth maps, and point maps.
        predictions = model(images)"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>vggt</span>.<span>models</span>.<span>vggt</span> <span>import</span> <span>VGGT</span>
<span>from</span> <span>vggt</span>.<span>utils</span>.<span>load_fn</span> <span>import</span> <span>load_and_preprocess_images</span>

<span>device</span> <span>=</span> <span>&#34;cuda&#34;</span> <span>if</span> <span>torch</span>.<span>cuda</span>.<span>is_available</span>() <span>else</span> <span>&#34;cpu&#34;</span>
<span># bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) </span>
<span>dtype</span> <span>=</span> <span>torch</span>.<span>bfloat16</span> <span>if</span> <span>torch</span>.<span>cuda</span>.<span>get_device_capability</span>()[<span>0</span>] <span>&gt;=</span> <span>8</span> <span>else</span> <span>torch</span>.<span>float16</span>

<span># Initialize the model and load the pretrained weights.</span>
<span># This will automatically download the model weights the first time it&#39;s run, which may take a while.</span>
<span>model</span> <span>=</span> <span>VGGT</span>.<span>from_pretrained</span>(<span>&#34;facebook/VGGT-1B&#34;</span>).<span>to</span>(<span>device</span>)

<span># Load and preprocess example images (replace with your own image paths)</span>
<span>image_names</span> <span>=</span> [<span>&#34;path/to/imageA.png&#34;</span>, <span>&#34;path/to/imageB.png&#34;</span>, <span>&#34;path/to/imageC.png&#34;</span>]  
<span>images</span> <span>=</span> <span>load_and_preprocess_images</span>(<span>image_names</span>).<span>to</span>(<span>device</span>)

<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>with</span> <span>torch</span>.<span>cuda</span>.<span>amp</span>.<span>autocast</span>(<span>dtype</span><span>=</span><span>dtype</span>):
        <span># Predict attributes including cameras, depth maps, and point maps.</span>
        <span>predictions</span> <span>=</span> <span>model</span>(<span>images</span>)</pre></div>
<p dir="auto">The model weights will be automatically downloaded from Hugging Face. If you encounter issues such as slow loading, you can manually download them <a href="https://huggingface.co/facebook/VGGT-1B/blob/main/model.pt" rel="nofollow">here</a> and load, or:</p>
<div dir="auto" data-snippet-clipboard-copy-content="model = VGGT()
_URL = &#34;https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt&#34;
model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))"><pre><span>model</span> <span>=</span> <span>VGGT</span>()
<span>_URL</span> <span>=</span> <span>&#34;https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt&#34;</span>
<span>model</span>.<span>load_state_dict</span>(<span>torch</span>.<span>hub</span>.<span>load_state_dict_from_url</span>(<span>_URL</span>))</pre></div>

<p dir="auto">You can also optionally choose which attributes (branches) to predict, as shown below. This achieves the same result as the example above. This example uses a batch size of 1 (processing a single scene), but it naturally works for multiple scenes.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from vggt.utils.pose_enc import pose_encoding_to_extri_intri
from vggt.utils.geometry import unproject_depth_map_to_point_map

with torch.no_grad():
    with torch.cuda.amp.autocast(dtype=dtype):
        images = images[None]  # add batch dimension
        aggregated_tokens_list, ps_idx = model.aggregator(images)
                
    # Predict Cameras
    pose_enc = model.camera_head(aggregated_tokens_list)[-1]
    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)
    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])

    # Predict Depth Maps
    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)

    # Predict Point Maps
    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)
        
    # Construct 3D Points from Depth Maps and Cameras
    # which usually leads to more accurate 3D points than point map branch
    point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0), 
                                                                extrinsic.squeeze(0), 
                                                                intrinsic.squeeze(0))

    # Predict Tracks
    # choose your own points to track, with shape (N, 2) for one scene
    query_points = torch.FloatTensor([[100.0, 200.0], 
                                        [60.72, 259.94]]).to(device)
    track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])"><pre><span>from</span> <span>vggt</span>.<span>utils</span>.<span>pose_enc</span> <span>import</span> <span>pose_encoding_to_extri_intri</span>
<span>from</span> <span>vggt</span>.<span>utils</span>.<span>geometry</span> <span>import</span> <span>unproject_depth_map_to_point_map</span>

<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>with</span> <span>torch</span>.<span>cuda</span>.<span>amp</span>.<span>autocast</span>(<span>dtype</span><span>=</span><span>dtype</span>):
        <span>images</span> <span>=</span> <span>images</span>[<span>None</span>]  <span># add batch dimension</span>
        <span>aggregated_tokens_list</span>, <span>ps_idx</span> <span>=</span> <span>model</span>.<span>aggregator</span>(<span>images</span>)
                
    <span># Predict Cameras</span>
    <span>pose_enc</span> <span>=</span> <span>model</span>.<span>camera_head</span>(<span>aggregated_tokens_list</span>)[<span>-</span><span>1</span>]
    <span># Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)</span>
    <span>extrinsic</span>, <span>intrinsic</span> <span>=</span> <span>pose_encoding_to_extri_intri</span>(<span>pose_enc</span>, <span>images</span>.<span>shape</span>[<span>-</span><span>2</span>:])

    <span># Predict Depth Maps</span>
    <span>depth_map</span>, <span>depth_conf</span> <span>=</span> <span>model</span>.<span>depth_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>)

    <span># Predict Point Maps</span>
    <span>point_map</span>, <span>point_conf</span> <span>=</span> <span>model</span>.<span>point_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>)
        
    <span># Construct 3D Points from Depth Maps and Cameras</span>
    <span># which usually leads to more accurate 3D points than point map branch</span>
    <span>point_map_by_unprojection</span> <span>=</span> <span>unproject_depth_map_to_point_map</span>(<span>depth_map</span>.<span>squeeze</span>(<span>0</span>), 
                                                                <span>extrinsic</span>.<span>squeeze</span>(<span>0</span>), 
                                                                <span>intrinsic</span>.<span>squeeze</span>(<span>0</span>))

    <span># Predict Tracks</span>
    <span># choose your own points to track, with shape (N, 2) for one scene</span>
    <span>query_points</span> <span>=</span> <span>torch</span>.<span>FloatTensor</span>([[<span>100.0</span>, <span>200.0</span>], 
                                        [<span>60.72</span>, <span>259.94</span>]]).<span>to</span>(<span>device</span>)
    <span>track_list</span>, <span>vis_score</span>, <span>conf_score</span> <span>=</span> <span>model</span>.<span>track_head</span>(<span>aggregated_tokens_list</span>, <span>images</span>, <span>ps_idx</span>, <span>query_points</span><span>=</span><span>query_points</span>[<span>None</span>])</pre></div>
<p dir="auto">Furthermore, if certain pixels in the input frames are unwanted (e.g., reflective surfaces, sky, or water), you can simply mask them by setting the corresponding pixel values to 0 or 1. Precise segmentation masks aren&#39;t necessary - simple bounding box masks work effectively (check this <a href="https://github.com/facebookresearch/vggt/issues/47" data-hovercard-type="issue" data-hovercard-url="/facebookresearch/vggt/issues/47/hovercard">issue</a> for an example).</p>

<p dir="auto">We provide multiple ways to visualize your 3D reconstructions and tracking results. Before using these visualization tools, install the required dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements_demo.txt"><pre>pip install -r requirements_demo.txt</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Interactive 3D Visualization</h3><a id="user-content-interactive-3d-visualization" aria-label="Permalink: Interactive 3D Visualization" href="#interactive-3d-visualization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Please note:</strong> VGGT typically reconstructs a scene in less than 1 second. However, visualizing 3D points may take tens of seconds due to third-party rendering, independent of VGGT&#39;s processing time. The visualization is slow especially when the number of images is large.</p>

<p dir="auto">Our Gradio-based interface allows you to upload images/videos, run reconstruction, and interactively explore the 3D scene in your browser. You can launch this in your local machine or try it on <a href="https://huggingface.co/spaces/facebook/vggt" rel="nofollow">Hugging Face</a>.</p>

<details>
<summary>Click to preview the Gradio interactive interface</summary>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/45f94454b15337d5f7b7b39344673976a1266a32e2096355f49311c396068d8c/68747470733a2f2f6a7974696d652e6769746875622e696f2f646174612f766767745f68665f64656d6f5f73637265656e2e706e67"><img src="https://camo.githubusercontent.com/45f94454b15337d5f7b7b39344673976a1266a32e2096355f49311c396068d8c/68747470733a2f2f6a7974696d652e6769746875622e696f2f646174612f766767745f68665f64656d6f5f73637265656e2e706e67" alt="Gradio Web Interface Preview" data-canonical-src="https://jytime.github.io/data/vggt_hf_demo_screen.png"/></a></p>
</details>

<p dir="auto">Run the following command to run reconstruction and visualize the point clouds in viser. Note this script requires a path to a folder containing images. It assumes only image files under the folder. You can set <code>--use_point_map</code> to use the point cloud from the point map branch, instead of the depth-based point cloud.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python demo_viser.py --image_folder path/to/your/images/folder"><pre>python demo_viser.py --image_folder path/to/your/images/folder</pre></div>

<p dir="auto">To visualize point tracks across multiple images:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from vggt.utils.visual_track import visualize_tracks_on_images
track = track_list[-1]
visualize_tracks_on_images(images, track, (conf_score&gt;0.2) &amp; (vis_score&gt;0.2), out_dir=&#34;track_visuals&#34;)"><pre><span>from</span> <span>vggt</span>.<span>utils</span>.<span>visual_track</span> <span>import</span> <span>visualize_tracks_on_images</span>
<span>track</span> <span>=</span> <span>track_list</span>[<span>-</span><span>1</span>]
<span>visualize_tracks_on_images</span>(<span>images</span>, <span>track</span>, (<span>conf_score</span><span>&gt;</span><span>0.2</span>) <span>&amp;</span> (<span>vis_score</span><span>&gt;</span><span>0.2</span>), <span>out_dir</span><span>=</span><span>&#34;track_visuals&#34;</span>)</pre></div>
<p dir="auto">This plots the tracks on the images and saves them to the specified output directory.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Single-view Reconstruction</h2><a id="user-content-single-view-reconstruction" aria-label="Permalink: Single-view Reconstruction" href="#single-view-reconstruction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Our model shows surprisingly good performance on single-view reconstruction, although it was never trained for this task. The model does not need to duplicate the single-view image to a pair, instead, it can directly infer the 3D structure from the tokens of the single view image. Feel free to try it with our demos above, which naturally works for single-view reconstruction.</p>
<p dir="auto">We did not quantitatively test monocular depth estimation performance ourselves, but <a href="https://github.com/kabouzeid">@kabouzeid</a> generously provided a comparison of VGGT to recent methods <a href="https://github.com/facebookresearch/vggt/issues/36" data-hovercard-type="issue" data-hovercard-url="/facebookresearch/vggt/issues/36/hovercard">here</a>. VGGT shows competitive or better results compared to state-of-the-art monocular approaches such as DepthAnything v2 or MoGe, despite never being explicitly trained for single-view tasks.</p>

<p dir="auto">We benchmark the runtime and GPU memory usage of VGGT&#39;s aggregator on a single NVIDIA H100 GPU across various input sizes.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Input Frames</strong></th>
<th>1</th>
<th>2</th>
<th>4</th>
<th>8</th>
<th>10</th>
<th>20</th>
<th>50</th>
<th>100</th>
<th>200</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Time (s)</strong></td>
<td>0.04</td>
<td>0.05</td>
<td>0.07</td>
<td>0.11</td>
<td>0.14</td>
<td>0.31</td>
<td>1.04</td>
<td>3.12</td>
<td>8.75</td>
</tr>
<tr>
<td><strong>Memory (GB)</strong></td>
<td>1.88</td>
<td>2.07</td>
<td>2.45</td>
<td>3.23</td>
<td>3.63</td>
<td>5.58</td>
<td>11.41</td>
<td>21.15</td>
<td>40.63</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Note that these results were obtained using Flash Attention 3, which is faster than the default Flash Attention 2 implementation while maintaining almost the same memory usage. Feel free to compile Flash Attention 3 from source to get better performance.</p>

<p dir="auto">Our work builds upon a series of previous research projects. If you&#39;re interested in understanding how our research evolved, check out our previous works:</p>
<markdown-accessiblity-table></markdown-accessiblity-table>

<p dir="auto">Thanks to these great repositories: <a href="https://github.com/facebookresearch/PoseDiffusion">PoseDiffusion</a>, <a href="https://github.com/facebookresearch/vggsfm">VGGSfM</a>, <a href="https://github.com/facebookresearch/co-tracker">CoTracker</a>, <a href="https://github.com/facebookresearch/dinov2">DINOv2</a>, <a href="https://github.com/naver/dust3r">Dust3r</a>, <a href="https://github.com/microsoft/moge">Moge</a>, <a href="https://github.com/facebookresearch/pytorch3d">PyTorch3D</a>, <a href="https://github.com/xiongzhu666/Sky-Segmentation-and-Post-processing">Sky Segmentation</a>, <a href="https://github.com/DepthAnything/Depth-Anything-V2">Depth Anything V2</a>, <a href="https://github.com/YvanYin/Metric3D">Metric3D</a> and many other inspiring works in the community.</p>

<ul>
<li> Release the training code</li>
<li> Release VGGT-500M and VGGT-200M</li>
</ul>

<p dir="auto">See the <a href="https://jamiepalatnik.com/facebookresearch/vggt/blob/main/LICENSE.txt">LICENSE</a> file for details about the license under which this code is made available.</p>
</article></div></div>
  </body>
</html>
