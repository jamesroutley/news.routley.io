<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggerganov/llama.cpp/pull/1827">Original</a>
    <h1>Llama.cpp: Full CUDA GPU Acceleration</h1>
    
    <div id="readability-page-1" class="page"><div disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/TheBloke/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TheBloke">@TheBloke</a> I pushed three new branches: <code>cuda-full-gpu-no-k</code>, <code>cuda-full-gpu-2-no-v</code>, and <code>cuda-full-gpu-2-no-kv</code> where I simply disabled moving the K and/or V portions of the KV cache to VRAM. It would be useful if you could test whether any of those have better performance than master when not all layers can be offloaded.</p>
</blockquote>
<p dir="auto">I gave it a try on my low spec RTX 2060 laptop with 16 GB RAM and a Core i7 9750H, running Windows 11 and a 13B ggml 5_1 model. I was always making sure I start with a VRAM usage of 0,2/6,0 GB.</p>
<p dir="auto">First, master.</p>
<div data-snippet-clipboard-copy-content="llama_print_timings:        load time = 48485.97 ms
llama_print_timings:      sample time =   131.38 ms /   180 runs   (    0.73 ms per token)
llama_print_timings: prompt eval time = 63274.61 ms /  1849 tokens (   34.22 ms per token)
llama_print_timings:        eval time = 78150.91 ms /   179 runs   (  436.60 ms per token)
llama_print_timings:       total time = 147501.87 ms

VRAM usage during inference: 5,2/6,0 GB 
llama_model_load_internal: offloading 12 layers to GPU
llama_model_load_internal: total VRAM used: 3235 MB
"><pre><code>llama_print_timings:        load time = 48485.97 ms
llama_print_timings:      sample time =   131.38 ms /   180 runs   (    0.73 ms per token)
llama_print_timings: prompt eval time = 63274.61 ms /  1849 tokens (   34.22 ms per token)
llama_print_timings:        eval time = 78150.91 ms /   179 runs   (  436.60 ms per token)
llama_print_timings:       total time = 147501.87 ms

VRAM usage during inference: 5,2/6,0 GB 
llama_model_load_internal: offloading 12 layers to GPU
llama_model_load_internal: total VRAM used: 3235 MB

</code></pre></div>
<p dir="auto">No KV</p>
<div data-snippet-clipboard-copy-content="llama_print_timings:        load time = 46211.44 ms
llama_print_timings:      sample time =   137.22 ms /   180 runs   (    0.76 ms per token)
llama_print_timings: prompt eval time = 61512.56 ms /  1849 tokens (   33.27 ms per token)
llama_print_timings:        eval time = 78198.88 ms /   179 runs   (  436.87 ms per token)
llama_print_timings:       total time = 144787.18 ms

llama_model_load_internal: offloading 12 layers to GPU
llama_model_load_internal: total VRAM used: 3235 MB

VRAM usage during inference: 5,2/6,0 GB"><pre><code>llama_print_timings:        load time = 46211.44 ms
llama_print_timings:      sample time =   137.22 ms /   180 runs   (    0.76 ms per token)
llama_print_timings: prompt eval time = 61512.56 ms /  1849 tokens (   33.27 ms per token)
llama_print_timings:        eval time = 78198.88 ms /   179 runs   (  436.87 ms per token)
llama_print_timings:       total time = 144787.18 ms

llama_model_load_internal: offloading 12 layers to GPU
llama_model_load_internal: total VRAM used: 3235 MB

VRAM usage during inference: 5,2/6,0 GB
</code></pre></div>
<p dir="auto">No K</p>
<div data-snippet-clipboard-copy-content="llama_print_timings:        load time = 52890.79 ms
llama_print_timings:      sample time =   131.02 ms /   180 runs   (    0.73 ms per token)
llama_print_timings: prompt eval time = 69005.32 ms /  1849 tokens (   37.32 ms per token)
llama_print_timings:        eval time = 82346.11 ms /   179 runs   (  460.03 ms per token)
llama_print_timings:       total time = 155854.00 ms

llama_model_load_internal: offloading 8 layers to GPU
llama_model_load_internal: total VRAM used: 2328 MB

VRAM usage during inference: 5,1/6,0 GB"><pre><code>llama_print_timings:        load time = 52890.79 ms
llama_print_timings:      sample time =   131.02 ms /   180 runs   (    0.73 ms per token)
llama_print_timings: prompt eval time = 69005.32 ms /  1849 tokens (   37.32 ms per token)
llama_print_timings:        eval time = 82346.11 ms /   179 runs   (  460.03 ms per token)
llama_print_timings:       total time = 155854.00 ms

llama_model_load_internal: offloading 8 layers to GPU
llama_model_load_internal: total VRAM used: 2328 MB

VRAM usage during inference: 5,1/6,0 GB
</code></pre></div>
<p dir="auto">No V</p>
<div data-snippet-clipboard-copy-content="llama_print_timings:        load time = 51836.95 ms
llama_print_timings:      sample time =   141.44 ms /   180 runs   (    0.79 ms per token)
llama_print_timings: prompt eval time = 68025.75 ms /  1849 tokens (   36.79 ms per token)
llama_print_timings:        eval time = 81405.72 ms /   179 runs   (  454.78 ms per token)
llama_print_timings:       total time = 153939.91 ms

llama_model_load_internal: offloading 8 layers to GPU
llama_model_load_internal: total VRAM used: 2328 MB

VRAM usage: 5,1/6,0 GB"><pre><code>llama_print_timings:        load time = 51836.95 ms
llama_print_timings:      sample time =   141.44 ms /   180 runs   (    0.79 ms per token)
llama_print_timings: prompt eval time = 68025.75 ms /  1849 tokens (   36.79 ms per token)
llama_print_timings:        eval time = 81405.72 ms /   179 runs   (  454.78 ms per token)
llama_print_timings:       total time = 153939.91 ms

llama_model_load_internal: offloading 8 layers to GPU
llama_model_load_internal: total VRAM used: 2328 MB

VRAM usage: 5,1/6,0 GB
</code></pre></div>
<p dir="auto">Note: Prompt ingestion time is flawed due to a slow startup (probably because I just have 16 GB) In reality, its roughly half than that.</p>
<p dir="auto">So yeah, no KV definately works the best in my case, with K or V its slower. I hope this is useful data for you. Let me know if you need more low to mid spec data, I will be happy to provide that!</p>
      </div>
</div></div>
  </body>
</html>
