<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggerganov/llama.cpp/pull/1827">Original</a>
    <h1>Llama.cpp: Full CUDA GPU Acceleration</h1>
    
    <div id="readability-page-1" class="page"><div data-quote-markdown=".js-comment-body" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-team-hovercards-enabled="" data-hpc="">
    <template>
  <div data-view-component="true">
  <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
    <p><span>
      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      <a href="https://github.co/hiddenchars" target="_blank">Learn more about bidirectional Unicode characters</a>
    </span></p>
</div></template>
<template>
  <span aria-label="This line has hidden Unicode characters" data-view-component="true">
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path>
</svg>
</span></template>

    <div>

      <div data-gid="PR_kwDOJH_K4M5Sz1PN" data-url="/ggerganov/llama.cpp/pull/1827/partials/body" data-channel="eyJjIjoicHVsbF9yZXF1ZXN0OjEzODkzMTkxMTciLCJ0IjoxNjg2NjU0NjU0fQ==--ed80cc392e2c706989fd969365aabadc2239cd05a6236db3d084b2747dd27aec">

<p><a href="https://github.com/JohannesGaessler" data-view-component="true"><img data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" src="https://avatars.githubusercontent.com/u/18492268?s=60&amp;v=4" alt="JohannesGaessler" size="40" height="40" width="40" data-view-component="true"/></a>
  
  
</p><div id="issue-1753438924">
  <div id="pullrequest-1389319117">
          

          <div>
        <div>
  
  <task-lists disabled="" sortable="">
    <div>
      <p dir="auto">This PR adds GPU acceleration for all remaining ggml tensors that didn&#39;t yet have it. Especially for long generations this makes a large difference because the KV cache is still CPU only on master and gets larger as the context fills up. Prompt processing is also significantly faster because the large batch size allows the more effective use of GPUs. For the following performance numbers PP is prompt processing, and TG 128/1024 are the generation of 128/1024 tokens with an empty prompt:</p>
<table role="table">
<thead>
<tr>
<th>GPU</th>
<th>Test</th>
<th>Model</th>
<th>t/s master</th>
<th>t/s PR</th>
<th>Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>RTX 3090</td>
<td>PP</td>
<td>7b q4_0</td>
<td>452</td>
<td>990</td>
<td>2.19</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>PP</td>
<td>13b q4_0</td>
<td>318</td>
<td>645</td>
<td>2.03</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>PP</td>
<td>33b q4_0</td>
<td>155</td>
<td>292</td>
<td>1.88</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>TG 128</td>
<td>7b q4_0</td>
<td>45.66</td>
<td>62.66</td>
<td>1.37</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>TG 128</td>
<td>13b q4_0</td>
<td>29.94</td>
<td>38.21</td>
<td>1.28</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>TG 128</td>
<td>33b q4_0</td>
<td>14.89</td>
<td>17.61</td>
<td>1.18</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>TG 1024</td>
<td>7b q4_0</td>
<td>31.08</td>
<td>56.34</td>
<td>1.81</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>TG 1024</td>
<td>13b q4_0</td>
<td>20.16</td>
<td>35.20</td>
<td>1.75</td>
</tr>
<tr>
<td>RTX 3090</td>
<td>TG 1024</td>
<td>33b q4_0</td>
<td>10.37</td>
<td>16.49</td>
<td>1.59</td>
</tr>
</tbody>
</table>
<p dir="auto">Note that I was only using a single thread for the PR since multiple threads have no benefit when all computations are on the GPU but they still add overhead.</p>
<p dir="auto">I added CUDA kernels for scale, cpy, diag_mask_inf, and soft_max. I also added two special kernels for doing matrix vector multiplication with permuted or not contiguous inputs; they are used in conjunction with the KV cache.</p>
<p dir="auto">Changes to <code>ggml.c</code>: I added a utility function <code>ggml_is_permuted</code>.</p>
<p dir="auto">Things that are still to do:</p>
<ul>
<li> Fix VRAM memory leaks.</li>
<li> Fix memory usage prints.</li>
<li> Check performance for lower-end GPUs and add a <code>--low-vram</code> option if necessary.</li>
<li> Check Windows performance and maybe disable features.</li>
<li> General code cleanup.</li>
</ul>
    </div>
  </task-lists>
  
</div>

      </div>

          <!-- '"` --><!-- </textarea></xmp> -->
          <div>
        <div data-view-component="true">
  <!-- '"` --><!-- </textarea></xmp> --><form data-turbo="false" action="/ggerganov/llama.cpp/reactions" accept-charset="UTF-8" method="post">
    
    <div>
          <tool-tip id="tooltip-8c922850-cda8-41a6-81c8-f57d950c92b2" for="reactions--reaction_button_component-7e3012" data-direction="n" data-type="description" data-view-component="true">mirek190, slaren, Glavin001, Extraltodeus, avischiffmann, oobabooga, sequoiar, titoBouzout, franchb, transitive-bullshit, and 56 more reacted with thumbs up emoji</tool-tip>
          <tool-tip id="tooltip-30573ed7-1b89-4acc-ba02-d01900ffd585" for="reactions--reaction_button_component-55a03b" data-direction="n" data-type="description" data-view-component="true">thushan, ggerganov, pabl-o-ce, bxh-io, dakl, theolivenbaum, mdawid, fredsadaghiani, melodysdreamj, DerekChia, and 8 more reacted with hooray emoji</tool-tip>
          <tool-tip id="tooltip-9af35088-7532-4098-88cc-53e9da90c749" for="reactions--reaction_button_component-262294" data-direction="n" data-type="description" data-view-component="true">vihangd, mudler, and Josua996 reacted with heart emoji</tool-tip>
          <tool-tip id="tooltip-66e1c5f1-c983-4c2c-9874-7559b45f77b5" for="reactions--reaction_button_component-f55cec" data-direction="n" data-type="description" data-view-component="true">Green-Sky, Extraltodeus, avischiffmann, TheSeamau5, tmostak, megupta, sequoiar, andrewschreiber, ijt, DaseinPhaos, and 29 more reacted with rocket emoji</tool-tip>
      
    </div>
</form></div>
      </div>

</div>
  </div>
</div>

       
            


      


  

        <div data-gid="IC_kwDOJH_K4M5erer8">
  
      
<div data-gid="IC_kwDOJH_K4M5erer8" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5erer8/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/cmp-nct/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/cmp-nct"><img src="https://avatars.githubusercontent.com/u/78893154?s=80&amp;v=4" width="40" height="40" alt="@cmp-nct"/></a>

</p>


  <div id="issuecomment-1588456188">

    <div data-body-version="5394167381037b38cde8ba6e018cd6535da4583db3d654e396a63670f2a294fe">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Stunning work, can&#39;t wait testing it tomorrow. Given the benchmark results this is a huge step forward..</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5erkky">
  
      
<div data-gid="IC_kwDOJH_K4M5erkky" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5erkky/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/thushan/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/thushan"><img src="https://avatars.githubusercontent.com/u/68254?s=80&amp;u=fcd03bd638aacd80f7fe4017f9fdca8e2e548b65&amp;v=4" width="40" height="40" alt="@thushan"/></a>

</p>


  <div id="issuecomment-1588480306">

    <div data-body-version="b6b7c94a51190c042003d975beb59aeeaf632703759367e3d26193e459cea520">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">This is huge news, it really whips the llama.cpp&#39;s ass on CUDA <g-emoji alias="partying_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f973.png">ðŸ¥³</g-emoji></p>
<p dir="auto">Early attempt this morning we&#39;re getting ~2.5-2.8x perf increase on 4090s and about 1.8-2x on 3090Ti.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5erlk7">
  
      
<div data-gid="IC_kwDOJH_K4M5erlk7" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5erlk7/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/ggerganov/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ggerganov"><img src="https://avatars.githubusercontent.com/u/1991296?s=80&amp;u=28314d364d7c28f8ec232fadb767970d3ad74e7b&amp;v=4" width="40" height="40" alt="@ggerganov"/></a>

</p>


  <div id="issuecomment-1588484411">

    <div data-body-version="75856569c683aa868db2de833437a08253c7535a28116827f902df4bb0f8572d">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Congrats <a data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler">@JohannesGaessler</a>!</p>
<blockquote>
<blockquote>
<p dir="auto">I notice that llama.cpp is also pegged at 100% of one core, so I&#39;m assuming a faster single-core CPU would likewise scale llama.cpp&#39;s figure.</p>
</blockquote>
<p dir="auto">Currently the llama.cpp main thread always waits for the GPU computation to finish before arranging the next tensor. But it should be possible to change the logic to instead start preparing the next tensor immediately. Then I think CPU performance will be largely irrelevant.</p>
</blockquote>
<p dir="auto">We will refactor and improve the CPU threading and synchronization logic of <code>ggml</code> soon</p>
      </div>
</task-lists>


        <div>

            <div>
              <div data-view-component="true">
  <!-- '"` --><!-- </textarea></xmp> --><form data-turbo="false" action="/ggerganov/llama.cpp/reactions" accept-charset="UTF-8" method="post">
    
      
    <div>
          <tool-tip id="tooltip-f9fc8aed-b90c-4da2-ac38-ddc659a0fc1b" for="reactions--reaction_button_component-04377d" data-direction="n" data-type="description" data-view-component="true">lin72h reacted with thumbs up emoji</tool-tip>
          <tool-tip id="tooltip-281a9723-8051-4e7f-b684-32a31c9f7226" for="reactions--reaction_button_component-cf48e4" data-direction="n" data-type="description" data-view-component="true">thushan, pabl-o-ce, deadprogram, Rafaelblsilva, nightlyworker, junaid33, Volrath50, PiotrDabkowski, Priestru, ericvolp12, and 3 more reacted with heart emoji</tool-tip>
      
    </div>
</form></div>
            </div>
        </div>
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        

        <div data-gid="IC_kwDOJH_K4M5eryQd">
  
      
<div data-gid="IC_kwDOJH_K4M5eryQd" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5eryQd/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/deadprogram/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/deadprogram"><img src="https://avatars.githubusercontent.com/u/5520?s=80&amp;v=4" width="40" height="40" alt="@deadprogram"/></a>

</p>


  

</div>


</div>


        

        <div data-gid="IC_kwDOJH_K4M5esIYc">
  
      
<div data-gid="IC_kwDOJH_K4M5esIYc" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5esIYc/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/Priestru/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/Priestru"><img src="https://avatars.githubusercontent.com/u/108554892?s=80&amp;u=eebbd5bf2d738b7bba42dd5b0d79c0a2db9b53eb&amp;v=4" width="40" height="40" alt="@Priestru"/></a>

</p>


  <div id="issuecomment-1588626972">

    <div data-body-version="06aaea7c75df0cbca7d62cd2431746b3677dfe84e4964b11cc47f32c090c28c1">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Would it support multi GPU?</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5esdyv">
  
      
<div data-gid="IC_kwDOJH_K4M5esdyv" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5esdyv/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler"><img src="https://avatars.githubusercontent.com/u/18492268?s=80&amp;u=1c80d97fe8e252e6f6efbb4fbfc17b5aecb8e9a6&amp;v=4" width="40" height="40" alt="@JohannesGaessler"/></a>

</p>


  <div id="issuecomment-1588714671">

    <div data-body-version="017e4561f0c6f5dd0fedc0ea453172736cd2b6fd78d1f67f6a27bce70948ebd2">
      <div>
  <p>
    <details>
        <summary data-view-component="true">    <span>
      <span><svg aria-label="Show options" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg></span>
    </span>
</summary>  

      <details-menu src="" preload="">
          <clipboard-copy aria-label="Copy link" for="issuecomment-1588714671-permalink" role="menuitem" data-view-component="true">
    
            Copy link

</clipboard-copy>      </details-menu>
    </details>
  </p>

  <p><span aria-label="This user has been invited to collaborate on the llama.cpp repository." data-view-component="true">
    <span data-view-component="true">Collaborator</span>
</span>

      
<span aria-label="This user is the author of this issue." data-view-component="true">
  <span data-view-component="true">Author</span>
</span>

  </p>

  <h3>
    

  </h3>
</div>


      <div>

        <task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">Hi, great PR, does this PR allows you to keep the cache after the generation is done? I think statefulness should speedup chats and libraries like Microsoft Guidance and Jsonformer</p>
</blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/LevanKvirkvelia/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/LevanKvirkvelia">@LevanKvirkvelia</a> I am not changing anything other than the location where the KC cache is stored.</p>
<blockquote>
<p dir="auto">We will refactor and improve the CPU threading and synchronization logic of ggml soon</p>
</blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/ggerganov/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ggerganov">@ggerganov</a> What I&#39;m talking about is something different. Currently <code>cudaDeviceSynchronize()</code> is called every time after a calculation is started. But it would be more efficient to instead synchronize at a later point when the results are actually needed which would allow the parallel rather than sequential use of CPU and GPU. Unless the threading PR implements this too.</p>
<blockquote>
<p dir="auto">Would it support multi GPU?</p>
</blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/Priestru/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/Priestru">@Priestru</a> It already does for the computationally most expensive operations. There is no multi GPU support for the KV cache specifically; before I add that I&#39;ll need to see if that would even be worthwhile in the first place.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5esfvT">
  
      
<div data-gid="IC_kwDOJH_K4M5esfvT" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5esfvT/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler"><img src="https://avatars.githubusercontent.com/u/18492268?s=80&amp;u=1c80d97fe8e252e6f6efbb4fbfc17b5aecb8e9a6&amp;v=4" width="40" height="40" alt="@JohannesGaessler"/></a>

</p>


  <div id="issuecomment-1588722643">

    <div data-body-version="024b910f4d112398e7b6b1c1d801f1476abbdb0586772723113c3772c2c4007f">
      <div>
  <p>
    <details>
        <summary data-view-component="true">    <span>
      <span><svg aria-label="Show options" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg></span>
    </span>
</summary>  

      <details-menu src="" preload="">
          <clipboard-copy aria-label="Copy link" for="issuecomment-1588722643-permalink" role="menuitem" data-view-component="true">
    
            Copy link

</clipboard-copy>      </details-menu>
    </details>
  </p>

  <p><span aria-label="This user has been invited to collaborate on the llama.cpp repository." data-view-component="true">
    <span data-view-component="true">Collaborator</span>
</span>

      
<span aria-label="This user is the author of this issue." data-view-component="true">
  <span data-view-component="true">Author</span>
</span>

  </p>

  <h3>
    

  </h3>
</div>


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/TheBloke/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TheBloke">@TheBloke</a> Are you setting <code>LLAMA_CUDA_DMMV_X</code> (default 32) and <code>LLAMA_CUDA_DMMV_Y</code> (default 1) at compile time? These values determine how much data the GPU processes at once for the computationally most expensive operations and setting higher values is beneficial on fast GPUs (but make sure they are powers of 2). On my RTX 3090 setting <code>LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=2</code> increases performance by 20%.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5essjP">
  
      
<div data-gid="IC_kwDOJH_K4M5essjP" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5essjP/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/KerfuffleV2/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/KerfuffleV2"><img src="https://avatars.githubusercontent.com/u/44031344?s=80&amp;u=d54fd79e825f871eeda9c046d07ca67aad7507f1&amp;v=4" width="40" height="40" alt="@KerfuffleV2"/></a>

</p>


  <div id="issuecomment-1588775119">

    <div data-body-version="6e78a5e185b580e7c9bfcac5d91b96381e39a8d0784515b08b882118215d87f9">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Just want to make sure you&#39;re aware this currently prevents using cuBLAS with large models on low VRAM GPUs like my 6GB 1060, even with no GPU offloading. I can&#39;t use full context (<code>-c 2048</code>) with <code>-ngl 0</code> - it still runs out of VRAM.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5es_pD">
  
      
<div data-gid="IC_kwDOJH_K4M5es_pD" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5es_pD/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/Dampfinchen/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/Dampfinchen"><img src="https://avatars.githubusercontent.com/u/59751859?s=80&amp;v=4" width="40" height="40" alt="@Dampfinchen"/></a>

</p>


  <div id="issuecomment-1588853315">

    <div data-body-version="e62c80d233a988f244b3e6826d5d954a20862e68699f511ec8ecd1bc2fe7f1f5">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Sadly with this PR, I get ggml-cuda.cu:1248: out of memory immediately after loading the 13B model.</p>
<p dir="auto">In the regular build, this was my speed (15 GPU layers for my RTX 2060, 6 threads, prompt of 1800 Tokens)</p>
<p dir="auto">Cold run:</p>
<p dir="auto">llama_print_timings:        load time = 44221.63 ms</p>
<p dir="auto">(I&#39;ve used 13 layers on the new master build and this PR which uses around 3,4 GB VRAM)</p>
<p dir="auto">So yeah, while this is good work and a big step forward for GGML in general, the biggest strength of GGML compared to GPTQ is that you can run large models on GPUs with insufficient VRAM at decent speed is suffering drastically with this approach.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        

        <div data-gid="C_kwDOJcXjyNoAKGE3ZmI0MjE0MzExZWQ5MmU4ZjM4YzAzOWIzYzM4MWYzODkxY2Q3OWM">
  
        <div>
  <div>
      <div data-view-component="true">
  
  
  <div data-view-component="true">          <div>
  <div>
    <div>
      
<div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18492268?s=40&amp;v=4" width="20" height="20" alt="@JohannesGaessler"/>
</a>  </p>
</div>

      

      

      

      
    </div>
  </div>
</div>

</div>
</div>  </div>
</div>


</div>

        <div data-gid="IC_kwDOJH_K4M5etK-k">
  
      
<div data-gid="IC_kwDOJH_K4M5etK-k" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etK-k/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/EwoutH/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/EwoutH"><img src="https://avatars.githubusercontent.com/u/15776622?s=80&amp;u=9c906c5065a3f9d057aebd5cd68e7b4e9df56b02&amp;v=4" width="40" height="40" alt="@EwoutH"/></a>

</p>


  <div id="issuecomment-1588899748">

    <div data-body-version="e14e7c90983de708672fc70857de9c59b356bc81b8c2dabad4ec0d88c7cc48e8">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Awesome work!</p>
<p dir="auto">Hereâ€™s the link to the <a href="https://www.reddit.com/r/LocalLLaMA/comments/147z6as/llamacpp_just_got_full_cuda_acceleration_and_now/" rel="nofollow">Reddit thread</a> on r/LocalLLaMA for reference, a lot of people share their benchmarks and experiences on this PR.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etNSP">
  
      
<div data-gid="IC_kwDOJH_K4M5etNSP" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etNSP/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/foolsh/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/foolsh"><img src="https://avatars.githubusercontent.com/u/6065273?s=80&amp;u=f42fb7d2623292fe4490b72d89364533a4a731f7&amp;v=4" width="40" height="40" alt="@foolsh"/></a>

</p>


  <div id="issuecomment-1588909199">

    <div data-body-version="ef8151b11d322c5068d066b7824eeac772a09ff07d3d8a56314d839506affc14">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Tested in a multiple gpu configuration, Tesla P40 24GB and NVIDIA GeForce GTX 1050 Ti 4GB.</p>
<p dir="auto">gcc 11.3.0, ubuntu 22.04.1, CUDA 12.1</p>
<div data-snippet-clipboard-copy-content="nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0"><pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0
</code></pre></div>
<p dir="auto">I get an abort when running in interactive mode with a reverse prompt.</p>
<p dir="auto"><code>CUDA error 1 at ggml-cuda.cu:1920: invalid argument</code></p>
<p dir="auto">It doesn&#39;t seem to be related to context length or n_predict, I ran them as low as 200 with the same error.</p>
<p dir="auto">The command...</p>
<p dir="auto"><code>../llama.cpp/main -m ../llama.cpp/models/OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_0.bin --temp 0.7 --n_predict 1038 --top_p 0.1 --top_k 40 -c 2000 --seed -1 --repeat_penalty 1.1764705882352942 -t 1 --main-gpu 0 --n-gpu-layers 61 -p &#34;USER: write a story about llamas\nASSISTANT:&#34; </code></p>
<p dir="auto">Finishes just fine. as well as...</p>
<p dir="auto"><code>./llama.cpp/main -m ../llama.cpp/models/OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_0.bin --temp 0.7 --n_predict 1038 --top_p 0.1 --top_k 40 -c 2000 --seed -1 --repeat_penalty 1.1764705882352942 -t 1 --main-gpu 0 --n-gpu-layers 61 -i</code></p>
<p dir="auto">Even when interrupted with ctrl-c and after giving input and returning control.</p>
<p dir="auto">However the command...</p>
<p dir="auto"><code>../llama.cpp/main -m ../llama.cpp/models/OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_0.bin --temp 0.7 --n_predict 1038 --top_p 0.1 --top_k 40 -c 2000 --seed -1 --repeat_penalty 1.1764705882352942 -t 1 --main-gpu 0 --n-gpu-layers 61 -i --reverse-prompt user:</code></p>
<p dir="auto">fails after any input given to user: even just hitting return.</p>
<p dir="auto">You&#39;re doing excellent work with this PR, keep it up!</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etPbY">
  
      
<div data-gid="IC_kwDOJH_K4M5etPbY" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etPbY/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/lhl/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/lhl"><img src="https://avatars.githubusercontent.com/u/2581?s=80&amp;v=4" width="40" height="40" alt="@lhl"/></a>

</p>


  <div id="issuecomment-1588917976">

    <div data-body-version="a02546cc9b75fc367ccd0a80baec2a75ee89873af60a3676e5266b210de6361e">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">I&#39;m getting the same <code>CUDA error 1 at ggml-cuda.cu:1920: invalid argument</code> error. It doesn&#39;t happen when I try a regular test, but if I make it go longer (eg <code>-n 2048 --ignore-eos</code>) it seems to happen eventually every time.</p>
<p dir="auto">This is on a 4090 w/ CUDA 12.1 on Arch Linux.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etRuv">
  
      
<div data-gid="IC_kwDOJH_K4M5etRuv" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etRuv/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler"><img src="https://avatars.githubusercontent.com/u/18492268?s=80&amp;u=1c80d97fe8e252e6f6efbb4fbfc17b5aecb8e9a6&amp;v=4" width="40" height="40" alt="@JohannesGaessler"/></a>

</p>


  <div id="issuecomment-1588927407">

    <div data-body-version="543eb679cbb9b1dca642da3daaba06fcce0880a827bd8ac29387f48d765d51c9">
      <div>
  <p>
    <details>
        <summary data-view-component="true">    <span>
      <span><svg aria-label="Show options" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg></span>
    </span>
</summary>  

      <details-menu src="" preload="">
          <clipboard-copy aria-label="Copy link" for="issuecomment-1588927407-permalink" role="menuitem" data-view-component="true">
    
            Copy link

</clipboard-copy>      </details-menu>
    </details>
  </p>

  <p><span aria-label="This user has been invited to collaborate on the llama.cpp repository." data-view-component="true">
    <span data-view-component="true">Collaborator</span>
</span>

      
<span aria-label="This user is the author of this issue." data-view-component="true">
  <span data-view-component="true">Author</span>
</span>

  </p>

  <h3>
    

  </h3>
</div>


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">The error seems to have to do with not all layers being on the GPU. The max. effective values for <code>--n-gpu-layers</code> are 33 for 7b, 41 for 13b, 61 for 33b, and 81 for 65b. Setting <code>--n-gpu-layers 99</code> will always offload all layers.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etX6w">
  
      
<div data-gid="IC_kwDOJH_K4M5etX6w" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etX6w/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler"><img src="https://avatars.githubusercontent.com/u/18492268?s=80&amp;u=1c80d97fe8e252e6f6efbb4fbfc17b5aecb8e9a6&amp;v=4" width="40" height="40" alt="@JohannesGaessler"/></a>

</p>


  <div id="issuecomment-1588952752">

    <div data-body-version="1fc661a76e129645cf5df272c313a18975caed118ae899d6209f885c42a804cf">
      <div>
  <p>
    <details>
        <summary data-view-component="true">    <span>
      <span><svg aria-label="Show options" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg></span>
    </span>
</summary>  

      <details-menu src="" preload="">
          <clipboard-copy aria-label="Copy link" for="issuecomment-1588952752-permalink" role="menuitem" data-view-component="true">
    
            Copy link

</clipboard-copy>      </details-menu>
    </details>
  </p>

  <p><span aria-label="This user has been invited to collaborate on the llama.cpp repository." data-view-component="true">
    <span data-view-component="true">Collaborator</span>
</span>

      
<span aria-label="This user is the author of this issue." data-view-component="true">
  <span data-view-component="true">Author</span>
</span>

  </p>

  <h3>
    

  </h3>
</div>


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/ggerganov/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ggerganov">@ggerganov</a> The issue that people are reporting seems to have to do with the generation exceeding the context size. I assume the KV cache needs to be modified when that happens and I did not consider that modification for my implementation. Unfortunately I&#39;m having trouble understanding the relevant code. Can you give me a quick rundown of what llama.cpp does when the context size is exceeded?</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etZZm">
  
      
<div data-gid="IC_kwDOJH_K4M5etZZm" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etZZm/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/TheBloke/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TheBloke"><img src="https://avatars.githubusercontent.com/u/784313?s=80&amp;u=5b2667f8e0620839f9ea612d367c151dd776e20e&amp;v=4" width="40" height="40" alt="@TheBloke"/></a>

</p>


  <div id="issuecomment-1588958822">

    <div data-body-version="c8a539efa043e4b2109ecfe0b8fd72392098e12c17380cbec74f62129fd64bc1">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=2</p>
</blockquote>
<p dir="auto">No sorry, I wasn&#39;t aware of those.</p>
<p dir="auto">I&#39;ve just tested but am having some issues:</p>
<p dir="auto">With <code>LLAMA_CUDA_DMMV_Y=2</code>, <code>4</code>, or<code> 8</code> I always get:</p>
<div data-snippet-clipboard-copy-content=".GGML_ASSERT: ggml-cuda.cu:2218: nrows % GGML_CUDA_DMMV_Y == 0"><pre><code>.GGML_ASSERT: ggml-cuda.cu:2218: nrows % GGML_CUDA_DMMV_Y == 0
</code></pre></div>
<p dir="auto">This appears to be irrespective of the <code>LLAMA_CUDA_DMMV_X</code> value (tested 32, 64, 128)</p>
<details>
  <summary>Full compile + execution log</summary>
<div data-snippet-clipboard-copy-content="root@5462b0a2cf6a:/workspace/llama.cpp-PR# make clean &amp;&amp; LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=2 make -j  ; ./main --color --threads 1 -ngl 100 -n 100 --ignore-eos -m /workspace/WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin  -p &#34;USER: write a story about llamas\nASSISTANT:&#34;
I llama.cpp build info:
I UNAME_S:  Linux
I UNAME_P:  x86_64
I UNAME_M:  x86_64
I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS
I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native
I LDFLAGS:
I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

rm -vf *.o main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server vdot build-info.h
removed &#39;common.o&#39;
removed &#39;ggml-cuda.o&#39;
removed &#39;ggml.o&#39;
removed &#39;k_quants.o&#39;
removed &#39;llama.o&#39;
removed &#39;main&#39;
removed &#39;quantize&#39;
removed &#39;quantize-stats&#39;
removed &#39;perplexity&#39;
removed &#39;embedding&#39;
removed &#39;vdot&#39;
removed &#39;build-info.h&#39;
I llama.cpp build info:
I UNAME_S:  Linux
I UNAME_P:  x86_64
I UNAME_M:  x86_64
I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include
I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include
I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o
cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c
nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=64 -DGGML_CUDA_DMMV_Y=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib

====  Run ./main -h for help.  ====

main: build = 678 (0fe5ff2)
main: seed  = 1686650041
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090
llama.cpp: loading model from /workspace/WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32001
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 14 (mostly Q4_K - Small)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =    0.07 MB
llama_model_load_internal: using CUDA for GPU acceleration
llama_model_load_internal: mem required  = 1932.72 MB (+ 1026.00 MB per state)
llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer
llama_model_load_internal: offloading 32 layers to GPU
llama_model_load_internal: offloading output layer to GPU
llama_model_load_internal: total VRAM used: 3987 MB
.GGML_ASSERT: ggml-cuda.cu:2218: nrows % GGML_CUDA_DMMV_Y == 0
Aborted (core dumped)
root@5462b0a2cf6a:/workspace/llama.cpp-PR#"><pre><code>root@5462b0a2cf6a:/workspace/llama.cpp-PR# make clean &amp;&amp; LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=2 make -j  ; ./main --color --threads 1 -ngl 100 -n 100 --ignore-eos -m /workspace/WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin  -p &#34;USER: write a story about llamas\nASSISTANT:&#34;
I llama.cpp build info:
I UNAME_S:  Linux
I UNAME_P:  x86_64
I UNAME_M:  x86_64
I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS
I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native
I LDFLAGS:
I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

rm -vf *.o main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server vdot build-info.h
removed &#39;common.o&#39;
removed &#39;ggml-cuda.o&#39;
removed &#39;ggml.o&#39;
removed &#39;k_quants.o&#39;
removed &#39;llama.o&#39;
removed &#39;main&#39;
removed &#39;quantize&#39;
removed &#39;quantize-stats&#39;
removed &#39;perplexity&#39;
removed &#39;embedding&#39;
removed &#39;vdot&#39;
removed &#39;build-info.h&#39;
I llama.cpp build info:
I UNAME_S:  Linux
I UNAME_P:  x86_64
I UNAME_M:  x86_64
I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include
I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include
I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o
cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c
nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=64 -DGGML_CUDA_DMMV_Y=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib
g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib

====  Run ./main -h for help.  ====

main: build = 678 (0fe5ff2)
main: seed  = 1686650041
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090
llama.cpp: loading model from /workspace/WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin
llama_model_load_internal: format     = ggjt v3 (latest)
llama_model_load_internal: n_vocab    = 32001
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: ftype      = 14 (mostly Q4_K - Small)
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =    0.07 MB
llama_model_load_internal: using CUDA for GPU acceleration
llama_model_load_internal: mem required  = 1932.72 MB (+ 1026.00 MB per state)
llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer
llama_model_load_internal: offloading 32 layers to GPU
llama_model_load_internal: offloading output layer to GPU
llama_model_load_internal: total VRAM used: 3987 MB
.GGML_ASSERT: ggml-cuda.cu:2218: nrows % GGML_CUDA_DMMV_Y == 0
Aborted (core dumped)
root@5462b0a2cf6a:/workspace/llama.cpp-PR#
</code></pre></div>
</details>
<p dir="auto">Testing with:</p>
<div data-snippet-clipboard-copy-content="LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=1
LLAMA_CUDA_DMMV_X=128 LLAMA_CUDA_DMMV_Y=1"><pre><code>LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=1
LLAMA_CUDA_DMMV_X=128 LLAMA_CUDA_DMMV_Y=1
</code></pre></div>
<p dir="auto">I wasn&#39;t able to notice any performance improvement testing 7B and 30B q4_K_S, testing with H100 + Intel(R) Xeon(R) Platinum 8480+ and 4090 + i9-13900K</p>
<p dir="auto">(Does the DMMV_X do anything without an increased DMMV_Y?)</p>
<details>
  <summary>Benchmark log on 4090 + i9-13900K, 7B and 30B q4_K_S</summary>
<div data-snippet-clipboard-copy-content="root@5462b0a2cf6a:/workspace/llama.cpp-PR# model=WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin ; for arg1 in 32 64 128 ; do compileargs=(LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=&#34;$arg1&#34; LLAMA_CUDA_DMMV_Y=1) ; ( make clean &amp;&amp; env &#34;${compileargs[@]}&#34; make -j ) &gt; /dev/null 2&gt;&amp;1 ; echo &#34;${compileargs[@]}: $model&#34;  ; for i in {1..3} ; do ( ./main --color --threads 1 -ngl 100 -n 500 --ignore-eos -m /workspace/&#34;$model&#34;  -p &#34;USER: write a story about llamas\nASSISTANT:&#34; 2&gt;&amp;1 ) | grep &#34;eval time.*runs&#34; ; done ; done
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=32 LLAMA_CUDA_DMMV_Y=1: WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time =  4524.39 ms /   498 runs   (    9.09 ms per token)
llama_print_timings:        eval time =  4513.35 ms /   498 runs   (    9.06 ms per token)
llama_print_timings:        eval time =  4517.48 ms /   498 runs   (    9.07 ms per token)
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=1: WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time =  4524.29 ms /   498 runs   (    9.08 ms per token)
llama_print_timings:        eval time =  4529.45 ms /   498 runs   (    9.10 ms per token)
llama_print_timings:        eval time =  4511.65 ms /   498 runs   (    9.06 ms per token)
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=128 LLAMA_CUDA_DMMV_Y=1: WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time =  4513.87 ms /   498 runs   (    9.06 ms per token)
llama_print_timings:        eval time =  4517.59 ms /   498 runs   (    9.07 ms per token)
llama_print_timings:        eval time =  4523.78 ms /   498 runs   (    9.08 ms per token)

root@5462b0a2cf6a:/workspace/llama.cpp-PR# model=wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin ; for arg1 in 32 64 128 ; do compileargs=(LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=&#34;$arg1&#34; LLAMA_CUDA_DMMV_Y=1) ; ( make clean &amp;&amp; env &#34;${compileargs[@]}&#34; make -j ) &gt; /dev/null 2&gt;&amp;1 ; echo &#34;${compileargs[@]}: $model&#34;  ; for i in {1..3} ; do ( ./main --color --threads 1 -ngl 100 -n 500 --ignore-eos -m /workspace/&#34;$model&#34;  -p &#34;USER: write a story about llamas\nASSISTANT:&#34; 2&gt;&amp;1 ) | grep &#34;eval time.*runs&#34; ; done ; done
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=32 LLAMA_CUDA_DMMV_Y=1: wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time = 17051.55 ms /   498 runs   (   34.24 ms per token)
llama_print_timings:        eval time = 17069.43 ms /   498 runs   (   34.28 ms per token)
llama_print_timings:        eval time = 17106.45 ms /   498 runs   (   34.35 ms per token)
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=1: wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time = 17083.03 ms /   498 runs   (   34.30 ms per token)
llama_print_timings:        eval time = 17097.12 ms /   498 runs   (   34.33 ms per token)
llama_print_timings:        eval time = 17098.24 ms /   498 runs   (   34.33 ms per token)
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=128 LLAMA_CUDA_DMMV_Y=1: wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time = 17187.51 ms /   498 runs   (   34.51 ms per token)
llama_print_timings:        eval time = 17134.88 ms /   498 runs   (   34.41 ms per token)
llama_print_timings:        eval time = 17135.03 ms /   498 runs   (   34.41 ms per token)"><pre><code>root@5462b0a2cf6a:/workspace/llama.cpp-PR# model=WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin ; for arg1 in 32 64 128 ; do compileargs=(LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=&#34;$arg1&#34; LLAMA_CUDA_DMMV_Y=1) ; ( make clean &amp;&amp; env &#34;${compileargs[@]}&#34; make -j ) &gt; /dev/null 2&gt;&amp;1 ; echo &#34;${compileargs[@]}: $model&#34;  ; for i in {1..3} ; do ( ./main --color --threads 1 -ngl 100 -n 500 --ignore-eos -m /workspace/&#34;$model&#34;  -p &#34;USER: write a story about llamas\nASSISTANT:&#34; 2&gt;&amp;1 ) | grep &#34;eval time.*runs&#34; ; done ; done
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=32 LLAMA_CUDA_DMMV_Y=1: WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time =  4524.39 ms /   498 runs   (    9.09 ms per token)
llama_print_timings:        eval time =  4513.35 ms /   498 runs   (    9.06 ms per token)
llama_print_timings:        eval time =  4517.48 ms /   498 runs   (    9.07 ms per token)
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=1: WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time =  4524.29 ms /   498 runs   (    9.08 ms per token)
llama_print_timings:        eval time =  4529.45 ms /   498 runs   (    9.10 ms per token)
llama_print_timings:        eval time =  4511.65 ms /   498 runs   (    9.06 ms per token)
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=128 LLAMA_CUDA_DMMV_Y=1: WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time =  4513.87 ms /   498 runs   (    9.06 ms per token)
llama_print_timings:        eval time =  4517.59 ms /   498 runs   (    9.07 ms per token)
llama_print_timings:        eval time =  4523.78 ms /   498 runs   (    9.08 ms per token)

root@5462b0a2cf6a:/workspace/llama.cpp-PR# model=wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin ; for arg1 in 32 64 128 ; do compileargs=(LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=&#34;$arg1&#34; LLAMA_CUDA_DMMV_Y=1) ; ( make clean &amp;&amp; env &#34;${compileargs[@]}&#34; make -j ) &gt; /dev/null 2&gt;&amp;1 ; echo &#34;${compileargs[@]}: $model&#34;  ; for i in {1..3} ; do ( ./main --color --threads 1 -ngl 100 -n 500 --ignore-eos -m /workspace/&#34;$model&#34;  -p &#34;USER: write a story about llamas\nASSISTANT:&#34; 2&gt;&amp;1 ) | grep &#34;eval time.*runs&#34; ; done ; done
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=32 LLAMA_CUDA_DMMV_Y=1: wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time = 17051.55 ms /   498 runs   (   34.24 ms per token)
llama_print_timings:        eval time = 17069.43 ms /   498 runs   (   34.28 ms per token)
llama_print_timings:        eval time = 17106.45 ms /   498 runs   (   34.35 ms per token)
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=64 LLAMA_CUDA_DMMV_Y=1: wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time = 17083.03 ms /   498 runs   (   34.30 ms per token)
llama_print_timings:        eval time = 17097.12 ms /   498 runs   (   34.33 ms per token)
llama_print_timings:        eval time = 17098.24 ms /   498 runs   (   34.33 ms per token)
LLAMA_CUBLAS=1 LLAMA_CUDA_DMMV_X=128 LLAMA_CUDA_DMMV_Y=1: wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin
llama_print_timings:        eval time = 17187.51 ms /   498 runs   (   34.51 ms per token)
llama_print_timings:        eval time = 17134.88 ms /   498 runs   (   34.41 ms per token)
llama_print_timings:        eval time = 17135.03 ms /   498 runs   (   34.41 ms per token)
</code></pre></div>
</details>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5eth3C">
  
      
<div data-gid="IC_kwDOJH_K4M5eth3C" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5eth3C/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/lhl/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/lhl"><img src="https://avatars.githubusercontent.com/u/2581?s=80&amp;v=4" width="40" height="40" alt="@lhl"/></a>

</p>


  <div id="issuecomment-1588993474">

    <div data-body-version="ef5490cdbbcd725063230de3f488f1b2bd32d86d5b836255b8f4edd1355cd472">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">The error seems to have to do with not all layers being on the GPU. The max. effective values for <code>--n-gpu-layers</code> are 33 for 7b, 41 for 13b, 61 for 33b, and 81 for 65b. Setting <code>--n-gpu-layers 99</code> will always offload all layers.</p>
</blockquote>
<p dir="auto">Yes, that fixes it. Curious why the off-by-one on the command line parameter? I&#39;ve been using <code>-ngl 40</code> with 13b models because n_layer is reported as 40 by llama.cpp itself.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etiBP">
  
      
<div data-gid="IC_kwDOJH_K4M5etiBP" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etiBP/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/shouyiwang/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/shouyiwang"><img src="https://avatars.githubusercontent.com/u/72741?s=80&amp;u=0dd76c93636c4f20ea47382cb2d2dd4564305a2d&amp;v=4" width="40" height="40" alt="@shouyiwang"/></a>

</p>


  <div id="issuecomment-1588994127">

    <div data-body-version="a0a7ec2df3c8ff57ad184d3fe1bbbdf4a66bbabb9f4950c7c8996c58b05cd711">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Is there any room for improvement in VRAM usage?</p>
<p dir="auto">For example, a 33B Q4_K_M model could comfortably fit in a 24GB GPU, but it cannot after this PR, resulting in much slower inference speed.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etjg1">
  
      
<div data-gid="IC_kwDOJH_K4M5etjg1" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etjg1/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler"><img src="https://avatars.githubusercontent.com/u/18492268?s=80&amp;u=1c80d97fe8e252e6f6efbb4fbfc17b5aecb8e9a6&amp;v=4" width="40" height="40" alt="@JohannesGaessler"/></a>

</p>


  <div id="issuecomment-1589000245">

    <div data-body-version="9e5fd3fbac036ae29d14d5aef3b957299a5291ed0c277356dcce4f426ca29360">
      <div>
  <p>
    <details>
        <summary data-view-component="true">    <span>
      <span><svg aria-label="Show options" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg></span>
    </span>
</summary>  

      <details-menu src="" preload="">
          <clipboard-copy aria-label="Copy link" for="issuecomment-1589000245-permalink" role="menuitem" data-view-component="true">
    
            Copy link

</clipboard-copy>      </details-menu>
    </details>
  </p>

  <p><span aria-label="This user has been invited to collaborate on the llama.cpp repository." data-view-component="true">
    <span data-view-component="true">Collaborator</span>
</span>

      
<span aria-label="This user is the author of this issue." data-view-component="true">
  <span data-view-component="true">Author</span>
</span>

  </p>

  <h3>
    

  </h3>
</div>


      <div>

        <task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">Yes, that fixes it. Curious why the off-by-one on the command line parameter? I&#39;ve been using -ngl 40 with 13b models because n_layer is reported as 40 by llama.cpp itself.</p>
</blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/lhl/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/lhl">@lhl</a> Because LLaMa has both repeating and non-repeating layers. The values up to 40 offload the repeating layers and 41 or higher offloads the non-repeating layers as well.</p>
<blockquote>
<p dir="auto">Is there any room for improvement in VRAM usage?</p>
</blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/shouyiwang/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/shouyiwang">@shouyiwang</a> Possibly but on a fundamental level the speed increase comes from the higher VRAM usage in the first place.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etjv2">
  
      
<div data-gid="IC_kwDOJH_K4M5etjv2" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etjv2/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/TheBloke/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TheBloke"><img src="https://avatars.githubusercontent.com/u/784313?s=80&amp;u=5b2667f8e0620839f9ea612d367c151dd776e20e&amp;v=4" width="40" height="40" alt="@TheBloke"/></a>

</p>


  <div id="issuecomment-1589001206">

    <div data-body-version="86125147abde3eecc3092c4fb3913f7068840a6e44048275eab417b612a7536b">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">Yeah I was just writing about that myself. On Reddit there&#39;s a couple of people pointing out that the increased VRAM requirements are pushing them over the VRAM limits of their cards, and thus resulting in overall worse performance with their chosen quant sizes. Two examples:</p>
<blockquote>
<p dir="auto">Previously, a 33b q4_k_m could fit into 24GB vram, but now it cannot, resulting in much slower speeds. For a 65b q4_k_s, it was possible to fit 47 layers into the GPU, but now only 35 can fit, resulting in a 20% decrease in speed.</p>
</blockquote>
<blockquote>
<p dir="auto">Interesting you say it&#39;s not harmed. My rtx 2060 gets wrecked(immediately oom) trying to load 13b with 13 gpu layers and a 1800 token prompt. Worked great in main.</p>
</blockquote>
<p dir="auto">Would it be possible to support both the old and new methods?  That is, a command line flag to control whether KV cache is in VRAM or not? So that people who feel they really don&#39;t want to drop down a quant size can still get the performance they had before, while also having the option to get the awesome new performance if they choose a quant big enough to fit entirely in their VRAM?</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etkrj">
  
      
<div data-gid="IC_kwDOJH_K4M5etkrj" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etkrj/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/shouyiwang/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/shouyiwang"><img src="https://avatars.githubusercontent.com/u/72741?s=80&amp;u=0dd76c93636c4f20ea47382cb2d2dd4564305a2d&amp;v=4" width="40" height="40" alt="@shouyiwang"/></a>

</p>


  <div id="issuecomment-1589005027">

    <div data-body-version="130f08babe5d119f487b02f1b0e93ab563a15283b359eec4cf7c7fdd0d2e5fee">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto">Would it be possible to support both the old and new methods? That is, a command line flag to control whether KV cache is in VRAM or not? So that people who feel they really don&#39;t want to drop down a quant size can still get the performance they had before, while also having the option to get the awesome new performance if they choose a quant big enough to fit entirely in their VRAM?</p>
</blockquote>
<p dir="auto">That would be great.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etlda">
  
      
<div data-gid="IC_kwDOJH_K4M5etlda" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etlda/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler"><img src="https://avatars.githubusercontent.com/u/18492268?s=80&amp;u=1c80d97fe8e252e6f6efbb4fbfc17b5aecb8e9a6&amp;v=4" width="40" height="40" alt="@JohannesGaessler"/></a>

</p>


  <div id="issuecomment-1589008218">

    <div data-body-version="f4676ad11b89e2d33bedb7d00f568c57e7dd32597d3c877334a9db881e0dc127">
      <div>
  <p>
    <details>
        <summary data-view-component="true">    <span>
      <span><svg aria-label="Show options" role="img" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M8 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3ZM1.5 9a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Zm13 0a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3Z"></path>
</svg></span>
    </span>
</summary>  

      <details-menu src="" preload="">
          <clipboard-copy aria-label="Copy link" for="issuecomment-1589008218-permalink" role="menuitem" data-view-component="true">
    
            Copy link

</clipboard-copy>      </details-menu>
    </details>
  </p>

  <p><span aria-label="This user has been invited to collaborate on the llama.cpp repository." data-view-component="true">
    <span data-view-component="true">Collaborator</span>
</span>

      
<span aria-label="This user is the author of this issue." data-view-component="true">
  <span data-view-component="true">Author</span>
</span>

  </p>

  <h3>
    

  </h3>
</div>


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">I plan to make moving the KV cache to VRAM optional but before I decide on a specific implementation for the user interface I&#39;ll need to do some performance testing. If it turns out that the KV cache is always less efficient in terms of t/s per VRAM then I think I&#39;ll just extend the logic for <code>--n-gpu-layers</code> to offload the KV cache after the regular layers if the value is high enough.</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etmfX">
  
      
<div data-gid="IC_kwDOJH_K4M5etmfX" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etmfX/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/TheBloke/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TheBloke"><img src="https://avatars.githubusercontent.com/u/784313?s=80&amp;u=5b2667f8e0620839f9ea612d367c151dd776e20e&amp;v=4" width="40" height="40" alt="@TheBloke"/></a>

</p>


  

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etx9I">
  
      
<div data-gid="IC_kwDOJH_K4M5etx9I" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etx9I/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/ggerganov/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ggerganov"><img src="https://avatars.githubusercontent.com/u/1991296?s=80&amp;u=28314d364d7c28f8ec232fadb767970d3ad74e7b&amp;v=4" width="40" height="40" alt="@ggerganov"/></a>

</p>


  <div id="issuecomment-1589059400">

    <div data-body-version="ba95ef6bef8c91a2f6a6d86767572fef8d7d6079c790607ec4e6d9fbe9c3d6bf">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <blockquote>
<p dir="auto"><a data-hovercard-type="user" data-hovercard-url="/users/ggerganov/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ggerganov">@ggerganov</a> The issue that people are reporting seems to have to do with the generation exceeding the context size. I assume the KV cache needs to be modified when that happens and I did not consider that modification for my implementation. Unfortunately I&#39;m having trouble understanding the relevant code. Can you give me a quick rundown of what llama.cpp does when the context size is exceeded?</p>
</blockquote>
<p dir="auto">See this comment for description of how the KV cache is managed upon filling up the context: <a data-error-text="Failed to load title" data-id="1620571749" data-permission-text="Title is private" data-url="https://github.com/ggerganov/llama.cpp/issues/71" data-hovercard-type="issue" data-hovercard-url="/ggerganov/llama.cpp/issues/71/hovercard?comment_id=1483907574&amp;comment_type=issue_comment" href="https://github.com/ggerganov/llama.cpp/issues/71#issuecomment-1483907574">#71 (comment)</a></p>
<p dir="auto">I haven&#39;t looked at the PR yet - will do so later. But in general, the CUDA code should not be re-implementing or emulating the context reprocessing described in the comment (we also call this &#34;context swap&#34;). <a href="https://github.com/ggerganov/llama.cpp/blob/74d4cfa3438cb58bd177eed30014e6588694aaa8/examples/main/main.cpp#L349-L374">This logic</a> is part of the <code>llama.cpp</code> implementation. Hope this helps - let me know if something is unclear</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        <div data-gid="IC_kwDOJH_K4M5etyAs">
  
      
<div data-gid="IC_kwDOJH_K4M5etyAs" data-url="/ggerganov/llama.cpp/comments/IC_kwDOJH_K4M5etyAs/partials/timeline_issue_comment">

  <p><a data-hovercard-type="user" data-hovercard-url="/users/TheBloke/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/TheBloke"><img src="https://avatars.githubusercontent.com/u/784313?s=80&amp;u=5b2667f8e0620839f9ea612d367c151dd776e20e&amp;v=4" width="40" height="40" alt="@TheBloke"/></a>

</p>


  <div id="issuecomment-1589059628">

    <div data-body-version="34135ea05f6181cad3a7d53bfdbf0fb765bca5e74270608b0551b8343e399436">
      


      <div>

        <task-lists disabled="" sortable="">
<div>
          <p dir="auto">By the way Johannes, if you want any perf tests done, I&#39;m happy to help.  I have 24/7 access to:</p>
<ul dir="auto">
<li>H100 80GB + Intel(R) Xeon(R) Platinum 8480+</li>
<li>A10 24GB + Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz</li>
</ul>
<p dir="auto">And I&#39;m happy to spin up Runpod containers which can provide any 3000 or 4000 series GPUs, plus A4000, A5000, A6000, L40, A30, A40, A100, etc.  CPU is more variable; most of the Runpod-owned systems use AMD EPYC, but community servers have a wider range of options, including consumer stuff like i9-13900K, i7-13700K, Ryzen 7800X, etc. But I can find a reasonable range of CPU + GPU combos, especially for the consumer GPUs.</p>
<p dir="auto">(What I can&#39;t find - and am desperate to test - is an H100 on an i9-13900K! <g-emoji alias="grinning" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f600.png">ðŸ˜€</g-emoji> )</p>
      </div>
</task-lists>


        
      </div>

      <!-- '"` --><!-- </textarea></xmp> -->    </div>
</div>

</div>


</div>


        

        <div data-gid="C_kwDOJcXjyNoAKGQ3M2E0NzY2NDkyNzRkYTY5MmVlNmQwNWZkMWM3NmYzMWExMjdiMzY">
  
        <div>
  <div>
      <div data-view-component="true">
  
  
  <div data-view-component="true">          <div>
  <div>
    <div>
      
<div>
  <p><a data-test-selector="commits-avatar-stack-avatar-link" data-hovercard-type="user" data-hovercard-url="/users/JohannesGaessler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/JohannesGaessler">
        <img data-test-selector="commits-avatar-stack-avatar-image" src="https://avatars.githubusercontent.com/u/18492268?s=40&amp;v=4" width="20" height="20" alt="@JohannesGaessler"/>
</a>  </p>
</div>

      

      

      

      
    </div>
  </div>
</div>

</div>
</div>  </div>
</div>


</div>



  <!-- Rendered timeline since 2023-06-13 04:03:25 -->
  



    </div>

    
  </div></div>
  </body>
</html>
