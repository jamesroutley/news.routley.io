<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggerganov/llama.cpp/pull/1827">Original</a>
    <h1>Llama.cpp: Full CUDA GPU Acceleration</h1>
    
    <div id="readability-page-1" class="page"><div disabled="" sortable="">
<div>
          <p dir="auto">Tested in a multiple gpu configuration, Tesla P40 24GB and NVIDIA GeForce GTX 1050 Ti 4GB.</p>
<p dir="auto">gcc 11.3.0, ubuntu 22.04.1, CUDA 12.1</p>
<div data-snippet-clipboard-copy-content="nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0"><pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Mon_Apr__3_17:16:06_PDT_2023
Cuda compilation tools, release 12.1, V12.1.105
Build cuda_12.1.r12.1/compiler.32688072_0
</code></pre></div>
<p dir="auto">I get an abort when running in interactive mode with a reverse prompt.</p>
<p dir="auto"><code>CUDA error 1 at ggml-cuda.cu:1920: invalid argument</code></p>
<p dir="auto">It doesn&#39;t seem to be related to context length or n_predict, I ran them as low as 200 with the same error.</p>
<p dir="auto">The command...</p>
<p dir="auto"><code>../llama.cpp/main -m ../llama.cpp/models/OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_0.bin --temp 0.7 --n_predict 1038 --top_p 0.1 --top_k 40 -c 2000 --seed -1 --repeat_penalty 1.1764705882352942 -t 1 --main-gpu 0 --n-gpu-layers 61 -p &#34;USER: write a story about llamas\nASSISTANT:&#34; </code></p>
<p dir="auto">Finishes just fine. as well as...</p>
<p dir="auto"><code>./llama.cpp/main -m ../llama.cpp/models/OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_0.bin --temp 0.7 --n_predict 1038 --top_p 0.1 --top_k 40 -c 2000 --seed -1 --repeat_penalty 1.1764705882352942 -t 1 --main-gpu 0 --n-gpu-layers 61 -i</code></p>
<p dir="auto">Even when interrupted with ctrl-c and after giving input and returning control.</p>
<p dir="auto">However the command...</p>
<p dir="auto"><code>../llama.cpp/main -m ../llama.cpp/models/OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_0.bin --temp 0.7 --n_predict 1038 --top_p 0.1 --top_k 40 -c 2000 --seed -1 --repeat_penalty 1.1764705882352942 -t 1 --main-gpu 0 --n-gpu-layers 61 -i --reverse-prompt user:</code></p>
<p dir="auto">fails after any input given to user: even just hitting return.</p>
<p dir="auto">You&#39;re doing excellent work with this PR, keep it up!</p>
      </div>
</div></div>
  </body>
</html>
