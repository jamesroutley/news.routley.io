<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://unsloth.ai/blog/gemma-bugs">Original</a>
    <h1>Fixing Gemma Bugs</h1>
    
    <div id="readability-page-1" class="page"><div data-ws-id="FDU7mYSdzrOfsj8qlMVE7" data-ws-component="Box"><p><span data-ws-id="cMTBOFTj10SdF_FgPp_G7" data-ws-component="Text">Over the past week, Unsloth has been hard at work finding and fixing Gemma bugs. At first, Google showcased Gemma’s promising results however, many problems like discrepancies in loss values made us step in to help Gemma live up to its initial promise.</span></p><ol data-ws-id="iit0latK7UnH8qtLN8mMr" data-ws-component="List"><li data-ws-id="tBwvPbfUe_kukVzyIVeYP" data-ws-component="ListItem">Must add &lt;bos&gt;</li><li data-ws-id="9maWltY1klpux3p7lIgP_" data-ws-component="ListItem">Paper typo? &lt;end_of_turn&gt;model</li><li data-ws-id="YtMDJOgaPC62tqJn2iPY4" data-ws-component="ListItem">sqrt(3072)=55.4256 but bfloat16 is 55.5</li><li data-ws-id="WTsgwfjw_KT_KTGt0ogRo" data-ws-component="ListItem">Layernorm (w+1) should be done in float32</li><li data-ws-id="IsXg90_6-j1fYQrcbb_PO" data-ws-component="ListItem">Keras mixed_bfloat16 RoPE is wrong</li><li data-ws-id="t6995_OgJetdN0wUoZNlY" data-ws-component="ListItem">RoPE is sensitive to a*(1/x) vs a/x</li><li data-ws-id="vK04wjH90gcEBQL3Ve4jE" data-ws-component="ListItem">RoPE should be float32 not bfloat16 (Fixed in Hugging Face 4.38.2)</li><li data-ws-id="_3czVtzCP3ZYqh2hDGPWl" data-ws-component="ListItem">GELU should be approx tanh not exact (Ongoing PR)</li></ol><p><span data-ws-id="DfS_TiWR42sOuRbWQGiQJ" data-ws-component="Text">💡 Detailed findings</span></p><p>1. Must add &lt;bos&gt;</p><p><span data-ws-id="dhc854aIgAMLFgDd8YDoI" data-ws-component="Text"><span data-ws-id="7YSilAqLL9f4b7AEfZj7O" data-ws-component="Text"><span data-ws-id="qFUAlnL4VlMmzyq5KOomf" data-ws-component="Text"><span data-ws-id="5PYR0xsR_wIcfFDphuPy6" data-ws-component="Text"><span data-ws-id="12u7qgcmLlo2aJzgMZ5YG" data-ws-component="Text">The most important caveat for finetuning is you must add the &lt;bos&gt; token (red loss). The blue loss is no &lt;bos&gt; token. Packing with TRL works, but has a higher base loss - it’s possible Gemma does not use the T5 packing trick anymore! See the <a data-ws-id="IE1tbDhMQmfLUw-cXC_6c" data-ws-component="RichTextLink" href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank">T5 paper page 12</a> or <a data-ws-id="i3vK1uEOJOxPJaR-OcEBo" data-ws-component="RichTextLink" href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/generator_utils.py#L598" target="_blank">TensorFlow</a><br/></span></span></span></span></span><img data-ws-id="YLgYf9h32CE8DOvLvfAYz" data-ws-component="Image" src="https://unsloth.ai/cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=1920&amp;quality=80&amp;format=auto" width="844" height="376" sizes="100vw" srcset="/cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=16&amp;quality=80&amp;format=auto 16w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=32&amp;quality=80&amp;format=auto 32w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=48&amp;quality=80&amp;format=auto 48w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=64&amp;quality=80&amp;format=auto 64w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=96&amp;quality=80&amp;format=auto 96w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=128&amp;quality=80&amp;format=auto 128w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=256&amp;quality=80&amp;format=auto 256w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=384&amp;quality=80&amp;format=auto 384w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=640&amp;quality=80&amp;format=auto 640w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=750&amp;quality=80&amp;format=auto 750w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=828&amp;quality=80&amp;format=auto 828w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=1080&amp;quality=80&amp;format=auto 1080w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=1200&amp;quality=80&amp;format=auto 1200w, /cgi/image/BOS_token_eFS7AjFOGqSV0mw7Qnk58.png?width=1920&amp;quality=80&amp;format=auto 1920w" decoding="async" loading="lazy"/></p><p>2. Paper typo? &lt;end_of_turn&gt;model</p><p><span data-ws-id="IC17AYnvT07IeQ8CLvMYE" data-ws-component="Text"><span data-ws-id="DpDF6kZ7tguLhQXIEo41x" data-ws-component="Text">There is a typo in the technical report. &lt;end_of_turn&gt;model is wrong, and should just be &lt;end_of_turn&gt;. And you must add newlines at the end of &lt;start_of_turn&gt;model.</span><span data-ws-id="bgKkXApanaYauYUQQgNWW" data-ws-component="Text"><ol data-ws-id="0xkVraBft-EaOiIRhoz3X" data-ws-component="List"><li data-ws-id="Wq2Z2mTxz4-ch7army12l" data-ws-component="ListItem">No &lt;bos&gt; token generates [Knock knock.]</li><li data-ws-id="yZTIrsWZGYXJSZ2V5yUuv" data-ws-component="ListItem">With &lt;bos&gt; token generates [Gemma who?] which is correct.</li><li data-ws-id="F18jwe69GZkxI8NMOD8mp" data-ws-component="ListItem">With &lt;bos&gt; but no newline generates [\nSure, here is the complete response:]</li><li data-ws-id="IcSFf0OvcRp51dD_WLJe-" data-ws-component="ListItem">With &lt;end_of_turn&gt;model generates [Gemma what?]</li></ol></span><span data-ws-id="yGxoz_TvaEFchZtOZHxSl" data-ws-component="Text">If we assume the report is correct, and [Gemmo who?] is the correct choice, then it’s just &lt;end_of_turn&gt; and not &lt;end_of_turn&gt;model. Also a newline is a must, and the &lt;bos&gt; token.</span></span></p><p>3. sqrt(3072)=55.4256 but bfloat16 is 55.5</p><p><span data-ws-id="z237vc05CB8HyFLBMNq8a" data-ws-component="Text">Interestingly, Gemma multiplies the embeddings by sqrt(hidden_dim). However, there is a precision problem! Gemma uses jnp.sqrt(self.embed_dim) .astype(x.dtype) which means sqrt(3072) = 55.4256, but casting it to bfloat16 rounds it to 55.5. For Gemma 2b, sqrt(2048) = 45.2548, but casting it to bfloat16 makes it 45.25.</span><span data-ws-id="Ipe6wWjnbYdKl0-Jsm6dq" data-ws-component="Text">See <a data-ws-id="Nnw_oSP1ZnGqd7lIVKnUa" data-ws-component="RichTextLink" href="https://github.com/google-deepmind/gemma/blob/main/gemma/modules.py#L60" target="_blank">Gemma repo</a> and <a data-ws-id="_rb8MukikjuDQCezvuLdV" data-ws-component="RichTextLink" href="https://github.com/keras-team/keras-nlp/blob/v0.8.2/keras_nlp/models/gemma/gemma_backbone.py#L150" target="_blank">Keras</a> for reference.</span><img data-ws-id="7KsVfYJiPI7ppBVetRoNJ" data-ws-component="Image" src="https://unsloth.ai/cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=1920&amp;quality=80&amp;format=auto" width="711" height="376" sizes="100vw" srcset="/cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=16&amp;quality=80&amp;format=auto 16w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=32&amp;quality=80&amp;format=auto 32w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=48&amp;quality=80&amp;format=auto 48w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=64&amp;quality=80&amp;format=auto 64w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=96&amp;quality=80&amp;format=auto 96w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=128&amp;quality=80&amp;format=auto 128w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=256&amp;quality=80&amp;format=auto 256w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=384&amp;quality=80&amp;format=auto 384w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=640&amp;quality=80&amp;format=auto 640w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=750&amp;quality=80&amp;format=auto 750w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=828&amp;quality=80&amp;format=auto 828w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=1080&amp;quality=80&amp;format=auto 1080w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=1200&amp;quality=80&amp;format=auto 1200w, /cgi/image/Embedding_i3Hbs2CnyVrNF-otch_tL.png?width=1920&amp;quality=80&amp;format=auto 1920w" decoding="async" loading="lazy"/></p><p>4. Layernorm (w+1) should be done in float32</p><p><span data-ws-id="dADw6x20oia-0LS_o7fE6" data-ws-component="Text">The layernorms must be upcasted to float32 and not bfloat16 or float16 halfway. It must be done unlike Llama’s RMS Layernorm which downcasted before multiplying by the weights. We must downcast at the end.</span><img data-ws-id="q_AKE47y4Lj3xCV0Fq9Ny" data-ws-component="Image" src="https://unsloth.ai/cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=1920&amp;quality=80&amp;format=auto" width="763" height="498" sizes="100vw" srcset="/cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=16&amp;quality=80&amp;format=auto 16w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=32&amp;quality=80&amp;format=auto 32w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=48&amp;quality=80&amp;format=auto 48w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=64&amp;quality=80&amp;format=auto 64w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=96&amp;quality=80&amp;format=auto 96w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=128&amp;quality=80&amp;format=auto 128w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=256&amp;quality=80&amp;format=auto 256w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=384&amp;quality=80&amp;format=auto 384w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=640&amp;quality=80&amp;format=auto 640w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=750&amp;quality=80&amp;format=auto 750w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=828&amp;quality=80&amp;format=auto 828w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=1080&amp;quality=80&amp;format=auto 1080w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=1200&amp;quality=80&amp;format=auto 1200w, /cgi/image/RMS_Layernorm_IQDKDDYqOynaWPbbJT71e.png?width=1920&amp;quality=80&amp;format=auto 1920w" decoding="async" loading="lazy"/></p><p>5. Keras mixed_bfloat16 RoPE is wrong</p><p><span data-ws-id="beyvvvHBiRpu1HQolkRQ8" data-ws-component="Text">TPUs on Colab cast RoPE positions in int32, whilst in Keras, they’re cast to compute_dtype (bfloat16), causing [8190, 8191] to become [8192, 8192]</span><img data-ws-id="YqfVVACTcOCnuH_5T1mK5" data-ws-component="Image" src="https://unsloth.ai/cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=1920&amp;quality=80&amp;format=auto" width="798" height="534" sizes="100vw" srcset="/cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=16&amp;quality=80&amp;format=auto 16w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=32&amp;quality=80&amp;format=auto 32w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=48&amp;quality=80&amp;format=auto 48w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=64&amp;quality=80&amp;format=auto 64w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=96&amp;quality=80&amp;format=auto 96w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=128&amp;quality=80&amp;format=auto 128w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=256&amp;quality=80&amp;format=auto 256w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=384&amp;quality=80&amp;format=auto 384w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=640&amp;quality=80&amp;format=auto 640w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=750&amp;quality=80&amp;format=auto 750w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=828&amp;quality=80&amp;format=auto 828w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=1080&amp;quality=80&amp;format=auto 1080w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=1200&amp;quality=80&amp;format=auto 1200w, /cgi/image/RoPE_Precision_qQH2lVYi4pol4uB136Iuo.png?width=1920&amp;quality=80&amp;format=auto 1920w" decoding="async" loading="lazy"/></p><p>6. RoPE is sensitive to a*(1/x) vs a/x</p><p><span data-ws-id="sIR8rn2e8H7AYhfmR8jlS" data-ws-component="Text">Weirdly precalculating RoPE Embeddings seem to have precision issues when doing the reciprocal first then multiplying, whilst doing a division attains higher accuracy. I call this the RoPE Creation fix, where we follow exactly how Deepmind creates the RoPE sin and cos matrices. You can see the error decreases quite a bit in float32, and has some effect in mixed precision.</span><img data-ws-id="k41yrYSwiZsTDDt4K8O7o" data-ws-component="Image" src="https://unsloth.ai/cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=1200&amp;quality=80&amp;format=auto" width="584" height="485" sizes="100vw" srcset="/cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=16&amp;quality=80&amp;format=auto 16w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=32&amp;quality=80&amp;format=auto 32w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=48&amp;quality=80&amp;format=auto 48w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=64&amp;quality=80&amp;format=auto 64w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=96&amp;quality=80&amp;format=auto 96w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=128&amp;quality=80&amp;format=auto 128w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=256&amp;quality=80&amp;format=auto 256w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=384&amp;quality=80&amp;format=auto 384w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=640&amp;quality=80&amp;format=auto 640w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=750&amp;quality=80&amp;format=auto 750w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=828&amp;quality=80&amp;format=auto 828w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=1080&amp;quality=80&amp;format=auto 1080w, /cgi/image/Gemma_Log_L2_Norm_yD92-zaqpLVFE3QWhxhzs.png?width=1200&amp;quality=80&amp;format=auto 1200w" decoding="async" loading="lazy"/></p><p>7. RoPE should be float32. Fixed it in HF 4.38.2.</p><p><span data-ws-id="7wb2Z0IHym3EkeJitXlSK" data-ws-component="Text">We already <a data-ws-id="mvGfptG34eHxhPvj28IL6" data-ws-component="RichTextLink" href="https://github.com/huggingface/transformers/pull/29285" target="_blank">pushed our first fix</a> for Gemma which reduced errors a lot in transformers. Upgrade your transformers version to 4.38.2 to get the fix. Essentially we found the same issue like in the Keras codebase.<a data-ws-id="hi9K5OUFcSwyRIpLGD70p" data-ws-component="RichTextLink" href="https://unsloth.ai/blog/gemma" target="_blank"> I wrote a detailed bug report and analysis on the PR.</a></span><img data-ws-id="LczhqYwkFPULQaZgdMf5b" data-ws-component="Image" src="https://unsloth.ai/cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=1920&amp;quality=80&amp;format=auto" width="900" height="393" sizes="100vw" srcset="/cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=16&amp;quality=80&amp;format=auto 16w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=32&amp;quality=80&amp;format=auto 32w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=48&amp;quality=80&amp;format=auto 48w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=64&amp;quality=80&amp;format=auto 64w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=96&amp;quality=80&amp;format=auto 96w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=128&amp;quality=80&amp;format=auto 128w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=256&amp;quality=80&amp;format=auto 256w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=384&amp;quality=80&amp;format=auto 384w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=640&amp;quality=80&amp;format=auto 640w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=750&amp;quality=80&amp;format=auto 750w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=828&amp;quality=80&amp;format=auto 828w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=1080&amp;quality=80&amp;format=auto 1080w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=1200&amp;quality=80&amp;format=auto 1200w, /cgi/image/RoPE_Loss_6BEgnNC4RZzxsBDvHHSXk.png?width=1920&amp;quality=80&amp;format=auto 1920w" decoding="async" loading="lazy"/></p><p>8. GELU should be approx tanh not exact. Ongoing PR</p><p><span data-ws-id="M4aWXzIzWGFus3yl4ZDMl" data-ws-component="Text">And finally <a data-ws-id="zQQfHPkRHILLEF9GA1-6r" data-ws-component="RichTextLink" href="https://twitter.com/danielhanchen/status/1763613620909580505" target="_blank">we identified the approx GELU issue</a></span><img data-ws-id="wftVJDr6yu57o8X7adY0d" data-ws-component="Image" src="https://unsloth.ai/cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=1200&amp;quality=80&amp;format=auto" width="592" height="594" sizes="100vw" srcset="/cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=16&amp;quality=80&amp;format=auto 16w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=32&amp;quality=80&amp;format=auto 32w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=48&amp;quality=80&amp;format=auto 48w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=64&amp;quality=80&amp;format=auto 64w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=96&amp;quality=80&amp;format=auto 96w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=128&amp;quality=80&amp;format=auto 128w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=256&amp;quality=80&amp;format=auto 256w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=384&amp;quality=80&amp;format=auto 384w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=640&amp;quality=80&amp;format=auto 640w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=750&amp;quality=80&amp;format=auto 750w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=828&amp;quality=80&amp;format=auto 828w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=1080&amp;quality=80&amp;format=auto 1080w, /cgi/image/Approx_Gelu_MqgVNPUOIJJL0PByVo10d.png?width=1200&amp;quality=80&amp;format=auto 1200w" decoding="async" loading="lazy"/></p><p><span data-ws-id="fScAlwUZw2KH-MeNARqS5" data-ws-component="Text">💕 Support us! </span></p><p><span data-ws-id="tDojOliVZs2SWPOBiui18" data-ws-component="Text">As a team of just 2 brothers with 0 revenue or funding, it would be amazing if you could support us via our <a data-ws-id="--CXwhjqZf6gKdQTHjT-5" data-ws-component="RichTextLink" href="https://ko-fi.com/unsloth" target="_blank">Ko-fi donation page</a>. Shout out to: prateekgupta, machine1235 and cnbeining who are new supporters! 🙏</span></p><p>Thank you for reading!</p><p><span data-ws-id="JGl1GzVHwbHoexHSHBCSv" data-ws-component="Text"><span data-ws-id="sumPuFo-iDSnXx-gPzs0e" data-ws-component="Span">Daniel &amp; Michael Han </span>🦥</span></p></div></div>
  </body>
</html>
