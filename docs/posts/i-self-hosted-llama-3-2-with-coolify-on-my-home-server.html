<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://geek.sg/blog/how-i-self-hosted-llama-32-with-coolify-on-my-home-server-a-step-by-step-guide">Original</a>
    <h1>I Self-Hosted Llama 3.2 with Coolify on My Home Server</h1>
    
    <div id="readability-page-1" class="page"><div><p>Inspired by numerous people migrating their Next.js applications from Vercel to self-hosted VPS on Hetzner due to pricing concerns, I decided to explore self-hosting some of my non-critical applications. Additionally, I wanted to push my technical boundaries by running Llama 3.2 using Ollama and making its API available to power AI applications for my business, Wisp.</p><p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/f532cf09-87ff-435b-bede-be0677b4419f.jpg/public" alt="Home server as a step stool"/></p><p>The objective was to breathe new life into an old home server that once ran high-frequency trading and MEV algorithms but had since become a step stool for my daughter to climb onto the TV console.</p><p>This blog post chronicles my journey of setting up Coolify to run Ollama (using Llama 3.2) on my home server, with a particular focus on the challenges and triumphs of enabling GPU acceleration using the CUDA toolkit.</p><p>(Update)</p><p>Since some of you are curious, this is how Llama 3.2 3B perform on a GeForce RTX 2060:</p><p><iframe allowfullscreen="true" src="https://www.youtube.com/embed/3vhJ6fNW8AI"></iframe></p><h2>My Goals</h2><p>The primary idea was to leverage my home server, previously gathering dust, to perform valuable tasks. Specifically, I wanted it to host and automate AI-related functions. Additionally, this setup would provide a centralized location to run Supabase for storing various data. The broader goal includes:</p><ul><li><p><strong>Serving a Next.js website</strong>: This site should be live on the public Internet, auto-deploy from the master branch, work with a public subdomain, and maintain no open public ports for security.</p></li><li><p><strong>Running Llama 3.2</strong>: Utilizing the GPU for agentic tasks and making a locally accessible API.</p></li><li><p><strong>Wildcard domain</strong>: Enabling new services to spin up effortlessly under varied subdomains.</p></li></ul><h2>Overall Experience</h2><p>Setting up this environment was no small feat, but each step was a valuable learning experience. Here&#39;s a walkthrough of my journey:</p><ol><li><p><strong>Installing Ubuntu 24</strong>: This was a breeze, requiring only a single reboot.</p></li><li><p><strong>Coolify Installation</strong>: The process was smooth thanks to the handy install script, which ensured most prerequisites were met. A minor hiccup was resolved by running commands as root to avoid permission issues with the <code>/data</code> directory.</p></li><li><p><strong>Configuring Coolify</strong>: Initially, setting the server as <a target="_blank" href="http://localhost">localhost</a> prevented assigning a domain via Cloudflare Tunnel. The fix involved adding the host as a second &#39;server&#39;. Moreover, configuring the tunnel and SSL correctly took time but was crucial for security and functionality.</p></li><li><p><strong>Cloudflare Tunnel</strong>: Patience was key here. The wildcard domain setup was essential, and understanding the nuances of Cloudflare’s free SSL certificate coverage saved time and money.</p></li><li><p><strong>Deployment Wins</strong>: Successfully hosting my personal blog using Coolify over a Cloudflare Tunnel was a significant morale boost, fueling the energy needed to proceed with the Ollama setup.</p></li><li><p><strong>Running Ollama</strong>: Coolify made deploying the Ollama service straightforward. However, initial trials showed slow inference speeds and heavy CPU usage.</p></li><li><p><strong>Enabling GPU</strong>: Ubuntu 24 had the NVIDIA drivers pre-installed, but CUDA toolkit installation and configuration posed challenges. Persistent efforts led to discovering the need for the <code>nvidia-container-toolkit</code> and Docker service configuration modifications to enable GPU usage. The results were remarkable, reducing inference time by over 10x.</p></li><li><p><strong>API Exposure</strong>: Securing the LLM API with an API key became the next challenge. After unsuccessful attempts with nginx, I found potential solutions by using Caddy. Something I&#39;ll work on next after writing this post.</p></li></ol><h2><strong>Server Specifications</strong></h2><p>For context, here are the specifications of my home server:</p><ul><li><p><strong>CPU</strong>: AMD Ryzen 9 5950X 16-Core</p></li><li><p><strong>GPU</strong>: GeForce RTX 2060</p></li><li><p><strong>RAM</strong>: 4 x 16GB DDR4 3200 MT/s</p></li><li><p><strong>SSD</strong>: 2TB SSD + 8TB SSD</p></li></ul><h2><strong>Step-by-Step Guide</strong></h2><h3><strong>1. Install Ubuntu (For a New Setup)</strong></h3><p>Start by installing Ubuntu on your home server. Follow the detailed guide available on the <a target="_blank" href="https://ubuntu.com/tutorials/install-ubuntu-desktop"><strong>official Ubuntu website</strong></a>.</p><p><strong>Important Settings:</strong></p><ul><li><p>Avoid using LVM or disk encryption for a smoother reboot experience and easier server management. Note that this trade-off means anyone with physical access can read your data.</p></li><li><p>Ensure the installation of third-party drivers to automatically get the NVIDIA driver.</p></li></ul><h3><strong>Install SSH</strong></h3><p>Enable SSH to access your server remotely, which is especially useful if you’re managing it from another machine on your local network. Refer to this <a target="_blank" href="https://linuxize.com/post/how-to-enable-ssh-on-ubuntu-20-04/"><strong>SSH setup guide for Ubuntu 20.04</strong></a>, suitable for Ubuntu 24 as well.</p><h3><strong>Update and Upgrade APT</strong></h3><p>Always perform an update and upgrade for your packages:</p><pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y</code></pre><h3>Useful Commands</h3><p>Here are some additional commands for printing information about your machine and setup:</p><ul><li><p>View CPU usage: <code>htop</code></p></li><li><p>List NVIDIA graphics card information: <code>lspci | grep -i nvidia</code></p></li><li><p>Print architecture, OS distro, release: <code>uname -m &amp;&amp; cat /etc/*release</code></p></li><li><p>Print physical RAM installed: <code>sudo dmidecode --type memory | less</code></p></li><li><p>Print processor info: <code>cat /proc/cpuinfo | grep &#39;name&#39;| uniq</code></p></li><li><p>Check NVIDIA driver information: <code>nvidia-smi</code></p></li></ul><h3><strong>2. Installing Coolify</strong></h3><p><a target="_blank" href="https://coolify.io/">Coolify</a> is an open-source platform designed to make deploying and managing your applications on self-hosted environments easier. Its key feature is allowing users to manage full-stack applications, databases, and services without relying on complex Kubernetes setups. Coolify streamlines deployments through a user-friendly interface, supporting services like Docker, GitHub, and GitLab.</p><p>To install Coolify, follow the automated installation instructions from <a target="_blank" href="https://coolify.io/docs/installation/">their documentation</a>:</p><pre><code>curl -fsSL https://cdn.coollabs.io/coolify/install.sh | bash</code></pre><p><strong>Important Notes</strong>:</p><ul><li><p>Ensure you’re logged in as the root user to avoid permission issues.</p></li><li><p>The installation script checks prerequisites, but you may need to troubleshoot some dependency errors if they arise.</p></li></ul><p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/43c2d6eb-aba3-45ab-aad9-0f47e8895fa5.png/public" alt="Coolify onboarding screen"/></p><p>Once Coolify is installed:</p><ul><li><p><strong>Visit the coolify dashboard</strong> at <code>http://localhost:8000</code> . This is also available on the network. Replace localhost with the ip address of the server if you are accessing it from another machine.</p></li><li><p><strong>Set up an admin account</strong> - this is stored locally on your server.</p></li><li><p><strong>Create your first project</strong> by adding a resource and deploying it to <code>localhost</code>. In my case, I deployed my personal blog first to test the setup.</p></li></ul><h3><strong>3. Setting Up a Cloudflare Tunnel on Coolify</strong></h3><p>Cloudflare Tunnel is a secure way to expose services running on your local network to the public internet without having to open ports on your router. For my setup, this was a key feature, as I wanted to keep my server behind a firewall while still allowing external access to some services.</p><p>Cloudflare’s Zero Trust platform ensures that all traffic is encrypted and routed securely, preventing unauthorized access.</p><p>To set up a Cloudflare Tunnel, follow this instructions on Coolify’s <a target="_blank" href="https://coolify.io/docs/knowledge-base/cloudflare/tunnels">official documentation</a>. The key is to focus on setting up <strong>wildcard subdomains</strong> for all your services.</p><p>A few key caveats:</p><ol><li><p><strong>Localhost Server Issue</strong>: You cannot assign a domain to the pre-created <code>localhost</code> server directly. To solve this, add your host as a second server within Coolify, using the IP address <code>172.17.0.1</code> for <code>host.docker.internal</code> (since coolify will show an error that <code>host.docker.internal</code> has already been assigned to a server).</p></li><li><p><strong>Wildcard Domain Setup</strong>: Make sure you use a top-level domain like <code>*.example.com</code>. If you use a wildcard on a subdomain, Cloudflare will not provide a free SSL certificate, unless you opt for the ACM plan.</p></li></ol><p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/4c338964-bf9d-4466-975a-fca268f56795.png/public"/></p><p>After setting up the Cloudflare Tunnel:</p><ul><li><p>Deploy your resources to the newly added server.</p></li><li><p>Change the domain in the service configuration to match your new subdomain.</p></li><li><p>Once everything is deployed, you should be able to access your service live from its custom domain.</p></li></ul><h3><strong>4. Deploying Ollama</strong></h3><p>Once you’ve set up your Coolify project, the next step is to deploy Ollama. This service allows you to run large language models (LLMs) like Llama 3.2 on your server with a web-based interface.</p><ol><li><p><strong>Add a New Resource</strong>: In your Coolify project, select &#34;Add Resource&#34; and choose <strong>Ollama with Open WebUI</strong> as the service you want to deploy to your new server.</p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/2b634b6c-17db-4f5c-afba-615b4cc1cef0.png/public" alt="Deploying Ollama in Coolify"/></li><li><p><strong>Configure the Domain</strong>: After adding Ollama, configure the domain for the Open WebUI service. Assign a domain from the wildcard domain you set up earlier through Cloudflare Tunnel. This will allow you to access the Ollama WebUI directly from the internet.</p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/d7eb2920-5e28-47b9-9d02-9cb3740ca3b5.png/public" alt="Configuring domain on Coolify resource"/></li><li><p><strong>Deploy the Service</strong>: Once everything is set up, click on &#34;Deploy.&#34;</p><p>You should now be able to access the Open WebUI via the assigned domain. Upon your first login, you&#39;ll be prompted to create an admin account. This is important for managing models and access through the UI.</p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/69c5be6f-4570-4bea-b334-123c5b9cb25f.png/public" alt="WebUI for Ollama"/></li><li><p><strong>Install Llama 3.2</strong>: With your admin account set up, you can now install the latest Llama model. Head to <a target="_blank" href="https://ollama.com/library">Ollama&#39;s library</a> and search for the Llama model you want to use. I opted for <strong>Llama 3.2</strong>, which can be installed using the tag <code>llama3.2</code>.</p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/6d081703-a0eb-4412-97d0-05bb765f3afc.png/public" alt="Downloading models using WebUI with Ollama"/></li><li><p><strong>Try Your First Chat</strong>: Once installed, initiate your first chat with Llama 3.2 via the web interface. During this phase, your model will likely run on the CPU, so expect to hear your machine working hard (with increased CPU fan noise).</p><p>To monitor your machine’s performance during this, use the following commands:</p><ul><li><p><code>htop</code> to keep an eye on <strong>CPU usage</strong>.</p></li><li><p><code>watch -n 0.5 nvidia-smi</code> to track <strong>GPU usage</strong> (though at this stage, GPU may not be utilized yet).</p></li></ul><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/6052bfb2-63bb-49cf-a75b-c4f643dee830.png/public" alt="Crazy CPU usage when Llama 3.2 runs using CPU"/></li></ol><h3><strong>5. Configuring Ollama to Use GPU</strong></h3><p>Large language models (LLMs) like Llama 3.2 perform significantly better with GPU acceleration. GPUs, particularly those from NVIDIA, are optimized for the heavy parallel computations involved in AI workloads, which is where CUDA (Compute Unified Device Architecture) comes into play. The CUDA toolkit enables direct GPU acceleration for applications like Ollama, drastically reducing inference time and CPU load.</p><p>This is arguably the most challenging step in setting up Ollama on your server, but here’s a breakdown of the process:</p><ol><li><p><strong>(Already Done)</strong>: Install the NVIDIA driver (this should have been handled during your Ubuntu installation).</p></li><li><p><strong>Install the NVIDIA CUDA Toolkit</strong>: This toolkit is necessary to unlock GPU acceleration.</p></li><li><p><strong>(Optional)</strong>: Test that the CUDA toolkit is working correctly.</p></li><li><p><strong>Install the NVIDIA Container Toolkit</strong>: This will allow Docker containers (like Ollama) to access the GPU.</p></li><li><p><strong>Enable the Ollama service in Coolify to use the GPU</strong>.</p></li></ol><p><strong>Install the NVIDIA CUDA Toolkit</strong></p><p>Follow NVIDIA’s official <a target="_blank" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu">installation guide</a> to install the CUDA toolkit for your system. I recommend using the <strong>network repository installation method</strong> for the most flexibility and ease of updates.</p><ol><li><p>Install the new <code>cuda-keyring</code> package:</p><pre><code>wget https://developer.download.nvidia.com/compute/cuda/repos/&lt;distro&gt;/&lt;arch&gt;/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb</code></pre><p>Replace <code>&lt;distro&gt;/&lt;arch&gt;</code> with the appropriate value for your distribution and architecture:</p><ul><li><p><code>ubuntu2004/arm64</code></p></li><li><p><code>ubuntu2004/sbsa</code></p></li><li><p><code>ubuntu2004/x86_64</code></p></li><li><p><code>ubuntu2204/sbsa</code></p></li><li><p><code>ubuntu2204/x86_64</code></p></li><li><p><code>ubuntu2404/sbsa</code></p></li><li><p><code>ubuntu2404/x86_64</code></p></li></ul></li><li><p>Update the APT repository cache:</p><pre><code>sudo apt-get update</code></pre></li><li><p>Install the CUDA SDK:</p><pre><code>sudo apt-get install cuda-toolkit</code></pre></li><li><p>Set up the environment for CUDA by adding its binaries to your PATH:</p><pre><code>export PATH=/usr/local/cuda-12.6/bin${PATH:+:${PATH}}</code></pre></li><li><p>Reboot the system to ensure all configurations take effect:</p><pre><code>sudo reboot</code></pre></li></ol><p><strong>(Optional) Test that CUDA Toolkit Works</strong></p><p>To ensure that your system is correctly configured for GPU acceleration, test the CUDA installation by compiling and running sample programs provided by NVIDIA (<a target="_blank" href="https://github.com/nvidia/cuda-samples">https://github.com/nvidia/cuda-samples</a>).</p><ol><li><p>First, install the necessary build tools:</p><pre><code>sudo apt install build-essential</code></pre></li><li><p>Clone the CUDA sample repository and build the sample projects:</p><pre><code>git clone https://github.com/nvidia/cuda-samples
cd cuda-samples
make</code></pre></li><li><p>Navigate to the compiled binaries and run the <code>deviceQuery</code> tool to verify your GPU and CUDA installation:</p><pre><code>cd bin/x86_64/linux/release
./deviceQuery</code></pre><p>You should see detailed information about your GPU and CUDA environment, confirming that the toolkit is working correctly.</p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/fba84378-4c3c-4712-9230-ba2476654826.png/public" alt="deviceQuery showing CUDA Capable device(s)"/></li></ol><p><strong>Install NVIDIA Container Toolkit</strong></p><p>To enable Docker containers to access your GPU, you&#39;ll need to install the NVIDIA Container Toolkit. This toolkit allows Docker to offload GPU-intensive operations to your NVIDIA GPU, essential for speeding up tasks like model inference with Ollama.</p><p>Follow the steps below from <a target="_blank" href="https://hub.docker.com/r/ollama/ollama">Ollama docker docs</a> to install the NVIDIA Container Toolkit:</p><ol><li><p><strong>Configure the repository</strong>:</p><pre><code>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
    | sed &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39; \
    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update</code></pre></li><li><p><strong>Install the NVIDIA Container Toolkit packages</strong>:</p><pre><code>sudo apt-get install -y nvidia-container-toolkit</code></pre></li></ol><p>With the container toolkit installed, Docker will now be able to use your GPU.</p><p><strong>Enable Ollama Service in Coolify to Use GPU</strong></p><p>To enable GPU support for the Ollama service in Coolify, you&#39;ll need to modify the Docker Compose file to allow access to all the GPUs on the host machine.</p><ol><li><p><strong>Edit the Compose File</strong>: Navigate to the Ollama service in Coolify and select &#34;Edit Compose File.&#34;</p><img src="https://imagedelivery.net/lLmNeOP7HXG0OqaG97wimw/clwdlsohu0000v1qer09du8mo/b3cff181-410b-49fc-82fe-6bfbe0280854.png/public" alt="Edit compose file in Coolify"/></li><li><p><strong>Add GPU Configuration</strong>: Append the following configuration under the <code>ollama-api</code> resource. This enables Docker to utilize all GPUs available on the host system:</p><pre><code>    deploy:
      resources:
        reservations:
          devices:
            -
              driver: nvidia
              count: all
              capabilities:
                - gpu</code></pre></li><li><p><strong>Restart the Service</strong>: After saving the changes, restart the Ollama service by clicking the &#34;Restart&#34; button in Coolify.</p></li></ol><p>Once restarted, Ollama will now leverage your GPU for model inference. You can verify that it&#39;s using the GPU by running:</p><ul><li><p><code>htop</code> to check CPU usage (it should remain low).</p></li><li><p><code>watch -n 0.5 nvidia-smi</code> to see the GPU usage in action.</p></li></ul><p><strong>Testing GPU Performance</strong></p><p>Try initiating another conversation with Llama 3.2 via the web UI. This time, you should notice a significant reduction in CPU load, as the GPU will handle the inference tasks.</p><p>Congratulations! You’ve successfully configured Ollama to use GPU acceleration through Coolify on your home server!</p><h3><strong>Next Steps</strong></h3><p>The final step in securing your setup is to expose the LLM API to the internet while ensuring it’s protected by an API key. Using Caddy, you can enforce API key access for the Ollama service.</p><p>For a detailed guide, refer to this <a target="_blank" href="https://github.com/ollama/ollama/issues/849">discussion</a>.</p><h2>Conclusion</h2><p>In this post, I detailed my journey of setting up Llama 3.2 on my home server, utilizing GPU acceleration to handle AI workloads efficiently. Starting from a simple Ubuntu setup, I navigated the complexities of installing NVIDIA drivers, configuring Docker for GPU support, and deploying Ollama using Coolify. With this setup, I now have a powerful AI system running locally, handling agentic tasks with ease.</p><p>This guide walks through the entire process, from software installations to troubleshooting, and provides a blueprint for anyone looking to do the same.</p><h3><strong>References</strong></h3><ul><li><p><a target="_blank" href="https://www.reddit.com/r/ollama/comments/1c8ddv8/ollama_doesnt_use_gpu_pls_help/">Reddit: Ollama Not Using GPU</a></p></li><li><p><a target="_blank" href="https://www.reddit.com/r/LocalLLaMA/comments/1cew9fb/is_ollama_supposed_to_run_on_your_gpu/">Reddit: Is Ollama Supposed to Run on GPU?</a></p></li><li><p><a target="_blank" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/">NVIDIA CUDA Installation Guide</a></p></li><li><p><a target="_blank" href="https://hub.docker.com/r/ollama/ollama">Ollama Docker Hub</a></p></li><li><p><a target="_blank" href="https://stackoverflow.com/questions/70761192/docker-compose-equivalent-of-docker-run-gpu-all-option">StackOverflow: Docker Compose GPU Setup</a></p></li></ul><p><small><a target="_blank" href="https://www.wisp.blog/?utm_source=web&amp;utm_campaign=attribution&amp;utm_content=cm2bbhb1u000111hp13ioe9i3">Powered by wisp</a></small></p></div></div>
  </body>
</html>
