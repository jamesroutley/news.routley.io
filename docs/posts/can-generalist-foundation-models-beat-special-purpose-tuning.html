<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2311.16452">Original</a>
    <h1>Can generalist foundation models beat special-purpose tuning?</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nori,+H">Harsha Nori</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+Y+T">Yin Tat Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Sheng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Carignan,+D">Dean Carignan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Edgar,+R">Richard Edgar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fusi,+N">Nicolo Fusi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=King,+N">Nicholas King</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Larson,+J">Jonathan Larson</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yuanzhi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+W">Weishung Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+R">Renqian Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=McKinney,+S+M">Scott Mayer McKinney</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ness,+R+O">Robert Osazuwa Ness</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Poon,+H">Hoifung Poon</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qin,+T">Tao Qin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Usuyama,+N">Naoto Usuyama</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=White,+C">Chris White</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Horvitz,+E">Eric Horvitz</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2311.16452.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4&#39;s capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model&#39;s out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Eric Horvitz [<a href="https://arxiv.org/show-email/e2d76e18/2311.16452">view email</a>]      </p></div></div>
  </body>
</html>
