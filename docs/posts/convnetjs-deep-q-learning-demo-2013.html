<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html">Original</a>
    <h1>ConvNetJS Deep Q Learning Demo (2013)</h1>
    
    <div id="readability-page-1" class="page"><div id="wrap">
   <h2><a href="http://cs.stanford.edu/people/karpathy/convnetjs/">ConvNetJS</a> Deep Q Learning Demo</h2>
   
   <p>
   This demo follows the description of the Deep Q Learning algorithm described in 
   <a href="http://arxiv.org/pdf/1312.5602v1.pdf">Playing Atari with Deep Reinforcement Learning</a>, 
   a paper from NIPS 2013 Deep Learning Workshop from DeepMind. The paper is a nice demo of a fairly
   standard (model-free) Reinforcement Learning algorithm (Q Learning) learning to play Atari games.
   </p>
   <p>
   In this demo, instead of Atari games, we&#39;ll start out with something more simple: 
   a 2D agent that has 9 eyes pointing in different angles ahead and every eye senses 3 values
   along its direction (up to a certain maximum visibility distance): distance to a wall, distance to 
   a green thing, or distance to a red thing. The agent navigates by using one of 5 actions that turn 
   it different angles. The red things are apples and the agent gets reward for eating them. The green
   things are poison and the agent gets negative reward for eating them. The training takes a few tens
   of minutes with current parameter settings.
   </p>
   <p>
   Over time, the agent learns to avoid states that lead to states with low rewards, and picks actions
   that lead to better states instead.
   </p>
   
   <p>
   The textfield below gets eval()&#39;d to produce the Q-learner for this demo. This allows you to fiddle with 
   various parameters and settings and also shows how you can use the API for your own purposes. 
   All of these settings are optional but are listed to give an idea of possibilities.
   Feel free to change things around and hit reload! Documentation for all
   options is the paper linked to above, and there are also 
   comments for every option in the source code javascript file.
   </p>
   
   <p>It&#39;s very simple to use deeqlearn.Brain: Initialize your network:</p>
   <pre>   var brain = new deepqlearn.Brain(num_inputs, num_actions);
   </pre>
   <p>And to train it proceed in loops as follows:</p>
   <pre>   var action = brain.forward(array_with_num_inputs_numbers);
   // action is a number in [0, num_actions) telling index of the action the agent chooses
   // here, apply the action on environment and observe some reward. Finally, communicate it:
   brain.backward(reward); // &lt;-- learning magic happens here
   </pre>
   <p>That&#39;s it! Let the agent learn over time (it will take opt.learning_steps_total), and it
   will only get better and better at accumulating reward as it learns. Note that the agent will still take
   random actions with probability opt.epsilon_min even once it&#39;s fully trained. 
   To completely disable this randomness, or change it, you can disable the learning and set epsilon_test_time to 0:</p>
   <pre>   brain.epsilon_test_time = 0.0; // don&#39;t make any random choices, ever
   brain.learning = false;
   var action = brain.forward(array_with_num_inputs_numbers); // get optimal action from learned policy
   </pre>
   
   
   
   <p><b>Left</b>: Current input state (quite a useless thing to look at). <b>Right</b>: Average reward over time (this should go up as agent becomes better on average at collecting rewards)</p>
   <canvas id="vis_canvas" width="350" height="150"></canvas>
   <canvas id="graph_canvas" width="350" height="150"></canvas></div></div>
  </body>
</html>
