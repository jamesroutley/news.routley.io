<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lesswrong.com/posts/vfRpzyGsikujm9ujj/a-brief-history-of-computers">Original</a>
    <h1>A brief history of computers</h1>
    
    <div id="readability-page-1" class="page"><article>
  <p>I&#39;ve been learning about the neural tangent kernel and some things confused me.</p>
<p>In the NTK paper, the network layers have the form $\frac{1}{\sqrt{n_A}}A$ where $n_A$ is the
number of neurons in $A$. Why the square root?</p>
<p>So I worked it out.</p>
<h2 id="setup">Setup</h2>
<p>Input: $x: \mathbb{^*R_{\lim}}$, where $\mathbb{^*R_{\lim}}$ means a limited
(hyper)real number. Limited just means it&#39;s not infinitely big.</p>
<p>Output: $y: \mathbb{^*R_{\lim}}$.</p>
<p>The inputs and outputs are scalar, but this is just for simple exposition. The
analysis works for vector input/outputs.</p>
<p>Layers: $A, B$ of shapes $H \times 1$ and $1 \times H$ where $H$ is hyperfinite
(infinitely big natural number). Every entry in the matrices is limited.</p>
<p>Activation function: $r = x \mapsto \max(x,0)$, the relu.</p>
<p>$\text{Net}(x) := B (r (Ax))$. I left out biases here, but the analysis doesn&#39;t
depend on them anyway.</p>
<p>Diagram:</p>
<pre><code>    x: (1,)
    |
    V
+---+
|   |
| A |
|(H,1)
|   |
|   |
|   |
|   |
+---+
    |
    | ReLU
    V
+-----------------+
|    B: (1,H)     |
|                 |
+-----------------+
    |
    V
    y: (1,)
</code></pre>
<h2 id="now-what">Now what?</h2>
<p>Brute force. Let&#39;s run $x$ through the net. First is multiplying by $A$ to get
$Ax$. Name it $v$.</p>
<p>$v := Ax$ has shape $(H,)$ (using NumPy notation), a vector of unlimited length.
This is the first sign that something may be up: each of its entries are limited
(because $x$ is of limited length with limited entries), but their norm
($\sqrt{v_1^2+v_2^2 \dots v_H^2}$) may not be since there are an unlimited
number of entries.</p>
<p>Example: Let $v$ be all 1s. Then its norm is $\sqrt{1+1 \dots 1}$ $H$-many
times, which equals $\sqrt{H}$.</p>
<p>We want our network to have limited (aka assignable) values, so what to do? We
can renormalize by inserting a constant in front of the matrix $A$ so its output
is limited. But which constant?</p>
<p>Actually, $\sqrt{H}$ is it.</p>
<h2 id="proof">Proof</h2>
<p>$|v|$ is less than or equal to its maximum element (by absolute value) repeated
$H$-many times. Let $M = \max_{i=0 \dots H} |v_i|$. This exists because H is
hyper<em>finite</em>, and so has a maximum element because finite sets do.</p>
<p>In math: $|v| \leq |(M, M, \dots, M)|$. The magnitude of the right hand side is
$\sqrt{H M^2} = $\sqrt{H}\cdot M$. The factor of $M$ is irrelevant since we only
care about the output being limited, and dividing out the $\sqrt{H}$ will do it.
Any value of a lower order would still diverge, and a higher order (like
$H^{0.6}$ or something) would make the output infinitesimal. Only $\sqrt{H}$ is of the
correct order.</p>
<p>By similar reasoning, for each layer we divide by the square root of the number
of elements in the layer matrix. The activation functions don&#39;t matter too much.</p>
<h2 id="another-way">Another way</h2>
<p>This cannot really be implemented, but is mathematically equivalent. We stop worrying about the latent variables having unlimited
norm and just let them be unlimited since algebraically, they act the same as
any other number.</p>
<p>In that case, let&#39;s consider $\hat{y} = B(r(Ax))$ and assume all the entries
are positive so the relu is trivial and can be ignored, leaving $\hat{y} = B A x$.</p>
<p>The norm $|B A x|$ is bounded above by $|B||A||x|$, which (using the reasoning
above), is of order $\sqrt{H} \sqrt{H} \cdot 1$ where the $1$ is the order of
$x$ (because it&#39;s limited), which doesn&#39;t really matter. This suggests a
different but equivalent renormalization strategy: divide by the product of the square roots of
the number of elements in each layer $\prod_l{\sqrt{n_l}}$, but not at each
step. Only once, at the very end.</p>
<p>In practice, we would want to make each step stay limited (so that it could even
be put on a computer for one thing), but for theoretical
analysis, leaving out normalizing constants until the very end is nice.</p>

</article><p>If you have anything you want to say to me (compliments, criticism, requests for future posts, etc.), please fill out my <a href="https://docs.google.com/forms/d/e/1FAIpQLScY0Iyy94BJqaROoFv1S-2wYf3TraWXi5UFpOVT9k41VPD24g/viewform">feedback form</a></p></div>
  </body>
</html>
