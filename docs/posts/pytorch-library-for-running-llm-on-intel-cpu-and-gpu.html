<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/intel-analytics/ipex-llm">Original</a>
    <h1>PyTorch Library for Running LLM on Intel CPU and GPU</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 1.75C0 .784.784 0 1.75 0h12.5C15.216 0 16 .784 16 1.75v9.5A1.75 1.75 0 0 1 14.25 13H8.06l-2.573 2.573A1.458 1.458 0 0 1 3 14.543V13H1.75A1.75 1.75 0 0 1 0 11.25Zm1.75-.25a.25.25 0 0 0-.25.25v9.5c0 .138.112.25.25.25h2a.75.75 0 0 1 .75.75v2.19l2.72-2.72a.749.749 0 0 1 .53-.22h6.5a.25.25 0 0 0 .25-.25v-9.5a.25.25 0 0 0-.25-.25Zm7 2.25v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 9a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Important</p><p dir="auto"><em><strong><code>bigdl-llm</code> has now become <code>ipex-llm</code> (see the migration guide <a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/bigdl_llm_migration.html" rel="nofollow">here</a>); you may find the original <code>BigDL</code> project <a href="https://github.com/intel-analytics/BigDL-2.x">here</a>.</strong></em></p>
</div>
<hr/>

<p dir="auto"><strong><code>IPEX-LLM</code></strong> is a PyTorch library for running <strong>LLM</strong> on Intel CPU and GPU <em>(e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max)</em> with very low latency<sup><a href="#user-content-fn-1-81ce39395d1a85f86f714ef670a086f0" id="user-content-fnref-1-81ce39395d1a85f86f714ef670a086f0" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>.</p>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<ul dir="auto">
<li><em>It is built on top of <strong>Intel Extension for PyTorch</strong> (<strong><code>IPEX</code></strong>), as well as the excellent work of <strong><code>llama.cpp</code></strong>, <strong><code>bitsandbytes</code></strong>, <strong><code>vLLM</code></strong>, <strong><code>qlora</code></strong>, <strong><code>AutoGPTQ</code></strong>, <strong><code>AutoAWQ</code></strong>, etc.</em></li>
<li><em>It provides seamless integration with <a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/llama_cpp_quickstart.html" rel="nofollow">llama.cpp</a>, <a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/webui_quickstart.html" rel="nofollow">Text-Generation-WebUI</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels">HuggingFace tansformers</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning">HuggingFace PEFT</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LangChain">LangChain</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LlamaIndex">LlamaIndex</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/Deepspeed-AutoTP">DeepSpeed-AutoTP</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/vLLM-Serving">vLLM</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/src/ipex_llm/serving/fastchat">FastChat</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/DPO">HuggingFace TRL</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Applications/autogen">AutoGen</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/ModelScope-Models">ModeScope</a>, etc.</em></li>
<li><em><strong>50+ models</strong> have been optimized/verified on <code>ipex-llm</code> (including LLaMA2, Mistral, Mixtral, Gemma, LLaVA, Whisper, ChatGLM, Baichuan, Qwen, RWKV, and more); see the complete list <a href="#verified-models">here</a>.</em></li>
</ul>
</div>

<ul dir="auto">
<li>[2024/03] <code>bigdl-llm</code> has now become <code>ipex-llm</code> (see the migration guide <a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/bigdl_llm_migration.html" rel="nofollow">here</a>); you may find the original <code>BigDL</code> project <a href="https://github.com/intel-analytics/bigdl-2.x">here</a>.</li>
<li>[2024/02] <code>ipex-llm</code> now supports directly loading model from <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/ModelScope-Models">ModelScope</a> (<a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/ModelScope-Models">魔搭</a>).</li>
<li>[2024/02] <code>ipex-llm</code> added inital <strong>INT2</strong> support (based on llama.cpp <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF-IQ2">IQ2</a> mechanism), which makes it possible to run large-size LLM (e.g., Mixtral-8x7B) on Intel GPU with 16GB VRAM.</li>
<li>[2024/02] Users can now use <code>ipex-llm</code> through <a href="https://github.com/intel-analytics/text-generation-webui">Text-Generation-WebUI</a> GUI.</li>
<li>[2024/02] <code>ipex-llm</code> now supports <em><a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Inference/Self_Speculative_Decoding.html" rel="nofollow">Self-Speculative Decoding</a></em>, which in practice brings <strong>~30% speedup</strong> for FP16 and BF16 inference latency on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/Speculative-Decoding">GPU</a> and <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Speculative-Decoding">CPU</a> respectively.</li>
<li>[2024/02] <code>ipex-llm</code> now supports a comprehensive list of LLM <strong>finetuning</strong> on Intel GPU (including <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/LoRA">LoRA</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/QLoRA">QLoRA</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/DPO">DPO</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/QA-LoRA">QA-LoRA</a> and <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/ReLora">ReLoRA</a>).</li>
<li>[2024/01] Using <code>ipex-llm</code> <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/QLoRA">QLoRA</a>, we managed to finetune LLaMA2-7B in <strong>21 minutes</strong> and LLaMA2-70B in <strong>3.14 hours</strong> on 8 Intel Max 1550 GPU for <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/QLoRA/alpaca-qlora">Standford-Alpaca</a> (see the blog <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/finetuning-llms-on-intel-gpus-using-bigdl-llm.html" rel="nofollow">here</a>).</li>
</ul>
<details><summary>More updates</summary>
</details> 

<p dir="auto">See the <em><strong>optimized performance</strong></em> of <code>chatglm2-6b</code> and <code>llama-2-13b-chat</code> models on 12th Gen Intel Core CPU and Intel Arc GPU below.</p>



<ul dir="auto">
<li><a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/install_windows_gpu.html" rel="nofollow">Windows GPU</a>: installing <code>ipex-llm</code> on Windows with Intel GPU</li>
<li><a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/install_linux_gpu.html" rel="nofollow">Linux GPU</a>: installing <code>ipex-llm</code> on Linux with Intel GPU</li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/docker/llm">Docker</a>: using <code>ipex-llm</code> dockers on Intel CPU and GPU</li>
<li><em>For more details, please refer to the <a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Overview/install.html" rel="nofollow">installation guide</a></em></li>
</ul>

<ul dir="auto">
<li><a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/llama_cpp_quickstart.html" rel="nofollow">llama.cpp</a>: running <strong>ipex-llm for llama.cpp</strong> (<em>using C++ interface of <code>ipex-llm</code> as an accelerated backend for <code>llama.cpp</code> on Intel GPU</em>)</li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/vLLM-Serving">vLLM</a>: running <code>ipex-llm</code> in <code>vLLM</code> on both Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/vLLM-Serving">GPU</a> and <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/vLLM-Serving">CPU</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/src/ipex_llm/serving/fastchat">FastChat</a>: running <code>ipex-llm</code> in <code>FastChat</code> serving on on both Intel GPU and CPU</li>
<li><a href="https://github.com/intel-analytics/Langchain-Chatchat">LangChain-Chatchat RAG</a>: running <code>ipex-llm</code> in <code>LangChain-Chatchat</code> (<em>Knowledge Base QA using <strong>RAG</strong> pipeline</em>)</li>
<li><a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/webui_quickstart.html" rel="nofollow">Text-Generation-WebUI</a>: running <code>ipex-llm</code> in <code>oobabooga</code> <strong>WebUI</strong></li>
<li><a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/benchmark_quickstart.html" rel="nofollow">Benchmarking</a>: running  (latency and throughput) benchmarks for <code>ipex-llm</code> on Intel CPU and GPU</li>
</ul>

<ul dir="auto">
<li>Low bit inference
<ul dir="auto">
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model">INT4 inference</a>: <strong>INT4</strong> LLM inference on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model">GPU</a> and <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model">CPU</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types">FP8/FP4 inference</a>: <strong>FP8</strong> and <strong>FP4</strong> LLM inference on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types">GPU</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types">INT8 inference</a>: <strong>INT8</strong> LLM inference on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/More-Data-Types">GPU</a> and <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/More-Data-Types">CPU</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF-IQ2">INT2 inference</a>: <strong>INT2</strong> LLM inference (based on llama.cpp IQ2 mechanism) on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF-IQ2">GPU</a></li>
</ul>
</li>
<li>FP16/BF16 inference
<ul dir="auto">
<li><strong>FP16</strong> LLM inference on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/Speculative-Decoding">GPU</a>, with possible <a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Inference/Self_Speculative_Decoding.html" rel="nofollow">self-speculative decoding</a> optimization</li>
<li><strong>BF16</strong> LLM inference on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Speculative-Decoding">CPU</a>, with possible <a href="https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Inference/Self_Speculative_Decoding.html" rel="nofollow">self-speculative decoding</a> optimization</li>
</ul>
</li>
<li>Save and load
<ul dir="auto">
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Save-Load">Low-bit models</a>: saving and loading <code>ipex-llm</code> low-bit models</li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GGUF">GGUF</a>: directly loading GGUF models into <code>ipex-llm</code></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/AWQ">AWQ</a>: directly loading AWQ models into <code>ipex-llm</code></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Advanced-Quantizations/GPTQ">GPTQ</a>: directly loading GPTQ models into <code>ipex-llm</code></li>
</ul>
</li>
<li>Finetuning
<ul dir="auto">
<li>LLM finetuning on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning">GPU</a>, including <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/LoRA">LoRA</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/QLoRA">QLoRA</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/DPO">DPO</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/QA-LoRA">QA-LoRA</a> and <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/ReLora">ReLoRA</a></li>
<li>QLoRA finetuning on Intel <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/QLoRA-FineTuning">CPU</a></li>
</ul>
</li>
<li>Integration with community libraries
<ul dir="auto">
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels">HuggingFace tansformers</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/PyTorch-Models">Standard PyTorch model</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/Deepspeed-AutoTP">DeepSpeed-AutoTP</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/HF-PEFT">HuggingFace PEFT</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LLM-Finetuning/DPO">HuggingFace TRL</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LangChain">LangChain</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/LlamaIndex">LlamaIndex</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Applications/autogen">AutoGen</a></li>
<li><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/ModelScope-Models">ModeScope</a></li>
</ul>
</li>
<li><a href="https://github.com/intel-analytics/ipex-llm-tutorial">Tutorials</a></li>
</ul>
<p dir="auto"><em>For more details, please refer to the <code>ipex-llm</code> document <a href="https://ipex-llm.readthedocs.io/" rel="nofollow">website</a>.</em></p>

<p dir="auto">Over 50 models have been optimized/verified on <code>ipex-llm</code>, including <em>LLaMA/LLaMA2, Mistral, Mixtral, Gemma, LLaVA, Whisper, ChatGLM2/ChatGLM3, Baichuan/Baichuan2, Qwen/Qwen-1.5, InternLM</em> and more; see the list below.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>CPU Example</th>
<th>GPU Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLaMA <em>(such as Vicuna, Guanaco, Koala, Baize, WizardLM, etc.)</em></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Native-Models">link1</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/vicuna">link2</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/vicuna">link</a></td>
</tr>
<tr>
<td>LLaMA 2</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Native-Models">link1</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/llama2">link2</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/llama2">link</a></td>
</tr>
<tr>
<td>ChatGLM</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm">link</a></td>
<td></td>
</tr>
<tr>
<td>ChatGLM2</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm2">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/chatglm2">link</a></td>
</tr>
<tr>
<td>ChatGLM3</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/chatglm3">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/chatglm3">link</a></td>
</tr>
<tr>
<td>Mistral</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/mistral">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/mistral">link</a></td>
</tr>
<tr>
<td>Mixtral</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/mixtral">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/mixtral">link</a></td>
</tr>
<tr>
<td>Falcon</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/falcon">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/falcon">link</a></td>
</tr>
<tr>
<td>MPT</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/mpt">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/mpt">link</a></td>
</tr>
<tr>
<td>Dolly-v1</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/dolly_v1">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/dolly-v1">link</a></td>
</tr>
<tr>
<td>Dolly-v2</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/dolly_v2">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/dolly-v2">link</a></td>
</tr>
<tr>
<td>Replit Code</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/replit">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/replit">link</a></td>
</tr>
<tr>
<td>RedPajama</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Native-Models">link1</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/redpajama">link2</a></td>
<td></td>
</tr>
<tr>
<td>Phoenix</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Native-Models">link1</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phoenix">link2</a></td>
<td></td>
</tr>
<tr>
<td>StarCoder</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/Native-Models">link1</a>, <a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/starcoder">link2</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/starcoder">link</a></td>
</tr>
<tr>
<td>Baichuan</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/baichuan">link</a></td>
</tr>
<tr>
<td>Baichuan2</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/baichuan2">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/baichuan2">link</a></td>
</tr>
<tr>
<td>InternLM</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/internlm">link</a></td>
</tr>
<tr>
<td>Qwen</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/qwen">link</a></td>
</tr>
<tr>
<td>Qwen1.5</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen1.5">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/qwen1.5">link</a></td>
</tr>
<tr>
<td>Qwen-VL</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/qwen-vl">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/qwen-vl">link</a></td>
</tr>
<tr>
<td>Aquila</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/aquila">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/aquila">link</a></td>
</tr>
<tr>
<td>Aquila2</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/aquila2">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/aquila2">link</a></td>
</tr>
<tr>
<td>MOSS</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/moss">link</a></td>
<td></td>
</tr>
<tr>
<td>Whisper</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/whisper">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/whisper">link</a></td>
</tr>
<tr>
<td>Phi-1_5</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-1_5">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/phi-1_5">link</a></td>
</tr>
<tr>
<td>Flan-t5</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/flan-t5">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/flan-t5">link</a></td>
</tr>
<tr>
<td>LLaVA</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/PyTorch-Models/Model/llava">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/PyTorch-Models/Model/llava">link</a></td>
</tr>
<tr>
<td>CodeLlama</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/codellama">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/codellama">link</a></td>
</tr>
<tr>
<td>Skywork</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/skywork">link</a></td>
<td></td>
</tr>
<tr>
<td>InternLM-XComposer</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm-xcomposer">link</a></td>
<td></td>
</tr>
<tr>
<td>WizardCoder-Python</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/wizardcoder-python">link</a></td>
<td></td>
</tr>
<tr>
<td>CodeShell</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/codeshell">link</a></td>
<td></td>
</tr>
<tr>
<td>Fuyu</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/fuyu">link</a></td>
<td></td>
</tr>
<tr>
<td>Distil-Whisper</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/distil-whisper">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/distil-whisper">link</a></td>
</tr>
<tr>
<td>Yi</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/yi">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/yi">link</a></td>
</tr>
<tr>
<td>BlueLM</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/bluelm">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/bluelm">link</a></td>
</tr>
<tr>
<td>Mamba</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/PyTorch-Models/Model/mamba">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/PyTorch-Models/Model/mamba">link</a></td>
</tr>
<tr>
<td>SOLAR</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/solar">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/solar">link</a></td>
</tr>
<tr>
<td>Phixtral</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phixtral">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/phixtral">link</a></td>
</tr>
<tr>
<td>InternLM2</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/internlm2">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/internlm2">link</a></td>
</tr>
<tr>
<td>RWKV4</td>
<td></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/rwkv4">link</a></td>
</tr>
<tr>
<td>RWKV5</td>
<td></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/rwkv5">link</a></td>
</tr>
<tr>
<td>Bark</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/PyTorch-Models/Model/bark">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/PyTorch-Models/Model/bark">link</a></td>
</tr>
<tr>
<td>SpeechT5</td>
<td></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/PyTorch-Models/Model/speech-t5">link</a></td>
</tr>
<tr>
<td>DeepSeek-MoE</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/deepseek-moe">link</a></td>
<td></td>
</tr>
<tr>
<td>Ziya-Coding-34B-v1.0</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/ziya">link</a></td>
<td></td>
</tr>
<tr>
<td>Phi-2</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/phi-2">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/phi-2">link</a></td>
</tr>
<tr>
<td>Yuan2</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/yuan2">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/yuan2">link</a></td>
</tr>
<tr>
<td>Gemma</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/gemma">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/gemma">link</a></td>
</tr>
<tr>
<td>DeciLM-7B</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/deciLM-7b">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/deciLM-7b">link</a></td>
</tr>
<tr>
<td>Deepseek</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/deepseek">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/deepseek">link</a></td>
</tr>
<tr>
<td>StableLM</td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/CPU/HF-Transformers-AutoModels/Model/stablelm">link</a></td>
<td><a href="https://github.com/intel-analytics/ipex-llm/blob/main/python/llm/example/GPU/HF-Transformers-AutoModels/Model/stablelm">link</a></td>
</tr>
</tbody>
</table>
<section data-footnotes="">
<ol dir="auto">
<li id="user-content-fn-1-81ce39395d1a85f86f714ef670a086f0">
<p dir="auto">Performance varies by use, configuration and other factors. <code>ipex-llm</code> may not optimize to the same degree for non-Intel products. Learn more at <a href="http://www.Intel.com/PerformanceIndex">www.Intel.com/PerformanceIndex</a>. <a href="#user-content-fnref-1-81ce39395d1a85f86f714ef670a086f0" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
</ol>
</section>
</article></div></div>
  </body>
</html>
