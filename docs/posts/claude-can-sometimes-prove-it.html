<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.galois.com/articles/claude-can-sometimes-prove-it">Original</a>
    <h1>Claude can sometimes prove it</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>Let me get right to the point without any <a href="https://www.galois.com/articles/specifications-dont-exist">nonsense about aliens</a>: </p><ol role="list"><li><a href="https://www.anthropic.com/claude-code">Claude Code</a>, the new AI coding agent from <a href="https://www.anthropic.com">Anthropic</a>, is pretty good at interactive theorem proving (ITP). </li><li>I find this very surprising, and you probably should too. </li></ol><p>Interactive theorem proving tools such as <a href="https://lean-lang.org">Lean</a> are the most powerful and trustworthy kind of formal methods tool. They have been used to formally verify important things such as <a href="https://github.com/hacl-star/hacl-star">cryptographic libraries</a>, <a href="https://compcert.org/compcert-C.html">compilers</a>, and <a href="https://sel4.systems">operating systems</a>. Unfortunately, even experts find ITP proofs time-consuming and error-prone. That’s why it’s exciting—and very surprising!—to find that Claude Code is so good at ITP. Today, Claude Code can complete many complex proof steps independently, but it still needs a ‘project manager’ (me) to guide it through the whole formalization. But I think Claude Code points to a world where experts aren’t necessary, and theorem provers can be used by many more people. </p><p>The rest of this post digs into what Claude Code can actually do. But if you’re interested in automated reasoning or formal verification, I recommend you <strong>stop reading</strong>, go sign up for Claude Code, <a href="https://github.com/google-gemini/gemini-cli">Gemini CLI</a>, <a href="https://aider.chat/">Aider</a>, <a href="https://developers.openai.com/codex/cli/">Codex</a>, or some other coding agent, and try it out on a problem you know well. It’ll cost about $20 / month for something useful, and maybe $100 / month for access to a state-of-the-art model. I reckon you’ll be able to get surprising successes (and interesting failures) with about two hours of work. </p><p><em>(If you actually do this, </em><a href="mailto:miked@galois.com"><em>email me</em></a><em> and tell me how it went).</em> </p><h3>Interactive Theorem Proving is Hard </h3><p>There’s a deep tradeoff in automated reasoning between restricting the underlying math—which tends to make reasoning easier to automate—and making it more powerful and general. For example, an <a href="https://en.wikipedia.org/wiki/Satisfiability_modulo_theories">SMT solver</a> can only solve queries expressed in a simple logical language. As a result, SMT solvers are predictable enough to run <a href="https://www.amazon.science/blog/a-billion-smt-queries-a-day">a billion times a day</a> as part of AWS’s S3 bucket security controls. No-one has to write a proof on a whiteboard; instead, the solver solves each theorem quickly and cheaply.<sup>1</sup></p><p>At the other end of the spectrum, interactive theorem proving tools are designed to be as general as possible so we can use them for Real Math™—for example, <a href="https://lean-lang.org/use-cases/flt/">Fermat’s Last Theorem</a>. Unfortunately, this power makes ITP tools notoriously difficult to use. Nick Benton <a href="https://nickbenton.name/mop.pdf">memorably wrote in 2006</a> <em>“I have rarely felt as stupid and frustrated as I did during my first few weeks using Coq.”</em> I taught myself Lean this year—almost twenty years after Benton. The specific issues he pointed to are long solved, but I <em>still</em> felt stupid and frustrated, far beyond any other kind of tool I’ve used. </p><p>Why is ITP so hard? Some reasons are quite obvious: interfaces are confusing, libraries are sparse, documentation is poor, error messages are mysterious. But these aren’t <em>interesting</em> problems, and anyway they are getting fixed. Lean, in particular, is improving at astonishing pace thanks to focused work by the <a href="https://lean-lang.org/fro/">Lean FRO</a>. </p><p>Deeper than this, and keenly felt if not always said, ITP is <em>cognitively demanding in multiple ways</em>. It requires the ability to juggle abstractions as in pen-and-paper mathematics, a willingness to wrangle complex constraints in the exotic languages used by ITP tools, <em>and</em> a tolerance for microscopic pedantry which most people find almost impossible to summon. </p><p>I <a href="https://mikedodds.github.io/files/talks/2024-12-5-proofs-in-the-wild.pdf">once joked</a> that the dominant ITP strategy was the “PSMGS cluster” - <em>Proof Search by Miserable Graduate Students</em>. The <a href="https://github.com/hacl-star/hacl-star">most</a> <a href="https://compcert.org/compcert-C.html">impressive</a> <a href="https://sel4.systems">results</a> in ITP have been achieved by years of very tedious work by very clever people, slowly grinding each key theorem down to a nub, case by case by case. A few supremely talented people can do this well, but they are vanishingly rare and their time is very expensive. This puts a very hard limit on how broadly ITP tools can be used, because very few potential projects pencil out in terms of <a href="https://www.galois.com/articles/what-works-and-doesnt-selling-formal-methods">costs and benefits</a>. </p><h3>Theorem Proving Isn’t Just Proving Theorems    </h3><p>Sadly I am not supremely talented (I am a lazy idiot) and so I found learning Lean <em>annoying</em> and <em>difficult</em>. </p><p>So I started looking for shortcuts. </p><p><em>What about SMT solvers? </em>Lean has impressive SMT support including the new <a href="https://lean-lang.org/doc/reference/4.22.0/The--grind--tactic/">grind</a> tactic. That’s useful when it works, but SMT only works on a very limited set of math properties. Other kinds of non-AI automation such as <a href="https://github.com/JOSHCLUNE/LeanHammer">hammers</a> are similar: very useful, but typically only on a limited range of problems. </p><p><em>It’s 2025, can’t AI help me? </em>There’s been a lot of work on AI mathematics; I even <a href="https://www.galois.com/articles/o3-frontier-math-and-the-future-of-mathematics">wrote about some previous results</a>. I installed the fine-tuned AI models from a couple of recent papers, tried them out on some example tasks, and… none of them really helped.<sup>2 </sup> </p><p>The problem (I realized) is that the models I had chosen were designed to prove <em>individual Lean theorems</em>. That’s only one small part of how I use Lean for ITP. When I’m formalizing a theory, I’m typically doing some combination of the following tasks: </p><ol role="list"><li><strong>Conceptual mathematics</strong> - thinking through what concepts and theorems my theory needs </li><li><strong>Mapping into Lean</strong> - converting my concepts into Lean’s syntax / types / libraries </li><li><strong>Decomposing theorems</strong> - breaking up big properties into smaller, more manageable lemmas </li><li><strong>Proving theorems</strong> - actually using Lean’s tactic language to prove an individual theorem <em>(this is where the AI models I picked up tried to help)</em></li><li><strong>Debugging failures</strong> - figuring out why the formalization was rejected by Lean </li></ol><p>(There are other tasks I might do. For example, I sometimes write property-based tests using <a href="https://github.com/leanprover-community/plausible">Plausible</a>, and any long-lived proof will eventually need to be <a href="https://www.galois.com/articles/proofs-should-repair-themselves">updated and repaired</a>)  </p><p>There’s often a push-pull between these tasks. To give a recent example, suppose I want to prove that a function is <a href="https://en.wikipedia.org/wiki/Associative_property">associative</a> <em>(task 4)</em>. I realize that this theorem is false—<em>oops</em>—because of the underlying type that I&#39;ve chosen <em>(task 5)</em>. Maybe this is a problem I can avoid by decomposing other theorems differently <em>(task 3)</em>. But in this case, I realize it’s a hard requirement, and I need to replace the current type with something else—for example, a <a href="https://leanprover-community.github.io/mathlib4_docs/Mathlib/Data/Finmap.html">finite map</a>—and then refactor the whole formalization accordingly <em>(task 2)</em>. I also need to check whether my theory is mathematically correct, or if I’ve made some conceptual mistake which renders important theorems unprovable <em>(task 1)</em>. </p><p>My most frustrating experiences with Lean have rarely involved proofs of individual theorems. More often, some previous decision has unexpectedly blocked me, and I’m facing a choice between several different complex refactors to fix it. This is what happened with the example above—in the end, I decided to rewrite everything with finite maps, which meant rewriting several definitions and proofs. </p><p>In other words, realistic uses of Lean look more like software engineering on proofs—<em>proof engineering,</em> in other words. Proofs of individual theorems are important, but much of the work with Lean involves adjacent tasks such as selecting abstractions, refactoring code, analyzing requirements, fixing failures. </p><p><em>Okay </em>(I thought) <em>maybe what I need is something different, more like an AI designed for software engineering. </em></p><h3>Claude Atlas </h3><p>Claude Code is an AI coding agent, and no, I didn’t understand what that was either, so let me explain. A chatbot like ChatGPT will typically take a single request and give a single answer, while an <em>AI agent</em> can break down a request into many sub-tasks and then perform them. This is particularly useful for software engineering, where a single large task might require reading documents, modifying files, running tools, and so on. </p><p>More practically speaking, you install Claude Code as a command line application. It pops up a text box a bit like ChatGPT, you type in requests, and the agent works on them. For example, you might ask: </p><ul role="list"><li><em>“Review all uncommitted changes, and check they meet the commit standards in ./README.md. If there are no issues, create a commit and push it” </em></li><li><em>“Pick a representation for thread pools and write an appropriate definition. Make sure that the definition matches all the properties needed by the overall theory, as documented in plan.md” </em></li><li><em>“The theorem compose_is_assoc seems to be unprovable. Can you figure out why and come up with a plan for fixing it? Document it in a timestamped note” </em></li></ul><p>Let’s consider the last request, asking for a fix to compose_is_assoc. Claude Code might break down the task as follows: </p><ol role="list"><li>Write a TODO list for itself describing how to answer the request </li><li>Search the local codebase for the file that contains compose_is_assoc</li><li>Read the surrounding context for the theorem in the file </li><li>Run lake build &lt;filename&gt;  to confirm that the proof actually failing, and review the resulting error messages </li><li>Find and read the local definitions of the types involved with the theorem </li><li>Look at the online documentation for Lean types that come from standard libraries </li><li>Think about what might resolve the issue (entertainingly, ultrathink is a <a href="https://www.anthropic.com/engineering/claude-code-best-practices">magic keyword</a> that increases the agent’s ‘thinking budget’) </li><li>Write a response proposing a high-level fix, and then ask the user for permission to edit the file</li><li>Rerun the build to check the fix worked  </li></ol><p>(You may be thinking that letting an AI agent run arbitrary command-line tools seems hilariously dangerous. <a href="https://arstechnica.com/information-technology/2025/07/ai-coding-assistants-chase-phantoms-destroy-real-user-data/?utm_source=chatgpt.com">It is!</a> By default, each command requires user confirmation, with a user-configurable whitelist. However, footguns abound and I <strong>strongly recommend against </strong>experimenting on data you care about. Designing AI agents so they can be used safely and securely seems like an important and very under-studied problem.) </p><p>Behind the scenes, each step above might involve several calls into <a href="https://docs.anthropic.com/en/docs/about-claude/models/overview">Anthropic’s cloud-hosted AI models</a>. For a complex request like this one, the whole process can take several minutes. This can actually be kind of <em>boring</em> as a user - you push go and watch the agent grind away. </p><p>Obviously, the AI agent makes mistakes of all kinds: syntactic, semantic, conceptual. What I’ve found is that given a tool that can <em>detect</em> mistakes, the agent can often correct them. For example, suppose we want to prove a particular theorem. Typically, the agent will create a first version, and then loop: (1) run lake build to see if the current proof passes, (2) read the error messages, (3) think about what might be wrong, and (4) try to correct the errors. After several iterations, the proof is often correct. </p><p>I think this is part of why Claude Code is surprisingly good at theorem proving. Lean is very strict in the programs it will accept. This makes writing Lean arduous for humans, but it means an AI agent gets detailed feedback. To me, this suggests we may want to design tools differently for AI agents rather than human use. Instead of trying to avoid failures and only output highly processed information that a human can digest, we may want to be more strict but produce more information about what’s going wrong. </p><h3>Claude Can Write Lean </h3><p>I used Claude Code to work on a Lean formalization of an old paper, <a href="https://mikedodds.github.io/files/publications/2009-03-22-deny-guarantee-reasoning.pdf">Deny-Guarantee Reasoning</a> <em>(Dodds, Feng, Parkinson, Vafeiadis - 2009)</em>. This paper isn’t too mathematically complex, but since I last worked on it 15 years ago, I&#39;d forgotten almost all of the details. The theory includes: </p><ul role="list"><li>A simplified formal model of fork-join concurrent programs </li><li>A specialized notion of a ‘permission’ - a logical control on what actions each thread can perform </li><li>A <a href="https://en.wikipedia.org/wiki/Hoare_logic">Hoare logic</a> for verifying that threads obey their assigned permissions </li><li>Many theorems about the way that threads, permissions, and the logic interact </li></ul><p>I started from an empty repository and built the formalization as follows: </p><ul role="list"><li>I got Claude Code to convert the <a href="https://mikedodds.github.io/files/publications/2009-03-22-deny-guarantee-reasoning.pdf">research paper</a> and <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-736.html">technical report</a> into plain text using <a href="https://poppler.freedesktop.org">Poppler</a>. </li><li>I asked Claude to ultrathink and write a formalization plan, with a comprehensive roadmap of what should be formalized when. </li><li>I acted as a ‘project manager,’ directing Claude Code to work through the research plan step by step. I would typically begin by asking the agent to “start working on the next stage of the formalization plan.” </li><li>I instructed the agent to separate theorem definitions from proofs. The first version of a theorem would have a dummy proof (written sorry in Lean). Then I instructed the agent to work on the proof in a subsequent step of the formalization plan. </li><li>After any significant progress which resulted in a clean build, I asked the agent to create a commit.</li><li>With a few exceptions (mostly tricky parse errors) I avoided writing any Lean code myself. </li></ul><p>To my surprise, <em>this approach worked</em>. You can see the Lean development on GitHub <a href="https://github.com/septract/denyguarantee-v3">here</a>. I’m about 50% of the way through the formalization plan, with 2,535 lines of Lean code total, and about 1,232 lines of proof. </p><p>The agent was able to work on all the different levels I talked about above, and it was able to figure where switches between levels were needed. The refactoring to finite maps I talked about above was something <em>the agent did for me</em> (although this was at the edge of its capability - it went down one blind alley, and I supervised it carefully on the second attempt). </p><h3>Claude Sometimes Can’t Write Lean </h3><p>Before you get too excited, here’s a killer caveat: <strong>I think formalizing using AI was way slower than if I’d done it by hand</strong>. Let me explain. </p><p>My experience as a ‘project manager’ for Claude was quite varied:  </p><ul role="list"><li><em>Miracles (rare, but exciting) - the agent got a large, complex task correct in just one iteration. </em></li><li><em>Slow progress (median case) - the agent got a change directionally correct on the first iteration, but needed one or more further iterations with me to complete it. </em></li><li><em>Thrashing (fairly common) - the agent got stuck on some error, and repeatedly tried the same or similar strategies without making any real progress. Many of these cases involved changes with a large ‘blast radius,’ such as redefining one of the core types used in the theory. </em></li><li><em>Shallow persistent mistakes (fairly rare) - the agent made a simple mistake and failed to fix it over multiple iterations. Many of these mistakes were related to parsing, perhaps because parse errors are less semantically rich than other Lean errors. </em></li><li><em>Deep persistent mistakes (rare, but very important) - the agent made a conceptual mistake which I didn’t notice and didn’t result in a build error. These were hard to find and fix because the agent typically reified its faulty understanding into comments, and then believed the comments during later iterations. </em></li></ul><p>I found that Claude Code could resolve most mistakes with a small amount of nudging. I can also imagine that many of these mistakes could be avoided or resolved more quickly if the agent had access to better tools. For example, about half way through the project, I installed the <a href="https://github.com/oOo0oOo/lean-lsp-mcp">lean-mcp-lsp</a> package, which lets the agent query the proof state, search the code, run snippets of test code, and other small features. This made Claude Code noticeably better at proving theorems, I suspect because it had more ways to diagnose errors and test hypotheses. </p><p>The last type of error, <em>deep persistent mistakes</em>, were rare but very costly. In these situations, the agent itself was confused, and often made many changes that embedded the confused understanding. Unpicking its plausible-sounding nonsense was very time consuming.<sup>3</sup> In these cases, my experience with ITP was absolutely necessary, because it allowed me to step back and see the whole picture in a way that an AI agent today mostly cannot. The high cost of these rare errors is the reason that I think the project probably took <em>longer</em> with the AI agent than it would have done by hand. </p><p>I also don’t feel anywhere near as confident in the formalization as I would if it had been produced by an expert. The theorems are checked by Lean and I think we should have high trust in them. But more than 60% of the Lean code consists of <em>definitions</em> not theorems. The fact that the definitions are consistent enough that we can prove properties about them should also give us a bit of confidence. And the definitions look reasonably plausible, which should give us a bit more. But to be truly certain, we would have to carefully audit the whole formalization, which itself would be very arduous. In the best case, we would have a small set of core definitions which are obviously correct, but I suspect for most formalizations, that simply won’t be possible. </p><h3>I Am Surprised and So Are You </h3><p>So Claude Code can write Lean proofs, how surprising is that really? Oh traveller, let us journey through the mists of time to the inconceivably distant year of… <em>*cue wibbling flashback effect*</em> … 2024. </p><p>As of last year, if you wanted to formalize a paper in Lean, your options were (1) go to grad school and learn to do it yourself, (2) hope one of the few theorem proving experts got interested in your problem, or (3) <em>give up</em>. There was no next-best tool which AI agents are supplanting; this capability simply did not exist. </p><p>What’s even crazier to me is that Claude Code isn’t designed or marketed as a theorem proving tool. I think its success here comes from related AI capabilities—agency, long-term planning, task decomposition, software engineering, and traditional math. It seems likely that Anthropic trains its models on Lean math benchmarks, and who knows, maybe they’ve thrown some random formal methods problems. But I think it’s also possible I’m one of the first humans to ever apply Claude Code to a formal methods paper. </p><p>Of course, the experiment I’ve described above <em>isn’t </em>Claude Code autonomously formalizing my old paper from end to end. Rather the agent took a series of individually-impressive but self-contained steps, with me evaluating the result and nudging it back on track when necessary. That’s consistent with <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">METR’s benchmarking</a>, which shows current AI agents can’t yet perform very long tasks. However, METR’s results also show this ability increasing rapidly with each new AI model. If this trend continues, we may soon have AI models that can prove theorems as well or better than human experts, just thanks to their native capabilities. </p><p>Even if AI doesn’t get much smarter, Claude Code will be more useful when it can just think faster. Managing the agent is currently kind of frustrating because it involves a lot of <em>dead time</em>. Each request takes maybe 5-10 minutes to complete, during which you can’t do much else of use, and can’t easily swap to another task.</p><p>I think there are also easy opportunities to make Claude Code as it is today more effective. Adding <a href="https://github.com/oOo0oOo/lean-lsp-mcp">lean-mcp-lsp</a> made a big difference, which suggests even simple tools can help if they give the agent more information. We could also give the agent access to the theorem-focused AI models I mentioned earlier and let the agent decide when to use them. Many of the errors I encountered seemed like random failures, and we might be able to avoid them by running multiple agents in parallel and picking winners. The bar for improvement is honestly quite low: in my experiments, I did the dumbest thing I could think of and it worked pretty well. </p><h3>Look Just Try Claude Code </h3><p>As long as I’ve been in the field, automated reasoning has evolved slowly. We are used to small-% improvements achieved through clever, careful work. Claude Code breaks that pattern. Even with the limitations it has today, it can do things that seemed utterly out of reach even a year ago. Even more surprising, this capability doesn’t come from some fancy solver or novel algorithm; Claude Code wasn’t designed for theorem proving at all! </p><p>I think what Claude Code really points to is <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">the bitter lesson</a> coming for formal methods, just as it did for image recognition, language translation, and program synthesis. Just as in those fields, in the long run I think formal methods <em>ideas</em> will be essential in making AI-driven proof successful. That’s because AIs will need many of the same tools as humans do to decompose, analyze, and debug proofs. But before that happens, I think we will see much of the clever, careful work we esteem rendered obsolete by AI-driven tools that have no particular domain expertise. The lesson will be bitter indeed, and many people are resisting it.</p><p>I think the result will be worth the pain, however. The reason ITP has never been widely adopted is that it is simply too cognitively demanding for most humans. Claude Code points to a future where theorem proving is <em>solved</em> - cheap, abundant, and automatic. I think that would be a good future, and if it happens, we should be ready and know what problem we want to solve next. </p><p>‍<em>Further reading - Galois intern </em><a href="https://binarynewts.andrew.cmu.edu/"><em>Twain Byrnes</em></a><em> on using AI to reason about memory safety of C code: </em><a href="https://www.galois.com/articles/escaping-isla-nublar-coming-around-to-llms-for-formal-methods"><em>Escaping Isla Nublar: Coming around to LLMs for Formal Methods</em></a></p><p>‍</p><h3><strong>Footnotes</strong></h3><p>[1] There’s a reasonable argument that a large % of all distinct theorems proved in human history have been proved by the AWS Zelkova access control tool.</p><p>[2] I’m not going to point to the specific papers because the scientific results seem strong and my evaluation of the models was very unsystematic - really just installing them and poking around</p><p>[3] METR recently released an <a href="https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/">entertaining study on coding agents</a> where they asked open-source developers to complete AI-assisted engineering tasks. On average, the developers thought that the AI agent increased their productivity, but the study showed their productivity actually decreased. One explanation is that current AI agents can consistently get code nearly right, but the gap to <em>really</em> right is bigger than we think.</p></div></div></div>
  </body>
</html>
