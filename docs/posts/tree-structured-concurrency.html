<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/">Original</a>
    <h1>Tree-Structured Concurrency</h1>
    
    <div id="readability-page-1" class="page"><article>
    <div itemprop="articleBody" id="markdown">
      
     
          <ol>
          
              <li>
                  <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/#what-is-structured-concurrency">what is structured concurrency?</a>
                  
              </li>
          
              <li>
                  <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/#unstructured-concurrency-an-example">unstructured concurrency: an example</a>
                  
              </li>
          
              <li>
                  <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/#structured-concurrency-an-example">structured concurrency: an example</a>
                  
              </li>
          
              <li>
                  <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/#what-s-the-worst-that-can-happen">what&#39;s the worst that can happen?</a>
                  
              </li>
          
              <li>
                  <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/#applying-structured-concurrency-to-your-programs">applying structured concurrency to your programs</a>
                  
              </li>
          
              <li>
                  <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/#pattern-managed-background-tasks">pattern: managed background tasks</a>
                  
              </li>
          
              <li>
                  <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/#guaranteeing-structure">guaranteeing structure</a>
                  
              </li>
          
              <li>
                  <a href="https://blog.yoshuawuyts.com/tree-structured-concurrency/#conclusion">conclusion</a>
                  
              </li>
          
          </ol>
      
      <p>For a while now I&#39;ve been trying to find a good way to explain what structured
concurrency is, and how it applies to Rust. I&#39;ve come up with zingers such as:
<em>&#34;Structured concurrency is structured programming as applied to concurrent
control-flow primitives&#34;</em>. But that requires me to start explaining what
structured programming is, and suddenly I find myself 2000 words deep into a
concept which seems natural to most people writing programs today .</p>
<p>Instead I want to try something different. In this post I want to provide you
with a practical introduction to structured concurrency. I will do my best to
explain what it is, why it&#39;s relevant, and how you can start applying it to your
rust projects today. Structured concurrency is a lens I use in almost all of my
reasoning about async Rust, and I think it might help others too. So let&#39;s dive in.</p>
<p><em>This post assumes some familiarity with <a href="https://blog.yoshuawuyts.com/why-async-rust/">async Rust</a> and <a href="https://blog.yoshuawuyts.com/async-cancellation-1/">async cancellation</a>.
If you aren&#39;t already, it might be helpful to skim through the earlier posts on
the topic.</em></p>

<h2 id="what-is-structured-concurrency">What is structured concurrency?</h2>
<p>Structured concurrency is a property of your program. It&#39;s not just any
structure, the structure of the program is guaranteed to be a <a href="https://blog.yoshuawuyts.com/graphs/#trees">tree</a> regardless
of how much concurrency is going on internally . A good way to think
about it is that if you could plot the live call-graph of your program as a
series of relationships it would neatly form a tree. No cycles . No
dangling nodes. Just a single tree.</p>


<figure>
    <img src="https://blog.yoshuawuyts.com/tree-structured-concurrency/tree_plain.jpg" alt="tree"/>
    <figcaption>
        Fig 1. The arrows point from parent nodes to
        child nodes. It has no cycles. A parent can have multiple children. But
        a child always has a single parent - except for the root node.
    </figcaption>
</figure>
<p>And this structure, at least in async Rust, provides three key properties:</p>
<ul>
<li><strong>Cancellation propagation:</strong> When you drop a future to cancel it, it&#39;s
guaranteed that all futures underneath it are also cancelled.</li>
<li><strong>Error propagation:</strong> When an error is created somewhere down in the
call-graph, it can always be propagated up to the callers until there is a
caller who is ready to handle it.</li>
<li><strong>Ordering of operations:</strong> When a function returns, you know it is done doing
work. No surprises that things are still happening long after you thought the
function had completed.</li>
</ul>
<p>These properties put together lead to something called a <strong>&#34;black box model of
execution&#34;</strong>: under a structured model of computing you don&#39;t need to know
anything about the inner workings of the functions you&#39;re calling, because their
behavior is guaranteed. A function will return when it&#39;s done, will cancel all
work when you ask it to, and you&#39;ll always receive an error if there is something
which needs handling. And as a result code under this model is <strong>composable</strong>.</p>
<figure>
    <img src="https://blog.yoshuawuyts.com/tree-structured-concurrency/tree_flow.jpg" title="figure 2" alt="tree"/>
    <figcaption>
        Fig 2. Under structured concurrency every future has a parent,
        cancellation flows downward, and errors flow upward. When a future
        returns, you can be sure it&#39;s done doing work. 
    </figcaption>
</figure>
<p>If your model of concurrency is <em>unstructured</em>, then you don&#39;t have these
guarantees. So in order to guarantee that say, cancellation is correctly
propagated, you&#39;ll need to inspect the inner workings of every function you&#39;re
calling. Code under this model is not composable, and requires manual checks and
bespoke solutions. This is both labor-intensive and prone to errors.</p>
<h2 id="unstructured-concurrency-an-example">Unstructured concurrency: an example</h2>
<p>Let&#39;s start by implementing a classic concurrency pattern: &#34;race&#34;. But rather
than using <em>structured</em> primitives, we can use the staples of unstructured
programming: the venerable <code>task::spawn</code> and <code>channel</code>. The way &#34;race&#34; works is
it takes two futures, and we try and get the output of whichever one completes
first is whose message we read. We could write it something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>async_std::{channel, task};
</span><span>
</span><span>let </span><span>(sender0, receiver) = channel::bounded(</span><span>1</span><span>);
</span><span>
</span><span>let</span><span> sender1 = sender0.</span><span>clone</span><span>();
</span><span>task::spawn(async </span><span>move </span><span>{         </span><span>// üëà Task &#34;C&#34;
</span><span>    task::sleep(Duration::from_millis(</span><span>100</span><span>));
</span><span>    sender1.</span><span>send</span><span>(&#34;</span><span>first</span><span>&#34;).await;
</span><span>});
</span><span>
</span><span>task::spawn(async </span><span>move </span><span>{         </span><span>// üëà Task &#34;B&#34;
</span><span>    task::sleep(Duration::from_millis(</span><span>100</span><span>));
</span><span>    sender0.</span><span>send</span><span>(&#34;</span><span>second</span><span>&#34;).await;
</span><span>});
</span><span>
</span><span>let</span><span> msg = receiver.</span><span>recv</span><span>().await; </span><span>// üëà Future &#34;A&#34;
</span><span>println!(&#34;</span><span>{msg}</span><span>&#34;);
</span></code></pre>
<p>While this implements &#34;race&#34; semantics correctly, it doesn&#39;t handle
cancellation. If one of the branches completes, we&#39;d ideally like to cancel the
other. And if the containing function is cancelled, both computations should be
cancelled. Because of how we&#39;ve structured the program neither task is anchored
to a parent future, and so we can&#39;t cancel either computation directly. Instead
the solution would be to come up with some design using more channels, anchor
the handles - or we could instead rewrite this using structured primitives.</p>
<figure>
    <img src="https://blog.yoshuawuyts.com/tree-structured-concurrency/race_tasks.jpg" title="figure 3" alt="tree"/>
    <figcaption>
        Fig 3. You can create a &#34;race&#34; operation by combining tasks and channels. Data
        can flow out of the tasks to the caller. But because the tasks aren&#39;t rooted
        in a parent task, cancellation doesn&#39;t propagate.
    </figcaption>
</figure>
<h2 id="structured-concurrency-an-example">Structured concurrency: an example</h2>
<p>We can rewrite the example above using structured primitives instead. Rather
than DIY-ing our own &#34;race&#34; implementation using tasks and channels, we should
instead be using a &#34;race&#34; primitive which implements those semantics for us -
and correctly handles cancellation. Using the <a href="http://docs.rs/futures-concurrency">futures-concurrency</a> library we
could do that as follows:</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>futures_concurrency::prelude::*;
</span><span>use </span><span>async_std::task;
</span><span>
</span><span>let</span><span> c = async {                   </span><span>// üëà Future &#34;C&#34;
</span><span>    task::sleep(Duration::from_millis(</span><span>100</span><span>));
</span><span>    &#34;</span><span>first</span><span>&#34;
</span><span>};
</span><span>
</span><span>let</span><span> b = async {                   </span><span>// üëà Future &#34;B&#34;
</span><span>    task::sleep(Duration::from_millis(</span><span>100</span><span>));
</span><span>    &#34;</span><span>second</span><span>&#34;
</span><span>};
</span><span>
</span><span>let</span><span> msg = (c, b).</span><span>race</span><span>().await;    </span><span>// üëà Future &#34;A&#34;
</span><span>println!(&#34;</span><span>{msg}</span><span>&#34;);
</span></code></pre>
<p>When one future completes here, the other future is cancelled. And should the
<code>Race</code> future be dropped, then both futures are cancelled. Both futures have a
parent future when executing. Cancellation propagates downwards. And while there
are no errors in this example, if we were working with fallible operations then
early returns would cause the future to complete early - and errors would be
handled as expected.</p>
<figure>
    <img src="https://blog.yoshuawuyts.com/tree-structured-concurrency/race_futures.jpg" title="figure 4" alt="tree"/>
    <figcaption>
        Fig 4. By using a structured &#34;race&#34; primitive all child futures are rooted in
        a parent future. Which allows both cancellation and errors to propagate. And
        the operation won&#39;t return until all child futures have dropped.
    </figcaption>
</figure>
<p>So far we&#39;ve looked at just the &#34;race&#34; operation, which encodes: <em>&#34;Wait for the
first future to complete, then cancel the other&#34;</em>. But other async concurrency
operations exist as well, such as:</p>
<ul>
<li><strong>join</strong>: wait for all futures to complete.</li>
<li><strong>race_ok</strong>: wait for the first future to complete which returns <code>Ok</code>.</li>
<li><strong>try_join</strong>: wait for all futures to complete, or return early if there is an error.</li>
<li><strong>merge</strong>: wait for all futures to complete, and yield items from a stream as
soon as they&#39;re ready.</li>
</ul>
<p>There are a few more such as &#34;zip&#34;, &#34;unzip&#34;, and &#34;chain&#34; - as well as dynamic
concurrency primitives such as &#34;task group&#34;, &#34;fallible task group&#34;, and more.
The point is that the set of concurrency <em>primitives</em> is bounded. But they can
be recombined in ways that makes it possible express any form of concurrency you
want. Not unlike how if a programming language supports branching, loops, and
function calls you can encode just about any control-flow logic you want,-
without ever needing to use &#34;goto&#34;.</p>
<h2 id="what-s-the-worst-that-can-happen">What&#39;s the worst that can happen?</h2>
<p>People sometimes ask: What&#39;s the worst that can happen when you don&#39;t have
structured concurrency? There are a number of bad outcomes possible, including
but not limited to: data loss, data corruption, and memory leaks.</p>
<p>While Rust guards against <em>data races</em> which fall under the category of &#34;memory
safety&#34;, Rust can&#39;t protect you from logic bugs. For example: if you execute a
<code>write</code> operation inside of a task whose handle isn&#39;t joined, then you&#39;ll need
to find some alternate mechanism to guarantee the ordering of that operation in
relation to the rest of the program. If you get that wrong you might
accidentally write to a closed resource and lose data. Or perform an
out-of-order write, and accidentally corrupt a resource . These kinds of bugs
are not in the same class as memory safety bugs. But they are nonetheless
serious, and they can be mitigated through principled API design.</p>

<h2 id="applying-structured-concurrency-to-your-programs">Applying structured concurrency to your programs</h2>
<p><strong>task::spawn</strong></p>
<p>When using or authoring async APIs in Rust, you should ask yourself the following questions to ensure structured concurrency:</p>
<ol>
<li><strong>Cancellation propagation</strong>: If this future or function is dropped, will cancellation propagate to all child futures?</li>
<li><strong>Error propagation</strong>: If an error happens anywhere in this future, can we either handle it directly or surface it to the caller?</li>
<li><strong>Ordering of operations</strong>: When this function returns, will any more work continue to happen in the background?</li>
</ol>
<p>If all of these properties are true, then once the function exits it&#39;s done
executing and you&#39;re good. This however leads us to a major issue in today&#39;s
async ecosystem: neither async-std nor tokio provide a <code>spawn</code> function which is
structured.  If you drop a task handle the task isn&#39;t cancelled, but instead it&#39;s
detached and will continue to run in the background. This means that
cancellation doesn&#39;t automatically propagate across task boundaries, causing it
to be unstructured.</p>
<p>The <a href="https://docs.rs/smol/latest/smol/struct.Task.html">smol</a> library gets closer
though. It has a task implementation which gets us closer to &#34;cancel on
drop&#34;-semantics out of the box. Though it doesn&#39;t get us all the way yet because
it doesn&#39;t guarantee an ordering of operations. When a smol <code>Task</code> is dropped
the task isn&#39;t guaranteed to have been cancelled, all it guarantees is that the
task will be cancelled at some point in the future.</p>
<p><strong>async drop</strong></p>
<p>Which brings us to the biggest piece missing from async Rust&#39;s structured
concurrency story: the lack of async Drop in the language. Smol&#39;s
tasks have an async
<a href="https://docs.rs/smol/latest/smol/struct.Task.html#method.cancel">cancel</a> method
which only resolves once the task has successfully been cancelled. Ideally we
could call this method in the destructor and wait for it. But in order to do
that today we&#39;d need to block the thread, and that can lead to throughput
issues. No, in practice what we really need for this to work well is async
destructors .</p>

<p><strong>what can you do today?</strong></p>
<p>But while we can&#39;t yet trivially fulfill all requirements for async structured
concurrency for async tasks, not all hope is lost. Without async Drop we can
already achieve 2/3 of the requirements for task spawning today. And if you&#39;re
using a runtime other than smol, <a href="https://github.com/yoshuawuyts/tasky/blob/fb8a8e7040c7cd68a5e38b895bbd032ded578a3f/src/lib.rs#L41-L75">adapting the spawn method</a> to work like
smol&#39;s does is not too much work. But most concurrency doesn&#39;t need tasks
because it isn&#39;t dynamic. For that you can take a look at the
<a href="http://docs.rs/futures-concurrency">futures-concurrency</a> library which implements composable primitives for
structured concurrency.</p>
<p>If you want to adopt structured concurrency in your codebase today, you can
start by adopting it for non-task-based concurrency. And for task-based
concurrency you can adopt the smol model of task spawning to benefit from most
of the benefits of structured concurrency today. And eventually the hope is we
can add some form of async Drop to the language to close out the remaining
holes.</p>
<h2 id="pattern-managed-background-tasks">Pattern: managed background tasks</h2>
<p>People frequently ask how they can implement &#34;background tasks&#34; under structured
concurrency. This is used in scenarios such as an HTTP request handler which
also wants to submit a piece of telemetry. Rather than blocking sending the
response on the telemetry, it spawns a &#34;background task&#34; to submit the telemetry
in the background, and immediately returns from the request. This can look
something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>let mut</span><span> app = tide::new();
</span><span>app.</span><span>at</span><span>(&#34;</span><span>/</span><span>&#34;).</span><span>post</span><span>(|_| async </span><span>move </span><span>{
</span><span>    task::spawn(async {  </span><span>// üëà Spawns a background task‚Ä¶
</span><span>        </span><span>let</span><span> _res = </span><span>send_telemetry</span><span>(data, more_data).await;
</span><span>        </span><span>// ‚Ä¶ what if `res` is an `Err`? How should we handle errors here?
</span><span>    });
</span><span>    Ok(&#34;</span><span>hello world</span><span>&#34;)   </span><span>// üëà ‚Ä¶and returns immediately after.
</span><span>});
</span><span>app.</span><span>listen</span><span>(&#34;</span><span>127.0.0.1:8080</span><span>&#34;).await?;
</span></code></pre>
<p>The phrase &#34;background task&#34; seems polite and unobtrusive. But from a structured
perspective it represents a computation without a parent - it is a <em>dangling
task</em>. The core of the pattern we&#39;re dealing with is that we want to create a
computation which outlives the lifetime of the request handler. We can resolve
this by rather than creating a dangling task to submit it to a task queue or
task group which outlives the request handler. 
Unlike a dangling task, a <em>task queue</em> or <em>task group</em> preserves structured
concurrency. Where a dangling task doesn&#39;t have a parent future and becomes
unreachable, using a task queue we transmit the ownership of a future to a
different object which outlives the current more ephemeral scope.</p>
<p>I&#39;ve heard people make the argument before that <code>task::spawn</code> is perfectly
structured, as long as you think of it as spawning on some sort of unreachable,
global task pool. But the question shouldn&#39;t be whether tasks are spawned on a task
pool, but what the relationship is of those tasks to the rest of the program.
Because we cannot cancel and recreate an unreachable task pool. Nor can we
receive errors from this pool, or wait for all tasks in it to complete. It
doesn&#39;t provide the properties we want from structured concurrency - so we
shouldn&#39;t consider it structured.</p>
<p>I don&#39;t feel like the ecosystem has any great solutions to this yet - in part
limited because we want <a href="https://tmandry.gitlab.io/blog/posts/2023-03-01-scoped-tasks/">&#34;scoped
tasks&#34;</a> which
basically require <a href="https://blog.yoshuawuyts.com/linearity-and-control/">linear
destructors</a> to function.
But <a href="https://docs.rs/async-task-group/latest/async_task_group/">other experiments
exist</a> so we can use
that plus channels to put something together which gives us what we want:</p>
<p><em>‚ö†Ô∏è Note: This code is not considered &#34;good&#34; by the author, and is merely used as an
example to show that this is possible to write today. More design work is
necessary to make this ergonomic ‚ö†Ô∏è</em></p>
<pre data-lang="rust"><code data-lang="rust"><span>// Create a channel to send and receive futures over.
</span><span>let </span><span>(sender, receiver) = async_channel::unbounded();
</span><span>
</span><span>// Create a structured task group at the top-level, next to the HTTP server
</span><span>//
</span><span>// If any errors are returned by the spawned tasks, all active tasks are cancelled
</span><span>// and the error is returned by the handle.
</span><span>let</span><span> telemetry_handle = async_task_group::group(|</span><span>group</span><span>| async </span><span>move </span><span>{
</span><span>    </span><span>while let </span><span>Some(telemetry_future) = receiver.</span><span>next</span><span>().await {
</span><span>        group.</span><span>spawn</span><span>(async </span><span>move </span><span>{
</span><span>            telemetry_future.await?;  </span><span>// üëà Propagate errors upwards
</span><span>            Ok(())
</span><span>        });
</span><span>    }
</span><span>    Ok(group)
</span><span>});
</span><span>
</span><span>// Create an application state for our HTTP server
</span><span>#[</span><span>derive</span><span>(Clone)]
</span><span>struct </span><span>State {
</span><span>    </span><span>sender</span><span>: async_channel::Sender&lt;impl Future&lt;Result&lt;_&gt;&gt;&gt;,
</span><span>}
</span><span>
</span><span>// Create the HTTP server
</span><span>let mut</span><span> app = tide::new();
</span><span>app.</span><span>at</span><span>(&#34;</span><span>/</span><span>&#34;).</span><span>post</span><span>(|</span><span>req</span><span>: Request&lt;State&gt;| async </span><span>move </span><span>{
</span><span>    state.sender.</span><span>send</span><span>(async {   </span><span>// üëà Sends a future to the handler loop‚Ä¶
</span><span>        </span><span>send_telemetry</span><span>(data, more_data).await?;
</span><span>        Ok(())
</span><span>    }).await;
</span><span>    Ok(&#34;</span><span>hello world</span><span>&#34;)           </span><span>// üëà ‚Ä¶and returns immediately after.
</span><span>});
</span><span>
</span><span>// Concurrently execute both the HTTP server and the telemetry handler,
</span><span>// and if either one stops working the other stops too.
</span><span>(app.</span><span>listen</span><span>(&#34;</span><span>127.0.0.1:8080</span><span>&#34;), telemetry_handle).</span><span>race</span><span>().await?;
</span></code></pre>
<p>Like I said: we need to do a lot more API work to be able to rival the
convenience of just firing off a dangling task. But what we lack for in API
convenience, we make up for in semantics. Unlike our earlier example this will
correctly propagates cancellation and errors, and every executing future is
owned by a parent future. We could even take things a step further and implement
things like retry-handlers with error quotas on top of this to create a more
resilient system. But hopefully this is enough already to get the idea across of
what we could be doing with this.</p>
<h2 id="guaranteeing-structure">Guaranteeing Structure</h2>
<p>I&#39;ve been asking myself for a while now: <em>&#34;Would it be possible for Rust to
enforce structured concurrency in the language and libraries?&#34;</em> I don&#39;t believe
this is something we guarantee from the language. But it is something <em>can</em>
guarantee for Rust&#39;s library code, and make it so most async code is structured
by default.</p>
<p>The reason why I don&#39;t believe it&#39;s fundamentally possible to guarantee
structure at the language level is because it&#39;s possible to express any kind of
program in Rust, which includes unstructured programs. Futures, channels, and
tasks as they exist today are all just regular library types. If we wanted to
enforce structure from the language, we would need to find a way to disallow the
creation of these libraries - and that seems impossible for a general-purpose
language .</p>

<p>Instead it seems more practical to me to adopt tree-structured concurrency as
the model we follow for async Rust. Not as a memory-safety guarantee, but as a
design discipline we apply across all of async Rust. APIs which are unstructured
should not be added to the stdlib. And our tooling should be aware that
unstructured code may exist, so it can flag it when it encounters it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post I&#39;ve shown what (tree-)structured concurrency is, why it&#39;s
important for correctness, and how you can apply it in your programs. I hope
that by defining structured concurrency in terms of guarantees about propagation
of errors and cancellation, we can create a practical model for people to
reason about async Rust with.</p>
<p>As <a href="https://opensource.googleblog.com/2023/06/rust-fact-vs-fiction-5-insights-from-googles-rust-journey-2022.html">recently reported by
Google</a>,
async Rust is one of the most difficult aspects of Rust to learn. It seems
likely that the lack of structure in async Rust code today did not help. In
async code today neither cancellation nor errors are guaranteed to propagate.
This means that if you want to reliably compose code, you need to have knowledge
of the inner workings of the code you&#39;re using. By adopting a (tree-)structured
model of concurrency these properties can instead be guaranteed from the outset,
which in turn would make Async Rust easier to reason about and teach. Because
<em>&#34;If it compiles it works&#34;</em> should apply to async Rust too.</p>
<p><em>Thanks to Iryna Shestak for illustrating and proof-reading this post.</em></p>

    </div>
  </article></div>
  </body>
</html>
