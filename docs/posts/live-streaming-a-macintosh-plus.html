<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jcs.org/2022/02/21/macplus_streaming">Original</a>
    <h1>Live Streaming a Macintosh Plus</h1>
    
    <div id="readability-page-1" class="page"><article>
	<header>
			
		
		
	</header>

	
	<p>Since recording a handful of
<a href="https://jcs.org/system6c">C Programming on System 6</a>
videos, I’ve occasionally wanted to live-stream the more casual daily
programming being done on my Macintosh Plus.
After getting all of the pieces together, I now have a working self-hosted
broadcasting setup.</p>

<p>If I happen to be programming on my Mac right now, you can watch
<a href="https://jcs.org/live">here at my website</a>.</p>



<h2 id="table-of-contents">Table of Contents</h2>

<ol id="markdown-toc">
  <li><a href="#getting-video-from-a-macintosh-plus" id="markdown-toc-getting-video-from-a-macintosh-plus">Getting Video From a Macintosh Plus</a></li>
  <li><a href="#hdmi-capture" id="markdown-toc-hdmi-capture">HDMI Capture</a></li>
  <li><a href="#streaming" id="markdown-toc-streaming">Streaming</a></li>
  <li><a href="#chat" id="markdown-toc-chat">Chat</a></li>
  <li><a href="#casual-broadcasting" id="markdown-toc-casual-broadcasting">Casual Broadcasting</a></li>
</ol>

<h2 id="getting-video-from-a-macintosh-plus">Getting Video From a Macintosh Plus</h2>

<p>Back in 2020, I
<a href="https://twitter.com/jcs/status/1320378225911058433">experimented</a>
with writing a pseudo-driver for my Macintosh Plus that would send its screen
data over the SCSI port to a Raspberry Pi with a
<a href="https://github.com/akuker/RASCSI">RaSCSI hat</a>,
which would then display the data on the Pi’s framebuffer, making it possible to
capture the Mac’s screen through the Pi’s HDMI output.
This proved to be
<a href="https://twitter.com/jcs/status/1320412458092273664">too slow</a>
to work and ate a lot of processing time on the Mac (though it was admittedly
not well optimized), so I went back to recording videos with an actual camera
(an iPad).</p>

<figure>
<a href="https://jcs.org/images/2022-02-21-rascsi_screenshot-large.jpg"><img src="https://jcs.org/images/2022-02-21-rascsi_screenshot-327x245.jpg" srcset="/images/2022-02-21-rascsi_screenshot-327x245.jpg 1x, /images/2022-02-21-rascsi_screenshot-654x490@2x.jpg 2x" width="327" height="245"/></a>
<a href="https://jcs.org/images/2022-02-21-rascsi_tv-large.jpg"><img src="https://jcs.org/images/2022-02-21-rascsi_tv-327x245.jpg" srcset="/images/2022-02-21-rascsi_tv-327x245.jpg 1x, /images/2022-02-21-rascsi_tv-654x490@2x.jpg 2x" width="327" height="245"/></a>
</figure>

<p>After watching
<a href="https://www.youtube.com/watch?v=pvjsXbz1xlk">Adrian Black’s video</a>
in 2021 about using an
<a href="https://github.com/hoglet67/RGBtoHDMI">RGBtoHDMI</a>
device to get video from a Macintosh Classic, I tried to get one but they were
not being sold anywhere.
The RGBtoHDMI is a board that attaches to a Raspberry Pi Zero’s GPIO header and
uses a
<abbr title="Complex Programmable Logic Device">CPLD</abbr>
to process analog or TTL digital video from a variety of sources and then feed
it to some bare-metal software running on the Pi to display on the Pi’s HDMI
output.</p>

<p>Building my own RGBtoHDMI ended up taking me quite a while.
I first had to send the open source
<a href="https://github.com/hoglet67/RGBtoHDMI/tree/master/kicad/v4/manufacturing">PCB designs</a>
to
<a href="https://www.pcbway.com/">PCBWay</a>
for printing (and I now have 9 extra PCBs) and then source all of the
components.
The most important component, the Xilinx XC9572XL CPLD, was also the hardest to
acquire due to the worldwide component shortage.
Finally, assembly required soldering a bunch of tiny capacitors and the CPLD’s
44 pins but I had never done surface-mount soldering before, so I was a bit
nervous.</p>

<figure>
<a href="https://jcs.org/images/2022-02-21-pcb-large.jpg"><img src="https://jcs.org/images/2022-02-21-pcb-216x161.jpg" srcset="/images/2022-02-21-pcb-216x161.jpg 1x, /images/2022-02-21-pcb-432x322@2x.jpg 2x" width="216" height="161"/></a>
<a href="https://jcs.org/images/2022-02-21-rgbtohdmi-large.jpg"><img src="https://jcs.org/images/2022-02-21-rgbtohdmi-216x161.jpg" srcset="/images/2022-02-21-rgbtohdmi-216x161.jpg 1x, /images/2022-02-21-rgbtohdmi-432x322@2x.jpg 2x" width="216" height="161"/></a>
<a href="https://jcs.org/images/2022-02-21-rgbtohdmi_working-large.jpg"><img src="https://jcs.org/images/2022-02-21-rgbtohdmi_working-216x161.jpg" srcset="/images/2022-02-21-rgbtohdmi_working-216x161.jpg 1x, /images/2022-02-21-rgbtohdmi_working-432x322@2x.jpg 2x" width="216" height="161"/></a>
</figure>

<p>With the RGBtoHDMI working, I needed to connect it to my Mac Plus.
I soldered in four wires on the wiring harness that connects the Mac Plus’s
analog and logic boards, tapping into the monochrome video signal, horizontal
sync, vertical sync, and ground.
(These wires carry only low-voltage TTL signals so it should be pretty safe, but
don’t take my word for it.)</p>

<table>
  <thead>
    <tr>
      <th>Macintosh Plus</th>
      <th>Wire</th>
      <th>RGBToHDMI</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Video - White (1)</td>
      <td>White</td>
      <td>GRN2 (Video)</td>
    </tr>
    <tr>
      <td>Horizontal Sync - Orange (3)</td>
      <td>Red</td>
      <td>HS (Horizontal Sync)</td>
    </tr>
    <tr>
      <td>Vertical Sync - Green (5)</td>
      <td>Green</td>
      <td>VS (Vertical Sync)</td>
    </tr>
    <tr>
      <td>Ground - Dark Purple (7)</td>
      <td>Black</td>
      <td>0V (Ground)</td>
    </tr>
  </tbody>
</table>

<p>I added heat-shrink tubing around the wires and ran them through the locking
desk connector on the rear of the Mac.
I also
<a href="https://www.thingiverse.com/thing:5004298">3D-printed a case</a>
for the Raspberry Pi Zero and RGBtoHDMI.</p>

<figure>
<a href="https://jcs.org/images/2022-02-21-wiring_diagram-large.jpg"><img src="https://jcs.org/images/2022-02-21-wiring_diagram-216x161.jpg" srcset="/images/2022-02-21-wiring_diagram-216x161.jpg 1x, /images/2022-02-21-wiring_diagram-432x322@2x.jpg 2x" width="216" height="161"/></a>
<a href="https://jcs.org/images/2022-02-21-mac_wiring-large.jpg"><img src="https://jcs.org/images/2022-02-21-mac_wiring-216x162.jpg" srcset="/images/2022-02-21-mac_wiring-216x162.jpg 1x, /images/2022-02-21-mac_wiring-432x324@2x.jpg 2x" width="216" height="162"/></a>
<a href="https://jcs.org/images/2022-02-21-mac_rear-large.jpg"><img src="https://jcs.org/images/2022-02-21-mac_rear-216x161.jpg" srcset="/images/2022-02-21-mac_rear-216x161.jpg 1x, /images/2022-02-21-mac_rear-432x322@2x.jpg 2x" width="216" height="161"/></a>
<figcaption>
Disregard the extra black and yellow wires, they go to a fan I have mounted in
the case
</figcaption>
</figure>

<p>Once the wires were correctly hooked up and the RGBtoHDMI was tuned with the
clock settings from Adrian’s video, I had a pixel-perfect copy of my Mac Plus’s
512x342 screen appearing in real-time on an HDMI screen (with padding to fill
its 640x480 framebuffer).</p>

<figure>
<a href="https://jcs.org/images/2022-02-21-screenshot2-large.jpg"><img src="https://jcs.org/images/2022-02-21-screenshot2-327x245.jpg" srcset="/images/2022-02-21-screenshot2-327x245.jpg 1x, /images/2022-02-21-screenshot2-654x490@2x.jpg 2x" width="327" height="245"/></a>
<a href="https://jcs.org/images/2022-02-21-screenshot1-large.jpg"><img src="https://jcs.org/images/2022-02-21-screenshot1-327x245.jpg" srcset="/images/2022-02-21-screenshot1-327x245.jpg 1x, /images/2022-02-21-screenshot1-654x490@2x.jpg 2x" width="327" height="245"/></a>
</figure>

<h2 id="hdmi-capture">HDMI Capture</h2>

<p>I purchased a
<a href="https://www.amazon.com/dp/B091NWB4RJ">cheap $20 HDMI capture device</a>
(with the most unfortunate 
<a href="https://www.nytimes.com/2020/02/11/style/amazon-trademark-copyright.html">Markov Chain brand name</a>)
which presents an HDMI source as a standard USB webcam.
On my OpenBSD laptop, it appears as a <code>uvideo</code> USB device with accompanying
<code>uaudio</code>:</p>

<div><div><pre><code>uvideo1 at uhub5 port 4 configuration 1 interface 0 &#34;MACROSILICON USB3. 0 capture&#34; rev 2.00/21.00 addr 14
video1 at uvideo1
uaudio1 at uhub5 port 4 configuration 1 interface 3 &#34;MACROSILICON USB3. 0 capture&#34; rev 2.00/21.00 addr 14
uaudio1: class v1, high-speed, sync, channels: 0 play, 1 rec, 1 ctls
audio2 at uaudio1
uhidev4 at uhub5 port 4 configuration 1 interface 4 &#34;MACROSILICON USB3. 0 capture&#34; rev 2.00/21.00 addr 14
uhidev4: iclass 3/0
uhid7 at uhidev4: input=0, output=0, feature=8
</code></pre></div></div>

<p>I was able to use the
<a href="https://man.openbsd.org/video.1"><code>video(1)</code></a>
utility to see a pixel-perfect (when using YUY2 encoding) copy of the Mac’s
screen.
In my testing, this cheap HDMI capture device was fully capable of displaying
the Mac’s screen at 30fps, though it is of course only a 640x480 1-bit video
stream.
I don’t know whether it (or OpenBSD) is capable of keeping up with a much higher
resolution, full-color source like capturing a game.</p>

<h2 id="streaming">Streaming</h2>

<p>In my usual fashion of wanting to retain control over my content, I didn’t want
to outsource my streaming to YouTube or Twitch.</p>

<p>Since the black and white screen data is so compact (a full screen as a 1-bit
PNG is between 5Kb and 9Kb) and I wanted to avoid blurring it by compressing it
as a video stream, I started
<a href="https://jcs.org/2022/02/16/ioctl">writing a program</a>
to read the YUY2 data directly from the device, discard unnecessary pixel data,
and then send it to a program running on my server that would serve it through
WebSockets.
The idea was that some Javascript could read the WebSocket data as a stream and
directly draw each frame on a <code>&lt;canvas&gt;</code> element.</p>

<p>I eventually scrapped that idea and turned to a more traditional streaming
setup with FFmpeg and the wonderful
<a href="https://github.com/aler9/rtsp-simple-server">rtsp-simple-server</a>.
After much reading documentation, testing, and guessing at why Firefox wouldn’t
play any video that I was sending (it is unable to play <code>avc1.f4001e</code> video), I
reached a solution that works:</p>

<div><div><pre><code>ffmpeg -f v4l2 -i /dev/video1 \
	-c:v libx264 -profile:v main -pix_fmt yuv420p -preset ultrafast \
	-filter:v &#34;crop=512:344:64:69&#34; \
	-max_muxing_queue_size 1024 -g 30 \
	-rtsp_transport tcp \
	-f rtsp rtsp://user:pass@jcs.org:8554/live
</code></pre></div></div>

<p>This instructs FFmpeg to read from the HDMI capture V4L2 device at
<code>/dev/video1</code>, encode it as x264 video with a pixel format of YUV420p needed
for Firefox, after cropping it down from 640x480 to 512x344.
It sends that stream as an RTSP TCP stream to rtsp-simple-server which is
running on my server, which then serves it as an
<a href="https://en.wikipedia.org/wiki/HTTP_Live_Streaming">HLS stream</a>.
To have FFmpeg broadcast while also saving a copy of the stream to a local file,
its <code>tee</code> output can be used instead:</p>

<div><div><pre><code>	[...]
	-rtsp_transport tcp \
	-f tee -map 0:v &#34;stream.mp4|[f=rtsp]rtsp://user:pass@jcs.org:8554/live&#34;
</code></pre></div></div>

<p>The Mac’s screen is 512x342, but for whatever reason I can’t get it to encode
properly at that resolution without adding a 1-pixel black bar at the top,
making it 512x343.
I made it 512x344 just to make it even, and then adjust the video position on my
website to show it as 512x342.</p>

<p>I’m not sending any audio to the stream, but doing so would just be an extra
<code>-c:a</code> flag to ffmpeg to also mux in audio from my laptop’s microphone.
Maybe I’ll do this with just ambient audio of my keyboard and mouse clicks when
I’m not listening to music.</p>

<p>Since the RGBtoHDMI software runs directly on the Raspberry Pi, it boots and
starts displaying HDMI data within a couple seconds of receiving USB power
without having to wait for a full Linux kernel to boot.</p>

<p>Once the video is available, I can run the FFmpeg script and start streaming to
rtsp-simple-server which then makes the stream available to anyone with my
<a href="https://jcs.org/live">Live</a>
page open.
Since I’m not using OBS or other heavy video muxing process, it’s all very
lightweight and my
<a href="https://jcs.org/2021/08/20/matebook">fanless OpenBSD laptop</a>
handles streaming while playing music and running my web browser without any
issues.</p>

<figure>
<a href="https://jcs.org/images/2022-02-21-setup-large.jpg"><img src="https://jcs.org/images/2022-02-21-setup-665x498.jpg" srcset="/images/2022-02-21-setup-665x498.jpg 1x, /images/2022-02-21-setup-1330x996@2x.jpg 2x" width="665" height="498"/></a>
</figure>

<p>On the server side I have nginx serving my usual website, but requests for
things in the <code>/live</code> directory are proxied back to rtsp-simple-server which
handles all of the HLS processing and serving of the individual video chunks and
playlist generation.
I am using
<a href="https://github.com/video-dev/hls.js/">hls.js</a>
to do the actual HLS processing in the browser since only Mobile Safari has
native support for a <code>&lt;video&gt;</code> tag with an HLS stream as its source.
I would have liked to do everything without Javascript, but even my non-video
solution with WebSockets would have required it.</p>

<p>I added some little flourishes on the
<a href="https://jcs.org/live">Live</a>
page like drawing a Mac monitor around the video stream, similar to my
<a href="https://jcs.org/amend">Amend</a>
and
<a href="https://jcs.org/wallops">Wallops</a>
screenshots.
I also used a CSS transition to “turn the screen on” when the video starts
playing by fading in the opacity change.</p>

<p>I’m also displaying a live viewer count which is done by a script on my server
<code>tail</code>ing the rtsp-simple-server log file and making a rough count of viewers
based on unique IPs that have fetched a video chunk within the past 30 seconds.
It writes this count to a text file that some Javascript <code>fetch</code>es every 10
seconds when the video is on and then updates the view counter under the screen.
It would have been nice if rtsp-simple-server could do this rough viewer count
calculation on its own and send the count as some kind of metadata in the HLS
playlist, so I could avoid the additional server process and each client having
to fetch this counter separately.</p>

<figure>
<a href="https://jcs.org/images/2022-02-21-live_page-large.png"><img src="https://jcs.org/images/2022-02-21-live_page-493x374.png" srcset="/images/2022-02-21-live_page-493x374.png 1x, /images/2022-02-21-live_page-986x748@2x.png 2x" width="493" height="374"/></a>
</figure>

<h2 id="chat">Chat</h2>

<p>One component of services like YouTube or Twitch that I did want is integrated
chat.
Users can send comments or questions about what’s going on on screen and I can
respond either in chat or by showing something on the stream.</p>

<p>Since I already have an
<a href="irc://irc.libera.chat/cyberpals">IRC channel</a>
for a community based around my other videos, I just used Libera Chat’s
Javascript chat widget in an <code>&lt;iframe&gt;</code> to allow users to join right from the
stream page.
Of course, for users that don’t want to chat in the browser, they can just join
in a normal IRC client and still interact with me and other viewers, something
that is not (easily) possible with a YouTube or Twitch stream.
I can also be in the IRC channel from my Mac with the
<a href="https://jcs.org/wallops">IRC client</a>
I just wrote for System 6.</p>

<h2 id="casual-broadcasting">Casual Broadcasting</h2>

<p>My
<a href="https://jcs.org/system6c">usual videos</a>
require an iPad on my desk in between my Mac and me<sup><a href="https://www.youtube.com/watch?v=YIp9bEV2-GI">†</a></sup> to film from both
cameras at once which sort of “detaches” me from experiencing the Mac and really
getting into the groove of programming since I have to do everything looking
through the iPad’s screen.
They also require an hour or more of my undivided attention when it’s quiet
enough in my house to record, and then another hour re-encoding the video, then
maybe another of editing the automated closed-captioning files.</p>

<p>Since my live-streaming setup only captures my Mac’s screen and no audio or
video of me, it allows me to broadcast on a whim without having to worry about
what I look like at the moment, what noise is being made elsewhere in my house
(usually my son running around downstairs) or what music I’m playing on my
laptop, and I can get up from my desk whenever I need to for short amounts of
time.</p>

<p>I may add an optional small video and audio stream of myself separately on the
page in the future, but for now I like that it’s just an ephemeral stream of my
Macintosh screen.</p>

</article></div>
  </body>
</html>
