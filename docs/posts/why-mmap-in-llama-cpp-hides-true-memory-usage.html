<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/jeremyphoward/status/1642726595436883969">Original</a>
    <h1>Why MMAP in llama.cpp hides true memory usage</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><div><div><div><div data-testid="primaryColumn"><div aria-label="Home timeline" tabindex="0"><div itemscope="" itemtype="https://schema.org/Collection"><meta content="Tweet with replies" itemprop="name"/><section aria-labelledby="accessible-list-56073" role="region"><div aria-label="Timeline: Conversation"><div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726595436883969" itemprop="identifier"/><meta content="1" itemprop="position"/><meta content="20" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:55.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:55.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726595436883969" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726595436883969" itemprop="mainEntityOfPage"/><article aria-labelledby="id__2142ktj1c2j id__jhn26ux1hok id__wtm5qhv4abn id__f3740nfhuye id__jv19xrgeu3i id__07fc131f46yu id__holbbtsc3aj id__x48igxoau7 id__acj1gfmm4q5 id__ckzubv92fme id__7wgtaqri69r id__fd7dc1yb1mr id__8zfjjxo0itg id__9bw2yw1083w id__z0ol8nmplnm id__6v3pwutw4i8 id__9o54p1xaf0b id__x6jvknnfaf id__9c8dzy6urya" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div><div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>There&#39;s a lot of folks under the misunderstanding that it&#39;s now possible to run a 30B param LLM in &lt;6GB, based on this GitHub discussion.

This is not the case. Understanding why gives us a chance to learn a lot of interesting stuff! </span><span>ðŸ§µ</span></p></div></div></div><div><div aria-labelledby="id__umg3p7f2lsb id__b5g7ym71k8e" id="id__8zfjjxo0itg"><div aria-labelledby="id__f2v040l7dz id__a5hqfapssbh" id="id__b5g7ym71k8e" data-testid="card.wrapper"><div aria-hidden="true" id="id__f2v040l7dz" data-testid="card.layoutLarge.media"><a href="https://t.co/9xOfNyLUFr" rel="noopener noreferrer nofollow" target="_blank" role="link" tabindex="-1"><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/card_img/1641894997917335554/6aldGTXq?format=jpg&amp;name=medium"/></p></div></div></div></a></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726598570045445" itemprop="identifier"/><meta content="2" itemprop="position"/><meta content="4" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:56.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:56.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726598570045445" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726595436883969" itemprop="isPartOf"/><article aria-labelledby="id__r7ub2wr1kqo id__akr3vzs8n9n id__cp8cfz54256 id__tzkg8nzlrj id__iz3b0wvv16r id__knwt7fw67rl id__wp8jg7cyu5 id__elxxa7lq9qr id__4dicdko91hd id__wshftpqx4x id__ss5v0l3o9lo id__4vs8zeij65a id__8akyrkgczs id__pmudmb9f48 id__zp13dnl8nd id__sb74bukzk9r id__ow5870kky9 id__ybebys6xd7l id__hm6boy5j5gb" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><div dir="auto" lang="en" id="id__ss5v0l3o9lo" data-testid="tweetText"><p><span>The background is that the amazing </span></p><p><span> wrote this really cool commit for </span></p><p><span>&#39;s llama.cpp, which modifies how llama models are loaded into memory to use mmap</span></p></div></div></div><div aria-labelledby="id__znrau2jz1p id__v81lsaamrg" id="id__8akyrkgczs"><div aria-labelledby="id__dow5rrvrd2e id__wgddso25f" id="id__v81lsaamrg" data-testid="card.wrapper"><div aria-hidden="true" id="id__dow5rrvrd2e" data-testid="card.layoutLarge.media"><a href="https://t.co/yrhWlxd0Du" rel="noopener noreferrer nofollow" target="_blank" role="link" tabindex="-1"><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/card_img/1642716201850908672/ZDhwwU4n?format=jpg&amp;name=medium"/></p></div></div></div></a></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726601589952513" itemprop="identifier"/><meta content="3" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:56.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:56.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726601589952513" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726598570045445" itemprop="isPartOf"/><article aria-labelledby="id__qaoak23pxa id__qlxcs9qrktq id__y2ev6byxeod id__1f9ffsx3eif id__qtemvci75t id__ormplqiyyi id__i62z0v66sx id__nr3xjdusf7f id__mfh2mh8xrbh id__2259l7fgelt id__n02cu55fi4 id__t43t5wf8nj id__k2ljhng70e id__et1upnhk1av id__ssn9riagi4l id__pdxi0rno4f id__7hhuulj204r id__kyuc0gm4z38 id__416r8fqp82b" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Prior to this, llama.cpp (and indeed most deep learning frameworks) load the weights of a neural network by reading the file containing the weights and copying the contents into RAM. This is wasteful since a lot of bytes are moving around before you can even use the model</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726604144263170" itemprop="identifier"/><meta content="4" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:57.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:57.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726604144263170" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726601589952513" itemprop="isPartOf"/><article aria-labelledby="id__du9pforuvas id__iwojfkbdw1 id__yy6no0fkeq id__vs6jjlfvv2g id__n5w6vd5bgu8 id__asyq4o1yhmm id__b84ihdulgqh id__nxn89d8bv2l id__tnd4d93qnu id__eaeg0kuxgy id__k9fe65tb9wo id__nu5kjknxy1 id__s9voapiiyj id__kynhdtwun1i id__5fdq1wkfcjj id__bqx5lufk6dj id__0c8odumrzfu id__aih8870kyk id__8f8i45jvkhf" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>There&#39;s a *lot* of bytes to move and store, since a 30B model has (by definition!) around 30 billion parameters. If they&#39;ve been squished down to just 4-bit ints, that&#39;s still 30*4/8=15 billion bytes just for the model. Then we need memory for the data and activations too!</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726606799273984" itemprop="identifier"/><meta content="5" itemprop="position"/><meta content="4" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:58.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:58.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726606799273984" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726604144263170" itemprop="isPartOf"/><article aria-labelledby="id__fplv43xextr id__6qyfqyikt3i id__vpcw65at0hs id__jmrjgoe7fis id__1gda3jbwpj7 id__xssekia1rs id__71u4a2qqcxj id__5twc8bs0hph id__iwp8f2wx1u id__f4mbx5vii9a id__ps8gfs4s8eb id__b8w1ppqz4lp id__75pazfngj6h id__czw1oekrhyi id__k7zoyrc8dvh id__ixietezr9nb id__0cexnz9hvwzv id__mfou2z82jp id__ex1cufswxjq" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>mmap is a really nifty feature of modern operating systems which lets you not immediately read a file into memory, but instead just tell the OS you&#39;d like to access some bits in that file sometime, and you can just grab the bits you need if/when you need them</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726609496031234" itemprop="identifier"/><meta content="6" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:58.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:58.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726609496031234" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726606799273984" itemprop="isPartOf"/><article aria-labelledby="id__x4fuqc9xe9 id__c8b9yawb9ul id__kiq2za9zqsi id__916v83r3ywe id__tycoz7ewej id__gbfpa7c1hjc id__gix3dihvkh6 id__d6uqd8u6flh id__makx2jnxp0l id__wq9zsee1t9 id__b9uwmcvvbl8 id__2frdlsikb1v id__rnxmoj0ev5l id__o068eachzo id__zyhyrcrv2pi id__7wolq5bycci id__7lj598s37t9 id__g7d25bjhptb id__do45u8w9qjd" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>The OS is pretty amazing at handling these requests. If you ask for the same bits twice, it won&#39;t re-read the disk, but instead will re-use the bits that are already in RAM, is possible. This is the &#34;cache&#34;.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726612306399232" itemprop="identifier"/><meta content="7" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:59.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:59.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726612306399232" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726609496031234" itemprop="isPartOf"/><article aria-labelledby="id__ib5qs9hgshd id__4eoe8sccklz id__ro79ysx9q1 id__1fk8fevtqmx id__gicy6vkvb08 id__66vm6sbcxkc id__so7o1wk5al id__k3qc16f385 id__qbxv1vts2qq id__xlfw2p05af id__6ow30ym2upu id__47p92ni1qqq id__ogwg6byyzf id__q0c5ju2p07 id__hi91nmq5ssk id__93e1narna49 id__3oq6zqydzps id__rcpdrxsvlbk id__v94woi7184p" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Early responses to the issue were under the mistaken impression that following the mmap commit, llama.cpp was now only &#34;paging in&#34; (i.e. reading from disk) a subset of the model, by grabbing only the parameters needed for a particular batch of data</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726614785220608" itemprop="identifier"/><meta content="8" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:00.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:00.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726614785220608" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726612306399232" itemprop="isPartOf"/><article aria-labelledby="id__ik38jbo4jz id__8mi92llryx id__xec4t57xpo id__3sjpz6h59sv id__93n0zsmsyu id__fsr7gp3ds1j id__4umjao40hzj id__byrrpavda3v id__6f8s1f0fv3r id__8mta0zq6tfm id__ehfebqmjedj id__9njl8dn4yl id__t7hm5jgsyc id__4lr8g3uwo7i id__b8dusw2rgg id__8ggzb3ufzy id__0u7ef5kmhtc id__46quu7p46as id__4uiyicvjumr" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>This is based on a misunderstanding of how transformers models work. Transformers are very simple models. They are functions which apply the following two operations a bunch of times:
- An MLP with 1 hidden layer
- Attention</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726617503105024" itemprop="identifier"/><meta content="9" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:00.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:00.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726617503105024" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726614785220608" itemprop="isPartOf"/><article aria-labelledby="id__yuzu5pjeu1 id__e53o0l5kps7 id__vp4zbzb337 id__s0gbiureem id__tlburjdqren id__l0shph6mybm id__kyz67lfm86 id__7z1vjxf2c4x id__c4c7a7ligbh id__85tat3uzbbj id__713keobego4 id__oj5jrekp7q id__srfj17hymf id__hwce6anihiu id__o1xnsx4ml0k id__xzj0sgjgn4 id__1e7bqjf58ajj id__4z73j3sc753 id__chqx7dzbrwa" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>An MLP (multi-layer) perceptron with one hidden layer is a matrix multiplication followed by an activation function (an elementwise operation such as ReLU: `max(0,x)`), followed by another matrix multiplication.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726620082606080" itemprop="identifier"/><meta content="10" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:01.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:01.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726620082606080" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726617503105024" itemprop="isPartOf"/><article aria-labelledby="id__vgup6y97mwh id__3rvaejhecil id__iyh9dq04w4g id__birwidiwpp7 id__rnmnd5y6zi id__fsnhetbskcs id__h29koyv89bl id__p80e3sur6l id__vpt1wxn7ona id__bx712wsdtm9 id__tesxeiwp64a id__rqi7x6opq3 id__3ja74dqapv5 id__k483mze5q4k id__e6b8e4fnncn id__lze8rltw73l id__4cpev42liwj id__9h169b8a57t id__fcp3ri8269j" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Attention is the operation shown in this code snippet. This one does &#34;self attention&#34; (i.e q, k, and v are all applied to the same input); there&#39;s also &#34;cross attention&#34; that uses different inputs.</span></p></div></div><div aria-labelledby="id__90g7bm4fvke id__u5fjov1gyzi" id="id__3ja74dqapv5"><div><div><div><div><div><div role="button" tabindex="0"><div><div><div aria-label="Image" data-testid="tweetPhoto"><p><img alt="Image" draggable="true" src="https://pbs.twimg.com/media/FswfxD8aQAA4qoO?format=jpg&amp;name=medium"/></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726623106703360" itemprop="identifier"/><meta content="11" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:01.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:01.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726623106703360" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726620082606080" itemprop="isPartOf"/><article aria-labelledby="id__wyqjdglptng id__g69rwj31lxo id__6c80zgz9ggu id__wfhaa1xfhjl id__ms57xytth1 id__ydn0lazml9 id__1st083ifmlsj id__y4ok36m7my id__ao2x70o8rwf id__9pb3yts4roc id__eaemmxn1one id__hq21zgn8u2v id__2i5n0imwawf id__rzcmg4d9k2c id__426p2s4z6uf id__8ihzbndw0q id__7d4390zfbst id__z9yprnzg19l id__dtaqnryjmhv" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>OK that was a slight simplification. Transformers actually use &#34;multi-head&#34; attention which groups channels first. But as you see from the implementation below, it&#39;s still just using some matrix multiplies and activation functions.</span></p></div></div><div aria-labelledby="id__6lnmh6rj1g2 id__znouve2gedp" id="id__2i5n0imwawf"><div><div><div><div><div><div role="button" tabindex="0"><div><div><div aria-label="Image" data-testid="tweetPhoto"><p><img alt="Image" draggable="true" src="https://pbs.twimg.com/media/FswgbtJagAAuQbd?format=jpg&amp;name=large"/></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726626080477184" itemprop="identifier"/><meta content="12" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:02.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:02.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726626080477184" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726623106703360" itemprop="isPartOf"/><article aria-labelledby="id__4lzvx470xm9 id__xup5x5bw2mr id__pavpjcbt9fi id__ir6zzyw4ewc id__wwapjtbjsb id__c6o3kqu0oqv id__lognjuk79xs id__ki7kytm12hq id__fhqlcl3srmm id__yixnv2ne7tj id__kbgv2rx594 id__98xn57l6kh id__aug0kwmrx2w id__83fsex4wbjg id__6mwq5di30z id__c66c85zp4vk id__j0ewi8purfd id__0bvlktnydab8 id__pbfpd3mrek" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>So every parameter is used, because they&#39;re just matrix multiplications and element-wise operations that they&#39;re used in. There&#39;s no true sparsity occurring here -- there&#39;s just some &#34;soft attention&#34; caused by the use of softmax.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726628638994432" itemprop="identifier"/><meta content="13" itemprop="position"/><meta content="4" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:03.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:03.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726628638994432" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726626080477184" itemprop="isPartOf"/><article aria-labelledby="id__c1f7i1518xf id__30zfh2z2l8a id__lim276p0dd id__uq9q7sgz2d id__xw2tb9wpogg id__45auioxh5hi id__vdj7ls18di id__p87gzc065f id__5ojorc1ewm4 id__po9172knvnb id__aulk03mfg7c id__lh02v98lymj id__tsdv82ar309 id__m4nd3ze0wy id__6dvb8m8tzue id__d5lltvlxky5 id__57yme6k4eca id__1lx27gkf1a5 id__097mxfrhdpw9" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Therefore, we don&#39;t really get to benefit fully from the magic of mmap, since we need all those parameters for every batch of data. It&#39;s still going to save some unnecessary memory copying so it&#39;s probably a useful thing to use here, but we still need RAM for all the parameters</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726631654699009" itemprop="identifier"/><meta content="14" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:04.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:04.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726631654699009" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726628638994432" itemprop="isPartOf"/><article aria-labelledby="id__rseuqy66sz id__3q6eh4ridh6 id__mbkh17q0tup id__1g2mdq6f5kl id__zx1x0fur8jb id__h6pfqmo5n4j id__l86nd0y1e8 id__oupn3yezxa id__t0j6k5prze id__wj5ykin0967 id__t92bkmahscn id__elutrd11hlg id__qqwby5bm7k8 id__n5giohdjlo id__bgdvogrniwa id__xtpzouac61 id__s83ptua4rhe id__ptdkqyb7l8h id__hop2kxjhtih" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>The way mmap handles memory makes it a bit tricky for the OS to report on a per-process level how that memory is being used. So it generally will just show it as &#34;cache&#34; use. That&#39;s why (AFAICT) there was some initial misunderstanding regarding the memory savings</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726634687184896" itemprop="identifier"/><meta content="15" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:04.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:04.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726634687184896" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726631654699009" itemprop="isPartOf"/><article aria-labelledby="id__wqmfdumtrks id__rvp5w6fpc7 id__cmrz8zd58p8 id__msf3mmbwvbn id__89g9n4gmket id__bj112d6pjl id__wt56pyjmpg8 id__mgfvvirvc7 id__5s5diohpo1w id__x3hozoyv6gn id__yydhl6kwjd id__w4rlalvra2 id__pbdfhzdt718 id__rmv0lgpagaq id__7nexprcgqmk id__smibme34qeg id__h421clvqpm4 id__rzdg5av7ae id__4anr4gx5513" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>One interesting point though (mentioned in the GH issue) is that you can now have multiple processes all using the same model, and they&#39;ll share the memory -- pretty cool!</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726637270892544" itemprop="identifier"/><meta content="16" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:05.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:05.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726637270892544" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726634687184896" itemprop="isPartOf"/><article aria-labelledby="id__yuqjqq3sd4p id__x45q9fzvaud id__zydbu72bag id__qono76ipqka id__l6cbgb03hu id__dugjd779u8 id__joxhbtyafr id__iv3nw5c02ql id__bg2lp3evbxg id__kt1elq9xnk id__411pi43ncrb id__51umrvv7q0v id__sk9az3gxhbc id__xf89pm8x3hm id__ggd25jjfzs id__k2l4w9q7k2e id__j0hfmif8u88 id__m1pakx3a51 id__m0z9at182tp" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Note however that none of this helps us use the GPU better -- it&#39;s just for CPU use (or systems with integrated memory like Apple silicon).

By far the best speed is with an NVIDIA GPU, and that still requires copying all the params over to the card before using the model.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726639795843073" itemprop="identifier"/><meta content="17" itemprop="position"/><meta content="7" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:05.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:05.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726639795843073" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726637270892544" itemprop="isPartOf"/><article aria-labelledby="id__7njrgbvt3ht id__2zvgl4exybw id__ajg9z7wj3i id__r70883ti2ji id__uu5qhg7gims id__83wtlf3osdt id__9r0h160f1el id__np89dpswym id__sh3wtw0wt2p id__7td4ht8e0k7 id__m3t2b0543jb id__ng5i3qebhg id__46fmqrllcys id__vpy69u7tijf id__qvm0suldk2l id__7exd8po9vm id__xpw3ps0wwj id__y04299vndwc id__gxts095qnyj" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Hopefully you found that a useful overview of mmap and transformers. BTW these topics are covered in depth in our upcoming course (the above code is copied from it) which will be released in the next few days, so keep an eye out for that!</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726816338284544" itemprop="identifier"/><meta content="18" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:48.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:48.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726816338284544" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726639795843073" itemprop="isPartOf"/><article aria-labelledby="id__khdf1rfi9do id__wjtcc2i53d id__v5q3zn7juv id__hcbuhxxuhj7 id__voq8209q4t id__8lcq6cu2ejl id__glhl1rox709 id__n7gqqzno3bk id__khfls0kj55r id__6plf0kqkp77 id__gctzcgksydw id__87xhrioq19s id__5yla91iusy id__z9e16twkoic id__eabsgj6pcab id__jutgtcyrhoe id__aed9q269ye8 id__nheilf9c4t id__1hupzoqr4s1" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><div dir="auto" lang="en" id="id__gctzcgksydw" data-testid="tweetText"><p><span>h/t </span></p><p><span> for encouraging me to write this thread.</span></p></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642728014051168256" itemprop="identifier"/><meta content="19" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:17:33.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:17:33.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642728014051168256" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726816338284544" itemprop="isPartOf"/><meta content="https://twitter.com/jeremyphoward/status/1642726620082606080" itemprop="isBasedOn"/><article aria-labelledby="id__nzdd1a2z8w id__qrd17azeg5j id__re7jbdw728 id__z9gvyiwfxj id__f0k8ow40b97 id__njly8dqfo4q id__r62q1xvxwm id__qlirlbr0ikk id__3mew9h4hcvf id__kingt8wxjj id__k8xcmlhzs1c id__6zo34sz336c id__ue1666v715 id__6chotw7cgrt id__zv8j1txarx id__0s9tgbbslwgl id__velk1c4ayk id__i1rp1tpal5r id__g67ftyusl25" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Minor correction: the normalization layer I used here, Batchnorm2D, isn&#39;t what&#39;s used in a normal language model. I took the code from an image model we built for the course. In most language models we&#39;d use Layernorm for normalization.</span></p></div></div><div aria-labelledby="id__ox7boowop7 id__ssviwsyqwd" id="id__ue1666v715"><div id="id__ox7boowop7"><p><span>Quote Tweet</span></p><div tabindex="0"><div><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div><div><p><span>Attention is the operation shown in this code snippet. This one does &#34;self attention&#34; (i.e q, k, and v are all applied to the same input); there&#39;s also &#34;cross attention&#34; that uses different inputs.</span></p><p><span>Show this thread</span></p></div><div><div><div><div><div><div><div><div><div aria-label="Image" data-testid="tweetPhoto"><p><img alt="Image" draggable="true" src="https://pbs.twimg.com/media/FswfxD8aQAA4qoO?format=jpg&amp;name=medium"/></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642734538081312768" itemprop="identifier"/><meta content="20" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:43:29.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:43:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642734538081312768" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642728014051168256" itemprop="isPartOf"/><meta content="https://twitter.com/rlacombe/status/1642728470995140608" itemprop="isBasedOn"/><article aria-labelledby="id__azcyrn1ihtv id__z4a0t4kvfu id__fbkaj1mt6wc id__z6542m79xb id__wdyes3hxsf id__9ll7gca1hz6 id__zwki0cepcf id__h3ofzmi47fv id__mqhlhuo4bri id__d1lptdxek2g id__nn5p8kwjirk id__bvtg9ayimpi id__78fxtaohb9k id__xol5nac7zt id__i4zxb3nasc id__90qy0gohgjj id__d7zvhk5unw id__bh62zmztez9 id__l72ok69jdl" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>D&#39;oh I meant to mention this and forgot!

This is a great point. The first layer of a language model is an &#34;embedding&#34; layer. Mathematically, it&#39;s matrix multiply by a bunch of one-hot encoded vectors. But it&#39;s implemented as an array index lookup.</span></p></div></div><div aria-labelledby="id__570z8uugjpp id__kjbi6y996v" id="id__78fxtaohb9k"><div id="id__570z8uugjpp"><p><span>Quote Tweet</span></p><div tabindex="0"><div><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div data-testid="UserAvatar-Container-rlacombe"><div><div><div><div><a href="https://twitter.com/rlacombe" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1628925690237095937/p8MjHgFg_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div><div><p>Replying to <span dir="ltr"><span>@jeremyphoward</span></span> <span>and</span> <span dir="ltr"><span>@iScienceLuvr</span></span></p><p><span>What about embeddings though? Presumably each sequence uses a tiny subset of the vocabulary, so we likely donâ€™t load most embedding matrix parameters?</span></p></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642734540593692673" itemprop="identifier"/><meta content="21" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:43:29.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:43:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642734540593692673" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642734538081312768" itemprop="isPartOf"/><article aria-labelledby="id__98ah59wkvf4 id__551x55son0a id__ummn3s2sofa id__s4hobdmu9ai id__91d3h1k6b3u id__obtzbii2r7l id__kicxp8n0csa id__yhvojz35d5d id__8zj3g1xydr id__lirgaf3t52r id__e256pwtzel id__xh7tzjn7x3 id__zdayh9wuq6j id__zjvs5z45r3o id__ex2eb26t5qa id__puhqmivpgvp id__q9k49txxixo id__x0xcs8w862 id__jhg5f8ygtjo" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Depending on how it&#39;s implemented, it&#39;s possible that this could avoid reading some of the embedding weights into memory. However, the OS has a minimum amount of stuff it reads at once around a location, so it might still end up reading most or all of the embeddings.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642734542833475584" itemprop="identifier"/><meta content="22" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:43:30.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:43:30.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642734542833475584" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642734540593692673" itemprop="isPartOf"/><article aria-labelledby="id__0j5m1glvpx9d id__o0qef2a02d8 id__oj902xc68y id__2k315dd83sf id__ezmqi72u79g id__hbh2c5n355o id__mjwxfi481u id__kxnfmruzpg id__t8dp4d5lj5g id__zk6bjapsi7q id__h2rjnxtp43k id__q4tjoqqtykl id__bmii5o4r0sa id__nym52xpgnp id__djduavla9q5 id__a13vgomn0jf id__uj3w3z3pund id__j9zivf0sy6 id__72fjftoq7qo" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Having said that, the embedding weights are a fairly small proportion of the model parameters, so this won&#39;t generally make a big difference either way.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642735748620197889" itemprop="identifier"/><meta content="23" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:48:17.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:48:17.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642735748620197889" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642734542833475584" itemprop="isPartOf"/><meta content="https://twitter.com/igorcosta/status/1642734042545291271" itemprop="isBasedOn"/><article aria-labelledby="id__smb07ho4d7 id__q9ykrtgj1z id__iyigvacqbe id__adfcxr5zs7i id__vo4y7bnjq1j id__4pg13no5tqn id__dfvjkpzeqph id__79xxhkaar7j id__oir2rhnkq9p id__u08jxx82faa id__inq740505bp id__cxr6fmdgwh id__xlhi1hhplcf id__v59rrv9dkh id__1yvw2zc1wufj id__lbuut1c5n2 id__qne10kfupn id__frpyzcvmphp id__mvvzh37zps" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Oops another correction! I forgot to mention positional encoding. This is the layer after the embeddings. It can be implemented using sin/cos across a few freqs (&#34;sinusoidal embeddings&#34;) or the code snippet below (&#34;learnable embeddings&#34;). 
</span></p></div></div><div aria-labelledby="id__rk3nl32mntl id__4qlm3g0mfi2" id="id__xlhi1hhplcf"><div><div><div><div><div><div role="button" tabindex="0"><div><div><div aria-label="Image" data-testid="tweetPhoto"><p><img alt="Image" draggable="true" src="https://pbs.twimg.com/media/Fswq23oagAAy0rH?format=jpg&amp;name=medium"/></p></div></div></div></div></div></div></div></div></div><div id="id__rk3nl32mntl"><p><span>Quote Tweet</span></p><div tabindex="0"><div><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div data-testid="UserAvatar-Container-igorcosta"><div><div><div><div><a href="https://twitter.com/igorcosta" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1445989849396252682/-OZu2XPU_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div><div><div><div><p>Replying to <span dir="ltr"><span>@jeremyphoward</span></span></p><p><span>Nice thread </span><span><span dir="ltr">@jeremyphoward</span></span><span>. Also, point to note, is positional encoding since it&#39;s crucial in providing information about the position of tokens in the input sequence since transformers do not have any inherent understanding of the sequence order.</span></p></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642735752202297345" itemprop="identifier"/><meta content="24" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:48:18.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:48:18.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642735752202297345" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642735748620197889" itemprop="isPartOf"/><article aria-labelledby="id__2liyopw9sem id__1i7oeai5ofeh id__0e2wmn3qk8ht id__59nor18p42j id__w5wg6vbvwb id__6k6iay3krla id__ua9jnwuzvjn id__fbodvupldcs id__c5dsk0gqq1a id__f9fw83r92u id__4m65uyo44v id__ura00j8pya id__tzcgumyzk4 id__loifqtojb7 id__ks28vf3v7e id__gtiik2pteel id__g8mh71xlhs id__5w8iuvctf4d id__gny000915fh" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Positional encoding is used to assign a unique vector to each position in the input token vector, so that the model can learn that the location of a words matters -- not just its presence/absence.

(They have few parameters do don&#39;t impact memory significantly.)</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642746913895354368" itemprop="identifier"/><meta content="25" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T04:32:39.000Z" itemprop="dateCreated"/><meta content="2023-04-03T04:32:39.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642746913895354368" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642735752202297345" itemprop="isPartOf"/><meta content="https://twitter.com/amasad/status/1642381582572093441" itemprop="isBasedOn"/><article aria-labelledby="id__83drp8xg52i id__hicpeow36ju id__46whco2nkfb id__k3320a201si id__91hxgtqg359 id__adstsmu580o id__arrrctlugi8 id__xw9x9ay3o4 id__kaktu4avlrb id__k72s5bhy9p8 id__rkzqp3iz5zi id__964yug9qds id__9mnzobcre6j id__0pgy597k0gv id__icprwojt2qa id__fpkqj35r9a id__jgwytvvjna id__m6nku4zebt9 id__o2lnk8khdhl" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://twitter.com/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><div dir="auto" lang="en" id="id__rkzqp3iz5zi" data-testid="tweetText"><p><span>Heh turns out </span></p><p><span> has already weighed in with his thoughts! :D</span></p></div></div></div><div aria-labelledby="id__xa2fr3w00rd id__5q055erlnkw" id="id__9mnzobcre6j"><div id="id__xa2fr3w00rd"><p><span>Quote Tweet</span></p><div tabindex="0"><div><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div data-testid="UserAvatar-Container-amasad"><div><div><div><div><a href="https://twitter.com/amasad" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1518391475687493633/Bb6zQKr8_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div><div><p><span>For a day programmers thought mmap possessed magical compression abilities, loading the 20GB LLaMa weights into 6 GB of RAM. Turns out it&#39;s an accounting bug. This must be the first instance of collective hacker magical thinking.</span></p><p><span>Show this thread</span></p></div></div></div></div></div></div></div></div></div></article></div></div></div></div></section></div></div></div></div></div></div></div></div>
  </body>
</html>
