<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://twitter.com/jeremyphoward/status/1642726595436883969">Original</a>
    <h1>Why MMAP in llama.cpp hides true memory usage</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><div><div><div><div data-testid="primaryColumn"><div aria-label="Home timeline" tabindex="0"><div itemscope="" itemtype="https://schema.org/Collection"><meta content="Tweet with replies" itemprop="name"/><section aria-labelledby="accessible-list-2834" role="region"><div aria-label="Timeline: Conversation"><div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726595436883969" itemprop="identifier"/><meta content="1" itemprop="position"/><meta content="19" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:55.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:55.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726595436883969" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726595436883969" itemprop="mainEntityOfPage"/><article aria-labelledby="id__w34f9xnocje id__1t57peyjhjn id__8l5r01fzub9 id__xnsbck9jhz id__11qeqfwk6xic id__b2efycgyk34 id__ab8ds3q193e id__eyvtjb9h93 id__bmjsipy107j id__0zxhwotp811p id__nrw3sq0cc9j id__i0wk5gffako id__tpxoivpiz3 id__sfa1cg6al6 id__r9qagrtp4cq id__nwon021tmr id__0mamkfmcymxr id__e2eqpxqhz7a id__ns9i4a2xopl" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div><div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>There&#39;s a lot of folks under the misunderstanding that it&#39;s now possible to run a 30B param LLM in &lt;6GB, based on this GitHub discussion.

This is not the case. Understanding why gives us a chance to learn a lot of interesting stuff! </span><span>ðŸ§µ</span></p></div></div></div><div><div aria-labelledby="id__vejqzjbf1r id__xw7bjzl7n9q" id="id__tpxoivpiz3"><div aria-labelledby="id__pv66zjxt2h id__8kxisdyaq1s" id="id__xw7bjzl7n9q" data-testid="card.wrapper"><div aria-hidden="true" id="id__pv66zjxt2h" data-testid="card.layoutLarge.media"><a href="https://t.co/9xOfNyLUFr" rel="noopener noreferrer nofollow" target="_blank" role="link" tabindex="-1"><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/card_img/1641894997917335554/6aldGTXq?format=jpg&amp;name=medium"/></p></div></div></div></a></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726598570045445" itemprop="identifier"/><meta content="2" itemprop="position"/><meta content="4" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:56.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:56.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726598570045445" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726595436883969" itemprop="isPartOf"/><article aria-labelledby="id__ljurt7ma7t9 id__1s3y6rtrx2h id__6sta4u1sxtg id__bbzkuicxsqm id__5ng19pbga4j id__exx7lbhrnr9 id__fo27ah6xok id__xs0eao913if id__prd0c6dife id__90jnyrh8dqm id__psxx6c1cpak id__yr8aokxarkq id__gh9bmmj6yo id__jez9fdgqt4m id__lvzig5hs1mk id__7sbqujr24ql id__6wj79nrk8kf id__y5bzbtimuv id__qlkniop0ecs" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><div dir="auto" lang="en" id="id__psxx6c1cpak" data-testid="tweetText"><p><span>The background is that the amazing </span></p><p><span> wrote this really cool commit for </span></p><p><span>&#39;s llama.cpp, which modifies how llama models are loaded into memory to use mmap</span></p></div></div></div><div aria-labelledby="id__np9hh76owq id__oz9s30mzri" id="id__gh9bmmj6yo"><div aria-labelledby="id__zgggx5x8sgn id__8ez83kzu23u" id="id__oz9s30mzri" data-testid="card.wrapper"><div aria-hidden="true" id="id__zgggx5x8sgn" data-testid="card.layoutLarge.media"><a href="https://t.co/yrhWlxd0Du" rel="noopener noreferrer nofollow" target="_blank" role="link" tabindex="-1"><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/card_img/1642716201850908672/ZDhwwU4n?format=jpg&amp;name=medium"/></p></div></div></div></a></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726601589952513" itemprop="identifier"/><meta content="3" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:56.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:56.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726601589952513" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726598570045445" itemprop="isPartOf"/><article aria-labelledby="id__ip51ermr9ze id__yl1c0egmy49 id__l117z5407k id__tgx0jtt19r id__mky82zkitim id__v93gjvv61b id__1qzr6qwxtfx id__f9wzaiptymg id__y76dxnomfel id__fjj1i58miur id__xlypx2mhqa id__ysyzi1kwlp id__ct273gwgmef id__wwn5l0z7qh id__6vhs8bg889a id__gjgssk7xxxu id__ply1y875or id__8j53qk4vnux id__4tfadg5tcv9" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Prior to this, llama.cpp (and indeed most deep learning frameworks) load the weights of a neural network by reading the file containing the weights and copying the contents into RAM. This is wasteful since a lot of bytes are moving around before you can even use the model</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726604144263170" itemprop="identifier"/><meta content="4" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:57.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:57.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726604144263170" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726601589952513" itemprop="isPartOf"/><article aria-labelledby="id__uy1015wa1l id__napjsrkkidn id__hs9plym5vom id__wn8ikjvxkdo id__huzsd71j6h id__lqa6l4vyac id__kp5djru6k1 id__ps9xxvn68u id__my37qad987r id__3b6si9ri86j id__3t76n0irjde id__bp3w7d38mbi id__izmlx4csm7c id__iqxmhhkuq0s id__pojphx73r8 id__j1o5s2miv1 id__eqeh6ivnrj id__okl1jmkwypi id__kfgvplypfg" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>There&#39;s a *lot* of bytes to move and store, since a 30B model has (by definition!) around 30 billion parameters. If they&#39;ve been squished down to just 4-bit ints, that&#39;s still 30*4/8=15 billion bytes just for the model. Then we need memory for the data and activations too!</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726606799273984" itemprop="identifier"/><meta content="5" itemprop="position"/><meta content="4" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:58.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:58.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726606799273984" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726604144263170" itemprop="isPartOf"/><article aria-labelledby="id__oiyq14nixgf id__jd9zm0kiwip id__87ju7u8l88n id__hfmggonxffj id__6uhfgtbawdq id__8epqzitvxa id__6z8dhe85thg id__wzadk8compm id__tg2wxult5kq id__o6ctxmti3so id__sa2nme8xbkn id__t8325zyeb3o id__xdcit0nbwsb id__xqs51rj6o9 id__sd5e9kfxm8 id__vhdr9weq4el id__0dgqm22b75j4 id__ifxfeuiv2gb id__macrw91jkg" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>mmap is a really nifty feature of modern operating systems which lets you not immediately read a file into memory, but instead just tell the OS you&#39;d like to access some bits in that file sometime, and you can just grab the bits you need if/when you need them</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726609496031234" itemprop="identifier"/><meta content="6" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:58.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:58.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726609496031234" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726606799273984" itemprop="isPartOf"/><article aria-labelledby="id__1rxow8on6hz id__dzf9irxhwjm id__18sqwh1lwn8i id__e0ppn82fke5 id__bi4lj5g0ue id__72byqb2s3na id__ggqv3dp8fzi id__g40d58vnvbt id__xp6bd4iwghe id__weg9xz4vyoa id__phsz088upm id__6b2mxa6pdn id__yq7rrliktaa id__1i4ykl2djzx id__8dzxon5d19 id__u3e5oocznrg id__6cd3mpffpsj id__2ituskdfoig id__02n9tdkagym4" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>The OS is pretty amazing at handling these requests. If you ask for the same bits twice, it won&#39;t re-read the disk, but instead will re-use the bits that are already in RAM, is possible. This is the &#34;cache&#34;.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726612306399232" itemprop="identifier"/><meta content="7" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:11:59.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:11:59.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726612306399232" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726609496031234" itemprop="isPartOf"/><article aria-labelledby="id__lqt1fy8n14 id__aqcvunb82c id__o0q2edjd0h id__doiecnqlv5h id__atrn3b1rsrp id__rxn3hdjh0r id__fqwdpvsvnm id__0eusxlewar5b id__thkuvdpi0eo id__77qd1cnmptk id__gg7fq9wa8qm id__cc0euvt4lzu id__h4tjtvhwfe id__y0tezyvwxgh id__a2zd7cdsqva id__wxfsfmojws id__ebg25u3ob8c id__8bca2dbyics id__85pa87sm5et" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Early responses to the issue were under the mistaken impression that following the mmap commit, llama.cpp was now only &#34;paging in&#34; (i.e. reading from disk) a subset of the model, by grabbing only the parameters needed for a particular batch of data</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726614785220608" itemprop="identifier"/><meta content="8" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:00.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:00.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726614785220608" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726612306399232" itemprop="isPartOf"/><article aria-labelledby="id__jpg3h2t68kk id__21ksidebwpd id__855bub015d7 id__yg31cp6j65m id__57uw6kmv1lq id__0023zcxpglv1a id__31gu67msgu7 id__u93inkbp9sb id__0nw5fnkf5kt id__yoax660f80m id__zitc5pejugh id__az9og8cjigu id__f7xe8jcl3hl id__bjyq7e633z id__o1ghm5queba id__uz0qgwc4rep id__j4ut08brzz id__iof9squxqo id__wg4sbmeygr" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>This is based on a misunderstanding of how transformers models work. Transformers are very simple models. They are functions which apply the following two operations a bunch of times:
- An MLP with 1 hidden layer
- Attention</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726617503105024" itemprop="identifier"/><meta content="9" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:00.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:00.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726617503105024" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726614785220608" itemprop="isPartOf"/><article aria-labelledby="id__vis1iap4lnd id__fyma0wkdzii id__0bhbtnenqj87 id__kd0ie2fky2b id__3cbsblwzl58 id__nhte8ymrgy id__446ap3nj5hk id__cium0zppkr6 id__qq7awqdvyrh id__bgah0abz2er id__r7gjiinxxyo id__vkwyygshyuf id__cyofusjhwxl id__5yd4acaknij id__947gty8yz7 id__537752tbxy6 id__kewa1hzc4qa id__ulndg9qg46 id__5zoy4n14zz7" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>An MLP (multi-layer) perceptron with one hidden layer is a matrix multiplication followed by an activation function (an elementwise operation such as ReLU: `max(0,x)`), followed by another matrix multiplication.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726620082606080" itemprop="identifier"/><meta content="10" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:01.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:01.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726620082606080" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726617503105024" itemprop="isPartOf"/><article aria-labelledby="id__02githtwx4t id__3eaasnqy1g7 id__kh5vrrxwovj id__mlhex41rzd9 id__jsjip4al9e id__eqsz7y8kvwj id__jwz1zwjloqj id__m5s0ppw4y2j id__nq19cp0a1o id__d36lzvn00yi id__ufzrz3vmpqq id__54d45vy26xr id__vaxczd53dxm id__cwnk00tc40b id__9esyqw8qxvd id__rp9xj9w24b id__s720svizonr id__8f524wmzajr id__yczl46u76yn" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Attention is the operation shown in this code snippet. This one does &#34;self attention&#34; (i.e q, k, and v are all applied to the same input); there&#39;s also &#34;cross attention&#34; that uses different inputs.</span></p></div></div><div aria-labelledby="id__44n3n4yi7e id__adwk04n969" id="id__vaxczd53dxm"><div><div><div><div><div><div role="button" tabindex="0"><div><div><div aria-label="Image" data-testid="tweetPhoto"><p><img alt="Image" draggable="true" src="https://pbs.twimg.com/media/FswfxD8aQAA4qoO?format=jpg&amp;name=medium"/></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726623106703360" itemprop="identifier"/><meta content="11" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:01.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:01.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726623106703360" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726620082606080" itemprop="isPartOf"/><article aria-labelledby="id__gaswgno8z1w id__tr7qt4enyv id__jm87x8kfrr id__4l9glejctmq id__d52xehc7tpq id__7w5v70b1yho id__jh1i9maow7m id__raj057c69nc id__m0myptrwt6k id__hdh67b7ib77 id__59yu6h5bdng id__771tmrp9ikx id__hcvr68bxlqg id__r0tmld5b8o id__8ogqfij8pf4 id__qabx22fv2i9 id__pxze21bp11a id__tu92hw7lk7s id__mw6l8upl3lb" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>OK that was a slight simplification. Transformers actually use &#34;multi-head&#34; attention which groups channels first. But as you see from the implementation below, it&#39;s still just using some matrix multiplies and activation functions.</span></p></div></div><div aria-labelledby="id__wb2u7bcezg id__rtt2dv35wb" id="id__hcvr68bxlqg"><div><div><div><div><div><div role="button" tabindex="0"><div><div><div aria-label="Image" data-testid="tweetPhoto"><p><img alt="Image" draggable="true" src="https://pbs.twimg.com/media/FswgbtJagAAuQbd?format=jpg&amp;name=large"/></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726626080477184" itemprop="identifier"/><meta content="12" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:02.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:02.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726626080477184" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726623106703360" itemprop="isPartOf"/><article aria-labelledby="id__35fwh3va7vn id__ccnhn2bfv4d id__9ewltctbted id__8geaqjmedfw id__ubpwnblrtcg id__4iyc71prqff id__m3xs30sfg2 id__n99aonkfmba id__nhmft3rxqgp id__lzd1qaiija id__odmfefvbcxr id__52afton8pvr id__bvvz6i3v5wf id__b3b4xvzg7tk id__j2woky4tygb id__1iqjtgarfdk id__916hq1coovp id__gqqosjg4pma id__m4guf43595o" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>So every parameter is used, because they&#39;re just matrix multiplications and element-wise operations that they&#39;re used in. There&#39;s no true sparsity occurring here -- there&#39;s just some &#34;soft attention&#34; caused by the use of softmax.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726628638994432" itemprop="identifier"/><meta content="13" itemprop="position"/><meta content="4" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:03.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:03.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726628638994432" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726626080477184" itemprop="isPartOf"/><article aria-labelledby="id__r3m2gqjb47i id__mhec25pyghe id__s9i7p3srbvt id__2m848h3yyg9 id__2qfyoptaysc id__rbbcsqsl4ol id__nv5n2inqb9 id__umki7bhjiio id__ymomzjy0r3 id__z0n3g1cwk7d id__fzm6057a5r id__xgeqgmdk63o id__fq6o7sg98it id__ljs3xfspvg id__m7awq1jmvop id__akpgkhrjo8i id__xacwcd0pt3q id__rvr8kkro8s id__gpl678po5vj" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Therefore, we don&#39;t really get to benefit fully from the magic of mmap, since we need all those parameters for every batch of data. It&#39;s still going to save some unnecessary memory copying so it&#39;s probably a useful thing to use here, but we still need RAM for all the parameters</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726631654699009" itemprop="identifier"/><meta content="14" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:04.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:04.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726631654699009" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726628638994432" itemprop="isPartOf"/><article aria-labelledby="id__a7hyobeeqzg id__sukk5onleng id__dgs4ru1i45u id__3gcdqus06fa id__h1njfwq7z1h id__dj7j59fhrta id__rhao9ayfvvq id__m27evwq1jh id__mtnjwzm7idc id__vbc74u5j1cl id__cj5ok9dhzu5 id__qcyv4lsei8d id__7v3a13rm7dv id__vgkks15u8p id__rtwfn27jobd id__ia0p70yiq6k id__z7skffwxt2b id__ixabs1eerr id__tj4an28fp7" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>The way mmap handles memory makes it a bit tricky for the OS to report on a per-process level how that memory is being used. So it generally will just show it as &#34;cache&#34; use. That&#39;s why (AFAICT) there was some initial misunderstanding regarding the memory savings</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726634687184896" itemprop="identifier"/><meta content="15" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:04.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:04.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726634687184896" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726631654699009" itemprop="isPartOf"/><article aria-labelledby="id__02yik8ta69mo id__8mwxaqizmy id__xx7khfo7ofr id__kdq7vyjwqyh id__p7lsi7hcfd id__f76bi73k5js id__7ck8592crf8 id__ws8u58qqba id__qntlb9t37um id__g8h57g4aktk id__rqpk401493d id__445rwiw317t id__0bbmt6vv2iz9 id__091z45ajy5mo id__ipm8c651e1 id__oeu0t7ate2i id__l01x7ontam id__72c646h614w id__okd355w9ej" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>One interesting point though (mentioned in the GH issue) is that you can now have multiple processes all using the same model, and they&#39;ll share the memory -- pretty cool!</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726637270892544" itemprop="identifier"/><meta content="16" itemprop="position"/><meta content="3" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:05.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:05.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726637270892544" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726634687184896" itemprop="isPartOf"/><article aria-labelledby="id__i4ahy9r54g id__avyx4x6tc9f id__lyqy32s3yto id__hhaz9v9le6q id__og3z744mq0m id__gt2aqbyscft id__wr99n6w4pvb id__d5ulaqgz5j8 id__ay723qv00r id__qx1eymso85a id__f2t2eiypwe id__5m7abwd5j6d id__rbqzm1yx2i id__b3i9znz4r34 id__7ul6n9vsb3q id__u5q972apvy id__shq3hhpclrl id__rk6y2917k4 id__h06lvykvbm" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Note however that none of this helps us use the GPU better -- it&#39;s just for CPU use (or systems with integrated memory like Apple silicon).

By far the best speed is with an NVIDIA GPU, and that still requires copying all the params over to the card before using the model.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726639795843073" itemprop="identifier"/><meta content="17" itemprop="position"/><meta content="7" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:05.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:05.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726639795843073" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726637270892544" itemprop="isPartOf"/><article aria-labelledby="id__snwx08s3ahg id__g9a8u6rvvvb id__q2zpv4bkdq id__pzowvx5kz5h id__6uk4tovv0q6 id__7qqj8jpnpo4 id__oazfzqhtga id__ucgcufsli5k id__6n5znizs1ib id__jjlhb1ozdcf id__oznd8b3cbka id__7q3ojai98u9 id__z437r7ndkq id__sgf7ww3ezr id__t34lkby7ei id__i1s2amyl6ci id__4co1rws0p5o id__uiwk9txdf3 id__xyebgc1ovs" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Hopefully you found that a useful overview of mmap and transformers. BTW these topics are covered in depth in our upcoming course (the above code is copied from it) which will be released in the next few days, so keep an eye out for that!</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642726816338284544" itemprop="identifier"/><meta content="18" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:12:48.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:12:48.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642726816338284544" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726639795843073" itemprop="isPartOf"/><article aria-labelledby="id__tzgkyjv3o5h id__r6hvvttjdu id__mwqcjlis4q id__7ejp4m3j2x id__9pa7oqs3qd id__mwn5e8el5y id__k4l6o6mq8ti id__bp2n8oxt98v id__gz7qtnqsmw id__zhei73xwiz id__v2eh4u0ysje id__w4p6pb80tqj id__9k0tytxd4ul id__q9klzirij1 id__4r83j2g847q id__fj9fxzpt03f id__1usah470ckf id__cj4dyw67brp id__xajcvpnpjl" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><div dir="auto" lang="en" id="id__v2eh4u0ysje" data-testid="tweetText"><p><span>h/t </span></p><p><span> for encouraging me to write this thread.</span></p></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642728014051168256" itemprop="identifier"/><meta content="19" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:17:33.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:17:33.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642728014051168256" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642726816338284544" itemprop="isPartOf"/><meta content="https://twitter.com/jeremyphoward/status/1642726620082606080" itemprop="isBasedOn"/><article aria-labelledby="id__nmudafd9xuk id__4eqge14hyzl id__xa3762txxro id__iz2td2j4y2g id__10v3pywkc7f id__eppi19n89z id__x12ahfblqmo id__ql4nf1pky0n id__872tf3xchys id__6f722elaneu id__znc3tb64za id__gz431rhfagj id__uw83ybqfnm id__qjxxnoe5ph id__kk0hnfcbkc id__ouo3t2oazk id__ygshyhsrt6k id__8wcm2uidf8p id__uc4rz9lj16c" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Minor correction: the normalization layer I used here, Batchnorm2D, isn&#39;t what&#39;s used in a normal language model. I took the code from an image model we built for the course. In most language models we&#39;d use Layernorm for normalization.</span></p></div></div><div aria-labelledby="id__7zpyurd0v9v id__btuwu1wbth8" id="id__uw83ybqfnm"><div id="id__7zpyurd0v9v"><p><span>Quote Tweet</span></p><div tabindex="0"><div><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div><div><p><span>Attention is the operation shown in this code snippet. This one does &#34;self attention&#34; (i.e q, k, and v are all applied to the same input); there&#39;s also &#34;cross attention&#34; that uses different inputs.</span></p><p><span>Show this thread</span></p></div><div><div><div><div><div><div><div><div><div aria-label="Image" data-testid="tweetPhoto"><p><img alt="Image" draggable="true" src="https://pbs.twimg.com/media/FswfxD8aQAA4qoO?format=jpg&amp;name=medium"/></p></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642734538081312768" itemprop="identifier"/><meta content="20" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:43:29.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:43:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642734538081312768" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642728014051168256" itemprop="isPartOf"/><meta content="https://twitter.com/rlacombe/status/1642728470995140608" itemprop="isBasedOn"/><article aria-labelledby="id__95ly1m1xrk id__nusrgjxdkf id__27ln5qxaogc id__nfy89n9qrni id__7iw36t2m7ke id__14o14hy1ng3 id__1q0x9157tz1 id__p6z1hyh25vd id__rfv6bk57xtm id__rc6pg2oeqi id__u11ces98v1b id__ax0qhdokvu9 id__bajni81ppk4 id__vzakv5tv6c id__k6b5quj3hzs id__4oai9dka506 id__0b8e5062sgrp id__7gijkd7dr2w id__kmk2hp2zoc" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>D&#39;oh I meant to mention this and forgot!

This is a great point. The first layer of a language model is an &#34;embedding&#34; layer. Mathematically, it&#39;s matrix multiply by a bunch of one-hot encoded vectors. But it&#39;s implemented as an array index lookup.</span></p></div></div><div aria-labelledby="id__92ocprhyul id__fxrmqyqow3t" id="id__bajni81ppk4"><div id="id__92ocprhyul"><p><span>Quote Tweet</span></p><div tabindex="0"><div><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div data-testid="UserAvatar-Container-rlacombe"><div><div><div><div><a href="https://raphlinus.github.io/rlacombe" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1628925690237095937/p8MjHgFg_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div><div><p>Replying to <span dir="ltr"><span>@jeremyphoward</span></span> <span>and</span> <span dir="ltr"><span>@iScienceLuvr</span></span></p><p><span>What about embeddings though? Presumably each sequence uses a tiny subset of the vocabulary, so we likely donâ€™t load most embedding matrix parameters?</span></p></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642734540593692673" itemprop="identifier"/><meta content="21" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:43:29.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:43:29.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642734540593692673" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642734538081312768" itemprop="isPartOf"/><article aria-labelledby="id__5cwp3ktl6zj id__acsuuiuj0ld id__orspf5hsc7k id__nhpwt2yte7q id__y3hf067prt id__bngz9ifa8ki id__p5jmhkuhjgc id__3vguj1v4jo9 id__3f7ootbmun5 id__sf6qcmrn18a id__xonicarcme id__zwmc60uvwha id__ky8oppgh4 id__7thqhrpx7j5 id__7ps6kxvuj4u id__j6h5ebxv6ti id__n2vvauqw6gm id__943luek9jj id__654czo6qxdg" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Depending on how it&#39;s implemented, it&#39;s possible that this could avoid reading some of the embedding weights into memory. However, the OS has a minimum amount of stuff it reads at once around a location, so it might still end up reading most or all of the embeddings.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642734542833475584" itemprop="identifier"/><meta content="22" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:43:30.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:43:30.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642734542833475584" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642734540593692673" itemprop="isPartOf"/><article aria-labelledby="id__ybxvdr5w6eh id__2is849gizy5 id__4bsvpnd16ky id__62wpu4my5eb id__9nf0batib9i id__f0bu2l7w6fo id__3uj1re1454f id__mll67tlit9g id__fvygya8hpkh id__f8x2h6ph5zq id__c982k9s52ua id__xsidgn5xqnr id__zffcfjm035s id__jikct1z6x8m id__fes14pdh5pa id__78twub34jns id__notmhxjl8t id__udovbtoc72f id__97xm0euy5iu" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Having said that, the embedding weights are a fairly small proportion of the model parameters, so this won&#39;t generally make a big difference either way.</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642735748620197889" itemprop="identifier"/><meta content="23" itemprop="position"/><meta content="2" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:48:17.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:48:17.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642735748620197889" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642734542833475584" itemprop="isPartOf"/><meta content="https://twitter.com/igorcosta/status/1642734042545291271" itemprop="isBasedOn"/><article aria-labelledby="id__bktg8tjmeg9 id__nmvhmx02czi id__dxjkzgavx3t id__k5b1o7pewp id__qgzjb042lbl id__86sbz7tquu id__uy1vuqcopna id__fvvctpj1zb5 id__t0cr496f29c id__k83tl2kfmbp id__h1hxav4c15n id__jogg3xm4woq id__cpz6xqej0bq id__iw15t3omz7g id__8esqdrq2vs id__41iqu6gql0p id__189ttn38th5 id__027rscgxajo id__pvwdy3d2tz" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Oops another correction! I forgot to mention positional encoding. This is the layer after the embeddings. It can be implemented using sin/cos across a few freqs (&#34;sinusoidal embeddings&#34;) or the code snippet below (&#34;learnable embeddings&#34;). 
</span></p></div></div><div aria-labelledby="id__vya4s47wgod id__pv3u6yyt1d" id="id__cpz6xqej0bq"><div><div><div><div><div><div role="button" tabindex="0"><div><div><div aria-label="Image" data-testid="tweetPhoto"><p><img alt="Image" draggable="true" src="https://pbs.twimg.com/media/Fswq23oagAAy0rH?format=jpg&amp;name=medium"/></p></div></div></div></div></div></div></div></div></div><div id="id__vya4s47wgod"><p><span>Quote Tweet</span></p><div tabindex="0"><div><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div data-testid="UserAvatar-Container-igorcosta"><div><div><div><div><a href="https://raphlinus.github.io/igorcosta" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1445989849396252682/-OZu2XPU_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div><div><div><div><p>Replying to <span dir="ltr"><span>@jeremyphoward</span></span></p><p><span>Nice thread </span><span><span dir="ltr">@jeremyphoward</span></span><span>. Also, point to note, is positional encoding since it&#39;s crucial in providing information about the position of tokens in the input sequence since transformers do not have any inherent understanding of the sequence order.</span></p></div></div></div></div></div></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642735752202297345" itemprop="identifier"/><meta content="24" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T03:48:18.000Z" itemprop="dateCreated"/><meta content="2023-04-03T03:48:18.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642735752202297345" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642735748620197889" itemprop="isPartOf"/><article aria-labelledby="id__crb6h1x1cv4 id__hc30gbnd4v id__ypqr30om5z id__54mdq264h9v id__4n5epa1txs id__fb5ibvxsjfn id__thd5ui92kd id__cngvwd8mxuv id__wibccxhxv8f id__ew60dxa03jf id__vd98nzr6ztk id__70jpikv9wcv id__0mv7uce7gpd id__fbz8548npc id__z1mhfgvsnqt id__6h27dvijmt3 id__w4zxrku5db8 id__0u7whimx3un id__7piavtvjx12" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><p><span>Positional encoding is used to assign a unique vector to each position in the input token vector, so that the model can learn that the location of a words matters -- not just its presence/absence.

(They have few parameters do don&#39;t impact memory significantly.)</span></p></div></div></div></div></div></div></article></div></div><div><div itemprop="hasPart" itemscope="" itemtype="https://schema.org/SocialMediaPosting"><meta content="1642746913895354368" itemprop="identifier"/><meta content="25" itemprop="position"/><meta content="1" itemprop="commentCount"/><meta content="" itemprop="contentRating"/><meta content="2023-04-03T04:32:39.000Z" itemprop="dateCreated"/><meta content="2023-04-03T04:32:39.000Z" itemprop="datePublished"/><meta content="https://twitter.com/jeremyphoward/status/1642746913895354368" itemprop="url"/><meta content="https://twitter.com/jeremyphoward/status/1642735752202297345" itemprop="isPartOf"/><meta content="https://twitter.com/amasad/status/1642381582572093441" itemprop="isBasedOn"/><article aria-labelledby="id__xdhbvd98wko id__bcmhtw3a7f9 id__xxove21x5 id__2on676o8vzr id__2c774uf50ur id__k95qp8ds0ke id__zicmlcuptm id__yk53psct2x8 id__obljpuuyg7 id__1w7ygic4ax4 id__hsw1s04qqtv id__2zmst3xoyqb id__ptfg5m89s9a id__z51lmovhgw8 id__tt662110deh id__n8iud7kqs6c id__f86gmub3zpi id__rj8sig7d9ej id__9tb9jbgsk1a" role="article" tabindex="-1" data-testid="tweet"><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div><div data-testid="UserAvatar-Container-jeremyphoward"><div><div><div><div><a href="https://raphlinus.github.io/jeremyphoward" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div><div><div itemprop="articleBody" data-testid="tweetTextAnnotations"><div><div dir="auto" lang="en" id="id__hsw1s04qqtv" data-testid="tweetText"><p><span>Heh turns out </span></p><p><span> has already weighed in with his thoughts! :D</span></p></div></div></div><div aria-labelledby="id__rlrpvrffs5p id__ll3ym5z0cn" id="id__ptfg5m89s9a"><div id="id__rlrpvrffs5p"><p><span>Quote Tweet</span></p><div tabindex="0"><div><div><div><div><div><div data-testid="Tweet-User-Avatar"><div><div data-testid="UserAvatar-Container-amasad"><div><div><div><div><a href="https://raphlinus.github.io/amasad" aria-hidden="true" role="link" tabindex="-1"><div><div><div><div aria-label=""><p><img alt="" draggable="true" src="https://pbs.twimg.com/profile_images/1518391475687493633/Bb6zQKr8_400x400.jpg"/></p></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div></div><div><p><span>For a day programmers thought mmap possessed magical compression abilities, loading the 20GB LLaMa weights into 6 GB of RAM. Turns out it&#39;s an accounting bug. This must be the first instance of collective hacker magical thinking.</span></p><p><span>Show this thread</span></p></div></div></div></div></div></div></div></div></div></article></div></div></div></div></section></div></div></div></div></div></div></div></div>
  </body>
</html>
