<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/homebrewltd/ichigo">Original</a>
    <h1>Ichigo: Local real-time voice AI</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">

<p dir="auto"><a href="https://homebrew.ltd/blog/llama3-just-got-ears" rel="nofollow"><img src="https://camo.githubusercontent.com/48964d2402a8321c666c6d9eb0295cecc411e80164ac9d1c9d503dc4fd51a1af/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50726f6a6563742d426c6f672d477265656e" data-canonical-src="https://img.shields.io/badge/Project-Blog-Green"/></a>
<a href="https://demo.homebrew.ltd/" rel="nofollow"><img src="https://camo.githubusercontent.com/077a46a77537465f89791c312e298ab686aa499d8560122566402be093330727/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50726f6a6563742d44656d6f2d76696f6c6574" data-canonical-src="https://img.shields.io/badge/Project-Demo-violet"/></a>
<a href="https://huggingface.co/homebrewltd" rel="nofollow"><img src="https://camo.githubusercontent.com/f13663d6c08b11b860678d7250d85b8bf65f6e5d8a6d3a7848869d6d212a5b1e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d4d6f64656c732d626c7565" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue"/></a>
<a href="https://huggingface.co/homebrewltd" rel="nofollow"><img src="https://camo.githubusercontent.com/a12502be4a0a419f31d6f7c2cab553e567d41337e5be8e6007bcc56bba82656a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d446174612d677265656e" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Data-green"/></a></p>
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/homebrewltd/ichigo/blob/main/images/ichigov0.2.jpeg"><img src="https://github.com/homebrewltd/ichigo/raw/main/images/ichigov0.2.jpeg" width="400"/></a></p><p dir="auto">Homebrewed early-fusion speech model</p>
</div>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p dir="auto">Update: September 30, 2024</p>
<ul dir="auto">
<li>We have rebranded from llama3-s to üçì Ichigo.</li>
<li>Our custom-built early-fusion speech model now has a name and a voice.</li>
<li>It has improved multiturn capabilities and can now refuse to process inaudible queries.</li>
</ul>
</div>
<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z"></path></svg>Warning</p><p dir="auto">üçì Ichigo is an open research experiment</p>
<ul dir="auto">
<li>Join us in the  <code>#research</code> channel in <a href="https://discord.com/invite/FTk2MvZwJH" rel="nofollow">Homebrew&#39;s Discord</a></li>
<li>We livestream training runs in <code>#research-livestream</code></li>
</ul>
</div>

<p dir="auto">üçì Ichigo is an open, ongoing research experiment to extend a text-based LLM to have native &#34;listening&#34; ability. Think of it as an open data, open weight, on device Siri.</p>
<p dir="auto">It uses an <a href="https://medium.com/@raj.pulapakura/multimodal-models-and-fusion-a-complete-guide-225ca91f6861#:~:text=3.3.,-Early%20Fusion&amp;text=Early%20fusion%20refers%20to%20combining,fused%20representation%20through%20the%20model." rel="nofollow">early fusion</a> technique inspired by <a href="https://arxiv.org/abs/2405.09818" rel="nofollow">Meta&#39;s Chameleon paper</a>.</p>
<p dir="auto">We <del>build</del> train in public:</p>
<ul dir="auto">
<li><a href="https://homebrew.ltd/blog/llama-learns-to-talk" rel="nofollow">Ichigo v0.3 Checkpoint Writeup</a></li>
<li><a href="https://homebrew.ltd/blog/llama3-just-got-ears" rel="nofollow">Ichigo v0.2 Checkpoint Writeup</a></li>
<li><a href="https://homebrew.ltd/blog/can-llama-3-listen" rel="nofollow">Ichigo v0.1 Checkpoint Writeup</a></li>
</ul>

<ul dir="auto">
<li>4 Oct: <a href="https://huggingface.co/collections/homebrewltd/ichigo-66ffc7484ef31ec5596ef6d0" rel="nofollow">Ichigo v0.3</a> models are now available. Utilizing cleaner and improved data, our model has achieved an enhanced MMLU score of 63.79 and demonstrates stronger speech instruction-following capabilities, even in multi-turn interactions. Additionally, by incorporating noise-synthetic data, we have successfully trained the model to refuse processing non-speech audio inputs from users, further improving its functionality and user experience.</li>
<li>23 Aug: We‚Äôre excited to share <a href="https://huggingface.co/homebrewltd/llama3.1-s-instruct-v0.2" rel="nofollow">Ichigo-llama3.1-s-instruct-v0.2</a>, our latest multimodal checkpoint with improved speech understanding by enhancing the model&#39;s audio instruction-following capabilities through training on interleaving synthetic data.</li>
<li>17 Aug: We pre-trained our LLaMA 3.1 model on continuous speech data, tokenized using WhisperSpeechVQ. The final loss converged to approximately 1.9, resulting in our checkpoint: <a href="https://huggingface.co/homebrewltd/llama3.1-s-base-v0.2" rel="nofollow">Ichigo-llama3.1-s-base-v0.2</a></li>
<li>1 Aug: Identified typo in original training recipe, causing significant degradation (MMLU: 0.6 -&gt; 0.2), proposed fixes.</li>
<li>30 July: Presented llama3-s progress at: <a href="https://lu.ma/ws8t6wom?tk=wZvFmm" rel="nofollow">AI Training: From PyTorch to GPU Clusters</a></li>
<li>19 July: <a href="https://huggingface.co/homebrewltd/llama3-s-2024-07-19" rel="nofollow">llama3-s-2024-07-19</a> understands synthetic voice with limited results</li>
<li>1 July: <a href="https://huggingface.co/homebrewltd/llama3-s-2024-07-08" rel="nofollow">llama3-s-2024-07-08</a> showed converging loss (1.7) with limited data</li>
</ul>

<p dir="auto">üçì Ichigo is an open research project. We&#39;re looking for collaborators, and will likely move towards crowdsourcing speech datasets in the future.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Quickstart with Google Colab</h3><a id="user-content-quickstart-with-google-colab" aria-label="Permalink: Quickstart with Google Colab" href="#quickstart-with-google-colab"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Checkout this notebook to try our latest model:</p>
<p dir="auto"><a href="https://colab.research.google.com/drive/18IiwN0AzBZaox5o0iidXqWD1xKq11XbZ?usp=sharing" rel="nofollow"><img src="https://camo.githubusercontent.com/96889048f8a9014fdeba2a891f97150c6aac6e723f5190236b10215a97ed41f3/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Open In Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>

<p dir="auto">For detailed information on synthetic generation, please refer to the <a href="https://github.com/homebrewltd/ichigo/blob/main/synthetic_data/README.md">Synthetic Generation Guide</a>.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Organize the input/output directory</h3><a id="user-content-organize-the-inputoutput-directory" aria-label="Permalink: Organize the input/output directory" href="#organize-the-inputoutput-directory"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ol dir="auto">
<li>First Clone the Repo from github:</li>
</ol>
<div data-snippet-clipboard-copy-content="git clone --recurse-submodules https://github.com/homebrewltd/llama3-s.git"><pre><code>git clone --recurse-submodules https://github.com/homebrewltd/llama3-s.git
</code></pre></div>
<ol start="2" dir="auto">
<li>The folder structure is as follows:</li>
</ol>
<div data-snippet-clipboard-copy-content="Ichigo
‚îú‚îÄ‚îÄ HF_Trainer                               # HF training code (deprecated)
‚îú‚îÄ‚îÄ synthetic_data                           # Synthetic data generation pipeline
    ‚îú‚îÄ‚îÄ configs                              # Audio pipeline configs
        ‚îú‚îÄ‚îÄ audio_to_audio                   # Parler audio (.wav) to semantic tokens
        ‚îú‚îÄ‚îÄ synthetic_generation_config      # TTS semantic tokens
‚îú‚îÄ‚îÄ scripts                                  # Setup scripts for Runpod
‚îú‚îÄ‚îÄ torchtune                                # Submodule: our fork of fsdp with checkpointing
‚îú‚îÄ‚îÄ model_zoo                                # Model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Meta-Llama-3-8B-Instruct
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Meta-Llama-3-70B-Instruct
‚îú‚îÄ‚îÄ demo                                     # Selfhost this demo (vllm)
‚îú‚îÄ‚îÄ inference                                # Google Colab"><pre><code>Ichigo
‚îú‚îÄ‚îÄ HF_Trainer                               # HF training code (deprecated)
‚îú‚îÄ‚îÄ synthetic_data                           # Synthetic data generation pipeline
    ‚îú‚îÄ‚îÄ configs                              # Audio pipeline configs
        ‚îú‚îÄ‚îÄ audio_to_audio                   # Parler audio (.wav) to semantic tokens
        ‚îú‚îÄ‚îÄ synthetic_generation_config      # TTS semantic tokens
‚îú‚îÄ‚îÄ scripts                                  # Setup scripts for Runpod
‚îú‚îÄ‚îÄ torchtune                                # Submodule: our fork of fsdp with checkpointing
‚îú‚îÄ‚îÄ model_zoo                                # Model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Meta-Llama-3-8B-Instruct
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Meta-Llama-3-70B-Instruct
‚îú‚îÄ‚îÄ demo                                     # Selfhost this demo (vllm)
‚îú‚îÄ‚îÄ inference                                # Google Colab
</code></pre></div>

<ol dir="auto">
<li>Install Dependencies</li>
</ol>
<div data-snippet-clipboard-copy-content="python -m venv hf_trainer
chmod +x scripts/install.sh
./scripts/install.sh"><pre><code>python -m venv hf_trainer
chmod +x scripts/install.sh
./scripts/install.sh
</code></pre></div>
<p dir="auto">Restart shell now</p>
<div data-snippet-clipboard-copy-content="chmod +x scripts/setup.sh
./scripts/setup.sh
source myenv/bin/activate"><pre><code>chmod +x scripts/setup.sh
./scripts/setup.sh
source myenv/bin/activate
</code></pre></div>
<ol start="2" dir="auto">
<li>Logging Huggingface</li>
</ol>
<div data-snippet-clipboard-copy-content="huggingface-cli login --token=&lt;token&gt;"><pre><code>huggingface-cli login --token=&lt;token&gt;
</code></pre></div>
<ol start="3" dir="auto">
<li>Training</li>
</ol>
<div data-snippet-clipboard-copy-content="export CUTLASS_PATH=&#34;cutlass&#34;
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
accelerate launch --config_file ./accelerate_config.yaml train.py "><pre><code>export CUTLASS_PATH=&#34;cutlass&#34;
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
accelerate launch --config_file ./accelerate_config.yaml train.py 
</code></pre></div>

<ol dir="auto">
<li>Install Package</li>
</ol>
<div data-snippet-clipboard-copy-content="python -m venv torchtune
pip install torch torchvision tensorboard
cd ./torchtune
pip install -e ."><pre><code>python -m venv torchtune
pip install torch torchvision tensorboard
cd ./torchtune
pip install -e .
</code></pre></div>
<p dir="auto">You can also download the model using tune:</p>
<div data-snippet-clipboard-copy-content="tune download homebrewltd/llama3.1-s-whispervq-init --hf-token &lt;token&gt;  --output-dir ../model_zoo/llama3.1-s-whispervq-init --ignore-patterns &#34;original/consolidated*&#34;"><pre><code>tune download homebrewltd/llama3.1-s-whispervq-init --hf-token &lt;token&gt;  --output-dir ../model_zoo/llama3.1-s-whispervq-init --ignore-patterns &#34;original/consolidated*&#34;
</code></pre></div>
<p dir="auto">Setup the Dataset from HF path by change the path and change the name of the model in the following YAML file.</p>
<div data-snippet-clipboard-copy-content="nano torchtune/recipes/configs/jan-llama3-s/8B_full.yaml"><pre><code>nano torchtune/recipes/configs/jan-llama3-s/8B_full.yaml
</code></pre></div>
<ol start="2" dir="auto">
<li>Training Multi GPU (1-8GPUs Supported)</li>
</ol>
<div data-snippet-clipboard-copy-content="tune run --nproc_per_node 4 full_finetune_fsdp2 --config recipes/configs/jan-llama3-1-s/8B_full.yaml"><pre><code>tune run --nproc_per_node 4 full_finetune_fsdp2 --config recipes/configs/jan-llama3-1-s/8B_full.yaml
</code></pre></div>


<p dir="auto">For instructions on how to self-host the Ichigo web UI demo using Docker, please visit: <a href="https://github.com/homebrewltd/ichigo-demo/tree/docker">Ichigo demo</a>. To try our demo on a single RTX 4090 GPU, you can go directly to: <a href="https://ichigo.homebrew.ltd" rel="nofollow">https://ichigo.homebrew.ltd</a>.</p>

<p dir="auto">We offer code for users to create a web UI demo. Please follow the instructions below:</p>
<div data-snippet-clipboard-copy-content="python -m venv demo
source demo/bin/activate
# First install all required packages
pip install --no-cache-dir -r ./demo/requirements.txt"><pre><code>python -m venv demo
source demo/bin/activate
# First install all required packages
pip install --no-cache-dir -r ./demo/requirements.txt
</code></pre></div>
<p dir="auto">Then run the command below to launch a Gradio demo locally. You can add the variables <code>use-4bit</code> and <code>use-8bit</code> for quantized usage:</p>
<div data-snippet-clipboard-copy-content="python -m demo.app --host 0.0.0.0 --port 7860 --max-seq-len 1024 "><pre><code>python -m demo.app --host 0.0.0.0 --port 7860 --max-seq-len 1024 
</code></pre></div>
<p dir="auto">You can also host a demo using vLLM for faster inference but its not support streaming output:</p>

<p dir="auto"><strong>Alternatively, you can easily try our demo on <a href="https://huggingface.co/spaces/jan-hq/Llama3.1-s-v0.2" rel="nofollow">HuggingFace</a> ü§ó</strong></p>

<div dir="auto" data-snippet-clipboard-copy-content="@misc{chameleonteam2024chameleonmixedmodalearlyfusionfoundation,
      title={Chameleon: Mixed-Modal Early-Fusion Foundation Models}, 
      author={Chameleon Team},
      year={2024},
      eprint={2405.09818},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      journal={arXiv preprint}
}

@misc{zhang2024adamminiusefewerlearning,
      title={Adam-mini: Use Fewer Learning Rates To Gain More}, 
      author={Yushun Zhang and Congliang Chen and Ziniu Li and Tian Ding and Chenwei Wu and Yinyu Ye and Zhi-Quan Luo and Ruoyu Sun},
      year={2024},
      eprint={2406.16793},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      journal={arXiv preprint}
}

@misc{defossez2022highfi,
      title={High Fidelity Neural Audio Compression},
      author={D√©fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},
      year={2022},
      eprint={2210.13438},
      archivePrefix={arXiv},
      journal={arXiv preprint}
}

@misc{WhisperSpeech,
      title={WhisperSpeech: An Open Source Text-to-Speech System Built by Inverting Whisper}, 
      author={Collabora and LAION},
      year={2024},
      url={https://github.com/collabora/WhisperSpeech},
      note={GitHub repository}
}"><pre><span>@misc</span>{<span>chameleonteam2024chameleonmixedmodalearlyfusionfoundation</span>,
      <span>title</span>=<span><span>{</span>Chameleon: Mixed-Modal Early-Fusion Foundation Models<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Chameleon Team<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2405.09818<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CL<span>}</span></span>,
      <span>journal</span>=<span><span>{</span>arXiv preprint<span>}</span></span>
}

<span>@misc</span>{<span>zhang2024adamminiusefewerlearning</span>,
      <span>title</span>=<span><span>{</span>Adam-mini: Use Fewer Learning Rates To Gain More<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Yushun Zhang and Congliang Chen and Ziniu Li and Tian Ding and Chenwei Wu and Yinyu Ye and Zhi-Quan Luo and Ruoyu Sun<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2406.16793<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.LG<span>}</span></span>,
      <span>journal</span>=<span><span>{</span>arXiv preprint<span>}</span></span>
}

<span>@misc</span>{<span>defossez2022highfi</span>,
      <span>title</span>=<span><span>{</span>High Fidelity Neural Audio Compression<span>}</span></span>,
      <span>author</span>=<span><span>{</span>D√©fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2022<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2210.13438<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>journal</span>=<span><span>{</span>arXiv preprint<span>}</span></span>
}

<span>@misc</span>{<span>WhisperSpeech</span>,
      <span>title</span>=<span><span>{</span>WhisperSpeech: An Open Source Text-to-Speech System Built by Inverting Whisper<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Collabora and LAION<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>url</span>=<span><span>{</span>https://github.com/collabora/WhisperSpeech<span>}</span></span>,
      <span>note</span>=<span><span>{</span>GitHub repository<span>}</span></span>
}</pre></div>

<ul dir="auto">
<li><a href="https://github.com/pytorch/torchtune">Torchtune</a>: The codebase we built upon</li>
<li><a href="https://github.com/huggingface/accelerate">Accelerate</a>: Library for easy use of distributed training</li>
<li><a href="https://github.com/collabora/WhisperSpeech">WhisperSpeech</a>: Text-to-speech model for synthetic audio generation</li>
<li><a href="https://github.com/facebookresearch/encodec">Encodec</a>: High-fidelity neural audio codec for efficient audio compression</li>
<li><a href="https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6" rel="nofollow">Llama3</a>: the Family of Models that we based on that has the amazing language capabilities !!!</li>
</ul>
</article></div></div>
  </body>
</html>
