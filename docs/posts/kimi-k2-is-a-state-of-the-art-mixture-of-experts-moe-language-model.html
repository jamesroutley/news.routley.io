<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/MoonshotAI/Kimi-K2">Original</a>
    <h1>Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
      <img src="https://github.com/MoonshotAI/Kimi-K2/raw/main/figures/kimi-logo.png" width="30%" alt="Kimi K2: Open Agentic Intelligence"/>
  </picture></themed-picture>
</div>
<hr/>
<p><a href="https://www.kimi.com" rel="nofollow"><img alt="Chat" src="https://camo.githubusercontent.com/0f9611ba3f06f6384e6697a1ff058dbfa409ae34be8d16f2f081373e4264db5d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09fa496253230436861742d4b696d692532304b322d6666366236623f636f6c6f723d313738336666266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/ðŸ¤–%20Chat-Kimi%20K2-ff6b6b?color=1783ff&amp;logoColor=white"/></a>
  <a href="https://www.moonshot.ai" rel="nofollow"><img alt="Homepage" src="https://camo.githubusercontent.com/363748881dfa50ee8685f8102f4a76f9243539794b98ac0b712335db30a19acb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f486f6d65706167652d4d6f6f6e73686f7425323041492d77686974653f6c6f676f3d4b696d69266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Homepage-Moonshot%20AI-white?logo=Kimi&amp;logoColor=white"/></a>
</p>
<p><a href="https://huggingface.co/moonshotai" rel="nofollow"><img alt="Hugging Face" src="https://camo.githubusercontent.com/63d4f12d5898e6165263bbac2a1fa514c3feca3bdbbb242e55f16b36e25b13e4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d4d6f6f6e73686f7425323041492d6666633130373f636f6c6f723d666663313037266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Moonshot%20AI-ffc107?color=ffc107&amp;logoColor=white"/></a>
  <a href="https://twitter.com/kimi_moonshot" rel="nofollow"><img alt="Twitter Follow" src="https://camo.githubusercontent.com/ad7c3bc00829407e645fcd147b224e5fb918d26c392178fdae654c0c4960552e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722d4b696d692e61692d77686974653f6c6f676f3d78266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Twitter-Kimi.ai-white?logo=x&amp;logoColor=white"/></a>
    <a href="https://discord.gg/TYU2fdJykW" rel="nofollow"><img alt="Discord" src="https://camo.githubusercontent.com/6412b1a7b2e950146a80e6fb1cdb33dd44c303a1b06ff41f27f2b43c939e33c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d4b696d692e61692d77686974653f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" data-canonical-src="https://img.shields.io/badge/Discord-Kimi.ai-white?logo=discord&amp;logoColor=white"/></a>
</p>
<p><a href="https://github.com/moonshotai/Kimi-K2/blob/main/LICENSE"><img alt="License" src="https://camo.githubusercontent.com/7723033f85cd3b5d3cc89d760c5280697bb7007f725e3101b462cae892632b82/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d6f6469666965645f4d49542d6635646535333f26636f6c6f723d663564653533" data-canonical-src="https://img.shields.io/badge/License-Modified_MIT-f5de53?&amp;color=f5de53"/></a>
</p>
<p dir="auto">
<b>ðŸ“°Â Â <a href="https://moonshotai.github.io/Kimi-K2/" rel="nofollow">Tech Blog</a></b> Â Â Â  | Â Â Â  <b>ðŸ“„Â Â Paper Link (coming soon)</b>
</p>

<p dir="auto">Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.</p>

<ul dir="auto">
<li>Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability.</li>
<li>MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up.</li>
<li>Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving.</li>
</ul>

<ul dir="auto">
<li><strong>Kimi-K2-Base</strong>: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.</li>
<li><strong>Kimi-K2-Instruct</strong>: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.</li>
</ul>
<div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
      <img src="https://github.com/MoonshotAI/Kimi-K2/raw/main/figures/banner.png" width="80%" alt="Evaluation Results"/>
  </picture></themed-picture>
</div>

<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Architecture</strong></td>
<td>Mixture-of-Experts (MoE)</td>
</tr>
<tr>
<td><strong>Total Parameters</strong></td>
<td>1T</td>
</tr>
<tr>
<td><strong>Activated Parameters</strong></td>
<td>32B</td>
</tr>
<tr>
<td><strong>Number of Layers</strong> (Dense layer included)</td>
<td>61</td>
</tr>
<tr>
<td><strong>Number of Dense Layers</strong></td>
<td>1</td>
</tr>
<tr>
<td><strong>Attention Hidden Dimension</strong></td>
<td>7168</td>
</tr>
<tr>
<td><strong>MoE Hidden Dimension</strong> (per Expert)</td>
<td>2048</td>
</tr>
<tr>
<td><strong>Number of Attention Heads</strong></td>
<td>64</td>
</tr>
<tr>
<td><strong>Number of Experts</strong></td>
<td>384</td>
</tr>
<tr>
<td><strong>Selected Experts per Token</strong></td>
<td>8</td>
</tr>
<tr>
<td><strong>Number of Shared Experts</strong></td>
<td>1</td>
</tr>
<tr>
<td><strong>Vocabulary Size</strong></td>
<td>160K</td>
</tr>
<tr>
<td><strong>Context Length</strong></td>
<td>128K</td>
</tr>
<tr>
<td><strong>Attention Mechanism</strong></td>
<td>MLA</td>
</tr>
<tr>
<td><strong>Activation Function</strong></td>
<td>SwiGLU</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>

<div dir="auto"><h4 tabindex="-1" dir="auto">Instruction model evaluation results</h4><a id="user-content-instruction-model-evaluation-results" aria-label="Permalink: Instruction model evaluation results" href="#instruction-model-evaluation-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto">
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Benchmark</th>
<th>Metric</th>
<th><sup>Kimi K2 Instruct</sup></th>
<th><sup>DeepSeek-V3-0324</sup></th>
<th><sup>Qwen3-235B-A22B </sup></th>
<th><sup>Claude Sonnet 4 </sup></th>
<th><sup>Claude Opus 4 </sup></th>
<th><sup>GPT-4.1</sup></th>
<th><sup>Gemini 2.5 Flash </sup></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="9"><strong>Coding Tasks</strong></td>
</tr>
<tr>
<td>LiveCodeBench v6</td>
<td>Pass@1</td>
<td><strong>53.7</strong></td>
<td>46.9</td>
<td>37.0</td>
<td>48.5</td>
<td>47.4</td>
<td>44.7</td>
<td>44.7</td>
</tr>
<tr>
<td>OJBench</td>
<td>Pass@1</td>
<td><strong>27.1</strong></td>
<td>24.0</td>
<td>11.3</td>
<td>15.3</td>
<td>19.6</td>
<td>19.5</td>
<td>19.5</td>
</tr>
<tr>
<td>MultiPL-E</td>
<td>Pass@1</td>
<td><ins><strong>85.7</strong></ins></td>
<td>83.1</td>
<td>78.2</td>
<td>88.6</td>
<td><strong>89.6</strong></td>
<td>86.7</td>
<td>85.6</td>
</tr>
<tr>
<td>SWE-bench Verified </td>
<td>Single Patch w/o Test (Acc)</td>
<td><ins><strong>51.8</strong></ins></td>
<td>36.6</td>
<td>39.4</td>
<td>50.2</td>
<td><strong>53.0</strong></td>
<td>40.8</td>
<td>32.6</td>
</tr>
<tr>
<td rowspan="2">SWE-bench Verified </td>
<td>Single Attempt (Acc)</td>
<td><ins><strong>65.8</strong></ins></td>
<td>38.8</td>
<td>34.4</td>
<td><strong>72.7</strong><sup>*</sup></td>
<td>72.5<sup>*</sup></td>
<td>54.6</td>
<td>â€”</td>
</tr>
<tr>

<td>Multiple Attempts (Acc)</td>
<td><ins><strong>71.6</strong></ins></td>
<td>â€”</td>
<td>â€”</td>
<td><strong>80.2</strong></td>
<td>79.4<sup>*</sup></td>
<td>â€”</td>
<td>â€”</td>
</tr>
<tr>
<td>SWE-bench Multilingual</td>
<td>Single Attempt (Acc)</td>
<td><ins><strong>47.3</strong> </ins></td>
<td>25.8</td>
<td>20.9</td>
<td><strong>51.0</strong></td>
<td>â€”</td>
<td>31.5</td>
<td>â€”</td>
</tr>
<tr>
<td rowspan="2">TerminalBench</td>
<td>Inhouse Framework (Acc)</td>
<td><ins><strong>30.0</strong></ins></td>
<td>â€”</td>
<td>â€”</td>
<td>35.5</td>
<td><strong>43.2</strong></td>
<td>8.3</td>
<td>â€”</td>
</tr>
<tr>

<td>Terminus (Acc)</td>
<td><ins><strong>25.0</strong> </ins></td>
<td>16.3</td>
<td>6.6</td>
<td>â€”</td>
<td>â€”</td>
<td><strong>30.3</strong></td>
<td>16.8</td>
</tr>
<tr>
<td>Aider-Polyglot</td>
<td>Acc</td>
<td>60.0</td>
<td>55.1</td>
<td><ins><strong>61.8</strong></ins></td>
<td>56.4</td>
<td><strong>70.7</strong></td>
<td>52.4</td>
<td>44.0</td>
</tr>
<tr>
<td colspan="9"><strong>Tool Use Tasks</strong></td>
</tr>
<tr>
<td>Tau2 retail</td>
<td>Avg@4</td>
<td><ins><strong>70.6</strong></ins></td>
<td>69.1</td>
<td>57.0</td>
<td>75.0</td>
<td><strong>81.8</strong></td>
<td>74.8</td>
<td>64.3</td>
</tr>
<tr>
<td>Tau2 airline</td>
<td>Avg@4</td>
<td><ins><strong>56.5</strong></ins></td>
<td>39.0</td>
<td>26.5</td>
<td>55.5</td>
<td><strong>60.0</strong></td>
<td>54.5</td>
<td>42.5</td>
</tr>
<tr>
<td>Tau2 telecom</td>
<td>Avg@4</td>
<td><strong>65.8</strong></td>
<td>32.5</td>
<td>22.1</td>
<td>45.2</td>
<td>57.0</td>
<td>38.6</td>
<td>16.9</td>
</tr>
<tr>
<td>AceBench</td>
<td>Acc</td>
<td><ins><strong>76.5</strong></ins></td>
<td>72.7</td>
<td>70.5</td>
<td>76.2</td>
<td>75.6</td>
<td><strong>80.1</strong></td>
<td>74.5</td>
</tr>
<tr>
<td colspan="9"><strong>Math &amp; STEM Tasks</strong></td>
</tr>
<tr>
<td>AIME 2024</td>
<td>Avg@64</td>
<td><strong>69.6</strong></td>
<td>59.4<sup>*</sup></td>
<td>40.1<sup>*</sup></td>
<td>43.4</td>
<td>48.2</td>
<td>46.5</td>
<td>61.3</td>
</tr>
<tr>
<td>AIME 2025</td>
<td>Avg@64</td>
<td><strong>49.5</strong></td>
<td>46.7</td>
<td>24.7<sup>*</sup></td>
<td>33.1<sup>*</sup></td>
<td>33.9<sup>*</sup></td>
<td>37.0</td>
<td>46.6</td>
</tr>
<tr>
<td>MATH-500</td>
<td>Acc</td>
<td><strong>97.4</strong></td>
<td>94.0<sup>*</sup></td>
<td>91.2<sup>*</sup></td>
<td>94.0</td>
<td>94.4</td>
<td>92.4</td>
<td>95.4</td>
</tr>
<tr>
<td>HMMT 2025</td>
<td>Avg@32</td>
<td><strong>38.8</strong></td>
<td>27.5</td>
<td>11.9</td>
<td>15.9</td>
<td>15.9</td>
<td>19.4</td>
<td>34.7</td>
</tr>
<tr>
<td>CNMO 2024</td>
<td>Avg@16</td>
<td>74.3</td>
<td><ins><strong>74.7</strong></ins></td>
<td>48.6</td>
<td>60.4</td>
<td>57.6</td>
<td>56.6</td>
<td><strong>75.0</strong></td>
</tr>
<tr>
<td>PolyMath-en</td>
<td>Avg@4</td>
<td><strong>65.1</strong></td>
<td>59.5</td>
<td>51.9</td>
<td>52.8</td>
<td>49.8</td>
<td>54.0</td>
<td>49.9</td>
</tr>
<tr>
<td>ZebraLogic</td>
<td>Acc</td>
<td><strong>89.0</strong></td>
<td>84.0</td>
<td>37.7<sup>*</sup></td>
<td>73.7</td>
<td>59.3</td>
<td>58.5</td>
<td>57.9</td>
</tr>
<tr>
<td>AutoLogi</td>
<td>Acc</td>
<td><ins><strong>89.5</strong></ins></td>
<td>88.9</td>
<td>83.3</td>
<td><strong>89.8</strong></td>
<td>86.1</td>
<td>88.2</td>
<td>84.1</td>
</tr>
<tr>
<td>GPQA-Diamond</td>
<td>Avg@8</td>
<td><strong>75.1</strong></td>
<td>68.4<sup>*</sup></td>
<td>62.9<sup>*</sup></td>
<td>70.0<sup>*</sup></td>
<td>74.9<sup>*</sup></td>
<td>66.3</td>
<td>68.2</td>
</tr>
<tr>
<td>SuperGPQA</td>
<td>Acc</td>
<td><strong>57.2</strong></td>
<td>53.7</td>
<td>50.2</td>
<td>55.7</td>
<td>56.5</td>
<td>50.8</td>
<td>49.6</td>
</tr>
<tr>
<td>Humanity&#39;s Last Exam</td>
<td>-</td>
<td>4.7</td>
<td>5.2</td>
<td><ins><strong>5.7</strong></ins></td>
<td>5.8</td>
<td><strong>7.1</strong></td>
<td>3.7</td>
<td>5.6</td>
</tr>
<tr>
<td colspan="9"><strong>General Tasks</strong></td>
</tr>
<tr>
<td>MMLU</td>
<td>EM</td>
<td><ins><strong>89.5</strong></ins></td>
<td>89.4</td>
<td>87.0</td>
<td>91.5</td>
<td><strong>92.9</strong></td>
<td>90.4</td>
<td>90.1</td>
</tr>
<tr>
<td>MMLU-Redux</td>
<td>EM</td>
<td><ins><strong>92.7</strong></ins></td>
<td>90.5</td>
<td>89.2</td>
<td>93.6</td>
<td><strong>94.2</strong></td>
<td>92.4</td>
<td>90.6</td>
</tr>
<tr>
<td>MMLU-Pro</td>
<td>EM</td>
<td>81.1</td>
<td><ins><strong>81.2</strong></ins><sup>*</sup></td>
<td>77.3</td>
<td>83.7</td>
<td><strong>86.6</strong></td>
<td>81.8</td>
<td>79.4</td>
</tr>
<tr>
<td>IFEval</td>
<td>Prompt Strict</td>
<td><strong>89.8</strong></td>
<td>81.1</td>
<td>83.2<sup>*</sup></td>
<td>87.6</td>
<td>87.4</td>
<td>88.0</td>
<td>84.3</td>
</tr>
<tr>
<td>Multi-Challenge</td>
<td>Acc</td>
<td><strong>54.1</strong></td>
<td>31.4</td>
<td>34.0</td>
<td>46.8</td>
<td>49.0</td>
<td>36.4</td>
<td>39.5</td>
</tr>
<tr>
<td>SimpleQA</td>
<td>Correct</td>
<td><ins><strong>31.0</strong></ins></td>
<td>27.7</td>
<td>13.2</td>
<td>15.9</td>
<td>22.8</td>
<td><strong>42.3</strong></td>
<td>23.3</td>
</tr>
<tr>
<td>Livebench</td>
<td>Pass@1</td>
<td><strong>76.4</strong></td>
<td>72.4</td>
<td>67.6</td>
<td>74.8</td>
<td>74.6</td>
<td>69.8</td>
<td>67.8</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</div>
<sup>
â€¢ Bold denotes global SOTA, and underlined denotes open-source SOTA.
</sup></article></div></div>
  </body>
</html>
