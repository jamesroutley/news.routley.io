<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hardmath123.github.io/conways-gradient.html">Original</a>
    <h1>Conway&#39;s Gradient of Life</h1>
    
    <div id="readability-page-1" class="page"><article id="postcontent">
            <section>
                
                <center><em><p>Approximate Atavising with Differentiable Automata</p>
</em></center>
                <h4>Tuesday, May 5, 2020 · 3 min read</h4>
<p>And now, a magic trick. Before you is a 239-by-200 Conway’s Game of Life board:</p>
<p><img src="https://hardmath123.github.io/static/conways-gradient/conway-out.png" alt="a life config"/></p>
<p>What happens if we start from this configuration and take a single step of the
game? Here is a snapshot of the next generation:</p>
<p><img src="https://hardmath123.github.io/static/conways-gradient/conway-out-step.png" alt="a life config, stepped"/></p>
<p>Amazing! It’s a <a href="https://mancala.fandom.com/wiki/John_Horton_Conway">portrait</a>
of John Conway! (Squint!)</p>
<hr/>
<p>How does this trick work? (It’s not a hoax — you can try it yourself at
<a href="https://copy.sh/life/">copy.sh/life</a> using <a href="https://hardmath123.github.io/static/conways-gradient/conway.rle">this RLE
file</a>.)</p>
<p>Well, let’s start with how it <em>doesn’t</em> work. Reversing Life configurations
exactly — the art of “atavising” — is <a href="https://nbickford.wordpress.com/2012/04/15/reversing-the-game-of-life-for-fun-and-profit/">a hard search
problem</a>,
and doing it at this scale would be computationally infeasible. Imagine
searching through $ 2^{239\times200} $ possible configurations! Surely that
is hopeless… indeed, the talk I linked shares some clever algorithms that
nonetheless take a full 10 minutes to find a predecessor for a tiny 10-by-10
board.</p>
<p>But it turns out that <em>approximately</em> reversing a Life configuration is much
easier — instead of a tricky discrete search problem, we have an easy
<em>continuous optimization</em> problem for which we can use our favorite algorithm,
gradient descent.</p>
<p>Here is the idea: Start with a random board configuration $ b $ and compute
$ b^\prime $, the result after one step of Life. Now, for some target image
$ t $ compute the derivative $ \frac{\partial}{\partial b} \sum|b^\prime-t|
$, which tells us how to change $ b $ to make $ b^\prime $ closer to $ t
$. Then take a step of gradient descent, and rinse and repeat!</p>
<p>Okay, okay, I know what you’re thinking: <em>Life isn’t differentiable!</em> You’re
right. Life is played on a grid of bools, and there is no way the map $ b
\mapsto b^\prime $ is continuous, let alone differentiable.</p>
<p>But suppose we could make a “best effort” differentiable analogue? Let us play
Life on a grid of real numbers that are 0 for “dead” cells and 1 for “live”
cells. Can we “implement” a step of Life using only differentiable operations?
Let’s try.</p>
<p>We will look at each cell $ c $ individually. The first step is to count our
live neighbors. Well, if “live” cells are 1 and “dead” cells are 0, then we can
simply add up the values of all 8 of our neighbors to get our live neighbor
count $ n $. Indeed, if we wanted to be clever, we could implement this
efficiently as a convolution with this kernel:</p>
<p>\[
\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1
\end{bmatrix}
\]</p>
<p>Good, so we know $ n $ and of course our own 0/1 value $ c $. The next step
is to figure out whether this cell will be alive in the next generation. Here
are the rules:</p>
<ol>
<li>If the cell is alive, $ c = 1 $, then it stays alive if and only if $ n =
2 $ or $ n = 3 $.</li>
<li>If the cell is dead, $ c = 0 $, then it comes to life if and only if $ n
= 3 $.</li>
</ol>
<p>Let us approach (2) first. What function is 1 when $ n = 3 $ and 0 otherwise?
The “spike-at-3” function, of course. That’s not differentiable, but a narrow
Gaussian centered at 3 is <em>almost</em> the same thing! If scaled appropriately, the
Gaussian is one at input 3, and <em>almost</em> zero everywhere else. See this graph:</p>
<p><img src="https://hardmath123.github.io/static/conways-gradient/approx-graph.png" alt="approximating a spike with a less spiky
spike"/></p>
<p>Similarly, for (1) an appropriately-scaled Gaussian centered at 2.5 gets the
job done. Finally, we can mux these two cases under gate $ c $ by simple
linear interpolation. Because $ c \approx 0 $ or $ c \approx 1 $, we know
that $ ca + (1-c)b $ is just like writing <code>if c then a else b</code>.</p>
<p>That’s it! We now have a differentiable function, which looks like this:</p>
<p><img src="https://hardmath123.github.io/static/conways-gradient/approx-map.png" alt="the same, in 2d"/></p>
<p>Note that it’s worth “clamping” the output of that function to 0 or 1 using,
say, the tanh function. That way cell values always end up close to 0 or 1.</p>
<p>Okay, great! Now all that’s left to do is to write this in PyTorch and call
.backward()…</p>
<p>…and it begins to learn, within seconds! Here is a GIF of $ b $ at every
100 steps of gradient descent.</p>
<p><img src="https://hardmath123.github.io/static/conways-gradient/learn.gif" alt="gif of it learning"/></p>
<p>(Note: This GIF is where the target image $ t $ is an all-white rectangle; my
janky PyTorch gradient descent finds sparse configurations that give birth to
an “overpopulated” field where nearly 90% of cells are alive.)</p>
<hr/>
<p>Looking at that GIF, the beautiful labyrinthine patterns reminded me of
something seemingly unrelated: the skin of a giant pufferfish. Here’s a picture
from Wikipedia:</p>
<p><img src="https://hardmath123.github.io/static/conways-gradient/pufferfish.jpg" alt="pufferfish"/></p>
<p>And here’s a zoomed-in 100-by-100 section of the finished product from above:</p>
<p><img src="https://hardmath123.github.io/static/conways-gradient/seed.png" alt="pufferfish in Life?"/></p>
<p>Patterns like the one on the pufferfish come about as a result of
“symmetry-breaking,” when small perturbations disturb an unstable homogeneous
state. The ideas were first described by Alan Turing (and thus the patterns are
called <a href="https://en.wikipedia.org/wiki/Turing_pattern">Turing Patterns</a>). Here’s
his <a href="http://www.dna.caltech.edu/courses/cs191/paperscs191/turing.pdf">1952
paper</a>.</p>
<p>I can’t help but wonder if there’s a reaction-diffusion-model-esque effect at
work here as well, the symmetry being broken by the random initialization of $
b $. If that’s the case, it would create quite a wonderful connection between
cells, cellular automata, Turing-completeness, and Turing patterns…</p>
<p>(Curious? <a href="https://hardmath123.github.io/static/conways-gradient/atavise.py">Here</a> is the Python program I
used to play these games. Enjoy!)</p>

            </section>

            

        </article></div>
  </body>
</html>
