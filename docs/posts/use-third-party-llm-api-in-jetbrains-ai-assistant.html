<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Stream29/ProxyAsLocalModel">Original</a>
    <h1>Show HN: Use Third Party LLM API in JetBrains AI Assistant</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Proxy remote LLM API as Local model. Especially works for using custom LLM in JetBrains AI Assistant.</p>
<p dir="auto">Powered by Ktor and kotlinx.serialization. Thanks to their no-reflex features.</p>

<p dir="auto">Currently, JetBrains AI Assistant provides a free plan with very limited quotes. I tried out and my quote ran out quickly.</p>
<p dir="auto">I already bought other LLM API tokens, such like Gemini and Qwen. So I started to think of using them in AI Assistant. Unfortunately, only local models from LM Studio and Ollama are supported. So I started to work on this proxy application that proxy third party LLM API as LM Studio and Ollama API so that I can use them in my JetBrains IDEs.</p>
<p dir="auto">This is Just a simple task, so I started to use the official SDKs as clients and write a simple Ktor server that provides endpoints as LM Studio and Ollama. The problem appears when I try to distribute it as a GraalVM native image. The official Java SDKS uses too many dynamic features, making it hard to compile into a native image, even with a tracing agent. So I decided to implement a simple client of streaming chat completion API by myself with Ktor and kotlinx.serialization which are both no-reflex, functional and DSL styled.</p>
<p dir="auto">As you can see, this application is distributed as a fat runnable jar and a GraalVM native image, which makes it cross-platform and fast to start.</p>
<p dir="auto">The development of this application gives me confidence in Kotlin/Ktor/kotlinx.serialization. The Kotlin world uses more functional programming and less reflexion, which makes it more suitable for GraalVM native image, with faster startup and less memory usage.</p>

<p dir="auto">Proxy from: OpenAI, Claude, DashScope(Alibaba Qwen), Gemini, Deepseek, Mistral, SiliconFlow.</p>
<p dir="auto">Proxy as: LM Studio, Ollama.</p>
<p dir="auto">Streaming chat completion API only.</p>

<p dir="auto">This application is a proxy server, distributed as a fat runnable jar and a GraalVM native image (Windows x64).</p>
<p dir="auto">Run the application, and you will see a help message:</p>
<div data-snippet-clipboard-copy-content="2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.
2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\config.yml with schema annotation.
2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\config.yml
2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234
2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434
2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []"><pre><code>2025-05-02 10:43:53 INFO  Help - It looks that you are starting the program for the first time here.
2025-05-02 10:43:53 INFO  Help - A default config file is created at your_path\config.yml with schema annotation.
2025-05-02 10:43:53 INFO  Config - Config file watcher started at your_path\config.yml
2025-05-02 10:43:53 INFO  LM Studio Server - LM Studio Server started at 1234
2025-05-02 10:43:53 INFO  Ollama Server - Ollama Server started at 11434
2025-05-02 10:43:53 INFO  Model List - Model list loaded with: []
</code></pre></div>
<p dir="auto">Then you can edit the config file to set up your proxy server.</p>

<p dir="auto">This config file is automatically hot-reloaded when you change it. Only the influenced parts of the server will be updated.</p>
<p dir="auto">When first generating the config file, it will be created with schema annotations. This will bring completion and check in your editor.</p>

<div dir="auto" data-snippet-clipboard-copy-content="# $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json
lmStudio:
  port: 1234 # This is default value
  enabled: true # This is default value
  host: 0.0.0.0 # This is default value
  path: /your/path # Will be add before the original endpoints, default value is empty
ollama:
  port: 11434 # This is default value
  enabled: true # This is default value
  host: 0.0.0.0 # This is default value
  path: /your/path # Will be add before the original endpoints, default value is empty
client:
  socketTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds
  connectionTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds
  requestTimeout: 1919810 # Long.MAX_VALUE is default value, in milliseconds
  retry: 3 # This is default value
  delayBeforeRetry: 1000 # This is default value, in milliseconds

apiProviders:
  OpenAI:
    type: OpenAi
    baseUrl: https://api.openai.com/v1
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gpt-4o
  Claude:
    type: Claude
    apiKey: &lt;your_api_key&gt;
    modelList:
      - claude-3-7-sonnet
  Qwen:
    type: DashScope
    apiKey: &lt;your_api_key&gt;
    modelList: # This is default value
      - qwen-max
      - qwen-plus
      - qwen-turbo
      - qwen-long
  DeepSeek:
    type: DeepSeek
    apiKey: &lt;your_api_key&gt;
    modelList: # This is default value
      - deepseek-chat
      - deepseek-reasoner
  Mistral:
    type: Mistral
    apiKey: &lt;your_api_key&gt;
    modelList: # This is default value
      - codestral-latest
      - mistral-large
  SiliconFlow:
    type: SiliconFlow
    apiKey: &lt;your_api_key&gt;
    modelList:
      - Qwen/Qwen3-235B-A22B
      - Pro/deepseek-ai/DeepSeek-V3
      - THUDM/GLM-4-32B-0414
  Gemini:
    type: Gemini
    apiKey: &lt;your_api_key&gt;
    modelList:
      - gemini-2.5-flash-preview-04-17"><pre><span><span>#</span> $schema: https://github.com/Stream29/ProxyAsLocalModel/raw/master/config_v3.schema.json</span>
<span>lmStudio</span>:
  <span>port</span>: <span>1234</span> <span><span>#</span> This is default value</span>
  <span>enabled</span>: <span>true </span><span><span>#</span> This is default value</span>
  <span>host</span>: <span>0.0.0.0 </span><span><span>#</span> This is default value</span>
  <span>path</span>: <span>/your/path </span><span><span>#</span> Will be add before the original endpoints, default value is empty</span>
<span>ollama</span>:
  <span>port</span>: <span>11434</span> <span><span>#</span> This is default value</span>
  <span>enabled</span>: <span>true </span><span><span>#</span> This is default value</span>
  <span>host</span>: <span>0.0.0.0 </span><span><span>#</span> This is default value</span>
  <span>path</span>: <span>/your/path </span><span><span>#</span> Will be add before the original endpoints, default value is empty</span>
<span>client</span>:
  <span>socketTimeout</span>: <span>1919810</span> <span><span>#</span> Long.MAX_VALUE is default value, in milliseconds</span>
  <span>connectionTimeout</span>: <span>1919810</span> <span><span>#</span> Long.MAX_VALUE is default value, in milliseconds</span>
  <span>requestTimeout</span>: <span>1919810</span> <span><span>#</span> Long.MAX_VALUE is default value, in milliseconds</span>
  <span>retry</span>: <span>3</span> <span><span>#</span> This is default value</span>
  <span>delayBeforeRetry</span>: <span>1000</span> <span><span>#</span> This is default value, in milliseconds</span>

<span>apiProviders</span>:
  <span>OpenAI</span>:
    <span>type</span>: <span>OpenAi</span>
    <span>baseUrl</span>: <span>https://api.openai.com/v1</span>
    <span>apiKey</span>: <span>&lt;your_api_key&gt;</span>
    <span>modelList</span>:
      - <span>gpt-4o</span>
  <span>Claude</span>:
    <span>type</span>: <span>Claude</span>
    <span>apiKey</span>: <span>&lt;your_api_key&gt;</span>
    <span>modelList</span>:
      - <span>claude-3-7-sonnet</span>
  <span>Qwen</span>:
    <span>type</span>: <span>DashScope</span>
    <span>apiKey</span>: <span>&lt;your_api_key&gt;</span>
    <span>modelList</span>: <span><span>#</span> This is default value</span>
      - <span>qwen-max</span>
      - <span>qwen-plus</span>
      - <span>qwen-turbo</span>
      - <span>qwen-long</span>
  <span>DeepSeek</span>:
    <span>type</span>: <span>DeepSeek</span>
    <span>apiKey</span>: <span>&lt;your_api_key&gt;</span>
    <span>modelList</span>: <span><span>#</span> This is default value</span>
      - <span>deepseek-chat</span>
      - <span>deepseek-reasoner</span>
  <span>Mistral</span>:
    <span>type</span>: <span>Mistral</span>
    <span>apiKey</span>: <span>&lt;your_api_key&gt;</span>
    <span>modelList</span>: <span><span>#</span> This is default value</span>
      - <span>codestral-latest</span>
      - <span>mistral-large</span>
  <span>SiliconFlow</span>:
    <span>type</span>: <span>SiliconFlow</span>
    <span>apiKey</span>: <span>&lt;your_api_key&gt;</span>
    <span>modelList</span>:
      - <span>Qwen/Qwen3-235B-A22B</span>
      - <span>Pro/deepseek-ai/DeepSeek-V3</span>
      - <span>THUDM/GLM-4-32B-0414</span>
  <span>Gemini</span>:
    <span>type</span>: <span>Gemini</span>
    <span>apiKey</span>: <span>&lt;your_api_key&gt;</span>
    <span>modelList</span>:
      - <span>gemini-2.5-flash-preview-04-17</span></pre></div>
</article></div></div>
  </body>
</html>
