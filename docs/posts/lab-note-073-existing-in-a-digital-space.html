<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://interjectedfuture.com/lab-note-073-existing-in-a-digital-space/">Original</a>
    <h1>Lab note #073 Existing in a digital space</h1>
    
    <div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://interjectedfuture.com/tag/lab-notes/">lab notes</a>
            

            <div>
                <p><a href="https://calvin.sh/author/wil/">
                                <img src="https://www.gravatar.com/avatar/915aac3dfde2fb502ce415d77643a72d?s=250&amp;d=mm&amp;r=x" alt="Wil Chung"/>
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2025-06-30">30 Jun 2025</time>
                            <span><span>—</span> 3 min read</span>
                    </p>
                </div>
            </div>

                <figure>
        <img srcset="/content/images/size/w320/2025/06/ChatGPT-Image-Jun-30--2025--11_33_34-AM.png 320w,
                    /content/images/size/w600/2025/06/ChatGPT-Image-Jun-30--2025--11_33_34-AM.png 600w,
                    /content/images/size/w960/2025/06/ChatGPT-Image-Jun-30--2025--11_33_34-AM.png 960w,
                    /content/images/size/w1200/2025/06/ChatGPT-Image-Jun-30--2025--11_33_34-AM.png 1200w,
                    /content/images/size/w2000/2025/06/ChatGPT-Image-Jun-30--2025--11_33_34-AM.png 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://calvin.sh/content/images/size/w1200/2025/06/ChatGPT-Image-Jun-30--2025--11_33_34-AM.png" alt="Lab note #073 Existing in a digital space"/>
    </figure>

        </header>

        <section>
            <p>I ended up deciding on SolidJs. For one, I spent way too much time deliberating on a choice that wouldn&#39;t matter as much at the end of the day. But I think I wanted to give functional programming a good shake, since it&#39;s aligned with my values and ideals. However, at the end of the day I couldn&#39;t choose Elm, because of the friction over ports, and that it forces you to draw boundaries at when you have side effects. It&#39;s not conducive to computational flows that is interspersed with side effects, but is a single complete idea.</p><p>Two main risk factors that I eliminated last week.</p><ol><li>The stack for implementing free-floating avatars.</li><li>Multi-tool use in a single cycle</li></ol><p>I didn&#39;t want to have the AI feel like it&#39;s constrained to a sidebar, so I wanted to experiment with whether it can have an avatar that exists in the same digital space of the artifact as you, the user. This is still a left-over sentiment from my experiment with <a href="https://interjectedfuture.com/lab-notes-026-tool-based-manipulation/" rel="noreferrer">tool-based manipulation</a>. While there is an advantage to having a chat history, I don&#39;t think it should be the default. I want the default to be direct-manipulation, not just for the user, but for the AI. </p><p>Between absolutely positioned DOMs and using the canvas for an avatar, I opted for the latter. Mostly for flexibility of effects. I wasn&#39;t sure how canvas and DOM could communicate, but there are query APIs provided by the browser that enabled me to build functions like, &#34;move to this DOM&#34; or &#34;shoot at this DOM&#34;. LLM Vibe coding helped a lot to surface these APIs that I wasn&#39;t aware of before.</p><figure><a href="https://s3.us-east-1.amazonaws.com/archives.wilchung.com/public/rift_and_reson_demo.mov?ref=interjectedfuture.com"><img src="https://interjectedfuture.com/content/images/2025/06/image-5.png" alt="" loading="lazy" width="2000" height="1271" srcset="https://interjectedfuture.com/content/images/size/w600/2025/06/image-5.png 600w, https://interjectedfuture.com/content/images/size/w1000/2025/06/image-5.png 1000w, https://interjectedfuture.com/content/images/size/w1600/2025/06/image-5.png 1600w, https://interjectedfuture.com/content/images/2025/06/image-5.png 2348w" sizes="(min-width: 720px) 720px"/></a><figcaption><span>Click for a movie demo</span></figcaption></figure><p>The avatar for the AI is the flame, and when it manipulates the canvas blocks it shoots a particle at it. The graphics aren&#39;t final, but I wanted to see how it felt to have something on the screen manipulate things. Generally, I think it feels good, but it&#39;s a balance between visual feedback and distraction. It still needs a lot of polish.</p><p>One of the issues was that my agent tended to stop after a single tool use. Sometimes, after two. Sometimes, it repeats the same tool use over and over again. So I didn&#39;t know whether it was an inherent limitation, or that it&#39;s an implementation issue. Surprise! It&#39;s an implementation issue.</p><p>First, I wasn&#39;t inserting the results of the tool call in the message history, so the LLM thought that a tool use ended up being a no-op, so it tried again. Easy fix.</p><p>Second, my prompt wasn&#39;t good enough. I fired up o3, described what I was trying to do, as well as my current prompt. It was able to improve my prompt to be more explicit in ways that I wouldn&#39;t have thought to be more explicit. Sometimes, just getting the LLM to generate prompts for itself (or simpler models) has been a big unlock for me when using these things.</p><p>I have re-done what kinds of canvas blocks are available, and that ended up being an interesting thing in itself. I&#39;ll have to finish it this week.</p><hr/><p>Here&#39;s what I looked at this week:</p><ul><li><a href="https://hamel.dev/notes/llm/rag/p1-intro.html?ref=interjectedfuture.com">P1: I don’t use RAG, I just retrieve documents – Hamel&#39;s Blog</a> #[[Retrieval Assisted Generation]]</li><li><a href="https://www.usenix.org/publications/loginonline/memory-safety-merely-table-stakes?ref=interjectedfuture.com">Memory Safety is Merely Table Stakes | USENIX</a> #[[Foreign Function Interface]]</li><li><a href="https://web.archive.org/web/20250623055214/https://addyo.substack.com/p/the-prompt-engineering-playbook-for">The Prompt Engineering Playbook for Programmers</a> #[[Prompt Engineering]]</li><li><a href="https://www.stainless.com/blog/mcp-is-eating-the-world--and-its-here-to-stay?ref=interjectedfuture.com">MCP is eating the world—and it&#39;s here to stay</a> #[[Model Context Protocol]]</li><li><a href="https://eieio.games/blog/a-million-realtime-chess-boards-in-a-single-process/?ref=interjectedfuture.com">Running a million-board chess MMO in a single process · eieio.games</a> #[[Postmortem Engineering]]</li><li><a href="https://paper.dropbox.com/doc/Custom-Elements-in-Elm-1Dd7LMDwgJLNGHazDImSm?ref=interjectedfuture.com">Custom Elements in Elm – Dropbox Paper</a> #[[Elm]]</li></ul>
        </section>

    </article>


</div></div>
  </body>
</html>
