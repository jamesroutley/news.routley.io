<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.johndcook.com/blog/2025/09/17/learning-languages-with-the-help-of-algorithms/">Original</a>
    <h1>Learning Languages with the Help of Algorithms</h1>
    
    <div id="readability-page-1" class="page"><div>
		<p><a href="https://www.johndcook.com/wp-content/uploads/2025/09/commonwords.jpg"><img fetchpriority="high" decoding="async" src="https://www.johndcook.com/wp-content/uploads/2025/09/commonwords-300x223.jpg" alt="" width="983" height="731" srcset="https://www.johndcook.com/wp-content/uploads/2025/09/commonwords-300x223.jpg 300w, https://www.johndcook.com/wp-content/uploads/2025/09/commonwords-768x571.jpg 768w, https://www.johndcook.com/wp-content/uploads/2025/09/commonwords.jpg 800w" sizes="(max-width: 983px) 100vw, 983px"/></a></p>
<p>Suppose you’re learning a new language and want to boost your vocabulary in a very time-efficient way.</p>
<p>People have many ways to learn a language, different for each person. Suppose you wanted to improve your vocabulary by reading books in that language. To get the most impact, you’d like to pick books that cover as many common words in the language as possible.</p>
<p>Here is a formalization. Suppose for a large set of m books of average length n words, you want to pick the one book that has the highest vocabulary impact from the set of books. This vocabulary impact of a book is measured by a weighted sum across all vocabulary words in the book, each word weighted by how common the word is in the language, as measured by word frequency across all books; this essentially gives a probability weight of each word in the language.</p>
<p>That’s an easy problem to solve. First, optionally filter out stop words like (in the case of English) “the“ and “and”  considered in some sense to have “not much meaning.“ Second, across all books build a unique word list along with counts of number of occurrences of each word. Finally, for each book evaluate the coverage of those words, computing a score as described above.</p>
<p>Computing the unique word list and scores can be done by a hashing process that runs in linear order mn time typically, worst case mn log(mn). To compute the score also costs average linear time. So the entire process can be done in linear time.</p>
<p>What if you want to find the best <em>two</em> books to read, with the best joint vocabulary coverage? To find the optimal solution, the best known time is not linear but is quadratic for the general case.</p>
<p>How about the best k books for arbitrary k &gt; 0?</p>
<p>This is an NP-hard problem, meaning that the compute time to solve this problem exactly for the best known algorithms for the general case grows exponentially in k as the size k of your desired set of reading books increases.</p>
<p>So you cannot expect to solve this problem exactly for large k. All hope is not lost, however. This is a maximal weighted cover problem, residing in a subset of the NP hard problems known as submodular problems.  Because of this, approximate algorithms are known that guarantee approximation accuracy within a certain known factor of the true best solution (see [1-5]).</p>
<p>These algorithms are described in [6], [7], [8]. The basic idea of the algorithms is to add a single high-impact book at a time to the running set of high-impact books—a greedy algorithm. It is not guaranteed to be the best book list, but it is reasonable.</p>
<p>The helpful Python submodlib package runs very fast on this problem.</p>
<p>One can improve the quality of the result by spending more on computation.  First, you could use a blocking strategy to compute precisely the best two books to add at each step, or three books, and so forth (computational complexity is quadratic, cubic and so forth). Similarly one could use a look-ahead strategy: add two books to the list that are together the best by an exact computation, then leave one off and find the best second and third books, and so forth. These in general do not improve on the submodularity bound, however in practice the result is more accurate.</p>
<p>One can also use various heuristics to improve performance in some cases. For example, if a book has little or no vocabulary that is not present in the other books, it can sometimes be safely discarded. However, in general the exact case remains hard (unless P=NP).</p>
<h4>References</h4>
<p>[1] Abhimanyu Das and David Kempe. <a href="https://arxiv.org/abs/1102.3975">Submodular meets Spectral: Greedy Algorithms for Subset Selection, Sparse Approximation and Dictionary Selection</a>. Proceedings of ICML, 2011.</p>
<p>[2] Abhimanyu Das and David Kempe.<a href="https://jmlr.org/papers/volume19/16-534/16-534.pdf"> Approximate Submodularity and its Applications: Subset Selection, Sparse Approximation and Dictionary Selection</a>.</p>
<p>[3] M. Conforti and G. Cornuéjols. <a href="https://www.sciencedirect.com/science/article/pii/0166218X84900039">Submodular set functions, matroids and the greedy algorithm: tight worst-case bounds</a>. Discrete Applied Mathematics, 7(3):251–274, 1984.</p>
<p>[4] Maxim Sviridenko, Jan Vondrák, and Justin Ward. <a href="https://arxiv.org/abs/1311.4728">Optimal approximation for submodular and supermodular optimization with bounded curvature</a>. Proceedings of SODA, 2013.</p>
<p>[5] Rishabh Iyer, Stefanie Jegelka, and Jeff Bilmes. <a href="https://arxiv.org/abs/1311.2110">Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions</a>. 2013.</p>
<p>[6] Nemhauser, George L., Laurence A. Wolsey, and Marshall L. Fisher. “<a href="https://www.cs.cmu.edu/~aarti/Class/10704_Fall16/submod-max.pdf">An analysis of approximations for maximizing submodular set functions—I</a>.” Mathematical programming 14, no. 1 (1978): 265-294.</p>
<p>[7] “Maximum coverage problem,” <a href="https://en.wikipedia.org/wiki/Maximum_coverage_problem">https://en.wikipedia.org/wiki/Maximum_coverage_problem</a>.</p>
<p>[8] Wei, Kai, Rishabh Iyer, and Jeff Bilmes. “<a href="https://people.ece.uw.edu/bilmes/p/mypubs/wei2014-fast-multi-stage-icml.extended.pdf">Fast multi-stage submodular maximization</a>.” In International conference on machine learning, pp. 1494-1502. PMLR, 2014.</p>

			</div></div>
  </body>
</html>
