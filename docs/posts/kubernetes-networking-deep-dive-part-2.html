<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://techblog.hughtipping.com/p/kubernetes-networking-deep-dive-part-f73">Original</a>
    <h1>Kubernetes Networking Deep Dive: Part 2</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><span>This is the second post in a four-part series tracing packets through a Kubernetes cluster. In </span><a href="https://techblog.hughtipping.com/p/kubernetes-networking-deep-dive-part" rel="">Part 1</a><span>, we covered the foundational concepts: network namespaces, veth pairs, CNI, and kube-proxy. Now we trace actual packets between pods.</span></p><p><span>Pod-to-pod communication is often called “</span><em>east-west traffic</em><span>” and is the most common network traffic in a cluster. A pod connects to a service, a microservice calls another, in-cluster databases receive queries from application pods, etc. This traffic stays </span><em>within</em><span> the cluster and does not go through a load balancer.</span></p><p>Let’s look at two scenarios: pods on the same node (communication through a bridge) and pods on different nodes (requiring overlay encapsulation or direct routing).</p><p>For upcoming examples, let’s use the following setup:</p><ul><li><p>Pod A: IP 10.244.0.5, running on Node 1 (192.168.1.10)</p></li><li><p>Pod B: IP 10.244.0.6, also running on Node 1</p></li><li><p>Pod C: IP 10.244.1.5, running on Node 2 (192.168.1.11)</p></li></ul><p>Pod A runs a client application. Pod B and Pod C each run a server listening on port 8080. Let’s trace what happens when Pod A makes an HTTP request to Pod B (on the same node) and then to Pod C (on a different node).</p><p>When two pods are running on the same node, the network traffic never leaves the host. The packet travels via the Linux bridge (that acts as a virtual switch on each node)  connecting all the local pod veth endpoints.</p><p><strong>Step 1: Application makes connection (OSI Layer 7)</strong></p><p><span>The application in Pod A calls the </span><code>connect()</code><span> system call to establish a TCP connection to 10.244.0.6:8080. (A standard socket operation)</span></p><p><strong>Step 2: Kernel starts building the TCP/IP packet (OSI Layers 4 and 3)</strong></p><p>The kernel’s TCP/IP stack creates the packet:</p><ul><li><p>Source IP: 10.244.0.5</p></li><li><p>Destination IP: 10.244.0.6</p></li><li><p>Source port: ephemeral, chosen by the OS (e.g., 45678)</p></li><li><p>Destination port: 8080</p></li><li><p>Protocol: TCP</p></li></ul><p><strong>Step 3: Routing is decided in Pod A’s networking namespace (OSI Layer 3)</strong></p><p>The kernel looks at Pod A’s routing table to determine the outgoing interface:</p><pre><code><code># Inside Pod A
ip route

# Output:
# default via 10.244.0.1 dev eth0
# 10.244.0.0/24 dev eth0 proto kernel scope link src 10.244.0.5
</code></code></pre><p><span>The destination 10.244.0.6 matches the </span><code>10.244.0.0/24</code><span> route, which says “send directly via eth0.” A gateway is not needed because the destination is on the same subnet.</span></p><p><strong>Step 4: ARP resolution (Layer 2)</strong></p><p>Before sending the packet, the kernel will need the MAC address of 10.244.0.6. It checks the ARP** ( cache (a.k.a. Neighbor Table) or sends an ARP request:</p><pre><code><code># Inside Pod A&#39;s namespace; 
ip neigh show

# Output:
# 10.244.0.6 dev eth0 lladdr 62:a1:b2:c3:d4:e6 REACHABLE
# 10.244.0.1 dev eth0 lladdr 8a:1b:2c:3d:4e:5f REACHABLE
</code></code></pre><p>The kernel builds an Ethernet frame with:</p><ul><li><p>Source MAC: Pod A’s eth0 MAC</p></li><li><p>Destination MAC: Pod B’s eth0 MAC (62:a1:b2:c3:d4:e6)</p></li></ul><p>**ARP = Address Resolution Protocol: For discovery of OSI Layer 2 (MAC) address.</p><p><strong>Step 5: Packet leaves Pod A via veth pair (OSI Layer 2)</strong></p><p><span>The frame exits through Pod A’s eth0, which is one end of a veth pair. The packet emerges from the other end (veth-pod-a) in the </span><em>node’s</em><span> namespace.</span></p><p><strong>Step 6: Bridge forwards the frame (OSI Layer 2)</strong></p><p><span>The host-side veth is attached to a bridge (commonly named </span><code>cni0</code><span>, </span><code>cbr0</code><span>, or </span><code>docker0</code><span> depending on CNI). The bridge operates like a Layer 2 switch:</span></p><ol><li><p>It receives the frame on port veth-pod-a</p></li><li><p>It looks up the destination MAC in its forwarding table</p></li><li><p>It finds that MAC 62:a1:b2:c3:d4:e6 is reachable on port veth-pod-b</p></li><li><p>It forwards the frame out that port</p></li></ol><p>You can view the bridge’s MAC table:</p><pre><code><code># On the host (FDB = Forwarding Database)
bridge fdb show br cni0 | grep -i &#34;62:a1&#34;

# Output:
# 62:a1:b2:c3:d4:e6 dev veth-pod-b master cni0
</code></code></pre><p><strong>Step 7: Packet enters Pod B via veth pair (OSI Layer 2)</strong></p><p>The frame enters veth-pod-b in the host namespace and emerges from eth0 in Pod B’s namespace.</p><p><strong>Step 8: Kernel delivers to application (OSI Layers 3, 4, 7 → Back up the OSI stack)</strong></p><p>Pod B’s kernel:</p><ol><li><p>Receives the packet</p></li><li><p>Takes out the the Ethernet header (no longer needed), sees that it is an IP packet</p></li><li><p>Verifies that the destination IP matches its own (10.244.0.6)</p></li><li><p>Gets rid of the IP header (no longer needed here), sees it is TCP and destined for port 8080</p></li><li><p>Delivers the payload to the application listening on that port</p></li></ol><p>The return traffic (Pod B’s response) follows the same path in reverse.</p><p>You can check out this traffic at different points:</p><pre><code><code># Capture on the bridge (sees ALL local pod traffic)
sudo tcpdump -i cni0 -nn host 10.244.0.5 and host 10.244.0.6

# Output:
# 14:23:01.234567 IP 10.244.0.5.45678 &gt; 10.244.0.6.8080: Flags [S], seq 123456789
# 14:23:01.234789 IP 10.244.0.6.8080 &gt; 10.244.0.5.45678: Flags [S.], seq 987654321, ack 123456790

# Capture on a specific veth (sees only that pod&#39;s traffic, great for troubleshooting)
sudo tcpdump -i veth-pod-a -nn port 8080
</code></code></pre><p><span>For direct pod-to-pod communication (not through a Service), iptables is not heavily involved. The packet passes through the FORWARD chain in the filter table, but unless you have </span><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/" rel="">NetworkPolicies</a><span> configured, the default is to ACCEPT.</span></p><p>Another fun iptables command for you:</p><pre><code><code>sudo iptables -L FORWARD -n -v | head -5
# Output:
# Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
#  pkts bytes target     prot opt in     out     source               destination
#  1.2M  890M KUBE-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0
#  1.2M  890M CNI-FORWARD   all  --  *      *       0.0.0.0/0            0.0.0.0/0
</code></code></pre><p>The CNI-FORWARD chain generally contains rules for NetworkPolicy enforcement if your CNI supports it (note that not all do!)</p><p>When pods are on different nodes, the packet has to travel on the physical network connecting the two nodes. This is where the capabilities of the CNI plugin matters. Let’s take a high-level look at both overlay (VXLAN) and routed (BGP) approaches.</p><p>Node 1 has a packet destined for 10.244.1.5 (Pod C on Node 2). The physical network between nodes has no information about pod IP addresses. It only knows how to route traffic between node IP addresses (192.168.1.10 and 192.168.1.11).</p><p>We have two solutions:</p><ol><li><p><strong>Overlay</strong><span>: </span><em>Encapsulate</em><span> the pod-to-pod packet inside a node-to-node packet</span></p></li><li><p><strong>Routed</strong><span>: Configure the physical network to route the pod CIDRs themselves </span></p></li></ol><p><span>VXLAN (Virtual Extensible LAN) creates a Layer 2 overlay on top of a Layer 3 network. Eek, what’s that?</span></p><p><strong>Step 1: Application initiates connection (OS Layer 7)</strong></p><p>Pod A’s application connects to 10.244.1.5:8080 (Pod C). Standard socket connection like above.</p><p><strong>Step 2: Kernel builds the TCP/IP packet (OSI Layers 4 and 3)</strong></p><ul><li><p>Source IP: 10.244.0.5</p></li><li><p>Destination IP: 10.244.1.5</p></li><li><p>Source ephemeral port: 45678</p></li><li><p>Destination port: 8080</p></li></ul><p><strong>Step 3: Routing decision done in Pod A’s namespace (OSI Layer 3)</strong></p><pre><code><code># Inside Pod A
ip route

# Output:
# default via 10.244.0.1 dev eth0
# 10.244.0.0/24 dev eth0 proto kernel scope link src 10.244.0.5
</code></code></pre><p><span>The destination 10.244.1.5 </span><em>does not match</em><span> 10.244.0.0/24, so the packet will go to the default gateway (10.244.0.1).</span></p><p><strong>Step 4: Packet reaches host namespace via veth</strong></p><p>The packet exits eth0 in Pod A, and emerges from veth-pod-a in the host namespace.</p><p><strong>Step 5: Host’s routing table lookup (OSI Layer 3)</strong></p><p>The host’s kernel takes a look at its routing table:</p><pre><code><code># On Node 1
ip route

# Output (Flannel VXLAN example):
# default via 192.168.1.1 dev eth0
# 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1
# 10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink
# 10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink
</code></code></pre><p><span>The destination 10.244.1.5 matches </span><code>10.244.1.0/24 via flannel.1</code><span>. (if that’s the CNI you’re using, but note that the Flannel CNI does not have NetworkPolicy capabilities). The </span><code>flannel.1</code><span> interface is a VXLAN Tunnel Endpoint (VTEP).</span></p><p><strong>Step 6: VXLAN encapsulation (OSI Layer 2 over Layer 3)</strong></p><p>The flannel.1 interface:</p><ol><li><p>Looks up which node owns 10.244.1.0/24 (Node 2 in our case: 192.168.1.11)</p></li><li><p><span>It then </span><em>encapsulates</em><span> the original Ethernet frame within a VXLAN header</span></p></li><li><p>It wraps that in a UDP packet (destination port 4789)</p></li><li><p>And finally it’s wrapped in an IP packet (192.168.1.10 to 192.168.1.11)</p></li></ol><p>You can see the VXLAN FDB (forwarding database):</p><pre><code><code># On Node 1
bridge fdb show dev flannel.1

# Output:
# 5a:2b:3c:4d:5e:6f dst 192.168.1.11 self permanent
# 7a:8b:9c:0d:1e:2f dst 192.168.1.12 self permanent
</code></code></pre><p>This maps the VTEP MAC addresses to node IPs.</p><p><strong>Step 7: Outer packet sent to Node 2 (Layers 3 and 2)</strong></p><p>The encapsulated packet is then routed normally over the network like any other packet going between nodes to 192.168.1.11:</p><ul><li><p>Source IP: 192.168.1.10 (Node 1)</p></li><li><p>Destination IP: 192.168.1.11 (Node 2)</p></li><li><p>Protocol: UDP</p></li><li><p>Destination port: 4789 (VXLAN)</p></li><li><p>Payload: VXLAN header + original Ethernet frame</p></li></ul><p><strong>Step 8: Node 2 receives and removes the encapsulation (OSI Layer 3)</strong></p><p>Node 2’s kernel:</p><ol><li><p>Receives UDP packet on port 4789</p></li><li><p>Recognizes it as VXLAN traffic for the flannel.1 interface</p></li><li><p>Nixes the Ethernet, IP, UDP, and VXLAN headers (tear that envelope open!)</p></li><li><p>Extracts the original Ethernet frame</p></li></ol><p><strong>Step 9: Host routing to the destination local pod (Layer 3)</strong></p><p>The extracted packet has destination 10.244.1.5. Node 2’s routing table:</p><pre><code><code># On Node 2
ip route

# Output:
# 10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1
</code></code></pre><p>A local route so the packet goes to the cni0 bridge.</p><p><strong>Step 10: Bridge forwards to Pod C (Layer 2)</strong></p><p>The bridge looks up the MAC address for 10.244.1.5 and forwards the frame to veth-pod-c.</p><p><strong>Step 11: Pod C receives packet (Layers 3, 4, 7)</strong></p><p>The packet enters Pod C’s namespace through eth0. The kernel delivers it to the application on port 8080.</p><p>At step 7, the packet on the wire looks something like this. (Yeah a bit in the weeds. I had help drawing this.)</p><pre><code>┌────────────────────────────────────────────────────────────────────────────┐
│                    VXLAN ENCAPSULATED PACKET                               │
├───────────────┬────────────────┬──────────┬────────┬──────────┬───────────┤
│ Outer         │ Outer IP       │ UDP      │ VXLAN  │ Inner    │ Inner IP  │
│ Ethernet      │ Header         │ Header   │ Header │ Ethernet │ Packet    │
├───────────────┼────────────────┼──────────┼────────┼──────────┼───────────┤
│ dst: router   │ src: 192.168.  │ src:     │ VNI:   │ dst: Pod │ src:      │
│ MAC           │ 1.10           │ random   │ 1      │ C MAC    │ 10.244.   │
│               │ dst: 192.168.  │ dst:     │        │ src: Pod │ 0.5       │
│ src: Node1    │ 1.11           │ 4789     │        │ A MAC    │ dst:      │
│ MAC           │                │          │        │          │ 10.244.   │
│               │                │          │        │          │ 1.5       │
├───────────────┼────────────────┼──────────┼────────┼──────────┼───────────┤
│    14 bytes   │    20 bytes    │  8 bytes │8 bytes │ 14 bytes │ 20+ bytes │
└───────────────┴────────────────┴──────────┴────────┴──────────┴───────────┘
                │                                    │
                │◄──────── 50 bytes overhead ───────►│

Total overhead: 50 bytes (outer Ethernet + outer IP + UDP + VXLAN + inner Ethernet)
This is why pod MTU is typically 1450 when node MTU is 1500.</code></pre><pre><code><code># On Node 1, capture VXLAN-encapsulated traffic
sudo tcpdump -i eth0 -nn udp port 4789

# Output:
# 14:30:01.123 IP 192.168.1.10.52341 &gt; 192.168.1.11.4789: VXLAN, flags [I] (0x08), vni 1
# IP 10.244.0.5.45678 &gt; 10.244.1.5.8080: Flags [S], seq 123456789

# Capture on the VXLAN interface (sees non-encapsulated traffic)
sudo tcpdump -i flannel.1 -nn host 10.244.1.5

# Output:
# 14:30:01.123 IP 10.244.0.5.45678 &gt; 10.244.1.5.8080: Flags [S], seq 123456789
</code></code></pre><p><span>In routed (BGP***) mode, there is no encapsulation. Pod IPs are </span><em>advertised</em><span> via BGP (or even static routes) so the physical network knows how to route them.</span></p><p>Routed mode requires one of:</p><ul><li><p>BGP peering </p><ul><li><p>between nodes and network routers</p></li><li><p>between nodes themselves</p></li></ul></li><li><p>Static routes configured on network infrastructure</p></li><li><p>Cloud VPC route table entries</p></li></ul><p>***BGP = Border Gateway Protocol: standardized gateway protocol to exchange routing and reachability information among autonomous systems</p><p><strong>Steps 1-4: Same as overlay</strong></p><p>Pod A builds a packet for 10.244.1.5, it exits via veth to the host namespace.</p><p><strong>Step 5: Host routing table lookup (Layer 3)</strong></p><p>Notice that the host’s routing table in BGP mode looks a bit different:</p><pre><code><code># On Node 1 (Calico BGP mode)
ip route

# Output:
# default via 192.168.1.1 dev eth0
# 10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1
# 10.244.1.0/24 via 192.168.1.11 dev eth0 proto bird
# 10.244.2.0/24 via 192.168.1.12 dev eth0 proto bird
</code></code></pre><p><span>The route for 10.244.1.0/24 points </span><em>directly to Node 2’s IP</em><span> (192.168.1.11) </span><em>via the physical interface (eth0 - on the host, not to be confused with a pod’s eth0)</em><span>. The </span><code>proto bird</code><span> indicates these routes were installed by the BIRD BGP daemon used by Calico CNI (for this example).</span></p><p><strong>Step 6: Packet sent to Node 2 (OSI Layer 3)</strong></p><p>The packet is sent directly with:</p><ul><li><p>Source IP: 10.244.0.5 (Pod A, unchanged)</p></li><li><p>Destination IP: 10.244.1.5 (Pod C, unchanged)</p></li></ul><p>At Layer 2:</p><ul><li><p>Source MAC: Node 1’s eth0</p></li><li><p>Destination MAC: Next hop (a router or Node 2 if on same Layer 2 network segment. In this case it’s the same segment.)</p></li></ul><p>Remember that there is no encapsulation. The pod IPs are visible on the physical network.</p><p><strong>Step 7: Physical network routing</strong></p><p>The physical network needs to know how to route 10.244.1.0/24 to Node 2. This happens via:</p><ul><li><p>BGP: Nodes advertise their pod CIDRs. Routers learn the routes.</p></li><li><p>Static routes: Network admin configures routes on routers (I’m not fond of static routing. It makes maintenance tougher.)</p></li><li><p>Cloud VPC: Cloud provider handles the routing.</p></li></ul><p><strong>Step 8: Node 2 receives packet (OSI Layer 3)</strong></p><p>Node 2 receives a packet with destination 10.244.1.5. Its routing table:</p><pre><code><code># On Node 2
ip route

# Output:
# 10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1
</code></code></pre><p>The destination is local so route to cni0 bridge.</p><p><strong>Steps 9-10: Same as with overlay</strong></p><p>Bridge forwards to Pod C. Application receives the packet.</p><p>Again, in the weeds.</p><pre><code><code># View BGP-learned routes (Calico with BIRD)
sudo calicoctl node status

# Output:
# IPv4 BGP status
# +--------------+-------------------+-------+----------+-------------+
# | PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |
# +--------------+-------------------+-------+----------+-------------+
# | 192.168.1.11 | node-to-node mesh | up    | 10:23:45 | Established |
# | 192.168.1.12 | node-to-node mesh | up    | 10:23:47 | Established |
# +--------------+-------------------+-------+----------+-------------+

# View routes learned via BGP
ip route show proto bird

# Output:
# 10.244.1.0/24 via 192.168.1.11 dev eth0
# 10.244.2.0/24 via 192.168.1.12 dev eth0

# Capture unencapsulated pod traffic on physical interface
sudo tcpdump -i eth0 -nn host 10.244.1.5

# Output:
# 14:35:01.234 IP 10.244.0.5.45678 &gt; 10.244.1.5.8080: Flags [S], seq 123456789
</code></code></pre><p>Some CNIs (notably Calico) support IPinIP as a lighter-weight alternative to VXLAN. IPinIP encapsulates the original IP packet directly in another IP packet, without the UDP and VXLAN headers.</p><pre><code><code>┌────────────────────────────────────────────────────────────────┐
│                    IPINIP PACKET                               │
├───────────────┬────────────────┬───────────────────────────────┤
│ Outer         │ Outer IP       │ Inner IP Packet               │
│ Ethernet      │ Header         │ (original pod-to-pod packet)  │
├───────────────┼────────────────┼───────────────────────────────┤
│ dst: router   │ src: 192.168.  │ src: 10.244.0.5               │
│ MAC           │ 1.10           │ dst: 10.244.1.5               │
│               │ dst: 192.168.  │ + TCP header + payload        │
│               │ 1.11           │                               │
│               │ proto: 4 (IPIP)│                               │
├───────────────┼────────────────┼───────────────────────────────┤
│    14 bytes   │    20 bytes    │         20+ bytes             │
└───────────────┴────────────────┴───────────────────────────────┘

Overhead: 20 bytes (just the outer IP header)
Pod MTU can be 1480 instead of 1450.
</code></code></pre><p>Calico can use IPinIP for cross-subnet traffic and direct routing for same-subnet traffic (”CrossSubnet” mode).</p><p>Here are some nerdy numbers I looked up to help you compare overhead for different approaches. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!vWyG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!vWyG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png 424w, https://substackcdn.com/image/fetch/$s_!vWyG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png 848w, https://substackcdn.com/image/fetch/$s_!vWyG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png 1272w, https://substackcdn.com/image/fetch/$s_!vWyG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!vWyG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png" width="1276" height="908" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/bb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:908,&#34;width&#34;:1276,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:152101,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://techblog.hughtipping.com/i/187330795?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!vWyG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png 424w, https://substackcdn.com/image/fetch/$s_!vWyG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png 848w, https://substackcdn.com/image/fetch/$s_!vWyG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png 1272w, https://substackcdn.com/image/fetch/$s_!vWyG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb95d80c-3b45-4130-971b-1b8f304feba6_1276x908.png 1456w" sizes="100vw" loading="lazy"/></picture><div></div></div></a></figure></div><p><span>Regardless of the approach, the Linux connection tracking system (conntrack) maintains state for TCP and UDP flows. This is important for return traffic (and </span><em>stateful</em><span> firewalling).</span></p><pre><code><code># View connection tracking entries
sudo conntrack -L | grep 10.244.0.5

# Output:
# tcp      6 117 TIME_WAIT src=10.244.0.5 dst=10.244.1.5 sport=45678 dport=8080 
#          src=10.244.1.5 dst=10.244.0.5 sport=8080 dport=45678 [ASSURED] use=1
</code></code></pre><p>The conntrack entry shows both directions of the connection</p><ul><li><p><span>src=10.244.0.5 dst=</span><strong>10.244.1.5</strong><span> sport=45678 dport=</span><strong>8080</strong></p></li><li><p><span>src=</span><strong>10.244.1.5</strong><span> dst=10.244.0.5 sport=</span><strong>8080</strong><span> dport=45678</span></p></li></ul><p>which lets the kernel match return packets to the original flow.</p><p>Here is a non-exhaustive list of commands to help you troubleshoot communication problems between pods.</p><pre><code><code># From inside a pod, check route to destination
ip route get 10.244.1.5

# Output:
# 10.244.1.5 via 10.244.0.1 dev eth0 src 10.244.0.5

# On the host, check route to remote pod CIDR
ip route get 10.244.1.5

# Output (VXLAN):
# 10.244.1.5 via 10.244.1.0 dev flannel.1 src 10.244.0.0
# Output (BGP):
# 10.244.1.5 via 192.168.1.11 dev eth0 src 192.168.1.10
</code></code></pre><pre><code><code># Verify bridge exists and has interfaces
bridge link show

# Output:
# 8: veth12345@if2: &lt;BROADCAST,MULTICAST,UP&gt; mtu 1450 master cni0 state forwarding
# 9: veth67890@if2: &lt;BROADCAST,MULTICAST,UP&gt; mtu 1450 master cni0 state forwarding

# Check bridge MAC table
bridge fdb show br cni0 | head
</code></code></pre><pre><code><code># Verify VXLAN interface exists
ip -d link show flannel.1

# Output:
# 4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN
#     link/ether 5a:2b:3c:4d:5e:6f brd ff:ff:ff:ff:ff:ff promiscuity 0
#     vxlan id 1 local 192.168.1.10 dev eth0 srcport 0 0 dstport 4789 nolearning

# Check VXLAN FDB entries
bridge fdb show dev flannel.1
</code></code></pre><pre><code><code># Inside source pod
tcpdump -i eth0 -nn host 10.244.1.5

# On source node bridge
sudo tcpdump -i cni0 -nn host 10.244.0.5

# On source node physical interface (shows encapsulated or raw traffic)
sudo tcpdump -i eth0 -nn host 192.168.1.11  # or host 10.244.1.5 for BGP

# On source node VXLAN interface (shows inner traffic)
sudo tcpdump -i flannel.1 -nn host 10.244.1.5
</code></code></pre><p>Pod-to-pod communication takes different paths depending on pod locality:</p><p><strong>Same-node</strong><span>: Packets traverse veth pairs and a Linux bridge. This is Layer 2 switching within the host. iptables is not involved unless NetworkPolicies are in place.</span></p><p><strong>Cross-node with overlay (VXLAN/IPinIP)</strong><span>: Packets are encapsulated with an outer header containing node IPs. The physical network only sees traffic between nodes. The inner pod IPs are hidden.</span></p><p><strong>Cross-node with BGP routing</strong><span>: Packets are sent directly with pod IPs intact. The physical network must have routes for pod CIDRs. No encapsulation overhead.</span></p><p>Part 3 will trace north-south traffic: a packet from an external user through a LoadBalancer service, into the cluster, and back. This is where iptables and kube-proxy become central to the packet’s journey.</p></div></div></div>
  </body>
</html>
