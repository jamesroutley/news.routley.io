<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/klara-research/klarity">Original</a>
    <h1>Show HN: Klarity ‚Äì Open-source tool to analyze uncertainty/entropy in LLM output</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/klara-research/klarity/blob/main/assets/klaralabs.png"><img src="https://github.com/klara-research/klarity/raw/main/assets/klaralabs.png" alt="Klara Labs" width="200"/></a></p></div>

<p dir="auto">Klarity is a tool for analyzing uncertainty in generative model outputs. It combines both raw probability analysis and semantic understanding to provide deep insights into model behavior during text generation. The library offers:</p>
<ul dir="auto">
<li><strong>Dual Entropy Analysis</strong>: Combines raw probability entropy with semantic similarity-based entropy</li>
<li><strong>Semantic Clustering</strong>: Groups similar predictions to understand model decision-making</li>
<li><strong>Structured Output</strong>: Returns detailed JSON analysis of generation patterns</li>
<li><strong>AI-powered Analysis</strong>: Uses a separate model to analyze generation patterns and provide human-readable insights</li>
</ul>


<p dir="auto">Install directly from GitHub:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install git+https://github.com/klara-research/klarity.git"><pre>pip install git+https://github.com/klara-research/klarity.git</pre></div>
<p dir="auto">Basic usage example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList
from klarity import UncertaintyEstimator
from klarity.core.analyzer import EntropyAnalyzer

# Initialize your model
model_name = &#34;Qwen/Qwen2.5-7B&#34;
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Initialize your insight model
insight_model_name = &#34;Qwen/Qwen2.5-7B-Instruct&#34;  # You can choose any model
insight_model = AutoModelForCausalLM.from_pretrained(insight_model_name)
insight_tokenizer = AutoTokenizer.from_pretrained(insight_model_name)

# Create estimator
estimator = UncertaintyEstimator(
    top_k=100,
    analyzer=EntropyAnalyzer(
        min_token_prob=0.01,
        insight_model=insight_model,
        insight_tokenizer=insight_tokenizer
    )
)
uncertainty_processor = estimator.get_logits_processor()

# Set up generation
prompt = &#34;Your prompt&#34;
inputs = tokenizer(prompt, return_tensors=&#34;pt&#34;)

# Generate with uncertainty analysis
generation_output = model.generate(
    **inputs,
    max_new_tokens=20,
    temperature=0.7,
    top_p=0.9,
    logits_processor=LogitsProcessorList([uncertainty_processor]),
    return_dict_in_generate=True,
    output_scores=True,
)

# Analyze the generation
result = estimator.analyze_generation(
    generation_output,
    tokenizer,
    uncertainty_processor
)

generated_text = tokenizer.decode(generation_output.sequences[0], skip_special_tokens=True)

# Inspect results
print(f&#34;\nPrompt: {prompt}&#34;)
print(f&#34;Generated text: {generated_text}&#34;)

print(&#34;\nDetailed Token Analysis:&#34;)
for idx, metrics in enumerate(result.token_metrics):
    print(f&#34;\nStep {idx}:&#34;)
    print(f&#34;Raw entropy: {metrics.raw_entropy:.4f}&#34;)
    print(f&#34;Semantic entropy: {metrics.semantic_entropy:.4f}&#34;)
    print(&#34;Top 3 predictions:&#34;)
    for i, pred in enumerate(metrics.token_predictions[:3], 1):
        print(f&#34;  {i}. {pred.token} (prob: {pred.probability:.4f})&#34;)

# Show comprehensive insight
print(&#34;\nComprehensive Analysis:&#34;)
print(result.overall_insight)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>, <span>LogitsProcessorList</span>
<span>from</span> <span>klarity</span> <span>import</span> <span>UncertaintyEstimator</span>
<span>from</span> <span>klarity</span>.<span>core</span>.<span>analyzer</span> <span>import</span> <span>EntropyAnalyzer</span>

<span># Initialize your model</span>
<span>model_name</span> <span>=</span> <span>&#34;Qwen/Qwen2.5-7B&#34;</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_name</span>)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_name</span>)

<span># Initialize your insight model</span>
<span>insight_model_name</span> <span>=</span> <span>&#34;Qwen/Qwen2.5-7B-Instruct&#34;</span>  <span># You can choose any model</span>
<span>insight_model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>insight_model_name</span>)
<span>insight_tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>insight_model_name</span>)

<span># Create estimator</span>
<span>estimator</span> <span>=</span> <span>UncertaintyEstimator</span>(
    <span>top_k</span><span>=</span><span>100</span>,
    <span>analyzer</span><span>=</span><span>EntropyAnalyzer</span>(
        <span>min_token_prob</span><span>=</span><span>0.01</span>,
        <span>insight_model</span><span>=</span><span>insight_model</span>,
        <span>insight_tokenizer</span><span>=</span><span>insight_tokenizer</span>
    )
)
<span>uncertainty_processor</span> <span>=</span> <span>estimator</span>.<span>get_logits_processor</span>()

<span># Set up generation</span>
<span>prompt</span> <span>=</span> <span>&#34;Your prompt&#34;</span>
<span>inputs</span> <span>=</span> <span>tokenizer</span>(<span>prompt</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>)

<span># Generate with uncertainty analysis</span>
<span>generation_output</span> <span>=</span> <span>model</span>.<span>generate</span>(
    <span>**</span><span>inputs</span>,
    <span>max_new_tokens</span><span>=</span><span>20</span>,
    <span>temperature</span><span>=</span><span>0.7</span>,
    <span>top_p</span><span>=</span><span>0.9</span>,
    <span>logits_processor</span><span>=</span><span>LogitsProcessorList</span>([<span>uncertainty_processor</span>]),
    <span>return_dict_in_generate</span><span>=</span><span>True</span>,
    <span>output_scores</span><span>=</span><span>True</span>,
)

<span># Analyze the generation</span>
<span>result</span> <span>=</span> <span>estimator</span>.<span>analyze_generation</span>(
    <span>generation_output</span>,
    <span>tokenizer</span>,
    <span>uncertainty_processor</span>
)

<span>generated_text</span> <span>=</span> <span>tokenizer</span>.<span>decode</span>(<span>generation_output</span>.<span>sequences</span>[<span>0</span>], <span>skip_special_tokens</span><span>=</span><span>True</span>)

<span># Inspect results</span>
<span>print</span>(<span>f&#34;<span>\n</span>Prompt: <span><span>{</span><span>prompt</span><span>}</span></span>&#34;</span>)
<span>print</span>(<span>f&#34;Generated text: <span><span>{</span><span>generated_text</span><span>}</span></span>&#34;</span>)

<span>print</span>(<span>&#34;<span>\n</span>Detailed Token Analysis:&#34;</span>)
<span>for</span> <span>idx</span>, <span>metrics</span> <span>in</span> <span>enumerate</span>(<span>result</span>.<span>token_metrics</span>):
    <span>print</span>(<span>f&#34;<span>\n</span>Step <span><span>{</span><span>idx</span><span>}</span></span>:&#34;</span>)
    <span>print</span>(<span>f&#34;Raw entropy: <span><span>{</span><span>metrics</span>.<span>raw_entropy</span>:.4f<span>}</span></span>&#34;</span>)
    <span>print</span>(<span>f&#34;Semantic entropy: <span><span>{</span><span>metrics</span>.<span>semantic_entropy</span>:.4f<span>}</span></span>&#34;</span>)
    <span>print</span>(<span>&#34;Top 3 predictions:&#34;</span>)
    <span>for</span> <span>i</span>, <span>pred</span> <span>in</span> <span>enumerate</span>(<span>metrics</span>.<span>token_predictions</span>[:<span>3</span>], <span>1</span>):
        <span>print</span>(<span>f&#34;  <span><span>{</span><span>i</span><span>}</span></span>. <span><span>{</span><span>pred</span>.<span>token</span><span>}</span></span> (prob: <span><span>{</span><span>pred</span>.<span>probability</span>:.4f<span>}</span></span>)&#34;</span>)

<span># Show comprehensive insight</span>
<span>print</span>(<span>&#34;<span>\n</span>Comprehensive Analysis:&#34;</span>)
<span>print</span>(<span>result</span>.<span>overall_insight</span>)</pre></div>

<p dir="auto">Klarity provides a structured JSON analysis for each generation, the insight quality depends on the model that you choose.</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &#34;uncertainty_points&#34;: [
        {
            &#34;step&#34;: int,          # Generation step number
            &#34;entropy&#34;: float,     # Entropy value at this step
            &#34;options&#34;: string[],  # Top competing options
            &#34;type&#34;: string       # Type of uncertainty
        }
    ],
    &#34;high_confidence&#34;: [
        {
            &#34;step&#34;: int,          # Generation step number
            &#34;probability&#34;: float, # Token probability
            &#34;token&#34;: string,     # Chosen token
            &#34;context&#34;: string    # Contextual information
        }
    ],
    &#34;risk_areas&#34;: [
        {
            &#34;type&#34;: string,      # Type of risk identified
            &#34;steps&#34;: int[],      # Affected steps
            &#34;reasoning&#34;: string  # Explanation of the risk
        }
    ],
    &#34;suggestions&#34;: [
        {
            &#34;issue&#34;: string,     # Identified issue
            &#34;improvement&#34;: string # Suggested improvement
        }
    ]
}"><pre>{
    <span>&#34;uncertainty_points&#34;</span>: [
        {
            <span>&#34;step&#34;</span>: <span>int</span>,          <span># Generation step number</span>
            <span>&#34;entropy&#34;</span>: <span>float</span>,     <span># Entropy value at this step</span>
            <span>&#34;options&#34;</span>: <span>string</span>[<span></span>],  <span># Top competing options</span>
            <span>&#34;type&#34;</span>: <span>string</span>       <span># Type of uncertainty</span>
        }
    ],
    <span>&#34;high_confidence&#34;</span>: [
        {
            <span>&#34;step&#34;</span>: <span>int</span>,          <span># Generation step number</span>
            <span>&#34;probability&#34;</span>: <span>float</span>, <span># Token probability</span>
            <span>&#34;token&#34;</span>: <span>string</span>,     <span># Chosen token</span>
            <span>&#34;context&#34;</span>: <span>string</span>    <span># Contextual information</span>
        }
    ],
    <span>&#34;risk_areas&#34;</span>: [
        {
            <span>&#34;type&#34;</span>: <span>string</span>,      <span># Type of risk identified</span>
            <span>&#34;steps&#34;</span>: <span>int</span>[<span></span>],      <span># Affected steps</span>
            <span>&#34;reasoning&#34;</span>: <span>string</span>  <span># Explanation of the risk</span>
        }
    ],
    <span>&#34;suggestions&#34;</span>: [
        {
            <span>&#34;issue&#34;</span>: <span>string</span>,     <span># Identified issue</span>
            <span>&#34;improvement&#34;</span>: <span>string</span> <span># Suggested improvement</span>
        }
    ]
}</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">ü§ñ Supported Frameworks &amp; Models</h2><a id="user-content--supported-frameworks--models" aria-label="Permalink: ü§ñ Supported Frameworks &amp; Models" href="#-supported-frameworks--models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<p dir="auto">Currently supported:</p>
<ul dir="auto">
<li>‚úÖ Hugging Face Transformers</li>
</ul>
<p dir="auto">Planned support:</p>
<ul dir="auto">
<li>‚è≥ PyTorch</li>
</ul>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Type</th>
<th>Status</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen2.5-0.5B</td>
<td>Base</td>
<td>‚úÖ Tested</td>
<td>Full Support</td>
</tr>
<tr>
<td>Qwen2.5-0.5B-Instruct</td>
<td>Instruct</td>
<td>‚úÖ Tested</td>
<td>Full Support</td>
</tr>
<tr>
<td>Qwen2.5-7B</td>
<td>Base</td>
<td>‚úÖ Tested</td>
<td>Full Support</td>
</tr>
<tr>
<td>Qwen2.5-7B-Instruct</td>
<td>Instruct</td>
<td>‚úÖ Tested</td>
<td>Full Support</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<div dir="auto"><h3 tabindex="-1" dir="auto">Custom Analysis Configuration</h3><a id="user-content-custom-analysis-configuration" aria-label="Permalink: Custom Analysis Configuration" href="#custom-analysis-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">You can customize the analysis parameters:</p>
<div dir="auto" data-snippet-clipboard-copy-content="analyzer = EntropyAnalyzer(
    min_token_prob=0.01,  # Minimum probability threshold
    semantic_similarity_threshold=0.8  # Threshold for semantic grouping
)"><pre><span>analyzer</span> <span>=</span> <span>EntropyAnalyzer</span>(
    <span>min_token_prob</span><span>=</span><span>0.01</span>,  <span># Minimum probability threshold</span>
    <span>semantic_similarity_threshold</span><span>=</span><span>0.8</span>  <span># Threshold for semantic grouping</span>
)</pre></div>

<p dir="auto">Contributions are welcome! Areas we&#39;re looking to improve:</p>
<ul dir="auto">
<li>Additional framework support</li>
<li>More tested models</li>
<li>Enhanced semantic analysis</li>
<li>Additional analysis metrics</li>
<li>Documentation and examples</li>
</ul>
<p dir="auto">Please see our <a href="https://github.com/klara-research/klarity/blob/main/CONTRIBUTING.md">Contributing Guide</a> for details.</p>

<p dir="auto">MIT License. See <a href="https://github.com/klara-research/klarity/blob/main/LICENSE">LICENSE</a> for more information.</p>

<ul dir="auto">
<li><a href="https://klaralabs.com" rel="nofollow">Website</a></li>
<li><a href="https://github.com/klara-research/klarity/issues">GitHub Issues</a> for bugs and features</li>
<li><a href="https://github.com/klara-research/klarity/discussions">GitHub Discussions</a> for questions</li>
</ul>
</article></div></div>
  </body>
</html>
