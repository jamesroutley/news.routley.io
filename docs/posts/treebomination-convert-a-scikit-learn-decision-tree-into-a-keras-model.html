<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Dobiasd/treebomination">Original</a>
    <h1>Treebomination: Convert a scikit-learn decision tree into a Keras model</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Dobiasd/treebomination/blob/main/treebomination.jpg"><img src="https://github.com/Dobiasd/treebomination/raw/main/treebomination.jpg" alt="logo"/></a></p>
<p dir="auto"><a href="https://github.com/Dobiasd/treebomination/actions"><img src="https://github.com/Dobiasd/treebomination/workflows/ci/badge.svg" alt="CI"/></a>
<a href="https://github.com/Dobiasd/treebomination/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/2a83e26b0ab480b4de7de3976dd042b5ba623e0009a14132bbefb3d9bbb44079/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d4954253230312e302d626c75652e737667" alt="(License MIT 1.0)" data-canonical-src="https://img.shields.io/badge/license-MIT%201.0-blue.svg"/></a></p>
<p dir="auto"><strong>Disclaimer: This is just a fun experiment, I conducted for my own curiosity and entertainment. It&#39;s not intended to be useful for anything else.</strong></p>

<p dir="auto">Treebomination is a way to convert a <code>sklearn.tree.DecisionTreeRegressor</code> into a (roughly) equivalent <code>tf.keras.Model</code>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-when-is-this-helpful" aria-hidden="true" href="#when-is-this-helpful"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>When is this helpful?</h2>
<ul dir="auto">
<li>You irrationally dislike decision trees (e.g., for their stepwise behavior) and feel neural networks (with their smoothness) are much cooler. <g-emoji alias="zany_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f92a.png">ğŸ¤ª</g-emoji></li>
<li>You want to prove a point about neural networks. <g-emoji alias="man_teacher" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f3eb.png">ğŸ‘¨â€ğŸ«</g-emoji></li>
<li>You think that converting the tree to a NN and then fine-tuning it might decrease the value of your less metric on your test set. <g-emoji alias="magic_wand" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1fa84.png">ğŸª„</g-emoji></li>
<li>You have a well-working decision tree but want to only use TensorFlow or <a href="https://github.com/Dobiasd/frugally-deep">frugally-deep</a> in production. <g-emoji alias="floppy_disk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png">ğŸ’¾</g-emoji></li>
<li>You want to back up the claims of your marketing department about your team using &#34;AI&#34;. <g-emoji alias="man_office_worker" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bc.png">ğŸ‘¨â€ğŸ’¼</g-emoji></li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-when-is-not-useful" aria-hidden="true" href="#when-is-not-useful"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>When is not useful?</h2>
<ul dir="auto">
<li>You care about the performance of your predictions. <g-emoji alias="snail" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40c.png">ğŸŒ</g-emoji></li>
<li>You care about the precision of your results. <g-emoji alias="bow_and_arrow" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3f9.png">ğŸ¹</g-emoji></li>
<li>You care about the size of your final application. <g-emoji alias="rhinoceros" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f98f.png">ğŸ¦</g-emoji></li>
</ul>
<p dir="auto">So, <strong>it is highly recommended to not actually use this for anything serious</strong>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage" aria-hidden="true" href="#usage"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage</h2>
<div dir="auto" data-snippet-clipboard-copy-content="from sklearn.tree import DecisionTreeRegressor
from treebomination import treebominate

my_decision_tree_regressor = DecisionTreeRegressor()
# ... training ...
model = treebominate(my_decision_tree_regressor)"><pre>from sklearn.tree import DecisionTreeRegressor
from treebomination import treebominate

my_decision_tree_regressor = <span>DecisionTreeRegressor</span>()
<span><span>#</span> ... training ...</span>
model = treebominate(my_decision_tree_regressor)</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-origin-story" aria-hidden="true" href="#origin-story"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Origin story</h2>
<p dir="auto">From some unbridled thoughts:</p>
<ul dir="auto">
<li>A Decision tree is a fancy way of having nested <code>if</code> statements.</li>
<li>A simple logistic regression on a one-dimensional input acts like a fuzzy threshold (or an <code>if</code> statement).</li>
<li>A neuron in an artificial neural network acts can act as a single logistic regression node.</li>
</ul>
<p dir="auto">The following idea arose: There should be a morphism from binary decision trees to neural networks,
it should<g-emoji alias="tm" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2122.png">â„¢ï¸</g-emoji> be possible to emulate every decision tree with a neural network,
i.e., derive the network architecture from the tree and initialize the weights and biases
such that the output of the network is similar to the output of the tree.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-structure-of-the-generated-neural-networks" aria-hidden="true" href="#structure-of-the-generated-neural-networks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Structure of the generated neural networks</h2>
<p dir="auto">There might be much more intelligent ways to &#34;encode&#34; a decision tree as a neural network,
but treebomination uses the following approach.</p>
<p dir="auto">Each decision node from the tree is simulated by two neurons
(each one represented as a dense layer with a singleton shape).
The threshold-ish behavior results from the neuron having a very high input weight (steep and &#34;sudden&#34; sigmoid)
and the bias chosen such that the &#34;middle&#34; of the sigmoid falls into the (scaled) threshold value.
The output values (booleans, encoded as fuzzy 0 and 1) signal if this path of the tree is taken.
For subsequent neurons, this incoming signal is multiplied onto their output value, such that
not-taken paths are silenced for further output.
The final &#34;leaf&#34; neurons use linear activation, have a bias of <code>0</code>,
and their initial weight set to the intended output value.
Since only one of these final neurons gets an input signal, their outputs can be combined by summing them up.</p>
<p dir="auto">Initializing the neural network this way makes it output (almost) the exact same predictions as the tree does.</p>
<p dir="auto">Even a very simple (<code>max_depth=3</code>) <code>DecisionTreeRegressor</code> like the following:</p>
<div data-snippet-clipboard-copy-content="|--- feature_3 &lt;= 7.50
|   |--- feature_3 &lt;= 6.50
|   |   |--- feature_15 &lt;= 1131.50
|   |   |   |--- value: [115593.60]
|   |   |--- feature_15 &gt;  1131.50
|   |   |   |--- value: [149818.43]
|   |--- feature_3 &gt;  6.50
|   |   |--- feature_15 &lt;= 2093.50
|   |   |   |--- value: [197758.96]
|   |   |--- feature_15 &gt;  2093.50
|   |   |   |--- value: [284680.23]
|--- feature_3 &gt;  7.50
|   |--- feature_3 &lt;= 8.50
|   |   |--- feature_15 &lt;= 1928.00
|   |   |   |--- value: [250284.08]
|   |   |--- feature_15 &gt;  1928.00
|   |   |   |--- value: [314964.80]
|   |--- feature_3 &gt;  8.50
|   |   |--- feature_31 &lt;= 517.50
|   |   |   |--- value: [372716.17]
|   |   |--- feature_31 &gt;  517.50
|   |   |   |--- value: [745000.00]"><pre><code>|--- feature_3 &lt;= 7.50
|   |--- feature_3 &lt;= 6.50
|   |   |--- feature_15 &lt;= 1131.50
|   |   |   |--- value: [115593.60]
|   |   |--- feature_15 &gt;  1131.50
|   |   |   |--- value: [149818.43]
|   |--- feature_3 &gt;  6.50
|   |   |--- feature_15 &lt;= 2093.50
|   |   |   |--- value: [197758.96]
|   |   |--- feature_15 &gt;  2093.50
|   |   |   |--- value: [284680.23]
|--- feature_3 &gt;  7.50
|   |--- feature_3 &lt;= 8.50
|   |   |--- feature_15 &lt;= 1928.00
|   |   |   |--- value: [250284.08]
|   |   |--- feature_15 &gt;  1928.00
|   |   |   |--- value: [314964.80]
|   |--- feature_3 &gt;  8.50
|   |   |--- feature_31 &lt;= 517.50
|   |   |   |--- value: [372716.17]
|   |   |--- feature_31 &gt;  517.50
|   |   |   |--- value: [745000.00]
</code></pre></div>
<p dir="auto">results in a ridiculously complex neural-network architecture.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Dobiasd/treebomination/blob/main/model.png"><img src="https://github.com/Dobiasd/treebomination/raw/main/model.png" alt="model"/></a></p>
<p dir="auto">In reality, trees are often much deeper than that, which not only results in a very large (and slow) model,
but also the precision of the results suffers.</p>
<p dir="auto">But hey, at least in this toy example (trained on the numerical features from
the <a href="https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques" rel="nofollow">Kaggle competition &#34;House Prices - Advanced Regression Techniques&#34;</a>,
see <a href="https://github.com/Dobiasd/treebomination/blob/main/treebomination/tests.py">tests</a>)
the R2 score of the NN (<code>0.766</code>), is slightly higher than the one of the tree (<code>0.765</code>).
With a quick re-training on the same data, it even improves a bit more (to <code>0.770</code>). <g-emoji alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png">ğŸ‰</g-emoji></p>
</article>
          </div></div>
  </body>
</html>
