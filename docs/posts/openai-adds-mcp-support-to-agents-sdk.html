<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openai.github.io/openai-agents-python/mcp/">Original</a>
    <h1>OpenAI adds MCP support to Agents SDK</h1>
    
    <div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div>
          
            
              
              
            
            
              
              
            
          
          
            <div data-md-component="content">
              <article>
                
                  




<p>The <a href="https://modelcontextprotocol.io/introduction">Model context protocol</a> (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:</p>
<blockquote>
<p>MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.</p>
</blockquote>
<p>The Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.</p>
<h2 id="mcp-servers">MCP servers</h2>
<p>Currently, the MCP spec defines two kinds of servers, based on the transport mechanism they use:</p>
<ol>
<li><strong>stdio</strong> servers run as a subprocess of your application. You can think of them as running &#34;locally&#34;.</li>
<li><strong>HTTP over SSE</strong> servers run remotely. You connect to them via a URL.</li>
</ol>
<p>You can use the <a title="MCPServerStdio" href="https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerStdio"><code>MCPServerStdio</code></a> and <a title="MCPServerSse" href="https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerSse"><code>MCPServerSse</code></a> classes to connect to these servers.</p>
<p>For example, this is how you&#39;d use the <a href="https://www.npmjs.com/package/@modelcontextprotocol/server-filesystem">official MCP filesystem server</a>.</p>
<div><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span>async</span> <span>with</span> <span>MCPServerStdio</span><span>(</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span>params</span><span>=</span><span>{</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>        <span>&#34;command&#34;</span><span>:</span> <span>&#34;npx&#34;</span><span>,</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>        <span>&#34;args&#34;</span><span>:</span> <span>[</span><span>&#34;-y&#34;</span><span>,</span> <span>&#34;@modelcontextprotocol/server-filesystem&#34;</span><span>,</span> <span>samples_dir</span><span>],</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span>}</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span>)</span> <span>as</span> <span>server</span><span>:</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span>tools</span> <span>=</span> <span>await</span> <span>server</span><span>.</span><span>list_tools</span><span>()</span>
</span></code></pre></div>
<h2 id="using-mcp-servers">Using MCP servers</h2>
<p>MCP servers can be added to Agents. The Agents SDK will call <code>list_tools()</code> on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server&#39;s tools. When the LLM calls a tool from an MCP server, the SDK calls <code>call_tool()</code> on that server.</p>
<div><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span>agent</span><span>=</span><span>Agent</span><span>(</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>    <span>name</span><span>=</span><span>&#34;Assistant&#34;</span><span>,</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>    <span>instructions</span><span>=</span><span>&#34;Use the tools to achieve the task&#34;</span><span>,</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>    <span>mcp_servers</span><span>=</span><span>[</span><span>mcp_server_1</span><span>,</span> <span>mcp_server_2</span><span>]</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span>)</span>
</span></code></pre></div>
<h2 id="caching">Caching</h2>
<p>Every time an Agent runs, it calls <code>list_tools()</code> on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass <code>cache_tools_list=True</code> to both <a title="MCPServerStdio" href="https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerStdio"><code>MCPServerStdio</code></a> and <a title="MCPServerSse" href="https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerSse"><code>MCPServerSse</code></a>. You should only do this if you&#39;re certain the tool list will not change.</p>
<p>If you want to invalidate the cache, you can call <code>invalidate_tools_cache()</code> on the servers.</p>
<h2 id="end-to-end-examples">End-to-end examples</h2>
<p>View complete working examples at <a href="https://github.com/openai/openai-agents-python/tree/main/examples/mcp">examples/mcp</a>.</p>
<h2 id="tracing">Tracing</h2>
<p><a href="https://openai.github.io/openai-agents-python/tracing/">Tracing</a> automatically captures MCP operations, including:</p>
<ol>
<li>Calls to the MCP server to list tools</li>
<li>MCP-related info on function calls</li>
</ol>
<p><img alt="MCP Tracing Screenshot" src="https://openai.github.io/openai-agents-python/assets/images/mcp-tracing.jpg"/></p>












                
              </article>
            </div>
          
          

        </div>
        
      </main>
      
        
      
    </div></div>
  </body>
</html>
