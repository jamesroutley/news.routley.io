<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.moderndescartes.com/essays/claude_pipeline">Original</a>
    <h1>Claude as Pipeline Orchestrator</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
	

<p> Originally posted 2025-08-23</p>
<p> Tagged: <a href="https://b.43z.one/essays/tags/software_engineering">software engineering</a>, <a href="https://b.43z.one/essays/tags/llms">llms</a></p>
<p> <em>Obligatory disclaimer: all opinions are mine and not of my employer </em></p>
<hr/>

<p>I’ve been using LLMs to scrape PDFs of chemistry exams while
retaining chemical equation formatting, images, and test structure. This
has been a far harder task than I anticipated, and I’ve run into many
limitations with the current generation of multimodal LLMs.</p>
<p>I wrote a <a href="https://b.43z.one/essays/blind_llms/">separate essay</a> on these
limitations, but in this essay I want to talk about pipeline
orchestration and design. I started with traditional software-based
pipeline, but due the limitations mentioned above, I started drowning in
fault-tolerance complications – retries, progress saving, resumption,
error-correction, etc.. I tried using Claude as a pipeline orchestrator
and was very pleasantly surprised at how well it worked.</p>
<p>I believe that wrapping your subroutines in MCP servers and then
using Claude as your orchestrator should be a strong default option for
writing pipelines. When you work this way, you get a pipeline with
<strong>rich ad-hoc logging/debugging/recovery features</strong> for
free.</p>
<h2 id="struggles-of-a-traditional-software-pipeline">Struggles of a
traditional software pipeline</h2>
<p>Heavily simplified, here’s what my traditional software pipeline
looked like.</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span># orchestrator.py</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span>def</span> process_file(pdf_path):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span># A first pass into markdown makes subsequent steps much more reliable.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span># This step emits image tags inline with text, preserving positional semantics.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span># It also generates a description of each image ID.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    markdown_txt <span>=</span> transcribe_pdf(pdf_path)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    parsed_problems: <span>list</span>[Problem] <span>=</span> []</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    current_problem: <span>int</span> <span>=</span> <span>0</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span># LLM can only reliably parse a few problems at a time</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span># They also need context reminders for what problem they&#39;re up to</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span>while</span> (problem_batch <span>:=</span> parse_problems(markdown_txt, current_problem, num_to_process<span>=</span><span>5</span>)):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        parsed_problems.extend(problem_batch)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span># What happens in the LLM skips a problem? Or it just decides to</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span># re-parse the same problems repeatedly, despite being told it&#39;s on #17 now?</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        current_problem <span>=</span> <span>max</span>(current_problem, <span>*</span>(p.number <span>for</span> p <span>in</span> problem_batch))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    image_manifest: <span>dict</span>[<span>str</span>, <span>str</span>] <span>=</span> []</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span>for</span> p <span>in</span> parsed_problems:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        image_manifest.update(<span>**</span>p.images)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span># At first, I tried getting the LLM to tell me what page the image was on.</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span># It just hallucinated page numbers. </span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span># So instead, I loop over pages, asking the LLM to play image search.</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    images: <span>list</span>[PIL.Image] <span>=</span> []</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span>for</span> page <span>in</span> pdf:</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        images.extend(image_search(page, image_manifest))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    image_id_map: <span>dict</span>[<span>str</span>, <span>str</span>] <span>=</span> upload_all_images(images)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    parsed_problems <span>=</span> [replace_image_references(p, image_id_map) <span>for</span> p <span>in</span> parsed_problems]</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span>return</span> parsed_problems</span></code></pre></div>
<p>This is a pretty typical processing pipeline. It was difficult to
debug for many reasons.</p>
<ul>
<li>LLMs hard fail all the time – whether it’s because of not emitting
parseable JSON, text escaping/encoding errors, my OpenRouter credits
running out, output token limit reached, rate limit reached, etc..</li>
<li>LLMs also soft fail – for example, not respecting the
“current_problem_number” hint.</li>
<li>All of my subroutines – <code>transcribe_pdf</code>,
<code>parse_problems</code>, <code>image_search</code> – were themselves
LLM calls with tool calling/structured responses to do work. I had to
co-design the pipeline to accept and return operational metadata. For
example, <code>parse_problems</code> parsed problems, but it also had a
way to inject the <code>current_problem_number</code> into the system
prompt. to say “Please parse up to 5 problems, starting from problem
{current_problem}. If there are no more problems to parse, set
<code>problems_remaining=False</code>” and return an empty list.”</li>
</ul>
<p>I started to build in idempotency, resumability, progress-saving
utilities, etc. etc., but in the end, it was too annoying to iterate on
this pipeline.</p>
<h2 id="claude-orchestrated-version">Claude-orchestrated version</h2>
<p>First, I extracted the step that transcribed the PDF into
markdown.</p>
<p>Then, I drastically simplified the parse_problems code. Now, it
basically consists of the Pydantic spec and documentation for what a
single problem should look like. No manual batching, no operational data
channels, no LLM calls. (Claude calls me, rather than me calling the
LLM.)</p>
<p>Finally, I added an MCP server to the <code>claude</code> CLI tool
exposing the <code>register_problem</code> tool. I wrote the following
plaintext instructions:</p>
<pre><code>Upload the problems contained in the given file by using the
create_problem_source, register_problem MCP tools, in that order.

# Step 1: Create Problem Source

This step registers a problem source, so that all uploaded
problems can be grouped together in the database.

# Step 2: Register problems

This step uploads the problems to the database. 

... detailed instructions on how to transcribe problems,
instructions on how to use LaTeX and mhchem to transcribe
chemical reactions, transcribe [[image_001]] tags verbatim...</code></pre>
<p>Then, I opened up the <code>claude</code> CLI and said, “please
process <span data-cites="markdown_file">@markdown_file</span> according to <span data-cites="instructions">@instructions</span>”.</p>
<p>And it basically just worked! Claude seamlessly figured out how many
problems were in the markdown file, that it should create a TODO list to
track progress, and then it chugged away until it was done.</p>
<p>On top of that, I now had the following benefits</p>
<ul>
<li>Pausability (just hit escape)</li>
<li>A persistent session, so that if the processing stopped for whatever
reason, I could literally just tell Claude, “hey I just topped off my
openrouter credits, resume from problem 17”, and it would just pick up
at the correct location.</li>
<li>A text interface debugger, so that I could literally ask Claude to
print out various things to help me debug where something was
breaking.</li>
<li>Status updates on what exactly Claude was working on at any given
moment</li>
<li>A built-in debugging interface to inspect the specific MCP calls
being made, so that I could hunt down where exactly a double-escaping
issue got introduced.</li>
<li>A text interface to tell Claude, “hey, just upload the first 10
problems” for quick debugging/iteration.</li>
<li>My LLM costs are folded into my Claude Pro subscription. 😈</li>
</ul>
<h3 id="agent-vs.-claude">“Agent” vs. “Claude”</h3>
<p>I’ve mentioned Claude by name several times here, instead of writing
“agent”. This is because most of these benefits are not actually due to
the agentic nature of Claude. Instead, it’s the utility features that
the Claude CLI comes with: automatic logging, persistent sessions, an
interactive CLI to communicate with Claude, and so on.</p>
<p>To Anthropic’s credit, Claude also seems generally good at the
agentic orchestration thing, on top of all the developer experience
niceness. I haven’t tried the equivalent Gemini/OpenAI interfaces, so
they may be just as good.</p>
<h2 id="you-should-never-write-a-software-pipeline.-ever.-again.">You
should never write a software pipeline. Ever. Again.</h2>
<p>Okay, that’s a bit of an overstatement.</p>
<p>But seriously. Each of these
debugging/logging/concurrency/resumability features is something that a
real production pipeline needs to build at some point. Usually, you get
to that point by building the simple pipeline first. Then, you have to
manually debug it. Then, after you ship it to production, you build
debugging/logging features and fix a long tail of breakages over the
next three months as you discover new ways for the pipeline to fail.
This is easily <em>months</em> of work, and you get it for free by
letting Claude be your orchestrator.</p>
<h3 id="the-fine-print">The Fine Print</h3>
<p>Here are some gotchas:</p>
<ul>
<li>Claude cannot reliably transcribe long IDs into function calls. As a
result, images were referred to with short tags <code>image_001</code>;
problems were referred to by source number <code>problem 17</code>, and
so on.</li>
<li>Claude has a context limit of ~200K tokens.</li>
<li>Claude gives up on waiting for subroutines after 2 minutes (not yet
configurable).</li>
<li>Nondeterministic failure. The typical LLM weirdnesses.</li>
</ul>
<p>The first two issues are essentially data-plane vs. control-plane
issues. In a traditional control plane/data plane separation, the
orchestrator (control plane, often Python) issues commands that take
place on many processes/machines (often written in a faster language
than Python). You can’t pass large amounts of data through the control
plane, and you can’t do much computation either, since the control plane
would quickly become a bottleneck. So the control plane must be careful
to only touch or deal with a small fraction of the overall work. It can
shuttle UUIDs around in order to keep track of which work items are
being assigned to which machine, but it can’t do the work itself.</p>
<p>With Claude as orchestrator, you follow the same control/data plane
separation philosophy. Its “memory” is equivalent to a mere ~1MB (should
be enough for anybody, right?), and has a weird flavor of compute
limitation (IDs and other “exact” values have to be simple enough for an
LLM to transcribe).</p>
<h2 id="conclusion">Conclusion</h2>
<p>For my PDF transcription task, the task was small enough that I could
basically merge the data and control planes - the same Claude agent was
responsible for handling the parsing and reformatting of all PDF
contents, as well as making all the tool calls to render the entries to
my database. This made my pipeline orchestration experience particularly
pleasant.</p>
<p>To get around these scaling limitations, I can think of two obvious
tactics. One is Claude’s subagents feature, which can help with the
context window limitations. The second is having Claude just open up an
interactive Python shell and run Python commands (e.g. it would run your
pipeline by calling functions in a persistent interpreter, rather than
by invoking MCP servers). This way, it could use Python variables
instead of transcribing UUID, and its effective memory would be
gigabytes, not single-digit megabytes in size. I’m reading through Armin
Ronacher’s <a href="https://lucumr.pocoo.org/2025/8/18/code-mcps/">latest blog post on
replacing MCPs with code</a> with great interest.</p>
<p>In the very, very near future, somebody will likely figure out how to
tie together Claude and a Python interpreter in a way that will complete
the vision of Claude as orchestrator, without the context window/copy
accuracy limitations.</p>


    </div>
</div></div>
  </body>
</html>
