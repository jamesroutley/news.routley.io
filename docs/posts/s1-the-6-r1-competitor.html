<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://timkellogg.me/blog/2025/02/03/s1">Original</a>
    <h1>S1: The $6 R1 Competitor?</h1>
    
    <div id="readability-page-1" class="page"><div>
        <main>
            <article id="post">
	
	<p id="time">
		<time pubdate="true">
			Mon February 03, 2025
		</time>
	</p>
	
		<img src="https://cdn.pixabay.com/photo/2024/08/13/16/50/ai-generated-8966531_960_720.png" alt="S1: The $6 R1 Competitor?" id="post-image"/>
	
	<p>A new paper <a href="https://arxiv.org/abs/2501.19393">released on Friday</a> is making waves in the AI community, not because of the model 
it describes, but because it shows how close we are to some very large breakthroughs in AI. The model
is just below state of the art, but it can run on my laptop. More important, it sheds light on how all
this stuff works, and it’s <strong>not complicated</strong>.</p>


<p>OpenAI were the first to claim the inference-time scaling laws. Basically, an LLM can get higher performance
if it can “think” longer before answering. But, like, <strong>how do you do it?</strong> How do you make it think longer?</p>

<p>OpenAI and R1 had cool graphs showing performance scaling with average thinking time (this from
the s1 paper):</p>

<p><img src="https://timkellogg.me/images/s1-inference-scaling.jpg" alt=""/></p>

<p>But <strong>how</strong> do they control the length of an LLM response? Everyone skipped over that part, but s1
shows us details, <em>and it is fun</em>.</p>

<p><em>Context: When an LLM “thinks” at inference time, it puts it’s thoughts inside <code>&lt;think&gt;</code> and 
<code>&lt;/think&gt;</code> XML tags. Once it gets past the end tag the model is taught to change voice into a confident
and authoritative tone for the final answer.</em></p>

<p>In s1, when the LLM tries to stop thinking with <code>&#34;&lt;/think&gt;&#34;</code>, they force it to keep going
by replacing it with <code>&#34;Wait&#34;</code>. It’ll then begin to second guess and double check it’s answer. They
do this to trim or extend thinking time (trimming is just abruptly inserting <code>&#34;&lt;/think&gt;&#34;</code>).</p>

<p>It’s really dumb, I love it. <a href="https://bsky.app/profile/r.whal.ing/post/3lheatlmonk26">It feels like the kind of hack I would try</a>.</p>

<p>So for <code>o3-mini-low</code> versus <code>o3-mini-high</code>, that’s likely how they do it. They probably trained 3
models, and with each with a different average thinking time (as measured during training). Eventually the 
training process begins to encode that behavior into the model weights.</p>

<h2 id="the-entropix-tie-in">The Entropix Tie In</h2>
<p>The trick is so dumb you can do it at inference time too. I’m kicking myself for not understanding
this earlier, because it’s what entropix is all about, and <a href="https://timkellogg.me/blog/2024/10/10/entropix">I wrote a lot about entropix</a>.</p>

<p>In <a href="https://github.com/xjdr-alt/entropix">entropix</a>, they look at the entropy &amp; varentropy of the logits (and attention) to change how the
tokens are selected. In fact, they used tokens like “Wait” to force the LLM to second guess itself.
Although there was more to it, they also tweaked sampler setting to make
it more creative, or to go into aggressive exploration mode, all depending on the internal state
of the model.</p>

<p>My hunch is that we’ll <strong>see more</strong> of entropix, or something directly inspired from it. Although, it’s
unclear if it’ll appear predominately in training or inference time.</p>


<p>Why did it cost only $6? Because they used a <strong>small model</strong> and hardly any data.</p>

<p>After sifting their dataset of 56K examples down to just the best 1K, they found that the core 1K 
is all that’s needed to achieve o1-preview performance on a 32B model. <strong>Adding data didn’t raise 
performance <em>at all</em></strong>.</p>

<p>32B is a small model, I can run that on my laptop. They used 16 NVIDIA H100s for 26 minutes per 
training run, that equates to around $6.</p>

<p>The low cost means you can do a lot of runs, and they did. As we’ll see, they heavily used a 
technique called ablation, re-running the entire training process with <strong>small variations</strong> in 
configuration to prove what works and what doesn’t.</p>

<p>For example, how did they figure out it should be “Wait” and not “Hmm”? <strong>They measured!</strong></p>

<p><img src="https://timkellogg.me/images/s1-ablations.png" alt=""/></p>

<p>They also measured properties of the training dataset, which examples provided the most signal:</p>

<p><img src="https://timkellogg.me/images/s1-diversity.png" alt=""/></p>

<p>They did a ton of these ablation experiments. This is how you <strong>make progress</strong>.</p>

<p>We like to think that
OpenAI or DeepSeek are simply packed full of brilliant people and they make a wild guess, spend
$10,000,000.00 on a training run and BAM! an innovation is created. But no, even the smartest
people make hundreds of <strong>tiny experiments</strong>.</p>

<p>Innovations like s1 that dramatically lower costs mean that researchers can learn and understand
these models faster. And that directly translates to a <strong>faster pace</strong> of AI development.</p>


<p>Again, AI is inseparable from politics, sorry.</p>

<p>There’s debate about OpenAI &amp; Anthropic’s vast funding. It’s tempting to see cost reducing 
innovations like s1 or DeepSeek V3 and assume that OpenAI &amp; Anthropic’s vast datacenters are
a waste of money. I’d argue that no, having 10,000 H100s just means that you can do 625 times
<strong>more experiments</strong> than s1 did.</p>

<p>If you believe that AI development is a prime national security advantage, then you absolutely
should want even more money poured into AI development, to make it go <strong>even faster</strong>.</p>

<h2 id="distealing">Distealing</h2>
<p>Note that this s1 dataset is <strong>distillation</strong>. Every example is a thought trace generated by 
another model, Qwen2.5,
prompted to think before answering. OpenAI has been accusing DeepSeek of creating their
<a href="https://github.com/deepseek-ai/DeepSeek-V3/tree/main">V3 model</a> by distilling from o1, which is against their terms of service. There’s still no strong public
evidence in either direction, so accusations are mostly empty, but s1 gives a lot of credence.</p>

<p>Going forward, it’ll be nearly impossible to prevent distealing (unauthorized distilling). One thousand
examples is definitely within the range of what a single person might do in normal usage, no less 
ten or a hundred people. I doubt that OpenAI has a realistic path to <strong>preventing</strong> or even <strong>detecting</strong>
distealing outside of simply not releasing models.</p>

<p>Note that OpenAI released their o3 model as
<a href="https://openai.com/index/introducing-deep-research/">deep research</a>, an agent instead of direct access to the model API. This might be a trend now,
“agents” serving as a way to avoid releasing direct access to a model.</p>


<p>S1 is important because it illustrates the current pace of AI development that’s happening in
the open. When you consider how much compute is available to the likes of OpenAI and Anthropic,
the potential true pace of AI development is <strong>mind melting</strong>.</p>

<p>S1 isn’t a replication of R1 or o1. Those were demonstrations in pure reinforcement learning (RL).
S1 shows that supervised fine tuning (SFT) shows just as much potential. That means researchers have
multiple paths to investigate for pushing forward inference-time scaling.</p>

<p>I think it’s safe to say that we’re going to see some very big things in ‘25. It’s barely February…</p>


<ul>
  <li><a href="https://bsky.app/profile/timkellogg.me/post/3lheinvmvps26">Bluesky</a></li>
  <li><a href="https://www.linkedin.com/posts/tim-kellogg-69802913_s1-the-6-r1-competitor-this-isnt-a-r1-activity-7292585670580031488-kI5n?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAKsUpQBLx0MUlgAizVDjPDC7fqIYBdcMV8">LinkedIn</a></li>
  <li><a href="https://x.com/kellogh/status/1886858322944708873?s=12">Twitter/X</a></li>
  <li><a href="https://www.threads.net/@kelloggt/post/DFqZ5nKvjx6?xmt=AQGzVA_0V-POcY93RHdY_09tY7e9NCi3R7aung-wBm7czQ">Threads</a></li>
  <li><a href="https://news.ycombinator.com/item?id=42946854">Hacker News</a></li>
</ul>


</article>



        </main>
    </div></div>
  </body>
</html>
