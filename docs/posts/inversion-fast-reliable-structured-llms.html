<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://rysana.com/inversion">Original</a>
    <h1>Inversion: Fast, Reliable Structured LLMs</h1>
    
    <div id="readability-page-1" class="page"><section><div><div><div><div><p>Today we&#39;re excited to announce <strong>Inversion</strong> - our family of structured language models designed to solve the speed, reliability, and reasoning issues in traditional AI systems.</p><div dir="ltr" data-orientation="horizontal"><div data-state="active" data-orientation="horizontal" role="tabpanel" aria-labelledby="radix-:R7ffaufla:-trigger-speed" id="radix-:R7ffaufla:-content-speed" tabindex="0"><div><p><span>Inference speed in characters per second, 33rd/66th/99th percentile across 600 extraction &amp; reasoning tests.<!-- --> <span>(<!-- -->higher<!-- --> is better)</span></span></p></div></div></div><div><p>Our first generation models are state of the art in structured tasks such as extraction and function calling while running up to <strong>100× faster</strong>, with <strong>10× lower latency</strong>, outputting <strong>100% reliable structure</strong> with <strong>10,000× less overhead</strong> than the best alternatives, and boasting the deepest support for typed JSON output available anywhere.*</p>
<p>Inversion models do more with less - they use less compute, less time, and less data to produce outputs with higher quality, reliability, and reasoning.</p>
<h2>100× faster typed LLM inference</h2>
<p>We started building what became Inversion back in February 2023, inspired by the newfound abilities of general-purpose large language models to understand systems and glue together human intent and machine action through natural language and structured data.</p>
<p>As we built products on top of these models, we found that the models were far too unreliable, expensive, and slow for production use in structured data tasks. We needed to create a new kind of model that could handle our workloads in the real world at scale.</p>
<p>The key insight is that <strong>structured inference</strong> is fundamentally accelerative - and that if we build models that can always reliably output structured data with constraints, we can massively improve both the speed and quality of the outputs.</p>
</div><div><p><span>Time-to-first-token in milliseconds, min/avg/max across 600 extraction &amp; reasoning tests.<!-- --> <span>(<!-- -->lower<!-- --> is better)</span></span></p></div><div><p>We set ourselves to the task of taking the quality of outputs from the best available LLMs for workloads like function calling - or <a href="https://rysana.com/docs/lusat">actions/workflows</a> and dynamic UI generation - down from around one minute to under 200 milliseconds, which is roughly the time it takes for a human to perceive a response as instant.</p>
<p>We also wanted to ensure that the outputs would <strong>always</strong> match the data types we expected, in our case this meant being validated against a JSON schema for function arguments or component props.</p>
<p>Such a system would unlock a cambrian explosion of new viable applications for AI, with reliable real-time feel - everything from humanoid robot assistants and game NPCs that react to complex dynamic environments, to natural language interfaces and agents that can understand and act on complex human intent in the blink of an eye.</p>
<p>This means:</p>
<ol>
<li>We needed to process schemas/grammars in nearly no time,</li>
<li>We needed to bring down time-to-first-token to nearly no time,</li>
<li>We needed to accelerate inference to over 10,000 char/s.</li>
</ol>
</div><div><p><span>Time to compile output constraints in microseconds, min/avg/max across 400 JSON schema tests. No caching, fully dynamic, identical hardware. Ratios are average of test ratios, not ratio of test averages.<!-- --> <span>(<!-- -->lower<!-- --> is better)</span></span></p></div><div><p>The first set of components we built were the systems we use to process data structures and constrain model outputs with them. We invented a new kind of compiler and physics-based projection model that achieves <strong>stricter</strong> constraints than the best comparable libraries for typed JSON generation, with around <strong>10,000× faster compilation</strong>.</p>
<p>The <strong>Inversion</strong> compiler processes a typical never-before-seen JSON schema in around 400 μs (microseconds) and samples model constraints at runtime in around 20 μs, supporting up to <strong>50,000 tokens per second</strong> inference with perfectly structured output.</p>
</div><div><p><span>Percentage of invalid output data structure across 600 extraction &amp; reasoning tests.<!-- --> <span>(<!-- -->lower<!-- --> is better)</span></span></p></div><div><p>Read more about the supported types and constraints in <a href="https://rysana.com/docs/api/json-schema">the docs</a>.</p>
<p>The developer experience of knowing you&#39;re going to get exactly the type you ask for is bliss.</p>
<p>Always-valid outputs are a game changer for structured workloads, dramatically improving the reliability and reasoning level of LLMs across most tasks. Inversion models often match or beat all other models we&#39;ve tested against, even compared to models with around 10× or 100× as many params.</p>
</div><div><p><span>Percentage of correct outputs across 600 tests, grouped by: actions/functions, extraction, and typed data generation.<!-- --> <span>(<!-- -->higher<!-- --> is better)</span></span></p></div><div><p>We&#39;re expanding access to the first generation of Inversion models shortly, and have begun building the next generation of models targeting on the order of 100,000 char/s inference.</p>
<h2>Accelerating structured inference</h2>
<p>You might be wondering - how does any of this actually work?</p>
<p>Why is Inversion so far ahead on multiple fronts that might otherwise seem at odds with each other, demanding a trade-off, like constraint and speed?</p>
<p>When we started with Inversion, it was actually slightly <em>slower</em> than similar models - barely faster than the strongest model and barely more reliable, as of early last spring. The first step was to achieve guaranteed structure on common JSON types, which is where the compiler originated.</p>
<p>Next, we leveraged the compiled structures to &#34;invert&#34; the inference process, using the constraint of the output to dynamically scale up and down the amount of compute required to produce each token.</p>
<p>You can think of this as switching on and off large clusters of individual neurons in the model based on the position within the output structure, instead of only modifying token sampling. Mathematically, this is a <em>projection</em> from the full model onto a smaller model with undesired sublayers pruned away.</p>
</div><div><p><span>Throughput in characters per second across tests in Inversion models. Third party models to compare in grey.<!-- --> <span>(<!-- -->higher<!-- --> is better)</span></span></p></div><div><p>We built a simple version of this last summer that gained a ~10× inference speed boost, but to achieve the current level of performance we had to completely rewrite our inference and learning systems from scratch, and train new kinds of neural networks to augment the transformers with dynamic acceleration.</p>
<p>Today, our models combine dozens of major improvements and hundreds of minor optimizations over the status quo at every addressable level of the stack, and we have been consistently improving their efficiency month by month.</p>
<h2>Inversion v2 &amp; onwards</h2>
<p>We&#39;re also working on a fundamentally new class of models for the next generation of Inversion - they are not ready yet, but so far we expect another several orders of magnitude improvement across the board, with as many heavy workloads as possible completing in single or double digit milliseconds for fractions of a cent in compute cost and at unprecedented reliability and quality.</p>
<p>One particular improvement we&#39;re excited about is in attention, where we&#39;re building towards processing especially large input prompts in milliseconds instead of minutes.</p>
<p>We&#39;re also excited about:</p>
</div><ul><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="22 7 13.5 15.5 8.5 10.5 2 17"></polyline><polyline points="16 7 22 7 22 13"></polyline></svg>Breaking the trend of pre-trained and generic models, by moving to an architecture that can adapt to every user on the fly and constantly improve every day with updated information.</li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20.91 8.84 8.56 2.23a1.93 1.93 0 0 0-1.81 0L3.1 4.13a2.12 2.12 0 0 0-.05 3.69l12.22 6.93a2 2 0 0 0 1.94 0L21 12.51a2.12 2.12 0 0 0-.09-3.67Z"></path><path d="m3.09 8.84 12.35-6.61a1.93 1.93 0 0 1 1.81 0l3.65 1.9a2.12 2.12 0 0 1 .1 3.69L8.73 14.75a2 2 0 0 1-1.94 0L3 12.51a2.12 2.12 0 0 1 .09-3.67Z"></path><line x1="12" x2="12" y1="22" y2="13"></line><path d="M20 13.5v3.37a2.06 2.06 0 0 1-1.11 1.83l-6 3.08a1.93 1.93 0 0 1-1.78 0l-6-3.08A2.06 2.06 0 0 1 4 16.87V13.5"></path></svg>Generative UI composition on the fly, with typed sandboxed code generation in real time.</li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="m5 8 6 6"></path><path d="m4 14 6-6 2-3"></path><path d="M2 5h12"></path><path d="M7 2h1"></path><path d="m22 22-5-10-5 10"></path><path d="M14 18h6"></path></svg>Improved multilingual support, with deep understanding of dialect and personal nuance.</li><li><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12h14"></path><path d="M12 5v14"></path></svg>Much, much more!</li></ul><div><p>We&#39;ve made incredibly promising advances so far towards these next generation systems, and we&#39;re excited to share more about it in the coming months.</p>
<h2>Built for developers</h2>
<p>We&#39;re aiming to deliver the best possible developer experience, see an example of various approaches to making a request through JS/TS (with <code>zod</code>), Python (with <code>pydantic</code>), cURL (with JSON Schema), or a custom grammar below:</p>
</div><div><h2>Our shared future</h2>
<p>We see a path into the future where AI evolves with humans, shaping our shared future as powerful &amp; helpful allies. As AI progresses, we will work to ensure that it remains accessible &amp; beneficial for all.</p>
<p>Together we&#39;ll create a future where technology augments human genius, trivializes mundane tasks, and empowers everyone to live better lives &amp; pursue their passions.</p>
<p>Join us on this journey as we share insights &amp; access to the technology we&#39;re building.</p>
<p>Reach us on X <a href="https://x.com/RysanaAI">@RysanaAI</a>.</p>
</div><div><div><div><h2>Subscribe to our newsletter</h2><p>Receive an email when we publish a new post. No spam, just the good stuff.</p></div></div></div><div><h3>Summary</h3>
<p>We&#39;ve created <strong>Inversion</strong> - a family of structured language models designed to solve the speed, reliability, and reasoning issues in traditional AI systems.</p>
<p>Our first generation models are state of the art in structured tasks such as extraction and function calling while running up to <strong>100× faster</strong>, with <strong>10× lower latency</strong>, outputting <strong>100% reliable structure</strong> with <strong>10,000× less overhead</strong> than the best alternatives, and boasting the deepest support for typed JSON output available anywhere.</p>
<p>We&#39;re expanding access to the first generation of Inversion models shortly, and have begun building the next generation of models targeting on the order of 100,000 char/s inference.</p>
<p>Be among the first to try Inversion. <a href="https://rysana.com/request-access">Sign up for early access</a>.</p>
</div><p><span>*All approximate numbers based on 1000 tests as of March 18th 2024. Results may vary. Experimental models not yet finalized. Models are tested by a single client making sequential requests to each model API server in rapid succession. Inference speed is calculated as the total output characters divided by the total request time. Throughput is calculated as the total output characters divided by the total request time minus the time to first tokens. Latency is calculated as the time from start of request to receiving the first tokens on the requesting client. Type error rate is the percentage of outputs that fail to parse according to the requested schema.</span></p></div></div></div></div></section></div>
  </body>
</html>
