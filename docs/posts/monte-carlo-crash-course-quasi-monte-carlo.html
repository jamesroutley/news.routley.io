<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thenumb.at/QMC/">Original</a>
    <h1>Monte Carlo Crash Course: Quasi-Monte Carlo</h1>
    
    <div id="readability-page-1" class="page"><div><main><ul><li><a href="https://thenumb.at/Probability">Continuous Probability</a></li><li><a href="https://thenumb.at/Monte-Carlo">Exponentially Better Integration</a></li><li><a href="https://thenumb.at/Sampling">Sampling</a></li><li><a href="https://thenumb.at/Rendering">Case Study: Rendering</a></li><li><strong><a href="https://thenumb.at/QMC">Quasi-Monte Carlo</a></strong></li><li><em>Coming Soon…</em></li></ul><hr/><p>We’ve learned how to define and apply Monte Carlo integration—fundamentally, it’s the only tool we need.
In the remaining chapters, we’ll explore ways to reduce variance and successfully sample difficult distributions.</p><ul><li><a href="#variance--correlation">Variance &amp; Correlation</a></li><li><a href="#stratified-sampling">Stratified Sampling</a></li><li><a href="#adaptive-sampling">Adaptive Sampling</a></li><li><a href="#latin-hypercube">Latin Hypercube</a></li><li><a href="#quasi-monte-carlo-1">Quasi-Monte Carlo</a></li><li><a href="#low-discrepancy-sequences">Low-Discrepancy Sequences</a></li></ul><h2 id="variance--correlation">Variance &amp; Correlation</h2><p>In <a href="https://thenumb.at/Monte-Carlo/#escaping-the-curse">chapter two</a>, we determined that the variance of a Monte Carlo estimator is inversely proportional to its sample count.
Empirically, we confirmed that our integrators’ expected error scaled with $$\frac{1}{\sqrt{N}}$$ in any dimension.</p><p>Although dramatically faster than <em>exponential</em>, if we want a very accurate result, $$\frac{1}{\sqrt{N}}$$ may still be too slow.
In practice, we can only scale sample count quadratically so many times.</p><p><img src="https://thenumb.at/QMC/error.svg"/></p><p>We also assumed that our samples are independent, so their variance is additive.
However, our proof that Monte Carlo integration is unbiased didn’t rely on independence—so what if we relaxed that assumption?</p><p>\[\begin{align*}
\mathrm{Var}[X + Y] = \mathrm{Var}[X] + \mathrm{Var}[Y] + 2\mathrm{Cov}[X,Y]
\end{align*}\]</p><p>If $$X$$ and $$Y$$ are <em>negatively</em> correlated, $$\mathrm{Cov}[X,Y] &lt; 0$$, decreasing the variance of $$X+Y$$.
If we can assure that our samples are negatively correlated, our Monte Carlo estimator might converge faster than $$\frac{1}{\sqrt{N}}$$.</p><h3 id="poisson-disk-sampling">Poisson Disk Sampling</h3><p>Perceptually, negatively correlated samples look “more random” than uncorrelated samples.</p><div><p><img src="https://thenumb.at/QMC/uncorrelated.svg"/></p><p><img src="https://thenumb.at/QMC/ncorrelated.svg"/></p></div><p>That’s because uncorrelated samples often appear in clusters and may leave significant chunks of the domain entirely unsampled.
Negative correlation causes the opposite behavior: the more samples an area contains, the less likely it is to be sampled, and vice versa.</p><p>So, how can we generate negatively correlated samples?
One approach is rejection sampling: simply discard samples that fall too close to any previous sample.
This algorithm is known as <em>Poisson disk sampling</em>.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><p>Poisson disk sampling is useful for pre-generating samples with a minimum separation distance, but isn’t always applicable to Monte Carlo integration, where we need a progressive sampler that eventually covers the entire domain.</p><h2 id="stratified-sampling">Stratified Sampling</h2><p>If we don’t need a minimum separation distance, a faster way to generate negatively correlated samples is <em>stratified sampling</em>.
Stratification will let us combine the strengths of <a href="https://thenumb.at/Monte-Carlo/#bias-and-consistency">quadrature</a> and Monte Carlo integration.</p><p>Instead of generating $$N$$ independent samples of the entire domain, a stratified sampler partitions the domain into $$M$$ equal-sized regions and takes $$\frac{N}{M}$$ independent samples of each one.
Since no more than $$\frac{N}{M}$$ samples can occur in any particular region, the samples are negatively correlated.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><p>Let’s consider a Monte Carlo estimator that uses $$N$$ stratified samples of $$\Omega$$.
Grouping samples by region lets us rearrange the estimator into a collection of $$\frac{N}{M}$$-sample estimators for each region $$\Omega_m$$:</p><p>\[\begin{align*}
F_\text{Stratified} &amp;= \sum_{n=0}^N \frac{f(\Omega_n)}{p(\Omega_n)}\\
&amp;= \sum_{m=0}^M \sum_{n=0}^\frac{N}{M} \frac{f(\Omega_{m,n})}{p(\Omega_{m,n})}\\
&amp;= \sum_{m=0}^M F_{\Omega_m}
\end{align*}\]</p><p>Intuitively, stratification partitions our integral across the regions $$\Omega_m$$ and computes an independent $$\frac{N}{M}$$-sample estimate of each term.
Hence, linearity of expectation implies that our stratified estimator is unbiased.</p><p>\[\begin{align*}
\mathbb{E}[F_\text{Stratified}]
&amp;= \mathbb{E}[F_{\Omega_0} + F_{\Omega_1} + \dots]\\
&amp;= \mathbb{E}[F_{\Omega_0}] + \mathbb{E}[F_{\Omega_1}] + \dots\\
&amp;= \int_{\Omega_0} f(\omega)\, d\omega + \int_{\Omega_1} f(\omega)\, d\omega + \dots\\
&amp;= \int_\Omega f(\omega)\, d\omega
\end{align*}\]</p><p>But did stratification reduce variance?
Let’s try dividing our familiar circular estimator into $$M=64$$ regions:</p><p>We’ll find that the stratified estimator has fairly low error, especially when $$N$$ is small.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></p><p>Precisely how much stratification decreases variance depends on the behavior of $$f$$, but we may prove that stratification at least never <em>increases</em> variance.</p><h3 id="why-stratify">Why Stratify?</h3><p>Let’s compare an $$N$$-sample uniform estimator on $$\Omega$$ against a stratified estimator that uniformly samples partitions $$A$$ and $$B$$.
Recalling <a href="https://thenumb.at/Monte-Carlo/#escaping-the-curse">chapter two</a>, we can compute these estimators’ variances:</p><p>\[\begin{align*}
\mathrm{Var}[F_\text{Uniform}] &amp;= \vphantom{\Bigg|}\frac{|\Omega|^2}{N}\mathrm{Var}[f(\Omega)]\\
\mathrm{Var}[F_\text{Stratified}] &amp;= \mathrm{Var}[F_A] + \mathrm{Var}[F_B] \tag{$F_A,F_B$ indep.}\\
&amp;= \frac{|\Omega|^2}{2N}\left(\vphantom{\big|}\mathrm{Var}[f(A)]+\mathrm{Var}[f(B)]\right)
\end{align*}\]</p><p>\[\begin{align*}
\mathrm{Var}[F_\text{Uniform}] &amp;= \vphantom{\Bigg|}\frac{|\Omega|^2}{N}\mathrm{Var}[f(\Omega)]\\
\mathrm{Var}[F_\text{Stratified}] &amp;= \mathrm{Var}[F_A] + \mathrm{Var}[F_B] \\&amp;\tag{$F_A,F_B$ indep.}\\
&amp;= \frac{|\Omega|^2}{2N}\left(\vphantom{\big|}\mathrm{Var}[f(A)]+\mathrm{Var}[f(B)]\right)
\end{align*}\]</p><p>To relate these quantities, we may condition $$\mathrm{Var}[f(\Omega)]$$ on the sampled region.
We will write $$\mu_\mathcal{X}$$ to denote the expected value $$\mathbb{E}[f(\mathcal{X})]$$.</p><p>\[\begin{align*}
\mathrm{Var}[f(\Omega)] &amp;= \mathrm{Var}[f(\Omega)\ |\ A]\cdot\mathbb{P}\{A\} + \mathrm{Var}[f(\Omega)\ |\ B]\cdot\mathbb{P}\{B\}\\
&amp;= \frac{1}{2}\left(\mathbb{E}[(f(\Omega)-\mu_\Omega)^2\ |\ A] + \mathbb{E}[(f(\Omega)-\mu_\Omega)^2\ |\ B]\right)\\
&amp;= \frac{1}{2}\left(\mathbb{E}[f(A)]^2+\mathbb{E}[f(B)]^2-2\left(\frac{\mu_A+\mu_B}{2}\right)^2\right)\\
&amp;= \frac{1}{2}\left(\mathbb{E}[f(A)]^2-\mu_A^2+\mathbb{E}[f(B)]^2-\mu_B^2+\frac{1}{2}\left(\mu_A^2-2\mu_A\mu_B+\mu_B^2\right)\right)\\
&amp;= \frac{\mathrm{Var}[f(A)]+\mathrm{Var}[f(B)]}{2}+\frac{(\mu_A-\mu_B)^2}{4}\\
&amp;\ge \frac{\mathrm{Var}[f(A)]+\mathrm{Var}[f(B)]}{2}\\
\end{align*}\]</p><p>The squared term is never negative, so we know that $$\mathrm{Var}[f(A)] + \mathrm{Var}[f(B)]$$ is at most $$2\cdot\mathrm{Var}[f(\Omega)]$$.
Hence, $$F_\text{Stratified}$$ cannot have higher variance than $$F_\text{Uniform}$$, and has lower variance when $$\mu_A \neq \mu_B$$.</p><p><img src="https://thenumb.at/QMC/avg.svg"/></p><p>This result makes some intuitive sense—if $$f$$ has a different average on $$A$$ and $$B$$, samples constrained to $$A$$ or $$B$$ must have locally lower variance than $$f$$ as a whole.</p><h3 id="dynamic-stratification">Dynamic Stratification</h3><p>So, should we be using stratified sampling everywhere?
Often, yes—partitioning the domain can only reduce variance—but note that stratifying across a fixed $$64$$ regions did not reduce variance <em>asymptotically</em>.</p><p>Even when $$f$$ is <a href="https://en.wikipedia.org/wiki/Bounded_variation">sufficiently nice</a>, stratification only reduces error in inverse proportion to the regions’ volume.
In a $$d$$-dimensional domain, we should expect our estimator to converge with the following expression:<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p><div><p><img src="https://thenumb.at/QMC/curse.svg"/></p><p>\[\begin{align*}
\sigma_\text{Stratified} &amp;\propto \frac{1}{\sqrt{N}\sqrt[d]{M}} \\ &amp;= N^{-\frac{1}{2}}M^{-\frac{1}{d}}
\end{align*}\]</p></div><p>That is, $$M$$ is subject to the <a href="https://thenumb.at/Monte-Carlo/#the-curse-of-dimensionality">curse of dimensionality</a>.
Plus, we can’t use more regions than our sample count, so $$M \le N$$.
Nonetheless, we can find an algorithmic improvement by dynamically scaling $$M \propto \sqrt{N}$$:<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p><p>\[\begin{align*}
\sigma_\text{Stratified} &amp;\propto N^{-\frac{1}{2}}{\sqrt{N}}^{-\frac{1}{d}} \\ &amp;= N^{-\frac{d+1}{2d}}
\end{align*}\]</p><p>In one dimension ($$d = 1$$), dynamic stratification produces $$\sigma \propto N^{-1}$$.
Back in <a href="https://thenumb.at/Monte-Carlo/#quadrature">chapter two</a>, we used quadrature to integrate $$\sqrt{x}$$ with error proportional to $$N^{-1}$$.
Using dynamic stratification, we get an <em>unbiased</em> estimator with the same convergence rate!</p><p>In two dimensions, dynamic stratification results in $$\sigma \propto N^{-\frac{3}{4}}$$, which converges faster than naive Monte Carlo, but in higher dimensions, we rapidly approach our existing result of $$\sigma \propto N^{-\frac{1}{2}}$$.
Hence, dynamic stratification is usually only worthwhile in a small number of dimensions.</p><h2 id="adaptive-sampling">Adaptive Sampling</h2><p>Another extension of stratified sampling is <em>adaptive sampling</em>.
Instead of assigning the same number of samples to each region, adaptive sampling uses more samples in regions with higher variance.</p><p>Above, we determined that the variance of a stratified estimator is a weighted sum, where $$N_A+N_B=N$$:</p><p>\[\begin{align*}
\mathrm{Var}[F_\text{Stratified}] &amp;= \mathrm{Var}[F_A] + \mathrm{Var}[F_B]\\
&amp;\propto \frac{\sigma_A^2}{N_A} + \frac{\sigma_B^2}{N_B}
\end{align*}\]</p><p>We also assumed $$N_A = N_B$$, but that wasn’t required to show that our stratified estimator was unbiased.
So, if $$\sigma_A^2&gt;\sigma_B^2$$, using $$N_A&gt;N_B$$ would decrease the total.
To find the optimal sample distribution, we may minimize this sum using a <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multiplier</a>:</p><p>\[\begin{align*}
&amp;&amp;&amp; \min_{N_A+N_B=N} \left\{\frac{\sigma_A^2}{N_A} + \frac{\sigma_B^2}{N_B}\right\}\\
&amp;\implies&amp;&amp; \begin{cases}
\frac{\partial}{\partial N_A}\left(\frac{\sigma_A^2}{N_A} + \frac{\sigma_B^2}{N_B} - \lambda(N_A+N_B-N)\right) = 0 \\
\frac{\partial}{\partial N_B}\left(\frac{\sigma_A^2}{N_A} + \frac{\sigma_B^2}{N_B} - \lambda(N_A+N_B-N)\right) = 0
\end{cases}\\
&amp;\implies&amp;&amp;
N_A = N\cdot\frac{\sigma_A}{\sigma_A+\sigma_B} \\
&amp;&amp;&amp;
N_B = N\cdot\frac{\sigma_B}{\sigma_A+\sigma_B} \tag{Algebra}
\end{align*}\]</p><p>As you might expect, we should partition samples between $$F_A$$ and $$F_B$$ in proportion to their standard deviation.
Let’s attempt to implement this improvement in our circular estimator.
Intuitively, we should be able to ignore regions where $$f$$ is constant (i.e. $$F$$ has zero deviation):</p><p><img src="https://thenumb.at/QMC/adapt.svg"/></p><p>However, we can’t assume to know the standard deviation of each region beforehand, so we must estimate it during integration.
To guide adaptive sampling, each region $$\Omega_m$$ will track its sample count $$N_m$$, as well as estimates of $$\mathbb{E}[f(\Omega_m)]$$ and $$\mathbb{E}[f(\Omega_m)^2]$$.<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup></p><p>\[\begin{align*}
\sigma_{\Omega_m} &amp;= \sqrt{\mathrm{Var}[F_{\Omega_m}]} \\
&amp;= \sqrt{\frac{\mathbb{E}[f(\Omega_m)^2] - \mathbb{E}[f(\Omega_m)]^2}{N_m}}
\end{align*}\]</p><p>Unfortunately, this estimate can be very imprecise, so using it for adaptive sampling isn’t always easy.
One approach is to randomly select the next region to sample weighted by estimated deviation (drawn in red):</p><p>This strategy works, but even when a region has zero estimated deviation, we must choose it with non-zero probability, since the true value may not be zero.
We can see this situation play out when our estimator discovers that a mostly-covered region actually has non-zero variance.</p><p>The core ideas of adaptive sampling and dynamic stratification—assigning samples where they’re needed and progressively refining the sampling pattern—can be combined to form the <a href="https://en.wikipedia.org/wiki/Multilevel_Monte_Carlo_method">multi-level Monte Carlo</a> method, which we won’t explore in this chapter.</p><h2 id="latin-hypercube">Latin Hypercube</h2><p>Any way we slice it, stratified sampling will eventually run up against the curse of dimensionality.
In practice, we may need computationally cheaper methods of generating negatively correlated samples.
One such method is known as <em>Latin hypercube</em> sampling.<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup></p><p>Instead of attempting to distribute samples across an exponential number of regions, a Latin hypercube sampler stratifies each dimension <em>independently</em>.
For example, in two dimensions, we start by generating two lists of one-dimensional samples stratified across $$D$$ regions:</p><img src="https://thenumb.at/QMC/indep.svg"/><p>We then shuffle $$\mathbf{x}$$ and $$\mathbf{y}$$, randomizing their order.
Finally, to produce samples of $$\Omega$$, we simply take each pair $$(\mathbf{x}_i,\mathbf{y}_i)$$ from the shuffled lists.<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup>
This procedure is typically performed in batches of $$D$$ samples.<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup></p><p>Since we’re implicitly partitioning $$\Omega$$ into $$D^d$$ regions, we can think of a Latin hypercube sampler as a sparse approximation of a stratified sampler over this much larger space.
However, perfect stratification would require placing at most $$\frac{N}{D^d}$$ samples in each region, which our sampler does not achieve.</p><p>Given any particular region, we know exactly $$\frac{N}{D}$$ samples have a matching position along each dimension, so up to $$\frac{N}{D}$$ samples could occur in this region.
This bound is exponentially weaker than full stratification, but still leads to negative correlation.</p><p>Note that regardless of correlation, Latin hypercube samples are uniform, and by similar logic as stratified sampling, may be used for Monte Carlo integration.</p><p>In our circular estimator, we’ll find that Latin hypercube samples tend to reduce error at small $$N$$, but they’re clearly less effective than stratified samples.
Latin hypercube samples become more useful when full stratification is infeasible, e.g. in four or more dimensions.</p><p>The Latin hypercube approach can also be combined with stratified sampling to create multi-level samplers, such as <a href="https://graphics.pixar.com/library/MultiJitteredSampling/paper.pdf">correlated multi-jittered sampling</a>.</p><h2 id="quasi-monte-carlo-1">Quasi-Monte Carlo</h2><p>All of the sampling algorithms we’ve examined so far are fundamentally random, but with additional restrictions that introduce negative correlation.
Alternatively, if we’re willing to introduce <a href="https://thenumb.at/Monte-Carlo/#bias-and-consistency">bias</a>, we can do away with randomness entirely!</p><p>In <a href="https://thenumb.at/Sampling/#pseudo-random-numbers">chapter three</a>, we discussed pseudo-random number generators (PRNGs), which deterministically compute a sequence of samples that “look uniformly random.”
Our only source of non-determinism was the seed: given the same initial state, a PRNG always produces the same sequence.
Hence, for a <strong>fixed seed</strong>, a PRNG is just a particular sequence of numbers $$x_i$$—but we can still use it for Monte Carlo integration.</p><p>\[
F_\text{QMC} = \frac{1}{N} \sum_{i=1}^N f(x_i)
\]</p><p>The result is known as a <em>Quasi-Monte Carlo</em> estimator.<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>
QMC estimators are <a href="https://thenumb.at/Monte-Carlo/#bias-and-consistency">biased</a>: they always return the same value, so averaging multiple runs does not increase accuracy.
Fortunately, they are also <a href="https://thenumb.at/Monte-Carlo/#bias-and-consistency">consistent</a>: increasing the sample count $$N$$ causes the estimator to converge to the exact result.<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup></p><p><img src="https://thenumb.at/QMC/consistent.svg"/></p><h3 id="discrepancy">Discrepancy</h3><p>Empirically, we’ve already confirmed that QMC estimators are consistent—most of the interactive figures use a PRNG with a fixed seed.
But what is it about pseudo-random sequences that make QMC work?
Lacking the tools of probability, how can we relate the accuracy of an estimator to the quantity and quality of its samples?</p><p>This is the purpose of the <em>Koksma–Hlawka inequality</em>, which we will present without proof:</p><p>\[
\Bigg|\, \frac{1}{N}\sum_{i=1}^N f(x_i) - \int_\Omega f(\omega)\, d\omega\, \Bigg| \le V(f) D^*_N(x_1,\dots,x_N)
\]</p><p>Here, $$x_i$$ is our sample sequence, $$f$$ has <a href="https://en.wikipedia.org/wiki/Bounded_variation">bounded variation</a> $$V(f)$$, and $$D^*_N$$ is the “star-discrepancy” of $$x$$.
Bounded variation is morally the same constraint that our Monte Carlo estimator for $$f$$ had finite variance.</p><p>Importantly, our estimator’s error is (at worst) proportional to this property $$D^*_N$$, which only depends on $$x$$.</p><p>\[
\bigg|\, F_\text{QMC} - \int_\Omega f(\omega)\, d\omega\, \bigg| \propto D^*_N(x_1,\dots,x_N)
\]</p><p>Intuitively, we can think of star-discrepancy as a deterministic equivalent of negative correlation.
Taking $$\Omega$$ to be the unit square, $$D^*_N$$ is defined as the worst-case difference between the ratio of samples falling inside a rectangle $$\mathcal{R}$$ and the volume of $$\mathcal{R}$$.
$$\mathcal{R}$$ must also include the origin as its bottom-left corner.<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup></p><p>\[
D^*_N(x_1,\dots,x_N) = \sup_{\mathcal{R}\in\Omega} \Bigg|\, \frac{\text{Samples}(\mathcal{R})}{N} - ||\mathcal{R}|| \,\Bigg|
\]</p><p>We can see that discrepancy penalizes regions that have more (or fewer) samples than they should:</p><p><img src="https://thenumb.at/QMC/discrepancy.svg"/></p><p>Uniformly random point sets—as well as those produced by PRNGs—have the property that $$D^*_N$$ tends to zero with increasing sample count.
Hence, using such sequences in a QMC estimator will cause it to converge for all $$f$$ with bounded variation.</p><h2 id="low-discrepancy-sequences">Low-Discrepancy Sequences</h2><p>To justify using a biased estimator, it should exhibit a faster convergence rate than unbiased alternatives.
But simply fixing a PRNG seed surely doesn’t accelerate convergence, right?
Indeed, in $$d$$ dimensions, the star-discrepancy of a uniformly random point set only converges with the following expression:</p><div><p>\[
D^*_N(\text{Uniform}) \propto \sqrt{\frac{\log^dN}{N}}
\]</p></div><p>The Koksma–Hlawka inequality therefore doesn’t tell us anything—we already knew uniform Monte Carlo converges with $$\frac{1}{\sqrt{N}}$$.
However, there’s no reason we have to use uniformly random points. Instead, we can make use of certain <em>low-discrepancy sequences</em>, which provide near-linear convergence.</p><p>\[
D^*_N(\text{Low-Discrepancy}) \propto \frac{\log^dN}{N}
\]</p><p>While this convergence rate is always asymptotically faster than $$\frac{1}{\sqrt{N}}$$, in high dimensions, the associated constant factor (which depends on $$d$$) can still make the approach impractical.</p><h3 id="the-halton-sequence">The Halton Sequence</h3><p>There are many low-discrepancy sequences, each making use of different mathematical tools.
One popular choice is the <a href="https://en.wikipedia.org/wiki/Halton_sequence">Halton sequence</a>.
To compute a one-dimensional Halton sequence $$g_b(n)$$, first choose a base $$b$$ and find the base-$$b$$ digits of $$n$$:</p><p>\[
n = \sum_k \text{Digit}_k(n)b^k
\]</p><p>Then mirror the digits about the decimal place.
In base ten, we would have $$g_{10}(1234) = 0.4321$$.</p><p>\[
g_b(n) = \sum_k \text{Digit}_k(n)b^{-k-1}
\]</p><p>This operation is also known as the <em>radical inverse</em> $$\Psi_b(n)$$.
To create a $$d$$-dimensional Halton sequence, simply join several one-dimensional sequences with co-prime bases $$b_1\dots b_d$$.</p><p>\[
g(n) = (g_{b_1}(n),\dots,g_{b_d}(n))
\]</p><p>Halton sequences exhibit star-discrepancy proportional to $$\frac{\log^d N}{N}$$, but proving this result is beyond the scope of this chapter.
Instead, let’s examine the two-dimensional $$(2,3)$$ Halton sequence:</p><p>Naturally, we can use Halton samples in our circular estimator:</p><p>We’ll find that convergence is effectively linear, significantly outpacing our earlier estimators.<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup></p><p>Does this result imply we should abandon randomness and use low-discrepancy sequences for everything?
In few dimensions, QMC can be ideal—but working with high-dimensional low-discrepancy sequences turns out to be difficult.</p><h3 id="scrambling">Scrambling</h3><p>By definition, a $$d$$-dimensional Halton sequence uses $$d$$ different co-prime bases.
To illustrate the difficulties encountered in higher dimensions, let’s consider only the points generated by bases 29 and 31.</p><p>\[
g(n) = (\dots,g_{29}(n),g_{31}(n),\dots)
\]</p><p>Note $$g_{29}(n),g_{31}(n)$$ is also the projection of $$g(n)$$ onto the corresponding axes.
Although this sequence is still “low-discrepancy,” its absolute discrepancy starts out quite high:</p><p>Using sequences like $$(29,31)$$ in QMC estimators can be fraught, since reducing bias to an acceptable level requires many samples.
This problem gets worse as we increase dimensionality.</p><p>Fortunately, there’s another technique (known as <em>scrambling</em>) that makes higher-dimensional bases significantly more usable.
Scrambling introduces an extra step to the radical inverse:</p><p>\[
g_b(n) = \sum_k \rho(\text{Digit}_k(n))b^{-k-1}
\]</p><p>Where $$\rho$$ is a permutation of the digits $$0\dots b$$, typically chosen randomly.
Scrambling dramatically reduces discrepancy at low sample counts, making the resulting bias less objectionable.</p><p>The Halton sequence succinctly exemplifies the benefits and pitfalls of QMC, but it’s certainly not the only useful low-discrepancy sequence.
In particular, <a href="https://pbr-book.org/3ed-2018/Sampling_and_Reconstruction/(0,_2)-Sequence_Sampler">Sobol’ sequences</a> are widely used in practice, as they can achieve a kind of optimal discrepancy while admitting an efficient implementation.</p><hr/><p>Written on <time datetime="2025-08-02T00:00:00+00:00">August 2, 2025</time></p></main></div></div>
  </body>
</html>
