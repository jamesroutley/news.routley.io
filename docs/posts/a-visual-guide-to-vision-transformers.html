<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html">Original</a>
    <h1>A Visual Guide to Vision Transformers</h1>
    
    <div id="readability-page-1" class="page"><div data-v-a3c25e27=""><div data-v-a3c25e27=""><!--[--><!--]--><!----><main data-v-a3c25e27=""><div data-v-a3c25e27=""><div data-v-444dbe18=""><p data-v-444dbe18="">This is a visual guide to Vision Transformers (ViTs), a class of deep learning models that have achieved state-of-the-art performance on image classification tasks. Vision Transformers apply the transformer architecture, originally designed for natural language processing (NLP), to image data. This guide will walk you through the key components of Vision Transformers in a scroll story format, using visualizations and simple explanations to help you understand how these models work and how the flow of the data through the model looks like.</p><p data-v-444dbe18=""><strong data-v-444dbe18="">Please enjoy and start scrolling!</strong></p><h3 id="_0-lets-start-with-the-data" tabindex="-1" data-v-444dbe18="">0) Lets start with the data <a href="#_0-lets-start-with-the-data" aria-label="Permalink to &#34;0) Lets start with the data&#34;" data-v-444dbe18="">​</a></h3><p data-v-444dbe18="">Like normal convolutional neural networks, vision transformers are trained in a supervised manner. This means that the model is trained on a dataset of images and their corresponding labels.</p><div data-v-444dbe18=""><!----><div data-v-444dbe18=""><h3 data-v-444dbe18="">1) Focus on one data point </h3><p data-v-444dbe18="">To get a better understanding of what happens inside a vision transformer lets focus on a single data point (batch size of 1). And lets ask the question: How is this data point prepared in order to be consumed by a transformer?</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">2) Forget the label for the moment</h3><p data-v-444dbe18="">The label will become more relevant later. For now the only thing that we are left with is a single image.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">3) Create patches of the image</h3><p data-v-444dbe18="">To prepare the image for the use inside the transformer we divide the image into equally sized patches of size <strong data-v-444dbe18="">p x p</strong>.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">4) Flatting of the images patches</h3><p data-v-444dbe18="">The patches are now flattened into vectors of dimension <strong data-v-444dbe18=""> p&#39;= p²*c </strong> where <strong data-v-444dbe18=""> p </strong> is the size of the patch and <strong data-v-444dbe18=""> c </strong> is the number of channels.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">5) Creating patch embeddings</h3><p data-v-444dbe18="">These image patch vectors are now encoded using a linear transformation. The resulting <strong data-v-444dbe18="">Patch Embedding Vector</strong> has a fixed size <strong data-v-444dbe18="">d</strong>.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">6) Embedding all patches</h3><p data-v-444dbe18="">Now that we have embedded our image patches into vectors of fixed size, we are left with an array of size <strong data-v-444dbe18="">n x d</strong> where <strong data-v-444dbe18=""> n </strong> is the the number of image patches and <strong data-v-444dbe18=""> d </strong> is the size of the <strong data-v-444dbe18="">patch embedding</strong></p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">7) Appending a classification token</h3><p data-v-444dbe18="">In order for us to effectively train our model we extend the array of patch embeddings by an additional vector called <strong data-v-444dbe18="">classification token (cls token)</strong>. This vector is a learnable parameter of the network and is randomly initialized. Note: We only have one cls token and we append the same vector for all data points.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">8) Add positional embedding Vectors</h3><p data-v-444dbe18="">Currently our <strong data-v-444dbe18=""> patch embeddings </strong> have no positional information associated with them. We remedy that by adding a learnable randomly initialized <strong data-v-444dbe18="">positional embedding vector</strong> to all our patch embeddings. We also add a such a positional embedding vector to our <strong data-v-444dbe18=""> classification token</strong>.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">9) Transformer Input</h3><p data-v-444dbe18="">After the positional embedding vectors have been added we are left with an array of size <strong data-v-444dbe18=""> (n+1) x d </strong>. This will be our input for the transformer which will be explained in greater detail in the next steps</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.1) Transformer: QKV Creation </h3><p data-v-444dbe18="">Our transformer input patch embedding vectors are linearly embedded into multiple large vectors. These new vectors are than separated into three equal sized parts. The <strong data-v-444dbe18=""> Q - Query Vector</strong>, the <strong data-v-444dbe18=""> K - Key Vector </strong> and the <strong data-v-444dbe18=""> V - Value Vector </strong>. We will have (n+1) of a all of those vectors. </p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.2) Transformer: Attention Score Calculation </h3><p data-v-444dbe18="">To calculate our attention scores A we will now multiply all of our query vectors Q with all of our key vectors K.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.3)Transformer: Attention Score Matrix </h3><p data-v-444dbe18="">Now that we have the attention score matrix A we apply a `softmax` function to every row such that every row sums up to 1.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.4)Transformer: Aggregated Contextual Information Calculation </h3><p data-v-444dbe18="">To calculate the <strong data-v-444dbe18="">aggregated contextual information</strong> for the first patch embedding vector. We focus on the <strong data-v-444dbe18="">first row</strong> of the attention matrix. And use the entires as weights for our <strong data-v-444dbe18="">Value Vectors V</strong>. The result is our <strong data-v-444dbe18="">aggregated contextual information</strong> vector for the first image patch embedding.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.5)Transformer: Aggregated Contextual Information for every patch</h3><p data-v-444dbe18="">Now we repeat this process for every row of our attention score matrix and the result will be N+1 aggregated contextual information vectors. One for every patch + one for the classification token. This steps concludes our first Attention Head.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.6)Transformer: Multi-Head Attention</h3><p data-v-444dbe18="">Now because we are dealing multi head attention we repeat the entire process from step <strong data-v-444dbe18="">10.1 - 10-5</strong> again with a different QKV mapping. For our explanatory setup we assume 2 Heads but typically a VIT has many more. In the end this results in multiple Aggregated contextual information vectors.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.7)Transformer: Last Attention Layer Step</h3><p data-v-444dbe18="">These heads are stacked together and are mapped to vectors of size <strong data-v-444dbe18="">d</strong> which was the same size as our patch embeddings had.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.8)Transformer: Attention Layer Result</h3><p data-v-444dbe18="">The previous step concluded the attention layer and we are left with the same amount of embeddings of <strong data-v-444dbe18="">exactly the same size</strong> as we used as input.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.9)Transformer: Residual connections</h3><p data-v-444dbe18="">Transformers make heavy use of <strong data-v-444dbe18="">residual connections</strong> which simply means adding the input of the previous layer to the output the current layer. This is also something that we will do now. </p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.10)Transformer: Residual connection Result</h3><p data-v-444dbe18="">The addition results in vectors of the same size. </p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.11)Transformer: Feed Forward Network </h3><p data-v-444dbe18="">Now these outputs are feed through a feed forward neural network with non linear activation functions</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">10.12)Transformer: Final Result </h3><p data-v-444dbe18="">After the transformer step there is another residual connections which we will skip here for brevity. And so the last step concluded the transformer layer. In the end the transformer produced outputs of the same size as input.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">11) Repeat Transformers </h3><p data-v-444dbe18="">Repeat the entire transformer calculation <strong data-v-444dbe18="">Steps 10.1 - Steps 10.12</strong> for the Transformer several times e.g. 6 times.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">12) Identify Classification token output</h3><p data-v-444dbe18="">Last step is to identify the classification token output. This vector will be used in the final step of our Vision Transformer journey.</p></div><div data-v-444dbe18=""><h3 data-v-444dbe18="">13) Final Step: Predicting classification probabilities</h3><p data-v-444dbe18="">In the final and last step we use this classification output token and another fully connected neural network to predict the classification probabilities of our input image.</p></div></div><h3 id="_14-training-of-the-vision-transformer" tabindex="-1" data-v-444dbe18="">14) Training of the Vision Transformer <a href="#_14-training-of-the-vision-transformer" aria-label="Permalink to &#34;14) Training of the Vision Transformer&#34;" data-v-444dbe18="">​</a></h3><p data-v-444dbe18="">We train the Vision Transformer using a standard cross-entropy loss function, which compares the predicted class probabilities with the true class labels. The model is trained using backpropagation and gradient descent, updating the model parameters to minimize the loss function.</p><h2 id="conclusion" tabindex="-1" data-v-444dbe18="">Conclusion <a href="#conclusion" aria-label="Permalink to &#34;Conclusion&#34;" data-v-444dbe18="">​</a></h2><p data-v-444dbe18="">In this visual guide, we have walked through the key components of Vision Transformers, from the data preparation to the training of the model. We hope this guide has helped you understand how Vision Transformers work and how they can be used to classify images.</p><p data-v-444dbe18="">I prepared this little <a href="https://colab.research.google.com/drive/1r_cmWE9RbjkwyqKbhnX5GAtenoFvBurM?usp=sharing" target="_blank" rel="noreferrer" data-v-444dbe18="">Colab Notebook</a> to help you understand the Vision Transformer even better. Please have look for the &#39;Blogpost&#39; comment. The code was taken from @lucidrains great <a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py" target="_blank" rel="noreferrer" data-v-444dbe18="">VIT Pytorch implementation</a> be sure to checkout his work.</p><p data-v-444dbe18="">If you have any questions or feedback, please feel free to reach out to me. Thank you for reading!</p><h2 id="acknowledgements" tabindex="-1" data-v-444dbe18="">Acknowledgements <a href="#acknowledgements" aria-label="Permalink to &#34;Acknowledgements&#34;" data-v-444dbe18="">​</a></h2><ul data-v-444dbe18=""><li data-v-444dbe18=""><a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py" target="_blank" rel="noreferrer" data-v-444dbe18="">VIT Pytorch implementation</a></li><li data-v-444dbe18="">All images have been taken from Wikipedia and are licensed under the Creative Commons Attribution-Share Alike 4.0 International license.</li></ul></div></div></main><!--[--><!--]--></div></div></div>
  </body>
</html>
