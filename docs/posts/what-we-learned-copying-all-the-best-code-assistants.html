<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.val.town/blog/fast-follow/">Original</a>
    <h1>What we learned copying all the best code assistants</h1>
    
    <div id="readability-page-1" class="page"><div data-astro-cid-bvzihdzo=""> <article itemscope="" itemtype="https://schema.org/Article" data-astro-cid-bvzihdzo="">  <div data-astro-cid-bvzihdzo=""> <div data-astro-cid-bvzihdzo="">  <p><img src="https://blog.jacobvosmaer.nl/_astro/steve.X7ylcW9k_ZMjRK9.webp" srcset="/_astro/steve.X7ylcW9k_ZMjRK9.webp 1x, /_astro/steve.X7ylcW9k_Z1ReC3G.webp 2x, /_astro/steve.X7ylcW9k_282LrH.webp 3x" alt="Steve Krouse" title="Steve Krouse" data-astro-cid-h4usap26="true" width="20" height="20" loading="lazy" decoding="async"/> 
on
<time datetime="2025-01-03T00:00:00.000Z"> Jan 3, 2025 </time>  </p> </div>  <p>Since the beginning of Val Town, our users have been clamouring for the state-of-the-art <abbr title="Large Language Model">LLM</abbr> code generation experience. When we launched our code hosting service in 2022, the state-of-the-art was GitHub Copilot. But soon it was ChatGPT, then Claude Artifacts, and now <a href="https://bolt.new/">Bolt</a>, <a href="https://www.cursor.com/">Cursor</a>, and <a href="https://codeium.com/windsurf">Windsurf</a>. We’ve been trying our best to keep up. Looking back over 2024, our efforts have mostly been a series of <em>fast-follows</em>, copying the innovation of others. Some have been successful, and others false-starts. This article is a historical account of our efforts, giving credit where it is due.</p>
<h3 id="github-copilot-completions"><a href="#github-copilot-completions">GitHub Copilot Completions</a></h3>
<p>The story starts, of course, with GitHub Copilot. From day 1, Val Town users asked for a GitHub-Copilot-like completions experience.</p>
<p>We were wary of building this ourselves, but one day we stumbled upon Asad Memon’s <a href="https://github.com/asadm/codemirror-copilot">codemirror-copilot</a>, and hooked it up. That gave us our first taste of LLM-driven autocomplete, but behind the scenes, it was using ChatGPT. The prompt essentially asked ChatGPT to cosplay as an autocomplete service and fill in the text at the user’s cursor. So it was fairly slow, occasionally the model would forget its role and do something unexpected, and it didn’t have the accuracy of a purpose-built autocomplete model.</p>
<p>We wanted a faster, more accurate autocomplete sytem, one that used a model trained for the task - which is technically called <a href="https://arxiv.org/abs/2207.14255">‘Fill in the Middle’</a>. Finding an option that we could use within a product like Val Town was tricky – Copilot and most of its competitors lack documented or open APIs. But <a href="https://codeium.com/">Codeium</a> did, and they also had very good accuracy and performance. We <a href="https://blog.val.town/blog/val-town-newsletter-16/#-codeium-completions">launched Codeium completions</a> in April 2024 and open-sourced our <a href="https://github.com/val-town/codemirror-codeium">codemirror-codeium</a> component. It’s been pretty great. It’s enabled by default for new users.</p>
<p><img src="https://blog.jacobvosmaer.nl/_astro/codeium.DmWVlUN2_ZSR3s9.webp" alt="Codeium" width="903" height="324" loading="lazy" decoding="async"/></p>
<h3 id="chatgpt"><a href="#chatgpt">ChatGPT</a></h3>
<p>Then came ChatGPT. We found our users asking it to write Val Town code, and copying and pasting it back into Val Town. We figured we could automate that process for our users: provide an interface with a pre-filled system prompt and a one-click way to save the generated code as a val. The <a href="https://blog.val.town/blog/val-town-newsletter-18/#-townie">first version of Townie</a> was born: a simple chat interface, very much inspired by ChatGPT, powered by GPT-3.5.</p>
<p><img src="https://blog.jacobvosmaer.nl/_astro/townie.B_9WgDMi_Z1eUctW.webp" alt="Townie" width="2310" height="2012" loading="lazy" decoding="async"/></p>
<p>It was just ok. It didn’t get much use, mostly because it was hard to iterate on its results. Getting good results from an LLM usually requires a conversation because programming-via-English is pretty imprecise, and you need follow-up requests to clarify your needs.</p>
<h3 id="chatgpt-tool-use"><a href="#chatgpt-tool-use">ChatGPT Tool Use</a></h3>
<p>Earlier this year, ChatGPT Function Calling, now called ‘tool-use’, was seen as the next big thing. The promise was that with a good OpenAPI spec, AI would be able to do just about anything on Val Town. So we dutifully cleaned up our OpenAPI spec, and <a href="https://blog.val.town/blog/openapi/#our-ai-townie-can-now-call-our-rest-api">rebuilt Townie around it</a>.</p>
<iframe width="100%" height="420" src="https://www.youtube-nocookie.com/embed/clj1prZunW0?si=O7l6qvW8B2OB79NT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<p>It was, ahem, fine. Function calling was a disappointment. You do all the work to provide the LLM with a strict definition of what functions it can call and with which arguments. But even with all of that, the LLM would hallucinate functions that didn’t exist. Function calling has improved since, with the introduction of <a href="https://platform.openai.com/docs/guides/function-calling#structured-outputs">Structured Outputs</a>.</p>
<p>But for us, the issue was that the interface was too generic. In theory, it was capable of doing anything (editing your blobs or sqlite data), but it wasn’t very useful at any specific thing. Most notably, it wasn’t a good interface for iterating on code. It could write a first version of code, but it wasn’t optimized to let you run that code, see the output, debug it, let you ask the AI for more help. In other words, the feedback loop was bad.</p>
<h3 id="claude-artifacts"><a href="#claude-artifacts">Claude Artifacts</a></h3>
<p>We had begun to see the potential of Claude for code generation with the amazing results produced by <a href="https://websim.ai/">Websim</a>. But it was the <a href="https://www.anthropic.com/news/claude-3-5-sonnet">launch of Claude 3.5 Sonnet and Claude Artifacts</a> that really got our attention. Claude 3.5
Sonnet was dramatically better at generating code than anything we’d seen before. It blew all of our minds. And Claude Artifacts solved the tight feedback loop problem that we saw with our ChatGPT tool-use version. And thus after about a month of prototyping and building, the <a href="https://blog.val.town/blog/codegen/">current version of Townie</a> was born in August 2024.</p>
<p>For a couple weeks there, it felt like we had one of the best tools in the space. Townie can generate a fullstack app, with a frontend, backend, and database, in minutes, and fully deployed. The space has since gotten crowded. Live by the fast follow; die by the fast follow.</p>
<h3 id="our-contributions"><a href="#our-contributions">Our Contributions</a></h3>
<p>While we were out in front, we invested in trying to stay there, and we made some contributions of our own that have since found there way into other tools in the space.</p>
<h4 id="speed"><a href="#speed">Speed</a></h4>
<p>The biggest problem with all current codegen systems is the speed of generation. It takes minutes to generate just a couple hundred lines of code. If you regenerate the whole file every time – which is how most systems work – that means minutes between every feedback loop. (Not to mention the cost of regenerating the whole file every time, even when you are making a small change.)</p>
<p>We worked hard to get the LLM producing diffs, based on <a href="https://aider.chat/docs/unified-diffs.html">work we saw in Aider</a>. We were able to get it working most of the time, but not reliably enough. It’s now off by default, but you can ask Townie to “reply in diff” if you’d like to try your luck with it.</p>
<p>Our system prompt has always been open (you can view it in your Townie settings), so you can see how we’re doing that. Here’s the relevant section:</p>
<div><figure><pre data-language="md"><code><div><p><span>Follow the requirements above and respond by generating code in a format based on whether or not the user explicitly requests diff format in their most recent prompt:</span></p></div><div></div><div><p><span>-</span><span> If the user does not explicitly request diff format in their prompt, generate the entire val:</span></p></div><div><p><span><span>  </span></span><span>Use &lt;</span><span>existing_code</span><span>&gt; as the basis for generating code if it is provided.</span></p></div><div><p><span><span>  </span></span><span>Write code that is complete and directly runnable.</span></p></div><div><p><span><span>  </span></span><span>DO NOT omit code or use comments such as &#34;more content here&#34; or &#34;code remains unchanged.&#34;</span></p></div><div><p><span><span>  </span></span><span>Write the code in `val code fences.</span></p></div><div><p><span>Include the val type as metadata on the code fence, e.g.: `val type=script</span></p></div><div><p><span><span>  </span></span><span>If this is a new val, decide what val type is appropriate based on the user&#39;s prompt. Default to choosing http type vals unless the user has requested specific functionality that requires a different type.</span></p></div><div></div><div><p><span>-</span><span> If the user requests diff format in their prompt, follow these steps:</span></p></div><div><p><span><span>  </span></span><span>Write a valid unified diff with change hunk headers. The file headers can be omitted.</span></p></div><div><p><span><span>  </span></span><span>Base the diff off of the &lt;</span><span>existing_code</span><span>&gt; tags below.</span></p></div><div><p><span><span>  </span></span><span>Use the ```diff language code fence.</span></p></div></code></pre></figure></div>
<p>We’ve gotten scared off of investing more time in diffs right now, but I expect it may have been solved by others in the space already, or will be shortly. Anthropic’s long-rumored “fast-edit mode” solve this problem in one fell swoop. OpenAI launched their own <a href="https://platform.openai.com/docs/guides/predicted-outputs">Predicted Outputs</a>, which is also compelling, but then we’d have to switch to OpenAI. Or maybe the solution is simply faster models, smaller, mini-models, or faster chips, like Groq or Cerebras. A couple weeks ago I built <a href="https://cerebrascoder.com">Cerebras Coder</a> to demonstrate how powerful an instant feedback loop is for code generation. <a href="https://cerebrascoder.com">Try it out yourself</a> or <a href="https://www.val.town/v/stevekrouse/cerebras_coder">fork it here</a>.</p>
<video controls=""><source src="/video/cerebras-coder.mp4"/></video>
<p>DeepSeek <a href="https://x.com/deepseek_ai/status/1872242657348710721">recently open-sourced an almost-Sonnet-3.5-level model that’s twice as fast and trained for only $6m</a>. A boy can dream of a world where Sonnet-3.5-level codegen (or even smarter!) is available on a chip like Cerebras at a fraction of Anthropic’s cost. I think that would unleash a whole new class of innovation here.</p>
<h4 id="autodetecting-errors"><a href="#autodetecting-errors">Autodetecting errors</a></h4>
<p>We did contribute one possibly-novel UI interaction, where the LLM automatically detects errors and asks you if you’d like it to try to solve them. We detect server-side errors by polling our backend for 500 errors in your logs. We detect client-side errors in the iframe by prompting Townie to import <a href="https://www.val.town/v/std/catch">this client-side library</a>, which pushes errors up to the parent window.</p>
<video controls=""><source src="/video/TownieErrorDetection.mp4"/></video>
<p>It’s not <em>particularly</em> novel (in that others would have thought of this if we didn’t), but maybe the folks at <a href="https://support.anthropic.com/en/articles/9949260-try-fixing-with-claude-for-artifact-errors">Anthropic</a> or Bolt saw our implementation and it inspired their own. I’d like to think we’re not <em>only</em> free-riding in this space.</p>
<h3 id="hosted-runtime-and-included-apis"><a href="#hosted-runtime-and-included-apis">Hosted runtime and included APIs</a></h3>
<p>Maybe some of our UI ideas made it into GitHub Spark too, including deployment-free hosting, persistent data storage, and the ability to use LLMs in your apps without a your own API key – their versions of <a href="https://docs.val.town/std/sqlite/">@std/sqlite</a> and <a href="https://docs.val.town/std/openai/">@std/openai</a>, respectively. In other words, you can say, “make me a ChatGPT clone with persistent thread history”, and in about 30 seconds, you’ll have a deployed app that does exactly that.</p>
<video controls=""><source src="/video/TownieChatGPTClone.mp4"/></video>
<p>But we’re not the first hosting company to provide an LLM tool; that
honor likely goes to Vercel’s <a href="https://v0.dev/">v0</a>.</p>
<h3 id="cursor"><a href="#cursor">Cursor</a></h3>
<p>The next big thing was <a href="https://www.cursor.com/">Cursor</a>. I must admit that I never personally fell in love with it, but given how many people I respect love it, I think that’s a me-problem. I think Cursor is best for development in larger codebases, but recently my work has been on making vals in Val Town which are usually under 1,000 lines of code. (Our upcoming launch of multi-file Projects, now in private beta, will change this.) However Cursor is a real pioneer in the space, and has some UI interactions there that we have an eye to copy.</p>
<h3 id="windsurf"><a href="#windsurf">Windsurf</a></h3>
<p>Over the holiday, I fell in love with <a href="https://codeium.com/windsurf">Windsurf</a> by the folks at Codeium. Its Cascade feature is a chat interface, which has tool use and multi-turn agentic capabilities, to search through your codebase and edit multiple files. It feels a bit like we’re coming full-circle back to when we did our tool-use version of Townie. However, I think we now all understand that you can’t simply give your OpenAPI spec to an LLM and expect good results. The magic of Windsurf is that they carefully crafted what actions their agent can take, and that it can take multiple actions in a row without your input.</p>
<p>I am salivating at the idea of giving Townie some of these capabilities. Imagine if Townie could search through all public vals, and maybe even npm, or the public internet, to find code, docs, and other resources to help you.</p>
<h3 id="devin"><a href="#devin">Devin</a></h3>
<p>Watching Windsurf take multiple actions on my behalf without my input is very inspirational. I’m dreaming of a world where Townie not only detects errors, but also automatically tries to fix them, possibly multiple times, possibly in parallel across different branches, without any human interaction. Here, of course, we’d be getting into territory mostly explored by the folks at <a href="https://devin.ai/">Devin</a>.</p>
<p>For starters, we could feed back screenshots of the generated website back to the LLM. But soon you’d want to give the LLM access to a full web browser so it can itself poke around the app, like a human would, to see what features work and which ones don’t. Maybe then it’d even write some tests, also like a human would, to make sure things don’t break as it continues to iterate.</p>
<p>I have a vague sense by the end of this year that you’ll be able to tell Townie to “make a fully realistic Hacker News Clone, with user accounts, nested comments, upvotes, downvotes” and it could iterate for potentially hours on your behalf. You could even go to bed and wake up with it done.</p>
<h3 id="collaboration-and-competition"><a href="#collaboration-and-competition">Collaboration and competition</a></h3>
<p>Is this fast-following competitive or is it collaborative? So far it’s been feeling mostly collaborative. The pie is so freaking large — there are millions and maybe billions who are jumping at the chance to code — that we’re all happy to help each other scramble to keep up with the demand. I love that, and hope it remains this way. We at Val Town certainly don’t keep (m)any secrets. Our system prompt is open, and we blog about all our interesting technical choices. This very post is a case in point.</p>
<h3 id="should-we-bow-out"><a href="#should-we-bow-out">Should we bow out?</a></h3>
<p>All this copying, and how fast everything is moving begs the question: Should we get out of this race entirely? How can we hope to compete against better funded competitors? Should we instead focus on improving our core differentiator, and do a better job integrating with AI editors like VSCode, Cursor, Windsurf, and Bolt? Maybe! We’re planning to dip our toes into integrating: we plan to improve our local development experience, which would allow editors like VSCode, Cursor, and Windsurf to directly edit files in Val Town. We also plan to improve our API, so tools like Bolt could “deploy to Val Town”, like they currently deploy to Netlify.</p>
<p>However, it still feels like there’s a lot to be gained with a fully-integrated web AI code editor experience in Val Town – even if we can only get 80% of the features that the big dogs have, and a couple months later. It doesn’t take <em>that</em> much work to copy the best features we see in other tools. The benefits to a fully integrated experience seems well worth that cost. In short, we’ve had a lot of success fast-following so far, and think it’s worth continuing to do so.</p>
<h3 id="townie"><a href="#townie">Townie</a></h3>
<p>If you’ve made it this far in the article, you should really <a href="https://www.val.town/townie">try out Townie</a>. It’s still is one of the best tools to create fullstack web apps. Make yourself a <a href="https://www.val.town/v/danphilibin/what_did_i_work_on_today">‘what did I work on today’ app that pulls from Linear and GitHub</a> or <a href="https://x.com/destroytoday/status/1856709997737984089">a tool to extract dominant colors from an image</a> or <a href="https://deeperfates.com/">an AI clone for your personality</a>. Your imagination is the limit. And if you do, please let me know (<a href="mailto:steve@val.town">steve@val.town</a>) what features from other tools you’d like to see in Townie. We’re eager to learn from you.</p>
<p><em>Thanks <a href="https://macwright.com/">Tom MacWright</a>, <a href="https://janpaulposma.nl/">JP Posma</a>, and <a href="https://simonwillison.net/">Simon Willison</a> for feedback on drafts of this article.</em></p>  <a href="https://github.com/val-town/val-town-blog/edit/main/src/content/blog/fast-follow.mdx" data-astro-cid-npoeh54f=""><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" data-astro-cid-npoeh54f=""><path stroke-linecap="round" stroke-linejoin="round" d="M16.862 4.487l1.687-1.688a1.875 1.875 0 112.652 2.652L6.832 19.82a4.5 4.5 0 01-1.897 1.13l-2.685.8.8-2.685a4.5 4.5 0 011.13-1.897L16.863 4.487zm0 0L19.5 7.125" data-astro-cid-npoeh54f=""></path></svg>
Edit this page
</a> </div> </article> </div></div>
  </body>
</html>
