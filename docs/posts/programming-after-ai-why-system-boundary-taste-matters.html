<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://interjectedfuture.com/programming-after-ai-why-system-boundary-taste-matters/">Original</a>
    <h1>Programming After AI: Why System Boundary Taste Matters</h1>
    
    <div id="readability-page-1" class="page"><section>
            <p>Since coding agents came on the scene, a subset of programmers panic about AI replacing their jobs, but I think they&#39;re panicking about the wrong thing. The fear seems focused on the mechanical aspects—will AI write better functions than me? Will it debug faster? Will it remember more API endpoints? But I&#39;d argue a tacit skill of senior engineers is knowing where to draw system boundaries. AI can generate code from specifications, but it can&#39;t decide which abstractions will make sense, or how a system should evolve as its context changes.</p><p>This distinction matters because most programmers spend their careers optimizing for immediate readability, treating code as literature to be consumed by their present-day colleagues (though it might not feel like that at times). But the boundary decisions you make today—how you partition responsibility, where you allow flexibility, which dimensions of change you prepare for—these determine whether your system gracefully adapts or buckles under the weight of accumulated compromises. AI doesn&#39;t live with technical debt. It doesn&#39;t feel the pain of a poorly chosen abstraction echoing through years of maintenance. It generates clean solutions to well-defined problems–under strict guidance–but problem definition itself remains stubbornly human for the foreseeable future.</p><p>The history of data storage offers a surprisingly clear lens for understanding a common instance of this boundary drawing challenge, though most programmers don&#39;t realize they&#39;re repeating the same fundamental mistakes. Consider how we think about data structures: arrays optimize for sequential access, hash tables for key-based lookup, trees for ordered traversal. Each makes a specific trade-off, committing upfront to particular access patterns while making others expensive or impossible. This is boundary thinking in its most concrete form—you&#39;re literally deciding how to organize information in memory, and that organization constrains every future interaction with your data.</p><p>Early database systems followed this same rigid pattern. Hierarchical databases like IBM&#39;s IMS organized information as trees, forcing you to traverse a single predetermined path to reach any piece of data. The approach made perfect sense if you knew exactly how your data would be accessed. CSS selectors work this way and selecting the DOM hierarchy work this way. Even XML databases with XPath queries suffer the same limitation (for those of you of a specific vintage). But asking &#34;show me all elements with red backgrounds&#34; requires visiting every node. You get blazing performance for the access pattern you optimized for, and awkward workarounds for everything else.</p><p>The weakness of hierarchical systems becomes apparent the moment you encounter cross-cutting concerns—requirements that don&#39;t respect your carefully constructed boundaries. When you don&#39;t know your access patterns in advance, or when users start asking questions that cut across your hierarchies in unexpected ways, you discover that your optimization has become a prison. The system serves its structure rather than serving its users.</p><p>Relational databases revolutionized this by essentially deferring the boundary decision. Codd&#39;s insight with relational algebra wasn&#39;t just about mathematical elegance—it was about recognizing that the most important access patterns are often the ones you can&#39;t predict. Instead of hardcoding how you&#39;ll slice your data, relational systems let you decide at query time. You can ask for entity.property just as easily as property.entity, or any combination that serves your immediate need. The system adapts to your questions rather than forcing your questions to adapt to the system&#39;s structure.</p><p>The ability to query across different dimensions for a data set isn&#39;t merely a database story—it&#39;s a philosophy about system design that most programmers never internalize. The success of relational databases hints at a deeper principle: the most robust systems are those that can be reorganized along dimensions their original designers never considered.</p><p>Which brings me to what I think is a surprising insight when I first heard it: entity-component systems (ECS) in games are just relational databases in disguise. Entity-component systems are way of bookkeeping various entities in the game, such as the player, monsters, power up items, etc., but allowed the game to process them by <em>system</em>, such as the physics and rendering.</p><p> Games discovered this pattern not through theoretical exploration but through necessity. Traditional object-oriented game engines organize code around conceptual entities—Player class, Enemy class, PowerUp class—and this works beautifully until you need systems that operate across entity types. Physics affects everything, rendering affects everything, AI systems need to query spatial relationships between arbitrary objects. The entity-centric boundaries that make perfect sense to human designers become computational and design bottlenecks.</p><p>The design bottleneck can easily happen, when you&#39;re trying to find the fun by recombining various sub-systems. When designing a shooting game, it&#39;s natural to decide and assume that a camera is fixed on the player, and any AI is attached to the monster. But what if the fun is in a homing missile? Now we need to attach the AI to a projectile. And if the fun is in seeing from the perspective of the homing missile as it&#39;s searching its target? Well, that breaks the assumption that the camera subsystem is always on the player. </p><p><a href="https://www.youtube.com/watch?v=wo84LFzx5nI&amp;ref=interjectedfuture.com" rel="noreferrer">Casey Muratori&#39;s talk on rejecting &#34;compile-time hierarchies&#34;</a> articulates this tension precisely. Object-oriented programming convinced an entire generation that entity-first thinking was universally correct, that domains should always be modeled by drawing boundaries around conceptual objects. This works when your domain naturally clusters around stable entities—Alan Kay&#39;s work in molecular biology, where cells and proteins have clear identities, or Bjarne Stroustrup&#39;s distributed systems, where processes and machines form natural boundaries. But for most computational problems, entity-first thinking is the wrong choice, a premature optimization that constrains future flexibility.</p><p>I conceptualize this alternative access as a &#34;transpose.&#34; Instead of organizing around entity-to-property relationships, you use property-to-entity relationships. Instead of asking &#34;what methods does a Player have?&#34;, you ask &#34;what entities need physics calculations? What entities need rendering updates?&#34; You&#39;re drawing boundaries around capabilities and systems rather than around conceptual objects. The mental shift feels subtle, but the architectural implications are profound.</p><p>ECS systems make this transpose concrete. A physics system operates on all entities that have physics components—position, velocity, mass. A rendering system operates on all entities with visual components—sprites, animations, shaders. You&#39;re essentially running queries: &#34;give me all entities that have both Position and Velocity components so I can update their motion.&#34; The similarity to SQL becomes obvious once you notice it: both systems let you join arbitrary data on the fly rather than pre-committing to rigid hierarchies.</p><p>Games generally avoid traditional databases because their computational budget remains extreme—every microsecond matters when you&#39;re targeting 60fps with complex simulations. But ECS gives them database-like query flexibility while maintaining cache-friendly performance characteristics. They&#39;ve independently rediscovered relational thinking, optimized for their specific constraints.</p><p>Some games push even further into what I can only describe as inference territory. The Chemistry Engine in Breath of the Wild doesn&#39;t just query existing properties—it derives new facts from interactions between materials. Fire plus wood yields burning wood; ice plus fire becomes water. The system knows rules about how properties interact and infers new states dynamically. Baba is You takes this to an almost philosophical extreme: the game mechanics themselves emerge from logical rules that get recomputed as you physically rearrange the rule statements. &#34;Baba IS You&#34; becomes &#34;Rock IS You&#34; when you push the words around. The entire game state derives through inference from a changing set of logical facts.</p><p>This suggests a future direction that excites me: systems that can not only query from arbitrary dimensions but infer relationships you didn&#39;t even know existed. Datalog and inference engines let you specify facts about your domain and ask &#34;what else must be true?&#34; rather than explicitly computing every possible state. Epic&#39;s Verse language, developed with Simon Peyton Jones, hints at this direction—a functional logical language where you can &#34;run functions in reverse,&#34; using inference to find inputs that produce desired outputs. Game designers are starting to explore what becomes possible when computation becomes less about explicit control flow and more about declarative reasoning.</p><p>I don&#39;t expect widespread adoption of inference-based systems soon—the tooling remains esoteric, and most developers are still learning to think in terms of queries rather than objects. But the underlying trend seems clear: as AI commoditizes code generation, the valuable human skill becomes problem modeling. Not the mechanics of translating requirements into syntax, but the deeper challenge of understanding which boundaries will serve you well as requirements evolve.</p><p>The programmers who survive will be those who develop intuition for an angle of attack for their problem–one of which is to decide when their domain needs property-first thinking rather than entity-first thinking. The programmers who  thrive understand when rigid boundaries serve their purposes and when flexible boundaries justify their complexity. They&#39;ll know how to design systems that can be queried from dimensions they haven&#39;t yet imagined, because they&#39;ll understand that the most important access patterns are often the ones you can&#39;t predict.</p><p>While AI writes increasingly sophisticated code, humans who understand boundary trade-offs will architect the systems that matter. The question isn&#39;t whether AI can code—it&#39;s whether you can think about problems in ways that remain valuable as the tools evolve.</p>
        </section></div>
  </body>
</html>
