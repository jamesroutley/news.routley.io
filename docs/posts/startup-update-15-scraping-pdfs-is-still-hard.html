<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.moderndescartes.com/essays/8_4_2025">Original</a>
    <h1>Startup update 15: Scraping PDFs is still hard</h1>
    
    <div id="readability-page-1" class="page"><div id="content">


<div>
    <div>
	

<p> Originally posted 2025-08-04</p>
<p> Tagged: <a href="https://www.moderndescartes.com/essays/tags/cartesian_tutor">cartesian_tutor</a>, <a href="https://www.moderndescartes.com/essays/tags/llms">llms</a></p>
<p> <em>Obligatory disclaimer: all opinions are mine and not of my employer </em></p>
<hr/>

<h2 id="progress-update">Progress update</h2>
<p>I did a bunch of scraping this week to try and get past years‚Äô
chemistry olympiads in a format that could be presented on the app.
Turned out to be surprisingly hard: chemistry relies on diagrams in a
variety of places - the problem statement, in multiple choice options,
in the solutions; at varied spots within multipart questions. The types
of diagrams include plots, lewis dot diagrams, organic structures,
molecular orbital diagrams, etc.. So figuring out how to get images
scraped out of PDFs and then associated/represented with the right place
in the database schema is a bit annoying. Unfortunately, this is both
table stakes and annoying to implement. I also anticipate that getting
an automated grader working later on will also be annoying. That‚Äôs this
week‚Äôs excuse for not having something new up ü´†</p>
<p>On the plus side, having gone through a few of the recent exams more
closely, I am getting reacquainted with the difficulty inflation of
recent years and the types of content that I‚Äôll have to do for my app.
I‚Äôm taking notes on problem type composition, obscure tricks that I
forgot I knew and will have to write lessons for, test-taking tips, etc.
etc.</p>
<p>I am also dreading figuring out how to generate / grade / teach
organic chemistry using LLMs. LLMs are really just not there yet in
terms of being able to reason graphically about electron pushing or
whatnot. (Maybe if I ask them to output SVGs?). I think this might just
have to be old-school textbook style or some other placeholder. OTOH, if
I do eventually figure this out, I think the pre-med market for teaching
organic chemistry becomes a natural target audience.</p>
<h2 id="on-scraping-pdfs">On scraping PDFs</h2>
<p>This is stupidly hard, STILL. <a href="https://news.ycombinator.com/item?id=42952605">This ‚Äúold‚Äù (from 5
months ago) post from HN</a> led me to believe that we were here. We are
not here, yet.</p>
<p>Structured PDF extraction feels like a Venn diagram, where it‚Äôs
Extract Images (Bounding boxes) from PDF, Extract Structured Content
from PDF, and Extract from Long PDFs. The kicker: it‚Äôs not even a ‚Äúpick
2 of 3‚Äù situation, but a ‚Äúpick 1 of 3‚Äù situation.</p>
<p>Long PDF X Structured Content: Scraping 2-3 page PDFs seems to work
fine, but as you get to 10 pages, the likelihood that the parser goes
through and successfully outputs a correctly formatted JSON/CSV/whatever
gets lower and lower. Markdown does not count as ‚Äústructured‚Äù in my
book.</p>
<p>Long PDF X Images: Gemini gives bounding boxes nicely, but it has no
way of referring to multiple images: it can‚Äôt say ‚Äúbounding box
X1,X2,Y1,Y2 from attachment 3 of 8‚Äù.</p>
<p>Images X Structured Content: Gemini handles PDFs natively, but Claude
requires image conversion first. if you want bounding boxes, Gemini also
requires image conversion. Structured extraction from images is okayyy
but comes with higher error rate compared to PDF as input.</p>
<p>So then I end up in this unfortunate situation where I‚Äôm
orchestrating a multipart pipeline:</p>
<ul>
<li>First convert the PDF to a markdown file, preserving <em>text</em>
content, but without being too precious about precise formatting.
Drastically shortens/shrinks the size of the input from 1MB 10-page PDF
to a 30kb text file. This step loses image content and relative
positioning.</li>
<li>Then convert the markdown blob to a more structured format.</li>
<li>Then pass the PDF, page-wise, through a bounding box extractor,
while simultaneously asking the LLM to annotate the extracted image with
‚Äúthis comes from problem 7, subpart (c)‚Äù, so that I can later piece
things together</li>
</ul>
<p>Debugging the orchestration layer while also debugging these
individual steps was stupidly annoying. I am now experimenting with
providing the above steps as an MCP server, and letting the Claude CLI /
desktop app orchestrate everything, instead of me writing the
orchestration code myself. The fun part of this is that I can now file
my scraping costs under my Claude Pro subscription instead of filing it
under pay-per-token! (Sorry, I may be part of the reason why these
subscriptions are getting limited‚Ä¶) The other fun part of this is that
when you use Claude Desktop as the orchestration layer, you get to
naturally use the Claude interface to debug what the orchestration layer
is doing - what artifacts it‚Äôs generating, where in the process it is,
and if it ran out of context window (type ‚Äúcontinue‚Äù to continue, with
surprisingly graceful partial-task recovery).</p>


    </div>
</div>


<hr/>



</div></div>
  </body>
</html>
