<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/facebookresearch/MobileLLM">Original</a>
    <h1>MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">This repository contains the training code of MobileLLM introduced in our work: &#34;<a href="https://arxiv.org/abs/2402.14905" rel="nofollow">MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases</a>&#34;, published in ICML 2024.</p>
<p dir="auto">In this work, we comprehensively consider multiple design factors to obtain high-quality LLMs with fewer than a billion parameters. We integrated (1) SwiGLU activation function, (2) deep and thin architectures, (3) embedding sharing, (4) grouped-query attention to build MobileLLM. MobileLLM-125M/350M attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M SoTA models on zero-shot commonsense reasoning tasks. In our updated version, we further demonstrate that our design philosophy scales effectively to larger models, with SoTA results for MobileLLM-600M/1B/1.5B.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/facebookresearch/MobileLLM/blob/main/mobilellm.png"><img width="50%" src="https://github.com/facebookresearch/MobileLLM/raw/main/mobilellm.png"/></a>
</p>

<p dir="auto">If you find our code useful for your research, please consider citing:</p>
<div data-snippet-clipboard-copy-content="@article{liu2024mobilellm,
    title={MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases},
    author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
    journal={arXiv preprint arXiv:2402.14905},
    year={2024}
}"><pre><code>@article{liu2024mobilellm,
    title={MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases},
    author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and others},
    journal={arXiv preprint arXiv:2402.14905},
    year={2024}
}
</code></pre></div>


<ul dir="auto">
<li>python 3.9, pytorch &gt;= 2.0</li>
<li>pip install -r requirement.txt</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Step 2. Data preprocessing</h3><a id="user-content-step-2-data-preprocessing" aria-label="Permalink: Step 2. Data preprocessing" href="#step-2-data-preprocessing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Dividing a tokenized dataset or tokenize your own dataset, and even distribute it across the total number of training nodes, where each node comprises 1x8 GPUs. Next, organize the data into the following structure:</p>
<ul dir="auto">
<li>basepath
<ul dir="auto">
<li>1
<ul dir="auto">
<li>xxx.jsonl</li>
</ul>
</li>
<li>2
<ul dir="auto">
<li>xxx.jsonl</li>
</ul>
</li>
<li>...</li>
<li>#nodes
<ul dir="auto">
<li>xxx.jsonl</li>
</ul>
</li>
</ul>
</li>
</ul>
<p dir="auto">Each line of a jsonl file is a key-value pair of tokenized data {&#34;token_ids&#34;: [1,2,3,4,...]}.</p>
<p dir="auto">Our training code is compatible with the data pre-processing method in <a href="https://github.com/LLM360/amber-data-prep">https://github.com/LLM360/amber-data-prep</a>.</p>

<p dir="auto">The script <code>pretrain.sh</code> is provided to initiate training on a 1x8 node setup using torchrun. This script can be modified to adjust the <code>--nnodes</code> parameter and other settings to suit different multi-node configurations, such as those using slurm or torchx. The learning rate in the script is for 1x8 node with a batch size of 32. If you increase the number of nodes or the batch size, you need to increase the learning rate linearly.</p>
<p dir="auto">Steps to run:</p>
<ul dir="auto">
<li>In <code>pretrain.sh</code> file, specify the  <code>--train_data_local_path</code> to the pre-processed data in Step 2 and <code>--input_model_filename</code> to <code>./configs/{model_size}/</code>.</li>
<li>Run <code>bash pretrain.sh </code></li>
</ul>

<p dir="auto">The model weights is still under legal review. If you have any questions, feel free to email (zechunliu at meta dot com) and (cszhao at meta dot com)</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">Results on Zero-shot Common Sense Reasoning tasks</h2><a id="user-content-results-on-zero-shot-common-sense-reasoning-tasks" aria-label="Permalink: Results on Zero-shot Common Sense Reasoning tasks" href="#results-on-zero-shot-common-sense-reasoning-tasks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<table>
<thead>
<tr>
<th>model</th>
<th>boolq</th>
<th>piqa</th>
<th>siqa</th>
<th>hellaswag</th>
<th>winogrande</th>
<th>arc_easy</th>
<th>arc_challenge</th>
<th>obqa</th>
<th>avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPT-125M</td>
<td>41.3</td>
<td>25.2</td>
<td>57.5</td>
<td>62.0</td>
<td>41.9</td>
<td>31.1</td>
<td>31.2</td>
<td>50.8</td>
<td>42.6</td>
</tr>
<tr>
<td>GPT-neo-125M</td>
<td>40.7</td>
<td>24.8</td>
<td>61.3</td>
<td>62.5</td>
<td>41.9</td>
<td>29.7</td>
<td>31.6</td>
<td>50.7</td>
<td>42.9</td>
</tr>
<tr>
<td>Pythia-160M</td>
<td>40.0</td>
<td>25.3</td>
<td>59.5</td>
<td>62.0</td>
<td>41.5</td>
<td>29.9</td>
<td>31.2</td>
<td>50.9</td>
<td>42.5</td>
</tr>
<tr>
<td><strong>MobileLLM-125M</strong></td>
<td>43.9</td>
<td>27.1</td>
<td>60.2</td>
<td>65.3</td>
<td>42.4</td>
<td>38.9</td>
<td>39.5</td>
<td>53.1</td>
<td><strong>46.3</strong></td>
</tr>
<tr>
<td><strong>MobileLLM-LS-125M</strong></td>
<td>45.8</td>
<td>28.7</td>
<td>60.4</td>
<td>65.7</td>
<td>42.9</td>
<td>39.5</td>
<td>41.1</td>
<td>52.1</td>
<td><strong>47.0</strong></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>model</th>
<th>boolq</th>
<th>piqa</th>
<th>siqa</th>
<th>hellaswag</th>
<th>winogrande</th>
<th>arc_easy</th>
<th>arc_challenge</th>
<th>obqa</th>
<th>avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPT-350M</td>
<td>41.9</td>
<td>25.7</td>
<td>54.0</td>
<td>64.8</td>
<td>42.6</td>
<td>36.2</td>
<td>33.3</td>
<td>52.4</td>
<td>43.9</td>
</tr>
<tr>
<td>Pythia-410M</td>
<td>47.1</td>
<td>30.3</td>
<td>55.3</td>
<td>67.2</td>
<td>43.1</td>
<td>40.1</td>
<td>36.2</td>
<td>53.4</td>
<td>46.6</td>
</tr>
<tr>
<td><strong>MobileLLM-350M</strong></td>
<td>53.8</td>
<td>33.5</td>
<td>62.4</td>
<td>68.6</td>
<td>44.7</td>
<td>49.6</td>
<td>40.0</td>
<td>57.6</td>
<td><strong>51.3</strong></td>
</tr>
<tr>
<td><strong>MobileLLM-LS-350M</strong></td>
<td>54.4</td>
<td>32.5</td>
<td>62.8</td>
<td>69.8</td>
<td>44.1</td>
<td>50.6</td>
<td>45.8</td>
<td>57.2</td>
<td><strong>52.1</strong></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>model</th>
<th>boolq</th>
<th>piqa</th>
<th>siqa</th>
<th>hellaswag</th>
<th>winogrande</th>
<th>arc_easy</th>
<th>arc_challenge</th>
<th>obqa</th>
<th>avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen1.5-500M</td>
<td>54.7</td>
<td>32.1</td>
<td>46.9</td>
<td>68.9</td>
<td>46.0</td>
<td>48.8</td>
<td>37.7</td>
<td>55.0</td>
<td>48.8</td>
</tr>
<tr>
<td>BLOOM-560M</td>
<td>43.7</td>
<td>27.5</td>
<td>53.7</td>
<td>65.1</td>
<td>42.5</td>
<td>36.5</td>
<td>32.6</td>
<td>52.2</td>
<td>44.2</td>
</tr>
<tr>
<td>MobiLlama-800M</td>
<td>52.0</td>
<td>31.7</td>
<td>54.6</td>
<td>73.0</td>
<td>43.3</td>
<td>52.3</td>
<td>42.5</td>
<td>56.3</td>
<td>50.7</td>
</tr>
<tr>
<td><strong>MobileLLM-600M</strong></td>
<td>58.1</td>
<td>35.8</td>
<td>61.0</td>
<td>72.3</td>
<td>44.9</td>
<td>55.9</td>
<td>47.9</td>
<td>58.6</td>
<td><strong>54.3</strong></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>model</th>
<th>boolq</th>
<th>piqa</th>
<th>siqa</th>
<th>hellaswag</th>
<th>winogrande</th>
<th>arc_easy</th>
<th>arc_challenge</th>
<th>obqa</th>
<th>avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pythia-1B</td>
<td>49.9</td>
<td>30.4</td>
<td>58.7</td>
<td>69.2</td>
<td>43.3</td>
<td>47.4</td>
<td>38.6</td>
<td>52.2</td>
<td>48.7</td>
</tr>
<tr>
<td>MobiLlama-1B</td>
<td>59.7</td>
<td>38.4</td>
<td>59.2</td>
<td>74.5</td>
<td>44.9</td>
<td>62.0</td>
<td>43.7</td>
<td>59.0</td>
<td>55.2</td>
</tr>
<tr>
<td>Falcon-1B</td>
<td>59.5</td>
<td>38.4</td>
<td>63.9</td>
<td>74.6</td>
<td>44.6</td>
<td>62.9</td>
<td>45.6</td>
<td>60.9</td>
<td>56.3</td>
</tr>
<tr>
<td>BLOOM-1.1B</td>
<td>47.6</td>
<td>27.3</td>
<td>58.6</td>
<td>67.0</td>
<td>42.4</td>
<td>42.2</td>
<td>36.6</td>
<td>53.8</td>
<td>46.9</td>
</tr>
<tr>
<td>TinyLlama-1.1B</td>
<td>59.2</td>
<td>37.1</td>
<td>58.1</td>
<td>72.9</td>
<td>43.9</td>
<td>59.1</td>
<td>44.7</td>
<td>58.8</td>
<td>54.2</td>
</tr>
<tr>
<td><strong>MobileLLM-1B</strong></td>
<td>63.0</td>
<td>39.0</td>
<td>66.7</td>
<td>74.4</td>
<td>45.0</td>
<td>61.4</td>
<td>46.8</td>
<td>62.3</td>
<td><strong>57.3</strong></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>model</th>
<th>boolq</th>
<th>piqa</th>
<th>siqa</th>
<th>hellaswag</th>
<th>winogrande</th>
<th>arc_easy</th>
<th>arc_challenge</th>
<th>obqa</th>
<th>avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-neo-1.3B</td>
<td>51.3</td>
<td>33.0</td>
<td>61.8</td>
<td>70.9</td>
<td>43.7</td>
<td>48.6</td>
<td>41.2</td>
<td>54.5</td>
<td>50.6</td>
</tr>
<tr>
<td>OPT-1.3B</td>
<td>54.4</td>
<td>31.7</td>
<td>58.4</td>
<td>71.5</td>
<td>44.7</td>
<td>53.7</td>
<td>44.6</td>
<td>59.1</td>
<td>52.3</td>
</tr>
<tr>
<td>BLOOM-1.7B</td>
<td>50.9</td>
<td>31.2</td>
<td>61.7</td>
<td>70.0</td>
<td>43.2</td>
<td>47.2</td>
<td>36.2</td>
<td>56.1</td>
<td>49.6</td>
</tr>
<tr>
<td>Qwen1.5-1.8B</td>
<td>61.1</td>
<td>36.5</td>
<td>68.3</td>
<td>74.1</td>
<td>47.2</td>
<td>60.4</td>
<td>42.9</td>
<td>61.2</td>
<td>56.5</td>
</tr>
<tr>
<td>GPT-neo-2.7B</td>
<td>55.8</td>
<td>34.3</td>
<td>62.4</td>
<td>72.9</td>
<td>43.6</td>
<td>55.6</td>
<td>40.0</td>
<td>57.9</td>
<td>52.8</td>
</tr>
<tr>
<td>OPT-2.7B</td>
<td>56.6</td>
<td>34.6</td>
<td>61.8</td>
<td>74.5</td>
<td>45.6</td>
<td>60.2</td>
<td>48.2</td>
<td>59.6</td>
<td>55.1</td>
</tr>
<tr>
<td>Pythia-2.8B</td>
<td>59.4</td>
<td>38.9</td>
<td>66.1</td>
<td>73.8</td>
<td>44.5</td>
<td>59.6</td>
<td>45.0</td>
<td>59.4</td>
<td>55.8</td>
</tr>
<tr>
<td>BLOOM-3B</td>
<td>55.1</td>
<td>33.6</td>
<td>62.1</td>
<td>70.5</td>
<td>43.2</td>
<td>53.9</td>
<td>41.6</td>
<td>58.2</td>
<td>52.3</td>
</tr>
<tr>
<td><strong>MobileLLM-1.5B</strong></td>
<td>67.5</td>
<td>40.9</td>
<td>65.7</td>
<td>74.8</td>
<td>46.4</td>
<td>64.5</td>
<td>50.5</td>
<td>64.7</td>
<td><strong>59.4</strong></td>
</tr>
</tbody>
</table>

<p dir="auto">This code is partially based on HuggingFace transformer repo.</p>

<p dir="auto">Zechun Liu, Meta Inc (zechunliu at meta dot com)</p>
<p dir="auto">Changsheng Zhao, Meta Inc (cszhao at meta dot com)</p>

<p dir="auto">BiT is CC-BY-NC 4.0 licensed as of now.</p>
</article></div></div>
  </body>
</html>
