<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://edgebit.io/blog/automated-dependency-updates-with-ai/">Original</a>
    <h1>Principles for Building One-Shot AI Agents</h1>
    
    <div id="readability-page-1" class="page"><div>
	
	<div>
		<div>
			<p>EdgeBit is a security platform that helps application engineering teams to find <em>and fix</em> security vulnerabilities. The <a href="https://edgebit.io/blog/announcing-ai-dependency-autofix/">Dependency Autofix</a> feature contains an extremely accurate reachability engine to identify impact to your app. Most updates have no impact, so engineers using EdgeBit can dedicate efforts on impactful upgrades. This translates to more time spent “on-mission” instead of managing dependencies.</p>
<p>This post will cover how to identify areas that can 1) use focused tools, 2) smartly handle errors and 3) harness the persistence of an AI agent to unlock massive efficiency gains, like we have done for Dependency Autofix, with some data to back it up.</p>
<h2 id="what-is-a-one-shot-agent">What is a One-Shot Agent</h2>
<p>What is a “one-shot” AI Agent?</p>
<p>EdgeBit’s Dependency Autofix is built around one-shot code maintenance workflows - no human input is required. Unlike the typical AI within an IDE experience, where a developer manually accepts or rejects changes, our AI agents handle updates autonomously with a high degree of confidence.</p>
<p>Our confidence comes from three sources:</p>
<ol>
<li>static analysis that <em>deeply</em> understands how your app uses its dependencies</li>
<li>calculating and executing dependency updates</li>
<li>agentic workflow that is consistent and correct (what this post is about)</li>
</ol>
<p>Confidence is extremely important because one-shot agents must do something correctly or bail out before they do damage or ask a human for review. Since we want to keep engineers focused “on-mission”, we don’t want to call for review often.</p>
<h2 id="agent-vs-pipeline">Agent vs Pipeline</h2>
<p>Prior to introducing an agentic workflow for updating dependencies, our workflow was pipeline-based. This meant it was fairly deterministic since it’s given a concrete list of inputs and proceeds linearly.</p>
<p><img src="https://edgebit.io/img/blog-autofix-pipeline.svg"/></p><p>Experiments with an unrestricted and fully agentic workflow for automated dependency updates yielded a gain in “fuzziness” around inputs (what to update) and outputs (adapt code to API changes), but this was offset by a lot of chaos in the middle.</p>
<p><img src="https://edgebit.io/img/blog-autofix-chaos-agent.svg"/></p><p>Our goal was to keep as much of the determinism from the pipeline as possible, while benefiting from the desired fuzziness. We outline the three key principles that helped us achieve this, supported by data to validate our approach.</p>
<p><img src="https://edgebit.io/img/blog-autofix-focused-agent.svg"/></p><p>Our agent is run with a framework that produces a single binary containing all of the logic, system prompting and tools embedded within it, with certain tools executing Docker containers for isolation and repeatability. This is ideal for running within customer infrastructure or within the ephemeral VMs in our SaaS.</p>
<p>Here’s an example task prompt:</p>
<blockquote>
<p>Update javascript dependency react-redux to 9.2.0. Update first, then if there is potential impact to the app, adapt code to changes in the most minimal way, in a separate commit.</p>
</blockquote>
<p>Read on to learn how we achieved parity with our pipeline-based approach with a one-shot agent. But first, some evidence that a focused agent works.</p>
<h3 id="agent-consistency-amp-correctness">Agent Consistency &amp; Correctness</h3>
<p>We ran 10 iterations of this prompt, upgrading a package in 3 different size codebases to push the boundaries of consistency and correctness.</p>
<p>The <code>Basic App</code> is very simple but the upgrade brought in a complete rewrite of the library, although it remained API-compatible. This tests if the agent will get distracted by lots of churn.</p>
<p>The <code>Web App</code> required callsite mutation due to a CommonJS to ESM migration between the major versions.</p>
<p>The <code>Complex Webapp</code> was a very large codebase that required a library update but no call site mutation although 16 call sites called into the updated library. This allowed us to test the tendency for a model to stay in its lane, making only required changes.</p>
<div>
  <p><img src="https://edgebit.io/img/blog-ai-consistency.svg"/>
  </p>
</div>
<h3 id="one-shot-vs-claude-code">One-Shot vs Claude Code</h3>
<p>The web app with Redis test case was also benchmarked against Claude Code by simulating a one-shot workflow which was manually agreeing with every change it wanted to make.</p>
<p>As you can see, Claude Code was very inconsistent and had issues with almost every run. Incomplete runs typically didn’t update the lockfile along with the <code>package.json</code> or including Yarn metadata in our NPM project’s <code>package.json</code>.</p>
<p>In a few code mutation runs, Claude modified details about how our Redis sessions were namespaced, which would break expectations in the app.</p>
<div>
  <p><img src="https://edgebit.io/img/blog-ai-claude.svg"/>
  </p>
</div>
<p>When results matter, the correct context costs more. Running the best tools for the job cost more. When working on a one-shot workflow, we always prefer a correct result over almost anything.</p>
<p><img src="https://edgebit.io/img/blog-ai-cost.svg"/></p>
<p>Focused tools naturally have boundaries to their problem space. This allows them to exit early before wasting tons of tokens or, if conditions won’t allow them to be successful, return a meaningful error. This is really important feedback to the agent, vs having it interpret generic failures and spiral off into empty space.</p>
<p>Without this, a frequent failure mode is for two very generic tools like <code>exec</code> and <code>fs_put_file</code> to cause a loop of chaos, when simplified, looks like this:</p>

<pre><code><span><span>Agent</span>  <span>Info</span>   <span>Lets test out our changes</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>exec `node index.js`</span></span>
<span><span>Tool</span>   <span>Error</span>  <span>exit(1)</span></span>
<span><span>Agent</span>  <span>Info</span>   <span>Hmm, let&#39;s try a different way</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>Wrote changes to index.js</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>exec `node index.js`</span></span>
<span><span>Tool</span>   <span>Error</span>  <span>exit(1)</span></span>
<span><span>Agent</span>  <span>Info</span>   <span>Hmm, looks like the app won&#39;t run, let&#39;s write a test script</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>Wrote changes to test-update.js</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>exec `node test-update.js`</span></span>
<span><span>Tool</span>   <span>Error</span>  <span>exit(1)</span></span>
<span><span>Agent</span>  <span>Info</span>   <span>Lets try something else</span></span></code></pre>
<p>The solution is to provide a tool with a helpful description that wraps your common tasks, like building, testing, updating, downloading, etc.</p>
<h3 id="tools-reinforcing-workflows">Tools Reinforcing Workflows</h3>
<p>Focused tools also enable the use of well-proven libraries in places where models frequently fail, like semantic version comparisons. The LLM will reliably declare that <code>9.1.1 is already greater than 9.2.2, so the update is already installed</code>.</p>
<p>Providing just two tools can nudge the agent to anchor its logic in ground truth:</p>
<ol>
<li>executing an update: <code>js-update</code></li>
<li>understanding our dependencies: <code>inventory</code></li>
</ol>
<p>Let’s explore a brief example of how these tools work together.</p>
<p>The inventory tool hints in the description that it’s useful for pre and post-update which guides its use. It understands that lock files and similar metadata should be used as the source of truth. A nice side effect is that file reads now take the expected nanoseconds and you’re not wasting tokens interpreting files via the model.</p>


<div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> (<span>t</span> <span>*</span><span>InventoryTool</span>) <span>Description</span>() <span>string</span> {
</span></span><span><span>  <span>return</span> <span>&#34;Provides a list of all dependencies in the codebase, optionally filtered
</span></span></span><span><span><span>    by package name or ecosystem. Useful to research versions before an upgrade or
</span></span></span><span><span><span>    verifying an upgrade has been applied.&#34;</span>
</span></span><span><span>}</span></span></code></pre></div>
<p>The input/output descriptions also focus the agent on how to use the tool:</p>


<div><pre tabindex="0"><code data-lang="go"><span><span><span>type</span> <span>InventoryInput</span> <span>struct</span> {
</span></span><span><span>  <span>PackageNameFilter</span>      []<span>string</span> <span>`json:&#34;packageNameFilter&#34; jsonschema_description:&#34;Optional
</span></span></span><span><span><span>                                  list of complete package names to look up in our list of
</span></span></span><span><span><span>                                  dependencies. e.g. &#39;go.opentelemetry.io/otel/trace&#39;&#34;`</span>
</span></span><span><span>  <span>PackageEcosystemFilter</span> []<span>string</span> <span>`json:&#34;packageEcosystemFilter&#34; jsonschema_description:
</span></span></span><span><span><span>                                  &#34;Optional list of ecosystems to filter packages by in our
</span></span></span><span><span><span>                                  list of dependencies, eg &#39;gomod&#39; or &#39;npm&#39;&#34;`</span>
</span></span><span><span>  <span>Directory</span>              <span>string</span>   <span>`json:&#34;directory&#34; jsonschema_description:&#34;Optional
</span></span></span><span><span><span>                                  directory to run Inventory in&#34;`</span>
</span></span><span><span>}</span></span></code></pre></div>
<p>This can be input into the <code>js-update</code> tool and sanity checked with a soft or hard failure as discussed below.</p>


<div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> (<span>t</span> <span>*</span><span>UpdateTool</span>) <span>Execute</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>input</span> <span>Input</span>) (<span>Output</span>, <span>error</span>) {
</span></span><span><span>  <span>requestedPkgVersion</span>, <span>err</span> <span>:=</span> <span>semver</span>.<span>NewVersion</span>(<span>input</span>.<span>PackageVersion</span>)
</span></span><span><span>  <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
</span></span><span><span>    <span>// hard failure
</span></span></span><span><span><span></span>    <span>return</span> <span>Output</span>{}, <span>fmt</span>.<span>Errorf</span>(<span>“</span><span>Provide</span> <span>a</span> <span>valid</span> <span>semantic</span> <span>version</span> <span>or</span> <span>omit</span> <span>to</span> <span>find</span>
</span></span><span><span>      <span>the</span> <span>ideal</span> <span>version</span> <span>automatically</span><span>”</span>)
</span></span><span><span>  }
</span></span><span><span>  <span>// ...
</span></span></span><span><span><span></span>}</span></span></code></pre></div>
<h2 id="principle-2-fail-hard-or-soft-instead-of-being-wrong">Principle 2: Fail Hard or Soft, Instead of Being Wrong</h2>
<p>The opportunity to establish hard and soft failures is what allows EdgeBit to be consistently successful with one-shot dependency updates and code maintenance to adapt to new API changes. The last thing we want is to be wrong about fixing a security vulnerability or causing impact to your app – we’d rather fail and exit.</p>
<h3 id="hard-failures-exiting-the-bounding-box">Hard Failures: Exiting the Bounding Box</h3>
<p>Our mental model for failures is to establish a bounding box that represents our problem space and fail when we’d exit this area. Note that our problem space is not just “dependency updates” but “dependency updates for this specific app”.</p>
<p>If we can’t calculate an update graph that fulfills all of the version ranges for your dependencies, we must fail and communicate that to the user or make more drastic changes to the constraints.</p>
<p><img src="https://edgebit.io/img/blog-boundary-failure.svg"/></p><p>A bad scenario is letting the agent brute force its way into the update by guessing that version 1.2.3 is ok because lots of other apps in its training data have updated to it.</p>
<pre><code><span><span>Agent</span>  <span>Info</span>   <span>Let&#39;s update the lockfile to 1.2.3</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>Wrote package-lock.json</span>

</span></code></pre>
<p>Going from bad to worse, mutating a lockfile directly is 1) a bad practice and 2) can impact the app when 1.2.3 can’t actually be installed at build or deploy time. Now we’ve broken the app, ensured a human has to scramble to fix our mess and eradicated all the trust we’d built up.</p>
<p>Instead, we can simply log a fatal error as we exit the bounding box: valid updates <em>for this app</em>. This sets us up to try again or flag this update for review.</p>
<h3 id="soft-failures-a-gentle-nudge">Soft Failures: A Gentle Nudge</h3>
<p>Specialized tools can send back soft failures for the agent to try again or find a different path. A good example of this is version numbers. Our tools typically take in a desired package name or name + version.</p>
<p><img src="https://edgebit.io/img/blog-boundary-try-again.svg"/></p><p>Many times the LLM wants to pass the version <code>latest</code> when we expect an actual version. Passing back a soft failure hints that a real version is required, and if it doesn’t have one in its context, to use another tool to find it:</p>
<pre><code><span><span>Agent</span>  <span>Info</span>   <span>Let&#39;s use the js-update tool to update foo</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>Update to foo@latest</span></span>
<span><span>Tool</span>   <span>Error</span>  <span>Provide a valid version or omit to find the ideal version automatically</span></span>
<span><span>Agent</span>  <span>Info</span>   <span>Hmm, looks like we need to omit the version</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>Update to foo</span></span></code></pre>
<p>This gentle nudge means the agent will try to use our tool again and stay on the happy path.</p>
<h3 id="well-never-reach-our-goal">We’ll never reach our goal</h3>
<p>A different type of hard failure is when we detect that we’ll never reach our overall goal. This requires a goal that can be programmatically verified outside of the LLM.</p>
<p>For a dependency update, the update tool checks our metadata for <code>len(update.changes) &gt; 0</code>, which verifies that we updated <em>something</em>. If that is 0, no amount of running research tools, analyzing symbols, etc will allow us to be successful, so we exit. This is primarily a time and cost savings, so we can try again.</p>


<div><pre tabindex="0"><code data-lang="go"><span><span><span>if</span> len(<span>update</span>.<span>Changes</span>) <span>==</span> <span>0</span> {
</span></span><span><span>	<span>log</span>.<span>Fatal</span>(<span>&#34;No changes found in proposed update&#34;</span>)
</span></span><span><span>}</span></span></code></pre></div>
<h2 id="principle-3-persistence-when-being-too-determined-backfires">Principle 3: Persistence: When Being Too Determined Backfires</h2>
<p>LLMs are extremely persistent, especially when you instruct them to reason and research. We’ve seen them get caught in loops, churning through tokens, trying in vain to accomplish a goal. Here are two examples of how our focused tools and hard/soft failures mostly stop them from happening.</p>
<h3 id="installing-node">Installing Node</h3>
<p>EdgeBit works best when we control how <code>npm</code> is invoked and if <code>node</code> is run, so we don’t provide them in the container where the <code>exec</code> tool runs. It’s extremely persistent in attempting to install it:</p>
<pre><code><span><span>Tool</span>   <span>Info</span>   <span>exec `npm run`</span></span>
<span><span>Tool</span>   <span>Error</span>  <span>\&#34;npm\&#34; executable file not found in $PATH: unknown.</span></span>
<span><span>Agent</span>  <span>Info</span>   <span>Hmm, looks like we need to try a different way</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>exec `node index.js`</span></span>
<span><span>Tool</span>   <span>Error</span>  <span>\&#34;node\&#34; executable file not found in $PATH: unknown.</span></span>
<span><span>Agent</span>  <span>Info</span>   <span>Hmm, let&#39;s install node and npm</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>exec `apt-get install`</span></span>
<span><span>Tool</span>   <span>Error</span>  <span>\&#34;apt-get\&#34; executable file not found in $PATH: unknown.</span></span>
<span><span>Agent</span>  <span>Info</span>   <span>Hmm, let&#39;s try another way</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>exec `&#34;curl -L https://deb.nodesource.com/setup_18.x -o nodesource_setup.sh`</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>                                  Dload  Upload   Total   Spent    Left  Speed</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0</span></span>
<span><span>Tool</span>   <span>Info</span>   <span>exec `bash nodesource_setup.sh`</span></span>
<span><span>Tool</span>   <span>Error</span>  <span>exit(1):</span></span>
<span><span>Agent</span>  <span>Info</span>   <span>Hmm, something went wrong, let&#39;s move on</span></span>



</code></pre>
<h3 id="researching-github-projects">Researching GitHub Projects</h3>
<p>Another common loop to get into was our agent researching various locations on GitHub for info about breaking changes. In most cases it was paging through tons of results and churning through $1+ of tokens. If it guessed or hallucinated the project name incorrectly, it would then fall back to scouring NPM and other sources.</p>
<p>We exposed a project research tool that simply wrapped our SaaS platform’s source intelligence service. This was a huge win: we already had this data, focusing the agent was easy with a good tool description and no tokens were burned in the process. If we can’t find intel on the requested package, this is a soft failure point that can guide the agent vs its misdiagnosing of a 404.</p>
<p>Here’s how minimal a tool can be while providing the agent a ton of direction:</p>


<div><pre tabindex="0"><code data-lang="go"><span><span><span>func</span> (<span>t</span> <span>*</span><span>ResearchTool</span>) <span>Description</span>() <span>string</span> {
</span></span><span><span>	<span>return</span> <span>&#34;Use as the only source to research a package&#39;s release notes to understand
</span></span></span><span><span><span>    upgrade instructions and breaking changes. Compare contents with the impacted
</span></span></span><span><span><span>    call sites to verify missing breaking changes or find unmentioned breaking changes.&#34;</span>
</span></span><span><span>}
</span></span><span><span><span>func</span> (<span>t</span> <span>*</span><span>ResearchTool</span>) <span>Execute</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>input</span> <span>Input</span>) (<span>Output</span>, <span>error</span>) {
</span></span><span><span>  <span>//…
</span></span></span><span><span><span></span>  <span>err</span> <span>:=</span> <span>utils</span>.<span>PostJSON</span>(<span>ctx</span>, <span>apiURL</span>, <span>headers</span>, <span>payload</span>, <span>&amp;</span><span>intelResp</span>)
</span></span><span><span>  <span>//…
</span></span></span><span><span><span></span>  <span>releaseURL</span> <span>:=</span> <span>parseResponse</span>(<span>&amp;</span><span>intelResp</span>)
</span></span><span><span>  <span>err</span> = <span>utils</span>.<span>FetchJSON</span>(<span>ctx</span>, <span>releaseURL</span>, <span>nil</span>, <span>&amp;</span><span>notesResp</span>)
</span></span><span><span>  <span>//…
</span></span></span><span><span><span></span>  <span>return</span> <span>Output</span>{
</span></span><span><span>    <span>ReleaseNotes</span>: <span>notesResp</span>[<span>&#34;body&#34;</span>].(<span>string</span>),
</span></span><span><span>  }, <span>nil</span>
</span></span><span><span>}</span></span></code></pre></div>
<h2 id="building-one-shot-agents">Building One-Shot Agents</h2>
<p>With these principles, automated code maintenance becomes possible, safe and highly effective for engineering teams managing app dependencies and fixing security vulnerabilities. Developers can eliminate manual effort, minimize errors, and patch updates faster.</p>
<p>Other code maintenance tasks are ripe for one-shot automation. Identifying areas that can use focused tools, smartly handle errors and harness the persistence of the agent will unlock massive efficiency gains for software engineering teams — especially by removing tasks they hate in order to focus on their core mission and things that are fun.</p>

		</div>
	</div>
</div></div>
  </body>
</html>
