<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.brendangregg.com/perf.html">Original</a>
    <h1>Linux Perf Examples</h1>
    
    <div id="readability-page-1" class="page"><div>



<div><p><a href="https://www.brendangregg.com/perf_events/perf_events_map.png"><img src="https://www.brendangregg.com/perf_events/perf_events_map.png" width="500"/></a></p></div>
<p>These are some examples of using the <a href="https://perf.wiki.kernel.org/index.php/Main_Page">perf</a> Linux profiler, which has also been called Performance Counters for Linux (PCL), Linux perf events (LPE), or perf_events. Like <a href="http://web.eece.maine.edu/~vweaver/projects/perf_events/">Vince Weaver</a>, I&#39;ll call it perf_events so that you can search on that term later. Searching for just &#34;perf&#34; finds sites on the police, petroleum, weed control, and a <a href="https://www.brendangregg.com/perf_events/omg-so-perf.jpg">T-shirt</a>. This is not an official perf page, for either perf_events or the T-shirt.</p>


<p>perf_events is an event-oriented observability tool, which can help you solve advanced performance and troubleshooting functions. Questions that can be answered include:</p>

<ul>
<li>Why is the kernel on-CPU so much? What code-paths?</li>
<li>Which code-paths are causing CPU level 2 cache misses?</li>
<li>Are the CPUs stalled on memory I/O?</li>
<li>Which code-paths are allocating memory, and how much?</li>
<li>What is triggering TCP retransmits?</li>
<li>Is a certain kernel function being called, and how often?</li>
<li>What reasons are threads leaving the CPU?</li>
</ul>

<p>perf_events is part of the Linux kernel, under tools/perf. While it uses many Linux tracing features, some are not yet exposed via the perf command, and need to be used via the ftrace interface instead. My <a href="https://github.com/brendangregg/perf-tools">perf-tools</a> collection (github) uses both perf_events and ftrace as needed.</p>

<p>This page includes my examples of perf_events. A table of contents:</p>

<ul></ul>

<p>Key sections to start with are: <a href="#Events">Events</a>, <a href="#OneLiners">One-Liners</a>, <a href="#Presentations">Presentations</a>, <a href="#Prerequisites">Prerequisites</a>, <a href="#CPUstatistics">CPU statistics</a>, <a href="#TimedProfiling">Timed Profiling</a>, and <a href="#FlameGraphs">Flame Graphs</a>.
Also see my <a href="#Posts">Posts</a> about perf_events, and <a href="#Links">Links</a> for the main (official) perf_events page, awesome tutorial, and other links.
The next sections introduce perf_events further, starting with a screenshot, one-liners, and then background.</p>

<p>This page is under construction, and there&#39;s a lot more to perf_events that I&#39;d like to add. Hopefully this is useful so far.</p>

<h2>1. Screenshot</h2>

<p>Starting with a screenshot, here&#39;s perf version 3.9.3 tracing disk I/O:</p>

<pre># <b>perf record -e block:block_rq_issue -ag</b>
^C
# <b>ls -l perf.data</b>
-rw------- 1 root root 3458162 Jan 26 03:03 perf.data
# <b>perf report</b>
[...]
# Samples: 2K of event &#39;block:block_rq_issue&#39;
# Event count (approx.): 2216
#
# Overhead       Command      Shared Object                Symbol
# ........  ............  .................  ....................
#
    32.13%            dd  [kernel.kallsyms]  [k] blk_peek_request
                      |
                      --- blk_peek_request
                          virtblk_request
                          __blk_run_queue
                         |          
                         |--98.31%-- queue_unplugged
                         |          blk_flush_plug_list
                         |          |          
                         |          |--91.00%-- blk_queue_bio
                         |          |          generic_make_request
                         |          |          submit_bio
                         |          |          ext4_io_submit
                         |          |          |          
                         |          |          |--58.71%-- ext4_bio_write_page
                         |          |          |          mpage_da_submit_io
                         |          |          |          mpage_da_map_and_submit
                         |          |          |          write_cache_pages_da
                         |          |          |          ext4_da_writepages
                         |          |          |          do_writepages
                         |          |          |          __filemap_fdatawrite_range
                         |          |          |          filemap_flush
                         |          |          |          ext4_alloc_da_blocks
                         |          |          |          ext4_release_file
                         |          |          |          __fput
                         |          |          |          ____fput
                         |          |          |          task_work_run
                         |          |          |          do_notify_resume
                         |          |          |          int_signal
                         |          |          |          close
                         |          |          |          0x0
                         |          |          |          
                         |          |           --41.29%-- mpage_da_submit_io
[...]
</pre>

<p>A <tt>perf record</tt> command was used to trace the block:block_rq_issue probe, which fires when a block device I/O request is issued (disk I/O). Options included -a to trace all CPUs, and -g to capture call graphs (stack traces). Trace data is written to a perf.data file, and tracing ended when Ctrl-C was hit. A summary of the perf.data file was printed using <tt>perf report</tt>, which builds a tree from the stack traces, coalescing common paths, and showing percentages for each path.</p>

<p>The perf report output shows that 2,216 events were traced (disk I/O), 32% of which from a <tt>dd</tt> command. These were issued by the kernel function blk_peek_request(), and walking down the stacks, about half of these 32% were from the close() system call.</p>

<p>Note that I use the &#34;#&#34; prompt to signify that these commands were run as root, and I&#39;ll use &#34;$&#34; for user commands. Use sudo as needed.</p>

<h2>2. One-Liners</h2>

<p>Some useful one-liners I&#39;ve gathered or written. Terminology I&#39;m using, from lowest to highest overhead:</p>

<ul>
<li><b>statistics</b>/<b>count</b>: increment an integer counter on events</li>
<li><b>sample</b>: collect details (eg, instruction pointer or stack) from a subset of events (once every ...)</li>
<li><b>trace</b>: collect details from every event</li>
</ul>

<h3>Listing Events</h3>

<pre><SPAN color="#4040f0"># Listing all currently known events:</SPAN>
perf list

<SPAN color="#4040f0"># Listing sched tracepoints:</SPAN>
perf list &#39;sched:*&#39;
</pre>

<h3>Counting Events</h3>

<pre><SPAN color="#4040f0"># CPU counter statistics for the specified command:</SPAN>
perf stat <i>command</i>

<SPAN color="#4040f0"># Detailed CPU counter statistics (includes extras) for the specified command:</SPAN>
perf stat -d <i>command</i>

<SPAN color="#4040f0"># CPU counter statistics for the specified PID, until Ctrl-C:</SPAN>
perf stat -p <i>PID</i>

<SPAN color="#4040f0"># CPU counter statistics for the entire system, for 5 seconds:</SPAN>
perf stat -a sleep 5

<SPAN color="#4040f0"># Various basic CPU statistics, system wide, for 10 seconds:</SPAN>
perf stat -e cycles,instructions,cache-references,cache-misses,bus-cycles -a sleep 10

<SPAN color="#4040f0"># Various CPU level 1 data cache statistics for the specified command:</SPAN>
perf stat -e L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores <i>command</i>

<SPAN color="#4040f0"># Various CPU data TLB statistics for the specified command:</SPAN>
perf stat -e dTLB-loads,dTLB-load-misses,dTLB-prefetch-misses <i>command</i>

<SPAN color="#4040f0"># Various CPU last level cache statistics for the specified command:</SPAN>
perf stat -e LLC-loads,LLC-load-misses,LLC-stores,LLC-prefetches <i>command</i>

<SPAN color="#4040f0"># Using raw PMC counters, eg, counting unhalted core cycles:</SPAN>
perf stat -e r003c -a sleep 5 

<SPAN color="#4040f0"># PMCs: counting cycles and frontend stalls via raw specification:</SPAN>
perf stat -e cycles -e cpu/event=0x0e,umask=0x01,inv,cmask=0x01/ -a sleep 5

<SPAN color="#4040f0"># Count syscalls per-second system-wide:</SPAN>
perf stat -e raw_syscalls:sys_enter -I 1000 -a

<SPAN color="#4040f0"># Count system calls by type for the specified PID, until Ctrl-C:</SPAN>
perf stat -e &#39;syscalls:sys_enter_*&#39; -p <i>PID</i>

<SPAN color="#4040f0"># Count system calls by type for the entire system, for 5 seconds:</SPAN>
perf stat -e &#39;syscalls:sys_enter_*&#39; -a sleep 5

<SPAN color="#4040f0"># Count scheduler events for the specified PID, until Ctrl-C:</SPAN>
perf stat -e &#39;sched:*&#39; -p <i>PID</i>

<SPAN color="#4040f0"># Count scheduler events for the specified PID, for 10 seconds:</SPAN>
perf stat -e &#39;sched:*&#39; -p <i>PID</i> sleep 10

<SPAN color="#4040f0"># Count ext4 events for the entire system, for 10 seconds:</SPAN>
perf stat -e &#39;ext4:*&#39; -a sleep 10

<SPAN color="#4040f0"># Count block device I/O events for the entire system, for 10 seconds:</SPAN>
perf stat -e &#39;block:*&#39; -a sleep 10

<SPAN color="#4040f0"># Count all vmscan events, printing a report every second:</SPAN>
perf stat -e &#39;vmscan:*&#39; -a -I 1000
</pre>

<h3>Profiling</h3>

<pre><SPAN color="#4040f0"># Sample on-CPU functions for the specified command, at 99 Hertz:</SPAN>
perf record -F 99 <i>command</i>

<SPAN color="#4040f0"># Sample on-CPU functions for the specified PID, at 99 Hertz, until Ctrl-C:</SPAN>
perf record -F 99 -p <i>PID</i>

<SPAN color="#4040f0"># Sample on-CPU functions for the specified PID, at 99 Hertz, for 10 seconds:</SPAN>
perf record -F 99 -p <i>PID</i> sleep 10

<SPAN color="#4040f0"># Sample CPU stack traces (via frame pointers) for the specified PID, at 99 Hertz, for 10 seconds:</SPAN>
perf record -F 99 -p <i>PID</i> -g -- sleep 10

<SPAN color="#4040f0"># Sample CPU stack traces for the PID, using dwarf (dbg info) to unwind stacks, at 99 Hertz, for 10 seconds:</SPAN>
perf record -F 99 -p <i>PID</i> --call-graph dwarf sleep 10

<SPAN color="#4040f0"># Sample CPU stack traces for the entire system, at 99 Hertz, for 10 seconds (&lt; Linux 4.11):</SPAN>
perf record -F 99 -ag -- sleep 10

<SPAN color="#4040f0"># Sample CPU stack traces for the entire system, at 99 Hertz, for 10 seconds (&gt;= Linux 4.11):</SPAN>
perf record -F 99 -g -- sleep 10

<SPAN color="#4040f0"># If the previous command didn&#39;t work, try forcing perf to use the cpu-clock event:</SPAN>
perf record -F 99 -e cpu-clock -ag -- sleep 10

<SPAN color="#4040f0"># Sample CPU stack traces for a container identified by its /sys/fs/cgroup/perf_event cgroup:</SPAN>
perf record -F 99 -e cpu-clock --cgroup=docker/1d567f4393190204<i>...etc...</i> -a -- sleep 10

<SPAN color="#4040f0"># Sample CPU stack traces for the entire system, with dwarf stacks, at 99 Hertz, for 10 seconds:</SPAN>
perf record -F 99 -a --call-graph dwarf sleep 10

<SPAN color="#4040f0"># Sample CPU stack traces for the entire system, using last branch record for stacks, ... (&gt;= Linux 4.?):</SPAN>
perf record -F 99 -a --call-graph lbr sleep 10

<SPAN color="#4040f0"># Sample CPU stack traces, once every 10,000 Level 1 data cache misses, for 5 seconds:</SPAN>
perf record -e L1-dcache-load-misses -c 10000 -ag -- sleep 5

<SPAN color="#4040f0"># Sample CPU stack traces, once every 100 last level cache misses, for 5 seconds:</SPAN>
perf record -e LLC-load-misses -c 100 -ag -- sleep 5 

<SPAN color="#4040f0"># Sample on-CPU kernel instructions, for 5 seconds:</SPAN>
perf record -e cycles:k -a -- sleep 5 

<SPAN color="#4040f0"># Sample on-CPU user instructions, for 5 seconds:</SPAN>
perf record -e cycles:u -a -- sleep 5 

<SPAN color="#4040f0"># Sample on-CPU user instructions precisely (using PEBS), for 5 seconds:</SPAN>
perf record -e cycles:up -a -- sleep 5 

<SPAN color="#4040f0"># Perform branch tracing (needs HW support), for 1 second:</SPAN>
perf record -b -a sleep 1

<SPAN color="#4040f0"># Sample CPUs at 49 Hertz, and show top addresses and symbols, live (no perf.data file):</SPAN>
perf top -F 49

<SPAN color="#4040f0"># Sample CPUs at 49 Hertz, and show top process names and segments, live:</SPAN>
perf top -F 49 -ns comm,dso
</pre>

<h3>Static Tracing</h3>

<pre><SPAN color="#4040f0"># Trace new processes, until Ctrl-C:</SPAN>
perf record -e sched:sched_process_exec -a

<SPAN color="#4040f0"># Sample (take a subset of) context-switches, until Ctrl-C:</SPAN>
perf record -e context-switches -a

<SPAN color="#4040f0"># Trace all context-switches, until Ctrl-C:</SPAN>
perf record -e context-switches -c 1 -a

<SPAN color="#4040f0"># Include raw settings used (see: man perf_event_open):</SPAN>
perf record -vv -e context-switches -a

<SPAN color="#4040f0"># Trace all context-switches via sched tracepoint, until Ctrl-C:</SPAN>
perf record -e sched:sched_switch -a

<SPAN color="#4040f0"># Sample context-switches with stack traces, until Ctrl-C:</SPAN>
perf record -e context-switches -ag

<SPAN color="#4040f0"># Sample context-switches with stack traces, for 10 seconds:</SPAN>
perf record -e context-switches -ag -- sleep 10

<SPAN color="#4040f0"># Sample CS, stack traces, and with timestamps (&lt; Linux 3.17, -T now default):</SPAN>
perf record -e context-switches -ag -T

<SPAN color="#4040f0"># Sample CPU migrations, for 10 seconds:</SPAN>
perf record -e migrations -a -- sleep 10

<SPAN color="#4040f0"># Trace all connect()s with stack traces (outbound connections), until Ctrl-C:</SPAN>
perf record -e syscalls:sys_enter_connect -ag

<SPAN color="#4040f0"># Trace all accepts()s with stack traces (inbound connections), until Ctrl-C:</SPAN>
perf record -e syscalls:sys_enter_accept* -ag

<SPAN color="#4040f0"># Trace all block device (disk I/O) requests with stack traces, until Ctrl-C:</SPAN>
perf record -e block:block_rq_insert -ag

<SPAN color="#4040f0"># Sample at most 100 block device requests per second, until Ctrl-C:</SPAN>
perf record -F 100 -e block:block_rq_insert -a

<SPAN color="#4040f0"># Trace all block device issues and completions (has timestamps), until Ctrl-C:</SPAN>
perf record -e block:block_rq_issue -e block:block_rq_complete -a

<SPAN color="#4040f0"># Trace all block completions, of size at least 100 Kbytes, until Ctrl-C:</SPAN>
perf record -e block:block_rq_complete --filter &#39;nr_sector &gt; 200&#39;

<SPAN color="#4040f0"># Trace all block completions, synchronous writes only, until Ctrl-C:</SPAN>
perf record -e block:block_rq_complete --filter &#39;rwbs == &#34;WS&#34;&#39;

<SPAN color="#4040f0"># Trace all block completions, all types of writes, until Ctrl-C:</SPAN>
perf record -e block:block_rq_complete --filter &#39;rwbs ~ &#34;*W*&#34;&#39;

<SPAN color="#4040f0"># Sample minor faults (RSS growth) with stack traces, until Ctrl-C:</SPAN>
perf record -e minor-faults -ag

<SPAN color="#4040f0"># Trace all minor faults with stack traces, until Ctrl-C:</SPAN>
perf record -e minor-faults -c 1 -ag

<SPAN color="#4040f0"># Sample page faults with stack traces, until Ctrl-C:</SPAN>
perf record -e page-faults -ag

<SPAN color="#4040f0"># Trace all ext4 calls, and write to a non-ext4 location, until Ctrl-C:</SPAN>
perf record -e &#39;ext4:*&#39; -o /tmp/perf.data -a 

<SPAN color="#4040f0"># Trace kswapd wakeup events, until Ctrl-C:</SPAN>
perf record -e vmscan:mm_vmscan_wakeup_kswapd -ag

<SPAN color="#4040f0"># Add Node.js USDT probes (Linux 4.10+):</SPAN>
perf buildid-cache --add `which node`

<SPAN color="#4040f0"># Trace the node http__server__request USDT event (Linux 4.10+):</SPAN>
perf record -e sdt_node:http__server__request -a
</pre>

<h3>Dynamic Tracing</h3>

<pre><SPAN color="#4040f0"># Add a tracepoint for the kernel tcp_sendmsg() function entry (&#34;--add&#34; is optional):</SPAN>
perf probe --add tcp_sendmsg

<SPAN color="#4040f0"># Remove the tcp_sendmsg() tracepoint (or use &#34;--del&#34;):</SPAN>
perf probe -d tcp_sendmsg

<SPAN color="#4040f0"># Add a tracepoint for the kernel tcp_sendmsg() function return:</SPAN>
perf probe &#39;tcp_sendmsg%return&#39;

<SPAN color="#4040f0"># Show available variables for the kernel tcp_sendmsg() function (needs debuginfo):</SPAN>
perf probe -V tcp_sendmsg

<SPAN color="#4040f0"># Show available variables for the kernel tcp_sendmsg() function, plus external vars (needs debuginfo):</SPAN>
perf probe -V tcp_sendmsg --externs

<SPAN color="#4040f0"># Show available line probes for tcp_sendmsg() (needs debuginfo):</SPAN>
perf probe -L tcp_sendmsg

<SPAN color="#4040f0"># Show available variables for tcp_sendmsg() at line number 81 (needs debuginfo):</SPAN>
perf probe -V tcp_sendmsg:81

<SPAN color="#4040f0"># Add a tracepoint for tcp_sendmsg(), with three entry argument registers (platform specific):</SPAN>
perf probe &#39;tcp_sendmsg %ax %dx %cx&#39;

<SPAN color="#4040f0"># Add a tracepoint for tcp_sendmsg(), with an alias (&#34;bytes&#34;) for the %cx register (platform specific):</SPAN>
perf probe &#39;tcp_sendmsg bytes=%cx&#39;

<SPAN color="#4040f0"># Trace previously created probe when the bytes (alias) variable is greater than 100:</SPAN>
perf record -e probe:tcp_sendmsg --filter &#39;bytes &gt; 100&#39;

<SPAN color="#4040f0"># Add a tracepoint for tcp_sendmsg() return, and capture the return value:</SPAN>
perf probe &#39;tcp_sendmsg%return $retval&#39;

<SPAN color="#4040f0"># Add a tracepoint for tcp_sendmsg(), and &#34;size&#34; entry argument (reliable, but needs debuginfo):</SPAN>
perf probe &#39;tcp_sendmsg size&#39;

<SPAN color="#4040f0"># Add a tracepoint for tcp_sendmsg(), with size and socket state (needs debuginfo):</SPAN>
perf probe &#39;tcp_sendmsg size sk-&gt;__sk_common.skc_state&#39;

<SPAN color="#4040f0"># Tell me how on Earth you would do this, but don&#39;t actually do it (needs debuginfo):</SPAN>
perf probe -nv &#39;tcp_sendmsg size sk-&gt;__sk_common.skc_state&#39;

<SPAN color="#4040f0"># Trace previous probe when size is non-zero, and state is not TCP_ESTABLISHED(1) (needs debuginfo):</SPAN>
perf record -e probe:tcp_sendmsg --filter &#39;size &gt; 0 &amp;&amp; skc_state != 1&#39; -a

<SPAN color="#4040f0"># Add a tracepoint for tcp_sendmsg() line 81 with local variable seglen (needs debuginfo):</SPAN>
perf probe &#39;tcp_sendmsg:81 seglen&#39;

<SPAN color="#4040f0"># Add a tracepoint for do_sys_open() with the filename as a string (needs debuginfo):</SPAN>
perf probe &#39;do_sys_open filename:string&#39;

<SPAN color="#4040f0"># Add a tracepoint for myfunc() return, and include the retval as a string:</SPAN>
perf probe &#39;myfunc%return +0($retval):string&#39;

<SPAN color="#4040f0"># Add a tracepoint for the user-level malloc() function from libc:</SPAN>
perf probe -x /lib64/libc.so.6 malloc

<SPAN color="#4040f0"># Add a tracepoint for this user-level static probe (USDT, aka SDT event):</SPAN>
perf probe -x /usr/lib64/libpthread-2.24.so %sdt_libpthread:mutex_entry

<SPAN color="#4040f0"># List currently available dynamic probes:</SPAN>
perf probe -l
</pre>

<h3>Mixed</h3>

<pre><SPAN color="#4040f0"># Trace system calls by process, showing a summary refreshing every 2 seconds:</SPAN>
perf top -e raw_syscalls:sys_enter -ns comm

<SPAN color="#4040f0"># Trace sent network packets by on-CPU process, rolling output (no clear):</SPAN>
stdbuf -oL perf top -e net:net_dev_xmit -ns comm | strings

<SPAN color="#4040f0"># Sample stacks at 99 Hertz, and, context switches:</SPAN>
perf record -F99 -e cpu-clock -e cs -a -g 

<SPAN color="#4040f0"># Sample stacks to 2 levels deep, and, context switch stacks to 5 levels (needs 4.8):</SPAN>
perf record -F99 -e cpu-clock/max-stack=2/ -e cs/max-stack=5/ -a -g 
</pre>

<h3>Special</h3>

<pre><SPAN color="#4040f0"># Record cacheline events (Linux 4.10+):</SPAN>
perf c2c record -a -- sleep 10

<SPAN color="#4040f0"># Report cacheline events from previous recording (Linux 4.10+):</SPAN>
perf c2c report
</pre>

<h3>Reporting</h3>

<pre><SPAN color="#4040f0"># Show perf.data in an ncurses browser (TUI) if possible:</SPAN>
perf report

<SPAN color="#4040f0"># Show perf.data with a column for sample count:</SPAN>
perf report -n

<SPAN color="#4040f0"># Show perf.data as a text report, with data coalesced and percentages:</SPAN>
perf report --stdio

<SPAN color="#4040f0"># Report, with stacks in folded format: one line per stack (needs 4.4):</SPAN>
perf report --stdio -n -g folded

<SPAN color="#4040f0"># List all events from perf.data:</SPAN>
perf script

<SPAN color="#4040f0"># List all perf.data events, with data header (newer kernels; was previously default):</SPAN>
perf script --header

<SPAN color="#4040f0"># List all perf.data events, with customized fields (&lt; Linux 4.1):</SPAN>
perf script -f time,event,trace

<SPAN color="#4040f0"># List all perf.data events, with customized fields (&gt;= Linux 4.1):</SPAN>
perf script -F time,event,trace

<SPAN color="#4040f0"># List all perf.data events, with my recommended fields (needs record -a; newer kernels):</SPAN>
perf script --header -F comm,pid,tid,cpu,time,event,ip,sym,dso 

<SPAN color="#4040f0"># List all perf.data events, with my recommended fields (needs record -a; older kernels):</SPAN>
perf script -f comm,pid,tid,cpu,time,event,ip,sym,dso

<SPAN color="#4040f0"># Dump raw contents from perf.data as hex (for debugging):</SPAN>
perf script -D

<SPAN color="#4040f0"># Disassemble and annotate instructions with percentages (needs some debuginfo):</SPAN>
perf annotate --stdio
</pre>

<p>These one-liners serve to illustrate the capabilities of perf_events, and can also be used a bite-sized tutorial: learn perf_events one line at a time. You can also print these out as a perf_events cheatsheet.</p>

<h2>3. Presentations</h2>

<h3>Kernel Recipes (2017)</h3>

<p>At <a href="https://kernel-recipes.org/en/2017/talks/perf-in-netflix/">Kernel Recipes 2017</a> I gave an updated talk on Linux perf at Netflix, focusing on getting CPU profiling and flame graphs to work. This talk includes a crash course on perf_events, plus gotchas such as fixing stack traces and symbols when profiling Java, Node.js, VMs, and containers.</p>

<p>A video of the talk is on <a href="https://www.youtube.com/watch?v=UVM3WX8Lq2k">youtube</a> and the slides are on <a href="https://www.slideshare.net/brendangregg/kernel-recipes-2017-using-linux-perf-at-netflix">slideshare</a>:</p>

<p>
<a href="https://www.youtube.com/watch?v=UVM3WX8Lq2k"><img src="https://www.brendangregg.com/Images/youtube-KernelRecipes2017.png"/></a>
<!-- <iframe width="520" height="291" src="https://www.youtube.com/embed/UVM3WX8Lq2k" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe> -->
</p>
<!-- 595 485 -->

<p>There&#39;s also an older version of this talk from 2015, which I&#39;ve <a href="https://www.brendangregg.com/blog/2015-02-27/linux-profiling-at-netflix.html">posted</a> about.</p>

<!--
<a name="SCALE13x"></a>
<h3>SCALE13x (2015)</h3>

<p>I gave my original perf talk at the Southern California Linux Expo (<a href="http://www.socallinuxexpo.org/scale/13x">SCALE 13x</a>), covering CPU profiling and a tour of other features.</p>

<p>A video of the talk is on <a href="https://www.youtube.com/watch?v=_Ik8oiQvWgo">youtube</a>, and the slides are on <a href="http://www.slideshare.net/brendangregg/scale2015-linux-perfprofiling">slideshare</a>.

<p><iframe src="https://www.slideshare.net/slideshow/embed_code/44966387" width="375" height="311" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:0px; max-width: 100%;" allowfullscreen> </iframe>
<iframe width="520" height="291" src="https://www.youtube.com/embed/_Ik8oiQvWgo" frameborder="0" allowfullscreen></iframe></p>

<p>In a <a href="/blog/2015-02-27/linux-profiling-at-netflix.html">post</a> about this talk, I included the interactive CPU flame graph SVG I was demonstrating.</p>
-->



<p>The following sections provide some background for understanding perf_events and how to use it. I&#39;ll describe the prerequisites, audience, usage, events, and tracepoints.</p>

<h2>4.1. Prerequisites</h2>

<p>The <tt>perf</tt> tool is in the <b>linux-tools-common</b> package. Start by adding that, then running &#34;<tt>perf</tt>&#34; to see if you get the USAGE message. It may tell you to install another related package (linux-tools-<i>kernelversion</i>).</p>

<p>You can also build and add <tt>perf</tt> from the Linux kernel source. See the <a href="#Building">Building</a> section.</p>

<p>To get the most out <tt>perf</tt>, you&#39;ll want symbols and stack traces. These may work by default in your Linux distribution, or they may require the addition of packages, or recompilation of the kernel with additional config options.</p>

<h2>4.2. Symbols</h2>

<p>perf_events, like other debug tools, needs symbol information (symbols). These are used to translate memory addresses into function and variable names, so that they can be read by us humans. Without symbols, you&#39;ll see hexadecimal numbers representing the memory addresses profiled.</p>

<p>The following <tt>perf report</tt> output shows stack traces, however, only hexadecimal numbers can be seen:</p>

<pre>    57.14%     sshd  libc-2.15.so        [.] connect           
               |
               --- connect
                  |          
                  |--25.00%-- 0x7ff3c1cddf29
                  |          
                  |--25.00%-- 0x7ff3bfe82761
                  |          0x7ff3bfe82b7c
                  |          
                  |--25.00%-- 0x7ff3bfe82dfc
                   --25.00%-- [...]
</pre>

<p>If the software was added by packages, you may find debug packages (often &#34;-dbgsym&#34;) which provide the symbols.  Sometimes <tt>perf report</tt> will tell you to install these, eg: &#34;no symbols found in /bin/dd, maybe install a debug package?&#34;.</p>

<p>Here&#39;s the same <tt>perf report</tt> output seen earlier, after adding openssh-server-dbgsym and libc6-dbgsym (this is on ubuntu 12.04):</p>

<pre>    57.14%     sshd  libc-2.15.so        [.] __GI___connect_internal
               |
               --- __GI___connect_internal
                  |          
                  |--25.00%-- add_one_listen_addr.isra.0
                  |          
                  |--25.00%-- __nscd_get_mapping
                  |          __nscd_get_map_ref
                  |          
                  |--25.00%-- __nscd_open_socket
                   --25.00%-- [...]
</pre>

<p>I find it useful to add both libc6-dbgsym and coreutils-dbgsym, to provide some symbol coverage of user-level OS codepaths.</p>

<p>Another way to get symbols is to compile the software yourself. For example, I just compiled node (Node.js):</p>

<pre># file node-v0.10.28/out/Release/node 
node-v0.10.28/out/Release/node: ELF 64-bit LSB executable, ... <b>not stripped</b>
</pre>

<p>This has not been stripped, so I can profile node and see more than just hex. If the result is stripped, configure your build system not to run strip(1) on the output binaries.</p>

<p>Kernel-level symbols are in the kernel debuginfo package, or when the kernel is compiled with CONFIG_KALLSYMS.</p>

<h2>4.3. JIT Symbols (Java, Node.js)</h2>

<p>Programs that have virtual machines (VMs), like Java&#39;s JVM and node&#39;s v8, execute their own virtual processor, which has its own way of executing functions and managing stacks. If you profile these using perf_events, you&#39;ll see symbols for the VM engine, which have some use (eg, to identify if time is spent in GC), but you won&#39;t see the language-level context you might be expecting. Eg, you won&#39;t see Java classes and methods.</p>

<p>perf_events has JIT support to solve this, which requires the VM to maintain a /tmp/perf-PID.map file for symbol translation. Java can do this with <a href="https://github.com/jvm-profiling-tools/perf-map-agent">perf-map-agent</a>, and Node.js 0.11.13+ with --perf_basic_prof. See my blog post <a href="http://www.brendangregg.com/blog/2014-09-17/node-flame-graphs-on-linux.html">Node.js flame graphs on Linux</a> for the steps.</p>

<p>Note that Java may not show full stacks to begin with, due to hotspot on x86 omitting the frame pointer (just like gcc). On newer versions (JDK 8u60+), you can use the -XX:+PreserveFramePointer option to fix this behavior, and profile fully using perf. See my Netflix Tech Blog post, <a href="http://techblog.netflix.com/2015/07/java-in-flames.html">Java in Flames</a>, for a full writeup, and my <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#Java">Java flame graphs</a> section, which links to an older patch and includes an example resulting flame graph. I also summarized the latest in my JavaOne 2016 talk <a href="http://www.slideshare.net/brendangregg/java-performance-analysis-on-linux-with-flame-graphs">Java Performance Analysis on Linux with Flame Graphs</a>.</p>

<h2>4.4 Stack Traces</h2>

<p>Always compile with frame pointers. Omitting frame pointers is an evil compiler optimization that breaks debuggers, and sadly, is often the default. Without them, you may see incomplete stacks from perf_events, like seen in the earlier sshd symbols example. There are three ways to fix this: either using dwarf data to unwind the stack, using last branch record (LBR) if available (a processor feature), or returning the frame pointers.</p>

<p>There are other stack walking techniques, like BTS (Branch Trace Store), and the new ORC unwinder. I&#39;ll add docs for them at some point (and as perf support arrives).</p>

<p><b>Frame Pointers</b></p>

<p>The earlier sshd example was a default build of OpenSSH, which uses compiler optimizations (-O2), which in this case has omitted the frame pointer. Here&#39;s how it looks after recompiling OpenSSH with <b>-fno-omit-frame-pointer</b>:</p>

<pre>    100.00%     sshd  libc-2.15.so   [.] __GI___connect_internal
               |
               --- __GI___connect_internal
                  |          
                  |--30.00%-- add_one_listen_addr.isra.0
                  |          add_listen_addr
                  |          fill_default_server_options
                  |          main
                  |          __libc_start_main
                  |          
                  |--20.00%-- __nscd_get_mapping
                  |          __nscd_get_map_ref
                  |          
                  |--20.00%-- __nscd_open_socket
                   --30.00%-- [...]
</pre>

<p>Now the ancestry from add_one_listen_addr() can be seen, down to main() and __libc_start_main().</p>

<p>The kernel can suffer the same problem. Here&#39;s an example CPU profile collected on an idle server, with stack traces (-g):</p>

<pre>    99.97%  swapper  [kernel.kallsyms]  [k] default_idle
            |
            --- default_idle

     0.03%     sshd  [kernel.kallsyms]  [k] iowrite16   
               |
               --- iowrite16
                   __write_nocancel
                   (nil)
</pre>

<p>The kernel stack traces are incomplete. Now a similar profile with <b>CONFIG_FRAME_POINTER=y</b>:</p>

<pre>    99.97%  swapper  [kernel.kallsyms]  [k] default_idle
            |
            --- default_idle
                cpu_idle
               |          
               |--87.50%-- start_secondary
               |          
                --12.50%-- rest_init
                          start_kernel
                          x86_64_start_reservations
                          x86_64_start_kernel

     0.03%     sshd  [kernel.kallsyms]  [k] iowrite16
               |
               --- iowrite16
                   vp_notify
                   virtqueue_kick
                   start_xmit
                   dev_hard_start_xmit
                   sch_direct_xmit
                   dev_queue_xmit
                   ip_finish_output
                   ip_output
                   ip_local_out
                   ip_queue_xmit
                   tcp_transmit_skb
                   tcp_write_xmit
                   __tcp_push_pending_frames
                   tcp_sendmsg
                   inet_sendmsg
                   sock_aio_write
                   do_sync_write
                   vfs_write
                   sys_write
                   system_call_fastpath
                   __write_nocancel
</pre>

<p>Much better -- the entire path from the write() syscall (__write_nocancel) to iowrite16() can be seen.</p>

<p><b>Dwarf</b></p>

<p>Since about the 3.9 kernel, perf_events has supported a workaround for missing frame pointers in user-level stacks: libunwind, which uses dwarf. This can be enabled using &#34;--call-graph dwarf&#34; (or &#34;-g dwarf&#34;).</p>

<p>Also see the <a href="#Building">Building</a> section for other notes about building perf_events, as without the right library, it may build itself without dwarf support.</p>

<p><b>LBR</b></p>

<p>You must have Last Branch Record access to be able to use this. It is disabled in most cloud environments, where you&#39;ll get this error:</p>

<pre># <b>perf record -F 99 -a --call-graph lbr</b>
Error:
PMU Hardware doesn&#39;t support sampling/overflow-interrupts.
</pre>

<p>Here&#39;s an example of it working:</p>

<pre># <b>perf record -F 99 -a --call-graph lbr</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.903 MB perf.data (163 samples) ]
# <b>perf script</b>
[...]
stackcollapse-p 23867 [007] 4762187.971824:   29003297 cycles:ppp:
                  1430c0 Perl_re_intuit_start (/usr/bin/perl)
                  144118 Perl_regexec_flags (/usr/bin/perl)
                   cfcc9 Perl_pp_match (/usr/bin/perl)
                   cbee3 Perl_runops_standard (/usr/bin/perl)
                   51fb3 perl_run (/usr/bin/perl)
                   2b168 main (/usr/bin/perl)

stackcollapse-p 23867 [007] 4762187.980184:   31532281 cycles:ppp:
                   e3660 Perl_sv_force_normal_flags (/usr/bin/perl)
                  109b86 Perl_leave_scope (/usr/bin/perl)
                  1139db Perl_pp_leave (/usr/bin/perl)
                   cbee3 Perl_runops_standard (/usr/bin/perl)
                   51fb3 perl_run (/usr/bin/perl)
                   2b168 main (/usr/bin/perl)

stackcollapse-p 23867 [007] 4762187.989283:   32341031 cycles:ppp:
                   cfae0 Perl_pp_match (/usr/bin/perl)
                   cbee3 Perl_runops_standard (/usr/bin/perl)
                   51fb3 perl_run (/usr/bin/perl)
                   2b168 main (/usr/bin/perl)
</pre>

<p>Nice! Note that LBR is usually limited in stack depth (either 8, 16, or 32 frames), so it may not be suitable for deep stacks or flame graph generation, as flame graphs need to walk to the common root for merging.</p>

<p>Here&#39;s that same program sampled using the by-default frame pointer walk:</p>

<pre># <b>perf record -F 99 -a -g</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.882 MB perf.data (81 samples) ]
# <b>perf script</b>
[...]
stackcollapse-p 23883 [005] 4762405.747834:   35044916 cycles:ppp:
                  135b83 [unknown] (/usr/bin/perl)

stackcollapse-p 23883 [005] 4762405.757935:   35036297 cycles:ppp:
                   ee67d Perl_sv_gets (/usr/bin/perl)

stackcollapse-p 23883 [005] 4762405.768038:   35045174 cycles:ppp:
                  137334 [unknown] (/usr/bin/perl)
</pre>

<p>You can recompile Perl with frame pointer support (in its ./Configure, it asks what compiler options: add -fno-omit-frame-pointer). Or you can use LBR if it&#39;s available, and you don&#39;t need very long stacks.</p>

<h2>4.5. Audience</h2>

<p>To use perf_events, you&#39;ll either:</p>

<ul>
<li>Develop your own commands</li>
<li>Run example commands</li>
</ul>

<p>Developing new invocations of perf_events requires the study of kernel and application code, which isn&#39;t for everyone. Many more people will use perf_events by running commands developed by other people, like the examples on this page. This can work out fine: your organization may only need one or two people who can develop perf_events commands or source them, and then share them for use by the entire operation and support groups.</p>

<p>Either way, you need to know the capabilities of perf_events so you know when to reach for it, whether that means searching for an example command or writing your own. One goal of the examples that follow is just to show you what can be done, to help you learn these capabilities. You should also browse examples on other sites (<a href="#Links">Links</a>).</p>

<p>If you&#39;ve never used perf_events before, you may want to test before production use (it has had <a href="http://web.eecs.utk.edu/~vweaver1/projects/perf-events/kernel_panics.html">kernel panic</a> bugs in the past).  My experience has been a good one (no panics).</p>

<h2>4.6. Usage</h2>

<p>perf_events provides a command line tool, <tt>perf</tt>, and subcommands for various profiling activities. This is a single interface for the different instrumentation frameworks that provide the various events.</p>

<p>The <tt>perf</tt> command alone will list the subcommands; here is perf version 4.10 (for the Linux 4.10 kernel):</p>

<pre># <b>perf</b>

 usage: perf [--version] [--help] [OPTIONS] COMMAND [ARGS]

 The most commonly used perf commands are:
   annotate        Read perf.data (created by perf record) and display annotated code
   archive         Create archive with object files with build-ids found in perf.data file
   bench           General framework for benchmark suites
   buildid-cache   Manage build-id cache.
   buildid-list    List the buildids in a perf.data file
   config          Get and set variables in a configuration file.
   data            Data file related processing
   diff            Read perf.data files and display the differential profile
   evlist          List the event names in a perf.data file
   inject          Filter to augment the events stream with additional information
   kmem            Tool to trace/measure kernel memory properties
   kvm             Tool to trace/measure kvm guest os
   list            List all symbolic event types
   lock            Analyze lock events
   mem             Profile memory accesses
   record          Run a command and record its profile into perf.data
   report          Read perf.data (created by perf record) and display the profile
   sched           Tool to trace/measure scheduler properties (latencies)
   script          Read perf.data (created by perf record) and display trace output
   stat            Run a command and gather performance counter statistics
   test            Runs sanity tests.
   timechart       Tool to visualize total system behavior during a workload
   top             System profiling tool.
   probe           Define new dynamic tracepoints
   trace           strace inspired tool

 See &#39;perf help COMMAND&#39; for more information on a specific command.
</pre>

<p>Apart from separate help for each subcommand, there is also documentation in the kernel source under tools/perf/Documentation. <tt>perf</tt> has evolved, with different functionality added over time, so on an older kernel you may be missing some subcommands or functionality. Also, its usage may not feel consistent as you switch between activities. It&#39;s best to think of it as a multi-tool.</p>

<p>perf_events can instrument in three ways (now using the perf_events terminology):</p>

<ul>
<li><b>counting</b> events in-kernel context, where a summary of counts is printed by <tt>perf</tt>. This mode does not generate a perf.data file.</li>
<li><b>sampling</b> events, which writes event data to a kernel buffer, which is read at a gentle asynchronous rate by the <tt>perf</tt> command to write to the perf.data file. This file is then read by the <tt>perf report</tt> or <tt>perf script</tt> commands.</li>
<li><b>bpf</b> programs on events, a new feature in Linux 4.4+ kernels that can execute custom user-defined programs in kernel space, which can perform efficient filters and summaries of the data. Eg, efficiently-measured latency histograms.</li>
</ul>

<p>Try starting by counting events using the <tt>perf stat</tt> command, to see if this is sufficient. This subcommand costs the least overhead.</p>

<p>When using the sampling mode with <tt>perf record</tt>, you&#39;ll need to be a little careful about the overheads, as the capture files can quickly become hundreds of Mbytes. It depends on the rate of the event you are tracing: the more frequent, the higher the overhead and larger the perf.data size.</p>

<p>To really cut down overhead and generate more advanced summaries, write BPF programs executed by perf. See the <a href="#eBPF">eBPF</a> section.</p>

<h2>4.7. Usage Examples</h2>

<p>These example sequences have been chosen to illustrate some different ways that <tt>perf</tt> is used, from gathering to reporting.</p>

<p>Performance counter summaries, including IPC, for the gzip command:</p>
<pre># <b>perf stat gzip largefile</b>
</pre>

<p>Count all scheduler process events for 5 seconds, and count by tracepoint:</p>
<pre># <b>perf stat -e &#39;sched:sched_process_*&#39; -a sleep 5</b>
</pre>

<p>Trace all scheduler process events for 5 seconds, and count by both tracepoint and process name:</p>
<pre># <b>perf record -e &#39;sched:sched_process_*&#39; -a sleep 5</b>
# <b>perf report</b>
</pre>

<p>Trace all scheduler process events for 5 seconds, and dump per-event details:</p>
<pre># <b>perf record -e &#39;sched:sched_process_*&#39; -a sleep 5</b>
# <b>perf script</b>
</pre>

<p>Trace read() syscalls, when requested bytes is less than 10:</p>
<pre># <b>perf record -e &#39;syscalls:sys_enter_read&#39; --filter &#39;count &lt; 10&#39; -a</b>
</pre>

<p>Sample CPU stacks at 99 Hertz, for 5 seconds:</p>
<pre># <b>perf record -F 99 -ag -- sleep 5</b>
# <b>perf report</b>
</pre>

<p>Dynamically instrument the kernel tcp_sendmsg() function, and trace it for 5 seconds, with stack traces:</p>
<pre># <b>perf probe --add tcp_sendmsg</b>
# <b>perf record -e probe:tcp_sendmsg -ag -- sleep 5</b>
# <b>perf probe --del tcp_sendmsg</b>
# <b>perf report</b>
</pre>

<p>Deleting the tracepoint (--del) wasn&#39;t necessary; I included it to show how to return the system to its original state.</p>

<h3>Caveats</h3>

<p>The use of <tt>-p PID</tt> as a filter doesn&#39;t work properly on some older kernel versions (Linux 3.x): perf hits 100% CPU and needs to be killed. It&#39;s annoying. The workaround is to profile all CPUs (<tt>-a</tt>), and filter PIDs later.</p>

<h2>4.8. Special Usage</h2>

<p>There&#39;s a number of subcommands that provide special purpose functionality. These include:</p>

<ul>
<li><b>perf c2c</b> (Linux 4.10+): cache-2-cache and cacheline false sharing analysis.</li>
<li><b>perf kmem</b>: kernel memory allocation analysis.</li>
<li><b>perf kvm</b>: KVM virtual guest analysis.</li>
<li><b>perf lock</b>: lock analysis.</li>
<li><b>perf mem</b>: memory access analysis.</li>
<li><b>perf sched</b>: kernel scheduler statistics. <a href="#SchedulerAnalysis">Examples</a>.</li>
</ul>

<p>These make use of perf&#39;s existing instrumentation capabilities, recording selected events and reporting them in custom ways.</p>



<p>perf_events instruments &#34;events&#34;, which are a unified interface for different kernel instrumentation frameworks. The following map (from my <a href="#SCALE13x">SCaLE13x talk</a>) illustrates the event sources:</p>

<center><a href="https://www.brendangregg.com/perf_events/perf_events_map.png"><img src="https://www.brendangregg.com/perf_events/perf_events_map.png" width="500"/></a></center>

<p>The types of events are:</p>

<ul>
<li><b>Hardware Events</b>: CPU performance monitoring counters.</li>
<li><b>Software Events</b>: These are low level events based on kernel counters. For example, CPU migrations, minor faults, major faults, etc.</li>
<li><b>Kernel Tracepoint Events</b>: This are static kernel-level instrumentation points that are hardcoded in interesting and logical places in the kernel.</li>
<li><b>User Statically-Defined Tracing (USDT)</b>: These are static tracepoints for user-level programs and applications.</li>
<li><b>Dynamic Tracing</b>: Software can be dynamically instrumented, creating events in any location. For kernel software, this uses the kprobes framework. For user-level software, uprobes.</li>
<li><b>Timed Profiling</b>: Snapshots can be collected at an arbitrary frequency, using <tt>perf record -F<i>Hz</i></tt>. This is commonly used for CPU usage profiling, and works by creating custom timed interrupt events.</li>
</ul>


<p>Details about the events can be collected, including timestamps, the code path that led to it, and other specific details. The capabilities of perf_events are enormous, and you&#39;re likely to only ever use a fraction.</p>

<p>Currently available events can be listed using the <tt>list</tt> subcommand:</p>

<pre># <b>perf list</b>
List of pre-defined events (to be used in -e):
  cpu-cycles OR cycles                               [Hardware event]
  instructions                                       [Hardware event]
  cache-references                                   [Hardware event]
  cache-misses                                       [Hardware event]
  branch-instructions OR branches                    [Hardware event]
  branch-misses                                      [Hardware event]
  bus-cycles                                         [Hardware event]
  stalled-cycles-frontend OR idle-cycles-frontend    [Hardware event]
  stalled-cycles-backend OR idle-cycles-backend      [Hardware event]
  ref-cycles                                         [Hardware event]
  cpu-clock                                          [Software event]
  task-clock                                         [Software event]
  page-faults OR faults                              [Software event]
  context-switches OR cs                             [Software event]
  cpu-migrations OR migrations                       [Software event]
  minor-faults                                       [Software event]
  major-faults                                       [Software event]
  alignment-faults                                   [Software event]
  emulation-faults                                   [Software event]
  L1-dcache-loads                                    [Hardware cache event]
  L1-dcache-load-misses                              [Hardware cache event]
  L1-dcache-stores                                   [Hardware cache event]
[...]
  rNNN                                               [Raw hardware event descriptor]
  cpu/t1=v1[,t2=v2,t3 ...]/modifier                  [Raw hardware event descriptor]
   (see &#39;man perf-list&#39; on how to encode it)
  mem:&lt;addr&gt;[:access]                                [Hardware breakpoint]
  probe:tcp_sendmsg                                  [Tracepoint event]
[...]
  sched:sched_process_exec                           [Tracepoint event]
  sched:sched_process_fork                           [Tracepoint event]
  sched:sched_process_wait                           [Tracepoint event]
  sched:sched_wait_task                              [Tracepoint event]
  sched:sched_process_exit                           [Tracepoint event]
[...]
# <b>perf list | wc -l</b>
     657
</pre>

<p>When you use dynamic tracing, you are extending this list. The <tt>probe:tcp_sendmsg</tt> tracepoint in this list is an example, which I added by instrumenting tcp_sendmsg().  Profiling (sampling) events are not listed.</p>

<h2>5.1. Software Events</h2>

<p>There is a small number of fixed software events provided by perf:</p>

<pre># <b>perf list</b>

List of pre-defined events (to be used in -e):

  alignment-faults                                   [Software event]
  bpf-output                                         [Software event]
  context-switches OR cs                             [Software event]
  cpu-clock                                          [Software event]
  cpu-migrations OR migrations                       [Software event]
  dummy                                              [Software event]
  emulation-faults                                   [Software event]
  major-faults                                       [Software event]
  minor-faults                                       [Software event]
  page-faults OR faults                              [Software event]
  task-clock                                         [Software event]
[...]
</pre>

<p>These are also documented in the man page perf_event_open(2):</p>

<pre>[...]
                   PERF_COUNT_SW_CPU_CLOCK
                          This reports the CPU clock, a  high-resolution  per-
                          CPU timer.

                   PERF_COUNT_SW_TASK_CLOCK
                          This reports a clock count specific to the task that
                          is running.

                   PERF_COUNT_SW_PAGE_FAULTS
                          This reports the number of page faults.

                   PERF_COUNT_SW_CONTEXT_SWITCHES
                          This counts context switches.  Until  Linux  2.6.34,
                          these  were all reported as user-space events, after
                          that they are reported as happening in the kernel.

                   PERF_COUNT_SW_CPU_MIGRATIONS
                          This reports the number of  times  the  process  has
                          migrated to a new CPU.

                   PERF_COUNT_SW_PAGE_FAULTS_MIN
                          This  counts the number of minor page faults.  These
                          did not require disk I/O to handle.
[...]
</pre>

<p>The kernel also supports <a href="#Tracepoints">traecpoints</a>, which are very similar to software events, but have a different more extensible API.</p>

<p>Software events may have a default period. This means that when you use them for sampling, you&#39;re sampling a subset of events, not tracing every event. You can check with perf record -vv:</p>

<pre># <b>perf record -vv -e context-switches /bin/true</b>
Using CPUID GenuineIntel-6-55
------------------------------------------------------------
perf_event_attr:
  type                             1
  size                             112
  config                           0x3
  <b>{ sample_period, sample_freq }   4000</b>
  sample_type                      IP|TID|TIME|PERIOD
  disabled                         1
  inherit                          1
  mmap                             1
  comm                             1
  <b>freq                             1</b>
  enable_on_exec                   1
[...]
</pre>

<p>See the perf_event_open(2) man page for a description of these fields. This default means is that the kernel adjusts the rate of sampling so that it&#39;s capturing about 4,000 context switch events per second. If you really meant to record them all, use -c 1:</p>

<pre># <b>perf record -vv -e context-switches -c 1 /bin/true</b>
Using CPUID GenuineIntel-6-55
------------------------------------------------------------
perf_event_attr:
  type                             1
  size                             112
  config                           0x3
  <b>{ sample_period, sample_freq }   1</b>
  sample_type                      IP|TID|TIME
  disabled                         1
  inherit                          1
  mmap                             1
  comm                             1
  enable_on_exec                   1
</pre>

<p>Check the rate of events using <tt>perf stat</tt> first, so that you can estimate the volume of data you&#39;ll be capturing. Sampling a subset by default may be a good thing, especially for high frequency events like context switches.</p>

<p>Many other events (like tracepoints) have a default of 1 anyway. You&#39;ll encounter a non-1 default for many software and hardware events.</p>

<h2>5.2. Hardware Events (PMCs)</h2>

<p>perf_events began life as a tool for instrumenting the processor&#39;s performance monitoring unit (PMU) hardware counters, also called performance monitoring counters (PMCs), or performance instrumentation counters (PICs). These instrument low-level processor activity, for example, CPU cycles, instructions retired, memory stall cycles, level 2 cache misses, etc. Some will be listed as Hardware Cache Events.</p>

<p>PMCs are documented in the <i>Intel 64 and IA-32 Architectures Software Developer&#39;s Manual Volume 3B: System Programming Guide, Part 2</i> and the <i>BIOS and Kernel Developer&#39;s Guide (BKDG) For AMD Family 10h Processors</i>. There are thousands of different PMCs available.</p>

<p>A typical processor will implement PMCs in the following way: only a few or several can be recorded at the same time, from the many thousands that are available. This is because they are a fixed hardware resource on the processor (a limited number of registers), and are programmed to begin counting the selected events.</p>

<p>For examples of using PMCs, see <a href="#CPUstatistics">CPU Statistics</a>.</p>

<h2>5.3. Kernel Tracepoints</h2>

<p>These tracepoints are hard coded in interesting and logical locations of the kernel, so that higher-level behavior can be easily traced. For example, system calls, TCP events, file system I/O, disk I/O, etc. These are grouped into libraries of tracepoints; eg, &#34;sock:&#34; for socket events, &#34;sched:&#34; for CPU scheduler events.
A key value of tracepoints is that they should have a stable API, so if you write tools that use them on one kernel version, they should work on later versions as well.</p>

<p>Tracepoints are usually added to kernel code by placing a macro from include/trace/events/*. XXX cover implementation.</p>

<p>Summarizing the tracepoint library names and numbers of tracepoints, on my Linux 4.10 system:</p>

<pre># <b>perf list | awk -F: &#39;/Tracepoint event/ { lib[$1]++ } END {
    for (l in lib) { printf &#34;  %-16.16s %d\n&#34;, l, lib[l] } }&#39; | sort | column</b>
<!--
    alarmtimer     4	    libata         6	    scsi           5
    block          19	    mce            1	    sdt_node       1
    btrfs          51	    mdio           1	    signal         2
    cgroup         9	    migrate        2	    skb            3
    clk            14	    mmc            2	    sock           2
    cma            2	    module         5	    spi            7
    compaction     14	    mpx            5	    swiotlb        1
    cpuhp          3	    msr            3	    syscalls       614
    dma_fence      8	    napi           1	    task           2
    exceptions     2	    net            10	    thermal        7
    ext4           95	    nmi            1	    thermal_power_ 2
    fib            3	    oom            1	    timer          13
    fib6           1	    page_isolation 1	    tlb            1
    filelock       10	    pagemap        2	    udp            1
    filemap        2	    power          22	    vmscan         15
    ftrace         1	    printk         1	    vsyscall       1
    gpio           2	    random         15	    workqueue      4
    huge_memory    4	    ras            4	    writeback      30
    i2c            8	    raw_syscalls   2	    x86_fpu        14
    iommu          7	    rcu            1	    xen            35
    irq            5	    regmap         15	    xfs            495
    irq_vectors    22	    regulator      7	    xhci-hcd       9
    jbd2           16	    rpm            4
    kmem           12	    sched          24
-->    alarmtimer     4	    i2c            8	    page_isolation 1	    swiotlb        1
    block          19	    iommu          7	    pagemap        2	    syscalls       614
    btrfs          51	    irq            5	    power          22	    task           2
    cgroup         9	    irq_vectors    22	    printk         1	    thermal        7
    clk            14	    jbd2           16	    random         15	    thermal_power_ 2
    cma            2	    kmem           12	    ras            4	    timer          13
    compaction     14	    libata         6	    raw_syscalls   2	    tlb            1
    cpuhp          3	    mce            1	    rcu            1	    udp            1
    dma_fence      8	    mdio           1	    regmap         15	    vmscan         15
    exceptions     2	    migrate        2	    regulator      7	    vsyscall       1
    ext4           95	    mmc            2	    rpm            4	    workqueue      4
    fib            3	    module         5	    sched          24	    writeback      30
    fib6           1	    mpx            5	    scsi           5	    x86_fpu        14
    filelock       10	    msr            3	    sdt_node       1	    xen            35
    filemap        2	    napi           1	    signal         2	    xfs            495
    ftrace         1	    net            10	    skb            3	    xhci-hcd       9
    gpio           2	    nmi            1	    sock           2
    huge_memory    4	    oom            1	    spi            7
</pre>

<p>These include:</p>
<ul>
<li><b>block</b>: block device I/O</li>
<li><b>ext4</b>: file system operations</li>
<li><b>kmem</b>: kernel memory allocation events</li>
<li><b>random</b>: kernel random number generator events</li>
<li><b>sched</b>: CPU scheduler events</li>
<li><b>syscalls</b>: system call enter and exits</li>
<li><b>task</b>: task events</li>
</ul>

<p>It&#39;s worth checking the list of tracepoints after every kernel upgrade, to see if any are new.  The value of adding them <a href="http://lwn.net/Articles/346470/">has been</a> <a href="http://lwn.net/Articles/346483/">debated</a> from time to time, with it wondered how many people will use them (I do). There is a balance to aim for: I&#39;d include the smallest number of probes that sufficiently covers common needs, and anything unusual or uncommon can be left to dynamic tracing.</p>

<p>For examples of using tracepoints, see <a href="#StaticKernelTracing">Static Kernel Tracing</a>.</p>

<h2>5.4. User-Level Statically Defined Tracing (USDT)</h2>

<p>Similar to kernel tracepoints, these are hardcoded (usually by placing macros) in the application source at logical and interesting locations, and presented (event name and arguments) as a stable API. Many applications already include tracepoints, added to support <a href="https://www.brendangregg.com/dtrace.html">DTrace</a>. However, many of these applications do not compile them in by default on Linux. Often you need to compile the application yourself using a --with-dtrace flag.</p>

<p>For example, compiling USDT events with this version of Node.js:</p>

<pre>$ <b>sudo apt-get install systemtap-sdt-dev</b>       # adds &#34;dtrace&#34;, used by node build
$ <b>wget https://nodejs.org/dist/v4.4.1/node-v4.4.1.tar.gz</b>
$ <b>tar xvf node-v4.4.1.tar.gz </b>
$ <b>cd node-v4.4.1</b>
$ <b>./configure --with-dtrace</b>
$ <b>make -j 8</b>
</pre>

<p>To check that the resulting node binary has probes included:</p>

<pre>$ <b>readelf -n node</b>

Displaying notes found at file offset 0x00000254 with length 0x00000020:
  Owner                 Data size	Description
  GNU                  0x00000010	NT_GNU_ABI_TAG (ABI version tag)
    OS: Linux, ABI: 2.6.32

Displaying notes found at file offset 0x00000274 with length 0x00000024:
  Owner                 Data size	Description
  GNU                  0x00000014	NT_GNU_BUILD_ID (unique build ID bitstring)
    Build ID: 1e01659b0aecedadf297b2c56c4a2b536ae2308a

Displaying notes found at file offset 0x00e70994 with length 0x000003c4:
  Owner                 Data size	Description
  stapsdt              0x0000003c	NT_STAPSDT (SystemTap probe descriptors)
    Provider: node
    Name: <b>gc__start</b>
    Location: 0x0000000000dc14e4, Base: 0x000000000112e064, Semaphore: 0x000000000147095c
    Arguments: 4@%esi 4@%edx 8@%rdi
  stapsdt              0x0000003b	NT_STAPSDT (SystemTap probe descriptors)
    Provider: node
    Name: <b>gc__done</b>
    Location: 0x0000000000dc14f4, Base: 0x000000000112e064, Semaphore: 0x000000000147095e
    Arguments: 4@%esi 4@%edx 8@%rdi
  stapsdt              0x00000067	NT_STAPSDT (SystemTap probe descriptors)
    Provider: node
    Name: <b>http__server__response</b>
    Location: 0x0000000000dc1894, Base: 0x000000000112e064, Semaphore: 0x0000000001470956
    Arguments: 8@%rax 8@-1144(%rbp) -4@-1148(%rbp) -4@-1152(%rbp)
  stapsdt              0x00000061	NT_STAPSDT (SystemTap probe descriptors)
    Provider: node
    Name: <b>net__stream__end</b>
    Location: 0x0000000000dc1c44, Base: 0x000000000112e064, Semaphore: 0x0000000001470952
    Arguments: 8@%rax 8@-1144(%rbp) -4@-1148(%rbp) -4@-1152(%rbp)
  stapsdt              0x00000068	NT_STAPSDT (SystemTap probe descriptors)
    Provider: node
    Name: <b>net__server__connection</b>
    Location: 0x0000000000dc1ff4, Base: 0x000000000112e064, Semaphore: 0x0000000001470950
    Arguments: 8@%rax 8@-1144(%rbp) -4@-1148(%rbp) -4@-1152(%rbp)
  stapsdt              0x00000060	NT_STAPSDT (SystemTap probe descriptors)
    Provider: node
    Name: <b>http__client__response</b>
    Location: 0x0000000000dc23c5, Base: 0x000000000112e064, Semaphore: 0x000000000147095a
    Arguments: 8@%rdx 8@-1144(%rbp) -4@%eax -4@-1152(%rbp)
  stapsdt              0x00000089	NT_STAPSDT (SystemTap probe descriptors)
    Provider: node
    Name: <b>http__client__request</b>
    Location: 0x0000000000dc285e, Base: 0x000000000112e064, Semaphore: 0x0000000001470958
    Arguments: 8@%rax 8@%rdx 8@-2184(%rbp) -4@-2188(%rbp) 8@-2232(%rbp) 8@-2240(%rbp) -4@-2192(%rbp)
  stapsdt              0x00000089	NT_STAPSDT (SystemTap probe descriptors)
    Provider: node
    Name: <b>http__server__request</b>
    Location: 0x0000000000dc2e69, Base: 0x000000000112e064, Semaphore: 0x0000000001470954
    Arguments: 8@%r14 8@%rax 8@-4344(%rbp) -4@-4348(%rbp) 8@-4304(%rbp) 8@-4312(%rbp) -4@-4352(%rbp)
</pre>

<p>For examples of using USDT events, see <a href="#StaticUserTracing">Static User Tracing</a>.</p>

<h2>5.5. Dynamic Tracing</h2>

<p>The difference between tracepoints and dynamic tracing is shown in the following figure, which illustrates the coverage of common tracepoint libraries:</p>

<p><img src="https://www.brendangregg.com/perf_events/perf_tracepoints_1700.png" width="850" height="256"/></p>

<p>While dynamic tracing can see everything, it&#39;s also an unstable interface since it is instrumenting raw code. That means that any dynamic tracing tools you develop may break after a kernel patch or update. Try to use the static tracepoints first, since their interface should be much more stable. They can also be easier to use and understand, since they have been designed with a tracing end-user in mind.</p>

<p>One benefit of dynamic tracing is that it can be enabled on a live system without restarting anything. You can take an already-running kernel or application and then begin dynamic instrumentation, which (safely) patches instructions in memory to add instrumentation. That means there is zero overhead or tax for this feature until you begin using it. One moment your binary is running unmodified and at full speed, and the next, it&#39;s running some extra instrumentation instructions that you dynamically added. Those instructions should eventually be removed once you&#39;ve finished using your session of dynamic tracing.</p>

<p>The overhead while dynamic tracing is in use, and extra instructions are being executed, is relative to the frequency of instrumented events multiplied by the work done on each instrumentation.</p>

<p>For examples of using dynamic tracing, see <a href="#DynamicTracingEg">6.5. Dynamic Tracing</a>.</p>



<p>These are some examples of perf_events, collected from a variety of 3.x Linux systems.</p>

<h2>6.1. CPU Statistics</h2>

<p>The <tt>perf stat</tt> command instruments and summarizes key CPU counters (PMCs). This is from <tt>perf</tt> version 3.5.7.2:</p>

<pre># <b>perf stat gzip file1</b>

 Performance counter stats for &#39;gzip file1&#39;:

       1920.159821 task-clock                #    0.991 CPUs utilized          
                13 context-switches          #    0.007 K/sec                  
                 0 CPU-migrations            #    0.000 K/sec                  
               258 page-faults               #    0.134 K/sec                  
     5,649,595,479 cycles                    #    2.942 GHz                     [83.43%]
     1,808,339,931 stalled-cycles-frontend   #   32.01% frontend cycles idle    [83.54%]
     1,171,884,577 stalled-cycles-backend    #   20.74% backend  cycles idle    [66.77%]
     8,625,207,199 instructions              #    1.53  insns per cycle        
                                             #    0.21  stalled cycles per insn [83.51%]
     1,488,797,176 branches                  #  775.351 M/sec                   [82.58%]
        53,395,139 branch-misses             #    3.59% of all branches         [83.78%]

       1.936842598 seconds time elapsed
</pre>

<p>This includes instructions per cycle (IPC), labled &#34;insns per cycle&#34;, or in earlier versions, &#34;IPC&#34;. This is a commonly examined metric, either IPC or its invert, CPI. Higher IPC values mean higher instruction throughput, and lower values indicate more stall cycles. I&#39;d generally interpret high IPC values (eg, over 1.0) as good, indicating optimal processing of work. However, I&#39;d want to double check what the instructions are, in case this is due to a spin loop: a high rate of instructions, but a low rate of actual work completed.</p>

<p>There are some advanced metrics now included in <tt>perf stat</tt>: frontend cycles idle, backend cycles idle, and stalled cycles per insn. To really understand these, you&#39;ll need some knowledge of CPU microarchitecture.</p>

<h3>CPU Microarchitecture</h3>

<p>The frontend and backend metrics refer to the CPU pipeline, and are also based on stall counts. The frontend processes CPU instructions, in order. It involves instruction fetch, along with branch prediction, and decode. The decoded instructions become micro-operations (uops) which the backend processes, and it may do so out of order. For a longer summary of these components, see Shannon Cepeda&#39;s great posts on <a href="http://software.intel.com/en-us/blogs/2011/11/22/pipeline-speak-learning-more-about-intel-microarchitecture-codename-sandy-bridge">frontend</a> and <a href="http://software.intel.com/en-us/blogs/2011/12/01/pipeline-speak-part-2-the-second-part-of-the-sandy-bridge-pipeline">backend</a>.</p>

<p>The backend can also process multiple uops in parallel; for modern processors, three or four. Along with pipelining, this is how IPC can become greater than one, as more than one instruction can be completed (&#34;retired&#34;) per CPU cycle.</p>

<p>Stalled cycles per instruction is similar to IPC (inverted), however, only counting stalled cycles, which will be for memory or resource bus access. This makes it easy to interpret: stalls are latency, reduce stalls. I really like it as a metric, and hope it becomes as commonplace as IPC/CPI. Lets call it SCPI.</p>

<h3>Detailed Mode</h3>

<p>There is a &#34;detailed&#34; mode for <tt>perf stat</tt>:</p>

<pre># <b>perf stat -d gzip file1</b>

 Performance counter stats for &#39;gzip file1&#39;:

       1610.719530 task-clock                #    0.998 CPUs utilized          
                20 context-switches          #    0.012 K/sec                  
                 0 CPU-migrations            #    0.000 K/sec                  
               258 page-faults               #    0.160 K/sec                  
     5,491,605,997 cycles                    #    3.409 GHz                     [40.18%]
     1,654,551,151 stalled-cycles-frontend   #   30.13% frontend cycles idle    [40.80%]
     1,025,280,350 stalled-cycles-backend    #   18.67% backend  cycles idle    [40.34%]
     8,644,643,951 instructions              #    1.57  insns per cycle        
                                             #    0.19  stalled cycles per insn [50.89%]
     1,492,911,665 branches                  #  926.860 M/sec                   [50.69%]
        53,471,580 branch-misses             #    3.58% of all branches         [51.21%]
     1,938,889,736 L1-dcache-loads           # 1203.741 M/sec                   [49.68%]
       154,380,395 L1-dcache-load-misses     #    7.96% of all L1-dcache hits   [49.66%]
                 0 LLC-loads                 #    0.000 K/sec                   [39.27%]
                 0 LLC-load-misses           #    0.00% of all LL-cache hits    [39.61%]

       1.614165346 seconds time elapsed
</pre>

<p>This includes additional counters for Level 1 data cache events, and last level cache (LLC) events.</p>

<h3>Specific Counters</h3>

<p>Hardware cache event counters, seen in <tt>perf list</tt>, can be instrumented. Eg:</p>

<pre># <b>perf list | grep L1-dcache</b>
  L1-dcache-loads                                    [Hardware cache event]
  L1-dcache-load-misses                              [Hardware cache event]
  L1-dcache-stores                                   [Hardware cache event]
  L1-dcache-store-misses                             [Hardware cache event]
  L1-dcache-prefetches                               [Hardware cache event]
  L1-dcache-prefetch-misses                          [Hardware cache event]
# <b>perf stat -e L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores gzip file1</b>

 Performance counter stats for &#39;gzip file1&#39;:

     1,947,551,657 L1-dcache-loads
                                            
       153,829,652 L1-dcache-misses
         #    7.90% of all L1-dcache hits  
     1,171,475,286 L1-dcache-stores
                                           

       1.538038091 seconds time elapsed
</pre>

<p>The percentage printed is a convenient calculation that perf_events has included, based on the counters I specified. If you include the &#34;cycles&#34; and &#34;instructions&#34; counters, it will include an IPC calculation in the output.</p>

<p>These hardware events that can be measured are often specific to the processor model. Many may not be available from within a virtualized environment.</p>

<h3>Raw Counters</h3>

<p>The <i>Intel 64 and IA-32 Architectures Software Developer&#39;s Manual Volume 3B: System Programming Guide, Part 2</i> and the <i>BIOS and Kernel Developer&#39;s Guide (BKDG) For AMD Family 10h Processors</i> are full of interesting counters, but most cannot be found in <tt>perf list</tt>. If you find one you want to instrument, you can specify it as a raw event with the format: rUUEE, where UU == umask, and EE == event number. Here&#39;s an example where I&#39;ve added a couple of raw counters:</p>

<pre># <b>perf stat -e cycles,instructions,r80a2,r2b1 gzip file1</b>

 Performance counter stats for &#39;gzip file1&#39;:

     5,586,963,328 cycles                    #    0.000 GHz                    
     8,608,237,932 instructions              #    1.54  insns per cycle        
         9,448,159 raw 0x80a2                                                  
    11,855,777,803 raw 0x2b1                                                   

       1.588618969 seconds time elapsed
</pre>

<p>If I did this right, then r80a2 has instrumented RESOURCE_STALLS.OTHER, and r2b1 has instrumented UOPS_DISPATCHED.CORE: the number of uops dispatched each cycle. It&#39;s easy to mess this up, and you&#39;ll want to double check that you are on the right page of the manual for your processor.</p>

<p>If you do find an awesome raw counter, please <a href="http://dir.gmane.org/gmane.linux.kernel.perf.user">suggest</a> it be added as an alias in perf_events, so we all can find it in <tt>perf list</tt>.</p>

<h3>Other Options</h3>

<p>The perf subcommands, especially <tt>perf stat</tt>, have an extensive option set which can be listed using &#34;-h&#34;. I&#39;ve included the full output for <tt>perf stat</tt> here from version 3.9.3, not as a reference, but as an illustration of the interface:</p>

<pre># <b>perf stat -h</b>

 usage: perf stat [&lt;options&gt;] [&lt;command&gt;]

    -e, --event &lt;event&gt;   event selector. use &#39;perf list&#39; to list available events
        --filter &lt;filter&gt;
                          event filter
    -i, --no-inherit      child tasks do not inherit counters
    -p, --pid &lt;pid&gt;       stat events on existing process id
    -t, --tid &lt;tid&gt;       stat events on existing thread id
    -a, --all-cpus        system-wide collection from all CPUs
    -g, --group           put the counters into a counter group
    -c, --scale           scale/normalize counters
    -v, --verbose         be more verbose (show counter open errors, etc)
    -r, --repeat &lt;n&gt;      repeat command and print average + stddev (max: 100)
    -n, --null            null run - dont start any counters
    -d, --detailed        detailed run - start a lot of events
    -S, --sync            call sync() before starting a run
    -B, --big-num         print large numbers with thousands&#39; separators
    -C, --cpu &lt;cpu&gt;       list of cpus to monitor in system-wide
    -A, --no-aggr         disable CPU count aggregation
    -x, --field-separator &lt;separator&gt;
                          print counts with custom separator
    -G, --cgroup &lt;name&gt;   monitor event in cgroup name only
    -o, --output &lt;file&gt;   output file name
        --append          append to the output file
        --log-fd &lt;n&gt;      log output to fd, instead of stderr
        --pre &lt;command&gt;   command to run prior to the measured command
        --post &lt;command&gt;  command to run after to the measured command
    -I, --interval-print &lt;n&gt;
                          print counts at regular interval in ms (&gt;= 100)
        --aggr-socket     aggregate counts per processor socket
</pre>

<p>Options such as --repeat, --sync, --pre, and --post can be quite useful when doing automated testing or micro-benchmarking.</p>

<h2>6.2. Timed Profiling</h2>

<p>perf_events can profile CPU usage based on sampling the instruction pointer or stack trace at a fixed interval (timed profiling).</p>

<p>Sampling CPU stacks at 99 Hertz (-F 99), for the entire system (-a, for all CPUs), with stack traces (-g, for call graphs), for 10 seconds:</p>

<pre># <b>perf record -F 99 -a -g -- sleep 30</b>
[ perf record: Woken up 9 times to write data ]
[ perf record: Captured and wrote 3.135 MB perf.data (~136971 samples) ]
# ls -lh perf.data
-rw------- 1 root root 3.2M Jan 26 07:26 perf.data
</pre>

<p>The choice of 99 Hertz, instead of 100 Hertz, is to avoid accidentally sampling in lockstep with some periodic activity, which would produce skewed results. This is also coarse: you may want to increase that to higher rates (eg, up to 997 Hertz) for finer resolution, especially if you are sampling short bursts of activity and you&#39;d still like enough resolution to be useful. Bear in mind that higher frequencies means higher overhead.</p>

<p>The perf.data file can be processed in a variety of ways.  On recent versions, the <tt>perf report</tt> command launches an ncurses navigator for call graph inspection.  Older versions of perf (or if you use --stdio in the new version) print the call graph as a tree, annotated with percentages:</p>

<pre># <b>perf report --stdio</b>
# ========
# captured on: Mon Jan 26 07:26:40 2014
# hostname : dev2
# os release : 3.8.6-ubuntu-12-opt
# perf version : 3.8.6
# arch : x86_64
# nrcpus online : 8
# nrcpus avail : 8
# cpudesc : Intel(R) Xeon(R) CPU X5675 @ 3.07GHz
# cpuid : GenuineIntel,6,44,2
# total memory : 8182008 kB
# cmdline : /usr/bin/perf record -F 99 -a -g -- sleep 30 
# event : name = cpu-clock, type = 1, config = 0x0, config1 = 0x0, config2 = ...
# HEADER_CPU_TOPOLOGY info available, use -I to display
# HEADER_NUMA_TOPOLOGY info available, use -I to display
# pmu mappings: software = 1, breakpoint = 5
# ========
#
# Samples: 22K of event &#39;cpu-clock&#39;
# Event count (approx.): 22751
#
# Overhead  Command      Shared Object                           Symbol
# ........  .......  .................  ...............................
#
    94.12%       dd  [kernel.kallsyms]  [k] _raw_spin_unlock_irqrestore
                 |
                 --- _raw_spin_unlock_irqrestore
                    |          
                    |--96.67%-- extract_buf
                    |          extract_entropy_user
                    |          urandom_read
                    |          vfs_read
                    |          sys_read
                    |          system_call_fastpath
                    |          read
                    |          
                    |--1.69%-- account
                    |          |          
                    |          |--99.72%-- extract_entropy_user
                    |          |          urandom_read
                    |          |          vfs_read
                    |          |          sys_read
                    |          |          system_call_fastpath
                    |          |          read
                    |           --0.28%-- [...]
                    |          
                    |--1.60%-- mix_pool_bytes.constprop.17
[...]
</pre>

<p>This tree starts with the on-CPU functions and works back through the ancestry. This approach is called a &#34;callee based call graph&#34;.  This can be flipped by using -G for an &#34;inverted call graph&#34;, or by using the &#34;caller&#34; option to -g/--call-graph, instead of the &#34;callee&#34; default.</p>

<p>The hottest (most frequent) stack trace in this perf call graph occurred in 90.99% of samples, which is the product of the overhead percentage and top stack leaf (94.12% x 96.67%, which are relative rates). <tt>perf report</tt> can also be run with &#34;-g graph&#34; to show absolute overhead rates, in which case &#34;90.99%&#34; is directly displayed on the stack leaf:</p>

<pre>    94.12%       dd  [kernel.kallsyms]  [k] _raw_spin_unlock_irqrestore
                 |
                 --- _raw_spin_unlock_irqrestore
                    |          
                    |--90.99%-- extract_buf
[...]
</pre>

<p>If user-level stacks look incomplete, you can try <tt>perf record</tt> with &#34;--call-graph dwarf&#34; as a different technique to unwind them. See the <a href="#StackTraces">Stacks</a> section.</p>

<p>The output from <tt>perf report</tt> can be many pages long, which can become cumbersome to read. Try generating <a href="#FlameGraphs">Flame Graphs</a> from the same data.</p>

<h2>6.3. Event Profiling</h2>

<p>Apart from sampling at a timed interval, taking samples triggered by CPU hardware counters is another form of CPU profiling, which can be used to shed more light on cache misses, memory stall cycles, and other low-level processor events. The available events can be found using <tt>perf list</tt>:</p>

<pre># <b>perf list | grep Hardware</b>
  cpu-cycles OR cycles                               [Hardware event]
  instructions                                       [Hardware event]
  cache-references                                   [Hardware event]
  cache-misses                                       [Hardware event]
  branch-instructions OR branches                    [Hardware event]
  branch-misses                                      [Hardware event]
  bus-cycles                                         [Hardware event]
  stalled-cycles-frontend OR idle-cycles-frontend    [Hardware event]
  stalled-cycles-backend OR idle-cycles-backend      [Hardware event]
  ref-cycles                                         [Hardware event]
  L1-dcache-loads                                    [Hardware cache event]
  L1-dcache-load-misses                              [Hardware cache event]
  L1-dcache-stores                                   [Hardware cache event]
  L1-dcache-store-misses                             [Hardware cache event]
[...]
</pre>

<p>For many of these, gathering a stack on every occurrence would induce far too much overhead, and would slow down the system and change the performance characteristics of the target. It&#39;s usually sufficient to only instrument a small fraction of their occurrences, rather than all of them. This can be done by specifying a threshold for triggering event collection, using &#34;-c&#34; and a count.</p>

<p>For example, the following one-liner instruments Level 1 data cache load misses, collecting a stack trace for one in every 10,000 occurrences:</p>

<pre># <b>perf record -e L1-dcache-load-misses -c 10000 -ag -- sleep 5</b>
</pre>

<p>The mechanics of &#34;-c count&#34; are implemented by the processor, which only interrupts the kernel when the threshold has been reached.</p>

<p>See the earlier Raw Counters section for an example of specifying a custom counter, and the next section about skew.</p>

<h3>Skew and PEBS</h3>

<p>There&#39;s a problem with event profiling that you don&#39;t really encounter with CPU profiling (timed sampling). With timed sampling, it doesn&#39;t matter if there was a small sub-microsecond delay between the interrupt and reading the instruction pointer (IP). Some CPU profilers introduce this jitter on purpose, as another way to avoid lockstep sampling. But for event profiling, it does matter: if you&#39;re trying to capture the IP on some PMC event, and there&#39;s a delay between the PMC overflow and capturing the IP, then the IP will point to the wrong address. This is skew. Another contributing problem is that micro-ops are processed in parallel and out-of-order, while the instruction pointer points to the resumption instruction, not the instruction that caused the event. I&#39;ve talked about this <a href="https://www.slideshare.net/brendangregg/scale2015-linux-perfprofiling/63">before</a>.</p>

<p>The solution is &#34;precise sampling&#34;, which on Intel is PEBS (Precise Event-Based Sampling), and on AMD it is IBS (Instruction-Based Sampling). These use CPU hardware support to capture the real state of the CPU at the time of the event. perf can use precise sampling by adding a :p modifier to the PMC event name, eg, &#34;-e instructions:p&#34;. The more p&#39;s, the more accurate. Here are the docs from <a href="https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/perf-list.txt">tools/perf/Documentation/perf-list.txt</a>:</p>

<pre>The &#39;p&#39; modifier can be used for specifying how precise the instruction
address should be. The &#39;p&#39; modifier can be specified multiple times:

 0 - SAMPLE_IP can have arbitrary skid
 1 - SAMPLE_IP must have constant skid
 2 - SAMPLE_IP requested to have 0 skid
 3 - SAMPLE_IP must have 0 skid
</pre>

<p>In some cases, perf will default to using precise sampling without you needing to specify it. Run &#34;perf record -vv ...&#34; to see the value of &#34;precise_ip&#34;. Also note that only some PMCs support PEBS.</p>

<p>If PEBS isn&#39;t working at all for you, check dmesg:</p>

<pre># dmesg | grep -i pebs
[    0.387014] Performance Events: PEBS fmt1+, SandyBridge events, 16-deep LBR, full-width counters, Intel PMU driver.
[    0.387034] core: PEBS disabled due to CPU errata, please upgrade microcode
</pre>

<p>The fix (on Intel):</p>

<pre># apt-get install -y intel-microcode
[...]
intel-microcode: microcode will be updated at next boot
Processing triggers for initramfs-tools (0.125ubuntu5) ...
update-initramfs: Generating /boot/initrd.img-4.8.0-41-generic
# reboot

(system reboots)

# dmesg | grep -i pebs
[    0.386596] Performance Events: PEBS fmt1+, SandyBridge events, 16-deep LBR, full-width counters, Intel PMU driver.
#
</pre>

<p>XXX: Need to cover more PEBS problems and other caveats.</p>

<h2>6.4. Static Kernel Tracing</h2>

<p>The following examples demonstrate static tracing: the instrumentation of tracepoints and other static events.</p>

<h3>Counting Syscalls</h3>

<p>The following simple one-liner counts system calls for the executed command, and prints a summary (of non-zero counts):</p>

<pre># <b>perf stat -e &#39;syscalls:sys_enter_*&#39; gzip file1 2&gt;&amp;1 | awk &#39;$1 != 0&#39;</b>

 Performance counter stats for &#39;gzip file1&#39;:

                 1 syscalls:sys_enter_utimensat               
                 1 syscalls:sys_enter_unlink                  
                 5 syscalls:sys_enter_newfstat                
             1,603 syscalls:sys_enter_read                    
             3,201 syscalls:sys_enter_write                   
                 5 syscalls:sys_enter_access                  
                 1 syscalls:sys_enter_fchmod                  
                 1 syscalls:sys_enter_fchown                  
                 6 syscalls:sys_enter_open                    
                 9 syscalls:sys_enter_close                   
                 8 syscalls:sys_enter_mprotect                
                 1 syscalls:sys_enter_brk                     
                 1 syscalls:sys_enter_munmap                  
                 1 syscalls:sys_enter_set_robust_list         
                 1 syscalls:sys_enter_futex                   
                 1 syscalls:sys_enter_getrlimit               
                 5 syscalls:sys_enter_rt_sigprocmask          
                14 syscalls:sys_enter_rt_sigaction            
                 1 syscalls:sys_enter_exit_group              
                 1 syscalls:sys_enter_set_tid_address         
                14 syscalls:sys_enter_mmap                    

       1.543990940 seconds time elapsed
</pre>

<p>In this case, a gzip command was analyzed. The report shows that there were 3,201 write() syscalls, and half that number of read() syscalls. Many of the other syscalls will be due to process and library initialization.</p>

<p>A similar report can be seen using <tt>strace -c</tt>, the system call tracer, however it may induce much higher overhead than perf, as perf buffers data in-kernel.</p>

<h3>perf vs strace</h3>

<p>To explain the difference a little further: the current implementation of strace uses ptrace(2) to attach to the target process and stop it during system calls, like a debugger. This is violent, and can cause serious overhead. To demonstrate this, the following syscall-heavy program was run by itself, with perf, and with strace. I&#39;ve only included the line of output that shows its performance:</p>

<pre># <b>dd if=/dev/zero of=/dev/null bs=512 count=10000k</b>
5242880000 bytes (5.2 GB) copied, 3.53031 s, 1.5 GB/s

# <b>perf stat -e &#39;syscalls:sys_enter_*&#39; dd if=/dev/zero of=/dev/null bs=512 count=10000k</b>
5242880000 bytes (5.2 GB) copied, 9.14225 s, 573 MB/s

# <b>strace -c dd if=/dev/zero of=/dev/null bs=512 count=10000k</b>
5242880000 bytes (5.2 GB) copied, 218.915 s, 23.9 MB/s
</pre>

<p>With <tt>perf</tt>, the program ran 2.5x slower. But <b>with <tt>strace</tt>, it ran 62x slower</b>. That&#39;s likely to be a worst-case result: if syscalls are not so frequent, the difference between the tools will not be as great.</p>

<p>Recent version of <tt>perf</tt> have included a <tt>trace</tt> subcommand, to provide some similar functionality to <tt>strace</tt>, but with much lower overhead.</p>

<h3>New Processes</h3>

<p>Tracing new processes triggered by a &#34;man ls&#34;:</p>

<pre># <b>perf record -e sched:sched_process_exec -a</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.064 MB perf.data (~2788 samples) ]
# <b>perf report -n --sort comm --stdio</b>
[...]
# Overhead       Samples  Command
# ........  ............  .......
#
    11.11%             1    troff
    11.11%             1      tbl
    11.11%             1  preconv
    11.11%             1    pager
    11.11%             1    nroff
    11.11%             1      man
    11.11%             1   locale
    11.11%             1   grotty
    11.11%             1    groff
</pre>

<p>Nine different commands were executed, each once. I used -n to print the &#34;Samples&#34; column, and &#34;--sort comm&#34; to customize the remaining columns.</p>

<p>This works by tracing sched:sched_process_exec, when a process runs exec() to execute a different binary. This is often how new processes are created, but not always. An application may fork() to create a pool of worker processes, but not exec() a different binary. An application may also reexec: call exec() again, on itself, usually to clean up its address space. In that case, it&#39;s will be seen by this exec tracepoint, but it&#39;s not a new process.</p>

<p>The sched:sched_process_fork tracepoint can be traced to only catch new processes, created via fork(). The downside is that the process identified is the parent, not the new target, as the new process has yet to exec() it&#39;s final program.</p>

<h3>Outbound Connections</h3>

<p>There can be times when it&#39;s useful to double check what network connections are initiated by a server, from which processes, and why. You might be surprised. These connections can be important to understand, as they can be a source of latency.</p>

<p>For this example, I have a completely idle ubuntu server, and while tracing I&#39;ll login to it using ssh. I&#39;m going to trace outbound connections via the connect() syscall. Given that I&#39;m performing an <i>inbound</i> connection over SSH, will there be any outbound connections at all?</p>

<pre># <b>perf record -e syscalls:sys_enter_connect -a</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.057 MB perf.data (~2489 samples) ]
# <b>perf report --stdio</b>
# ========
# captured on: Tue Jan 28 10:53:38 2014
# hostname : ubuntu
# os release : 3.5.0-23-generic
# perf version : 3.5.7.2
# arch : x86_64
# nrcpus online : 2
# nrcpus avail : 2
# cpudesc : Intel(R) Core(TM) i7-3820QM CPU @ 2.70GHz
# cpuid : GenuineIntel,6,58,9
# total memory : 1011932 kB
# cmdline : /usr/bin/perf_3.5.0-23 record -e syscalls:sys_enter_connect -a 
# event : name = syscalls:sys_enter_connect, type = 2, config = 0x38b, ...
# HEADER_CPU_TOPOLOGY info available, use -I to display
# HEADER_NUMA_TOPOLOGY info available, use -I to display
# ========
#
# Samples: 21  of event &#39;syscalls:sys_enter_connect&#39;
# Event count (approx.): 21
#
# Overhead  Command       Shared Object                       Symbol
# ........  .......  ..................  ...........................
#
    52.38%     sshd  libc-2.15.so        [.] __GI___connect_internal
    19.05%   groups  libc-2.15.so        [.] __GI___connect_internal
     9.52%     sshd  libpthread-2.15.so  [.] __connect_internal     
     9.52%     mesg  libc-2.15.so        [.] __GI___connect_internal
     9.52%     bash  libc-2.15.so        [.] __GI___connect_internal
</pre>

<p>The report shows that sshd, groups, mesg, and bash are all performing connect() syscalls. Ring a bell?</p>

<p>The stack traces that led to the connect() can explain why:</p>

<pre># <b>perf record -e syscalls:sys_enter_connect -ag</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.057 MB perf.data (~2499 samples) ]
# <b>perf report --stdio</b>
[...]
    55.00%     sshd  libc-2.15.so        [.] __GI___connect_internal
               |
               --- __GI___connect_internal
                  |          
                  |--27.27%-- add_one_listen_addr.isra.0
                  |          
                  |--27.27%-- __nscd_get_mapping
                  |          __nscd_get_map_ref
                  |          
                  |--27.27%-- __nscd_open_socket
                   --18.18%-- [...]
    20.00%   groups  libc-2.15.so        [.] __GI___connect_internal
             |
             --- __GI___connect_internal
                |          
                |--50.00%-- __nscd_get_mapping
                |          __nscd_get_map_ref
                |          
                 --50.00%-- __nscd_open_socket
    10.00%     mesg  libc-2.15.so        [.] __GI___connect_internal
               |
               --- __GI___connect_internal
                  |          
                  |--50.00%-- __nscd_get_mapping
                  |          __nscd_get_map_ref
                  |          
                   --50.00%-- __nscd_open_socket
    10.00%     bash  libc-2.15.so        [.] __GI___connect_internal
               |
               --- __GI___connect_internal
                  |          
                  |--50.00%-- __nscd_get_mapping
                  |          __nscd_get_map_ref
                  |          
                   --50.00%-- __nscd_open_socket
     5.00%     sshd  libpthread-2.15.so  [.] __connect_internal     
               |
               --- __connect_internal
</pre>

<p>Ah, these are nscd calls: the name service cache daemon.  If you see hexadecimal numbers and not function names, you will need to install debug info: see the earlier section on <a href="#Symbols">Symbols</a>. These nscd calls are likely triggered by calling getaddrinfo(), which server software may be using to resolve IP addresses for logging, or for matching hostnames in config files. Browsing the stack traces should identify why.</p>

<p>For sshd, this was called via add_one_listen_addr(): a name that was only visible after adding the openssh-server-dbgsym package. Unfortunately, the stack trace doesn&#39;t continue after add_one_listen_add(). I can browse the OpenSSH code to figure out the reasons we&#39;re calling into add_one_listen_add(), or, I can get the stack traces to work. See the earlier section on <a href="#StackTraces">Stack Traces</a>.</p>

<p>I took a quick look at the OpenSSH code, and it looks like this code-path is due to parsing ListenAddress from the sshd_config file, which can contain either an IP address or a hostname.</p>

<h3>Socket Buffers</h3>

<p>Tracing the consumption of socket buffers, and the stack traces, is one way to identify what is leading to socket or network I/O.</p>

<pre># <b>perf record -e &#39;skb:consume_skb&#39; -ag</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.065 MB perf.data (~2851 samples) ]
# <b>perf report</b>
[...]
    74.42%  swapper  [kernel.kallsyms]  [k] consume_skb
            |
            --- consume_skb
                arp_process
                arp_rcv
                __netif_receive_skb_core
                __netif_receive_skb
                netif_receive_skb
                virtnet_poll
                net_rx_action
                __do_softirq
                irq_exit
                do_IRQ
                ret_from_intr
                default_idle
                cpu_idle
                start_secondary

    25.58%     sshd  [kernel.kallsyms]  [k] consume_skb
               |
               --- consume_skb
                   dev_kfree_skb_any
                   free_old_xmit_skbs.isra.24
                   start_xmit
                   dev_hard_start_xmit
                   sch_direct_xmit
                   dev_queue_xmit
                   ip_finish_output
                   ip_output
                   ip_local_out
                   ip_queue_xmit
                   tcp_transmit_skb
                   tcp_write_xmit
                   __tcp_push_pending_frames
                   tcp_sendmsg
                   inet_sendmsg
                   sock_aio_write
                   do_sync_write
                   vfs_write
                   sys_write
                   system_call_fastpath
                   __write_nocancel
</pre>

<p>The swapper stack shows the network receive path, triggered by an interrupt. The sshd path shows writes.</p>

<h2>6.5. Static User Tracing</h2>

<p>Support was added in later 4.x series kernels. The following demonstrates Linux 4.10 (with an additional patchset), and tracing the Node.js USDT probes:</p>

<pre># <b>perf buildid-cache --add `which node`</b>
# <b>perf list | grep sdt_node</b>
  sdt_node:gc__done                                  [SDT event]
  sdt_node:gc__start                                 [SDT event]
  sdt_node:http__client__request                     [SDT event]
  sdt_node:http__client__response                    [SDT event]
  sdt_node:http__server__request                     [SDT event]
  sdt_node:http__server__response                    [SDT event]
  sdt_node:net__server__connection                   [SDT event]
  sdt_node:net__stream__end                          [SDT event]
# <b>perf record -e sdt_node:http__server__request -a</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.446 MB perf.data (3 samples) ]
# <b>perf script</b>
            node  7646 [002]   361.012364: sdt_node:http__server__request: (dc2e69)
            node  7646 [002]   361.204718: sdt_node:http__server__request: (dc2e69)
            node  7646 [002]   361.363043: sdt_node:http__server__request: (dc2e69)
</pre>

<p>XXX fill me in, including how to use arguments.</p>

<p>If you are on an older kernel, say, Linux 4.4-4.9, you can probably get these to work with adjustments (I&#39;ve even hacked them up with <a href="http://www.brendangregg.com/blog/2015-07-03/hacking-linux-usdt-ftrace.html">ftrace</a> for older kernels), but since they have been in development, I haven&#39;t seen documentation outside of lkml, so you&#39;ll need to figure it out. (On this kernel range, you might find more documentation for tracing these with <a href="https://www.brendangregg.com/ebpf.html#bcc">bcc/eBPF</a>, including using the trace.py tool.)</p>

<h2>6.6. Dynamic Tracing</h2>

<p>For kernel analysis, I&#39;m using CONFIG_KPROBES=y and CONFIG_KPROBE_EVENTS=y, to enable kernel dynamic tracing, and CONFIG_FRAME_POINTER=y, for frame pointer-based kernel stacks. For user-level analysis, CONFIG_UPROBES=y and CONFIG_UPROBE_EVENTS=y, for user-level dynamic tracing.</p>

<h3>Kernel: tcp_sendmsg()</h3>

<p>This example shows instrumenting the kernel tcp_sendmsg() function on the Linux 3.9.3 kernel:</p>

<pre># <b>perf probe --add tcp_sendmsg</b>
Failed to find path of kernel module.
Added new event:
  probe:tcp_sendmsg    (on tcp_sendmsg)

You can now use it in all perf tools, such as:

	perf record -e probe:tcp_sendmsg -aR sleep 1
</pre>

<p>This adds a new tracepoint event. It suggests using the -R option, to collect raw sample records, which is already the default for tracepoints. Tracing this event for 5 seconds, recording stack traces:</p>

<pre># <b>perf record -e probe:tcp_sendmsg -a -g -- sleep 5</b>
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.228 MB perf.data (~9974 samples) ]
</pre>

<p>And the report:</p>

<pre># <b>perf report --stdio</b>
# ========
# captured on: Fri Jan 31 20:10:14 2014
# hostname : pgbackup
# os release : 3.9.3-ubuntu-12-opt
# perf version : 3.9.3
# arch : x86_64
# nrcpus online : 8
# nrcpus avail : 8
# cpudesc : Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
# cpuid : GenuineIntel,6,45,7
# total memory : 8179104 kB
# cmdline : /lib/modules/3.9.3/build/tools/perf/perf record -e probe:tcp_sendmsg -a -g -- sleep 5 
# event : name = probe:tcp_sendmsg, type = 2, config = 0x3b2, config1 = 0x0, config2 = 0x0, ...
# HEADER_CPU_TOPOLOGY info available, use -I to display
# HEADER_NUMA_TOPOLOGY info available, use -I to display
# pmu mappings: software = 1, tracepoint = 2, breakpoint = 5
# ========
#
# Samples: 12  of event &#39;probe:tcp_sendmsg&#39;
# Event count (approx.): 12
#
# Overhead  Command      Shared Object           Symbol
# ........  .......  .................  ...............
#
   100.00%     sshd  [kernel.kallsyms]  [k] tcp_sendmsg
               |
               --- tcp_sendmsg
                   sock_aio_write
                   do_sync_write
                   vfs_write
                   sys_write
                   system_call_fastpath
                   __write_nocancel
                  |          
                  |--8.33%-- 0x50f00000001b810
                   --91.67%-- [...]
</pre>

<p>This shows the path from the write() system call to tcp_sendmsg().</p>

<p>You can delete these dynamic tracepoints if you want after use, using <tt>perf probe --del</tt>.</p>

<h3>Kernel: tcp_sendmsg() with size</h3>

<p>If your kernel has debuginfo (CONFIG_DEBUG_INFO=y), you can fish out kernel variables from functions. This is a simple example of examining a size_t (integer), on Linux 3.13.1.</p>

<p>Listing variables available for tcp_sendmsg():</p>

<pre># <b>perf probe -V tcp_sendmsg</b>
Available variables at tcp_sendmsg
        @&lt;tcp_sendmsg+0&gt;
                size_t  size
                struct kiocb*   iocb
                struct msghdr*  msg
                struct sock*    sk
</pre>

<p>Creating a probe for tcp_sendmsg() with the &#34;size&#34; variable:</p>

<pre># <b>perf probe --add &#39;tcp_sendmsg size&#39;</b>
Added new event:
  probe:tcp_sendmsg    (on tcp_sendmsg with size)

You can now use it in all perf tools, such as:

	perf record -e probe:tcp_sendmsg -aR sleep 1
</pre>

<p>Tracing this probe:</p>

<pre># <b>perf record -e probe:tcp_sendmsg -a</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.052 MB perf.data (~2252 samples) ]
# <b>perf script</b>
# ========
# captured on: Fri Jan 31 23:49:55 2014
# hostname : dev1
# os release : 3.13.1-ubuntu-12-opt
# perf version : 3.13.1
# arch : x86_64
# nrcpus online : 2
# nrcpus avail : 2
# cpudesc : Intel(R) Xeon(R) CPU E5645 @ 2.40GHz
# cpuid : GenuineIntel,6,44,2
# total memory : 1796024 kB
# cmdline : /usr/bin/perf record -e probe:tcp_sendmsg -a 
# event : name = probe:tcp_sendmsg, type = 2, config = 0x1dd, config1 = 0x0, config2 = ...
# HEADER_CPU_TOPOLOGY info available, use -I to display
# HEADER_NUMA_TOPOLOGY info available, use -I to display
# pmu mappings: software = 1, tracepoint = 2, breakpoint = 5
# ========
#
            sshd  1301 [001]   502.424719: probe:tcp_sendmsg: (ffffffff81505d80) size=b0
            sshd  1301 [001]   502.424814: probe:tcp_sendmsg: (ffffffff81505d80) size=40
            sshd  2371 [000]   502.952590: probe:tcp_sendmsg: (ffffffff81505d80) size=27
            sshd  2372 [000]   503.025023: probe:tcp_sendmsg: (ffffffff81505d80) size=3c0
            sshd  2372 [001]   503.203776: probe:tcp_sendmsg: (ffffffff81505d80) size=98
            sshd  2372 [001]   503.281312: probe:tcp_sendmsg: (ffffffff81505d80) size=2d0
            sshd  2372 [001]   503.461358: probe:tcp_sendmsg: (ffffffff81505d80) size=30
            sshd  2372 [001]   503.670239: probe:tcp_sendmsg: (ffffffff81505d80) size=40
            sshd  2372 [001]   503.742565: probe:tcp_sendmsg: (ffffffff81505d80) size=140
            sshd  2372 [001]   503.822005: probe:tcp_sendmsg: (ffffffff81505d80) size=20
            sshd  2371 [000]   504.118728: probe:tcp_sendmsg: (ffffffff81505d80) size=30
            sshd  2371 [000]   504.192575: probe:tcp_sendmsg: (ffffffff81505d80) size=70
[...]
</pre>

<p>The size is shown as hexadecimal.</p>

<h3>Kernel: tcp_sendmsg() line number and local variable</h3>

<p>With debuginfo, perf_events can create tracepoints for lines within kernel functions. Listing available line probes for tcp_sendmsg():</p>

<pre># <b>perf probe -L tcp_sendmsg</b>
&lt;tcp_sendmsg@/mnt/src/linux-3.14.5/net/ipv4/tcp.c:0&gt;
      0  int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
                        size_t size)
      2  {
                struct iovec *iov;
                struct tcp_sock *tp = tcp_sk(sk);
                struct sk_buff *skb;
      6         int iovlen, flags, err, copied = 0;
      7         int mss_now = 0, size_goal, copied_syn = 0, offset = 0;
                bool sg;
                long timeo;
[...]
     79                 while (seglen &gt; 0) {
                                int copy = 0;
     81                         int max = size_goal;
         
                                skb = tcp_write_queue_tail(sk);
     84                         if (tcp_send_head(sk)) {
     85                                 if (skb-&gt;ip_summed == CHECKSUM_NONE)
                                                max = mss_now;
     87                                 copy = max - skb-&gt;len;
                                }
         
     90                         if (copy &lt;= 0) {
         new_segment:
[...]
</pre>

<p>This is Linux 3.14.5; your kernel version may look different. Lets check what variables are available on line 81:</p>

<pre># <b>perf probe -V tcp_sendmsg:81</b>
Available variables at tcp_sendmsg:81
        @&lt;tcp_sendmsg+537&gt;
                bool    sg
                int     copied
                int     copied_syn
                int     flags
                int     mss_now
                int     offset
                int     size_goal
                long int        timeo
                size_t  seglen
                struct iovec*   iov
                struct sock*    sk
                unsigned char*  from
</pre>

<p>Now lets trace line 81, with the seglen variable that is checked in the loop:</p>

<pre># <b>perf probe --add &#39;tcp_sendmsg:81 seglen&#39;</b>
Added new event:
  probe:tcp_sendmsg    (on tcp_sendmsg:81 with seglen)

You can now use it in all perf tools, such as:

	perf record -e probe:tcp_sendmsg -aR sleep 1

# <b>perf record -e probe:tcp_sendmsg -a</b>
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.188 MB perf.data (~8200 samples) ]
# <b>perf script</b>
            sshd  4652 [001] 2082360.931086: probe:tcp_sendmsg: (ffffffff81642ca9) seglen=0x80
   app_plugin.pl  2400 [001] 2082360.970489: probe:tcp_sendmsg: (ffffffff81642ca9) seglen=0x20
        postgres  2422 [000] 2082360.970703: probe:tcp_sendmsg: (ffffffff81642ca9) seglen=0x52
   app_plugin.pl  2400 [000] 2082360.970890: probe:tcp_sendmsg: (ffffffff81642ca9) seglen=0x7b
        postgres  2422 [001] 2082360.971099: probe:tcp_sendmsg: (ffffffff81642ca9) seglen=0xb
   app_plugin.pl  2400 [000] 2082360.971140: probe:tcp_sendmsg: (ffffffff81642ca9) seglen=0x55
[...]
</pre>

<p>This is pretty amazing. Remember that you can also include in-kernel filtering using --filter, to match only the data you want.</p>

<h3>User: malloc()</h3>

<p>While this is an interesting example, I want to say right off the bat that malloc() calls are very frequent, so you will need to consider the overheads of tracing calls like this.</p>

<p>Adding a libc malloc() probe:</p>

<pre># <b>perf probe -x /lib/x86_64-linux-gnu/libc-2.15.so --add malloc</b>
Added new event:
  probe_libc:malloc    (on 0x82f20)

You can now use it in all perf tools, such as:

	perf record -e probe_libc:malloc -aR sleep 1
</pre>

<p>Tracing it system-wide:</p>

<pre># <b>perf record -e probe_libc:malloc -a</b>
^C[ perf record: Woken up 12 times to write data ]
[ perf record: Captured and wrote 3.522 MB perf.data (~153866 samples) ]
</pre>

<p>The report:</p>

<pre># <b>perf report -n</b>
[...]
# Samples: 45K of event &#39;probe_libc:malloc&#39;
# Event count (approx.): 45158
#
# Overhead       Samples          Command  Shared Object      Symbol
# ........  ............  ...............  .............  ..........
#
    42.72%         19292       apt-config  libc-2.15.so   [.] malloc
    19.71%          8902             grep  libc-2.15.so   [.] malloc
     7.88%          3557             sshd  libc-2.15.so   [.] malloc
     6.25%          2824              sed  libc-2.15.so   [.] malloc
     6.06%          2738            which  libc-2.15.so   [.] malloc
     4.12%          1862  update-motd-upd  libc-2.15.so   [.] malloc
     3.72%          1680             stat  libc-2.15.so   [.] malloc
     1.68%           758            login  libc-2.15.so   [.] malloc
     1.21%           546        run-parts  libc-2.15.so   [.] malloc
     1.21%           545               ls  libc-2.15.so   [.] malloc
     0.80%           360        dircolors  libc-2.15.so   [.] malloc
     0.56%           252               tr  libc-2.15.so   [.] malloc
     0.54%           242              top  libc-2.15.so   [.] malloc
     0.49%           222       irqbalance  libc-2.15.so   [.] malloc
     0.44%           200             dpkg  libc-2.15.so   [.] malloc
     0.38%           173         lesspipe  libc-2.15.so   [.] malloc
     0.29%           130  update-motd-fsc  libc-2.15.so   [.] malloc
     0.25%           112            uname  libc-2.15.so   [.] malloc
     0.24%           108              cut  libc-2.15.so   [.] malloc
     0.23%           104           groups  libc-2.15.so   [.] malloc
     0.21%            94  release-upgrade  libc-2.15.so   [.] malloc
     0.18%            82        00-header  libc-2.15.so   [.] malloc
     0.14%            62             mesg  libc-2.15.so   [.] malloc
     0.09%            42  update-motd-reb  libc-2.15.so   [.] malloc
     0.09%            40             date  libc-2.15.so   [.] malloc
     0.08%            35             bash  libc-2.15.so   [.] malloc
     0.08%            35         basename  libc-2.15.so   [.] malloc
     0.08%            34          dirname  libc-2.15.so   [.] malloc
     0.06%            29               sh  libc-2.15.so   [.] malloc
     0.06%            26        99-footer  libc-2.15.so   [.] malloc
     0.05%            24              cat  libc-2.15.so   [.] malloc
     0.04%            18             expr  libc-2.15.so   [.] malloc
     0.04%            17         rsyslogd  libc-2.15.so   [.] malloc
     0.03%            12             stty  libc-2.15.so   [.] malloc
     0.00%             1             cron  libc-2.15.so   [.] malloc
</pre>

<p>This shows the most malloc() calls were by apt-config, while I was tracing.</p>

<h3>User: malloc() with size</h3>

<p>As of the Linux 3.13.1 kernel, this is not supported yet:</p>

<pre># <b>perf probe -x /lib/x86_64-linux-gnu/libc-2.15.so --add &#39;malloc size&#39;</b>
Debuginfo-analysis is not yet supported with -x/--exec option.
  Error: Failed to add events. (-38)
</pre>

<p>As a workaround, you can access the registers (on Linux 3.7+). For example, on x86_64:</p>

<pre># <b>perf probe -x /lib64/libc-2.17.so &#39;--add=malloc size=%di&#39;</b>
       probe_libc:malloc    (on 0x800c0 with size=%di)
</pre>

<p>These registers (&#34;%di&#34; etc) are dependent on your processor architecture. To figure out which ones to use, see the <a href="https://en.wikipedia.org/wiki/X86_calling_conventions#System_V_AMD64_ABI">X86 calling conventions</a> on Wikipedia, or page 24 of the <a href="http://x86-64.org/documentation/abi.pdf">AMD64 ABI</a> (PDF). (Thanks Jose E. Nunez for digging out these references.) </p>

<h2>6.7. Scheduler Analysis</h2>

<p>The <tt>perf sched</tt> subcommand provides a number of tools for analyzing kernel CPU scheduler behavior. You can use this to identify and quantify issues of scheduler latency.</p>

<p>The current overhead of this tool (as of up to Linux 4.10) may be noticeable, as it instruments and dumps scheduler events to the perf.data file for later analysis. For example:</p>

<pre># <b>perf sched record -- sleep 1</b>
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 1.886 MB perf.data (13502 samples) ]
</pre>

<p>That&#39;s 1.9 Mbytes for one second, including 13,502 samples. The size and rate will be relative to your workload and number of CPUs (this example is an 8 CPU server running a software build). How this is written to the file system has been optimized: it only woke up one time to read the event buffers and write them to disk, which greatly reduces overhead. That said, there are still significant overheads with instrumenting all scheduler events and writing event data to the file system. These events:</p>

<!--- untruncated
# ========
# captured on: Sun Feb 26 19:40:00 2017
# hostname : bgregg-xenial
# os release : 4.10
# perf version : 4.10
# arch : x86_64
# nrcpus online : 8
# nrcpus avail : 8
# cpudesc : Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz
# cpuid : GenuineIntel,6,62,4
# total memory : 15401700 kB
# cmdline : /usr/bin/perf sched record -- sleep 1 
# event : name = sched:sched_switch, , id = { 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759 }, type = 2, size = 112, config = 0x122, { sample_period, sample_freq } 
= 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, mmap = 1, comm = 1, task = 1, sample_id_all = 1, exclude_guest = 1, mm
ap2 = 1, comm_exec = 1
# event : name = sched:sched_stat_wait, , id = { 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767 }, type = 2, size = 112, config = 0x11a, { sample_period, sample_freq
 } = 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, sample_id_all = 1, exclude_guest = 1
# event : name = sched:sched_stat_sleep, , id = { 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775 }, type = 2, size = 112, config = 0x119, { sample_period, sample_fre
q } = 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, sample_id_all = 1, exclude_guest = 1
# event : name = sched:sched_stat_iowait, , id = { 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783 }, type = 2, size = 112, config = 0x118, { sample_period, sample_fr
eq } = 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, sample_id_all = 1, exclude_guest = 1
# event : name = sched:sched_stat_runtime, , id = { 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791 }, type = 2, size = 112, config = 0x116, { sample_period, sample_f
req } = 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, sample_id_all = 1, exclude_guest = 1
# event : name = sched:sched_process_fork, , id = { 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799 }, type = 2, size = 112, config = 0x11c, { sample_period, sample_f
req } = 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, sample_id_all = 1, exclude_guest = 1
# event : name = sched:sched_wakeup, , id = { 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807 }, type = 2, size = 112, config = 0x124, { sample_period, sample_freq } 
= 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, sample_id_all = 1, exclude_guest = 1
# event : name = sched:sched_wakeup_new, , id = { 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815 }, type = 2, size = 112, config = 0x123, { sample_period, sample_fre
q } = 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, sample_id_all = 1, exclude_guest = 1
# event : name = sched:sched_migrate_task, , id = { 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823 }, type = 2, size = 112, config = 0x121, { sample_period, sample_f
req } = 1, sample_type = IP|TID|TIME|ID|CPU|PERIOD|RAW, read_format = ID, disabled = 1, inherit = 1, sample_id_all = 1, exclude_guest = 1
# HEADER_CPU_TOPOLOGY info available, use -I to display
# HEADER_NUMA_TOPOLOGY info available, use -I to display
# pmu mappings: breakpoint = 5, power = 7, software = 1, tracepoint = 2, msr = 6
# HEADER_CACHE info available, use -I to display
# missing features: HEADER_BRANCH_STACK HEADER_GROUP_DESC HEADER_AUXTRACE HEADER_STAT 
# ========
#
            perf 16984 [005] 991962.879960: sched:sched_stat_runtime: comm=perf pid=16984 runtime=3901506 [ns] vruntime=1652494271 [ns]
            perf 16984 [005] 991962.879966:       sched:sched_wakeup: comm=perf pid=16999 prio=120 target_cpu=005
            perf 16984 [005] 991962.879971:       sched:sched_switch: prev_comm=perf prev_pid=16984 prev_prio=120 prev_state=R ==> next_comm=perf next_pid=16999 next
_prio=120
            perf 16999 [005] 991962.880058: sched:sched_stat_runtime: comm=perf pid=16999 runtime=98309 [ns] vruntime=1640592580 [ns]
             cc1 16881 [000] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16881 runtime=3999231 [ns] vruntime=78979518773 [ns]
          :17024 17024 [004] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=17024 runtime=3866637 [ns] vruntime=78103071578 [ns]
             cc1 16900 [001] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16900 runtime=3006028 [ns] vruntime=77728971017 [ns]
             cc1 16825 [006] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16825 runtime=3999423 [ns] vruntime=78764960571 [ns]
             cc1 16880 [002] 991962.880059: sched:sched_stat_runtime: comm=cc1 pid=16880 runtime=3999502 [ns] vruntime=78594775533 [ns]
             cc1 16945 [003] 991962.880059: sched:sched_stat_runtime: comm=cc1 pid=16945 runtime=3008672 [ns] vruntime=83611052305 [ns]
-->

<pre># <b>perf script --header</b>
# ========
# captured on: Sun Feb 26 19:40:00 2017
# hostname : bgregg-xenial
# os release : 4.10-virtual
# perf version : 4.10
# arch : x86_64
# nrcpus online : 8
# nrcpus avail : 8
# cpudesc : Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz
# cpuid : GenuineIntel,6,62,4
# total memory : 15401700 kB
# cmdline : /usr/bin/perf sched record -- sleep 1 
# event : name = <b>sched:sched_switch</b>, , id = { 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759 }, type = 2, size = 11...
# event : name = <b>sched:sched_stat_wait</b>, , id = { 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767 }, type = 2, size =...
# event : name = <b>sched:sched_stat_sleep</b>, , id = { 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775 }, type = 2, size ...
# event : name = <b>sched:sched_stat_iowait</b>, , id = { 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783 }, type = 2, size...
# event : name = <b>sched:sched_stat_runtime</b>, , id = { 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791 }, type = 2, siz...
# event : name = <b>sched:sched_process_fork</b>, , id = { 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799 }, type = 2, siz...
# event : name = <b>sched:sched_wakeup</b>, , id = { 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807 }, type = 2, size = 11...
# event : name = <b>sched:sched_wakeup_new</b>, , id = { 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815 }, type = 2, size ...
# event : name = <b>sched:sched_migrate_task</b>, , id = { 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823 }, type = 2, siz...
# HEADER_CPU_TOPOLOGY info available, use -I to display
# HEADER_NUMA_TOPOLOGY info available, use -I to display
# pmu mappings: breakpoint = 5, power = 7, software = 1, tracepoint = 2, msr = 6
# HEADER_CACHE info available, use -I to display
# missing features: HEADER_BRANCH_STACK HEADER_GROUP_DESC HEADER_AUXTRACE HEADER_STAT 
# ========
#
    perf 16984 [005] 991962.879966:       sched:sched_wakeup: comm=perf pid=16999 prio=120 target_cpu=005
[...]
</pre>

<p>If overhead is a problem, you can use my <a href="https://www.brendangregg.com/ebpf.html#bcc">eBPF/bcc Tools</a> including runqlat and runqlen which use in-kernel summaries of scheduler events, reducing overhead further. An advantage of <tt>perf sched</tt> dumping all events is that you aren&#39;t limited to the summary. If you caught an intermittent event, you can analyze those recorded events in custom ways until you understood the issue, rather than needing to catch it a second time.</p>

<p>The captured trace file can be reported in a number of ways, summarized by the help message:</p>

<pre># <b>perf sched -h</b>

 Usage: perf sched [<options>] {record|<b>latency|map|replay|script|timehist</b>}

    -D, --dump-raw-trace  dump raw trace in ASCII
    -f, --force           don&#39;t complain, do it
    -i, --input <file>    input file name
    -v, --verbose         be more verbose (show symbol address, etc)
</file></options></pre>

<p><b>perf sched latency</b> will summarize scheduler latencies by task, including average and maximum delay:</p>

<pre># <b>perf sched latency</b>

 -----------------------------------------------------------------------------------------------------------------
  Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at       |
 -----------------------------------------------------------------------------------------------------------------
  cat:(6)               |     12.002 ms |        6 | avg:   17.541 ms | max:   29.702 ms | max at: 991962.948070 s
  ar:17043              |      3.191 ms |        1 | avg:   13.638 ms | max:   13.638 ms | max at: 991963.048070 s
  rm:(10)               |     20.955 ms |       10 | avg:   11.212 ms | max:   19.598 ms | max at: 991963.404069 s
  objdump:(6)           |     35.870 ms |        8 | avg:   10.969 ms | max:   16.509 ms | max at: 991963.424443 s
  :17008:17008          |    462.213 ms |       50 | avg:   10.464 ms | max:   35.999 ms | max at: 991963.120069 s
  grep:(7)              |     21.655 ms |       11 | avg:    9.465 ms | max:   24.502 ms | max at: 991963.464082 s
  fixdep:(6)            |     81.066 ms |        8 | avg:    9.023 ms | max:   19.521 ms | max at: 991963.120068 s
  mv:(10)               |     30.249 ms |       14 | avg:    8.380 ms | max:   21.688 ms | max at: 991963.200073 s
  ld:(3)                |     14.353 ms |        6 | avg:    7.376 ms | max:   15.498 ms | max at: 991963.452070 s
  recordmcount:(7)      |     14.629 ms |        9 | avg:    7.155 ms | max:   18.964 ms | max at: 991963.292100 s
  svstat:17067          |      1.862 ms |        1 | avg:    6.142 ms | max:    6.142 ms | max at: 991963.280069 s
  cc1:(21)              |   6013.457 ms |     1138 | avg:    5.305 ms | max:   44.001 ms | max at: 991963.436070 s
  gcc:(18)              |     43.596 ms |       40 | avg:    3.905 ms | max:   26.994 ms | max at: 991963.380069 s
  ps:17073              |     27.158 ms |        4 | avg:    3.751 ms | max:    8.000 ms | max at: 991963.332070 s
<!--
  sed:17075             |      2.993 ms |        3 | avg:    3.704 ms | max:    6.216 ms | max at: 991963.352070 s
  make:(18)             |     53.968 ms |      114 | avg:    2.331 ms | max:   12.986 ms | max at: 991962.976087 s
  perl:17068            |      4.266 ms |        2 | avg:    2.184 ms | max:    2.851 ms | max at: 991963.288076 s
  chown:(6)             |     17.345 ms |        6 | avg:    2.080 ms | max:    4.834 ms | max at: 991963.432089 s
  lsb_release:17136     |     81.920 ms |        4 | avg:    2.028 ms | max:    3.999 ms | max at: 991963.600070 s
  echo:17089            |      1.718 ms |        1 | avg:    2.012 ms | max:    2.012 ms | max at: 991963.356069 s
  ls:17101              |      3.340 ms |        1 | avg:    1.953 ms | max:    1.953 ms | max at: 991963.404071 s
  as:(16)               |    121.816 ms |       69 | avg:    1.837 ms | max:   23.199 ms | max at: 991963.091685 s
  xargs:17077           |      2.655 ms |        4 | avg:    1.801 ms | max:    4.933 ms | max at: 991963.304068 s
  bash:(2)              |      6.363 ms |        7 | avg:    1.799 ms | max:    3.639 ms | max at: 991963.356077 s
  sh:(25)               |    107.078 ms |      108 | avg:    1.025 ms | max:   16.309 ms | max at: 991963.340100 s
  :17024:17024          |    151.717 ms |      232 | avg:    0.957 ms | max:   16.010 ms | max at: 991963.192087 s
  :17016:17016          |    208.478 ms |      356 | avg:    0.956 ms | max:   20.679 ms | max at: 991962.908075 s
  recordProgramSt:17113 |     71.450 ms |       14 | avg:    0.920 ms | max:    3.993 ms | max at: 991963.472077 s
  cut:17076             |      2.069 ms |        2 | avg:    0.892 ms | max:    1.441 ms | max at: 991963.300073 s
  chmod:(2)             |      3.692 ms |        2 | avg:    0.846 ms | max:    1.017 ms | max at: 991963.380073 s
  redis-server:(2)      |      1.659 ms |       22 | avg:    0.575 ms | max:    3.833 ms | max at: 991963.536074 s
  catalina.sh:17063     |     23.114 ms |       26 | avg:    0.509 ms | max:    3.867 ms | max at: 991963.292070 s
  ksoftirqd/7:52        |      0.821 ms |       22 | avg:    0.248 ms | max:    2.443 ms | max at: 991963.418516 s
-->[...]
</pre>

<p>To shed some light as to how this is instrumented and calculated, I&#39;ll show the events that led to the top event&#39;s &#34;Maximum delay at&#34; of 29.702 ms. Here are the raw events from <tt>perf sched script</tt>:

</p><pre>      sh 17028 [001] 991962.918368:   sched:sched_wakeup_new: comm=sh pid=17030 prio=120 target_cpu=002
[...]
     cc1 16819 [002] 991962.948070:       sched:sched_switch: prev_comm=cc1 prev_pid=16819 prev_prio=120
                                                            prev_state=R ==&gt; next_comm=sh next_pid=17030 next_prio=120
[...]
</pre>

<p>The time from the wakeup (991962.918368, which is in seconds) to the context switch (991962.948070) is 29.702 ms. This process is listed as &#34;sh&#34; (shell) in the raw events, but execs &#34;cat&#34; soon after, so is shown as &#34;cat&#34; in the <tt>perf sched latency</tt> output.</p>

<p><b>perf sched map</b> shows all CPUs and context-switch events, with columns representing what each CPU was doing and when. It&#39;s the kind of data you see visualized in scheduler analysis GUIs (including <tt>perf timechart</tt>, with the layout rotated 90 degrees). Example output:</p>

<pre># <b>perf sched map</b>
                      *A0           991962.879971 secs A0 =&gt; perf:16999
                       A0     *B0   991962.880070 secs B0 =&gt; cc1:16863
          *C0          A0      B0   991962.880070 secs C0 =&gt; :17023:17023
  *D0      C0          A0      B0   991962.880078 secs D0 =&gt; ksoftirqd/0:6
   D0      C0 *E0      A0      B0   991962.880081 secs E0 =&gt; ksoftirqd/3:28
   D0      C0 *F0      A0      B0   991962.880093 secs F0 =&gt; :17022:17022
  *G0      C0  F0      A0      B0   991962.880108 secs G0 =&gt; :17016:17016
   G0      C0  F0     *H0      B0   991962.880256 secs H0 =&gt; migration/5:39
   G0      C0  F0     *I0      B0   991962.880276 secs I0 =&gt; perf:16984
   G0      C0  F0     *J0      B0   991962.880687 secs J0 =&gt; cc1:16996
   G0      C0 *K0      J0      B0   991962.881839 secs K0 =&gt; cc1:16945
   G0      C0  K0      J0 *L0  B0   991962.881841 secs L0 =&gt; :17020:17020
   G0      C0  K0      J0 *M0  B0   991962.882289 secs M0 =&gt; make:16637
   G0      C0  K0      J0 *N0  B0   991962.883102 secs N0 =&gt; make:16545
   G0     *O0  K0      J0  N0  B0   991962.883880 secs O0 =&gt; cc1:16819
   G0 *A0  O0  K0      J0  N0  B0   991962.884069 secs 
   G0  A0  O0  K0 *P0  J0  N0  B0   991962.884076 secs P0 =&gt; rcu_sched:7
   G0  A0  O0  K0 *Q0  J0  N0  B0   991962.884084 secs Q0 =&gt; cc1:16831
   G0  A0  O0  K0  Q0  J0 *R0  B0   991962.884843 secs R0 =&gt; cc1:16825
   G0 *S0  O0  K0  Q0  J0  R0  B0   991962.885636 secs S0 =&gt; cc1:16900
   G0  S0  O0 *T0  Q0  J0  R0  B0   991962.886893 secs T0 =&gt; :17014:17014
   G0  S0  O0 *K0  Q0  J0  R0  B0   991962.886917 secs 
<!--
  *T0  S0  O0  K0  Q0  J0  R0  B0   991962.887396 secs 
  *U0  S0  O0  K0  Q0  J0  R0  B0   991962.888039 secs U0 => cc1:16752
   U0  S0 *P0  K0  Q0  J0  R0  B0   991962.888077 secs 
   U0  S0 *O0  K0  Q0  J0  R0  B0   991962.888084 secs 
   U0  S0 *V0  K0  Q0  J0  R0  B0   991962.889559 secs V0 => redis-server:1429
   U0  S0 *W0  K0  Q0  J0  R0  B0   991962.889628 secs W0 => cc1:16880
   U0  S0  W0  K0  Q0  J0  R0 *X0   991962.892078 secs X0 => ksoftirqd/7:52
   U0  S0  W0 *Y0  Q0  J0  R0  X0   991962.892083 secs Y0 => rngd:1528
   U0  S0  W0 *E0  Q0  J0  R0  X0   991962.892096 secs 
   U0  S0  W0  E0  Q0  J0  R0 *Z0   991962.892101 secs Z0 => cc1:16910
   U0  S0  W0 *A1  Q0  J0  R0  Z0   991962.892106 secs A1 => make:17026
   U0  S0  W0 *B1  Q0  J0  R0  Z0   991962.894912 secs B1 => redis-server:1871
   U0  S0  W0 *K0  Q0  J0  R0  Z0   991962.894971 secs 
  *C1  S0  W0  K0  Q0  J0  R0  Z0   991962.896070 secs C1 => cc1:16881
   C1  S0 *P0  K0  Q0  J0  R0  Z0   991962.896073 secs 
   C1  S0 *O0  K0  Q0  J0  R0  Z0   991962.896081 secs 
   C1  S0  O0  K0  Q0  J0 *D1  Z0   991962.896168 secs D1 => as:16832
-->[...]
</pre>

<p>This is an 8 CPU system, and you can see the 8 columns for each CPU starting from the left. Some CPU columns begin blank, as we&#39;ve yet to trace an event on that CPU at the start of the profile. They quickly become populated.</p>

<p>The two character codes you see (&#34;A0&#34;, &#34;C0&#34;) are identifiers for tasks, which are mapped on the right (&#34;=&gt;&#34;). This is more compact than using process (task) IDs. The &#34;*&#34; shows which CPU had the context switch event, and the new event that was running. For example, the very last line shows that at 991962.886917 (seconds) CPU 4 context-switched to K0 (a &#34;cc1&#34; process, PID 16945).</p>

<p>That example was from a busy system. Here&#39;s an idle system:</p>

<pre># <b>perf sched map</b>
                      *A0           993552.887633 secs A0 =&gt; perf:26596
  *.                   A0           993552.887781 secs .  =&gt; swapper:0
   .                  *B0           993552.887843 secs B0 =&gt; migration/5:39
   .                  *.            993552.887858 secs 
   .                   .  *A0       993552.887861 secs 
   .                  *C0  A0       993552.887903 secs C0 =&gt; bash:26622
   .                  *.   A0       993552.888020 secs 
   .          *D0      .   A0       993552.888074 secs D0 =&gt; rcu_sched:7
   .          *.       .   A0       993552.888082 secs 
   .           .      *C0  A0       993552.888143 secs 
   .      *.   .       C0  A0       993552.888173 secs 
   .       .   .      *B0  A0       993552.888439 secs 
   .       .   .      *.   A0       993552.888454 secs 
   .      *C0  .       .   A0       993552.888457 secs 
   .       C0  .       .  *.        993552.889257 secs 
   .      *.   .       .   .        993552.889764 secs 
   .       .  *E0      .   .        993552.889767 secs E0 =&gt; bash:7902
<!--
   .       .   E0     *F0  .        993552.890459 secs F0 => bash:26623
   .       .   E0     *.   .        993552.890568 secs 
   .       .   E0     *F0  .        993552.890618 secs 
   .       .  *.       F0  .        993552.890638 secs 
   .       .   .      *B0  .        993552.890895 secs 
   .       .   .      *.   .        993552.890908 secs 
   .       .  *F0      .   .        993552.890911 secs 
   .  *G0  .   F0      .   .        993552.890917 secs G0 => rngd:1528
   .  *.   .   F0      .   .        993552.890928 secs 
   .   .   .  *.       .   .        993552.891960 secs 
   .   .   .   .  *E0  .   .        993552.891965 secs 
   .   .   .   .   E0  .  *G0       993552.892079 secs 
   .   .   .   .   E0  .  *.        993552.892089 secs 
   .   .   .   .   E0 *H0  .        993552.892683 secs H0 => bash:26624
-->[...]
</pre>

<p>Idle CPUs are shown as &#34;.&#34;.</p>

<p>Remember to examine the timestamp column to make sense of this visualization (GUIs use that as a dimension, which is easier to comprehend, but here the numbers are just listed). It&#39;s also only showing context switch events, and not scheduler latency. The newer <tt>timehist</tt> command has a visualization (-V) that can include wakeup events.</p>

<p><b>perf sched timehist</b> was added in Linux 4.10, and shows the scheduler latency by event, including the time the task was waiting to be woken up (<tt>wait time</tt>) and the scheduler latency after wakeup to running (<tt>sch delay</tt>). It&#39;s the scheduler latency that we&#39;re more interested in tuning. Example output:</p>

<pre># <b>perf sched timehist</b>
Samples do not have callchains.
           time    cpu  task name                       wait time  sch delay   run time
                        [tid/pid]                          (msec)     (msec)     (msec)
--------------- ------  ------------------------------  ---------  ---------  ---------
  991962.879971 [0005]  perf[16984]                         0.000      0.000      0.000 
  991962.880070 [0007]  :17008[17008]                       0.000      0.000      0.000 
  991962.880070 [0002]  cc1[16880]                          0.000      0.000      0.000 
  991962.880078 [0000]  cc1[16881]                          0.000      0.000      0.000 
  991962.880081 [0003]  cc1[16945]                          0.000      0.000      0.000 
  991962.880093 [0003]  ksoftirqd/3[28]                     0.000      0.007      0.012 
  991962.880108 [0000]  ksoftirqd/0[6]                      0.000      0.007      0.030 
  991962.880256 [0005]  perf[16999]                         0.000      0.005      0.285 
  991962.880276 [0005]  migration/5[39]                     0.000      0.007      0.019 
  991962.880687 [0005]  perf[16984]                         0.304      0.000      0.411 
  991962.881839 [0003]  cat[17022]                          0.000      0.000      1.746 
  991962.881841 [0006]  cc1[16825]                          0.000      0.000      0.000 
<!--
  991962.882289 [0006]  :17020                              0.000      0.007      0.448 
  991962.883102 [0006]  make[16637]                         0.000      0.004      0.813 
  991962.883880 [0002]  sh[17023]                           0.000      0.000      3.810 
  991962.884069 [0001]  cc1[16900]                          0.000      0.000      0.000 
  991962.884076 [0004]  :17024[17024]                       0.000      0.000      0.000 
  991962.884084 [0004]  rcu_sched[7]                        0.000      0.006      0.008 
  991962.884843 [0006]  make[16545]                         0.000      0.004      1.740 
  991962.885636 [0001]  sleep[16999]                        3.813      0.000      1.566 
  991962.886893 [0003]  cc1[16945]                          1.758      0.000      5.053 
  991962.886917 [0003]  :17014                              0.000      0.015      0.024 
  991962.887396 [0000]  :17016[17016]                       0.000      0.000      7.288 
  991962.888039 [0000]  :17014                              0.478      0.004      0.643 
  991962.888077 [0002]  cc1[16819]                          0.000      0.000      4.196 
  991962.888084 [0002]  rcu_sched[7]                        3.993      0.004      0.007 
  991962.889559 [0002]  cc1[16819]                          0.007      0.000      1.474 
  991962.889628 [0002]  redis-server[1429]                  0.000      0.008      0.069 
  991962.892078 [0007]  cc1[16863]                          0.000      0.000     12.007 
  991962.892083 [0003]  cc1[16945]                          0.024      0.000      5.165 
  991962.892096 [0003]  rngd[1528]                          0.000      0.012      0.012 
-->[...]
  991963.885740 [0001]  :17008[17008]                      25.613      0.000      0.057 
  991963.886009 [0001]  sleep[16999]                     1000.104      0.006      0.269 
  991963.886018 [0005]  cc1[17083]                         19.998      0.000      9.948 
</pre>

<p>This output includes the <tt>sleep</tt> command run to set the duration of perf itself to one second. Note that <tt>sleep</tt>&#39;s wait time is 1000.104 milliseconds because I had run &#34;sleep 1&#34;: that&#39;s the time it was asleep waiting its timer wakeup event. Its scheduler latency was only 0.006 milliseconds, and its time on-CPU was 0.269 milliseconds.</p>

<!--
# <b>perf sched timehist -M</b>
Samples do not have callchains.
           time    cpu  task name                       wait time  sch delay   run time
                        [tid/pid]                          (msec)     (msec)     (msec)
--------------- ------  ------------------------------  ---------  ---------  ---------
  991962.879971 [0005]  perf[16984]                         0.000      0.000      0.000 
  991962.880070 [0007]  :17008[17008]                       0.000      0.000      0.000 
  991962.880070 [0002]  cc1[16880]                          0.000      0.000      0.000 
  991962.880078 [0000]  cc1[16881]                          0.000      0.000      0.000 
  991962.880081 [0003]  cc1[16945]                          0.000      0.000      0.000 
  991962.880093 [0003]  ksoftirqd/3[28]                     0.000      0.007      0.012 
  991962.880108 [0000]  ksoftirqd/0[6]                      0.000      0.007      0.030 
  991962.880256 [0005]  perf[16999]                         0.000      0.005      0.285 
  991962.880264 [0005]  migration/5[39]                                                  migrated: perf[16999] cpu 5 => 1
  991962.880276 [0005]  migration/5[39]                     0.000      0.007      0.019 
  991962.880682 [0005]  perf[16984]                                                      migrated: cc1[16996] cpu 0 => 5
  991962.880687 [0005]  perf[16984]                         0.304      0.000      0.411 
  991962.881839 [0003]  cat[17022]                          0.000      0.000      1.746 
  991962.881841 [0006]  cc1[16825]                          0.000      0.000      0.000 
  991962.882289 [0006]  :17020                              0.000      0.007      0.448 
  991962.883094 [0006]  make[16637]                                                      migrated: make[16545] cpu 7 => 6
  991962.883102 [0006]  make[16637]                         0.000      0.004      0.813 
  991962.883880 [0002]  sh[17023]                           0.000      0.000      3.810 
  991962.884069 [0001]  cc1[16900]                          0.000      0.000      0.000 
  991962.884076 [0004]  :17024[17024]                       0.000      0.000      0.000 
  991962.884084 [0004]  rcu_sched[7]                        0.000      0.006      0.008 
  991962.884843 [0006]  make[16545]                         0.000      0.004      1.740 
  991962.885636 [0001]  sleep[16999]                        3.813      0.000      1.566 
  991962.886893 [0003]  cc1[16945]                          1.758      0.000      5.053 
  991962.886917 [0003]  :17014                              0.000      0.015      0.024 
  991962.887386 [0000]  :17016[17016]                                                    migrated: :17014 cpu 3 => 0
  991962.887396 [0000]  :17016[17016]                       0.000      0.000      7.288 
  991962.888039 [0000]  :17014                              0.478      0.004      0.643 
  991962.888069 [0002]  cc1[16819]                                                       migrated: rcu_sched[7] cpu 4 => 2
  991962.888077 [0002]  cc1[16819]                          0.000      0.000      4.196 
  991962.888084 [0002]  rcu_sched[7]                        3.993      0.004      0.007 
-->

<p>There are a number of options to timehist, including -V to add a CPU visualization column, -M to add migration events, and -w for wakeup events. For example:</p>

<pre># <b>perf sched timehist -MVw</b>
Samples do not have callchains.
           time    cpu  012345678  task name           wait time  sch delay   run time
                                   [tid/pid]              (msec)     (msec)     (msec)
--------------- ------  ---------  ------------------  ---------  ---------  ---------
  991962.879966 [0005]             perf[16984]                                          awakened: perf[16999]
  991962.879971 [0005]       s     perf[16984]             0.000      0.000      0.000                                 
  991962.880070 [0007]         s   :17008[17008]           0.000      0.000      0.000                                 
  991962.880070 [0002]    s        cc1[16880]              0.000      0.000      0.000                                 
  991962.880071 [0000]             cc1[16881]                                           awakened: ksoftirqd/0[6]
  991962.880073 [0003]             cc1[16945]                                           awakened: ksoftirqd/3[28]
  991962.880078 [0000]  s          cc1[16881]              0.000      0.000      0.000                                 
  991962.880081 [0003]     s       cc1[16945]              0.000      0.000      0.000                                 
  991962.880093 [0003]     s       ksoftirqd/3[28]         0.000      0.007      0.012                                 
  991962.880108 [0000]  s          ksoftirqd/0[6]          0.000      0.007      0.030                                 
  991962.880249 [0005]             perf[16999]                                          awakened: migration/5[39]
  991962.880256 [0005]       s     perf[16999]             0.000      0.005      0.285                                 
  991962.880264 [0005]        m      migration/5[39]                                      migrated: perf[16999] cpu 5 =&gt; 1
  991962.880276 [0005]       s     migration/5[39]         0.000      0.007      0.019                                 
  991962.880682 [0005]        m      perf[16984]                                          migrated: cc1[16996] cpu 0 =&gt; 5
  991962.880687 [0005]       s     perf[16984]             0.304      0.000      0.411                                 
  991962.881834 [0003]             cat[17022]                                           awakened: :17020
<!--
  991962.881839 [0003]     s       cat[17022]              0.000      0.000      1.746                                 
  991962.881841 [0006]        s    cc1[16825]              0.000      0.000      0.000                                 
  991962.882284 [0006]             :17020                                               awakened: make[16637]
  991962.882289 [0006]        s    :17020                  0.000      0.007      0.448                                 
  991962.883094 [0006]         m     make[16637]                                                      migrated: make[16545] cpu 7 => 6
  991962.883098 [0006]             make[16637]                                                      awakened: make[16545]
  991962.883102 [0006]        s    make[16637]                         0.000      0.004      0.813                                 
  991962.883770 [0002]             sh[17023]                                                        awakened: sh[17025]
  991962.883880 [0002]    s        sh[17023]                           0.000      0.000      3.810                                 
  991962.884069 [0004]             :17024[17024]                                                    awakened: rcu_sched[7]
  991962.884069 [0001]   s         cc1[16900]                          0.000      0.000      0.000                                 
  991962.884076 [0004]      s      :17024[17024]                       0.000      0.000      0.000                                 
  991962.884084 [0004]      s      rcu_sched[7]                        0.000      0.006      0.008                                 
  991962.884703 [0006]             make[16545]                                                      awakened: make[17026]
  991962.884843 [0006]        s    make[16545]                         0.000      0.004      1.740                                 
  991962.885636 [0001]   s         sleep[16999]                        3.813      0.000      1.566                                 
  991962.886877 [0000]             :17016[17016]                                                    awakened: :17014
  991962.886893 [0003]     s       cc1[16945]                          1.758      0.000      5.053                                 
  991962.886917 [0003]     s       :17014                              0.000      0.015      0.024                                 
  991962.887386 [0000]   m           :17016[17016]                                                    migrated: :17014 cpu 3 => 0
  991962.887392 [0000]             :17016[17016]                                                    awakened: :17014
  991962.887396 [0000]  s          :17016[17016]                       0.000      0.000      7.288                 
-->[...]
  991963.885734 [0001]             :17008[17008]                                        awakened: sleep[16999]
  991963.885740 [0001]   s         :17008[17008]          25.613      0.000      0.057                           
  991963.886005 [0001]             sleep[16999]                                         awakened: perf[16984]
  991963.886009 [0001]   s         sleep[16999]         1000.104      0.006      0.269
  991963.886018 [0005]       s     cc1[17083]             19.998      0.000      9.948 
</pre>

<p>The CPU visualization column (&#34;012345678&#34;) has &#34;s&#34; for context-switch events, and &#34;m&#34; for migration events, showing the CPU of the event. If you run <tt>perf sched record -g</tt>, then the stack traces are appended on the right in a single line (not shown here).</p>

<p>The last events in that output include those related to the &#34;sleep 1&#34; command used to time perf. The wakeup happened at 991963.885734, and at 991963.885740 (6 microseconds later) CPU 1 begins to context-switch to the sleep process. The column for that event still shows &#34;:17008[17008]&#34; for what was on-CPU, but the target of the context switch (sleep) is not shown. It is in the raw events:</p>

<pre>  :17008 17008 [001] 991963.885740:       sched:sched_switch: prev_comm=cc1 prev_pid=17008 prev_prio=120
                                                             prev_state=R ==&gt; next_comm=<b>sleep</b> next_pid=16999 next_prio=120
</pre>

<p>The 991963.886005 event shows that the perf command received a wakeup while sleep was running (almost certainly sleep waking up its parent process because it terminated), and then we have the context switch on 991963.886009 where sleep stops running, and a summary is printed out: 1000.104 ms waiting (the &#34;sleep 1&#34;), with 0.006 ms scheduler latency, and 0.269 ms of CPU runtime.</p>

<p>Here I&#39;ve decorated the timehist output with the details of the context switch destination in red:</p>

<pre>  991963.885734 [0001]             :17008[17008]                                        awakened: sleep[16999]
  991963.885740 [0001]   s         :17008[17008]          25.613      0.000      0.057  <SPAN color="#a00000">next: sleep[16999]</SPAN>
  991963.886005 [0001]             sleep[16999]                                         awakened: perf[16984]
  991963.886009 [0001]   s         sleep[16999]         1000.104      0.006      0.269  <SPAN color="#a00000">next: cc1[17008]</SPAN>
  991963.886018 [0005]       s     cc1[17083]             19.998      0.000      9.948  <SPAN color="#a00000">next: perf[16984]</SPAN>
</pre>

<p>When sleep finished, a waiting &#34;cc1&#34; process then executed. perf ran on the following context switch, and is the last event in the profile (perf terminated). I&#39;ve added this as a -n/--next option to perf (should arrive in Linux 4.11 or 4.12).</p>

<p><b>perf sched script</b> dumps all events (similar to <tt>perf script</tt>):</p>

<pre># <b>perf sched script</b>
<!--
    perf 16984 [005] 991962.879960: sched:sched_stat_runtime: comm=perf pid=16984 runtime=3901506 [ns] vruntime=1652494271 [ns]
    perf 16984 [005] 991962.879966:       sched:sched_wakeup: comm=perf pid=16999 prio=120 target_cpu=005
    perf 16984 [005] 991962.879971:       sched:sched_switch: prev_comm=perf prev_pid=16984 prev_prio=120 prev_state=R ==> next_comm=perf next_pid=16999 next
    perf 16999 [005] 991962.880058: sched:sched_stat_runtime: comm=perf pid=16999 runtime=98309 [ns] vruntime=1640592580 [ns]
     cc1 16881 [000] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16881 runtime=3999231 [ns] vruntime=78979518773 [ns]
  :17024 17024 [004] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=17024 runtime=3866637 [ns] vruntime=78103071578 [ns]
     cc1 16900 [001] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16900 runtime=3006028 [ns] vruntime=77728971017 [ns]
     cc1 16825 [006] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16825 runtime=3999423 [ns] vruntime=78764960571 [ns]
     cc1 16880 [002] 991962.880059: sched:sched_stat_runtime: comm=cc1 pid=16880 runtime=3999502 [ns] vruntime=78594775533 [ns]
     cc1 16945 [003] 991962.880059: sched:sched_stat_runtime: comm=cc1 pid=16945 runtime=3008672 [ns] vruntime=83611052305 [ns] -->
    perf 16984 [005] 991962.879960: sched:sched_stat_runtime: comm=perf pid=16984 runtime=3901506 [ns] vruntime=165...
    perf 16984 [005] 991962.879966:       sched:sched_wakeup: comm=perf pid=16999 prio=120 target_cpu=005
    perf 16984 [005] 991962.879971:       sched:sched_switch: prev_comm=perf prev_pid=16984 prev_prio=120 prev_stat...
    perf 16999 [005] 991962.880058: sched:sched_stat_runtime: comm=perf pid=16999 runtime=98309 [ns] vruntime=16405...
     cc1 16881 [000] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16881 runtime=3999231 [ns] vruntime=7897...
  :17024 17024 [004] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=17024 runtime=3866637 [ns] vruntime=7810...
     cc1 16900 [001] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16900 runtime=3006028 [ns] vruntime=7772...
     cc1 16825 [006] 991962.880058: sched:sched_stat_runtime: comm=cc1 pid=16825 runtime=3999423 [ns] vruntime=7876...
</pre>

<p>Each of these events (&#34;sched:sched_stat_runtime&#34; etc) are tracepoints you can instrument directly using perf record.</p>

<p>As I&#39;ve shown earlier, this raw output can be useful for digging further than the summary commands.</p>

<p><b>perf sched replay</b> will take the recorded scheduler events, and then simulate the workload by spawning threads with similar runtimes and context switches. Useful for testing and developing scheduler changes and configuration. Don&#39;t put too much faith in this (and other) workload replayers: they can be a useful load generator, but it&#39;s difficult to simulate the real workload completely. Here I&#39;m running replay with <tt>-r -1</tt>, to repeat the workload:</p>

<pre># <b>perf sched replay -r -1</b>
run measurement overhead: 84 nsecs
sleep measurement overhead: 146710 nsecs
the run test took 1000005 nsecs
the sleep test took 1107773 nsecs
nr_run_events:        4175
nr_sleep_events:      4710
nr_wakeup_events:     2138
task      0 (             swapper:         0), nr_events: 13
task      1 (             swapper:         1), nr_events: 1
task      2 (             swapper:         2), nr_events: 1
task      3 (            kthreadd:         4), nr_events: 1
task      4 (            kthreadd:         6), nr_events: 29
[...]
task    530 (                  sh:     17145), nr_events: 4
task    531 (                  sh:     17146), nr_events: 7
task    532 (                  sh:     17147), nr_events: 4
task    533 (                make:     17148), nr_events: 10
task    534 (                  sh:     17149), nr_events: 1
------------------------------------------------------------
#1  : 965.996, ravg: 966.00, cpu: 798.24 / 798.24
#2  : 902.647, ravg: 966.00, cpu: 1157.53 / 798.24
#3  : 945.482, ravg: 966.00, cpu: 925.25 / 798.24
#4  : 943.541, ravg: 966.00, cpu: 761.72 / 798.24
#5  : 914.643, ravg: 966.00, cpu: 1604.32 / 798.24
<!--
#6  : 916.842, ravg: 966.00, cpu: 885.45 / 798.24
#7  : 923.348, ravg: 966.00, cpu: 812.36 / 798.24
#8  : 895.975, ravg: 966.00, cpu: 699.68 / 798.24
#9  : 932.765, ravg: 966.00, cpu: 1240.33 / 798.24
#10 : 945.305, ravg: 966.00, cpu: 885.12 / 798.24
-->[...]
</pre>

<h2>6.8. eBPF</h2>

<p>As of Linux 4.4, perf has some enhanced BPF support (aka eBPF or just &#34;BPF&#34;), with more in later kernels. BPF makes perf tracing programmatic, and takes perf from being a counting &amp; sampling-with-post-processing tracer, to a fully in-kernel programmable tracer.</p>

<p>eBPF is currently a little restricted and difficult to use from perf. It&#39;s getting better all the time. A different and currently easier way to access eBPF is via the bcc Python interface, which is described on my <a href="https://www.brendangregg.com/ebpf.html">eBPF Tools</a> page. On this page, I&#39;ll discuss perf.</p>

<h3>Prerequisites</h3>

<p>Linux 4.4 at least. Newer versions have more perf/BPF features, so the newer the better. Also clang (eg, <tt>apt-get install clang</tt>).</p>

<h3>kmem_cache_alloc from Example</h3>

<p>This program traces the kernel kmem_cache_alloc() function, only if its calling function matches a specified range, filtered in kernel context. You can imagine doing this for efficiency: instead of tracing all allocations, which can be very frequent and add significant overhead, you filter for just a range of kernel calling functions of interest, such as a kernel module. I&#39;ll loosely match tcp functions as an example, which are in memory at these addresses:</p>

<pre># <b>grep tcp /proc/kallsyms | more</b>
[...]
ffffffff817c1bb0 t tcp_get_info_chrono_stats
ffffffff817c1c60 T tcp_init_sock
ffffffff817c1e30 t tcp_splice_data_recv
ffffffff817c1e70 t tcp_push
ffffffff817c20a0 t tcp_send_mss
ffffffff817c2170 t tcp_recv_skb
ffffffff817c2250 t tcp_cleanup_rbuf
[...]
ffffffff818524f0 T tcp6_proc_exit
ffffffff81852510 T tcpv6_exit
ffffffff818648a0 t tcp6_gro_complete
ffffffff81864910 t tcp6_gro_receive
ffffffff81864ae0 t tcp6_gso_segment
ffffffff8187bd89 t tcp_v4_inbound_md5_hash
</pre>

<p>I&#39;ll assume these functions are contiguous, so that by tracing the range 0xffffffff817c1bb0 to 0xffffffff8187bd89, I&#39;m matching much of tcp.</p>

<p>Here is my BPF program, kca_from.c:</p>

<pre>#include &lt;uapi/linux/bpf.h&gt;
#include &lt;uapi/linux/ptrace.h&gt;

#define SEC(NAME) __attribute__((section(NAME), used))

/*
 * Edit the following to match the instruction address range you want to
 * sample. Eg, look in /proc/kallsyms. The addresses will change for each
 * kernel version and build.
 */
#define RANGE_START  0xffffffff817c1bb0
#define RANGE_END    0xffffffff8187bd89

struct bpf_map_def {
	unsigned int type;
	unsigned int key_size;
	unsigned int value_size;
	unsigned int max_entries;
};

static int (*probe_read)(void *dst, int size, void *src) =
    (void *)BPF_FUNC_probe_read;
static int (*get_smp_processor_id)(void) =
    (void *)BPF_FUNC_get_smp_processor_id;
static int (*perf_event_output)(void *, struct bpf_map_def *, int, void *,
    unsigned long) = (void *)BPF_FUNC_perf_event_output;

struct bpf_map_def SEC(&#34;maps&#34;) channel = {
	.type = BPF_MAP_TYPE_PERF_EVENT_ARRAY,
	.key_size = sizeof(int),
	.value_size = sizeof(u32),
	.max_entries = __NR_CPUS__,
};

SEC(&#34;func=kmem_cache_alloc&#34;)
int func(struct pt_regs *ctx)
{
	u64 ret = 0;
	// x86_64 specific:
	probe_read(&amp;ret, sizeof(ret), (void *)(ctx-&gt;bp+8));
	if (ret &gt;= RANGE_START &amp;&amp; ret &lt; RANGE_END) {
		perf_event_output(ctx, &amp;channel, get_smp_processor_id(), 
		    &amp;ret, sizeof(ret));
	}
	return 0;
}

char _license[] SEC(&#34;license&#34;) = &#34;GPL&#34;;
int _version SEC(&#34;version&#34;) = LINUX_VERSION_CODE;
</pre>

<p>Now I&#39;ll execute it, then dump the events:</p>

<pre># <b>perf record -e bpf-output/no-inherit,name=evt/ -e ./kca_from.c/map:channel.event=evt/ -a -- sleep 1</b>
bpf: builtin compilation failed: -95, try external compiler
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.214 MB perf.data (3 samples) ]

# <b>perf script</b>
 testserver00001 14337 [003] 481432.395181:          0     evt:  ffffffff81210f51 kmem_cache_alloc (/lib/modules/...)
      BPF output: 0000: 0f b4 7c 81 ff ff ff ff  ..|.....
                  0008: 00 00 00 00              ....    

    redis-server  1871 [005] 481432.395258:          0     evt:  ffffffff81210f51 kmem_cache_alloc (/lib/modules/...)
      BPF output: 0000: 14 55 7c 81 ff ff ff ff  .U|.....
                  0008: 00 00 00 00              ....    

    redis-server  1871 [005] 481432.395456:          0     evt:  ffffffff81210f51 kmem_cache_alloc (/lib/modules/...)
      BPF output: 0000: fe dc 7d 81 ff ff ff ff  ..}.....
                  0008: 00 00 00 00              ....    
</pre>

<p>It worked: the &#34;BPF output&#34; records contain addresses in our range: 0xffffffff817cb40f, and so on. kmem_cache_alloc() is a frequently called function, so that it only matched a few entries in one second of tracing is an indication it is working (I can also relax that range to confirm it).</p>

<p>Adding stack traces with -g:</p>

<pre># <b>perf record -e bpf-output/no-inherit,name=evt/ -e ./kca_from.c/map:channel.event=evt/ -a -g -- sleep 1</b>
bpf: builtin compilation failed: -95, try external compiler
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.215 MB perf.data (3 samples) ]

# <b>perf script</b>
testserver00001 16744 [002] 481518.262579:          0                 evt: 
                  410f51 kmem_cache_alloc (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9cb40f tcp_conn_request (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9da243 tcp_v4_conn_request (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9d0936 tcp_rcv_state_process (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9db102 tcp_v4_do_rcv (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9dcabf tcp_v4_rcv (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b4af4 ip_local_deliver_finish (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b4dff ip_local_deliver (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b477b ip_rcv_finish (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b50fb ip_rcv (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  97119e __netif_receive_skb_core (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  971708 __netif_receive_skb (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9725df process_backlog (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  971c8e net_rx_action (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  a8e58d __do_softirq (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  a8c9ac do_softirq_own_stack (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  28a061 do_softirq.part.18 (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  28a0ed __local_bh_enable_ip (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b8ff3 ip_finish_output2 (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b9f43 ip_finish_output (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9ba9f6 ip_output (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9ba155 ip_local_out (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9ba48a ip_queue_xmit (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9d3823 tcp_transmit_skb (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9d5345 tcp_connect (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9da764 tcp_v4_connect (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9f1abc __inet_stream_connect (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9f1d38 inet_stream_connect (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  952fd9 SYSC_connect (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  953c1e sys_connect (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  a8b9fb entry_SYSCALL_64_fastpath (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                   10800 __GI___libc_connect (/lib/x86_64-linux-gnu/libpthread-2.23.so)

      BPF output: 0000: 0f b4 7c 81 ff ff ff ff  ..|.....
                  0008: 00 00 00 00              ....    

redis-server  1871 [003] 481518.262670:          0                 evt: 
                  410f51 kmem_cache_alloc (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9c5514 tcp_poll (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9515ba sock_poll (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  485699 sys_epoll_ctl (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  a8b9fb entry_SYSCALL_64_fastpath (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  106dca epoll_ctl (/lib/x86_64-linux-gnu/libc-2.23.so)

      BPF output: 0000: 14 55 7c 81 ff ff ff ff  .U|.....
                  0008: 00 00 00 00              ....    

redis-server  1871 [003] 481518.262870:          0                 evt: 
                  410f51 kmem_cache_alloc (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9ddcfe tcp_time_wait (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9cefff tcp_fin (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9cf630 tcp_data_queue (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9d0abd tcp_rcv_state_process (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9db102 tcp_v4_do_rcv (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9dca8b tcp_v4_rcv (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b4af4 ip_local_deliver_finish (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b4dff ip_local_deliver (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b477b ip_rcv_finish (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b50fb ip_rcv (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  97119e __netif_receive_skb_core (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  971708 __netif_receive_skb (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9725df process_backlog (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  971c8e net_rx_action (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  a8e58d __do_softirq (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  a8c9ac do_softirq_own_stack (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  28a061 do_softirq.part.18 (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  28a0ed __local_bh_enable_ip (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b8ff3 ip_finish_output2 (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9b9f43 ip_finish_output (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9ba9f6 ip_output (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9ba155 ip_local_out (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9ba48a ip_queue_xmit (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9d3823 tcp_transmit_skb (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9d3e24 tcp_write_xmit (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9d4c31 __tcp_push_pending_frames (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9d6881 tcp_send_fin (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9c70b7 tcp_close (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  9f161c inet_release (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  95181f sock_release (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  951892 sock_close (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  43b2f7 __fput (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  43b46e ____fput (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  2a3cfe task_work_run (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  2032ba exit_to_usermode_loop (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  203b29 syscall_return_slowpath (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                  a8ba88 entry_SYSCALL_64_fastpath (/lib/modules/4.10.0-rc8-virtual/build/vmlinux)
                   105cd __GI___libc_close (/lib/x86_64-linux-gnu/libpthread-2.23.so)

      BPF output: 0000: fe dc 7d 81 ff ff ff ff  ..}.....
                  0008: 00 00 00 00              ....    
</pre>

<p>This confirms the parent functions that were matched by the range.</p>

<h3>More Examples</h3>

<p>XXX fill me in.</p>



<p>perf_events has a builtin visualization: timecharts, as well as text-style visualization via its text user interface (TUI) and tree reports. The following two sections show visualizations of my own: flame graphs and heat maps. The software I&#39;m using is open source and on github, and produces these from perf_events collected data.</p>

<h2>7.1. Flame Graphs</h2>

<p><a href="https://www.brendangregg.com/flamegraphs.html">Flame Graphs</a> can be produced from perf_events profiling data using the <a href="https://github.com/brendangregg/FlameGraph">FlameGraph tools</a> software. This visualizes the same data you see in <tt>perf report</tt>, and works with any perf.data file that was captured with stack traces (-g).</p>

<h3>Example</h3>

<p>This example CPU flame graph shows a network workload for the 3.2.9-1 Linux kernel, running as a KVM instance (<a href="https://www.brendangregg.com/FlameGraphs/cpu-linux-tcpsend.svg">SVG</a>, <a href="https://www.brendangregg.com/FlameGraphs/cpu-linux-tcpsend.png">PNG</a>):</p>



<p>Flame Graphs show the sample population across the x-axis, and stack depth on the y-axis.  Each function (stack frame) is drawn as a rectangle, with the width relative to the number of samples.  See the <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs">CPU Flame Graphs</a> page for the full description of how these work.</p>

<p>You can use the mouse to explore where kernel CPU time is spent, quickly quantifying code-paths and determining where performance tuning efforts are best spent.  This example shows that most time was spent in the vp_notify() code-path, spending 70.52% of all on-CPU samples performing iowrite16(), which is handled by the KVM hypervisor. This information has been extremely useful for directing KVM performance efforts.</p>

<p>A similar network workload on a bare metal Linux system looks quite different, as networking isn&#39;t processed via the virtio-net driver, for a start.</p>

<h3>Generation</h3>

<p>The example flame graph was generated using perf_events and the <a href="https://github.com/brendangregg/FlameGraph">FlameGraph tools</a> (this is the old-fashioned method for Linux 2.6.X onwards; see the later Newer perf section):</p>

<pre># <b>git clone https://github.com/brendangregg/FlameGraph</b>  # or download it from github
# <b>cd FlameGraph</b>
# <b>perf record -F 99 -ag -- sleep 60</b>
# <b>perf script | ./stackcollapse-perf.pl &gt; out.perf-folded</b>
# <b>cat out.perf-folded | ./flamegraph.pl &gt; perf-kernel.svg</b>
</pre>

<p>The first perf command profiles CPU stacks, as explained earlier. I adjusted the rate to 99 Hertz here; I actually generated the flame graph from a 1000 Hertz profile, but I&#39;d only use that if you had a reason to go faster, which costs more in overhead. The samples are saved in a perf.data file, which can be viewed using <tt>perf report</tt>:</p>

<pre># <b>perf report --stdio</b>
[...]
# Overhead          Command          Shared Object                               Symbol
# ........  ...............  .....................  ...................................
#
    72.18%            iperf  [kernel.kallsyms]      [k] iowrite16
                      |
                      --- iowrite16
                         |          
                         |--99.53%-- vp_notify
                         |          virtqueue_kick
                         |          start_xmit
                         |          dev_hard_start_xmit
                         |          sch_direct_xmit
                         |          dev_queue_xmit
                         |          ip_finish_output
                         |          ip_output
                         |          ip_local_out
                         |          ip_queue_xmit
                         |          tcp_transmit_skb
                         |          tcp_write_xmit
                         |          |          
                         |          |--98.16%-- tcp_push_one
                         |          |          tcp_sendmsg
                         |          |          inet_sendmsg
                         |          |          sock_aio_write
                         |          |          do_sync_write
                         |          |          vfs_write
                         |          |          sys_write
                         |          |          system_call
                         |          |          0x369e40e5cd
                         |          |          
                         |           --1.84%-- __tcp_push_pending_frames
[...]
</pre>

<p>This tree follows the flame graph when reading it top-down.  When using -g/--call-graph (for &#34;caller&#34;, instead of the &#34;callee&#34; default), it generates a tree that follows the flame graph when read bottom-up.  The hottest stack trace in the flame graph (@70.52%) can be seen in this perf call graph as the product of the top three nodes (72.18% x 99.53% x 98.16%).</p>

<p>The perf report tree (and the ncurses navigator) do an excellent job at presenting this information as text.  However, with text there are limitations.  The output often does not fit in one screen (you could say it doesn&#39;t need to, if the bulk of the samples are identified on the first page).  Also, identifying the hottest code paths requires reading the percentages.  With the flame graph, all the data is on screen at once, and the hottest code-paths are immediately obvious as the widest functions.</p>

<p>For generating the flame graph, the <tt>perf script</tt> command dumps the stack samples, which are then aggregated by stackcollapse-perf.pl and folded into single lines per-stack.  That output is then converted by flamegraph.pl into the SVG.  I included a gratuitous &#34;cat&#34; command to make it clear that flamegraph.pl can process the output of a pipe, which could include Unix commands to filter or preprocess (grep, sed, awk).</p>

<h3>Piping</h3>

<p>A flame graph can be generated directly by piping all the steps:</p>

<pre># <b>perf script | ./stackcollapse-perf.pl | ./flamegraph.pl &gt; perf-kernel.svg</b>
</pre>

<p>In practice I don&#39;t do this, as I often re-run flamegraph.pl multiple times, and this one-liner would execute everything multiple times.  The output of <tt>perf script</tt> can be dozens of Mbytes, taking many seconds to process. By writing stackcollapse-perf.pl to a file, you&#39;ve cached the slowest step, and can also edit the file (vi) to delete unimportant stacks, such as CPU idle threads.</p>

<h3>Filtering</h3>

<p>The one-line-per-stack output of stackcollapse-perf.pl is also convenient for grep(1). E.g.:</p>

<pre># perf script | ./stackcollapse-perf.pl &gt; out.perf-folded

# <b>grep -v cpu_idle</b> out.perf-folded | ./flamegraph.pl &gt; nonidle.svg

# <b>grep ext4</b> out.perf-folded | ./flamegraph.pl &gt; ext4internals.svg

# <b>egrep &#39;system_call.*sys_(read|write)&#39;</b> out.perf-folded | ./flamegraph.pl &gt; rw.svg
</pre>

<p>I frequently elide the cpu_idle threads in this way, to focus on the real threads that are consuming CPU resources. If I miss this step, the cpu_idle threads can often dominate the flame graph, squeezing the interesting code paths.</p>

<p>Note that it would be a little more efficient to process the output of <tt>perf report</tt> instead of <tt>perf script</tt>; better still, <tt>perf report</tt> could have a report style (eg, &#34;-g folded&#34;) that output folded stacks directly, obviating the need for stackcollapse-perf.pl. There could even be a perf mode that output the SVG directly (which wouldn&#39;t be the first one; see perf-timechart), although, that would miss the value of being able to grep the folded stacks (which I use frequently).</p>

<p>There are more examples of perf_events CPU flame graphs on the <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#Examples">CPU flame graph</a> page, including a <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#perf">summary</a> of these instructions. I have also shared an example of using perf for a <a href="https://www.brendangregg.com/FlameGraphs/offcpuflamegraphs.html#BlockIO">Block Device I/O Flame Graph</a>.</p>

<h3>Newer perf</h3>

<p>perf has added features to aid flame graph generation. XXX
</p>

<h2>7.2. Heat Maps</h2>

<p>Since perf_events can record high resolution timestamps (microseconds) for events, some latency measurements can be derived from trace data.</p>

<h3>Example</h3>

<p>The following heat map visualizes disk I/O latency data collected from perf_events (<a href="https://www.brendangregg.com/perf_events/perf_block_latencyheatmap.svg">SVG</a>, <a href="https://www.brendangregg.com/perf_events/perf_block_latencyheatmap.png">PNG</a>):</p>



<p>Mouse-over blocks to explore the latency distribution over time. The x-axis is the passage of time, the y-axis latency, and the z-axis (color) is the number of I/O at that time and latency range. The distribution is bimodal, with the dark line at the bottom showing that many disk I/O completed with sub-millisecond latency: cache hits. There is a cloud of disk I/O from about 3 ms to 25 ms, which would be caused by random disk I/O (and queueing). Both these modes averaged to the 9 ms we saw earlier.</p>

<p>The following iostat output was collected at the same time as the heat map data was collected (shows a typical one second summary):</p>

<pre># <b>iostat -x 1</b>
[...]
Device: rrqm/s wrqm/s    r/s   w/s   rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm  %util
vda       0.00   0.00   0.00  0.00    0.00  0.00     0.00     0.00  0.00    0.00    0.00  0.00   0.00
vdb       0.00   0.00 334.00  0.00 2672.00  0.00    16.00     2.97  9.01    9.01    0.00  2.99 100.00
</pre>

<p>This workload has an average I/O time (await) of 9 milliseconds, which sounds like a fairly random workload on 7200 RPM disks. The problem is that we don&#39;t know the distribution from the iostat output, or any similar latency average. There could be latency outliers present, which is not visible in the average, and yet are causing problems. The heat map did show I/O up to 50 ms, which you might not have expected from that iostat output. There could also be multiple modes, as we saw in the heat map, which are also not visible in an average.</p>

<h3>Gathering</h3>

<p>I used perf_events to record the block request (disk I/O) issue and completion static tracepoints:</p>

<pre># <b>perf record -e block:block_rq_issue -e block:block_rq_complete -a sleep 120</b>
[ perf record: Woken up 36 times to write data ]
[ perf record: Captured and wrote 8.885 MB perf.data (~388174 samples) ]
# <b>perf script</b>
[...]
     randread.pl  2522 [000]  6011.824759: block:block_rq_issue: 254,16 R 0 () 7322849 + 16 [randread.pl]
     randread.pl  2520 [000]  6011.824866: block:block_rq_issue: 254,16 R 0 () 26144801 + 16 [randread.pl]
         swapper     0 [000]  6011.828913: block:block_rq_complete: 254,16 R () 31262577 + 16 [0]
     randread.pl  2521 [000]  6011.828970: block:block_rq_issue: 254,16 R 0 () 70295937 + 16 [randread.pl]
         swapper     0 [000]  6011.835862: block:block_rq_complete: 254,16 R () 26144801 + 16 [0]
     randread.pl  2520 [000]  6011.835932: block:block_rq_issue: 254,16 R 0 () 5495681 + 16 [randread.pl]
         swapper     0 [000]  6011.837988: block:block_rq_complete: 254,16 R () 7322849 + 16 [0]
     randread.pl  2522 [000]  6011.838051: block:block_rq_issue: 254,16 R 0 () 108589633 + 16 [randread.pl]
         swapper     0 [000]  6011.850615: block:block_rq_complete: 254,16 R () 108589633 + 16 [0]
[...]
</pre>

<p>The full output from <tt>perf script</tt> is about 70,000 lines. I&#39;ve included some here so that you can see the kind of data available.</p>

<h3>Processing</h3>

<p>To calculate latency for each I/O, I&#39;ll need to pair up the issue and completion events, so that I can calculate the timestamp delta. The columns look straightforward (and are in include/trace/events/block.h), with the 4th field the timestamp in seconds (with microsecond resolution), the 6th field the disk device ID (major, minor), and a later field (which varies based on the tracepoint) has the disk offset. I&#39;ll use the disk device ID and offset as the unique identifier, assuming the kernel will not issue concurrent I/O to the exact same location.</p>

<p>I&#39;ll use awk to do these calculations and print the completion times and latency:</p>

<pre># <b>perf script | awk &#39;{ gsub(/:/, &#34;&#34;) } $5 ~ /issue/ { ts[$6, $10] = $4 }
    $5 ~ /complete/ { if (l = ts[$6, $9]) { printf &#34;%.f %.f\n&#34;, $4 * 1000000,
    ($4 - l) * 1000000; ts[$6, $10] = 0 } }&#39; &gt; out.lat_us</b>
# <b>more out.lat_us</b>
6011793689 8437
6011797306 3488
6011798851 1283
6011806422 11248
6011824680 18210
6011824693 21908
[...]
</pre>

<p>I converted both columns to be microseconds, to make the next step easier.</p>

<h3>Generation</h3>

<p>Now I can use my trace2heatmap.pl program (<a href="https://github.com/brendangregg/HeatMap">github</a>), to generate the interactive SVG heatmap from the trace data (and uses microseconds by default):</p>

<pre># <b>./trace2heatmap.pl --unitstime=us --unitslat=us --maxlat=50000 out.lat_us &gt; out.svg</b>
</pre>

<p>When I generated the heatmap, I truncated the y scale to 50 ms. You can adjust it to suit your investigation, increasing it to see more of the latency outliers, or decreasing it to reveal more resolution for the lower latencies: for example, with a <a href="https://www.brendangregg.com/perf_events/perf_block_latencyheatmap2.svg">250 us limit</a>.</p>

<h3>Overheads</h3>

<p>While this can be useful to do, be mindful of overheads. In my case, I had a low rate of disk I/O (~300 IOPS), which generated an 8 Mbyte trace file after 2 minutes. If your disk IOPS were 100x that, your trace file will also be 100x, and the overheads for gathering and processing will add up.</p>

<p>For more about latency heatmaps, see my <a href="http://www.slideshare.net/brendangregg/lisa2010-visualizations">LISA 2010</a> presentation slides, and my <a href="http://cacm.acm.org/magazines/2010/7/95062-visualizing-system-latency/fulltext">CACM 2010</a> article, both about heat maps. Also see my <a href="https://www.brendangregg.com/blog/2014-07-01/perf-heat-maps.html">Perf Heat Maps</a> blog post.</p>



<p>Notes on specific targets.</p>

<p>Under construction.</p>

<h2>8.1. Java</h2>

<h2>8.2. Node.js</h2>

<ul>
<li>Node.js V8 JIT internals with annotation support https://twitter.com/brendangregg/status/755838455549001728</li>
</ul>



<p>There&#39;s more capabilities to perf_events than I&#39;ve demonstrated here. I&#39;ll add examples of the other subcommands when I get a chance.</p>

<p>Here&#39;s a preview of <tt>perf trace</tt>, which was added in <a href="http://kernelnewbies.org/Linux_3.7">3.7</a>, demonstrated on 3.13.1:</p>

<pre># <b>perf trace ls</b>
     0.109 ( 0.000 ms):  ... [continued]: read()) = 1
     0.430 ( 0.000 ms):  ... [continued]: execve()) = -2
     0.565 ( 0.051 ms): execve(arg0: 140734989338352, arg1: 140734989358048, arg2: 40612288, arg3: 1407...
     0.697 ( 0.051 ms): execve(arg0: 140734989338353, arg1: 140734989358048, arg2: 40612288, arg3: 1407...
     0.797 ( 0.046 ms): execve(arg0: 140734989338358, arg1: 140734989358048, arg2: 40612288, arg3: 1407...
     0.915 ( 0.045 ms): execve(arg0: 140734989338359, arg1: 140734989358048, arg2: 40612288, arg3: 1407...
     1.030 ( 0.044 ms): execve(arg0: 140734989338362, arg1: 140734989358048, arg2: 40612288, arg3: 1407...
     1.414 ( 0.311 ms): execve(arg0: 140734989338363, arg1: 140734989358048, arg2: 40612288, arg3: 1407...
     2.156 ( 1.053 ms):  ... [continued]: brk()) = 0xac9000
     2.319 ( 1.215 ms):  ... [continued]: access()) = -1 ENOENT No such file or directory
     2.479 ( 1.376 ms):  ... [continued]: mmap()) = 0xb3a84000
     2.634 ( 0.052 ms): access(arg0: 139967406289504, arg1: 4, arg2: 139967408408688, arg3: 13996740839...
     2.787 ( 0.205 ms):  ... [continued]: open()) = 3
     2.919 ( 0.337 ms):  ... [continued]: fstat()) = 0
     3.049 ( 0.057 ms): mmap(arg0: 0, arg1: 22200, arg2: 1, arg3: 2, arg4: 3, arg5: 0         ) = 0xb3a...
     3.177 ( 0.184 ms):  ... [continued]: close()) = 0
     3.298 ( 0.043 ms): access(arg0: 139967406278152, arg1: 0, arg2: 6, arg3: 7146772199173811245, arg4...
     3.432 ( 0.049 ms): open(arg0: 139967408376811, arg1: 524288, arg2: 0, arg3: 139967408376810, arg4:...
     3.560 ( 0.045 ms): read(arg0: 3, arg1: 140737350651528, arg2: 832, arg3: 139967408376810, arg4: 14...
     3.684 ( 0.042 ms): fstat(arg0: 3, arg1: 140737350651216, arg2: 140737350651216, arg3: 354389249727...
     3.814 ( 0.054 ms): mmap(arg0: 0, arg1: 2221680, arg2: 5, arg3: 2050, arg4: 3, arg5: 0    ) = 0xb36...
[...]
</pre>

<p>An advantage is that this is buffered tracing, which costs much less overhead than strace, as I described <a href="#perf_vs_strace">earlier</a>. The <tt>perf trace</tt> output seen from this 3.13.1 kernel does, however, looks suspicious for a number of reasons. I think this is still an in-development feature. It reminds me of my <a href="http://www.brendangregg.com/dtrace.html#dtruss">dtruss</a> tool, which has a similar role, before I added code to print each system call in a custom and appropriate way.</p>



<p>The steps to build perf_events depends on your kernel version and Linux distribution. In summary:</p>

<ol>
<li>Get the Linux kernel source that matches your currently running kernel (eg, from the linux-source package, or <a href="http://kernel.org">kernel.org</a>).</li>
<li>Unpack the kernel source.</li>
<li><tt>cd tools/perf</tt></li>
<li><tt>make</tt></li>
<li>Fix all errors, and most warnings, from (4).</li>
</ol>

<p>The first error may be that you are missing make, or a compiler (gcc). Once you have those, you may then see various warnings about missing libraries, which disable perf features. I&#39;d install as many as possible, and take note of the ones you are missing.</p>

<p>These perf build warnings are <i>really helpful</i>, and are generated by its Makefile. Here&#39;s the makefile from 3.9.3:</p>

<pre># <b>grep found Makefile</b>
msg := $(warning No libelf found, disables &#39;probe&#39; tool, please install elfutils-libelf-devel/libelf-dev);
msg := $(error No gnu/libc-version.h found, please install glibc-dev[el]/glibc-static);
msg := $(warning No libdw.h found or old libdw.h found or elfutils is older than 0.138, disables dwarf support.
 Please install new elfutils-devel/libdw-dev);
msg := $(warning No libunwind found, disabling post unwind support. Please install libunwind-dev[el] &gt;= 0.99);
msg := $(warning No libaudit.h found, disables &#39;trace&#39; tool, please install audit-libs-devel or libaudit-dev);
msg := $(warning newt not found, disables TUI support. Please install newt-devel or libnewt-dev);
msg := $(warning GTK2 not found, disables GTK2 support. Please install gtk2-devel or libgtk2.0-dev);
$(if $(1),$(warning No $(1) was found))
msg := $(warning No bfd.h/libbfd found, install binutils-dev[el]/zlib-static to gain symbol demangling)
msg := $(warning No numa.h found, disables &#39;perf bench numa mem&#39; benchmark, please install numa-libs-devel or
 libnuma-dev);
</pre>

<p>Take the time to read them. This list is likely to grow as new features are added to perf_events.</p>

<p>The following notes show what I&#39;ve specifically done for kernel versions and distributions, in case it is helpful.</p> 

<h3>Packages: Ubuntu, 3.8.6</h3>

<p>Packages required for key functionality: gcc make bison flex elfutils libelf-dev libdw-dev libaudit-dev. You may also consider python-dev (for python scripting) and binutils-dev (for symbol demangling), which are larger packages.</p>

<h3>Kernel Config: 3.8.6</h3>

<p>Here are some kernel CONFIG options for perf_events functionality:</p>

<pre># for perf_events:
CONFIG_PERF_EVENTS=y
# for stack traces:
CONFIG_FRAME_POINTER=y
# kernel symbols:
CONFIG_KALLSYMS=y
# tracepoints:
CONFIG_TRACEPOINTS=y
# kernel function trace:
CONFIG_FTRACE=y
# kernel-level dynamic tracing:
CONFIG_KPROBES=y
CONFIG_KPROBE_EVENTS=y
# user-level dynamic tracing:
CONFIG_UPROBES=y
CONFIG_UPROBE_EVENTS=y
# full kernel debug info:
CONFIG_DEBUG_INFO=y
# kernel lock tracing:
CONFIG_LOCKDEP=y
# kernel lock tracing:
CONFIG_LOCK_STAT=y
# kernel dynamic tracepoint variables:
CONFIG_DEBUG_INFO=y
</pre>

<p>You may need to build your own kernel to enable these. The exact set you need depends on your needs and kernel version, and list is likely to grow as new features are added to perf_events.</p>

<h2>10.1. Static Builds</h2>

<p>I&#39;ve sometimes done this so that I have a single perf binary that can be copied into Docker containers for execution. Steps, given the Linux source:</p>

<pre>cd tools/perf
vi Makefile.perf
  LDFLAGS=-static
make clean; make
</pre>



<p>If you see hexadecimal numbers instead of symbols, or have truncated stack traces, see the <a href="#Prerequisites">Prerequisites</a> section.</p>

<p>Here are some rough notes from other issues I&#39;ve encountered.</p>

<p>This sometimes works (3.5.7.2) and sometimes throws the following error (3.9.3):</p>
<pre>ubuntu# <b>perf stat -e &#39;syscalls:sys_enter_*&#39; -a sleep 5</b>
Error:
Too many events are opened.
Try again after reducing the number of events.
</pre>

<p>This can be fixed by increasing the file descriptor limit using ulimit -n.</p>

<p>Type 3 errors:</p>
<pre>ubuntu# <b>perf report</b>
0xab7e48 [0x30]: failed to process type: 3
# ========
# captured on: Tue Jan 28 21:08:31 2014
# hostname : pgbackup
# os release : 3.9.3-ubuntu-12-opt
# perf version : 3.9.3
# arch : x86_64
# nrcpus online : 8
# nrcpus avail : 8
# cpudesc : Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
# cpuid : GenuineIntel,6,45,7
# total memory : 8179104 kB
# cmdline : /lib/modules/3.9.3-ubuntu-12-opt/build/tools/perf/perf record
 -e sched:sched_process_exec -a 
# event : name = sched:sched_process_exec, type = 2, config = 0x125, config1 = 0x0,
 config2 = 0x0, excl_usr = 0, excl_kern = 0, excl_host = 0, excl_guest = 1, precise_ip = 0
# HEADER_CPU_TOPOLOGY info available, use -I to display
# HEADER_NUMA_TOPOLOGY info available, use -I to display
# pmu mappings: software = 1, tracepoint = 2, breakpoint = 5
# ========
#
Warning: Timestamp below last timeslice flush
</pre>



<p>perf_events has the capabilities from many other tools rolled into one: strace(1), for tracing system calls, tcpdump(8), for tracing network packets, and blktrace(1), for tracing block device I/O (disk I/O), and other targets including file system and scheduler events. Tracing all events from one tool is not only convenient, it also allows direct correlations, including timestamps, between different instrumentation sources. Unlike these other tools, some assembly is required, which may not be for everyone (as explained in <a href="#Audience">Audience</a>).</p>



<p>Resources for further study.</p>

<h2>13.1. Posts</h2>

<p>I&#39;ve been writing blog posts on specific perf_events topics. My suggested reading order is from oldest to newest (top down):</p>

<ul>
<li>22 Jun 2014: <a href="https://www.brendangregg.com/blog/2014-06-22/perf-cpu-sample.html">perf CPU Sampling</a></li>
<li>29 Jun 2014: <a href="https://www.brendangregg.com/blog/2014-06-29/perf-static-tracepoints.html">perf Static Tracepoints</a></li>
<li>01 Jul 2014: <a href="https://www.brendangregg.com/blog/2014-07-01/perf-heat-maps.html">perf Heat Maps</a></li>
<li>03 Jul 2014: <a href="https://www.brendangregg.com/blog/2014-07-03/perf-counting.html">perf Counting</a></li>
<li>10 Jul 2014: <a href="https://www.brendangregg.com/blog/2014-07-10/perf-hacktogram.html">perf Hacktogram</a></li>
<li>11 Sep 2014: <a href="https://www.brendangregg.com/blog/2014-09-11/perf-kernel-line-tracing.html">Linux perf Rides the Rocket: perf Kernel Line Tracing</a></li>
<li>17 Sep 2014: <a href="https://www.brendangregg.com/blog/2014-09-17/node-flame-graphs-on-linux.html">node.js Flame Graphs on Linux</a></li>
<li>26 Feb 2015: <a href="https://www.brendangregg.com/blog/2015-02-26/linux-perf-off-cpu-flame-graph.html">Linux perf_events Off-CPU Time Flame Graph</a></li>
<li>27 Feb 2015: <a href="https://www.brendangregg.com/blog/2015-02-27/linux-profiling-at-netflix.html">Linux Profiling at Netflix</a></li>
<li>24 Jul 2015: <a href="http://techblog.netflix.com/2015/07/java-in-flames.html">Java Mixed-Mode Flame Graphs</a> (<a href="https://www.brendangregg.com/Articles/Netflix_Java_in_Flames.pdf">PDF</a>)</li>
<li>30 Apr 2016: <a href="https://www.brendangregg.com/blog/2016-04-30/linux-perf-folded.html">Linux 4.5 perf folded format</a></li>
</ul>

<p>And posts on ftrace:</p>

<ul>
<li>13 Jul 2014: <a href="https://www.brendangregg.com/blog/2014-07-13/linux-ftrace-function-counting.html">Linux ftrace Function Counting</a></li>
<li>16 Jul 2014: <a href="https://www.brendangregg.com/blog/2014-07-16/iosnoop-for-linux.html">iosnoop for Linux</a></li>
<li>23 Jul 2014: <a href="https://www.brendangregg.com/blog/2014-07-23/linux-iosnoop-latency-heat-maps.html">Linux iosnoop Latency Heat Maps</a></li>
<li>25 Jul 2014: <a href="https://www.brendangregg.com/blog/2014-07-25/opensnoop-for-linux.html">opensnoop for Linux</a></li>
<li>28 Jul 2014: <a href="https://www.brendangregg.com/blog/2014-07-28/execsnoop-for-linux.html">execsnoop for Linux: See Short-Lived Processes</a></li>
<li>30 Aug 2014: <a href="https://www.brendangregg.com/blog/2014-08-30/ftrace-the-hidden-light-switch.html">ftrace: The Hidden Light Switch</a></li>
<li>06 Sep 2014: <a href="https://www.brendangregg.com/blog/2014-09-06/linux-ftrace-tcp-retransmit-tracing.html">tcpretrans: Tracing TCP retransmits</a></li>
<li>31 Dec 2014: <a href="https://www.brendangregg.com/blog/2014-12-31/linux-page-cache-hit-ratio.html">Linux Page Cache Hit Ratio</a></li>
<li>28 Jun 2015: <a href="https://www.brendangregg.com/blog/2015-06-28/linux-ftrace-uprobe.html">uprobe: User-Level Dynamic Tracing</a></li>
<li>03 Jul 2015: <a href="https://www.brendangregg.com/blog/2015-07-03/hacking-linux-usdt-ftrace.html">Hacking Linux USDT</a></li>
</ul>

<h2>13.2. Links</h2>

<p>perf_events:</p>

<ul>
<li><a href="https://github.com/brendangregg/perf-tools">perf-tools</a> (github), a collection of my performance analysis tools based on Linux perf_events and ftrace.</li>
<li><a href="https://perf.wiki.kernel.org/index.php/Main_Page">perf Main Page</a>.</li>
<li>The excellent <a href="https://perf.wiki.kernel.org/index.php/Tutorial">perf Tutorial</a>, which focuses more on CPU hardware counters.</li>
<li>The <a href="http://web.eece.maine.edu/~vweaver/projects/perf_events/">Unofficial Linux Perf Events Web-Page</a> by Vince Weaver.</li>
<li>The <a href="http://dir.gmane.org/gmane.linux.kernel.perf.user">perf user</a> mailing list.</li>
<li>Mischa Jonker&#39;s presentation <a href="http://events.linuxfoundation.org/sites/events/files/slides/ELCE%20-%20fighting%20latency.pdf">Fighting latency: How to optimize your system using perf</a> (PDF) (2013).</li>
<li>The <a href="http://skreened.com/rexlambo/omg-so-perf">OMG SO PERF T-shirt</a> (site has coarse language).</li>
<li>Shannon Cepeda&#39;s great posts on pipeline speak: <a href="http://software.intel.com/en-us/blogs/2011/11/22/pipeline-speak-learning-more-about-intel-microarchitecture-codename-sandy-bridge">frontend</a> and <a href="http://software.intel.com/en-us/blogs/2011/12/01/pipeline-speak-part-2-the-second-part-of-the-sandy-bridge-pipeline">backend.</a></li>
<li>Jiri Olsa&#39;s <a href="https://lkml.org/lkml/2012/4/17/165">dwarf mode callchain</a> patch.</li>
<li>Linux kernel source: <a href="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/tools/perf/Documentation/examples.txt">tools/perf/Documentation/examples.txt</a>.</li>
<li>Linux kernel source: <a href="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/tools/perf/Documentation/perf-record.txt">tools/perf/Documentation/perf-record.txt</a>.</li>
<li>... and other documentation under tools/perf/Documentation.</li>
<li>A good case study for <a href="https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/">Transparent Hugepages: measuring the performance impact</a> using perf and PMCs.</li>
<li>Julia Evans created a <a href="https://twitter.com/b0rk/status/945900285460926464">perf cheatsheet</a> based on my one-liners (2017).</li>
<li>Denis Bakhvalov has a great post on <a href="https://easyperf.net/blog/2018/06/08/Advanced-profiling-topics-PEBS-and-LBR">PEBS and LBR internals</a>.</li>
</ul>

<p>ftrace:</p>

<ul>
<li><a href="https://github.com/brendangregg/perf-tools">perf-tools</a> (github), a collection of my performance analysis tools based on Linux perf_events and ftrace.</li>
<li><a href="https://lwn.net/Articles/608497/">Ftrace: The hidden light switch</a>, by myself for lwn.net, Aug 2014.</li>
<li>Linux kernel source: <a href="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/trace/ftrace.txt">Documentation/trace/ftrace.txt</a>.</li>
<li>lwn.net <a href="http://lwn.net/Articles/370423/">Secrets of the Ftrace function tracer</a>, by Steven Rostedt, Jan 2010.</li>
<li>lwn.net <a href="http://lwn.net/Articles/365835/">Debugging the kernel using Ftrace - part 1</a>, by Steven Rostedt, Dec 2009.</li>
<li>lwn.net <a href="http://lwn.net/Articles/366796/">Debugging the kernel using Ftrace - part 2</a>, by Steven Rostedt, Dec 2009.</li>
</ul>



<p>Have a question? If you work at Netflix, contact me. If not, please use the <a href="http://dir.gmane.org/gmane.linux.kernel.perf.user">perf user</a> mailing list, which I and other perf users are on.</p>

</div></div>
  </body>
</html>
