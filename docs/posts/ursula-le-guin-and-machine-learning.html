<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thewitchofendor.com/2022/05/04/ursula-le-guin-and-machine-learning/">Original</a>
    <h1>Ursula Le Guin and Machine Learning</h1>
    
    <div id="readability-page-1" class="page"><div>
			
<p id="2323">Today I read a paper on self-attention transformers in neural networks:</p>



<p><a rel="noreferrer noopener" href="http://nlp.seas.harvard.edu/annotated-transformer/" target="_blank"></a><a href="http://nlp.seas.harvard.edu/annotated-transformer/">http://nlp.seas.harvard.edu/annotated-transformer/</a></p>



<p><a href="http://nlp.seas.harvard.edu/annotated-transformer/" rel="noreferrer noopener" target="_blank">nlp.seas.harvard.edu</a></p>



<p id="e357">Forget Norvig &amp; Russel; read Ursula Le Guin to learn about Machine Learning.</p>



<p id="d250">Hereâ€™s why</p>



<p id="5362">#machinelearning #ursulaleguin #scifi</p>



<p id="0984">ðŸ§µðŸ‘‡</p>



<p id="0dae">In Le Guinâ€™s Left Hand of Darkness, Foretellers tell the future. But the future they tell depends on the question that is asked.</p>



<p id="1d99">Machine learning is all about the question you ask the model.</p>



<p id="b4d2">The question is the, why?</p>



<p id="2a95">Traditional computational problems, mostly analytic problems, are all about the HOW.</p>



<p id="ec00">They are about breaking something down into constituent steps and doing those steps quickly.</p>



<p id="fde6">A self-attention #transformer parallelizes steps that used to run sequentially.</p>



<p id="7156">There is a small impact on quality but an enormous gain on speed.</p>



<p id="7132">Self-attention is a HOW innovation, not a machine learning innovation.</p>



<p id="5058">It optimizes the steps of an algorithm.</p>



<p id="b031">BUT just like we cannot separate form from content, we cannot separate solution from speed</p>



<p id="f02d">Self-attention works well with language problems, like translation.</p>



<p id="6a7f">It does not sacrifice quality.</p>



<p id="ac13">In self-attention applied to linguistic problems, we keep track of the position â€” like the position of a letter in a word.</p>



<p id="320b">The position is mapped to a sinusoidal curve.</p>



<p id="8e72"><em>PE</em>(<em>pos</em>,2<em>i</em>)â€‹=sin(<em>pos</em>/100002<em>i</em>/<em>d</em>modelâ€‹)</p>



<p id="a006">(see tweet 1 for the reference)</p>



<p id="443d">Mapping from a position in a word to a curve is profound. I want to pause for a moment. Mapping is a symbolic gesture that allows us to extract more meaning.</p>



<p id="f29a">We move from a lower-dimensional space to a high-dimensional space: a letterâ€™s position to a point on a curve in space and time.</p>



<p id="c17d">When I confront a problem or a block, I move to a higher-dimensional space.</p>



<p id="f93c">I become aware of my current context and move into an overview of that context.</p>



<p id="07a6">When I return to the space of the problem, my location and perspective have changed.</p>



<p id="5008">Owen Barfield wrote in saving the appearances not to mistake the model for reality. And Gregory Bateson, among others, wrote not to confuse the map with the geography.</p>



<p id="e386">But what is the geography in machine learning?</p>



<p id="1dd9">There is no geography in machine learning. We are mapping an imaginary world.</p>



<p id="eafe">Some imaginary worlds are more useful than others.</p>



<p id="51a6">When the foretellers in Le Guin tell the future, they also move into a higher dimensional space â€” an alternate reality.</p>



<p id="9731">The question frames the space of this alternate reality.</p>



<p id="e897">For true machine learning innovation, we need to look at the quality and speed of answers and the quality of the questions.</p>



<p id="62fa">Iâ€™m doing #solidity this month, but maybe next month, Iâ€™ll do #machinelearning.</p>



<p id="d2d0"><em>Read this post and more on myÂ </em><a href="https://typeshare.co/hackerm0m/posts/-N1ASjkwvLPrK05Zu5zw" rel="noreferrer noopener" target="_blank"><em>Typeshare Social Blog</em></a></p>
					</div></div>
  </body>
</html>
