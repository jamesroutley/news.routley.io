<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jigsawstack.com/blog/jigsawstack-mixture-of-agents-moa-outperform-any-single-llm-and-reduce-cost-with-prompt-engine">Original</a>
    <h1>Show HN: Prompt Engine â€“ Auto pick LLMs based on your prompts</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><div><div><div><p>12 Sep 2024<!-- --> â€¢ <!-- -->4<!-- --> min read</p></div></div></div><div><p><img alt="JigsawStack Mixture-Of-Agents (MoA): Outperform any single LLM and reduce cost with Prompt Engine" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1726175190201/ccdb62cb-93c5-48ba-add6-4c8389925c66.jpeg?auto=compress,format&amp;format=webp" loading="lazy"/></p></div><p>Applications that use LLMs typically tend to use more than one model or provider, depending on your use case.</p>
<p>This is because not all LLMs are built the same, while GPT-4o might be good at basic customer support chat, Claude 3.5 Sonnet would be good at understanding code.</p>
<p>More and more apps are using a Mixture of Agent orchestration to power their application, resulting in better consistency and quality of output with less breakage.</p>
<p>Some stats ðŸ‘‡</p>
<div><p><img alt="" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1724783677160/12f26cc4-8bdd-41cc-9406-88f3a965b448.png?auto=compress,format&amp;format=webp" loading="lazy"/></p></div>
<p>Ref: <a href="https://state-of-llm.streamlit.app/#top-orchestration-tools">https://state-of-llm.streamlit.app/#top-orchestration-tools</a></p>
<p>Prompting is a pain in the ass, especially when moving between models while controlling cost or even switching models without breaking your entire code base.</p>
<p>Frameworks like <a href="https://www.langchain.com/">LangChain</a> make it way easier to add structure between LLMs, but that doesn&#39;t help with the quality of model response or picking a model that would make sense in balancing the cost to performance to quality ratio.</p>
<p>It gets more difficult when you want consistent JSON data or to reduce your cost with prompt caching.</p>
<p>There are just too many variables to think of...</p>
<p>We built <a href="https://jigsawstack.com/prompt-engine">Prompt Engine</a> to solve this problem, so all you have to do is write a solid prompt for your use case, and the engine takes care of the rest.</p>
<p>Here&#39;s how it works ðŸ‘‡</p>
<p>Two parts make up the Prompt Engine, the first is creating the engine and storing it, and the second is executing it.</p>
<div><p><img alt="" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1724792958648/4ce20700-916c-4b91-9049-b3a6df114bdd.png?auto=compress,format&amp;format=webp" loading="lazy"/></p></div>
<p>Creating a prompt engine takes in three things: the prompt, dynamic variables, and the output structure you expect every single time you run the prompt. The last two are optional.</p>
<p>The engine automatically enhances the initial prompt to improve accuracy, reduce token usage, and prevent output structure breakage.</p>
<p>Then, from a list of 50+ LLMs, 5 of the best LLMs that closest relate to the prompt are bucketed together.</p>
<p>This forms a single engine, which is represented by an ID.</p>
<p>Here&#39;s the code example of how you can create a Prompt Engine ðŸ‘‡</p>
<div><pre><code><span>const</span> result = <span>await</span> jigsaw.<span>prompt_engine</span>.<span>create</span>({
  <span>prompt</span>: <span>&#34;How to cook {dish}&#34;</span>, 
  <span>inputs</span>: [{ <span>key</span>: <span>&#34;dish&#34;</span> }], 
  <span>return_prompt</span>: [ 
    {
      <span>step</span>: <span>&#34;step counter&#34;</span>,
      <span>instructions</span>: <span>&#34;details of this step&#34;</span>,
    },
  ], 
});

<span>console</span>.<span>log</span>(<span>&#34;id: &#34;</span>, result.<span>prompt_engine_id</span>)
</code></pre></div>
<p><a href="https://docs.jigsawstack.com/api-reference/prompt-engine/create">Creating Prompt Engine docs</a></p><p>Now, for the fun part, running the engine.</p>
<div><p><img alt="" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1724879889824/b9df951a-e596-424c-b99f-ae0bdbb6e6b2.png?auto=compress,format&amp;format=webp" loading="lazy"/></p></div>
<p>After creating a Prompt Engine, you&#39;ll get an ID that represents it, using that ID, you can run your engine and pass in any dynamic values defined earlier.</p>
<p>When running the prompt engine, the 5 LLMs that were initially bucketed together when creating the prompt will run in parallel.</p>
<p>Each output will be ranked from best to worst by a smaller model that&#39;s good at ranking and comparing outputs based on the initial prompt and the similarity of each output.</p>
<p>The top 2 to 3 outputs will be combined into a single output while maintaining the conditions you set in the <code>return_prompt</code> field.</p>
<p>Over time, as you run the engine, the bucket of LLMs gets reduced as the engine learns the common LLMs that tend to perform better with your prompt, leading to faster and higher quality outputs.</p>
<p>This significantly increases the quality and consistency of running prompts on LLMs while reducing hallucinations.</p>
<p>Each run gets cached, so running the same execution multiple times reduces cost and response time significantly.</p>
<p>Here&#39;s the code on how to run a Prompt Engine ðŸ‘‡</p>
<div><pre><code><span>const</span> resp = <span>await</span> jigsaw.<span>prompt_engine</span>.<span>run</span>({
  <span>id</span>: result.<span>prompt_engine_id</span>, 
  <span>input_values</span>: {
    <span>dish</span>: <span>&#34;Singaporean chicken rice&#34;</span>, 
  },
});

<span>console</span>.<span>log</span>(<span>&#34;Output: &#34;</span>, resp.<span>result</span>);
</code></pre></div>
<p><a href="https://docs.jigsawstack.com/api-reference/prompt-engine/run">Running Prompt Engine docs</a></p><p>Want to run a Prompt without creating an engine first?</p>
<p>That&#39;s possible, check out <a href="https://docs.jigsawstack.com/api-reference/prompt-engine/run-direct">the docs to learn more</a>.</p>
<p>However, it isn&#39;t recommended. It might be a good way for you to quickly iterate on prompts and test, but the benefits of using the Prompt Engine get lost when you don&#39;t create and store the engine.</p>
<p>While the concept of how the Prompt Engine functions behind the scenes might be complex, JigsawStack has made it as simple as calling a function like any other LLM.</p>
<p>You can <a href="https://docs.jigsawstack.com/quick-start/node/introduction">learn more here</a> to get started with the SDK.</p>
<p><a href="https://arxiv.org/abs/2406.04692">Check out one of the papers used as a reference.</a></p><h2>What&#39;s next?</h2>
<p>The prompt engine models are consistently kept up to date with the latest models from providers like OpenAI, Anthropic, MistralAI, and even open-source models on Hugging Face that have commercially available licenses.</p>
<p>When upgrading to newer models, it can often break your code. Prompt Engine provides backward compatibility that gives you access to the best without breaking the old.</p>
<h2>Join the JigsawStack Community</h2>
<p>We invite you to join our growing community of developers on <a href="https://discord.gg/GnvmdZ7z">Discord</a> and <a href="https://x.com/jigsawstack">Twitter</a>. Share your projects, ask questions, and collaborate with others who are just as passionate about innovation as you are.</p>
<p>Thank you for being a part of the JigsawStack journey. We canâ€™t wait to see where you take it next!</p></div></div></div></div>
  </body>
</html>
