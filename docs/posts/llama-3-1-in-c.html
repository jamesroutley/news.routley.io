<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/trholding/llama2.c/blob/master/runq.c">Original</a>
    <h1>Llama 3.1 in C</h1>
    
    <div id="readability-page-1" class="page"><div><section aria-labelledby="file-name-id-wide file-name-id-mobile"><div><div id="highlighted-line-menu-positioner"><div id="copilot-button-positioner"><div><div role="presentation" aria-hidden="true" data-tab-size="8" data-paste-markdown-skip="true" data-hpc="true"><div><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p><p>44</p><p>45</p><p>46</p><p>47</p><p>48</p><p>49</p><p>50</p><p>51</p><p>52</p><p>53</p><p>54</p><p>55</p><p>56</p><p>57</p><p>58</p><p>59</p><p>60</p><p>61</p><p>62</p><p>63</p><p>64</p><p>65</p><p>66</p><p>67</p><p>68</p><p>69</p><p>70</p><p>71</p><p>72</p><p>73</p><p>74</p><p>75</p><p>76</p><p>77</p><p>78</p><p>79</p><p>80</p><p>81</p><p>82</p><p>83</p><p>84</p><p>85</p><p>86</p><p>87</p><p>88</p><p>89</p><p>90</p><p>91</p><p>92</p><p>93</p><p>94</p><p>95</p><p>96</p><p>97</p><p>98</p><p>99</p><p>100</p><p>101</p><p>102</p><p>103</p><p>104</p><p>105</p><p>106</p><p>107</p><p>108</p><p>109</p><p>110</p><p>111</p><p>112</p><p>113</p><p>114</p><p>115</p><p>116</p><p>117</p><p>118</p><p>119</p><p>120</p><p>121</p><p>122</p><p>123</p><p>124</p><p>125</p><p>126</p><p>127</p><p>128</p><p>129</p><p>130</p><p>131</p><p>132</p><p>133</p><p>134</p><p>135</p><p>136</p><p>137</p><p>138</p><p>139</p><p>140</p><p>141</p><p>142</p><p>143</p><p>144</p><p>145</p><p>146</p><p>147</p><p>148</p><p>149</p><p>150</p><p>151</p><p>152</p><p>153</p><p>154</p><p>155</p><p>156</p><p>157</p><p>158</p><p>159</p><p>160</p><p>161</p><p>162</p><p>163</p><p>164</p><p>165</p><p>166</p><p>167</p><p>168</p><p>169</p><p>170</p><p>171</p><p>172</p><p>173</p><p>174</p><p>175</p><p>176</p><p>177</p><p>178</p><p>179</p><p>180</p><p>181</p><p>182</p><p>183</p><p>184</p><p>185</p><p>186</p><p>187</p><p>188</p><p>189</p><p>190</p><p>191</p><p>192</p><p>193</p><p>194</p><p>195</p><p>196</p><p>197</p><p>198</p><p>199</p><p>200</p><p>201</p><p>202</p><p>203</p><p>204</p><p>205</p><p>206</p><p>207</p><p>208</p><p>209</p><p>210</p><p>211</p><p>212</p><p>213</p><p>214</p><p>215</p><p>216</p><p>217</p><p>218</p><p>219</p><p>220</p><p>221</p><p>222</p><p>223</p><p>224</p><p>225</p><p>226</p><p>227</p><p>228</p><p>229</p><p>230</p><p>231</p><p>232</p><p>233</p><p>234</p><p>235</p><p>236</p><p>237</p><p>238</p><p>239</p><p>240</p><p>241</p><p>242</p><p>243</p><p>244</p><p>245</p><p>246</p><p>247</p><p>248</p><p>249</p><p>250</p><p>251</p><p>252</p><p>253</p><p>254</p><p>255</p><p>256</p><p>257</p><p>258</p><p>259</p><p>260</p><p>261</p><p>262</p><p>263</p><p>264</p><p>265</p><p>266</p><p>267</p><p>268</p><p>269</p><p>270</p><p>271</p><p>272</p><p>273</p><p>274</p><p>275</p><p>276</p><p>277</p><p>278</p><p>279</p><p>280</p><p>281</p><p>282</p><p>283</p><p>284</p><p>285</p><p>286</p><p>287</p><p>288</p><p>289</p><p>290</p><p>291</p><p>292</p><p>293</p><p>294</p><p>295</p><p>296</p><p>297</p><p>298</p><p>299</p><p>300</p><p>301</p><p>302</p><p>303</p><p>304</p><p>305</p><p>306</p><p>307</p><p>308</p><p>309</p><p>310</p><p>311</p><p>312</p><p>313</p><p>314</p><p>315</p><p>316</p><p>317</p><p>318</p><p>319</p><p>320</p><p>321</p><p>322</p><p>323</p><p>324</p><p>325</p><p>326</p><p>327</p><p>328</p><p>329</p><p>330</p><p>331</p><p>332</p><p>333</p><p>334</p><p>335</p><p>336</p><p>337</p><p>338</p><p>339</p><p>340</p><p>341</p><p>342</p><p>343</p><p>344</p><p>345</p><p>346</p><p>347</p><p>348</p><p>349</p><p>350</p><p>351</p><p>352</p><p>353</p><p>354</p><p>355</p><p>356</p><p>357</p><p>358</p><p>359</p><p>360</p><p>361</p><p>362</p><p>363</p><p>364</p><p>365</p><p>366</p><p>367</p><p>368</p><p>369</p><p>370</p><p>371</p><p>372</p><p>373</p><p>374</p><p>375</p><p>376</p><p>377</p><p>378</p><p>379</p><p>380</p><p>381</p><p>382</p><p>383</p><p>384</p><p>385</p><p>386</p><p>387</p><p>388</p><p>389</p><p>390</p><p>391</p><p>392</p><p>393</p><p>394</p><p>395</p><p>396</p><p>397</p><p>398</p><p>399</p><p>400</p><p>401</p><p>402</p><p>403</p><p>404</p><p>405</p><p>406</p><p>407</p><p>408</p><p>409</p><p>410</p><p>411</p><p>412</p><p>413</p><p>414</p><p>415</p><p>416</p><p>417</p><p>418</p><p>419</p><p>420</p><p>421</p><p>422</p><p>423</p><p>424</p><p>425</p><p>426</p><p>427</p><p>428</p><p>429</p><p>430</p><p>431</p><p>432</p><p>433</p><p>434</p><p>435</p><p>436</p><p>437</p><p>438</p><p>439</p><p>440</p><p>441</p><p>442</p><p>443</p><p>444</p><p>445</p><p>446</p><p>447</p><p>448</p><p>449</p><p>450</p><p>451</p><p>452</p><p>453</p><p>454</p><p>455</p><p>456</p><p>457</p><p>458</p><p>459</p><p>460</p><p>461</p><p>462</p><p>463</p><p>464</p><p>465</p><p>466</p><p>467</p><p>468</p><p>469</p><p>470</p><p>471</p><p>472</p><p>473</p><p>474</p><p>475</p><p>476</p><p>477</p><p>478</p><p>479</p><p>480</p><p>481</p><p>482</p><p>483</p><p>484</p><p>485</p><p>486</p><p>487</p><p>488</p><p>489</p><p>490</p><p>491</p><p>492</p><p>493</p><p>494</p><p>495</p><p>496</p><p>497</p><p>498</p><p>499</p><p>500</p><p>501</p><p>502</p><p>503</p><p>504</p><p>505</p><p>506</p><p>507</p><p>508</p><p>509</p><p>510</p><p>511</p><p>512</p><p>513</p><p>514</p><p>515</p><p>516</p><p>517</p><p>518</p><p>519</p><p>520</p><p>521</p><p>522</p><p>523</p><p>524</p><p>525</p><p>526</p><p>527</p><p>528</p><p>529</p><p>530</p><p>531</p><p>532</p><p>533</p><p>534</p><p>535</p><p>536</p><p>537</p><p>538</p><p>539</p><p>540</p><p>541</p><p>542</p><p>543</p><p>544</p><p>545</p><p>546</p><p>547</p><p>548</p><p>549</p><p>550</p><p>551</p><p>552</p><p>553</p><p>554</p><p>555</p><p>556</p><p>557</p><p>558</p><p>559</p><p>560</p><p>561</p><p>562</p><p>563</p><p>564</p><p>565</p><p>566</p><p>567</p><p>568</p><p>569</p><p>570</p><p>571</p><p>572</p><p>573</p><p>574</p><p>575</p><p>576</p><p>577</p><p>578</p><p>579</p><p>580</p><p>581</p><p>582</p><p>583</p><p>584</p><p>585</p><p>586</p><p>587</p><p>588</p><p>589</p><p>590</p><p>591</p><p>592</p><p>593</p><p>594</p><p>595</p><p>596</p><p>597</p><p>598</p><p>599</p><p>600</p><p>601</p><p>602</p><p>603</p><p>604</p><p>605</p><p>606</p><p>607</p><p>608</p><p>609</p><p>610</p><p>611</p><p>612</p><p>613</p><p>614</p><p>615</p><p>616</p><p>617</p><p>618</p><p>619</p><p>620</p><p>621</p><p>622</p><p>623</p><p>624</p><p>625</p><p>626</p><p>627</p><p>628</p><p>629</p><p>630</p><p>631</p><p>632</p><p>633</p><p>634</p><p>635</p><p>636</p><p>637</p><p>638</p><p>639</p><p>640</p><p>641</p><p>642</p><p>643</p><p>644</p><p>645</p><p>646</p><p>647</p><p>648</p><p>649</p><p>650</p><p>651</p><p>652</p><p>653</p><p>654</p><p>655</p><p>656</p><p>657</p><p>658</p><p>659</p><p>660</p><p>661</p><p>662</p><p>663</p><p>664</p><p>665</p><p>666</p><p>667</p><p>668</p><p>669</p><p>670</p><p>671</p><p>672</p><p>673</p><p>674</p><p>675</p><p>676</p><p>677</p><p>678</p><p>679</p><p>680</p><p>681</p><p>682</p><p>683</p><p>684</p><p>685</p><p>686</p><p>687</p><p>688</p><p>689</p><p>690</p><p>691</p><p>692</p><p>693</p><p>694</p><p>695</p><p>696</p><p>697</p><p>698</p><p>699</p><p>700</p><p>701</p><p>702</p><p>703</p><p>704</p><p>705</p><p>706</p><p>707</p><p>708</p><p>709</p><p>710</p><p>711</p><p>712</p><p>713</p><p>714</p><p>715</p><p>716</p><p>717</p><p>718</p><p>719</p><p>720</p><p>721</p><p>722</p><p>723</p><p>724</p><p>725</p><p>726</p><p>727</p><p>728</p><p>729</p><p>730</p><p>731</p><p>732</p><p>733</p><p>734</p><p>735</p><p>736</p><p>737</p><p>738</p><p>739</p><p>740</p><p>741</p><p>742</p><p>743</p><p>744</p><p>745</p><p>746</p><p>747</p><p>748</p><p>749</p><p>750</p><p>751</p><p>752</p><p>753</p><p>754</p><p>755</p><p>756</p><p>757</p><p>758</p><p>759</p><p>760</p><p>761</p><p>762</p><p>763</p><p>764</p><p>765</p><p>766</p><p>767</p><p>768</p><p>769</p><p>770</p><p>771</p><p>772</p><p>773</p><p>774</p><p>775</p><p>776</p><p>777</p><p>778</p><p>779</p><p>780</p><p>781</p><p>782</p><p>783</p><p>784</p><p>785</p><p>786</p><p>787</p><p>788</p><p>789</p><p>790</p><p>791</p><p>792</p><p>793</p><p>794</p><p>795</p><p>796</p><p>797</p><p>798</p><p>799</p><p>800</p><p>801</p><p>802</p><p>803</p><p>804</p><p>805</p><p>806</p><p>807</p><p>808</p><p>809</p><p>810</p><p>811</p><p>812</p><p>813</p><p>814</p><p>815</p><p>816</p><p>817</p><p>818</p><p>819</p><p>820</p><p>821</p><p>822</p><p>823</p><p>824</p><p>825</p><p>826</p><p>827</p><p>828</p><p>829</p><p>830</p><p>831</p><p>832</p><p>833</p><p>834</p><p>835</p><p>836</p><p>837</p><p>838</p><p>839</p><p>840</p><p>841</p><p>842</p><p>843</p><p>844</p><p>845</p><p>846</p><p>847</p><p>848</p><p>849</p><p>850</p><p>851</p><p>852</p><p>853</p><p>854</p><p>855</p><p>856</p><p>857</p><p>858</p><p>859</p><p>860</p><p>861</p><p>862</p><p>863</p><p>864</p><p>865</p><p>866</p><p>867</p><p>868</p><p>869</p><p>870</p><p>871</p><p>872</p><p>873</p><p>874</p><p>875</p><p>876</p><p>877</p><p>878</p><p>879</p><p>880</p><p>881</p><p>882</p><p>883</p><p>884</p><p>885</p><p>886</p><p>887</p><p>888</p><p>889</p><p>890</p><p>891</p><p>892</p><p>893</p><p>894</p><p>895</p><p>896</p><p>897</p><p>898</p><p>899</p><p>900</p><p>901</p><p>902</p><p>903</p><p>904</p><p>905</p><p>906</p><p>907</p><p>908</p><p>909</p><p>910</p><p>911</p><p>912</p><p>913</p><p>914</p><p>915</p><p>916</p><p>917</p><p>918</p><p>919</p><p>920</p><p>921</p><p>922</p><p>923</p><p>924</p><p>925</p><p>926</p><p>927</p><p>928</p><p>929</p><p>930</p><p>931</p><p>932</p><p>933</p><p>934</p><p>935</p><p>936</p><p>937</p><p>938</p><p>939</p><p>940</p><p>941</p><p>942</p><p>943</p><p>944</p><p>945</p><p>946</p><p>947</p><p>948</p><p>949</p><p>950</p><p>951</p><p>952</p><p>953</p><p>954</p><p>955</p><p>956</p><p>957</p><p>958</p><p>959</p><p>960</p><p>961</p><p>962</p><p>963</p><p>964</p><p>965</p><p>966</p><p>967</p><p>968</p><p>969</p><p>970</p><p>971</p><p>972</p><p>973</p><p>974</p><p>975</p><p>976</p><p>977</p><p>978</p><p>979</p><p>980</p><p>981</p><p>982</p><p>983</p><p>984</p><p>985</p><p>986</p><p>987</p><p>988</p><p>989</p><p>990</p><p>991</p><p>992</p><p>993</p><p>994</p><p>995</p><p>996</p><p>997</p><p>998</p><p>999</p><p>1000</p></div><div><div><div><p><span>/* Inference for Llama 2 &amp; Llama 3 / 3.1 Transformer model in pure C, int8 quantized forward pass. */</span></p></div></div><div><div><p><span>/* The Llama 2 Everywhere @trholding (Vulcan) fork   */</span></p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// L2E : Global Variables</span></p></div></div><div><div><p><span>int</span> <span>buffertokens</span> <span>=</span> <span>1</span>;     <span>// output token buffer size</span></p></div></div><div><div><p><span>int</span> <span>stats</span> <span>=</span> <span>1</span>;     <span>// extended status info</span></p></div></div><div><div><p><span>int</span> <span>llamaver</span> <span>=</span> <span>2</span>; <span>// llama version (default is 2, valid 2 &amp; 3)</span></p></div></div><div><div><p><span>float</span> <span>rope_tf</span> <span>=</span> <span>10000.0</span>; <span>// Rope tetha or frequency, 10000.0 =&gt; llama2, 500000.0 &gt; llama3</span></p></div></div><div><div><p><span>int</span> <span>BOS</span> <span>=</span> <span>1</span>; <span>// Beginning of Sentence token value, llama2 = 1 , llama3 = 128000</span></p></div></div><div><div><p><span>int</span> <span>EOS</span> <span>=</span> <span>2</span>; <span>// End of Sentence token value, llama2 = 2 , llama3 = 128009 (end of text)</span></p></div></div><div><div><p><span>char</span> <span>system_template</span>[<span>1024</span>]<span>=</span><span>&#34;&#34;</span>;</p></div></div><div><div><p><span>char</span> <span>user_template</span>[<span>1024</span>]<span>=</span><span>&#34;&#34;</span>;</p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// L2E Humanoid : Linux Kernel Support Directives</span></p></div></div><div><div><p><span>#define</span> <span>_DEFTOSTR</span>(<span>LSTR</span>) #LSTR</p></div></div><div><div><p><span>#define</span> <span>DEFTOSTR</span>(<span>LSTR</span>) _DEFTOSTR(LSTR)</p></div></div><div><div><p><span>#define</span> <span>LOOPSTATUS</span> 0 // Status off</p></div></div><div><div><p><span>#define</span> <span>LOOPSTATUS</span> 1 // Status on</p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// L2E Asteroid : Unikraft Unikernel Support Directives</span></p></div></div><div><div><p><span>#define</span> <span>LOOPSTATUS</span> 1 // Status on</p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// INCBIN Embedding Support Directives</span></p></div></div><div><div><p><span>// https://github.com/graphitemaster/incbin</span></p></div></div><div><div><p><span>// String substitution macro needed to pass paths to INCBIN</span></p></div></div><div><div><p><span>#define</span> <span>ADDPATH</span>(<span>FPATH</span>) TOSTR(FPATH)</p></div></div><div><div><p><span>#define</span> <span>TOSTR</span>(<span>FPATH</span>) #FPATH</p></div></div><div><div><p><span>#ifdef</span> <span>INC_BIN</span> <span>// Support for embedding model and tokenizer</span></p></div></div><div><div><p><span>#define</span> <span>INCBIN_PREFIX</span> emb_</p></div></div><div><div><p><span>#define</span> <span>INCBIN_STYLE</span> INCBIN_STYLE_SNAKE</p></div></div><div><div><p><span>#define</span> <span>MODPATH</span> out/model.bin // default model path</p></div></div><div><div><p><span>#define</span> <span>TOKPATH</span> tokenizer.bin // default tokenizer path</p></div></div><div><div><p><span>INCBIN</span>(<span>Model</span>, <span>ADDPATH</span>(<span>MODPATH</span>)); <span>// Model path is passed via makefile</span></p></div></div><div><div><p><span>INCBIN</span>(<span>Tokenizer</span>, <span>ADDPATH</span>(<span>TOKPATH</span>)); <span>// Tokenizer path is passed via makefile</span></p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// strliteral (STRLIT) Embedding Support Directives</span></p></div></div><div><div><p><span>// https://github.com/mortie/strliteral</span></p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// Actually Portable Executable Format Preprocessor Directives</span></p></div></div><div><div><p><span>#ifdef</span> <span>COSMO_BLINK</span> <span>// Support ARM 64 Bit via Blink VM Emulation</span></p></div></div><div><div><p><span>__static_yoink</span>(<span>&#34;blink_linux_aarch64&#34;</span>);  <span>// for raspberry pi</span></p></div></div><div><div><p><span>__static_yoink</span>(<span>&#34;blink_xnu_aarch64&#34;</span>);    <span>// is apple silicon</span></p></div></div><div><div><p><span>#ifdef</span> <span>COSMO_METAL</span> <span>// Support VGA Console when running bare metal</span></p></div></div><div><div><p><span>__static_yoink</span>(<span>&#34;vga_console&#34;</span>);</p></div></div><div><div><p><span>#ifdef</span> <span>COSMO_ZIP</span> <span>// Support embedded models via Zip Archive support</span></p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>#if</span> defined(<span>CLBLAST</span>) <span>||</span> defined(<span>OPENBLAS</span>) <span>||</span> defined(<span>CBLAS</span>) <span>||</span> defined(<span>BLIS</span>) <span>||</span> defined(<span>MKL</span>) <span>||</span> defined(<span>ARMPL</span>) <span>||</span> defined(<span>AAF</span>)</p></div></div><div><div><p><span>#include</span> <span>&lt;clblast_netlib_c.h&gt;</span></p></div></div><div><div><p><span>#include</span> <span>&lt;Accelerate/Accelerate.h&gt;</span></p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// OpenMP and OpenACC Support</span></p></div></div><div><div><p><span>// Macro that makes a pragma enabled with string substitution</span></p></div></div><div><div><p><span>#define</span> <span>MKPRAGMA_</span>(<span>x</span>) _Pragma (#x)</p></div></div><div><div><p><span>#define</span> <span>MK_PRAGMA</span>(<span>x</span>) MKPRAGMA_(x)</p></div></div><div><div><p><span>// Portable OpenMP and OpenACC pragma macros</span></p></div></div><div><div><p><span>#define</span> <span>ACCELS</span>() MK_PRAGMA(omp parallel for)</p></div></div><div><div><p><span>#define</span> <span>ACCEL</span>(...) MK_PRAGMA(omp parallel for private(__VA_ARGS__))</p></div></div><div><div><p><span>#define</span> <span>ACCELRD</span>(<span>VAR</span>) MK_PRAGMA(omp parallel for reduction(+:VAR))</p></div></div><div><div><p><span>#define</span> <span>ACCELS</span>() MK_PRAGMA(acc parallel loop)</p></div></div><div><div><p><span>#define</span> <span>ACCEL</span>(...) MK_PRAGMA(acc parallel loop private(__VA_ARGS__))</p></div></div><div><div><p><span>#define</span> <span>ACCELRD</span>(<span>VAR</span>) MK_PRAGMA(acc parallel loop reduction(+:VAR))</p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>const</span> <span>int</span> <span>GS</span> <span>=</span> <span>64</span>; <span>// group size 64 for Cheap Acceleration Tech :)</span></p></div></div><div><div><p><span>int</span> <span>GS</span> <span>=</span> <span>0</span>; <span>// group size global for quantization of the weights</span></p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>int</span> <span>dim</span>; <span>// transformer dimension</span></p></div></div><div><div><p><span>int</span> <span>hidden_dim</span>; <span>// for ffn layers</span></p></div></div><div><div><p><span>int</span> <span>n_layers</span>; <span>// number of layers</span></p></div></div><div><div><p><span>int</span> <span>n_heads</span>; <span>// number of query heads</span></p></div></div><div><div><p><span>int</span> <span>n_kv_heads</span>; <span>// number of key/value heads (can be &lt; query heads because of multiquery)</span></p></div></div><div><div><p><span>int</span> <span>vocab_size</span>; <span>// vocabulary size, usually 256 (byte-level)</span></p></div></div><div><div><p><span>int</span> <span>seq_len</span>; <span>// max sequence length</span></p></div></div><div><div><p><span>int8_t</span><span>*</span> <span>q</span>;    <span>// quantized values</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>s</span>; <span>// scaling factors</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>q_tokens</span>; <span>// (vocab_size, dim)</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>token_embedding_table</span>; <span>// same, but dequantized</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>rms_att_weight</span>; <span>// (layer, dim) rmsnorm weights</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>rms_ffn_weight</span>; <span>// (layer, dim)</span></p></div></div><div><div><p><span>// weights for matmuls. note dim == n_heads * head_size</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>wq</span>; <span>// (layer, dim, n_heads * head_size)</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>wk</span>; <span>// (layer, dim, n_kv_heads * head_size)</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>wv</span>; <span>// (layer, dim, n_kv_heads * head_size)</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>wo</span>; <span>// (layer, n_heads * head_size, dim)</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>w1</span>; <span>// (layer, hidden_dim, dim)</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>w2</span>; <span>// (layer, dim, hidden_dim)</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>w3</span>; <span>// (layer, hidden_dim, dim)</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>rms_final_weight</span>; <span>// (dim,)</span></p></div></div><div><div><p><span>// (optional) classifier weights for the logits, on the last layer</span></p></div></div><div><div><p><span>// current wave of activations</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>x</span>; <span>// activation at current time stamp (dim,)</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>xb</span>; <span>// same, but inside a residual branch (dim,)</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>xb2</span>; <span>// an additional buffer just for convenience (dim,)</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>hb</span>; <span>// buffer for hidden dimension in the ffn (hidden_dim,)</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>hb2</span>; <span>// buffer for hidden dimension in the ffn (hidden_dim,)</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>xq</span>; <span>// quantized x (dim,)</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>hq</span>; <span>// quantized hb (hidden_dim,)</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>q</span>; <span>// query (dim,)</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>v</span>; <span>// value (dim,)</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>att</span>; <span>// buffer for scores/attention values (n_heads, seq_len)</span></p></div></div><div><div><p><span>float</span> <span>*</span><span>logits</span>; <span>// output logits</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>key_cache</span>;   <span>// (layer, seq_len, dim)</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>value_cache</span>; <span>// (layer, seq_len, dim)</span></p></div></div><div><div><p><span>Config</span> <span>config</span>; <span>// the hyperparameters of the architecture (the blueprint)</span></p></div></div><div><div><p><span>TransformerWeights</span> <span>weights</span>; <span>// the weights of the model</span></p></div></div><div><div><p><span>RunState</span> <span>state</span>; <span>// buffers for the &#34;wave&#34; of activations in the forward pass</span></p></div></div><div><div><p><span>// some more state needed to properly clean up the memory mapping (sigh)</span></p></div></div><div><div><p><span>int</span> <span>fd</span>; <span>// file descriptor for memory mapping</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>data</span>; <span>// memory mapped data pointer</span></p></div></div><div><div><p><span>ssize_t</span> <span>file_size</span>; <span>// size of the checkpoint file in bytes</span></p></div></div><div><div><p><span>void</span> <span>malloc_run_state</span>(<span>RunState</span><span>*</span> <span>s</span>, <span>Config</span><span>*</span> <span>p</span>) {</p></div></div><div><div><p><span>// we calloc instead of malloc to keep valgrind happy</span></p></div></div><div><div><p><span>int</span> <span>kv_dim</span> <span>=</span> (<span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> <span>p</span><span>-&gt;</span><span>n_kv_heads</span>) / <span>p</span><span>-&gt;</span><span>n_heads</span>;</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>x</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>xb</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>xb2</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>hb</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>hidden_dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>hb2</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>hidden_dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>xq</span> <span>=</span> (<span>QuantizedTensor</span>) { .<span>q</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>dim</span>, <span>sizeof</span>(<span>int8_t</span>)), .<span>s</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>dim</span>, <span>sizeof</span>(<span>float</span>)) };</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>hq</span> <span>=</span> (<span>QuantizedTensor</span>) { .<span>q</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>hidden_dim</span>, <span>sizeof</span>(<span>int8_t</span>)), .<span>s</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>hidden_dim</span>, <span>sizeof</span>(<span>float</span>)) };</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>q</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>k</span> <span>=</span> <span>calloc</span>(<span>kv_dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>v</span> <span>=</span> <span>calloc</span>(<span>kv_dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>att</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>n_heads</span> <span>*</span> <span>p</span><span>-&gt;</span><span>seq_len</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>logits</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>vocab_size</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>key_cache</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>n_layers</span> <span>*</span> <span>p</span><span>-&gt;</span><span>seq_len</span> <span>*</span> <span>kv_dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>s</span><span>-&gt;</span><span>value_cache</span> <span>=</span> <span>calloc</span>(<span>p</span><span>-&gt;</span><span>n_layers</span> <span>*</span> <span>p</span><span>-&gt;</span><span>seq_len</span> <span>*</span> <span>kv_dim</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>// ensure all mallocs went fine</span></p></div></div><div><div><p><span>if</span> (!<span>s</span><span>-&gt;</span><span>x</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>xb</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>xb2</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>hb</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>hb2</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>q</span></p></div></div><div><div><p><span>||</span> !<span>s</span><span>-&gt;</span><span>k</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>v</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>att</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>logits</span> <span>||</span> !<span>s</span><span>-&gt;</span><span>key_cache</span></p></div></div><div><div><p><span>fprintf</span>(<span>stderr</span>, <span>&#34;malloc failed!\n&#34;</span>);</p></div></div><div><div><p><span>void</span> <span>free_run_state</span>(<span>RunState</span><span>*</span> <span>s</span>) {</p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// Quantization functions</span></p></div></div><div><div><p><span>void</span> <span>dequantize</span>(<span>QuantizedTensor</span> <span>*</span><span>qx</span>, <span>float</span><span>*</span> <span>x</span>, <span>int</span> <span>n</span>) {</p></div></div><div><div><p><span>ACCELS</span>() <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>n</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>x</span>[<span>i</span>] <span>=</span> <span>qx</span><span>-&gt;</span><span>q</span>[<span>i</span>] <span>*</span> <span>qx</span><span>-&gt;</span><span>s</span>[<span>i</span> / <span>GS</span>];</p></div></div><div><div><p><span>void</span> <span>quantize</span>(<span>QuantizedTensor</span> <span>*</span><span>qx</span>, <span>float</span><span>*</span> <span>x</span>, <span>int</span> <span>n</span>) {</p></div></div><div><div><p><span>ACCELS</span>() <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>group</span> <span>=</span> <span>0</span>; <span>group</span> <span>&lt;</span> <span>num_groups</span>; <span>group</span><span>++</span>) {</p></div></div><div><div><p><span>// find the max absolute value in the current group</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>GS</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>float</span> <span>val</span> <span>=</span> <span>fabs</span>(<span>x</span>[<span>group</span> <span>*</span> <span>GS</span> <span>+</span> <span>i</span>]);</p></div></div><div><div><p><span>// calculate and write the scaling factor</span></p></div></div><div><div><p><span>float</span> <span>scale</span> <span>=</span> <span>wmax</span> / <span>Q_MAX</span>;</p></div></div><div><div><p><span>// calculate and write the quantized values</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>GS</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>float</span> <span>quant_value</span> <span>=</span> <span>x</span>[<span>group</span> <span>*</span> <span>GS</span> <span>+</span> <span>i</span>] / <span>scale</span>; <span>// scale</span></p></div></div><div><div><p><span>int8_t</span> <span>quantized</span> <span>=</span> (<span>int8_t</span>) <span>round</span>(<span>quant_value</span>); <span>// round and clamp</span></p></div></div><div><div><p><span>qx</span><span>-&gt;</span><span>q</span>[<span>group</span> <span>*</span> <span>GS</span> <span>+</span> <span>i</span>] <span>=</span> <span>quantized</span>;</p></div></div><div><div><p><span>/* initialize `n` x quantized tensor (with `size_each` elements), starting from memory pointed at *ptr */</span></p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>init_quantized_tensors</span>(<span>void</span> <span>*</span><span>*</span><span>ptr</span>, <span>int</span> <span>n</span>, <span>int</span> <span>size_each</span>) {</p></div></div><div><div><p><span>QuantizedTensor</span> <span>*</span><span>res</span> <span>=</span> <span>malloc</span>(<span>n</span> <span>*</span> <span>sizeof</span>(<span>QuantizedTensor</span>));</p></div></div><div><div><p><span>/* map quantized int8 values*/</span></p></div></div><div><div><p><span>p</span> <span>=</span> (<span>int8_t</span><span>*</span>)<span>p</span> <span>+</span> <span>size_each</span>;</p></div></div><div><div><p><span>p</span> <span>=</span> (<span>float</span><span>*</span>)<span>p</span> <span>+</span> <span>size_each</span> / <span>GS</span>;</p></div></div><div><div><p><span>*</span><span>ptr</span> <span>=</span> <span>p</span>; <span>// advance ptr to current position</span></p></div></div><div><div><p><span>void</span> <span>memory_map_weights</span>(<span>TransformerWeights</span> <span>*</span><span>w</span>, <span>Config</span><span>*</span> <span>p</span>, <span>void</span><span>*</span> <span>ptr</span>, <span>uint8_t</span> <span>shared_classifier</span>) {</p></div></div><div><div><p><span>int</span> <span>head_size</span> <span>=</span> <span>p</span><span>-&gt;</span><span>dim</span> / <span>p</span><span>-&gt;</span><span>n_heads</span>;</p></div></div><div><div><p><span>// first are the parameters that are kept in fp32 (the rmsnorm (1D) weights)</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>fptr</span> <span>=</span> (<span>float</span><span>*</span>) <span>ptr</span>; <span>// cast our pointer to float*</span></p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>rms_att_weight</span> <span>=</span> <span>fptr</span>;</p></div></div><div><div><p><span>fptr</span> <span>+=</span> <span>p</span><span>-&gt;</span><span>n_layers</span> <span>*</span> <span>p</span><span>-&gt;</span><span>dim</span>;</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>rms_ffn_weight</span> <span>=</span> <span>fptr</span>;</p></div></div><div><div><p><span>fptr</span> <span>+=</span> <span>p</span><span>-&gt;</span><span>n_layers</span> <span>*</span> <span>p</span><span>-&gt;</span><span>dim</span>;</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>rms_final_weight</span> <span>=</span> <span>fptr</span>;</p></div></div><div><div><p><span>// now read all the quantized weights</span></p></div></div><div><div><p><span>ptr</span> <span>=</span> (<span>void</span><span>*</span>)<span>fptr</span>; <span>// now cast the pointer back to void*</span></p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>q_tokens</span> <span>=</span> <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>1</span>, <span>p</span><span>-&gt;</span><span>vocab_size</span> <span>*</span> <span>p</span><span>-&gt;</span><span>dim</span>);</p></div></div><div><div><p><span>// dequantize token embedding table</span></p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>token_embedding_table</span> <span>=</span> <span>malloc</span>(<span>p</span><span>-&gt;</span><span>vocab_size</span> <span>*</span> <span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>dequantize</span>(<span>w</span><span>-&gt;</span><span>q_tokens</span>, <span>w</span><span>-&gt;</span><span>token_embedding_table</span>, <span>p</span><span>-&gt;</span><span>vocab_size</span> <span>*</span> <span>p</span><span>-&gt;</span><span>dim</span>);</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>wq</span> <span>=</span> <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>p</span><span>-&gt;</span><span>n_layers</span>, <span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> (<span>p</span><span>-&gt;</span><span>n_heads</span> <span>*</span> <span>head_size</span>));</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>wk</span> <span>=</span> <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>p</span><span>-&gt;</span><span>n_layers</span>, <span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> (<span>p</span><span>-&gt;</span><span>n_kv_heads</span> <span>*</span> <span>head_size</span>));</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>wv</span> <span>=</span> <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>p</span><span>-&gt;</span><span>n_layers</span>, <span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> (<span>p</span><span>-&gt;</span><span>n_kv_heads</span> <span>*</span> <span>head_size</span>));</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>wo</span> <span>=</span> <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>p</span><span>-&gt;</span><span>n_layers</span>, (<span>p</span><span>-&gt;</span><span>n_heads</span> <span>*</span> <span>head_size</span>) <span>*</span> <span>p</span><span>-&gt;</span><span>dim</span>);</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>w1</span> <span>=</span> <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>p</span><span>-&gt;</span><span>n_layers</span>, <span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> <span>p</span><span>-&gt;</span><span>hidden_dim</span>);</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>w2</span> <span>=</span> <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>p</span><span>-&gt;</span><span>n_layers</span>, <span>p</span><span>-&gt;</span><span>hidden_dim</span> <span>*</span> <span>p</span><span>-&gt;</span><span>dim</span>);</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>w3</span> <span>=</span> <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>p</span><span>-&gt;</span><span>n_layers</span>, <span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> <span>p</span><span>-&gt;</span><span>hidden_dim</span>);</p></div></div><div><div><p><span>w</span><span>-&gt;</span><span>wcls</span> <span>=</span> <span>shared_classifier</span> ? <span>w</span><span>-&gt;</span><span>q_tokens</span> : <span>init_quantized_tensors</span>(<span>&amp;</span><span>ptr</span>, <span>1</span>, <span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> <span>p</span><span>-&gt;</span><span>vocab_size</span>);</p></div></div><div><div><p><span>#if</span> defined (<span>INC_BIN</span>) <span>||</span> defined(<span>STRLIT</span>)</p></div></div><div><div><p><span>void</span> <span>read_checkpoint</span>(<span>char</span><span>*</span> <span>checkpoint</span>, <span>Config</span><span>*</span> <span>config</span>, <span>TransformerWeights</span><span>*</span> <span>weights</span>, </p></div></div><div><div><p><span>int</span><span>*</span> <span>fd</span>, <span>float</span><span>*</span><span>*</span> <span>data</span>, <span>ssize_t</span><span>*</span> <span>file_size</span>) {</p></div></div><div><div><p><span>// Calculate the file size from the raw data</span></p></div></div><div><div><p><span>*</span><span>file_size</span> <span>=</span> <span>strlen</span>(<span>checkpoint</span>);</p></div></div><div><div><p><span>// memory map the Transformer weights into the data pointer</span></p></div></div><div><div><p><span>*</span><span>fd</span> <span>=</span> <span>-1</span>; <span>// No file descriptor is needed since we&#39;re not opening a file</span></p></div></div><div><div><p><span>*</span><span>data</span> <span>=</span> (<span>float</span><span>*</span>) <span>checkpoint</span>;</p></div></div><div><div><p><span>// Create a byte pointer to navigate the data</span></p></div></div><div><div><p><span>uint8_t</span><span>*</span> <span>ptr</span> <span>=</span> (<span>uint8_t</span><span>*</span>) <span>*</span><span>data</span>;</p></div></div><div><div><p><span>// read in magic number (uint32), has to be 0x616b3432, i.e. &#34;ak42&#34; in ASCII</span></p></div></div><div><div><p><span>uint32_t</span> <span>magic_number</span> <span>=</span> <span>*</span>(<span>uint32_t</span><span>*</span>) <span>ptr</span>;</p></div></div><div><div><p><span>if</span> (<span>magic_number</span> <span>!=</span> <span>0x616b3432</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;Bad magic number\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>// read in the version number (uint32), has to be 2</span></p></div></div><div><div><p><span>int</span> <span>version</span> <span>=</span> <span>*</span>(<span>int</span><span>*</span>) <span>ptr</span>;</p></div></div><div><div><p><span>if</span> (<span>version</span> <span>!=</span> <span>2</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;Bad version %d, need version 2\n&#34;</span>, <span>version</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>int</span> <span>header_size</span> <span>=</span> <span>256</span>; <span>// the header size for version 2 in bytes</span></p></div></div><div><div><p><span>memcpy</span>(<span>config</span>, <span>ptr</span>, <span>sizeof</span>(<span>Config</span>));</p></div></div><div><div><p><span>uint8_t</span> <span>shared_classifier</span> <span>=</span> <span>*</span>(<span>uint8_t</span><span>*</span>) <span>ptr</span>;</p></div></div><div><div><p><span>int</span> <span>group_size</span> <span>=</span> <span>*</span>(<span>int</span><span>*</span>) <span>ptr</span>;</p></div></div><div><div><p><span>GS</span> <span>=</span> <span>group_size</span>; <span>// set as global, as it will be used in many places</span></p></div></div><div><div><p><span>void</span><span>*</span> <span>weights_ptr</span> <span>=</span> ((<span>char</span><span>*</span>)<span>*</span><span>data</span>) <span>+</span> <span>header_size</span>; <span>// skip header bytes</span></p></div></div><div><div><p><span>memory_map_weights</span>(<span>weights</span>, <span>config</span>, <span>weights_ptr</span>, <span>shared_classifier</span>);</p></div></div><div><div><p><span>void</span> <span>read_checkpoint</span>(<span>char</span><span>*</span> <span>checkpoint</span>, <span>Config</span><span>*</span> <span>config</span>, <span>TransformerWeights</span><span>*</span> <span>weights</span>,</p></div></div><div><div><p><span>int</span><span>*</span> <span>fd</span>, <span>float</span><span>*</span><span>*</span> <span>data</span>, <span>ssize_t</span><span>*</span> <span>file_size</span>) {</p></div></div><div><div><p><span>FILE</span> <span>*</span><span>file</span> <span>=</span> <span>fopen</span>(<span>checkpoint</span>, <span>&#34;rb&#34;</span>);</p></div></div><div><div><p><span>if</span> (!<span>file</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;Couldn&#39;t open file %s\n&#34;</span>, <span>checkpoint</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>// read in magic number (uint32), has to be 0x616b3432, i.e. &#34;ak42&#34; in ASCII</span></p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>&amp;</span><span>magic_number</span>, <span>sizeof</span>(<span>uint32_t</span>), <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>if</span> (<span>magic_number</span> <span>!=</span> <span>0x616b3432</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;Bad magic number\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>// read in the version number (uint32), has to be 2</span></p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>&amp;</span><span>version</span>, <span>sizeof</span>(<span>int</span>), <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>if</span> (<span>version</span> <span>!=</span> <span>2</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;Bad version %d, need version 2\n&#34;</span>, <span>version</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>int</span> <span>header_size</span> <span>=</span> <span>256</span>; <span>// the header size for version 2 in bytes</span></p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>config</span>, <span>sizeof</span>(<span>Config</span>), <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>uint8_t</span> <span>shared_classifier</span>; <span>// a byte to indicate if the classifier is shared</span></p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>&amp;</span><span>shared_classifier</span>, <span>sizeof</span>(<span>uint8_t</span>), <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>int</span> <span>group_size</span>; <span>// the group size used in quantization</span></p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>&amp;</span><span>group_size</span>, <span>sizeof</span>(<span>int</span>), <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>GS</span> <span>=</span> <span>group_size</span>; <span>// set as global, as it will be used in many places</span></p></div></div><div><div><p><span>// figure out the file size</span></p></div></div><div><div><p><span>fseek</span>(<span>file</span>, <span>0</span>, <span>SEEK_END</span>); <span>// move file pointer to end of file</span></p></div></div><div><div><p><span>*</span><span>file_size</span> <span>=</span> <span>ftell</span>(<span>file</span>); <span>// get the file size, in bytes</span></p></div></div><div><div><p><span>// memory map the Transformer weights into the data pointer</span></p></div></div><div><div><p><span>*</span><span>fd</span> <span>=</span> <span>open</span>(<span>checkpoint</span>, <span>O_RDONLY</span>); <span>// open in read only mode</span></p></div></div><div><div><p><span>if</span> (<span>*</span><span>fd</span> <span>==</span> <span>-1</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;open failed!\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>*</span><span>data</span> <span>=</span> <span>mmap</span>(<span>NULL</span>, <span>*</span><span>file_size</span>, <span>PROT_READ</span>, <span>MAP_PRIVATE</span>, <span>*</span><span>fd</span>, <span>0</span>);</p></div></div><div><div><p><span>if</span> (<span>*</span><span>data</span> <span>==</span> <span>MAP_FAILED</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;mmap failed!\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>void</span><span>*</span> <span>weights_ptr</span> <span>=</span> ((<span>char</span><span>*</span>)<span>*</span><span>data</span>) <span>+</span> <span>header_size</span>; <span>// skip header bytes. char is 1 byte</span></p></div></div><div><div><p><span>memory_map_weights</span>(<span>weights</span>, <span>config</span>, <span>weights_ptr</span>, <span>shared_classifier</span>);</p></div></div><div><div><p><span>void</span> <span>build_transformer</span>(<span>Transformer</span> <span>*</span><span>t</span>, <span>char</span><span>*</span> <span>checkpoint_path</span>) {</p></div></div><div><div><p><span>// read in the Config and the Weights from the checkpoint</span></p></div></div><div><div><p><span>read_checkpoint</span>(<span>checkpoint_path</span>, <span>&amp;</span><span>t</span><span>-&gt;</span><span>config</span>, <span>&amp;</span><span>t</span><span>-&gt;</span><span>weights</span>, <span>&amp;</span><span>t</span><span>-&gt;</span><span>fd</span>, <span>&amp;</span><span>t</span><span>-&gt;</span><span>data</span>, <span>&amp;</span><span>t</span><span>-&gt;</span><span>file_size</span>);</p></div></div><div><div><p><span>// allocate the RunState buffers</span></p></div></div><div><div><p><span>malloc_run_state</span>(<span>&amp;</span><span>t</span><span>-&gt;</span><span>state</span>, <span>&amp;</span><span>t</span><span>-&gt;</span><span>config</span>);</p></div></div><div><div><p><span>void</span> <span>free_transformer</span>(<span>Transformer</span><span>*</span> <span>t</span>) {</p></div></div><div><div><p><span>free</span>(<span>t</span><span>-&gt;</span><span>weights</span>.<span>q_tokens</span>);</p></div></div><div><div><p><span>free</span>(<span>t</span><span>-&gt;</span><span>weights</span>.<span>token_embedding_table</span>);</p></div></div><div><div><p><span>if</span>(<span>t</span><span>-&gt;</span><span>weights</span>.<span>wcls</span> <span>!=</span> <span>t</span><span>-&gt;</span><span>weights</span>.<span>q_tokens</span>) { <span>free</span>(<span>t</span><span>-&gt;</span><span>weights</span>.<span>wcls</span>); }</p></div></div><div><div><p><span>// close the memory mapping</span></p></div></div><div><div><p><span>if</span> (<span>t</span><span>-&gt;</span><span>data</span> <span>!=</span> <span>MAP_FAILED</span>) { <span>munmap</span>(<span>t</span><span>-&gt;</span><span>data</span>, <span>t</span><span>-&gt;</span><span>file_size</span>); }</p></div></div><div><div><p><span>if</span> (<span>t</span><span>-&gt;</span><span>fd</span> <span>!=</span> <span>-1</span>) { <span>close</span>(<span>t</span><span>-&gt;</span><span>fd</span>); }</p></div></div><div><div><p><span>// free the RunState buffers</span></p></div></div><div><div><p><span>free_run_state</span>(<span>&amp;</span><span>t</span><span>-&gt;</span><span>state</span>);</p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// neural net blocks; the dynamics of the Transformer</span></p></div></div><div><div><p><span>void</span> <span>rmsnorm</span>(<span>float</span><span>*</span> <span>o</span>, <span>float</span><span>*</span> <span>x</span>, <span>float</span><span>*</span> <span>weight</span>, <span>int</span> <span>size</span>) {</p></div></div><div><div><p><span>// calculate sum of squares</span></p></div></div><div><div><p><span>ss</span> <span>=</span> <span>cblas_sdot</span>(<span>size</span>, <span>x</span>, <span>1.0f</span>, <span>x</span>, <span>1.0f</span>);</p></div></div><div><div><p><span>ACCELRD</span>(<span>ss</span>) <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>j</span> <span>=</span> <span>0</span>; <span>j</span> <span>&lt;</span> <span>size</span>; <span>j</span><span>++</span>) {</p></div></div><div><div><p><span>ACCELS</span>() <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>j</span> <span>=</span> <span>0</span>; <span>j</span> <span>&lt;</span> <span>size</span>; <span>j</span><span>++</span>) {</p></div></div><div><div><p><span>o</span>[<span>j</span>] <span>=</span> <span>weight</span>[<span>j</span>] <span>*</span> (<span>ss</span> <span>*</span> <span>x</span>[<span>j</span>]);</p></div></div><div><div><p><span>void</span> <span>softmax</span>(<span>float</span><span>*</span> <span>x</span>, <span>int</span> <span>size</span>) {</p></div></div><div><div><p><span>// find max value (for numerical stability)</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>1</span>; <span>i</span> <span>&lt;</span> <span>size</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>size</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>x</span>[<span>i</span>] <span>=</span> <span>expf</span>(<span>x</span>[<span>i</span>] <span>-</span> <span>max_val</span>);</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>size</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>void</span> <span>matmul</span>(<span>float</span><span>*</span> <span>xout</span>, <span>QuantizedTensor</span> <span>*</span><span>x</span>, <span>QuantizedTensor</span> <span>*</span><span>w</span>, <span>int</span> <span>n</span>, <span>int</span> <span>d</span>) {</p></div></div><div><div><p><span>// W (d,n) @ x (n,) -&gt; xout (d,)</span></p></div></div><div><div><p><span>// by far the most amount of time is spent inside this little function</span></p></div></div><div><div><p><span>// inputs to this function are both quantized</span></p></div></div><div><div><p><span>ACCEL</span>(<span>i</span>) <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>d</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>// do the matmul in groups of GS</span></p></div></div><div><div><p><span>for</span> (<span>j</span> <span>=</span> <span>0</span>; <span>j</span> &lt;= <span>n</span> <span>-</span> <span>GS</span>; <span>j</span> <span>+=</span> <span>GS</span>) {</p></div></div><div><div><p><span>// unroll the inner loop by a factor of 4</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>k</span> <span>=</span> <span>0</span>; <span>k</span> <span>&lt;</span> <span>GS</span>; <span>k</span> <span>+=</span> <span>4</span>) {</p></div></div><div><div><p><span>ival</span> <span>+=</span> ((<span>int32_t</span>) <span>x</span><span>-&gt;</span><span>q</span>[<span>j</span> <span>+</span> <span>k</span>]) <span>*</span> ((<span>int32_t</span>) <span>w</span><span>-&gt;</span><span>q</span>[<span>in</span> <span>+</span> <span>j</span> <span>+</span> <span>k</span>]);</p></div></div><div><div><p><span>ival</span> <span>+=</span> ((<span>int32_t</span>) <span>x</span><span>-&gt;</span><span>q</span>[<span>j</span> <span>+</span> <span>k</span> <span>+</span> <span>1</span>]) <span>*</span> ((<span>int32_t</span>) <span>w</span><span>-&gt;</span><span>q</span>[<span>in</span> <span>+</span> <span>j</span> <span>+</span> <span>k</span> <span>+</span> <span>1</span>]);</p></div></div><div><div><p><span>ival</span> <span>+=</span> ((<span>int32_t</span>) <span>x</span><span>-&gt;</span><span>q</span>[<span>j</span> <span>+</span> <span>k</span> <span>+</span> <span>2</span>]) <span>*</span> ((<span>int32_t</span>) <span>w</span><span>-&gt;</span><span>q</span>[<span>in</span> <span>+</span> <span>j</span> <span>+</span> <span>k</span> <span>+</span> <span>2</span>]);</p></div></div><div><div><p><span>ival</span> <span>+=</span> ((<span>int32_t</span>) <span>x</span><span>-&gt;</span><span>q</span>[<span>j</span> <span>+</span> <span>k</span> <span>+</span> <span>3</span>]) <span>*</span> ((<span>int32_t</span>) <span>w</span><span>-&gt;</span><span>q</span>[<span>in</span> <span>+</span> <span>j</span> <span>+</span> <span>k</span> <span>+</span> <span>3</span>]);</p></div></div><div><div><p><span>val</span> <span>+=</span> ((<span>float</span>) <span>ival</span>) <span>*</span> <span>w</span><span>-&gt;</span><span>s</span>[(<span>in</span> <span>+</span> <span>j</span>) / <span>GS</span>] <span>*</span> <span>x</span><span>-&gt;</span><span>s</span>[<span>j</span> / <span>GS</span>];</p></div></div><div><div><p><span>void</span> <span>matmul</span>(<span>float</span><span>*</span> <span>xout</span>, <span>QuantizedTensor</span> <span>*</span><span>x</span>, <span>QuantizedTensor</span> <span>*</span><span>w</span>, <span>int</span> <span>n</span>, <span>int</span> <span>d</span>) {</p></div></div><div><div><p><span>// W (d,n) @ x (n,) -&gt; xout (d,)</span></p></div></div><div><div><p><span>// by far the most amount of time is spent inside this little function</span></p></div></div><div><div><p><span>// inputs to this function are both quantized</span></p></div></div><div><div><p><span>ACCEL</span>(<span>i</span>) <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>d</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>// do the matmul in groups of GS</span></p></div></div><div><div><p><span>for</span> (<span>j</span> <span>=</span> <span>0</span>; <span>j</span> &lt;= <span>n</span> <span>-</span> <span>GS</span>; <span>j</span> <span>+=</span> <span>GS</span>) {</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>k</span> <span>=</span> <span>0</span>; <span>k</span> <span>&lt;</span> <span>GS</span>; <span>k</span><span>++</span>) {</p></div></div><div><div><p><span>ival</span> <span>+=</span> ((<span>int32_t</span>) <span>x</span><span>-&gt;</span><span>q</span>[<span>j</span> <span>+</span> <span>k</span>]) <span>*</span> ((<span>int32_t</span>) <span>w</span><span>-&gt;</span><span>q</span>[<span>in</span> <span>+</span> <span>j</span> <span>+</span> <span>k</span>]);</p></div></div><div><div><p><span>val</span> <span>+=</span> ((<span>float</span>) <span>ival</span>) <span>*</span> <span>w</span><span>-&gt;</span><span>s</span>[(<span>in</span> <span>+</span> <span>j</span>) / <span>GS</span>] <span>*</span> <span>x</span><span>-&gt;</span><span>s</span>[<span>j</span> / <span>GS</span>];</p></div></div><div><div><p><span>float</span><span>*</span> <span>forward</span>(<span>Transformer</span><span>*</span> <span>transformer</span>, <span>int</span> <span>token</span>, <span>int</span> <span>pos</span>) {</p></div></div><div><div><p><span>// a few convenience variables</span></p></div></div><div><div><p><span>Config</span><span>*</span> <span>p</span> <span>=</span> <span>&amp;</span><span>transformer</span><span>-&gt;</span><span>config</span>;</p></div></div><div><div><p><span>TransformerWeights</span><span>*</span> <span>w</span> <span>=</span> <span>&amp;</span><span>transformer</span><span>-&gt;</span><span>weights</span>;</p></div></div><div><div><p><span>RunState</span><span>*</span> <span>s</span> <span>=</span> <span>&amp;</span><span>transformer</span><span>-&gt;</span><span>state</span>;</p></div></div><div><div><p><span>int</span> <span>kv_dim</span> <span>=</span> (<span>p</span><span>-&gt;</span><span>dim</span> <span>*</span> <span>p</span><span>-&gt;</span><span>n_kv_heads</span>) / <span>p</span><span>-&gt;</span><span>n_heads</span>;</p></div></div><div><div><p><span>int</span> <span>kv_mul</span> <span>=</span> <span>p</span><span>-&gt;</span><span>n_heads</span> / <span>p</span><span>-&gt;</span><span>n_kv_heads</span>; <span>// integer multiplier of the kv sharing in multiquery</span></p></div></div><div><div><p><span>int</span> <span>hidden_dim</span> <span>=</span>  <span>p</span><span>-&gt;</span><span>hidden_dim</span>;</p></div></div><div><div><p><span>int</span> <span>head_size</span> <span>=</span> <span>dim</span> / <span>p</span><span>-&gt;</span><span>n_heads</span>;</p></div></div><div><div><p><span>// copy the token embedding into x</span></p></div></div><div><div><p><span>memcpy</span>(<span>x</span>, <span>w</span><span>-&gt;</span><span>token_embedding_table</span> <span>+</span> <span>token</span><span>*</span><span>dim</span>, <span>dim</span> <span>*</span> <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>// forward all the layers</span></p></div></div><div><div><p><span>for</span>(<span>int</span> <span>l</span> <span>=</span> <span>0</span>; <span>l</span> <span>&lt;</span> <span>p</span><span>-&gt;</span><span>n_layers</span>; <span>l</span><span>++</span>) {</p></div></div><div><div><p><span>rmsnorm</span>(<span>s</span><span>-&gt;</span><span>xb</span>, <span>x</span>, <span>w</span><span>-&gt;</span><span>rms_att_weight</span> <span>+</span> <span>l</span><span>*</span><span>dim</span>, <span>dim</span>);</p></div></div><div><div><p><span>// qkv matmuls for this position</span></p></div></div><div><div><p><span>quantize</span>(<span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>s</span><span>-&gt;</span><span>xb</span>, <span>dim</span>);</p></div></div><div><div><p><span>matmul</span>(<span>s</span><span>-&gt;</span><span>q</span>, <span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>w</span><span>-&gt;</span><span>wq</span> <span>+</span> <span>l</span>, <span>dim</span>, <span>dim</span>);</p></div></div><div><div><p><span>matmul</span>(<span>s</span><span>-&gt;</span><span>k</span>, <span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>w</span><span>-&gt;</span><span>wk</span> <span>+</span> <span>l</span>, <span>dim</span>, <span>kv_dim</span>);</p></div></div><div><div><p><span>matmul</span>(<span>s</span><span>-&gt;</span><span>v</span>, <span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>w</span><span>-&gt;</span><span>wv</span> <span>+</span> <span>l</span>, <span>dim</span>, <span>kv_dim</span>);</p></div></div><div><div><p><span>// RoPE relative positional encoding: complex-valued rotate q and k in each head</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>dim</span>; <span>i</span><span>+=</span><span>2</span>) {</p></div></div><div><div><p><span>int</span> <span>head_dim</span> <span>=</span> <span>i</span> % <span>head_size</span>;</p></div></div><div><div><p><span>float</span> <span>freq</span> <span>=</span> <span>1.0f</span> / <span>powf</span>(<span>rope_tf</span>, <span>head_dim</span> / (<span>float</span>)<span>head_size</span>);</p></div></div><div><div><p><span>int</span> <span>rotn</span> <span>=</span> <span>i</span> <span>&lt;</span> <span>kv_dim</span> ? <span>2</span> : <span>1</span>; <span>// how many vectors? 2 = q &amp; k, 1 = q only</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>v</span> <span>=</span> <span>0</span>; <span>v</span> <span>&lt;</span> <span>rotn</span>; <span>v</span><span>++</span>) {</p></div></div><div><div><p><span>float</span><span>*</span> <span>vec</span> <span>=</span> <span>v</span> <span>==</span> <span>0</span> ? <span>s</span><span>-&gt;</span><span>q</span> : <span>s</span><span>-&gt;</span><span>k</span>; <span>// the vector to rotate (query or key)</span></p></div></div><div><div><p><span>vec</span>[<span>i</span>]   <span>=</span> <span>v0</span> <span>*</span> <span>fcr</span> <span>-</span> <span>v1</span> <span>*</span> <span>fci</span>;</p></div></div><div><div><p><span>vec</span>[<span>i</span><span>+</span><span>1</span>] <span>=</span> <span>v0</span> <span>*</span> <span>fci</span> <span>+</span> <span>v1</span> <span>*</span> <span>fcr</span>;</p></div></div><div><div><p><span>// save key,value at this time step (pos) to our kv cache</span></p></div></div><div><div><p><span>int</span> <span>loff</span> <span>=</span> <span>l</span> <span>*</span> <span>p</span><span>-&gt;</span><span>seq_len</span> <span>*</span> <span>kv_dim</span>; <span>// kv cache layer offset for convenience</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>key_cache_row</span> <span>=</span> <span>s</span><span>-&gt;</span><span>key_cache</span> <span>+</span> <span>loff</span> <span>+</span> <span>pos</span> <span>*</span> <span>kv_dim</span>;</p></div></div><div><div><p><span>float</span><span>*</span> <span>value_cache_row</span> <span>=</span> <span>s</span><span>-&gt;</span><span>value_cache</span> <span>+</span> <span>loff</span> <span>+</span> <span>pos</span> <span>*</span> <span>kv_dim</span>;</p></div></div><div><div><p><span>memcpy</span>(<span>key_cache_row</span>, <span>s</span><span>-&gt;</span><span>k</span>, <span>kv_dim</span> <span>*</span> <span>sizeof</span>(<span>*</span><span>key_cache_row</span>));</p></div></div><div><div><p><span>memcpy</span>(<span>value_cache_row</span>, <span>s</span><span>-&gt;</span><span>v</span>, <span>kv_dim</span> <span>*</span> <span>sizeof</span>(<span>*</span><span>value_cache_row</span>));</p></div></div><div><div><p><span>// multihead attention. iterate over all heads</span></p></div></div><div><div><p><span>ACCEL</span>(<span>h</span>) <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>h</span> <span>=</span> <span>0</span>; <span>h</span> <span>&lt;</span> <span>p</span><span>-&gt;</span><span>n_heads</span>; <span>h</span><span>++</span>) {</p></div></div><div><div><p><span>// get the query vector for this head</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>q</span> <span>=</span> <span>s</span><span>-&gt;</span><span>q</span> <span>+</span> <span>h</span> <span>*</span> <span>head_size</span>;</p></div></div><div><div><p><span>// attention scores for this head</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>att</span> <span>=</span> <span>s</span><span>-&gt;</span><span>att</span> <span>+</span> <span>h</span> <span>*</span> <span>p</span><span>-&gt;</span><span>seq_len</span>;</p></div></div><div><div><p><span>// iterate over all timesteps, including the current one</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>t</span> <span>=</span> <span>0</span>; <span>t</span> &lt;= <span>pos</span>; <span>t</span><span>++</span>) {</p></div></div><div><div><p><span>// get the key vector for this head and at this timestep</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>k</span> <span>=</span> <span>s</span><span>-&gt;</span><span>key_cache</span> <span>+</span> <span>loff</span> <span>+</span> <span>t</span> <span>*</span> <span>kv_dim</span> <span>+</span> (<span>h</span> / <span>kv_mul</span>) <span>*</span> <span>head_size</span>;</p></div></div><div><div><p><span>// calculate the attention score as the dot product of q and k</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>head_size</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>score</span> /= <span>sqrtf</span>(<span>head_size</span>);</p></div></div><div><div><p><span>// save the score to the attention buffer</span></p></div></div><div><div><p><span>// softmax the scores to get attention weights, from 0..pos inclusively</span></p></div></div><div><div><p><span>// weighted sum of the values, store back into xb</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>xb</span> <span>=</span> <span>s</span><span>-&gt;</span><span>xb</span> <span>+</span> <span>h</span> <span>*</span> <span>head_size</span>;</p></div></div><div><div><p><span>memset</span>(<span>xb</span>, <span>0</span>, <span>head_size</span> <span>*</span> <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>t</span> <span>=</span> <span>0</span>; <span>t</span> &lt;= <span>pos</span>; <span>t</span><span>++</span>) {</p></div></div><div><div><p><span>// get the value vector for this head and at this timestep</span></p></div></div><div><div><p><span>float</span><span>*</span> <span>v</span> <span>=</span> <span>s</span><span>-&gt;</span><span>value_cache</span> <span>+</span> <span>loff</span> <span>+</span> <span>t</span> <span>*</span> <span>kv_dim</span> <span>+</span> (<span>h</span> / <span>kv_mul</span>) <span>*</span> <span>head_size</span>;</p></div></div><div><div><p><span>// get the attention weight for this timestep</span></p></div></div><div><div><p><span>// accumulate the weighted value into xb</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>head_size</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>// final matmul to get the output of the attention</span></p></div></div><div><div><p><span>quantize</span>(<span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>s</span><span>-&gt;</span><span>xb</span>, <span>dim</span>);</p></div></div><div><div><p><span>matmul</span>(<span>s</span><span>-&gt;</span><span>xb2</span>, <span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>w</span><span>-&gt;</span><span>wo</span> <span>+</span> <span>l</span>, <span>dim</span>, <span>dim</span>);</p></div></div><div><div><p><span>// residual connection back into x</span></p></div></div><div><div><p><span>ACCELS</span>() <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>dim</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>rmsnorm</span>(<span>s</span><span>-&gt;</span><span>xb</span>, <span>x</span>, <span>w</span><span>-&gt;</span><span>rms_ffn_weight</span> <span>+</span> <span>l</span><span>*</span><span>dim</span>, <span>dim</span>);</p></div></div><div><div><p><span>// Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))</span></p></div></div><div><div><p><span>// first calculate self.w1(x) and self.w3(x)</span></p></div></div><div><div><p><span>quantize</span>(<span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>s</span><span>-&gt;</span><span>xb</span>, <span>dim</span>);</p></div></div><div><div><p><span>matmul</span>(<span>s</span><span>-&gt;</span><span>hb</span>, <span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>w</span><span>-&gt;</span><span>w1</span> <span>+</span> <span>l</span>, <span>dim</span>, <span>hidden_dim</span>);</p></div></div><div><div><p><span>matmul</span>(<span>s</span><span>-&gt;</span><span>hb2</span>, <span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>w</span><span>-&gt;</span><span>w3</span> <span>+</span> <span>l</span>, <span>dim</span>, <span>hidden_dim</span>);</p></div></div><div><div><p><span>ACCELS</span>() <span>// OMP/OACC Macro</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>hidden_dim</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>// silu(x)=x*(x), where (x) is the logistic sigmoid</span></p></div></div><div><div><p><span>val</span> *= (<span>1.0f</span> / (<span>1.0f</span> <span>+</span> <span>expf</span>(<span>-</span><span>val</span>)));</p></div></div><div><div><p><span>// elementwise multiply with w3(x)</span></p></div></div><div><div><p><span>// final matmul to get the output of the ffn</span></p></div></div><div><div><p><span>quantize</span>(<span>&amp;</span><span>s</span><span>-&gt;</span><span>hq</span>, <span>s</span><span>-&gt;</span><span>hb</span>, <span>hidden_dim</span>);</p></div></div><div><div><p><span>matmul</span>(<span>s</span><span>-&gt;</span><span>xb</span>, <span>&amp;</span><span>s</span><span>-&gt;</span><span>hq</span>, <span>w</span><span>-&gt;</span><span>w2</span> <span>+</span> <span>l</span>, <span>hidden_dim</span>, <span>dim</span>);</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>dim</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>rmsnorm</span>(<span>x</span>, <span>x</span>, <span>w</span><span>-&gt;</span><span>rms_final_weight</span>, <span>dim</span>);</p></div></div><div><div><p><span>// classifier into logits</span></p></div></div><div><div><p><span>quantize</span>(<span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>x</span>, <span>dim</span>);</p></div></div><div><div><p><span>matmul</span>(<span>s</span><span>-&gt;</span><span>logits</span>, <span>&amp;</span><span>s</span><span>-&gt;</span><span>xq</span>, <span>w</span><span>-&gt;</span><span>wcls</span>, <span>dim</span>, <span>p</span><span>-&gt;</span><span>vocab_size</span>);</p></div></div><div><div><p><span>// ----------------------------------------------------------------------------</span></p></div></div><div><div><p><span>// The Byte Pair Encoding (BPE) Tokenizer that translates strings &lt;-&gt; tokens</span></p></div></div><div><div><p><span>TokenIndex</span> <span>*</span><span>sorted_vocab</span>;</p></div></div><div><div><p><span>unsigned <span>int</span></span> <span>max_token_length</span>;</p></div></div><div><div><p><span>unsigned <span>char</span></span> <span>byte_pieces</span>[<span>512</span>]; <span>// stores all single-byte strings</span></p></div></div><div><div><p><span>int</span> <span>compare_tokens</span>(<span>const</span> <span>void</span> <span>*</span><span>a</span>, <span>const</span> <span>void</span> <span>*</span><span>b</span>) {</p></div></div><div><div><p><span>return</span> <span>strcmp</span>(((<span>TokenIndex</span><span>*</span>)<span>a</span>)<span>-&gt;</span><span>str</span>, ((<span>TokenIndex</span><span>*</span>)<span>b</span>)<span>-&gt;</span><span>str</span>);</p></div></div><div><div><p><span>void</span> <span>build_tokenizer</span>(<span>Tokenizer</span><span>*</span> <span>t</span>, <span>char</span><span>*</span> <span>tokenizer_path</span>, <span>int</span> <span>vocab_size</span>) {</p></div></div><div><div><p><span>// i should have written the vocab_size into the tokenizer file... sigh</span></p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>vocab_size</span> <span>=</span> <span>vocab_size</span>;</p></div></div><div><div><p><span>// malloc space to hold the scores and the strings</span></p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>vocab</span> <span>=</span> (<span>char</span><span>*</span><span>*</span>)<span>malloc</span>(<span>vocab_size</span> <span>*</span> <span>sizeof</span>(<span>char</span><span>*</span>));</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>vocab_scores</span> <span>=</span> (<span>float</span><span>*</span>)<span>malloc</span>(<span>vocab_size</span> <span>*</span> <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>sorted_vocab</span> <span>=</span> <span>NULL</span>; <span>// initialized lazily</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>256</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>byte_pieces</span>[<span>i</span> <span>*</span> <span>2</span>] <span>=</span> (<span>unsigned <span>char</span></span>)<span>i</span>;</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>byte_pieces</span>[<span>i</span> <span>*</span> <span>2</span> <span>+</span> <span>1</span>] <span>=</span> <span>&#39;\0&#39;</span>;</p></div></div><div><div><p><span>#if</span> defined (<span>INC_BIN</span>) <span>||</span> defined(<span>STRLIT</span>)</p></div></div><div><div><p><span>// Parse the data from tokenizer_path</span></p></div></div><div><div><p><span>char</span><span>*</span> <span>token_data</span> <span>=</span> <span>tokenizer_path</span>;</p></div></div><div><div><p><span>int</span> <span>token_data_offset</span> <span>=</span> <span>0</span>;</p></div></div><div><div><p><span>// Read the max_token_length from token_data</span></p></div></div><div><div><p><span>memcpy</span>(<span>&amp;</span><span>t</span><span>-&gt;</span><span>max_token_length</span>, <span>token_data</span>, <span>sizeof</span>(<span>int</span>));</p></div></div><div><div><p><span>token_data_offset</span> <span>+=</span> <span>sizeof</span>(<span>int</span>);</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>vocab_size</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>// Read the vocab_scores from token_data</span></p></div></div><div><div><p><span>memcpy</span>(<span>t</span><span>-&gt;</span><span>vocab_scores</span> <span>+</span> <span>i</span>, <span>token_data</span> <span>+</span> <span>token_data_offset</span>, <span>sizeof</span>(<span>float</span>));</p></div></div><div><div><p><span>token_data_offset</span> <span>+=</span> <span>sizeof</span>(<span>float</span>);</p></div></div><div><div><p><span>// Read the length of the vocabulary token</span></p></div></div><div><div><p><span>memcpy</span>(<span>&amp;</span><span>len</span>, <span>token_data</span> <span>+</span> <span>token_data_offset</span>, <span>sizeof</span>(<span>int</span>));</p></div></div><div><div><p><span>token_data_offset</span> <span>+=</span> <span>sizeof</span>(<span>int</span>);</p></div></div><div><div><p><span>// Allocate memory for the vocabulary token and copy the data</span></p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>vocab</span>[<span>i</span>] <span>=</span> (<span>char</span><span>*</span>)<span>malloc</span>(<span>len</span> <span>+</span> <span>1</span>);</p></div></div><div><div><p><span>memcpy</span>(<span>t</span><span>-&gt;</span><span>vocab</span>[<span>i</span>], <span>token_data</span> <span>+</span> <span>token_data_offset</span>, <span>len</span>);</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>vocab</span>[<span>i</span>][<span>len</span>] <span>=</span> <span>&#39;\0&#39;</span>; <span>// add the string terminating token</span></p></div></div><div><div><p><span>token_data_offset</span> <span>+=</span> <span>len</span>;</p></div></div><div><div><p><span>FILE</span> <span>*</span><span>file</span> <span>=</span> <span>fopen</span>(<span>tokenizer_path</span>, <span>&#34;rb&#34;</span>);</p></div></div><div><div><p><span>if</span> (!<span>file</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;couldn&#39;t load %s\n&#34;</span>, <span>tokenizer_path</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>&amp;</span><span>t</span><span>-&gt;</span><span>max_token_length</span>, <span>sizeof</span>(<span>int</span>), <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;failed read\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>vocab_size</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>t</span><span>-&gt;</span><span>vocab_scores</span> <span>+</span> <span>i</span>, <span>sizeof</span>(<span>float</span>), <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;failed read\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>);}</p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>&amp;</span><span>len</span>, <span>sizeof</span>(<span>int</span>), <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;failed read\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>vocab</span>[<span>i</span>] <span>=</span> (<span>char</span> <span>*</span>)<span>malloc</span>(<span>len</span> <span>+</span> <span>1</span>);</p></div></div><div><div><p><span>if</span> (<span>fread</span>(<span>t</span><span>-&gt;</span><span>vocab</span>[<span>i</span>], <span>len</span>, <span>1</span>, <span>file</span>) <span>!=</span> <span>1</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;failed read\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>vocab</span>[<span>i</span>][<span>len</span>] <span>=</span> <span>&#39;\0&#39;</span>; <span>// add the string terminating token</span></p></div></div><div><div><p><span>void</span> <span>free_tokenizer</span>(<span>Tokenizer</span><span>*</span> <span>t</span>) {</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>t</span><span>-&gt;</span><span>vocab_size</span>; <span>i</span><span>++</span>) { <span>free</span>(<span>t</span><span>-&gt;</span><span>vocab</span>[<span>i</span>]); }</p></div></div><div><div><p><span>char</span><span>*</span> <span>decode</span>(<span>Tokenizer</span><span>*</span> <span>t</span>, <span>int</span> <span>prev_token</span>, <span>int</span> <span>token</span>) {</p></div></div><div><div><p><span>char</span> <span>*</span><span>piece</span> <span>=</span> <span>t</span><span>-&gt;</span><span>vocab</span>[<span>token</span>];</p></div></div><div><div><p><span>// following BOS (1) or (2) token, sentencepiece decoder strips any leading whitespace (see PR #89)</span></p></div></div><div><div><p><span>if</span> (<span>prev_token</span> <span>==</span> <span>BOS</span> <span>&amp;&amp;</span> <span>piece</span>[<span>0</span>] <span>==</span> <span>&#39; &#39;</span>) { <span>piece</span><span>++</span>; }</p></div></div><div><div><p><span>// careful, some tokens designate raw bytes, and look like e.g. &#39;&lt;0x01&gt;&#39;</span></p></div></div><div><div><p><span>// parse this and convert and return the actual byte</span></p></div></div><div><div><p><span>if</span> (<span>sscanf</span>(<span>piece</span>, <span>&#34;&lt;0x%02hhX&gt;&#34;</span>, <span>&amp;</span><span>byte_val</span>) <span>==</span> <span>1</span>) {</p></div></div><div><div><p><span>piece</span> <span>=</span> (<span>char</span><span>*</span>)<span>t</span><span>-&gt;</span><span>byte_pieces</span> <span>+</span> <span>byte_val</span> <span>*</span> <span>2</span>;</p></div></div><div><div><p><span>void</span> <span>safe_printf</span>(<span>char</span> <span>*</span><span>piece</span>) {</p></div></div><div><div><p><span>// piece might be a raw byte token, and we only want to print printable chars or whitespace</span></p></div></div><div><div><p><span>// because some of the other bytes can be various control codes, backspace, etc.</span></p></div></div><div><div><p><span>if</span> (<span>piece</span> <span>==</span> <span>NULL</span>) { <span>return</span>; }</p></div></div><div><div><p><span>if</span> (<span>piece</span>[<span>0</span>] <span>==</span> <span>&#39;\0&#39;</span>) { <span>return</span>; }</p></div></div><div><div><p><span>unsigned <span>char</span></span> <span>byte_val</span> <span>=</span> <span>piece</span>[<span>0</span>];</p></div></div><div><div><p><span>if</span> (!(<span>isprint</span>(<span>byte_val</span>) <span>||</span> <span>isspace</span>(<span>byte_val</span>))) {</p></div></div><div><div><p><span>return</span>; <span>// bad byte, don&#39;t print it</span></p></div></div><div><div><p><span>int</span> <span>str_lookup</span>(<span>char</span> <span>*</span><span>str</span>, <span>TokenIndex</span> <span>*</span><span>sorted_vocab</span>, <span>int</span> <span>vocab_size</span>) {</p></div></div><div><div><p><span>// efficiently find the perfect match for str in vocab, return its index or -1 if not found</span></p></div></div><div><div><p><span>TokenIndex</span> <span>tok</span> <span>=</span> { .<span>str</span> <span>=</span> <span>str</span> }; <span>// acts as the key to search for</span></p></div></div><div><div><p><span>TokenIndex</span> <span>*</span><span>res</span> <span>=</span> <span>bsearch</span>(<span>&amp;</span><span>tok</span>, <span>sorted_vocab</span>, <span>vocab_size</span>, <span>sizeof</span>(<span>TokenIndex</span>), <span>compare_tokens</span>);</p></div></div><div><div><p><span>return</span> <span>res</span> <span>!=</span> <span>NULL</span> ? <span>res</span><span>-&gt;</span><span>id</span> : <span>-1</span>;</p></div></div><div><div><p><span>void</span> <span>encode</span>(<span>Tokenizer</span><span>*</span> <span>t</span>, <span>char</span> <span>*</span><span>text</span>, <span>int8_t</span> <span>bos</span>, <span>int8_t</span> <span>eos</span>, <span>int</span> <span>*</span><span>tokens</span>, <span>int</span> <span>*</span><span>n_tokens</span>) {</p></div></div><div><div><p><span>// encode the string text (input) into an upper-bound preallocated tokens[] array</span></p></div></div><div><div><p><span>// bos != 0 means prepend the BOS token, eos != 0 means append the EOS token</span></p></div></div><div><div><p><span>if</span> (<span>text</span> <span>==</span> <span>NULL</span>) { <span>fprintf</span>(<span>stderr</span>, <span>&#34;cannot encode NULL text\n&#34;</span>); <span>exit</span>(<span>EXIT_FAILURE</span>); }</p></div></div><div><div><p><span>if</span> (<span>t</span><span>-&gt;</span><span>sorted_vocab</span> <span>==</span> <span>NULL</span>) {</p></div></div><div><div><p><span>// lazily malloc and sort the vocabulary</span></p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>sorted_vocab</span> <span>=</span> <span>malloc</span>(<span>t</span><span>-&gt;</span><span>vocab_size</span> <span>*</span> <span>sizeof</span>(<span>TokenIndex</span>));</p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span> <span>=</span> <span>0</span>; <span>i</span> <span>&lt;</span> <span>t</span><span>-&gt;</span><span>vocab_size</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>sorted_vocab</span>[<span>i</span>].<span>str</span> <span>=</span> <span>t</span><span>-&gt;</span><span>vocab</span>[<span>i</span>];</p></div></div><div><div><p><span>t</span><span>-&gt;</span><span>sorted_vocab</span>[<span>i</span>].<span>id</span> <span>=</span> <span>i</span>;</p></div></div><div><div><p><span>qsort</span>(<span>t</span><span>-&gt;</span><span>sorted_vocab</span>, <span>t</span><span>-&gt;</span><span>vocab_size</span>, <span>sizeof</span>(<span>TokenIndex</span>), <span>compare_tokens</span>);</p></div></div><div><div><p><span>// create a temporary buffer that will store merge candidates of always two consecutive tokens</span></p></div></div><div><div><p><span>// *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)</span></p></div></div><div><div><p><span>char</span><span>*</span> <span>str_buffer</span> <span>=</span> <span>malloc</span>((<span>t</span><span>-&gt;</span><span>max_token_length</span><span>*</span><span>2</span> <span>+</span><span>1</span> <span>+</span><span>2</span>) <span>*</span> <span>sizeof</span>(<span>char</span>));</p></div></div><div><div><p><span>// add optional BOS token, if desired</span></p></div></div><div><div><p><span>if</span> (<span>bos</span>) <span>tokens</span>[(<span>*</span><span>n_tokens</span>)<span>++</span>] <span>=</span> <span>BOS</span>;</p></div></div><div><div><p><span>// add_dummy_prefix is true by default</span></p></div></div><div><div><p><span>// so prepend a dummy prefix token to the input string, but only if text != &#34;&#34;</span></p></div></div><div><div><p><span>// TODO: pretty sure this isn&#39;t correct in the general case but I don&#39;t have the</span></p></div></div><div><div><p><span>// energy to read more of the sentencepiece code to figure out what it&#39;s doing</span></p></div></div><div><div><p><span>int</span> <span>dummy_prefix</span> <span>=</span> <span>str_lookup</span>(<span>&#34; &#34;</span>, <span>t</span><span>-&gt;</span><span>sorted_vocab</span>, <span>t</span><span>-&gt;</span><span>vocab_size</span>);</p></div></div><div><div><p><span>tokens</span>[(<span>*</span><span>n_tokens</span>)<span>++</span>] <span>=</span> <span>dummy_prefix</span>;</p></div></div><div><div><p><span>// Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:</span></p></div></div><div><div><p><span>// Code point  UTF-8 conversion</span></p></div></div><div><div><p><span>// First code point	Last code point	Byte 1	Byte 2	Byte 3	Byte 4</span></p></div></div><div><div><p><span>// U+0000	U+007F	    0xxxxxxx</span></p></div></div><div><div><p><span>// U+0080	U+07FF	    110xxxxx	10xxxxxx</span></p></div></div><div><div><p><span>// U+0800	U+FFFF	    1110xxxx	10xxxxxx	10xxxxxx</span></p></div></div><div><div><p><span>// U+10000	U+10FFFF    11110xxx	10xxxxxx	10xxxxxx	10xxxxxx</span></p></div></div><div><div><p><span>// process the raw (UTF-8) byte sequence of the input string</span></p></div></div><div><div><p><span>for</span> (<span>char</span> <span>*</span><span>c</span> <span>=</span> <span>text</span>; <span>*</span><span>c</span> <span>!=</span> <span>&#39;\0&#39;</span>; <span>c</span><span>++</span>) {</p></div></div><div><div><p><span>// reset buffer if the current byte is ASCII or a leading byte</span></p></div></div><div><div><p><span>// 0xC0 is 11000000, so (*c &amp; 0xC0) keeps the first 2 bits and zeros the rest</span></p></div></div><div><div><p><span>// in UTF-8, all continuation bytes start with &#34;10&#34; in first two bits</span></p></div></div><div><div><p><span>// so in English this is: &#34;if this byte is not a continuation byte&#34;</span></p></div></div><div><div><p><span>if</span> ((<span>*</span><span>c</span> <span>&amp;</span> <span>0xC0</span>) <span>!=</span> <span>0x80</span>) {</p></div></div><div><div><p><span>// this byte must be either a leading byte (11...) or an ASCII char (0x...)</span></p></div></div><div><div><p><span>// =&gt; reset our location, as we&#39;re starting a new UTF-8 codepoint</span></p></div></div><div><div><p><span>// append the current byte to the buffer</span></p></div></div><div><div><p><span>str_buffer</span>[<span>str_len</span><span>++</span>] <span>=</span> <span>*</span><span>c</span>; <span>// ++ is post-increment, incremented after this line</span></p></div></div><div><div><p><span>str_buffer</span>[<span>str_len</span>] <span>=</span> <span>&#39;\0&#39;</span>;</p></div></div><div><div><p><span>// while the next character is a continuation byte, continue appending</span></p></div></div><div><div><p><span>// but if there are too many of them, just stop to avoid overruning str_buffer size.</span></p></div></div><div><div><p><span>if</span> ((<span>*</span>(<span>c</span><span>+</span><span>1</span>) <span>&amp;</span> <span>0xC0</span>) <span>==</span> <span>0x80</span> <span>&amp;&amp;</span> <span>str_len</span> <span>&lt;</span> <span>4</span>) {</p></div></div><div><div><p><span>// ok c+1 is not a continuation byte, so we&#39;ve read in a full codepoint</span></p></div></div><div><div><p><span>int</span> <span>id</span> <span>=</span> <span>str_lookup</span>(<span>str_buffer</span>, <span>t</span><span>-&gt;</span><span>sorted_vocab</span>, <span>t</span><span>-&gt;</span><span>vocab_size</span>);</p></div></div><div><div><p><span>// we found this codepoint in vocab, add it as a token</span></p></div></div><div><div><p><span>tokens</span>[(<span>*</span><span>n_tokens</span>)<span>++</span>] <span>=</span> <span>id</span>;</p></div></div><div><div><p><span>// byte_fallback encoding: just encode each byte as a token</span></p></div></div><div><div><p><span>// +3 is here because the first 3 vocab elements are &lt;unk&gt;, &lt;s&gt;, &lt;/s&gt;</span></p></div></div><div><div><p><span>// so the individual bytes only start at index 3</span></p></div></div><div><div><p><span>for</span> (<span>int</span> <span>i</span><span>=</span><span>0</span>; <span>i</span> <span>&lt;</span> <span>str_len</span>; <span>i</span><span>++</span>) {</p></div></div><div><div><p><span>tokens</span>[(<span>*</span><span>n_tokens</span>)<span>++</span>] <span>=</span> (<span>unsigned <span>char</span></span>)<span>str_buffer</span>[<span>i</span>] <span>+</span> <span>3</span>;</p></div></div><div><div><p><span>str_len</span> <span>=</span> <span>0</span>; <span>// protect against a sequence of stray UTF8 continuation bytes</span></p></div></div><div><div><p><span>// merge the best consecutive pair or triple each iteration, according to the scores in vocab_scores</span></p></div></div><div><div><p><span>float</span> <span>best_score</span> <span>=</span> <span>-1e10</span>;</p></div></div><div><div><p><span>int</span> <span>best_merge</span> <span>=</span> <span>0</span>; <span>// length of the best merge sequence (2 for pair, 3 for triple)</span></p></div></div></div></div></div></div></div></div></section></div></div>
  </body>
</html>
