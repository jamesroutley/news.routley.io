<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pierce.dev/notes/go-ahead-self-host-postgres#user-content-fn-1">Original</a>
    <h1>Go ahead, self-host Postgres</h1>
    
    <div id="readability-page-1" class="page"><div><p>Self-hosting a database sounds terrifying. That narrative has certainly been pushed over the last 10 years by the big cloud providers:</p>
<ul>
<li>Hosting your own database is dangerous</li>
<li>How are you going to get all the 9s of reliability doing it yourself?</li>
<li>You&#39;ll have access to dedicated database engineers that you couldn&#39;t (or wouldn&#39;t want to) hire yourself</li>
</ul>
<p>The rumors obscure the truth.</p>
<ul>
<li>Most cloud hosts are just running a slightly modified version of the open source Postgres server anyway<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup></li>
<li>Database engineering is not a silver bullet if your queries are sub-optimal. Abstracting away your engine too much from your code doesn&#39;t let you benchmark what&#39;s going on and work around the otherwise reasonable constraints of how the engine is actually querying your code.</li>
</ul>
<p>I&#39;ve had data corruption when using a 3rd party vendor just the same as I&#39;ve had when self-hosting. And with a serious markup, what&#39;s the point?</p>
<p>I&#39;ve been running my own self-hosted postgres for the better part of two years now, serving thousands of users and tens of millions of queries daily<sup><a href="#user-content-fn-5" id="user-content-fnref-5" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. I expected it would give me much more trouble than it has. It&#39;s caused me exactly 30mins of stress during a manual migration and that&#39;s all. Aside from that it&#39;s been fast, stable, and much cheaper.</p>
<p>I sleep just fine at night thank you.</p>
<h2 id="blue-skies-and-the-white-cloud">Blue skies and the white cloud</h2>
<p>Let me rewind for a second. The &#34;database as a service&#34; narrative wasn&#39;t always the dominant one. From the 80s to the early 2000s, everyone ran their own databases because there wasn&#39;t really an alternative. You had your application server and your database server, often on the same physical machine. It was pretty dang fast<sup><a href="#user-content-fn-6" id="user-content-fnref-6" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup> because it was communicating over localhost before forwarding the final payload over the network.</p>
<p>Amazon launched RDS in 2009. The pitch was compelling: we&#39;ll handle backups, patching, high availability, and monitoring. You just connect and go. The early pricing was reasonable too - a small RDS instance cost about the same as a dedicated server, but with less operational overhead. If you had to scale your database specs independent from your web service, it made some good sense to delegate responsibility.</p>
<p>The real shift happened around 2015 when cloud adoption accelerated. Companies started to view <em>any</em> infrastructure management as &#34;undifferentiated heavy lifting&#34;<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup>. Running your own database became associated with legacy thinking. A new orthodoxy emerged of focusing on your application logic and letting AWS handle the infrastructure.</p>
<p>Fast forward to 2025 and I hope the pendulum might be swinging back. RDS pricing has grown considerably more aggressive. A db.r6g.xlarge instance (4 vCPUs, 32GB RAM) now costs $328/month before you add storage, backups, or multi-AZ deployment. For that price, you could rent a dedicated server with 32 cores and 256GB of RAM.</p>
<h2 id="unpacking-the-cloud">Unpacking the cloud</h2>
<p>For the most part managed database services aren&#39;t running some magical proprietary technology. They&#39;re just running the same open-source Postgres you can download with some operational tooling wrapped around it.</p>
<p>Take AWS RDS. Under the hood, it&#39;s:</p>
<ul>
<li>Standard Postgres compiled with some AWS-specific monitoring hooks</li>
<li>A custom backup system using EBS <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html" data-snapshot-id="4196c4ad50694776e515625faa7c1dc1" data-snapshot-url="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html" data-snapshot-date="2025-07-02T08:55:01.569843">snapshots</a></li>
<li>Automated configuration management via Chef/Puppet/Ansible</li>
<li>Load balancers and connection pooling (PgBouncer)</li>
<li>Monitoring integration with CloudWatch</li>
<li>Automated failover scripting</li>
</ul>
<p>None of this is technically complex. The value proposition is operational: they handle the monitoring, alerting, backup verification, and incident response. It&#39;s also a production ready configuration at minute zero of your first deployment. But the actual database engine? It&#39;s the same Postgres running the same SQL queries with the same performance characteristics.</p>
<p>I helped prove this to myself when I migrated off RDS. I took a <code>pg_dump</code> of my RDS instance, restored it to a self-hosted server with identical specs, and ran my application&#39;s test suite. Performance was identical. In some cases, it was actually better because I could tune parameters that RDS locks down.</p>
<h2 id="the-real-operational-complexity">The real operational complexity</h2>
<p>I spent a weekend migrating to a dedicated server from DigitalOcean<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">5</a></sup>: 16 vCPU / 32GB Memory / 400GB disk. The migration took about 4 hours of actual work and then a few weeks to get comfortable with its performance characteristics before migrating our live app.</p>
<p>For my products with high availability requirements, this stack takes me about half an hour per month. For the stacks that get less traffic I&#39;m fully hands off - set it and forget it. This is roughly my cadence for my primary deploys:</p>
<p><strong>Weekly tasks</strong> (10 minutes):</p>
<ul>
<li>Check backup verification (automated, just reviewing alerts)</li>
<li>Review slow query logs</li>
<li>Check disk space trends</li>
</ul>
<p><strong>Monthly tasks</strong> (30 minutes):</p>
<ul>
<li>Apply Postgres security updates</li>
<li>Review and rotate backup retention</li>
<li>Capacity planning based on growth trends</li>
</ul>
<p><strong>Quarterly tasks (optional)</strong> (2 hours):</p>
<ul>
<li>Update monitoring dashboards</li>
<li>Review and optimize configuration parameters</li>
<li>Test disaster recovery procedures</li>
</ul>
<p>The main operational difference is that you&#39;re responsible for incident response. If your database goes down at 3 AM, you need to fix it. But here&#39;s the thing: RDS goes down too<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">6</a></sup>. And when it does, you&#39;re still the one getting paged, you just have fewer tools to fix the problem.</p>
<p>For the most part I&#39;ve found that unless I&#39;m actively messing with the database, it&#39;s really stable. After all you&#39;re just renting some remote machine in a data center somewhere. All updates are up to you - so you have a good idea when to schedule the most risky windows.</p>
<h2 id="when-self-hosting-doesnt-make-sense">When self-hosting doesn&#39;t make sense</h2>
<p>I&#39;d argue self-hosting is the right choice for <em>basically</em> everyone, with the few exceptions at both ends of the extreme:</p>
<ol>
<li>If you&#39;re just starting out in software &amp; want to get something working quickly with vibe coding, it&#39;s easier to treat Postgres as just another remote API that you can call from your single deployed app</li>
<li>If you&#39;re a <em>really</em> big company and are reaching the scale where you need trained database engineers to just work on your stack, you might get economies of scale by just outsourcing that work to a cloud company that has guaranteed talent in that area. The second full freight salaries come into play, outsourcing looks a bit cheaper.</li>
<li>Regulated workloads (PCI-DSS, FedRAMP, HIPAA, etc.) sometimes require a managed platform with signed BAAs or explicit compliance attestations.</li>
</ol>
<h2 id="configurations-to-look-out-for">Configurations to look out for</h2>
<p>There are a lot of knobs and dials that let you tune Postgres <em>exactly</em> how you want it. At a point that power is helpful, but I find that degree of flexibility overwhelming early on. I&#39;ve found the following to be the key things to bear in mind when self-hosting:</p>
<p><strong>Memory Configuration</strong>: This is where most people mess up. Pulling the standard <code>postgres</code> docker image won&#39;t cut it. You have to configure memory bounds with static limits that correspond to hardware. I&#39;ve <a href="https://github.com/piercefreeman/autopg" data-snapshot-id="b9fb9b474ce6673169887d1102a3e4bf" data-snapshot-url="https://github.com/piercefreeman/autopg" data-snapshot-date="2025-07-02T07:58:54.204846">automated</a> some of these configurations. But whether you do it manually or use some auto-config, tweaking these params is a must.</p>
<p>The key parameters:</p>
<ul>
<li><code>shared_buffers</code>: Start around 25 % of RAM; modern PG happily uses tens of GB.</li>
<li><code>effective_cache_size</code>: Set to 75% of system RAM (this tells Postgres how much memory the OS will use for caching)</li>
<li><code>work_mem</code>: Be conservative here. Set it to total RAM / max_connections / 2, or use a fixed value like 32MB</li>
<li><code>maintenance_work_mem</code>: Can be generous (1-2GB), only used during VACUUM and index operations</li>
</ul>
<p><strong>Connection Management</strong>: RDS enforces their own max connections, but when self hosting you get the opportunity to choose your own:</p>
<pre><code># Connection settings
max_connections = 200
shared_preload_libraries = &#39;pg_stat_statements&#39;
log_connections = on
log_disconnections = on
</code></pre>
<p>Wahoo! More connections = more parallelism right?</p>
<p>No such free lunch I&#39;m afraid. Making fresh connections in postgres has pretty expensive overhead, so you almost always want to put a load balancer on front of it. I&#39;m using pgbouncer on all my projects by default - even when load might not call for it. Python asyncio applications just work better with a centralized connection pooler.</p>
<p>And yes, I&#39;ve <a href="https://github.com/piercefreeman/autopg/tree/main/autopgpool" data-snapshot-id="023f5303f782c2b4c1bfd977bb401765" data-snapshot-url="https://github.com/piercefreeman/autopg/tree/main/autopgpool" data-snapshot-date="2025-07-02T08:25:51.770693">automated</a> some of the config there too.</p>
<p><strong>Storage Tuning</strong>: NVMe SSDs make having content on disk less harmful than conventional spinning hard drives, so you&#39;ll want to pay attention to the disk type that you&#39;re hosted on:</p>
<pre><code># Storage optimization for NVMe
random_page_cost = 1.1                 # Down from default 4.0
seq_page_cost = 1.0                    # Keep at default
effective_io_concurrency = 200         # Up from default 1
</code></pre>
<p>These settings tell Postgres that random reads are almost as fast as sequential reads on NVMe drives, which dramatically improves query planning.</p>
<p><strong>WAL Configuration</strong>: Write-Ahead Logging is critical for durability and performance:</p>
<pre><code># WAL settings
wal_level = replica                     # Enable streaming replication
max_wal_size = 2GB                     # Allow larger checkpoints
min_wal_size = 1GB                     # Prevent excessive recycling
checkpoint_completion_target = 0.9      # Spread checkpoint I/O over 90% of interval
</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>I&#39;m not advocating that everyone should self-host everything. But the pendulum has swung too far toward managed services. There&#39;s a large sweet spot where self-hosting makes perfect sense, and more teams should seriously consider it.</p>
<p>Start small. If you&#39;re paying more than $200/month for RDS, spin up a test server and migrate a non-critical database. You might be surprised by how straightforward it is.</p>
<p>The future of infrastructure is almost certainly more hybrid than it&#39;s been recently: managed services where they add genuine value, self-hosted where they&#39;re just expensive abstractions. Postgres often falls into the latter category.</p>
</div></div>
  </body>
</html>
