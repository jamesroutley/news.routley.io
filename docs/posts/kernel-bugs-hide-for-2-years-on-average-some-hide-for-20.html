<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pebblebed.com/blog/kernel-bugs">Original</a>
    <h1>Kernel bugs hide for 2 years on average. Some hide for 20</h1>
    
    <div id="readability-page-1" class="page"><div><!----><p>There are bugs in your kernel right now that won&#39;t be found for years. I know because I analyzed 125,183 of them, every bug with a traceable <code>Fixes:</code> tag in the Linux kernel&#39;s 20-year git history.</p>
<p>The average kernel bug lives <strong>2.1 years</strong> before discovery. But some subsystems are far worse: CAN bus drivers average <strong>4.2 years</strong>, SCTP networking <strong>4.0 years</strong>. The longest-lived bug in my dataset, a buffer overflow in ethtool, sat in the kernel for <strong>20.7 years</strong>. The one which I&#39;ll dissect in detail is refcount leak in netfilter, and it lasted <strong>19 years</strong>.</p>
<p>I built a tool that catches 92% of historical bugs in a held-out test set at commit time. Here&#39;s what I learned.</p>
<table>
<thead>
<tr>
<th>Key findings at a glance</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>125,183</strong></td>
<td>Bug-fix pairs with traceable <code>Fixes:</code> tags</td>
</tr>
<tr>
<td><strong>123,696</strong></td>
<td>Valid records after filtering (0 &lt; lifetime &lt; 27 years)</td>
</tr>
<tr>
<td><strong>2.1 years</strong></td>
<td>Average time a bug hides before discovery</td>
</tr>
<tr>
<td><strong>20.7 years</strong></td>
<td>Longest-lived bug (ethtool buffer overflow)</td>
</tr>
<tr>
<td><strong>0% → 69%</strong></td>
<td>Bugs found within 1 year (2010 vs 2022)</td>
</tr>
<tr>
<td><strong>92.2%</strong></td>
<td>Recall of VulnBERT on held-out 2024 test set</td>
</tr>
<tr>
<td><strong>1.2%</strong></td>
<td>False positive rate (vs 48% for vanilla CodeBERT)</td>
</tr>
</tbody></table>
<h2>The initial discovery</h2>
<p>I started by mining the most recent 10,000 commits with <code>Fixes:</code> tags from the Linux kernel. After filtering out invalid references (commits that pointed to hashes outside the repo, malformed tags, or merge commits), I had <strong>9,876 valid vulnerability records</strong>. For the lifetime analysis, I excluded 27 same-day fixes (bugs introduced and fixed within hours), leaving <strong>9,849 bugs</strong> with meaningful lifetimes.</p>
<p>The results were striking:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Bugs analyzed</td>
<td>9,876</td>
</tr>
<tr>
<td>Average lifetime</td>
<td><strong>2.8 years</strong></td>
</tr>
<tr>
<td>Median lifetime</td>
<td>1.0 year</td>
</tr>
<tr>
<td>Maximum</td>
<td>20.7 years</td>
</tr>
</tbody></table>
<p>Almost 20% of bugs had been hiding for <strong>5+ years</strong>. The networking subsystem looked particularly bad at <strong>5.1 years</strong> average. I found a refcount leak in netfilter that had been in the kernel for <strong>19 years</strong>.</p>
<p><img src="https://pebblebed.com/blog/kernel-bugs/bug_lifetime_histogram.png" alt="Initial Bug Lifetime Distribution"/>
<em>Initial findings: Half of bugs found within a year, but 20% hide for 5+ years.</em></p>
<p>But something nagged at me: my dataset only contained fixes from 2025. Was I seeing the full picture, or just the tip of the iceberg?</p>
<h2>Going deeper: Mining the full history</h2>
<p>I rewrote my miner to capture <strong>every</strong> <code>Fixes:</code> tag since Linux moved to git in 2005. Six hours later, I had 125,183 vulnerability records which was 12x larger than my initial dataset.</p>
<p>The numbers changed significantly:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>2025 Only</th>
<th>Full History (2005-2025)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Bugs analyzed</strong></td>
<td>9,876</td>
<td><strong>125,183</strong></td>
</tr>
<tr>
<td><strong>Average lifetime</strong></td>
<td>2.8 years</td>
<td><strong>2.1 years</strong></td>
</tr>
<tr>
<td><strong>Median lifetime</strong></td>
<td>1.0 year</td>
<td><strong>0.7 years</strong></td>
</tr>
<tr>
<td><strong>5+ year bugs</strong></td>
<td>19.4%</td>
<td><strong>13.5%</strong></td>
</tr>
<tr>
<td><strong>10+ year bugs</strong></td>
<td>6.6%</td>
<td><strong>4.2%</strong></td>
</tr>
</tbody></table>
<p><img src="https://pebblebed.com/blog/kernel-bugs/bug_lifetime_histogram_full.png" alt="Full Dataset Bug Lifetime Distribution"/>
<em>Full history: 57% of bugs found within a year. The long tail is smaller than it first appeared.</em></p>
<p><strong>Why the difference?</strong> My initial 2025-only dataset was biased. Fixes in 2025 include:</p>
<ul>
<li><strong>New bugs</strong> introduced recently and caught quickly</li>
<li><strong>Ancient bugs</strong> that finally got discovered after years of hiding</li>
</ul>
<p>The ancient bugs skewed the average upward. When you include the full history with all the bugs that were introduced AND fixed within the same year, the average drops from 2.8 to 2.1 years.</p>
<h2>The real story: We&#39;re getting faster (but it&#39;s complicated)</h2>
<p>The most striking finding from the full dataset: <strong>bugs introduced in recent years appear to get fixed much faster</strong>.</p>
<table>
<thead>
<tr>
<th>Year Introduced</th>
<th>Bugs</th>
<th>Avg Lifetime</th>
<th>% Found &lt;1yr</th>
</tr>
</thead>
<tbody><tr>
<td>2010</td>
<td>1,033</td>
<td>9.9 years</td>
<td>0%</td>
</tr>
<tr>
<td>2014</td>
<td>3,991</td>
<td>3.9 years</td>
<td>31%</td>
</tr>
<tr>
<td>2018</td>
<td>11,334</td>
<td>1.7 years</td>
<td>54%</td>
</tr>
<tr>
<td>2022</td>
<td>11,090</td>
<td>0.8 years</td>
<td>69%</td>
</tr>
</tbody></table>
<p>Bugs introduced in 2010 took nearly 10 years to find and bugs introduced in 2024 are found in 5 months. At first glance it looks like a 20x improvement!</p>
<p>But here&#39;s the catch: <strong>this data is right-censored</strong>. Bugs introduced in 2022 <em>can&#39;t</em> have a 10-year lifetime yet since we&#39;re only in 2026. We might find more 2022 bugs in 2030 that bring the average up.</p>
<p>The fairer comparison is &#34;% found within 1 year&#34; and that IS improving: from 0% (2010) to 69% (2022). That&#39;s real progress, likely driven by:</p>
<ul>
<li>Syzkaller (released 2015)</li>
<li>KASAN, KMSAN, KCSAN sanitizers</li>
<li>Better static analysis</li>
<li>More contributors reviewing code</li>
</ul>
<p><strong>But there&#39;s a backlog.</strong> When I look at just the bugs fixed in 2024-2025:</p>
<ul>
<li>60% were introduced in the last 2 years (new bugs, caught quickly)</li>
<li>18% were introduced 5-10 years ago</li>
<li><strong>6.5% were introduced 10+ years ago</strong></li>
</ul>
<p>We&#39;re simultaneously catching new bugs faster AND slowly working through ~5,400 ancient bugs that have been hiding for over 5 years.</p>
<h2>The methodology</h2>
<p>The kernel has a convention: when a commit fixes a bug, it includes a <code>Fixes:</code> tag pointing to the commit that introduced the bug.</p>
<pre><code>commit de788b2e6227
Author: Florian Westphal &lt;fw@strlen.de&gt;
Date:   Fri Aug 1 17:25:08 2025 +0200

    netfilter: ctnetlink: fix refcount leak on table dump

    Fixes: d205dc40798d (&#34;netfilter: ctnetlink: ...&#34;)
</code></pre>
<p>I wrote a miner that:</p>
<ol>
<li>Runs <code>git log --grep=&#34;Fixes:&#34;</code> to find all fixing commits</li>
<li>Extracts the referenced commit hash from the <code>Fixes:</code> tag</li>
<li>Pulls dates from both commits</li>
<li>Classifies subsystem from file paths (70+ patterns)</li>
<li>Detects bug type from commit message keywords</li>
<li>Calculates the lifetime</li>
</ol>
<pre><code>fixes_pattern = r&#39;Fixes:\s*([0-9a-f]{12,40})&#39;
match = re.search(fixes_pattern, commit_message)
if match:
    introducing_hash = match.group(1)
    lifetime_days = (fixing_date - introducing_date).days
</code></pre>
<p><strong>Dataset details:</strong></p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Kernel version</td>
<td>v6.19-rc3</td>
</tr>
<tr>
<td>Mining date</td>
<td>January 6, 2026</td>
</tr>
<tr>
<td>Fixes mined since</td>
<td>2005-04-16 (git epoch)</td>
</tr>
<tr>
<td>Total records</td>
<td>125,183</td>
</tr>
<tr>
<td>Unique fixing commits</td>
<td>119,449</td>
</tr>
<tr>
<td>Unique bug-introducing authors</td>
<td>9,159</td>
</tr>
<tr>
<td>With CVE ID</td>
<td>158</td>
</tr>
<tr>
<td>With Cc: stable</td>
<td>27,875 (22%)</td>
</tr>
</tbody></table>
<p><strong>Coverage note:</strong> The kernel has ~448,000 commits mentioning &#34;fix&#34; in some form, but only ~124,000 (28%) use proper <code>Fixes:</code> tags. My dataset captures the well-documented bugs aka the ones where maintainers traced the root cause.</p>
<h2>It varies by subsystem</h2>
<p>Some subsystems have bugs that persist far longer than others:</p>
<table>
<thead>
<tr>
<th>Subsystem</th>
<th>Bug Count</th>
<th>Avg Lifetime</th>
</tr>
</thead>
<tbody><tr>
<td>drivers/can</td>
<td>446</td>
<td><strong>4.2 years</strong></td>
</tr>
<tr>
<td>networking/sctp</td>
<td>279</td>
<td><strong>4.0 years</strong></td>
</tr>
<tr>
<td>networking/ipv4</td>
<td>1,661</td>
<td><strong>3.6 years</strong></td>
</tr>
<tr>
<td>usb</td>
<td>2,505</td>
<td>3.5 years</td>
</tr>
<tr>
<td>tty</td>
<td>1,033</td>
<td>3.5 years</td>
</tr>
<tr>
<td>netfilter</td>
<td>1,181</td>
<td>2.9 years</td>
</tr>
<tr>
<td>networking</td>
<td>6,079</td>
<td>2.9 years</td>
</tr>
<tr>
<td>memory</td>
<td>2,459</td>
<td>1.8 years</td>
</tr>
<tr>
<td>gpu</td>
<td>5,212</td>
<td>1.4 years</td>
</tr>
<tr>
<td>bpf</td>
<td>959</td>
<td><strong>1.1 years</strong></td>
</tr>
</tbody></table>
<p><img src="https://pebblebed.com/blog/kernel-bugs/subsystem_comparison_full.png" alt="Bug Lifetime by Subsystem"/>
<em>CAN bus and SCTP bugs persist longest. BPF and GPU bugs get caught fastest.</em></p>
<p>CAN bus drivers and SCTP networking have bugs that persist longest probably because both are niche protocols with less testing coverage. GPU (especially Intel i915) and BPF bugs get caught fastest, probably thanks to dedicated fuzzing infrastructure.</p>
<p><strong>Interesting finding from comparing 2025-only vs full history:</strong></p>
<table>
<thead>
<tr>
<th>Subsystem</th>
<th>2025-only Avg</th>
<th>Full History Avg</th>
<th>Difference</th>
</tr>
</thead>
<tbody><tr>
<td>networking</td>
<td>5.2 years</td>
<td>2.9 years</td>
<td><strong>-2.3 years</strong></td>
</tr>
<tr>
<td>filesystem</td>
<td>3.8 years</td>
<td>2.6 years</td>
<td>-1.2 years</td>
</tr>
<tr>
<td>drivers/net</td>
<td>3.3 years</td>
<td>2.2 years</td>
<td>-1.1 years</td>
</tr>
<tr>
<td>gpu</td>
<td>1.4 years</td>
<td>1.4 years</td>
<td>0 years</td>
</tr>
</tbody></table>
<p>Networking looked terrible in the 2025-only data (5.2 years!) but is actually closer to average in the full history (2.9 years). The 2025 fixes were catching a backlog of ancient networking bugs. GPU looks the same either way, and those bugs get caught consistently fast.</p>
<h2>Some bug types hide longer than others</h2>
<p>Race conditions are the hardest to find, averaging <strong>5.1 years</strong> to discovery:</p>
<table>
<thead>
<tr>
<th>Bug Type</th>
<th>Count</th>
<th>Avg Lifetime</th>
<th>Median</th>
</tr>
</thead>
<tbody><tr>
<td>race-condition</td>
<td>1,188</td>
<td><strong>5.1 years</strong></td>
<td>2.6 years</td>
</tr>
<tr>
<td>integer-overflow</td>
<td>298</td>
<td><strong>3.9 years</strong></td>
<td>2.2 years</td>
</tr>
<tr>
<td>use-after-free</td>
<td>2,963</td>
<td>3.2 years</td>
<td>1.4 years</td>
</tr>
<tr>
<td>memory-leak</td>
<td>2,846</td>
<td>3.1 years</td>
<td>1.4 years</td>
</tr>
<tr>
<td>buffer-overflow</td>
<td>399</td>
<td>3.1 years</td>
<td>1.5 years</td>
</tr>
<tr>
<td>refcount</td>
<td>2,209</td>
<td>2.8 years</td>
<td>1.3 years</td>
</tr>
<tr>
<td>null-deref</td>
<td>4,931</td>
<td>2.2 years</td>
<td>0.7 years</td>
</tr>
<tr>
<td>deadlock</td>
<td>1,683</td>
<td>2.2 years</td>
<td>0.8 years</td>
</tr>
</tbody></table>
<p>Why do race conditions hide so long? They&#39;re non-deterministic and only trigger under specific timing conditions that might occur once per million executions. Even sanitizers like KCSAN can only flag races they observe.</p>
<p><strong>30% of bugs are self-fixes</strong> where the same person who introduced the bug eventually fixed it. I guess code ownership matters.</p>
<h2>Why some bugs hide longer</h2>
<p><strong>Less fuzzing coverage.</strong> Syzkaller excels at syscall fuzzing but struggles with stateful protocols. Fuzzing netfilter effectively requires generating valid packet sequences that traverse specific connection tracking states.</p>
<p><strong>Harder to trigger.</strong> Many networking bugs require:</p>
<ul>
<li>Specific packet sequences</li>
<li>Race conditions between concurrent flows</li>
<li>Memory pressure during table operations</li>
<li>Particular NUMA topologies</li>
</ul>
<p><strong>Older code with fewer eyes.</strong> Core networking infrastructure like <code>nf_conntrack</code> was written in the mid-2000s. It works, so nobody rewrites it. But &#34;stable&#34; means fewer developers actively reviewing.</p>
<h2>Case study: 19 years in the kernel</h2>
<p>One of the oldest networking bug in my dataset was introduced in <strong>August 2006</strong> and fixed in <strong>August 2025</strong>:</p>
<pre><code>// ctnetlink_dump_table() - the buggy code path
if (res &lt; 0) {
    nf_conntrack_get(&amp;ct-&gt;ct_general);  // increments refcount
    cb-&gt;args[1] = (unsigned long)ct;
    break;
}
</code></pre>
<p><strong>The irony:</strong> Commit <code>d205dc40798d</code> was itself a fix: &#34;[NETFILTER]: ctnetlink: fix deadlock in table dumping&#34;. Patrick McHardy was fixing a deadlock by removing a <code>_put()</code> call. In doing so, he introduced a refcount leak that would persist for 19 years.</p>
<p>The bug: the code doesn&#39;t check if <code>ct == last</code>. If the current entry is the same as the one we already saved, we&#39;ve now incremented its refcount twice but will only decrement it once. The object never gets freed.</p>
<pre><code>// What should have been checked:
if (res &lt; 0) {
    if (ct != last)  // &lt;-- this check was missing for 19 years
        nf_conntrack_get(&amp;ct-&gt;ct_general);
    cb-&gt;args[1] = (unsigned long)ct;
    break;
}
</code></pre>
<p><strong>The consequence:</strong> Memory leaks accumulate. Eventually <code>nf_conntrack_cleanup_net_list()</code> waits forever for the refcount to hit zero. The netns teardown hangs. If you&#39;re using containers, this blocks container cleanup indefinitely.</p>
<p><strong>Why it took 19 years:</strong> You had to run <code>conntrack_resize.sh</code> in a loop for ~20 minutes under memory pressure. The fix commit says: &#34;This can be reproduced by running conntrack_resize.sh selftest in a loop. It takes ~20 minutes for me on a preemptible kernel.&#34; Nobody ran that specific test sequence for two decades.</p>
<h2>Incomplete fixes are common</h2>
<p>Here&#39;s a pattern I keep seeing: someone notices undefined behavior, ships a fix, but the fix doesn&#39;t fully close the hole.</p>
<p><strong>Case study: netfilter set field validation</strong></p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Commit</th>
<th>What happened</th>
</tr>
</thead>
<tbody><tr>
<td>Jan 2020</td>
<td><code>f3a2181e16f1</code></td>
<td>Stefano Brivio adds support for sets with multiple ranged fields. Introduces <code>NFTA_SET_DESC_CONCAT</code> for specifying field lengths.</td>
</tr>
<tr>
<td>Jan 2024</td>
<td><code>3ce67e3793f4</code></td>
<td>Pablo Neira notices the code doesn&#39;t validate that field lengths sum to the key length. Ships a fix. Commit message: &#34;I did not manage to crash nft_set_pipapo with mismatch fields and set key length so far, but this is UB which must be disallowed.&#34;</td>
</tr>
<tr>
<td>Jan 2025</td>
<td><code>1b9335a8000f</code></td>
<td>Security researcher finds a bypass. The 2024 fix was incomplete—there were still code paths that could mismatch. Real fix shipped.</td>
</tr>
</tbody></table>
<p>The 2024 fix was an acknowledgment that something was wrong, but Pablo couldn&#39;t find a crash, so the fix was conservative. A year later, someone found the crash.</p>
<p><strong>This pattern suggests a detection opportunity:</strong> commits that say things like &#34;this is undefined behavior&#34; or &#34;I couldn&#39;t trigger this but...&#34; are flags. The author knows something is wrong but hasn&#39;t fully characterized the bug. These deserve extra scrutiny.</p>
<h2>The anatomy of a long-lived bug</h2>
<p>Looking at the bugs that survive 10+ years, I see common patterns:</p>
<p><strong>1. Reference counting errors</strong></p>
<pre><code>kref_get(&amp;obj-&gt;ref);
// ... error path returns without kref_put()
</code></pre>
<p>These don&#39;t crash immediately. They leak memory slowly. In a long-running system, you might not notice until months later when OOM killer starts firing.</p>
<p><strong>2. Missing NULL checks after dereference</strong></p>
<pre><code>struct foo *f = get_foo();
f-&gt;bar = 1;              // dereference happens first
if (!f) return -EINVAL;  // check comes too late
</code></pre>
<p>The compiler might optimize away the NULL check since you already dereferenced. These survive because the pointer is rarely NULL in practice.</p>
<p><strong>3. Integer overflow in size calculations</strong></p>
<pre><code>size_t total = n_elements * element_size;  // can overflow
buf = kmalloc(total, GFP_KERNEL);
memcpy(buf, src, n_elements * element_size);  // copies more than allocated
</code></pre>
<p>If <code>n_elements</code> comes from userspace, an attacker can cause allocation of a small buffer followed by a large copy.</p>
<p><strong>4. Race conditions in state machines</strong></p>
<pre><code>spin_lock(&amp;lock);
if (state == READY) {
    spin_unlock(&amp;lock);
    // window here where another thread can change state
    do_operation();  // assumes state is still READY
}
</code></pre>
<p>These require precise timing to hit. They might manifest as rare crashes that nobody can reproduce.</p>
<h2>Can we catch these bugs automatically?</h2>
<p>Every day a bug lives in the kernel is another day millions of devices are vulnerable. Android phones, servers, embedded systems, cloud infrastructure, all running kernel code with bugs that won&#39;t be found for years.</p>
<p>I built VulnBERT, a model that predicts whether a commit introduces a vulnerability.</p>
<p><strong>Model evolution:</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Recall</th>
<th>FPR</th>
<th>F1</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td>Random Forest</td>
<td>76.8%</td>
<td>15.9%</td>
<td>0.80</td>
<td>Hand-crafted features only</td>
</tr>
<tr>
<td>CodeBERT (fine-tuned)</td>
<td>89.2%</td>
<td>48.1%</td>
<td>0.65</td>
<td>High recall, unusable FPR</td>
</tr>
<tr>
<td><strong>VulnBERT</strong></td>
<td><strong>92.2%</strong></td>
<td><strong>1.2%</strong></td>
<td><strong>0.95</strong></td>
<td>Best of both approaches</td>
</tr>
</tbody></table>
<p><strong>The problem with vanilla CodeBERT:</strong> I first tried fine-tuning CodeBERT directly. Results: 89% recall but <strong>48% false positive rate</strong> (measured on the same test set). Unusable, flagging half of all commits.</p>
<p>Why so bad? CodeBERT learns shortcuts: &#34;big diff = dangerous&#34;, &#34;lots of pointers = risky&#34;. These correlations exist in training data but don&#39;t generalize. The model pattern-matches on surface features, not actual bug patterns.</p>
<p><strong>The VulnBERT approach:</strong> Combine neural pattern recognition with human domain expertise.</p>
<pre><code>┌─────────────────────────────────────────────────────────────────────┐
│                            INPUT: Git Diff                          │
└───────────────────────────────┬─────────────────────────────────────┘
                                │
                ┌───────────────┴───────────────┐
                ▼                               ▼
┌───────────────────────────┐   ┌───────────────────────────────────┐
│   Chunked Diff Encoder    │   │   Handcrafted Feature Extractor   │
│   (CodeBERT + Attention)  │   │   (51 engineered features)        │
└─────────────┬─────────────┘   └─────────────────┬─────────────────┘
              │ [768-dim]                         │ [51-dim]
              └───────────────┬───────────────────┘
                              ▼
              ┌───────────────────────────────┐
              │     Cross-Attention Fusion    │
              │     &#34;When code looks like X,  │
              │      feature Y matters more&#34;  │
              └───────────────┬───────────────┘
                              ▼
              ┌───────────────────────────────┐
              │        Risk Classifier        │
              └───────────────────────────────┘
</code></pre>
<p><strong>Three innovations that drove performance:</strong></p>
<p><strong>1. Chunked encoding for long diffs.</strong> CodeBERT&#39;s 512-token limit truncates most kernel diffs (often 2000+ tokens). I split into chunks, encode each, then use learned attention to aggregate:</p>
<pre><code># Learnable attention over chunks
chunk_attention = nn.Sequential(
    nn.Linear(hidden_size, hidden_size // 4),
    nn.Tanh(),
    nn.Linear(hidden_size // 4, 1)
)
attention_weights = F.softmax(chunk_attention(chunk_embeddings), dim=1)
pooled = (attention_weights * chunk_embeddings).sum(dim=1)
</code></pre>
<p>The model learns which chunks matter aka the one with <code>spin_lock</code> without <code>spin_unlock</code>, not the boilerplate.</p>
<p><strong>2. Feature fusion via cross-attention.</strong> Neural networks miss domain-specific patterns. I extract 51 handcrafted features using regex and AST-like analysis of the diff:</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Features</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Basic (4)</strong></td>
<td><code>lines_added</code>, <code>lines_removed</code>, <code>files_changed</code>, <code>hunks_count</code></td>
</tr>
<tr>
<td><strong>Memory (3)</strong></td>
<td><code>has_kmalloc</code>, <code>has_kfree</code>, <code>has_alloc_no_free</code></td>
</tr>
<tr>
<td><strong>Refcount (5)</strong></td>
<td><code>has_get</code>, <code>has_put</code>, <code>get_count</code>, <code>put_count</code>, <code>unbalanced_refcount</code></td>
</tr>
<tr>
<td><strong>Locking (5)</strong></td>
<td><code>has_lock</code>, <code>has_unlock</code>, <code>lock_count</code>, <code>unlock_count</code>, <code>unbalanced_lock</code></td>
</tr>
<tr>
<td><strong>Pointers (4)</strong></td>
<td><code>has_deref</code>, <code>deref_count</code>, <code>has_null_check</code>, <code>has_deref_no_null_check</code></td>
</tr>
<tr>
<td><strong>Error handling (6)</strong></td>
<td><code>has_goto</code>, <code>goto_count</code>, <code>has_error_return</code>, <code>has_error_label</code>, <code>error_return_count</code>, <code>has_early_return</code></td>
</tr>
<tr>
<td><strong>Semantic (13)</strong></td>
<td><code>var_after_loop</code>, <code>iterator_modified_in_loop</code>, <code>list_iteration</code>, <code>list_del_in_loop</code>, <code>has_container_of</code>, <code>has_cast</code>, <code>cast_count</code>, <code>sizeof_type</code>, <code>sizeof_ptr</code>, <code>has_arithmetic</code>, <code>has_shift</code>, <code>has_copy</code>, <code>copy_count</code></td>
</tr>
<tr>
<td><strong>Structural (11)</strong></td>
<td><code>if_count</code>, <code>else_count</code>, <code>switch_count</code>, <code>case_count</code>, <code>loop_count</code>, <code>ternary_count</code>, <code>cyclomatic_complexity</code>, <code>max_nesting_depth</code>, <code>function_call_count</code>, <code>unique_functions_called</code>, <code>function_definitions</code></td>
</tr>
</tbody></table>
<p>The key bug-pattern features:</p>
<pre><code>&#39;unbalanced_refcount&#39;: 1,    # kref_get without kref_put → leak
&#39;unbalanced_lock&#39;: 1,        # spin_lock without spin_unlock → deadlock
&#39;has_deref_no_null_check&#39;: 0,# *ptr without if(!ptr) → null deref
&#39;has_alloc_no_free&#39;: 0,      # kmalloc without kfree → memory leak
</code></pre>
<p>Cross-attention learns <em>conditional</em> relationships. When CodeBERT sees locking patterns AND <code>unbalanced_lock=1</code>, that&#39;s HIGH risk. Neither signal alone is sufficient, it&#39;s the combination.</p>
<pre><code># Feature fusion via cross-attention
feature_embedding = feature_projection(handcrafted_features)  # 51 → 768
attended, _ = cross_attention(
    query=code_embedding,      # What patterns does the code have?
    key=feature_embedding,     # What do the hand-crafted features say?
    value=feature_embedding
)
fused = fusion_layer(torch.cat([code_embedding, attended], dim=-1))
</code></pre>
<p><strong>3. Focal loss for hard examples.</strong> The training data is imbalanced where most commits are safe. Standard cross-entropy wastes gradient updates on easy examples. Focal loss:</p>
<pre><code>Standard loss when p=0.95 (easy):  0.05
Focal loss when p=0.95:            0.000125  (400x smaller)
</code></pre>
<p>The model focuses on ambiguous commits: the hard 5% that matter.</p>
<p><strong>Impact of each component</strong> (estimated from ablation experiments):</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>F1 Score</th>
</tr>
</thead>
<tbody><tr>
<td>CodeBERT baseline</td>
<td>~76%</td>
</tr>
<tr>
<td>+ Focal loss</td>
<td>~80%</td>
</tr>
<tr>
<td>+ Feature fusion</td>
<td>~88%</td>
</tr>
<tr>
<td>+ Contrastive learning</td>
<td>~91%</td>
</tr>
<tr>
<td><strong>Full VulnBERT</strong></td>
<td><strong>95.4%</strong></td>
</tr>
</tbody></table>
<p>Note: Individual component impacts are approximate; interactions between components make precise attribution difficult.</p>
<p>The key insight: neither neural networks nor hand-crafted rules alone achieve the best results. The combination does.</p>
<p><strong>Results on temporal validation</strong> (train ≤2023, test 2024):</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Target</th>
<th>Result</th>
</tr>
</thead>
<tbody><tr>
<td>Recall</td>
<td>90%</td>
<td><strong>92.2%</strong> ✓</td>
</tr>
<tr>
<td>FPR</td>
<td>&lt;10%</td>
<td><strong>1.2%</strong> ✓</td>
</tr>
<tr>
<td>Precision</td>
<td>—</td>
<td>98.7%</td>
</tr>
<tr>
<td>F1</td>
<td>—</td>
<td>95.4%</td>
</tr>
<tr>
<td>AUC</td>
<td>—</td>
<td>98.4%</td>
</tr>
</tbody></table>
<p><em>What these metrics mean:</em></p>
<ul>
<li><strong>Recall (92.2%)</strong>: Of all actual bug-introducing commits, we catch 92.2%. Missing 7.8% of bugs.</li>
<li><strong>False Positive Rate (1.2%)</strong>: Of all safe commits, we incorrectly flag 1.2%. Low FPR = fewer false alarms.</li>
<li><strong>Precision (98.7%)</strong>: Of commits we flag as risky, 98.7% actually are. When we raise an alarm, we&#39;re almost always right.</li>
<li><strong>F1 (95.4%)</strong>: Harmonic mean of precision and recall. Single number summarizing overall performance.</li>
<li><strong>AUC (98.4%)</strong>: Area under ROC curve. Measures ranking quality—how well the model separates bugs from safe commits across all thresholds.</li>
</ul>
<p>The model correctly differentiates the <strong>same bug</strong> at different stages:</p>
<table>
<thead>
<tr>
<th>Commit</th>
<th>Description</th>
<th>Risk</th>
</tr>
</thead>
<tbody><tr>
<td><code>acf44a2361b8</code></td>
<td><strong>Fix</strong> for UAF in xe_vfio</td>
<td>12.4% LOW ✓</td>
</tr>
<tr>
<td><code>1f5556ec8b9e</code></td>
<td><strong>Introduced</strong> the UAF</td>
<td>83.8% HIGH ✓</td>
</tr>
</tbody></table>
<h3>What the model sees: The 19-year bug</h3>
<p>When analyzing the bug-introducing commit <code>d205dc40798d</code>:</p>
<pre><code>-    if (ct == last) {
-        nf_conntrack_put(&amp;last-&gt;ct_general);  // removed!
-    }
+    if (ct == last) {
+        last = NULL;
         continue;
     }
     if (ctnetlink_fill_info(...) &lt; 0) {
         nf_conntrack_get(&amp;ct-&gt;ct_general);  // still here
</code></pre>
<p><strong>Extracted features:</strong></p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Value</th>
<th>Signal</th>
</tr>
</thead>
<tbody><tr>
<td><code>get_count</code></td>
<td>1</td>
<td><code>nf_conntrack_get()</code> present</td>
</tr>
<tr>
<td><code>put_count</code></td>
<td>0</td>
<td><code>nf_conntrack_put()</code> was removed</td>
</tr>
<tr>
<td><code>unbalanced_refcount</code></td>
<td><strong>1</strong></td>
<td>Mismatch detected</td>
</tr>
<tr>
<td><code>has_lock</code></td>
<td>1</td>
<td>Uses <code>read_lock_bh()</code></td>
</tr>
<tr>
<td><code>list_iteration</code></td>
<td>1</td>
<td>Uses <code>list_for_each_prev()</code></td>
</tr>
</tbody></table>
<p><strong>Model prediction:</strong> 72% risk: HIGH</p>
<p>The <code>unbalanced_refcount</code> feature fires because <code>_put()</code> was removed but <code>_get()</code> remains. Classic refcount leak pattern.</p>
<h2>Limitations</h2>
<p><strong>Dataset limitations:</strong></p>
<ul>
<li>Only captures bugs with <code>Fixes:</code> tags (~28% of fix commits). Selection bias: well-documented bugs tend to be more serious.</li>
<li>Mainline only, doesn&#39;t include stable-branch-only fixes or vendor patches</li>
<li>Subsystem classification is heuristic-based (regex on file paths)</li>
<li>Bug type detection based on keyword matching in commit messages and many bugs are &#34;unknown&#34; type</li>
<li>Lifetime calculation uses author dates, not commit dates, rebasing can skew timestamps</li>
<li>Some &#34;bugs&#34; may be theoretical (comments like &#34;fix possible race&#34; without confirmed trigger)</li>
</ul>
<p><strong>Model limitations:</strong></p>
<ul>
<li>92.2% recall is on a <strong>held-out 2024 test set</strong>, not a guarantee for future bugs</li>
<li>Can&#39;t catch semantic bugs (logic errors with no syntactic signal)</li>
<li>Cross-function blind spots (bug spans multiple files)</li>
<li>Training data bias (learns patterns from bugs that <em>were found</em>, novel patterns may be missed)</li>
<li>False positives on intentional patterns (init/cleanup in different commits)</li>
<li>Tested only on Linux kernel code, may not generalize to other codebases</li>
</ul>
<p><strong>Statistical limitations:</strong></p>
<ul>
<li>Survivorship bias in year-over-year comparisons (recent bugs can&#39;t have long lifetimes yet)</li>
<li>Correlation ≠ causation for subsystem/bug-type lifetime differences</li>
</ul>
<p><strong>What this means:</strong> VulnBERT is a triage tool, not a guarantee. It catches 92% of bugs with recognizable patterns. The remaining 8% and novel bug classes still need human review and fuzzing.</p>
<h2>What&#39;s next</h2>
<p>92.2% recall with 1.2% FPR is production-ready. But there&#39;s more to do:</p>
<ul>
<li><strong>RL-based exploration</strong>: Instead of static pattern matching, train an agent to explore code paths and find bugs autonomously. The current model predicts risk; an RL agent could <em>generate</em> triggering inputs.</li>
<li><strong>Syzkaller integration</strong>: Use fuzzer coverage as a reward signal. If the model flags a commit and Syzkaller finds a crash in that code path, that&#39;s strong positive signal.</li>
<li><strong>Subsystem-specific models</strong>: Networking bugs have different patterns than driver bugs. A model fine-tuned on netfilter might outperform the general model on netfilter commits.</li>
</ul>
<p>The goal isn&#39;t to replace human reviewers but to point them at the 10% of commits most likely to be problematic, so they can focus attention where it matters.</p>
<h2>Reproducing this</h2>
<p>The dataset extraction uses the kernel&#39;s <code>Fixes:</code> tag convention. Here&#39;s the core logic:</p>
<pre><code>def extract_fixes_tag(commit_msg: str) -&gt; Optional[str]:
    &#34;&#34;&#34;Extract the commit ID from a Fixes: tag&#34;&#34;&#34;
    pattern = r&#39;Fixes:\s*([a-f0-9]{12,40})&#39;
    match = re.search(pattern, commit_msg, re.IGNORECASE)
    return match.group(1) if match else None

# Mine all Fixes: tags from git history
git log --since=&#34;2005-04-16&#34; --grep=&#34;Fixes:&#34; --format=&#34;%H&#34;

# For each fixing commit:
#   - Extract introducing commit hash
#   - Get dates from both commits
#   - Calculate lifetime
#   - Classify subsystem from file paths
</code></pre>
<p>Full miner code and dataset: <a href="https://github.com/quguanni/kernel-vuln-data">github.com/quguanni/kernel-vuln-data</a></p>
<hr/>
<h2>TL;DR</h2>
<ul>
<li><strong>125,183 bugs</strong> analyzed from 20 years of Linux kernel git history (123,696 with valid lifetimes)</li>
<li><strong>Average bug lifetime:</strong> 2.1 years (2.8 years in 2025-only data due to survivorship bias in recent fixes)</li>
<li><strong>0% → 69%</strong> of bugs found within 1 year (2010 vs 2022) (real improvement from better tooling)</li>
<li><strong>13.5% of bugs hide for 5+ years</strong> (these are the dangerous ones)</li>
<li>Race conditions hide longest (<strong>5.1 years</strong> average)</li>
<li><strong>VulnBERT catches 92.2%</strong> of bugs on held-out 2024 test set with only 1.2% FPR (98.4% AUC)</li>
<li><strong>Dataset:</strong> <a href="https://github.com/quguanni/kernel-vuln-data">github.com/quguanni/kernel-vuln-data</a></li>
</ul>
<hr/>
<p><em>If you&#39;re working on kernel security, vulnerability detection, or ML for code analysis, I&#39;d love to talk: <a href="mailto:jenny@pebblebed.com">jenny@pebblebed.com</a></em></p>
<!----></div></div>
  </body>
</html>
