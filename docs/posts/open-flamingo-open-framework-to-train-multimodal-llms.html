<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://laion.ai/blog/open-flamingo/">Original</a>
    <h1>Open Flamingo ‚Äì open framework to train multimodal LLMs</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>Overview.</strong>
We are thrilled to announce the release of OpenFlamingo, an open-source reproduction of DeepMind&#39;s Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs). Check out our <a href="https://github.com/mlfoundations/open_flamingo">GitHub repository</a> and <a href="https://7164d2142d11.ngrok.app">demo</a> to get started!</p>
<p>For this first release, our contributions are as follows:</p>
<ul>
<li>üèãÔ∏è A Python framework to train Flamingo-style LMMs (based on Lucidrains&#39; <a href="https://github.com/lucidrains/flamingo-pytorch">flamingo implementation</a> and David Hansmair&#39;s <a href="https://github.com/dhansmair/flamingo-mini">flamingo-mini repository</a>).</li>
<li>ü™Ö A large-scale multimodal dataset with interleaved image and text sequences.</li>
<li>üß™ An in-context learning evaluation benchmark for vision-language tasks.</li>
<li>ü§ñ A first version of our OpenFlamingo-9B model based on LLaMA, with much better models to come!</li>
</ul>
<p>The recent progress in open-source LMMs with the release of <a href="https://arxiv.org/abs/2301.12597">BLIP-2</a> and <a href="https://jykoh.com/fromage">FROMAGe</a> has shown the exciting potential of multimodal systems. We hope that OpenFlamingo will help drive progress in multimodal machine learning, and we have more exciting contributions in the pipeline, so stay tuned!</p>
<p><strong>Goal.</strong>
Our goal with OpenFlamingo is to develop a multimodal system that can tackle a diverse range of vision-language tasks. Ultimately, we aim to match the power and versatility of GPT-4 in handling visual and text input. To achieve this goal, we are creating an open-source version of <a href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">DeepMind&#39;s Flamingo</a> model, a LMM capable of processing and reasoning about images, videos, and text. We are committed to build fully open-source models, and believe this transparency is essential for fostering collaboration, accelerating progress, and democratizing access to state-of-the-art LMMs. Our release is the first step towards this goal.</p>
<p>We are sharing the first checkpoint of our OpenFlamingo-9B model. While the model is not yet fully optimized, it demonstrates the potential of this project. By working together and receiving feedback from the community, we can train better LMMs. We encourage the community to participate in the development process by providing feedback and contributing to the repository.</p>
<p><strong>Technical Details.</strong>
Our implementation largely follows that of <a href="https://arxiv.org/abs/2204.14198">Flamingo</a>. Flamingo models are trained on large-scale web corpora containing interleaved text and images, which is crucial for endowing them with in-context few-shot learning capabilities. OpenFlamingo implements the same architecture (Perceiver resamplers, cross-attention layers) proposed in the original Flamingo paper. However, since the training data for Flamingo is not available to the public, we use open-source datasets for training our models. Specifically, the released OpenFlamingo-9B checkpoint is trained on 5M samples from our new Multimodal C4 dataset and 10M samples from <a href="https://huggingface.co/datasets/laion/laion2B-en">LAION-2B</a>.</p>
<h2><a id="multimodal-c4" href="#multimodal-c4" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Multimodal C4</strong></h2>
<p>The Multimodal-C4 dataset is an expansion of the text-only <a href="https://www.tensorflow.org/datasets/catalog/c4">C4 dataset</a>, which was used to train  <a href="https://arxiv.org/abs/1910.10683">T5 models</a>. For each document in the <a href="https://www.tensorflow.org/datasets/catalog/c4#c4en_default_config">C4 en.clean</a> dataset, we retrieve the original webpage from <a href="https://commoncrawl.org/">Common Crawl</a>, then collect the downloadable images. Data cleaning is carried out through deduplication and content filtering, which aims to eliminate non-safe for work (NSFW) and unrelated images, such as advertisements. Additionally, we run face detection and discard images with positive identifications. Finally, images and sentences are interleaved using bipartite matching within a document: CLIP ViT/L-14 image-text similarities serve as edge weights. Multimodal-C4 consists of approximately 75 million documents, encompassing around 400M images and 38B tokens. A full release with more detail is coming soon.</p>
<p><img src="https://laion.ai/images/blog/mmc4-example.png" alt=""/></p>
<h2><a id="benchmark" href="#benchmark" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Benchmark</strong></h2>
<p>To measure the performance of OpenFlamingo, we evaluate on a diverse set of downstream tasks. Our aim is to eventually build an open-source version of Flamingo‚Äôs benchmark and extend past that to standardize vision-language task evaluation. Currently we support visual question-answering (<a href="https://visualqa.org/index.html">VQAv2</a>, <a href="https://okvqa.allenai.org">OK-VQA</a>), captioning (<a href="https://cocodataset.org/#home">COCO</a>, <a href="https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset">Flickr30k</a>), and image classification (<a href="https://image-net.org/index.php">ImageNet</a>) tasks. Expect us to add many more evaluation sets that probe model reasoning, biases, and more! You can access the benchmark on the OpenFlamingo repo.</p>
<h2><a id="model-release" href="#model-release" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Model release</strong></h2>
<p><img src="https://laion.ai/images/blog/flamingo-llama.png" alt=""/></p>
<p>As part of our release, we are also providing a checkpoint from our under-development OpenFlamingo-9B, a LMM built on top of <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA 7B</a> and <a href="https://openai.com/research/clip">CLIP ViT/L-14</a>. This model is still a work in progress but it can already bring a lot of value to the community. For instance,</p>
<p><img src="https://laion.ai/images/blog/flamingo-9B-sample-one.png" alt=""/>
<img src="https://laion.ai/images/blog/flamingo-9B-sample-two.png" alt=""/></p>
<p><strong>Performance</strong></p>
<p>We evaluated our checkpoint on COCO and VQAv2. Here we report the validation performance using a different number of shots.</p>
<p>COCO (CIDEr)</p>
<table>
  <tbody><tr>
   <td>
   </td>
   <td>0-shot
   </td>
   <td>4-shot
   </td>
   <td>8-shot
   </td>
   <td>16-shot
   </td>
   <td>32-shot
   </td>
  </tr>
  <tr>
   <td>OpenFlamingo-9B*
   </td>
   <td>65.5
   </td>
   <td>74.3
   </td>
   <td>79.3
   </td>
   <td>81.8
   </td>
   <td>84.5
   </td>
  </tr>
  <tr>
   <td>DeepMind Flamingo-9B
   </td>
   <td>79.4
   </td>
   <td>93.1
   </td>
   <td>99.0
   </td>
   <td>102.2
   </td>
   <td>106.3
   </td>
  </tr>
</tbody></table>
<hr/>
<p>VQAv2 (VQA accuracy)</p>
<table>
  <tbody><tr>
   <td>
   </td>
   <td>0-shot
   </td>
   <td>4-shot
   </td>
   <td>8-shot
   </td>
   <td>16-shot
   </td>
   <td>32-shot
   </td>
  </tr>
  <tr>
   <td>OpenFlamingo-9B*
   </td>
   <td>43.5
   </td>
   <td>44.0
   </td>
   <td>47.5
   </td>
   <td>48.9
   </td>
   <td>50.3
   </td>
  </tr>
  <tr>
   <td>DeepMind Flamingo-9B
   </td>
   <td>51.8
   </td>
   <td>56.3
   </td>
   <td>58.0
   </td>
   <td>59.4
   </td>
   <td>60.4
   </td>
  </tr>
</tbody></table>
<p>*Note that we report validation performance (using the same setup outlined in Flamingo paper) for OpenFlamingo-9B while DeepMind Flamingo-9B performance is on test data.</p>
<p><strong>Safety and ethical considerations</strong></p>
<p>As OpenFlamingo-9B is built on top of frozen <a href="https://arxiv.org/abs/2302.13971">LLaMA</a> and <a href="https://arxiv.org/abs/2103.00020">CLIP</a> models, you can expect OpenFlamingo to inherit the harms of the parent models. We understand that by releasing these models, they may be used in harmful ways. However, it is important for the research community to study the harms of large multimodal models, and we believe that open-sourcing these models will enable the community to develop better ways to mitigate these harms in future models.</p>
<p>We emphasize that OpenFlamingo-9B is a research artifact and not a finished product. It can produce unintended, inappropriate, offensive, and/or inaccurate results. We thus advocate for caution and thorough evaluations before using our models in any real applications.</p>
<h3><a id="contributions" href="#contributions" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Contributions</h3>
<p><strong>Thanks to:</strong></p>
<ul>
<li><a href="https://homes.cs.washington.edu/~jpgard/">Josh Gardner</a> and <a href="https://yonatanbitton.github.io/">Yonatan Bitton</a> for implementing the evaluation benchmark.</li>
<li><a href="https://kalyani7195.github.io/">Kalyani Marathe</a> for implementing the data pipeline and improving code quality.</li>
<li><a href="https://www.linkedin.com/in/yusufhanafy/">Yusuf Hanafy</a> for working on the demo.</li>
<li><a href="https://wanrong-zhu.com/">Wanrong Zhu</a>, <a href="https://jmhessel.com/">Jack Hessel</a>, and <a href="https://sagadre.github.io/">Samir Gadre</a> for building the Multimodal C4 dataset.</li>
<li><a href="https://scholar.google.de/citations?user=p1FuAMkAAAAJ&amp;hl=en">Jenia Jitsev</a> for helping us with large scale training.</li>
<li><a href="https://mitchellnw.github.io/">Mitchell Wortsman</a>, <a href="https://gabrielilharco.com/">Gabriel Ilharco</a>, <a href="https://simonster.com/">Simon Kornblith</a>, <a href="https://koh.pw/">Pang Wei Koh</a> for technical discussions and for feedback on this blog.</li>
<li><a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a> for being our main advisor on this project and for their support.</li>
</ul>
<h3><a id="acknowledgements" href="#acknowledgements" aria-hidden="true"><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements</h3>
<p>This code is based on Lucidrains&#39; <a href="https://github.com/lucidrains/flamingo-pytorch">flamingo implementation</a> and David Hansmair&#39;s <a href="https://github.com/dhansmair/flamingo-mini">flamingo-mini repo</a>. Thank you for making your code public! We also thank the <a href="https://github.com/mlfoundations/open_clip">OpenCLIP</a> team as we use their data loading code and take inspiration from their library design.</p>
<p>We would like to thank <a href="https://www.jbalayrac.com/">Jean-Baptiste Alayrac</a> and <a href="https://antoine77340.github.io/">Antoine Miech</a> for their advice, <a href="https://www.rohantaori.com/">Rohan Taori</a>, <a href="https://nicholasschiefer.com/">Nicholas Schiefer</a>, <a href="https://hai.stanford.edu/people/deep-ganguli">Deep Ganguli</a>, <a href="https://thomasliao.com/">Thomas Liao</a>, <a href="https://thashim.github.io/">Tatsunori Hashimoto</a>, and <a href="https://nicholas.carlini.com/">Nicholas Carlini</a> for their help with assessing the safety risks of our release. This research is supported in part by NSF Institute on the Foundations of Machine Learning (IFML). Thanks to <a href="https://stability.ai">Stability AI</a> for providing us with compute resources to train these models!</p>
</div></div>
  </body>
</html>
