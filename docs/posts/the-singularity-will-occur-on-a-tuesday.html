<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://campedersen.com/singularity">Original</a>
    <h1>The Singularity will occur on a Tuesday</h1>
    
    <div id="readability-page-1" class="page"><div><div><main><article><figure><img src="https://campedersen.com/_next/static/media/always-has-been.7ceff529.jpg" alt="Always has been astronaut meme"/><figcaption><p>&#34;Wait, the singularity is just humans freaking out?&#34; &#34;Always has been.&#34;</p></figcaption></figure>
<p>Everyone in <span>San Francisco</span> is talking about the singularity. At dinner parties, at coffee shops, at the OpenClaw meetup where Ashton Kutcher showed up for some reason. The conversations all have the same shape: someone says it&#39;s coming, someone says it&#39;s hype, and nobody has a number.</p>
<p>This seems like the wrong question. If things are accelerating (and they measurably are) the interesting question isn&#39;t <em>whether</em>. It&#39;s <em>when</em>. And if it&#39;s accelerating, we can calculate <span>exactly when</span>.</p>
<p>I collected five real metrics of AI progress, fit a hyperbolic model to each one independently, and found the one with genuine curvature toward a <span>pole</span>. The date has <span>millisecond precision</span>. There is a countdown.</p>
<p>(I am aware this is <span>unhinged</span>. We&#39;re doing it anyway.)</p>
<!-- -->
<h2>The Data</h2>
<p>Five metrics, chosen for what I&#39;m calling their <em>anthropic significance</em> (anthropic here in the Greek sense (&#34;pertaining to humans&#34;), not the company, though they appear in the dataset with suspicious frequency):</p>
<ol>
<li><strong>MMLU scores</strong>: the SAT for language models</li>
<li><strong>Tokens per dollar</strong>: cost collapse of intelligence (log-transformed, because the Gemini Flash outlier spans 150× the range otherwise)</li>
<li><strong>Frontier release intervals</strong>: shrinking gap between <span>&#34;holy shit&#34;</span> moments</li>
<li><strong>arXiv &#34;emergent&#34; papers</strong> (<span>trailing 12mo</span>): field excitement, measured <span>memetically</span></li>
<li><strong>Copilot code share</strong>: fraction of code written by AI</li>
</ol>
<!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><p>Calibrating instruments...</p><!--/$-->
<p>Each metric <span>normalized</span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0, 1]</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>[</span><span>0</span><span>,</span><span></span><span>1</span><span>]</span></span></span></span>. Release intervals inverted (shorter = better). Tokens per dollar log-transformed before normalizing (the raw values span <span>five orders of magnitude</span>; without the log, Gemini Flash at 2.5M tokens/$ dominates the fit and everything else is noise). Each series keeps its own scale, no merging into a single ensemble.</p>
<!-- -->
<h2>Why Hyperbolic</h2>
<p>Most people extrapolate AI with <span>exponentials</span>. Wrong move!</p>
<p>An exponential <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>a</mi><msup><mi>e</mi><mrow><mi>b</mi><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">f(t) = ae^{bt}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>f</span><span>(</span><span>t</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>a</span><span><span>e</span><span><span><span><span><span><span></span><span><span><span>b</span><span>t</span></span></span></span></span></span></span></span></span></span></span></span> approaches infinity only as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">t \to \infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span></span><span>→</span><span></span></span><span><span></span><span>∞</span></span></span></span>. You&#39;d be waiting forever. Literally.</p>
<p>We need a function that hits infinity at a <em>finite</em> time. That&#39;s the whole point of a singularity: a pole, a <span>vertical asymptote</span>, <span>the math breaking</span>:</p>
<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>k</mi><mrow><msub><mi>t</mi><mi>s</mi></msub><mo>−</mo><mi>t</mi></mrow></mfrac><mo>+</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">x(t) = \frac{k}{t_s - t} + c</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span><span>(</span><span>t</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span>t</span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span>k</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>+</span><span></span></span><span><span></span><span>c</span></span></span></span></p>
<p>As <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>→</mo><msubsup><mi>t</mi><mi>s</mi><mo>−</mo></msubsup></mrow><annotation encoding="application/x-tex">t \to t_s^-</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span></span><span>→</span><span></span></span><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span><span><span></span><span><span>−</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, the denominator goes to zero. <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">x(t) \to \infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span><span>(</span><span>t</span><span>)</span><span></span><span>→</span><span></span></span><span><span></span><span>∞</span></span></span></span>. Not a bug. <span><em>The</em> feature.</span></p>
<!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><p>Calibrating instruments...</p><!--/$-->
<p><strong>Polynomial</strong> growth (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">t^n</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>n</span></span></span></span></span></span></span></span></span></span></span>) never reaches infinity at finite time. You could wait until <span>heat death</span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mn>47</mn></msup></mrow><annotation encoding="application/x-tex">t^{47}</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span><span>47</span></span></span></span></span></span></span></span></span></span></span></span> would still be finite. Polynomials are for people who think AGI is &#34;decades away.&#34;</p>
<p><strong>Exponential</strong> growth reaches infinity at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">t = \infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>t</span><span></span><span>=</span><span></span></span><span><span></span><span>∞</span></span></span></span>. Technically a singularity, but an infinitely patient one. Moore&#39;s Law was exponential. We are no longer on Moore&#39;s Law.</p>
<p><strong>Hyperbolic</strong> growth is what happens when the thing that&#39;s growing <em>accelerates its own growth</em>. Better AI → better AI research tools → better AI → better tools. Positive feedback with <span>supralinear dynamics</span>. The singularity is real and finite.</p>
<!-- -->
<h2>The Fit</h2>
<p>The procedure is straightforward, which should concern you.</p>
<p>The model fits a separate hyperbola to each metric:</p>
<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><msub><mi>k</mi><mi>j</mi></msub><mrow><msub><mi>t</mi><mi>s</mi></msub><mo>−</mo><msub><mi>t</mi><mi>i</mi></msub></mrow></mfrac><mo>+</mo><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">y_i^{(j)} = \frac{k_j}{t_s - t_i} + c_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>i</span></span></span><span><span></span><span><span><span>(</span><span>j</span><span>)</span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>=</span><span></span></span><span><span></span><span><span></span><span><span><span><span><span><span></span><span><span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span>−</span><span><span>t</span><span><span><span><span><span><span></span><span><span>i</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span><span><span></span><span></span></span><span><span></span><span><span><span><span>k</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span><span></span></span><span></span><span>+</span><span></span></span><span><span></span><span><span>c</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></p>
<p>Each series <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>j</span></span></span></span> gets its own <span>scale <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">k_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span> and <span>offset <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. The singularity time <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is shared. MMLU scores and tokens-per-dollar have no business being on the same y-axis, but they can agree on when the pole is.</p>
<p>For each candidate <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, the per-series fits are <span>linear in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">k_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>k</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_j</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>c</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span>. The question is: which <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> makes the hyperbola fit best?</p>
<p>Here&#39;s the thing nobody tells you about fitting singularities: most metrics don&#39;t actually have one. If you minimize total <span>RSS</span> across all series, the best <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is always at infinity. A distant hyperbola <span>degenerates</span> into a line, and lines fit noisy data just fine. The &#34;singularity date&#34; ends up being whatever you set as the search boundary. You&#39;re finding the edge of your search grid, not a singularity.</p>
<p>So instead, we look for the real signal. For each series independently, <span>grid search</span> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and find the <span>R²</span> peak: the date where hyperbolic fits <em>better</em> than any nearby alternative. If a series genuinely curves toward a pole, its R² will peak at some finite <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> and then decline. If it&#39;s really just linear, R² will keep increasing as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">t_s \to \infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span><span></span><span>→</span><span></span></span><span><span></span><span>∞</span></span></span></span> and never peak. No peak, no signal, no vote!</p>
<p><span>One series peaks!</span> arXiv &#34;emergent&#34; (the count of AI papers about emergence) has a clear, unambiguous R² maximum. The other four are <span>monotonically</span> better fit by a line. The singularity date comes from the one metric that&#39;s actually going hyperbolic.</p>
<p>This is more honest than forcing five metrics to average out to a date that none of them individually support.</p>
<p>Same inputs → same date. <span>Deterministic</span>. The <span>stochasticity</span> is in the universe, not the model.</p>
<!-- -->
<h2>The Date</h2>
<!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><p>Calibrating instruments...</p><!--/$-->
<p>The fit <span>converged</span>! Each series has its own <span>R²</span> at the shared <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>, so you can see exactly which metrics the hyperbola captures well and which it doesn&#39;t. arXiv&#39;s R² is the one that matters. It&#39;s the series that actually peaked.</p>
<p>The 95% confidence interval comes from <span>profile likelihood</span> on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. We slide the singularity date forward and backward until the fit degrades past an <span>F-threshold</span>.</p>
<!-- -->
<h2>The Countdown</h2>
<!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><p>Calibrating instruments...</p><!--/$-->
<!-- -->
<h2>Sensitivity</h2>
<p>How much does the date move if we drop one metric entirely?</p>
<!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><p>Calibrating instruments...</p><!--/$-->
<p>If dropping a single series shifts <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> by years, that series was doing all the work. If the shifts are zero, the dropped series never had a signal in the first place.</p>
<p>The table tells the story plainly: arXiv is doing all the work. Drop it and the date jumps to the search boundary (no remaining series has a finite peak). Drop anything else and nothing moves. They were never contributing to the date, only providing context curves at the shared <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>.</p>
<p>Note: Copilot has exactly 2 data points and 2 parameters (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>k</span></span></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>c</span></span></span></span>), so it fits any hyperbola perfectly. Zero RSS, zero influence on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. It&#39;s along for the ride!</p>
<!-- -->
<h2>What <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> Actually Means</h2>
<p>The model says <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">y \to \infty</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>y</span><span></span><span>→</span><span></span></span><span><span></span><span>∞</span></span></span></span> at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span>. But what does &#34;infinity&#34; mean for arXiv papers about emergence? It doesn&#39;t mean infinitely many papers get published on a Tuesday in 2034.</p>
<p>It means the model breaks. <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the point where the current trajectory&#39;s curvature can no longer be sustained. The system either breaks through into something qualitatively new, or it saturates and the hyperbola was wrong. A <span>phase transition marker</span>, not a physical prediction.</p>
<figure><img src="https://campedersen.com/_next/static/media/wile-e-coyote.817e5484.jpg" alt="Wile E. Coyote holding a BYE sign as he falls off a cliff"/><figcaption><p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the moment he looks down.</p></figcaption></figure>
<p>But here&#39;s the part that should unsettle you: <strong>the metric that&#39;s actually going hyperbolic is human attention, not machine capability.</strong></p>
<p>MMLU, tokens per dollar, release intervals. The actual capability and infrastructure metrics. All linear. No pole. No singularity signal. The only curve pointing at a finite date is the count of papers about emergence. Researchers noticing and naming new behaviors. <em>Field excitement, measured memetically.</em></p>
<p>The data says: machines are improving at a constant rate. Humans are freaking out about it at an accelerating rate that accelerates its own acceleration.</p>
<p>That&#39;s a very different singularity than the one people argue about.</p>
<!-- -->
<h2>The Social Singularity</h2>
<p>If <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> marks when the rate of AI surprises exceeds human capacity to process them, the interesting question isn&#39;t what happens to the machines. It&#39;s what happens to us.</p>
<p>And the uncomfortable answer is: <span>it&#39;s already happening.</span></p>
<figure><img src="https://campedersen.com/_next/static/media/this-is-fine.4256436c.jpg" alt="This is fine dog sitting in a burning room"/></figure>
<p><strong>The labor market isn&#39;t adjusting. It&#39;s snapping.</strong> In 2025, <a href="https://fortune.com/2025/12/09/forever-layoffs-job-security-k-shaped-economy-white-collar-recession-challenger-glassdoor/" target="_blank" rel="noreferrer">1.1 million layoffs were announced</a>. Only the sixth time that threshold has been breached since 1993. Over <a href="https://www.cnbc.com/2025/11/04/white-collar-layoffs-ai-cost-cutting-tariffs.html" target="_blank" rel="noreferrer">55,000 explicitly cited AI</a>. But <a href="https://hbr.org/2026/01/companies-are-laying-off-workers-because-of-ais-potential-not-its-performance" target="_blank" rel="noreferrer">HBR found</a> that companies are cutting based on AI&#39;s <em>potential</em>, not its performance. The displacement is anticipatory. The curve doesn&#39;t need to reach the pole. It just needs to <span><em>look like it will</em></span>.</p>
<p><strong>Institutions can&#39;t keep up.</strong> The EU AI Act&#39;s high-risk rules have <a href="https://artificialintelligenceact.eu/" target="_blank" rel="noreferrer">already been delayed to 2027</a>. The US <a href="https://www.metricstream.com/blog/ai-regulation-trends-ai-policies-us-uk-eu.html" target="_blank" rel="noreferrer">revoked its own 2023 AI executive order</a> in January 2025, then issued a new one in December trying to preempt state laws. California and Colorado are <a href="https://www.softwareimprovementgroup.com/blog/us-ai-legislation-overview/" target="_blank" rel="noreferrer">going their own way anyway</a>. The laws being written today regulate 2023&#39;s problems. By the time legislation catches up to GPT-4, we&#39;re on GPT-7. When governments visibly can&#39;t keep up, trust doesn&#39;t erode. It <a href="https://phys.org/news/2025-10-falters-weak-digital-misinformation.html" target="_blank" rel="noreferrer">collapses</a>. Global trust in AI has dropped to 56%.</p>
<p><strong>Capital is concentrating at dot-com levels.</strong> The top 10 S&amp;P 500 stocks (almost all AI-adjacent) hit <a href="https://www.apolloacademy.com/wp-content/uploads/2025/09/ExtremeAIConcentration-090825.pdf" target="_blank" rel="noreferrer">40.7% of index weight</a> in 2025, surpassing the dot-com peak. Since ChatGPT launched, AI-related stocks have captured <a href="https://www.cnbc.com/2025/10/22/your-portfolio-may-be-more-tech-heavy-than-you-think.html" target="_blank" rel="noreferrer">75% of S&amp;P 500 returns, 80% of earnings growth, and 90% of capital spending growth</a>. The <span>Shiller CAPE</span> is at <a href="https://www.ainvest.com/news/market-concentration-500-record-year-history-warns-2026-2512/" target="_blank" rel="noreferrer">39.4</a>. The last time it was this high was <span>1999</span>. The money flooding in doesn&#39;t require AI to actually reach superintelligence. It just requires enough people to believe the curve keeps going up.</p>
<p><strong>People are losing the thread.</strong> Therapists are <a href="https://www.cnbc.com/2026/01/24/ai-artificial-intelligence-worries-therapy.html" target="_blank" rel="noreferrer">reporting a surge</a> in what they&#39;re calling <span>FOBO</span> (Fear of Becoming Obsolete). The clinical language is striking: patients describe it as <em>&#34;the universe saying, &#39;You are no longer needed.&#39;&#34;</em> <a href="https://allwork.space/2026/01/long-term-fears-build-as-60-of-u-s-workers-say-ai-will-cut-more-jobs-than-it-adds-in-2026/" target="_blank" rel="noreferrer">60% of US workers</a> believe AI will cut more jobs than it creates. AI usage is up 13% year-over-year, but confidence in it has <a href="https://fortune.com/2026/01/21/ai-workers-toxic-relationship-trust-confidence-collapses-training-manpower-group/" target="_blank" rel="noreferrer">dropped 18%</a>. <span>The more people use it, the less they trust it.</span></p>
<p><strong>The epistemics are cracking.</strong> Less than <a href="https://research.aimultiple.com/reproducible-ai/" target="_blank" rel="noreferrer">a third of AI research is reproducible</a>. Under 5% of researchers share their code. Corporate labs are publishing less. The gap between what frontier labs know and what the public knows is growing, and the people making policy are operating on information that&#39;s <a href="https://www.cfr.org/articles/how-2026-could-decide-future-artificial-intelligence" target="_blank" rel="noreferrer">already obsolete</a>. The experts who testify before Congress contradict each other, because the field is moving <span>faster than expertise can form.</span></p>
<p><strong>The politics are realigning.</strong> <a href="https://time.com/7371825/trump-data-center-ai-backlash-ai-america-china/" target="_blank" rel="noreferrer">TIME</a> is writing about populist AI backlash. <a href="https://www.foreignaffairs.com/united-states/coming-ai-backlash" target="_blank" rel="noreferrer">Foreign Affairs</a> published &#34;The Coming AI Backlash: How the Anger Economy Will Supercharge Populism.&#34; <a href="https://www.huffpost.com/entry/the-coming-war-over-ai-will-define-the-2026-midterms_n_6953f7fbe4b06a970133dfb7" target="_blank" rel="noreferrer">HuffPost</a> says AI will define the 2026 midterms. MAGA is <a href="https://www.cnn.com/2026/02/02/politics/artificial-intelligence-maga-divide-trump" target="_blank" rel="noreferrer">splitting</a> over whether AI is pro-business or anti-worker. Sanders proposed a data center moratorium. The old left-right axis is buckling under the weight of a question it wasn&#39;t built to answer.</p>
<p>All of this is happening <strong><span>eight years before <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span></span></strong>. The social singularity is <span>front-running</span> the technical one. The institutional and psychological disruption doesn&#39;t wait for capabilities to go vertical. It starts as soon as the <em>trajectory</em> becomes legible.</p>
<p>The pole at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> isn&#39;t when machines become superintelligent. It&#39;s when humans lose the ability to make coherent collective decisions about machines. The actual capabilities are almost beside the point. The social fabric frays at the seams of attention and institutional response time, not at the frontier of model performance.</p>
<!-- -->
<h2>Caveats</h2>
<p><strong>The date comes from one series.</strong> arXiv &#34;emergent&#34; is the only metric with genuine hyperbolic curvature. The other four are better fit by straight lines. The singularity date is really &#34;the date when AI emergence research goes vertical.&#34; Whether field excitement is a <span>leading indicator</span> or a <span>lagging one</span> is the crux of whether this means anything.</p>
<figure><img src="https://campedersen.com/_next/static/media/honest-work.9594c4e9.jpg" alt="It ain&#39;t much, but it&#39;s honest work"/></figure>
<p><strong>The model assumes <span>stationarity</span>.</strong> Like assuming the weather will continue to be &#34;changing.&#34; The curve will bend, either into a <span>logistic</span> (the hype saturates) or into something the model can&#39;t represent (genuine phase transition). <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">t_s</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>t</span><span><span><span><span><span><span></span><span><span>s</span></span></span></span><span>​</span></span><span><span><span></span></span></span></span></span></span></span></span></span> marks where the current regime can&#39;t continue, not what comes after.</p>
<p><strong>MMLU is hitting its ceiling.</strong> Benchmark saturation introduces a <span>leptokurtic compression artifact</span>. MMLU&#39;s low R² reflects this. The hyperbola is <span>the wrong shape</span> for saturating data.</p>
<p><strong>Tokens per dollar is <span>log-transformed</span></strong> (values span five orders of magnitude) <strong>and <span>non-monotonic</span></strong> (GPT-4 cost more than 3.5; Opus 4.5 costs more than DeepSeek-R1). The cost curve isn&#39;t smooth: it&#39;s <span>Pareto advances</span> interspersed with <span>&#34;we spent more on this one.&#34;</span></p>
<p><strong>Five metrics isn&#39;t enough.</strong> More series with genuine hyperbolic curvature would make the date less dependent on arXiv alone. A proper study would add SWE-bench, ARC, GPQA, compute purchases, talent salaries. I used five because <span>five fits in a table</span>.</p>
<p><strong>Copilot has two data points.</strong> Two parameters, two points, zero <span>degrees of freedom</span>, zero RSS contribution. The sensitivity analysis confirms it doesn&#39;t matter.</p>
<!-- -->
<h2>Conclusion</h2>
<p>Real data. Real model. Real date!</p>
<p>The math found one metric curving toward a pole on a specific day at a specific millisecond: the rate at which humans are discovering emergent AI behaviors. The other four metrics are linear. The machines are improving steadily. <span>We are the ones accelerating!</span></p>
<p>The social consequences of that acceleration (labor displacement, institutional failure, capital concentration, epistemic collapse, political realignment) are not predictions for 2034. They are descriptions of 2026. The singularity in the data is a singularity in human attention, and it is already exerting gravitational force on everything it touches.</p>
<p>I see no reason to let <span>epistemological humility</span> interfere with a <span>perfectly good timer</span>.</p>
<span>See you on the other side!</span></article><!--$--><!--/$--></main></div></div></div>
  </body>
</html>
