<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2503.01710">Original</a>
    <h1>Spark-TTS: Text-2-Speech Model Single-Stream Decoupled Tokens [pdf]</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+X" rel="nofollow">Xinsheng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+M" rel="nofollow">Mingqi Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+Z" rel="nofollow">Ziyang Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z" rel="nofollow">Ziyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+S" rel="nofollow">Songxiang Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+L" rel="nofollow">Linqin Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+Z" rel="nofollow">Zheng Liang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Q" rel="nofollow">Qixi Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+R" rel="nofollow">Rui Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+X" rel="nofollow">Xiaoqin Feng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bian,+W" rel="nofollow">Weizhen Bian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+Z" rel="nofollow">Zhen Ye</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+S" rel="nofollow">Sitong Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+R" rel="nofollow">Ruibin Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Z" rel="nofollow">Zhixian Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+X" rel="nofollow">Xinfa Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+J" rel="nofollow">Jiahao Pan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+L" rel="nofollow">Liumeng Xue</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+P" rel="nofollow">Pengcheng Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Y" rel="nofollow">Yunlin Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z" rel="nofollow">Zhifei Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X" rel="nofollow">Xie Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+L" rel="nofollow">Lei Xie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+Y" rel="nofollow">Yike Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+W" rel="nofollow">Wei Xue</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2503.01710">View PDF</a>
    <a href="https://arxiv.org/html/2503.01710v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at <a href="https://github.com/SparkAudio/Spark-TTS" rel="external noopener nofollow">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Xinsheng Wang [<a href="https://arxiv.org/show-email/244045c4/2503.01710" rel="nofollow">view email</a>]      </p></div></div>
  </body>
</html>
