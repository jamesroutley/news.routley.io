<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ss.ekzhang.com/p/jax-js-an-ml-library-for-the-web">Original</a>
    <h1>Show HN: Jax-JS, array library in JavaScript targeting WebGPU</h1>
    
    <div id="readability-page-1" class="page"><div><div dir="auto"><p><span>Iâ€™m excited to release </span><a href="https://jax-js.com/" rel="">jax-js</a><span>, a machine learning library for the web.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!7bb1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!7bb1!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png 424w, https://substackcdn.com/image/fetch/$s_!7bb1!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png 848w, https://substackcdn.com/image/fetch/$s_!7bb1!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png 1272w, https://substackcdn.com/image/fetch/$s_!7bb1!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!7bb1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png" width="446" height="167.80936454849498" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:450,&#34;width&#34;:1196,&#34;resizeWidth&#34;:446,&#34;bytes&#34;:26596,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:&#34;https://ekzhang.substack.com/i/179060245?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!7bb1!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png 424w, https://substackcdn.com/image/fetch/$s_!7bb1!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png 848w, https://substackcdn.com/image/fetch/$s_!7bb1!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png 1272w, https://substackcdn.com/image/fetch/$s_!7bb1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1466b600-4dc2-4a2c-9639-843f5a1c700a_1196x450.png 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a></figure></div><p><span>You can think of it as a reimplementation of Google DeepMindâ€™s </span><a href="https://docs.jax.dev/en/latest/" rel="">JAX</a><span> framework (similar to PyTorch) in pure JavaScript.</span></p><p><span>jax-js runs completely in the browser by generating fast </span><a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API" rel="">WebGPU</a><span> and </span><a href="https://webassembly.org/" rel="">Wasm</a><span> kernels.</span></p><p>Starting in February this year, I spent nights and weekends working on a new ML library for the browser. I wanted a cross-platform way to run numerical programs on the frontend web, so you can do machine learning.</p><p><span>Python and JavaScript are the </span><a href="https://survey.stackoverflow.co/2025/technology#most-popular-technologies-language" rel="">most popular languages</a><span> in the world:</span></p><ol><li><p><strong>JavaScript</strong><span> is the language of the web.</span></p></li><li><p><strong>Python</strong><span> is simple, expressive and now ubiquitous in ML thanks to frameworks like PyTorch and JAX.</span></p></li></ol><p>But most developers would balk at running any number crunching in JavaScript. While the JavaScript JIT is really good, itâ€™s not optimized for tight numerical loops. JavaScript doesnâ€™t even have a fast, native integer data type! So how can you run fast numerical code on the web?</p><p>The answer is to rely on new browser technologies â€” WebAssembly and WebGPU, which allow you to run programs at near-native speeds. WebAssembly is a low-level portable bytecode, and WebGPU is GPU shaders on the web.</p><p><span>If we can use these native runtimes, then this lends itself to a programming model similar to JAX, where you </span><em>trace</em><span> programs and </span><em>JIT</em><span> </span><em>compile</em><span> them to GPU kernels. Here, instead of Nvidia CUDA, we write pure JavaScript to generate WebAssembly and WebGPU kernels. Then we can run them and execute instructions at near-native speed, skipping the JavaScript interpreter bottleneck.</span></p><p>That is what I ended up doing in jax-js, and now it â€œjust worksâ€.</p><p>You can install jax-js as a library. It has 0 dependencies and is pure JS.</p><pre><code>npm install @jax-js/jax</code></pre><p>Then you can use it with an API almost identical to JAX.</p><pre><code>import { numpy as np } from &#34;@jax-js/jax&#34;;

const ar = np.array([1, 5, 6, 7]);
console.log(ar.mul(10).js());  // -&gt; [10, 50, 60, 70]</code></pre><p>Under the hood, this generates a WebAssembly kernel and dispatches it.</p><blockquote><p><strong>Note:</strong><span> There are some surface-level syntax differences here, versus JAX:</span></p><ul><li><p><span>JavaScript doesnâ€™t have operator overloading like Python. Instead of </span><code>ar * 10</code><span> in Python, you have to call </span><code>ar.mul(10)</code><span>.</span></p></li><li><p><span>The </span><code>.js()</code><span> method converts a jax.Array object back into a plain JS array.</span></p></li><li><p><span>JS has no reference-counted destructor method to free memory, so array values in jax-js have </span><a href="https://doc.rust-lang.org/rust-by-example/scope/move.html" rel="">move semantics</a><span> like Rust, with </span><code>.ref</code><span> incrementing their reference counts.</span></p></li></ul></blockquote><p>If youâ€™d like to use WebGPU, just start your program with:</p><pre><code>import { init, setDevice } from &#34;@jax-js/jax&#34;;

await init(&#34;webgpu&#34;);
setDevice(&#34;webgpu&#34;);</code></pre><p><span>You can leverage grad, vmap, and other features of JAX. Hereâ€™s automatic differentiation with </span><code>grad()</code><span>:</span></p><pre><code>import { grad, numpy as np } from â€œ@jax-js/jaxâ€;

const f = (x: np.Array) =&gt; np.sqrt(x.ref.mul(x).sum());
const df = grad(f);

const x = np.array([1, 2, 3, 4]);
console.log(df(x).js());</code></pre><p><span>And hereâ€™s an example the compiler fusing operations with </span><code>jit()</code><span>. The following function gets translated into a compiled GPU compute kernel:</span></p><pre><code>import { jit, numpy as np } from &#34;@jax-js/jax&#34;;

const f = jit((x: np.Array) =&gt; {
  return np.sqrt(x.add(2).mul(Math.PI)).sum();
});</code></pre><p>With these simple building blocks, you can implement most machine learning algorithms and backpropagate through them.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8Tz2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8Tz2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png 424w, https://substackcdn.com/image/fetch/$s_!8Tz2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png 848w, https://substackcdn.com/image/fetch/$s_!8Tz2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png 1272w, https://substackcdn.com/image/fetch/$s_!8Tz2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!8Tz2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png" width="1456" height="554" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:554,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:366314,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://ekzhang.substack.com/i/179060245?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8Tz2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png 424w, https://substackcdn.com/image/fetch/$s_!8Tz2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png 848w, https://substackcdn.com/image/fetch/$s_!8Tz2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png 1272w, https://substackcdn.com/image/fetch/$s_!8Tz2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a346a77-cf0a-407d-aecc-ec882cd2ebe0_3442x1310.png 1456w" sizes="100vw" loading="lazy"/></picture><div></div></div></a></figure></div><p><span>Here is </span><a href="https://jax-js.com/mnist" rel="">a runnable example</a><span> of training a neural network from scratch on MNIST dataset in your browser. It learns to &gt;99% accuracy in seconds, and everything from dataset loading to matmul kernels is </span><em>pure frontend JavaScript code</em><span>.</span></p><p><span>Itâ€™s remarkable to write ML programs with hot module reloading. You can edit code in real time </span><em>while</em><span> the model is training!</span></p><p>â€”</p><p><span>You can also build applications. </span><a href="https://jax-js.com/mobileclip" rel="">Hereâ€™s a demo I built yesterday</a><span>: download the whole text of </span><em>Great Expectations</em><span> (180,000 words), run it through a CLIP-based embedding model, and semantic search it in real timeâ€”all from your browser.</span></p><div data-component-name="VideoEmbedPlayer" id="media-e2749108-67b6-417e-a78d-9a55826b1f1b"></div><p><em>(The text embedding actually runs at a respectable ~500 GFLOP/s on my M1 Pro with just jax.jit(), despite me not having tried to optimize it at all yet. Not bad, crunching 500,000,000,000 calculations/second in browser on a 4-year-old laptop!)</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!bRAB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!bRAB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png 424w, https://substackcdn.com/image/fetch/$s_!bRAB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png 848w, https://substackcdn.com/image/fetch/$s_!bRAB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png 1272w, https://substackcdn.com/image/fetch/$s_!bRAB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!bRAB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png" width="610" height="246.7651098901099" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:589,&#34;width&#34;:1456,&#34;resizeWidth&#34;:610,&#34;bytes&#34;:263760,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://ekzhang.substack.com/i/179060245?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!bRAB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png 424w, https://substackcdn.com/image/fetch/$s_!bRAB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png 848w, https://substackcdn.com/image/fetch/$s_!bRAB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png 1272w, https://substackcdn.com/image/fetch/$s_!bRAB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bad2a9e-a8ab-4041-a13c-79f5f6abf67d_2520x1020.png 1456w" sizes="100vw" loading="lazy"/></picture><div></div></div></a><figcaption>Running with batch size 16 (x77 token context), each CLIP transformer inference takes 200 ms, for an estimated 485 GFLOP/s end-to-end.</figcaption></figure></div><p><span>For a lot of inference use cases, you might find a â€œmodel runtimeâ€ like </span><a href="https://onnxruntime.ai/docs/tutorials/web/" rel="">ONNX</a><span> to add prebuilt ML models to your browser, where the ML developers hand off pre-packaged weights to be used in product. With jax-js, itâ€™s a bit different, and Iâ€™m imagining how a full ML framework, usually relegated to the backend, can run in a browser.</span></p><p><span>As for performance, it hasnâ€™t been my primary focus so far, as just â€œgetting the ML framework workingâ€ comes first. I have checked that jax-jsâ€™s generated kernels for </span><a href="https://jax-js.com/bench/matmul" rel="">matmuls are fast</a><span> (&gt;3 TFLOP on Macbook M4 Pro). But thereâ€™s a lot of room to improve (e.g., </span><a href="https://jax-js.com/bench/conv2d" rel="">conv2d</a><span> is slow), and I havenâ€™t done much optimization work on transformer inference in particular yet. Thereâ€™s plenty of low-hanging fruit.</span></p><h2>Project release<div><div></div></div></h2><p><span>I am open-sourcing jax-js today at </span><a href="https://github.com/ekzhang/jax-js" rel="">ekzhang/jax-js</a><span>.</span></p><p>There are rough edges in this initial release, but itâ€™s ready to try out now.</p><p>Links:</p><ul><li><p><a href="https://jax-js.com" rel="">Website</a></p><ul><li><p><a href="https://jax-js.com/repl" rel="">Try it out! (REPL)</a></p></li><li><p><a href="https://jax-js.com/docs/" rel="">API reference</a></p></li></ul></li><li><p><a href="https://github.com/ekzhang/jax-js" rel="">GitHub repository</a></p></li></ul><p>I look forward to seeing what you create. ğŸ¥°</p><p>â€</p><p>â€</p><p>â€</p><p>â€</p><p>â€</p><h2>Appendix<div><div></div></div></h2><p>This is a personal project and not related to Thinking Machines Lab. I started working on jax-js before starting my current job, and in a way, itâ€™s partly how I ended up working in ML. Turns out this stuff is kind of fun!</p><p>If youâ€™re still reading, helloâ€”I have a bunch more details to share.</p><h3>Acknowledgements<div><div></div></div></h3><p>Thanks to:</p><ul><li><p><span>The authors of </span><a href="https://docs.jax.dev/en/latest/" rel="">JAX</a><span> for making an important ML library thatâ€™s a joy to use.</span></p><ul><li><p><span>Thanks to Matthew Johnson, Dougal Maclaurin, and others for </span><a href="https://docs.jax.dev/en/latest/autodidax.html" rel="">Autodidax</a><span>, an instructive implementation of the JAX core from scratch.</span></p></li><li><p>And thanks for all of the JAX ecosystem libraries as well.</p></li></ul></li><li><p><a href="https://github.com/tinygrad/tinygrad" rel="">Tinygrad</a><span> for a very excellent autograd library â€” you showed that code-generating kernels from scratch canâ€™t really be that </span><em>intrinsically</em><span> complex!</span></p><ul><li><p><span>Many parts of jax-js in the backend internals follow Tinygradâ€™s design closely. The biggest example of this is </span><a href="https://github.com/ekzhang/jax-js/blob/jax/v0.0.5/src/shape.ts" rel="">ShapeTracker</a><span>, which was directly ported.</span></p></li></ul></li><li><p><span>Chrome, Safari, and Firefox for WebGPU, now </span><a href="https://chromestatus.com/metrics/feature/timeline/popularity/3888" rel="">used in 2% of all websites</a><span>.</span></p></li><li><p>The open-source community, for inspiration and for showing that ML on the web is actually possible!</p><ul><li><p><a href="https://www.tensorflow.org/js" rel="">TensorFlow.js</a></p></li><li><p><a href="https://www.npmjs.com/package/onnxruntime-web" rel="">onnxruntime-web</a></p></li><li><p><a href="https://github.com/praeclarum/webgpu-torch" rel="">webgpu-torch</a><span>, </span><a href="https://github.com/zanussbaum/surfgrad" rel="">surfgrad</a><span>, and </span><a href="https://jott.live/markdown/mm_wasm" rel="">wasmblr</a></p></li><li><p><a href="https://github.com/mrdoob/three.js/wiki/Three.js-Shading-Language" rel="">Three.js Shading Language</a><span> (</span><a href="https://github.com/holtsetio/flow/blob/master/src/mls-mpm/mlsMpmSimulator.js" rel="">example</a><span>)</span></p></li></ul></li><li><p><a href="https://pytorch.org/" rel="">PyTorch</a><span>, </span><a href="https://github.com/ml-explore/mlx" rel="">MLX</a><span>, and </span><a href="https://numpy.org/" rel="">NumPy</a></p></li></ul><h3>How it works: An overview of internals<div><div></div></div></h3><p>In general, I think there are roughly two parts to an ML library:</p><ol><li><p><strong>â€œFrontendâ€ (think JAX):</strong><span> The interface for creating and manipulating arrays, the autograd engine, JIT, typing and transformations. Also where you interact with a sync/async boundary and how you track memory allocations.</span></p></li><li><p><strong>â€œBackendâ€ (think XLA):</strong><span> Actual kernels for executing operations. The frontend has some kind of representation of a kernel, it dispatches it to the backend, which then optimizes it, compiles it down to native code (CPU or GPU) and runs it very efficiently.</span></p></li></ol><p><span>This dichotomy obviously isnâ€™t perfect (e.g., where do </span><a href="https://github.com/triton-lang/triton" rel="">Triton</a><span>/</span><a href="https://docs.jax.dev/en/latest/pallas/" rel="">Pallas</a><span> fit in? how about warp-specialized </span><a href="https://docs.nvidia.com/cuda/cutile-python/" rel="">cuTile</a><span>?), and there are certainly concerns that span both parts. But itâ€™s how jax-js works.</span></p><p><strong>Letâ€™s start with the backend and build our way up.</strong><span> In jax-js, the backend code is actually quite self-contained; they implement the Backend interface (abridged):</span></p><pre><code>/** A device backend. */
export interface Backend {
  /** Allocate a new slot with reference count 1. */
  malloc(size: number, initialData?: Uint8Array): Slot;

  /** Increment the reference count of the slot. */
  incRef(slot: Slot): void;

  /**
   * Decrement the reference count of the slot. If the reference count reaches
   * zero, it is freed. This should throw if the slot was already freed.
   */
  decRef(slot: Slot): void;

  /** Read a range of bytes from a buffer. */
  read(
    slot: Slot,
    start?: number,
    count?: number,
  ): Promise&lt;Uint8Array&lt;ArrayBuffer&gt;&gt;;

  /** Prepare an expression to be executed later. */
  prepare(kernel: Kernel): Promise&lt;Executable&gt;;

  /**
   * Run a backend operation that was previously prepared.
   *
   * The operation may not run immediately, but operations are guaranteed to run
   * in the dispatch order. Also, `read()` will wait for all pending operations
   * on that slot to finish.
   */
  dispatch(exe: Executable, inputs: Slot[], outputs: Slot[]): void;
}</code></pre><p><span>In other words, backends need to be able to malloc/free chunks of memory for tensors, and to execute </span><code>Kernel</code><span> objects. Inside a </span><code>Kernel</code><span> there is:</span></p><ul><li><p>A pointwise operation on one or more tensors, with</p></li><li><p>Lazy shape-tracking information for how to index the tensors, and</p></li><li><p><span>A reduction to be performed (optional).</span></p></li></ul><p><span>The pointwise operation is constructed from a pure expression tree, an </span><code>AluExp</code><span>, where each node is a symbolic </span><code>AluOp</code><span>. There are 28 AluOps â€” you donâ€™t need so many distinct operations when you can depend on kernel fusion!</span></p><p>Note that no automatic differentiation happens here; these are pure low-level operations, so we can introduce arbitrary building blocks this way.</p><pre><code>/** Symbolic form for each mathematical operation. */
export enum AluOp {
  Add = â€œAddâ€,
  Sub = â€œSubâ€,
  Mul = â€œMulâ€,
  Idiv = â€œIdivâ€,
  Mod = â€œModâ€,
  Min = â€œMinâ€,
  Max = â€œMaxâ€,

  Sin = â€œSinâ€,
  Cos = â€œCosâ€,
  Asin = â€œAsinâ€,
  Atan = â€œAtanâ€,
  Exp = â€œExpâ€,
  Log = â€œLogâ€,
  Erf = â€œErfâ€,
  Erfc = â€œErfcâ€,
  Sqrt = â€œSqrtâ€,
  Reciprocal = â€œReciprocalâ€,
  Cast = â€œCastâ€,
  Bitcast = â€œBitcastâ€,

  Cmplt = â€œCmpltâ€,
  Cmpne = â€œCmpneâ€,
  Where = â€œWhereâ€, // Ternary operator: `cond ? a : b`

  Threefry2x32 = â€œThreefry2x32â€, // PRNG operation, arg = â€˜xorâ€™ | 0 | 1

  // Const is a literal constant, while GlobalIndex takes data from an array
  // buffer. Special and Variable are distinguished since the former is for
  // indices like the global invocation, while the latter is a value.
  Const = â€œConstâ€, // arg = value
  Special = â€œSpecialâ€, // arg = [variable, n]
  Variable = â€œVariableâ€, // arg = variable
  GlobalIndex = â€œGlobalIndexâ€, // arg = [gid, len]; src = [bufidx]
  GlobalView = â€œGlobalViewâ€, // arg = [gid, ShapeTracker], src = [indices...]
}</code></pre><p><span>When auto-generating GPU kernels, theyâ€™re pretty simple for pointwise ops. The tricky part is if thereâ€™s a reduction (aka. </span><a href="https://en.wikipedia.org/wiki/Tensor_contraction" rel="">tensor contraction</a><span>), most commonly in matmuls and convolutions. These can be optimized pretty well on the web by unrolling judiciously and tiling the loads/stores.</span></p><p><span>An example WebGPU matmul kernel for </span><code>float32[4096,4096]</code><span> matrices generated by jax-js is shown below.</span></p><pre><code>@group(0) @binding(0) var&lt;storage, read&gt; in0 : array&lt;f32&gt;;
@group(0) @binding(1) var&lt;storage, read&gt; in1 : array&lt;f32&gt;;
@group(0) @binding(2) var&lt;storage, read_write&gt; result : array&lt;f32&gt;;

@compute @workgroup_size(256)
fn main(@builtin(global_invocation_id) id : vec3&lt;u32&gt;) {
  if (id.x &gt;= 1048576) { return; }
  let gidx: i32 = i32(id.x);
  var acc0: f32 = f32(0);
  var acc1: f32 = f32(0);
  var acc2: f32 = f32(0);
  var acc3: f32 = f32(0);
  var acc4: f32 = f32(0);
  var acc5: f32 = f32(0);
  var acc6: f32 = f32(0);
  var acc7: f32 = f32(0);
  var acc8: f32 = f32(0);
  var acc9: f32 = f32(0);
  var acc10: f32 = f32(0);
  var acc11: f32 = f32(0);
  var acc12: f32 = f32(0);
  var acc13: f32 = f32(0);
  var acc14: f32 = f32(0);
  var acc15: f32 = f32(0);
  for (var ridx: i32 = 0; ridx &lt; 1024; ridx++) {
    let x0: i32 = ((gidx / 8192) * 131072) + ((((gidx / 8) % 8) * 16384) + (ridx * 4));
    let x1: f32 = in0[x0];
    let x2: i32 = (((gidx / 64) % 128) * 32) + (((gidx % 8) * 4) + (ridx * 16384));
    let x3: f32 = in1[x2];
    let x4: f32 = in0[x0 + 1];
    let x6: f32 = in0[x0 + 2];
    let x8: f32 = in0[x0 + 3];
    let x10: f32 = in0[x0 + 4096];
    let x11: f32 = in0[x0 + 4097];
    let x12: f32 = in0[x0 + 4098];
    let x13: f32 = in0[x0 + 4099];
    let x14: f32 = in0[x0 + 8192];
    let x15: f32 = in0[x0 + 8193];
    let x16: f32 = in0[x0 + 8194];
    let x17: f32 = in0[x0 + 8195];
    let x18: f32 = in0[x0 + 12288];
    let x19: f32 = in0[x0 + 12289];
    let x20: f32 = in0[x0 + 12290];
    let x21: f32 = in0[x0 + 12291];
    let x22: f32 = in1[x2 + 1];
    let x26: f32 = in1[x2 + 2];
    let x30: f32 = in1[x2 + 3];
    let x5: f32 = in1[x2 + 4096];
    let x23: f32 = in1[x2 + 4097];
    let x27: f32 = in1[x2 + 4098];
    let x31: f32 = in1[x2 + 4099];
    let x7: f32 = in1[x2 + 8192];
    let x24: f32 = in1[x2 + 8193];
    let x28: f32 = in1[x2 + 8194];
    let x32: f32 = in1[x2 + 8195];
    let x9: f32 = in1[x2 + 12288];
    let x25: f32 = in1[x2 + 12289];
    let x29: f32 = in1[x2 + 12290];
    let x33: f32 = in1[x2 + 12291];
    acc0 += x1 * x3 + x4 * x5 + x6 * x7 + x8 * x9;
    acc1 += x10 * x3 + x11 * x5 + x12 * x7 + x13 * x9;
    acc2 += x14 * x3 + x15 * x5 + x16 * x7 + x17 * x9;
    acc3 += x18 * x3 + x19 * x5 + x20 * x7 + x21 * x9;
    acc4 += x1 * x22 + x4 * x23 + x6 * x24 + x8 * x25;
    acc5 += x10 * x22 + x11 * x23 + x12 * x24 + x13 * x25;
    acc6 += x14 * x22 + x15 * x23 + x16 * x24 + x17 * x25;
    acc7 += x18 * x22 + x19 * x23 + x20 * x24 + x21 * x25;
    acc8 += x1 * x26 + x4 * x27 + x6 * x28 + x8 * x29;
    acc9 += x10 * x26 + x11 * x27 + x12 * x28 + x13 * x29;
    acc10 += x14 * x26 + x15 * x27 + x16 * x28 + x17 * x29;
    acc11 += x18 * x26 + x19 * x27 + x20 * x28 + x21 * x29;
    acc12 += x1 * x30 + x4 * x31 + x6 * x32 + x8 * x33;
    acc13 += x10 * x30 + x11 * x31 + x12 * x32 + x13 * x33;
    acc14 += x14 * x30 + x15 * x31 + x16 * x32 + x17 * x33;
    acc15 += x18 * x30 + x19 * x31 + x20 * x32 + x21 * x33;
  }
  let x34: i32 = ((gidx / 8192) * 131072) + ((((gidx / 64) % 128) * 32) + ((((gidx / 8) % 8) * 16384) + ((gidx % 8) * 4)));
  result[x34] = acc0;
  result[x34 + 4096] = acc1;
  result[x34 + 8192] = acc2;
  result[x34 + 12288] = acc3;
  result[x34 + 1] = acc4;
  result[x34 + 4097] = acc5;
  result[x34 + 8193] = acc6;
  result[x34 + 12289] = acc7;
  result[x34 + 2] = acc8;
  result[x34 + 4098] = acc9;
  result[x34 + 8194] = acc10;
  result[x34 + 12290] = acc11;
  result[x34 + 3] = acc12;
  result[x34 + 4099] = acc13;
  result[x34 + 8195] = acc14;
  result[x34 + 12291] = acc15;
}</code></pre><p><span>If youâ€™re writing a native library, this isnâ€™t good enough. For example, you have to at least use tensor cores </span><code>mma.sync.aligned.*</code><span> on Nvidia GPUs! But on the web, it gets to pretty comparable performance with the best open-source libraries, and it seems that </span><a href="https://github.com/google/dawn" rel="">Dawn</a><span> is alright at bridging any gaps with optimization.</span></p><p><strong>Onto the frontend.</strong><span> This is the core of the library, and where the actual autograd and tracing happens. We follow the JAX design quite closely, where there is a set of primitives along with an ambient </span><em>interpreter stack</em><span>. This isâ€¦ quite difficult, magical, and took me a while to figure out. To learn more see:</span></p><ul><li><p><a href="https://docs.jax.dev/en/latest/autodidax.html" rel="">Autodidax: JAX core from scratch</a><span> (2021)</span></p></li><li><p><a href="https://arxiv.org/abs/1804.00746" rel="">The simple essence of automatic differentiation</a><span> (Elliott 2018)</span></p></li></ul><p><em>(One particularly cool moment about this way of building an ML library is that you get reverse-mode AD â€œfor freeâ€ by inverting/transposing the forward-mode rules. I found this really beautiful after I wrapped my head around it; itâ€™s quite mathematically pleasing. Another cool moment is when you first get arbitrary 2nd, 3rd, â€¦ n-th order derivatives after just implementing the first-order derivative rules â€” GradientTape could never!)</em></p><p>Honestly this is probably the most lost Iâ€™ve ever felt in writing code. Itâ€™s like, nested mutually recursive interpreters to model functors in the â€œcategory of tensors.â€</p><p>Anyway, once I reviewed my differential geometry notes from college and dusted off my understanding of tangents, pulling back cotangents, functors and so on, I think I eventually figured it out. Though I still had tiny bugs for the next 6 months. ğŸ˜‚</p><p><span>The list of high-level </span><code>Primitive</code><span> in jax-js is below:</span></p><pre><code>/**
 * Frontend primitive operations, which are lowered into Kernel objects before
 * being dispatched to the backend.
 *
 * Any operation between arrays can be described in these parts. This is also
 * the set of primitives that can occur in Jaxpr programs, and the level at
 * which transformations like vmap, grad, and jvp occur. They are loosely based
 * on [XLA](https://openxla.org/xla/operation_semantics).
 *
 * All n-ary operations support broadcasting, with NumPy semantics.
 */
export enum Primitive {
  Add = â€œaddâ€,
  Mul = â€œmulâ€,
  Idiv = â€œidivâ€,
  Neg = â€œnegâ€,
  Reciprocal = â€œreciprocalâ€,
  StopGradient = â€œstop_gradientâ€,
  Cast = â€œcastâ€,
  Bitcast = â€œbitcastâ€,
  RandomBits = â€œrandom_bitsâ€,
  Sin = â€œsinâ€,
  Cos = â€œcosâ€,
  Asin = â€œasinâ€,
  Atan = â€œatanâ€,
  Exp = â€œexpâ€,
  Log = â€œlogâ€,
  Erf = â€œerfâ€,
  Erfc = â€œerfcâ€,
  Sqrt = â€œsqrtâ€,
  Min = â€œminâ€,
  Max = â€œmaxâ€,
  Reduce = â€œreduceâ€,
  Dot = â€œdotâ€, // sum(x*y, axis=-1)
  Conv = â€œconvâ€, // see lax.conv_general_dilated
  Pool = â€œpoolâ€,
  PoolTranspose = â€œpool_transposeâ€,
  Compare = â€œcompareâ€,
  Where = â€œwhereâ€,
  Transpose = â€œtransposeâ€,
  Broadcast = â€œbroadcastâ€,
  Reshape = â€œreshapeâ€,
  Flip = â€œflipâ€,
  Shrink = â€œshrinkâ€,
  Pad = â€œpadâ€,
  Gather = â€œgatherâ€,
  JitCall = â€œjit_callâ€,
}</code></pre><p>Notice that many of these are similar to the backend operations above, but some are different. In particular, there are convolutions and matrix multiplications here. These are useful to see in the frontend IR (and for autograd) but can be lowered to a simpler form before the kernels are generated on the backend.</p><p><span>By default, an operation is just lowered directly to a backend kernel after passing through any necessary transformations (</span><code>vmap</code><span>, </span><code>jvp</code><span>, </span><code>grad</code><span>). But if youâ€™re using the </span><code>jit</code><span>, jax-js will trace your program to produce a â€œJaxprâ€ (list of operations) followed by </span><a href="https://substack.com/home/post/p-163548742" rel="">automatic kernel fusion</a><span> to generate kernels, specialized to each input shape.</span></p><h3>Bugs<div><div></div></div></h3><p>Itâ€™s very hard to build an ML framework and a long task! So far, jax-js has implemented a lot of core functionality in JAX, but thereâ€™s still much more. If thereâ€™s an API or operation you want to see, please consider adding it or filing an issue (examples: np.split, FFT, AdamW).</p><p>I have a pretty varied, portable test suite that runs fast:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!E_s1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!E_s1!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png 424w, https://substackcdn.com/image/fetch/$s_!E_s1!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png 848w, https://substackcdn.com/image/fetch/$s_!E_s1!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png 1272w, https://substackcdn.com/image/fetch/$s_!E_s1!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!E_s1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png" width="566" height="395.8580060422961" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:926,&#34;width&#34;:1324,&#34;resizeWidth&#34;:566,&#34;bytes&#34;:252710,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://ekzhang.substack.com/i/179060245?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!E_s1!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png 424w, https://substackcdn.com/image/fetch/$s_!E_s1!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png 848w, https://substackcdn.com/image/fetch/$s_!E_s1!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png 1272w, https://substackcdn.com/image/fetch/$s_!E_s1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee5101ba-af11-4ba0-b041-0b5fd0624d32_1324x926.png 1456w" sizes="100vw" loading="lazy"/></picture><div></div></div></a></figure></div><p><span>So we are in a good position to find bugs and fix them. But making an ML library is quite difficult, and WebGPU is a nascent technology (e.g., I somehow gave my MacBook </span><a href="https://x.com/ekzhang1/status/1957107092868727225" rel="">kernel panics</a><span>)â€”there will be bugs! Please report.</span></p><h3>Technical: Performance<div><div></div></div></h3><p><span>We havenâ€™t spent a ton of time optimizing yet, but performance is generally pretty good. </span><code>jit</code><span> is very helpful for fusing operations together, and itâ€™s a feature only available on the web in jax-js. The default kernel-tuning heuristics get about 3000 GFLOP/s for matrix multiplication on an M4 Pro chip (</span><a href="https://jax-js.com/bench/matmul" rel="">try it</a><span>).</span></p><p><span>On that specific benchmark, itâ€™s actually more GFLOP/s than both </span><a href="https://github.com/tensorflow/tfjs" rel="">TensorFlow.js</a><span> and </span><a href="https://www.npmjs.com/package/onnxruntime-web" rel="">ONNX</a><span>, which both use handwritten libraries of custom kernels (versus jax-js, which generates kernels with an ML compiler).</span></p><p>Some particularly useful / low-hanging fruit to look at:</p><ul><li><p>The WebAssembly backend currently is quite simple, I didnâ€™t spend a ton of time optimizing it, but measurably it could be &gt;150x faster on my MacBook Pro. This difference comes from a few things multiplying:</p><ul><li><p>Donâ€™t recompute loop indices each time, we could improve FLOPs by ~1-3x.</p></li><li><p>Do loop unrolling/tiling, will improve FLOPs by ~2-3x.</p></li><li><p>Use SIMD instructions. This would improve FLOPs by 4x.</p></li><li><p><span>Add multi-threading (10x on my laptop), to use all available cores. Requires SharedArrayBuffer (</span><a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/crossOriginIsolated" rel="">crossOriginIsolated</a><span>) / there are some caveats here to sync/async handling, needs to be done carefully.</span></p></li></ul></li><li><p>Running the forward pass of the MobileCLIP2 transformer model is only about 1/3 the FLOPs compared to pure 4096x4096 matmul. Maybe we can improve this, especially in the causal self-attention layer.</p></li><li><p><span>Although WebGPU is rapidly gaining in popularity </span><a href="https://caniuse.com/webgpu" rel="">and support</a><span>, itâ€™s probably worth having a WebGL backend as well, as a fallback thatâ€™s guaranteed to work in pretty much all browsers and is still pretty fast. This isnâ€™t a huge amount of work; the WebGPU backend is &lt;700 lines of code for example.</span></p></li></ul><h3>Technical: Feature parity<div><div></div></div></h3><p><span>jax-js strives for </span><em>approximate</em><span> API compatibility with the JAX python library (and through that, NumPy). But some features vary for a few reasons:</span></p><ol><li><p><strong>Data model:</strong><span> jax-js has </span><em>ownership</em><span> of arrays using the </span><code>.ref</code><span> system, which obviates the need for APIs like </span><code>jit()</code><span>â€˜s </span><code>donate_argnums</code><span> and </span><code>numpy.asarray()</code><span>.</span></p></li><li><p><strong>Language primitives:</strong><span> JavaScript has no named arguments, so method call signatures may take objects instead of Pythonâ€™s keyword arguments. Also, PyTrees are translated in spirit to â€œJsTreeâ€ in jax-js, but their specification is different.</span></p></li><li><p><strong>Maturity:</strong><span> JAX has various types like </span><code>complex64</code><span>, advanced functions like </span><code>hessenberg()</code><span>, and advanced higher-order features like </span><code>lax.while_loop()</code><span> that we havenâ€™t implemented. Some of these are not easy to implement on GPU.</span></p></li></ol><p>Other features just arenâ€™t implemented yet. But those can probably be added easily!</p><p><span>Iâ€™ve made a table of every JAX library feature and its implementation status in jax-js, </span><a href="https://github.com/ekzhang/jax-js/blob/main/FEATURES.md" rel="">see here</a><span>. There are a couple big ones that stand out.</span></p><p>Youâ€™re welcome to contribute, though Iâ€™d also love if you could try using jax-js. :D</p></div></div></div>
  </body>
</html>
