<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://tech.marksblogg.com/minimalist-guide-compression.html">Original</a>
    <h1>Minimalist Guide to Lossless Compression (2019)</h1>
    
    <div id="readability-page-1" class="page"><div id="article_text">
            <p>Lossless compression is the act of making a dataset smaller than its original form while still being able to transform the compressed version back into the original source material. This contrasts lossy compression which produces a derivative dataset that, while being something humans can appreciate, cannot recreate the original source material.</p>
<p>In a world where storage is cheap, why do we still compress data? The answer to that is datasets are growing at a faster rate than storage throughput or network bandwidth. 100 MB of data can be compressed to 10 MB in less than a second and transferred in under a second over a 100 Mbps connection. Most decompression systems run at 100 to 500 MB/s meaning the compression overhead isn&#39;t enough to burden the wall clock time needed to deliver the original 100 MB of content. Not only does this make the best use of the networking bottleneck, but it also frees up resources for other payloads.</p>
<p>Also, consider that if you can only read at 100 MB/s off a mechanical drive but your CPU can decompress data at ~500 MB/s then the mechanical drive is able to provide 5x the throughput you&#39;d otherwise expect thanks to compression.</p>
<p>The world of compression is a rabbit hole of intellectual curiosity. In this post, I&#39;ll describe the highlights I&#39;ve come across while trying to improve compression performance in database systems I deploy.</p>
<div id="from-entropy-to-lempel-ziv">
<h2>From Entropy to Lempel-Ziv</h2>
<p>Entropy, an Information Theory term coined by <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a> in 1948, describes the minimum number of bits, on average, needed to encode a dataset. Think of entropy as the number of yes/no questions that would need to be answered in order to describe any one piece of information within a dataset.</p>
<p>Compression aims to reduce that ceiling of bits by transforming data in such a way that patterns emerge exposing duplication within a given dataset.</p>
<p>The act of producing optimally large, repeating tokens within a dataset was the key concept behind <a href="https://en.wikipedia.org/wiki/Abraham_Lempel">Abraham Lempel</a>&#39;s and <a href="https://en.wikipedia.org/wiki/Yaakov_Ziv">Yaakov Ziv</a>&#39;s work on the LZ77 and LZ78 algorithms developed in 1977 and 1978 respectively. Their work has had a huge impact on the field of lossless compression. Many algorithms used in popular software today are variations and tweaks to those two algorithms developed in the late 1970s.</p>
<p>Below is a family tree of related dictionary-based compression algorithms which trace their linages back to LZ77 and LZ78. I&#39;ve also included the year of earliest publication I could find for each of them and a few noteworthy software and file format implementations.</p>
<div><pre><span></span>├── LZ77 (1977)
│   ├── LZR (1981)
│   ├── LZSS (1982, WINRAR)
│   │   ├── LZB (1987)
│   │   └── LZH (1987)
│   ├── DEFLATE (1989, ZIP, gzip, PNG, zlib)
│   │   └── DEFLATE64 (1999)
│   ├── ROLZ (1991)
│   │   ├── LZP (1995)
│   │   └── LZRW1 (1991)
│   │       ├── LZJB (1998, ZFS)
│   │       └── LZRW1-A, 2, 3, 3-A, 4, 5 (&lt;1996 and onward)
│   ├── LZS (1994, Cisco IOS)
│   ├── LZX (1995, CHM files)
│   ├── LZO (1996, Nintendo 64, PlayStation)
│   ├── LZMA (1998, 7ZIP, xz)
│   │   ├── LZMA2 (2009)
│   │   └── LZHAM (2009)
│   ├── Statistical Lempel-Ziv (2001)
│   ├── SLZ (&lt;2009)
│   ├── LZ4 (2011, Hadoop, ZFS, bcolz)
│   │   └── ZStandard (2015, Redshift, ORC, Parquet, RocksDB, bcolz)
│   ├── Snappy (2011, ORC, Parquet, MariaDB, Cassandra, MongoDB, Lucene, bcolz)
│   └── Brotli (2013, Google Chrome, Firefox, nginx)
│       └── LZFSE (2015, iOS, Mac OSX)
└── LZ78 (1978)
    ├── LZW (1984, PKZIP, GIF, TIFF, PDF)
    │   ├── LZMW (1985)
    │   │   └── LZAP (1988)
    │   ├── LZC (2007)
    │   │   └── LZT (1987)
    │   └── LZWL (2006)
    └── LZJ (1985)
</pre></div>
<p>In contrast, these are a few non-dictionary-based compression systems.</p>
<div><pre><span></span>├── PPM (1984)
│   └── PAQ (2002)
│      └── 20+ Variants (2003-2009)
└── bzip2 (1996)
</pre></div>
<p>As you can see, the compression world may not be fast-moving but rarely does a year pass without some sort of iteration and improvement.</p>
</div>
<div id="lossless-compression-benchmarks">
<h2>Lossless Compression Benchmarks</h2>
<p>In 2015, <a href="http://pskibinski.pl/">Przemyslaw Skibinski</a> started a project on GitHub called <a href="https://github.com/inikep/lzbench">lzbench</a>. This project aims to compile the source code of a wide range of lossless compression algorithms into a single binary and then benchmark them against various workloads. The project&#39;s <a href="https://github.com/inikep/lzbench#benchmarks">README</a> contains benchmark results for a 212 MB file extracted from the <a href="http://www.data-compression.info/Corpora/SilesiaCorpus/index.html">Silesia</a> compression corpus. Each of the compression methods used is run with a variety of settings; this gives an idea of what sort of results could be expected from a performance tuning exercise.</p>
<p>In the above benchmark results, blosclz, density, lz4, lz4fast, lzo1x and shrinker were all tuned to compress at more than 400 MB/s while maintaining at least a 5:3 compression ratio.</p>
<p>The benchmark shows memcpy, which simply copied the data from one memory location to another, running at ~8.5 GB/s. Given the computational overheads of each of the compressors, this transfer rate can be seen as a ceiling on the hardware used (<a href="https://github.com/Blosc/c-blosc#what-is-it">C-Blosc</a> claims it can get past memory-bound performance issues but I&#39;ve yet to study their claim in detail). The density and lz4fast benchmarks stand out as their compression speeds of 800 and 785 MB/s respectively, were the fastest of any compressor able to achieve a 3:2 compression ratio.</p>
<p>Of the compressors that are able to achieve a 3:1 compression ratio, only Zstandard could do so at 88 MB/s. More than 75% of the other compressors that could achieve that compression ratio couldn&#39;t do so at any more than 15 MB/s, some 6x slower than Zstandard.</p>
<p>The vast majority of decompressors could break the 250 MB/s barrier, 25% of the decompressors broke the 1 GB/s barrier and a few, including LZ4, could be tuned to decompress in excess of 2 GB/s. Decompression at this rate would demand either RAM drives, RAID 0 or NVMe storage in order to keep up with these levels of throughput. The SATA bus on most systems is limited to 1.2 GB/s so this would be a bottleneck in need of addressing if it were included in the storage pipeline.</p>
<p>Lastly, the compression ratio of more than 4:1 that xz achieved is interesting. This compressor is popular in packaging software. A software package could be downloaded 100s of millions of times so it&#39;s worth the effort to find the best compression ratio possible given the amount of bandwidth and the diversity of network connections involved in software distribution. This goal can excuse the 2 MB/s compression rate xz managed during the compression process.</p>
<p>Do bear in mind that some decompression tools can require an excessive amount of memory and compute power. The high-ratio compression systems can suffer from this greatly so this trade-off should be considered as well.</p>
</div>
<div id="sort-then-compress">
<h2>Sort, then Compress</h2>
<p>Many lossless compression systems can be aided by being fed sorted data. The sliding window compressors use rarely cover the entire dataset and the more clustered the values the easier it is to detect repeating patterns. Most SQL-based systems don&#39;t guarantee the order of rows returned without an <tt>ORDER BY</tt> clause so the sorted form of the data on disk should be of little concern (with Redshift&#39;s sort key a notable exception).</p>
<p>Below I&#39;ve set up a standalone Hadoop system running HDFS and Presto using the instructions from my <a href="https://tech.marksblogg.com/hadoop-3-single-node-install-guide.html">Hadoop 3 Install Guide</a>. I&#39;ve taken the first 6 ORC files representing 120 million of the 1.1 billion taxi rides that have taken place in New York City over six years. This <a href="https://tech.marksblogg.com/billion-nyc-taxi-rides-redshift.html">post</a> describes how I produced this dataset in CSV format and I&#39;ve run a large number of <a href="https://tech.marksblogg.com/benchmarks.html">benchmarks</a> where I&#39;ve converted that CSV data into ORC format before examining query performance.</p>
<p>Below are a few modifications I&#39;ve made to the Presto configuration in my stand-alone guide. First, to sort 120M rows of data in Presto will require a memory limit of at least 8 GB.</p>
<div><pre><span></span>$ sudo vi /opt/presto/etc/config.properties
</pre></div>
<div><pre><span></span>coordinator=true
node-scheduler.include-coordinator=true
http-server.http.port=8080
query.max-memory=8GB
query.max-memory-per-node=8GB
query.max-total-memory-per-node=8GB
discovery-server.enabled=true
discovery.uri=http://localhost:8080
</pre></div>
<div><pre><span></span>$ sudo /opt/presto/bin/launcher restart
</pre></div>
<p>Next, I&#39;ll create a warehouse folder as the tables created below will run via the Hive connector and it defaults to store tables it helps create in <tt>/user/hive/warehouse</tt> on HDFS.</p>
<div><pre><span></span>$ hdfs dfs -mkdir -p /user/hive/warehouse
</pre></div>
<p>I&#39;ll then copy the first six ORC files I have saved in my home folder onto HDFS.</p>
<div><pre><span></span>$ hdfs dfs -mkdir /trips_orc
$ hdfs dfs -copyFromLocal \
    ~/orc/00000[0-5]_0 \
    /trips_orc/
</pre></div>
<p>I&#39;ll create a schema for the <tt>trips_orc</tt> table in Hive. This lets Presto do schema-on-read and understand the column layout of the ORC files.</p>

<div><pre><span></span><span>CREATE</span> <span>EXTERNAL</span> <span>TABLE</span> <span>trips_orc</span> <span>(</span>
    <span>trip_id</span>                 <span>INT</span><span>,</span>
    <span>vendor_id</span>               <span>STRING</span><span>,</span>
    <span>pickup_datetime</span>         <span>TIMESTAMP</span><span>,</span>
    <span>dropoff_datetime</span>        <span>TIMESTAMP</span><span>,</span>
    <span>store_and_fwd_flag</span>      <span>STRING</span><span>,</span>
    <span>rate_code_id</span>            <span>SMALLINT</span><span>,</span>
    <span>pickup_longitude</span>        <span>DOUBLE</span><span>,</span>
    <span>pickup_latitude</span>         <span>DOUBLE</span><span>,</span>
    <span>dropoff_longitude</span>       <span>DOUBLE</span><span>,</span>
    <span>dropoff_latitude</span>        <span>DOUBLE</span><span>,</span>
    <span>passenger_count</span>         <span>SMALLINT</span><span>,</span>
    <span>trip_distance</span>           <span>DOUBLE</span><span>,</span>
    <span>fare_amount</span>             <span>DOUBLE</span><span>,</span>
    <span>extra</span>                   <span>DOUBLE</span><span>,</span>
    <span>mta_tax</span>                 <span>DOUBLE</span><span>,</span>
    <span>tip_amount</span>              <span>DOUBLE</span><span>,</span>
    <span>tolls_amount</span>            <span>DOUBLE</span><span>,</span>
    <span>ehail_fee</span>               <span>DOUBLE</span><span>,</span>
    <span>improvement_surcharge</span>   <span>DOUBLE</span><span>,</span>
    <span>total_amount</span>            <span>DOUBLE</span><span>,</span>
    <span>payment_type</span>            <span>STRING</span><span>,</span>
    <span>trip_type</span>               <span>SMALLINT</span><span>,</span>
    <span>pickup</span>                  <span>STRING</span><span>,</span>
    <span>dropoff</span>                 <span>STRING</span><span>,</span>

    <span>cab_type</span>                <span>STRING</span><span>,</span>

    <span>precipitation</span>           <span>SMALLINT</span><span>,</span>
    <span>snow_depth</span>              <span>SMALLINT</span><span>,</span>
    <span>snowfall</span>                <span>SMALLINT</span><span>,</span>
    <span>max_temperature</span>         <span>SMALLINT</span><span>,</span>
    <span>min_temperature</span>         <span>SMALLINT</span><span>,</span>
    <span>average_wind_speed</span>      <span>SMALLINT</span><span>,</span>

    <span>pickup_nyct2010_gid</span>     <span>SMALLINT</span><span>,</span>
    <span>pickup_ctlabel</span>          <span>STRING</span><span>,</span>
    <span>pickup_borocode</span>         <span>SMALLINT</span><span>,</span>
    <span>pickup_boroname</span>         <span>STRING</span><span>,</span>
    <span>pickup_ct2010</span>           <span>STRING</span><span>,</span>
    <span>pickup_boroct2010</span>       <span>STRING</span><span>,</span>
    <span>pickup_cdeligibil</span>       <span>STRING</span><span>,</span>
    <span>pickup_ntacode</span>          <span>STRING</span><span>,</span>
    <span>pickup_ntaname</span>          <span>STRING</span><span>,</span>
    <span>pickup_puma</span>             <span>STRING</span><span>,</span>

    <span>dropoff_nyct2010_gid</span>    <span>SMALLINT</span><span>,</span>
    <span>dropoff_ctlabel</span>         <span>STRING</span><span>,</span>
    <span>dropoff_borocode</span>        <span>SMALLINT</span><span>,</span>
    <span>dropoff_boroname</span>        <span>STRING</span><span>,</span>
    <span>dropoff_ct2010</span>          <span>STRING</span><span>,</span>
    <span>dropoff_boroct2010</span>      <span>STRING</span><span>,</span>
    <span>dropoff_cdeligibil</span>      <span>STRING</span><span>,</span>
    <span>dropoff_ntacode</span>         <span>STRING</span><span>,</span>
    <span>dropoff_ntaname</span>         <span>STRING</span><span>,</span>
    <span>dropoff_puma</span>            <span>STRING</span>
<span>)</span> <span>STORED</span> <span>AS</span> <span>orc</span>
  <span>LOCATION</span> <span>&#39;/trips_orc/&#39;</span><span>;</span>
</pre></div>
<p>The following SQL will create four new tables. Each will be in GZIP-compressed, ORC format. Each one will pick a different field to sort on. Note I&#39;m only storing four columns from the original table for the sake of both time and memory consumption during this operation.</p>

<div><pre><span></span><span>CREATE</span> <span>TABLE</span> <span>sorted_by_vendor_id</span>
<span>WITH</span> <span>(</span><span>format</span><span>=</span><span>&#39;ORC&#39;</span><span>)</span> <span>AS</span>
    <span>SELECT</span> <span>trip_id</span><span>,</span>
           <span>vendor_id</span><span>,</span>
           <span>pickup_datetime</span><span>,</span>
           <span>pickup_longitude</span>
    <span>FROM</span> <span>trips_orc</span>
    <span>ORDER</span> <span>BY</span> <span>vendor_id</span><span>;</span>

<span>CREATE</span> <span>TABLE</span> <span>sorted_by_trip_id</span>
<span>WITH</span> <span>(</span><span>format</span><span>=</span><span>&#39;ORC&#39;</span><span>)</span> <span>AS</span>
    <span>SELECT</span> <span>trip_id</span><span>,</span>
           <span>vendor_id</span><span>,</span>
           <span>pickup_datetime</span><span>,</span>
           <span>pickup_longitude</span>
    <span>FROM</span> <span>trips_orc</span>
    <span>ORDER</span> <span>BY</span> <span>trip_id</span><span>;</span>

<span>CREATE</span> <span>TABLE</span> <span>sorted_by_pickup_datetime</span>
<span>WITH</span> <span>(</span><span>format</span><span>=</span><span>&#39;ORC&#39;</span><span>)</span> <span>AS</span>
    <span>SELECT</span> <span>trip_id</span><span>,</span>
           <span>vendor_id</span><span>,</span>
           <span>pickup_datetime</span><span>,</span>
           <span>pickup_longitude</span>
    <span>FROM</span> <span>trips_orc</span>
    <span>ORDER</span> <span>BY</span> <span>pickup_datetime</span><span>;</span>

<span>CREATE</span> <span>TABLE</span> <span>sorted_by_pickup_longitude</span>
<span>WITH</span> <span>(</span><span>format</span><span>=</span><span>&#39;ORC&#39;</span><span>)</span> <span>AS</span>
    <span>SELECT</span> <span>trip_id</span><span>,</span>
           <span>vendor_id</span><span>,</span>
           <span>pickup_datetime</span><span>,</span>
           <span>pickup_longitude</span>
    <span>FROM</span> <span>trips_orc</span>
    <span>ORDER</span> <span>BY</span> <span>pickup_longitude</span><span>;</span>
</pre></div>
<p>I&#39;ll execute the above with Presto.</p>
<div><pre><span></span>$ presto <span>\</span>
    --server localhost:8080 <span>\</span>
    --catalog hive <span>\</span>
    --schema default <span>\</span>
    --file sort.sql
</pre></div>
<p>The resulting table sizes were as follows:</p>
<div><pre><span></span>  GB | sorted by
----------------------
0.91 | pickup_longitude
1.06 | trip_id
1.12 | pickup_datetime
1.33 | vendor_id
</pre></div>
<p>The largest table is 1.46x bigger than the smallest.</p>
<p>There is an argument that one should test the first 50K-odd records of any table against all possible sort keys when compressing a given dataset. There is no speeding up a solid-state drive by 1.46x but the above happily reduced the throughput requirements by that amount.</p>
<p>If the memory usage of sorting your entire dataset exceeded your cluster&#39;s capacity you could look to sort on each table partition one at a time instead. Good is not the enemy of perfect and reclaiming storage and throughput capacity will help make the most of your cluster&#39;s hardware.</p>
</div>
<div id="an-attack-vector">
<h2>An Attack Vector</h2>
<p>Compression has also been an attack vector for decades now. There have been <a href="https://github.com/nemequ/compfuzz/wiki/Results">&#34;Compression Fuzzing&#34;</a> efforts to help uncover vulnerabilities but given decompression utilities are some of the most widely-deployed software in the world, any exploit can have a global impact.</p>
<p>A few examples include the <a href="https://en.wikipedia.org/wiki/Zip_bomb">&#34;ZIP of death&#34;</a> which is a 42 KB ZIP file that extracts 4.5 PB of data. Unsuspecting web applications that decompressed user-submitted content need to be hardened for this sort of attack. <a href="https://en.wikipedia.org/wiki/BREACH">BREACH</a> exploited an HTTPS compression vulnerability and <a href="https://en.wikipedia.org/wiki/CRIME">CRIME</a> was another exploit disclosed around the same time that worked over HTTPS and SPDY connections that used compression.</p>
<p>Incidents like the above are making compression designers <a href="https://news.ycombinator.com/item?id=18720554">contemplate</a> what attack vectors could result from forth-coming dictionary encoding methods.</p>
</div>

        </div><p>
            Thank you for taking the time to read this post. I offer both consulting and hands-on development services to clients in North America and Europe. If you&#39;d like to discuss how my offerings can help your business please contact me via <a href="https://uk.linkedin.com/in/marklitwintschik/">LinkedIn</a>.
        </p></div>
  </body>
</html>
