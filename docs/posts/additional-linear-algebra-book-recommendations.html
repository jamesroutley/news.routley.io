<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://minireference.com/blog/additional-linear-algebra-book-recommendations/">Original</a>
    <h1>Additional linear algebra book recommendations</h1>
    
    <div id="readability-page-1" class="page"><div><div><div data-hpc="true"><article itemprop="text">
<p dir="auto">A minimal re-implementation of Flash Attention with CUDA and PyTorch.
The official <a href="https://github.com/Dao-AILab/flash-attention">implementation</a> can be quite daunting for a CUDA beginner
(like myself), so this repo tries to be small and educational.</p>
<ul dir="auto">
<li>The entire forward pass is written in ~100 lines in <code>flash.cu</code>.</li>
<li>The variable names follow the notations from the original <a href="https://arxiv.org/abs/2205.14135" rel="nofollow">paper</a>.</li>
</ul>


<ul dir="auto">
<li>PyTorch (with CUDA)</li>
<li><code>Ninja</code> for loading in C++</li>
</ul>

<p dir="auto">Compare the wall-clock time between manual attention and minimal flash attention:</p>

<p dir="auto">Sample output on a <a href="https://aws.amazon.com/ec2/instance-types/g4/" rel="nofollow">T4</a>:</p>
<div data-snippet-clipboard-copy-content="=== profiling manual attention ===
...
Self CPU time total: 52.389ms
Self CUDA time total: 52.545ms

=== profiling minimal flash attention === 
...  
Self CPU time total: 11.452ms
Self CUDA time total: 3.908ms"><pre><code>=== profiling manual attention ===
...
Self CPU time total: 52.389ms
Self CUDA time total: 52.545ms

=== profiling minimal flash attention === 
...  
Self CPU time total: 11.452ms
Self CUDA time total: 3.908ms
</code></pre></div>
<p dir="auto">Speed-up achieved!</p>

<p dir="auto">Try out this <a href="https://colab.research.google.com/gist/tspeterkim/143bc7be7a845656817cf94c5228598e/demo-flash-attention-minimal.ipynb" rel="nofollow">online colab demo</a>.</p>

<ul dir="auto">
<li>No backward pass! To be honest, I found it a lot more complex than the forward pass, which was enough to show the
use of shared memory to avoid large N^2 read/writes.</li>
<li>In the inner loop, I assign each thread to a row of the output matrix. This differs from the original implementation.</li>
<li>This thread-per-row simplification makes the matrix multiplications very slow. This is probably why for longer
sequences and larger block sizes, this gets slower than the manual implementation.</li>
<li>Q,K,Vs are in float32, unlike the original implementation which uses float16.</li>
<li>The block size is <a href="https://github.com/tspeterkim/flash-attention-minimal/blob/9b7ca8ef4e6afdbfeb149a9cd488c8dea9af9ad6/flash.cu#L85">fixed</a> at compile time to 32.</li>
</ul>

<ul>
<li> Add backward pass</li>
<li> Speed up matmults</li>
<li> Dynamically set block size</li>
</ul>
</article></div></div></div></div>
  </body>
</html>
