<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://orlp.net/blog/taming-float-sums/">Original</a>
    <h1>Taming floating-point sums</h1>
    
    <div id="readability-page-1" class="page"><article>

<time datetime="2024-05-25">2024-05-25</time>
<p>Suppose you have an array of floating-point numbers, and wish to sum them.
You might naively think you can simply add them, e.g. in Rust:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>naive_sum(arr: &amp;[</span><span>f32</span><span>]) -&gt; </span><span>f32 </span><span>{
</span><span>    </span><span>let </span><span>mut out = </span><span>0.0</span><span>;
</span><span>    </span><span>for</span><span> x in arr {
</span><span>        out += *x;
</span><span>    }
</span><span>    out
</span><span>}
</span></code></pre>
<p>This however can easily result in an arbitrarily large accumulated error. Let’s try it out:</p>
<pre data-lang="rust"><code data-lang="rust"><span>naive_sum(&amp;vec![</span><span>1.0</span><span>;     </span><span>1_000_000</span><span>]) =  </span><span>1000000.0
</span><span>naive_sum(&amp;vec![</span><span>1.0</span><span>;    </span><span>10_000_000</span><span>]) = </span><span>10000000.0
</span><span>naive_sum(&amp;vec![</span><span>1.0</span><span>;   </span><span>100_000_000</span><span>]) = </span><span>16777216.0
</span><span>naive_sum(&amp;vec![</span><span>1.0</span><span>; </span><span>1_000_000_000</span><span>]) = </span><span>16777216.0
</span></code></pre>
<p>Uh-oh… What happened? The problem is that the next 32-bit floating-point
number after <code>16777216</code> is <code>16777218</code>. So when you compute <code>16777216 + 1</code>,
it rounds back to the nearest floating-point number with an even mantissa,
which happens to be <code>16777216</code> again. We’re stuck.</p>
<p>Luckily, there are better ways to sum an array.</p>
<h2 id="pairwise-summation"><a href="#pairwise-summation" aria-label="Anchor link for: pairwise-summation">Pairwise summation</a></h2>
<p>A method that’s a bit more clever is to use <a href="https://en.wikipedia.org/wiki/Pairwise_summation">pairwise summation</a>.
Instead of a completely linear sum with a single accumulator it recursively
sums an array by splitting the array in half, summing the halves, and then adding
the sums.</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>pairwise_sum(arr: &amp;[</span><span>f32</span><span>]) -&gt; </span><span>f32 </span><span>{
</span><span>    </span><span>if</span><span> arr.len() == </span><span>0 </span><span>{ </span><span>return </span><span>0.0</span><span>; }
</span><span>    </span><span>if</span><span> arr.len() == </span><span>1 </span><span>{ </span><span>return</span><span> arr[</span><span>0</span><span>]; }
</span><span>    </span><span>let </span><span>(first, second) = arr.split_at(arr.len() / </span><span>2</span><span>);
</span><span>    pairwise_sum(first) + pairwise_sum(second)
</span><span>}
</span></code></pre>
<p>This is more accurate:</p>
<pre data-lang="rust"><code data-lang="rust"><span>pairwise_sum(&amp;vec![</span><span>1.0</span><span>;     </span><span>1_000_000</span><span>]) =    </span><span>1000000.0
</span><span>pairwise_sum(&amp;vec![</span><span>1.0</span><span>;    </span><span>10_000_000</span><span>]) =   </span><span>10000000.0
</span><span>pairwise_sum(&amp;vec![</span><span>1.0</span><span>;   </span><span>100_000_000</span><span>]) =  </span><span>100000000.0
</span><span>pairwise_sum(&amp;vec![</span><span>1.0</span><span>; </span><span>1_000_000_000</span><span>]) = </span><span>1000000000.0
</span></code></pre>
<p>However, this is rather slow. To get a summation routine that goes as fast as
possible while still being reasonably accurate we should not recurse down
all the way to length-1 arrays, as this gives too much call overhead. We can
still use our naive sum for small sizes, and only recurse on large sizes.
This does make our worst-case error worse by a constant factor, but in turn
makes the pairwise sum almost as fast as a naive sum.</p>

<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>block_pairwise_sum(arr: &amp;[</span><span>f32</span><span>]) -&gt; </span><span>f32 </span><span>{
</span><span>    </span><span>if</span><span> arr.len() &gt; </span><span>256 </span><span>{
</span><span>        </span><span>let</span><span> split = (arr.len() / </span><span>2</span><span>).next_multiple_of(</span><span>256</span><span>);
</span><span>        </span><span>let </span><span>(first, second) = arr.split_at(split);
</span><span>        block_pairwise_sum(first) + block_pairwise_sum(second)
</span><span>    } </span><span>else </span><span>{
</span><span>        naive_sum(arr)
</span><span>    }
</span><span>}
</span></code></pre>
<h2 id="kahan-summation"><a href="#kahan-summation" aria-label="Anchor link for: kahan-summation">Kahan summation</a></h2>
<p>The worst-case round-off error of naive summation scales with $O(n \epsilon)$
when summing $n$ elements, where $\epsilon$ is the <a href="https://en.wikipedia.org/wiki/Machine_epsilon">machine
epsilon</a> of your floating-point
type (here $2^{-24}$). Pairwise summation improves this to  $O((\log n) \epsilon + n\epsilon^2)$. However, <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm">Kahan
summation</a> improves this
further to $O(n\epsilon^2)$, eliminating the $\epsilon$ term entirely, leaving only
the $\epsilon^2$ term which is negligible unless you sum a very large amount of
numbers.</p>

<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>fn </span><span>kahan_sum(arr: &amp;[</span><span>f32</span><span>]) -&gt; </span><span>f32 </span><span>{
</span><span>    </span><span>let </span><span>mut sum = </span><span>0.0</span><span>;
</span><span>    </span><span>let </span><span>mut c = </span><span>0.0</span><span>;
</span><span>    </span><span>for</span><span> x in arr {
</span><span>        </span><span>let</span><span> y = *x - c;
</span><span>        </span><span>let</span><span> t = sum + y;
</span><span>        c = (t - sum) - y;
</span><span>        sum = t;
</span><span>    }
</span><span>    sum
</span><span>}
</span></code></pre>
<p>The Kahan summation works by maintaining the sum in two registers, the actual
bulk sum and a small error correcting term $c$. If you were using infinitely
precise arithmetic $c$ would always be zero, but with floating-point it might
not be. The downside is that each number now takes four operations to add to the
sum instead of just one.</p>
<p>To mitigate this we can do something similar to what we did with the pairwise
summation. We can first accumulate blocks into sums naively before combining the
block sums with Kaham summation to reduce overhead at the cost of accuracy:</p>
<pre data-lang="rust"><code data-lang="rust"><span>pub </span><span>fn </span><span>block_kahan_sum(arr: &amp;[</span><span>f32</span><span>]) -&gt; </span><span>f32 </span><span>{
</span><span>    </span><span>let </span><span>mut sum = </span><span>0.0</span><span>;
</span><span>    </span><span>let </span><span>mut c = </span><span>0.0</span><span>;
</span><span>    </span><span>for</span><span> chunk in arr.chunks(</span><span>256</span><span>) {
</span><span>        </span><span>let</span><span> x = naive_sum(chunk);
</span><span>        </span><span>let</span><span> y = x - c;
</span><span>        </span><span>let</span><span> t = sum + y;
</span><span>        c = (t - sum) - y;
</span><span>        sum = t;
</span><span>    }
</span><span>    sum
</span><span>}
</span></code></pre>
<h2 id="exact-summation"><a href="#exact-summation" aria-label="Anchor link for: exact-summation">Exact summation</a></h2>
<p>I know of at least two general methods to produce the <em>correctly-rounded</em> sum of a sequence
of floating-point numbers. That is, it logically computes the sum with
infinite precision before rounding it back to a floating-point value at the end.</p>
<p>The first method is based on the <a href="https://en.wikipedia.org/wiki/2Sum">2Sum</a>
primitive which is an error-free transform from two numbers $x, y$ to $s, t$
such that $x + y = s + t$, where $t$ is a small error. By applying this
repeatedly until the errors vanish you can get a correctly-rounded sum.
Keeping track of what to add in what order can be tricky, and the worst-case
requires $O(n^2)$ additions to make all the terms vanish. This is what’s
implemented in Python’s <a href="https://github.com/python/cpython/blob/de19694cfbcaa1c85c3a4b7184a24ff21b1c0919/Modules/mathmodule.c#L1321"><code>math.fsum</code></a>
and in the Rust crate <a href="https://docs.rs/fsum/latest/fsum/"><code>fsum</code></a> which use
extra memory to keep the partial sums around. The <a href="https://docs.rs/accurate/latest/accurate/"><code>accurate</code></a> crate also
implements this using in-place mutation in <a href="https://docs.rs/accurate/latest/accurate/sum/fn.i_fast_sum_in_place.html"><code>i_fast_sum_in_place</code></a>.</p>
<p>Another method is to keep a large buffer of integers around, one per exponent.
Then when adding a floating-point number you decompose it into a an exponent
and mantissa, and add the mantissa to the corresponding integer in the buffer. 
If the integer <code>buf[i]</code> overflows you increment the integer in <code>buf[i + w]</code>,
where <code>w</code> is the width of your integer.</p>
<p>This can actually compute a completely exact sum, without any rounding at all,
and is effectively just an overly permissive representation of a fixed-point
number optimized for accumulating floats. This latter method is $O(n)$ time, but
uses a large but constant amount of memory ($\approx$ 1 KB for <code>f32</code>, $\approx$
16 KB for <code>f64</code>). An advantage of this method is that it’s also an online
algorithm - both adding a number to the sum and getting the current total are
amortized $O(1)$.</p>
<p>A variant of this method is implemented in the <a href="https://docs.rs/accurate/latest/accurate/"><code>accurate</code></a>
crate
as <a href="https://docs.rs/accurate/latest/accurate/sum/struct.OnlineExactSum.html"><code>OnlineExactSum</code></a>
crate which uses floats instead of integers for the buffer.</p>
<h2 id="unleashing-the-compiler"><a href="#unleashing-the-compiler" aria-label="Anchor link for: unleashing-the-compiler">Unleashing the compiler</a></h2>
<p>Besides accuracy, there is another problem with <code>naive_sum</code>. The Rust compiler
is not allowed to reorder floating-point additions, because floating-point
addition is not associative. So it cannot autovectorize the <code>naive_sum</code> to use
SIMD instructions to compute the sum, nor use instruction-level parallelism.</p>
<p>To solve this there are compiler intrinsics in Rust that do float sums while
allowing associativity, such as
<a href="https://doc.rust-lang.org/nightly/std/intrinsics/fn.fadd_fast.html"><code>std::intrinsics::fadd_fast</code></a>.
However, these instructions are <em>incredibly dangerous</em>, as they assume that both
the input and output are finite numbers (no infinities, no NaNs), or otherwise
they are undefined behavior. This functionally makes them unusable, as only in
the most restricted scenarios when computing a sum do you know that all inputs
are finite numbers, and that their sum cannot overflow.</p>
<p>I recently uttered my annoyance 
with these operators to <a href="https://github.com/saethlin">Ben Kimock</a>, and together
we proposed (and he implemented) a new set of operators:
<a href="https://doc.rust-lang.org/nightly/std/intrinsics/fn.fadd_algebraic.html"><code>std::intrinsics::fadd_algebraic</code></a>
and <a href="https://doc.rust-lang.org/nightly/std/intrinsics/fn.fadd_algebraic.html?search=_algebraic">friends</a>.</p>
<p>I proposed we call the operators <em>algebraic</em>, as they allow (in theory) any
transformation that is justified by real algebra. For example, substituting
${x - x \to 0}$, ${cx + cy \to c(x + y)}$, or ${x^6 \to (x^2)^3.}$
In general these operators are treated as-if they are done using real numbers,
and can map to any set of floating-point instructions that would be equivalent
to the original expression, assuming the floating-point instructions would be
exact.</p>

<p>Using those new instructions it is trivial to generate an autovectorized sum:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#![allow(internal_features)]
</span><span>#![feature(core_intrinsics)]
</span><span>use </span><span>std::intrinsics::fadd_algebraic;
</span><span>
</span><span>fn </span><span>naive_sum_autovec(arr: &amp;[</span><span>f32</span><span>]) -&gt; </span><span>f32 </span><span>{
</span><span>    </span><span>let </span><span>mut out = </span><span>0.0</span><span>;
</span><span>    </span><span>for</span><span> x in arr {
</span><span>        out = fadd_algebraic(out, *x);
</span><span>    }
</span><span>    out
</span><span>}
</span></code></pre>
<p>If we compile with <code>-C target-cpu=broadwell</code> we see that the compiler
automatically generated the following tight loop for us, using 4 accumulators
and AVX2 instructions:</p>
<pre data-lang="asm"><code data-lang="asm"><span>.LBB0_5:
</span><span>    </span><span>vaddps  </span><span>ymm0, ymm0, ymmword ptr [rdi + </span><span>4</span><span>*r8]
</span><span>    </span><span>vaddps  </span><span>ymm1, ymm1, ymmword ptr [rdi + </span><span>4</span><span>*r8 + </span><span>32</span><span>]
</span><span>    </span><span>vaddps  </span><span>ymm2, ymm2, ymmword ptr [rdi + </span><span>4</span><span>*r8 + </span><span>64</span><span>]
</span><span>    </span><span>vaddps  </span><span>ymm3, ymm3, ymmword ptr [rdi + </span><span>4</span><span>*r8 + </span><span>96</span><span>]
</span><span>    </span><span>add     </span><span>r8, </span><span>32
</span><span>    </span><span>cmp     </span><span>rdx, r8
</span><span>    </span><span>jne     </span><span>.LBB0_5
</span></code></pre>
<p>This will process 128 bytes of floating-point data (so 32 elements) in 7
instructions. Additionally, all the <code>vaddps</code> instructions are independent of
each other as they accumulate to different registers. If we analyze this with
<a href="https://uica.uops.info/">uiCA</a> we see that it estimates the above loop to take
4 cycles to complete, processing 32 bytes / cycle. At 4GHz that’s up to 128GB/s!
Note that that’s way above what my machine’s RAM bandwidth is, so you will only
achieve that speed when summing data that is already in cache.</p>
<p>With this in mind we can also easily define <code>block_pairwise_sum_autovec</code> and
<code>block_kahan_sum_autovec</code> by replacing their calls to <code>naive_sum</code> with
<code>naive_sum_autovec</code>.</p>
<h2 id="accuracy-and-speed"><a href="#accuracy-and-speed" aria-label="Anchor link for: accuracy-and-speed">Accuracy and speed</a></h2>
<p>Let’s take a look at how the different summation methods compare. As a
relatively arbitrary benchmark, let’s sum 100,000 random floats ranging from
-100,000 to +100,000. This is 400 KB worth of data, so it still fits in cache on
my AMD Threadripper 2950x.</p>
<p>All the code is available on <a href="https://github.com/orlp/sum-bench">Github</a>.
Compiled with <code>RUSTFLAGS=-C target-cpu=native</code> and <code>--release</code> I get the
following results:</p>
<table><thead><tr><th>Algorithm</th><th>Throughput</th><th>Mean absolute error</th></tr></thead><tbody>
<tr><td><code>naive</code></td><td>5.5 GB/s</td><td>71.796</td></tr>
<tr><td><code>pairwise</code></td><td>0.9 GB/s</td><td>1.5528</td></tr>
<tr><td><code>kahan</code></td><td>1.4 GB/s</td><td>0.2229</td></tr>
<tr><td><code>block_pairwise</code></td><td>5.8 GB/s</td><td>3.8597</td></tr>
<tr><td><code>block_kahan</code></td><td>5.9 GB/s</td><td>4.2184</td></tr>
<tr><td><code>naive_autovec</code></td><td>118.6 GB/s</td><td>14.538</td></tr>
<tr><td><code>block_pairwise_autovec</code></td><td>71.7 GB/s</td><td>1.6132</td></tr>
<tr><td><code>block_kahan_autovec</code></td><td>98.0 GB/s</td><td>1.2306</td></tr>
<tr><td><code>crate_accurate_buffer</code></td><td>1.1 GB/s</td><td>0.0015</td></tr>
<tr><td><code>crate_accurate_inplace</code></td><td>1.9 GB/s</td><td>0.0015</td></tr>
<tr><td><code>crate_fsum</code></td><td>1.2 GB/s</td><td>0.0000</td></tr>
</tbody></table>

<p>First I’d like to note that there’s <strong>more than a 100x</strong> performance difference
between the fastest and slowest method. For summing an array! Now this might not
be entirely fair as the slowest methods are computing something significantly
harder, but there’s still a 20x performance difference between a seemingly
reasonable naive implementation and the fastest one.</p>
<p>We find that in general the <code>_autovec</code> methods that use <code>fadd_algebraic</code> are
faster <em>and</em> more accurate than the ones using regular floating-point addition.
The reason they’re more accurate as well is the same reason a pairwise sum is
more accurate: any reordering of the additions is better as the default
long-chain-of-additions is already the worst case for accuracy in a sum.</p>
<p>Limiting ourselves to <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto-optimal</a> choices
we get the following four implementations:</p>
<table><thead><tr><th>Algorithm</th><th>Throughput</th><th>Mean absolute error</th></tr></thead><tbody>
<tr><td><code>naive_autovec</code></td><td>118.6 GB/s</td><td>14.538</td></tr>
<tr><td><code>block_kahan_autovec</code></td><td>98.0 GB/s</td><td>1.2306</td></tr>
<tr><td><code>crate_accurate_inplace</code></td><td>1.9 GB/s</td><td>0.0015</td></tr>
<tr><td><code>crate_fsum</code></td><td>1.2 GB/s</td><td>0.0000</td></tr>
</tbody></table>
<p>Note that implementation differences can be quite impactful, and there are
likely dozens more methods of compensated summing I did not compare here.</p>
<p>For most cases I think <code>block_kahan_autovec</code> wins here, having good accuracy
(that doesn’t degenerate with larger inputs) at nearly the maximum speed. For
most applications the extra accuracy from the correctly-rounded sums is
unnecessary, and they are 50-100x slower.</p>
<p>By splitting the loop up into an explicit remainder plus a tight loop of
256-element sums we can squeeze out a bit more performance, and avoid a couple
floating-point ops for the last chunk:</p>
<pre data-lang="rust"><code data-lang="rust"><span>#![allow(internal_features)]
</span><span>#![feature(core_intrinsics)]
</span><span>use </span><span>std::intrinsics::fadd_algebraic;
</span><span>
</span><span>fn </span><span>sum_block(arr: &amp;[</span><span>f32</span><span>]) -&gt; </span><span>f32 </span><span>{
</span><span>    arr.iter().fold(</span><span>0.0</span><span>, |x, y| fadd_algebraic(x, *y))
</span><span>}
</span><span>
</span><span>pub </span><span>fn </span><span>sum_orlp(arr: &amp;[</span><span>f32</span><span>]) -&gt; </span><span>f32 </span><span>{
</span><span>    </span><span>let </span><span>mut chunks = arr.chunks_exact(</span><span>256</span><span>);
</span><span>    </span><span>let </span><span>mut sum = </span><span>0.0</span><span>;
</span><span>    </span><span>let </span><span>mut c = </span><span>0.0</span><span>;
</span><span>    </span><span>for</span><span> chunk in &amp;mut chunks {
</span><span>        </span><span>let</span><span> y = sum_block(chunk) - c;
</span><span>        </span><span>let</span><span> t = sum + y;
</span><span>        c = (t - sum) - y;
</span><span>        sum = t;
</span><span>    }
</span><span>    sum + (sum_block(chunks.remainder()) - c)
</span><span>}
</span></code></pre>
<table><thead><tr><th>Algorithm</th><th>Throughput</th><th>Mean absolute error</th></tr></thead><tbody>
<tr><td><code>sum_orlp</code></td><td>112.2 GB/s</td><td>1.2306</td></tr>
</tbody></table>
<p>You can of course tweak the number 256, I found that using 128 was $\approx$ 20%
slower, and that 512 didn’t really improve performance but did cost accuracy.</p>
<h2 id="conclusion"><a href="#conclusion" aria-label="Anchor link for: conclusion">Conclusion</a></h2>
<p>I think the <code>fadd_algebraic</code> and similar algebraic intrinsics are very useful
for achieving high-speed floating-point routines, and that other languages
should add them as well. A global <code>-ffast-math</code> is not good enough, as we’ve
seen above the best implementation was a hybrid between automatically optimized
math for speed, and manually implemented non-associative compensated operations.</p>
<p>Finally, if you are using LLVM, beware of <code>-ffast-math</code>. It is <strong>undefined
behavior</strong> to produce a NaN or infinity while that flag is set in LLVM. I have
no idea why they chose this hardcore stance which makes virtually every program
that uses it unsound. If you are targetting LLVM with your language, avoid the
<code>nnan</code> and <code>ninf</code> <a href="https://llvm.org/docs/LangRef.html#fastmath">fast-math flags</a>.</p>

</article></div>
  </body>
</html>
