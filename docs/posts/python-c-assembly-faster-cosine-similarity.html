<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ashvardanian.com/posts/python-c-assembly-comparison/">Original</a>
    <h1>Python, C, Assembly ‚Äì Faster Cosine Similarity</h1>
    
    <div id="readability-page-1" class="page"><div><p><a href="https://ashvardanian.com/tags/less-slow/">In this fourth article of the ‚ÄúLess Slow‚Äù series</a>, I‚Äôm accelerating <a href="https://github.com/unum-cloud">Unum‚Äôs open-source Vector Search primitives</a> used by some great database and cloud providers to replace Meta‚Äôs <a href="https://github.com/facebookresearch/faiss">FAISS</a> and scale-up search in their products. This time, our focus is on the most frequent operation for these tasks - computing the the Cosine Similarity/Distance between two vectors. It‚Äôs so common, even doubling it‚Äôs performance can have a noticeable impact on applications economics. But compared to a pure Python baseline our <strong>single-threaded performance grew by a factor of 2&#39;500x</strong>. Let‚Äôs go through optimizations one by one:</p><ul><li><a href="#python">Kicking off with basic Python</a>.</li><li><a href="#numpy-between-python-and-c">Noticed a 35x slowdown when not using NumPy right</a>.</li><li><a href="#scipy">Managed a 2-5x speed boost with SciPy</a>.</li><li><a href="#c">Jumped to a 200x speedup using C</a>.</li><li><a href="#assembly">A big leap to 400x faster with SIMD intrinsics</a>.</li><li><a href="#bmi2-and-goto">Touched 747x combining AVX-512 and BMI2</a>.</li><li><a href="#fp16">Climbed to 1,260x adding AVX-512FP16</a>.</li><li><a href="#vnni">The final high: 2,521x faster with AVX-512VNNI</a>.</li></ul><p><img loading="lazy" src="https://ashvardanian.com/python-c-assembly-comparison/python-vs-c-vs-assembly.png" alt="Cover"/></p><p>Some highlights:</p><ul><li>Again, <code>goto</code> doesn‚Äôt get the love it deserves in C.</li><li>BMI2 instructions on x86 are consistently overlooked‚Ä¶ thanks to AMD Zen2, I guess.</li><li>AVX-512FP16 is probably the most important extension in the current AI race.</li></ul><blockquote><p>I‚Äôm still scratching my head on the <strong>4VNNI</strong> extensions of AVX-512, and couldn‚Äôt find a good way to benefit from them here or even in the polynomial approximations of the <a href="https://ashvardanian.com/posts/gcc-12-vs-avx512fp16/">Jensen Shannon divergence computations in the last post</a>, so please let me know where I should try them ü§ó</p></blockquote><h2 id="cosine-similarity">Cosine Similarity</h2><p>Cosine Similarity is a way to check if two ‚Äúvectors‚Äù are looking in the same direction, regardless of their magnitude. It is a widespread metric used in Machine Learning and Information Retrieval, and it is defined as:</p><p>$$
\cos(\theta) = \frac{A \cdot B}{|A| |B|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
$$</p><p>Where $A$ and $B$ are two vectors with $n$ dimensions. The cosine similarity is a value between $-1$ and $1$, where $1$ means that the two vectors are pointing in the same direction, $-1$ implies that they are pointing in opposite directions and $0$ means that they are orthogonal. Cosine Distance, in turn, is a distance function, which is defined as $1 - \cos(\theta)$. It is a value between $0$ and $2$, where $0$ means that the two vectors are identical, and $2$ means that they are pointing in opposite directions. <em>I may use the terms interchangeably, but they are not exactly the same.</em></p><h2 id="python">Python</h2><p>The first implementation is the most naive one, written in pure Python‚Ä¶ by ChatGPT. It is a direct translation of the mathematical formula and is very easy to read and understand.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-0-1"><a href="#hl-0-1">1</a>
</span><span id="hl-0-2"><a href="#hl-0-2">2</a>
</span><span id="hl-0-3"><a href="#hl-0-3">3</a>
</span><span id="hl-0-4"><a href="#hl-0-4">4</a>
</span><span id="hl-0-5"><a href="#hl-0-5">5</a>
</span><span id="hl-0-6"><a href="#hl-0-6">6</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>cosine_distance</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
</span></span><span><span>    <span>dot_product</span> <span>=</span> <span>sum</span><span>(</span><span>ai</span> <span>*</span> <span>bi</span> <span>for</span> <span>ai</span><span>,</span> <span>bi</span> <span>in</span> <span>zip</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>))</span>
</span></span><span><span>    <span>magnitude_a</span> <span>=</span> <span>math</span><span>.</span><span>sqrt</span><span>(</span><span>sum</span><span>(</span><span>ai</span> <span>*</span> <span>ai</span> <span>for</span> <span>ai</span> <span>in</span> <span>a</span><span>))</span>
</span></span><span><span>    <span>magnitude_b</span> <span>=</span> <span>math</span><span>.</span><span>sqrt</span><span>(</span><span>sum</span><span>(</span><span>bi</span> <span>*</span> <span>bi</span> <span>for</span> <span>bi</span> <span>in</span> <span>b</span><span>))</span>
</span></span><span><span>    <span>cosine_similarity</span> <span>=</span> <span>dot_product</span> <span>/</span> <span>(</span><span>magnitude_a</span> <span>*</span> <span>magnitude_b</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>1</span> <span>-</span> <span>cosine_similarity</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>I‚Äôll run on 4th Gen Intel Xeon CPUs, codenamed Sapphire Rapids, and available on AWS as the <code>r7iz</code> instances. Before running a benchmark, let‚Äôs generate random numbers and put them into a list, simulating the 1536-dimensional ‚Äúembeddings‚Äù from the OpenAI Ada service.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-1-1"><a href="#hl-1-1">1</a>
</span><span id="hl-1-2"><a href="#hl-1-2">2</a>
</span><span id="hl-1-3"><a href="#hl-1-3">3</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="py"><span><span><span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>0</span><span>)</span>
</span></span><span><span><span>a_list</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>rand</span><span>(</span><span>1536</span><span>)</span><span>.</span><span>tolist</span><span>()</span>
</span></span><span><span><span>b_list</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>rand</span><span>(</span><span>1536</span><span>)</span><span>.</span><span>tolist</span><span>()</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>Running the benchmark with the <code>%timeit</code> utility, we get <strong>93.2 ¬µs</strong> ¬± 1.75 ¬µs per loop.. Or roughly 100 microseconds per call. Is that good or bad?</p><p>Our solution is Pythonic but inefficient, as it traverses each <code>list</code> twice. So let‚Äôs use the common Pythonic <code>zip</code> idiom:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-2-1"><a href="#hl-2-1"> 1</a>
</span><span id="hl-2-2"><a href="#hl-2-2"> 2</a>
</span><span id="hl-2-3"><a href="#hl-2-3"> 3</a>
</span><span id="hl-2-4"><a href="#hl-2-4"> 4</a>
</span><span id="hl-2-5"><a href="#hl-2-5"> 5</a>
</span><span id="hl-2-6"><a href="#hl-2-6"> 6</a>
</span><span id="hl-2-7"><a href="#hl-2-7"> 7</a>
</span><span id="hl-2-8"><a href="#hl-2-8"> 8</a>
</span><span id="hl-2-9"><a href="#hl-2-9"> 9</a>
</span><span id="hl-2-10"><a href="#hl-2-10">10</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="py"><span><span><span>def</span> <span>cosine_distance</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
</span></span><span><span>    <span>dot_product</span> <span>=</span> <span>0</span>
</span></span><span><span>    <span>magnitude_a</span> <span>=</span> <span>0</span>
</span></span><span><span>    <span>magnitude_b</span> <span>=</span> <span>0</span>
</span></span><span><span>    <span>for</span> <span>ai</span><span>,</span> <span>bi</span> <span>in</span> <span>zip</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
</span></span><span><span>        <span>dot_product</span> <span>+=</span> <span>ai</span> <span>*</span> <span>bi</span>
</span></span><span><span>        <span>magnitude_a</span> <span>+=</span> <span>ai</span> <span>*</span> <span>ai</span>
</span></span><span><span>        <span>magnitude_b</span> <span>+=</span> <span>bi</span> <span>*</span> <span>bi</span>
</span></span><span><span>    <span>cosine_similarity</span> <span>=</span> <span>dot_product</span> <span>/</span> <span>(</span><span>magnitude_a</span> <span>*</span> <span>magnitude_b</span><span>)</span>
</span></span><span><span>    <span>return</span> <span>1</span> <span>-</span> <span>cosine_similarity</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>Running the benchmark, we get <strong>65.3 ¬µs</strong> ¬± 716 ns per loop., resulting <strong>30% savings!</strong> I believe it‚Äôs a fair baseline.</p><blockquote><p>As <a href="https://news.ycombinator.com/item?id=38684461">pointed on HackerNews</a>, I forgot to apply the square root for <code>magnitude_a</code> and <code>magnitude_b</code>.</p></blockquote><h2 id="numpy-between-python-and-c">NumPy: Between Python and C</h2><p>NumPy is a powerful tool in Python‚Äôs toolbox, helping folks work fast with arrays. Many people see it as the go-to for doing science stuff in Python. A lot of machine learning tools lean on it. Since it‚Äôs built with C, you‚Äôd think it‚Äôd be speedy. Let‚Äôs take our basic Python lists and make them sharper with NumPy. We‚Äôre looking at single-precision, half-precision numbers, and 8-bit integers.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-3-1"><a href="#hl-3-1">1</a>
</span><span id="hl-3-2"><a href="#hl-3-2">2</a>
</span><span id="hl-3-3"><a href="#hl-3-3">3</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="py"><span><span><span>floats</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>rand</span><span>(</span><span>1536</span><span>)</span><span>.</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>float32</span><span>)</span>
</span></span><span><span><span>halfs</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>rand</span><span>(</span><span>1536</span><span>)</span><span>.</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>float16</span><span>)</span>
</span></span><span><span><span>ints</span> <span>=</span> <span>(</span><span>np</span><span>.</span><span>random</span><span>.</span><span>rand</span><span>(</span><span>1536</span><span>)</span> <span>*</span> <span>100</span><span>)</span><span>.</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>int8</span><span>)</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>These are popular choices for quantization (making data smaller) in tools like <a href="https://github.com/facebookresearch/faiss">Meta‚Äôs FAISS</a> and <a href="https://github.com/unum-cloud/usearch">Unum‚Äôs USearch</a>. Half-precision is pretty handy, working well most of the time. But using integers? That depends on the AI model you‚Äôre using. Thanks to <a href="https://www.tensorflow.org/model_optimization/guide/quantization/training">Quantization Aware Training</a>, two of my faves ‚Äî <a href="https://huggingface.co/intfloat/e5-base-v2">Microsoft‚Äôs E5</a> for just text and <a href="https://github.com/unum-cloud/uform">Unum‚Äôs UForm</a> for multi-modal data ‚Äî work well even compressed to 8-bit numbers.</p><hr/><p>After getting our vectors set up, I used our <code>cosine_distance</code> function to see how similar the three arrays are:</p><ul><li><code>floats</code>: <strong>349 ¬µs</strong> ¬± 5.71 ¬µs per loop.</li><li><code>halfs</code>: <strong>525 ¬µs</strong> ¬± 9.69 ¬µs per loop.</li><li><code>ints</code>: <strong>2.31 ms</strong> ¬± 26 ¬µs per loop.</li></ul><p>But here‚Äôs the problem. Instead of getting faster, things went <strong>35x slower</strong> than our <strong>65.3 ¬µs</strong> starting point. Why?</p><ol><li><p><strong>Memory Management</strong>: Sure, NumPy uses C arrays, which are cool. But every time we loop through them, we turn small byte stuff into bigger Python stuff. And with memory being unpredictable, it‚Äôs surprising things didn‚Äôt go even slower.</p></li><li><p><strong>Half-Precision Support</strong>: Most new devices support half-precision. But the software side? Not so much. Only a few AI tools use it, and they often focus on GPU stuff, leaving out the CPU. So, converting half-precision things on the go can be slow.</p></li><li><p><strong>Integer Overflows</strong>: Doing math with our tiny integers isn‚Äôt smooth. We keep getting these annoying overflow warnings. The CPU spends more time checking things than actually doing the math. We often see things like: <em>‚ÄúRuntimeWarning: overflow encountered in scalar multiply‚Äù</em>.</p></li></ol><p>Here‚Äôs a tip: If you‚Äôre using NumPy, go all in. Mixing it with regular Python can really slow you down!</p><h2 id="scipy">SciPy</h2><p>NumPy is also the foundation of the SciPy library, which provides many valuable functions for scientific computing, including the <code>scipy.distance.spatial.cosine</code>. It will use the native NumPy operations for as much as possible.</p><ul><li><code>floats</code>: <strong>15.8 ¬µs</strong> ¬± 416 ns per loop.</li><li><code>halfs</code>: <strong>46.6 ¬µs</strong> ¬± 291 ns per loop.</li><li><code>ints</code>: <strong>12.2 ¬µs</strong> ¬± 37.5 ns per loop.</li></ul><p>Now, we see the true potential of NumPy, and the underlying Basic Linear Algebra Subroutines (BLAS) libraries implemented in C, Fortran, and Assembly. Our pure Python baseline was <strong>65.3 ¬µs</strong>, and we now got 2-5 times faster, depending on the data type. Notably, <code>halfs</code> are still slow. Checking the specs of a similar CPU, we can clearly see support for <code>f16c</code> instructions, which means that the CPU can at least decode half-precision values without software emulation, and we shouldn‚Äôt be experiencing this much throttling.</p><p><a href="https://www.techpowerup.com/cpu-specs/xeon-platinum-8470q.c3091"><img loading="lazy" src="https://ashvardanian.com/python-c-assembly-comparison/sapphire-rapids-specs.png" alt="Sapphire Rapids Features"/></a></p><h2 id="c">C</h2><p>C is the lowest-level hardware-agnostic programming language - hence the best way to optimize small numerical functions, like the Cosine Similarity and Distance.
It is also trivial to wrap C functions into pure CPython bindings for the default Python runtime, and if you use the FastCall convention, like we do, you can make your custom code as fast as the built-in Python functions, like what I‚Äôve described recently with StringZilla, replacing Python‚Äôs default <code>str</code> string class with a faster alternative.
Unlike C++, however, C doesn‚Äôt support ‚Äúgenerics‚Äù or ‚Äútemplate functions‚Äù.
So we have to separately implement the <code>cosine_distance</code> function for each data type we want to support, or use the ugly ‚Äúmacros‚Äù:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-4-1"><a href="#hl-4-1"> 1</a>
</span><span id="hl-4-2"><a href="#hl-4-2"> 2</a>
</span><span id="hl-4-3"><a href="#hl-4-3"> 3</a>
</span><span id="hl-4-4"><a href="#hl-4-4"> 4</a>
</span><span id="hl-4-5"><a href="#hl-4-5"> 5</a>
</span><span id="hl-4-6"><a href="#hl-4-6"> 6</a>
</span><span id="hl-4-7"><a href="#hl-4-7"> 7</a>
</span><span id="hl-4-8"><a href="#hl-4-8"> 8</a>
</span><span id="hl-4-9"><a href="#hl-4-9"> 9</a>
</span><span id="hl-4-10"><a href="#hl-4-10">10</a>
</span><span id="hl-4-11"><a href="#hl-4-11">11</a>
</span><span id="hl-4-12"><a href="#hl-4-12">12</a>
</span><span id="hl-4-13"><a href="#hl-4-13">13</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="c"><span><span><span>#define SIMSIMD_MAKE_COS(name, input_type, accumulator_type, converter)                                                \
</span></span></span><span><span><span>    inline static simsimd_f32_t simsimd_##name##_##input_type##_cos(                                                   \
</span></span></span><span><span><span>        simsimd_##input_type##_t const* a, simsimd_##input_type##_t const* b, simsimd_size_t n) {                      \
</span></span></span><span><span><span>        simsimd_##accumulator_type##_t ab = 0, a2 = 0, b2 = 0;                                                         \
</span></span></span><span><span><span>        for (simsimd_size_t i = 0; i != n; ++i) {                                                                      \
</span></span></span><span><span><span>            simsimd_##accumulator_type##_t ai = converter(a[i]);                                                       \
</span></span></span><span><span><span>            simsimd_##accumulator_type##_t bi = converter(b[i]);                                                       \
</span></span></span><span><span><span>            ab += ai * bi;                                                                                             \
</span></span></span><span><span><span>            a2 += ai * ai;                                                                                             \
</span></span></span><span><span><span>            b2 += bi * bi;                                                                                             \
</span></span></span><span><span><span>        }                                                                                                              \
</span></span></span><span><span><span>        return ab != 0 ? 1 - ab * SIMSIMD_RSQRT(a2) * SIMSIMD_RSQRT(b2) : 1;                                           \
</span></span></span><span><span><span>    }
</span></span></span></code></pre></td></tr></tbody></table></div></div><p>This is a real snippet from the library and depends on yet another macro - <code>SIMSIMD_RSQRT(x)</code>, defined as the <code>1 / sqrtf(x)</code> by default.
We later instantiate it for all the data types we need:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-5-1"><a href="#hl-5-1">1</a>
</span><span id="hl-5-2"><a href="#hl-5-2">2</a>
</span><span id="hl-5-3"><a href="#hl-5-3">3</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="c"><span><span><span>SIMSIMD_MAKE_COS</span><span>(</span><span>serial</span><span>,</span> <span>f32</span><span>,</span> <span>f32</span><span>,</span> <span>SIMSIMD_IDENTIFY</span><span>)</span> <span>// `simsimd_serial_f32_cos`
</span></span></span><span><span><span></span><span>SIMSIMD_MAKE_COS</span><span>(</span><span>serial</span><span>,</span> <span>f16</span><span>,</span> <span>f32</span><span>,</span> <span>SIMSIMD_UNCOMPRESS_F16</span><span>)</span> <span>// `simsimd_serial_f16_cos`
</span></span></span><span><span><span></span><span>SIMSIMD_MAKE_COS</span><span>(</span><span>serial</span><span>,</span> <span>i8</span><span>,</span> <span>i32</span><span>,</span> <span>SIMSIMD_IDENTIFY</span><span>)</span> <span>// `simsimd_serial_i8_cos`
</span></span></span></code></pre></td></tr></tbody></table></div></div><p>Those macros will generate the following functions:</p><ul><li><code>simsimd_serial_f32_cos</code>: for 32-bit <code>floats</code>.</li><li><code>simsimd_serial_f16_cos</code>: for 16-bit <code>halfs</code>, accumulating 32-bit values.</li><li><code>simsimd_serial_i8_cos</code>: for 8-bit <code>ints</code>, accumulating 32-bit values.</li></ul><p>We benchmark those using the Google Benchmark library:</p><ul><li><code>floats</code>: 1956 ns ~ <strong>33x</strong> faster than Python.</li><li><code>halfs</code>: 1118 ns ~ <strong>58x</strong> faster than Python.</li><li><code>ints</code>: 299 ns ~ <strong>218x</strong> faster than Python.</li></ul><p>That‚Äôs a great result, but this code relies on the compiler to perform heavy lifting and produce efficient Assembly. As we know, sometimes even the most recent compilers, like <a href="https://ashvardanian.com/posts/gcc-12-vs-avx512fp16/">GCC 12, can be 119x slower than hand-written Assembly</a>. Even on the simplest data-parallel tasks, like computing the Jensen Shannon divergence of two discrete probability distributions.</p><blockquote><p lang="en" dir="ltr">I‚Äôve found a fairly simple data-parallel workload, where AVX-512FP16 <a href="https://twitter.com/hashtag/SIMD?src=hash&amp;ref_src=twsrc%5Etfw">#SIMD</a> code beats O3/fast-math assembly produced by GCC 12 by a factor of 118x ü§Øü§Øü§Ø<a href="https://t.co/cFftGbMBQe">https://t.co/cFftGbMBQe</a></p>‚Äî Ash Vardanian (@ashvardanian) <a href="https://twitter.com/ashvardanian/status/1716581551717433572?ref_src=twsrc%5Etfw">October 23, 2023</a></blockquote><h2 id="assembly">Assembly</h2><p>Assembly is the lowest-level programming language, and it is the closest to the hardware. It is also the most difficult to write and maintain, as it is not portable, and it is very easy to make mistakes. But it is also the most rewarding, as it allows us to write the most efficient code and to use the most advanced hardware features, like AVX-512. AVX-512, in turn, is not a monolith. It‚Äôs a very complex instruction set with the following subsets:</p><ul><li>AVX-512F: 512-bit SIMD instructions.</li><li>AVX-512DQ: double-precision floating-point instructions.</li><li>AVX-512BW: byte and word instructions.</li><li>AVX-512VL: vector length extensions.</li><li>AVX-512CD: conflict detection instructions.</li><li>AVX-512ER: exponential and reciprocal instructions.</li><li>AVX-512PF: prefetch instructions.</li><li>AVX-512VBMI: vector byte manipulation instructions.</li><li>AVX-512IFMA: integer fused multiply-add instructions.</li><li>AVX-512VBMI2: vector byte manipulation instructions 2.</li><li><strong>AVX-512VNNI</strong>: vector neural network instructions.</li><li>AVX-512BITALG: bit algorithms instructions.</li><li>AVX-512VPOPCNTDQ: vector population count instructions.</li><li>AVX-5124VNNIW: vector neural network instructions word variable precision.</li><li>AVX-5124FMAPS: fused multiply-add instructions single precision.</li><li>AVX-512VP2INTERSECT: vector pairwise intersect instructions.</li><li>AVX-512BF16: <code>bfloat16</code> instructions.</li><li><strong>AVX-512FP16</strong>: half-precision floating-point instructions.</li></ul><p>Luckily, we won‚Äôt need all of them today. If you are curious, you can read more about them in the <a href="https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html">Intel 64 and IA-32 Architectures Software Developer‚Äôs Manual</a>‚Ä¶ but be ready, it‚Äôs a very long read.</p><p>Moreover, we don‚Äôt have to write the Assembly per se, as we can use the <a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">SIMD Intrinsics</a> to essentially write the Assembly instructions without leaving C.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-6-1"><a href="#hl-6-1"> 1</a>
</span><span id="hl-6-2"><a href="#hl-6-2"> 2</a>
</span><span id="hl-6-3"><a href="#hl-6-3"> 3</a>
</span><span id="hl-6-4"><a href="#hl-6-4"> 4</a>
</span><span id="hl-6-5"><a href="#hl-6-5"> 5</a>
</span><span id="hl-6-6"><a href="#hl-6-6"> 6</a>
</span><span id="hl-6-7"><a href="#hl-6-7"> 7</a>
</span><span id="hl-6-8"><a href="#hl-6-8"> 8</a>
</span><span id="hl-6-9"><a href="#hl-6-9"> 9</a>
</span><span id="hl-6-10"><a href="#hl-6-10">10</a>
</span><span id="hl-6-11"><a href="#hl-6-11">11</a>
</span><span id="hl-6-12"><a href="#hl-6-12">12</a>
</span><span id="hl-6-13"><a href="#hl-6-13">13</a>
</span><span id="hl-6-14"><a href="#hl-6-14">14</a>
</span><span id="hl-6-15"><a href="#hl-6-15">15</a>
</span><span id="hl-6-16"><a href="#hl-6-16">16</a>
</span><span id="hl-6-17"><a href="#hl-6-17">17</a>
</span><span id="hl-6-18"><a href="#hl-6-18">18</a>
</span><span id="hl-6-19"><a href="#hl-6-19">19</a>
</span><span id="hl-6-20"><a href="#hl-6-20">20</a>
</span><span id="hl-6-21"><a href="#hl-6-21">21</a>
</span><span id="hl-6-22"><a href="#hl-6-22">22</a>
</span><span id="hl-6-23"><a href="#hl-6-23">23</a>
</span><span id="hl-6-24"><a href="#hl-6-24">24</a>
</span><span id="hl-6-25"><a href="#hl-6-25">25</a>
</span><span id="hl-6-26"><a href="#hl-6-26">26</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="c"><span><span><span>__attribute__</span><span>((</span><span>target</span><span>(</span><span>&#34;avx512f,avx512vl&#34;</span><span>)))</span> <span>//
</span></span></span><span><span><span></span><span>inline</span> <span>static</span> <span>simsimd_f32_t</span>
</span></span><span><span><span>simsimd_avx512_f32_cos</span><span>(</span><span>simsimd_f32_t</span> <span>const</span><span>*</span> <span>a</span><span>,</span> <span>simsimd_f32_t</span> <span>const</span><span>*</span> <span>b</span><span>,</span> <span>simsimd_size_t</span> <span>n</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>__m512</span> <span>ab_vec</span> <span>=</span> <span>_mm512_set1_ps</span><span>(</span><span>0</span><span>);</span>
</span></span><span><span>    <span>__m512</span> <span>a2_vec</span> <span>=</span> <span>_mm512_set1_ps</span><span>(</span><span>0</span><span>);</span>
</span></span><span><span>    <span>__m512</span> <span>b2_vec</span> <span>=</span> <span>_mm512_set1_ps</span><span>(</span><span>0</span><span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>for</span> <span>(</span><span>simsimd_size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>n</span><span>;</span> <span>i</span> <span>+=</span> <span>16</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>__mmask16</span> <span>mask</span> <span>=</span> <span>n</span> <span>-</span> <span>i</span> <span>&gt;=</span> <span>16</span> <span>?</span> <span>0xFFFF</span> <span>:</span> <span>((</span><span>1u</span> <span>&lt;&lt;</span> <span>(</span><span>n</span> <span>-</span> <span>i</span><span>))</span> <span>-</span> <span>1u</span><span>);</span>
</span></span><span><span>        <span>__m512</span> <span>a_vec</span> <span>=</span> <span>_mm512_maskz_loadu_ps</span><span>(</span><span>mask</span><span>,</span> <span>a</span> <span>+</span> <span>i</span><span>);</span>
</span></span><span><span>        <span>__m512</span> <span>b_vec</span> <span>=</span> <span>_mm512_maskz_loadu_ps</span><span>(</span><span>mask</span><span>,</span> <span>b</span> <span>+</span> <span>i</span><span>);</span>
</span></span><span><span>        <span>ab_vec</span> <span>=</span> <span>_mm512_fmadd_ps</span><span>(</span><span>a_vec</span><span>,</span> <span>b_vec</span><span>,</span> <span>ab_vec</span><span>);</span>
</span></span><span><span>        <span>a2_vec</span> <span>=</span> <span>_mm512_fmadd_ps</span><span>(</span><span>a_vec</span><span>,</span> <span>a_vec</span><span>,</span> <span>a2_vec</span><span>);</span>
</span></span><span><span>        <span>b2_vec</span> <span>=</span> <span>_mm512_fmadd_ps</span><span>(</span><span>b_vec</span><span>,</span> <span>b_vec</span><span>,</span> <span>b2_vec</span><span>);</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>
</span></span><span><span>    <span>simsimd_f32_t</span> <span>ab</span> <span>=</span> <span>_mm512_reduce_add_ps</span><span>(</span><span>ab_vec</span><span>);</span>
</span></span><span><span>    <span>simsimd_f32_t</span> <span>a2</span> <span>=</span> <span>_mm512_reduce_add_ps</span><span>(</span><span>a2_vec</span><span>);</span>
</span></span><span><span>    <span>simsimd_f32_t</span> <span>b2</span> <span>=</span> <span>_mm512_reduce_add_ps</span><span>(</span><span>b2_vec</span><span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>__m128d</span> <span>a2_b2</span> <span>=</span> <span>_mm_set_pd</span><span>((</span><span>double</span><span>)</span><span>a2</span><span>,</span> <span>(</span><span>double</span><span>)</span><span>b2</span><span>);</span>
</span></span><span><span>    <span>__m128d</span> <span>rsqrts</span> <span>=</span> <span>_mm_mask_rsqrt14_pd</span><span>(</span><span>_mm_setzero_pd</span><span>(),</span> <span>0xFF</span><span>,</span> <span>a2_b2</span><span>);</span>
</span></span><span><span>    <span>double</span> <span>rsqrts_array</span><span>[</span><span>2</span><span>];</span>
</span></span><span><span>    <span>_mm_storeu_pd</span><span>(</span><span>rsqrts_array</span><span>,</span> <span>rsqrts</span><span>);</span>
</span></span><span><span>    <span>return</span> <span>1</span> <span>-</span> <span>ab</span> <span>*</span> <span>rsqrts_array</span><span>[</span><span>0</span><span>]</span> <span>*</span> <span>rsqrts_array</span><span>[</span><span>1</span><span>];</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>Let‚Äôs start with a relatively simple implementation:</p><ul><li>A single <code>for</code>-loop iterates through 2 vectors, scanning up to 16 entries simultaneously.</li><li>When it reaches the end of the vectors, it uses a mask to load the remaining entries with the <code>((1u &lt;&lt; (n - i)) - 1u)</code> mask.</li><li>It doesn‚Äôt expect the vectors to be aligned, so it uses the <code>loadu</code> instructions.</li><li>It avoids separate additions using the <code>fmadd</code> instruction, which computes <code>a * b + c</code>.</li><li>It uses the <code>reduce_add</code> intrinsic to sum all the elements in the vector, which is not a SIMD code, but the compiler can optimize that part for us.</li><li>It uses the <code>rsqrt14</code> instruction to compute the reciprocal square root of the sum of <code>a2</code> and <code>b2</code>, very accurately approximating <code>1/sqrt(a2)</code> and <code>1/sqrt(b2)</code>, and avoiding LibC calls.</li></ul><p>Benchmarking this code and its symmetric counterparts for other data types, we get the following:</p><ul><li><code>floats</code>: 118 ns ~ <strong>553x</strong> faster than Python.</li><li><code>halfs</code>: 79 ns ~ <strong>827x</strong> faster than Python.</li><li><code>ints</code>: 158 ns ~ <strong>413x</strong> faster than Python.</li></ul><p>We are now in the higher three digits.</p><h3 id="bmi2-and-goto">BMI2 and <code>goto</code></h3><p>The world is full of prejudice and unfairness, and some of the biggest ones in programming are:</p><ul><li>considering a <code>goto</code> to be an anti-pattern.</li><li>not using the BMI2 family of assembly instructions.</li></ul><p>The first one is handy when optimizing low-level code and avoiding unnecessary branching. The second is a tiny set of <code>bzhi</code>, <code>pdep</code>, <code>pext</code>, and a few other, convenient for bit manipulation! We will restore fairness and introduce them to our code, replacing double-precision computations with single-precision.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-7-1"><a href="#hl-7-1"> 1</a>
</span><span id="hl-7-2"><a href="#hl-7-2"> 2</a>
</span><span id="hl-7-3"><a href="#hl-7-3"> 3</a>
</span><span id="hl-7-4"><a href="#hl-7-4"> 4</a>
</span><span id="hl-7-5"><a href="#hl-7-5"> 5</a>
</span><span id="hl-7-6"><a href="#hl-7-6"> 6</a>
</span><span id="hl-7-7"><a href="#hl-7-7"> 7</a>
</span><span id="hl-7-8"><a href="#hl-7-8"> 8</a>
</span><span id="hl-7-9"><a href="#hl-7-9"> 9</a>
</span><span id="hl-7-10"><a href="#hl-7-10">10</a>
</span><span id="hl-7-11"><a href="#hl-7-11">11</a>
</span><span id="hl-7-12"><a href="#hl-7-12">12</a>
</span><span id="hl-7-13"><a href="#hl-7-13">13</a>
</span><span id="hl-7-14"><a href="#hl-7-14">14</a>
</span><span id="hl-7-15"><a href="#hl-7-15">15</a>
</span><span id="hl-7-16"><a href="#hl-7-16">16</a>
</span><span id="hl-7-17"><a href="#hl-7-17">17</a>
</span><span id="hl-7-18"><a href="#hl-7-18">18</a>
</span><span id="hl-7-19"><a href="#hl-7-19">19</a>
</span><span id="hl-7-20"><a href="#hl-7-20">20</a>
</span><span id="hl-7-21"><a href="#hl-7-21">21</a>
</span><span id="hl-7-22"><a href="#hl-7-22">22</a>
</span><span id="hl-7-23"><a href="#hl-7-23">23</a>
</span><span id="hl-7-24"><a href="#hl-7-24">24</a>
</span><span id="hl-7-25"><a href="#hl-7-25">25</a>
</span><span id="hl-7-26"><a href="#hl-7-26">26</a>
</span><span id="hl-7-27"><a href="#hl-7-27">27</a>
</span><span id="hl-7-28"><a href="#hl-7-28">28</a>
</span><span id="hl-7-29"><a href="#hl-7-29">29</a>
</span><span id="hl-7-30"><a href="#hl-7-30">30</a>
</span><span id="hl-7-31"><a href="#hl-7-31">31</a>
</span><span id="hl-7-32"><a href="#hl-7-32">32</a>
</span><span id="hl-7-33"><a href="#hl-7-33">33</a>
</span><span id="hl-7-34"><a href="#hl-7-34">34</a>
</span><span id="hl-7-35"><a href="#hl-7-35">35</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="c"><span><span><span>__attribute__</span><span>((</span><span>target</span><span>(</span><span>&#34;avx512f,avx512vl,bmi2&#34;</span><span>)))</span> <span>//
</span></span></span><span><span><span></span><span>inline</span> <span>static</span> <span>simsimd_f32_t</span>
</span></span><span><span><span>simsimd_avx512_f32_cos</span><span>(</span><span>simsimd_f32_t</span> <span>const</span><span>*</span> <span>a</span><span>,</span> <span>simsimd_f32_t</span> <span>const</span><span>*</span> <span>b</span><span>,</span> <span>simsimd_size_t</span> <span>n</span><span>)</span> <span>{</span>
</span></span><span><span>    <span>__m512</span> <span>ab_vec</span> <span>=</span> <span>_mm512_set1_ps</span><span>(</span><span>0</span><span>);</span>
</span></span><span><span>    <span>__m512</span> <span>a2_vec</span> <span>=</span> <span>_mm512_set1_ps</span><span>(</span><span>0</span><span>);</span>
</span></span><span><span>    <span>__m512</span> <span>b2_vec</span> <span>=</span> <span>_mm512_set1_ps</span><span>(</span><span>0</span><span>);</span>
</span></span><span><span>    <span>__m512</span> <span>a_vec</span><span>,</span> <span>b_vec</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>simsimd_avx512_f32_cos_cycle</span><span>:</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>n</span> <span>&lt;</span> <span>16</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>__mmask16</span> <span>mask</span> <span>=</span> <span>_bzhi_u32</span><span>(</span><span>0xFFFFFFFF</span><span>,</span> <span>n</span><span>);</span>
</span></span><span><span>        <span>a_vec</span> <span>=</span> <span>_mm512_maskz_loadu_ps</span><span>(</span><span>mask</span><span>,</span> <span>a</span><span>);</span>
</span></span><span><span>        <span>b_vec</span> <span>=</span> <span>_mm512_maskz_loadu_ps</span><span>(</span><span>mask</span><span>,</span> <span>b</span><span>);</span>
</span></span><span><span>        <span>n</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>        <span>a_vec</span> <span>=</span> <span>_mm512_loadu_ps</span><span>(</span><span>a</span><span>);</span>
</span></span><span><span>        <span>b_vec</span> <span>=</span> <span>_mm512_loadu_ps</span><span>(</span><span>b</span><span>);</span>
</span></span><span><span>        <span>a</span> <span>+=</span> <span>16</span><span>,</span> <span>b</span> <span>+=</span> <span>16</span><span>,</span> <span>n</span> <span>-=</span> <span>16</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>    <span>ab_vec</span> <span>=</span> <span>_mm512_fmadd_ps</span><span>(</span><span>a_vec</span><span>,</span> <span>b_vec</span><span>,</span> <span>ab_vec</span><span>);</span>
</span></span><span><span>    <span>a2_vec</span> <span>=</span> <span>_mm512_fmadd_ps</span><span>(</span><span>a_vec</span><span>,</span> <span>a_vec</span><span>,</span> <span>a2_vec</span><span>);</span>
</span></span><span><span>    <span>b2_vec</span> <span>=</span> <span>_mm512_fmadd_ps</span><span>(</span><span>b_vec</span><span>,</span> <span>b_vec</span><span>,</span> <span>b2_vec</span><span>);</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>n</span><span>)</span>
</span></span><span><span>        <span>goto</span> <span>simsimd_avx512_f32_cos_cycle</span><span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>simsimd_f32_t</span> <span>ab</span> <span>=</span> <span>_mm512_reduce_add_ps</span><span>(</span><span>ab_vec</span><span>);</span>
</span></span><span><span>    <span>simsimd_f32_t</span> <span>a2</span> <span>=</span> <span>_mm512_reduce_add_ps</span><span>(</span><span>a2_vec</span><span>);</span>
</span></span><span><span>    <span>simsimd_f32_t</span> <span>b2</span> <span>=</span> <span>_mm512_reduce_add_ps</span><span>(</span><span>b2_vec</span><span>);</span>
</span></span><span><span>
</span></span><span><span>    <span>// Compute the reciprocal square roots of a2 and b2
</span></span></span><span><span><span></span>    <span>__m128</span> <span>rsqrts</span> <span>=</span> <span>_mm_rsqrt14_ps</span><span>(</span><span>_mm_set_ps</span><span>(</span><span>0.f</span><span>,</span> <span>0.f</span><span>,</span> <span>a2</span> <span>+</span> <span>1.e-9</span><span>f</span><span>,</span> <span>b2</span> <span>+</span> <span>1.e-9</span><span>f</span><span>));</span>
</span></span><span><span>    <span>simsimd_f32_t</span> <span>rsqrt_a2</span> <span>=</span> <span>_mm_cvtss_f32</span><span>(</span><span>rsqrts</span><span>);</span>
</span></span><span><span>    <span>simsimd_f32_t</span> <span>rsqrt_b2</span> <span>=</span> <span>_mm_cvtss_f32</span><span>(</span><span>_mm_shuffle_ps</span><span>(</span><span>rsqrts</span><span>,</span> <span>rsqrts</span><span>,</span> <span>_MM_SHUFFLE</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>)));</span>
</span></span><span><span>    <span>return</span> <span>1</span> <span>-</span> <span>ab</span> <span>*</span> <span>rsqrt_a2</span> <span>*</span> <span>rsqrt_b2</span><span>;</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>Result for <code>floats</code>: 87.4 ns ~ <strong>747x</strong> faster than Python.</p><blockquote><p>One of the reasons the BMI2 extensions didn‚Äôt take off originally is that, AMD processors before Zen 3 implemented <code>pdep</code> and <code>pext</code> with a latency of 18 cycles. Newer CPUs do it with the latency of 3 cycles.</p></blockquote><h3 id="fp16">FP16</h3><p>Ice Lake CPUs supported all the instructions we‚Äôve used so far. In contrast, the newer Sapphire Rapids instructions add native support for half-precision math.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-8-1"><a href="#hl-8-1"> 1</a>
</span><span id="hl-8-2"><a href="#hl-8-2"> 2</a>
</span><span id="hl-8-3"><a href="#hl-8-3"> 3</a>
</span><span id="hl-8-4"><a href="#hl-8-4"> 4</a>
</span><span id="hl-8-5"><a href="#hl-8-5"> 5</a>
</span><span id="hl-8-6"><a href="#hl-8-6"> 6</a>
</span><span id="hl-8-7"><a href="#hl-8-7"> 7</a>
</span><span id="hl-8-8"><a href="#hl-8-8"> 8</a>
</span><span id="hl-8-9"><a href="#hl-8-9"> 9</a>
</span><span id="hl-8-10"><a href="#hl-8-10">10</a>
</span><span id="hl-8-11"><a href="#hl-8-11">11</a>
</span><span id="hl-8-12"><a href="#hl-8-12">12</a>
</span><span id="hl-8-13"><a href="#hl-8-13">13</a>
</span><span id="hl-8-14"><a href="#hl-8-14">14</a>
</span><span id="hl-8-15"><a href="#hl-8-15">15</a>
</span><span id="hl-8-16"><a href="#hl-8-16">16</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="c"><span><span><span>simsimd_avx512_f16_cos_cycle</span><span>:</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>n</span> <span>&lt;</span> <span>32</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>__mmask32</span> <span>mask</span> <span>=</span> <span>_bzhi_u32</span><span>(</span><span>0xFFFFFFFF</span><span>,</span> <span>n</span><span>);</span>
</span></span><span><span>        <span>a_i16_vec</span> <span>=</span> <span>_mm512_maskz_loadu_epi16</span><span>(</span><span>mask</span><span>,</span> <span>a</span><span>);</span>
</span></span><span><span>        <span>b_i16_vec</span> <span>=</span> <span>_mm512_maskz_loadu_epi16</span><span>(</span><span>mask</span><span>,</span> <span>b</span><span>);</span>
</span></span><span><span>        <span>n</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>        <span>a_i16_vec</span> <span>=</span> <span>_mm512_loadu_epi16</span><span>(</span><span>a</span><span>);</span>
</span></span><span><span>        <span>b_i16_vec</span> <span>=</span> <span>_mm512_loadu_epi16</span><span>(</span><span>b</span><span>);</span>
</span></span><span><span>        <span>a</span> <span>+=</span> <span>32</span><span>,</span> <span>b</span> <span>+=</span> <span>32</span><span>,</span> <span>n</span> <span>-=</span> <span>32</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>    <span>ab_vec</span> <span>=</span> <span>_mm512_fmadd_ph</span><span>(</span><span>_mm512_castsi512_ph</span><span>(</span><span>a_i16_vec</span><span>),</span> <span>_mm512_castsi512_ph</span><span>(</span><span>b_i16_vec</span><span>),</span> <span>ab_vec</span><span>);</span>
</span></span><span><span>    <span>a2_vec</span> <span>=</span> <span>_mm512_fmadd_ph</span><span>(</span><span>_mm512_castsi512_ph</span><span>(</span><span>a_i16_vec</span><span>),</span> <span>_mm512_castsi512_ph</span><span>(</span><span>a_i16_vec</span><span>),</span> <span>a2_vec</span><span>);</span>
</span></span><span><span>    <span>b2_vec</span> <span>=</span> <span>_mm512_fmadd_ph</span><span>(</span><span>_mm512_castsi512_ph</span><span>(</span><span>b_i16_vec</span><span>),</span> <span>_mm512_castsi512_ph</span><span>(</span><span>b_i16_vec</span><span>),</span> <span>b2_vec</span><span>);</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>n</span><span>)</span>
</span></span><span><span>        <span>goto</span> <span>simsimd_avx512_f16_cos_cycle</span><span>;</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>The body of our function has changed:</p><ul><li>we are now scanning 32 entries per cycle, as the scalars are 2x smaller.</li><li>we are using the <code>_ph</code> intrinsics, the half-precision counterparts of the <code>_ps</code> intrinsics.</li></ul><p>Result for <code>halfs</code>: 51.8 ns ~ <strong>1&#39;260x</strong> faster than Python.</p><h3 id="vnni">VNNI</h3><p>Today‚Äôs final instruction we‚Äôll explore is the <code>vpdpbusd</code> from the AVX-512VNNI subset. It multiplies 8-bit integers, generating 16-bit intermediate results, which are then added to a 32-bit accumulator. With smaller scalars, we can house 64 of them in a single ZMM register.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-9-1"><a href="#hl-9-1"> 1</a>
</span><span id="hl-9-2"><a href="#hl-9-2"> 2</a>
</span><span id="hl-9-3"><a href="#hl-9-3"> 3</a>
</span><span id="hl-9-4"><a href="#hl-9-4"> 4</a>
</span><span id="hl-9-5"><a href="#hl-9-5"> 5</a>
</span><span id="hl-9-6"><a href="#hl-9-6"> 6</a>
</span><span id="hl-9-7"><a href="#hl-9-7"> 7</a>
</span><span id="hl-9-8"><a href="#hl-9-8"> 8</a>
</span><span id="hl-9-9"><a href="#hl-9-9"> 9</a>
</span><span id="hl-9-10"><a href="#hl-9-10">10</a>
</span><span id="hl-9-11"><a href="#hl-9-11">11</a>
</span><span id="hl-9-12"><a href="#hl-9-12">12</a>
</span><span id="hl-9-13"><a href="#hl-9-13">13</a>
</span><span id="hl-9-14"><a href="#hl-9-14">14</a>
</span><span id="hl-9-15"><a href="#hl-9-15">15</a>
</span><span id="hl-9-16"><a href="#hl-9-16">16</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="c"><span><span><span>simsimd_avx512_i8_cos_cycle</span><span>:</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>n</span> <span>&lt;</span> <span>64</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>__mmask64</span> <span>mask</span> <span>=</span> <span>_bzhi_u64</span><span>(</span><span>0xFFFFFFFFFFFFFFFF</span><span>,</span> <span>n</span><span>);</span>
</span></span><span><span>        <span>a_vec</span> <span>=</span> <span>_mm512_maskz_loadu_epi8</span><span>(</span><span>mask</span><span>,</span> <span>a</span><span>);</span>
</span></span><span><span>        <span>b_vec</span> <span>=</span> <span>_mm512_maskz_loadu_epi8</span><span>(</span><span>mask</span><span>,</span> <span>b</span><span>);</span>
</span></span><span><span>        <span>n</span> <span>=</span> <span>0</span><span>;</span>
</span></span><span><span>    <span>}</span> <span>else</span> <span>{</span>
</span></span><span><span>        <span>a_vec</span> <span>=</span> <span>_mm512_loadu_epi8</span><span>(</span><span>a</span><span>);</span>
</span></span><span><span>        <span>b_vec</span> <span>=</span> <span>_mm512_loadu_epi8</span><span>(</span><span>b</span><span>);</span>
</span></span><span><span>        <span>a</span> <span>+=</span> <span>64</span><span>,</span> <span>b</span> <span>+=</span> <span>64</span><span>,</span> <span>n</span> <span>-=</span> <span>64</span><span>;</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>    <span>ab_i32s_vec</span> <span>=</span> <span>_mm512_dpbusd_epi32</span><span>(</span><span>ab_i32s_vec</span><span>,</span> <span>a_vec</span><span>,</span> <span>b_vec</span><span>);</span>
</span></span><span><span>    <span>a2_i32s_vec</span> <span>=</span> <span>_mm512_dpbusd_epi32</span><span>(</span><span>a2_i32s_vec</span><span>,</span> <span>a_vec</span><span>,</span> <span>a_vec</span><span>);</span>
</span></span><span><span>    <span>b2_i32s_vec</span> <span>=</span> <span>_mm512_dpbusd_epi32</span><span>(</span><span>b2_i32s_vec</span><span>,</span> <span>b_vec</span><span>,</span> <span>b_vec</span><span>);</span>
</span></span><span><span>    <span>if</span> <span>(</span><span>n</span><span>)</span>
</span></span><span><span>        <span>goto</span> <span>simsimd_avx512_i8_cos_cycle</span><span>;</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>After compiling, you might expect to see three <code>vpdpbusd</code> invocations. However, the compiler had other ideas. Instead of sticking to our expected flow, it duplicated <code>vpdpbusd</code> calls in both branches of the <code>if</code> condition to minimize code sections and jumps. Here‚Äôs a glimpse of our main operational section:</p><pre tabindex="0"><code data-lang="assembly">.L2:
        ; Check `if (n &lt; 64)` and jump to the L7 if it&#39;s true.
        cmp     rax, rdx                            
        je      .L7
        ; Load 64 bytes from `a` and `b` into `zmm1` and `zmm0`.
        vmovdqu8        zmm1, ZMMWORD PTR [rdi]
        vmovdqu8        zmm0, ZMMWORD PTR [rsi]
        ; Increment `a` and `b` pointers by 64.
        add     rdi, 64
        add     rsi, 64
        ; Perform mixed-precision dot-products.
        vpdpbusd        zmm4, zmm1, zmm0 ; ab = b * a
        vpdpbusd        zmm3, zmm1, zmm1 ; b2 = b * b
        vpdpbusd        zmm2, zmm0, zmm0 ; a2 = a * a
        ; Subtract the number of remaining entries and jump back.
        sub     rdx, 64
        jne     .L2
.L7:
        ; Process the tail, by building the `k1` mask first.
        ; We avoid `k0`, which is a hardcoded constant used to indicate unmasked operations.
        mov     rdx, -1
        bzhi    rax, rdx, rax
        kmovq   k1, rax
        ; Load under 64 bytes from `a` and `b` into `zmm1` and `zmm0`,
        ; using the mask from the `k1`, which can be applied to any AVX-512 argument.
        vmovdqu8        zmm1{k1}{z}, ZMMWORD PTR [rdi]
        vmovdqu8        zmm0{k1}{z}, ZMMWORD PTR [rsi]
        ; Perform mixed-precision dot-products.
        vpdpbusd        zmm3, zmm1, zmm1 ; b2 = b * b
        vpdpbusd        zmm4, zmm1, zmm0 ; ab = b * a
        vpdpbusd        zmm2, zmm0, zmm0 ; a2 = a * a
        ; Exit the loop.
        jmp     .L4
</code></pre><p>It‚Äôs often intriguing to see how compilers shuffle around my instructions, especially when it seems somewhat arbitrary in brief code segments. Our heavy SIMD will anyways be decoded into micro-ops, and then the CPU rearranges them all over again, disregarding compiler‚Äôs sequence. Still, upon testing this with the Google Benchmark library, we observed the following for <code>ints</code>: <strong>25.9 ns</strong>, which is roughly <strong>2&#39;521 times faster</strong> than our original Python baseline. Delivered as promised ü§ó</p><h2 id="conclusion">Conclusion</h2><p>There‚Äôs a common belief that you need a massive infrastructure, akin to what giants like OpenAI or Google possess, to create impactful technology. However, I‚Äôm a proponent of the idea that many amazing innovations can spring from humble beginnings ‚Äì even on a shoestring budget. If you share this sentiment and are keen on optimizing and innovating, you might be interested in some other libraries I‚Äôve had a hand in. üòâ</p><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td><a href="https://github.com/unum-cloud/usearch"><img loading="lazy" src="https://img.shields.io/github/stars/unum-cloud/usearch?style=social&amp;label=USearch%20repository" alt="USearch"/></a></td><td><a href="https://github.com/ashvardanian/stringzilla"><img loading="lazy" src="https://img.shields.io/github/stars/ashvardanian/stringzilla?style=social&amp;label=StringZilla%20repository" alt="StringZilla"/></a></td><td><a href="https://github.com/unum-cloud/uform"><img loading="lazy" src="https://img.shields.io/github/stars/unum-cloud/uform?style=social&amp;label=UForm%20repository" alt="UForm"/></a></td><td><a href="https://github.com/ashvardanian/simsimd"><img loading="lazy" src="https://img.shields.io/github/stars/ashvardanian/simsimd?style=social&amp;label=SimSIMD%20repository" alt="SimSIMD"/></a></td><td><a href="https://github.com/unum-cloud/ucall"><img loading="lazy" src="https://img.shields.io/github/stars/unum-cloud/ucall?style=social&amp;label=UCall%20repository" alt="UCall"/></a></td></tr></tbody></table><h2 id="appendix-comparing-compilers">Appendix: Comparing Compilers</h2><p>For those eager to delve deeper, examining the Assembly generated by different compilers can be insightful. A popular comparison is between GCC and Intel‚Äôs new ICX compiler, with the latter now being based on LLVM. Before diving into the Assembly details, let‚Äôs first benchmark their performance.</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-11-1"><a href="#hl-11-1">1</a>
</span><span id="hl-11-2"><a href="#hl-11-2">2</a>
</span><span id="hl-11-3"><a href="#hl-11-3">3</a>
</span><span id="hl-11-4"><a href="#hl-11-4">4</a>
</span><span id="hl-11-5"><a href="#hl-11-5">5</a>
</span><span id="hl-11-6"><a href="#hl-11-6">6</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span>$ cmake -DCMAKE_C_COMPILER<span>=</span><span>&#34;gcc-12&#34;</span> -DCMAKE_CXX_COMPILER<span>=</span><span>&#34;g++-12&#34;</span> -DCMAKE_BUILD_TYPE<span>=</span>Release -DSIMSIMD_BUILD_BENCHMARKS<span>=</span><span>1</span> -B ./build_release
</span></span><span><span>$ cmake --build build_release --config Release <span>&amp;&amp;</span> ./build_release/simsimd_bench --benchmark_out<span>=</span><span>&#34;gcc-12.json&#34;</span> --benchmark_filter<span>=</span>1536d
</span></span><span><span>
</span></span><span><span>$ <span>source</span> /opt/intel/oneapi/setvars.sh
</span></span><span><span>$ cmake -DCMAKE_C_COMPILER<span>=</span><span>&#34;icx&#34;</span> -DCMAKE_CXX_COMPILER<span>=</span><span>&#34;icpx&#34;</span> -DCMAKE_BUILD_TYPE<span>=</span>Release -DSIMSIMD_BUILD_BENCHMARKS<span>=</span><span>1</span> -B ./build_release
</span></span><span><span>$ cmake --build build_release --config Release <span>&amp;&amp;</span> ./build_release/simsimd_bench --benchmark_out<span>=</span><span>&#34;intel-2023.json&#34;</span> --benchmark_filter<span>=</span>1536d
</span></span></code></pre></td></tr></tbody></table></div></div><p>The above script compiles the code using both compilers and then runs each binary, exporting the results into distinct JSON files. Afterwards, you can use Google Benchmark‚Äôs lesser-known tool, <code>compare.py</code>, to determine if there are significant performance differences and whether a deeper dive into the Assembly is warranted:</p><div><div><table><tbody><tr><td><pre tabindex="0"><code><span id="hl-12-1"><a href="#hl-12-1"> 1</a>
</span><span id="hl-12-2"><a href="#hl-12-2"> 2</a>
</span><span id="hl-12-3"><a href="#hl-12-3"> 3</a>
</span><span id="hl-12-4"><a href="#hl-12-4"> 4</a>
</span><span id="hl-12-5"><a href="#hl-12-5"> 5</a>
</span><span id="hl-12-6"><a href="#hl-12-6"> 6</a>
</span><span id="hl-12-7"><a href="#hl-12-7"> 7</a>
</span><span id="hl-12-8"><a href="#hl-12-8"> 8</a>
</span><span id="hl-12-9"><a href="#hl-12-9"> 9</a>
</span><span id="hl-12-10"><a href="#hl-12-10">10</a>
</span><span id="hl-12-11"><a href="#hl-12-11">11</a>
</span><span id="hl-12-12"><a href="#hl-12-12">12</a>
</span><span id="hl-12-13"><a href="#hl-12-13">13</a>
</span><span id="hl-12-14"><a href="#hl-12-14">14</a>
</span><span id="hl-12-15"><a href="#hl-12-15">15</a>
</span><span id="hl-12-16"><a href="#hl-12-16">16</a>
</span><span id="hl-12-17"><a href="#hl-12-17">17</a>
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span>$ mkdir -p gbench
</span></span><span><span>$ wget https://github.com/google/benchmark/blob/main/tools/compare.py?raw<span>=</span><span>true</span> -O compare.py
</span></span><span><span>$ wget https://github.com/google/benchmark/blob/main/tools/gbench/report.py?raw<span>=</span><span>true</span> -O gbench/report.py
</span></span><span><span>$ wget https://github.com/google/benchmark/blob/main/tools/gbench/util.py?raw<span>=</span><span>true</span> -O gbench/util.py
</span></span><span><span>$ python compare.py benchmarks gcc-12.json intel-2023.json
</span></span><span><span>
</span></span><span><span>Comparing gcc-12.json to intel-2023.json
</span></span><span><span>Benchmark                                                         Time             CPU      Time Old      Time New       CPU Old       CPU New
</span></span><span><span>----------------------------------------------------------------------------------------------------------------------------------------------
</span></span><span><span>avx2_f16_cos_1536d/min_time:10.000/threads:1                   +0.0002         +0.0002           <span>235</span>           <span>235</span>           <span>235</span>           <span>235</span>
</span></span><span><span>avx2_i8_cos_1536d/min_time:10.000/threads:1                    -0.0034         -0.0034            <span>91</span>            <span>91</span>            <span>91</span>            <span>91</span>
</span></span><span><span>avx512_f16_cos_1536d/min_time:10.000/threads:1                 +0.0024         +0.0024            <span>52</span>            <span>52</span>            <span>52</span>            <span>52</span>
</span></span><span><span>avx512_i8_cos_1536d/min_time:10.000/threads:1                  +0.0110         +0.0110            <span>26</span>            <span>26</span>            <span>26</span>            <span>26</span>
</span></span><span><span>avx512_f32_cos_1536d/min_time:10.000/threads:1                 +0.0206         +0.0206            <span>87</span>            <span>89</span>            <span>87</span>            <span>89</span>
</span></span><span><span>serial_f16_cos_1536d/min_time:10.000/threads:1                 -0.0013         -0.0013          <span>1988</span>          <span>1985</span>          <span>1988</span>          <span>1985</span>
</span></span><span><span>serial_f32_cos_1536d/min_time:10.000/threads:1                 -0.0000         -0.0000          <span>1118</span>          <span>1118</span>          <span>1118</span>          <span>1118</span>
</span></span><span><span>serial_i8_cos_1536d/min_time:10.000/threads:1                  -0.0001         -0.0001           <span>299</span>           <span>299</span>           <span>299</span>           <span>299</span>
</span></span></code></pre></td></tr></tbody></table></div></div><p>From the results, we observe minimal runtime differences between the two compilers. The generated Assembly is remarkably similar, particularly in the critical sections. Intel‚Äôs output is often 10% shorter which is typically advantageous. The most pronounced differences emerge outside of the <code>for</code> loop. Notably, the <code>_mm512_reduce_add_epi32</code> intrinsic doesn‚Äôt correspond directly to a specific SIMD instruction, granting the compilers a bit more leeway:</p><ul><li>GCC opts for a lengthier reduction using <code>vextracti64x4</code> and <code>vextracti64x2</code>.</li><li>Intel prefers the succinct <code>vextracti128</code>.</li></ul><p>Both compilers employ the <code>vpshufd</code> for shuffling but use varied masks. For a detailed exploration, visit the <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1DIApACYAQuYukl9ZATwDKjdAGFUtAK4sGIAMwA7KSuADJ4DJgAcj4ARpjEIGYAnKQADqgKhE4MHt6%2BAcEZWY4C4ZExLPGJKbaY9qUMQgRMxAR5Pn5BdQ05za0E5dFxCUmpCi1tHQXdEwNDldVjAJS2qF7EyOwc5v4RyN5YANQm/m54LCwRBMQRAHQIp9gmGgCCL68EAJ5pmFhURyotFQTAIRyyLAh6AA%2BlR/GZoQRTlY3t9fv8jsgEK1wRcodC8AAORHIj5ov6YAFeBhZYCRdBHYFGXGQi4wrIAL0wJP8KPeb2h0NBNzwsS8BG50Kggq8Shh0OWyw%2BguFtzFEsFUEmwEwBAg5jMTAAbqoAKxcMxG2ikY1mi1UG0m81mWIAd0ddstDAYeFIsSuZgNiuWRwA9KGPhFaBFMOCWo5kCz8XCEUi3hD8bbnQTiWgFBAM2yc4jMQIJgAqI5MUhJ">Compiler Explorer</a>.</p></div></div>
  </body>
</html>
