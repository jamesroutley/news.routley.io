<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/GuyTevet/motion-diffusion-model">Original</a>
    <h1>Human Motion Diffusion Model</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a href="https://paperswithcode.com/sota/motion-synthesis-on-humanact12?p=human-motion-diffusion-model" rel="nofollow"><img src="https://camo.githubusercontent.com/43ef625e7862f992809254e0727ca3ebadd0629201f7828a1446e4b28c69fc0b/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f68756d616e2d6d6f74696f6e2d646966667573696f6e2d6d6f64656c2f6d6f74696f6e2d73796e7468657369732d6f6e2d68756d616e6163743132" alt="PWC" data-canonical-src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/human-motion-diffusion-model/motion-synthesis-on-humanact12"/></a>
<a href="https://paperswithcode.com/sota/motion-synthesis-on-humanml3d?p=human-motion-diffusion-model" rel="nofollow"><img src="https://camo.githubusercontent.com/ae66c61846df7c9ff0e8232b2b9b73773a0668d94135ada63bed41cfe882f212/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f68756d616e2d6d6f74696f6e2d646966667573696f6e2d6d6f64656c2f6d6f74696f6e2d73796e7468657369732d6f6e2d68756d616e6d6c3364" alt="PWC" data-canonical-src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/human-motion-diffusion-model/motion-synthesis-on-humanml3d"/></a>
<a href="https://arxiv.org/abs/2209.14916" rel="nofollow"><img src="https://camo.githubusercontent.com/5fb2c238ea39767f2859dd51fce4b4c99bd06e2b69fa51b9031898f1dc67e7a3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d253343323230392e31343931362533452d253343434f4c4f522533452e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-%3C2209.14916%3E-%3CCOLOR%3E.svg"/></a></p>
<p dir="auto">The official PyTorch implementation of the paper <a href="https://arxiv.org/abs/2209.14916" rel="nofollow"><strong>&#34;Human Motion Diffusion Model&#34;</strong></a>.</p>
<p dir="auto">Please visit our <a href="https://guytevet.github.io/mdm-page/" rel="nofollow"><strong>webpage</strong></a> for more details.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GuyTevet/mdm-page/raw/main/static/figures/github.gif"><img src="https://github.com/GuyTevet/mdm-page/raw/main/static/figures/github.gif" alt="teaser" data-animated-image=""/></a></p>
<h4 dir="auto"><a id="user-content-bibtex" aria-hidden="true" href="#bibtex"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Bibtex</h4>
<p dir="auto">If you find this code useful in your research, please cite:</p>
<div data-snippet-clipboard-copy-content="@article{tevet2022human,
  title={Human Motion Diffusion Model},
  author={Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Bermano, Amit H and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2209.14916},
  year={2022}
}"><pre><code>@article{tevet2022human,
  title={Human Motion Diffusion Model},
  author={Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Bermano, Amit H and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2209.14916},
  year={2022}
}
</code></pre></div>
<h2 dir="auto"><a id="user-content-news" aria-hidden="true" href="#news"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>News</h2>
<p dir="auto"><g-emoji alias="loudspeaker" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png">ðŸ“¢</g-emoji> <strong>9/Oct/22</strong> - Added training and evaluation scripts.
Note slight env changes adapting to the new code. If you already have an installed environment, run <code>bash prepare/download_glove.sh; pip install clearml</code> to adapt.</p>
<p dir="auto"><g-emoji alias="loudspeaker" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png">ðŸ“¢</g-emoji> <strong>6/Oct/22</strong> - First release - sampling and rendering using pre-trained models.</p>
<h2 dir="auto"><a id="user-content-getting-started" aria-hidden="true" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Getting started</h2>
<p dir="auto">This code was tested on <code>Ubuntu 18.04.5 LTS</code> and requires:</p>
<ul dir="auto">
<li>Python 3.7</li>
<li>conda3 or miniconda3</li>
<li>CUDA capable GPU (one is enough)</li>
</ul>
<h3 dir="auto"><a id="user-content-1-setup-environment" aria-hidden="true" href="#1-setup-environment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>1. Setup environment</h3>
<p dir="auto">Install ffmpeg (if not already installed):</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt update
sudo apt install ffmpeg"><pre>sudo apt update
sudo apt install ffmpeg</pre></div>
<p dir="auto">For windows use <a href="https://www.geeksforgeeks.org/how-to-install-ffmpeg-on-windows/" rel="nofollow">this</a> instead.</p>
<p dir="auto">Setup conda env:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda env create -f environment.yml
conda activate mdm
python -m spacy download en_core_web_sm
pip install git+https://github.com/openai/CLIP.git"><pre>conda env create -f environment.yml
conda activate mdm
python -m spacy download en_core_web_sm
pip install git+https://github.com/openai/CLIP.git</pre></div>
<p dir="auto">Download dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bash prepare/download_smpl_files.sh
bash prepare/download_glove.sh"><pre>bash prepare/download_smpl_files.sh
bash prepare/download_glove.sh</pre></div>
<h3 dir="auto"><a id="user-content-2-get-data" aria-hidden="true" href="#2-get-data"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>2. Get data</h3>
<p dir="auto">There are two paths to get the data:</p>
<p dir="auto">(a) <strong>Go the easy way if</strong> you just want to generate text-to-motion (excluding editing which does require motion capture data)</p>
<p dir="auto">(b) <strong>Get full data</strong> to train and evaluate the model.</p>
<h4 dir="auto"><a id="user-content-a-the-easy-way-text-only" aria-hidden="true" href="#a-the-easy-way-text-only"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>a. The easy way (text only)</h4>
<p dir="auto"><strong>HumanML3D</strong> - Clone HumanML3D, then copy the data dir to our repository:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd ..
git clone https://github.com/EricGuo5513/HumanML3D.git
unzip ./HumanML3D/HumanML3D/texts.zip -d ./HumanML3D/HumanML3D/
cp -r HumanML3D/HumanML3D motion-diffusion-model/dataset/HumanML3D
cd motion-diffusion-model"><pre><span>cd</span> ..
git clone https://github.com/EricGuo5513/HumanML3D.git
unzip ./HumanML3D/HumanML3D/texts.zip -d ./HumanML3D/HumanML3D/
cp -r HumanML3D/HumanML3D motion-diffusion-model/dataset/HumanML3D
<span>cd</span> motion-diffusion-model</pre></div>
<h4 dir="auto"><a id="user-content-b-full-data-text--motion-capture" aria-hidden="true" href="#b-full-data-text--motion-capture"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>b. Full data (text + motion capture)</h4>
<p dir="auto"><strong>HumanML3D</strong> - Follow the instructions in <a href="https://github.com/EricGuo5513/HumanML3D.git">HumanML3D</a>,
then copy the result dataset to our repository:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D"><pre>cp -r ../HumanML3D/HumanML3D ./dataset/HumanML3D</pre></div>
<p dir="auto"><strong>KIT</strong> - Download from <a href="https://github.com/EricGuo5513/HumanML3D.git">HumanML3D</a> (no processing needed this time) and the place result in <code>./dataset/KIT-ML</code></p>
<h3 dir="auto"><a id="user-content-3-download-the-pretrained-models" aria-hidden="true" href="#3-download-the-pretrained-models"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>3. Download the pretrained models</h3>
<p dir="auto">Download the model(s) you wish to use, then unzip and place it in <code>./save/</code>. <strong>For text-to-motion, you need only the first one.</strong></p>
<p dir="auto"><strong>HumanML3D</strong></p>
<p dir="auto"><a href="https://drive.google.com/file/d/1PE0PK8e5a5j-7-Xhs5YET5U5pGh0c821/view?usp=sharing" rel="nofollow">humanml-encoder-512</a> (best model)</p>
<p dir="auto"><a href="https://drive.google.com/file/d/1q3soLadvVh7kJuJPd2cegMNY2xVuVudj/view?usp=sharing" rel="nofollow">humanml-decoder-512</a></p>
<p dir="auto"><a href="https://drive.google.com/file/d/1GnsW0K3UjuOkNkAWmjrGIUmeDDZrmPE5/view?usp=sharing" rel="nofollow">humanml-decoder-with-emb-512</a></p>
<p dir="auto"><strong>KIT</strong></p>
<p dir="auto"><a href="https://drive.google.com/file/d/1SHCRcE0es31vkJMLGf9dyLe7YsWj7pNL/view?usp=sharing" rel="nofollow">kit-encoder-512</a></p>
<h2 dir="auto"><a id="user-content-generate-text-to-motion" aria-hidden="true" href="#generate-text-to-motion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generate text-to-motion</h2>
<h3 dir="auto"><a id="user-content-generate-from-test-set-prompts" aria-hidden="true" href="#generate-from-test-set-prompts"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generate from test set prompts</h3>
<div dir="auto" data-snippet-clipboard-copy-content="python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --num_samples 10 --num_repetitions 3"><pre>python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --num_samples 10 --num_repetitions 3</pre></div>
<h3 dir="auto"><a id="user-content-generate-from-your-text-file" aria-hidden="true" href="#generate-from-your-text-file"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generate from your text file</h3>
<div dir="auto" data-snippet-clipboard-copy-content="python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --input_text ./assets/example_text_prompts.txt"><pre>python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --input_text ./assets/example_text_prompts.txt</pre></div>
<h3 dir="auto"><a id="user-content-generate-a-single-prompt" aria-hidden="true" href="#generate-a-single-prompt"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Generate a single prompt</h3>
<div dir="auto" data-snippet-clipboard-copy-content="python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --text_prompt &#34;the person walked forward and is picking up his toolbox.&#34;"><pre>python -m sample --model_path ./save/humanml_trans_enc_512/model000200000.pt --text_prompt <span><span>&#34;</span>the person walked forward and is picking up his toolbox.<span>&#34;</span></span></pre></div>
<p dir="auto"><strong>You can also define:</strong></p>
<ul dir="auto">
<li><code>--device</code> id.</li>
<li><code>--seed</code> to sample different prompts.</li>
<li><code>--motion_length</code> in seconds (maximum is 9.8[sec]).</li>
</ul>
<p dir="auto"><strong>Running those will get you:</strong></p>
<ul dir="auto">
<li><code>results.npy</code> file with text prompts and xyz positions of the generated animation</li>
<li><code>sample##_rep##.mp4</code> - a stick figure animation for each generated motion.</li>
</ul>
<p dir="auto">It will look something like this:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GuyTevet/motion-diffusion-model/blob/main/assets/example_stick_fig.gif"><img src="https://github.com/GuyTevet/motion-diffusion-model/raw/main/assets/example_stick_fig.gif" alt="example" data-animated-image=""/></a></p>
<p dir="auto">You can stop here, or render the SMPL mesh using the following script.</p>
<h3 dir="auto"><a id="user-content-render-smpl-mesh" aria-hidden="true" href="#render-smpl-mesh"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Render SMPL mesh</h3>
<p dir="auto">To create SMPL mesh per frame run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m visualize.render_mesh --input_path /path/to/mp4/stick/figure/file"><pre>python -m visualize.render_mesh --input_path /path/to/mp4/stick/figure/file</pre></div>
<p dir="auto"><strong>This script outputs:</strong></p>
<ul dir="auto">
<li><code>sample##_rep##_smpl_params.npy</code> - SMPL parameters (thetas, root translations, vertices and faces)</li>
<li><code>sample##_rep##_obj</code> - Mesh per frame in <code>.obj</code> format.</li>
</ul>
<p dir="auto"><strong>Notes:</strong></p>
<ul dir="auto">
<li>The <code>.obj</code> can be integrated into Blender/Maya/3DS-MAX and rendered using them.</li>
<li>This script is running <a href="https://smplify.is.tue.mpg.de/" rel="nofollow">SMPLify</a> and needs GPU as well (can be specified with the <code>--device</code> flag).</li>
<li><strong>Important</strong> - Do not change the original <code>.mp4</code> path before running the script.</li>
</ul>
<p dir="auto"><strong>Notes for 3d makers:</strong></p>
<ul dir="auto">
<li>You have two ways to animate the sequence:
<ol dir="auto">
<li>Use the <a href="https://smpl.is.tue.mpg.de/index.html" rel="nofollow">SMPL add-on</a> and the theta parameters saved to <code>sample##_rep##_smpl_params.npy</code> (we always use beta=0 and the gender-neutral model).</li>
<li>A more straightforward way is using the mesh data itself. All meshes have the same topology (SMPL), so you just need to keyframe vertex locations.
Since the OBJs are not preserving vertices order, we also save this data to the <code>sample##_rep##_smpl_params.npy</code> file for your convenience.</li>
</ol>
</li>
</ul>
<h3 dir="auto"><a id="user-content-editing" aria-hidden="true" href="#editing"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Editing</h3>
<p dir="auto">ETA - Nov 22</p>
<h2 dir="auto"><a id="user-content-train-your-own-mdm" aria-hidden="true" href="#train-your-own-mdm"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Train your own MDM</h2>
<p dir="auto"><strong>HumanML3D</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m train.train_mdm --save_dir save/my_humanml_trans_enc_512 --dataset humanml"><pre>python -m train.train_mdm --save_dir save/my_humanml_trans_enc_512 --dataset humanml</pre></div>
<p dir="auto"><strong>KIT</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m train.train_mdm --save_dir save/my_kit_trans_enc_512 --dataset kit"><pre>python -m train.train_mdm --save_dir save/my_kit_trans_enc_512 --dataset kit</pre></div>
<ul dir="auto">
<li>Use <code>--device</code> to define GPU id.</li>
<li>Use <code>--arch</code> to choose one of the architectures reported in the paper <code>{trans_enc, trans_dec, gru}</code> (<code>trans_enc</code> is default).</li>
<li>Add <code>--train_platform_type {ClearmlPlatform, TensorboardPlatform}</code> to track results with either <a href="https://clear.ml/" rel="nofollow">ClearML</a> or <a href="https://www.tensorflow.org/tensorboard" rel="nofollow">Tensorboard</a>.</li>
<li>Add <code>--eval_during_training</code> to run a short (90 minutes) evaluation for each saved checkpoint.
This will slow down training but will give you better monitoring.</li>
</ul>
<h2 dir="auto"><a id="user-content-evaluate" aria-hidden="true" href="#evaluate"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Evaluate</h2>
<ul dir="auto">
<li>Takes about 20 hours (on a single GPU)</li>
<li>The output of this script for the pre-trained models (as was reported in the paper) is provided in the checkpoints zip file.</li>
</ul>
<p dir="auto"><strong>HumanML3D</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m eval.eval_humanml --model_path ./save/humanml_trans_enc_512/model000475000.pt"><pre>python -m eval.eval_humanml --model_path ./save/humanml_trans_enc_512/model000475000.pt</pre></div>
<p dir="auto"><strong>KIT</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m eval.eval_humanml --model_path ./save/kit_trans_enc_512/model000400000.pt"><pre>python -m eval.eval_humanml --model_path ./save/kit_trans_enc_512/model000400000.pt</pre></div>
<h2 dir="auto"><a id="user-content-acknowledgments" aria-hidden="true" href="#acknowledgments"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Acknowledgments</h2>
<p dir="auto">This code is standing on the shoulders of giants. We want to thank the following contributors
that our code is based on:</p>
<p dir="auto"><a href="https://github.com/openai/guided-diffusion">guided-diffusion</a>, <a href="https://github.com/GuyTevet/MotionCLIP">MotionCLIP</a>, <a href="https://github.com/EricGuo5513/text-to-motion">text-to-motion</a>, <a href="https://github.com/Mathux/ACTOR">actor</a>, <a href="https://github.com/wangsen1312/joints2smpl">joints2smpl</a>.</p>
<h2 dir="auto"><a id="user-content-license" aria-hidden="true" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License</h2>
<p dir="auto">This code is distributed under an <a href="https://github.com/GuyTevet/motion-diffusion-model/blob/main/LICENSE">MIT LICENSE</a>.</p>
<p dir="auto">Note that our code depends on other libraries, including CLIP, SMPL, SMPL-X, PyTorch3D, and uses datasets that each have their own respective licenses that must also be followed.</p>
</article>
          </div></div>
  </body>
</html>
