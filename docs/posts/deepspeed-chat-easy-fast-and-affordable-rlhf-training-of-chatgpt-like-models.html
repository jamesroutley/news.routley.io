<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md">Original</a>
    <h1>DeepSpeed Chat: Easy, fast and affordable RLHF training of ChatGPT-like models</h1>
    
    <div id="readability-page-1" class="page"><div>

    <div data-target="readme-toc.content">
      
  


        <div id="readme">
    <article itemprop="text">
<p><a target="_blank" rel="noopener noreferrer" href="https://ntietz.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/hero-figure-black.png"><img src="https://ntietz.com/microsoft/DeepSpeed/raw/master/blogs/assets/images/hero-figure-black.png" width="1000px" alt="DeepSpeed-Chat!"/></a>
</p>

<p dir="auto">ChatGPT like models have taken the AI world by a storm, and it would not be an overstatement to say that its impact on the digital world has been revolutionary. These models are incredibly versatile, capable of performing tasks like summarization, coding, and translation with results that are on-par or even exceeding the capabilities of human experts. Given the sheer power of these models, multiple efforts are underway in the AI open-source community to make ChatGPT-style models more accessible (e.g. ChatLLaMa, Alpaca, Vicuna, Databricks-Dolly, etc.).</p>
<p dir="auto">Despite these incredible efforts, there is still a lack of an end-to-end RLHF pipeline capable of training powerful ChatGPT like model that is easily accessible to the AI community. For instance, training a modest 6.7B ChatGPT model with existing systems typically requires expensive multi-GPU setup that is beyond the reach of many data scientists. Even with access to such computing resources, training efficiency is often less than 5% of what these machines are capable of (as illustrated <a href="#effective-throughput-and-scalability-analysis">shortly</a>). And finally, existing solutions simply cannot support easy, fast and affordable training state-of-the-art ChatGPT models with hundreds of billions of parameters, even given access to multi-GPU clusters.</p>
<p dir="auto">These limitations stem from a lack of a robust system design that is capable of effectively supporting the complex InstructGPT’s RLHF training pipeline that is quite different from the standard pre-training and fine-tuning pipelines that existing DL systems are designed for. Therefore, in the spirit of democratizing ChatGPT-like models, and making RLHF training truly accessible to the AI community, today we are releasing DeepSpeed-Chat with the following three capabilities:</p>
<p dir="auto">(i) <em><strong>Easy-to-use Training and Inference Experience for ChatGPT Like Models</strong></em>: A single script capable of taking a pre-trained Huggingface model, running it through all three steps of InstructGPT training using DeepSpeed-RLHF system and producing your very own ChatGPT like model. In addition, we provide an inference API for testing conversation-style interactions after the model is trained.</p>
<p dir="auto">(ii) <em><strong>DeepSpeed-RLHF Pipeline</strong></em>: DeepSpeed-RLHF pipeline primarily replicates the training pipeline from the InstructGPT paper with careful attention to ensure completeness and one-to-one correspondence with the three-steps that includes a) Supervised Fine-tuning (SFT), b) Reward Model Fine-tuning and c) Reinforcement Learning with Human Feedback (RLHF). Additionally, we offer data abstraction and blending capabilities to enable training with multiple data sources.</p>
<p dir="auto">(iii) <em><strong>DeepSpeed-RLHF System</strong></em>: A robust and sophisticated RLHF system that combines the training and inference prowess of DeepSpeed into single unified Hybrid Engine (DeepSpeed-HE) for RLHF. The Hybrid-Engine is capable of seamlessly transitioning between inference and training modes within RLHF, allowing it to leverage various optimizations from DeepSpeed-Inference such as tensor-parallelism and high-performance transformer kernels for generation, while also benefiting from the multitude of ZeRO- and LoRA-based memory optimization strategies for RL training. DeepSpeed-HE is also aware of the full RLHF pipeline, allowing it to make optimal decisions in terms of memory management and data movement across different phases of RLHF.</p>
<p dir="auto">DeepSpeed-RLHF system is capable of unparalleled efficiency at scale, making complex RLHF training fast, affordable, and easily accessible to the AI community:</p>
<p dir="auto"><em><strong>Efficiency and Affordability</strong></em>: In terms of efficiency, <a href="#throughput-and-model-size-scalability-comparisons-with-existing-rlhf-systems">DeepSpeed-HE is over 15x faster than existing systems</a>, making RLHF training both fast and affordable. For instance, DeepSpeed-HE can train an OPT-13B in just 9 hours and OPT-30B in 18 hours on Azure Cloud for under $300 and $600, respectively.</p>
<div dir="auto">
<table>
<thead>
<tr>
<th>GPUs</th>
<th>OPT-6.7B</th>
<th>OPT-13B</th>
<th>OPT-30B</th>
<th>OPT-66B</th>
</tr>
</thead>
<tbody>
<tr>
<td>8x A100-40GB</td>
<td>5.7 hours</td>
<td>10.8 hours</td>
<td>1.85 days</td>
<td>NA</td>
</tr>
<tr>
<td>8x A100-80GB</td>
<td>4.1 hours ($132)</td>
<td>9 hours ($290)</td>
<td>18 hours ($580)</td>
<td>2.1 days ($1620)</td>
</tr>
</tbody>
</table>
<p dir="auto"><em>Table 1. Single-Node 8x A100: Training Time and Corresponding Approximate Cost on Azure.</em></p>
</div>
<p dir="auto"><em><strong>Excellent Scalability</strong></em>: DeepSpeed-HE supports models with hundreds of billions of parameters and can achieve excellent scalability on multi-node multi-GPU systems. As a result, even a 13B model can be trained in 1.25 hours and a massive 175B model can be trained with DeepSpeed-HE in under a day.</p>
<div dir="auto">
<table>
<thead>
<tr>
<th>GPUs</th>
<th>OPT-13B</th>
<th>OPT-30B</th>
<th>OPT-66B</th>
<th>OPT-175B</th>
</tr>
</thead>
<tbody>
<tr>
<td>64x A100-80G</td>
<td>1.25 hours ($320)</td>
<td>4 hours ($1024)</td>
<td>7.5 hours ($1920)</td>
<td>20 hours ($5120)</td>
</tr>
</tbody>
</table>
<p dir="auto"><em>Table 2. Multi-Node 64x A100-80GB: Training Time and Corresponding Approximate Cost on Azure.</em></p>
</div>
<blockquote>
<p dir="auto"><em><strong>Very Important Details</strong></em>: The numbers in both tables above are for Step 3 of the training and based on actual measured training throughput on DeepSpeed-RLHF curated dataset and training recipe which trains for one epoch on a total of 135M tokens. We have in total 67.5M query tokens (131.9k queries with sequence length 256) and 67.5M generated tokens (131.9k answers with sequence length 256), and a maximum global batch size per step of 0.5M tokens (1024 query-answer pairs). We urge readers to pay attention to these specifications before making any cost and e2e time comparisons with DeepSpeed-RLHF. See our <a href="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/BenckmarkSetting.md">benchmark settings</a> page for more details.</p>
</blockquote>
<p dir="auto"><em><strong>Democratizing RLHF Training</strong></em>: With just a single GPU, DeepSpeed-HE supports training models with over 13 billion parameters, enabling data scientists without access to multi-GPU systems to create not just toy RLHF models but large and powerful ones that can be used in real-world scenarios.</p>
<div dir="auto">
<table>
<thead>
<tr>
<th></th>
<th>V100 32G</th>
<th>A6000 48G</th>
<th>A100 40G</th>
<th>A100 80G</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model Size</td>
<td>OPT-2.7B</td>
<td>OPT-6.7B</td>
<td>OPT-6.7B</td>
<td>OPT-13B</td>
</tr>
</tbody>
</table>
<p dir="auto">Table 3. Max Model Size Supported by DeepSpeed-HE on a Single GPU</p>
</div>
<p dir="auto">Next, we dive deeper into the three capabilities of DeepSpeed-Chat introduced above.</p>

<p dir="auto">We start with the easy-to-use experience by showing how you can train OPT-13B and then OPT-66B models with DeepSpeed-RLHF system. If you are short on time, you can even train an OPT-1.3B model on a single consumer-grade GPU in just two hours.  We also demonstrate how you can use our DeepSpeed-chat RLHF API to develop your own custom pipelines.</p>
<h2 dir="auto"><a id="user-content-training-your-first-chatgpt-style-model-is-so-easy-with-deepspeed-chats-rlhf-examples" aria-hidden="true" href="#training-your-first-chatgpt-style-model-is-so-easy-with-deepspeed-chats-rlhf-examples"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training your first ChatGPT-Style model is so easy with DeepSpeed-Chat’s RLHF examples</h2>
<p dir="auto"><em>a) One single script completes all three stages of RLHF training and generates your first ChatGPT model!</em></p>
<p dir="auto">We use an example of pretrained OPT-13B as the actor model and OPT-350M as the reward model in the following single script to generate a final 13B ChatGPT-style model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/microsoft/DeepSpeed.git
cd DeepSpeed
pip install .

git clone https://github.com/microsoft/DeepSpeedExamples.git
cd DeepSpeedExamples/applications/DeepSpeed-Chat/
pip install -r requirements.txt

python train.py --actor-model facebook/opt-13b --reward-model facebook/opt-350m --num-gpus 8"><pre><span>git</span> <span>clone</span> <span>https</span>:<span>//</span><span>github</span>.<span>com</span><span>/</span><span>microsoft</span><span>/</span><span>DeepSpeed</span>.<span>git</span>
<span>cd</span> <span>DeepSpeed</span>
<span>pip</span> <span>install</span> .

<span>git</span> <span>clone</span> <span>https</span>:<span>//</span><span>github</span>.<span>com</span><span>/</span><span>microsoft</span><span>/</span><span>DeepSpeedExamples</span>.<span>git</span>
<span>cd</span> <span>DeepSpeedExamples</span><span>/</span><span>applications</span><span>/</span><span>DeepSpeed</span><span>-</span><span>Chat</span><span>/</span>
<span>pip</span> <span>install</span> <span>-</span><span>r</span> <span>requirements</span>.<span>txt</span>

<span>python</span> <span>train</span>.<span>py</span> <span>-</span><span>-</span><span>actor</span><span>-</span><span>model</span> <span>facebook</span><span>/</span><span>opt</span><span>-</span><span>13</span><span>b</span> <span>-</span><span>-</span><span>reward</span><span>-</span><span>model</span> <span>facebook</span><span>/</span><span>opt</span><span>-</span><span>350</span><span>m</span> <span>-</span><span>-</span><span>num</span><span>-</span><span>gpus</span> <span>8</span></pre></div>
<p dir="auto">In about half a day, your 13B model would be fully trained with its checkpoints ready.  The following table demonstrates a breakdown of the training time for each of the three steps:</p>
<div dir="auto">
<table>
<thead>
<tr>
<th>Model Sizes</th>
<th>Step 1</th>
<th>Step 2</th>
<th>Step 3</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actor: OPT-13B, Reward: OPT-350M</td>
<td>2.5hr</td>
<td>0.25hr</td>
<td>10.8hr</td>
<td>13.6hr</td>
</tr>
</tbody>
</table>
<p dir="auto">Table 4. E2E time breakdown for training a 13 billion parameter ChatGPT model via DeepSpeed-Chat on a single DGX node with 8 NVIDIA A100-40G GPUs.</p>
</div>
<p dir="auto"><em>b) Test your final model with DeepSpeed-Chat’s Inference API</em></p>
<p dir="auto">Now it is time to plug-in and test your final model trained by DeepSpeed-Chat! You can play with not only Q&amp;A sessions but also conversation-style interactions.</p>
<div data-snippet-clipboard-copy-content="Human:        Do you know Microsoft?
Assistant:    Yes, I’m a big fan of Microsoft.  What about them?
Human:        Can you explain it to a 6-year old child? I wonder how I should describe it
Assistant:    Sure, I can try.  Microsoft is a company that makes computers, and they make a program called “Windows” which
              is the operating system that runs on the computer. It’s like the “brain” of the computer.  It’s where all the
              programs and files are stored. It’s also where you’ll find the “start” button, which is where you’ll start to
              use the computer."><pre><code>Human:        Do you know Microsoft?
Assistant:    Yes, I’m a big fan of Microsoft.  What about them?
Human:        Can you explain it to a 6-year old child? I wonder how I should describe it
Assistant:    Sure, I can try.  Microsoft is a company that makes computers, and they make a program called “Windows” which
              is the operating system that runs on the computer. It’s like the “brain” of the computer.  It’s where all the
              programs and files are stored. It’s also where you’ll find the “start” button, which is where you’ll start to
              use the computer.
</code></pre></div>
<h2 dir="auto"><a id="user-content-want-to-try-different-model-sizes-and-configurations-you-got-it" aria-hidden="true" href="#want-to-try-different-model-sizes-and-configurations-you-got-it"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Want to try different model sizes and configurations? You got it!</h2>
<p dir="auto">We understand users often like to play with different model sizes and configurations to meet their training time, resources, and quality requirements. With DeepSpeed-Chat, users can easily do that. For example, if you want to train a larger and higher-quality model on your GPU cluster for your research or business, you can simply use the same script with your desired model size e.g., 66B and GPU counts e.g., 64 GPUs:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python train.py --actor-model facebook/opt-66b --reward-model facebook/opt-350m --num-gpus 64"><pre><span>python</span> <span>train</span>.<span>py</span> <span>-</span><span>-</span><span>actor</span><span>-</span><span>model</span> <span>facebook</span><span>/</span><span>opt</span><span>-</span><span>66</span><span>b</span> <span>-</span><span>-</span><span>reward</span><span>-</span><span>model</span> <span>facebook</span><span>/</span><span>opt</span><span>-</span><span>350</span><span>m</span> <span>-</span><span>-</span><span>num</span><span>-</span><span>gpus</span> <span>64</span></pre></div>
<p dir="auto">Within 9 hours, you can have your 66 billion parameters ChatGPT model ready to be served in your favorite front-end GUI:</p>
<div dir="auto">
<table>
<thead>
<tr>
<th>Model Sizes</th>
<th>Step 1</th>
<th>Step 2</th>
<th>Step 3</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actor: OPT-66B, Reward: OPT-350M</td>
<td>82 mins</td>
<td>5 mins</td>
<td>7.5hr</td>
<td>9hr</td>
</tr>
</tbody>
</table>
<p dir="auto">Table 5. E2E time breakdown for training a 66 billion parameter ChatGPT model via DeepSpeed-Chat on 8 DGX nodes with 8 NVIDIA A100-80G GPUs/node.</p>
</div>
<p dir="auto">If you only have around 1-2 hours for coffee or lunch break, you can also try to train a small/toy model with DeepSpeed-Chat. For example, we prepared a training example for a 1.3B model with a single dataset to test our framework on your consumer-grade GPUs. The best part is that you will have your model checkpoint ready to play with when you are back from your lunch break!</p>
<div dir="auto" data-snippet-clipboard-copy-content="python train.py --actor-model facebook/opt-1.3b --reward-model facebook/opt-350m --num-gpus 1"><pre><span>python</span> <span>train</span>.<span>py</span> <span>-</span><span>-</span><span>actor</span><span>-</span><span>model</span> <span>facebook</span><span>/</span><span>opt</span><span>-</span><span>1.3</span><span>b</span> <span>-</span><span>-</span><span>reward</span><span>-</span><span>model</span> <span>facebook</span><span>/</span><span>opt</span><span>-</span><span>350</span><span>m</span> <span>-</span><span>-</span><span>num</span><span>-</span><span>gpus</span> <span>1</span></pre></div>
<div dir="auto">
<table>
<thead>
<tr>
<th>Model Sizes</th>
<th>Step 1</th>
<th>Step 2</th>
<th>Step 3</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Actor: OPT-1.3B, Reward: OPT-350M</td>
<td>2900 secs</td>
<td>670 secs</td>
<td>1.2hr</td>
<td>2.2hr</td>
</tr>
</tbody>
</table>
<p dir="auto"><em>Table 6. E2E time breakdown for training a 1.3 billion parameter ChatGPT model via DeepSpeed-Chat on a single commodity NVIDIA A6000 GPU with 48GB memory.</em></p>
</div>
<h2 dir="auto"><a id="user-content-customizing-your-own-rlhf-training-pipeline-using-deepspeed-chats-rlhf-apis" aria-hidden="true" href="#customizing-your-own-rlhf-training-pipeline-using-deepspeed-chats-rlhf-apis"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Customizing your own RLHF training pipeline using DeepSpeed-Chat’s RLHF APIs</h2>
<p dir="auto">DeepSpeed-Chat allows users to build their very own RLHF training pipeline using our flexible APIs shown below, which users can use to reconstruct their own RLHF training strategy. This enables a general interface and backend for creating a wide range of RLHF algorithms for research exploration.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
engine = DeepSpeedRLHFEngine(
  actor_model_name_or_path=args.actor_model_name_or_path,
  critic_model_name_or_path=args.critic_model_name_or_path,
  tokenizer=tokenizer,
  num_total_iters=num_total_iters,
  args=args)

trainer = DeepSpeedPPOTrainer(engine=engine, args=args)

for prompt_batch in prompt_train_dataloader:
  out = trainer.generate_experience(prompt_batch)
  actor_loss, critic_loss = trainer.train_rlhf(out)"><pre><span>engine</span> <span>=</span> <span>DeepSpeedRLHFEngine</span>(
  <span>actor_model_name_or_path</span><span>=</span><span>args</span>.<span>actor_model_name_or_path</span>,
  <span>critic_model_name_or_path</span><span>=</span><span>args</span>.<span>critic_model_name_or_path</span>,
  <span>tokenizer</span><span>=</span><span>tokenizer</span>,
  <span>num_total_iters</span><span>=</span><span>num_total_iters</span>,
  <span>args</span><span>=</span><span>args</span>)

<span>trainer</span> <span>=</span> <span>DeepSpeedPPOTrainer</span>(<span>engine</span><span>=</span><span>engine</span>, <span>args</span><span>=</span><span>args</span>)

<span>for</span> <span>prompt_batch</span> <span>in</span> <span>prompt_train_dataloader</span>:
  <span>out</span> <span>=</span> <span>trainer</span>.<span>generate_experience</span>(<span>prompt_batch</span>)
  <span>actor_loss</span>, <span>critic_loss</span> <span>=</span> <span>trainer</span>.<span>train_rlhf</span>(<span>out</span>)</pre></div>

<p dir="auto">To provide a seamless training experience, we follow InstructGPT and include a full-fledged end-to-end training pipeline in DeepSpeed-Chat as shown in Figure 1.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://ntietz.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/ds-chat-overview.png"><img src="https://ntietz.com/microsoft/DeepSpeed/raw/master/blogs/assets/images/ds-chat-overview.png" width="800px" alt="DeepSpeed-Chat!"/></a></p><p dir="auto"><em>Figure 1: The illustration of DeepSpeed-Chat’s RLHF training pipeline with optional features.</em></p>
</div>
<p dir="auto">Our pipeline includes three main steps:</p>
<ul dir="auto">
<li><strong>Step 1: Supervised finetuning (SFT)</strong>, where human responses to various queries are carefully selected to finetune the pretrained language models.</li>
<li><strong>Step 2: Reward model finetuning</strong>, where a separate (usually smaller than the SFT) model (RW) is trained with a dataset that has human-provided rankings of multiple answers to the same query.</li>
<li><strong>Step 3: RLHF training</strong>, where the SFT model is further finetuned with the reward feedback from the RW model using the Proximal Policy Optimization (PPO) algorithm.</li>
</ul>
<p dir="auto">We provide two additional features in Step 3 to help improve model quality:</p>
<ul dir="auto">
<li><strong>Exponential Moving Average (EMA) collection</strong>, where an EMA based checkpoint can be chosen for the final evaluation.</li>
<li><strong>Mixture Training</strong>, which mixes the pretraining objective (i.e., the next word prediction) with the PPO objective to prevent regression performance on public benchmarks like SQuAD2.0.</li>
</ul>
<p dir="auto">The two training features, EMA and Mixed Training, are often omitted by other recent efforts since they can be optional. However, according to InstructGPT, EMA checkpoints generally provide better response quality than conventional final trained model and Mixture Training can help the model retain the pre-training benchmark solving ability. As such, we provide them for users to fully get the training experience as described in InstructGPT and strike for higher model quality.</p>
<p dir="auto">In addition to being highly consistent with InstructGPT paper, we also provide convenient features to support researchers and practitioners to train their own RLHF model with multiple data resources:</p>
<ul dir="auto">
<li><em><strong>Data Abstraction and Blending Capabilities:</strong></em> DeepSpeed-Chat is able to train the model with multiple datasets for better model quality. It is equipped with (1) an abstract dataset layer to unify the format of different datasets; and (2) data splitting/blending capabilities so that the multiple datasets are properly blended then split across the 3 training stages.</li>
</ul>
<p dir="auto">To illustrate the effectiveness of our training pipeline, we demonstrate the model quality with multi-round conversation as shown in the experience section.</p>

<p dir="auto">Step 1 and Step 2 of the instruct-guided RLHF pipeline resemble regular fine-tuning of large models, and they are powered by ZeRO-based optimizations and flexible combination of parallelism strategies in DeepSpeed training to achieve scale and speed.  Step 3 of the pipeline, on the other hand, is the most complex part to handle in terms of performance implications. Each iteration requires efficient processing of two phases a) inference phase for token/experience generation, producing inputs for the training and b) training phase to update the weights of actor and reward models, as well as the interaction and scheduling between them.  It introduces two major costs: (1) the memory cost, as several copies of the SFT and RW models need to be served throughout stage 3; and (2) the predominant generation phase, which if not accelerated properly, will significantly slow down the entire stage 3. Additionally, the two important features we added in Stage 3, including Exponential Moving Average (EMA) collection and Mixture Training, will incur additional memory and training costs.</p>
<p dir="auto">To tackle these challenges, we composed the full system capability of DeepSpeed Training and Inference into a unified infrastructure that we call <strong>Hybrid Engine</strong>. It leverages the original DeepSpeed engines for fast training mode while effortlessly applying DeepSpeed inference engine for generation/evaluation mode, providing a significantly faster training system for RLHF training at Stage 3. As Figure 2 shows, the transition between DeepSpeed training and inference engine is seamless: by having the typical eval and train modes enabled for the actor model, when running for inference and training pipeline, DeepSpeed selects its different optimizations to run the model faster and improve the overall system throughput.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://ntietz.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/hybrid-engine.png"><img src="https://ntietz.com/microsoft/DeepSpeed/raw/master/blogs/assets/images/hybrid-engine.png" width="600px" alt="DeepSpeed-Chat!"/></a></p><p dir="auto"><em>Figure 2. DeepSpeed Hybrid Engine design for accelerating the most time-consuming portion of a RLHF pipeline.</em></p>
</div>
<p dir="auto">During its inference execution for experience generation phase of RLHF training, DeepSpeed Hybrid Engine uses a light-weight memory management system to handle the KV-cache and intermediate results, together with highly optimized inference-adapted kernels and tensor parallelism implementation, to achieve significant boost in throughput (tokens-per-second) compared to the existing solutions.</p>
<p dir="auto">During the training execution, Hybrid Engine enables memory optimization techniques such as DeepSpeed’s ZeRO family of technologies and Low Rank Adaption (LoRA).   We designed and implemented these system optimizations in a way that they are compatible with each other and can be composed together to deliver the highest training efficiency under the unified Hybrid Engine.</p>
<p dir="auto">Hybrid Engine can seamlessly change model partitioning across training and inference to support tensor-parallelism based inferencing and ZeRO-based sharding mechanism for training. It can also reconfigure the memory system to maximize memory availability during each of these modes. This allows for improved performance by avoiding memory allocation bottlenecks and supporting large batch sizes. Packed with a spectrum of system technologies from DeepSpeed training and inference, Hybrid Engine pushes the boundary of modern RLHF training and delivers unparalleled scale and system efficiency for RLHF workloads.</p>

<h2 dir="auto"><a id="user-content-capability-recap" aria-hidden="true" href="#capability-recap"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Capability Recap</h2>
<p dir="auto">As discussed, DeepSpeed-HE is an amalgamation of powerful system technologies for inference and training, architected to achieve excellent scale and efficiency for DeepSpeed-RLHF pipeline across a wide range of hardware, making RLHF training fast, affordable, and easily accessible to AI community.</p>
<p dir="auto">In terms of efficiency and affordability, as shown in Table 1, DeepSpeed-HE can train OPT-13B in just 9 hours and OPT-30B in 18 hours on Azure Cloud for under $300 and $600, respectively. In terms of speed and scalability, as shown in Table 2, even a 13B model can be trained in 1.25 hours and a massive 175B model can be trained in under a day using a 64 GPU cluster. And in terms of accessibility and democratization of RLHF, DeepSpeed-HE supports training models with over 13 billion parameters on a single GPU as shown in Table 3.</p>
<h2 dir="auto"><a id="user-content-throughput-and-model-size-scalability-comparisons-with-existing-rlhf-systems" aria-hidden="true" href="#throughput-and-model-size-scalability-comparisons-with-existing-rlhf-systems"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Throughput and Model Size Scalability Comparisons with Existing RLHF Systems</h2>
<p dir="auto">Compared to other RLHF systems like Colossal-AI or HuggingFace powered by native PyTorch, DeepSpeed-RLHF excels in system performance and model scalability:</p>
<ul dir="auto">
<li>With respect to throughput, DeepSpeed enables over 10x improvement for RLHF training on a single GPU (Figure 3). On multi-GPU setup, it enables  6 – 19x  speedup over Colossal-AI and 1.4 – 10.5x over HuggingFace DDP (Figure 4).</li>
<li>With respect to model scalability, Colossal-AI can run a max model size of 1.3B on a single GPU and 6.7B on a single A100 40G node, DeepSpeed-HE can run 6.5B and 50B models respectively on the same hardware, up to 7.5x larger.</li>
</ul>
<p dir="auto">Therefore, with over an order of magnitude higher throughput, DeepSpeed-HE unlocks the ability to train significantly larger actor models under the same latency budget or train models of similar size at over 10x lower cost, compared to existing RLHF systems like Colossal-AI or HuggingFace DDP.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://ntietz.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/figure3.png"><img src="https://ntietz.com/microsoft/DeepSpeed/raw/master/blogs/assets/images/figure3.png" width="600px"/></a></p><p dir="auto"><em>Figure 3. Step 3 throughput comparison against two other system frameworks for accelerating RLHF </em></p>
</div>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://ntietz.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/figure4.png"><img src="https://ntietz.com/microsoft/DeepSpeed/raw/master/blogs/assets/images/figure4.png" width="600px"/></a></p><p dir="auto"><em>Figure 4. End-to-end training throughput comparison for step 3 of the training pipeline (the most time </em></p>
</div>
<p dir="auto">This improvement in efficiency stems from DeepSpeed-HE’s ability to accelerate RLHF generation phase of the RLHF processing leveraging DeepSpeed inference optimizations. Figure 5 shows the time breakdown for a 1.3B parameter model at an RLHF training iteration: majority of the time goes to the generation phase. By leveraging high performance inference kernels from DeepSpeed, DeepSpeed-HE can achieve up to 9x throughput improvement during this phase over HuggingFace and 15x over Colossal-AI allowing it to achieve unparallel end-to-end efficiency.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://ntietz.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/figure5.png"><img src="https://ntietz.com/microsoft/DeepSpeed/raw/master/blogs/assets/images/figure5.png" width="600px"/></a></p><p dir="auto"><em>Figure 5. Superior generation phase acceleration from DeepSpeed Chat’s Hybrid Engine: A time/sequence breakdown for training OPT-1.3B actor model + OPT-350M reward model on a single DGX node with 8 A100-40G GPUs.</em></p>
</div>
<h2 dir="auto"><a id="user-content-effective-throughput-and-scalability-analysis" aria-hidden="true" href="#effective-throughput-and-scalability-analysis"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Effective Throughput and Scalability Analysis</h2>
<p dir="auto"><em><strong>(I) Effective Throughput Analysis.</strong></em> The effective throughput of DeepSpeed-HE during Stage 3 of the RLHF training depends on the throughput that it achieves during the generation and RL training phases. In our RLHF pipeline, the generation phase comprises approximately 20% of the total computation while the RL training phase comprises of remaining 80% (see <a href="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/BenckmarkSetting.md">benchmark settings</a> page for details). However, despite having a small proportion, the former can take a large portion of the e2e time as it requires running the actor model once for each of the 256 generated tokens with initial prompt of 256 tokens, making it memory bandwidth bound and difficult to achieve high throughput. In contrast, the RL training phase is compute bound running the reference actor model with just a couple of forward and backward passes with full 512 tokens from both prompt and generation per sample and can achieve good throughput.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://ntietz.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/Throughputs-OPT.png"><img src="https://ntietz.com/microsoft/DeepSpeed/raw/master/blogs/assets/images/Throughputs-OPT.png" width="600px"/></a></p><p dir="auto"><em>Figure 6. RLHF Generation, training, and effective throughput with DeepSpeed-HE for different model sizes, at the GPU count that maximizes efficiency.</em></p>
</div>
<p dir="auto">To maximize the effective throughput, DeepSpeed-HE optimizes both phases. First, it uses the largest batch size possible to get higher efficiency on both phases. Second, during the generation phase, it leverages high-performance transformer kernels to maximize GPU memory bandwidth utilization when the model fits in single GPU memory, and leverage tensor-parallelism (TP) when it does not. Using TP in the generation phase instead of ZeRO to fit the model reduces the inter-GPU communication and maintains high GPU memory bandwidth utilization.</p>
<p dir="auto">Figure 6 shows the best achievable effective throughput for DeepSpeed-HE in terms of TFlops/GPU for model sizes ranging from 1.3B to 175B. It also shows the throughput achieved by each of the generation and training phases. DeepSpeed-HE is the most efficient for models in the range 6.7B-66B. Going beyond this range to 175B, the throughput drops due to the limited memory to support larger batch sizes, while still achieving 1.2x better efficiency than the small 1.3B model. The per-GPU throughput of these gigantic models could improve further when we scale them to more GPUs with more memory available for larger batch sizes.</p>
<p dir="auto">Furthermore, we would like to point out that our effective performance is 19x higher than existing systems, as shown in Figure 4, which suggests that they are operating at lower than 5% of the peak. This demonstrates the challenge of optimizing RLHF workloads as well as the effectiveness of our system despite the challenge.</p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://ntietz.com/microsoft/DeepSpeed/blob/master/blogs/assets/images/figure7.png"><img src="https://ntietz.com/microsoft/DeepSpeed/raw/master/blogs/assets/images/figure7.png" width="600px"/></a></p><p dir="auto"><em>Figure 7. Scalability for training 13B (left) and 66B (right) actor model+350M reward model on an increasing number of DGX nodes with 8 A100-40/80G GPUs</em></p>
</div>
<p dir="auto"><em><strong>(II) Scalability Analysis.</strong></em> The best effective throughput for different model sizes is achieved at different GPU count. This is in part because some of the larger model sizes require more memory to run. However, a large part of this behavior stems from DeepSpeed-HE’s scalability properties that we discuss next.</p>
<p dir="auto">Figure 7 shows that DeepSeed-RLHF has achieved good scaling overall on up to 64 GPUs. However, if we look more closely, it shows that DeepSpeed-RLHF training achieves super-linear scaling at small scale, followed by near linear or sub-linear scaling at larger scales. This is due to interaction between memory availability and max global batch size.</p>
<p dir="auto">As DeepSpeed-HE is powered by ZeRO-based technology for training, it allows model states to be partitioned across the available GPUs. As a result, the memory consumption per GPU reduces with the increase in the number of GPUs, allowing DeepSpeed-HE to support a larger batch per GPU resulting in super-linear scaling. However, at large scale, while the available memory continues to increase, the maximum global batch size (1024, in our case, with a sequence length of 512) limits the batch size per GPU, resulting in near-linear or sub-linear scaling.
As a result, for a given max global batch size, DeepSpeed-HE achieves the best throughput and cost efficiency at the boundary of super-linear and sub-linear scalability, and the exact point is mostly determined by the largest batch size that can be run per GPU as the function of available memory and global batch size.</p>

<p dir="auto">We are very excited to share that DeepSpeed-Chat is now open-sourced and available to the AI community.</p>
<ul dir="auto">
<li>
<p dir="auto">To get started, please visit our github page for DeepSpeed-Chat: <a href="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat">GitHub Landing Page</a></p>
</li>
<li>
<p dir="auto">We will continue to improve DeepSpeed-Chat with your feedback and support. Our <a href="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/README.md#-deepspeed-chats-roadmap-">roadmap</a> shows currently supported features as well as ones that are planned for future.</p>
</li>
</ul>
<p dir="auto">DeepSpeed-Chat is part of the bigger DeepSpeed ecosystem comprising of a multitude of Deep Learning systems and modeling technologies. To learn more,</p>
<ul dir="auto">
<li>Please visit our <a href="https://www.deepspeed.ai/" rel="nofollow">website</a> for detailed blog posts, tutorials, and helpful documentation.</li>
<li>You can also follow us on our <a href="https://twitter.com/MSFTDeepSpeed" rel="nofollow">English Twitter</a> and <a href="https://twitter.com/MSFTDeepSpeedJP" rel="nofollow">Japanese Twitter</a> for latest news on DeepSpeed.</li>
</ul>
<p dir="auto">DeepSpeed welcomes your contributions! We encourage you to report issues, contribute PRs, and join discussions on the <a href="https://github.com/microsoft/DeepSpeed/">DeepSpeed GitHub</a> page. Please see our <a href="https://github.com/microsoft/DeepSpeed/blob/master/CONTRIBUTING.md">contributing guide</a> for more details. We are open to collaborations with universities, research labs, companies, such as those working together on deep learning research, applying DeepSpeed to empower real-world AI models and applications, and so on. For such requests (and other requests unsuitable for GitHub), please directly email to <a href="mailto:deepspeed-info@microsoft.com">deepspeed-info@microsoft.com</a>.</p>
</article>
  </div>

    </div>

  </div></div>
  </body>
</html>
