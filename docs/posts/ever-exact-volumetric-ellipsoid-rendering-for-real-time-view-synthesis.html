<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://half-potato.gitlab.io/posts/ever/">Original</a>
    <h1>Ever: Exact Volumetric Ellipsoid Rendering for Real-Time View Synthesis</h1>
    
    <div id="readability-page-1" class="page"><div><p>An overview of the quality benefits of our EEVR technique. Left: On the Zip-NeRF dataset, our model produces sharper and more accurate renderings than 3DGS and successor splatting-based techniques. Middle: Because our method correctly blends primitive colors according to the physics of volume rendering, it produces fewer “foggy” artifacts than splatting. Right: Our method correctly blends primitive colors, which is not possible using a splatting regardless of how primitives are sorted (globally or ray-wise).</p>

<p>We present Exact Ellipsoid Volumetric Rendering (EEVR), a method for real-time differentiable emission-only volume rendering.
Unlike recent rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive based representation allows for exact volume rendering, rather than alpha compositing 3D Gaussian billboards.
As such, unlike 3DGS our formulation does not suffer from popping artifacts and view dependent density, but still achieves frame rates of <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><mtext> ⁣</mtext><mn>30</mn></mrow><annotation encoding="application/x-tex">\sim\!30</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>∼</span><span></span><span></span></span><span><span></span><span>3</span><span>0</span></span></span></span></span> FPS at 720p on an NVIDIA RTX4090.
Because our approach is built upon ray tracing it supports rendering techniques such as defocus blur and camera distortion (e.g. such as from fisheye cameras), which are difficult to achieve by rasterization.
We show that our method has higher performance and fewer blending issues than 3DGS and other subsequent works, especially on the challenging large-scale scenes from the Zip-NeRF dataset where it achieves SOTA results among real-time techniques.</p>
<p><iframe width="100%" height="500rem" src="https://www.youtube.com/embed/dqLi2-v38LE?si=KtxK2qOiNoq_E-tE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"></iframe></p>

<p><span><span>
      <a href="https://ntietz.com/static/22616100527f122662ff5882c163d039/6c86f/figure2.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="figure2" title="" src="https://ntietz.com/static/22616100527f122662ff5882c163d039/d9199/figure2.png" srcset="/static/22616100527f122662ff5882c163d039/8ff5a/figure2.png 240w,
/static/22616100527f122662ff5882c163d039/e85cb/figure2.png 480w,
/static/22616100527f122662ff5882c163d039/d9199/figure2.png 960w,
/static/22616100527f122662ff5882c163d039/07a9c/figure2.png 1440w,
/static/22616100527f122662ff5882c163d039/6c86f/figure2.png 1720w" sizes="(max-width: 960px) 100vw, 960px" loading="lazy" decoding="async"/>
  </a>
    </span></span>
(a) Here we show a toy “flatland” scene containing two primitives (one red, one blue) with a camera orbiting them, viewed from above.
We render this orbit using three different techniques, where each camera position yields a one-dimensional “image” (a scanline) which are stacked vertically to produce these epipolar plane image (EPI) visualizations.
(b, c) The approximations made by approximate splatting-based techniques result in improper blending due to discontinuities, which are visible as horizontal lines across the EPI.
In contrast, (d) our method’s exact rendering yields a smooth EPI, with bands of purple from color blending.</p>
<p><span><span>
      <a href="https://ntietz.com/static/3abe684ea62fade02044bab35b8096fe/4ff83/figure3.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="figure3" title="" src="https://ntietz.com/static/3abe684ea62fade02044bab35b8096fe/4ff83/figure3.png" srcset="/static/3abe684ea62fade02044bab35b8096fe/8ff5a/figure3.png 240w,
/static/3abe684ea62fade02044bab35b8096fe/e85cb/figure3.png 480w,
/static/3abe684ea62fade02044bab35b8096fe/4ff83/figure3.png 843w" sizes="(max-width: 843px) 100vw, 843px" loading="lazy" decoding="async"/>
  </a>
    </span></span>
A visualization of our rendering procedure. Top: We cast a ray through a field of constant density ellipsoids and compute each ray-ellipsoid collision distance to get the endpoints of each step function. When the ray enters each primitive, the density along the ray increases. When it exits, the density drops back down a corresponding amount. Bottom: This lets us analytically integrate the volume rendering equation through the field.</p>













<h2 id="bibtex"><a href="#bibtex" aria-label="bibtex permalink"><span><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></span></a>Bibtex</h2></div></div>
  </body>
</html>
