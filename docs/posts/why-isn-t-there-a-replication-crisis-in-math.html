<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jaydaigle.net/blog/replication-crisis-math/">Original</a>
    <h1>Why isn&#39;t there a replication crisis in math?</h1>
    
    <div id="readability-page-1" class="page"><div>

  

  

  <p>One important thing that I think about a lot, even though I have no formal expertise, is the <a href="https://www.vox.com/future-perfect/21504366/science-replication-crisis-peer-review-statistics">replication crisis</a>.  A shocking fraction of published research in many fields, including medicine and psychology, is flatly wrong—the results of the studies can’t be obtained in the same way again, and the conclusions don’t hold up to further investigation.  Medical researcher  John Ioannidis brought this problem to wide attention in 2005 with a paper titled <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124">Why Most Published Research Findings Are False</a>; attempts to replicate the results of major psychology papers suggest that <a href="https://www.theatlantic.com/science/archive/2018/11/psychologys-replication-crisis-real/576223/">only about half of them hold up</a>.  A recent analysis gives <a href="https://apnews.com/article/science-business-health-cancer-marcia-mcnutt-93219170405e3de753651b89d4308461">a similar result for cancer research</a>.</p>

<p>This is a real crisis for the whole process of science.  If we can’t rely on the results of famous, large, well-established studies, it’s hard to feel secure in <em>any</em> of our knowledge.  It’s probably the most important problem facing the entire project of science right now.</p>

<p>There’s a lot to say about the mathematics we use in social science research,  especially statistically, and how bad math feeds the replication crisis.<strong title="I&#39;m a big fan of the [Data Colada] project, and of  [Andrew Gelman&#39;s writing] on the subject"><sup id="fnref:1"><a href="#fn:1">1</a></sup></strong>  But I want to approach it from a different angle.  <strong>Why doesn’t <em>the field of mathematics</em> have a replication crisis?</strong>  And what does that tell us about other fields, that do?</p>

<h3 id="why-doesnt-math-have-a-replication-crisis">Why doesn’t math have a replication crisis?</h3>

<h5 id="maybe-mathematicians-dont-make-mistakes">Maybe mathematicians don’t make mistakes</h5>

<p>Have you, uh, <a href="https://mathwithbaddrawings.com/2017/01/11/why-are-mathematicians-so-bad-at-arithmetic/">met any mathematicians</a>?</p>

<p><a href="https://mathwithbaddrawings.com/2017/01/11/why-are-mathematicians-so-bad-at-arithmetic/"><img src="https://jaydaigle.net/assets/blog/replication-crisis-math/sign-error.jpg" alt="Cartoon: &#34;So the tip is...$70?  But the meal was only $32...&#34; &#13;&#13; &#34;Maybe we made a sign error, and they owe us $70.&#34;" width="75%"/></a></p>
<p><em>Comic by Ben Orlin at <a href="https://mathwithbaddrawings.com/2017/01/11/why-are-mathematicians-so-bad-at-arithmetic/">Math with Bad Drawings</a></em></p>

<p><em>At Caltech, they made the youngest non-math major split the check: the closer you were to high school, the more you remembered of basic arithmetic.  But everyone knew the math majors were hopeless.</em></p>

<p>More seriously, it’s reasonably well-known among mathematicians that <strong>published math papers are <a href="https://twitter.com/benskuhn/status/1419281164951556097"><em>full</em> of errors</a></strong>.  Many of them are eventually fixed, and most of the errors are in a deep sense “unimportant” mistakes.  But the frequency with which proof formalization efforts <a href="https://mathoverflow.net/questions/291158/proofs-shown-to-be-wrong-after-formalization-with-proof-assistant">find flaws in widely-accepted proofs</a> suggests that there are plenty more errors in published papers that no one has noticed.</p>

<p>So math has, if not a replication crisis, at least a replication problem.  Many of our published papers are flawed.  But it doesn’t seem like we have a crisis.</p>

<h5 id="maybe-our-mistakes-get-caught">Maybe our mistakes get caught</h5>

<p>In the social sciences, replicating a paper is hard.  You have to get new funding and run a new version of the same experiment.  There’s a lot of dispute about how closely you need to replicate all the mechanics of the original experiment for it to “count” as a replication, and sometimes you can’t get a lot of the details you’d need to do it right—especially if the original authors aren’t feeling helpful.<strong title="In theory, all papers should include enough information that you can replicate all the experiments they describe.  In practice, I think this basically never happens.  There&#39;s just too much information, and it&#39;s hard to even guess which things are going to be important."><sup id="fnref:2"><a href="#fn:2">2</a></sup></strong>  And after all that work, people won’t even be impressed, because you didn’t do anything original!</p>

<p>But one of the distinctive things about math is that our papers aren’t just records of experiments we did elsewhere.  In experimental sciences, the experiment is the “real work” and the paper is just a description of it.  But <strong>in math, the paper, itself, is the “real work”</strong>.  Our papers don’t describe everything we do, of course.  There’s a lot of intellectual exploration and just straight-up messing around that doesn’t get written down anywhere.  But the paper contains a (hopefully) complete version of the argument that we’ve constructed.</p>

<p>And that means that <strong>you can <em>replicate</em> a math paper by <em>reading</em> it</strong>.  When I’ve served as a peer reviewer I’ve read the papers closely and checked all the steps of the proofs, and that means that I have replicated the results.  And any time you want to use an argument from someone else’s paper, you have to work through the details, and that means you’re replicating it again.</p>

<p>The replication crisis is partly the discovery that many major social science results do not replicate.  But it’s also the discovery that we hadn’t been trying to replicate them, and we really should have been.  In the social sciences we fooled ourselves into thinking our foundation was stronger than it was, by never testing it.  But in math we couldn’t avoid testing it.</p>

<h5 id="maybe-the-crisis-is-here-and-we-just-havent-noticed">Maybe the crisis is here, and we just haven’t noticed</h5>

<p>As our mathematics gets more advanced and our results get more complicated, this replication process becomes harder: it takes more time, knowledge, and expertise to understand a single paper.  If replication gets hard enough, we may fall into crisis.  The crisis might even <a href="https://link.springer.com/article/10.1007/s00283-020-10037-7">already be here</a>; the problems in psychological and medical research existed for decades before they were widely appreciated.</p>

<p>There’s some fascinating work in using <a href="https://www.nature.com/articles/d41586-021-01627-2">computer tools to formally verify proofs</a>, but this is still a niche practice.  In theory we are continually re-checking all our work, but in practice that’s inconsistent, so it’s hard to be sure how deep the problems run.  (Especially since flawed papers <a href="https://twitter.com/zbMATH/status/1474326312517271560">don’t really get retracted</a> and you pretty much have to talk to active researchers in a field to know which papers you can trust.)</p>

<p><img src="https://jaydaigle.net/assets/blog/replication-crisis-math/trust.jpg" alt="Picture of kitten in bubble bath with caption: &#34;my trust, u loses it.&#34; " width="50%"/></p>

<p>But while this is a real possibility that people should take seriously, I’m skeptical that we’re in the middle of a true crisis of replicability.<strong title="I&#39;m sure every practitioner in every field says that, though, even years after the problems become obvious to anyone who looks.  So take this with a grain of salt."><sup id="fnref:3"><a href="#fn:3">3</a></sup></strong>  <strong>Many papers have errors, yes—but our major results generally hold up, even when the intermediate steps are wrong!</strong>  Our errors can usually be fixed without really changing our conclusions.</p>

<p>Since our main conclusions hold up, we don’t need to fix any downstream papers that relied on those conclusions.  We don’t need to substantially revise what we thought we knew.  We don’t need to jettison entire fields of research, the way <a href="https://replicationindex.com/2017/02/02/reconstruction-of-a-train-wreck-how-priming-research-went-of-the-rails/comment-page-1/">psychology had to abandon the literature on social priming</a>.  There are problems, to be sure, and we could always do better.  But it’s not a crisis.</p>

<h5 id="mysterious-intuition">“Mysterious” intuition</h5>

<p>But isn’t it…<em>weird</em>…that our results hold up when our methods don’t?  How does that even work?</p>

<p>We get away with it becuase we can be right for the wrong reasons—<strong>we mostly only try to prove things that are basically true</strong>.  Ben Kuhn tweeted a very accurate-feeling summary of the whole situation <a href="https://twitter.com/benskuhn/status/1419281164951556097">in this twitter thread</a>:</p>
<blockquote>
  <p>[D]espite the fact that error-correction is really hard, publishing actually false results was quite rare because “people’s intuition about what’s true is mysteriously really good.”  Because we mostly only try to prove true things, our conclusions are right even when our proofs are wrong.<strong title="A friend asks: if we mostly know what&#39;s true already, why do we need to actually find the proofs?  The bad answer is &#34;you&#39;re not doing math if you don&#39;t prove things&#34;.  The good answer is that finding proofs is how we train this mysteriously good intuition; if we didn&#39;t work out proofs in detail, we wouldn&#39;t be able to make good guesses about the next steps."><sup id="fnref:4"><a href="#fn:4">4</a></sup></strong></p>
</blockquote>

<p>This can make it weirdly difficult to resolve disagreements about whether a proof is actually correct.  In a recent example, Shinichi Mochizuki claims that he has <a href="https://www.quantamagazine.org/titans-of-mathematics-clash-over-epic-proof-of-abc-conjecture-20180920/">proven the \(abc\) conjecture</a>, while most mathematicians don’t believe his argument is valid.  But everyone involved is pretty confident the \(abc\) conjecture is true; the disagreement is about whether the proof itself is good.</p>

<p><img src="https://jaydaigle.net/assets/blog/replication-crisis-math/proof.jpg" alt="Picture of cat walking through kitchen covered in trash: &#34;come find me when you have proof.&#34;" width="75%"/></p>

<p><em>Circumstantial evidence isn’t enough to make mathematicians happy.</em></p>

<p>If we find a counterexample to \(abc\) then Mochizuki is clearly wrong, but so is everyone else.  If we find a consensus proof of \(abc\), then Mochizuki’s conclusion is right, but that does very little to make his argument more convincing.  He could, very easily, just be lucky.</p>

<h3 id="butpsychologists-have-intuition-too">But—Psychologists have intuition, too</h3>

<p>A lot of psychology results that don’t replicate look a little different from this perspective.  Does standing in a <a href="https://en.wikipedia.org/wiki/Power_posing">power pose</a> for a few seconds make you feel more confident?  Probably!  It sure feels like it does (seriously, stand up and give it a try right now); and it would be weird if it made you feel <em>worse</em>.  Does it affect you enough, for a long enough time, to matter much?  Probably not.  That would also be weird.</p>

<p><img src="https://jaydaigle.net/assets/blog/replication-crisis-math/power-pose.jpg" alt="Picture of Amy Cuddy standing in front of a picture of Wonder Woman, in matching poses" width="50%"/></p>

<p><em>Amy Cuddy demonstrating a power pose. </em></p>

<p>The studies we’ve done, when analyzed properly, don’t show a clear, consistent, and measurable effect from a few seconds of power posing. But that’s what you’d expect, right?  There’s probably an effect, but it should be too small to reasonably measure.  And that’s totally consistent with everything we’ve found.</p>

<p>Amy Cuddy<strong title="I&#39;m going to pick on Amy Cuddy and power posing a lot.  That&#39;s not entirely fair to Cuddy; the pattern I&#39;m describing is extremely common and easy to fall into, and I could make the same argument about [social priming research] or the [hungry judges study] or the dozens of others.  (That&#39;s why it&#39;s a &#34;replication crisis&#34; and not a &#34;this one researcher made a mistake one time crisis&#34;.)  But for simplicity I&#39;m going to stick to the same example for most of this post."><sup id="fnref:5"><a href="#fn:5">5</a></sup></strong> had the intuition that power posing would increase confidence, and set out to prove it—just like Mochizuki had the intuition that the \(abc\) conjecture was true, and set out to prove it.   Mochizuki’s proof was bad, but his top-line conclusion was probably right because the \(abc\) conjecture is probably correct.  And Cuddy’s studies were flawed, but her intuition at the start was probably right, so her top-line conclusion is probably true.</p>

<p>Well, sort of.</p>

<h5 id="defaulting-to-zero">Defaulting to zero</h5>

<p>Let’s turn Cuddy’s question around for a bit.<strong title="Mathematicians love doing this.  I&#39;m a mathematician, so I love doing this. But it&#39;s genuinely a useful way to think about what&#39;s going on."><sup id="fnref:6"><a href="#fn:6">6</a></sup>  What are the chances that power posing has <em>exactly zero</em> affect on your psychology?  That would be extremely surprising.  Most things you do affect your mindset at least a little.<strong title="This is your regular reminder to stand up, stretch, and drink some water."><sup id="fnref:7"><a href="#fn:7">7</a></sup></strong></strong></p>

<p>So our expectation should be: either power posing makes you a little more confident, or it makes you a little less confident.  It also probably makes you either a little more friendly or a little less friendly, a little more or a little less experimental, a little more or a little less agreeable—<strong>an effect of exactly zero would be a surprise</strong>.</p>

<p>But for confidence specifically, it would also be kind of surprising if power posing made you feel less confident.  So my default assumption is that power posing causes a small increase in confidence.  And nominally, Cuddy’s research asked whether that default assumption is correct.</p>

<p>But that’s just not a great question.  It doesn’t really matter if standing in a power pose makes you feel marginally better for five seconds.   Not worth a book deal and a TED talk, and barely worth publishing.  <strong>Cuddy’s research was interesting because it suggested the effect of power posing was not only positive, but <em>large</em></strong>—enough to make a dramatic, usable impact over an extended period of time.</p>

<p>If Cuddy’s results were true, they would be both surprising and important.  But that’s just another way of saying they’re probably not true.</p>

<h5 id="power-and-precision">Power and Precision</h5>

<p>Notice: we’ve shifted to  a new, different question.  We started out asking “does power posing make you more confident”, but now we’re answering “how much more confident does power posing make you”.  This is a better question, sure, but it’s different.  And <strong>the statistical tools appropriate to the first question don’t really work for the new and better one.</strong></p>

<p><a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing">Statistical hypothesis testing</a> is designed to give a yes/no answer to “is this effect real”.  Hypothesis testing is surprisingly complicated to actually explain correctly, and probably deserves an essay or two on its own.<strong title="I originally tried to write a concise explanation to include here.  It hit a thousand words and was nowhere near finished, so I decided to save it for later."><sup id="fnref:8"><a href="#fn:8">8</a></sup></strong></p>

<p><img src="https://jaydaigle.net/assets/blog/replication-crisis-math/hypothesis-testing.png" alt="Diagram of true and false positives and negatives on a bell curve" width="75%"/></p>

<p><em>I swear this picture makes sense.</em></p>

<p>To wildly oversimplify, we measure something, and check if that measurement is so big that it’s unlikely to occur by chance.  If yes, we conclude that there’s a real effect from whatever we’re studying.  If not, we generally conclude that there’s no effect.</p>

<p>But what if the effect is real, but very small?  With this method, we conclude the effect is real if our measurements are big enough.  <strong>But if the effect is small, our measurements won’t be <em>big</em>.  Our study might not have enough <a href="https://en.wikipedia.org/wiki/Power_of_a_test">power</a> to find the effect</strong> even if it is real.<strong title="This means we have to be really careful about interpreting studies that don&#39;t find any effect.  A study with low power will find &#34;[no evidence](https://twitter.com/zeynep/status/1366175070507384836?lang=en)&#34; of an effect even if the effect is very real, and that can be [just as misleading](https://twitter.com/CT_Bergstrom/status/1487491536010944512) as the errors I&#39;m discussing in this essay. 

 More careful researchers will say they &#34;fail to reject the null hypothesis&#34; or &#34;fail to find an effect&#34;.  If everyone were always that careful I wouldn&#39;t need to write this essay."><sup id="fnref:9"><a href="#fn:9">9</a></sup></strong></p>

<p>We could run a more powerful study and find evidence of smaller effects if we could make more precise measurements.  This approach has worked really well in fields like physics and chemistry, and a lot of fundamental physical discoveries were driven by new technology that allowed the measurement of smaller effects.  Galileo’s experiments with falling speeds required him to invent <a href="https://www.thegreatcoursesdaily.com/the-rolling-ball-experiments-galileos-terrestrial-mechanics/">improved timekeeping methods</a>, and Coulomb developed his inverse-square law after <a href="https://en.wikipedia.org/wiki/Coulomb%27s_law#History">his torsion balance</a> allowed him to precisely measure electromagnetic attraction.  In the modern era, we built extremely sensitive measurement devices to try to measure <a href="https://en.wikipedia.org/wiki/LIGO">gravity waves</a> and detect <a href="https://en.wikipedia.org/wiki/Higgs_boson#Search_and_discovery">the Higgs boson</a>.</p>

<p>If power posing increases confidence by 1% for thirty seconds, that  would actually be perfectly fine if we could measure confidence to within a hundredth of a percent on a second-to-second basis.  But social psychology experiments just don’t work that way—at least, not with our current technology.  There’s too much randomness and behavioral variation.  Effects of that size just aren’t detectable.</p>

<p>This doesn’t have to be a problem!  If we want to know “how big is the effect of power posing”, the answer is “too small to detect”.  That’s a fine answer.  It tells you that you shouldn’t build any complicated apparatus based on exploiting the power pose.  (Or write <a href="https://www.goodreads.com/book/show/25066556-presence">entire books</a> on how it can change your life.)</p>

<p>But the question we started with was “does power posing have an effect at all?”.  If the effect is small, we might struggle to tell whether it’s real or not.</p>

<h5 id="but-we-already-know-the-answer">But we already know the answer!</h5>

<p>Imagine you’re a psychologist researching power posing.  You measure a small effect, which could just be do to chance.  But you’re pretty sure that the effect is real; clearly you didn’t do a good enough job in your study!  It’s probably <a href="https://en.wikipedia.org/wiki/Publication_bias">not even worth publishing</a>.</p>

<p><img src="https://jaydaigle.net/assets/blog/replication-crisis-math/X-Men-question-answer.gif" alt="Gif from X-Men movie. &#34; Why do you ask questions to which you already know the answers? " width="75%"/></p>

<p>So you try again.  Or someone else tries again.  And eventually someone runs a study that <em>does</em> see a large effect.  (Occasionally the large effect is due to fraud.  Usually it’s methodology with subtler flaws that the researcher doesn’t notice.  And sometimes it’s just luck: you’ll get a one-in-twenty outcome once in every twenty tries.)</p>

<p>Now we’re all happy.  We were pretty sure that we would see an effect if we looked closely enough.  And there it is!  At this point no one has an incentive to look for flaws in the study.  The result makes sense.  (You might remember we said this is the state of a lot of mathematical research.)</p>

<p>But there are two major problems we can run into here.  The first is that <strong>our intuition can, in fact, be wrong</strong>.  If your process can only ever prove things that you already believed, it’s not a good process; you can’t really learn anything.  Andrew Gelman <a href="https://statmodeling.stat.columbia.edu/2021/11/18/fake-drug-studies/">recently made this observation about fraudulent medical research</a>:</p>

<blockquote>
  <p>If you frame the situation as, “These drugs work, we just need the paperwork to get them approved, and who cares if we cut a few corners, even if a couple people die of unfortunate reactions to these drugs, they’re still saving thousands of lives,” then, sure, when you think of aggregate utility we shouldn’t worry too much about some fraud here and there…</p>
</blockquote>

<blockquote>
  <p>But I don’t know that this optimistic framing is correct. I’m concerned that bad drugs are being approved instead of good drugs….Also, negative data—examples where the treatment fails to work as expected—provide valuable information, and by not doing real trials you’re depriving yourself of opportunities to get this feedback.</p>
</blockquote>

<p>Shoddy research practices make sense if you see scientific studies purely as bureaucratic hoops you have to jump through: it’s “obviously true” that power posing will make you bolder and more confident, and the study is just a box you have to check before you can go around saying that out loud.  But <strong>if you want to learn things, or be surprised by your data, you need to be more careful</strong>.</p>

<h3 id="effect-sizes-matter">Effect Sizes Matter</h3>

<h5 id="overestimation">Overestimation</h5>

<p>The second problem can bite you even if your original intuition is right.  You start out just wanting to know “is there an effect, y/n?”, but your experiment will make a measurement.  You will get an estimate of the <em>size</em> of the effect.  And that estimate will be wrong.</p>

<p>Your estimate will be wrong for a silly, almost tautological reason: <strong>if you can only detect large effects, then any effect you detect will be large</strong>.  If you keep looking for an effect, over and over again, until finally one study gets lucky and sees it, that study will almost necessarily give <a href="https://statmodeling.stat.columbia.edu/2014/11/17/power-06-looks-like-get-used/">a wild overestimate</a> of the effect size.</p>

<p><img src="https://statmodeling.stat.columbia.edu/wp-content/uploads/2014/11/Screen-Shot-2014-11-17-at-11.19.42-AM.png" alt="A diagram of the effects of low-power studies. &#13;&#13;
This is what &#34;power = 0.06&#34; looks like.  Get used to it.&#13; &#13;
Type S error probability: If the estimate is statistically significant, it has a 24% chance of having the wrong sign. &#13;&#13;
Exaggeration ratio: If the estimate is statistically significant, it must be at least 9 times higher than the effect size." width="75%"/></p>

<p><em>If the effect is small relative to your measurement precision, your results are guaranteed to be misleading.  Figure by <a href="https://statmodeling.stat.columbia.edu/2014/11/17/power-06-looks-like-get-used/">Andrew Gelman</a>.</em></p>

<p>And this is how you wind up with shoddy research telling you that all sorts of things have shockingly large and dramatic impacts on…whatever you’re studying.  You start out with the intuition that power posing should increase confidence, which is reasonable enough.  You run studies, and eventually one of them agrees with you: power posing does make you more confident.  But not just a little.  In your study, people who did a little power posing saw big benefits.</p>

<p>To your surprise, you’ve discovered a life-changing innovation.  You issue press releases, write a book, give a TED talk, spread the good news of how much you can benefit from this little tweak to your life.</p>

<p>Then other researchers try to probe the effect further—and it vanishes.  Most studies don’t find clear evidence at all.  The ones that do find something show much smaller effects than you had found.  Of course they do.  Your study had an unusually rare result, because that’s why it got published in the first place.</p>

<h5 id="dont-forget-your-prior">Don’t forget your prior</h5>

<p>Notice how, in all of this, we lost sight of our original hypothesis.  It seemed basically reasonable to think power posing might perk you up a bit.  That’s what we originally wanted to test, and that’s the conviction that made us keep trying.  But we <em>didn’t</em> start out thinking that it would have a huge, life-altering impact.</p>

<p><strong>A really large result should feel just as weird as no result at all, if not weirder</strong>.  And when we stop to think about that, we know it; some research suggests that <a href="https://twitter.com/BrianNosek/status/1034093709971873794">social scientists have a pretty good idea which results are actually plausible</a>, and which are nonsense overestimates  But since we started with the question “is there an effect at all”, the large result we got <em>feels</em> like it confirms our original belief, even though it really doesn’t.</p>

<p>This specific combination is dangerous.  The direction of the effect is reasonable and expected, so we accept the study as plausible.  The size of the effect is shocking, which makes the study <em>interesting</em>, and gets news coverage and book deals and TED talks.</p>

<p>And this process repeats itself over and over, and the field builds up a huge library of incredible results that <a href="https://statmodeling.stat.columbia.edu/2017/12/15/piranha-problem-social-psychology-behavioral-economics-button-pushing-model-science-eats/">can’t possibly all be true</a>.  Eventually the music stops, and there’s a crisis, and that’s where we are today.  But it all starts somewhere reasonable: with people trying to prove something that is obviously true.</p>

<h5 id="so-how-is-math-different">So how is math different?</h5>

<p>This is exactly the situation we said math was in.  Mathematicians have pretty good idea of what results should be true; but so do psychologists!  Mathematicians sometimes make mistakes, but since they’re mostly trying to prove true things, it all works out okay.  Social scientists are also (generally) trying to prove true things, but it doesn’t work out nearly so well.  Why not?</p>

<p>In math, a result that’s too good <em>looks</em> just as troubling as one that isn’t good enough.  The idea of “<a href="https://en.wikipedia.org/wiki/Proving_too_much">proving too much</a>” is a core tool for reasoning about mathematical arguments.   It’s common to critique a proposed proof with something like “if that argument worked, it would prove all numbers are even, and we know that’s wrong”.  This happens at all levels of math, whether you’re in college taking Intro to Proofs, or vetting a high-profile attempt to solve a major open problem.  <strong>We’re in the habit of checking whether a result is—literally!—too good to be true</strong>.</p>

<p><img src="https://jaydaigle.net/assets/blog/replication-crisis-math/anti-gravity-cat.jpg" alt="Picture of a floating cat.  &#34;damn anti-gravity cat always disproving ma theorem&#34;" width="50%"/></p>

<p>We could bring a similar approach to social science research. Daniël Lakens <a href="http://daniellakens.blogspot.com/2017/07/impossibly-hungry-judges.html">uses this sort of argument</a> to critique a <a href="https://www.pnas.org/content/108/17/6889.short">famous study</a> on hunger and judicial decisions:</p>

<blockquote>
  <p>I think we should dismiss this finding, simply because it is impossible. When we interpret how impossibly large the effect size is, anyone with even a modest understanding of psychology should be able to conclude that it is impossible that this data pattern is caused by a psychological mechanism. As psychologists, we shouldn’t teach or cite this finding, nor use it in policy decisions as an example of psychological bias in decision making.</p>
</blockquote>

<p>Other researchers have found <a href="https://mindhacks.com/2016/12/08/rational-judges-not-extraneous-factors-in-decisions/">specific problems with the study</a>, but Lakens’s point is that we could dismiss the result even before they did.  If a proposed proof of Fermat’s last theorem also shows there are no solutions to $a^2 + b^2 = c^2$, we know it’s <em>wrong</em>, even before we find the specific flaw in the argument.  And if a study suggest humans aren’t capable of making reasoned decisions at 11:30 AM, it’s confounded by <em>something</em>, even if we don’t know what.</p>

<p>And yet, while I don’t believe in these studies, and I don’t believe their effect sizes, I still believe their basic claims.   I believe that people make worse decisions when they’re hungry.  (I know I do.)  I believe standing in a power pose can make you feel stronger and more assertive. I believe that <a href="https://www.vox.com/2016/3/14/11219446/psychology-replication-crisis">exercising self-control can deplete your willpower</a>.</p>

<p>But as a mathematician, I’m forced to admit: we don’t have  proof.</p>

<hr/>

<p><em>Do you think we have a replication crisis in math?  Disagree with me about the replication crisis?  Think you make better decisions when you’re hungry?  Tweet me <a href="https://twitter.com/profjaydaigle">@ProfJayDaigle</a> or leave a comment below.</em></p>



   <em></em>


<hr/>

Tags:   
  <strong><a href="https://jaydaigle.net/blog/#!/philosophy%20of%20math"><i></i>
      philosophy of math</a></strong>
  
  <strong><a href="https://jaydaigle.net/blog/#!/science"><i></i>
      science</a></strong>
  
  <strong><a href="https://jaydaigle.net/blog/#!/replication%20crisis"><i></i>
      replication crisis</a></strong>
  

  



  
  





  
   </div></div>
  </body>
</html>
