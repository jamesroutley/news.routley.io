<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sharnoff.io/blog/why-rust-compiler-slow">Original</a>
    <h1>Why is the Rust compiler so slow?</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
        
<div>
    
    <p><span>26 Jun 2025 at 00:34:13 +01:00</span>
    
</p>

	<p>I spent a month repeatedly building my website in Docker, and now have horrors to share.</p>

    <hr/>

    
<p>I&#39;ve got a problem.</p>
<p>My website (the one you&#39;re reading right now) is mainly served by a single Rust binary.
For <em>far too long</em> now, every time I wanted to make a change, I would:</p>
<ol>
<li>Build a new statically linked binary (with <code>--target=x86_64-unknown-linux-musl</code>)</li>
<li>Copy it to my server</li>
<li>Restart the website</li></ol>
<p>This is... not ideal.</p>
<p>So instead, I&#39;d like to switch to deploying my website with containers (be it Docker, Kubernetes, or otherwise),
matching the vast majority of software deployed any time in the last decade.</p>
<p>The only issue is that fast Rust builds with Docker are not simple.</p>
<details open="">
<summary><b>Update (2025-06-27)</b></summary>
<p>I first posted this <a href="https://bsky.app/profile/sharnoff.io/post/3lsir37t5ws2h">on Bluesky</a> — there was some <a href="https://bsky.app/profile/jamesmunns.com/post/3lsiwsadv222l">good discussion there</a> ❤️</p>
<p>Special thanks to Piotr Osiewicz &amp; Wesley Moore for suggestions that saved a ton more time.</p>
<p>It was also reposted over on <a href="https://www.reddit.com/r/rust/comments/1lkxad3/why_is_the_rust_compiler_so_slow/">r/rust</a>, <a href="https://www.reddit.com/r/programming/comments/1lldxe8/why_is_the_rust_compiler_so_slow/">r/programming</a> and <a href="https://news.ycombinator.com/item?id=44390488">hackernews</a>. Plenty of entertaining comments, if you&#39;re in
the mood.</p>
</details>
<a href="#basics"><h2 id="basics">Basics: Rust in Docker</h2></a>
<a href="#rust-in-docker-simple"><h3 id="rust-in-docker-simple">Rust in Docker, the simple way</h3></a>
<p>To get your Rust program in a container, the typical approach you might find would be something
like:</p>
<pre><code><span><span>FROM</span> rust:1.87-alpine3.22 <span>AS</span> builder</span>

<span><span>RUN</span> apk add musl-dev</span>

<span><span>WORKDIR</span> /workdir</span>
<span><span>COPY</span> . .</span>


<span><span>RUN</span> cargo build --package web-http-server --target=x86_64-unknown-linux-musl</span>


<span><span>FROM</span> alpine:3.20</span>
<span><span>COPY</span> <span><span>--from</span><span>=</span><span>builder</span></span> /workdir/target/x86_64-unknown-linux-musl/release/web-http-server /usr/bin/web-http-server</span>

<span><span>ENTRYPOINT</span> [<span>&#34;/usr/bin/web-http-server&#34;</span>]</span>
</code></pre>
<p><strong>Unfortunately, this will rebuild everything from scratch whenever there&#39;s any change</strong>.</p>
<p>In my case, building from scratch takes about 4 minutes (including 10s to download the crates every time).</p>
<pre><code><span><span>$</span> <span><span>cargo</span> build <span>--release</span> <span>--target</span><span>=</span>x86_64-unknown-linux-musl <span>--package</span> web-http-server</span></span>
<span>    Updating crates.io index
 Downloading crates ...
  Downloaded anstream v0.6.18
  Downloaded http-body v1.0.1

... many more lines ...

   Compiling web-http-server v0.1.0 (/workdir/web-http-server)
    Finished `release` profile [optimized + debuginfo] target(s) in 3m 51s
</span></code></pre>
<p>Sure, it could be worse. But I&#39;ve grown accustomed to speedy local builds, thanks to incremental compilation — I don&#39;t
want to wait that long on every tiny change!</p>
<a href="#rust-in-docker-cargo-chef"><h3 id="rust-in-docker-cargo-chef">Rust in Docker, with better caching</h3></a>
<p>Thankfully, there&#39;s a tool to help with this!</p>
<p>Luca Palmieri&#39;s <a href="https://github.com/LukeMathWalker/cargo-chef"><code>cargo-chef</code></a> makes it easy to pre-build all of the dependencies as a separate layer in the docker
build cache, so that changes in your codebase only trigger re-compilation of your codebase (and not your dependencies).</p>
<p>I&#39;ll save the detailed explanation for <a href="https://lpalmieri.com/posts/fast-rust-docker-builds/">Luca&#39;s blog post</a>, but broadly <code>cargo-chef</code> creates a simplified &#34;recipe&#34; file from
the current workspace, which can be &#34;cooked&#34; to cache the dependencies without being invalidated by changes in the
workspace.</p>
<p>My website pulls in a few hundred dependencies, so this <em>should</em> help!</p>
<pre><code>...

<span><span>FROM</span> ... <span>AS</span> planner</span>
<span><span>COPY</span> . .</span>
<span><span>RUN</span> cargo chef prepare --recipe-path=/workdir/recipe.json</span>

<span><span>FROM</span> ... <span>AS</span> cooker</span>


<span><span>COPY</span> <span><span>--from</span><span>=</span><span>planner</span></span> /workdir/recipe.json recipe.json</span>
<span><span>RUN</span> cargo chef cook --release --recipe-path=/workdir/recipe.json <span>\</span>
    --target=x86_64-unknown-linux-musl</span>



<span><span>FROM</span> cooker <span>AS</span> builder</span>
<span><span>COPY</span> . .</span>
<span><span>RUN</span> cargo build --release --package web-http-server <span>\</span>
    --target=x86_64-unknown-linux-musl</span>
</code></pre>
<p>Unfortunately though, it doesn&#39;t have quite the speedup we&#39;re looking for — most of the time is still in the final
binary:</p>
<pre><code><span><span>$</span> <span></span></span>
<span><span>$</span> <span><span>cargo</span> chef cook <span>--release</span> <span>..</span>.</span></span>
<span>    Updating crates.io index
 Downloading crates ...
 ...
   Compiling web-http-server v0.0.1 (/workdir/web-http-server)
    Finished `release` profile [optimized + debuginfo] target(s) in 1m 07s

</span><span><span>$</span> <span></span></span>
<span><span>$</span> <span><span>cargo</span> build <span>--release</span> <span>..</span>.</span></span>
<span>   Compiling web-http-server v0.1.0 (/workdir/web-http-server)
    Finished `release` profile [optimized + debuginfo] target(s) in 2m 50s
</span></code></pre>
<p>Weirdly, only 25% of the time is actually spent on the dependencies! As far as I could tell, my code isn&#39;t doing
anything fundamentally unreasonable. It&#39;s ~7k lines of gluing together various larger dependencies (<code>axum</code>, <code>reqwest</code>,
<code>tokio-postgres</code>, among others.)</p>
<p><em>(Just to double-check, I tried running <code>cargo build</code> with <code>--verbose</code>. It really was just a single
invocation of <code>rustc</code> that took almost 3 minutes!)</em></p>
<a href="#whats-rustc-doing"><h2 id="whats-rustc-doing">What&#39;s <code>rustc</code> doing for all that time?</h2></a>
<p>Following <a href="https://fasterthanli.me/articles/why-is-my-rust-build-so-slow">this excellent post by fasterthanlime</a>, I first tried using <code>cargo --timings</code> to get some more information:</p>
<pre><code><span><span>$</span> <span><span>cargo</span> build <span>--release</span> <span>--timings</span> <span>..</span>.</span></span>
<span>   Compiling web-http-server v0.1.0 (/workdir/web-http-server)
      Timing report saved to /workdir/target/cargo-timings/cargo-timing-20250607T192029.207407545Z.html
    Finished `release` profile [optimized + debuginfo] target(s) in 2m 54s
</span></code></pre>
<p>In addition to that <code>cargo-timing-&lt;timestamp&gt;.html</code> file, there&#39;s also a <code>cargo-timing.html</code>. We&#39;ll
just copy out the canonical version:</p>
<pre><code>...

<span><span>FROM</span> cooker <span>AS</span> builder</span>
<span><span>COPY</span> . .</span>
<span><span>RUN</span> cargo build --timings --release --target=x86_64-unknown-linux-musl --package web-http-server</span>

<span><span>RUN</span> mv target/cargo-timings/cargo-timing-*.html cargo-timing.html</span>

<span><span>FROM</span> alpine:3.22</span>
<span><span>COPY</span> <span><span>--from</span><span>=</span><span>builder</span></span> /workdir/target/x86_64-unknown-linux-musl/release/web-http-server /usr/bin/web-http-server</span>

<span><span>COPY</span> <span><span>--from</span><span>=</span><span>builder</span></span> /workdir/cargo-timing.html cargo-timing.html</span>
</code></pre>
<p>And with a little bit of container wrangling...</p>
<pre><code><span>id</span><span>=</span><span>&#34;<span><span>$(</span><span>docker</span> container create <span>&lt;</span>IMAGE<span>&gt;</span><span>)</span></span>&#34;</span>
<span>docker</span> <span>cp</span> <span>&#34;<span>$id</span>:/cargo-timing.html&#34;</span> cargo-timing.html
<span>docker</span> container <span>rm</span> <span>-f</span> <span>&#34;<span>$id</span>&#34;</span>
</code></pre>
<p>... we should be able to see what&#39;s going on! Let&#39;s have a look:</p>
<p><img src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/cargo-timings-2025-06-07T20:24:32Z.png" alt="Screenshot of the cargo build timings page, showing only web-http-server being compiled and taking 174 seconds"/></p>
<p><em>Oh.</em> There&#39;s not really much information there!</p>
<a href="#cargo-timings-weirdness"><h3 id="cargo-timings-weirdness">What&#39;s going on here?</h3></a>
<p><code>cargo build --timings</code> shows a bunch of information about <em>how long each crate took to compile</em>. But here, we only care
about the compilation time of the final crate!</p>
<p>That aside, this does help give us more accurate timing. Measuring outside the compiler adds some extra moving
pieces, or requires searching the output of <code>cargo build</code> — so using <code>cargo</code>&#39;s self-reported timings will make more
precise analysis a bit easier, later on.</p>
<p>Just to check, the value here of 174.1s roughly matches the &#34;2m 54s&#34; we saw from the <code>cargo build</code> output.</p>
<a href="#rustc-self-profile"><h2 id="rustc-self-profile">Actually asking <code>rustc</code> this time</h2></a>
<p>The post from fasterthanlime had one more tip we can use — <code>rustc</code>&#39;s self-profiling feature, via the <code>-Zself-profile</code>
flag.</p>
<p>Normally, you&#39;d probably run something like:</p>
<pre><code><span>RUSTC_BOOTSTRAP</span><span>=</span><span>1</span> <span>cargo</span> rustc <span>--release</span> -- <span>-Z</span> self-profile
</code></pre>
<p><em>(note: This is using <code>cargo rustc</code> to pass extra flags to <code>rustc</code>, with <code>RUSTC_BOOTSTRAP=1</code> to allow using the <code>-Z</code>
unstable flags on a stable compiler.)</em></p>
<p>Unfortunately, this won&#39;t work here — the change in arguments will invalidate the cached dependencies from
<code>cargo chef cook</code>, and there&#39;s no equivalent way to pass additional <code>rustc</code> flags through <code>cargo-chef</code>.</p>
<p>Instead, we can funnel everything via the <code>RUSTFLAGS</code> environment variable:</p>
<pre><code>
<span>RUSTC_BOOTSTRAP</span><span>=</span><span>1</span> <span>RUSTFLAGS</span><span>=</span><span>&#39;-Zself-profile&#39;</span> <span>cargo</span> chef cook <span>--release</span> <span>..</span>.


<span>RUSTC_BOOTSTRAP</span><span>=</span><span>1</span> <span>RUSTFLAGS</span><span>=</span><span>&#39;-Zself-profile&#39;</span> <span>cargo</span> build <span>--release</span> <span>..</span>.
</code></pre>
<p>This gives us files like <code>web_http_server-&lt;random-number&gt;.mm_profdata</code>, which we can move and extract from the image in
the same way as we did for <code>cargo-timing.html</code>.</p>
<p><em>(note: It&#39;s much easier to automate if we remove the profiling data that was added from <code>cargo chef cook</code> before the
final build. That&#39;s omitted here, for brevity.)</em></p>
<a href="#using-rustc-profdata"><h3 id="using-rustc-profdata">Actually using the profdata</h3></a>
<p>The Rust folks maintain a suite of tools for exploring <code>rustc</code>&#39;s self-profiling output, over in
<a href="https://github.com/rust-lang/measureme">https://github.com/rust-lang/measureme</a>.</p>
<p>Some key ones:</p>
<ul>
<li><code>summary</code> – produces plaintext output summarizing the profiling data</li>
<li><code>flamegraph</code> – produces a <a href="https://www.brendangregg.com/flamegraphs.html">flamegraph</a> SVG</li>
<li><code>crox</code> – produces a <a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0#heading=h.yr4qxyxotyw">chrome tracing format</a> trace, compatible with <code>chrome://tracing</code> (in Chromium-based browsers)</li></ul>
<p>But let&#39;s install a couple of these to take a look at what we&#39;ve got:</p>
<pre><code><span>cargo</span> <span>install</span> <span>--git</span> https://github.com/rust-lang/measureme flamegraph summarize
</code></pre>
<p>I personally use Firefox, so we&#39;ll hold off on the chrome tracing stuff for now.</p>
<p>First, with <code>summarize</code> (which itself has the <code>summarize</code> and <code>diff</code> subcommands):</p>
<pre><code><span><span>$</span> <span>summarize summarize web_http_server.mm_profdata <span>|</span> <span>wc</span> <span>-l</span></span></span>
<span>945
</span><span><span>$</span> <span>summarize summarize web_http_server.mm_profdata <span>|</span> <span>head</span></span></span>
<span>+-------------------------------+-----------+-----------------+----------+------------+
| Item                          | Self time | % of total time | Time     | Item count |
+-------------------------------+-----------+-----------------+----------+------------+
| LLVM_lto_optimize             | 851.95s   | 33.389          | 851.95s  | 1137       |
+-------------------------------+-----------+-----------------+----------+------------+
| LLVM_module_codegen_emit_obj  | 674.94s   | 26.452          | 674.94s  | 1137       |
+-------------------------------+-----------+-----------------+----------+------------+
| LLVM_thin_lto_import          | 317.75s   | 12.453          | 317.75s  | 1137       |
+-------------------------------+-----------+-----------------+----------+------------+
| LLVM_module_optimize          | 189.00s   | 7.407           | 189.00s  | 17         |

thread &#39;main&#39; panicked at library/std/src/io/stdio.rs:1165:9:
failed printing to stdout: Broken pipe (os error 32)
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
</span></code></pre>
<p><em>(Oops! Classic CLI edge-case. <a href="https://github.com/rust-lang/measureme/pull/243">Easy enough to fix, though</a> 😊)</em></p>
<p>So at a high level, the two biggest things are <a href="https://www.llvm.org/docs/LinkTimeOptimization.html">link-time optimization</a> (LTO) and
<code>LLVM_module_codegen_emit_obj</code>, whatever that is.</p>
<p>Let&#39;s see if we can dig a bit deeper with the flamegraph:</p>
<pre><code><span><span>$</span> <span>flamegraph web_http_server.mm_profdata</span></span>
<span><span>$</span> <span></span></span>
<span><span>$</span> <span><span>find</span> <span>.</span> <span>-cmin</span> <span>1</span> <span>-type</span> f </span></span>
<span>./rustc.svg
</span></code></pre>
<p>Cool, we get an SVG!</p>
<p><a href="https://sharnoff.io/blog/why-rust-compiler-slow/assets/rustc-2025-06-07T21:24:16Z.svg?s=LLVM_lto_optimize"><img src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/rustc-2025-06-07T21:24:16Z.png" alt="Flamegraph of rustc execution, with codegen_module_perform_lto highlighted as taking 78.9% of the total time"/></a></p>
<p><em>(It&#39;s interactive! If you&#39;re curious, you can click through and play around with it yourself.)</em></p>
<p>So there&#39;s presumably some inter-mingling going on between codegen and LTO: <code>codegen_module_perform_lto</code> ends up falling
through to both <code>LLVM_lto_optimize</code>/<code>LLVM_thin_lto_import</code> and <code>LLVM_module_codegen</code>.</p>
<p>But either way, we&#39;ve got a problem with LTO: <code>codegen_module_perform_lto</code> took ~80% of the total time.</p>
<a href="#about-lto"><h2 id="about-lto">It&#39;s time to talk about LTO</h2></a>
<p>The Rust compiler splits up crates into &#34;<a href="https://doc.rust-lang.org/rustc/codegen-options/index.html#codegen-units">codegen units</a>&#34;, handing each to LLVM as a separate module to compile.
<em>In general</em>, optimizations take place within each codegen unit, and then they&#39;re linked together at the end.</p>
<p>LTO controls the set of optimizations that LLVM will make during that link-time — for example, inlining or
optimization across codegen units.</p>
<p>Cargo (via <code>rustc</code>) exposes a few <a href="https://doc.rust-lang.org/rustc/codegen-options/index.html#lto">options for LTO</a>:</p>
<ul>
<li>Off — all LTO disabled</li>
<li>&#34;thin&#34; LTO — in theory, similar performance benefits to &#34;fat&#34; LTO, but less expensive to run</li>
<li>&#34;fat&#34; LTO — maximum amount of LTO, across all crates at the same time</li></ul>
<p>And if the LTO option is not specified, <code>rustc</code> uses &#34;thin local LTO&#34;, which limits &#34;thin&#34; LTO only to a single crate at
a time.</p>
<a href="#lto-current-settings"><h3 id="lto-current-settings">What are the current settings</h3></a>
<p>Turns out that a few years back, I had set <code>lto = &#34;thin&#34;</code> in my <code>Cargo.toml</code>:</p>
<pre><code><span>[</span><span>profile.release</span><span>]</span>
<span>lto</span> <span>=</span> <span>&#34;thin&#34;</span>
<span>debug</span> <span>=</span> <span>&#34;full&#34;</span>
</code></pre>
<p>And, while we&#39;re at it, <a href="https://doc.rust-lang.org/cargo/reference/profiles.html#debug"><code>debug = &#34;full&#34;</code></a> enables all debug symbols (where they&#39;d normally be excluded by default for the
<code>release</code> profile). Maybe we should take a look at that as well.</p>
<a href="#lto-tweaking-settings"><h3 id="lto-tweaking-settings">Tweaking the (normal) settings</h3></a>
<p>Let&#39;s take a look at the compile times and binary sizes for a variety of <code>lto</code> and <code>debug</code> settings (using
<code>cargo build --timings</code> like before, for more precise timing).</p>
<div><table>
<thead><tr><th>Time / Size</th><th><code>debug=none</code></th><th><code>debug=line-tables-only</code></th><th><code>debug=limited</code></th><th><code>debug=full</code></th></tr></thead><tbody>
<tr><td>LTO disabled</td><td>50.0s / 21.0Mi</td><td>54.4s / 85.9Mi</td><td>54.8s / 105.9Mi</td><td>67.6s / 214.3Mi</td></tr>
<tr><td>Thin local LTO</td><td>67.5s / 20.1Mi</td><td>71.5s / 95.4Mi</td><td>73.6s / 117.0Mi</td><td>88.2s / 256.8Mi</td></tr>
<tr><td>&#34;Thin&#34; LTO</td><td>133.7s / 20.3Mi</td><td>141.7s / 80.6Mi</td><td>140.7s / 96.0Mi</td><td>172.2s / 197.5Mi</td></tr>
<tr><td>&#34;Fat&#34; LTO</td><td>189.1s / 15.9Mi</td><td>211.1s / 64.4Mi</td><td>212.5s / 75.8Mi</td><td>287.1s / 155.9Mi</td></tr></tbody><tbody></tbody></table></div>
<p>At a high level: It seems like the worst cases here are full debug symbols adding 30-50% to the compilation time, and
&#34;fat&#34; LTO taking about <strong>4 times longer</strong> than with LTO fully disabled.</p>
<p>That mostly tracks with what we&#39;d expect from the documentation — yeah, fat LTO takes longer. But when we disable
everything, we&#39;re still looking at 50 seconds compiling the final binary!</p>
<a href="#brief-note"><h2 id="brief-note">A brief note: 50 seconds is <em>fine</em>, actually!</h2></a>
<p>Look, 50 seconds is already a great improvement — and if it requires disabling LTO and debug symbols... my website
gets approximately zero load. <em>It would be totally fine.</em> It would be perfectly sustainable, even!</p>
<p><strong>There&#39;s no practical reason to keep digging here.</strong></p>
<p>But where&#39;s the fun in leaving it there? We should be able to do better, right?</p>
<a href="#brief-note-2"><h2 id="brief-note-2">Another brief note: Can&#39;t we just use incremental compilation?</h2></a>
<p>It&#39;s slightly more complicated, but yes, absolutely — for local development, at least. Consistently loading the build
cache isn&#39;t straightforward, but you&#39;d want to make the <code>/target</code> directory accessible with a <a href="https://docs.docker.com/build/cache/optimize/#use-cache-mounts">&#34;cache mount&#34;</a> in the
dockerfile, and persist that target directory between builds.</p>
<p>That said, I value that <code>docker build</code> <em>can</em> have a clean environment every time, and I think it&#39;s worthwhile to go
through docker&#39;s own caching system — which is why I&#39;m using <code>cargo-chef</code> in the first place.</p>
<a href="#et-tu-llvm_module_optimize"><h2 id="et-tu-llvm_module_optimize">Digging deeper: Et tu, <code>LLVM_module_optimize</code>?</h2></a>
<p>If we disable LTO and debug symbols, compiling the final binary still takes 50 seconds to do... something.</p>
<p>Let&#39;s re-run the self-profiling to check out what&#39;s going on.</p>
<p><a href="https://sharnoff.io/blog/why-rust-compiler-slow/assets/rustc-2025-06-09T21:56:36Z.svg?s=LLVM_module_optimize"><img src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/rustc-2025-06-09T21:56:36Z.png" alt="Flamegraph of rustc, with LLVM_module_optimize highlighted as taking 68.7% of total time"/></a></p>
<p>It&#39;s ~70% just <code>LLVM_module_optimize</code> — i.e. where LLVM is optimizing the code. Before diving into LLVM itself, let&#39;s
see if there&#39;s any easier knobs we can tune.</p>
<a href="#tuning-optimization"><h3 id="tuning-optimization">Tuning optimization</h3></a>
<p>The <code>release</code> profile uses <code>opt-level = 3</code> by default — maybe if we reduce the optimization level, we&#39;ll spend less
time on it.</p>
<p>We can actually do one better — since our dependencies are cached, and we only care about the final binary, we can get
most of the benefits by only reducing optimizations on the final binary:</p>
<pre><code><span>[</span><span>profile.release</span><span>]</span>
<span>lto</span> <span>=</span> <span>&#34;off&#34;</span>
<span>debug</span> <span>=</span> <span>&#34;none&#34;</span>
<span>opt-level</span> <span>=</span> <span>0</span> 




<span>[</span><span>profile.release.package.&#34;*&#34;</span><span>]</span>
<span>opt-level</span> <span>=</span> <span>3</span>
</code></pre>
<p>Like the previous options, there&#39;s a handful of <a href="https://doc.rust-lang.org/cargo/reference/profiles.html#opt-level"><code>opt-level</code>s</a> we can choose from:</p>
<ul>
<li><code>0</code> disables optimizations</li>
<li><code>1</code>, <code>2</code>, and <code>3</code> enable increasing levels of optimizations</li>
<li><code>&#34;s&#34;</code> and <code>&#34;z&#34;</code> are different flavors of prioritizing binary size</li></ul>
<p>Going through a handful of combinations here again:</p>
<div><table>
<thead><tr><th>Final / Deps</th><th>deps: <code>opt-level=3</code></th><th>deps: <code>opt-level=&#34;s&#34;</code></th><th>deps: <code>opt-level=&#34;z&#34;</code></th></tr></thead><tbody>
<tr><td>final: <code>opt-level=0</code></td><td>14.7s / 26.0Mi</td><td>15.0s / 25.9Mi</td><td>15.7s / 26.3Mi</td></tr>
<tr><td>final: <code>opt-level=1</code></td><td>48.8s / 21.5Mi</td><td>47.6s / 20.1Mi</td><td>47.8s / 20.6Mi</td></tr>
<tr><td>final: <code>opt-level=2</code></td><td>50.8s / 20.9Mi</td><td>55.2s / 20.2Mi</td><td>55.4s / 20.7Mi</td></tr>
<tr><td>final: <code>opt-level=3</code></td><td>51.0s / 21.0Mi</td><td>55.4s / 20.3Mi</td><td>55.2s / 20.8Mi</td></tr>
<tr><td>final: <code>opt-level=&#34;s&#34;</code></td><td>46.0s / 20.1Mi</td><td>45.7s / 18.9Mi</td><td>46.0s / 19.3Mi</td></tr>
<tr><td>final: <code>opt-level=&#34;z&#34;</code></td><td>42.7s / 20.1Mi</td><td>41.8s / 18.8Mi</td><td>41.8s / 19.3Mi</td></tr></tbody><tbody></tbody></table></div>
<p>Basically:</p>
<ul>
<li>The baseline for <em>any</em> level of optimizations on the final binary is about 50 seconds</li>
<li>If we disable all optimizations, then it&#39;s pretty quick: only ~15s</li></ul>
<a href="#profiling-llvm"><h2 id="profiling-llvm">What&#39;s LLVM&#39;s deal?</h2></a>
<p>Rust relies pretty heavily on optimizations, and while it&#39;d probably be fine to just blanket-disable them for the final
binary, it&#39;d be pretty cool if we can at least keep <em>some</em> optimizations!</p>
<p><strong>So let&#39;s try to figure out what&#39;s taking so long.</strong> <code>rustc</code>&#39;s self-profiling doesn&#39;t give us any more detail though,
so we&#39;ll have to get it from LLVM.</p>
<p>There&#39;s another <a href="https://github.com/rust-lang/rust/blob/d13a431a6cc69cd65efe7c3eb7808251d6fd7a46/src/doc/rustc-dev-guide/src/backend/debugging.md?plain=1#L209-L212">couple useful <code>rustc</code> flags here</a>:</p>
<ul>
<li><code>-Z time-llvm-passes</code> – emit LLVM profiling information as plaintext</li>
<li><code>-Z llvm-time-trace</code> – emit LLVM profiling information in the chrome tracing format (again with that one!)</li></ul>
<a href="#profiling-llvm-text"><h3 id="profiling-llvm-text">Profiling LLVM with <code>rustc</code> — plain text</h3></a>
<p>Like before, let&#39;s skip the chrome tracing format for now, and see what we can get from plain text.</p>
<pre><code>
<span>RUSTC_BOOTSTRAP</span><span>=</span><span>1</span> <span>RUSTFLAGS</span><span>=</span><span>&#39;-Ztime-llvm-passes&#39;</span> <span>cargo</span> chef cook <span>--release</span> <span>..</span>.


<span>RUSTC_BOOTSTRAP</span><span>=</span><span>1</span> <span>RUSTFLAGS</span><span>=</span><span>&#39;-Ztime-llvm-passes&#39;</span> <span>cargo</span> build <span>--release</span> <span>..</span>.
</code></pre>
<p>... Unfortunately if you try to <code>docker build</code> again, you&#39;ll immediately hit something like:</p>
<pre><code>[output clipped, log limit 2MiB reached]
</code></pre>
<p>This is because <a href="https://docs.docker.com/build/buildkit/">BuildKit</a> (if you&#39;re using ~recent Docker on Linux) has default output limits that are pretty small.</p>
<details>
<summary>We can just raise the limits, right?</summary>
<p>These limits are configured by the environment variables <code>BUILDKIT_STEP_LOG_MAX_SIZE</code> and <code>BUILDKTI_STEP_LOG_MAX_SPEED</code>.
But if we pass them to <code>docker build</code> with something like this:</p>
<pre><code><span>BUILDKIT_STEP_LOG_MAX_SIZE</span><span>=</span>-1 <span>BUILDKTI_STEP_LOG_MAX_SPEED</span><span>=</span>-1 <span>docker</span> build <span>..</span>.
</code></pre>
<p>... it won&#39;t work, because the configuration must be set on the docker <em>daemon</em>.</p>
<p>On most Linux distros, <code>dockerd</code> is run as a <code>systemd</code> unit.</p>
<details>
<summary>So just set it on the <code>systemd</code> unit?</summary>
<p>The Proper™ way to do this is by creating an override file – something like:</p>
<pre><code><span><span>$</span> <span>systemctl edit --drop-in<span>=</span>buildkit-env.conf docker.service</span></span>
</code></pre>
<p><em>(note: passing <code>--drop-in</code> allows naming the file something more descriptive than <code>override.conf</code>)</em></p>
<p>It opens a new file, where we can set the environment overrides:</p>
<pre><code><span><span>[</span><span>Service</span><span>]</span></span>
<span>Environment</span><span>=</span><span><span>&#34;BUILDKIT_STEP_LOG_MAX_SIZE=-1&#34;</span></span>
<span>Environment</span><span>=</span><span><span>&#34;BUILDKIT_STEP_LOG_MAX_SPEED=-1&#34;</span></span>
</code></pre>
<p>And once we&#39;re done:</p>
<pre><code><span><span>$</span> <span>systemctl restart docker.service</span></span>
</code></pre><details>
<summary>Checking our work...</summary>
<p>After restarting, we can double-check the environment with something like:</p>
<pre><code><span><span>$</span> <span>pgrep dockerd</span></span>
<span>1234567
</span><span><span>$</span> <span><span>cat</span> /proc/1234567/environ <span>|</span> <span>tr</span> <span>&#39;\0&#39;</span> <span>&#39;\n&#39;</span> <span>|</span> <span>grep</span> <span>-i</span> <span>&#39;buildkit&#39;</span></span></span>
<span>BUILDKIT_STEP_LOG_MAX_SIZE=-1
BUILDKIT_STEP_LOG_MAX_SPEED=-1
</span></code></pre>
<p><em>(note: <code>tr</code> is needed because the environment is a nul-separated string, and it&#39;s easier to search line-by-line)</em></p>
</details>
</details>
</details>
<p>So after getting unlimited <code>docker build</code> output on the terminal, what&#39;s in it?
<strong>~200k lines of plaintext</strong> — probably not what you want to be copying from your terminal, anyways.</p>
<p>So, redirecting to a file inside docker and copying that out like before, we get a bunch of pass/analysis timing
reports. They each look something like this:</p>
<pre><code>===-------------------------------------------------------------------------===
                          Pass execution timing report
===-------------------------------------------------------------------------===
  Total Execution Time: 0.0428 seconds (0.0433 wall clock)

   ---User Time---   --System Time--   --User+System--   ---Wall Time---  — Name ---
   0.0072 ( 19.2%)   0.0015 ( 27.4%)   0.0086 ( 20.2%)   0.0087 ( 20.0%)  InstCombinePass
   0.0040 ( 10.8%)   0.0006 ( 10.8%)   0.0046 ( 10.8%)   0.0047 ( 10.8%)  InlinerPass
   0.0024 (  6.4%)   0.0010 ( 18.0%)   0.0034 (  7.9%)   0.0034 (  7.8%)  SimplifyCFGPass
   0.0022 (  5.9%)   0.0002 (  4.5%)   0.0025 (  5.7%)   0.0024 (  5.6%)  EarlyCSEPass
   0.0021 (  5.5%)   0.0001 (  1.5%)   0.0021 (  5.0%)   0.0022 (  5.0%)  GVNPass
   0.0015 (  4.0%)   0.0001 (  2.2%)   0.0016 (  3.8%)   0.0018 (  4.2%)  ArgumentPromotionPass

   ... entries here continue, and more passes below, for hundreds of thousands of lines ...
</code></pre>
<p>It certainly is <em>possible</em> to parse and analyze these! But it&#39;s also hard to be certain about what you&#39;re looking at
when each pass execution is emitted separately and multi-threading can interfere with timing.</p>
<p>Let&#39;s see if there&#39;s a better way to get good data.</p>
<a href="#profiling-llvm-trace"><h3 id="profiling-llvm-trace">Profiling LLVM with <code>rustc</code> — actual tracing this time</h3></a>
<p>We skipped <code>-Z llvm-time-trace</code> earlier because it emits the chrome tracing format.</p>
<p>Let&#39;s revisit that:</p>
<pre><code>
<span>RUSTC_BOOTSTRAP</span><span>=</span><span>1</span> <span>RUSTFLAGS</span><span>=</span><span>&#39;-Zllvm-time-trace&#39;</span> <span>cargo</span> chef cook <span>--release</span> <span>..</span>.


<span>RUSTC_BOOTSTRAP</span><span>=</span><span>1</span> <span>RUSTFLAGS</span><span>=</span><span>&#39;-Zllvm-time-trace&#39;</span> <span>cargo</span> build <span>--release</span> <span>..</span>.
</code></pre>
<p>It produces a bunch of <code>$package-$hash.llvm_timings.json</code> files, alongside the normal compilation artifacts:</p>
<pre><code><span><span>$</span> <span><span>ls</span> <span>-lAh</span> target/x86_64-unknown-linux-musl/release/deps <span>|</span> <span>head</span></span></span>
<span>total 5G
-rw-r--r--    1 root     root       11.8K Jun  9 23:11 aho_corasick-ff268aeac1b7a243.d
-rw-r--r--    1 root     root       69.4M Jun  9 23:11 aho_corasick-ff268aeac1b7a243.llvm_timings.json
-rw-r--r--    1 root     root        6.6K Jun  9 23:11 allocator_api2-28ed2e0fa8ab7b44.d
-rw-r--r--    1 root     root      373.1K Jun  9 23:11 allocator_api2-28ed2e0fa8ab7b44.llvm_timings.json
-rw-r--r--    1 root     root        4.0K Jun  9 23:11 anstream-cf9519a72988d4c1.d
-rw-r--r--    1 root     root        4.4M Jun  9 23:11 anstream-cf9519a72988d4c1.llvm_timings.json
-rw-r--r--    1 root     root        2.4K Jun  9 23:11 anstyle-76a77f68346b4238.d
-rw-r--r--    1 root     root      885.3K Jun  9 23:11 anstyle-76a77f68346b4238.llvm_timings.json
-rw-r--r--    1 root     root        2.2K Jun  9 23:11 anstyle_parse-702e2f8f76fe1827.d
</span></code></pre>
<p><em>(Why <code>root</code>? Setting up rootless docker didn&#39;t work when I tried it a few years back, and I haven&#39;t bothered since)</em></p>
<p>So, deleting <code>*.llvm_timings.json</code> between <code>cargo-chef</code> and the final build, we can extract the singular profile for the
final binary into <code>web_http_server.llvm_timings.json</code>.</p>
<p>There&#39;s just one minor hiccup:</p>
<pre><code><span><span>$</span> <span><span>du</span> <span>-sh</span> web_http_server.llvm_timings.json</span></span>
<span>1.4G	web_http_server.llvm_timings.json
</span></code></pre>
<p>It&#39;s <em>enormous</em>. It&#39;s also all one single line!</p>
<p>In theory though, a wide variety of tools should be able to process this:</p>
<details>
<summary>Firefox profiling</summary>
<p>I&#39;m using Firefox, so why not <a href="https://profiler.firefox.com/">Firefox Profiler</a>? It should be able to handle it:</p>
<blockquote>
<p>The Firefox Profiler can also import profiles from other profilers, such as
<a href="https://profiler.firefox.com/docs/#/./guide-perf-profiling">Linux perf</a>,
<a href="https://profiler.firefox.com/docs/#/./guide-android-profiling">Android SimplePerf</a>,
the Chrome performance panel,
<a href="https://developer.android.com/studio/profile/cpu-profiler">Android Studio</a>,
or any file using the
<a href="https://valgrind.org/docs/manual/dh-manual.html">dhat format</a>
or
<a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview">Google’s Trace Event Format</a>.</p>
</blockquote>
<p>Unfortunately, this didn&#39;t work:</p>
<p><img src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/firefox-profiler-err-2025-06-14T13:29:33Z.png" alt="Screenshot of error message from Firefox Profiler, saying the profile couldn&#39;t be parsed as a UTF-8 string"/></p>
<p>Looking at the web console, we can see why it failed – it ran out of memory:</p>
<p><img src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/firefox-profiler-err-console-2025-06-14T13:34:11Z.png" alt="Screenshot of console error messages. The first reads reads NS_ERROR_OUT_OF_MEMORY. The two others say the array buffer couldn&#39;t be parsed as UTF-8."/></p>
</details>
<details>
<summary>perfetto.dev on Firefox</summary>
<p>When I searched for displaying these chrome tracing format traces, <a href="https://ui.perfetto.dev/">perfetto.dev</a> was another alternative that came up.
It&#39;s <a href="https://github.com/google/perfetto">also maintained by Google</a>.</p>
<p>When I first tried it, I was using a larger trace from a longer compilation, and it ran out of memory as well:</p>
<p><img src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/perfetto-ui-oom-2025-05-04T18:38:45Z.png" alt="Screenshot of an error pop-up, saying the WASM processor ran out of memory and advising to run perfetto locally. It also says the typca"/></p>
<p>I resorted to running the WASM processor locally, hitting <a href="https://github.com/google/perfetto/issues/1301">this bug affecting Firefox</a>.</p>
<p>At the time I gave up and used Chromium instead, but in the process of writing this post, I tried it again. The smaller trace allowed it to work:</p>
<p><img src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/perfetto-ui-2025-06-14T13:54:17Z.png" alt="Screenshot of the perfetto UI, showing a few rows of vibrantly colored flamegraphs"/></p>
<p>Either way though, I found I had <em>absolutely no clue</em> how to use this interface – and loading a complex trace from LLVM
probably also wasn&#39;t the best introduction point.</p>
</details>
<details>
<summary><code>chrome://tracing</code> on Chromium</summary>
<p>You&#39;d be forgiven for expecting this one to work the best out of all the options, but unfortunately it also failed –
albeit more amusingly than the others:</p>
<img alt="Screenshot of an error pop-up, saying &#34;Error while loading file: [object ProgressEvent]&#34;" src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/chrome-tracing-err-2025-05-04T17:43:44Z.png"/>
</details>
<p>None of these options worked for me — but it&#39;s a big JSON file with a <a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0">known format</a>, how hard can it be?</p>
<p>Turns out, a 1.4GiB single line of JSON makes all the normal tools complain:</p>
<ul>
<li>If you try to view it with <code>less</code>, scrolling blocks on processing the entire file</li>
<li>If you try to process it with <code>jq</code>, it has to load the entire 1.4GiB into <code>jq</code>&#39;s internal format (which expectedly
takes up <em>much</em> more than the original 1.4GiB)</li>
<li>Vim hangs when you open it</li>
<li>And you probably don&#39;t want to just <code>cat</code> it to the terminal — again, it&#39;s 1.4GiB!</li></ul>
<p>So instead, we can just look at a few hundred characters, at the start and end of the file:</p>
<pre><code><span><span>$</span> <span><span>head</span> <span>-c300</span> web_http_server.llvm_timings.json</span></span>
<span>{&#34;traceEvents&#34;:[{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8291351,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:6827,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Expand large div/rem&#34;}},{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298181,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:2,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Expand large fp convert&#34;}},{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298183,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:8,&#34;name&#34;:&#34;RunPa
</span><span><span>$</span> <span><span>tail</span> <span>-c300</span> web_http_server.llvm_timings.json</span></span>
<span>me&#34;:&#34;&#34;}},{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:43,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}},{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:44,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}},{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:29,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}],&#34;beginningOfTime&#34;:1749510885820760}
</span></code></pre>
<p>Matching this to the <a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0#heading=h.q8di1j2nawlp">&#34;JSON Object Format&#34;</a> from the chrome tracing spec, it seems we have a single JSON object like:</p>
<pre><code><span>{</span>
  <span>&#34;traceEvents&#34;</span><span>:</span> <span>[</span>
    <span>{</span><span>&#34;pid&#34;</span><span>:</span><span>25</span><span>,</span><span>&#34;tid&#34;</span><span>:</span><span>30</span><span>,</span><span>&#34;ts&#34;</span><span>:</span><span>8291351</span><span>,</span><span>&#34;ph&#34;</span><span>:</span><span>&#34;X&#34;</span><span>,</span><span>&#34;dur&#34;</span><span>:</span><span>6827</span><span>,</span><span>&#34;name&#34;</span><span>:</span><span>&#34;RunPass&#34;</span><span>,</span><span>&#34;args&#34;</span><span>:</span><span>{</span><span>&#34;detail&#34;</span><span>:</span><span>&#34;Expand large div/rem&#34;</span><span>}</span><span>}</span><span>,</span>
    <span>{</span><span>&#34;pid&#34;</span><span>:</span><span>25</span><span>,</span><span>&#34;tid&#34;</span><span>:</span><span>30</span><span>,</span><span>&#34;ts&#34;</span><span>:</span><span>8298181</span><span>,</span><span>&#34;ph&#34;</span><span>:</span><span>&#34;X&#34;</span><span>,</span><span>&#34;dur&#34;</span><span>:</span><span>2</span><span>,</span><span>&#34;name&#34;</span><span>:</span><span>&#34;RunPass&#34;</span><span>,</span><span>&#34;args&#34;</span><span>:</span><span>{</span><span>&#34;detail&#34;</span><span>:</span><span>&#34;Expand large fp convert&#34;</span><span>}</span><span>}</span><span>,</span>
    ...
  <span>]</span><span>,</span>
  <span>&#34;beginningOfTime&#34;</span><span>:</span> <span>1749510885820760</span>
<span>}</span>
</code></pre>
<p>We&#39;d be able to process it with normal tools if we split each event into its own object. That could be something like:</p>
<pre><code><span>cat</span> web_http_server.llvm_timings.json <span>\</span>
    <span>|</span> <span>sed</span> <span>-E</span> <span>&#39;s/},/}\n/g;s/^\{&#34;traceEvents&#34;:\[//g;s/\],&#34;beginningOfTime&#34;:[0-9]+}$//g&#39;</span> <span>\</span>
    <span>&gt;</span> web-http-server.llvm_timings.jsonl
</code></pre>
<p><em>(i.e.: turn <code>},</code> into a newline, strip the start of the object, strip the end of the object)</em></p>
<p>And <em>now</em> we can process this.</p>
<pre><code><span><span>$</span> <span><span>wc</span> <span>-l</span> web_http_server.llvm_timings.jsonl</span></span>
<span>7301865 web_http_server.llvm_timings.jsonl

</span><span><span>$</span> <span><span>head</span> web_http_server.llvm_timings.jsonl</span></span>
<span>{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8291351,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:6827,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Expand large div/rem&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298181,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:2,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Expand large fp convert&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298183,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:8,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Expand Atomic instructions&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298192,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:0,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Lower AMX intrinsics&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298193,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:0,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Lower AMX type for load/store&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298195,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:1,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Lower Garbage Collection Instructions&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298196,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:1,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Shadow Stack GC Lowering&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8298197,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:1164,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Remove unreachable blocks from the CFG&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8299362,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:1,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Instrument function entry/exit with calls to e.g. mcount() (post inlining)&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8299363,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:5,&#34;name&#34;:&#34;RunPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;Scalarize Masked Memory Intrinsics&#34;}}
</span></code></pre>
<a href="#whats-in-llvm-trace"><h2 id="whats-in-llvm-trace">What&#39;s in LLVM&#39;s trace events?</h2></a>
<p>It looks like these events all have <code>&#34;ph&#34;:&#34;X&#34;</code>.</p>
<p>According to the spec, the <code>ph</code> field gives the type of event, and <code>X</code> refers to &#34;complete&#34; events, recording how long a
particular piece of work took on a given thread (<code>tid</code>). The duration in microseconds is given by <code>dur</code>.</p>
<p>Aside from that, we also have <code>M</code> events:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>|</span> jq <span>-c</span> <span>&#39;select(.ph != &#34;X&#34;)&#39;</span> <span>|</span> <span>head</span></span></span>
<span>{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:27,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;process_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;rustc&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:27,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:32,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:33,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:34,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:39,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:40,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
{&#34;cat&#34;:&#34;&#34;,&#34;pid&#34;:25,&#34;tid&#34;:36,&#34;ts&#34;:0,&#34;ph&#34;:&#34;M&#34;,&#34;name&#34;:&#34;thread_name&#34;,&#34;args&#34;:{&#34;name&#34;:&#34;&#34;}}
</span></code></pre>
<p>These are &#34;metadata&#34; events — in our case, not much useful information.</p>
<p>And aside from these, there&#39;s nothing else:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>|</span> jq <span>-c</span> <span>&#39;select(.ph != &#34;X&#34; and .ph != &#34;M&#34;)&#39;</span></span></span>
<span>&lt;nothing&gt;
</span></code></pre>
<p>Going back to those <code>X</code> events — there were a bunch of them with <code>&#34;name&#34;:&#34;RunPass&#34;</code>. What else do we have?</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>|</span> jq <span>-c</span> <span>&#39;select(.ph == &#34;X&#34; and .name != &#34;RunPass&#34;)&#39;</span> <span>|</span> <span>head</span></span></span>
<span>{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8291349,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:32009,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc12___rust_alloc&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8323394,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:283,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc14___rust_dealloc&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8323678,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:216,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc14___rust_realloc&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8323895,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:179,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc19___rust_alloc_zeroed&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8324075,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:155,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc26___rust_alloc_error_handler&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8288691,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:35693,&#34;name&#34;:&#34;OptModule&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;5z12fn0vr5uv0i2pfsngwe5em&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:9730144,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:16,&#34;name&#34;:&#34;Annotation2MetadataPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;[module]&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:9730214,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:10,&#34;name&#34;:&#34;ForceFunctionAttrsPass&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;[module]&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:9730346,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:11,&#34;name&#34;:&#34;InnerAnalysisManagerProxy&lt;llvm::AnalysisManager&lt;llvm::Function&gt;, llvm::Module&gt;&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;[module]&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:9730416,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:17,&#34;name&#34;:&#34;TargetLibraryAnalysis&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;llvm.expect.i1&#34;}}
</span></code></pre>
<p>Neat! It looks like we might be able to demangle some of the symbols to get timings on individual functions.</p>
<p>If we track what&#39;s being run and how long it takes, we should be able to get a better sense of why our compile time is
so long.</p>
<p>Later on, there&#39;s aggregate information for certain types of events, like <code>Total OptFunction</code>. These are equivalent to
the sum of the duration for that event type (in this case, <code>OptFunction</code>). Let&#39;s see kind of operations are taking the
most time:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>|</span> jq <span>-r</span> <span>&#39;select(.name | startswith(&#34;Total &#34;)) | &#34;\(.dur / 1e6) \(.name)&#34;&#39;</span> <span>|</span> <span>sort</span> <span>-rn</span> <span>|</span> <span>head</span></span></span>
<span>665.369662 Total ModuleInlinerWrapperPass
656.465446 Total ModuleToPostOrderCGSCCPassAdaptor
632.441396 Total DevirtSCCRepeatedPass
627.236893 Total PassManager&lt;llvm::LazyCallGraph::SCC, llvm::AnalysisManager&lt;llvm::LazyCallGraph::SCC, llvm::LazyCallGraph&amp;&gt;, llvm::LazyCallGraph&amp;, llvm::CGSCCUpdateResult&amp;&gt;
536.738589 Total PassManager&lt;llvm::Function&gt;
372.768547 Total CGSCCToFunctionPassAdaptor
193.914869 Total ModuleToFunctionPassAdaptor
190.924012 Total OptModule
189.621119 Total OptFunction
182.250077 Total InlinerPass
</span></code></pre>
<p>This particular run took ~110 seconds on a 16-core machine, so it&#39;s clear that some passes are being double-counted
(which makes sense — we see both <code>ModuleInlinerWrapperPass</code> <em>and</em> <code>InlinerPass</code>, and it looks like <code>OptModule</code>
probably just calls <code>OptFunction</code>).</p>
<p>But broadly, it seems like optimization (<code>OptFunction</code>) and inlining (<code>InlinerPass</code>) are the two parts taking a lot of
time — let&#39;s see if we can do anything about it.</p>
<a href="#making-inlinerpass-faster"><h2 id="making-inlinerpass-faster">Can we make <code>InlinerPass</code> any faster?</h2></a>
<p>Hopefully, yes!</p>
<p>LLVM has a bunch of arguments that can be configured, which <code>rustc</code> exposes through the <code>-C llvm-args</code> flag. At time of
writing (June 2025), there&#39;s somewhere in the region of ~100 options that mention inlining (via <code>rustc -C llvm-args=&#39;--help-list-hidden&#39;</code>).
In particular, there&#39;s a <a href="https://github.com/llvm/llvm-project/blob/c7063380205d8776e281f7a6603119aa8ea28c12/llvm/lib/Analysis/InlineCost.cpp#L58-L176">bunch of relevant options</a> in the file controlling the cost analysis.</p>
<p>Now, I&#39;ll be honest, I know <em>very little</em> about LLVM&#39;s inlining. Most of the options refer to the &#34;cost&#34; associated
with the inlining, or with the function being inlined, etc. <strong>I&#39;m flying mostly bind here.</strong> But there&#39;s a few arguments
that seem like decent candidates for tuning:</p>
<ul>
<li><code>--inlinedefault-threshold=225</code> — &#34;Default amount of inlining to perform&#34;</li>
<li><code>--inline-threshold=225</code> — &#34;Control the amount of inlining to perform&#34;</li>
<li><code>--inlinehint-threshold=325</code> — &#34;Threshold for inlining functions with inline hint&#34;</li></ul>
<p>For all of these, the &#34;threshold&#34; roughly means &#34;allow inlining functions with cost <em>below</em> the threshold&#34;, so a higher
threshold means more inlining.</p>
<p>So if we set all of these to some value (e.g., <code>50</code>), we should see that there&#39;s less inlining, and in turn faster
compile times.</p>
<p>Something like:</p>
<pre><code><span>RUSTFLAGS</span><span>=</span><span>&#34;-Cllvm-args=-inline-threshold=50 -Cllvm-args=-inlinedefault-threshold=50 -Cllvm-args=-inlinehint-threshold=50&#34;</span> <span>..</span>.
</code></pre>
<p><em>(Why separate <code>-C llvm-args</code>? I couldn&#39;t find a way to make the whitespace happy through the <code>RUSTFLAGS</code> environment
variable — maybe it&#39;s possible if you set <code>build.rustflags</code> in <code>.cargo/config.toml</code>, but this solution worked 🤷)</em></p>
<p>In any case, reducing to a threshold of 50 <em>does</em> end up faster! About 42.2s, down from 48.8s.</p>
<p>Here&#39;s what that looks like across a handful of values:</p>
<p><img src="https://sharnoff.io/blog/why-rust-compiler-slow/assets/inline-thresholds.svg" alt="Graph of compile time with various inlining thresholds, from 40 seconds around zero to 49 seconds around 300"/></p>
<p><em>(note: The smallest value is 1, and not zero. Why 1? Sometimes zero has special behavior – setting to one seemed like a safer bet.)</em></p>
<p>Of these, it&#39;s hard to say exactly what the best value is, but for my use case (remember: my website gets ~zero load!),
setting the thresholds to 10 looks promising. We&#39;ll hold off on that for now though.</p>
<a href="#making-optfunction-faster"><h2 id="making-optfunction-faster">Can we make <code>OptFunction</code> any faster?</h2></a>
<p>Optimizing functions was the other expensive task we saw.</p>
<p>The knobs here are much less clear to me (we&#39;re already at <code>opt-level = 1</code>, and <code>opt-level = 0</code> compeltely disables
optimizations). So, let&#39;s see what exactly is taking so long.</p>
<p>First, a brief look at the event format:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>|</span> jq <span>-c</span> <span>&#39;select(.name == &#34;OptFunction&#34;)&#39;</span> <span>|</span> <span>head</span></span></span>
<span>{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:7995006,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:32052,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc12___rust_alloc&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8027059,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:242,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc14___rust_dealloc&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8027302,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:158,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc14___rust_realloc&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8027461,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:126,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc19___rust_alloc_zeroed&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:30,&#34;ts&#34;:8027589,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:150,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RNvCscSpY9Juk0HT_7___rustc26___rust_alloc_error_handler&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:31457262,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:24576,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_ZN10serde_json5value8to_value17h0315c73febebe85cE&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:31481850,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:11862,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_ZN10serde_json5value8to_value17h0516143613516496E&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:31493764,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:15830,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_ZN10serde_json5value8to_value17h0bdb4ac12d8ad59bE&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:31509615,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:8221,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_ZN10serde_json5value8to_value17h0c630b789ee318c2E&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:31517858,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:8670,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_ZN10serde_json5value8to_value17h12ba815471bb2bc8E&#34;}}
</span></code></pre>
<p>In its raw form, each of the events&#39; <code>.args.detail</code> field has the mangled symbol of the function being optimized. We can
&#34;demangle&#34; these back to the original Rust symbols with <a href="https://github.com/luser/rustfilt"><code>rustfilt</code></a> — for example:</p>
<pre><code><span><span>$</span> <span><span>cargo</span> <span>install</span> rustfilt</span></span>

<span><span>$</span> <span>rustfilt <span>&#39;_RNvCscSpY9Juk0HT_7___rustc12___rust_alloc&#39;</span></span></span>
<span>__rustc::__rust_alloc
</span><span><span>$</span> <span>rustfilt <span>&#39;_ZN10serde_json5value8to_value17h0315c73febebe85cE&#39;</span></span></span>
<span>serde_json::value::to_value
</span></code></pre>
<p>It&#39;s worth noting that in the list above, while there&#39;s several <code>serde_json::value::to_value</code> items, they actually have
distinct hashes:</p>
<pre><code><span><span>$</span> <span>rustfilt <span>-h</span> <span>&#39;_ZN10serde_json5value8to_value17h0315c73febebe85cE&#39;</span></span></span>
<span>serde_json::value::to_value::h0315c73febebe85c
</span><span><span>$</span> <span>rustfilt <span>-h</span> <span>&#39;_ZN10serde_json5value8to_value17h0516143613516496E&#39;</span></span></span>
<span>serde_json::value::to_value::h0516143613516496
</span><span><span>$</span> <span>rustfilt <span>-h</span> <span>&#39;_ZN10serde_json5value8to_value17h0bdb4ac12d8ad59bE&#39;</span></span></span>
<span>serde_json::value::to_value::h0bdb4ac12d8ad59b
</span><span><span>$</span> <span>rustfilt <span>-h</span> <span>&#39;_ZN10serde_json5value8to_value17h0c630b789ee318c2E&#39;</span></span></span>
<span>serde_json::value::to_value::h0c630b789ee318c2
</span><span><span>$</span> <span>rustfilt <span>-h</span> <span>&#39;_ZN10serde_json5value8to_value17h12ba815471bb2bc8E&#39;</span></span></span>
<span>serde_json::value::to_value::h12ba815471bb2bc8
</span></code></pre>
<p>... which makes sense, given that <code>serde_json::value::to_value</code> is a generic function — it might be that it&#39;s being
optimized with different generic parameters (&#34;monomorphizations&#34;).</p>
<a href="#why-optimizing-other-crates"><h3 id="why-optimizing-other-crates">Wait, why are we optimizing functions from other crates?</h3></a>
<p>The short answer is that optimization is done <em>in the context of the crate where a function is monomorphized</em>. So if we
define a type <code>Foo</code> and then call methods on <code>Option&lt;Foo&gt;</code>, those methods <em>with those types</em> will first exist in the
context of our crate — meaning it gets compiled and optimized with the same configuration as our crate.</p>
<p>With some knowledge about how the compiler works under the hood, this should hopefully make some sense — but from the
outside, it&#39;s certainly a little odd!</p>
<a href="#what-are-we-optimizing"><h3 id="what-are-we-optimizing">What&#39;s actually taking so long?</h3></a>
<p>Now that we know what we&#39;re looking at, we can start doing some analysis. For example, by finding the individual
functions we spent the most time optimizing:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-c</span> <span>&#39;select(.name == &#34;OptFunction&#34;)&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sc</span> <span>&#39;sort_by(-.dur) | .[] | { dur: (.dur / 1e6), detail: .args.detail }&#39;</span> <span>\</span>
    <span>|</span> <span>head</span></span></span>
<span>{&#34;dur&#34;:1.875744,&#34;detail&#34;:&#34;_ZN15web_http_server6photos11PhotosState3new28_$u7b$$u7b$closure$u7d$$u7d$17ha4de409b0951d78bE&#34;}
{&#34;dur&#34;:1.44252,&#34;detail&#34;:&#34;_ZN14tokio_postgres6client6Client5query28_$u7b$$u7b$closure$u7d$$u7d$17h18fb9179bb73bfa4E&#34;}
{&#34;dur&#34;:1.440186,&#34;detail&#34;:&#34;_ZN15web_http_server3run28_$u7b$$u7b$closure$u7d$$u7d$17h426fe76bd1b089abE&#34;}
{&#34;dur&#34;:1.397705,&#34;detail&#34;:&#34;_ZN15web_http_server6photos11PhotosState3new28_$u7b$$u7b$closure$u7d$$u7d$17ha4de409b0951d78bE&#34;}
{&#34;dur&#34;:1.170948,&#34;detail&#34;:&#34;_ZN14tokio_postgres11connect_raw11connect_raw28_$u7b$$u7b$closure$u7d$$u7d$17h0dfcfa0a648a93f8E&#34;}
{&#34;dur&#34;:1.158111,&#34;detail&#34;:&#34;_ZN14pulldown_cmark5parse15Parser$LT$F$GT$19handle_inline_pass117hc91a3dc90e0e9e0cE&#34;}
{&#34;dur&#34;:1.131707,&#34;detail&#34;:&#34;_ZN129_$LT$axum..boxed..MakeErasedHandler$LT$H$C$S$GT$$u20$as$u20$axum..boxed..ErasedIntoRoute$LT$S$C$core..convert..Infallible$GT$$GT$9clone_box17he7f38a2ccd053fbbE&#34;}
{&#34;dur&#34;:1.062162,&#34;detail&#34;:&#34;_ZN4core3ptr49drop_in_place$LT$http..extensions..Extensions$GT$17h89b138bb6c1aa101E&#34;}
{&#34;dur&#34;:1.026656,&#34;detail&#34;:&#34;_ZN15web_http_server3run28_$u7b$$u7b$closure$u7d$$u7d$17h426fe76bd1b089abE&#34;}
{&#34;dur&#34;:1.009844,&#34;detail&#34;:&#34;_ZN4core3ptr252drop_in_place$LT$$LT$alloc..vec..drain..Drain$LT$T$C$A$GT$$u20$as$u20$core..ops..drop..Drop$GT$..drop..DropGuard$LT$lol_html..selectors_vm..stack..StackItem$LT$lol_html..rewriter..rewrite_controller..ElementDescriptor$GT$$C$alloc..alloc..Global$GT$$GT$17h62ca0c07fce3ede0E&#34;}
</span></code></pre>
<p><em>(Why two separate <code>jq</code> invocations? If we did just one, the <code>-s</code>/<code>--slurp</code> call would load the entire file
into a single array before any processing, which is one of the key operations we&#39;re trying to avoid)</em></p>
<p>This is a surprising amount of time on individual functions! Profiling roughly doubled the total time to compile, but
even 1 second optimizing a single function is quite a long time!</p>
<p>But let&#39;s look into more detail here. We&#39;ve got:</p>
<ul>
<li><code>web_http_server::photos::PhotosState::new::{{closure}}</code> — this is <em>some</em> closure inside a giant, 400-line async
function that does the setup for <a href="https://sharnoff.io/photos">https://sharnoff.io/photos</a></li>
<li><code>web_http_server::run::{{closure}}</code> — this is inside the main entrypoint (also async), but all the closures are small
error-handling, like <code>.wrap_err_with(|| format!(&#34;failed to bind address {addr:?}&#34;))</code>
<ul>
<li>Maybe there&#39;s something weird going on here!</li></ul>
</li></ul>
<p>... and a handful of dependencies that also took a while:</p>
<ul>
<li><code>pulldown_cmark</code> has a <a href="https://github.com/pulldown-cmark/pulldown-cmark/blob/0.13.0/pulldown-cmark/src/parse.rs#L387-L862">500-line function</a> that&#39;s generic over a callback</li>
<li><code>tokio_postgres::connect_raw</code> is a simple closure inside a <a href="https://github.com/sfackler/rust-postgres/blob/tokio-postgres-v0.7.13/tokio-postgres/src/connect_raw.rs#L82-L127">reasonably-sized async function</a> – maybe this is for the
same reason as the closure in my <code>web_http_server::run</code>?</li>
<li>The drop implementation of <a href="https://docs.rs/http/1.2.0/http/struct.Extensions.html"><code>http::extensions::Extensions</code></a> <em>looks</em> like it should be simple (there&#39;s no explicit
destructor), but internally it&#39;s <a href="https://github.com/hyperium/http/blob/v1.2.0/src/extensions.rs#L6"><code>Option&lt;Box&lt;HashMap&lt;TypeId, Box&lt;dyn ...&gt;, BuildDefaultHasher&lt;..&gt;&gt;&gt;&gt;</code></a>. Maybe there&#39;s a
lot of complexity from inlining here?</li>
<li>And dropping <a href="https://doc.rust-lang.org/std/vec/struct.Drain.html"><code>vec::Drain&lt;T&gt;</code></a> with a series of nested <a href="https://docs.rs/lol_html"><code>lol_html</code></a> types also complains – maybe for similar reasons</li></ul>
<hr/>
<p><strong>Alternately</strong>, we could break it down by the outermost crate:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>xargs</span> <span>-l</span> <span>bash</span> <span>-c</span> <span>&#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;root\&#34;:\&#34;$(rustfilt &#34;$1&#34; | sed -E &#34;s/^([a-z_-]+)::.*/\1/g&#34;)\&#34;}&#34;&#39;</span> <span>\</span>
    <span>|</span> jq <span>-s</span> <span>-r</span> <span>&#39;group_by(.root) | map({ root: .[0].root, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[] | &#34;\(.dur / 1e6) \(.root)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>head</span> </span></span>
<span>61.534452 core
13.750173 web_http_server
11.237289 tokio
7.890088 tokio_postgres
6.851621 lol_html
4.470053 alloc
4.177471 feed_rs
3.269217 std
3.067573 hashbrown
3.063146 eyre
</span></code></pre>
<p>This is, of course, a very imperfect measure — the outermost crate isn&#39;t necessarily the best one to attribute the
compilation time to, and there&#39;s a lot of items like <code>&lt;Foo as Bar&gt;::baz</code> that aren&#39;t captured by this simple filtering.
But all that aside, it&#39;s still surprising that there&#39;s so much from <code>core</code>!</p>
<p>Digging further, 84% of that time is just parameterizations of <a href="https://doc.rust-lang.org/stable/core/ptr/fn.drop_in_place.html"><code>core::ptr::drop_in_place</code></a>!</p>
<a href="#closures-mangling-v0"><h3 id="closures-mangling-v0">Digging more into closures, with mangling v0</h3></a>
<p>The long compile times for closures seems very suspicious — maybe it&#39;s worth digging further. There&#39;s just one
problem: the symbols all end with <code>{{closure}}</code> without saying <em>which one</em> is taking all the time.</p>
<p>As it turns out, there&#39;s an easy fix! As of June 2025, <code>rustc</code> currently uses the &#34;legacy&#34; symbol mangling format by
default, but there&#39;s a newer option with more information: the <a href="https://doc.rust-lang.org/rustc/symbol-mangling/v0.html">v0 format</a>.</p>
<p>We can enable it by adding <code>RUSTFLAGS=&#34;-C symbol-mangling-version=v0&#34;</code> to our existing flags, which now look something
like:</p>
<pre><code>RUSTC_BOOTSTRAP=1 RUSTFLAGS=&#34;-Csymbol-mangling-version=v0 -Zllvm-time-trace&#34; cargo build --timings ...
</code></pre>
<p><em>(aside: The issue for that feature&#39;s been open for 6 years, why hasn&#39;t it been merged yet? Turns out, there&#39;s a lot of
upstream work required to add support in common tools like <code>gdb</code> and <code>perf</code>. A lot of that has been done, but not yet
everything.)</em></p>
<p>The end result of this is that we get <strong>much</strong> better symbols coming out of the LLVM trace. As an example, here&#39;s what
those <code>serde_json::value::to_value</code> symbols look like now:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>|</span> jq <span>-c</span> <span>&#39;select(.name == &#34;OptFunction&#34;)&#39;</span> <span>|</span> <span>grep</span> <span>-E</span> <span>&#39;serde_json.+value.+to_value&#39;</span> <span>|</span> <span>head</span></span></span>
<span>{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:34400185,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:7336,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RINvNtCs9KWWFfvvCPd_10serde_json5value8to_valueINtNCNvCs5etrU9lJXb7_15web_http_server5index012IndexContextNtNtNtNtBQ_4blog6handle7context9RootIndexNtNtNtNtBQ_6photos6handle7context9RootIndexEEBQ_&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:34407530,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:13226,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RINvNtCs9KWWFfvvCPd_10serde_json5value8to_valueNtNtNtNtCs5etrU9lJXb7_15web_http_server4blog6handle7context4PostEBR_&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:34420761,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:10344,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RINvNtCs9KWWFfvvCPd_10serde_json5value8to_valueNtNtNtNtCs5etrU9lJXb7_15web_http_server4blog6handle7context5IndexEBR_&#34;}}
{&#34;pid&#34;:25,&#34;tid&#34;:35,&#34;ts&#34;:34431114,&#34;ph&#34;:&#34;X&#34;,&#34;dur&#34;:11100,&#34;name&#34;:&#34;OptFunction&#34;,&#34;args&#34;:{&#34;detail&#34;:&#34;_RINvNtCs9KWWFfvvCPd_10serde_json5value8to_valueNtNtNtNtCs5etrU9lJXb7_15web_http_server6photos6handle7context11AlbumsIndexEBR_&#34;}}

</span><span><span>$</span> <span>rustfilt <span>&#39;_RINvNtCs9KWWFfvvCPd_10serde_json5value8to_valueINtNCNvCs5etrU9lJXb7_15web_http_server5index012IndexContextNtNtNtNtBQ_4blog6handle7context9RootIndexNtNtNtNtBQ_6photos6handle7context9RootIndexEEBQ_&#39;</span></span></span>
<span>serde_json::value::to_value::&lt;web_http_server::index::{closure#0}::IndexContext&lt;web_http_server::blog::handle::context::RootIndex, web_http_server::photos::handle::context::RootIndex&gt;&gt;
</span><span><span>$</span> <span>rustfilt <span>&#39;_RINvNtCs9KWWFfvvCPd_10serde_json5value8to_valueNtNtNtNtCs5etrU9lJXb7_15web_http_server4blog6handle7context4PostEBR_&#39;</span></span></span>
<span>serde_json::value::to_value::&lt;web_http_server::blog::handle::context::Post&gt;
</span><span><span>$</span> <span>rustfilt <span>&#39;_RINvNtCs9KWWFfvvCPd_10serde_json5value8to_valueNtNtNtNtCs5etrU9lJXb7_15web_http_server4blog6handle7context5IndexEBR_&#39;</span></span></span>
<span>serde_json::value::to_value::&lt;web_http_server::blog::handle::context::Index&gt;
</span><span><span>$</span> <span>rustfilt <span>&#39;_RINvNtCs9KWWFfvvCPd_10serde_json5value8to_valueNtNtNtNtCs5etrU9lJXb7_15web_http_server6photos6handle7context11AlbumsIndexEBR_&#39;</span></span></span>
<span>serde_json::value::to_value::&lt;web_http_server::photos::handle::context::AlbumsIndex&gt;
</span></code></pre>
<p>So not only do we get better closure labeling (see e.g. <code>{closure#0}</code>) but we also get full generics for everything!</p>
<p>Exactly what&#39;s taking so long <em>should</em> be much clearer now:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>xargs</span> <span>-l</span> <span>bash</span> <span>-c</span> <span>&#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sr</span> <span>&#39;sort_by(-.dur) | .[] | &#34;\(.dur / 1e4 | round | . / 1e2)s \(.fn)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>head</span> <span>-n5</span></span></span>
<span>1.99s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}
1.56s web_http_server::run::{closure#0}
1.41s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}
1.22s core::ptr::drop_in_place::&lt;axum::routing::Endpoint&lt;web_http_server::AppState&gt;&gt;
1.15s core::ptr::drop_in_place::&lt;axum::routing::method_routing::MethodEndpoint&lt;web_http_server::AppState, core::convert::Infallible&gt;&gt;
</span></code></pre>
<p>... but those first few closures are <em>tiny</em>:</p>
<pre><code><span>let</span> is_jpg <span>=</span> <span><span>|</span>path<span>:</span> <span>&amp;</span><span>Path</span><span>|</span></span> path<span>.</span><span>extension</span><span>(</span><span>)</span><span>.</span><span>and_then</span><span>(</span><span><span>|</span>s<span>|</span></span> s<span>.</span><span>to_str</span><span>(</span><span>)</span><span>)</span> <span>==</span> <span>Some</span><span>(</span><span>&#34;jpg&#34;</span><span>)</span><span>;</span>
</code></pre>
<p>and</p>
<pre><code><span>let</span> app <span>=</span> <span>axum<span>::</span></span><span>Router</span><span>::</span><span>new</span><span>(</span><span>)</span>
    
    <span>.</span><span>route</span><span>(</span><span>&#34;/feed.xml&#34;</span><span>,</span> <span>axum<span>::</span>routing<span>::</span></span><span>get</span><span>(</span><span>move</span> <span><span>|</span><span>|</span></span> <span>async</span> <span>move</span> <span>{</span> feed <span>}</span><span>)</span><span>)</span>
    
</code></pre>
<p>And if we remove these closures, replacing them with separately defined functions where possible, LLVM <em>still</em> reports
taking a long time to optimize <code>{closure#0}</code> in the outer function.</p>
<a href="#async-closure0"><h3 id="async-closure0">So where are those closures coming from?</h3></a>
<p>After dumping the LLVM IR with <code>RUSTFLAGS=&#34;--emit=llvm-ir&#34;</code> (which places it into <code>target/.../deps/*.ll</code>) and searching
through the generated functions, I found a line like:</p>
<pre><code>
</code></pre>
<p>That <code>process_photo</code> function was a nested async function, defined directly inside <code>PhotosState::new</code> — so why did the
symbol say it was defined inside a closure?</p>
<hr/>
<p>It&#39;s because <code>rustc</code> <em>internally represents async functions/blocks with a nested closure</em>. So all of these places that
we had async functions where compiling <code>closure#0</code> took a long time were actually just referring to the function itself!</p>
<p>With some quick github searching (<code>is:issue state:open async fn closure mangle</code>), it turned out there was already an
<a href="https://github.com/rust-lang/rust/issues/104830">open issue about this</a>!</p>
<a href="#big-async-functions-considered-harmful"><h3 id="big-async-functions-considered-harmful">Big async functions considered harmful?</h3></a>
<p>Going back to our list from before – those async functions where LLVM takes a long time to optimize <code>closure#0</code> are
really just spending a long time on the body of the function itself. It would make sense that big functions are hard to
optimize, and async functions doubly so.</p>
<p>It&#39;s <em>fairly</em> straightforward to identify all of the functions inside the main crate that are taking a long time:</p>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>xargs</span> <span>-l</span> <span>bash</span> <span>-c</span> <span>&#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sc</span> <span>&#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39;</span> <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;*web_http_server&#34;)) | &#34;\(.dur / 1e4 | round | . / 1e2)s \(.fn)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>head</span> <span>-n10</span></span></span>
<span>4.11s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}
3.05s web_http_server::run::{closure#0}
1.44s core::ptr::drop_in_place::&lt;web_http_server::run::{closure#0}&gt;
0.6s &lt;web_http_server::reading_list::handle::post_login as axum::handler::Handler&lt;(axum_core::extract::private::ViaRequest, axum::extract::state::State&lt;&amp;web_http_server::reading_list::ReadingListState&gt;, axum::extract::state::State&lt;&amp;tera::tera::Tera&gt;, axum_extra::extract::cookie::CookieJar, axum::form::Form&lt;web_http_server::reading_list::handle::LoginForm&gt;), web_http_server::AppState&gt;&gt;::call::{closure#0}
0.57s web_http_server::reading_list::fetch_posts_data::{closure#0}
0.51s &lt;web_http_server::reading_list::ReadingListState&gt;::make_pool::{closure#0}
0.44s &lt;web_http_server::reading_list::ReadingListState&gt;::refresh_single::{closure#0}
0.38s &lt;web_http_server::photos::PhotosState&gt;::process_photo::{closure#0}
0.38s &lt;web_http_server::html::WriteState&gt;::process_event
0.33s core::ptr::drop_in_place::&lt;&lt;web_http_server::reading_list::ReadingListState&gt;::run_refresh::{closure#0}::{closure#0}&gt;
</span></code></pre>
<p>Some of the most expensive functions here are around setup.</p>
<p><strong>Let&#39;s try breaking up just one function, to see if it helps.</strong> We&#39;ll start with <code>PhotosState::new</code>.</p>
<details>
<summary>before any changes: full timings for <code>PhotosState::new</code></summary>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>xargs</span> <span>-l</span> <span>bash</span> <span>-c</span> <span>&#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sc</span> <span>&#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39;</span> <span>\</span>
    <span>|</span> jq <span>&#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;web_http_server::photos::PhotosState&gt;::new&#34;)) | .dur&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sr</span> <span>&#39;add | . / 1e4 | round | . / 1e2 | &#34;\(.)s&#34;&#39;</span></span></span>
<span>5.3s

</span><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>xargs</span> <span>-l</span> <span>bash</span> <span>-c</span> <span>&#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sc</span> <span>&#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39;</span> <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;web_http_server::photos::PhotosState&gt;::new&#34;)) | &#34;\(.dur / 1e4 | round | . / 1e2)s \(.fn)&#34;&#39;</span></span></span>
<span>4.11s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}
0.27s core::ptr::drop_in_place::&lt;&lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}&gt;
0.24s core::ptr::drop_in_place::&lt;&lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#2}::{closure#0}&gt;
0.23s core::ptr::drop_in_place::&lt;&lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#2}&gt;
0.19s core::ptr::drop_in_place::&lt;&lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#6}::{closure#0}&gt;
0.11s core::ptr::drop_in_place::&lt;&lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#7}::{closure#0}&gt;
0.03s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#6}::{closure#0}
0.02s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#3}
0.02s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#11}
0.02s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#4}
0.02s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#5}
0.01s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#2}
0.01s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#7}::{closure#0}
0.01s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#2}::{closure#0}
0.01s &lt;web_http_server::photos::PhotosState&gt;::new::{closure#0}::{closure#1}::{closure#1}
</span></code></pre></details>
<p>On the first attempt, I tried breaking it up while also preserving the number of <code>.await</code>s – it&#39;s easy to do both
accidentally, and this would hopefully isolate which type of complexity is causing problems.</p>
<details>
<summary> After naive splitting: full timings for <code>photos::init</code> </summary>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> </span></span>
<span>    | xargs -l bash -c &#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39; \
    | jq -sc &#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39; \
    | jq &#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;*web_http_server::photos::(init|PhotosState&gt;::new)&#34;)) | .dur&#39; \
    | jq -sr &#39;add | . / 1e4 | round | . / 1e2 | &#34;\(.)s&#34;&#39;
4.66s

</span><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>xargs</span> <span>-l</span> <span>bash</span> <span>-c</span> <span>&#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sc</span> <span>&#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39;</span> <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;*web_http_server::photos::(init|PhotosState&gt;::new)&#34;)) | &#34;\(.dur / 1e4 | round | . / 1e2)s \(.fn)&#34;&#39;</span></span></span>
<span>3.37s web_http_server::photos::init::make_state::{closure#0}
0.24s core::ptr::drop_in_place::&lt;web_http_server::photos::init::image_process_futs::{closure#0}::{closure#0}&gt;
0.21s core::ptr::drop_in_place::&lt;web_http_server::photos::init::album_process_futs::{closure#0}&gt;
0.21s core::ptr::drop_in_place::&lt;web_http_server::photos::init::make_state::{closure#0}&gt;
0.16s core::ptr::drop_in_place::&lt;web_http_server::photos::init::image_process_futs::{closure#0}&gt;
0.12s core::ptr::drop_in_place::&lt;web_http_server::photos::init::album_process_futs::{closure#1}&gt;
0.06s web_http_server::photos::init::album_process_futs::{closure#0}
0.04s web_http_server::photos::init::image_process_futs::{closure#0}
0.03s web_http_server::photos::init::album_process_futs::{closure#1}
0.03s web_http_server::photos::init::album_process_futs
0.02s core::ptr::drop_in_place::&lt;web_http_server::photos::init::get_img_candidates::{closure#0}&gt;
0.02s web_http_server::photos::init::make_album_membership
0.02s web_http_server::photos::init::make_state::{closure#0}::{closure#1}
0.02s web_http_server::photos::init::make_albums_in_order
0.02s web_http_server::photos::init::image_process_futs
0.02s web_http_server::photos::init::make_state::{closure#0}::{closure#3}
0.02s web_http_server::photos::init::make_state::{closure#0}::{closure#2}
0.02s web_http_server::photos::init::image_process_futs::{closure#0}::{closure#0}
0.02s web_http_server::photos::init::make_state::{closure#0}::{closure#7}
0.01s web_http_server::photos::init::make_all_album
0.01s web_http_server::photos::init::make_recently_published_albums
0.01s web_http_server::photos::init::make_images_by_time
0s web_http_server::photos::init::get_img_candidates::{closure#0}::{closure#1}::{closure#1}
</span></code></pre></details>
<p>Interestingly, this didn&#39;t help all that much: only reducing the total time from 5.3s to 4.7s.</p>
<p>So to add to that, I tried merging a handful of neighboring <code>.await</code>s into their own functions — reducing the total
number from 10 to 3.</p>
<details>
<summary>After grouping <code>.await</code>s</summary>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> </span></span>
<span>    | xargs -l bash -c &#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39; \
    | jq -sc &#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39; \
    | jq &#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;*web_http_server::photos::(init|PhotosState&gt;::new)&#34;)) | .dur&#39; \
    | jq -sr &#39;add | . / 1e4 | round | . / 1e2 | &#34;\(.)s&#34;&#39;
6.24s

</span><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>xargs</span> <span>-l</span> <span>bash</span> <span>-c</span> <span>&#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sc</span> <span>&#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39;</span> <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;*web_http_server::photos::(init|PhotosState&gt;::new)&#34;)) | &#34;\(.dur / 1e4 | round | . / 1e2)s \(.fn)&#34;&#39;</span></span></span>
<span>2.7s web_http_server::photos::init::process_all_images::{closure#0}
1.93s web_http_server::photos::init::make_state::{closure#0}
0.25s core::ptr::drop_in_place::&lt;web_http_server::photos::init::image_process_futs::{closure#0}::{closure#0}&gt;
0.25s core::ptr::drop_in_place::&lt;web_http_server::photos::init::album_process_futs::{closure#0}&gt;
0.18s core::ptr::drop_in_place::&lt;web_http_server::photos::init::image_process_futs::{closure#0}&gt;
0.14s core::ptr::drop_in_place::&lt;web_http_server::photos::init::album_process_futs::{closure#1}&gt;
0.09s core::ptr::drop_in_place::&lt;web_http_server::photos::init::process_all_images::{closure#0}&gt;
0.08s core::ptr::drop_in_place::&lt;web_http_server::photos::init::join_image_futs&lt;web_http_server::photos::init::image_process_futs::{closure#0}&gt;::{closure#0}&gt;
0.07s core::ptr::drop_in_place::&lt;web_http_server::photos::init::make_state::{closure#0}&gt;
0.07s web_http_server::photos::init::album_process_futs::{closure#0}
0.06s core::ptr::drop_in_place::&lt;web_http_server::photos::init::parse::{closure#0}&gt;
0.04s core::ptr::drop_in_place::&lt;web_http_server::photos::init::join_album_futs::{closure#0}&gt;
0.04s web_http_server::photos::init::image_process_futs::{closure#0}
0.03s web_http_server::photos::init::album_process_futs
0.03s web_http_server::photos::init::make_album_membership
0.03s core::ptr::drop_in_place::&lt;web_http_server::photos::init::get_img_candidates::{closure#0}&gt;
0.03s web_http_server::photos::init::album_process_futs::{closure#1}
0.03s web_http_server::photos::init::make_albums_in_order
0.03s web_http_server::photos::init::image_process_futs
0.02s web_http_server::photos::init::process_all_images::{closure#0}::{closure#1}
0.02s web_http_server::photos::init::make_state::{closure#0}::{closure#0}
0.02s web_http_server::photos::init::make_state::{closure#0}::{closure#1}
0.02s web_http_server::photos::init::process_all_images::{closure#0}::{closure#2}
0.02s web_http_server::photos::init::image_process_futs::{closure#0}::{closure#0}
0.02s web_http_server::photos::init::make_all_album
0.01s web_http_server::photos::init::make_images_by_time
0.01s web_http_server::photos::init::make_recently_published_albums
0s web_http_server::photos::init::get_img_candidates::{closure#0}::{closure#1}::{closure#1}
</span></code></pre></details>
<p>But that took substantially longer! It increased from 4.66s to 6.24s!</p>
<hr/>
<p>At this point, it seemed like there was something strange happening with async functions. Otherwise, why would splitting
into more functions make things worse?</p>
<p>Under the hood, async functions desugar to a complex state machine. There might be something odd happening there, so if
we want to make that simpler in the caller, we can turn the <code>Future</code> into a trait object to obscure the implementation
behind it (typically <code>Pin&lt;Box&lt;dyn Future&gt;&gt;</code>).</p>
<p>So this time, let&#39;s add a new function like:</p>
<pre><code><span>fn</span> <span>erase</span><span>&lt;</span><span>&#39;a</span><span>,</span> <span>T</span><span>&gt;</span><span>(</span>
    fut<span>:</span> <span>impl</span> <span>&#39;a</span> <span>+</span> <span>Send</span> <span>+</span> <span>Future</span><span>&lt;</span><span>Output</span> <span>=</span> <span>T</span><span>&gt;</span><span>,</span>
<span>)</span> <span>-&gt;</span> <span>Pin</span><span>&lt;</span><span>Box</span><span>&lt;</span><span>dyn</span> <span>&#39;a</span> <span>+</span> <span>Send</span> <span>+</span> <span>Future</span><span>&lt;</span><span>Output</span> <span>=</span> <span>T</span><span>&gt;&gt;</span><span>&gt;</span> <span>{</span>
    <span>Box</span><span>::</span><span>pin</span><span>(</span>fut<span>)</span>
<span>}</span>
</code></pre>
<p>and using it everywhere we <code>.await</code>. For example:</p>
<pre><code>
<span>let</span> candidates <span>=</span> <span>get_img_candidates</span><span>(</span><span>)</span><span>.</span><span>await</span><span>?</span><span>;</span>


<span>let</span> candidates <span>=</span> <span>erase</span><span>(</span><span>get_img_candidates</span><span>(</span><span>)</span><span>)</span><span>.</span><span>await</span><span>?</span><span>;</span>
</code></pre><details>
<summary>Final change: <code>Pin<box<..>&gt;</box<..></code> the futures</summary>
<pre><code><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> </span></span>
<span>    | xargs -l bash -c &#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39; \
    | jq -sc &#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39; \
    | jq &#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;*web_http_server::photos::(init|PhotosState&gt;::new)&#34;)) | .dur&#39; \
    | jq -sr &#39;add | . / 1e4 | round | . / 1e2 | &#34;\(.)s&#34;&#39;
2.14s

</span><span><span>$</span> <span><span>cat</span> web_http_server.llvm_timings.jsonl <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.name == &#34;OptFunction&#34;) | &#34;\(.dur) \(.args.detail)&#34;&#39;</span> <span>\</span>
    <span>|</span> <span>xargs</span> <span>-l</span> <span>bash</span> <span>-c</span> <span>&#39;echo &#34;{\&#34;dur\&#34;:$0,\&#34;fn\&#34;:\&#34;$(rustfilt &#34;$1&#34;)\&#34;}&#34;&#39;</span> <span>\</span>
    <span>|</span> jq <span>-sc</span> <span>&#39;group_by(.fn) | map({ fn: .[0].fn, dur: (map(.dur) | add) }) | sort_by(-.dur) | .[]&#39;</span> <span>\</span>
    <span>|</span> jq <span>-r</span> <span>&#39;select(.fn | test(&#34;^(core::ptr::drop_in_place::&lt;)?&lt;*web_http_server::photos::(init|PhotosState&gt;::new)&#34;)) | &#34;\(.dur / 1e4 | round | . / 1e2)s \(.fn)&#34;&#39;</span></span></span>
<span>0.25s web_http_server::photos::init::process_all_images::{closure#0}
0.21s core::ptr::drop_in_place::&lt;web_http_server::photos::init::image_process_futs::{closure#0}::{closure#0}&gt;
0.2s core::ptr::drop_in_place::&lt;web_http_server::photos::init::image_process_futs::{closure#0}&gt;
0.2s web_http_server::photos::init::join_image_futs::&lt;web_http_server::photos::init::image_process_futs::{closure#0}&gt;::{closure#0}
0.19s core::ptr::drop_in_place::&lt;web_http_server::photos::init::album_process_futs::{closure#0}&gt;
0.13s web_http_server::photos::init::parse::{closure#0}
0.11s core::ptr::drop_in_place::&lt;web_http_server::photos::init::album_process_futs::{closure#1}&gt;
0.1s web_http_server::photos::init::get_img_candidates::{closure#0}
0.1s core::ptr::drop_in_place::&lt;web_http_server::photos::init::make_state::{closure#0}&gt;
0.06s core::ptr::drop_in_place::&lt;web_http_server::photos::init::process_all_images::{closure#0}&gt;
0.06s web_http_server::photos::init::album_process_futs::{closure#0}
0.06s web_http_server::photos::init::album_process_futs
0.05s web_http_server::photos::init::join_album_futs::{closure#0}
0.05s web_http_server::photos::init::make_albums_in_order
0.05s core::ptr::drop_in_place::&lt;web_http_server::photos::init::join_image_futs&lt;web_http_server::photos::init::image_process_futs::{closure#0}&gt;::{closure#0}&gt;
0.04s core::ptr::drop_in_place::&lt;web_http_server::photos::init::parse::{closure#0}&gt;
0.03s web_http_server::photos::init::image_process_futs::{closure#0}
0.03s web_http_server::photos::init::make_all_album
0.03s web_http_server::photos::init::album_process_futs::{closure#1}
0.02s core::ptr::drop_in_place::&lt;web_http_server::photos::init::join_album_futs::{closure#0}&gt;
0.02s core::ptr::drop_in_place::&lt;web_http_server::photos::init::get_img_candidates::{closure#0}&gt;
0.02s web_http_server::photos::init::make_state::{closure#0}::{closure#1}
0.02s web_http_server::photos::init::make_state::{closure#0}::{closure#0}
0.02s web_http_server::photos::init::make_recently_published_albums
0.02s web_http_server::photos::init::image_process_futs::{closure#0}::{closure#0}
0.01s web_http_server::photos::init::make_images_by_time
0.01s web_http_server::photos::init::erase::&lt;core::result::Result&lt;std::collections::hash::map::HashMap&lt;alloc::string::String, &amp;web_http_server::photos::Album&gt;, eyre::Report&gt;, web_http_server::photos::init::join_album_futs::{closure#0}&gt;
0.01s web_http_server::photos::init::erase::&lt;core::result::Result&lt;web_http_server::photos::init::ProcessedImages, eyre::Report&gt;, web_http_server::photos::init::process_all_images::{closure#0}&gt;
0.01s web_http_server::photos::init::erase::&lt;core::result::Result&lt;(web_http_server::photos::MapSettings, web_http_server::photos::FlexGridSettings, web_http_server::photos::parsed::HiddenAlbumsAndPhotos, web_http_server::photos::parsed::Albums), eyre::Report&gt;, web_http_server::photos::init::parse::{closure#0}&gt;
0.01s web_http_server::photos::init::process_all_images::{closure#0}::{closure#1}
0.01s web_http_server::photos::init::process_all_images::{closure#0}::{closure#2}
0.01s web_http_server::photos::init::make_state
0.01s web_http_server::photos::init::get_img_candidates::{closure#0}::{closure#1}::{closure#1}
</span></code></pre></details>
<p><strong>This one worked — down to 2.14s.</strong></p>
<p>So, a reduction from 5.3s to 2.14s – a notable improvement, albeit with a lot of effort to get there. (and, for the
record, when I wrapped the futures with <code>Box::pin</code> instead of a fresh function, it didn&#39;t make a difference here).</p>
<p>Re-running the build without profiling, this gives a total reduction from 48.8s to 46.8s. It&#39;s pretty small, but that&#39;s
from just a single function!</p>
<p><em>(Aside: What about <code>#[inline(never)]</code>? I tried it with and without – after boxing, compile times weren&#39;t any better
with inlining disabled for those functions, but it&#39;s still helpful for ensuring better attribution on the LLVM
timings.)</em></p>
<p><em>(Aside: What about disabling inlining on the <strong>poll</strong> functions? I also tried wrapping the async functions with a
<code>Future</code> implementation having <code>#[inline(never)]</code> on its poll function. That helped <strong>some</strong>, but wasn&#39;t as good as
boxing.)</em></p>
<a href="#putting-it-together"><h2 id="putting-it-together">Putting it together</h2></a>
<p>There&#39;s a number of approaches available — let&#39;s try:</p>
<ol>
<li>Reducing inlining with LLVM args;</li>
<li>Breaking up expensive functions in the main crate; and</li>
<li>Removing generics from dependencies to prevent needing to compile it in the main crate</li></ol>
<p>So, updating the final Dockerfile commands to read:</p>
<pre><code><span><span>RUN</span> RUSTFLAGS=<span>&#39;-Cllvm-args=-inline-threshold=10 -Cllvm-args=-inlinedefault-threshold=10 -Cllvm-args=-inlinehint-threshold=10&#39;</span> <span>\</span>
        cargo chef cook --release --target=x86_64-unknown-linux-musl --recipe-path=/workdir/recipe.json</span>

...

<span><span>RUN</span> RUSTFLAGS=<span>&#39;-Cllvm-args=-inline-threshold=10 -Cllvm-args=-inlinedefault-threshold=10 -Cllvm-args=-inlinehint-threshold=10&#39;</span> <span>\</span>
        cargo build --timings --release --target=x86_64-unknown-linux-musl --package web-http-server</span>
</code></pre>
<p>... and many more small changes to the main crate:</p>
<pre><code><span><span>$</span> <span><span>git</span> <span>diff</span> <span>--stat</span> base<span>..</span>HEAD -- web-http-server</span></span>
<span>...
 10 files changed, 898 insertions(+), 657 deletions(-)
</span></code></pre>
<p>... alongside some changes to larger dependencies:</p>
<ul>
<li>Making a generic function non-generic: <a href="https://github.com/pulldown-cmark/pulldown-cmark/pull/1045">https://github.com/pulldown-cmark/pulldown-cmark/pull/1045</a></li>
<li>Building a separate crate with non-generic versions: <a href="https://github.com/LukeMathWalker/cargo-chef/pull/309">changes in <code>cargo-chef</code></a>, with a new local crate exposing
non-generic versions of the APIs I use from <code>lol_html</code> and <code>deadpool_postgres</code></li></ul>
<p>... gives us a final compile time of <strong>32.3s</strong>.</p>
<hr/>
<a href="#update-2025-06-27"><h2 id="update-2025-06-27">Update 2025-06-27</h2></a>
<p>The plot thickens!</p>
<p>After I shared this post, a couple folks on Bluesky gave some good tips!</p>
<ol>
<li>Enabling <code>-Zshare-generics</code>; and</li>
<li>Switching away from Alpine</li></ol>
<a href="#z-share-generics"></a>
<p>Piotr Osiewicz on Bluesky <a href="https://bsky.app/profile/osiewicz.bsky.social/post/3lsjiadbh4s27">suggested enabling <code>-Zshare-generics</code></a>:</p>
<blockquote>
<p>It&#39;ll reuse instantiations of generics from the dependencies of a crate. It is not enabled for release builds (by
default), as it has a negative impact on codegen.</p>
<p>[ ... ]</p>
<p>The flag is available only on nightly, but it is enabled for dev builds even when using a stable tool chain.</p>
</blockquote>
<p>Sounds neat! Let&#39;s try enabling it!</p>
<pre><code><span>RUSTFLAGS</span><span>=</span><span>&#34;-Zshare-generics -Cllvm-args=-inline-threshold=10 -Cllvm-args=-inlinedefault-threshold=10 -Cllvm-args=-inlinehint-threshold=10&#34;</span> <span>..</span>.
</code></pre>
<p>The end result is interesting — <strong>a drop in total compile time from 32.3s to 29.1s</strong>, even though many of the
<code>core::ptr::drop_in_place</code>s that we were compiling before are still present.</p>
<p>Looking at the biggest times, filtering just for <code>drop_in_place</code> for concrete types exposed by other crates:</p>
<pre><code><span><span>$</span> <span></span></span>
<span><span>$</span> <span><span>cat</span> <span>..</span>. <span>|</span> jq <span>..</span>. <span>\</span>
    <span>|</span> <span>grep</span> <span>-P</span> <span>&#39;core::ptr::drop_in_place::&lt;(?!web_http_server)[a-zA-Z0-9_:]+&gt;$&#39;</span> <span>\</span>
    <span>|</span> <span>head</span> <span>-n5</span></span></span>
<span>0.42s core::ptr::drop_in_place::&lt;tracing_subscriber::filter::directive::ParseError&gt;
0.13s core::ptr::drop_in_place::&lt;http::uri::Uri&gt;
0.12s core::ptr::drop_in_place::&lt;toml_edit::item::Item&gt;
0.11s core::ptr::drop_in_place::&lt;std::io::error::Error&gt;
0.1s core::ptr::drop_in_place::&lt;hyper::body::incoming::Incoming&gt;

</span><span><span>$</span> <span></span></span>
<span><span>$</span> <span><span>cat</span> <span>..</span>. <span>|</span> jq <span>..</span>. <span>\</span>
    <span>|</span> <span>grep</span> <span>-P</span> <span>&#39;core::ptr::drop_in_place::&lt;(?!web_http_server)[a-zA-Z0-9_:]+&gt;$&#39;</span> <span>\</span>
    <span>|</span> <span>head</span> <span>-n5</span></span></span>
<span>0.59s core::ptr::drop_in_place::&lt;hyper::ext::Protocol&gt;
0.28s core::ptr::drop_in_place::&lt;http::header::map::HeaderMap&gt;
0.1s core::ptr::drop_in_place::&lt;std::io::error::Error&gt;
0.09s core::ptr::drop_in_place::&lt;http::uri::Uri&gt;
0.08s core::ptr::drop_in_place::&lt;tokio::runtime::io::registration::Registration&gt;
</span></code></pre>
<p>There&#39;s some change between them, but it&#39;s still compiling the same <code>core::ptr::drop_in_place</code> for a lot of
dependencies!</p>
<p>In spite of that though, the optimization time decreased substantially — from 21.7s to 17.4s if you look just at the
<code>drop_in_place</code> instantiations, and far more if you look at everything together (128s to 104s; across multiple threads,
with LLVM profiling&#39;s overhead).</p>
<a href="#switching-from-alpine"><h3 id="switching-from-alpine">Switching away from Alpine</h3></a>
<p>Wesley Moore on Lobsters (<a href="https://bsky.app/profile/wezm.net/post/3lskc72tz2s24">via Bluesky</a>) <a href="https://lobste.rs/s/72hbqg/why_is_rust_compiler_so_slow#c_oah8c1">suggested switching away from Alpine</a>, due to the impact that the default
allocator can have on compile times:</p>
<blockquote>
<p>In my experience the allocator can have a big impact on build times. For example when Chimera Linux switched from
scudo (which was already better than the default musl allocator) to mimalloc a clean build of Gleam went from 67s to
46s.</p>
<p>Similar results can be observed by switching the base image of a build in docker [ ... ]</p>
</blockquote>
<p><strong>This made a huge difference.</strong></p>
<p>After switching out alpine for debian and removing the <code>--target=x86_64-unknown-linux-musl</code>, the total compile time
dropped from 29.1s all the way down to 9.1s!</p>
<a href="#final-recap"><h2 id="final-recap">Final recap</h2></a>
<ol>
<li>We started at ~175s </li>
<li>Disabling LTO (and debug symbols!) got us to 51s (-71%) </li>
<li>Changing to <code>opt-level = 1</code> on the final crate got us to 48.8s (-4%)</li>
<li>Reducing inlining with <code>-C llvm-args</code> got us to 40.7s (-16%) </li>
<li>Local changes got us to 37.7s (-7%) </li>
<li>Changes with dependencies got us to 32.3s (-14%) </li></ol>
<p><strong>Updated 2025-06-27:</strong></p>
<ol start="7">
<li>Enabling <code>-Zshare-generics</code> got us to 29.1s (-10%) </li>
<li>And switching away from alpine got us to 9.1s (-69%) </li></ol>
<a href="#what-now"><h3 id="what-now">What now?</h3></a>
<p>While I did hit a lot of issues here, the tooling honestly worked really well – and the documentation was sufficient for
someone with relatively little experience to make meaningful improvements to their codebase.</p>
<p>Some of the issues are straightforward: bugfixes to provide a nicer experience for the next person that finds themselves
in a similar mess.</p>
<p>Others are more complicated:</p>
<ul>
<li>
<p>The compile time of deep call graphs of async functions needs to be improved – perhaps LLVM has a degenerate edge
case that&#39;s easy to trigger with what <code>rustc</code> generates, or maybe it&#39;s as simple as a bad heuristic that&#39;s
under-utilized in other languages.</p>
</li>
<li>
<p>It <em>might</em> be worthwhile for <code>rustc</code> to special-case <code>core::ptr::drop_in_place&lt;T&gt;</code> so that it&#39;s compiled in the crate
that defines <code>T</code>. That approach wouldn&#39;t work for everything – for example, generic types – but would prevent
downstream crates from needing to re-compile the same destructor multiple times.</p>
<ul>
<li><strong>Update 2025-06-27:</strong> <code>-Zshare-generics</code> <em>does</em> help here, but it&#39;s not a complete fix. In the meantime, however,
I found that this is actually something that&#39;s <a href="https://github.com/rust-lang/rust/issues/84175">been previously discussed</a> — unfortunately it seems like
<a href="https://github.com/rust-lang/rust/pull/108838#issuecomment-1471619165">it causes severe increases in compile time</a> due to compiling all the drop glue that would otherwise be unused.
It&#39;s possible that there&#39;s some middle ground here (e.g., signaling that you want to prioritize compile times of
the final binary to the detriment of the dependencies), but it&#39;s hard to say what the right approach is.</li></ul>
</li>
<li>
<p>There might also be room for tooling to help with isolating which parts of a codebase are taking up the most time
during compilation (and providing recommendations to mitigate) – although that&#39;s a longer project than just this post.</p>
</li></ul>
<p><strong>In the meantime, setting <code>opt-level = 0</code> might be just fine :)</strong></p>
<hr/>
<p><em>(questions? comments? Feel free to reach out below!)</em></p>


</div>


    </div></div>
  </body>
</html>
