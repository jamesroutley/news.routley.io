<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggml-org/llama.cpp/pull/14737">Original</a>
    <h1>Mistral Integration Improved in Llama.cpp</h1>
    
    <div id="readability-page-1" class="page"><div>
        <div>
  
  <task-lists disabled="" sortable="">
    <div>
      <h2 dir="auto">Description</h2>
<p dir="auto">This PR aims to enhance the integration of Mistral models with llama.cpp by addressing several key issues and introducing new features. Here are the details:</p>
<h3 dir="auto">Context</h3>
<ul dir="auto">
<li>The current HF conversion to GGUF does not work directly for Mistral models due to our format that is vLLM based. This means that we have to first convert weights to Hugging Face then to GGUF which is not ideal and can lead to conversion errors if the first conversion is not done correctly. It also means that adding new models to the llama.cpp ecosystem requires first adding them to Transformers.</li>
<li>We do not support chat templates natively which means chat templates are community based and not guaranteed to work correctly.</li>
<li>We are using <a href="https://github.com/mistralai/mistral-common">mistral-common</a> internally for tokenization and want the community to use it to unlock full capacities of our models. As mistral-common is a Python library, we have opened a <a href="https://github.com/mistralai/mistral-common/pull/113" data-hovercard-type="pull_request" data-hovercard-url="/mistralai/mistral-common/pull/113/hovercard">PR</a> to add a REST API via FastAPI to make it easier for users who are not in the Python ecosystem.</li>
</ul>
<h3 dir="auto">Using mistral-common with llama.cpp</h3>
<p dir="auto">We recommend that users only use the <code>llama-server</code> tool with the <code>/completions</code> route of the server for now, as it is the only one that supports tokens input. We also advise users to set <code>return_tokens=True</code> in their requests to let <code>mistral-common</code> handle detokenization.</p>
<h3 dir="auto">Added features</h3>
<ol dir="auto">
<li>Model conversion:</li>
</ol>
<p dir="auto">We have added a script to convert Mistral models to GGUF directly from Hugging Face. This script is located at <code>convert_mistral_to_gguf.py</code> and can be used to convert Mistral models to GGUF format.</p>
<ol start="2" dir="auto">
<li>Model architecture:</li>
</ol>
<p dir="auto">We registered the Mistral architecture in llama.cpp to support Mistral models natively. This allows users to use Mistral models with llama.cpp without having to convert them to Hugging Face first.</p>
<h3 dir="auto">Known Limitations:</h3>
<p dir="auto">Our approach does not support multimodality:</p>
<ul dir="auto">
<li>mistral-common handles processing multimodal data but they cannot be passed to llama.cpp via the route.</li>
<li>llama.cpp only supports multimodality via chat templates, which we do not support.</li>
</ul>
<p dir="auto">Also this approach requires users to only use the llama.cpp server with the <code>/completions</code> route.</p>
<h2 dir="auto">Example Code</h2>
<p dir="auto">To get started, install mistral-common using the following command:</p>
<h4 dir="auto">(Optional) Convert the model</h4>
<div dir="auto" data-snippet-clipboard-copy-content="HF_TOKEN=... python convert_mistral_to_gguf.py \
mistralai/Devstral-Small-2505 --remote --ctx-train 131072 --outtype bf16"><pre><span>HF_TOKEN</span><span>=</span>... <span>python</span> <span>convert_mistral_to_gguf</span>.<span>py</span> \
<span>mistralai</span><span>/</span><span>Devstral</span><span>-</span><span>Small</span><span>-</span><span>2505</span> <span>-</span><span>-</span><span>remote</span> <span>-</span><span>-</span><span>ctx</span><span>-</span><span>train</span> <span>131072</span> <span>-</span><span>-</span><span>outtype</span> <span>bf16</span></pre></div>
<h4 dir="auto">Launch the mistral-common and llama.cpp servers</h4>
<div dir="auto" data-snippet-clipboard-copy-content="pip install git+https://github.com/mistralai/mistral-common.git@improve_llama_cpp_integration[server]"><pre>pip install git+https://github.com/mistralai/mistral-common.git@improve_llama_cpp_integration[server]</pre></div>
<p dir="auto">Launch the mistral-common server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="HF_TOKEN=... mistral_common mistralai/Devstral-Small-2505 --port 6000"><pre>HF_TOKEN=... mistral_common mistralai/Devstral-Small-2505 --port 6000</pre></div>
<p dir="auto">Launch the llama.cpp server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./build/bin/llama-server -m models/Devstral-Small-2505-Q4_K_M.gguf --port 8080"><pre>./build/bin/llama-server -m models/Devstral-Small-2505-Q4_K_M.gguf --port 8080</pre></div>
<h4 dir="auto">Use the servers</h4>
<p dir="auto">Here is a code snippet demonstrating how to use the new features:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import requests

mistral_common_url = &#34;http://127.0.0.1:6000&#34;
llama_cpp_url = &#34;http://127.0.0.1:8080&#34;

def tokenize(messages, url):
    response = requests.post(f&#34;{url}/tokenize/messages&#34;, json=messages)
    return response.json()

def detokenize(tokens, url):
    response = requests.post(f&#34;{url}/detokenize&#34;, json={&#34;tokens&#34;: tokens})
    return response.json()

def detokenize_message(tokens, url):
    response = requests.post(f&#34;{url}/detokenize&#34;, json={&#34;tokens&#34;: tokens, &#34;as_message&#34;: True})
    return response.json()

def generate(tokens, url):
    response = requests.post(f&#34;{url}/completions&#34;, json={
        &#34;prompt&#34;: tokens,
        &#34;stream&#34;: False,
        &#34;return_tokens&#34;: True
    })
    return response.json()

messages = [
    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are Devstral a cool coding agent that can help users with their coding needs.&#34;},
    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who are you and what can you do?&#34;}
]

tokens = tokenize(messages, mistral_common_url)
print(tokens)

generated = generate(tokens, llama_cpp_url)[&#34;tokens&#34;]
print(generated)

detokenized = detokenize(generated, mistral_common_url)
print(detokenized)

detokenized_message = detokenize_message(generated, mistral_common_url)
print(detokenized_message)"><pre><span>import</span> <span>requests</span>

<span>mistral_common_url</span> <span>=</span> <span>&#34;http://127.0.0.1:6000&#34;</span>
<span>llama_cpp_url</span> <span>=</span> <span>&#34;http://127.0.0.1:8080&#34;</span>

<span>def</span> <span>tokenize</span>(<span>messages</span>, <span>url</span>):
    <span>response</span> <span>=</span> <span>requests</span>.<span>post</span>(<span>f&#34;<span><span>{</span><span>url</span><span>}</span></span>/tokenize/messages&#34;</span>, <span>json</span><span>=</span><span>messages</span>)
    <span>return</span> <span>response</span>.<span>json</span>()

<span>def</span> <span>detokenize</span>(<span>tokens</span>, <span>url</span>):
    <span>response</span> <span>=</span> <span>requests</span>.<span>post</span>(<span>f&#34;<span><span>{</span><span>url</span><span>}</span></span>/detokenize&#34;</span>, <span>json</span><span>=</span>{<span>&#34;tokens&#34;</span>: <span>tokens</span>})
    <span>return</span> <span>response</span>.<span>json</span>()

<span>def</span> <span>detokenize_message</span>(<span>tokens</span>, <span>url</span>):
    <span>response</span> <span>=</span> <span>requests</span>.<span>post</span>(<span>f&#34;<span><span>{</span><span>url</span><span>}</span></span>/detokenize&#34;</span>, <span>json</span><span>=</span>{<span>&#34;tokens&#34;</span>: <span>tokens</span>, <span>&#34;as_message&#34;</span>: <span>True</span>})
    <span>return</span> <span>response</span>.<span>json</span>()

<span>def</span> <span>generate</span>(<span>tokens</span>, <span>url</span>):
    <span>response</span> <span>=</span> <span>requests</span>.<span>post</span>(<span>f&#34;<span><span>{</span><span>url</span><span>}</span></span>/completions&#34;</span>, <span>json</span><span>=</span>{
        <span>&#34;prompt&#34;</span>: <span>tokens</span>,
        <span>&#34;stream&#34;</span>: <span>False</span>,
        <span>&#34;return_tokens&#34;</span>: <span>True</span>
    })
    <span>return</span> <span>response</span>.<span>json</span>()

<span>messages</span> <span>=</span> [
    {<span>&#34;role&#34;</span>: <span>&#34;system&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;You are Devstral a cool coding agent that can help users with their coding needs.&#34;</span>},
    {<span>&#34;role&#34;</span>: <span>&#34;user&#34;</span>, <span>&#34;content&#34;</span>: <span>&#34;Who are you and what can you do?&#34;</span>}
]

<span>tokens</span> <span>=</span> <span>tokenize</span>(<span>messages</span>, <span>mistral_common_url</span>)
<span>print</span>(<span>tokens</span>)

<span>generated</span> <span>=</span> <span>generate</span>(<span>tokens</span>, <span>llama_cpp_url</span>)[<span>&#34;tokens&#34;</span>]
<span>print</span>(<span>generated</span>)

<span>detokenized</span> <span>=</span> <span>detokenize</span>(<span>generated</span>, <span>mistral_common_url</span>)
<span>print</span>(<span>detokenized</span>)

<span>detokenized_message</span> <span>=</span> <span>detokenize_message</span>(<span>generated</span>, <span>mistral_common_url</span>)
<span>print</span>(<span>detokenized_message</span>)</pre></div>
<h2 dir="auto">Feedback and Contributions</h2>
<p dir="auto">We believe these changes will significantly improve the integration of Mistral models with llama.cpp and provide a better experience for our users. We welcome any feedback or suggestions to further enhance this integration. Also, as we have few experience in the codebase of llama.cpp, we welcome any help to improve the integration and make sure we respect the codebase and the community.</p>
    </div>
  </task-lists>
  
</div>

      </div></div>
  </body>
</html>
