<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/SubscriberLink/993828/eb9b437bf7604da3/">Original</a>
    <h1>Kernel optimization with BOLT (binary optimization and layout tool)</h1>
    
    <div id="readability-page-1" class="page"><div>

<p>
A pair of talks in the <a href="https://lpc.events/event/18/sessions/180/#20240918">toolchains
track</a> at the <a href="https://lpc.events/event/18/page/224-lpc-2024-overview">2024 Linux
Plumbers Conference</a> covered different tools that can be used to
optimize the kernel.  First up was Maksim Panchenko to describe the <a href="https://github.com/llvm/llvm-project/tree/main/bolt#bolt">binary
optimization and layout tool</a> (BOLT) that Meta uses on its production
kernels.  It optimizes the kernel binary by rearranging it to improve its
code locality for
better performance.  A <a href="https://lwn.net/Articles/995397/">subsequent article</a> will cover the second talk, which
looked at  <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45290.pdf">automatic
feedback-directed optimization</a> (AutoFDO) and other related techniques
that are used to optimize Google&#39;s kernels.
</p>

<p>
Panchenko began with a slide
showing a handful of companies and projects that use BOLT, which can be
seen in his <a href="https://lpc.events/event/18/contributions/1921/attachments/1465/3154/BOLT%20for%20Linux%20Kernel%20LPC%202024%20Final.pdf">slides</a>
or the
<a href="https://www.youtube.com/watch?v=p-u3e-1frnw&amp;t=57s">YouTube
video</a> of the talk.  It was designed
at first for large applications at Meta, but it turned out to also
accelerate compilers.  So, for example, it is used by Python since version 3.12; it is also used by LLVM, Rust, and others.
</p>

<p>
If you look back to ten years ago in the open-source world, getting the
maximum performance  from an application was a
matter of using GCC or Clang with <tt>-O3</tt>, <a href="https://en.wikipedia.org/wiki/Profile-guided_optimization">profile-guided
optimization</a> (PGO), and <a href="https://en.wikipedia.org/wiki/Interprocedural_optimization">link-time
optimization</a> (LTO).  But applying PGO was &#34;<q>kind of painful</q>&#34;, he
said, so only those who cared a lot about performance were using it.
Applying PGO to the kernel was even more painful, so most companies just used
<tt>-O2</tt> or <tt>-O3</tt> on their kernels.
</p>

<blockquote>
The staff here at LWN.net really appreciate the subscribers who make
our work possible. Is there a chance we could interest you in <a href="https://lwn.net/Promo/daroc2/claim">becoming one of them</a>?
</blockquote>
<p>
The main Meta application is a virtual machine running PHP code; around 2015, developers there came up with the idea of a binary optimizer that
would work with both GCC and Clang to speed up the generated code.  BOLT
turned out to exceed developers&#39; expectations;
it gave large gains and was able to accelerate the compilers themselves.
It has been used at Meta since 2016; &#34;<q>most of the cycles are spent in
binaries optimized by BOLT</q>&#34; throughout the Meta fleet.  BOLT was
released as open source, under the Apache 2.0 license, in 2018 and has been part of LLVM since 2022.
</p>

<p><a href="https://lwn.net/Articles/995370/">
<img src="https://static.lwn.net/images/2024/lpc-panchenko-sm.png" alt="[Maksim Panchenko]" title="Maksim Panchenko" width="239" height="280"/>
</a></p><p>
Generally, developers think about how the data structures for their programs will be
arranged in memory; it is less common for them to consider how the code
is arranged.  There are exceptions, including the Linux kernel developers,
but most times the focus is on the data cache.  The instruction cache is
much smaller than the data cache and has not grown much over time,
maybe doubling from 32KB to 64KB on Intel CPUs over the last 20 years.
But, for large applications that do not spend most of their time in tight
loops, the layout of code in memory matters a lot, so BOLT can make a major
difference, he said.
</p>

<p>
Compilers do not have enough information to optimally place code, even when
they have profiling information; it turns out that inlining functions
changes the profile.  The profile information may point to a function
<tt>foo()</tt> that is called a lot, but when it is inlined, that
information is not present.  BOLT operates on the binary to observe the
code that is being frequently executed so that all of it can be placed
close together in memory. Once the <a href="https://research.facebook.com/publications/bolt-a-practical-binary-optimizer-for-data-centers-and-beyond/">BOLT
paper</a> was released in 2019, he said, other efforts, including
<a href="https://research.google/pubs/propeller-a-profile-guided-relinking-optimizer-for-warehouse-scale-applications/">Propeller</a>, have come about; they are generally tied to a single toolchain,
though, while BOLT can be used with GCC or Clang and with different
linkers.
</p>

<p>
He showed a sort of heat map of the memory layout of the <a href="https://hhvm.com/">HHVM</a> runtime, which is what is used by Meta
for most of its workloads.  One image showed hot code spread
all over, while the post-BOLT image showed all of the hot code
confined to the same small region of memory.  That drastically reduces
instruction-cache misses, translation-lookaside buffer (TLB) misses, and
CPU time; for HHVM, BOLT produced a 7% performance increase, Panchenko said.
</p>

<p>
BOLT is a post-link optimizer that runs on ELF binaries, such as
<tt>vmlinux</tt>.  Even though it is part of the LLVM project, BOLT still
supports GCC code; it also supports the &#34;<q>most popular</q>&#34;
architectures: x86_64, Arm64, and RISC-V.
</p>

<p>
Applying BOLT to the kernel took some time, mostly in the form of ensuring
that the resulting kernel would run and not crash.  One of the big problems
encountered was in finding a good benchmark to use to measure the impact of
BOLT.  There are lots of different micro-benchmarks available, but &#34;<q>I
couldn&#39;t find any scalable, large-scale benchmark</q>&#34;.  In the end, he
used the <a href="https://github.com/facebook/rocksdb/wiki/Benchmarking-tools">RocksDB
db_bench fillseq</a> benchmark, which showed a 2.5% improvement just by
switching to a BOLT-optimized kernel.  He and others at Meta ran one
of the company&#39;s main services on a BOLT-optimized kernel that produced a 2% queries-per-second (QPS) improvement, &#34;<q>which was quite significant</q>&#34;. 
</p>

<p>
BOLT only changes branches and the location of the <a href="https://en.wikipedia.org/wiki/Basic_block">basic blocks</a> in the
binary to achieve its improvements.  Most of the time, there is no need to
recompile applications in order to apply BOLT; the exception is for
programs built with <a href="https://lists.llvm.org/pipermail/llvm-dev/2020-August/144012.html">split
functions</a>, which BOLT can do better than the compiler.  The application
does need to be relinked in order to produce relocation information.  With
that, and a profile of the kernel running a representative workload, BOLT
will only take around four seconds to optimize <tt>vmlinux</tt>, he said.
</p>

<p>
The profile can be generated with a variety of mechanisms, including the
last branch record (LBR) feature on Intel platforms and similar
branch-sampling features on other architectures.  If that is not available,
the code can be instrumented to gather the needed information, but that has
higher overhead than using LBR and the like.  There are other options, but
the profile quality, thus the BOLT optimizations, will not be as good.
</p>

<p>
BOLT needs an unstripped <tt>vmlinux</tt> binary because it uses the symbol
names for code discovery, Panchenko said.  BOLT can easily identify the
boundaries of code and data in the ELF binary using the text and data
segments that are defined.  The segments are further divided into sections
and BOLT uses the symbol-table information to identify the individual
functions therein.
</p>

<p>
Then BOLT disassembles the functions, though, unlike <a href="https://man7.org/linux/man-pages/man1/objdump.1.html">objdump</a>,
it will &#34;<q>symbolize the operands of the instructions</q>&#34;.
Distinguishing between constants and addresses in the instructions
can be a problem, however.  The relocation information that is inserted by the
linker helps with that; it can be used to &#34;<q>effectively do symbolic
disassembly</q>&#34; of the code.  
</p>

<p>
The resulting instruction stream can be optimized
using normal techniques, such as peephole optimization, but &#34;<q>this is not
very efficient</q>&#34;.  In order to do more optimizations on the code, an
intermediate representation (IR) of some kind is needed. The IR used is
&#34;<q>essentially a control-flow graph on top of MC instructions</q>&#34;,
Panchenko said; he was referring to the <a href="https://blog.llvm.org/2010/04/intro-to-llvm-mc-project.html">LLVM
machine code (MC) project</a>, which provides a number of tools and
libraries that BOLT uses.  The instructions look much like the assembly
code, but some may be annotated or modified to identify tail calls versus
other kinds of jumps, for example. &#34;<q>So if you look at BOLT disassembly,
you will have a much better idea of what&#39;s happening in your application
compared to regular objdump</q>&#34;.
</p>

<p>
BOLT uses the profile information for the basic blocks in the control-flow graph
in order to determine where the hot code resides.  To make the best
code-layout decisions, though, having weights on the edges, rather than
just execution counts for the basic blocks, would be useful.  The LBR
profiling can provide that information, but BOLT can recover some
information about edge weights even without it.
</p>

<p>
Then the graph is used to optimize the code; &#34;<q>The main optimization that 
gives us most of the gains is code reordering.</q>&#34;  The basic blocks can
be grouped together to reduce the instruction-cache footprint of the code.
That is done by breaking up the functions into fragments, some of which are
hot code and others that are rarely or never executed (e.g. error-handling
code).  Compilers already do reordering, but on the function level; BOLT
takes it one step further and reorders these function fragments.
</p>

<p>
Once the new code is generated, there is a question about where to put it,
he said.  Unless there is a big concern about disk space, it is more efficient to simply create a new text segment that
contains the hot code, which will generally be quite a bit smaller (for a
binary that is huge, &#34;<q>hundreds of megabytes</q>&#34;, 20MB or less of hot
code would end up in the new segment).  So it is not much overhead, in
terms of disk space, and &#34;<q>you get a <i>much</i> faster
application</q>&#34;.
</p>

<p>
Adding another
text segment to the kernel binary may not be viable, he said, so he turned
to an alternative that is &#34;<q>much more feasible</q>&#34;.
BOLT
will simply rewrite the existing functions in the binary; those functions are already
ordered by the compiler based on its analysis, so BOLT effectively uses
that.  BOLT could do a bit better function ordering based on the profile, but
that small gain can be sacrificed in order to easily get the basic-block
reordering, he said.  It also helps avoid over-specializing the code for
only the workload measured by the profile; for other workloads that were
not measured, some of the cold code will be executed, but it will still be
located nearby due to the compiler choices.
</p>

<p>
The kernel provides other challenges, because its code is modified at boot
time and also while it is running.  He spent a good chunk of the talk going
into the details of how that type of code is handled.  Things like static
calls,
SMP locks (that are patched out on uniprocessor systems), static keys, and alternative instructions for different
subarchitectures are handled with annotations in the disassembled code, which
is part of the metadata that BOLT uses to do its job.  For some, BOLT
simply does not operate on them, while others have mechanisms that the
optimizer can use on them.  All of that metadata can be dumped using BOLT tools.
</p>

<p>
Panchenko said that he was skipping over some topics, such as continuous
profiling, which can be applied to BOLT-optimized binaries as they run;
a new version of a binary can be produced that will reflect changes
in the code and the workload.  He also did not cover any of the other
optimizations that BOLT applies.  He finished by showing some of the output
that running BOLT produces, noting that a four-second demo of it operating would
not be all that interesting.
</p>

<p>
[ I would like to thank LWN&#39;s travel sponsor, the Linux Foundation, for
travel assistance to Vienna for the Linux Plumbers Conference. ]
</p></div></div>
  </body>
</html>
