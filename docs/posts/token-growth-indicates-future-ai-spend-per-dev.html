<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.kilocode.ai/p/future-ai-spend-100k-per-dev">Original</a>
    <h1>Token growth indicates future AI spend per dev</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><article><div><div><div dir="auto"><p>Kilo just broke through the 1 trillion tokens a month barrier on OpenRouter for the first time. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!v9-s!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!v9-s!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 424w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 848w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1272w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png" width="500" height="187.5" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:546,&#34;width&#34;:1456,&#34;resizeWidth&#34;:500,&#34;bytes&#34;:237722,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:&#34;https://blog.kilocode.ai/i/170429285?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!v9-s!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 424w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 848w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1272w, https://substackcdn.com/image/fetch/$s_!v9-s!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71ab674a-e015-4079-9d34-d0236ec96153_1930x724.png 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a></figure></div><p>Each of the open source family of AI coding tools (Cline, Roo, Kilo) is growing rapidly this month.</p><div><figure><a target="_blank" href="https://x.com/Kilo_Code/status/1953767203175543246" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!IDuV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 424w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 848w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1272w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png" width="728" height="248.5" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:497,&#34;width&#34;:1456,&#34;resizeWidth&#34;:728,&#34;bytes&#34;:386444,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:&#34;https://x.com/Kilo_Code/status/1953767203175543246&#34;,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:&#34;https://blog.kilocode.ai/i/170429285?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png&#34;,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!IDuV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 424w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 848w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1272w, https://substackcdn.com/image/fetch/$s_!IDuV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67a9212f-3d94-4d87-99ec-6b6df3c065ec_2544x868.png 1456w" sizes="100vw"/></picture></div></a></figure></div><p><span>Part of this growth is caused by Cursor and Claude starting to throttle their users. We wrote about </span><a href="https://blog.kilocode.ai/p/cursors-500-requests-unlimited-225" rel="">Cursor at the beginning of July</a><span> and about </span><a href="https://blog.kilocode.ai/p/the-ai-pricing-bait-and-switch" rel="">Claude in the second half of July</a><span>. Their throttling </span><a href="https://www.reddit.com/r/kilocode/comments/1mcdxr4/amazed_by_kilo_or_where_will_all_the_coders_go/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button" rel="">sent users to the open source family of AI coding tools</a><span> causing the increases you see in the graphs above. Cursor and Claude needed to throttle because the industry made a flawed assumption.</span></p><p>The industry expected that because the raw inference costs were coming down fast, the applications inference costs would come down fast as well but this assumption was wrong.</p><p><a href="https://a16z.com/llmflation-llm-inference-cost/" rel="">Raw inference costs did decrease by 10x year-over-year.</a><span> This expectation made startups bet on a business model where companies could afford to sell subscriptions at significant losses, knowing they&#39;d achieve healthy margins as costs plummeted.</span></p><p><a href="https://x.com/dobroslav_dev/status/1952369863344673194" rel="">Cursor&#39;s Ultra plan</a><span> exemplified this approach perfectly: charge users $200 while providing at least $400 worth of tokens, essentially operating at -100% gross margin.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!sMJm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!sMJm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 424w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 848w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1272w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png" width="542" height="436.5346869712352" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:952,&#34;width&#34;:1182,&#34;resizeWidth&#34;:542,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!sMJm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 424w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 848w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1272w, https://substackcdn.com/image/fetch/$s_!sMJm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd54bb2ba-591c-4b93-87bb-ab8776ff059b_1182x952.png 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p>The bet was that by the following year, the application inference would cost 90% less, creating a $160 gross profit (+80% gross margins). But this didn&#39;t happen, instead of declining the application inference costs actually grew!</p><p>Application inference costs increased for two reasons: the frontier model costs per token stayed constant and the token consumption per application grew a lot. We&#39;ll first dive into the reasons for the constant token price for frontier models and end with explaining the token consumption per application.</p><p><a href="https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed" rel="">The price per token for the frontier model stayed constant</a><span> because of the </span><a href="https://www.researchgate.net/figure/The-most-popular-large-AI-models-of-recent-years-This-figure-shows-the-main-features-of_fig1_387458379" rel="">increasing size of models</a><span> and more test-time scaling. Test time scaling, also called long thinking, is the </span><a href="https://blogs.nvidia.com/blog/ai-scaling-laws/" rel="">third way to scale AI</a><span> as shown in the graphic below.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!i474!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i474!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 424w, https://substackcdn.com/image/fetch/$s_!i474!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 848w, https://substackcdn.com/image/fetch/$s_!i474!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg" width="624" height="351" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/b558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:819,&#34;width&#34;:1456,&#34;resizeWidth&#34;:624,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null,&#34;isProcessing&#34;:false,&#34;align&#34;:null,&#34;offset&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!i474!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 424w, https://substackcdn.com/image/fetch/$s_!i474!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 848w, https://substackcdn.com/image/fetch/$s_!i474!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!i474!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb558bd73-0244-415d-aab5-81934c719436_1456x819.jpeg 1456w" sizes="100vw" loading="lazy"/></picture><div><div></div></div></div></a></figure></div><p><span>While the pre- and post-training scaling influenced only the training costs of models. But this test-time scaling increases the cost of inference. Thinking models like OpenAI&#39;s o1 series allocate massive computational effort during inference itself. These models can require </span><strong>over 100x compute for challenging queries</strong><span> compared to traditional single-pass inference.</span></p><p><span>Token consumption per application grew a lot because models allowed for </span><a href="https://www.meibel.ai/post/understanding-the-impact-of-increasing-llm-context-windows" rel="">longer context windows</a><span> and bigger suggestions from the models. The combination of a steady price per token and more token consumption caused app inference costs to grow about 10x over the last two years. Market leader Cursor introduced a $200 plan where before $20 was the default. The $200 plan has also been followed by Claude Code and others.</span></p><p>The top end of subscriptions is $200 today but power users find that they are extensively throttled if they use a lot of inference. That throttling comes in the form of rate limiting, using lower quality models, context window compression, and other techniques.</p><p>If you don&#39;t want to be throttled you need to pay for inference yourself. The open source family of coding tools (Cline, Roo, Kilo) is based on that principle: “never throttle the user”. Because the users directly see the costs these tools have also led the way in reducing costs by allowing the users to:</p><ol><li><p>Splitting work up in many smaller tasks that can each be run efficiently.</p></li><li><p>Using different modes, in Kilo we have an Orchestrator, Architect, Code, and Debug mode.</p></li><li><p>Combine closed-source models for architecting tasks (e.g. Sonnet 4) and open-source for coding (Qwen3)</p></li><li><p>Enhance the prompt with AI before submitting it</p></li><li><p>Optimize context efficiency with memory banks</p></li><li><p>Enable prompt caching</p></li><li><p>Allow termination of a running task when the model hallucinates</p></li></ol><p>Despite the efforts to reduce costs we do expect them to continue to grow for the power users.</p><p>We expect app inference costs to grow quickly. This is driven by two developments: more parallel agents and more work done before human feedback is needed.</p><p><span>People are </span><a href="https://www.reddit.com/r/ClaudeAI/comments/1kwm4gm/has_anyone_tried_parallelizing_ai_coding_agents/" rel="">experimenting</a><span> </span><a href="https://ainativedev.io/news/how-to-parallelize-ai-coding-agents" rel="">with</a><span> </span><a href="https://google.github.io/adk-docs/agents/workflow-agents/parallel-agents/" rel="">parallel</a><span> AI coding agents today with </span><a href="https://www.warp.dev/" rel="">Warp already having it available to people</a><span>. We expect parallel agents to become the default in the industry and look forward to introduce them in Kilo code sooner rather than later. This will greatly increase token consumption per human hour.</span></p><p>Agents are also able to work longer before needing human feedback. Because they are working more and pausing less this also increases token consumption per human hour.</p><p><span>Both effects together will push costs at the top level to $100k a year. Spending that magnitude of money on software is not without precedent, </span><a href="https://news.ycombinator.com/item?id=26658405" rel="">chip design licenses from Cadence or Synopsys are already $250k a year</a><span>.</span></p><p>While the prospect of $100k+ per year in costs is a lot it can always be worse.</p><p><span>AI costs for most engineers are approximately 1000x smaller than what is happening at the AI training stage. Here the costs for the normal &#39;inference engineer&#39; is dwarfed by the thousand times bigger impact of the AI ‘training engineer’. The ‘inference engineer’ we talked about above might make $100k and use $100k to be many times more productive than an engineer before AI. A top ‘training engineer’ directs $100m in spend and is paid $100m a year. Top frontier labs spend </span><a href="https://www.datacenterdynamics.com/en/news/openai-training-and-inference-costs-could-reach-7bn-for-2024-ai-startup-set-to-lose-5bn-report/" rel="">billions on AI training</a><span> and this compute work is directed by a handful of people. Mark Zuckerberg is rumored to have offered these people ‘signing bonuses’ of </span><a href="https://www.wsj.com/tech/ai/meta-ai-recruiting-mark-zuckerberg-5c231f75" rel="">$100m</a><span> to </span><a href="https://www.wsj.com/tech/ai/meta-zuckerberg-ai-recruiting-fail-e6107555" rel="">$1b</a><span> with unknown contract lengths. The difference in pay between inference and training engineers is because of their relative impact. You train a model with a handful of people while it is used by millions of people.</span></p></div></div></div></article></div></div></div><div><div id="discussion"><div><h4>Discussion about this post</h4></div></div></div></div>
  </body>
</html>
