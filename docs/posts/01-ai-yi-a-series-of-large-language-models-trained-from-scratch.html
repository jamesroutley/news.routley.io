<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/01-ai/Yi">Original</a>
    <h1>01-AI/Yi: A series of large language models trained from scratch</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<h2 tabindex="-1" id="user-content-introduction" dir="auto"><a href="#introduction">Introduction<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">The <strong>Yi</strong> series models are large language models trained from scratch by
developers at <a href="https://01.ai/" rel="nofollow">01.AI</a>. The first public release contains two
bilingual (English/Chinese) base models with the parameter sizes of 6B and 34B.
Both of them are trained with 4K sequence length and can be extended to 32K
during inference time.</p>
<h2 tabindex="-1" id="user-content-news" dir="auto"><a href="#news">News<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<ul dir="auto">
<li>ðŸŽ¯ <strong>2023/11/02</strong>: The base model of <code>Yi-6B</code> and <code>Yi-34B</code>.</li>
</ul>
<h2 tabindex="-1" id="user-content-model-performance" dir="auto"><a href="#model-performance">Model Performance<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>MMLU</th>
<th>CMMLU</th>
<th>C-Eval</th>
<th>GAOKAO</th>
<th>BBH</th>
<th>Common-sense Reasoning</th>
<th>Reading Comprehension</th>
<th>Math &amp; Code</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>5-shot</td>
<td>5-shot</td>
<td>5-shot</td>
<td>0-shot</td>
<td>3-shot@1</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>LLaMA2-34B</td>
<td>62.6</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>44.1</td>
<td>69.9</td>
<td>68.0</td>
<td>26.0</td>
</tr>
<tr>
<td>LLaMA2-70B</td>
<td>68.9</td>
<td>53.3</td>
<td>-</td>
<td>49.8</td>
<td>51.2</td>
<td>71.9</td>
<td>69.4</td>
<td>36.8</td>
</tr>
<tr>
<td>Baichuan2-13B</td>
<td>59.2</td>
<td>62.0</td>
<td>58.1</td>
<td>54.3</td>
<td>48.8</td>
<td>64.3</td>
<td>62.4</td>
<td>23.0</td>
</tr>
<tr>
<td>Qwen-14B</td>
<td>66.3</td>
<td>71.0</td>
<td>72.1</td>
<td>62.5</td>
<td>53.4</td>
<td>73.3</td>
<td>72.5</td>
<td><strong>39.8</strong></td>
</tr>
<tr>
<td>Skywork-13B</td>
<td>62.1</td>
<td>61.8</td>
<td>60.6</td>
<td>68.1</td>
<td>41.7</td>
<td>72.4</td>
<td>61.4</td>
<td>24.9</td>
</tr>
<tr>
<td>InternLM-20B</td>
<td>62.1</td>
<td>59.0</td>
<td>58.8</td>
<td>45.5</td>
<td>52.5</td>
<td>78.3</td>
<td>-</td>
<td>30.4</td>
</tr>
<tr>
<td>Aquila-34B</td>
<td>67.8</td>
<td>71.4</td>
<td>63.1</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Falcon-180B</td>
<td>70.4</td>
<td>58.0</td>
<td>57.8</td>
<td>59.0</td>
<td>54.0</td>
<td>77.3</td>
<td>68.8</td>
<td>34.0</td>
</tr>
<tr>
<td>Yi-6B</td>
<td>63.2</td>
<td>75.5</td>
<td>72.0</td>
<td>72.2</td>
<td>42.8</td>
<td>72.3</td>
<td>68.7</td>
<td>19.8</td>
</tr>
<tr>
<td><strong>Yi-34B</strong></td>
<td><strong>76.3</strong></td>
<td><strong>83.7</strong></td>
<td><strong>81.4</strong></td>
<td><strong>82.8</strong></td>
<td><strong>54.3</strong></td>
<td><strong>80.1</strong></td>
<td><strong>76.4</strong></td>
<td>37.1</td>
</tr>
</tbody>
</table>
<p dir="auto">While benchmarking open-source models, we have observed a disparity between the
results generated by our pipeline and those reported in public sources (e.g.
OpenCompass). Upon conducting a more in-depth investigation of this difference,
we have discovered that various models may employ different prompts,
post-processing strategies, and sampling techniques, potentially resulting in
significant variations in the outcomes. Our prompt and post-processing strategy
remains consistent with the original benchmark, and greedy decoding is employed
during evaluation without any post-processing for the generated content. For
scores that were not reported by the original authors (including scores reported
with different settings), we try to get results with our pipeline.</p>
<p dir="auto">To evaluate the model&#39;s capability extensively, we adopted the methodology
outlined in Llama2. Specifically, we included PIQA, SIQA, HellaSwag, WinoGrande,
ARC, OBQA, and CSQA to assess common sense reasoning. SquAD, QuAC, and BoolQ
were incorporated to evaluate reading comprehension. CSQA was exclusively tested
using a 7-shot setup, while all other tests were conducted with a 0-shot
configuration. Additionally, we introduced GSM8K (8-shot@1), MATH (4-shot@1),
HumanEval (0-shot@1), and MBPP (3-shot@1) under the category &#34;Math &amp; Code&#34;. Due
to technical constraints, we did not test Falcon-180 on QuAC and OBQA; the score
is derived by averaging the scores on the remaining tasks. Since the scores for
these two tasks are generally lower than the average, we believe that
Falcon-180B&#39;s performance was not underestimated.</p>
<h2 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">Feel free to <a href="https://github.com/01-ai/Yi/issues/new">create an issue</a> if you
encounter any problem when using the <strong>Yi</strong> series models.</p>
<h3 tabindex="-1" id="user-content-1-prepare-development-environment" dir="auto"><a href="#1-prepare-development-environment">1. Prepare development environment<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">The best approach to try the <strong>Yi</strong> series models is through Docker with GPUs. We
provide the following docker images to help you get started.</p>


<p dir="auto">Note that the <code>latest</code> tag always points to the latest code in the <code>main</code>
branch. To test a stable version, please replace it with a specific
<a href="https://github.com/01-ai/Yi/tags">tag</a>.</p>
<p dir="auto">If you prefer trying out with your local development environment. First, create
a virtual environment and clone this repo. Then install the dependencies with
<code>pip install -r requirements.txt</code>. For the best performance, we recommend you
also install the latest version (<code>&gt;=2.3.3</code>) of
<a href="https://github.com/Dao-AILab/flash-attention#installation-and-features">flash-attention</a>.</p>
<h3 tabindex="-1" id="user-content-2-download-the-model-optional" dir="auto"><a href="#2-download-the-model-optional">2. Download the model (optional)<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<p dir="auto">By default the model weights and tokenizer will be downloaded from
<a href="https://huggingface.co/01-ai" rel="nofollow">HuggingFace</a> automatically in the next step. You
can also download them manually from the following places:</p>
<ul dir="auto">
<li><a href="https://www.modelscope.cn/organization/01ai/" rel="nofollow">ModelScope</a></li>
<li>Mirror site (remember to extract the content with <code>tar</code>)
<ul dir="auto">
<li><a href="https://storage.lingyiwanwu.com/yi/models/Yi-6B.tar" rel="nofollow">Yi-6B.tar</a></li>
<li><a href="https://storage.lingyiwanwu.com/yi/models/Yi-34B.tar" rel="nofollow">Yi-34B.tar</a></li>
</ul>
</li>
</ul>
<h3 tabindex="-1" id="user-content-3-examples" dir="auto"><a href="#3-examples">3. Examples<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h3>
<h4 tabindex="-1" id="user-content-31-use-the-base-model" dir="auto"><a href="#31-use-the-base-model">3.1 Use the base model<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="python demo/text_generation.py"><pre>python demo/text_generation.py</pre></div>
<p dir="auto">To reuse the downloaded models in the previous step, you can provide the extra
<code>--model</code> argument:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python demo/text_generation.py  --model /path/to/model"><pre>python demo/text_generation.py  --model /path/to/model</pre></div>
<p dir="auto">Or if you&#39;d like to get your hands dirty:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(&#34;01-ai/Yi-34B&#34;, device_map=&#34;auto&#34;, torch_dtype=&#34;auto&#34;, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(&#34;01-ai/Yi-34B&#34;, trust_remote_code=True)
inputs = tokenizer(&#34;There&#39;s a place where time stands still. A place of breath taking wonder, but also&#34;, return_tensors=&#34;pt&#34;)
outputs = model.generate(inputs.input_ids.cuda(), max_new_tokens=256)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>&#34;01-ai/Yi-34B&#34;</span>, <span>device_map</span><span>=</span><span>&#34;auto&#34;</span>, <span>torch_dtype</span><span>=</span><span>&#34;auto&#34;</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>&#34;01-ai/Yi-34B&#34;</span>, <span>trust_remote_code</span><span>=</span><span>True</span>)
<span>inputs</span> <span>=</span> <span>tokenizer</span>(<span>&#34;There&#39;s a place where time stands still. A place of breath taking wonder, but also&#34;</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>)
<span>outputs</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>inputs</span>.<span>input_ids</span>.<span>cuda</span>(), <span>max_new_tokens</span><span>=</span><span>256</span>)
<span>print</span>(<span>tokenizer</span>.<span>decode</span>(<span>outputs</span>[<span>0</span>], <span>skip_special_tokens</span><span>=</span><span>True</span>))</pre></div>
<details>
<summary>Output</summary>
<p dir="auto"><strong>Prompt</strong>: There&#39;s a place where time stands still. A place of breath taking wonder, but also</p>
<p dir="auto"><strong>Generation</strong>: There&#39;s a place where time stands still. A place of breath taking wonder, but also of great danger. A place where the very air you breathe could kill you. A place where the only way to survive is to be prepared.
The place is called the Arctic.
The Arctic is a vast, frozen wilderness. It is a place of extremes. The temperatures can drop to -40 degrees Celsius. The winds can reach speeds of 100 kilometers per hour. The sun can shine for 24 hours a day, or not at all for weeks on end.
The Arctic is also a place of great beauty. The ice and snow are a pristine white. The sky is a deep blue. The sunsets are spectacular.
But the Arctic is also a place of great danger. The ice can be treacherous. The winds can be deadly. The sun can be blinding.
The Arctic is a place where the only way to survive is to be prepared.
The Arctic is a place of extremes. The temperatures can drop to -40 degrees Celsius. The winds can reach speeds of 100 kilometers per hour. The sun can shine for 24 hours a day, or not at all for weeks on end.
The Arctic is a place of great beauty. The ice and snow are a</p>
</details>
<p dir="auto">For more advanced usage, please refer the
<a href="https://github.com/01-ai/Yi/tree/main/demo">doc</a>.</p>
<h4 tabindex="-1" id="user-content-32-finetuning-from-the-base-model" dir="auto"><a href="#32-finetuning-from-the-base-model">3.2 Finetuning from the base model:<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="bash finetune/scripts/run_sft_Yi_6b.sh"><pre>bash finetune/scripts/run_sft_Yi_6b.sh</pre></div>
<p dir="auto">Once finished, you can compare the finetuned model and the base model with the
following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="bash finetune/scripts/run_eval.sh"><pre>bash finetune/scripts/run_eval.sh</pre></div>
<p dir="auto">For more advanced usage like fine-tuning based on your custom data, please refer
the <a href="https://github.com/01-ai/Yi/tree/main/finetune">doc</a>.</p>

<h2 tabindex="-1" id="user-content-disclaimer" dir="auto"><a href="#disclaimer">Disclaimer<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">We use data compliance checking algorithms during the training process, to ensure the compliance of the trained model to the best of our ability. Due to complex data and the diversity of language model usage scenarios, we cannot guarantee that the model will generate correct, and reasonable output in all scenarios. Please be aware that there is still a risk of the model producing problematic outputs. We will not be responsible for any risks and issues resulting from misuse, misguidance, illegal usage, and related misinformation, as well as any associated data security concerns.</p>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License<svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></h2>
<p dir="auto">The source code in this repo is licensed under the <a href="https://github.com/01-ai/Yi/blob/main/LICENSE">Apache 2.0
license</a>. The Yi series models
are fully open for academic research and free commercial usage with permission
via applications. All usage must adhere to the <a href="https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt">Model License
Agreement 2.0</a>.
To apply for the official commercial license, please contact us
(<a href="mailto:yi@01.ai">yi@01.ai</a>).</p>
</article>
          </div></div>
  </body>
</html>
