<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.alexmolas.com/2025/04/09/semantic-unit-testing.html">Original</a>
    <h1>Semantic unit testing: test code without executing it</h1>
    
    <div id="readability-page-1" class="page"><div>
    


<i><p>April 09, 2025 · <span title="Estimated read time">
  
  
    18 mins · 3350 words
  
</span>
 </p></i> 

<p>Left <a href="https://en.wallapop.com">Wallapop</a> a couple of weeks ago, heading to <a href="https://www.revenuecat.com/">RevenueCat</a> soon. In that classic ‘between jobs’ hacking window, I built <a href="https://github.com/alexmolas/suite">suite</a>: a Python library for semantic unit testing. What’s semantic unit testing? Think unit tests that understand context and meaning, not just <code>assert obj == expected</code>. Sound interesting? I’ll break down what semantic unit testing is, how suite works under the hood, and how you can integrate it.</p>

<!-- I quit my job at [Wallapop](https://en.wallapop.com) a couple of weeks ago, and I'll start a new job at [RevenueCat](https://www.revenuecat.com/) in some weeks, so I've had time to work on some side projects. One of these projects is [suite](https://github.com/alexmolas/suite), a python library for semantic unit testing. In this post, I'll explain what is semantic unit testing, how I have implemented it, and how you can use it. -->



<p>Semantic unit testing is a testing approach that evaluates whether a function’s implementation aligns with its documented behavior. The code is analyzed using LLMs to assess if the implementation matches the expected behavior described in the docstring. It’s basically having an AI review your code and documentation together to spot discrepancies or bugs, without running the code.</p>

<p>This is, instead of writing classic unit tests with pairs of <code>(input, output)</code>, the testing responsibility is passed to an LLM. The hypothesis is that a powerful model, with good enough context, should be able to detect bugs without having to run the code. The idea is that an LLM can analyze the code and its documentation much like a human developer would -carefully reading and thinking hard about it- but much more quickly.</p>

<p>I wrote a package for doing semantic testing called <a href="https://github.com/alexmolas/suite">suite</a> (Semantic UnIt TEsting). You can install it using <code>uv</code> as <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup></p>



<p>Here’s an example of how to perform a basic semantic test with <code>suite</code></p>

<div><div><pre><code><span>from</span> <span>suite</span> <span>import</span> <span>suite</span>

<span>tester</span> <span>=</span> <span>suite</span><span>(</span><span>model_name</span><span>=</span><span>&#34;openai/o3-mini&#34;</span><span>)</span>

<span>def</span> <span>multiply</span><span>(</span><span>x</span><span>:</span> <span>int</span><span>,</span> <span>y</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
    <span>&#34;&#34;&#34;Multiplies x by y

    Args:
        x (int): value
        y (int): value
    Returns:
        int: value
    &#34;&#34;&#34;</span>
    <span>return</span> <span>x</span> <span>+</span> <span>y</span>

<span>result</span> <span>=</span> <span>tester</span><span>(</span><span>multiply</span><span>)</span>
<span>print</span><span>(</span><span>result</span><span>)</span>

<span># {&#39;reasoning&#39;: &#34;The function&#39;s docstring states that it should multiply x by y. 
#   However, the implementation returns x + y, which is addition instead of multiplication. 
#   Therefore, the implementation does not correctly fulfill what is described in the docstring.&#34;,
# &#39;passed&#39;: False}
</span></code></pre></div></div>

<p>Basically, we have a <code>multiply</code> function that we want to test. To do so we create a <code>tester</code> instance that will use <code>o3-mini</code> as a judge. Then, we pass the method <code>multiply</code> to the <code>tester</code>, which internally will build a prompt containing all the information about it. Finally, the LLM will decide if the method is correctly implemented or contains a bug.</p>



<p>I’ve had this project on my todo list for a long time and I never had enough time and motivation to start it, however, some weeks ago <a href="https://koaning.io/">Vincent</a> released <a href="https://github.com/koaning/smartfunc">smartfunc</a> (a library to turn docstrings into LLM-functions), and motivated me to start the project -and to be honest I borrowed some design choices from Vincent’s code. I also used <a href="https://llm.datasette.io/en/stable/index.html">llm</a> library by <a href="https://simonwillison.net/">Simon Willison</a> to access different LLM providers easily.</p>

<p>The <code>suite</code> library does the following.</p>

<ol>
  <li>Receives a callable <code>func</code> as input.</li>
  <li>Reads <code>func</code> implementation and docstring (using <code>inspect</code> library).</li>
  <li>Analyzes the implementation of <code>func</code> to identify and extract any functions or methods that it calls internally.</li>
  <li>Recursively applies steps 1, 2 and 3 to <code>func</code> inner methods (up to some <code>max_depth</code>).</li>
  <li>Builds a <code>FunctionInfo</code> object with all the information about <code>func</code> we need.</li>
  <li>Uses <code>FunctionInfo</code> to write a prompt which is passed to an LLM.</li>
  <li>The LLM returns a structured output like <code>{&#34;reasoning&#34;: str, &#34;passed&#34; bool}</code></li>
</ol>

<p>Let’s see now how all of these works with a concrete example. Imagine we have a method that we use to deal a deck of cards among some players. To do so we have a method called <code>deal_cards</code> which implementation is below.</p>

<details>
<summary>Dealing cards code</summary>


<figure><pre><code data-lang="python"><span>import</span> <span>random</span>

<span>def</span> <span>shuffle_cards</span><span>(</span><span>cards</span><span>:</span> <span>list</span><span>[</span><span>str</span><span>])</span> <span>-&gt;</span> <span>list</span><span>[</span><span>str</span><span>]:</span>
    <span>&#34;&#34;&#34;
    Returns a shuffled copy of the given list of cards.

    Parameters:
        cards (list[str]): A list of card identifiers (e.g., &#34;Ace of Spades&#34;).

    Returns:
        list[str]: A new list containing the same cards in randomized order.
    &#34;&#34;&#34;</span>
    <span>shuffled</span> <span>=</span> <span>cards</span><span>.</span><span>copy</span><span>()</span>
    <span>random</span><span>.</span><span>shuffle</span><span>(</span><span>shuffled</span><span>)</span>
    <span>return</span> <span>shuffled</span>

<span>def</span> <span>split_cards</span><span>(</span><span>cards</span><span>:</span> <span>list</span><span>[</span><span>str</span><span>],</span> <span>number_of_players</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>list</span><span>[</span><span>list</span><span>[</span><span>str</span><span>]]:</span>
    <span>&#34;&#34;&#34;
    Splits a list of cards evenly among a given number of players in a round-robin fashion.

    Parameters:
        cards (list[str]): The full list of cards to distribute.
        number_of_players (int): The number of players to split the cards between.

    Returns:
        list[list[str]]: A list where each sublist represents a player&#39;s hand of cards.
        Cards are distributed one at a time to each player in turn.
    &#34;&#34;&#34;</span>
    <span>return</span> <span>[</span><span>cards</span><span>[</span><span>i</span><span>::</span><span>number_of_players</span><span>]</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>number_of_players</span><span>)]</span>

<span>def</span> <span>deal_cards</span><span>(</span><span>cards</span><span>:</span> <span>list</span><span>[</span><span>str</span><span>],</span> <span>number_of_players</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>list</span><span>[</span><span>list</span><span>[</span><span>str</span><span>]]:</span>
    <span>&#34;&#34;&#34;
    Shuffles a deck of cards and deals them evenly among a given number of players.

    This function combines shuffling and splitting the deck to simulate a card deal.

    Parameters:
        cards (list[str]): The full list of cards to shuffle and distribute.
        number_of_players (int): The number of players to deal cards to.

    Returns:
        list[list[str]]: A list of player hands after shuffling and dealing.
    &#34;&#34;&#34;</span>
    <span>shuffled</span> <span>=</span> <span>shuffle_cards</span><span>(</span><span>cards</span><span>)</span>
    <span>return</span> <span>split_cards</span><span>(</span><span>shuffled</span><span>,</span> <span>number_of_players</span><span>)</span></code></pre></figure>


</details>

<p>We want to test the method <code>deal_cards</code>, so the first thing we need is to retrieve all the information about this method. This includes its docstring, source code, dependencies, etc. To do so we can use the <code>FunctionInfo</code> class, which is a pydantic model that looks like this</p>

<div><div><pre><code><span>class</span> <span>FunctionInfo</span><span>(</span><span>BaseModel</span><span>):</span>
    <span>&#34;&#34;&#34;Information about a function extracted for semantic testing.&#34;&#34;&#34;</span>
    <span>name</span><span>:</span> <span>str</span>
    <span>docstring</span><span>:</span> <span>str</span> <span>|</span> <span>None</span>
    <span>source</span><span>:</span> <span>str</span> <span>|</span> <span>None</span>
    <span>source_file</span><span>:</span> <span>str</span> <span>|</span> <span>None</span>
    <span>dependencies</span><span>:</span> <span>list</span><span>[</span><span>&#34;FunctionInfo&#34;</span><span>]</span> <span>=</span> <span>[]</span>
    
    <span>@</span><span>classmethod</span>
    <span>def</span> <span>from_func</span><span>(</span>
        <span>cls</span><span>,</span>
        <span>func</span><span>:</span> <span>Callable</span><span>,</span>
        <span>max_depth</span><span>:</span> <span>int</span> <span>=</span> <span>2</span><span>):</span>
        <span>...</span>
</code></pre></div></div>

<p>This class has information about the callable name, docstring, source code, etc. It also contains information about the method dependencies, which are a list of <code>FunctionInfo</code>, one for each method that the callable uses internally. This piece of information is key to building a good context for the LLM since it allows us to get information about the code outside the method we want to test. It also has a classmethod that allows you to build this object directly from a callable. The <code>max_depth</code> parameter controls how deep you want to inspect the dependencies (ie: dependencies, dependencies of dependencies, etc.)</p>

<p>Then, we can run <code>FunctionInfo.from_func(deal_cards)</code> and we’ll have an object with all the information we need about <code>deal_cards</code>.</p>

<p>Now, we need a method that receives an instance of <code>FunctionInfo</code> and returns a prompt that can be sent to an LLM</p>

<div><div><pre><code><span>def</span> <span>format_prompt</span><span>(</span>
    <span>func_info</span><span>:</span> <span>FunctionInfo</span><span>,</span>
    <span>prompt_template</span><span>:</span> <span>str</span> <span>=</span> <span>DEFAULT_PROMPT_TEMPLATE</span><span>,</span>
    <span>dependencies_template</span><span>:</span> <span>str</span> <span>=</span> <span>DEFAULT_DEPENDENCY_TEMPLATE</span><span>,</span>
<span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
    <span>...</span>
</code></pre></div></div>

<p>Finally, for the <code>deal_cards</code> method, the resulting prompt is</p>

<details>
<summary>Final prompt</summary>


<figure><pre><code data-lang="markdown">You are evaluating whether a function implementation correctly matches its docstring.

Function name: deal_cards
Docstring: Shuffles a deck of cards and deals them evenly among a given number of players.

This function combines shuffling and splitting the deck to simulate a card deal.

Parameters:
 cards (list[str]): The full list of cards to shuffle and distribute.
 number_of_players (int): The number of players to deal cards to.

Returns:
 list[list[str]]: A list of player hands after shuffling and dealing.
Implementation: def deal_cards(cards: list[str], number_of_players: int) -&gt; list[list[str]]:
 &#34;&#34;&#34;
 Shuffles a deck of cards and deals them evenly among a given number of players.

 This function combines shuffling and splitting the deck to simulate a card deal.

 Parameters:
 cards (list[str]): The full list of cards to shuffle and distribute.
 number_of_players (int): The number of players to deal cards to.

 Returns:
 list[list[str]]: A list of player hands after shuffling and dealing.
 &#34;&#34;&#34;
 shuffled = shuffle_cards(cards)
 return split_cards(shuffled, number_of_players)

Dependencies: 
Dependency 1: shuffle_cards
Docstring: Returns a shuffled copy of the given list of cards.

Parameters:
 cards (list[str]): A list of card identifiers (e.g., &#34;Ace of Spades&#34;).

Returns:
 list[str]: A new list containing the same cards in randomized order.
Implementation: def shuffle_cards(cards: list[str]) -&gt; list[str]:
 &#34;&#34;&#34;
 Returns a shuffled copy of the given list of cards.

 Parameters:
 cards (list[str]): A list of card identifiers (e.g., &#34;Ace of Spades&#34;).

 Returns:
 list[str]: A new list containing the same cards in randomized order.
 &#34;&#34;&#34;
 shuffled = cards.copy()
 random.shuffle(shuffled)
 return shuffled<span>



</span>Dependency 1.1: method
Docstring: Shuffle list x in place, and return None.
Implementation:     def shuffle(self, x):
 &#34;&#34;&#34;Shuffle list x in place, and return None.&#34;&#34;&#34;

 randbelow = self._randbelow
 for i in reversed(range(1, len(x))):
 # pick an element in x[:i+1] with which to exchange x[i]
 j = randbelow(i + 1)
 x[i], x[j] = x[j], x[i]<span>



</span>Dependency 2: split_cards
Docstring: Splits a list of cards evenly among a given number of players in a round-robin fashion.

Parameters:
 cards (list[str]): The full list of cards to distribute.
 number_of_players (int): The number of players to split the cards between.

Returns:
 list[list[str]]: A list where each sublist represents a player&#39;s hand of cards.
 Cards are distributed one at a time to each player in turn.
Implementation: def split_cards(cards: list[str], number_of_players: int) -&gt; list[list[str]]:
 &#34;&#34;&#34;
 Splits a list of cards evenly among a given number of players in a round-robin fashion.

 Parameters:
 cards (list[str]): The full list of cards to distribute.
 number_of_players (int): The number of players to split the cards between.

 Returns:
 list[list[str]]: A list where each sublist represents a player&#39;s hand of cards.
 Cards are distributed one at a time to each player in turn.
 &#34;&#34;&#34;
 return [cards[i::number_of_players] for i in range(number_of_players)]<span>



</span>Does the implementation correctly fulfill what is described in the docstring?
Read the implementation carefully. Reason step by step and take your time.</code></pre></figure>


</details>

<p>The prompt includes information about the dependencies we implemented (<code>shuffle_cards</code> and <code>split_cards</code>) and also about external methods (<code>random.shuffle</code>).</p>

<p>Then, this prompt is sent to an LLM which returns an object of type <code>SuiteOutput</code></p>

<div><div><pre><code><span>class</span> <span>SuiteOutput</span><span>(</span><span>BaseModel</span><span>):</span>
    <span>reasoning</span><span>:</span> <span>str</span>
    <span>passed</span><span>:</span> <span>bool</span>

    <span>def</span> <span>__bool__</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>self</span><span>.</span><span>passed</span>
</code></pre></div></div>

<p>In my experience, models that support thinking and structured outputs yield better results. <code>o3-mini</code> works very well and it’s not crazy expensive.</p>



<p>Now that you know how <code>suite</code> works let’s get serious: you shouldn’t substitute your tests with this tool.</p>

<p>I know it’s weird to write this section in my post. I’m the author of the package and I should be publicizing it instead of explaining why is a bad idea to use it. But let’s be honest, we all know that LLMs are not the solution to every problem we have -and we already have enough AI influencers out there-, so I’ll try to be upfront and explain why I don’t think you should replace your unit tests with this approach.</p>

<h2 id="use-boring-technology">Use boring technology</h2>

<p><a href="https://mcfunley.com/choose-boring-technology">Boring technology works</a> so instead of using the new and shiny library you should be using the old and tested approaches that have been there for decades. I know it’s cool to play with LLMs and CV-driven-development forces you to try new technologies to make you more employable. But we’re here to make good software, and sometimes you don’t need the new toy in the market to do so.</p>

<h2 id="it-can-be-expensive">It can be expensive</h2>

<p>While developing <code>suite</code> I did some tests with the pandas library. In particular, I tested the <code>pd.json_normalize</code> method. With <code>max_depth=0</code> (ie: the smallest amount of information) I got a prompt of 112k tokens (the first Harry Potter book has <a href="https://x.com/rasbt/status/1656724322164015105">100k tokens</a>). And here I was just testing one method. Imagine if I tested all the pandas codebase!</p>

<h2 id="dont-trust-llms">Don’t trust LLMs</h2>

<p>I guess at this point I don’t need to tell you this but here we go: you can’t trust LLMs outputs. They are useful tools, but as with any tool you need to be careful of how you use it. LLMs tend to halluciante, and this make them dangerous tools. So I wouldn’t trust an LLM to tell me if my implementation of a method is correct or not.</p>

<p>Looking at the pace at which LLMs get better maybe this point will be obsolete in the following years -or even months. But for the time being I wouldn’t trust an LLM to decide if some code is safe enough to be deployed.</p>



<p>At this point, you might be wondering, “Why the hell do we need this?” or perhaps shouting, “Stop putting LLMs everywhere!” at your screen. Fair enough. Let me walk you through why this tool is worth considering and why it could be a valuable addition to your testing toolbox.</p>

<h2 id="comprehensive-coverage">Comprehensive Coverage</h2>

<p>Traditional unit testing focuses on specific inputs and outputs, often covering only a small part of your code. With <code>suite</code>, the game changes. Instead of just testing specific cases, it evaluates the semantic correctness of your functions by cross-referencing their implementation against the documentation. For example, imagine you’ve implemented a factorial function, something like this:</p>

<div><div><pre><code><span>def</span> <span>factorial</span><span>(</span><span>n</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
    <span>&#34;&#34;&#34;Calculates the factorial of a non-negative integer n.&#34;&#34;&#34;</span>
    <span>if</span> <span>n</span> <span>==</span> <span>0</span><span>:</span>
        <span>return</span> <span>1</span>
    <span>return</span> <span>n</span> <span>*</span> <span>factorial</span><span>(</span><span>n</span> <span>-</span> <span>1</span><span>)</span>
</code></pre></div></div>

<p>you write the tests, and everything passes</p>

<div><div><pre><code><span>assert</span> <span>factorial</span><span>(</span><span>0</span><span>)</span> <span>==</span> <span>1</span>  <span># Factorial of 0
</span><span>assert</span> <span>factorial</span><span>(</span><span>1</span><span>)</span> <span>==</span> <span>1</span>  <span># Factorial of 1
</span><span>assert</span> <span>factorial</span><span>(</span><span>5</span><span>)</span> <span>==</span> <span>120</span>  <span># Factorial of 5
</span></code></pre></div></div>

<p>But here’s the catch: you’re missing some edge cases. What about negative inputs? What happens if someone passes a non-integer? Or very large numbers?</p>

<p>The problem is that traditional tests can only cover a narrow slice of your function’s behavior. Writing unit tests is <strong>hard</strong> and <strong>boring</strong>, and when combined, these two elements often lead to disaster. Just because a high percentage of tests pass doesn’t mean your code is bug-free. With <code>suite</code>, you can sidestep the pain of writing every single test case manually. Instead, the LLM reviews your function’s behavior holistically, saving time and ensuring a broader set of scenarios are taken into account.</p>

<h2 id="trivial-integration-with-pytest">Trivial integration with pytest</h2>

<p>One of the reasons to use <code>suite</code> is its seamless integration with <code>pytest</code>. You can easily incorporate semantic testing into your existing test suite without disrupting your workflow</p>

<div><div><pre><code><span># test_module.py
</span>
<span>from</span> <span>module</span> <span>import</span> <span>function</span>
<span>from</span> <span>suite</span> <span>import</span> <span>suite</span>

<span>tester</span> <span>=</span> <span>suite</span><span>()</span>

<span>def</span> <span>test_function</span><span>():</span>
    <span>assert</span> <span>tester</span><span>(</span><span>function</span><span>)</span>
</code></pre></div></div>

<p>That’s it - clean and simple. When you run <code>pytest</code>, your semantic tests will execute alongside traditional tests. For teams with established testing practices, this trivial integration makes it easy to experiment with semantic testing without committing to a major workflow change.</p>

<h2 id="catch-bugs-early">Catch bugs early</h2>

<p>In a typical testing workflow, you write some basic tests to check the core functionality. When a bug inevitably shows up—usually after deployment—you go back and add more tests to cover it. This process is reactive, time-consuming, and frankly, a bit tedious.</p>

<p>With semantic unit testing, the LLM handles this in just one iteration. It checks the function’s behavior against its documentation right from the start, catching discrepancies upfront without waiting for them to surface in production. This approach ensures that you catch issues early, saving time and preventing bugs from ever making it to production.</p>

<h2 id="improve-your-testing-suite">Improve your testing suite</h2>

<p>Even if you use semantic unit testing, you’re likely still relying on traditional unit tests (after all, if it isn’t broken, don’t fix it). However, by incorporating semantic unit testing into your workflow, you can enhance your existing test suite. Semantic unit testing can point you out uncovered corner cases that you might want to add to your unit tests.</p>

<h2 id="you-can-run-it-locally">You can run it locally</h2>

<p>Thanks to the <a href="https://llm.datasette.io/en/stable/">llm</a> package you can use local models to run your semantic tests. Using <a href="https://github.com/taketwo/llm-ollama">llm-ollama</a> plugin you can use any model in ollama to run your tests without having to share your precious code with the evil AI companies.</p>

<h2 id="async-is-fast">Async is fast</h2>

<p><code>suite</code> allows you to run the tests asynchronously, and since the main bottleneck is IO (all the computations happen in a GPU in the cloud) it means that you can run your tests very fast. This is a huge advantage in comparison to standard tests, which need to be run sequentially.</p>



<p>Building <code>suite</code> has been a fun ride. It’s one of those projects that sat on my todo list for months (okay, maybe years) until the right mix of free time, curiosity, and external inspiration finally pushed it forward. I’m happy to say it’s now a real package on PyPI - and yes, I checked off one more item from my <a href="https://www.alexmolas.com/100-list">100 list</a>: <code>26. ✗ Publish a Python package (in pip) → ✅ Done.</code></p>

<p>Is semantic unit testing the future of testing? Probably not. At least not yet. LLMs are powerful, but they’re unpredictable and far from perfect. That said, they open up a fascinating new space in developer tooling - one where we offload some of the tedium to machines and focus more on building than babysitting our code.</p>

<p>I’m not here to sell you the idea that you should throw away your trusty test suite and blindly trust an LLM. Please don’t do that. But I do think there’s value in exploring tools like <code>suite</code> to complement what you already have. Use it as a sidekick, not as a replacement.</p>

<p>If you give it a try, I’d love to hear your feedback. And if you find bugs - well, just don’t tell the LLM.</p>

<hr/>





  </div></div>
  </body>
</html>
