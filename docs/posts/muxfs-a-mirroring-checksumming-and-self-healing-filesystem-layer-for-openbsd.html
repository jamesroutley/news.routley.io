<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sdadams.org/blog/introducing-muxfs/">Original</a>
    <h1>Muxfs â€“ a mirroring, checksumming, and self-healing filesystem layer for OpenBSD</h1>
    
    <div id="readability-page-1" class="page"><article>

<header>

<h3><em>a mirroring, checksumming, and self-healing filesystem layer for OpenBSD</em></h3>
<p>
Date published: 2022-08-08
</p>
</header>

<nav>
<h2>Contents</h2>
<ol>
 <li><a href="#problem">The problem: data corruption</a></li>
 <li>
  <a href="#candidates">Solutions I considered</a>
  <ol>
   <li><a href="#raid">Hardware RAID and <code>softraid</code></a></li>
   <li><a href="#scanners"><code>bitrot</code>, <code>yabitrot</code>, and <code>hashdeep</code></a></li>
  </ol>
 </li>
 <li><a href="#solution">My solution: <code>muxfs</code></a></li>
 <li><a href="#usage">Tutorial: <code>muxfs</code> storage array</a></li>
 <li><a href="#status"><code>muxfs</code> needs you!</a></li>
</ol>
</nav>

<section id="problem">
<h2>The problem: data corruption</h2>
<p>
I decided it was finally time to build a file server to centralize my files and guard them against bit-rot.
Although I would have preferred to use OpenBSD due to its straightforward configuration and sane defaults, I was surprised to find that none of the typical NAS filesystems were supported.
</p><p>
In particular I would need three features in such a filesystem.
Firstly I would need data to be stored redundantly across multiple disks so should one disk fail there would be at least one other disk from which the data can be recovered.
Secondly I would need all data and metadata to have their checksums stored alongside them so that if one disk yields corrupted data then the valid and invalid copies would be identifiable and restoration could proceed without the risk of propagating the corruption.
Finally such a filesystem should automatically check and repair data as it is accessed rather than processing the entire filesystem tree upon every check or repair job.
</p><p>
For this final point it is not that checking the entire tree should not be possible, rather that for this expensive task it should be necessary neither to manually invoke it frequently, nor to cron schedule it to be invoked frequently.
The inconvenience, and time and energy wasted aside regularly processing through the entire contents of a disk needlessly shortens its lifespan.
</p>
</section>

<section id="candidates">
<h2>Solutions I considered</h2>

<section id="raid">
<h3>Hardware RAID and <code>softraid</code></h3>
<p>
There is already plenty of discussion on the internet concerning the viability of RAID as a bit-rot mitigation tool so I will not go into much detail here, but for those unaware I suggest to at least read up on the term &#34;write hole&#34;.
</p><p>
The more expensive RAID cards can address such issues but might simply shift the problem over to, for example, the additional maintenance burden of checking and replacing the on-card batteries.
</p><p>
OpenBSD&#39;s <code>softraid(4)</code> is close to a solution to the problem, but we can see the following from the manual page:
</p>
<ul>
<li>
&#34;The driver relies on underlying hardware to properly fail chunks.&#34;
</li><li>
&#34;Currently there is no automated mechanism to recover from failed disks.&#34;
</li><li>
&#34;Certain RAID levels can protect against some data loss due to component failure.&#34;
</li>
</ul>
<p>
Note that the data protected from loss is qualified as <em>some</em>.
There is also no mention of support for a journal device that would be needed to address the write hole issue.
</p><p>
If my use case were a webserver serving many images or videos to a large userbase then I would store the content using <code>softraid</code> since small-but-rare corruptions to the content would be significantly less bad than the whole system grinding to a halt.
My use case however is to store master copies, particularly of things more sensitive to corruption such as source code.
</p></section>

<section id="scanners">
<h3><code>bitrot</code>, <code>yabitrot</code>, and <code>hashdeep</code></h3>
<p>
In ports we can find the scripts <code>bitrot</code> and <code>yabitrot</code>.
When run these will compute and store checksums for new files and files whose timestamps are newer than the time of the previous run.  They also point out any files whose timestamp is not newer but whose checksum does not match.
</p><p>
I am accustomed to using a similar program called <code>hashdeep</code> (<em>which I have patched to sum symlinks according to the string that is the link rather than the content of the file pointed to by the link</em>).
</p><p>
It should be pointed out that there are some shortcomings of the approach taken by these tools:
</p>
<ul>
<li>
These tools must be invoked manually or in a cron job.
Manual invocation is inconvenient, and such tasks are easily forgotten or dropped entirely.
A cron job requires the machine to be booted at the right time, and would need to be coordinated carefully with any users of the system so not to interrupt or confuse the running job.
</li><li>
If a corruption occurs between a file edit and the running of the tool then that corrupted data is marked as valid.
</li><li>
If the data corrupted includes the file metadata, particularly the timestamps, then the assumptions made by some of these tools are broken.
</li><li>
If a file is correctly removed from the collection then there is no way for the script to know that this is expected and will flag the removed file as missing until the checksum database is manually corrected.
</li><li>
These tools merely point out corruptions and require the user to manually restore files from their own backups.
</li>
</ul>
<p>
I concede however that the simplicity of the scripts is a boon, and I recommend the reader to consider them for their intended use cases.
</p>
</section>

<p>
Since my use case had not been solved I decided to write my own solution, <code><a href="https://sdadams.org/muxfs/">muxfs</a></code>, to address this gap in OpenBSD&#39;s capabilities.
</p>
</section>

<section id="solution">
<h2>My solution: <code>muxfs</code></h2>
<p>
I needed something that was not only automatic, but immediate, intercepting the data in-memory while its integrity could still be assured by ECC.
It was clear that I needed a filesystem driver.
After careful consideration I decided it would be wiser to write this first implementation as a FUSE filesystem.
If <code>muxfs</code> proves itself then the FUSE implementation can serve as a stepping-stone towards a more efficient in-kernel filesystem driver.
</p><p>
Having read through the UFS and FFS driver source code I was inspired not to reinvent the wheel, rather to make use of these drivers indirectly.
I chose to use directories as storage media instead of writing directly to block devices.
This way source code is not duplicated, and <code>muxfs</code> gets the benefit of the many years of development and testing that have gone into whatever filesystem the user places at these directory locations.
</p><p>
It was now obvious that to get data redundancy I could simply feed writes coming into the <code>muxfs</code> driver across to multiple directories.
If the user wants to spread the data across multiple disks, that would now be something they are in control of.
In fact if the user wanted to diversify their storage as a hedge against unforeseen technical faults then they could simultaneously mirror to HDDs and SSDs each containing a variety of filesystem types.
</p><p>
I decided that I wanted both content and metadata to be checksummed, the checksum of the content to be considered part of the checksummed metadata, and checksums to be linked from content to metadata all the way up to the metadata checksum of the root directory.
This means that the root directory&#39;s metadata checksum is representative of the whole filesystem tree contained inside that root directory.
</p><p>
Originally I was going to use SQLite to store the checksums, but I realised I didn&#39;t need it.
Instead I store the equivalent of one table per file in hidden directories named <code>.muxfs</code> found at the root of each provided directory.
All binary data in these &#34;tables&#34; are stored little-endian.
</p><p>
The design is to check all data read in from the directories before use.
This way corruptions can be found and fixed opportunistically.
It also prevents partial updates from inappropriately causing corrupted data nearby to be marked as valid.
</p><p>
A journal was not needed to address the write hole issue.
<code>muxfs</code> writes to each directory in series, which guarantees that as long as there is more than one directory in the array there will always be at least one in a valid state.
It is then simply a matter of running a <code>muxfs sync</code> to revert or update the interrupted directory to a valid state from one of the others.
</p><p>
I added an optimization for large files, which I define to be files of size larger than one <code>muxfs</code> native block size (currently 4096 bytes).
Files are divided into blocks of this size with one content checksum per block, and in the case of large files there is a tree of such checksums joining subsets layer by layer until a single checksum representing the whole file is obtained.
This way large files can be edited without <code>muxfs</code> needing to read and sum the whole file upon every write operation to it.
</p><p>
I chose to exclude timestamps from the checksummed metadata since I believe them to be too volatile to be worth tracking.
Along the way I dropped support for hardlinking since this would need a means of discovering all paths from the root to a given file, and searching the whole tree upon every write operation would not have been acceptable.
Special files and device nodes weren&#39;t a good fit for this filesystem either, so I dropped support for those too.
Fortunately none of these features would be needed for the use case I had in mind, and if it were necessary to preserve records of such files then they could be archived with <code>tar</code> and the resulting tarball, being a regular file, would be supported.
I am also open to adding support for these features if they are requested and if a sane proposal for how to support them is put together.
</p><p>
I wrote <code>muxfs</code> with source code audits in mind, so I have tried to keep the codebase small.
The C source files, excluding comments and empty lines, total just over 6k lines at 80 characters maximum per line.
The total for all files is a little under 9k lines.
I have taken the fail fast and fail hard approach to error handling to make bugs more obvious.
I have avoided dependencies wherever possible, and at the time of writing <code>muxfs</code> only depends on base.
I am also pleased to say that <code>muxfs</code> compiles rather quickly, and the binary is about 100KiB in size.
</p>
</section>

<section id="usage">
<h2>Tutorial: <code>muxfs</code> storage array</h2>
<p>
In this example we will take an amd64 OpenBSD 7.1 system which has two unused SATA HDDs and use <code>muxfs</code> to turn the HDDs into a high-integrity storage array.
Then we will temporarily attach an external USB HDD and use it to create an offline backup of this filesystem.
</p><p>
I will not cover installation of the OS or making the filesystem available over a network since these aspects should be no different from normal.
</p>
<h3>
CAUTION: <code>muxfs</code> is not yet considered stable.
Before you run these commands on your own system read the entire article, most importantly the section <a href="#status"><code>muxfs</code> needs you!</a>
</h3>
<p>
First log in as root, create a workspace directory to hold the <code>muxfs</code> source code and build artifacts, and <code>cd</code> into it.
</p>
<pre># <b>mktemp -d</b>
/tmp/tmp.Ab3De6Gh9J</pre>
<pre># <b>cd /tmp/tmp.Ab3De6Gh9J</b></pre>
<p>
Next we fetch the muxfs source code.
</p>
<pre># <b>curl -O &#39;https://sdadams.org/muxfs/muxfs-0.5-current.tgz.sha512&#39;</b>
# <b>curl -O &#39;https://sdadams.org/muxfs/muxfs-0.5-current.tgz&#39;</b></pre>
<p>
Check the sum of the downloaded tarball.
</p>
<pre># <b>sha512 -c muxfs-0.5-current.tgz.sha512</b>
(SHA512) muxfs-0.5-current.tgz: OK</pre>
<p>
If this displays anything other than <code>OK</code> then the tarball is corrupted and should not be used.
</p><p>
Unpack the tarball and <code>cd</code> into it.
</p>
<pre># <b>tar -xzf muxfs-0.5-current.tgz</b>
# <b>cd muxfs-0.5-current</b></pre>
<p>
Build and install muxfs.
Then check to ensure that the <code>muxfs</code> binary and its manual page are installed.
</p>
<pre># <b>make</b>
echo  &#39;/* gen.h contents not needed for unity build. */&#39;  &gt;gen.h
cc -std=c99 -pedantic -Wdeprecated -Wall -Wno-unused-function  -Werror -O2 -DNDE
BUG=1  -I.  -DMUXFS=static  -DMUXFS_DEC=static  -DMUXFS_DS_MALLOC=0  -Dmuxfs_chk
=muxfs_chk_p  -lfuse -lz  -o muxfs  unity.c</pre>
<pre># <b>make install</b>
install -o root -g bin -m 0755 muxfs    /usr/local/sbin/muxfs
install -o root -g bin -m 0644 muxfs.1  /usr/local/man/man1/muxfs.1</pre>
<pre># <b>whereis muxfs</b>
/usr/local/sbin/muxfs</pre>
<pre># <b>muxfs version</b>
muxfs 0.5-current</pre>
<pre># <b>makewhatis</b>
# <b>apropos muxfs</b>
muxfs(1) - the Multiplexed File System</pre>
<p>
Prepare a log file, then ensure that its mode is not readable by other, and that it is empty.
</p>
<pre># <b>install -o root -g wheel -m 0660 /dev/null /var/log/muxfs</b>
# <b>stat -f &#39;%p %z&#39; /var/log/muxfs</b>
10660 0</pre>
<p>
Next we will need to edit <code>syslog.conf(5)</code>, adding the following lines:
</p>
<pre>!muxfs
*.*	/var/log/muxfs</pre>
<p>
Then restart <code>syslogd(8)</code> to pick up the changes.
</p>
<pre># <b>rcctl restart syslogd</b>
syslogd(ok)
syslogd(ok)</pre>
<p>
Now let&#39;s examine the disks on the system.
</p>
<pre># <b>sysctl hw.diskcount</b>
hw.diskcount=3</pre>
<pre># <b>sysctl hw.disknames</b>
hw.disknames=sd0:0123456789abcdef,sd1:,sd2:</pre>
<p>
This system has three disks: the boot disk, and the two spare HDDs.
In this case we can see that only disk <code>sd0</code> has a disk label.
We can infer that <code>sd0</code> is the boot disk, and that <code>sd1</code> and <code>sd2</code> are the two spare HDDs.
Depending on your hardware your system may use <code>wd</code> instead of <code>sd</code> in the device names.
If this is the case then the following commands should still work if you make the corresponding device name substitutions.
</p><p>
Each of the HDDs will need the following layers applied in order:
</p>
<ol>
 <li>Master Boot Record (MBR)</li>
 <li>OpenBSD Disk Label</li>
 <li>Berkley Fast File System (<code>ffs</code>)</li>
</ol>
<h3>
CAUTION: The following commands will IRRECOVERABLY DELETE ALL DATA on the disks you apply them to.
Back up anything you do not want to lose to other devices before you proceed.
</h3>
<p>
Write an MBR to each of the disks.
</p><pre># <b>fdisk -iy sd1</b>
Writing MBR at offset 0.</pre>
<pre># <b>fdisk -iy sd2</b>
Writing MBR at offset 0.</pre>
<p>
Write a disk label to each of the disks.
</p>
<pre># <b>disklabel -E sd1</b>
Label editor (enter &#39;?&#39; for help at any prompt)
sd1&gt; <b>a a</b>
offset: [64]
size: <b>*</b>
FS type: [4.2BSD]
sd1*&gt; <b>w</b>
sd1&gt; <b>q</b>
No label changes.</pre>
<pre># <b>disklabel -E sd2</b>
Label editor (enter &#39;?&#39; for help at any prompt)
sd2&gt; <b>a a</b>
offset: [64]
size: <b>*</b>
FS type: [4.2BSD]
sd2*&gt; <b>w</b>
sd2&gt; <b>q</b>
No label changes.</pre>
<p>
Note down the new disk label IDs.
</p>
<pre># <b>sysctl hw.disknames</b>
hw.disknames=sd0:0123456789abcdef,sd1:1123456789abcdef,sd2:2123456789abcdef</pre>
<p>
Create an <code>ffs</code> filesystem on each of the new <code>disklabel</code> partitions.
</p>
<pre># <b>newfs -t ffs sd1a</b>
# <b>newfs -t ffs sd2a</b></pre>
<p>
Create directories to serve as the mount points for the individual <code>ffs</code> filesystems, and ensure that they are only accessible by root.
</p>
<pre># <b>install -d -o root -g wheel -m 0555 /var/muxfs</b>
# <b>install -d -o root -g wheel -m 0700 /var/muxfs/a</b>
# <b>install -d -o root -g wheel -m 0700 /var/muxfs/b</b>
# <b>stat -f &#39;%p&#39; /var/muxfs/a</b>
40700
# <b>stat -f &#39;%p&#39; /var/muxfs/b</b>
40700</pre>
<p>
This access restriction helps to prevent unwanted writes to the root filesystem if an error causes any of the filesystems to become unmounted.
</p><p>
Append the following lines to <code>fstab(5)</code> (replacing the 16-character IDs with those you noted from sysctl) to automatically mount the <code>ffs</code> filesystems at boot.
</p>
<pre>1123456789abcdef.a /var/muxfs/a ffs rw,nodev 0 2
2123456789abcdef.a /var/muxfs/b ffs rw,nodev 0 2</pre>
<p>
Note here that field 5 is <code>0</code>, indicating that <code>dump(8)</code> should ignore these filesystems.  Backing up the array using <code>dump(8)</code> would be wasteful, and we will cover a more appropriate backup mechanism, <code>muxfs sync</code>, below.
</p><p>
Request that the new filesystems in <code>fstab(5)</code> be mounted now, then check that they are mounted correctly.
</p>
<pre># <b>mount -a</b></pre>
<pre># <b>mount | grep &#39;sd[12]&#39;</b>
/dev/sd1a on /var/muxfs/a type ffs (local, nodev)
/dev/sd2a on /var/muxfs/b type ffs (local, nodev)</pre>
<p>
A <code>muxfs</code> array can be formatted to use either the <code>crc32</code>, <code>md5</code>, or <code>sha1</code> checksum algorithm.  <code>muxfs format</code> will use <code>md5</code> by default, but we will select it explicitly here.
</p>
<pre># <b>muxfs format -a md5 /var/muxfs/a /var/muxfs/b</b></pre>
<p>
Each of the directories should now contain a hidden directory, <code>.muxfs</code>, which contains data internal to the functioning of <code>muxfs</code>.
</p><p>
Create a directory onto which the <code>muxfs</code> array will be mounted, and check its permissions, as was done for the <code>ffs</code> filesystems.
</p>
<pre># <b>install -d -o root -g wheel -m 0700 /mnt/storage</b></pre>
<pre># <b>stat -f &#39;%p&#39; /mnt/storage</b>
40700</pre>
<p>
Now we can mount the <code>muxfs</code> array.
</p>
<pre># <b>muxfs mount /mnt/storage /var/muxfs/a /var/muxfs/b</b></pre>
<p>
Let&#39;s provoke <code>muxfs</code> to restore a file.
First we create a file of test data:
</p>
<pre># <b>echo &#39;Example line of text.&#39; &gt;/mnt/storage/test.txt</b>
# <b>sync</b></pre>
<p>
We can now see that the file is present in both of the directories in the array:
</p>
<pre># <b>ls /var/muxfs/{a,b}</b>
/var/muxfs/a:
.muxfs  test.txt

/var/muxfs/b:
.muxfs  test.txt
</pre>
<pre># <b>cat /var/muxfs/{a,b}/test.txt</b>
Example line of text.
Example line of text.</pre>
<p>
Corrupt the copy of the file in <code>/var/muxfs/a</code> by appending another line:
</p><pre># <b>echo &#39;Bad data.&#39; &gt;&gt;/var/muxfs/a/test.txt</b></pre>
<p>
Then when we read the file from the <code>muxfs</code> mount-point we will see that <code>muxfs</code> is triggered to restore the file from the copy in <code>/var/muxfs/b</code>:
</p><pre># <b>cat /mnt/storage/test.txt</b>
Example line of text.</pre>
<pre># <b>tail /var/log/muxfs</b>
... Restoring: 0:/test.txt
... Restored: 0:/test.txt</pre>
<p>
When you&#39;re done using the filesystem unmount it by calling <code>umount(8)</code> on the mount-point.
</p>
<pre># <b>umount /mnt/storage</b></pre>
<h3>Backups and restoration</h3>
<p>
It is still important to make offline and off-site backups of your data even when using <code>muxfs</code>.
For this we can use the <code>sync</code> command.
</p><p>
In the following commands I will assume that a detachable USB HDD has been formatted in a similar fashion to the individual HDDs in the array, and is mounted at <code>/mnt/backup</code>.
</p><p>
First ensure that your <code>muxfs</code> array is not mounted.
</p><pre># <b>umount /mnt/storage</b></pre>
<p>
Then to create or update another mirror at <code>/mnt/backup</code> use <code>muxfs sync</code>:
</p><pre># <b>muxfs sync /mnt/backup /var/muxfs/a /var/muxfs/b</b></pre>
<p>
This copy can be mounted as part of the array just like the other directories, and this process doubles as the means to replace a failed disk.
Keep in mind that <code>muxfs</code> directories do not respond well to being moved across filesystems.
<code>muxfs</code> uses inode numbers to match checksums to files, and these numbers cannot be preserved when copying or moving from one filesystem to another.
Whenever you need to move data between filesystems remember to use the <code>sync</code> command.
</p><p>
Suppose a write to <code>/var/muxfs/b</code> is interrupted by a power outage.
This should show up as a failure to <code>mount</code>.
The state of <code>/var/muxfs/b</code> can be efficiently restored to that of <code>/var/muxfs/a</code> with:
</p>
<pre># <b>muxfs sync /var/muxfs/b /var/muxfs/a</b></pre>
<p>
If you then want to have <code>muxfs</code> read the whole array and report any corrupted files it finds on the standard output then you can use <code>audit</code>.
</p><p>
Again first ensure that your array is not mounted:
</p>
<pre># <b>umount /mnt/storage</b></pre>
<p>
Then issue the <code>audit</code> command:
</p>
<pre># <b>muxfs audit /var/muxfs/a /var/muxfs/b</b>
/var/muxfs/a/path/to/corrupted1
/var/muxfs/a/path/to/corrupted2
/var/muxfs/b/path/to/corrupted3
/var/muxfs/b/path/to/corrupted4</pre>
<p>
Or if you want <code>muxfs</code> to attempt to restore the corrupted files it finds as it goes then use <code>heal</code> instead:
</p><pre># <b>muxfs heal /var/muxfs/a /var/muxfs/b</b>
/var/muxfs/a/path/to/corrupted1
/var/muxfs/a/path/to/corrupted2
/var/muxfs/b/path/to/corrupted3
/var/muxfs/b/path/to/corrupted4</pre>
<p>
The success or failure of restoration attempts can be monitored via the log file.
</p><pre># <b>tail -f /var/log/muxfs</b>
... Restoring: 0:/path/to/corrupted1
... Restored: 0:/path/to/corrupted1
... Restoring: 0:/path/to/corrupted2
... Restored: 0:/path/to/corrupted2
... Restoring: 1:/path/to/corrupted3
... Restored: 1:/path/to/corrupted3
... Restoring: 1:/path/to/corrupted4
... Restored: 1:/path/to/corrupted4</pre>
<p>
Here the number prefixed to the path is the index of the directory in the <code>muxfs</code> array.
</p><p>
You might be thinking &#34;Fantastic!  So my data is safe from bit-rot now, right?&#34;
Yes, well, almost...
</p>
</section>

<section id="status">
<h2><code>muxfs</code> needs you!</h2>
<p>
No filesystem can be considered stable without thorough testing and <code>muxfs</code> is no exception.
</p><p>
Even if I had tested <code>muxfs</code> enough to call it stable it still would not be responsible to expect you to simply take my word for it.
It is for this reason that I do not intend to release a version 1.0 until there are sufficient citations that I can make to positive, third-party evaluations of <code>muxfs</code>.
</p><p>
This is where you can help.
</p><p>
I need volunteers to test <code>muxfs</code>, provide feedback, and periodically publish test results.
The types of things we need to know about include: ease of use, clarity of documentation, needed additional features, bugs, performance, and security issues.
Try to be creative in your approach to testing.
The more angles we approach it from, the more stable it will become.
</p><p>
I do not recommend testing <code>muxfs</code> on a machine containing sensitive data, or with privileged access to other systems.
Instead I recommend to run <code>muxfs</code> on a dedicated machine, whether physical or virtual.
</p><p>
<code>muxfs</code> may still have bugs, and expects to run as root.
<code>pledge(2)</code> and <code>unveil(2)</code> have not yet been applied since I would prefer to get feedback on usability, and whether any additional features are needed, before I lock it down.
I would prefer for the policy to be <em>well-known</em> rather than changing from version to version.
This said, I am open to discussion on applying these sooner if requested.
In the mean time your own policy can be imposed with a small patch to <code>main()</code> in muxfs.c.
</p><p>
If you insist on testing <code>muxfs</code> with important data then please use <code>muxfs</code> as a mirror downstream of some other well-established storage system, and periodically compare the data against  the upstream to find discrepancies.
</p><p>
Feedback can be sent to &lt;<a href="mailto:muxfs@sdadams.org">muxfs@sdadams.org</a>&gt;.
For convenience I have enabled the Discussions feature in the <a href="https://github.com/s-d-adams/muxfs">github mirror</a>.
I also plan to spend some time in the #openbsd irc channel on irc.libera.chat; my nick there is sdadams.
</p>
</section>

</article></div>
  </body>
</html>
