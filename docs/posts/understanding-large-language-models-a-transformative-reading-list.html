<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sebastianraschka.com/blog/2023/llm-reading-list.html">Original</a>
    <h1>Understanding Large Language Models – A Transformative Reading List</h1>
    
    <div id="readability-page-1" class="page"><div>
      <div>
        




<div>
  

  <article>
    <p>Large language models have taken the public attention by storm – no pun intended.
In just half a decade large language models – transformers – have almost completely changed the field of natural language processing. Moreover, they have also begun to revolutionize fields such as computer vision and computational biology.</p>

<p>Since transformers have such a big impact on everyone’s research agenda, I wanted to flesh out a short reading list (an extended version of <a href="https://www.linkedin.com/feed/update/urn:li:activity:7028449312300834816?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7028449312300834816%2C7028519126105030656%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287028519126105030656%2Curn%3Ali%3Aactivity%3A7028449312300834816%29">my comment yesterday</a>) for machine learning researchers and practitioners getting started.</p>

<p>The following list below is meant to be read mostly chronologically, and I am entirely focusing on academic research papers. Of course, there are many additional resources out there that are useful. For example,</p>

<ul>
  <li>the <a href="http://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a> by Jay Alammar;</li>
  <li>a <a href="https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/">more technical blog article</a> by Lilian Weng;</li>
  <li><a href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/">a catalog and family tree</a> of all major transformers to date by Xavier Amatriain;</li>
  <li><a href="https://github.com/karpathy/nanoGPT">a minimal code implementation</a> of a generative language model for educational purposes by Andrej Karpathy;</li>
  <li><a href="https://sebastianraschka.com/blog/2021/dl-course.html#l19-self-attention-and-transformer-networks">a lecture series</a> and <a href="https://github.com/rasbt/machine-learning-book/tree/main/ch16">book chapter</a> by yours truly.</li>
</ul>
      <h2 id="understanding-the-main-architecture-and-tasks">
        
        
          Understanding the Main Architecture and Tasks <a href="#understanding-the-main-architecture-and-tasks">#</a>
        
        
      </h2>
    

<p>If you are new to transformers / large language models, it makes the most sense to start at the beginning.</p>

<p><strong>(1)</strong> <em>Neural Machine Translation by Jointly Learning to Align and Translate</em> (2014) by Bahdanau, Cho, and Bengio, <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a></p>

<p>I recommend beginning with the above paper if you have a few minutes to spare. It introduces an attention mechanism for recurrent neural networks (RNN) to improve long-range sequence modeling capabilities. This allows RNNs to translate longer sentences more accurately – the motivation behind developing the original transformer architecture later.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/attention-rnn.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a> </center>

<hr/>



<p><strong>(2)</strong> <em>Attention Is All You Need</em> (2017) by Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin, <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>

<p>The paper above introduces the original transformer architecture consisting of an encoder- and decoder part that will become relevant as separate modules later. Moreover, this paper introduces concepts such as the scaled dot product attention mechanism, multi-head attention blocks, and positional input encoding that remain the foundation of modern transformers.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/transformer.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a> </center>

<hr/>



<p><strong>(3)</strong> <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em> (2018) by Devlin, Chang, Lee, and Toutanova, <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>

<p>Following the original transformer architecture, large language model research started to bifurcate in two directions: encoder-style transformers for predictive modeling tasks such as text classification and decoder-style transformers for generative modeling tasks such as translation, summarization, and other forms of text creation.</p>

<p>The BERT paper above introduces the original concept of masked-language modeling, and next-sentence prediction remains an influential decoder-style architecture. If you are interested in this research branch, I recommend following up with <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>, which simplified the pretraining objectives by removing the next-sentence prediction tasks.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/bert.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a> </center>

<hr/>



<p><strong>(4)</strong> <em>Improving Language Understanding by Generative Pre-Training</em> (2018) by Radford and Narasimhan, <a href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035">https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035</a></p>

<p>The original GPT paper introduced the popular decoder-style architecture and pretraining via next-word prediction. Where BERT can be considered a bidirectional transformer due to its masked language model pretraining objective, GPT is a unidirectional, autoregressive model. While GPT embeddings can also be used for classification, the GPT approach is at the core of today’s most influential LLMs, such as chatGPT.</p>

<p>If you are interested in this research branch, I recommend following up with the <a href="https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe">GPT-2</a> and <a href="https://arxiv.org/abs/2005.14165">GPT-3</a> papers. In addition, we will cover the InstructGPT approach later as a separate entry.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/gpt.png" alt=""/></p>
<center> Source: <a href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035">https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035</a> </center>

<hr/>



<p><strong>(5)</strong> <em>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</em> (2019), by Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer, <a href="https://arxiv.org/abs/1910.13461">https://arxiv.org/abs/1910.13461</a>.</p>

<p>As mentioned earlier, BERT-type encoder-style LLMs are usually preferred for predictive modeling tasks, whereas GPT-type decoder-style LLMs are better at generating texts. To get the best of both worlds, the BART paper above combines both the encoder and decoder parts (not unlike the original transformer – the second paper in this list).</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/bart.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/1910.13461">https://arxiv.org/abs/1910.13461</a> </center>
      <h2 id="scaling-laws-and-improving-efficiency">
        
        
          Scaling Laws and Improving Efficiency <a href="#scaling-laws-and-improving-efficiency">#</a>
        
        
      </h2>
    

<p>If you want to learn more about the various techniques to improve the efficiency of transformers, I recommend the <a href="https://arxiv.org/abs/2009.06732">2020 <em>Efficient Transformers: A Survey</em></a> paper followed by the <a href="https://arxiv.org/abs/2302.01107">2023 <em>A Survey on Efficient Training of Transformers</em></a> paper.</p>

<p>In addition, below are two papers that I found particularly interesting and worth reading.</p>

<p><strong>(6)</strong> <em>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</em> (2022), by Dao, Fu, Ermon, Rudra, and Ré, <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a>.</p>

<p>While most transformer papers don’t bother about replacing the original scaled dot product mechanism for implementing self-attention, FlashAttention is one mechanism I have seen most often referenced lately.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/flash-attention.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a> </center>

<hr/>



<p><strong>(7)</strong> <em>Cramming: Training a Language Model on a Single GPU in One Day (2022) by Geiping and Goldstein</em>, <a href="https://arxiv.org/abs/2212.14034">https://arxiv.org/abs/2212.14034</a>.</p>

<p>In this paper, the researchers trained a masked language model / encoder-style LLM (here: BERT) for 24h on a single GPU. For comparison, the original 2018 BERT paper trained it on 16 TPUs for four days.
An interesting insight is that while smaller models have higher throughput, smaller models also learn less efficiently. Thus, larger models do not require more training time to reach a specific predictive performance threshold.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/cramming.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/2212.14034">https://arxiv.org/abs/2212.14034</a> </center>

<hr/>



<p><strong>(8)</strong> <em>Training Compute-Optimal Large Language Models</em> (2022) by Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de Las Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre, <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>.</p>

<p>This paper introduces the 70-billion parameter Chinchilla model that outperforms the popular 175-billion parameter GPT-3 model on generative modeling tasks. However, its main punchline is that contemporary large language models are “significantly undertrained.”</p>

<p>The paper defines the linear scaling law for large language model training. For example, while Chinchilla is only half the size of GPT-3, it outperformed GPT-3 because it was trained on 1.4 trillion (instead of just 300 billion) tokens. In other words, the number of training tokens is as vital as the model size.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/chinchilla.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a> </center>
      <h2 id="alignment--steering-large-language-models-to-intended-goals-and-interests">
        
        
          Alignment – Steering Large Language Models to Intended Goals and Interests <a href="#alignment--steering-large-language-models-to-intended-goals-and-interests">#</a>
        
        
      </h2>
    

<p>In recent years, we have seen many relatively capable large language models that can generate realistic texts (for example, GPT-3 and Chinchilla, among others). It seems that we have reached a ceiling in terms of what we can achieve with the commonly used pretraining paradigms.</p>

<p>To make language models more helpful and reduce misinformation and harmful language, researchers designed additional training paradigms to fine-tune the pretrained base models.</p>

<p><strong>(9)</strong>  <em>Training Language Models to Follow Instructions with Human Feedback</em> (2022) by by Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe, <a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a>.</p>

<p>In this so-called InstructGPT paper, the researchers use a reinforcement learning mechanism with humans in the loop (RLHF). They start with a pretrained GPT-3 base model and fine-tune it further using supervised learning on prompt-response pairs generated by humans (Step 1). Next, they ask humans to rank model outputs to train a reward model (step 2). Finally, they use the reward model to update the pretrained and fine-tuned GPT-3 model using reinforcement learning via proximal policy optimization (step 3).</p>

<p>As a sidenote, this paper is also known as the paper describing the idea behind ChatGPT – according to the recent rumors, ChatGPT is a scaled-up version of InstructGPT that has been fine-tuned on a larger dataset.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/instruct-gpt.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a> </center>

<hr/>



<p><strong>(10)</strong> <em>Constitutional AI: Harmlessness from AI Feedback</em> (2022) by Yuntao, Saurav, Sandipan, Amanda, Jackson, Jones, Chen, Anna, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, El Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, Kaplan, <a href="https://arxiv.org/abs/2212.08073">https://arxiv.org/abs/2212.08073</a>.</p>

<p>In this paper, the researchers are taking the alignment idea one step further, proposing a training mechanism for creating a “harmless” AI system. Instead of direct human supervision, the researchers propose a self-training mechanism that is based on a list of rules (which are provided by a human). Similar to the InstructGPT paper mentioned above, the proposed method uses a reinforcement learning approach.</p>

<p><img src="https://sebastianraschka.com/images/blog/2023/llm-reading-list/constitutional-ai.png" alt=""/></p>
<center> Source: <a href="https://arxiv.org/abs/2212.08073">https://arxiv.org/abs/2212.08073</a> </center>
      <h2 id="conclusion">
        
        
          Conclusion <a href="#conclusion">#</a>
        
        
      </h2>
    

<p>I tried to keep the list above nice and concise, focusing on the top-10 papers to understand the design, constraints, and evolution behind contemporary large language models.</p>

<p>For further reading, I suggest following the references in the papers mentioned above. Or, to give you some additional pointers, here are some additional resources:</p>

<p><strong>Open-source alternatives to GPT</strong></p>

<ul>
  <li>
    <p><em>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</em> (2022),  <a href="https://arxiv.org/abs/2211.05100">https://arxiv.org/abs/2211.05100</a></p>
  </li>
  <li>
    <p><em>OPT: Open Pre-trained Transformer Language Models</em> (2022), <a href="https://arxiv.org/abs/2205.01068">https://arxiv.org/abs/2205.01068</a></p>
  </li>
</ul>

<p><strong>ChatGPT alternatives</strong></p>

<ul>
  <li>
    <p><em>LaMDA: Language Models for Dialog Applications</em> (2022), <a href="https://arxiv.org/abs/2201.08239">https://arxiv.org/abs/2201.08239</a></p>
  </li>
  <li>
    <p>(Sparrow) <em>Improving Alignment of Dialogue Agents via Targeted Human Judgements</em> (2022), <a href="https://arxiv.org/abs/2209.14375">https://arxiv.org/abs/2209.14375</a></p>
  </li>
  <li>
    <p><em>BlenderBot 3: A Deployed Conversational Agent that Continually Learns to Responsibly Rngage</em>, <a href="https://arxiv.org/abs/2208.03188">https://arxiv.org/abs/2208.03188</a></p>
  </li>
</ul>

<p><strong>Large language models in computational biology</strong></p>

<ul>
  <li>
    <p><em>ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing</em> (2021), <a href="https://arxiv.org/abs/2007.06225">https://arxiv.org/abs/2007.06225</a></p>
  </li>
  <li>
    <p><em>Highly Accurate Protein Structure Prediction with AlphaFold</em> (2021), <a href="https://www.nature.com/articles/s41586-021-03819-2">https://www.nature.com/articles/s41586-021-03819-2</a></p>
  </li>
  <li>
    <p><em>Large Language Models Generate Functional Protein Sequences Across Diverse Families</em> (2023), <a href="https://www.nature.com/articles/s41587-022-01618-2">https://www.nature.com/articles/s41587-022-01618-2</a></p>
  </li>
</ul>

    </article>


  <!--<strong>Have feedback on this post? I would love to hear it. Let me know and send me a <a href="https://twitter.com/intent/tweet?text=. @rasbt http://sebastianraschka.com/blog/2023/llm-reading-list.html">tweet</a> or <a href="http://sebastianraschka.com/email.html">email</a>.</strong>-->

</div>
      </div>
    </div></div>
  </body>
</html>
