<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://supabase.com/blog/postgres-language-server-implementing-parser">Original</a>
    <h1>Postgres Language Server: Implementing the Parser</h1>
    
    <div id="readability-page-1" class="page"><article><div><p><img alt="Postgres Language Server: implementing the Parser" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="/_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=640&amp;q=75 640w, /_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=750&amp;q=75 750w, /_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=828&amp;q=75 828w, /_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=1080&amp;q=75 1080w, /_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=1200&amp;q=75 1200w, /_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=1920&amp;q=75 1920w, /_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=2048&amp;q=75 2048w, /_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=3840&amp;q=75 3840w" src="https://supabase.com/_next/image?url=%2Fimages%2Fblog%2Flwx-postgres-language-server%2Fpostgres-language-server-thumb.jpg&amp;w=3840&amp;q=75"/></p>
<p>During our previous Launch Week we <a href="https://news.ycombinator.com/item?id=37020610">announced</a> the development of a <a href="https://github.com/supabase/postgres_lsp">Postgres Language Server</a>. The response was more enthusiastic than we imagined and we received some excellent ideas.</p>
<p>This is an update on the Parser, which is the fundamental primitive of a Language Server. We’ve been through several iterations of the parser and we want to detail our explorations.</p>
<!-- -->
<details><summary>Want to learn some more acronyms?</summary><p>There will be a few acronyms in this post. We don’t want this post to be “inside baseball” so here are a few concepts that you need to know:</p><ul>
<li><em><strong>LSP / Language Server Protocol:</strong></em> Something that helps code editors understand code better. It provides functionality like linting and error detection.</li>
<li><strong>Postgres Language Server:</strong> a server that will help with Postgres programing - writing SQL, type inference, etc.</li>
<li><strong>Parser</strong>: A parser converts written code into a form that tools can work with. For example, it identifies variables in code. and then the tools can easily access those variables. We’ll describe this more below.</li>
</ul></details>
<h2 id="background-why-build-a-language-server">Background: Why build a Language Server?</h2>
<p>Postgres is gaining popularity, and yet the tooling around it still needs a lot of work. Writing SQL in editors like VSCode is a pain. One of the unique features of Supabase is the ability to access your Postgres database from a browser or mobile app through our <a href="https://supabase.com/docs/guides/api">Data APIs</a>. This means that developers are writing more <a href="https://www.postgresql.org/docs/current/plpgsql.html">PL/pgSQL</a>.</p>
<p>While code editors have great support for most programming languages, SQL support is underwhelming. We want to make Postgres as simple as Python.</p>
<h2 id="language-servers-core-the-role-of-the-parser">Language Server&#39;s Core: The Role of the Parser</h2>
<p>On the highest level, a language server is a thing which</p>
<ol>
<li>accepts input source code from the client</li>
<li>turns it into a semantic model of the code</li>
<li>and provides language-specific smarts back to the client.</li>
</ol>
<figure><div aria-owns="rmiz-modal-" data-rmiz=""></div></figure>
<p>The parser is the core of every language server. It takes the raw string, turns it into a stream of tokens, and builds a syntax tree. This syntax tree can then be used to extract structural information.</p>
<p>Usually, the parser builds a concrete <a href="https://en.wikipedia.org/wiki/Parse_tree">syntax tree</a> (CST) before turning it into an <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">abstract syntax tree</a> (AST). A CST preserves all syntax elements present in the source code with the goal of being able to re-create the exact original source, while an AST contains only the meaning of the source. For example, take a simple expression <code>2 * (7 + 3)</code>:</p>
<!-- -->

<h2 id="implementing-a-parser-for-postgres">Implementing a Parser for Postgres</h2>
<p>Implementing a parser for Postgres is challenging because of the ever-evolving and complex syntax of Postgres. Even <code>select</code> statements are very complex to parse. Then there are common table expressions, sub-queries and the like. This is one of the reasons why existing Postgres tooling is scarce, badly maintained, and often does not work well.</p>
<p>We decided to not create a custom parser. Instead, we leverage <a href="https://github.com/pganalyze/libpg_query">libpg_query</a> to parse SQL code reliably. The pganalyze team has a published a great blog post on <a href="https://pganalyze.com/blog/parse-postgresql-queries-in-ruby">why this approach is preferable</a>.</p>
<p>However, libpg<em>query is designed to parse _executable</em> SQL — not to provide language intelligence. Using it for a language server means adapting it to our specific use case. Let’s explore how we adapted it:</p>
<h3 id="tokenization">Tokenization</h3>
<p>Before any syntax tree can be built, the input string needs to be converted into a stream of tokens. libpg_query exposes a <code>scan</code> API that returns all non-whitespace tokens of the source, even for invalid SQL. For example, a simple statement <code>select &#39;1&#39;;</code> returns</p>

<p>Every <code>ScanToken</code> token contains its variant, a range, and a keyword kind. To simplify the implementation of the parser, we extract the text for each token using the range for now. We arrive at the following <code>Token</code> struct.</p>

<p>To have a complete token stream, we merge the results of <code>scan</code> with a list of all whitespace tokens in the source, where the latter are extracted by a simple regular expression. For a simple statement <code>select &#39;1&#39;;</code>, the following tokens are returned by the lexer.</p>

<h3 id="conversion-to-a-syntax-tree">Conversion to a Syntax Tree</h3>
<p>To transform the stream of tokens into a syntax tree, we face the first challenges with <code>libpg_query</code>. In a language server, it&#39;s important to handle incomplete or improperly formatted input gracefully. When an error occurs you don’t want the parser to “stop”. You want it to check for <em>all</em> errors in your code:</p>

<p>Unfortunately, the <code>parse</code> api from <code>libpg_query</code> only parses the entire input — if any SQL statement contains a syntax error, an error is returned for the entire input.</p>
<p>To overcome this, we implemented a resilient <code>source</code> parser. This parser breaks the input into individual SQL statements and parses them one by one, allowing us to handle syntax errors within each statement independently. It is implemented as a top-down <a href="https://en.wikipedia.org/wiki/LL_parser">LL parser</a>. Specifically, the parser iterates the token stream left-to-right, and checks if the cursor currently is at the start of a new statement. Once a statement is entered, it walks the tokens until <code>;</code> or another statement start is encountered while skipping sub-statements.</p>
<p>Luckily, Postgres statements always start with distinct keywords. An update statement is identifiable with <code>update</code>, a delete statement with <code>delete from</code>. There are a few statements that need more than the first few tokens to be distinguishable, but we only care about whether there is a statement, and not what statement there is exactly, so ambiguity is very much acceptable.</p>
<p>For the implementation, we only need to provide the distinct keywords each statement starts with and compare it to the current tokens using a lookahead mechanism.</p>

<h3 id="reverse-engineering-the-cst">Reverse-Engineering the CST</h3>
<p>The second limitation we encountered: <code>libpg_query</code> only exposes an API for the AST, not for the CST. To provide language intelligence on the source code, both are required. Since we do not want to implement our own parser, we need to work with what we have to build the CST: the AST and a stream of tokens. The goal is to reverse-engineer the AST into the CST. This involves re-using the AST nodes as CST nodes, and figuring out what token belongs beneath what node. The exemplary statement <code>select &#39;1&#39;;</code> should be be parsed into</p>

<p>To do that, we need to know the range of every node that is within the AST.</p>
<p>We made <em>numerous</em> iterations over the past few months to figure out how to accomplish this with minimal manual implementations. Before diving into details, lets take a closer look at the <code>parse</code> API of <code>libpg_query</code>. For the exemplary statement above, it returns (simplified for readability):</p>

<p>There are a few limitations:</p>
<ol>
<li>Some nodes do have a <code>location</code> property that indicates where the node starts in the source, but not all.</li>
<li>There is no information on the range of a node within the source.</li>
<li>Some locations are not what you would expect. For example the location of the expression node is the location of the operator, not the start of the left expression.</li>
</ol>
<p>To summarise: <em>we need a range for each node, and we only have a start position, but not always, and sometimes it is wrong.</em></p>
<p>Our first very iterations were naive. We explored what information we could extract from <code>scan</code> and <code>parse</code>, and if anything can help in reverse-engineering the CST.</p>
<p>It turns out, the most reliable way of determining the range of a node is by knowing all <strong>properties</strong> of that node, and its position in the tree.</p>
<p>A property is any text for which a Token can potentially be found in the source code. For example, a <code>SelectStmt</code> node has the <code>select</code> keyword as a property, and if there is a <code>from_clause</code>, a <code>from</code> keyword. For the exemplary statement above, the properties are</p>

<p>Note that we do not have an extra <code>String</code> node, and instead add its properties to the parent <code>AConst</code> node. This reason is that a <code>String</code> node does not bring any value to the CST, since we already know that <code>‘1&#39;</code> is a string from the kind of the respective <code>Token</code>. The same is true for all nodes that just contain type information such as <code>Integer</code>, <code>Boolean</code> and <code>Float</code>.</p>
<p>The position of any node is the same in AST and CST, and thereby can be reflected from it.</p>
<h3 id="implementation">Implementation</h3>
<p>Before the actual parsing begins, the AST returned by <code>parse</code> is converted into an uniform tree structure where each node holds the kind of the node, a list of properties, the depth and, if available, the location.</p>
<p>For example, for <code>select &#39;1&#39;;</code>:</p>

<p>To implement such a conversion we need a way to</p>
<ul>
<li>get an <code>Option&lt;usize&gt;</code> with the location for every node type, if any</li>
<li>compile a list of all properties that a node contains</li>
<li>walk down the AST until the leaves are reached</li>
</ul>
<p>Due to the strict type system of Rust, a manual implementation would be a significant and repetitive effort. With languages such as JavaScript, getting the location of a node would be as simple as <code>node?.location</code>. In Rust, a large match statement covering all possible nodes is required to do the same. Luckily, <code>libpg_query</code> exports a protobuf definition containing all AST nodes and their fields. For example, an <code>AExpr</code> node is defined as</p>

<p>We introspect that definition to generate code at build time using <a href="https://doc.rust-lang.org/reference/procedural-macros.html">procedural macros</a>.</p>
<p>Leveraging the powerful repetition feature of the <code>[quote](https://github.com/dtolnay/quote)</code> crate, the <code>match</code> statement of a <code>get_location</code> function can be implemented with just a few lines of code.</p>

<p>The <code>location_idents</code> function iterates all nodes, searches for a <code>location</code> property in the protobuf definition for each node, and returns a <code>TokenStream</code> with either <code>Some(n.location)</code> or <code>None</code> for each.</p>

<p>Similarly, we can generate code to recursively walk down the <code>FieldType == Node</code> and <code>FieldType == Vec&lt;Node&gt;</code> properties of the AST nodes. No manual work required.</p>
<p>Even the function that returns all properties for a node can be generated, at least partly. All AST fields of <code>FieldType == String</code> can always be added to the list of properties. In the example above, the <code>sval: “1”</code> of the <code>String</code> node makes up the properties of its parent, the <code>AConst</code> node. What is remaining are mostly just the keywords that need to be defined for every node. A <code>SelectStmt</code> node has the <code>select</code> keyword as a property, and if there is a <code>from_clause</code>, a <code>from</code> keyword.</p>


<h3 id="parsing-a-statement">Parsing a Statement</h3>
<p>After the tree has been generated, the parser goes through the tokens and finds the node in whose properties the current token can be found. But not every node is a possible successor. Lets look how the parser builds the CST for the statement <code>select &#39;1&#39; from contact</code> at a high level.</p>
<p>We start with the full tree, and all tokens:</p>
<!-- -->

<p>Starting with the root node, the parser first searches the current node for the token. In this case, with success. <code>Select</code> is removed from <code>SelectStmt</code> .</p>
<p>In the next iteration, we search for <code>&#39;1&#39;</code>. Since its not in the current node, a breadth-first search is used to find the property within children nodes. We arrive at <code>AConst</code> , open all nodes we encountered along the way, and advance.</p>
<!-- -->

<p>Since we arrived at a leaf node with no properties left, the next token can not be part of this node or any child. It can be closed immediately after advancing the token. We remove it from the tree and set the current node to its parent. The same now applies to <code>ResTarget</code>, so we arrive back at <code>SelectStmt</code>:</p>
<!-- -->

<p>The <code>from</code> token can once again be found within the current node and we just advance the parser. Since <code>SelectStmt</code> is not a leaf node, we stay where we are.</p>
<!-- -->

<p>From here, it repeats itself: <code>contact</code> is found within <code>RangeVar</code> using a breadth-first search. It becomes a leaf node that is closed after applying the token. Since no tokens are left, we finish parsing by closing <code>SelectStmt</code>, resulting in:</p>
<!-- -->

<p>Keep in mind, this illustration shows you only an overview of the process.If you are interested in the details, take a look at the source code <a href="https://github.com/supabase/postgres_lsp/blob/2bb50e7b0733bd4cf630aba453d5132d03dfc113/crates/parser/src/parse/libpg_query_node.rs">here</a>.</p>
<p>You may have noticed that neither <code>location</code> nor <code>depth</code> were mentioned. Both are only used to improve performance and safeguard. Among other things, branches with nodes behind the current position of the parser are skipped. Further, the parser panics when a node is opened and either its position or its depth does not match the current state of the parser. This means that the returned CST is guaranteed to be correct.</p>
<h3 id="limitations">Limitations</h3>
<p>If the SQL is invalid and <code>parse</code> returns an error, the returned CST is just a flat list of tokens. Consequently, the statement parser is not resilient. This is not great, but we have intentionally implemented it so that custom and resilient implementations can be added statement by statement later.</p>
<p>Ultimately, we want the <code>libpg_query</code>-based parser to just serve as a fallback. For now however, our goal is to provide a usable language server as fast as possible. And even if some intelligence features will only work on valid statements, we believe it is still better than what we have today: no intelligence at all.</p>
<h2 id="next-steps">Next Steps</h2>
<p>There are some minor improvements remaining for the parser. But the largest part are the manual implementations missing in <code>get_node_properties</code> . Its a time-consuming effort, and <strong><em>we would love to get help from the community</em></strong> here. Check out <a href="https://github.com/supabase/postgres_lsp/issues/51">this issue</a> if you like to support.</p>
<p>After that, we will move on to the semantic data model and the language server itself. Other parser features such as support for <a href="https://www.postgresql.org/docs/current/plpgsql.html">PL/pgSQL</a> function body parsing will be added later. We want to get this project into a usable state as fast as possible.</p></div></article></div>
  </body>
</html>
