<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.scannedinavian.com/monoidal-hashing.html">Original</a>
    <h1>Monoidal Hashing</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
	    
	    <p>
    Posted on 2025-12-03
    
</p>


<p>Can I beat rsync for both cpu and network costs when a file on the server and client are different?
That is, can I find the smallest difference between a file on the server and client, and send only the change, like rsync?</p>
<p>As of&lt;2025-12-03 Wed&gt;, I don’t know for sure, but I think it’s likely.</p>

<p>Originally, rsync used a sliding window was hashed, moving byte by byte. This was very slow. (but could use many cores/SIMD!)</p>
<p>The naive approach of breaking a file into 8k chunks fails if someone
adds a single new byte at the beginning. This is called the <strong>boundary
shift problem</strong>.</p>

<p><a href="https://en.wikipedia.org/wiki/Rolling_hash#Gear_fingerprint_and_content-based_chunking_algorithm_FastCDC">Content defined chunking</a> (CDC) attempts to describe chunks by their content.</p>
<p>The input is incrementally hashed, and a chunk boundary is created
when the hash satisifies some condition like “ends with two zeros”.</p>
<p>You can change the condition to change the average size of the blocks.
Maybe you create a block boundary when there’s only one zero at the end because you want big blocks?</p>
<p>But, CDC still uses expensive re-hashing of the input bytes.</p>
<p>Hash functions with <a href="https://en.wikipedia.org/wiki/Rolling_hash#Rabin_fingerprint">constant-time “update” operations</a> have decreased hashing costs, but CDC hashing is not associative, so you can’t use multiple cores!</p>
<p>That is, any change inside the stream will affect at least the boundary of this block and possibly the next block <a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>Most of the research on CDC has worked towards the goal of producing consistently sized chunks roughly within chosen limits.</p>

<p>A <a href="https://en.wikipedia.org/wiki/Monoid">monoid</a> is a binary associative operation with an identity.</p>
<p>Binary means the operation takes two inputs and produces one output.</p>
<p>Associative means <code>(a * (b * c))</code> gives the same result as <code>((a * b) * c)</code>.</p>
<p>Intuitive examples include addition with zero as the identity, and multiplication with one as the identity.</p>
<p>The downside of the those two examples is that addition and
multiplication are commutative, meaning <code>a * b</code> is the same as <code>b * a</code>
but monoids aren’t commutative! Because of that I consider string
concatenation a better example.</p>
<p>If <code>*</code> means ‘squish two strings together’, <code>((&#34;Monoids &#34; * &#34;are&#34;) * &#34; awesome&#34;)</code> will be the same as <code>(&#34;Monoids &#34; * (&#34;are&#34; * &#34; awesome&#34;))</code>
but you can see that changing the order of the inputs will give you a
different string.</p>

<p>Enter <a href="https://github.com/benwr/bromberg_sl2?tab=readme-ov-file#what-is-this-library-for">monoidal hashing</a>! My favorite arxiv intro is <a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> “Girth of the Cayley graph and Cayley hash functions”.</p>
<p>I found a neat <a href="https://github.com/benwr/bromberg_sl2?tab=readme-ov-file#what-is-this-library-for">rust crate</a> that does monoidal hashing, this is from the docs:</p>
<blockquote>
<p>This library implements a putatively-strong hash function H with the
useful property that it gives a monoid homomorphism. This means there
is a cheap operation <code>*</code> such that given strings <code>s1</code> and <code>s2</code>,
<code>H(s1 ++ s2)= H(s1) * H(s2).</code></p>
</blockquote>
<p>That means we can choose <strong>any</strong> blocksize we want, use as many cores/SIMD as we want, and it all works!</p>
<p>Cayley hashing is even <strong>bit</strong> level rather than byte level, so can detect bit flips (cosmic rays?).</p>

<p>But <strong>exactly how</strong> do we use this for deduplication?</p>
<p>A monoidal hash is superior to a <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle Tree</a>, because you will get a
different root hash from a Merkle Tree for different block sizes,
where a monoidal hash will always give the same root hash at any block
size.</p>
<p>(But Merkle Trees have so far been cryptographically safe, while
monoidal / Cayley hashing has seen some crypto attacks, see the
history in <a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> “Girth of the Cayley graph and Cayley hash functions”)</p>
<p>I think there’s some sensible calculation that combines the bandwidth
delay product between the hosts with the size of the file to give you a sensible chunk size?</p>
<p>But how can you be sure to beat the boundary shift problem?</p>

<p>Everyday hash values can only be compared for equality. Monoidal hash values can be mappended together on the other end of the wire.</p>
<p>That means I don’t need to send a Merkle Tree root to see if the files are equal, I could send a monoidal hash for each half of the file and the other end could calculate their own root hash.</p>
<p>I could send any number of monoidal hashes with their offsets into the file, and the other end could quickly figure out which parts of the file differ.</p>
<p>We don’t send a root hash, we send ingredients for the root hash to the other side.
Both sides will still build a root hash, but there’s no point in sending it!</p>
<p>Right now I strongly suspect a <a href="https://en.wikipedia.org/wiki/Finger_tree">finger tree</a> of monoidal hashes will be faster than rsync’s <a href="https://rsync.samba.org/tech_report/node4.html">two level hash bucket approach</a>.</p>
<p>I figure both sides semi-randomly choose blocksizes and offsets, then pitch the hashes across the wire, starting with the largest and working down to smaller chunks.</p>
<p>The other end does the same, with some kind of “agreement” message when common chunks are found.</p>
<p>That is, choose an average ‘block size’, choose a bunch of intervals, and create something roughly equivalent to a Merkle Tree by hashing each of the intervals, and all combinations.</p>
<p>One important thing will be to exchange hashes such that the associative property allows the other side to reassemble some of the pieces.</p>
<p>For example, one side could choose seven byte blocks, the other end could choose five byte blocks, and they’d match up on 35 byte boundaries, if there weren’t any insertions or deletions.</p>
<p>One benefit to monoidal hashes over content defined chunking is that you get associativity and you can still use multiple cores to hash file contents.</p>

<ul>
<li><label>hack it all together</label></li>
</ul>
<p>I planned to publish this months ago, but got distracted, so maybe someone else will be inspired to try this before I get back to it?</p>

<p>Before I learned about monoidal hashing, I expected <a href="https://inria.hal.science/hal-02303490/file/paper%20%281%29.pdf">Merkle Search Trees</a> to be the best approach.</p>
<p>I don’t believe that’s the case now, but I do plan on using them for building “appendables” to ‘diff’ encrypted data for a nifty work project!</p>
<p>You’ll probably enjoy reading about Merkle Search Trees if you like the ideas around monoidal hashing.</p>



	</div></div>
  </body>
</html>
