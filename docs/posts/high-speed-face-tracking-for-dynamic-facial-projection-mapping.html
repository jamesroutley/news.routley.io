<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.vision.ict.e.titech.ac.jp/projects/DFPM/">Original</a>
    <h1>High-Speed Face-Tracking for Dynamic Facial Projection Mapping</h1>
    
    <div id="readability-page-1" class="page"><div>
                                <div>
                                    <p><img alt="image" src="https://www.vision.ict.e.titech.ac.jp/projects/DFPM/YouTube_Thumbnail.jpg"/> </p>
                                     
                                    <iframe width="560" height="315" src="https://www.youtube.com/embed/wOVJYhBQNt8?autoplay=1" title="Dynamic Facial Projection Mapping by High-Speed Face-Tracking Method and Lens-Shift Co-Axial Setup" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
                                </div>
                                
                                <p>
                                    Dynamic Facial Projection Mapping (DFPM) overlays computer-generated images onto human faces to create immersive experiences that have been used in the makeup and entertainment industries. In this study, we propose two concepts to reduce the misalignment artifacts between projected images and target faces, which is a persistent challenge for DFPM. 
                                </p>

                                <center>
                                    <img src="https://www.vision.ict.e.titech.ac.jp/projects/DFPM/01.jpg" width="100%"/></center>

                                <p>
                                    Our first concept is a high-speed face-tracking method that exploits temporal information. We first introduce a cropped-area-limited inter/extrapolation-based face detection framework, which allows parallel execution with facial landmark detection. We then propose a novel hybrid facial landmark detection method that combines fast Ensemble of Regression Trees (ERT)-based detections and an auxiliary detection. ERT-based detections rapidly produce results in 0.107 ms using temporal information with the support of auxiliary detection to recover from detection errors. To train the facial landmark detection method, we propose an innovative method for simulating high-frame-rate video annotations to address the lack of publicly available high-frame-rate annotated datasets. 
                                </p>
                               

                            <center>
                                <img src="https://www.vision.ict.e.titech.ac.jp/projects/DFPM/03.jpg" width="100%"/><br/>
                            </center>

                            <p>
                                Our second concept is a lens-shift co-axial projector-camera setup that maintains a high optical alignment with only a 1.274-pixel error between 1 m and 2 m depth. This setup reduces misalignment by applying the same optical designs to the projector and camera without causing large misalignment as in conventional methods. 
                            </p>

                            <p>
                                Based on these concepts, we developed a novel high-speed DFPM system that achieves nearly perfect alignment with human visual perception.
                            </p>

                            <center>
                                <img src="https://www.vision.ict.e.titech.ac.jp/projects/DFPM/04.jpg" width="80%"/></center>


                            
                            <hr/>
                            <h4>Reference</h4>
                            <ul>
                                <li>
                                    Hao-Lun Peng, Kengo Sato, Soran Nakagawa, Yoshihiro Watanabe: Perceptually-Aligned Dynamic Facial Projection Mapping by High-Speed Face-Tracking Method and Lens-Shift Co-Axial Setup, IEEE Transactions on Visualization and Computer Graphics, <a href="https://ieeexplore.ieee.org/document/10845069">DOI: 10.1109/TVCG.2025.3527203</a> (2025)
                                </li>
                            </ul>
                            <hr/>
                            <h4>Related work</h4>
                            <ul>
                                <li>
                                    <a href="https://www.vision.ict.e.titech.ac.jp/projects/ColorMachine/">COLOR MACHINE: Makeup simulator based on dynamic projection mapping</a>
                                </li>
                                <li>
                                    <a href="http://www.vision.ict.e.titech.ac.jp/projects/faceDPM/">Dynamic Facial Projection Mapping -Basic Test-</a>
                                </li>
                                <li>
                                    <a href="https://www.vision.ict.e.titech.ac.jp/projects/WOW_TOKYO_AYABAMBI/">INORI -PRAYER-: Collaboration with WOW, TOKYO, and AyaBambi</a>
                                </li>
                            </ul>
                            </div></div>
  </body>
</html>
