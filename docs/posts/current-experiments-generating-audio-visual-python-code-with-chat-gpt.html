<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thewitchofendor.com/2023/10/13/current-experiments-generating-audio-visual-python-code-with-chat-gpt/">Original</a>
    <h1>Current Experiments: Generating Audio-Visual Python Code with Chat GPT</h1>
    
    <div id="readability-page-1" class="page"><div>
			
<p><a href="https://github.com/msrobot0/llmlexperiments/blob/main/October23/13/">Visit my repo</a> – also a better formatted readme – this one was generated by chat gpt </p>



		<figure>
			
			
		</figure>
		


<p>I’ve recently been profoundly inspired by the capabilities of large language models (LLMs) in generating code. My plan involves downloading an open-source model from Hugging Face, but for now, I’m excited to start this journey by utilizing Chat GPT.</p>



<p>This marks the inception of an experimental venture, and as I take these initial steps, I’m fully aware that I’m embarking on an unknown path.</p>



<h2 id="h-daily-experiment">Daily Experiment</h2>



<p>In my musical endeavors, I’ve been working with synthesizers for quite some time, often collaborating with my on-and-off band. We affectionately refer to our musical style as “making beeps and bloops.” Lately, my auditory landscape has been filled with drone music, ambient sounds, and the works of artists like Brian Eno. It’s got me thinking deeply about what synthesizers can achieve that traditional instruments cannot.</p>



<p>Defining what constitutes a “traditional” instrument is a task in itself. It relates to the idea of generating music from gestures, and while I recognize that working with synthesizers and twisting knobs is a form of gestural expression, I’m still exploring this concept.</p>



<p>I occasionally use analog synths, and I’m avoiding the binary/analog dichotomy, as it doesn’t fully capture the essence of what’s possible.</p>



<p>What’s truly compelling about synths is the degree of control they offer over audio frequencies and sound generation. This level of granularity allows for experimentation that I’ll delve into further at some point. For now, I’ll cryptically hint that sound becomes ‘unbundled’ from gesture, and perhaps even from traditional forms of musical performance.</p>



<p>So, here’s my central question: What unique capabilities do synthesizers possess that set them apart from traditional instruments?</p>



<p>Two particular thoughts intrigue me:</p>



<p>1) What does it feel like to exist within a single musical note?</p>



<p>In today’s experiment, and likely in many more to come, I’m delving into the idea of “living inside a note” through the concept of microtones. Microtones represent slight frequency variations within the framework of the Western musical note. While non-European cultures, and perhaps various folk traditions in Europe, embrace microtones, the level of granularity achievable with a synth transcends any known musical system. This is because microtones are continuous and defy conventional quantification, in contrast to the discrete and countable nature of traditionally named notes.</p>



<h2 id="h-audio-prompt-and-code">Audio: Prompt and Code</h2>



<p>Now, let’s explore the creation of audio in Python that delves into the realm of microtonal B-flat with a syncopated drone. This audio generation will be complemented by a generative animation using Python.</p>



<h2 id="h-generative-animation-with-pygame">Generative Animation with Pygame</h2>



<p>To create a generative animation that represents the audio generation, we’ll harness the power of Pygame, a popular library. But before we dive in, make sure to install Pygame:</p>



<pre><code>pip install pygame</code></pre>



<p>Now, let’s craft the Python script for our generative animation:</p>



<pre><code>import pygame
import random
import math

# Initialize Pygame
pygame.init()

# Screen dimensions
width, height = 800, 600
screen = pygame.display.set_mode((width, height))
pygame.display.set_caption(&#34;Generative Animation&#34;)

# Define colors with transparency
transparent_blue = (0, 0, 255, 128)
transparent_green = (0, 255, 0, 128)

# Create a clock to control frame rate
clock = pygame.time.Clock()

# Main loop
running = True
while running:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False

    # Clear the screen
    screen.fill((0, 0, 0))

    # Generate organic shapes
    for _ in range(10):
        x = random.randint(0, width)
        y = random.randint(0, height)
        radius = random.randint(10, 50)
        rotation = random.randint(0, 360)
        transparency = random.randint(50, 200)
        color = transparent_blue if random.random() &lt; 0.5 else transparent_green

        # Create organic shape
        surface = pygame.Surface((radius * 2, radius * 2), pygame.SRCALPHA)
        pygame.draw circle(surface, color, (radius, radius), radius)
        surface = pygame.transform.rotate(surface, rotation)

        screen.blit(surface, (x, y))

    # Update the display
    pygame.display.flip()

    # Limit the frame rate
    clock.tick(30)

pygame.quit()</code></pre>



<p>In this code:</p>



<ul>
<li>We’re utilizing Pygame to craft an animation enriched with organic shapes, primarily circles. These shapes possess a level of transparency, thanks to the <code>SRCALPHA</code> flag.</li>



<li>These organic shapes are randomly placed, rotated, and given a degree of transparency. The color choice alternates between transparent blue and transparent green.</li>



<li>The animation is showcased on the screen, and the frame rate is governed by the clock to ensure a smoother viewing experience.</li>
</ul>



<p>This code serves as a starting point. You can expand and modify it to create more intricate and captivating visuals to accompany your audio creation.</p>



<h2 id="h-the-audio-code-analysis">The Audio Code Analysis</h2>



<p>The code generated by Chat GPT didn’t perform as expected. I had to explore and discover undocumented features, such as the sine functionality in Pydub, which I hadn’t previously used. Eventually, I replaced the machine learning-generated code with something functional. It was a relatively straightforward process, taking less than 10 minutes. However, the result was somewhat underwhelming. My intention was to generate a 5-second audio piece, but what I obtained was more like a 1-second piece with 4 minutes of silence. It was quite avant-garde, reminiscent of Godard, but not the effect I was aiming for. Interestingly, after my audio sample, Aaron Copland’s “Quiet City” started playing. At first, I thought it was part of my audio sample and marveled at the apparent genius of my AI collaboration. It was only later that I realized the beautiful, ethereal music was Aaron Copland’s work. I highly recommend giving it a listen, although it was not what I initially intended.</p>



<h2 id="h-video-prompt-and-code">Video: Prompt and Code</h2>



<p>Now, let’s embark on the journey of creating a Python generative animation in blue tones, using organic shapes and transparency to represent the audio generation.</p>



<h2 id="h-creating-the-generative-animation">Creating the Generative Animation</h2>



<p>To craft a generative animation that mirrors the audio generation, we can employ libraries like Pygame. Here’s a simple example to get you started:</p>



<ol>
<li>First, make sure to install Pygame:</li>
</ol>



<pre><code>pip install pygame</code></pre>



<ol start="2">
<li>Now, let’s create a Python script for the generative animation:</li>
</ol>



<pre><code>import pygame
import random
import math

# Initialize Pygame
pygame.init()

# Screen dimensions
width, height = 800, 600
screen = pygame.display.set_mode((width, height))
pygame.display.set_caption(&#34;Generative Animation&#34;)

# Define colors with transparency
transparent_blue = (0, 0, 255, 128)
transparent_green = (0, 255, 0, 128)

# Create a clock to control frame rate
clock = pygame.time.Clock()

# Main loop
running = True
while running:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            running = False

    # Clear the screen
    screen.fill((0, 0, 0))

    # Generate organic shapes
    for _ in range(10):
        x = random.randint(0, width)
        y = random.randint(0, height)
        radius = random.randint(10, 50)
        rotation = random.randint(0, 360)
        transparency = random.randint(50, 200)
        color = transparent_blue if random.random() &lt; 0.5 else transparent_green

        # Create organic shape
        surface = pygame.Surface((radius * 2, radius * 2), pygame.SRCALPHA)
        pygame.draw.circle(surface, color, (radius, radius), radius)
        surface = pygame.transform.rotate(surface, rotation)

        screen.blit(surface, (x, y))

    # Update the display
    pygame.display.flip()

    # Limit the frame rate
    clock.tick(30)

pygame.quit()</code></pre>



<p>In this code:</p>



<ul>
<li>We use Pygame to create the animation, featuring organic shapes (circles) with transparency using the <code>SRCALPHA</code> flag.</li>



<li>The organic shapes are randomly placed, rotated, and given a degree of transparency. The choice of color alternates between transparent blue and transparent green.</li>



<li>The animation is displayed on the screen, and the frame rate is controlled with the clock for a smoother visual experience.</li>
</ul>



<p>This code provides a foundation, and you’re welcome to adapt and enhance it to create more complex and captivating visuals that complement the audio generated earlier.</p>



<h2 id="h-the-video-code-analysis">The Video Code Analysis</h2>



<p>While implementing the video generation, I encountered some initial challenges. I had to import <code>ImageFrame</code> from the Python Imaging Library (PIL) and comment out a line of audio code that referred to a non-existent audio reader.</p>



<p>During the process, I couldn’t resist making a few tweaks. I reduced the framerate to create a smoother appearance, increased the number and size of dust motes to enhance their overlap, and intentionally distorted the aspect ratio, giving the impression of a vintage television set from the 1950s to 1980s. While I wished for a more distinct blue tone and smoother transparency in grayscale, the final result may have been influenced by the properties of Pygame, the library used to create the video.</p>



<p>Unfortunately, the combination of audio and video didn’t perform seamlessly. I couldn’t hear the audio in QuickTime, and playing the video in VLC (VideoLAN Client) proved to be somewhat challenging. Consequently, I took the initiative to generate a new MP4 file by merging the video and audio using FFmpeg, bypassing Chat GPT:</p>



<pre><code>ffmpeg -i dust_motes_and_clouds.mp4 -i microtonal_bflat_syncopated_drone_5s.wav -c:v copy -map 0:v:0 -map 1:a:0 -c:a aac -b:a 192k output.mp4</code></pre>



<h2 id="h-conclusion">Conclusion</h2>



<p>This experimental journey proved to be both enjoyable and more time-consuming than anticipated. My hope is to turn this into a daily practice, with the expectation that it becomes more efficient in the future. It’s all part of the creative process and a testament to the dynamic world of art and technology.</p>



<p>OpenAI. (2023). <em>ChatGPT</em> (September 25 Version) [Large language model]. <a href="https://chat.openai.com" rel="nofollow">https://chat.openai.com</a></p>

					</div></div>
  </body>
</html>
