<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pdfa.org/a-case-study-in-pdf-forensics-the-epstein-pdfs/">Original</a>
    <h1>A case study in PDF forensics: The Epstein PDFs</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>The recent release of a <a href="https://www.justice.gov/epstein/doj-disclosures" target="_blank" rel="noopener">tranche of files</a> by the US Department of Justice (DoJ) under the “Epstein Files Transparency Act (H.R.4405)” has once again prompted many people to closely examine redacted and sanitized PDF documents. Our previous articles on the <a href="https://pdfa.org/corruption-in-pdf-federal-crimes-edition/">Manafort papers</a> and the <a href="https://pdfa.org/a-technical-and-cultural-assessment-of-the-mueller-report-pdf/">Mueller report</a>, as well as a study by Adhatarao, S. and Lauradoux, C. (2021) “<a href="https://doi.org/10.1145/3437880.3460405" target="_blank" rel="noopener">Exploitation and Sanitization of Hidden Data in PDF Files: Do Security Agencies Sanitize Their PDF files?</a>,” in <em>Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security</em>, illustrate the importance of robust sanitization and redaction workflows when handling sensitive documents prior to release.</p>
<p>This article examines a small random selection of the Epstein PDF files from a purely digital forensic perspective, focusing on the PDF syntax and idioms they contain, any malformations or unusual constructs, and other technical aspects.</p>
<p>PDFs are more challenging to analyze than many other formats because they are binary files that require specialized knowledge, expertise, and software. Please note that we did not analyze the contents of the PDF documents. Not every PDF was examined. Any mention of products (or appearance in screen-shots) does not imply any endorsement or support of any information, products, or providers whatsoever. We are not lawyers; this article does not constitute legal advice</p>
<p>We offer this information, in part, as some of the Epstein PDFs released by DoJ are beginning to appear on malware analysis sites (such as <a href="https://hybrid-analysis.com/sample/d8f6ba273d416f1b3160b94b5646374b218a9ecef9d2cb3ae33cd3788f8f862b/6946599fcca0461fcc056ab2" target="_blank" rel="noopener">Hybrid-Analysis</a>) with various kinds of incorrect analysis and misinformation.</p>
<h2 id="26-december-2025-update">26 December 2025 update</h2>
<p>After we&#39;d completed our analysis the DoJ released a new dataset, <a href="https://www.justice.gov/epstein/files/DataSet%208.zip">DataSet 8.zip</a>. This new ZIP file is 9.95 GB compressed and contains over 11,000 files, including 10,593 new PDFs totaling 1.8 GB and 29,343 pages (the longest document has 1,060 pages). DataSet 8 also contains many large MP4 movies, Excel spreadsheets, and various other files. The first PDF in the set of 10,593 PDFs is VOL00008\IMAGES\0001\EFTA00009676.pdf, and the last file is VOL00008\IMAGES\0011\EFTA00039023.pdf. A cursory analysis shows <span>pdfinfo</span> properties similar to those from the earlier datasets, but we have not otherwise analyzed this new dataset.</p>
<p>Since our original post, various social media and news platforms have also been announcing “recoverable redactions” from the “Epstein Files”. We stand by our analysis; DoJ has correctly redacted the EFTA PDFs in Datasets 01-07, and they <b><i>do not contain</i></b> recoverable text as alleged. As our article states, we did not analyze any other DoJ or Epstein-related documents.</p>
<p>For example, the featured image in <a href="https://www.theguardian.com/us-news/2025/dec/23/epstein-unredacted-files-social-media">this Guardian news article</a> (which was also picked up by the <a href="https://www.nytimes.com/2025/12/23/us/politics/epstein-files-redactions-doj.html">New York Times</a>) corresponds to VOL00004\IMAGES\0001EFTA00005855.pdf, as can be easily determined by searching for the Bates Numbers in the EFTA “.OPT” data files. The information in this EFTA PDF is <b><i>fully and correctly redacted</i></b>; there is <b><i>no hidden information</i></b>. The only extractable text is some garbled text from the poor-quality OCR and, as expected, the Bates Numbers on each page.</p>
<p>In the few reports we investigated (including from <a href="https://www.forbes.com/sites/daveywinder/2025/12/26/epstein-files-hacked---all-you-need-to-know">Forbes</a> and Ed Krassenstein on both <a href="https://x.com/EdKrassen/status/2003222444270661801">X (formerly Twitter)</a> and <a href="https://www.instagram.com/krassensteins?igsh=eGYyc3gwZXA2eW4=">Instagram</a>), these stories misrepresent <strong><i>other</i></strong> DoJ files that were <b><i>not</i></b> part of the major DataSets 01-07 release on December 19 under the EFTA. All PDFs released under EFTA have a Bates Number on every page starting &#34;EFTA&#34;. These include “<a href="https://www.justice.gov/multimedia/Court%20Records/Government%20of%20the%20United%20States%20Virgin%20Islands%20v.%20JPMorgan%20Chase%20Bank,%20N.A.,%20No.%20122-cv-10904%20(S.D.N.Y.%202022)/001-01.pdf">Case 1:22-cv-10904-JSR   Document 1-1,  Exhibit 1 to Government’s Complaint against JPMorgan Chase Bank, N.A.</a>” (see page 41) and “<a href="https://www.justice.gov/multimedia/Court%20Records/Matter%20of%20the%20Estate%20of%20Jeffrey%20E.%20Epstein,%20Deceased,%20No.%20ST-21-RV-00005%20(V.I.%20Super.%20Ct.%202021)/2022.03.17-1%20Exhibit%201.pdf">Case No: ST-20-CV-14 Government Exhibit 1</a>” (see page 19). These PDFs, previously released by the DoJ, do contain incorrect and ineffective redactions, with black boxes that simply obscure text, making “copy &amp; paste” easy to recover the text that&#39;s otherwise hidden. Clearly, DoJ processes and systems in the past have inadequately redacted information!</p>
<h2 id="the-files-we-examined">The files we examined</h2>
<p>The tranche released by DoJ on Friday, December 19 is available as seven “data sets”, most easily downloaded as seven ZIP archives totaling just under 2.97 GB. Each ZIP file contains a similar folder structure, with DataSet 6 being the odd one out with an extra top-level folder. Once unzipped, the total size is 2.99 GB. The tranche contains 4,085 PDF files, a single AVI (movie) file (located in the folder VOL00002\NATIVES\0001), and 2 data files (.DAT and .OPT) for each ZIP archive. The “.OPT” files appear to be CSV (<a href="https://en.wikipedia.org/wiki/Comma-separated_values" target="_blank" rel="noopener">Comma-Separated Values</a>) but lack a heading row, while the “.DAT” files contain information about the <a href="https://en.wikipedia.org/wiki/Bates_numbering" target="_blank" rel="noopener">Bates numbering</a>. The analysis we provide here is limited to the PDF files.</p>
<p>The PDF files are named and ordered sequentially within the folder structure, starting with “EFTA00000001.pdf” in VOL00001 and ending with “EFTA00009664.pdf” in VOL00007, indicating that <span><strong>at least 5,879 PDF files remain unreleased</strong></span>.</p>
<p>A random sampling of the PDFs for visual review suggests that they are a mix of single and multi-page full-page photos and scanned content. OCR (Optical Character Recognition) was used to provide some searchable and extractable text in at least some files. “Black box” style redactions (without text reasons) are apparent. When done correctly, this is the appropriate way to redact, far more robust than <a href="https://www.bitdefender.com/en-us/blog/hotforsecurity/stop-pixelating-new-tool-reveals-the-secrets-of-redacted-documents" target="_blank" rel="noopener">pixelating text</a>. The PDFs we sampled did not include any obviously “born digital” documents. Various news sites are reporting <a href="https://www.cbsnews.com/news/epstein-files-redaction-over-500-pages-entirely-blacked-out/" target="_blank" rel="noopener">very heavily redacted documents</a> within this tranche.</p>
<h2 id="file-validity">File validity</h2>
<p>A precursor to most forensic examinations is to establish whether the PDF files are technically valid (that is, conform to the rules of the PDF format), since analyzing malformed files can easily lead to incorrect results or wrong conclusions. Combining tools that use different methods provides the broadest possible information while ensuring that tooling limitations are fully understood. However, if the basic file structure or cross-reference information is incorrect, various software might then draw different conclusions and/or construct different Document Object Models (DOMs).</p>
<p>In addition to basic file structure, incremental updates (if any), and cross-reference information, PDF validity assessments include the objects that comprise the PDF’sDOM as well as the file structure, incremental updates, and cross-reference information. To assess relationships between objects in the PDF DOM, some forensic analysis tools leverage our <a href="https://github.com/pdf-association/arlington-pdf-model" target="_blank" rel="noopener">Arlington PDF Data Model</a>, while others use their own internal methods.</p>
<p>Our analysis of file validity, using a multitude of PDF forensic tools, identified only one minor defect (invalidity); 109 PDFs had a positive FontDescriptor <strong>Descent</strong> value rather than a negative one. This is a relatively common (but minor) error, typically associated with font substitution and font matching, that does not affect the validity of the files overall. One specific forensic tool reported a PDF version issue with some files, related to the document catalog <strong>Version</strong> entry, which prevented the tool from further verifying those specific PDFs.</p>
<h2 id="pdf-versions">PDF versions</h2>
<p>I’ve previously written about the <a href="https://pdfa.org/pdf-versions/">unreliability of PDF version numbers</a>. Still, for forensic purposes, they may provide insight into the DoJ’s software, and whether improved software could have performed better.</p>
<p>I used two different but commonly used PDF command-line <code>pdfinfo</code> utilities on different platforms (Windows and Ubuntu Linux) to summarize information about these PDF files. When run against the full tranche of PDFs, I got two very different sets of answers! Immediately, my <a href="https://en.wiktionary.org/wiki/Spidey_sense#English" target="_blank" rel="noopener">spidey senses</a> started to tingle, and I was once again reminded of a key lesson in digital document forensics – you should <em><span>never</span></em> trust a single tool!</p>
<table>
<tbody>
<tr>
<td><strong>Reported PDF Version</strong></td>
<td><strong>Count Tool A</strong></td>
<td><strong>Count Tool B</strong></td>
</tr>
<tr>
<td>1.3</td>
<td>209</td>
<td>3,817</td>
</tr>
<tr>
<td>1.4</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1.5</td>
<td>3,875</td>
<td>267</td>
</tr>
<tr>
<td><strong>TOTAL</strong> <em>(should be 4,085)</em></td>
<td>4,085</td>
<td>4,085</td>
</tr>
</tbody>
</table>

<p>The PDF version in the file header, “<code>%PDF-<em>x.y</em></code>”, is nominally the first line in every PDF file (based on the not-unreasonable assumption that the PDF files have no “junk bytes” before this PDF file identifier). Using the Linux command line, you can run in Linux “<code>head -n 1 <em>file.pdf</em></code>” to extract the first header line from each PDF and compare it with the reported results from each tool. Or run in Linux “<code>grep -P --text --byte-offset &#34;%PDF-\d\.\d&#34; *.pdf</code>” to confirm that there are no junk bytes prior to the PDF header line.</p>
<p>The reason for the difference reported in the table above is that Tool B is not accounting for the <strong>Version</strong> entry in the document catalog of PDFs with incremental updates. We’ll next investigate whether this is due to malformed files or a programming error. When properly accounting for incremental updates, however, Tool A is correct.</p>
<p>Using the same <code>pdfinfo</code> output (and again comparing results from both tools), we can also quickly establish the following facts:</p>
<ul>
<li>No PDF is tagged</li>
<li>No PDF is encrypted</li>
<li>No PDF is “optimized” (technically, Linearized PDF)</li>
<li>No PDF has any annotations</li>
<li>No PDF has any outlines (bookmarks)</li>
<li>No PDF contains any embedded files</li>
<li>None of the PDFs are forms</li>
<li>None of the PDFs contains JavaScript</li>
</ul>
<p>Page counts range from 1 (in 3,818 PDFs) to 119 pages (in two PDFs), totaling 9,659 pages across all 4,085 PDFs.</p>
<h2 id="incremental-updates">Incremental updates</h2>
<p>PDF’s incremental updates feature allows multiple revisions of a document to be stored in a PDF file. As the name implies, each set of deltas is appended to the original document, forming a chain of edits. When read by conforming PDF software, a PDF is <em><span>always</span></em> processed from the end of the file, effectively applying the deltas to the original document and to any previous incremental updates. Both the original document and each incremental update can be recognized by their respective “<code>xref</code>” and “<code>%%EOF</code>” markers (assuming that the PDF files are structured correctly).</p>
<p>For this investigation, we started by examining the very first PDF in the tranche: VOL00001\IMAGES\0001\EFTA00000001.pdf. This PDF had different PDF versions reported by different versions of <code>pdfinfo</code>. A simple trick to check if a PDF contains incremental updates is to search for these special markers while treating the PDF as a text file (<em>which it isn’t!</em>):</p>
<p><strong><code>$ grep -P --text -–byte-offset &#34;(xref)|(%%EOF)&#34; EFTA00000001.pdf</code></strong></p>
<p>These results (sorted by byte offset) indicate that EFTA00000001.pdf contains <em><span>two incremental updates after the original file</span></em>. The lack of an “<code>xref</code>” marker before the last two “<code>startxref</code>” markers indicate that neither incremental updates uses conventional cross-reference data, but may use cross-reference streams (if any objects are changed).</p>
<h2 id="bates-numbering">Bates numbering</h2>
<p>As referenced above, Bates numbering is the process by which every page is assigned a unique identifier. For this tranche of Epstein PDF files, Bates numbers were added to each page via a separate incremental update, as shown below in <a href="https://pdfa.org/resource/vscode-extension-pdf-cos-syntax-support/">Visual Studio Code with my pdf-cos-syntax extension</a>. Note that DoJ’s PDFs are primarily text-based internally, making forensic analysis a lot easier - and the files a lot bigger.</p>
<p><img decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/2984-3037.png" alt="Screenshot of VS Code discussed in the text." width="766" height="1284" srcset="https://pdfa.org/wp-content/uploads/2025/12/2984-3037.png 766w, https://pdfa.org/wp-content/uploads/2025/12/2984-3037-179x300.png 179w, https://pdfa.org/wp-content/uploads/2025/12/2984-3037-611x1024.png 611w, https://pdfa.org/wp-content/uploads/2025/12/2984-3037-600x1006.png 600w" sizes="(max-width: 766px) 100vw, 766px"/></p>
<p>Observations:</p>
<ul>
<li>Line 2984 is the end-of-file marker for the file version, and line 2985 starts a new incremental update section.</li>
<li>Lines 2985-2987 define object 26, the unembedded Helvetica font resource used by the Bates number.</li>
<li>Lines 2997-3020 are the modified page object (object 3), replacing the page object in previous revisions of the file.</li>
<li>Line 2999 is the page Contents array, comprising five separate content streams, with the 3rd stream (object 29) being the Bates numbering added in this incremental update. Object 30 is an empty content stream that could have been removed by an optimization process.</li>
<li>Line 3034 sets the Helvetica font to 12 point.</li>
<li>Line 3037 uses a hexadecimal string to paint the Bates number onto the page.</li>
</ul>
<p>The idiom for this final incremental update, which adds the Bates number to every page, appears in all the PDF files we selected at random for investigation. This specific incremental update always uses a cross-reference stream (<code>/Type /XRef</code>) and relies on the previous incremental update, in which the document catalog <strong>Version</strong> entry is set to PDF 1.5.</p>
<h2 id="the-first-incremental-update">The first incremental update</h2>
<p>The VSCode pdf-cos-syntax extension also indicates (correctly!) that the original PDF is missing the required (when the PDF contains binary data, which most do) comment as the second line of the file that indicates to software that the PDF file needs to be treated as binary data (ISO 32000-2:2020, §7.5.2). Although the missing comment does not make the PDF invalid per se, without such a marker close to the top of each PDF, software may think the PDF is a text file, and thus potentially corrupt the PDF by changing line endings, which would break the byte offsets in the cross-reference data. In this PDF, the first incremental update adds this marker comment after a lot of binary data, which is pointless.</p>
<p>As mentioned above, the first incremental update changed the document catalog <strong>Version</strong> entry to PDF 1.5, as we see in this next screenshot:</p>
<p><img decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/2924-3000.png" alt="Screenshot of VS Code discussed in the text." width="766" height="1284" srcset="https://pdfa.org/wp-content/uploads/2025/12/2924-3000.png 766w, https://pdfa.org/wp-content/uploads/2025/12/2924-3000-179x300.png 179w, https://pdfa.org/wp-content/uploads/2025/12/2924-3000-611x1024.png 611w, https://pdfa.org/wp-content/uploads/2025/12/2924-3000-600x1006.png 600w" sizes="(max-width: 766px) 100vw, 766px"/></p>
<p><span>Observations:</span></p>
<ul>
<li><span>Lines 2953-2984 are the incremental update section.</span></li>
<li><span>Line 2954 is a PDF comment. PDF comments always start with a PERCENT SIGN (<code>%</code>) and may occur in many places in PDF files. Effective sanitization and redaction workflows typically remove all comments from PDFs because they may inadvertently disclose information, but this exact comment appears in 3,608 other PDF files. The origin or meaning of this comment was not further investigated.</span></li>
<li><span>Line 2964 upgrades the PDF version to 1.5. At first glance, this may appear to be perfectly valid PDF, but it is technically incorrect because the file header is <code>%PDF-1.3</code> yet the <strong>Version</strong> key was only added in PDF 1.4 - this is what the strict file validation tool mentioned above had noticed. As object 24 is a compressed object stream (lines 2966-2973) and object 25 is a compressed cross-reference stream (lines 2974-2981), the indicated version should be PDF 1.5. As a practical matter, however, this level of technical detail does not impact operation or behavior of PDFs.</span></li>
<li><span>Line 2984 is the end-of-section “<code>%%EOF</code>” marker for this incremental update section.</span></li>
</ul>
<p><span>As this section of the PDF uses compressed object streams, specialized PDF forensic tools must be used… simple search methodologies, such as those mentioned above, may not identify everything!</span></p>
<p><span>We know that there are 7 objects (because we find /<code>N 7</code>) inside the object stream:</span></p>
<p><img loading="lazy" decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/pdf-debugger.png" alt="Screenshot of debugger displaying content discussed in the text." width="676" height="443" srcset="https://pdfa.org/wp-content/uploads/2025/12/pdf-debugger.png 676w, https://pdfa.org/wp-content/uploads/2025/12/pdf-debugger-300x197.png 300w, https://pdfa.org/wp-content/uploads/2025/12/pdf-debugger-600x393.png 600w" sizes="auto, (max-width: 676px) 100vw, 676px"/></p>
<p><span>As per PDF’s specification, ISO 32000-2:2020, §7.5.7, the first line of integers is interpreted as N pairs, where the first integer is the object number and the second integer is the byte offset relative to the first object in the object stream.</span></p>
<table>
<tbody>
<tr>
<td><strong>N</strong></td>
<td><strong>1st integer (object number)</strong></td>
<td><strong>2nd integer (start offset)</strong></td>
<td><strong>Explanation</strong></td>
<td><strong>Content</strong></td>
</tr>
<tr>
<td>1</td>
<td>19</td>
<td>0</td>
<td>Type1 Font object for OPBaseFont0 (Courier)</td>
<td><span>&lt;&lt;/BaseFont/Courier/Encoding&lt;&lt;/BaseEncoding/WinAnsiEncoding/Type/Encoding&gt;&gt;/Name/OPBaseFont0/Subtype/Type1/Type/Font&gt;&gt;</span></td>
</tr>
<tr>
<td>2</td>
<td>20</td>
<td>118</td>
<td>Type1 Font object for OPBaseFont1 (Helvetica)</td>
<td><span>&lt;&lt;/BaseFont/Helvetica/Encoding&lt;&lt;/BaseEncoding/WinAnsiEncoding/Type/Encoding&gt;&gt;/Name/OPBaseFont1/Subtype/Type1/Type/Font&gt;&gt;</span></td>
</tr>
<tr>
<td>3</td>
<td>17</td>
<td>238</td>
<td>Document information (Info) dictionary</td>
<td><span>&lt;&lt;/CreationDate(D:20251218143205)/Creator(OmniPage CSDK 21.1)/ModDate(D:20251218143205)/Producer(Processing-CLI)&gt;&gt;</span></td>
</tr>
<tr>
<td>4</td>
<td>18</td>
<td>352</td>
<td>ProcSet resources array</td>
<td><span>[/PDF/Text/ImageB/ImageC/ImageI]</span></td>
</tr>
<tr>
<td>5</td>
<td>22</td>
<td>384</td>
<td>Resources dictionary for the page</td>
<td><span>&lt;&lt;/Font&lt;&lt;/OPBaseFont0 19 0 R/OPBaseFont1 20 0 R&gt;&gt;/ProcSet 18 0 R/XObject&lt;&lt;/Im0 8 0 R&gt;&gt;&gt;&gt;</span></td>
</tr>
<tr>
<td>6</td>
<td>23</td>
<td>472</td>
<td>Array of 2 indirect references (to content streams)</td>
<td><span>[21 0 R 4 0 R]</span></td>
</tr>
<tr>
<td>7</td>
<td>3</td>
<td>486</td>
<td>Updated Page object</td>
<td><span>&lt;&lt;/Contents 23 0 R/MediaBox[0 0 864 576.75]/Parent 2 0 R/Resources 22 0 R/Thumb 11 0 R/Type/Page&gt;&gt;</span></td>
</tr>
</tbody>
</table>

<p>What is very interesting here – from a PDF forensics perspective – is the fact of a <strong><em><span>hidden document information dictionary</span></em></strong> that is not referenced from the last (final) incremental update trailer (i.e., there is no <strong>Info</strong> entry in object 31, lines 3050-3063 below). As such, this orphaned dictionary is invisible to PDF software! This oddity occurs in all other PDFs we’d randomly selected for investigation.</p>
<p><img loading="lazy" decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/2985-3067.png" alt="Screenshot of VS Code discussed in the text." width="777" height="1144" srcset="https://pdfa.org/wp-content/uploads/2025/12/2985-3067.png 777w, https://pdfa.org/wp-content/uploads/2025/12/2985-3067-204x300.png 204w, https://pdfa.org/wp-content/uploads/2025/12/2985-3067-695x1024.png 695w, https://pdfa.org/wp-content/uploads/2025/12/2985-3067-768x1131.png 768w, https://pdfa.org/wp-content/uploads/2025/12/2985-3067-600x883.png 600w" sizes="auto, (max-width: 777px) 100vw, 777px"/></p>
<p><span>Formatted nicely as an uncompressed object, this hidden document information dictionary inside the compressed object stream contains the following information (the CreationDate and ModDate appear to change in other randomly examined PDFs):</span></p>
<pre>     17 0 obj
     &lt;&lt;
          /CreationDate (D:20251218143205)
          /ModDate      (D:20251218143205)
          /Creator      (OmniPage CSDK 21.1)
          /Producer     (Processing-CLI)
     &gt;&gt;
     endobj</pre>
<p>This metadata clearly indicates the software DoJ used to manipulate these PDF files. Although not relevant to the content, this forensic discovery clearly shows that extra care is required when sanitizing PDFs.</p>
<h2 id="different-incremental-updates">Different incremental updates</h2>
<p>Another randomly selected PDF, VOL00003\IMAGES\0001\EFTA00003939.pdf contains 3 full-page images, and just a single incremental update that applies the Bates numbering. However, in this case the file header is <code>%PDF-1.5</code> yet both the original PDF and incremental update use conventional cross-reference tables! This isn’t problematic, but is certainly unexpected and inefficient since PDF 1.5 introduced compressed cross-reference streams.</p>
<p>By comparing the objects in the incremental cross-reference table to the original cross-reference table we can see that objects 66 to 69 – the 3 Page objects for the 3 page document – were redefined. This is just what is expected in order to add the Bates number to each page’s <strong>Contents</strong> stream as in the previous example.</p>

<p>Our initial examination using pdfinfo utilities did not identify any metadata in any of the PDFs in the tranche, either in the document information dictionary (PDF file trailer Info entry) or as an XMP metadata stream (<strong>Metadata</strong> entry).</p>
<p>However, since we know that (a) the tranche includes PDFs with incremental updates, and (b) that an orphaned document information dictionary exists, all revisions of a document should be thoroughly examined. Incremental updates may have marked other document information dictionaries or XMP metadata streams as free but not deleted the actual data.</p>
<p>XMP metadata is always encoded in PDF as a stream object, and since stream objects cannot be in compressed object streams, using forensic tools to search for keys “<code>/XML</code>” or “<code>/Metadata</code>” should always locate them. All modern office suites and PDF creation applications will generate XMP metadata when exporting to PDF. As XMP is usually uncompressed, searching for XML fragments may also be helpful (see below for an example XMP object fragment).</p>
<pre>     3 0 obj
     &lt;&lt;/Length 36996/Subtype/XML/Type/Metadata&gt;&gt;
     stream
     &lt;?xpacket begin=&#34;ï»¿&#34; id=&#34;W5M0MpCe … zNTczkc9d&#34;?&gt;
     &lt;x:xmpmeta xmlns:x=&#34;adobe:ns:meta/&#34; x:xmptk=&#34; … &#34;&gt;
         &lt;rdf:RDF 
     xmlns:rdf=&#34;http://www.w3.org/1999/02/22-rdf-syntax-ns#&#34;&gt;
              &lt;rdf:Description rdf:about=&#34;&#34;
                   xmlns:dc=&#34;http://purl.org/dc/elements/1.1/&#34;
                   xmlns:xmp=&#34;http://ns.adobe.com/xap/1.0/&#34;
     ...</pre>
<p>Not unsurprisingly for properly-redacted files, we did not find any XMP metadata streams or XML in any PDF. As a consequence, none of the PDFs can declare conformance to either PDF/A (ISO 19005 for long-term archiving) or PDF/UA (ISO 14289 for accessibility). Of course, as untagged PDFs, the files cannot conform to accessibility specifications such as PDF/UA or WCAG in any event. Additionally, none of the PDFs appear to include device-independent color spaces.</p>
<p>The presence of an <strong>Info</strong> entry in the trailer dictionary or (in PDFs with cross-reference streams) in the cross-reference stream dictionary indicates the presence of document information dictionaries. “<code>/Info</code>” does indeed occur in many of the PDFs, including multiple times in some PDFs, indicating potential changes via incremental updates. However, as discovered above, in some cases the final incremental update does not include an <strong>Info</strong> entry, thus “orphaning” any existing document information dictionaries.</p>
<p>ISO 32000-2:2020, Table 349 lists the defined entries in PDF’s document information dictionary (<strong>Title</strong>, <strong>Author</strong>, <strong>Subject</strong>, etc). Any vendor may add additional entries (such as <a href="https://developer.apple.com/documentation/coregraphics/kcgpdfcontextkeywords" target="_blank" rel="noopener">Apple does with its <span>/AAPL:Keywords</span> entry</a>), so redaction and sanitization software should be aware of extra entries.</p>
<p>From our random sampling, we identified one PDF with a non-trivial document information dictionary still present: VOL00002\IMAGES\0001\EFTA00003212.pdf. This is shown below in <a href="https://pdfa.org/resource/vscode-extension-pdf-cos-syntax-support/">Visual Studio Code with my pdf-cos-syntax extension</a>:</p>
<p><img loading="lazy" decoding="async" src="https://pdfa.org/wp-content/uploads/2025/12/1-69.png" alt="Screenshot of VS Code discussed in the text." width="766" height="1284" srcset="https://pdfa.org/wp-content/uploads/2025/12/1-69.png 766w, https://pdfa.org/wp-content/uploads/2025/12/1-69-179x300.png 179w, https://pdfa.org/wp-content/uploads/2025/12/1-69-611x1024.png 611w, https://pdfa.org/wp-content/uploads/2025/12/1-69-600x1006.png 600w" sizes="auto, (max-width: 766px) 100vw, 766px"/></p>
<p><span>Of additional interest in this specific PDF is that the comment at line 60 has survived DoJ’s sanitization and redaction workflow! Other PDF comments may therefore also be present in other files.</span></p>
<p><span>EFTA00003212.pdf appears to be a redacted image or an error from the DoJ workflow, as it is a single page with the text “No Images Produced”.</span></p>
<p><span>Simple searching of the standardized PDF document information dictionary entries gives the following (note that the technique used will not locate information in compressed object streams, as mentioned above):</span></p>
<table>
<tbody>
<tr>
<td><strong>Key name</strong></td>
<td><strong>Number of PDFs (max. = 4,085)</strong></td>
<td><strong>Comment</strong></td>
</tr>
<tr>
<td><strong>Info</strong></td>
<td>3,823</td>
<td>Some PDFs have empty <strong>Info</strong> dictionaries with no entries</td>
</tr>
<tr>
<td><strong>Title</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Author</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Subject</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Keywords</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Creator</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>Producer</strong></td>
<td>215</td>
<td>Always “pypdf” (denotes <a href="https://pypi.org/project/pypdf/" target="_blank" rel="noopener">https://pypi.org/project/pypdf/</a>)</td>
</tr>
<tr>
<td><strong>CreationDate</strong></td>
<td>3,609</td>
<td>Same PDFs that have <strong>ModDate</strong> with an identical value</td>
</tr>
<tr>
<td><strong>ModDate</strong></td>
<td>3,609</td>
<td>Same PDFs that have <strong>CreationDate</strong> with an identical value</td>
</tr>
<tr>
<td><strong>Trapped</strong></td>
<td>1</td>
<td>Only EFTA00003212.pdf</td>
</tr>
<tr>
<td><strong>APPL:Keywords</strong></td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>

<h3 id="date-analysis">Date analysis</h3>
<p>Detailed date analysis is a common task in the forensic analysis of potentially fraudulent or modified documents. However, in the case of redacted or sanitized documents, where the document is known to have been modified, this can be less useful.</p>
<p>The creation and modification dates for the 3,609 PDFs range from December 18, 2025, 14:32:05 (2:32 pm) to December 19, 2025, 23:26:13 (almost midnight). For all files, the creation and modification dates are always the same. This may also imply that the DoJ batch processing to prepare this tranche of PDFs took at least 36 hours!</p>
<p>What’s also interesting is that the <strong>CreationDate</strong> and <strong>ModDate</strong> fields in the hidden document information dictionary (inside the object stream of the first increment update – see above) appear to always be an exact match to both the <strong>CreationDate</strong> and <strong>ModDate</strong> of the original document. This implies that all dates across all incremental updates were updated in a single processing pass that applied the Bates numbering.</p>
<h2 id="photographs">Photographs</h2>
<p>There are no JPEG images (<strong>DCTDecode </strong>filter) in any PDF in the tranche, including the full-page photographs. Randomly viewing the photographic images at high magnification (zoom) in PDF viewers clearly shows JPEG “jaggy” <a href="https://en.wikipedia.org/wiki/Compression_artifact" target="_blank" rel="noopener">compression artifacts</a>. All photographic images appear to have been downscaled to 96 DPI (769 x 1152 or 1152 x 769 pixels), making text on random objects in the photos much harder to discern (see the <a href="#ocr">OCR discussion below</a>).</p>
<p>DoJ explicitly avoids JPEG images in the PDFs probably because they appreciate that JPEGs often contain identifiable information, such as EXIF, IPTC, or XMP metadata, as well as <a href="https://exiftool.org/#JPEG" target="_blank" rel="noopener">COM (comment) tags</a> in the JPEG bitstream. This information may disclose the camera model and serial number, GPS location, camera operator details, date/time of the photo, etc., and is more difficult to redact while retaining the JPEG data. The DoJ processing pipeline has therefore explicitly converted all lossy JPEG images to low DPI, FLATE-encoded bitmaps in the PDFs using an indexed device-dependent color space with a palette of 256 unique colors (which reduces the color fidelity compared to the original high-quality digital color photograph).</p>
<h2 id="scanned-documents-or-are-they">Scanned documents – or are they?</h2>
<p>Randomly inspecting the tranche discovers many documents that appear to have been created by a scanning process. On closer inspection, there are documents that have tell-tale artifacts from a physical scanning process, such as visible physical paper edges, punched holes, staple marks, spiral binding, stamps, paper scuff marks, color blotches and inconsistencies, handwritten notes or marginalia, varying paper skew, and platen marks from the physical paper scanning processes. For example, VOL00007\IMAGES\0001\EFTA00009440.pdf shows many of these aspects</p>
<p>There are also other documents that appear to <span><em>simulate</em></span> a scanned document but completely lack the “real-world noise” expected with physical paper-based workflows. The much crisper images appear almost perfect without random artifacts or background noise, and with the exact same amount of image skew across multiple pages. Thanks to the borders around each page of text, page skew can easily be measured, such as with VOL00007\IMAGES\0001\EFTA00009229.pdf. It is highly likely these PDFs were created by rendering original content (from a digital document) to an image (e.g., via print to image or save to image functionality) and then applying image processing such as skew, downscaling, and color reduction.</p>
<p>The use of the timeless monospaced (also known as fixed-width) “Courier” typeface means that the number of characters redacted can be easily determined by vertical alignment with text lines above and below each redaction. In some instances, this may reduce the possible number of options that represent the redacted content, allowing it to be more easily guessed. Although redaction of variable-width typefaces is far more complex, Bland, M., Iyer, A., and Levchenko, K. 2022 paper “<a href="http://arxiv.org/abs/2206.02285" target="_blank" rel="noopener">Story Beyond the Eye: Glyph Positions Break PDF Text Redaction</a>” showed that this is still possible with sufficient computing power and determination.</p>
<h3 id="optical-character-recognition-ocr"><a id="ocr"></a>Optical Character Recognition (OCR)</h3>
<p><a href="https://en.wikipedia.org/wiki/Optical_character_recognition" target="_blank" rel="noopener">OCR</a> is complex image processing that attempts to identify text in bitmap images. In PDF files, OCR-identified text is commonly placed on top of the image using the invisible text render mode. This enables users to then extract the text from the image.</p>
<p>Returning to the very first PDF file in the tranche, VOL00001\IMAGES\0001\EFTA00000001.pdf - this is a full-page photo of a hand-written sign where part of the hand-written information is explicitly redacted. The PDF contains largely inaccurate OCR-ed text, indicating that natural language processing (NLP), machine learning (ML), or even language aware dictionary-based algorithms were not used. This means that there will be more errors in the extracted text than is necessary.</p>
<p>With cloud platforms readily accessible and supporting advanced OCR at low cost, anyone is capable of re-processing the entire tranche of PDFs and comparing the OCR results to those provided by DoJ. Even though the page images are low-resolution (96 DPI), rerunning OCR may bring to light additional or corrected information hidden by the original OCR that failed to recognize everything correctly.</p>
<p>The “black box” redactions we investigated were all correctly applied directly into the image pixel data. They are not separate PDF rectangle objects simply floating above sensitive information that was still present in the image and easily discoverable. Yes, sometimes it is that easy…!</p>
<h2 id="conclusion">Conclusion</h2>
<p>We did not set out to comprehensively analyze every corner of every PDF file in the Epstein PDFs, but to present a basic walk-through of some of the challenges and tricks used to conduct a PDF forensic assessment. Our results above were from a small random sample of documents - there may well be outlier PDFs in the data sets that we did not encounter.</p>
<p>The DoJ has clearly created internal processes, systems, and workflows that can sanitize and redact information prior to publishing as PDF. This includes converting JPEG images to low-resolution pixel-only bitmaps, largely removing metadata, and rendering page images to bitmaps. OCR appears to have been widely applied, but is of variable quality.</p>
<p>Their PDF technology could be improved to vastly reduce file size by removing unnecessary objects (e.g., empty content streams, ProcSets, empty thumbnail references, etc.), simplifying and reducing content streams, applying all incremental updates (i.e., removing all incremental update sections), and always using compressed object streams and compressed cross-reference streams. Information leakage may also be occurring via PDF comments or orphaned objects inside compressed object streams, as I discovered above.</p>
<p>PDF forensics is a highly complex field, where variations in files and tool assumptions can easily yield false results. The PDF Association hosts a PDF Forensic Liaison Working Group to develop industry guidance on forensic examination of PDF files and to educate document examiners and other specialists about many of these aspects.</p>
</div></div>
  </body>
</html>
