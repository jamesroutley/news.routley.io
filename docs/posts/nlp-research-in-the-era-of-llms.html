<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nlpnewsletter.substack.com/p/nlp-research-in-the-era-of-llms">Original</a>
    <h1>NLP Research in the Era of LLMs</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><span>NLP research has undergone a paradigm shift over the last year. A range of large language models (LLMs) has validated the unreasonable effectiveness of scale</span><span>. Currently, the state of the art on most benchmarks is held by LLMs that are expensive to fine-tune and prohibitive to pre-train outside of a few industry labs.</span></p><p><span>In the past, a barrier to doing impactful research has often been a lack of awareness of fruitful research areas and compelling hypotheses to explore</span><span>. In contrast, </span><strong>NLP researchers today are faced with a constraint that is much harder to overcome: compute.</strong></p><p>In an era where running state-of-the-art models requires a garrison of expensive GPUs, what research is left for academics, PhD students, and newcomers to NLP without such deep pockets? Should they focus on the analysis of black-box models and niche topics ignored by LLM practitioners?</p><p>In this newsletter, I first argue why the current state of research is not as bleak—rather the opposite! I will then highlight five research directions that are important for the field and do not require much compute. I take inspiration from the following reviews of research directions in the era of LLMs:</p><ul><li><p><a href="https://arxiv.org/abs/2304.06035" rel="">Togelius &amp; Yannakakis. (Mar 2023). Choose Your Weapon: Survival Strategies for Depressed AI Academics</a></p></li><li><p><a href="https://arxiv.org/abs/2305.12544" rel="">Ignat et al. (May 2023). A PhD Student&#39;s Perspective on Research in NLP in the Era of Very Large Language Models</a></p></li><li><p><a href="https://arxiv.org/abs/2310.20633" rel="">Li et al. (Oct 2023). Defining a New NLP Playground</a></p></li><li><p><a href="https://arxiv.org/abs/2311.05020" rel="">Saphra et al. (Nov 2023). First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models</a></p></li><li><p><a href="https://aclanthology.org/2023.emnlp-main.0.pdf" rel="">Manning (Dec 2023). Academic NLP research in the Age of LLMs: Nothing but blue skies! </a><em><a href="https://aclanthology.org/2023.emnlp-main.0.pdf#page=34" rel="">EMNLP 2023 Keynote talk</a><span>, </span><a href="https://underline.io/events/431/sessions/16480/lecture/91315-academic-nlp-research-in-the-age-of-llms-nothing-but-blue-skies" rel="">recording</a><span> (requires EMNLP 2023 registration)</span></em></p></li></ul><p>I highly recommend these for different perspectives on current LLM research and for a broader overview of research topics beyond the ones presented in this article.</p><p><span>Research is cyclical. Computer scientist and </span><a href="https://www.aclweb.org/adminwiki/index.php?title=ACL_Lifetime_Achievement_Award_Recipients" rel="">ACL lifetime achievement award recipient</a><span> Karen Spärck Jones </span><a href="https://aclanthology.org/www.mt-archive.info/Zampolli-1994-Sparck-Jones.pdf" rel="">wrote in 1994</a><span>:</span></p><blockquote><p><em>Those […] who had been around for a long time, can see old ideas reappearing in new guises […]. But the new costumes are better made, of better materials, as well as more becoming: so research is not so much going round in circles as ascending a spiral.</em></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png" width="1155" height="372" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:372,&#34;width&#34;:1155,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:255738,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91e65b73-0428-415b-be9f-7d536e6eea69_1155x372.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>In the same vein, </span><a href="https://arxiv.org/abs/2311.05020" rel="">Saphra et al. (2023)</a><span> highlight the similarities between the current era of LLMs and the Statistical Machine Translation (SMT) era where translation performance was shown to scale by training a phrase-based language model on more and more web data.</span></p><p><span>More recently, we have seen the success of scale with the advent of word embeddings in 2013 and the emergence of pre-trained LMs in 2018.</span><span> In all cases, academic research was not left in the dust but went on to make contributions that shaped the next era, from KenLM (</span><a href="https://aclanthology.org/W11-2123/" rel="">Heafield, 2011</a><span>), an efficient LM library that enabled academics to outperform industry MT systems, to the word2vec alternative GloVe (</span><a href="https://aclanthology.org/D14-1162/" rel="">Pennington et al., 2014</a><span>), to pre-trained LMs developed in non-profits and academia such as ELMo (</span><a href="https://aclanthology.org/N18-1202/" rel="">Peters et al., 2018</a><span>) and ULMFiT (</span><a href="https://aclanthology.org/P18-1031/" rel="">Howard &amp; Ruder, 2018</a><span>).</span></p><p><strong>The main lesson here is that while massive compute often achieves breakthrough results, its usage is often inefficient. Over time, improved hardware, new techniques, and novel insights provide opportunities for dramatic compute reduction.</strong></p><p><span>In his </span><a href="https://smerity.com/articles/2018/limited_compute.html" rel="">2018 article</a><span>, Stephen Merity provides two examples of this trend where the first instance of a method was exorbitantly compute-intensive while only a year later, compute costs were dramatically reduced:</span></p><ul><li><p><span>New York Times (2012): &#34;</span><a href="https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html" rel="">How Many Computers to Identify a Cat? </a><strong><a href="https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html" rel="">16,000 (CPU cores)</a></strong><span>&#34;</span></p></li><li><p><span>Neural Architecture Search: &#34;</span><strong>32,400-43,200 GPU hours</strong><span>&#34;</span></p></li></ul><p>We could argue why the same trend may not be true for this era of LLMs. After all, new techniques can also be scaled up and scale ultimately prevails as we know. In addition, the current trend of closed-source models makes it harder to build on them.</p><p><span>On the other hand, new powerful open-source models are still released regularly</span><span>. Companies are also incentivized to invest in the development of smaller models in order to reduce inference cost. Finally, we are starting to see the limits of scale on the horizon: recent LLMs are reaching the limits of text data online and repeating data eventually leads to diminishing returns (</span><a href="https://arxiv.org/abs/2305.16264" rel="">Muennighoff et al., 2023</a><span>) while Moore’s law is </span><a href="https://www.investopedia.com/terms/m/mooreslaw.asp" rel="">approaching its physical limits</a><span>.</span></p><p>There are already recent examples that require a fraction of compute by using new methods and insights, demonstrating that this trend also holds in the era of LLMs:</p><ul><li><p><span>FlashAttention (</span><a href="https://arxiv.org/abs/2205.14135" rel="">Dao et al., 2022</a><span>) provides drastic speedups over standard attention through clever hardware optimization.</span></p></li><li><p><span>Parameter-efficient fine-tuning methods (see our </span><a href="https://tinyurl.com/modular-fine-tuning-tutorial?ref=ruder.io" rel="">EMNLP 2022 tutorial</a><span> for an overview) including </span><a href="https://github.com/adapter-hub/adapters" rel="">adapters</a><span> such as LoRA (</span><a href="https://arxiv.org/abs/2106.09685" rel="">Hu et al., 2021</a><span>) and QLoRA (</span><a href="https://arxiv.org/abs/2305.14314" rel="">Dettmers et al., 2023</a><span>) enable fine-tuning LLMs on a single GPU.</span></p></li><li><p><a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/" rel="">Phi-2</a><span>, a new 2.7B-parameter LLM released last week matches or outperforms models up to 25x its size.</span></p></li></ul><p><span>In the near term, the largest models using the most compute will continue to be the most capable. </span><strong>However, there remains a lot of room for innovation by focusing on strong smaller models and on areas where compute requirements will inexorably be eroded by research progress.</strong></p><p><span>While LLM projects typically require an exorbitant amount of resources, it is important to remind ourselves that </span><strong>research does not need to assemble full-fledged massively expensive systems in order to have impact</strong><span>. Chris Manning made the nice analogy in his EMNLP 2023 keynote that in the same vein, aerospace engineering students are not expected to engineer a new airplane during their studies.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp" width="1456" height="823" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/d6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:823,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:173950,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/webp&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6bd6fa4-bfbf-4af2-b66d-44417de1ce42_2560x1447.webp 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>Members of the MIT aerospace engineering lab prepare a model of a super-efficient commercial aircraft for testing.</figcaption></figure></div><p>With that in mind, let’s look at five important research areas that require less compute.</p><p><span>Rather than waiting for compute costs to go down, making LLMs more efficient can have a wide impact. When we talk about efficiency, we often think about making the model architecture itself more efficient. In fact, most works on efficient Transformers focused on a specific component, the attention mechanism (</span><a href="https://arxiv.org/abs/2009.06732" rel="">Tay et al., 2022</a><span>).</span></p><p><strong>However, when thinking about efficiency, it is useful to consider the entire LLM stack.</strong><span> Important components ripe for improvement are:</span></p><ol><li><p><strong>Data collection and preprocessing</strong><span>: improving data efficiency by better filtering and data selection.</span></p></li><li><p><strong>Model input</strong><span>: faster, more informed tokenization; better word representations via character-level modeling</span></p></li><li><p><strong>Model architecture</strong><span>: better scaling towards long-range sequences; more effective use of memory</span></p></li><li><p><strong>Training</strong><span>: more efficient methods to train small-scale LLMs via more effective distillation, better learning rate schedules and restarts, (partial) model compression, model surgery, etc.</span></p></li><li><p><strong>Downstream task adaptation</strong><span>: improved parameter-efficient fine-tuning; automatic prompt and chain-of-thought design; </span><a href="https://www.modulardeeplearning.com/" rel="">modular methods</a><span>; improved RLHF</span></p></li><li><p><strong>Inference</strong><span>: early predictions; prompt compression; human-in-the-loop interactions</span></p></li><li><p><strong>Data annotation</strong><span>: model-in-the-loop annotation; automatic arbitration and consolidation of annotations</span></p></li><li><p><strong>Evaluation</strong><span>: efficient automatic metrics; efficient benchmarks</span></p></li></ol><p><span>Given the wide range of LLM applications, it is increasingly </span><strong>important to consider the ‘human’ part in efficiency</strong><span>: from annotation, to learning from human preferences, to interacting with users, can we make the stages where human and LLM data intersect more efficient and reliable?</span></p><p><strong>Sparsity and low-rank approximations are two general principles</strong><span> that have been applied in a wide range of efficient methods (see our </span><a href="https://arxiv.org/abs/2302.11529" rel="">modular deep learning survey</a><span> for an overview) and are thus useful sources of inspiration: are there components that are modeled with an excess amount of parameters that can be approximated instead? Are there computations involving multiple steps that can be shortened?</span></p><p><span>In the age of LLMs, the clearest indicator that an efficient method works is that it reduces the coefficient (in other words, lowers the slope) of the corresponding scaling law, as seen for instance in </span><a href="https://arxiv.org/abs/2203.15556" rel="">Hoffmann et al. (2022)</a><span>.</span></p><p>But how can we validate a scaling law without massive compute? By prioritizing experimentation in small-scale regimes.</p><p><span>While it is generally prohibitive to apply a new method directly to the largest model, using it on a smaller, representative model can serve as a useful prototype and proof of concept.</span><span> These days in particular, </span><strong>one should not underestimate the pace of the ML and NLP community, which is receptive to and quick-to-adopt compelling new ideas.</strong></p><p><span>For instance, the recently proposed DPO method (</span><a href="https://arxiv.org/abs/2305.18290" rel="">Rafailov et al., 2023</a><span>) used a relatively small-scale experimental setting in the paper (GPT-2-large fine-tuned on IMDb reviews, among others). As the code was </span><a href="https://github.com/eric-mitchell/direct-preference-optimization" rel="">open-sourced</a><span> and compatible with common LLM frameworks, community members quickly applied it to more recent models such as </span><a href="https://huggingface.co/blog/dpo-trl" rel="">Llama-2</a><span> and </span><a href="https://huggingface.co/blog/alvarobartt/notus-7b-v1" rel="">Zephyr</a><span>.</span></p><p><span>Expect to see more of this mode of operation: </span><strong>academic researchers developing new methods that—after small-scale validation—are shared with the community for further experimentation and scaling up.</strong></p><p>Another setting where a focus on a small scale is increasingly valuable is analysis and model understanding. Through pre-training, models learn a wide array of natural language understanding capabilities—but under exactly what conditions these capabilities emerge remains unclear.</p><p><span>Large-scale pre-training, due to the massive nature of most of the components involved, mostly resists a controlled examination.</span><span> </span><strong>Instead, controlled small and synthetic settings that allow probing of specific hypotheses will be increasingly important to understand how LLMs learn and acquire capabilities.</strong><span> Such settings can include synthetic language such as bigram data (</span><a href="https://arxiv.org/abs/2306.00802" rel="">Bietti et al., 2023</a><span>) or “fake” English (</span><a href="https://openreview.net/forum?id=HJeT3yrtDr" rel="">K et al., 2020</a><span>), highly curated and domain-specific data, and data satisfying certain (distributional) characteristics; as well as more interpretable models such as small Transformers, backpack language models (</span><a href="https://aclanthology.org/2023.acl-long.506/" rel="">Hewitt et al., 2023</a><span>), and neural additive models (</span><a href="https://arxiv.org/abs/2004.13912" rel="">Agarwal et al., 2021</a><span>).</span></p><p>LLM mechanisms whose emergence is still poorly understood include the following:</p><ul><li><p><strong>in-context learning</strong><span>: ‘burstiness’ and the highly skewed distribution of language data are important (</span><a href="https://arxiv.org/abs/2205.05055" rel="">Chan et al., 2022</a><span>) but the in-context learning ability can also disappear again during training (</span><a href="https://arxiv.org/abs/2311.08360" rel="">Singh et al., 2023</a><span>)</span></p></li><li><p><strong>chain-of-thought prompting</strong><span>: local structure in the training data is important (</span><a href="https://openreview.net/pdf?id=rcXXNFVlEn" rel="">Prystawski et al., 2023</a><span>) but we don’t know how this relates to natural language data</span></p></li><li><p><strong>cross-lingual generalization</strong><span>: limited parameters, shared special tokens, shared position embeddings, and a common masking strategy contribute to multilinguality (</span><a href="https://aclanthology.org/2020.acl-main.421/" rel="">Artetxe et al., 2019</a><span>; </span><a href="https://aclanthology.org/2020.emnlp-main.358/" rel="">Dufter &amp; Schütze, 2020</a><span>) but it is unclear how this extends to diverse natural language data and typologically diverse languages</span></p></li><li><p><strong>other types of emerging abilities</strong><span> (see for instance </span><a href="https://openreview.net/pdf?id=ITw9edRDlD" rel="">Schaeffer et al., 2023</a><span>)</span></p></li></ul><p>Rather than trying to make large-scale settings smaller to reduce the amount of compute necessary to study them, we can also focus on settings that are intrinsically small-scale due to constraints on the data available.</p><p>While the largest LLMs are pre-trained on trillions of tokens, the downstream applications we would like to apply them to are often more limited in the data available to them.</p><p><span>This is true for many interdisciplinary areas such as NLP for Science, Education, Law, and Medicine. In many of these domains, there is very little high-quality data easily accessible online. </span><strong>LLMs thus must be combined with domain-specific strategies to achieve the biggest impact.</strong><span> See </span><a href="https://arxiv.org/abs/2310.20633" rel="">Li et al. (2023)</a><span> for a brief review of directions in NLP+X applications.</span></p><p><span>Another area where data is notoriously limited is multilinguality. </span><strong>For many languages, the amount of text data online is limited—but data may be available in other formats such as lexicons, undigitized books, podcasts, and videos.</strong><span> This requires new strategies to collect—and create—high-quality data. Furthermore, many languages and dialects are more commonly spoken than written, which makes multi-modal models important to serve such languages.</span></p><p><strong>As we reach the limits of data available online, even “high-resource” languages will face data constraints.</strong><span> New research will need to engage with these constraints rather than assuming an infinite-scale setting.</span></p><p><span>While few-shot prompting enables seamless application to many downstream tasks, it is insufficient to teach a model about the nuances of more complex applications and is </span><a href="https://docs.google.com/presentation/d/1seHOJ7B0bQEPJ3LBW5VmruMCILiVRoPb8nmU2OS-Eqc/edit?ref=ruder.io#slide=id.g1a37bfe6b5e_3_92" rel="">limited in other ways</a><span>. Alternatively, parameter-efficient fine-tuning enables a more holistic adaptation using little compute. Such fine-tuning—when updates are constrained to a subset of model parameters—gives rise to </span><a href="https://arxiv.org/abs/2302.11529" rel="">modular models</a><span>.</span></p><p>Given the diversity of LLM application areas and capabilities to master, another interesting direction is thus to leverage multiple modular ‘experts’ by learning to disentangle and combine the skills and knowledge learned across different domains.</p><p>Such modeling advances, however, are of little use if we do not have reliable means to evaluate them.</p><blockquote><p><em><span>&#34;[...] benchmarks shape a field, for better or worse. Good benchmarks are in alignment with real applications, but bad benchmarks are not, forcing engineers to choose between making changes that help end users or making changes that only help with marketing.&#34;—David A. Patterson; foreword to </span><a href="https://www.springer.com/gp/book/9783030417048?ref=ruder.io" rel="">Systems Benchmarking (2020)</a></em></p></blockquote><p><span>In 2021, a common sentiment was that NLP models had outpaced the benchmarks to test for them. I reviewed the situation in </span><a href="https://www.ruder.io/nlp-benchmarking/" rel="">this article</a><span>; not much has changed since then. More recent benchmarks designed to evaluate LLMs such as </span><a href="https://crfm.stanford.edu/helm/latest/#/leaderboard" rel="">HELM</a><span> (</span><a href="https://arxiv.org/abs/2211.09110" rel="">Liang et al., 2022</a><span>) and Super-NaturalInstructions (</span><a href="https://aclanthology.org/2022.emnlp-main.340/" rel="">Wang et al., 2022</a><span>) still mainly consist of standard NLP tasks—most of them sentence-level—while others such as MMLU (</span><a href="https://arxiv.org/abs/2009.03300" rel="">Hendrycks et al., 2021</a><span>) and AGIEval (</span><a href="https://arxiv.org/abs/2304.06364" rel="">Zhong et al., 2023</a><span>) focus on exams. These benchmarks do not reflect the diverse range of tasks where we would like to apply LLMs.</span></p><p>Another phenomenon to be aware of is leaderboard contamination: benchmark data that is available online is likely to have been included in the pre-training data of LLMs, making evaluation unreliable. Benchmarks should thus keep evaluation data secret or receive regular updates.</p><blockquote><p><em>&#34;When you can measure what you are speaking of and express it in numbers, you know that on which you are discussing. But when you cannot measure it and express it in numbers, your knowledge is of a very meagre and unsatisfactory kind.&#34;</em><span>—Lord Kelvin</span></p></blockquote><p><span>In addition, existing automatic metrics are ill-suited for more complex downstream applications and open-ended natural language generation tasks. LLMs can be incorporated into automatic metrics (</span><a href="https://aclanthology.org/2023.emnlp-main.153/" rel="">Liu et al., 2023</a><span>) but one must be aware of—and mitigate—their biases. For complex tasks, it may be useful to decompose them into subtasks that are easier to evaluate, for instance, via behavioral tests (</span><a href="https://aclanthology.org/2023.acl-long.396/" rel="">Hlavnova &amp; Ruder, 2023</a><span>).</span></p><p><span>As applications become more elaborate, even human evaluation, traditionally perceived to be the gold standard for any data, becomes less reliable. Disagreements may be less an indicator of ‘annotation noise’ and rather a sign of different perspectives (</span><a href="https://aclanthology.org/Q19-1043/" rel="">Pavlick &amp; Kwiatkowski, 2019</a><span>). For specialized applications, only domain experts may be qualified enough to provide accurate feedback. Leveraging and aggregating the feedback of a diverse set of annotators from different backgrounds is thus more important than ever.</span></p><p>Reasoning requires the use of logic to draw conclusions from new and existing information to arrive at a conclusion. With LLMs demonstrating surprising arithmetic and logical reasoning abilities, reasoning has received renewed attention and was well-represented in NeurIPS 2023 papers (see my previous newsletter): </p><p><span>Given that LLMs frequently hallucinate and struggle to generate code or plans that are directly executable, augmenting them with external tools or small domain-specific models is a promising direction to make them more robust. For instance, Parsel (</span><a href="https://nlpnewsletter.substack.com/p/neurips-2023-primer" rel="">Zelikman et al., 2023</a><span>) decomposes a code generation tasks into LLM-generated subfunctions that can be tested against input-output constraints using a code execution module.</span></p><p><span>Many complex real-world applications require different forms of reasoning so evaluating models’ reasoning abilities in realistic scenarios is an important challenge. Given that many real-world problems require weighing different options and preferences, it will be crucial to enable LLMs to present different solutions to users and to incorporate different cultural backgrounds into their decision-making. </span><a href="https://arxiv.org/abs/2305.12544" rel="">Ignat et al. (2023)</a><span> highlight other interesting research directions related to reasoning.</span></p><p>This post presented a selection of five research directions that are particularly important in my opinion—but in truth there are a plethora of potential opportunities to explore (see the other reviews at the beginning of this post). Now is the time to look beyond standard NLP tasks and be ambitious. After all:</p><blockquote><p><em><span>“Shoot for the moon. Even if you miss, you&#39;ll land among the stars.”</span></em></p></blockquote></div></div></div></article></div></div></div>
  </body>
</html>
