<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/blog/stackllama">Original</a>
    <h1>StackLlama: A hands-on guide to train LlaMa with RLHF</h1>
    
    <div id="readability-page-1" class="page"><div>
		
		<!-- HTML_TAG_START -->



<p>Models such as <a href="https://huggingface.co/blog/%5Bhttps://openai.com/blog/chatgpt%5D(https://openai.com/blog/chatgpt)">ChatGPT</a>, <a href="https://huggingface.co/blog/%5Bhttps://openai.com/research/gpt-4%5D(https://openai.com/research/gpt-4)">GPT-4</a>, and <a href="https://huggingface.co/blog/%5Bhttps://www.anthropic.com/index/introducing-claude%5D(https://www.anthropic.com/index/introducing-claude)">Claude</a> are powerful language models that have been fine-tuned using a method called Reinforcement Learning from Human Feedback (RLHF) to be better aligned with how we expect them to behave and would like to use them.</p>
<p>In this blog post, we show all the steps involved in training a <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai">LlaMa model</a> to answer questions on <a href="https://stackexchange.com">Stack Exchange</a> with RLHF through a combination of:</p>
<ul>
<li>Supervised Fine-tuning (SFT)</li>
<li>Reward / preference modeling (RM)</li>
<li>Reinforcement Learning from Human Feedback (RLHF)</li>
</ul>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/instructGPT.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/instructGPT.png" alt=""/></a>
<em>From InstructGPT paper: Ouyang, Long, et al. &#34;Training language models to follow instructions with human feedback.&#34; arXiv preprint arXiv:2203.02155 (2022).</em></p>
<p>By combining these approaches, we are releasing the StackLLaMA model. This model is available on the <a href="https://huggingface.co/trl-lib/llama-se-rl-peft">ü§ó Hub</a> (see <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">Meta&#39;s LLaMA release</a> for the original LLaMA model) and <a href="https://huggingface.co/docs/trl/index">the entire training pipeline</a> is available as part of the Hugging Face TRL library. To give you a taste of what the model can do, try out the demo below!</p>



<h2>
	<a id="the-llama-model" href="#the-llama-model">
		
	</a>
	<span>
		The LLaMA model
	</span>
</h2>
<p>When doing RLHF, it is important to start with a capable model: the RLHF step is only a fine-tuning step to align the model with how we want to interact with it and how we expect it to respond.  Therefore, we choose to use the recently introduced and performant <a href="https://arxiv.org/abs/2302.13971">LLaMA models</a>. The LLaMA models are the latest large language models developed by Meta AI. They come in sizes ranging from 7B to 65B parameters and were trained on between 1T and 1.4T tokens, making them very capable. We use the 7B model as the base for all the following steps!
To access the model, use the <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">form</a> from Meta AI.</p>
<h2>
	<a id="stack-exchange-dataset" href="#stack-exchange-dataset">
		
	</a>
	<span>
		Stack Exchange dataset
	</span>
</h2>
<p>Gathering human feedback is a complex and expensive endeavor. In order to bootstrap the process for this example while still building a useful model, we make use of the <a href="https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences">StackExchange dataset</a>. The dataset includes questions and their corresponding answers from the StackExchange platform (including StackOverflow for code and many other topics). It is attractive for this use case because the answers come together with the number of upvotes and a label for the accepted answer.</p>
<p>We follow the approach described in <a href="https://arxiv.org/abs/2112.00861">Askell et al. 2021</a> and assign each answer a score:</p>
<p><code>score = log2 (1 + upvotes) rounded to the nearest integer, plus 1 if the questioner accepted the answer (we assign a score of ‚àí1 if the number of upvotes is negative).</code></p>
<p>For the reward model, we will always need two answers per question to compare, as we‚Äôll see later. Some questions have dozens of answers, leading to many possible pairs. We sample at most ten answer pairs per question to limit the number of data points per question. Finally, we cleaned up formatting by converting HTML to Markdown to make the model‚Äôs outputs more readable. You can find the dataset as well as the processing notebook <a href="https://huggingface.co/datasets/lvwerra/stack-exchange-paired">here</a>.</p>
<h2>
	<a id="efficient-training-strategies" href="#efficient-training-strategies">
		
	</a>
	<span>
		Efficient training strategies
	</span>
</h2>
<p>Even training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the <a href="https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer">performance docs</a> in Transformers for more info). So a 7B parameter model would use <code>(2+8)*7B=70GB</code> just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn‚Äôt train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you‚Äôll run out sooner or later.</p>
<p>Another option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the <a href="https://github.com/huggingface/peft"><code>peft</code></a> library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. </p>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif" alt=""/></a></p>
<p>Loading the model in 8bit reduces the memory footprint drastically since you only need one byte per parameter for the weights (e.g. 7B LlaMa is 7GB in memory). Instead of training the original weights directly, LoRA adds small adapter layers on top of some specific layers (usually the attention layers); thus, the number of trainable parameters is drastically reduced.</p>
<p>In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters (depending on the batch size and sequence length) to fit the entire fine-tuning setup. As detailed in the attached blog post above, this enables fine-tuning larger models (up to 50-60B scale models on a NVIDIA A100 80GB) at low cost. </p>
<p>These techniques have enabled fine-tuning large models on consumer devices and Google Colab. Notable demos are fine-tuning <code>facebook/opt-6.7b</code> (13GB in <code>float16</code> ), and <code>openai/whisper-large</code> on Google Colab (15GB GPU RAM). To learn more about using <code>peft</code>, refer to our <a href="https://github.com/huggingface/peft">github repo</a> or the <a href="https://huggingface.co/blog/trl-peft">previous blog post</a>(<a href="https://huggingface.co/blog/trl-peft">https://huggingface.co/blog/trl-peft</a>)) on training 20b parameter models on consumer hardware.</p>
<p>Now we can fit very large models into a single GPU, but the training might still be very slow. The simplest strategy in this scenario is data parallelism: we replicate the same training setup into separate GPUs and pass different batches to each GPU. With this, you can parallelize the forward/backward passes of the model and scale with the number of GPUs. </p>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_ddp.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_ddp.png" alt="chapter10_ddp.png"/></a></p>
<p>We use either the <code>transformers.Trainer</code> or <code>accelerate</code>, which both support data parallelism without any code changes, by simply passing arguments when calling the scripts with <code>torchrun</code> or <code>accelerate launch</code>. The following runs a training script with 8 GPUs on a single machine with <code>accelerate</code> and <code>torchrun</code>, respectively.</p>
<pre><code>accelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py
torchrun --nnodes 1  --nproc_per_node 8 my_torch_script.py
</code></pre>
<h2>
	<a id="supervised-fine-tuning" href="#supervised-fine-tuning">
		
	</a>
	<span>
		Supervised fine-tuning
	</span>
</h2>
<p>Before we start training reward models and tuning our model with RL, it helps if the model is already good in the domain we are interested in. In our case, we want it to answer questions, while for other use cases, we might want it to follow instructions, in which case instruction tuning is a great idea. The easiest way to achieve this is by continuing to train the language model with the language modeling objective on texts from the domain or task. The <a href="https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences">StackExchange dataset</a> is enormous (over 10 million instructions), so we can easily train the language model on a subset of it.</p>
<p>There is nothing special about fine-tuning the model before doing RLHF - it‚Äôs just the causal language modeling objective from pretraining that we apply here. To use the data efficiently, we use a technique called packing: instead of having one text per sample in the batch and then padding to either the longest text or the maximal context of the model, we concatenate a lot of texts with a EOS token in between and cut chunks of the context size to fill the batch without any padding.</p>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_preprocessing-clm.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_preprocessing-clm.png" alt="chapter10_preprocessing-clm.png"/></a></p>
<p>With this approach the training is much more efficient as each token that is passed through the model is also trained in contrast to padding tokens which are usually masked from the loss. If you don&#39;t have much data and are more concerned about occasionally cutting off some tokens that are overflowing the context you can also use a classical data loader.</p>
<p>The packing is handled by the <code>ConstantLengthDataset</code> and we can then use the <code>Trainer</code> after loading the model with <code>peft</code>. First, we load the model in int8, prepare it for training, and then add the LoRA adapters.</p>
<pre><code>
model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        load_in_8bit=<span>True</span>,
        device_map={<span>&#34;&#34;</span>: Accelerator().local_process_index}
    )
model = prepare_model_for_int8_training(model)


lora_config = LoraConfig(
    r=<span>16</span>,
    lora_alpha=<span>32</span>,
    lora_dropout=<span>0.05</span>,
    bias=<span>&#34;none&#34;</span>,
    task_type=<span>&#34;CAUSAL_LM&#34;</span>,
)

model = get_peft_model(model, config)
</code></pre>
<p>We train the model for a few thousand steps with the causal language modeling objective and save the model. Since we will tune the model again with different objectives, we merge the adapter weights with the original model weights.</p>
<p><strong>Disclaimer:</strong> due to LLaMA&#39;s license, we release only the adapter weights for this and the model checkpoints in the following sections. You can apply for access to the base model&#39;s weights by filling out Meta AI&#39;s <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">form</a> and then converting them to the ü§ó Transformers format by running this <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py">script</a>. Note that you&#39;ll also need to install ü§ó Transformers from source until the <code>v4.28</code> is released.</p>
<p>Now that we have fine-tuned the model for the task, we are ready to train a reward model.</p>
<h2>
	<a id="reward-modeling-and-human-preferences" href="#reward-modeling-and-human-preferences">
		
	</a>
	<span>
		Reward modeling and human preferences
	</span>
</h2>
<p>In principle, we could fine-tune the model using RLHF directly with the human annotations. However, this would require us to send some samples to humans for rating after each optimization iteration. This is expensive and slow due to the number of training samples needed for convergence and the inherent latency of human reading and annotator speed.</p>
<p>A trick that works well instead of direct feedback is training a reward model on human annotations collected before the RL loop. The goal of the reward model is to imitate how a human would rate a text. There are several possible strategies to build a reward model: the most straightforward way would be to predict the annotation (e.g. a rating score or a binary value for ‚Äúgood‚Äù/‚Äùbad‚Äù). In practice, what works better is to predict the ranking of two examples, where the reward model is presented with two candidates <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> (y_k, y_j) </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>(</span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span></span></span> for a given prompt <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex"> x </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>x</span></span></span></span> and has to predict which one would be rated higher by a human annotator.</p>
<p>This can be translated into the following loss function:</p>
<p> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">loss</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>Œ∏</mi><mo stretchy="false">)</mo><mo>=</mo><mo>‚àí</mo><msub><mi>E</mi><mrow><mrow><mo fence="true">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>k</mi></msub><mo fence="true">)</mo></mrow><mo>‚àº</mo><mi>D</mi></mrow></msub><mrow><mo fence="true">[</mo><mi>log</mi><mo>‚Å°</mo><mrow><mo fence="true">(</mo><mi>œÉ</mi><mrow><mo fence="true">(</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mrow><mo fence="true">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">)</mo></mrow><mo>‚àí</mo><msub><mi>r</mi><mi>Œ∏</mi></msub><mrow><mo fence="true">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>k</mi></msub><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex"> \operatorname{loss}(\theta)=- E_{\left(x, y_j, y_k\right) \sim D}\left[\log \left(\sigma\left(r_\theta\left(x, y_j\right)-r_\theta\left(x, y_k\right)\right)\right)\right] </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>l</span><span>o</span><span>s</span><span>s</span></span><span>(</span><span>Œ∏</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>‚àí</span><span><span>E</span><span><span><span><span><span><span></span><span><span><span><span><span>(</span></span><span>x</span><span>,</span><span><span>y</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>,</span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span><span>)</span></span></span><span>‚àº</span><span>D</span></span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>[</span><span>lo<span>g</span></span><span></span><span><span>(</span><span>œÉ</span><span></span><span><span>(</span><span><span>r</span><span><span><span><span><span><span></span><span><span>Œ∏</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>(</span><span>x</span><span>,</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span></span><span>‚àí</span><span></span><span><span>r</span><span><span><span><span><span><span></span><span><span>Œ∏</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span></span><span><span>(</span><span>x</span><span>,</span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>k</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>)</span></span><span>)</span></span><span>)</span></span><span>]</span></span></span></span></span></p>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex"> r </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>r</span></span></span></span> is the model‚Äôs score and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex"> y_j </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>y</span><span><span><span><span><span><span></span><span><span>j</span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span></span></span></span> is the preferred candidate.</p>
<p>With the StackExchange dataset, we can infer which of the two answers was preferred by the users based on the score. With that information and the loss defined above, we can then modify the <code>transformers.Trainer</code> by adding a custom loss function. </p>
<pre><code><span><span>class</span> <span>RewardTrainer</span>(<span>Trainer</span>):</span>
    <span><span>def</span> <span>compute_loss</span>(<span>self, model, inputs, return_outputs=<span>False</span></span>):</span>
        rewards_j = model(input_ids=inputs[<span>&#34;input_ids_j&#34;</span>],  attention_mask=inputs[<span>&#34;attention_mask_j&#34;</span>])[<span>0</span>]
        rewards_k = model(input_ids=inputs[<span>&#34;input_ids_k&#34;</span>], attention_mask=inputs[<span>&#34;attention_mask_k&#34;</span>])[<span>0</span>]
        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()
        <span>if</span> return_outputs:
            <span>return</span> loss, {<span>&#34;rewards_j&#34;</span>: rewards_j, <span>&#34;rewards_k&#34;</span>: rewards_k}
        <span>return</span> loss
</code></pre>
<p>We utilize a subset of a 100,000 pair of candidates and evaluate on a held-out set of 50,000. With a modest training batch size of 4, we train the LLaMA model using the LoRA <code>peft</code> adapter for a single epoch using the Adam optimizer with BF16 precision. Our LoRA configuration is:</p>
<pre><code>peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=<span>False</span>,
    r=<span>8</span>,
    lora_alpha=<span>32</span>,
    lora_dropout=<span>0.1</span>,
)
</code></pre>
<p>The training is logged via <a href="https://wandb.ai/krasul/huggingface/runs/wmd8rvq6?workspace=user-krasul">Weights &amp; Biases</a> and took a few hours on 8-A100 GPUs using the ü§ó research cluster and the model achieves a final <strong>accuracy of 67%</strong>. Although this sounds like a low score, the task is also very hard, even for human annotators.</p>
<p>As detailed in the next section, the resulting adapter can be merged into the frozen model and saved for further downstream use.</p>
<h2>
	<a id="reinforcement-learning-from-human-feedback" href="#reinforcement-learning-from-human-feedback">
		
	</a>
	<span>
		Reinforcement Learning from Human Feedback
	</span>
</h2>
<p>With the fine-tuned language model and the reward model at hand, we are now ready to run the RL loop. It follows roughly three steps:</p>
<ol>
<li>Generate responses from prompts</li>
<li>Rate the responses with the reward model</li>
<li>Run a reinforcement learning policy-optimization step with the ratings</li>
</ol>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/trl_loop.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/trl_loop.png" alt="Untitled"/></a></p>
<p>The Query and Response prompts are templated as follows before being tokenized and passed to the model:</p>
<pre><code>Question: &lt;Query&gt;

Answer: &lt;Response&gt;
</code></pre>
<p>The same template was used for SFT, RM and RLHF stages.</p>
<p>A common issue with training the language model with RL is that the model can learn to exploit the reward model by generating complete gibberish, which causes the reward model to assign high rewards. To balance this, we add a penalty to the reward: we keep a reference of the model that we don‚Äôt train and compare the new model‚Äôs generation to the reference one by computing the KL-divergence:</p>
<p> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">R</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">r</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mi>Œ≤</mi><mi mathvariant="normal">KL</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \operatorname{R}(x, y)=\operatorname{r}(x, y)- \beta \operatorname{KL}(x, y) </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>R</span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span><span>r</span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span><span></span><span>‚àí</span><span></span></span><span><span></span><span>Œ≤</span><span></span><span><span>K</span><span>L</span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span></span></span></span></p>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex"> r </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>r</span></span></span></span> is the reward from the reward model and  <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">KL</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \operatorname{KL}(x,y) </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span><span>K</span><span>L</span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span></span></span></span> is the KL-divergence between the current  policy and the reference model. </p>
<p>Once more, we utilize <code>peft</code> for memory-efficient training, which offers an extra advantage in the RLHF context. Here, the reference model and policy share the same base, the SFT model, which we load in 8-bit and freeze during training. We exclusively optimize the policy&#39;s LoRA weights using PPO while sharing the base model&#39;s weights.</p>
<pre><code><span>for</span> epoch, batch <span>in</span> tqdm(<span>enumerate</span>(ppo_trainer.dataloader)):
    question_tensors = batch[<span>&#34;input_ids&#34;</span>]
        
    
    response_tensors = ppo_trainer.generate(
        question_tensors,
        return_prompt=<span>False</span>,
        length_sampler=output_length_sampler,
        **generation_kwargs,
    )
    batch[<span>&#34;response&#34;</span>] = tokenizer.batch_decode(response_tensors, skip_special_tokens=<span>True</span>)

    
    texts = [q + r <span>for</span> q, r <span>in</span> <span>zip</span>(batch[<span>&#34;query&#34;</span>], batch[<span>&#34;response&#34;</span>])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[<span>0</span>][<span>&#34;score&#34;</span>] - script_args.reward_baseline) <span>for</span> output <span>in</span> pipe_outputs]

    
    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)
    
    ppo_trainer.log_stats(stats, batch, rewards)
</code></pre>
<p>We train for 20 hours on 3x8 A100-80GB GPUs, using the ü§ó research cluster, but you can also get decent results much quicker (e.g. after ~20h on 8 A100 GPUs). All the training statistics of the training run are available on <a href="https://wandb.ai/lvwerra/trl/runs/ie2h4q8p">Weights &amp; Biases</a>.</p>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/wandb_reward.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/wandb_reward.png" alt="Per batch reward at each step during training. The model‚Äôs performance plateaus after around 1000 steps."/></a>
<em>Per batch reward at each step during training. The model‚Äôs performance plateaus after around 1000 steps.</em></p>
<p>So what can the model do after training? Let&#39;s have a look!</p>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/llama_prompt.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/llama_prompt.png" alt="llama prompt"/></a></p>
<p>Although we shouldn&#39;t trust its advice on LLaMA matters just, yet, the answer looks coherent and even provides a Google link. Let&#39;s have a look and some of the training challenges next.</p>
<h2>
	<a id="challenges-instabilities-and-workarounds" href="#challenges-instabilities-and-workarounds">
		
	</a>
	<span>
		Challenges, instabilities and workarounds
	</span>
</h2>
<p>Training LLMs with RL is not always plain sailing. The model we demo today is the result of many experiments, failed runs and hyper-parameter sweeps. Even then, the model is far from perfect. Here we will share a few of the observations and headaches we encountered on the way to making this example.</p>
<h3>
	<a id="higher-reward-means-better-performance-right" href="#higher-reward-means-better-performance-right">
		
	</a>
	<span>
		Higher reward means better performance, right?
	</span>
</h3>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_high_reward.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_high_reward.png" alt="Wow this run must be great, look at that sweet, sweet, reward!"/></a>
<em>Wow this run must be great, look at that sweet, sweet, reward!</em></p>
<p>In general in RL, you want to achieve the highest reward. In RLHF we use a Reward Model, which is imperfect and given the chance, the PPO algorithm will exploit these imperfections. This can manifest itself as sudden increases in reward, however when we look at the text generations from the policy, they mostly contain repetitions of the string ```, as the reward model found the stack exchange answers containing blocks of code usually rank higher than ones without it. Fortunately this issue was observed fairly rarely and in general the KL penalty should counteract such exploits.</p>
<h3>
	<a id="kl-is-always-a-positive-value-isnt-it" href="#kl-is-always-a-positive-value-isnt-it">
		
	</a>
	<span>
		KL is always a positive value, isn‚Äôt it?
	</span>
</h3>
<p>As we previously mentioned, a KL penalty term is used in order to push the model‚Äôs outputs remain close to that of the base policy. In general, KL divergence measures the distances between two distributions and is always a positive quantity. However, in <code>trl</code> we use an estimate of the KL which in expectation is equal to the real KL divergence.</p>
<p> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><msub><mi>L</mi><mrow><mi>p</mi><mi>e</mi><mi>n</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>‚Å°</mo><mrow><mo fence="true">(</mo><msubsup><mi>œÄ</mi><mi>œï</mi><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">L</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><msup><mi>œÄ</mi><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">T</mi></mrow></msup><mo stretchy="false">(</mo><mi>y</mi><mo>‚à£</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex"> KL_{pen}(x,y) = \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right) </annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>K</span><span><span>L</span><span><span><span><span><span><span></span><span><span><span>p</span><span>e</span><span>n</span></span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>x</span><span>,</span><span></span><span>y</span><span>)</span><span></span><span>=</span><span></span></span><span><span></span><span>lo<span>g</span></span><span></span><span><span><span>(</span></span><span><span>œÄ</span><span><span><span><span><span><span></span><span><span>œï</span></span></span><span><span></span><span><span><span><span>R</span><span>L</span></span></span></span></span></span><span>‚Äã</span></span><span><span><span></span></span></span></span></span></span><span>(</span><span>y</span><span></span><span>‚à£</span><span></span><span>x</span><span>)</span><span>/</span><span><span>œÄ</span><span><span><span><span><span><span></span><span><span><span><span>S</span><span>F</span><span>T</span></span></span></span></span></span></span></span></span></span><span>(</span><span>y</span><span></span><span>‚à£</span><span></span><span>x</span><span>)</span><span><span>)</span></span></span></span></span></span></p>
<p>Clearly, when a token is sampled from the policy which has a lower probability than the SFT model, this will lead to a negative KL penalty, but on average it will be positive otherwise you wouldn&#39;t be properly sampling from the policy. However, some generation strategies can force some tokens to be generated or some tokens can suppressed. For example when generating in batches finished sequences are padded and when setting a minimum length the EOS token is suppressed. The model can assign very high or low probabilities to those tokens which leads to negative KL. As the PPO algorithm optimizes for reward, it will chase after these negative penalties, leading to instabilities.</p>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_neg_kl.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_neg_kl.png" alt="Negative KL"/></a></p>
<p>One needs to be careful when generating the responses and we suggest to always use a simple sampling strategy first before resorting to more sophisticated generation methods.</p>
<h3>
	<a id="ongoing-issues" href="#ongoing-issues">
		
	</a>
	<span>
		Ongoing issues
	</span>
</h3>
<p>There are still a number of issues that we need to better understand and resolve. For example, there are occassionally spikes in the loss, which can lead to further instabilities. </p>
<p><a href="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_loss_spikes.png" rel="noopener nofollow"><img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_loss_spikes.png" alt="Loss spikes"/></a></p>
<p>As we identify and resolve these issues, we will upstream the changes <code>trl</code>, to ensure the community can benefit.</p>
<h2>
	<a id="conclusion" href="#conclusion">
		
	</a>
	<span>
		Conclusion
	</span>
</h2>
<p>In this post, we went through the entire training cycle for RLHF, starting with preparing a dataset with human annotations, adapting the language model to the domain, training a reward model, and finally training a model with RL. </p>
<p>By using <code>peft</code>, anyone can run our example on a single GPU! If training is too slow, you can use data parallelism with no code changes and scale training by adding more GPUs.</p>
<p>For a real use case, this is just the first step! Once you have a trained model, you must evaluate it and compare it against other models to see how good it is. This can be done by ranking generations of different model versions, similar to how we built the reward dataset. </p>
<p>Once you add the evaluation step, the fun begins: you can start iterating on your dataset and model training setup to see if there are ways to improve the model. You could add other datasets to the mix or apply better filters to the existing one. On the other hand, you could try different model sizes and architecture for the reward model or train for longer.</p>
<p>We are actively improving TRL to make all steps involved in RLHF more accessible and are excited to see the things people build with it! Check out the <a href="https://github.com/lvwerra/trl/issues">issues on GitHub</a> if you&#39;re interested in contributing.</p>
<h2>
	<a id="citation" href="#citation">
		
	</a>
	<span>
		Citation
	</span>
</h2>
<pre><code>@misc {beeching2023stackllama,
    author       = { Edward Beeching and
                     Younes Belkada and
                     Kashif Rasul and
                     Lewis Tunstall and
                     Leandro von Werra and
                     Nazneen Rajani and
                     Nathan Lambert
                   },
    title        = { StackLLaMA: An RL Fine-tuned LLaMA Model for Stack Exchange Question and Answering },
    year         = 2023,
    url          = { https://huggingface.co/blog/stackllama },
    doi          = { 10.57967/hf/0513 },
    publisher    = { Hugging Face Blog }
}
</code></pre>
<h2>
	<a id="acknowledgements" href="#acknowledgements">
		
	</a>
	<span>
		Acknowledgements
	</span>
</h2>
<p>We thank Philipp Schmid for sharing his wonderful <a href="https://huggingface.co/spaces/philschmid/igel-playground">demo</a> of streaming text generation upon which our demo was based. We also thank Omar Sanseviero and Louis Castricato for giving valuable and detailed feedback on the draft of the blog post.</p>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
