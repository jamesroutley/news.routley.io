<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.arcee.ai/blog/the-trinity-manifesto?src=hn">Original</a>
    <h1>Arcee Trinity Mini: US-Trained Moe Model</h1>
    
    <div id="readability-page-1" class="page"><div><section><div><div><div><div global-animation="on"><div><p>Over the last year, anyone who cares about open weight language models has been watching Chinese labs.</p><p>Qwen, DeepSeek and others now define a lot of what &#34;state of the art open MoE&#34; looks like. In the United States, most of the action has centered on polishing other people&#39;s checkpoints.</p><p>At Arcee AI we want to add something that has been missing in that picture: a serious open weight model family trained end to end in America, by an American company, with weights that businesses and developers can actually own.</p><p>That family is Trinity.</p><p>Trinity Nano and Trinity Mini are available now.</p><p>Trinity Large is currently training on 2048 B300 GPUs and will arrive in January 2026.</p><p>Trinity Mini is our fully post-trained reasoning model. Trinity Nano Preview is something different: a personality-forward chat model that pushes the limits of sparsity with only 800M non-embedding parameters active per token across 56 layers and 128 experts. It&#39;s charming, it&#39;s fun to talk to, and it may be unstable in edge cases. This is an experimental release, not a thinking model. Nano Preview is available to download from Hugging Face but won&#39;t be hosted on our API.</p><p>This is the story of why we decided to go all in on pretraining, how Nano and Mini came to life, and where Trinity is headed next.</p><h3>Why we decided to own pretraining</h3><p>For a while, our strategy looked like everyone else&#39;s. Take a strong open base, post train it hard, wire it into tools and RAG, and ship.</p><p>That approach carried us very far. You can get impressive behavior with a good base, careful data and an instruction stack that matches the product.</p><p>At the same time, a few pressures kept building:</p><ul role="list"><li><strong>Ceilings on certain workloads:</strong> On some high stakes use cases, we kept iterating on post training and could see clear diminishing returns. Failure patterns pointed back to missing capabilities in the foundation, not to tuning mistakes.</li><li><strong>Jurisdictional Safety:</strong> Enterprise buyers are increasingly asking where the base model came from, what data went into it, and which licenses govern it. &#34;We fine tuned a model with unknown data provenance&#34; does not satisfy compliance officers. An end-to-end US data pipe offers legal certainty that foreign black-box models cannot.</li><li><strong>Long term product vision:</strong> We strongly believe that within two years, all meaningful AI applications will look like systems that grow and learn inside the environments where their users interact with them. Those systems will adapt their own training loops, and build and train directly from live usage. To build that kind of software you need to control the weights and the training pipeline, not only the instruction layer.</li></ul><p>We still use and appreciate great open-source models from others. We just came to the conclusion that if we want to offer truly long-lived, self-improving systems to customers, we also need to train our own foundations.</p><h3>AFM 4.5B: proving we could do this</h3><p>Our first step was AFM-4.5B, a dense 4.5B model trained on about 8 trillion curated tokens in partnership with DatologyAI.</p><p>AFM-4.5B was our &#34;can we do this at all&#34; experiment:</p><ol role="list"><li>Stand up large-scale data (with DatologyAI) and training pipelines.</li><li>Validate that careful and considered data curation gives clean scaling behavior.</li><li>Get real experience with training end-to-end.</li></ol><p>It worked. AFM-4.5B gave us a solid base of training and infrastructure practices, and showed us where to focus on capability improvements, especially around math and code.</p><p>Those lessons feed directly into Trinity.</p><h3>From AFM to Trinity Nano and Mini</h3><p>Trinity is our open weight MoE family. We chose to leap directly toward the frontier and then worked backward from that goal, which meant designing Nano and Mini as the two form factors that could both serve real users today and teach us how to train something far larger.</p><ul role="list"><li><strong>Trinity Nano Preview:</strong> 6B parameter MoE (1B active, ~800M non-embedding), 56 layers, 128 experts with 8 active per token</li><li><strong>Trinity Mini:</strong> 26B parameter MoE (3B active), fully post-trained reasoning model</li></ul><p>Both are released under <strong>Apache 2.0</strong>. Download Nano Preview and Mini from <a href="https://huggingface.co/arcee-ai">Hugging Face</a>. Mini is also available through our API and OpenRouter. Nano Preview is download-only.</p><p>Originally we thought of Nano and Mini strictly as training wheels for Trinity Large. The plan was to iron out our MoE recipe, then move on. In practice, these models came out strong enough that they are now serious production targets:</p><ul role="list"><li>They are compact reasoning models tuned for agents, tools and other reasoning heavy workloads, with average output length comparable to current instruct models.</li><li>They are some of the most cost efficient models in the world, with API pricing of <strong>$0.045 / $0.15</strong> for the Trinity-Mini model, plus a free tier with rate limits to back that up.</li><li>They anchor a preview of our own <strong>chat and API platform at </strong><a href="http://chat.arcee.ai"><strong>chat.arcee.ai</strong></a>, which will also host Trinity Large.</li></ul><h2>The Trinity architecture</h2><p>Building on our AFM naming convention, we refer to this Trinity architecture as <code>afmoe</code>, which integrates leading global architectural advances such as gated attention and Muon within a clean, US-controlled data pipeline. Here is what the stack looks like.</p><h4>Attention</h4><p>The attention mechanism combines several techniques that have proven effective at scale. We use grouped-query attention, mapping multiple query heads to each key-value head to reduce memory bandwidth during inference. Before computing scaled dot-product attention, we apply RMSNorm to the queries and keys (QK-norm), which stabilizes training.</p><p>We also use gated attention, specifically the G1 configuration from the Qwen paper. After SDPA, the output is elementwise-gated before the output projection: <code>out_proj(sdpa_out * \\sigma(gate_proj(x)))</code>. This gives the model a learned ability to modulate attention outputs per-position.</p><p>Finally, we adopt a local/global attention pattern at a 3:1 ratio. Three local attention layers with RoPE are followed by one global attention layer without positional embeddings (NoPE). This pattern reduces compute on long sequences while preserving the model&#39;s ability to reason over distant context.</p><h4>Normalization</h4><p>For layer normalization, we use a simplified version of depth-scaled sandwich norm. Each sublayer computes <code>output = x + norm(module(norm(x)))</code> . To enable stable training at depth, we initialize the gamma parameters of each norm layer to <code>1/sqrt(L)</code> where L is the total layer count. We also apply a norm before the language modeling head.</p><h3>Mixture-of-Experts</h3><p>Our MoE layers follow the DeepSeekMoE design: fine-grained experts plus a shared expert. Each MoE layer has 128 total routed experts, of which 8 are active per token, alongside 1 shared expert that is always active. The first two layers of the model are dense rather than sparse, providing a shared representational foundation before specialization begins, which we found improves training stability early.</p><p>For routing, we use <strong>sigmoid routing</strong> as introduced in DeepSeek-V3. Routing scores are computed with sigmoid followed by normalization rather than softmax. We also adopt the <strong>aux-loss-free load balancing</strong> scheme: an independently updated bias term determines routing decisions but is excluded from the weighting computation for each expert&#39;s contribution. This eliminates the need for auxiliary load-balancing losses that can distort the training objective.</p><h3>Initialization</h3><p>We initialize all trainable parameters from a truncated normal distribution with standard deviation <code>0.5/sqrt(dim)</code>. During the forward pass, we multiply the embedding output by <code>sqrt(dim)</code>.</p><h4>Optimizer</h4><p>We train with Muon, using the distributed implementation from Microsoft&#39;s Dion repository. To transfer learning rates across parameter shapes, we set <code>adjusted_lr = lr * sqrt(max(1, fan_out / fan_in))</code>, which we empirically observe enables optimal learning rate transfer when scaling. We sweep the Adam learning rate and Muon learning rate separately. The learning rate schedule we use is WSD (warmup-stable-decay). We apply no weight decay to embeddings.</p><h4>Infrastructure</h4><p>Training runs on a modified version of TorchTitan in bf16 precision. Nano and Mini trained on 512 H200 GPUs using an HSDP parallelism setup with a global batch size of 8192 sequences at 4096 tokens each.</p><h4>Context extension</h4><p>We only expanded the global attention layers during context extension, which allowed the model to learn extended sequence lengths very quickly. Trinity Nano was trained at 256k sequence length (inference at 128k), and Trinity Mini was trained at 128k sequence length.</p><h3>Data and training</h3><div><p><img src="https://45777467.fs1.hubspotusercontent-na1.net/hubfs/45777467/datapoweredby_DatologyAI.svg" alt="Data powered by Datology"/>
</p></div><p>Trinity Nano and Mini train on <strong>10T tokens</strong>, organized into three phases with progressively higher quality and STEM concentration: 7T tokens in phase 1, 1.8T tokens in phase 2, and 1.2T tokens in phase 3. This curriculum allows the model to build broad coverage early and then sharpen on high-signal data. The mix reuses our curated AFM dataset and adds substantially more math and code.</p><p><strong>Datology</strong> continued to be a key partner on the data side. On the compute and systems side we worked closely with <strong>Prime Intellect</strong>. They not only served the H100 clusters Datology used to generate synthetic data, they have been deeply involved in helping scale our training setup to the GPU footprint required for a fully frontier sized model, including the current <strong>2048 B300 GPU</strong> configuration for Trinity Large.</p><p>
  <a href="https://45777467.fs1.hubspotusercontent-na1.net/hubfs/45777467/Trinity%20Mini%20Benchmark.png" target="_blank" rel="noopener">
    <img src="https://45777467.fs1.hubspotusercontent-na1.net/hubfs/45777467/Trinity%20Mini%20Benchmark.png" alt="Trinity Mini Benchmark" loading="lazy"/>
  </a>
</p><h3>Training at this scale is hard</h3><p>MoE training at scale is messy. There is no polite way to say it. It is fucking hard. Hereâ€™s how we prepared for Trinity-Large:</p><ul role="list"><li>Over the last month Datology has generated <strong>10 trillion unique synthetic tokens</strong> on clusters that peaked at <strong>2048 H100 GPUs</strong>.</li><li>We pair those with <strong>10 trillion web tokens</strong> to build a <strong>20T token</strong> dataset.</li><li>Prime Intellect&#39;s infrastructure and operational experience have been crucial here, from synthetic data generation runs to the ongoing <strong>2048 B300 GPU</strong> training job for Trinity-Large.</li></ul><p>The work is demanding, but it is also where most of the fun is. Every bug we chase and every learning curve we overcome feed directly into models that anyone can download and build upon.</p><h3>Why owning weights matters</h3><p>Looking forward, we see a clear pattern.</p><p>As applications get more ambitious, the boundary between &#34;model&#34; and &#34;product&#34; keeps moving. Systems will:</p><ul role="list"><li>Learn from the behavior of specific user populations.</li><li>Grow new skills from interactions with other tools and services.</li></ul><p>Those systems will blur the distinction between pretraining data, synthetic data, post training tasks and live feedback. They will evolve continuously in the environments where they are deployed.</p><p>To do that responsibly and effectively, you need control of the weights and the training loop. You need to decide what kind of data the model sees, what objectives it optimizes, and how its capabilities change over time.</p><p>Our goal with Trinity is to provide that foundation for businesses, enterprises and developers who want ownership rather than a black box.</p><h3>Trinity Large and what comes next</h3><p>All of this leads to Trinity Large.</p><ul role="list"><li>It trains on a <strong>20T token</strong> dataset, half synthetic and half web, built together with Datology and backed by Primeintellect&#39;s compute infrastructure.</li><li>It uses the same core MoE recipe as Nano and Mini, extended to a fully frontier sized configuration.</li><li>The training run is currently underway on <strong>2048 B300 GPUs</strong>, targeting release in <strong>January 2026</strong>.</li></ul><p>For most of this post we have talked about principles, data and architecture without naming the final size.</p><p>Trinity Large is a 420B parameter model with 13B active parameters per token.</p><p>Nano and Mini exist to make that possible, and to give the community strong open models to use right now while Large trains.</p><p>When Trinity Large ships, we will release a full technical report covering how we went from a 4.5B dense model to an open frontier MoE in just over six months.</p><h3>Try Trinity Nano and Mini today</h3><p>If you care about open weight models, and you want an American MoE family that aims squarely at the frontier while staying fully permissive, we invite you to start working with Trinity today.</p><ul role="list"><li>Download the weights at <a href="https://www.google.com/search?q=https://huggingface.com/arcee-ai"><strong>huggingface.com/arcee-ai</strong></a>.</li><li>Call the models through <a href="https://openrouter.ai/arcee-ai/trinity-mini:free"><strong>OpenRouter</strong></a> with generous free tiers.</li><li>Experiment with our preview <strong>chat and API platform at </strong><a href="http://chat.arcee.ai"><strong>chat.arcee.ai</strong></a>.</li></ul><p><strong>Break them. Push them.</strong> Tell us where they shine and, more importantly, where they fail. That feedback will shape Trinity Large and everything that follows.</p><p>We are building these models so that you can own them.</p></div></div></div></div></div></section></div></div>
  </body>
</html>
