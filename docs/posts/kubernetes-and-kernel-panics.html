<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://netflixtechblog.com/kubernetes-and-kernel-panics-ed620b9c6225?gi=2e8a65263c94">Original</a>
    <h1>Kubernetes and Kernel Panics</h1>
    
    <div id="readability-page-1" class="page"><section><div><div><div><div><div><div><div><div><div><div><div><a href="https://netflixtechblog.medium.com" rel="noopener follow"><div><div aria-hidden="false"><div><div><p><img alt="Netflix Technology Blog" src="https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"/></p></div></div></div></div></a><a href="https://netflixtechblog.com" rel="noopener  ugc nofollow"><div><div><div aria-hidden="false"><div><div><p><img alt="Netflix TechBlog" src="https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"/></p></div></div></div></div></div></a></div></div></div></div></div></div></div><p id="8759">How Netflix’s Container Platform Connects Linux Kernel Panics to Kubernetes Pods</p><p id="158b"><em>By Kyle Anderson</em></p><p id="53a2">With a recent effort to reduce customer (engineers, not end users) pain on our container platform <a href="https://netflixtechblog.com/tagged/titus" rel="noopener ugc nofollow" target="_blank">Titus</a>, I started investigating “orphaned” pods. There are pods that never got to finish and had to be garbage collected with no real satisfactory final status. Our Service job (think <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/" rel="noopener ugc nofollow" target="_blank">ReplicatSet</a>) owners don’t care too much, but our Batch users care a lot. Without a real return code, how can they know if it is safe to retry or not?</p><p id="e0e7">These orphaned pods represent real pain for our users, even if they are a small percentage of the total pods in the system. Where are they going, exactly? Why did they go away?</p><p id="5820">This blog post shows how to connect the dots from the worst case scenario (a kernel panic) through to Kubernetes (k8s) and eventually up to us operators so that we can track how and why our k8s nodes are going away.</p><p id="23e0">Orphaned pods get lost because the underlying k8s node object goes away. Once that happens a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection" rel="noopener ugc nofollow" target="_blank">GC</a> process deletes the pod. On Titus we run a custom controller to store the history of Pod and Node objects, so that we can save some explanation and show it to our users. This failure mode looks like this in our UI:</p><figure><div role="button" tabindex="0"><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/0*bPnudULpVKE1AKEH 640w, https://miro.medium.com/v2/resize:fit:720/0*bPnudULpVKE1AKEH 720w, https://miro.medium.com/v2/resize:fit:750/0*bPnudULpVKE1AKEH 750w, https://miro.medium.com/v2/resize:fit:786/0*bPnudULpVKE1AKEH 786w, https://miro.medium.com/v2/resize:fit:828/0*bPnudULpVKE1AKEH 828w, https://miro.medium.com/v2/resize:fit:1100/0*bPnudULpVKE1AKEH 1100w, https://miro.medium.com/v2/resize:fit:1400/0*bPnudULpVKE1AKEH 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/0*bPnudULpVKE1AKEH 640w, https://miro.medium.com/v2/resize:fit:720/0*bPnudULpVKE1AKEH 720w, https://miro.medium.com/v2/resize:fit:750/0*bPnudULpVKE1AKEH 750w, https://miro.medium.com/v2/resize:fit:786/0*bPnudULpVKE1AKEH 786w, https://miro.medium.com/v2/resize:fit:828/0*bPnudULpVKE1AKEH 828w, https://miro.medium.com/v2/resize:fit:1100/0*bPnudULpVKE1AKEH 1100w, https://miro.medium.com/v2/resize:fit:1400/0*bPnudULpVKE1AKEH 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="264" loading="lazy" role="presentation"/></picture></div></div><figcaption>What it looks like to our users when a k8s node and its pods disappear</figcaption></figure><p id="be89">This is <em>an </em>explanation, but it wasn’t very satisfying to me or to our users. <em>Why</em> was the agent lost?</p><p id="4839">Nodes can go away for any reason, especially in “the cloud”. When this happens, usually a k8s cloud-controller provided by the cloud vendor will detect that the actual server, in our case an EC2 Instance, has actually gone away, and will in turn delete the k8s node object. That still doesn’t really answer the question of <em>why</em>.</p><p id="67c5">How can we make sure that every instance that goes away has a reason, account for that reason, and bubble it up all the way to the pod? It all starts with an annotation:</p><pre><span id="2fb2">{</span></pre><p id="6cd5">Just making a place to put this data is a great start. Now all we have to do is make our GC controllers aware of this annotation, and then sprinkle it into any process that could potentially make a pod or node go away unexpectedly. Adding an annotation (as opposed to patching the status) preserves the rest of the pod as-is for historical purposes. (We also add annotations for what did the terminating, and a short <code>reason-code</code> for tagging)</p><p id="3ea2">The <code>pod-termination-reason</code> annotation is useful to populate human readable messages like:</p><ul><li id="2657">“This pod was preempted by a higher priority job ($id)”</li><li id="db59">“This pod had to be terminated because the underlying hardware failed ($failuretype)”</li><li id="c052">“This pod had to be terminated because $user ran sudo halt on the node”</li><li id="28df"><strong>“This pod died unexpectedly because the underlying node kernel panicked!”</strong></li></ul><p id="e870">But wait, how are we going to annotate a pod for a node that kernel panicked?</p><p id="5206">When the Linux kernel panics, there is just not much you can do. But what if you could send out some sort of “with my final breath, I curse Kubernetes!” UDP packet?</p><p id="23fc">Inspired by this <a href="https://research.google/pubs/pub45855/" rel="noopener ugc nofollow" target="_blank">Google Spanner paper</a>, where Spanner nodes send out a “last gasp” UDP packet to release leases &amp; locks, you too can configure your servers to do the same upon kernel panic using a stock Linux module: <code><a href="https://www.kernel.org/doc/Documentation/networking/netconsole.txt" rel="noopener ugc nofollow" target="_blank">netconsole</a></code>.</p><p id="50c5">The fact that the Linux kernel can even send out UDP packets with the string ‘kernel panic’, <em>while it is panicking</em>, is kind of amazing. This works because netconsole needs to be configured with almost the entire IP header filled out already beforehand. That is right, you have to tell Linux exactly what your source MAC, IP, and UDP Port are, as well as the destination MAC, IP, and UDP ports. You are practically constructing the UDP packet for the kernel. But, with that prework, when the time comes, the kernel can easily <a href="https://github.com/torvalds/linux/blob/94f6f0550c625fab1f373bb86a6669b45e9748b3/drivers/net/netconsole.c#L932" rel="noopener ugc nofollow" target="_blank">construct</a> the packet and get it out the (preconfigured) network interface as things come crashing down. Luckily the <code><a href="https://manpages.ubuntu.com/manpages/jammy/en/man8/netconsole-setup.8.html" rel="noopener ugc nofollow" target="_blank">netconsole-setup</a></code> command makes the setup pretty easy. All the configuration options can be set <a href="https://wiki.ubuntu.com/Kernel/Netconsole#Step_3:_Initialize_netconsole_at_boot_time" rel="noopener ugc nofollow" target="_blank">dynamically</a> as well, so that when the endpoint changes one can point to the new IP.</p><p id="18e5">Once this is setup, kernel messages will start flowing right after <code>modprobe</code>. Imagine the whole thing operating like a <code>dmesg | netcat -u $destination 6666</code>, but in kernel space.</p><p id="30f9">With <code>netconsole</code> setup, the last gasp from a crashing kernel looks like a set of UDP packets exactly like one might expect, where the data of the UDP packet is simply the text of the kernel message. In the case of a kernel panic, it will look something like this (one UDP packet per line):</p><pre><span id="c35d">Kernel panic - not syncing: buffer overrun at 0x4ba4c73e73acce54</span></pre><p id="bdc0">The last piece is to connect is Kubernetes (k8s). We need a k8s controller to do the following:</p><ol><li id="114a">Listen for netconsole UDP packets on port 6666, watching for things that look like kernel panics from nodes.</li><li id="10fd">Upon kernel panic, lookup the k8s node object associated with the IP address of the incoming netconsole packet.</li><li id="2e6b">For that k8s node, find all the pods bound to it, annotate, then delete those pods (they are toast!).</li><li id="0d85">For that k8s node, annotate the node and then delete it too (it is also toast!).</li></ol><p id="577a">Parts 1&amp;2 might look like this:</p><pre><span id="17e7">for {</span></pre><p id="9ed0">And then parts 3&amp;4 might look like this:</p><pre><span id="4b02">func handleKernelPanicOnNode(ctx context.Context, addr *net.UDPAddr, nodeInformer cache.SharedIndexInformer, podInformer cache.SharedIndexInformer, kubeClient kubernetes.Interface, line string) {</span></pre><p id="dec3">With that code in place, as soon as a kernel panic is detected, the pods and nodes immediately go away. No need to wait for any GC process. The annotations help document what happened to the node &amp; pod:</p><figure><div role="button" tabindex="0"><div><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cjClRuyUQ67lu2shmjCObQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*cjClRuyUQ67lu2shmjCObQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*cjClRuyUQ67lu2shmjCObQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*cjClRuyUQ67lu2shmjCObQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*cjClRuyUQ67lu2shmjCObQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cjClRuyUQ67lu2shmjCObQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjClRuyUQ67lu2shmjCObQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"/><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*cjClRuyUQ67lu2shmjCObQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*cjClRuyUQ67lu2shmjCObQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*cjClRuyUQ67lu2shmjCObQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*cjClRuyUQ67lu2shmjCObQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*cjClRuyUQ67lu2shmjCObQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*cjClRuyUQ67lu2shmjCObQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*cjClRuyUQ67lu2shmjCObQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"/><img alt="" width="700" height="278" loading="lazy" role="presentation"/></picture></div></div><figcaption>A real pod lost on a real k8s node that had a real kernel panic!</figcaption></figure><p id="4bd6">Marking that a job failed because of a kernel panic may not be <em>that</em> satisfactory to our customers. But they can take satisfaction in knowing that we now have the required observability tools to start fixing those kernel panics!</p><p id="5c9c">Do you also enjoy really getting to the bottom of why things fail in your systems or think kernel panics are cool? Join us on the <a href="https://jobs.netflix.com/jobs/198642264" rel="noopener ugc nofollow" target="_blank">Compute Team</a> where we are building a world-class container platform for our engineers.</p></div></div></div></div></section></div>
  </body>
</html>
