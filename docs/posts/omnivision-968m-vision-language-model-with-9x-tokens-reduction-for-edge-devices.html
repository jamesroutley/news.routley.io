<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://nexa.ai/blogs/omni-vision">Original</a>
    <h1>Omnivision-968M: Vision Language Model with 9x Tokens Reduction for Edge Devices</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><video controls="" controlslist="nodownload"><source src="https://public-storage.nexa4ai.com/omnivision-demo.mp4" type="video/mp4"/>Your browser does not support the video tag.</video></p></div><p>Omnivision is a compact, sub-billion (968M) multimodal model for processing both visual and text inputs, optimized for edge devices. Improved on LLaVA&#39;s architecture, it features:</p><ul><li><strong>9x Tokens Reduction</strong>: Reduces image tokens from 729 to 81, cutting latency and computational cost.</li><li><strong>Enhanced Accuracy</strong>: Reduces hallucinations using DPO training from trustworthy data.</li></ul><h4>Demo</h4><p><img src="https://cdn.sanity.io/images/jck7ws73/production/40d75a2b7cec2a6b2875d0c61dc8d24fbc57f343-3362x2166.png" alt="OmniVision generated description for an image with multiple object" loading="lazy"/></p><p>(OmniVision generated description for an image with multiple object)</p><p><img src="https://cdn.sanity.io/images/jck7ws73/production/ac9a39073987b2ff20d95c7c6255cec3eb8ee90f-3374x1792.png" alt="OmniVision generated description for an abstract art piece by Yayoi Kusama" loading="lazy"/></p><p>(OmniVision generated description for an abstract art piece by Yayoi Kusama)</p><h4>Get your hands on OmniVision</h4><h6>HuggingFace Space ðŸ¤—  </h6><p><a href="https://huggingface.co/spaces/NexaAIDev/omnivlm-dpo-demo" target="_blank">NexaAIDev/omnivlm-dpo-demo</a></p><h6>Run OmniVision on Your Device</h6><p><a href="https://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#install-option-1-executable-installer" target="_blank">Install Nexa SDK</a>, run this on your terminal:</p><p>Or run it with Streamlit local UI:</p><blockquote>ðŸ’» OmniVision FP16 version requires 988 MB RAM and 948 MB storage space.</blockquote><h4>Model Architecture</h4><p><img src="https://cdn.sanity.io/images/jck7ws73/production/73c39d254909c28892137ece72cd6c8aa58f1823-2256x1392.png" alt="Model Architecture for omni-vision" loading="lazy"/></p><p>OmniVision&#39;s architecture consists of three key components:</p><ul><li><strong>Base Language Model: </strong><a href="https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct" target="_blank">Qwen2.5-0.5B-Instruct</a> functions as the base model to process text inputs.</li><li><strong>Vision Encoder:</strong> <a href="https://huggingface.co/google/siglip-so400m-patch14-384" target="_blank">SigLIP-400M</a> operates at 384 resolution with 14Ã—14 patch size to generate image embeddings.</li><li><strong>Projection Layer: </strong>Multi-Layer Perceptron (MLP) aligns the vision encoder&#39;s embeddings with the language model&#39;s token space. Compared to vanilla Llava architecture, we designed a projector that reduce 9X image tokens.</li></ul><p>The vision encoder first transforms input images into embeddings, which are then processed by the projection layer to match the token space of Qwen2.5-0.5B-Instruct, enabling end-to-end visual-language understanding.</p><h4>Training Methodology</h4><p>We developed OmniVision through a three-stage training pipeline:</p><h6>Pretraining</h6><p>The initial stage focuses on establishing basic visual-linguistic alignments using image-caption pairs, during which only the projection layer parameters are unfrozen to learn these fundamental relationships. </p><h6>Supervised Fine-tuning (SFT)</h6><p>We enhance the model&#39;s contextual understanding using image-based question-answering datasets. This stage involves training on structured chat histories that incorporate images for the model to generate more contextually appropriate responses.</p><h6>Direct Preference Optimization (DPO)</h6><p>The final stage implements DPO by first generating responses to images using the base model. A <a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct" target="_blank">teacher model </a>then produces minimally edited corrections while maintaining high semantic similarity with the original responses, focusing specifically on accuracy-critical elements. These original and corrected outputs form chosen-rejected pairs. The fine-tuning targeted at essential model output improvements without altering the model&#39;s core response characteristics.</p><h4>Technical Innovations for Edge Deployment</h4><h6>9x Tokens Reduction through Token Compression</h6><p>Processing image tokens creates significant computational overhead in edge deployment of multimodal models. In the standard LLaVA architecture, each image generates 729 tokens (27x27), leading to high latency and computational costs. We developed a reshaping mechanism in the projection stage that transforms image embeddings from <code>[batch_size, 729, hidden_size]</code> to <code>[batch_size, 81, hidden_size*9]</code>. This reduces token count by 9x without compromising model performance.Our experiments show this compression method hugely improved model performance. Analysis suggests this improvement stems from the base Qwen model&#39;s handling of shorter sequences, where the compressed format provides more concentrated information representation.</p><h6>Minimal-Edit DPO for Enhanced Response Quality</h6><p>Traditional DPO methods can lead to significant shifts in model behavior. Our DPO implementation uses minimal-edit pairs for training. The teacher model makes small, targeted improvements to the base model&#39;s outputs while preserving their original structure. This approach ensures precise quality improvements without disrupting the model&#39;s core capabilities.</p><h4>Benchmark</h4><p>Below we demonstrate a figure to show how OmniVision performs against <a href="https://huggingface.co/qnguyen3/nanoLLaVA" target="_blank">nanoLLAVA</a>:</p><p><img src="https://cdn.sanity.io/images/jck7ws73/production/b1ec1bc392c94f5855b811e72700a87577943c13-768x680.png" alt="Radar chart" loading="lazy"/></p><p>We have also conducted a series of experiments on benchmark datasets, including MM-VET, ChartQA, MMMU, ScienceQA, POPE to evaluate the performance of Omnivision.</p><div><div><table><thead><tr><th scope="col"></th><th scope="col"><p>Nexa AI Omni-Vision</p></th><th scope="col"><p>nanoLLAVA</p></th><th scope="col"><p>Qwen2-VL-2B</p></th></tr></thead><tbody><tr><td><p>MM-VET</p></td><td><p>27.5</p></td><td><p>23.9</p></td><td><p>49.5</p></td></tr><tr><td><p>ChartQA (Test)</p></td><td><p>59.2</p></td><td><p>N/A</p></td><td><p>73.5</p></td></tr><tr><td><p>MMMU (Test)</p></td><td><p>41.8</p></td><td><p>28.6</p></td><td><p>41.1</p></td></tr><tr><td><p>MMMU (Eval)</p></td><td><p>39.9</p></td><td><p>30.4</p></td><td><p>41.1</p></td></tr><tr><td><p>ScienceQA (Eval)</p></td><td><p>62.2</p></td><td><p>59.0</p></td><td><p>N/A</p></td></tr><tr><td><p>ScienceQA (Test)</p></td><td><p>64.5</p></td><td><p>59.0</p></td><td><p>N/A</p></td></tr><tr><td><p>POPE</p></td><td><p>89.4</p></td><td><p>84.1</p></td><td><p>N/A</p></td></tr></tbody></table></div></div><p>In all the tasks, OmniVision outperforms nanoLLAVA, the previous world&#39;s smallest vision-language model.</p><h4>What&#39;s Next</h4><p>Omnivision is in early development and we are working to address current limitations:</p><ul><li>Expand DPO Training: Increase the scope of DPO (Direct Preference Optimization) training in an iterative process to continually improve model performance and response quality.</li><li>Improve document and text understanding.</li></ul><p>In the long term, we aim to develop OmniVision as a fully optimized, production-ready solution for edge AI multimodal applications.</p><p><img src="https://cdn.sanity.io/images/jck7ws73/production/e84cc3925dd5458951c6f121a4421717e25f85d0-1425x768.jpg" alt="Octopus can see!" loading="lazy"/></p><p><em>Kudos to &lt;Alex&gt;, &lt;Zack&gt; and Nexa AI team.</em></p><p><em>Blog written by &lt;Kai&gt;, &lt;Alan&gt;.</em></p></div></div>
  </body>
</html>
