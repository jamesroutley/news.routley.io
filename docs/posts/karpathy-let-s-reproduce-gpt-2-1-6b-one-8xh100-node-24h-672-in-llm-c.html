<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/karpathy/llm.c/discussions/677">Original</a>
    <h1>Karpathy: Let&#39;s reproduce GPT-2 (1.6B): one 8XH100 node 24h $672 in llm.c</h1>
    
    <div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="6923990" data-target-translation-type="discussion">
        <tr>
    <td>
        <p dir="auto">In this post we are reproducing GPT-2 in llm.c. This is <strong>&#34;the GPT-2&#34;</strong>, the full, 1558M parameter version that was introduced in OpenAI&#39;s blog post <a href="https://openai.com/index/better-language-models/" rel="nofollow">Better Language Models and their Implications</a> in February 14, 2019. llm.c does so directly in C/CUDA (total of ~5,000 lines of code), without the typical training stack that would involve the Python interpreter and a significantly more complex deep learning library like PyTorch/JAX, huggingface/transformers, or etc. In 2019, training GPT-2 was an involved project from an entire team and considered a big model run but, ~5 years later, due to improvements in compute (H100 GPUs), software (CUDA, cuBLAS, cuDNN, FlashAttention) and data (e.g. the FineWeb-Edu dataset), we can reproduce this model on a single 8XH100 node in 24 hours, and for $672, which is quite incredible. There are some caveats and asterisks involved though - llm.c is still not perfectly tuned, sufficiently stabilized (we still see loss spikes and bad activation ranges now and then), and our evals are not comprehensive (e.g. we do not carefully eval multilingual, code, math); A lot of this work is still ongoing.</p>
<p dir="auto"><strong>Unicorns</strong>. The natural place to start is probably with unicorns. In the GPT-2 blog post, OpenAI prompted GPT-2 with this unicorn prompt. Let&#39;s see what our model thinks about English-speaking unicorns in the Andes mountains:</p>
<p dir="auto"><strong>Prompt</strong>: <em>In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</em></p>
<p dir="auto">Fun! :) The model is fairly coherent and qualitatively somewhere around the level of GPT-2. You can find 20 samples from both GPT-2 and the llm.c model <a href="http://llmc.s3-us-west-2.amazonaws.com/html/gpt2_vs_llmc30kedu.html" rel="nofollow">here</a>, or generate many more using instructions down below.</p>
<p dir="auto"><strong>Training</strong>. Training a GPT-2 with llm.c is quite simple because it is written in C/CUDA, so there is no need for minconda, Python, PyTorch, etc. You will want an 8XH100 GPU box, I recommend spinning one up from <a href="https://lambdalabs.com" rel="nofollow">Lambda labs</a>. But llm.c is flexible on its compute - if you have only 1 GPU you can still get your GPT-2, you&#39;ll just have to wait 8 days instead of 1. If you have 16 GPUs (e.g. using the new Lambda 1 Click Clusters), you&#39;ll be able to train multinode and only have to wait 12 hours. Once you spin up your node, here are the complete instructions to train your GPT-2 (this only takes a ~minute from blank box to start stepping):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# install cudnn so we can use FlashAttention and run fast (optional)
# https://developer.nvidia.com/cudnn-downloads
# for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

# &#34;install&#34; cudnn-frontend to ~/
git clone https://github.com/NVIDIA/cudnn-frontend.git

# install MPI (optional, if you intend to use multiple GPUs)
# (you might also have to install NVIDIA NCCL if it doesn&#39;t come with your setup)
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

# download and enter llm.c repo
git clone https://github.com/karpathy/llm.c.git
cd llm.c

# download the &#34;starter pack&#34; (~1GB download)
# contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s
./dev/download_starter_pack.sh

# download the training dataset (FineWeb-Edu 100B token) .bin data shards
# note: this is a total of 1001 data shards. If you only want to test things
# out and don&#39;t want to do an actual run, feel free to append the number of
# training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)
# the full dataset is ~200GB, we can store it here in dev/data directory.
cd dev/data
./edu_fineweb.sh

# compile (~1 min 1st time for cuDNN mostly, few sec from then on)
cd ../../
make train_gpt2cu USE_CUDNN=1

# and train! (wait 24 hours here)
mpirun -np 8 ./train_gpt2cu \
	-i &#34;dev/data/edu_fineweb100B/edu_fineweb_train_*.bin&#34; \
	-j &#34;dev/data/edu_fineweb100B/edu_fineweb_val_*.bin&#34; \
	-o &#34;log_gpt2_1558M&#34; \
	-v 250 -s 300000 -g 384 \
	-h 1 \
	-b 16 -t 1024 \
	-d 1048576 \
	-r 0 \
	-z 1 \
	-c 0.1 \
	-k &#34;cosine&#34; \
	-l 0.0006 \
	-q 0.1 \
	-u 700 \
	-n 2000 \
	-x 32000 \
	-ge 1 \
	-y 1 \
	-e &#34;d48&#34;"><pre><span><span>#</span> install cudnn so we can use FlashAttention and run fast (optional)</span>
<span><span>#</span> https://developer.nvidia.com/cudnn-downloads</span>
<span><span>#</span> for me, CUDA 12 (run `nvcc --version`) running on Linux x86_64 Ubuntu 22.04</span>
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install libcudnn9-dev-cuda-12

<span><span>#</span> &#34;install&#34; cudnn-frontend to ~/</span>
git clone https://github.com/NVIDIA/cudnn-frontend.git

<span><span>#</span> install MPI (optional, if you intend to use multiple GPUs)</span>
<span><span>#</span> (you might also have to install NVIDIA NCCL if it doesn&#39;t come with your setup)</span>
sudo apt -y install openmpi-bin openmpi-doc libopenmpi-dev

<span><span>#</span> download and enter llm.c repo</span>
git clone https://github.com/karpathy/llm.c.git
<span>cd</span> llm.c

<span><span>#</span> download the &#34;starter pack&#34; (~1GB download)</span>
<span><span>#</span> contains GPT2-124M weights (used in tests), tokenizer, eval data .bin s</span>
./dev/download_starter_pack.sh

<span><span>#</span> download the training dataset (FineWeb-Edu 100B token) .bin data shards</span>
<span><span>#</span> note: this is a total of 1001 data shards. If you only want to test things</span>
<span><span>#</span> out and don&#39;t want to do an actual run, feel free to append the number of</span>
<span><span>#</span> training shards to download (e.g. for just 10 shards: ./edu_fineweb.sh 10)</span>
<span><span>#</span> the full dataset is ~200GB, we can store it here in dev/data directory.</span>
<span>cd</span> dev/data
./edu_fineweb.sh

<span><span>#</span> compile (~1 min 1st time for cuDNN mostly, few sec from then on)</span>
<span>cd</span> ../../
make train_gpt2cu USE_CUDNN=1

<span><span>#</span> and train! (wait 24 hours here)</span>
mpirun -np 8 ./train_gpt2cu \
	-i <span><span>&#34;</span>dev/data/edu_fineweb100B/edu_fineweb_train_*.bin<span>&#34;</span></span> \
	-j <span><span>&#34;</span>dev/data/edu_fineweb100B/edu_fineweb_val_*.bin<span>&#34;</span></span> \
	-o <span><span>&#34;</span>log_gpt2_1558M<span>&#34;</span></span> \
	-v 250 -s 300000 -g 384 \
	-h 1 \
	-b 16 -t 1024 \
	-d 1048576 \
	-r 0 \
	-z 1 \
	-c 0.1 \
	-k <span><span>&#34;</span>cosine<span>&#34;</span></span> \
	-l 0.0006 \
	-q 0.1 \
	-u 700 \
	-n 2000 \
	-x 32000 \
	-ge 1 \
	-y 1 \
	-e <span><span>&#34;</span>d48<span>&#34;</span></span></pre></div>
<p dir="auto">I will describe the args in a second. You&#39;ll see a bunch of prints scroll through and then the optimization will begin:</p>
<div data-snippet-clipboard-copy-content="num_parameters: 1557686400 =&gt; bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=&gt; setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
..."><pre><code>num_parameters: 1557686400 =&gt; bytes: 3115372800
allocated 2971 MiB for model parameters
batch_size B=16 * seq_len T=1024 * num_processes=8 and total_batch_size=1048576
=&gt; setting grad_accum_steps=8
created directory: log_gpt2_1558M
allocating 40409 MiB for activations
val loss 11.129390
allocating 2971 MiB for parameter gradients
allocating 742 MiB for AdamW optimizer state m
allocating 742 MiB for AdamW optimizer state v
allocating 742 MiB for master copy of params
step    1/32000 | loss 11.133732 (+nanz)| norm 52.9732 (+nanz)| lr 8.57e-07 | 3056.36 ms | 42.6% bf16 MFU | 343080 tok/s
step    2/32000 | loss 10.539388 (+nanz)| norm 43.5996 (+nanz)| lr 1.71e-06 | 2747.19 ms | 47.4% bf16 MFU | 381690 tok/s
step    3/32000 | loss 9.894109 (+nanz)| norm 23.2229 (+nanz)| lr 2.57e-06 | 2753.25 ms | 47.3% bf16 MFU | 381259 tok/s
step    4/32000 | loss 9.566241 (+nanz)| norm 28.4920 (+nanz)| lr 3.43e-06 | 2741.47 ms | 47.5% bf16 MFU | 381690 tok/s
step    5/32000 | loss 9.482848 (+nanz)| norm 23.7817 (+nanz)| lr 4.29e-06 | 2752.07 ms | 47.3% bf16 MFU | 381507 tok/s
step    6/32000 | loss 9.332832 (+nanz)| norm 15.9113 (+nanz)| lr 5.14e-06 | 2751.01 ms | 47.3% bf16 MFU | 381431 tok/s
step    7/32000 | loss 9.165650 (+nanz)| norm 10.5941 (+nanz)| lr 6.00e-06 | 2753.03 ms | 47.3% bf16 MFU | 381327 tok/s
step    8/32000 | loss 9.132234 (+nanz)| norm 16.2733 (+nanz)| lr 6.86e-06 | 2748.91 ms | 47.3% bf16 MFU | 381348 tok/s
step    9/32000 | loss 9.097384 (+nanz)| norm 12.1342 (+nanz)| lr 7.71e-06 | 2748.73 ms | 47.3% bf16 MFU | 381367 tok/s
step   10/32000 | loss 9.072879 (+nanz)| norm 10.5923 (+nanz)| lr 8.57e-06 | 2749.40 ms | 47.3% bf16 MFU | 381369 tok/s
...
</code></pre></div>
<p dir="auto">We can see that each step is about 2.75 seconds and there are 32,000 of them, so now we wait ~24 hours. At every step, this training run takes a chunk of ~1 million tokens of <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu" rel="nofollow">FineWeb-EDU</a> (these are educational web pages from the internet), and updates the 1558 million weights of the model to be slightly better at predicting the next token in a sequence. By the end we&#39;ll have processed 32,000 * 1048576 = 33.6B tokens in total. The loss goes down as we do a better job predicting the next token. The norm will stabilize around 0.1-1, the learning rate is being warmed up over the first few steps. Our model flops utilization (MFU) is around 50%, i.e. quite efficient.</p>
<p dir="auto">Now wait 24 hours for this to finish, then you can visualize the <code>main.log</code> log file using the <a href="https://github.com/karpathy/llm.c/blob/master/dev/vislog.ipynb">dev/vislog.ipynb</a> jupyter notebook. For this you will need to also have Python and matplotlib installed, and you will see the following:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/241138/345149886-0ddc8c19-aa6a-4342-9292-81f40e49d5ad.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MjkyMTMsIm5iZiI6MTcyMDcyODkxMywicGF0aCI6Ii8yNDExMzgvMzQ1MTQ5ODg2LTBkZGM4YzE5LWFhNmEtNDM0Mi05MjkyLTgxZjQwZTQ5ZDVhZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDE1MTNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lZmQ0ODMxM2VlOTJhNGI1N2ZhZjkzN2Q0YjllOTVmMjlkYmRjMmJmMWMxNzdlZTI5Zjc2ZjNkMzkxMzA3OWE3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.oYsiT2HNEnyr7EHtVSgvzu7W6CllsWEh7ctv3QljPAo"><img width="849" alt="image" src="https://private-user-images.githubusercontent.com/241138/345149886-0ddc8c19-aa6a-4342-9292-81f40e49d5ad.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MjkyMTMsIm5iZiI6MTcyMDcyODkxMywicGF0aCI6Ii8yNDExMzgvMzQ1MTQ5ODg2LTBkZGM4YzE5LWFhNmEtNDM0Mi05MjkyLTgxZjQwZTQ5ZDVhZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDE1MTNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lZmQ0ODMxM2VlOTJhNGI1N2ZhZjkzN2Q0YjllOTVmMjlkYmRjMmJmMWMxNzdlZTI5Zjc2ZjNkMzkxMzA3OWE3JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.oYsiT2HNEnyr7EHtVSgvzu7W6CllsWEh7ctv3QljPAo"/></a>
<p dir="auto"><strong>Evals</strong>. On the left we are tracking the loss on FineWeb-EDU validation data. If you simply run the GPT-2 released by OpenAI and evaluate its loss on this split, you get the red horizontal line (loss 2.83). You see that our run outperforms this very very quickly, by step ~5,000. However, this is not a fair comparison because GPT-2 was trained on the never-released WebText dataset, so there is a possibly large distribution shift. So e.g. if you finetune the OpenAI model for 1,000 steps at LR 1e-4, the loss quickly plunges to the blue line (loss 2.61), because it&#39;s quickly adapting to the new data statistics. I like to look at the validation loss as a sanity check, but for the actual comparison we&#39;d want to look at fixed, 3rd party evaluations. One of the well-behaved, smooth, common, often-cited evals that also offer early signal is the <a href="https://rowanzellers.com/hellaswag/" rel="nofollow">HellaSwag</a> eval. These are simple common sense scenarios and the model has to pick the correct continuation. We evaluate HellaSwag on the right pane, where we see that we cross over the GPT-2 model around step ~25K (earlier than GPT-2, which is estimated to have been trained on ~100B tokens. This possibly has to do with increased data quality, as we also observed in our earlier <a href="https://github.com/karpathy/llm.c/discussions/481" data-hovercard-type="discussion" data-hovercard-url="/karpathy/llm.c/discussions/481/hovercard">124M run</a>). The green line is the GPT-3 model of the same size, which is pretty much the same model architecture as GPT-2 with minor differences (context length 1024 -&gt; 2048) but trained for 300B tokens (i.e. ~10X more tokens than what we trained on here). I should say that even HellaSwag is not an ideal single point of comparison because it tests simple English and common sense, it does not test e.g. multilingual, math or code. It could have been that the WebText data mixture was a lot heavier on these, and these domains were &#34;stealing&#34; model capacity to some extent, we don&#39;t know because it was never released. Lastly, in general, good evals are harder at low model capability like GPT-2 because e.g. the models don&#39;t understand multiple choice, and their samples are not high enough quality to make above chance dent into standard math or code evals.</p>
<p dir="auto"><strong>Args guide</strong>. Let&#39;s look at the args we passed into the training now in more detail. The GPT-2 release from OpenAI included model weights but very few details, while GPT-3 release had no weights but many details. So in many cases, we follow the GPT-3 paper hyperparameters because the GPT-2 paper has very very little information:</p>
<ul dir="auto">
<li><code>-i -j</code> are training and validation splits token files, downloaded earlier with <code>edu_fineweb.sh</code></li>
<li><code>-o</code> is the output directory to write logs and checkpoints into</li>
<li><code>-v 250</code> asks to evaluate and log the validation loss every 250 steps</li>
<li><code>-s 300000</code> asks to sample some tokens every 300000 steps. Because the total number of steps will be less than this, this is hacky way to turn sampling off and we will only sample a single time at the very end.</li>
<li><code>-g 384</code> sets the number of tokens to be sampled at the end to be 384</li>
<li><code>-h 1</code> asks to evaluate the HellaSwag accuracy</li>
<li><code>-b 16</code> sets the micro-batch size to 16 . If you are running out of memory, decrease this value, e.g. try 8, 4, 2, all the way down to 1 potentially.</li>
<li><code>-t 1024</code> sets the maximum sequence length to 1024, as GPT-2 did</li>
<li><code>-d 1048576</code> asks that the total batch size be 2 to the power 20, following the GPT-3 paper hyperparameters table. The code will make sure to meet this desired total batch size and calculate the needed gradient accumulation &#34;inner loop&#34; steps of the optimization. For example up above, we saw that we have 8 GPUs each doing 16 X 1024 tokens, so that is 8 X 16 X 1024 = 131,072 tokens per micro-step (a single forward backward), so the code calculated gradient accumulation steps of 8 to meet the desired 1M batch size per step. i.e. it does forward+backward 8 times and then a single update.</li>
<li><code>-r 0</code> sets recompute to zero. Recompute is a way to trade off compute and memory. If <code>-r 1</code>, then we recompute a piece of the forward pass (the GeLU) during backward. This means we don&#39;t have to cache it and save memory, at the cost of some  more compute. So if you&#39;re running out of memory, try -r 1, or -r 2 (also recompute layernorms).</li>
<li><code>-z 1</code> turns on ZeRO-1 (i.e. optimizer state sharding) across multiple GPUs. If you&#39;re training with &gt; 1 GPU, this setting is a no-brainer and should basically always be on. On 1 GPU this setting is a no-op.</li>
<li><code>-c 0.1</code> sets the weight decay to 0.1. Only (2D) weights are decayed exactly as in GPT-2, and this number comes from the GPT-3 paper</li>
<li><code>-k &#34;cosine&#34;</code> sets the cosine learning rate schedule, which is the default so this is a bit spurious.</li>
<li><code>-l 0.0006</code> sets the maximum learning rate to 6e-4. The GPT-3 paper says to use 2e-4 for this model size, but here we triple and it and seems to train faster and without any issues. This wasn&#39;t tuned very carefully yet.</li>
<li><code>-q 0.1</code> says that we will decay the learning rate to 10% of max LR over the course of training, following GPT-3 paper.</li>
<li><code>-u 700</code> says that we will ramp up the learning rate from 0 to max learning rate over the first 700 iterations, which at total batch size 0.5M is 350M tokens, following GPT-3 paper.</li>
<li><code>-n 2000</code> asks to save model checkpoints every 2000 steps.</li>
<li><code>-x 32000</code> asks for 32K steps in total. I chose this number because it is a nice number, and just fits into 24 hours.</li>
<li><code>-ge 1</code> sets a very recently merged gelu recompute setting for CublasLt (optional)</li>
<li><code>-y 1</code> sets the &#34;resume&#34; flag on. If your training for any reason crashes or hangs, you can CTRL+C and re-run this command, and it will attempt to resume the optimization. llm.c is bitwise-deterministic, so you&#39;ll get the identical result as if you didn&#39;t crash.</li>
<li><code>-e &#34;d48&#34;</code> asks to initialize, a depth 48 GPT-2 model from scratch.</li>
</ul>
<p dir="auto"><strong>Memory guide.</strong> The biggest constraint most people will probably face is that their GPU doesn&#39;t have 80GB. That&#39;s okay you should still be able to run everything above if you are patient, it would just run slower. So if the model doesn&#39;t fit, what do you play with? The most important one is the micro batch size <code>-b</code>. Try to decrease it but keep it to nice numbers. So e.g. 16 -&gt; 8 -&gt; 4 -&gt; 2 -&gt; 1. From there, try to also play with the recompute setting <code>-r</code> which is 0 (fastest, a lot of memory), 1 (very slightly slower, but a huge memory saving), or 2 (slightly slower, smaller memory saving). The next thing you can do is disable master weights in fp32, which you can do with <code>-w 0</code> (1 is default). We won&#39;t maintain fp32 copy of params. Empirically in a few runs before this seems to be okay, likely due to our use of stochastic rounding. If even that doesn&#39;t fit (that&#39;s unlikely right?), you could try to decrease the maximum sequence length with <code>-t</code>, default is 1024 you can take it down to 512, 256, etc., but now you are making your model worse because you&#39;re decreasing its maximum attention span.</p>
<p dir="auto"><strong>Code.</strong> Certainly I feel biased but llm.c is quite beautiful:</p>
<ul dir="auto">
<li>It only requires basic CUDA dependencies to run.</li>
<li>It is a direct, minimal and readable implementation in C/CUDA. llm.c totals about 5,000 lines of C/CUDA code. We try to be mostly C, not C++ to keep it simple. Neural net training is just one while loop of the same, simple arithmetic operations (think +, -, *, /) on a single float array, it really shouldn&#39;t be that complicated.</li>
<li>It compiles and runs very quickly (few seconds), so you&#39;re doing more stepping and less waiting.</li>
<li>It allocates all of its GPU memory a single time at the start and from then on during training has an exactly constant memory footprint. So once you start stepping, you know you&#39;re good for the rest of the run and won&#39;t OOM.</li>
<li>It is bitwise deterministic.</li>
<li>It is efficient, at just below ~50% MFU.</li>
</ul>
<p dir="auto">The main entry point and the majority of the code is in the file <a href="https://github.com/karpathy/llm.c/blob/master/train_gpt2.cu">train_gpt2.cu</a>. It contains the GPT-2 model definition and the training loop in ~2,000 LOC, and it imports a bunch of helper files with various utilities and the individual layer implementations from the <code>llmc</code> directory. <code>cloc llmc</code> reports 23 files with 3170 LOC, and <code>cloc train_gpt2.cu</code> is 1353 LOC atm.</p>
<p dir="auto"><strong>Multi-node training</strong>. If you are part of the privileged GPU-rich upper class, llm.c supports multi-node training and the most GPUs I&#39;ve seen someone train llm.c with is ~500 GPUs. This biggest run I&#39;ve done personally so far is on Lambda&#39;s new 1-click cluster feature with 16XH100 GPUs in 2 nodes. The downsides of unemployment. The lambda team has put up <a href="https://github.com/LambdaLabsML/llm.c-1cc/tree/main">detailed instructions</a> on how you can train llm.c models on their 1-click clusters. E.g. with the 512-GPU H100 cluster for $2,300/hr, you might be able to train your GPT-2 in ~30 minutes. You&#39;d have to increase the total batch size (e.g. to ~8M) and possibly tune the hyperparameters a little. I haven&#39;t tried but it probably works and would be very cool :)</p>
<p dir="auto"><strong>PyTorch comparison</strong>. A relatively comparable run in PyTorch would I think look something like this, using our parallel PyTorch implementation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin &#34;dev/data/edu_fineweb100B/edu_fineweb_train_*.bin&#34; \
    --input_val_bin &#34;dev/data/edu_fineweb100B/edu_fineweb_val_*.bin&#34; \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1"><pre>torchrun --standalone --nproc_per_node=8 train_gpt2.py \
    --input_bin <span><span>&#34;</span>dev/data/edu_fineweb100B/edu_fineweb_train_*.bin<span>&#34;</span></span> \
    --input_val_bin <span><span>&#34;</span>dev/data/edu_fineweb100B/edu_fineweb_val_*.bin<span>&#34;</span></span> \
    --write_tensors 0 \
    --model d48 \
    --batch_size 8 --sequence_length 1024 --total_batch_size 1048576 \
    --dtype bfloat16 \
    --compile 1 \
    --tensorcores 1 \
    --flash 1 \
    --num_iterations 32000 \
    --warmup_iters 700 \
    --weight_decay 0.1 \
    --overfit_single_batch 0 \
    --learning_rate 0.0006 \
    --zero_stage 1</pre></div>
<p dir="auto">The PyTorch code is meant as a testing reference not an actual implementation, so the training loop is a little bit different in some places (e.g. the dataloader doesn&#39;t permute the shards, etc.), but this is still possibly useful as a point of reference. I also hacked the default vocab size to be 50257 -&gt; 50304 to get added efficiency, then the currently PyTorch nightly gives:</p>
<div data-snippet-clipboard-copy-content="step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
..."><pre><code>step   16/32000 | train loss 8.903997 | norm 8.3474 | lr 1.37e-05 | (3381.88 ms | 310057 tok/s)
step   17/32000 | train loss 8.870140 | norm 3.7936 | lr 1.46e-05 | (3381.95 ms | 310051 tok/s)
step   18/32000 | train loss 8.875732 | norm 9.4993 | lr 1.54e-05 | (3393.09 ms | 309033 tok/s)
step   19/32000 | train loss 8.817432 | norm 2.8345 | lr 1.63e-05 | (3379.75 ms | 310253 tok/s)
step   20/32000 | train loss 8.798056 | norm 4.1234 | lr 1.71e-05 | (3386.53 ms | 309631 tok/s)
step   21/32000 | train loss 8.777574 | norm 2.8010 | lr 1.80e-05 | (3386.05 ms | 309675 tok/s)
...
</code></pre></div>
<p dir="auto">Now I wouldn&#39;t say I have full confidence that the PyTorch script is maximally tuned, but the following observations can be made. PyTorch seems to be taking a lot more memory (this run is ~80GB), while llm.c is at 57GB (29% improvement). Memory is important because it allows you to crank up the batch size (e.g. llm.c can go up to 24 microbatch here), which goes a bit faster. Second, we&#39;re seeing about 3386 vs. 2750ms per iteration, so llm.c is stepping ~19% faster. Some of the gains here have known origin, e.g. llm.c includes optimizations like the Fused classifier that kicks off the backward pass, which is something torch.compile does not do today afaik. But it&#39;s also possible that this script isn&#39;t fully maximally tuned, but in any case I&#39;m showing the comparison in case 1) others would like to take a look, play with, compare, help tune and 2) to just say that llm.c is quite optimized and fast - in the specific case of GPT-2/3 training.</p>
<p dir="auto"><strong>The final model</strong>. A few links that may be helpful, for posterity:</p>
<ul dir="auto">
<li>The <a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/main.log" rel="nofollow">main.log</a> file.</li>
<li>The <a href="http://llmc.s3-us-west-2.amazonaws.com/gpt2_1558M/model_00032000.bin" rel="nofollow">model_00032000.bin</a> llm.c bin model file</li>
<li>The model converted to huggingface transformers GPT-2 model I uploaded here: <a href="https://huggingface.co/karpathy/gpt2_1558M_final2_hf" rel="nofollow">karpathy/gpt2_1558M_final2_hf</a>.</li>
</ul>
<p dir="auto"><strong>Model export</strong>. The model export can be done as follows, for example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export"><pre>python dev/eval/export_hf.py --input log_gpt2_128M/model_00032000.bin --output gpt2_1558M_export</pre></div>
<p dir="auto">This then lets you run the Eleuther eval harness, or run the huggingface sampling pipeline to get model samples:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# take model for spin
import torch

output = &#34;./gpt2_1558M_final2_hf&#34;

# set pytorch seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

prompt = &#34;In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.&#34;
from transformers import AutoModelForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(output)
model = AutoModelForCausalLM.from_pretrained(output, attn_implementation=&#34;flash_attention_2&#34;, torch_dtype=torch.bfloat16, device_map=&#39;cuda&#39;)
model.eval()
tokens = tokenizer.encode(prompt, return_tensors=&#34;pt&#34;)
tokens = tokens.to(&#39;cuda&#39;)

output = model.generate(tokens, max_new_tokens=500, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=50, num_return_sequences=4)
samples = tokenizer.batch_decode(output)
for sample in samples:
    print(&#39;-&#39;*30)
    print(sample)"><pre><span># take model for spin</span>
<span>import</span> <span>torch</span>

<span>output</span> <span>=</span> <span>&#34;./gpt2_1558M_final2_hf&#34;</span>

<span># set pytorch seeds</span>
<span>torch</span>.<span>manual_seed</span>(<span>42</span>)
<span>torch</span>.<span>cuda</span>.<span>manual_seed</span>(<span>42</span>)

<span>prompt</span> <span>=</span> <span>&#34;In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.&#34;</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>output</span>)
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>output</span>, <span>attn_implementation</span><span>=</span><span>&#34;flash_attention_2&#34;</span>, <span>torch_dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>, <span>device_map</span><span>=</span><span>&#39;cuda&#39;</span>)
<span>model</span>.<span>eval</span>()
<span>tokens</span> <span>=</span> <span>tokenizer</span>.<span>encode</span>(<span>prompt</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>)
<span>tokens</span> <span>=</span> <span>tokens</span>.<span>to</span>(<span>&#39;cuda&#39;</span>)

<span>output</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>tokens</span>, <span>max_new_tokens</span><span>=</span><span>500</span>, <span>pad_token_id</span><span>=</span><span>tokenizer</span>.<span>eos_token_id</span>, <span>do_sample</span><span>=</span><span>True</span>, <span>top_k</span><span>=</span><span>50</span>, <span>num_return_sequences</span><span>=</span><span>4</span>)
<span>samples</span> <span>=</span> <span>tokenizer</span>.<span>batch_decode</span>(<span>output</span>)
<span>for</span> <span>sample</span> <span>in</span> <span>samples</span>:
    <span>print</span>(<span>&#39;-&#39;</span><span>*</span><span>30</span>)
    <span>print</span>(<span>sample</span>)</pre></div>
<p dir="auto">Also have a look at <a href="https://github.com/karpathy/llm.c/tree/master/dev/eval">dev/eval</a> for instructions on how to run the Eleuther Evaluation Harness, the evals from the HuggingFace Open LLM Leaderboard, etc.</p>
<p dir="auto"><strong>400B token run</strong>. I have also made the attempt to train GPT-2 for significantly longer than 33B tokens. In particular, I changed -x to 400,000 to train for 420B tokens (even more than GPT-3 model of this size, which was trained with 300B). This model run looked great until about step 330,000:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/241138/347626140-8708850a-c29e-427e-8e14-fb6ba7d7776a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MjkyMTMsIm5iZiI6MTcyMDcyODkxMywicGF0aCI6Ii8yNDExMzgvMzQ3NjI2MTQwLTg3MDg4NTBhLWMyOWUtNDI3ZS04ZTE0LWZiNmJhN2Q3Nzc2YS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDE1MTNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03YjhjYjc0Njk2MWQ5M2E0NDM5OTBhMmRjMDIwNjU1YTRhMWU2MmQ0ZDQzMDBkYTJkMGMzYmNmZWYxMjYxN2ZlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.UBvGIyz_t8WD6-jZc7mexL-euSQzjkjAySi9haGpG4g"><img width="1293" alt="image" src="https://private-user-images.githubusercontent.com/241138/347626140-8708850a-c29e-427e-8e14-fb6ba7d7776a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjA3MjkyMTMsIm5iZiI6MTcyMDcyODkxMywicGF0aCI6Ii8yNDExMzgvMzQ3NjI2MTQwLTg3MDg4NTBhLWMyOWUtNDI3ZS04ZTE0LWZiNmJhN2Q3Nzc2YS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwNzExJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDcxMVQyMDE1MTNaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03YjhjYjc0Njk2MWQ5M2E0NDM5OTBhMmRjMDIwNjU1YTRhMWU2MmQ0ZDQzMDBkYTJkMGMzYmNmZWYxMjYxN2ZlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.UBvGIyz_t8WD6-jZc7mexL-euSQzjkjAySi9haGpG4g"/></a>
<p dir="auto">This model dramatically beats GPT-2 and GPT-3 of its size on HellaSwag (it gets up to ~61%), but sadly becomes unstable there on and explodes. There are more smaller spikes along the way but the code is configured to detect the more simple instantaneous instability and skips update (I used the flags <code>-sl 5.0 -sg 5.0</code>), which helps mitigate and defers issues. However, I think we&#39;re not yet being sufficiently careful with our initialization, activation ranges, and overall model training stability and there are deeper issues that gradually drift the model into instability, especially for larger models and over long training duration. To be continued. If you have ideas or recommendations for stabilizing LLM model training please contribute your experience in the discussion below.</p>
<p dir="auto"><strong>FAQ</strong>:</p>
<ul dir="auto">
<li>Can I <strong>sample</strong> from the model in llm.c? kind of, but it&#39;s inefficient and a bit weird, and even more hacky if you&#39;d like to prompt the model. Use the huggingface paths above for now.</li>
<li>Can I <strong>chat</strong> with it? no, this is currently only pretraining, not chat finetuning.</li>
<li>Can you train in <strong>fp8</strong>? No, we&#39;re currently mostly training in bf16, but early versions are very much work in progress.</li>
<li>I have a non-NVIDIA GPU can I run llm.c? No, llm.c supports C/CUDA only, but good forks exist (see main README). For example there is an actively maintained <a href="https://github.com/anthonix/llm.c">AMD fork</a> by <a data-hovercard-type="user" data-hovercard-url="/users/anthonix/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/anthonix">@anthonix</a> that is quite good.</li>
</ul>
<p dir="auto"><strong>GPT-2 (124M)</strong>. I wanted to also link to an earlier post on training the <a href="https://github.com/karpathy/llm.c/discussions/481" data-hovercard-type="discussion" data-hovercard-url="/karpathy/llm.c/discussions/481/hovercard">GPT-2 (124M) model</a> in llm.c, which has some more related information to llm.c runs. 124M is a smaller model in the GPT-2 miniseries, only 124M parameters compared to 1558M parameters.</p>
<p dir="auto"><strong>Authors</strong></p>
<p dir="auto">Substantial contributions to llm.c came from what now feels like the llm.c core dev team, in addition to self:</p>
<ul dir="auto">
<li><a data-hovercard-type="user" data-hovercard-url="/users/ngc92/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ngc92">@ngc92</a> in all aspects of the code base</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/ademeure/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ademeure">@ademeure</a> in CUDA kernel optimization, low precision training, cudnn, cublas, ...</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/gordicaleksa/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/gordicaleksa">@gordicaleksa</a> in all aspects of whatever is next on the TODO list, from algorithms to code to multi-node or etc.</li>
<li><a data-hovercard-type="user" data-hovercard-url="/users/rosslwheeler/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/rosslwheeler">@rosslwheeler</a> in CI and Windows support. If you&#39;re happily running llm.c on Windows you should definitely thank Ross :)</li>
<li><a href="https://lambdalabs.com" rel="nofollow">Lambda labs</a> for sponsoring the GPUs used in the development of llm.c. The history here is that I&#39;ve happily used Lambda for several years and then a few months ago I pretty please asked if they are open to not charging my account for llm.c dev work and they agreed so here we are thank you for supporting llm.c!</li>
</ul>
<p dir="auto"><strong>Coming up</strong>. Some of the next big steps we are interested in and looking at these days:</p>
<ol dir="auto">
<li>Further optimize GPT-2 training hyperparameters. For some reason, the hyperparameters cited by OpenAI in the GPT-3 paper appear to be quite suboptimal, e.g. @Yuchenj_UW on X found that you can 3X the learning rate and get faster training with no apparent downsides. There might be other similar low-hanging fruit.</li>
<li>Improve training and scaling stability, e.g. more stable optimizers, schedulers, clipping, norming, muP. (Some of these PRs already exist, if you have tips on stabilizing LLM runs please reach out with ideas to try!).</li>
<li>Mixed precision++: training with fp8 (imminent!).</li>
<li>Model inference, e.g. KV cache is the low hanging fruit here.</li>
<li>Finetuning: SFT, RLHF</li>
<li>Multimodal extensions, VQVAE and friends</li>
<li>More modern architectures, support for Llama / Gemma model series.</li>
</ol>
<p dir="auto">The goal of llm.c remains to have a simple, minimal, clean training stack for a full-featured LLM agent, in direct C/CUDA, and companion educational materials to bring many people up to speed in this awesome field.</p>
<p dir="auto">Please feel free to use the Discussions for any FAQ and related, or if you&#39;d like something faster, #llmc on <a href="https://discord.gg/3zy8kqD9Cp" rel="nofollow">Discord</a>, or #llmdotc on CUDA MODE Discord.</p>
<p dir="auto">We&#39;ll see you next time!</p>
    </td>
  </tr>

    </tbody>
  </div></div>
  </body>
</html>
