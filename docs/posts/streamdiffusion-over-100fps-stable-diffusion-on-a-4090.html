<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/cumulo-autumn/StreamDiffusion">Original</a>
    <h1>StreamDiffusion: Over 100fps Stable Diffusion on a 4090</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/README.md">English</a> | <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/README-ja.md">日本語</a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_07.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_07.gif" width="90%" data-animated-image=""/></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_09.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_09.gif" width="90%" data-animated-image=""/></a>
</p>

<p dir="auto"><strong>Authors:</strong> <a href="https://www.linkedin.com/in/akio-kodaira-1a7b98252/" rel="nofollow">Akio Kodaira</a>, <a href="https://www.chenfengx.com/" rel="nofollow">Chenfeng Xu</a>, Toshiki Hazama, <a href="https://twitter.com/__ramu0e__" rel="nofollow">Takanori Yoshimoto</a>, <a href="https://www.linkedin.com/in/kohei--ohno/" rel="nofollow">Kohei Ohno</a>, <a href="https://me.ddpn.world/" rel="nofollow">Shogo Mitsuhori</a>, <a href="https://twitter.com/toni_nimono" rel="nofollow">Soichi Sugano</a>, <a href="https://twitter.com/hanyingcl" rel="nofollow">Hanying Cho</a>, <a href="https://zhijianliu.com/" rel="nofollow">Zhijian Liu</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=ID9QePIAAAAJ" rel="nofollow">Kurt Keutzer</a></p>
<p dir="auto">StreamDiffusion is an innovative diffusion pipeline designed for real-time interactive generation. It introduces significant performance enhancements to current diffusion-based image generation techniques.</p>
<p dir="auto"><a href="https://arxiv.org/abs/2312.12491" rel="nofollow"><img src="https://camo.githubusercontent.com/47151ae40041cc004f656a042aafc1b5877958bbcc298d169fca2d778994138a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323330372e30343732352d6233316231622e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2307.04725-b31b1b.svg"/></a>
<a href="https://huggingface.co/papers/2312.12491" rel="nofollow"><img src="https://camo.githubusercontent.com/dbe4a949263f6758ffe4b0757f52aa29772753ceebcca38af41aef80b720f57f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d7061706572732d79656c6c6f77" alt="Hugging Face Papers" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-papers-yellow"/></a></p>
<p dir="auto">We sincerely thank <a href="https://twitter.com/AttaQjp" rel="nofollow">Taku Fujimoto</a> and <a href="https://twitter.com/radamar" rel="nofollow">Radamés Ajna</a> and Hugging Face team for their invaluable feedback, courteous support, and insightful discussions.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-key-features" aria-hidden="true" tabindex="-1" href="#key-features"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Key Features</h2>
<ol dir="auto">
<li>
<p dir="auto"><strong>Stream Batch</strong></p>
<ul dir="auto">
<li>Streamlined data processing through efficient batch operations.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Residual Classifier-Free Guidance</strong> - <a href="#residual-cfg-rcfg">Learn More</a></p>
<ul dir="auto">
<li>Improved guidance mechanism that minimizes computational redundancy.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Stochastic Similarity Filter</strong> - <a href="#stochastic-similarity-filter">Learn More</a></p>
<ul dir="auto">
<li>Improves GPU utilization efficiency through advanced filtering techniques.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>IO Queues</strong></p>
<ul dir="auto">
<li>Efficiently manages input and output operations for smoother execution.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Pre-Computation for KV-Caches</strong></p>
<ul dir="auto">
<li>Optimizes caching strategies for accelerated processing.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Model Acceleration Tools</strong></p>
<ul dir="auto">
<li>Utilizes various tools for model optimization and performance boost.</li>
</ul>
</li>
</ol>
<p dir="auto">When images are produced using our proposed StreamDiffusion pipeline in an environment with <strong>GPU: RTX 4090</strong>, <strong>CPU: Core i9-13900K</strong>, and <strong>OS: Ubuntu 22.04.3 LTS</strong>.</p>
<table>
<thead>
<tr>
<th>model</th>
<th>Denoising Step</th>
<th>fps on Txt2Img</th>
<th>fps on Img2Img</th>
</tr>
</thead>
<tbody>
<tr>
<td>SD-turbo</td>
<td>1</td>
<td>106.16</td>
<td>93.897</td>
</tr>
<tr>
<td>LCM-LoRA </td>
<td>4</td>
<td>38.023</td>
<td>37.133</td>
</tr>
</tbody>
</table>
<p dir="auto">Feel free to explore each feature by following the provided links to learn more about StreamDiffusion&#39;s capabilities. If you find it helpful, please consider citing our work:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{kodaira2023streamdiffusion,
      title={StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation},
      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},
      year={2023},
      eprint={2312.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre>@article{kodaira2023streamdiffusion,
      title={StreamDiffusion: A Pipeline-level Solution <span>for</span> Real-time Interactive Generation},
      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},
      year={2023},
      eprint={2312.12491},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-installation" aria-hidden="true" tabindex="-1" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-step0-clone-this-repository" aria-hidden="true" tabindex="-1" href="#step0-clone-this-repository"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step0: clone this repository</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/cumulo-autumn/StreamDiffusion.git"><pre>git clone https://github.com/cumulo-autumn/StreamDiffusion.git</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-step1-make-environment" aria-hidden="true" tabindex="-1" href="#step1-make-environment"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step1: Make Environment</h3>
<p dir="auto">You can install StreamDiffusion via pip, conda, or Docker(explanation below).</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n streamdiffusion python=3.10
conda activate streamdiffusion"><pre>conda create -n streamdiffusion python=3.10
conda activate streamdiffusion</pre></div>
<p dir="auto">OR</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m venv .venv
# Windows
.\.venv\Scripts\activate
# Linux
source .venv/bin/activate"><pre>python -m venv .venv
# Windows
.\.venv\Scripts\activate
# Linux
source .venv/bin/activate</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-step2-install-pytorch" aria-hidden="true" tabindex="-1" href="#step2-install-pytorch"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step2: Install PyTorch</h3>
<p dir="auto">Select the appropriate version for your system.</p>
<p dir="auto">CUDA 11.8</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118"><pre>pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118</pre></div>
<p dir="auto">CUDA 12.1</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121"><pre>pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121</pre></div>
<p dir="auto">details: <a href="https://pytorch.org/" rel="nofollow">https://pytorch.org/</a></p>
<h3 tabindex="-1" dir="auto"><a id="user-content-step3-install-streamdiffusion" aria-hidden="true" tabindex="-1" href="#step3-install-streamdiffusion"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Step3: Install StreamDiffusion</h3>
<h4 tabindex="-1" dir="auto"><a id="user-content-for-user" aria-hidden="true" tabindex="-1" href="#for-user"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>For User</h4>
<p dir="auto">Install StreamDiffusion</p>
<div dir="auto" data-snippet-clipboard-copy-content="#for Latest Version (recommended)
pip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]


#or


#for Stable Version
pip install streamdiffusion[tensorrt]"><pre><span><span>#</span>for Latest Version (recommended)</span>
pip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]


<span><span>#</span>or</span>


<span><span>#</span>for Stable Version</span>
pip install streamdiffusion[tensorrt]</pre></div>
<p dir="auto">Install TensorRT extension and pywin32
(※※pywin32 is required only for Windows.)</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m streamdiffusion.tools.install-tensorrt
# If you use Windows, you need to install pywin32 
pip install pywin32"><pre>python -m streamdiffusion.tools.install-tensorrt
<span><span>#</span> If you use Windows, you need to install pywin32 </span>
pip install pywin32</pre></div>
<h4 tabindex="-1" dir="auto"><a id="user-content-for-developer" aria-hidden="true" tabindex="-1" href="#for-developer"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>For Developer</h4>
<div dir="auto" data-snippet-clipboard-copy-content="python setup.py develop easy_install streamdiffusion[tensorrt]
python -m streamdiffusion.tools.install-tensorrt"><pre>python setup.py develop easy_install streamdiffusion[tensorrt]
python -m streamdiffusion.tools.install-tensorrt</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-docker-installation-tensorrt-ready" aria-hidden="true" tabindex="-1" href="#docker-installation-tensorrt-ready"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Docker Installation (TensorRT Ready)</h3>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/cumulo-autumn/StreamDiffusion.git
cd StreamDiffusion
docker build -t stream-diffusion:latest -f Dockerfile .
docker run --gpus all -it -v $(pwd):/home/ubuntu/streamdiffusion stream-diffusion:latest"><pre>git clone https://github.com/cumulo-autumn/StreamDiffusion.git
<span>cd</span> StreamDiffusion
docker build -t stream-diffusion:latest -f Dockerfile <span>.</span>
docker run --gpus all -it -v <span><span>$(</span>pwd<span>)</span></span>:/home/ubuntu/streamdiffusion stream-diffusion:latest</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-quick-start" aria-hidden="true" tabindex="-1" href="#quick-start"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Quick Start</h2>
<p dir="auto">You can try StreamDiffusion in <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/examples"><code>examples</code></a> directory.</p>
<table>
<thead>
<tr>
<th><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_02.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_02.gif" alt="画像3" data-animated-image=""/></a></th>
<th><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_03.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_03.gif" alt="画像4" data-animated-image=""/></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_04.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_04.gif" alt="画像5" data-animated-image=""/></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_05.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_05.gif" alt="画像6" data-animated-image=""/></a></td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto"><a id="user-content-real-time-txt2img-demo" aria-hidden="true" tabindex="-1" href="#real-time-txt2img-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Real-Time Txt2Img Demo</h2>
<p dir="auto">There is an interactive txt2img demo in <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/demo/realtime-txt2img"><code>demo/realtime-txt2img</code></a> directory!</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_01.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_01.gif" width="100%" data-animated-image=""/></a>
</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-usage-example" aria-hidden="true" tabindex="-1" href="#usage-example"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Usage Example</h2>
<p dir="auto">We provide a simple example of how to use StreamDiffusion. For more detailed examples, please refer to <a href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/examples"><code>examples</code></a> directory.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-image-to-image" aria-hidden="true" tabindex="-1" href="#image-to-image"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Image-to-Image</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from diffusers import AutoencoderTiny, StableDiffusionPipeline
from diffusers.utils import load_image

from streamdiffusion import StreamDiffusion
from streamdiffusion.image_utils import postprocess_image

# You can load any models using diffuser&#39;s StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(&#34;KBlueLeaf/kohaku-v2.1&#34;).to(
    device=torch.device(&#34;cuda&#34;),
    dtype=torch.float16,
)

# Wrap the pipeline in StreamDiffusion
stream = StreamDiffusion(
    pipe,
    t_index_list=[32, 45],
    torch_dtype=torch.float16,
)

# If the loaded model is not LCM, merge LCM
stream.load_lcm_lora()
stream.fuse_lora()
# Use Tiny VAE for further acceleration
stream.vae = AutoencoderTiny.from_pretrained(&#34;madebyollin/taesd&#34;).to(device=pipe.device, dtype=pipe.dtype)
# Enable acceleration
pipe.enable_xformers_memory_efficient_attention()


prompt = &#34;1girl with dog hair, thick frame glasses&#34;
# Prepare the stream
stream.prepare(prompt)

# Prepare image
init_image = load_image(&#34;assets/img2img_example.png&#34;).resize((512, 512))

# Warmup &gt;= len(t_index_list) x frame_buffer_size
for _ in range(2):
    stream(init_image)

# Run the stream infinitely
while True:
    x_output = stream(init_image)
    postprocess_image(x_output, output_type=&#34;pil&#34;)[0].show()
    input_response = input(&#34;Press Enter to continue or type &#39;stop&#39; to exit: &#34;)
    if input_response == &#34;stop&#34;:
        break"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>diffusers</span> <span>import</span> <span>AutoencoderTiny</span>, <span>StableDiffusionPipeline</span>
<span>from</span> <span>diffusers</span>.<span>utils</span> <span>import</span> <span>load_image</span>

<span>from</span> <span>streamdiffusion</span> <span>import</span> <span>StreamDiffusion</span>
<span>from</span> <span>streamdiffusion</span>.<span>image_utils</span> <span>import</span> <span>postprocess_image</span>

<span># You can load any models using diffuser&#39;s StableDiffusionPipeline</span>
<span>pipe</span> <span>=</span> <span>StableDiffusionPipeline</span>.<span>from_pretrained</span>(<span>&#34;KBlueLeaf/kohaku-v2.1&#34;</span>).<span>to</span>(
    <span>device</span><span>=</span><span>torch</span>.<span>device</span>(<span>&#34;cuda&#34;</span>),
    <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># Wrap the pipeline in StreamDiffusion</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    <span>t_index_list</span><span>=</span>[<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># If the loaded model is not LCM, merge LCM</span>
<span>stream</span>.<span>load_lcm_lora</span>()
<span>stream</span>.<span>fuse_lora</span>()
<span># Use Tiny VAE for further acceleration</span>
<span>stream</span>.<span>vae</span> <span>=</span> <span>AutoencoderTiny</span>.<span>from_pretrained</span>(<span>&#34;madebyollin/taesd&#34;</span>).<span>to</span>(<span>device</span><span>=</span><span>pipe</span>.<span>device</span>, <span>dtype</span><span>=</span><span>pipe</span>.<span>dtype</span>)
<span># Enable acceleration</span>
<span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()


<span>prompt</span> <span>=</span> <span>&#34;1girl with dog hair, thick frame glasses&#34;</span>
<span># Prepare the stream</span>
<span>stream</span>.<span>prepare</span>(<span>prompt</span>)

<span># Prepare image</span>
<span>init_image</span> <span>=</span> <span>load_image</span>(<span>&#34;assets/img2img_example.png&#34;</span>).<span>resize</span>((<span>512</span>, <span>512</span>))

<span># Warmup &gt;= len(t_index_list) x frame_buffer_size</span>
<span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>2</span>):
    <span>stream</span>(<span>init_image</span>)

<span># Run the stream infinitely</span>
<span>while</span> <span>True</span>:
    <span>x_output</span> <span>=</span> <span>stream</span>(<span>init_image</span>)
    <span>postprocess_image</span>(<span>x_output</span>, <span>output_type</span><span>=</span><span>&#34;pil&#34;</span>)[<span>0</span>].<span>show</span>()
    <span>input_response</span> <span>=</span> <span>input</span>(<span>&#34;Press Enter to continue or type &#39;stop&#39; to exit: &#34;</span>)
    <span>if</span> <span>input_response</span> <span>==</span> <span>&#34;stop&#34;</span>:
        <span>break</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-text-to-image" aria-hidden="true" tabindex="-1" href="#text-to-image"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-to-Image</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from diffusers import AutoencoderTiny, StableDiffusionPipeline

from streamdiffusion import StreamDiffusion
from streamdiffusion.image_utils import postprocess_image

# You can load any models using diffuser&#39;s StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(&#34;KBlueLeaf/kohaku-v2.1&#34;).to(
    device=torch.device(&#34;cuda&#34;),
    dtype=torch.float16,
)

# Wrap the pipeline in StreamDiffusion
# Requires more long steps (len(t_index_list)) in text2image
# You recommend to use cfg_type=&#34;none&#34; when text2image
stream = StreamDiffusion(
    pipe,
    t_index_list=[0, 16, 32, 45],
    torch_dtype=torch.float16,
    cfg_type=&#34;none&#34;,
)

# If the loaded model is not LCM, merge LCM
stream.load_lcm_lora()
stream.fuse_lora()
# Use Tiny VAE for further acceleration
stream.vae = AutoencoderTiny.from_pretrained(&#34;madebyollin/taesd&#34;).to(device=pipe.device, dtype=pipe.dtype)
# Enable acceleration
pipe.enable_xformers_memory_efficient_attention()


prompt = &#34;1girl with dog hair, thick frame glasses&#34;
# Prepare the stream
stream.prepare(prompt)

# Warmup &gt;= len(t_index_list) x frame_buffer_size
for _ in range(4):
    stream()

# Run the stream infinitely
while True:
    x_output = stream.txt2img()
    postprocess_image(x_output, output_type=&#34;pil&#34;)[0].show()
    input_response = input(&#34;Press Enter to continue or type &#39;stop&#39; to exit: &#34;)
    if input_response == &#34;stop&#34;:
        break"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>diffusers</span> <span>import</span> <span>AutoencoderTiny</span>, <span>StableDiffusionPipeline</span>

<span>from</span> <span>streamdiffusion</span> <span>import</span> <span>StreamDiffusion</span>
<span>from</span> <span>streamdiffusion</span>.<span>image_utils</span> <span>import</span> <span>postprocess_image</span>

<span># You can load any models using diffuser&#39;s StableDiffusionPipeline</span>
<span>pipe</span> <span>=</span> <span>StableDiffusionPipeline</span>.<span>from_pretrained</span>(<span>&#34;KBlueLeaf/kohaku-v2.1&#34;</span>).<span>to</span>(
    <span>device</span><span>=</span><span>torch</span>.<span>device</span>(<span>&#34;cuda&#34;</span>),
    <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)

<span># Wrap the pipeline in StreamDiffusion</span>
<span># Requires more long steps (len(t_index_list)) in text2image</span>
<span># You recommend to use cfg_type=&#34;none&#34; when text2image</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    <span>t_index_list</span><span>=</span>[<span>0</span>, <span>16</span>, <span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
    <span>cfg_type</span><span>=</span><span>&#34;none&#34;</span>,
)

<span># If the loaded model is not LCM, merge LCM</span>
<span>stream</span>.<span>load_lcm_lora</span>()
<span>stream</span>.<span>fuse_lora</span>()
<span># Use Tiny VAE for further acceleration</span>
<span>stream</span>.<span>vae</span> <span>=</span> <span>AutoencoderTiny</span>.<span>from_pretrained</span>(<span>&#34;madebyollin/taesd&#34;</span>).<span>to</span>(<span>device</span><span>=</span><span>pipe</span>.<span>device</span>, <span>dtype</span><span>=</span><span>pipe</span>.<span>dtype</span>)
<span># Enable acceleration</span>
<span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()


<span>prompt</span> <span>=</span> <span>&#34;1girl with dog hair, thick frame glasses&#34;</span>
<span># Prepare the stream</span>
<span>stream</span>.<span>prepare</span>(<span>prompt</span>)

<span># Warmup &gt;= len(t_index_list) x frame_buffer_size</span>
<span>for</span> <span>_</span> <span>in</span> <span>range</span>(<span>4</span>):
    <span>stream</span>()

<span># Run the stream infinitely</span>
<span>while</span> <span>True</span>:
    <span>x_output</span> <span>=</span> <span>stream</span>.<span>txt2img</span>()
    <span>postprocess_image</span>(<span>x_output</span>, <span>output_type</span><span>=</span><span>&#34;pil&#34;</span>)[<span>0</span>].<span>show</span>()
    <span>input_response</span> <span>=</span> <span>input</span>(<span>&#34;Press Enter to continue or type &#39;stop&#39; to exit: &#34;</span>)
    <span>if</span> <span>input_response</span> <span>==</span> <span>&#34;stop&#34;</span>:
        <span>break</span></pre></div>
<p dir="auto">You can make it faster by using SD-Turbo.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-faster-generation" aria-hidden="true" tabindex="-1" href="#faster-generation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Faster generation</h3>
<p dir="auto">Replace the following code in the above example.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipe.enable_xformers_memory_efficient_attention()"><pre><span>pipe</span>.<span>enable_xformers_memory_efficient_attention</span>()</pre></div>
<p dir="auto">To</p>
<div dir="auto" data-snippet-clipboard-copy-content="from streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt

stream = accelerate_with_tensorrt(
    stream, &#34;engines&#34;, max_batch_size=2,
)"><pre><span>from</span> <span>streamdiffusion</span>.<span>acceleration</span>.<span>tensorrt</span> <span>import</span> <span>accelerate_with_tensorrt</span>

<span>stream</span> <span>=</span> <span>accelerate_with_tensorrt</span>(
    <span>stream</span>, <span>&#34;engines&#34;</span>, <span>max_batch_size</span><span>=</span><span>2</span>,
)</pre></div>
<p dir="auto">It requires TensorRT extension and time to build the engine, but it will be faster than the above example.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-optionals" aria-hidden="true" tabindex="-1" href="#optionals"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Optionals</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-stochastic-similarity-filter" aria-hidden="true" tabindex="-1" href="#stochastic-similarity-filter"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Stochastic Similarity Filter</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/demo_06.gif"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/demo_06.gif" alt="demo" data-animated-image=""/></a></p>
<p dir="auto">Stochastic Similarity Filter reduces processing during video input by minimizing conversion operations when there is little change from the previous frame, thereby alleviating GPU processing load, as shown by the red frame in the above GIF. The usage is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="stream = StreamDiffusion(
    pipe,
    [32, 45],
    torch_dtype=torch.float16,
)
stream.enable_similar_image_filter(
    similar_image_filter_threshold,
    similar_image_filter_max_skip_frame,
)"><pre><span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    [<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
)
<span>stream</span>.<span>enable_similar_image_filter</span>(
    <span>similar_image_filter_threshold</span>,
    <span>similar_image_filter_max_skip_frame</span>,
)</pre></div>
<p dir="auto">There are the following parameters that can be set as arguments in the function:</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-similar_image_filter_threshold" aria-hidden="true" tabindex="-1" href="#similar_image_filter_threshold"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>similar_image_filter_threshold</code></h4>
<ul dir="auto">
<li>The threshold for similarity between the previous frame and the current frame before the processing is paused.</li>
</ul>
<h4 tabindex="-1" dir="auto"><a id="user-content-similar_image_filter_max_skip_frame" aria-hidden="true" tabindex="-1" href="#similar_image_filter_max_skip_frame"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a><code>similar_image_filter_max_skip_frame</code></h4>
<ul dir="auto">
<li>The maximum interval during the pause before resuming the conversion.</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content-residual-cfg-rcfg" aria-hidden="true" tabindex="-1" href="#residual-cfg-rcfg"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Residual CFG (RCFG)</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cumulo-autumn/StreamDiffusion/blob/main/assets/cfg_conparision.png"><img src="https://github.com/cumulo-autumn/StreamDiffusion/raw/main/assets/cfg_conparision.png" alt="rcfg"/></a></p>
<p dir="auto">RCFG is a method for approximately realizing CFG with competitive computational complexity compared to cases where CFG is not used. It can be specified through the cfg_type argument in the StreamDiffusion. There are two types of RCFG: one with no specified items for negative prompts RCFG Self-Negative and one where negative prompts can be specified RCFG Onetime-Negative. In terms of computational complexity, denoting the complexity without CFG as N and the complexity with a regular CFG as 2N, RCFG Self-Negative can be computed in N steps, while RCFG Onetime-Negative can be computed in N+1 steps.</p>
<p dir="auto">The usage is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# w/0 CFG
cfg_type = &#34;none&#34;
# CFG
cfg_type = &#34;full&#34;
# RCFG Self-Negative
cfg_type = &#34;self&#34;
# RCFG Onetime-Negative
cfg_type = &#34;initialize&#34;
stream = StreamDiffusion(
    pipe,
    [32, 45],
    torch_dtype=torch.float16,
    cfg_type=cfg_type,
)
stream.prepare(
    prompt=&#34;1girl, purple hair&#34;,
    guidance_scale=guidance_scale,
    delta=delta,
)"><pre><span># w/0 CFG</span>
<span>cfg_type</span> <span>=</span> <span>&#34;none&#34;</span>
<span># CFG</span>
<span>cfg_type</span> <span>=</span> <span>&#34;full&#34;</span>
<span># RCFG Self-Negative</span>
<span>cfg_type</span> <span>=</span> <span>&#34;self&#34;</span>
<span># RCFG Onetime-Negative</span>
<span>cfg_type</span> <span>=</span> <span>&#34;initialize&#34;</span>
<span>stream</span> <span>=</span> <span>StreamDiffusion</span>(
    <span>pipe</span>,
    [<span>32</span>, <span>45</span>],
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
    <span>cfg_type</span><span>=</span><span>cfg_type</span>,
)
<span>stream</span>.<span>prepare</span>(
    <span>prompt</span><span>=</span><span>&#34;1girl, purple hair&#34;</span>,
    <span>guidance_scale</span><span>=</span><span>guidance_scale</span>,
    <span>delta</span><span>=</span><span>delta</span>,
)</pre></div>
<p dir="auto">The delta has a moderating effect on the effectiveness of RCFG.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-development-team" aria-hidden="true" tabindex="-1" href="#development-team"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Development Team</h2>
<p dir="auto"><a href="https://twitter.com/cumulo_autumn" rel="nofollow">Aki</a>,
<a href="https://twitter.com/AttaQjp" rel="nofollow">Ararat</a>,
<a href="https://twitter.com/Chenfeng_X" rel="nofollow">Chenfeng Xu</a>,
<a href="https://twitter.com/ddPn08" rel="nofollow">ddPn08</a>,
<a href="https://twitter.com/ArtengMimi" rel="nofollow">kizamimi</a>,
<a href="https://twitter.com/__ramu0e__" rel="nofollow">ramune</a>,
<a href="https://twitter.com/hanyingcl" rel="nofollow">teftef</a>,
<a href="https://twitter.com/toni_nimono" rel="nofollow">Tonimono</a>,
<a href="https://twitter.com/IMG_5955" rel="nofollow">Verb</a>,</p>
<p dir="auto">(*alphabetical order)
<br/></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgements" aria-hidden="true" tabindex="-1" href="#acknowledgements"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgements</h2>
<p dir="auto">The video and image demos in this GitHub repository were generated using <a href="https://huggingface.co/latent-consistency/lcm-lora-sdv1-5" rel="nofollow">LCM-LoRA</a> + <a href="https://civitai.com/models/136268/kohaku-v2" rel="nofollow">KohakuV2</a> and <a href="https://arxiv.org/abs/2311.17042" rel="nofollow">SD-Turbo</a>.</p>
<p dir="auto">Special thanks to <a href="https://latent-consistency-models.github.io/" rel="nofollow">LCM-LoRA authors</a> for providing the LCM-LoRA and Kohaku BlueLeaf (<a href="https://twitter.com/KBlueleaf" rel="nofollow">@KBlueleaf</a>) for providing the KohakuV2 model and ,to <a href="https://ja.stability.ai/" rel="nofollow">Stability AI</a> for <a href="https://arxiv.org/abs/2311.17042" rel="nofollow">SD-Turbo</a>.</p>
<p dir="auto">KohakuV2 Models can be downloaded from  <a href="https://civitai.com/models/136268/kohaku-v2" rel="nofollow">Civitai</a>  and <a href="https://huggingface.co/KBlueLeaf/kohaku-v2.1" rel="nofollow">Hugging Face</a>.</p>
<p dir="auto">SD-Turbo is also available on <a href="https://huggingface.co/stabilityai/sd-turbo" rel="nofollow">Hugging Face Space</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contributors" aria-hidden="true" tabindex="-1" href="#contributors"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributors</h2>
<a href="https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors">
  <img src="https://camo.githubusercontent.com/850d0f07f10bcc27c428009a76e9837c0546b93021c6caee911f4758fc440e7a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d63756d756c6f2d617574756d6e2f53747265616d446966667573696f6e" data-canonical-src="https://contrib.rocks/image?repo=cumulo-autumn/StreamDiffusion"/>
</a>
</article>
          </div></div>
  </body>
</html>
