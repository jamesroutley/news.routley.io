<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://swe-to-mle.pages.dev/posts/makemore3-internals-of-mlp-and-visualization/">Original</a>
    <h1>Makemore3 Internals of MLP and Visualization</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><p>A look at episode #4: <a href="https://youtu.be/P6sfmUTpUmc?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener noreffer ">The spelled-out intro to language modeling: Building makemore Part 3: Activations &amp; Gradients, BatchNorm</a> from <a href="https://karpathy.ai/" target="_blank" rel="noopener noreffer ">Andrej Karpathy</a> amazing tutorial series.</p>

<p>
  <iframe src="https://www.youtube.com/embed/P6sfmUTpUmc" allowfullscreen="" title="YouTube Video"></iframe>
</p>

<p>It re-uses the N-gram character-level MLP from session #3 and discuss three kind of incremental improvements to training</p>
<h2 id="initial-weights">Initial weights</h2>
<p>While the model was training even with totally random weights this episode gives an intuition of why normally distributed values lead to faster training. Assigning clever weight at initialization time improve the loss of the first batches from <code>27</code> to <code>3.3</code> in our case. We waste less cycles reducing the weights of the net, and spend more cycles actually training.</p>
<h2 id="tanh-and-other-sigmoid-relu--are-destroying-your-backprop-gradients">Tanh (and other sigmoid, ReLU, …) are destroying your backprop gradients</h2>
<p>Most non linear layers tend to regularize values inside a range (e.g. <code>tanh()</code> clip values between <code>[-1, 1]</code>). Because of that it’s very dangerous to feed large (negative / positive) values to such layer.</p>
<p>The danger appear during backpropagation because the derivative of <code>tanh()</code> at <code>-1</code> and <code>1</code> is <code>0</code>. So the backpropagation gradients gets nullified. Intuitively this happen because <code>tanh(500) ~= 1.0</code> and <code>tanh(50) ~= 1</code>. Why bother changing the value from <code>500</code> to <code>50</code> if it will output <code>1.0</code> regardless and it’s not going to improve the loss.</p>
<h3 id="dead-neuron">Dead neuron</h3>
<p>An interesting titbit here. If every output of a <code>tanh()</code> become close to <code>1</code> (e.g. <code>neuron.out.min() &gt; 0.99</code>) then all the derivatives feeding into the neuron become <code>0</code> and there is no way to train this neuron anymore because all gradients comming to it are no-op. Effectively making it “dead”.</p>
<h2 id="batch-normalization">Batch normalization</h2>
<p>Batch normalization introduced in <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener noreffer ">BatchNorm paper</a> aims to prevent the non-linear-layers-being-mean-to-gradients™ behaviors by matching the values in the input layer of the <code>tanh()</code> layer with a Gaussian distribution.</p>
<p>This creates other problems (we now need to feed information about the distribution of our dataset) to run the network instead of a single element. But it also accidentally solves other problems (by making it harder to overfit the dataset).</p>
<p>Using batch normalization seems to be a cause of great frustration for the community, and Andrej suggests looking into “group normalization” and “layer normalization” instead.</p>
<h2 id="torchify-the-code-and-visualize-the-training">Torchify the code and visualize the training</h2>
<p>Finally the code gets re-written in PyTorch style and used to visualize how the weights of the neural net and training gradients behave based on the magnitude of the NN values. And why you should use some normalization to make your life easier.</p>
<h2 id="the-code">The code</h2>
<p>Here’s my take on the tutorial with additional notes. You can get the code on <a href="https://github.com/peluche/makemore" target="_blank" rel="noopener noreffer ">GitHub</a> or bellow.</p>


</div></div>
  </body>
</html>
