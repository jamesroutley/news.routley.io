<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://zznewclear13.github.io/posts/screen-space-reflection-en/">Original</a>
    <h1>Screen Space Reflection</h1>
    
    <div id="readability-page-1" class="page"><div><h2 id="screen-space-reflection">Screen Space Reflection</h2><p>While screen space reflection (SSR) is a well-known effect, this article aims to introduce a unique method for calculating screen space reflections – one that I haven’t encountered online before.</p><p>Many online tutorials cover screen space reflection already, and most of them follow a similar process: calculate the reflection direction in world space, use a mostly uniform step size to traverse the world space, and for each step, compute the normalized device coordinates (NDC). Then, compare the current depth with the depth value sampled from depth texture. If the ray depth is below the sampled depth, consider it a reflection intersection (hit) and sample the color value at that location. This method can yield visually pleasing reflection effects, but hardly anyone mentions the staggering number of iterations required. We’ll discuss this further shortly.</p><p>Additionally, achieving good reflection results for objects at varying distances often requires different step sizes, but few people delve into this consideration. Some slightly improved approaches involve binary search after ray hitting the scene to ensure smoother transitions between reflection colors. Others may prematurely terminate steps (known as early return) or interpolate reflection colors and environment reflection probe based on comparison between NDC coordinate and the [-1, 1] range.</p><p>Currently, the most effective screen space ray marching method involves precomputing a Hierarchical ZBuffer. By stepping into and out of different LODs, this approach achieves the same results with fewer iterations. However, Hierarchical ZBuffer is not a feature available in every projects.</p><p>The most valuable tutorial one can find online is <a href="http://casual-effects.blogspot.com/2014/08/screen-space-ray-tracing.html">Screen Space Ray Tracing</a> by Morgan McGuire. He also wrote a <a href="https://jcgt.org/published/0003/04/04/paper.pdf">paper</a> about his algorithm. In his article, McGuire highlights why stepping in world space can be problematic. After undergoing perspective transformation, the step positions in world space may not vary significantly in screen space, leading to the need for more iterations to achieve desirable reflection effects. Also, McGuire presents an ingenious approach in his article. He calculates the coordinates of the starting and ending points in both clip space and screen space. By linearly interpolating the z coordinate in clip space, the 1/w coordinate in clip space, and the xy coordinates in screen space, he eliminates the matrix computations required during each step. Definitely worth using!</p><p>The goal of this article is to get the correct reflection color using as few iterations as possible within a single shader. Random sampling, blurring, and Fresnel effect are not within the scope of this article. We will focus solely on Windows platform DX11 shaders, which allows us to avoid extensive platform-specific code. The Unity version used for this article is Unity 2022.3.21f1. The final shader code will be provided at the end of the article.</p><h2 id="calculation-of-reflections">Calculation of Reflections</h2><h3 id="parameters">Parameters</h3><p>The calculation of reflections typically relies on three essential parameters:</p><ol><li>Max Distance: This parameter considers reflections from objects within a certain range around the reflection point.</li><li>Step Count: Increasing the number of steps results in more accurate reflections but also impacts performance.</li><li>Thickness Params: In this article, an object’s default thickness is calculated as <code>depth * _Thickness.y + _Thickness.x</code>. This ensures that when a ray passes through behind an object, it is not considered an intersection.</li></ol><h3 id="comparion-of-depth-value">Comparion of Depth Value</h3><p>When considering what kind of depth value to compare during the stepping process, several factors come into play. We define the depth value we obtained from stepping as <code>rayDepth</code> and the depth value obtained from sampling as <code>sampleDepth</code>.</p><p>By directly sampling the depth texture, we obtain the depth value in normalized device coordinates. Therefore a straightforward approach is to compare these depths in NDC. When <code>rayDepth &lt; sampleDepth</code>, the ray intersects with the scene.</p><p>Alternatively, we can compare the actual depth values in view space. This approach allows us to specify a thickness value. If the depth difference exceeds this thickness, we consider the ray passing through behind an object without intersection. Specifically, when <code>rayDepth &gt; sampleDepth &amp;&amp; rayDepth &lt; sampleDepth + thickness</code>, the ray intersects with the scene.</p><p>One thing worth noting is the sampler used when sampling depth texture. Linear interpolation can mistakenly identify intersections at the edges of two faces with different depths, resulting in unwanted artifacts (small dots) on the screen. If available, using a separate texture to mark object edges can help exclude these intersection points. But in our shader, we will stick to a <code>Point Clamp</code> sampler.</p><h3 id="ray-marching">Ray Marching</h3><p>Here’s the workflow breakdown:</p><blockquote><ol><li>Define <code>k0</code> and <code>k1</code> as the reciprocals of the w-components of clip space coordinates for the starting and ending points.</li><li>Define <code>q0</code> and <code>q1</code> as the xyz-components of clip space coordinates for the starting and ending points.</li><li>Define <code>p0</code> and <code>p1</code> as the xy-components of normalized device coordinates for the starting and ending points.</li><li>Define <code>w</code> as a variable that linearly increases in (0, 1) based on <code>_StepCount</code>.</li><li>For each step, update the value of <code>w</code> and use it to linearly interpolate the three sets of components (<code>k</code>, <code>q</code>, and <code>p</code>).</li><li>Calculate <code>rayDepth</code> using <code>q.z * k</code>, sample the depth texture at <code>p</code> to obtain <code>sampleDepth</code>.</li><li>If <code>rayDepth &lt; sampleDepth</code>, the ray intersects with the scene, exit the loop and return <code>p</code>.</li><li>Sample the color texture at <code>p</code> to obtain the reflection color.</li></ol></blockquote><p>It looks like this (32 steps):
<img loading="lazy" src="https://zznewclear13.github.io/posts/images/ScreenSpaceReflection_Naive.png#center" alt="Screen Space Reflection Naive"/></p><p>Quite poor! The most noticeable issue is the stretching effect. There are primarily two reasons for this: First, we did not use thickness to determine whether the ray passes through behind an object, resulting in significant stretching below suspended objects. Second, we did not restrict positions outside the screen area, causing us to sample depth values from coordinates beyond the screen and return depth values at clampped positions.</p><h3 id="thickness-test">Thickness Test</h3><p>To address the thickness issue mentioned earlier, we introduce a method for determining whether the stepping position is behind an object. This method relies on the linear depths from the camera: <code>linearRayDepth</code> and <code>linearSampleDepth</code>.</p><p>As previously discussed, we use <code>linearSampleDepth * _Thickness.y + _Thickness.x</code> as the thickness of an object in the scene. To determine if the ray passes through behind an object, we compare <code>(linearRayDepth - linearSampleDepth - _Thickness.x) / linearSampleDepth</code> with <code>_Thickness.y</code>. If the former is greater than the latter, it indicates that the ray passes through behind an object.</p><pre><code data-lang="HLSL">    float getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams)
    {
        return (diff - thicknessParams.x) / linearSampleDepth;
    }
</code></pre><p>The workflow now becomes:</p><blockquote><ol start="7"><li>If <code>rayDepth &lt; sampleDepth</code> and <code>thicknessDiff &lt; _Thickness.y</code>, the ray intersects with the scene, exit the loop and return <code>p</code>.</li></ol></blockquote><p>It looks like this (32 steps):
<img loading="lazy" src="https://zznewclear13.github.io/posts/images/ScreenSpaceReflection_ThicknessTest.png#center" alt="Screen Space Reflection Thickness Test"/></p><h3 id="frustum-clipping">Frustum Clipping</h3><p>For point <code>p1</code> that falls outside the screen space, two issues arise: First, sampling the depth texture beyond the screen range yields incorrect depth values. Second, it reduces the effective sampling count. To address this, we can restrict <code>p1</code> within the screen space. Here’s a method for constraining the stepping endpoint within the view frustum. We define <code>nf</code> as the near and far clipping plane depths (positive values), define <code>s</code> as the slope of the view frustum in the horizontal and vertical directions (positive values). Numerically, <code>s</code> is given by <code>float2(aspect * tan(fovy * 0.5f), tan(fovy * 0.5f))</code>. Note that for ease of calculation, the z-components of <code>from</code> and <code>to</code> are positive.</p><pre><code data-lang="HLSL">#define INFINITY 1e10

float3 frustumClip(float3 from, float3 to, float2 nf, float2 s)
{
    float3 dir = to - from;
    float3 signDir = sign(dir);

    float nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f;
    float lenZ = (nfSlab - from.z) / dir.z;
    if (dir.z == 0.0f) lenZ = INFINITY;

    float2 ss = sign(dir.xy - s * dir.z) * s;
    float2 denom = ss * dir.z - dir.xy;
    float2 lenXY = (from.xy - ss * from.z) / denom;
    if (lenXY.x &lt; 0.0f || denom.x == 0.0f) lenXY.x = INFINITY;
    if (lenXY.y &lt; 0.0f || denom.y == 0.0f) lenXY.y = INFINITY;

    float len = min(min(1.0f, lenZ), min(lenXY.x, lenXY.y));
    float3 clippedVS = from + dir * len;
    return clippedVS;
}
</code></pre><p>Actually I have wrote a shadertoy to demonstrate frustum clipping in 2D, interactable with mouse:</p><figure><p>Frustum Clip 2D</p></figure><p>The workflow now becomes:</p><blockquote><ol start="0"><li>Frustum clip the stepping endpoint to <code>clippedPosVS</code>, and transform it to clip space position <code>endCS</code>.</li></ol></blockquote><p>It looks like this (32 steps):
<img loading="lazy" src="https://zznewclear13.github.io/posts/images/ScreenSpaceReflection_FrustumClip.png#center" alt="Screen Space Reflection Frustum Clip"/></p><p>The scene is starting to exhibit some reflection, although there’s still room for improvement. The view frustum clipping has indeed reduced the number of pixels between steps, filling in some of the gaps. However, the reflected colors appear distorted.</p><h3 id="binary-search">Binary Search</h3><p>In our previous step, although <code>p</code> is ensured to be at the intersected object, there is still some distance from the actual intersection point. To reduce this error, we can use binary search. Here’s how it works: We maintain two variables during stepping, <code>w1</code> and <code>w2</code>, representing the <code>w</code> values in last two steps (<code>w1 &gt; w2</code>). During each binary search iteration, we calculate <code>w = 0.5f * (w1 + w2)</code>, if an intersection is detected, update <code>w1 = w</code>, otherwise, update <code>w2 = w</code> and proceed to the next iteration.</p><p>The workflow now becomes:</p><blockquote><ol start="0"><li>Frustum clip the stepping endpoint to <code>clippedPosVS</code>, and transform it to clip space position <code>endCS</code>.</li><li>Define <code>k0</code> and <code>k1</code> as the reciprocals of the w-components of clip space coordinates for the starting and ending points.</li><li>Define <code>q0</code> and <code>q1</code> as the xyz-components of clip space coordinates for the starting and ending points.</li><li>Define <code>p0</code> and <code>p1</code> as the xy-components of normalized device coordinates for the starting and ending points.</li><li>Define <code>w1</code> as a variable that linearly increases in (0, 1) based on <code>_StepCount</code>, initialize <code>w1</code> and <code>w2</code> with 0.</li><li>For each step, <code>w2 = w1</code>, update the value of <code>w1</code> and use it to linearly interpolate the three sets of components (<code>k</code>, <code>q</code>, and <code>p</code>).</li><li>Calculate <code>rayDepth</code> using <code>q.z * k</code>, sample the depth texture at <code>p</code> to obtain <code>sampleDepth</code>.</li><li>If <code>rayDepth &lt; sampleDepth</code> and <code>thicknessDiff &lt; _Thickness.y</code>, the ray intersects with the scene, exit the loop.</li><li>Let <code>w</code> be the average of <code>w1</code> and <code>w2</code>. Repeat 5, 6, and 7 to check whether an intersection occurs until the binary search loop ends. We update either <code>w1</code> or <code>w2</code> in each step depending on the result.</li><li>Sample the color texture at <code>p</code> to obtain the reflection color.</li></ol></blockquote><p>It looks like this (32 steps, 5 binary searches):
<img loading="lazy" src="https://zznewclear13.github.io/posts/images/ScreenSpaceReflection_BinarySearch.png#center" alt="Screen Space Reflection Binary Search"/></p><p>The reflection effect now appears less distorted (particularly noticeable in the bottom left corner). However, there are still gaps between color segments due to our thickness test, which excludes potential intersections.</p><h3 id="potential-intersections">Potential Intersections</h3><p>To compute potential intersections, let’s revisit the workflow where we do thickness test. When the ray passes through behind an object, if it is above the scene during last step, we can calculate the difference (<code>thicknessDiff</code>) between <code>rayDepth</code> and <code>sampleDepth</code>. If this value is smaller than the minimum difference (<code>minThicknessDiff</code>), we consider it a potential intersection. We update <code>minThicknessDiff</code> and record the current <code>w1</code> and <code>w2</code> for subsequent binary search.</p><p>During binary search, if an actual intersection occurs, we follow the original code. If a potential intersection occurs, we also need to track <code>thicknessDiff</code> during binary search. We find the smallest <code>thicknessDiff</code> less than <code>_Thickness.y</code>, use current <code>w</code> to interpolate between <code>p0</code> and <code>p1</code> to obtain <code>p</code>, and finally use <code>p</code> to sample the color texture.</p><p>The workflow now becomes:</p><blockquote><ol start="0"><li>Frustum clip the stepping endpoint to <code>clippedPosVS</code>, and transform it to clip space position <code>endCS</code>.</li><li>Define <code>k0</code> and <code>k1</code> as the reciprocals of the w-components of clip space coordinates for the starting and ending points.</li><li>Define <code>q0</code> and <code>q1</code> as the xyz-components of clip space coordinates for the starting and ending points.</li><li>Define <code>p0</code> and <code>p1</code> as the xy-components of normalized device coordinates for the starting and ending points.</li><li>Define <code>w1</code> as a variable that linearly increases in (0, 1) based on <code>_StepCount</code>, initialize <code>w1</code> and <code>w2</code> with 0.</li><li>For each step, <code>w2 = w1</code>, update the value of <code>w1</code> and use it to linearly interpolate the three sets of components (<code>k</code>, <code>q</code>, and <code>p</code>).</li><li>Calculate <code>rayDepth</code> using <code>q.z * k</code>, sample the depth texture at <code>p</code> to obtain <code>sampleDepth</code>.</li><li>If <code>rayDepth &lt; sampleDepth</code> and <code>thicknessDiff &lt; _Thickness.y</code>, the ray intersects with the scene, exit the loop.</li><li>Otherwise, if <code>rayZ &lt; sampleZ</code>, <code>thicknessDiff &gt; _Thickness.y</code>, and the previous ray was in front of the scene, compare <code>thicknessDiff</code> with the minimum value. If smaller, update the minimum value, record the current <code>w1</code> and <code>w2</code>, and mark this as a potential intersection, continue looping.</li><li>If an actual intersection occurs, let <code>w</code> be the average of <code>w1</code> and <code>w2</code>. Repeat 5, 6, and 7 to check whether an intersection occurs until the binary search loop ends. We update either <code>w1</code> or <code>w2</code> in each step depending on the result.</li><li>If a potential intersection occurs, repeat 5, 6, and 7 to check whether an intersection occurs, and use the smallest <code>thicknessDiff</code> and <code>w</code> to update <code>p</code>.</li><li>Sample the color texture at <code>p</code> to obtain the reflection color.</li></ol></blockquote><p>It looks like this (32 steps, 5 binary searches):
<img loading="lazy" src="https://zznewclear13.github.io/posts/images/ScreenSpaceReflection_PotentialHit.png#center" alt="Screen Space Reflection Potential Hit"/>
And here is the result using 64 steps and 5 binary searches:
<img loading="lazy" src="https://zznewclear13.github.io/posts/images/ScreenSpaceReflection_64.png#center" alt="Screen Space Reflection Potential Hit"/></p><h3 id="screenspacereflectionshader">ScreenSpaceReflection.shader</h3><pre><code data-lang="HLSL">/*
// Copyright (c) 2024 zznewclear@gmail.com
// 
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the &#34;Software&#34;), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
// 
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
// 
// THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
*/

Shader &#34;zznewclear13/SSRShader&#34;
{
    Properties
    {
        [Toggle(USE_POTENTIAL_HIT)] _UsePotentialHit (&#34;Use Potential Hit&#34;, Float) = 1.0
        [Toggle(USE_FRUSTUM_CLIP)] _UseFrustumClip (&#34;Use Frustum Clip&#34;, Float) = 1.0
        [Toggle(USE_BINARY_SEARCH)] _UseBinarySearch (&#34;Use Binary Search&#34;, Float) = 1.0
        [Toggle(USE_THICKNESS)] _UseThickness (&#34;Use Thickness&#34;, Float) = 1.0
        
        _MaxDistance (&#34;Max Distance&#34;, Range(0.1, 100.0)) = 15.0
        [int] _StepCount (&#34;Step Count&#34;, Float) = 32
        _ThicknessParams (&#34;Thickness Params&#34;, Vector) = (0.1, 0.02, 0.0, 0.0)
    }

    HLSLINCLUDE
    #include &#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl&#34;
    #include &#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl&#34;
    #include &#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl&#34;
    
    #pragma shader_feature _ USE_POTENTIAL_HIT
    #pragma shader_feature _ USE_FRUSTUM_CLIP
    #pragma shader_feature _ USE_BINARY_SEARCH
    #pragma shader_feature _ USE_THICKNESS

    #define INFINITY 1e10
    #define DEPTH_SAMPLER sampler_PointClamp

    Texture2D _CameraOpaqueTexture;
    Texture2D _CameraDepthTexture;
    CBUFFER_START(UnityPerMaterial)
    float _MaxDistance;
    int _StepCount;
    float2 _ThicknessParams;
    CBUFFER_END

    struct Attributes
    {
        float4 positionOS   : POSITION;
        float3 normalOS     : NORMAL;
        float2 texcoord     : TEXCOORD0;
    };

    struct Varyings
    {
        float4 positionCS   : SV_POSITION;
        float3 positionWS   : TEXCOORD0;
        float3 normalWS     : TEXCOORD1;
        float2 uv           : TEXCOORD2;
        float3 viewWS       : TEXCOORD3;
    };

    Varyings vert(Attributes input)
    {
        Varyings output = (Varyings)0;
        VertexPositionInputs vpi = GetVertexPositionInputs(input.positionOS.xyz);
        VertexNormalInputs vni = GetVertexNormalInputs(input.normalOS);

        output.positionCS = vpi.positionCS;
        output.positionWS = vpi.positionWS;
        output.normalWS = vni.normalWS;
        output.uv = input.texcoord;
        output.viewWS = GetCameraPositionWS() - vpi.positionWS;
        return output;
    }

    float3 frustumClip(float3 from, float3 to, float2 nf, float2 s)
    {
        float3 dir = to - from;
        float3 signDir = sign(dir);

        float nfSlab = signDir.z * (nf.y - nf.x) * 0.5f + (nf.y + nf.x) * 0.5f;
        float lenZ = (nfSlab - from.z) / dir.z;
        if (dir.z == 0.0f) lenZ = INFINITY;

        float2 ss = sign(dir.xy - s * dir.z) * s;
        float2 denom = ss * dir.z - dir.xy;
        float2 lenXY = (from.xy - ss * from.z) / denom;
        if (lenXY.x &lt; 0.0f || denom.x == 0.0f) lenXY.x = INFINITY;
        if (lenXY.y &lt; 0.0f || denom.y == 0.0f) lenXY.y = INFINITY;

        float len = min(min(1.0f, lenZ), min(lenXY.x, lenXY.y));
        float3 clippedVS = from + dir * len;
        return clippedVS;
    }

    float getThicknessDiff(float diff, float linearSampleDepth, float2 thicknessParams)
    {
        return (diff - thicknessParams.x) / linearSampleDepth;
    }

    float4 frag(Varyings input) : SV_TARGET
    {
        float3 positionWS = input.positionWS;
        float3 normalWS = normalize(input.normalWS);
        float3 viewWS = normalize(input.viewWS);
        float3 reflWS = reflect(-viewWS, normalWS);
        float3 env = GlossyEnvironmentReflection(reflWS, 0.0f, 1.0f);
        float3 color = env;

        float3 originWS = positionWS;
        float3 endWS = positionWS + reflWS * _MaxDistance;

#if defined(USE_FRUSTUM_CLIP)
        float3 originVS = mul(UNITY_MATRIX_V, float4(originWS, 1.0f)).xyz;
        float3 endVS = mul(UNITY_MATRIX_V, float4(endWS, 1.0f)).xyz;
        float3 flipZ = float3(1.0f, 1.0f, -1.0f);
        float3 clippedVS = frustumClip(originVS * flipZ, endVS * flipZ, _ProjectionParams.yz, float2(1.0f, -1.0f) / UNITY_MATRIX_P._m00_m11);
        clippedVS *= flipZ;
        float4 originCS = mul(UNITY_MATRIX_VP, float4(originWS, 1.0f));
        float4 endCS = mul(UNITY_MATRIX_P, float4(clippedVS, 1.0f));
#else
        float4 originCS = mul(UNITY_MATRIX_VP, float4(originWS, 1.0f));
        float4 endCS = mul(UNITY_MATRIX_VP, float4(endWS, 1.0f));
#endif

        float k0 = 1.0f / originCS.w;
        float k1 = 1.0f / endCS.w;
        float3 q0 = originCS.xyz;
        float3 q1 = endCS.xyz;
        float2 p0 = originCS.xy * float2(1.0f, -1.0f) * k0 * 0.5f + 0.5f;
        float2 p1 = endCS.xy * float2(1.0f, -1.0f) * k1 * 0.5f + 0.5f;

#if defined(USE_POTENTIAL_HIT)
        float w1 = 0.0f;
        float w2 = 0.0f;
        bool hit = false;
        bool lastHit = false;
        bool potentialHit = false;
        float2 potentialW12 = float2(0.0f, 0.0f);
        float minPotentialHitPos = INFINITY;
        [unroll(64)]
        for (int i=0; i&lt;_StepCount; ++i)
        {
            w2 = w1;
            w1 += 1.0f / float(_StepCount);

            float3 q = lerp(q0, q1, w1);
            float2 p = lerp(p0, p1, w1);
            float k = lerp(k0, k1, w1);
            float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r;
            float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams);
            float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams);

            float hitDiff = linearRayDepth - linearSampleDepth;
            float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams);
            if (hitDiff &gt; 0.0f)
            {
                if (thicknessDiff &lt; _ThicknessParams.y)
                {
                    hit = true;
                    break;
                }
                else if(!lastHit)
                {
                    potentialHit = true;
                    if (minPotentialHitPos &gt; thicknessDiff)
                    {
                        minPotentialHitPos = thicknessDiff;
                        potentialW12 = float2(w1, w2);
                    }
                }
            }
            lastHit = hitDiff &gt; 0.0f;
        }
#else
        float w1 = 0.0f;
        float w2 = 0.0f;
        bool hit = false;
        [unroll(64)]
        for (int i=0; i&lt;_StepCount; ++i)
        {
            w2 = w1;
            w1 += 1.0f / float(_StepCount);

            float3 q = lerp(q0, q1, w1);
            float2 p = lerp(p0, p1, w1);
            float k = lerp(k0, k1, w1);
            float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r;
#if defined(USE_THICKNESS)
            float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams);
            float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams);

            float hitDiff = linearRayDepth - linearSampleDepth;
            float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams);
            if (hitDiff &gt; 0.0f &amp;&amp; thicknessDiff &lt; _ThicknessParams.y)
            {
                hit = true;
                break;
            }       
#else
            if (q.z * k &lt; sampleDepth)
            {
                hit = true;
                break;
            }
#endif
        }
#endif

#if defined(USE_POTENTIAL_HIT)
        if (hit || potentialHit)
        {
            if (!hit)
            {
                w1 = potentialW12.x;
                w2 = potentialW12.y;
            }

            bool realHit = false;
            float2 hitPos;
            float minThicknessDiff = _ThicknessParams.y;
            [unroll(5)]
            for (int i=0; i&lt;5; ++i)
            {
                float w = 0.5f * (w1 + w2);
                float3 q = lerp(q0, q1, w1);
                float2 p = lerp(p0, p1, w1);
                float k = lerp(k0, k1, w1);
                float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r;
                float linearSampleDepth = LinearEyeDepth(sampleDepth, _ZBufferParams);
                float linearRayDepth = LinearEyeDepth(q.z * k, _ZBufferParams);
                float hitDiff = linearRayDepth - linearSampleDepth;

                if (hitDiff &gt; 0.0f)
                {
                    w1 = w;
                    if (hit) hitPos = p;
                }
                else
                {
                    w2 = w;
                }

                float thicknessDiff = getThicknessDiff(hitDiff, linearSampleDepth, _ThicknessParams);
                float absThicknessDiff = abs(thicknessDiff);
                if (!hit &amp;&amp; absThicknessDiff &lt; minThicknessDiff) 
                {
                    realHit = true;
                    minThicknessDiff = thicknessDiff;
                    hitPos = p;
                }
            }

            if (hit || realHit) color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f;
        }
#elif defined(USE_BINARY_SEARCH)
        if (hit)
        {
            float2 hitPos;
            [unroll(5)]
            for (int i=0; i&lt;5; ++i)
            {
                float w = 0.5f * (w1 + w2);
                float3 q = lerp(q0, q1, w1);
                float2 p = lerp(p0, p1, w1);
                float k = lerp(k0, k1, w1);

                float sampleDepth = _CameraDepthTexture.Sample(DEPTH_SAMPLER, p).r;
                if (q.z * k &lt; sampleDepth)
                {
                    w1 = w;
                    hitPos = p;
                }
                else
                {
                    w2 = w;
                }
            }
            color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f;
        }
#else
        if (hit)
        {
            float2 hitPos = lerp(p0, p1, w1);
            color = _CameraOpaqueTexture.Sample(sampler_LinearClamp, hitPos).rgb * 0.3f;
        }
#endif

        return float4(color, 1.0f);
    }

    ENDHLSL

    SubShader
    {
        Tags { &#34;RenderType&#34;=&#34;Transparent&#34; &#34;Queue&#34;=&#34;Transparent&#34; }
        LOD 100

        Pass
        {
            HLSLPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            ENDHLSL
        }
    }
}
</code></pre><h3 id="future-optimization">Future Optimization</h3><p>Currently, there is one aspect worth optimizing: controlling the overall step count based on the pixel distance between <code>p0</code> and <code>p1</code>. We certainly don’t want to step 64 times for just 10 pixels. However, this is a relatively straightforward task, and I’ll leave it to someone with time to spare. As for random sampling, blurring, and Fresnel effects, let’s consider those when we really need them.</p><h2 id="postscript">Postscript</h2><p>This article was translated by Microsoft’s Copilot and I made a few adjustments. What an era we live in!</p></div></div>
  </body>
</html>
