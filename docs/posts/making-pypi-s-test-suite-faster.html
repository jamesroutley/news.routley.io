<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.trailofbits.com/2025/05/01/making-pypis-test-suite-81-faster/">Original</a>
    <h1>Making PyPI&#39;s test suite faster</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><main role="main"><article><div><p>Trail of Bits has collaborated with <a href="https://pypi.org/">PyPI</a> for several years to add
features and improve security defaults across the Python packaging ecosystem.</p><p>Our previous posts have focused on features like <a href="https://blog.trailofbits.com/2024/11/14/attestations-a-new-generation-of-signatures-on-pypi/">digital attestations</a>
and <a href="https://blog.trailofbits.com/2023/05/23/trusted-publishing-a-new-benchmark-for-packaging-security/">Trusted Publishing</a>, but today we’ll look at a equally critical aspect
of holistic software security: test suite performance.</p><p>A robust testing suite is essential to the security and reliability of a complex
codebase. However, as test coverage grows, so does execution time, creating
friction in the development process and disincentivizing frequent and meaningful
(i.e., deep) testing. In this post, we’ll detail how we methodically optimized
the test suite for <a href="https://github.com/pypi/warehouse">Warehouse</a> (the back end that powers PyPI), <strong>reducing
execution time from 163 seconds to 30 seconds</strong> while the <strong>test count grew
from 3,900 to over 4,700</strong>.</p><figure><img src="https://blog.trailofbits.com/img/pypi-test-improvements-results.png" alt="Testing time over the last year on Warehouse"/><figcaption>Figure 1: Warehouse test execution time over a 12-month period (March 2024 to April 2025).</figcaption></figure><p>We achieved a <strong>81% performance improvement</strong> through several steps:</p><ul><li>Parallelizing test execution with <a href="https://github.com/pytest-dev/pytest-xdist"><code>pytest-xdist</code></a> (67% relative reduction)</li><li>Using Python 3.12’s <a href="https://docs.python.org/3.12/library/sys.monitoring.html"><code>sys.monitoring</code></a> for more efficient coverage
instrumentation (53% relative reduction)</li><li>Optimizing test discovery with strategic testpaths configuration</li><li>Eliminating unnecessary imports that added startup overhead</li></ul><p>These optimizations are directly applicable to many Python projects,
particularly those with growing test suites that have become a
bottleneck in development workflows. By implementing even a subset of
these techniques, you can dramatically improve your own test performance
without any costs.</p><p><em>All times reported in this blog post are from running the Warehouse test suite
at the specified date, on a <a href="https://cloud.google.com/compute/docs/machine-types#n2-highcpu-32">n2-highcpu-32</a> machine. While not intended as
formal benchmarking results, these measurements provide clear evidence of the
impact of our optimizations.</em></p><h2 id="the-beast-warehouses-testing-suite">The beast: Warehouse’s testing suite</h2><p>PyPI is a critical component of the Python ecosystem: it serves over one
billion distribution downloads per day, and developers worldwide depend on
its reliability and integrity for the software artifacts that they
integrate into their stacks.</p><p>This criticality makes comprehensive testing non-negotiable, and Warehouse
correspondingly demonstrates exemplary testing practices: 4,734 tests (as of
April 2025) provide 100% branch coverage across the combination of unit and
integration suites. These tests are implemented using the <code>pytest</code> framework and
run on every pull request and merge as part of a robust CI/CD pipeline, which
additionally enforces 100% coverage as an acceptance requirement. On our
benchmark system, the current suite execution time is approximately 30 seconds.</p><p>This performance represents a dramatic improvement from March 2024, when the test suite:</p><ul><li>Contained approximately 3,900 tests (17.5% fewer tests)</li><li>Required 161 seconds to execute (5.4× longer)</li><li>Created significant friction in the development workflow</li></ul><p>Below, we’ll explore the systematic approach we took to achieve these
improvements, starting with the highest-impact changes and working through to
the finer optimizations that collectively transformed the testing experience for
PyPI contributors.</p><h2 id="parallelizing-test-execution-for-massive-gains">Parallelizing test execution for massive gains</h2><p>The most significant performance improvement came from a foundational computing
principle: parallelization. Tests are frequently well-suited for parallel
execution because well-designed test cases are isolated and have no side effects
or globally observable behavior. Warehouse’s unit and integration
tests were already well-isolated, making parallelization an obvious first
target for our optimization efforts.</p><p>We implemented parallel test execution using <a href="https://github.com/pytest-dev/pytest-xdist"><code>pytest-xdist</code></a>, a popular plugin
that distributes tests across multiple CPU cores.</p><p><code>pytest-xdist</code> configuration is straightforward: this single line change is enough!</p><figure><pre tabindex="0"><code data-lang="diff"><span><span># In pyproject.toml
</span></span><span><span>[tool.pytest.ini_options]
</span></span><span><span>addopts = [
</span></span><span><span> &#34;--disable-socket&#34;,
</span></span><span><span> &#34;--allow-hosts=localhost,::1,notdatadog,stripe&#34;,
</span></span><span><span> &#34;--durations=20&#34;,
</span></span><span><span><span>+  &#34;--numprocesses=auto&#34;,
</span></span></span><span><span><span></span>]
</span></span></code></pre><figcaption><span>Figure 2: Configuring pytest to run with pytest-xdist.</span></figcaption></figure><p>With this simple configuration, <code>pytest</code> automatically uses all available CPU
cores. On our 32-core test machine, this immediately yielded dramatic
improvements while <em>also</em> revealing several challenges that required careful
solutions.</p><h3 id="challenge-database-fixtures">Challenge: database fixtures</h3><p>Each test worker needed its isolated database instance to prevent cross-test contamination.</p><figure><pre tabindex="0"><code data-lang="diff"><span><span><span>@pytest.fixture(scope=&#34;session&#34;)
</span></span></span><span><span><span></span><span>- def database(request):
</span></span></span><span><span><span></span><span>+ def database(request, worker_id):
</span></span></span><span><span><span></span> config = get_config(request)
</span></span><span><span> pg_host = config.get(&#34;host&#34;)
</span></span><span><span> pg_port = config.get(&#34;port&#34;) or os.environ.get(&#34;PGPORT&#34;, 5432)
</span></span><span><span> pg_user = config.get(&#34;user&#34;)
</span></span><span><span><span>-   pg_db = f&#34;tests&#34;
</span></span></span><span><span><span></span><span>+   pg_db = f&#34;tests-{worker_id}&#34;
</span></span></span><span><span><span></span> pg_version = config.get(&#34;version&#34;, 16.1)
</span></span><span><span>
</span></span><span><span> janitor = DatabaseJanitor(
</span></span></code></pre><figcaption><span>Figure 3: Changes to the database fixture.</span></figcaption></figure><p>This change made each worker use its own database instance, preventing any
cross-contamination between different workers.</p><h3 id="challenge-coverage-reporting">Challenge: coverage reporting</h3><p>Test parallelization broke our coverage reporting since each worker process collected coverage data independently. Fortunately, this issue was covered in the <a href="https://coverage.readthedocs.io/en/latest/subprocess.html#implicit-coverage">coverage documentation</a>. We solved the issue by adding a <code>sitecustomize.py</code> file.</p><figure><pre tabindex="0"><code data-lang="python"><span><span><span>try</span><span>:</span>
</span></span><span><span>    <span>import</span> <span>coverage</span>
</span></span><span><span>    <span>coverage</span><span>.</span><span>process_startup</span><span>()</span>
</span></span><span><span><span>except</span> <span>ImportError</span><span>:</span>
</span></span><span><span>    <span>pass</span></span></span></code></pre><figcaption><span>Figure 4: Starting coverage instrumentation when using multiple workers.</span></figcaption></figure><h3 id="challenge-test-output-readability">Challenge: test output readability</h3><p>Parallel execution produced interleaved, difficult-to-read output. We integrated
<a href="https://github.com/Teemu/pytest-sugar">pytest-sugar</a> to provide cleaner, more
organized test results (<a href="https://github.com/pypi/warehouse/pull/16245">PR #16245</a>).</p><h3 id="results">Results</h3><p>These changes were merged in <a href="https://github.com/pypi/warehouse/pull/16206">PR #16206</a> and produced remarkable
results:</p><table><thead><tr><th></th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td>Test execution time</td><td>191s</td><td>63s</td><td>67% reduction</td></tr></tbody></table><p>This single optimization delivered most of our performance gains while requiring
relatively few code changes, demonstrating the importance of addressing
architectural bottlenecks before fine-tuning individual components.</p><h2 id="optimizing-coverage-with-python-312s-sysmonitoring">Optimizing coverage with Python 3.12’s <code>sys.monitoring</code></h2><p><em><a href="https://coverage.readthedocs.io/en/latest/changes.html#version-7-7-0-2025-03-16">Coverage 7.7.0+</a> Notice: When using <a href="https://coverage.readthedocs.io/en/latest/branch.html">branch coverage</a> with Python versions
prior to 3.14, the <code>COVERAGE_CORE=sysmon</code> setting is automatically disabled and
a warning is emitted.</em></p><p>Our analysis identified code coverage instrumentation as another significant
performance bottleneck. Coverage measurement is essential for testing quality,
but traditional implementation methods add considerable overhead to test
execution.</p><p><a href="https://peps.python.org/pep-0669/">PEP 669</a> introduced <code>sys.monitoring</code>, a
lighter-weight way to monitor the execution. The <code>coverage.py</code> library began
supporting this new API in version 7.4.0:</p><blockquote><p>In Python 3.12 and above, you can try an experimental core based on the new
<code>sys.monitoring module</code> by defining a <code>COVERAGE_CORE=sysmon</code> environment
variable. This should be faster, though plugins and dynamic contexts are not
yet supported with it.
(<a href="https://coverage.readthedocs.io/en/7.4.0/changes.html#version-7-4-0-2023-12-27">source</a>)</p></blockquote><h3 id="changes-in-warehouse">Changes in Warehouse</h3><figure><pre tabindex="0"><code data-lang="diff"><span><span># In Makefile
</span></span><span><span><span>-  docker compose run --rm --env COVERAGE=$(COVERAGE) tests bin/tests --postgresql-host db $(T) $(TESTARGS)
</span></span></span><span><span><span></span><span>+ docker compose run --rm --env COVERAGE=$(COVERAGE) --env COVERAGE_CORE=$(COVERAGE_CORE) tests bin/tests --postgresql-host db $(T) $(TESTARGS)
</span></span></span></code></pre><figcaption><span>Figure 5: Changes to the Makefile to allow setting the COVERAGE_CORE variable.</span></figcaption></figure><p>Using this new <code>coverage</code> feature was straightforward, thanks to
<a href="https://nedbatchelder.com/">Ned Batchelder</a>’s excellent documentation and hard work!</p><h3 id="change-impact">Change impact</h3><p>This change was merged in <a href="https://github.com/pypi/warehouse/pull/16621">PR #16621</a> and the results were also remarkable:</p><table><thead><tr><th></th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td>Test execution time</td><td>58s</td><td>27s</td><td>53% reduction</td></tr></tbody></table><p>This optimization highlights another advantage of Warehouse’s development
process: by adopting new Python versions (in this case, 3.12) relatively
quickly, Warehouse was able to leverage <code>sys.monitoring</code> and benefit
directly from the performance improvements it lends to <code>coverage</code>.</p><h2 id="accelerating-pytests-test-discovery-phase">Accelerating pytest’s test discovery phase</h2><h3 id="understanding-test-collection-overhead">Understanding test collection overhead</h3><p>In large projects, pytest’s test discovery process can become surprisingly expensive:</p><ol><li>Pytest recursively scans directories for test files</li><li>It imports each file to discover test functions and classes</li><li>It collects test metadata and applies filtering</li><li>Only then can actual test execution begin</li></ol><p>For PyPI’s 4,700+ tests, this discovery process alone consumed over 6 seconds—10% of our total test execution time after parallelization.</p><h3 id="strategic-optimization-with-testpaths">Strategic optimization with <code>testpaths</code></h3><p>Warehouse tests are all located in a single directory structure, making them
ideal candidates for a powerful <code>pytest</code> configuration option: <a href="https://docs.pytest.org/en/stable/reference/reference.html#confval-testpaths"><code>testpaths</code></a>.
This simple one-line change instructs <code>pytest</code> to look for tests only in the
specified directory, eliminating wasted effort scanning irrelevant paths:</p><figure><pre tabindex="0"><code data-lang="ini"><span><span><span>[tool.pytest.ini_options]</span>
</span></span><span><span><span>...</span>
</span></span><span><span><span>testpaths</span> <span>=</span> <span>[&#34;tests/&#34;]</span>
</span></span><span><span><span>...</span></span></span></code></pre><figcaption><span>Figure 6: Configuring pytest with testpaths.</span></figcaption></figure><figure><pre tabindex="0"><code data-lang="bash"><span><span>$ docker compose run --rm tests pytest --postgresql-host db --collect-only
</span></span><span><span><span># Before optimization:</span>
</span></span><span><span><span># 3,900+ tests collected in 7.84s</span>
</span></span><span><span>
</span></span><span><span><span># After optimization:</span>
</span></span><span><span><span># 3,900+ tests collected in 2.60s</span></span></span></code></pre><figcaption><span>Figure 7: Computing the test collection time.</span></figcaption></figure><p>This represents a 66% reduction in collection time.</p><h3 id="impact-analysis">Impact analysis</h3><p>This change, merged in <a href="https://github.com/pypi/warehouse/pull/16523">PR #16523</a>, reduced the the total
test time from 50 seconds to 48 seconds—not bad for a single configuration line
change.</p><p>While a 2-second improvement might seem modest compared to our parallelization
gains, it’s important to consider:</p><ul><li><strong>Cost-to-benefit ratio</strong>: This change required only a single line of configuration.</li><li><strong>Proportional impact</strong>: Collection represented 10% of our remaining test time.</li><li><strong>Cumulative effect</strong>: Every optimization compounds to create the overall improvement.</li></ul><p>This optimization applies to many Python projects. For maximum benefit, examine
your project structure and ensure <code>testpaths</code> points precisely to your test
directories without including unnecessary paths.</p><h2 id="removing-unnecessary-import-overhead">Removing unnecessary import overhead</h2><p>After implementing the previous optimizations, we turned to profiling import times
using Python’s <code>-X importtime</code> option. We were interested in how much time is
spent importing modules not used during the tests. Our analysis revealed that
the test suite spent significant time importing <code>ddtrace</code>, a module used
extensively in production but not during the tests.</p><figure><pre tabindex="0"><code data-lang="bash"><span><span><span># Before uninstall ddtrace</span>
</span></span><span><span>&gt; <span>time</span> pytest --help
</span></span><span><span>real    0m4.975s
</span></span><span><span>user    0m4.451s
</span></span><span><span>sys     0m0.515s
</span></span><span><span>
</span></span><span><span><span># After uninstall ddtrace</span>
</span></span><span><span>&gt; <span>time</span> pytest --help
</span></span><span><span>real    0m3.787s
</span></span><span><span>user    0m3.435s
</span></span><span><span>sys     0m0.346s</span></span></code></pre><figcaption><span>Figure 8: Time spent to load pytest with and without ddtrace.</span></figcaption></figure><table><thead><tr><th></th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td>Test execution time</td><td>29s</td><td>28s</td><td>3.4% reduction</td></tr></tbody></table><p>This simple change was merged in <a href="https://github.com/pypi/warehouse/pull/17232">PR #17232</a>, reducing our test
execution time from 29 seconds to 28 seconds—a modest but meaningful 3.4%
improvement. The key insight here is to identify dependencies that provide no
value during testing but incur significant startup costs.</p><h2 id="the-database-migration-squashing-experiment">The database migration squashing experiment</h2><p>As part of our systematic performance investigation, we analyzed the database
initialization phase to identify potential optimizations.</p><h3 id="quantifying-migration-overhead">Quantifying migration overhead</h3><p>Warehouse uses <a href="https://alembic.sqlalchemy.org/en/latest/">Alembic</a> to manage database migrations, with over 400 migrations
accumulated since 2015. During test initialization, each parallel test worker
must execute these migrations to establish a clean test database.</p><figure><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>time</span>
</span></span><span><span><span>import</span> <span>pathlib</span>
</span></span><span><span><span>import</span> <span>uuid</span>
</span></span><span><span>
</span></span><span><span><span>start</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>
</span></span><span><span><span>alembic</span><span>.</span><span>command</span><span>.</span><span>upgrade</span><span>(</span><span>cfg</span><span>.</span><span>alembic_config</span><span>(),</span> <span>&#34;head&#34;</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>end</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span> <span>-</span> <span>start</span>
</span></span><span><span><span>pathlib</span><span>.</span><span>Path</span><span>(</span><span>f</span><span>&#34;/tmp/migration-</span><span>{</span><span>uuid</span><span>.</span><span>uuid4</span><span>()</span><span>}</span><span>&#34;</span><span>)</span><span>.</span><span>write_text</span><span>(</span><span>f</span><span>&#34;</span><span>{</span><span>end</span><span>=}</span><span>\n</span><span>&#34;</span><span>)</span></span></span></code></pre><figcaption><span>Figure 9: A quick and dirty way to measure migration overhead.</span></figcaption></figure><p>Migrations take about 1s per worker, so that’s something we could further improve.</p><h3 id="prototyping-a-solution">Prototyping a solution</h3><p>While Alembic doesn’t officially support migration squashing, we developed a
proof-of-concept based on <a href="https://github.com/sqlalchemy/alembic/discussions/1259#discussioncomment-6163408">community feedback</a>. Our approach:</p><ol><li>Created a <em>squashed</em> migration representing the current schema state.</li><li>Implemented environment detection to choose between paths:<ul><li>Tests would use the single squashed migration</li><li>Production would continue using the full migration history</li></ul></li></ol><p>Our proof of concept further reduced test execution times by 13%.</p><h3 id="deciding-not-to-merge">Deciding not to merge</h3><p>After careful review, the project maintainers decided against merging
this change. The added complexity of managing squashed migrations and a second
migration path outweighed the time benefits.</p><p>This exploration illustrates a crucial principle of performance engineering: not
all optimizations that improve metrics should be implemented. A holistic
evaluation must also consider long-term maintenance costs. Sometimes, accepting a
performance overhead is the right architectural decision for the long-term
health of the project.</p><h2 id="test-performance-as-a-security-practice">Test performance as a security practice</h2><p>Optimizing test performance is not merely a developer convenience—it’s part of
a security mindset. Faster tests tighten feedback loops, encourage more
frequent testing, and enable developers to catch issues before they
reach production. Faster test time is a also a part of the security posture.</p><p>All the improvements described in this post were achieved without modifying test
logic or reducing coverage—a testament to how much performance can be gained
without security trade-offs.</p><h3 id="quick-wins-to-accelerate-your-test-suite">Quick wins to accelerate your test suite</h3><p>If you are looking to apply these techniques to your own test suites, here are
some advices on how to prioritize your optimization efforts for maximum impact.</p><ol><li>Parallelize your test suite: install <code>pytext-xdist</code> and
add <code>--numprocesses=auto</code> to your <code>pytest</code> configuration.</li><li>Optimize coverage instrumentation: if you’re on Python 3.12+, set
<code>export COVERAGE_CORE=sysmon</code> to use the lighter-weight monitoring API
in <code>coverage.py 7.4.0</code> and newer.</li><li>Speed up test discovery: Use <code>testpaths</code> in your <code>pytest</code> configuration to
focus test collection on only relevant directories and reduce collection
times.</li><li>Eliminate unnecessary imports: use <code>python -X importtime</code> to identify
slow-loading modules and remove them where possible.</li></ol><p>With a couple of highly targeted changes, you can achieve significant
improvements in your own test suites while maintaining their effectiveness as a
quality assurance tool.</p><h3 id="security-loves-speed">Security loves speed</h3><p>Fast tests enable developers to do the right thing. When your tests run in
seconds rather than minutes, security practices like <em>testing every change</em> and
<em>running the entire suite before merging</em> become realistic expectations rather than
aspirational guidelines. Your test suite is a frontline defense, but only if it
actually runs. Make it fast enough that no one thinks twice about running it.</p><h3 id="acknowledgments">Acknowledgments</h3><p>Warehouse is a community project, and we weren’t the only ones improving its
test suite. For instance, <a href="https://github.com/pypi/warehouse/pull/16295">PR #16295</a> and <a href="https://github.com/pypi/warehouse/pull/16384">PR #16384</a> by <a href="https://github.com/twm">@twm</a> also improved
performance by turning off file synchronization for <code>postgres</code> and caching DNS
requests.</p><p>This work would not have been possible without the broader community of open
source developers who maintain PyPI and the libraries that power it. In particular,
we would like to thank <a href="https://github.com/miketheman">@miketheman</a> for motivating and reviewing this work,
as well as for his own relentless improvements to Warehouse’s developer experience.
We also extend our sincere thanks to <a href="https://alpha-omega.dev/">Alpha-Omega</a> for funding this important work,
as well as for funding <a href="https://github.com/miketheman">@miketheman</a>’s own role as PyPI’s Security and Safety
Engineer.</p><p>Our optimizations also stand on the shoulders of projects like <code>pytest</code>, <code>pytest-xdist</code>,
and <code>coverage.py</code>, whose maintainers have invested countless hours in building robust,
performant foundations.</p></div></article></main></div></div></div></div>
  </body>
</html>
