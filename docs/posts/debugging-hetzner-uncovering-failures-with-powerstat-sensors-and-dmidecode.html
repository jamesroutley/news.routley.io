<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.ubicloud.com/blog/debugging-hetzner-uncovering-failures-with-powerstat-sensors-and-dmidecode">Original</a>
    <h1>Debugging Hetzner: Uncovering failures with powerstat, sensors, and dmidecode</h1>
    
    <div id="readability-page-1" class="page"><div data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" data-doc-height="1" role="banner"><div><p>EuroGPT Enterprise is open source, runs in Europe, and keeps your data private. <a href="https://www.ubicloud.com/use-cases/eurogpt-enterprise">Try it now</a></p></div><div><div><p><a href="https://www.ubicloud.com/"><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/64fe48116c52fe1a51e17279_ubicolud%20logo.png" loading="lazy" alt=""/></a></p></div></div></div><section><div><div><div id="w-node-decdb48f-56e8-4c35-c577-932285e9b439-0c072a00"><p>February 17, 2025 · 5 min read</p><div><p><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak.jpg" loading="lazy" sizes="40px" srcset="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak-p-500.jpg 500w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/669a6467d686c690fa7e7ac6_Burak.jpg 512w" alt="Burak Yucesoy"/></p><div><p>Burak Yucesoy</p><p>Principal Software Engineer</p></div></div><p>At Ubicloud, we build software that turns bare metal providers into cloud platforms. One of the providers we like is Hetzner because of their affordable and reliable servers.</p><div id="some-terminology"><h3>What Happened?</h3><p>Three weeks after purchasing our first AX162 server, one of the servers crashed. We checked the system logs and found NULL bytes. These usually mean there was an abrupt failure, like a power loss, which stopped the system from finishing its writing process. Hetzner performed a hardware check but found nothing unusual. A week later, we experienced another crash, followed by several more over the next few days.</p><ul role="list"><li>All crashes occurred on AX162 servers.</li><li><p>There were two types of crashes:</p><ul role="list"><li>The server comes back online after a manual restart.</li><li>The server wouldn&#39;t respond to restart requests or diagnostic codes sent by Hetzner engineers. Hetzner would replace the server in these cases.</li></ul></li><li>The servers usually run smoothly for an extended period. However, once a server experiences its first crash, further crashes become more likely. After the server experiences the first type of crash several times, it would eventually have the second type of crash and be replaced.</li></ul></div><div><h3 id="Red-Hat-Reference-Architecture">Initial Investigations</h3><p>We started testing different ideas to find out what caused the crashes.</p><div><h4>System Load</h4><p>We considered the possibility of increased load on the machine causing issues. The AX162 machines come with 96 vCPUs, and we had workloads that utilized all of them at the same time. Consistent high load, for example, could lead to increased temperatures and unexpected issues. However, when we reviewed the load levels at the times of crashes, we found several instances where crashes occurred even under low or no load.</p></div><div><h4>Temperature</h4><p>We wanted to check if there is a correlation between high temperatures and crashes. It is possible to collect the temperature of various components in the system with sensors command.</p><div><div><div><pre><code>$&gt; sensors
coretemp-isa-0000
Adapter: ISA adapter
Package id 0:  +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 0:        +45.0°C  (high = +100.0°C, crit = +100.0°C)
Core 4:        +46.0°C  (high = +100.0°C, crit = +100.0°C)
Core 8:        +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 9:        +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 10:       +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 11:       +51.0°C  (high = +100.0°C, crit = +100.0°C)
Core 12:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 13:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 14:       +49.0°C  (high = +100.0°C, crit = +100.0°C)
Core 15:       +49.0°C  (high = +100.0°C, crit = +100.0°C)</code></pre></div></div></div><p>We wrote a simple cron job to collect temperature data. When the servers crashed again, we checked the data. The temperature levels were not significantly higher than average at the time of the crashes.</p></div><div><h4>Faulty Components</h4><p>Commands like <span>lshw</span> and <span>dmidecode</span> are useful to gather information regarding hardware parts, including model and serial numbers.</p><div><div><div><pre><code>$&gt; dmidecode -t 2
# dmidecode 3.3
Getting SMBIOS data from sysfs.
SMBIOS 3.3.0 present.
Handle 0x0200, DMI type 2, 8 bytes
Base Board Information
        Manufacturer: Dell Inc.
        Product Name: 0H3K7P
        Version: A08
        Serial Number: .51R1H04.MXWSJ0039D004Z.</code></pre></div></div></div><p>We compared the components of AX162 servers that had crashed with those that hadn’t. We found no significant differences. We even checked how serial numbers increase, because we thought older components might fail more often. But crashes happened even in servers with the latest serial numbers.</p></div><div><h4>Power Consumption</h4><p>Power, rather than space, often limits data center expansion. To increase the number of machines under power constraints, data center operators usually cap power use per machine. However, this can cause motherboards to degrade more quickly. Although we didn’t know if Hetzner was limiting power consumption, the symptoms suggested this might be a factor. Repeated server crashes after a long period of stability usually mean the hardware is wearing out. We also eliminated all other hypotheses we had one by one, which only left power limiting as a strong hypothesis.</p><div><div><div><pre><code>$&gt; powerstat -R
  Time   User Nice  Sys  Idle   IO Run Ctxt/s  IRQ/s Fork Exec Exit  Watts
14:17:15  3.1  0.0  0.0  96.9  0.0   5    430   1593    0    0    0 166.54 
14:17:16  3.1  0.0  0.0  96.9  0.0   5    425   1638    1    1    1 166.51 
14:17:17  3.1  0.0  0.0  96.9  0.0   5    570   1737    0    0    0 166.50 
14:17:18  3.1  0.0  0.0  96.9  0.0   5    609   1787    0    0    0 166.48 
14:17:19  3.1  0.0  0.0  96.9  0.0   5    469   1662    0    0    0 166.49 
...
</code></pre></div></div></div><p>We then compared our measurements with the advertised amounts.</p><div><div><table><thead><tr><th>Model</th><th>Advertised Max. Power Consumption (Watt)</th><th>Measured Max. Power Consumption (Watt)</th></tr></thead><tbody><tr><td>AX161</td><td><p>147 (<a href="https://web.archive.org/web/20240223142827/https://www.hetzner.com/dedicated-rootserver/matrix-ax/" target="_blank">1</a>)</p></td><td>168</td></tr><tr><td>AX162</td><td><p>408 (<a href="https://web.archive.org/web/20240228172003/https://www.hetzner.com/dedicated-rootserver/matrix-ax/" target="_blank">2</a>)</p></td><td>266</td></tr><tr></tr></tbody></table></div></div><p>Based on these numbers, we suspected that Hetzner might indeed be limiting power usage.</p></div><div><h4>Data Collection on Crash Rates and Comparison</h4><p>Although we were observing an increased crash rate, we wanted to support this observation with data. A common way to measure hardware reliability is the Annualized Failure Rate (AFR). It&#39;s like the annual run rate, but for component failures. The formula for AFR is:</p><p><img src="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 97vw, (max-width: 991px) 94vw, (max-width: 1439px) 58vw, 748.703125px" srcset="https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-500.png 500w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-800.png 800w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR-p-1080.png 1080w, https://cdn.prod.website-files.com/64f9d9b4e737e7b37d4e39a4/67ab440611daa766d7af72f7_AFR.png 1186w" alt="afr calculation"/></p><p>AFR has its own limitations, but it is simple enough to give us a starting point, so we decided to use it. Here are our initial measurements:</p><div><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr></tr></tbody></table></div></div><p>Our observations indicated that AX162 servers are 16 times more likely to experience a failure compared to other models. The data also backed up our first finding: after a server crashes once, it is very likely to crash again. In fact, 80% of servers that crashed once had a second crash within 24 hours</p></div></div><div id="aws-firecracker"><h3>Observing Stability with New Hardware</h3><p>We submitted a detailed support ticket with the additional data on power limiting and annualized failure rates. Hetzner didn’t confirm or deny the possibility of power limiting but informed us that they had identified a defect in a batch of motherboards. They had recently received a new batch and recommended replacing the motherboards in our affected servers. Normally, replacing a big part of our fleet can disrupt customer workloads. However, we had already moved most critical tasks from the AX162 servers because they kept crashing, so replacing them was manageable.</p><div><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr><td>AX162 -v2</td><td><p>11</p></td><td>758</td><td>5.30</td></tr></tbody></table></div></div><p>AX162 servers with new motherboards crashed less frequently, but the crash rate was still high. After contacting Hetzner again, we learned of an even newer version of the motherboard with improved reliability. We migrated our servers to this latest version and began monitoring reliability.</p><div><div><table><thead><tr><th>Model</th><th>Total Failure Count</th><th>Total Days in Service</th><th>Annual Failure Rate</th></tr></thead><tbody><tr><td>AX161</td><td><p>11</p></td><td>3784</td><td>1.06</td></tr><tr><td>AX162</td><td><p>34</p></td><td>737</td><td>16.84</td></tr><tr><td>AX162 -v2</td><td><p>11</p></td><td>758</td><td>5.30</td></tr><tr><td>AX162 -v3</td><td><p>4</p></td><td>3738</td><td>0.39</td></tr></tbody></table></div></div></div><div id="ubicloud-compute"><h3>Process Improvements</h3><p>Adopting a new line of servers early on can come with unforeseen issues. We were quick to adopt the new servers because their specs were exciting. Also Hetzner’s decision to discontinue the AX161 model suggested the new line was production-ready. Looking back, waiting six months could have helped us avoid many issues. Early adopters usually find problems that get fixed later. Moving forward, we will make the following changes:</p><ul role="list"><li>We will conduct a thorough vetting of future server models.</li><li>We will introduce new hardware gradually, beginning with non-critical workloads.</li><li>We will add more bare metal providers to distribute the risk. In fact, we already support two more bare metal providers; Leaseweb and Latitude. We are also working on adding the fourth one.</li></ul><p>We hope our lessons offer valuable insights to others navigating similar issues. As we develop a solid, open-source alternative to traditional cloud providers, these experiences motivate us to keep improving. We aim to deliver cloud solutions that are both reliable and adaptable.</p></div></div></div></div></section></div>
  </body>
</html>
