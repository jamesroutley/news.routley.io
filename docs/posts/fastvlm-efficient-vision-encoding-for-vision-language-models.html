<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://machinelearning.apple.com/research/fast-vision-language-models">Original</a>
    <h1>FastVLM: Efficient Vision Encoding for Vision Language Models</h1>
    
    <div id="readability-page-1" class="page"><div role="main"><section></section><section><div><p>Vision Language Models (VLMs) enable visual understanding alongside textual inputs. They are typically built by passing visual tokens from a pretrained vision encoder to a pretrained Large Language Model (LLM) through a projection layer.  By leveraging the rich visual representations of the vision encoder and the world knowledge and reasoning capabilities of the LLM, VLMs can be useful for a wide range of applications, including accessibility assistants, UI navigation, robotics, and gaming.</p>
<p>VLM accuracy generally improves with higher input image resolution, creating a tradeoff between accuracy and efficiency. For many production use-cases, VLMs need to be both accurate and efficient to meet the low-latency demands of real-time applications and run on-device for privacy-preserving AI experiences.</p>
<p>In a <a href="https://machinelearning.apple.com/research/fastvlm-efficient-vision-encoding">paper</a> accepted to CVPR 2025, Apple ML researchers recently shared a new technique to address this challenge: FastVLM, a new type of VLM that significantly improves accuracy-latency trade-offs with a simple design. Leveraging a hybrid architecture visual encoder designed for high-resolution images, FastVLM delivers accurate, fast, and efficient visual query processing, making it suitable for powering real-time applications on-device. The inference code, model checkpoints, and an iOS/macOS demo app based on <a href="https://opensource.apple.com/projects/mlx/">MLX</a> are available <a href="https://github.com/apple/ml-fastvlm/" target="_blank" aria-label="here - Opens in a new window" rel="noopener nofollow">here</a>.</p>
<h2>Image Resolution and the Accuracy-Latency Tradeoff</h2>
<p>Generally, VLM accuracy improves with higher image resolution, especially for tasks needing detailed understanding, such as document analysis, UI recognition, or answering natural language queries about images. For example, in <a href="#figure1">Figure 1</a> below, we ask our VLM about the street sign visible in the image. On the left, the model receives a low-resolution image and cannot respond correctly. On the right, the VLM receives a high-resolution image and correctly identifies the traffic sign  which is a “Do Not Enter”.</p>
<figure id="figure1" aria-labelledby="figure-figure1-caption"><div></div><figcaption id="figure-figure1-caption" aria-hidden="false">Figure 1: Comparison of VLM performance with a low-resolution (left) and high-resolution (right) input image.</figcaption></figure>
<p>High resolution significantly increases time-to-first-token in VLMs. While using high-resolution images improves accuracy, it also reduces efficiency in two ways: 1) higher resolution images take longer for the vision encoder to process, and 2) the encoder creates more visual tokens, which increases the pre-filling time for the LLM. Both factors increase the time-to-first-token (TTFT), which is the sum of vision encoding time and LLM pre-filling time. As shown in <a href="#figure2">Figure 2</a> below, both vision encoding and LLM pre-filling times grow as image resolution increases, and at high resolutions, vision encoder latency becomes the dominant bottleneck. To address this, our research introduces FastVLM, a new vision language model that significantly improves efficiency without sacrificing accuracy.</p>
<figure id="figure2" aria-labelledby="figure-figure2-caption"><div><h3>Latency Breakdown</h3><h4 data-fb-ignore="ignore">For 1.5B VLM (fp16)</h4></div><figcaption id="figure-figure2-caption" aria-hidden="false">Figure 2: Vision latency dominates at high resolution. Breakdown of FastVLM’s time to the first token for different image resolutions. The vision encoder is FastViT-HD, and the LLM has 1.5B parameters.</figcaption></figure>
<h2>Hybrid Vision Encoders Deliver the Best Accuracy-Latency Tradeoff</h2>
<p>To identify which architecture delivers the best accuracy-latency tradeoff, we systematically compared existing pre-trained vision encoders with an experiment in which everything (training data, recipe, LLM, etc.) was kept the same, and only the vision encoder was changed. In <a href="#figure3">Figure 3</a> below, the x-axis shows TTFT, and the y-axis shows the average accuracy across different VLM tasks. We show two points for popular transformer-based encoders, <a href="https://arxiv.org/abs/2103.00020" target="_blank" aria-label="ViT-L/14 - Opens in a new window" rel="noopener nofollow">ViT-L/14</a>and <a href="https://arxiv.org/abs/2303.15343" target="_blank" aria-label="SigLIP-SO400 - Opens in a new window" rel="noopener nofollow">SigLIP-SO400</a>, pre-trained on image-text data at their native resolutions. We also show curves for <a href="https://arxiv.org/abs/2405.15738" target="_blank" aria-label="ConvNeXT - Opens in a new window" rel="noopener nofollow">ConvNeXT</a>(fully convolutional encoder) and FastViT (a hybrid encoder combining convolutional and transformer blocks) at various resolutions. FastViT, which is based on two of our previous works (<a href="https://machinelearning.apple.com/research/fastvit">FastViT</a>, ICCV 2023; and <a href="https://machinelearning.apple.com/research/mobileclip">MobileCLIP</a>, CVPR 2024), achieves the best accuracy-latency trade-off compared to other vision encoders—about 8 times smaller and 20 times faster than ViT-L/14.</p>
<figure id="figure3" aria-labelledby="figure-figure3-caption"><div><h3>Performance Comparison to FastViT</h3><div><div id="default-toggler-panel-0" role="tabpanel" aria-labelledby="default-toggler-0"></div></div></div><figcaption>Figure 3: Comparison of different vision architectures for visual encoding in VLMs. All vision
encoders are pre-trained with CLIP and trained using the same setup (dataset, recipe, LLM size).
The FastViT hybrid architecture achieves the best accuracy-latency trade-off. Avg-5 is the average
performance of the model on <a href="https://arxiv.org/abs/1902.09506" target="_blank" aria-label="GQA - Opens in a new window" rel="noopener nofollow">GQA</a>,
<a href="https://arxiv.org/abs/1904.08920" target="_blank" aria-label="TextVQA - Opens in a new window" rel="noopener nofollow">TextVQA</a>, <a href="https://arxiv.org/abs/2007.00398" target="_blank" aria-label="DocVQA - Opens in a new window" rel="noopener nofollow">DocVQA</a>,
<a href="https://arxiv.org/abs/2307.16125" target="_blank" aria-label="SeedBench - Opens in a new window" rel="noopener nofollow">SeedBench</a> and <a href="https://arxiv.org/abs/2305.10355" target="_blank" aria-label="POPE - Opens in a new window" rel="noopener nofollow">POPE</a>
benchmarks.</figcaption></figure>
<h2>FastViTHD: An Optimal Vision Encoder for VLMs</h2>
<p>While the FastViT hybrid backbone is a great choice for efficient VLMs, larger vision encoders are needed for improved accuracy on challenging tasks. Initially, we simply increased the size of each FastViT layer. However, this naive scaling made FastViT even less efficient than fully convolutional encoders at higher resolutions. To address this, we designed a new backbone, FastViTHD, specifically for high-resolution images. FastViTHD includes an extra stage compared to FastViT and is pre-trained using the MobileCLIP recipe to produce fewer but higher-quality visual tokens.</p>
<p>FastViTHD has better latency at high resolution images compared to FastViT, but to evaluate which is best in a VLM, we compared their performance when combined with LLMs of various sizes.  We evaluated different pairs of (image resolution, LLM size), and three LLMs with 0.5B, 1.5B, and 7B parameters (corresponding to each curve in <a href="#figure4">Figure 4</a> below) and pair it with vision backbone running at different resolutions.</p>
<p>As shown in <a href="#figure4">Figure 4</a>, using very high resolution images with a small LLM is not always the optimal choice; sometimes it is better to switch the LLM to a larger one instead of increasing the resolution. For each case, we show the Pareto-optimal curve with dashed lines, which shows the optimal (image resolution, LLM size) for a given runtime budget (TTFT here). Comparing Pareto-optimal curves, FastVLM (based on FastViTHD) offers a much better accuracy-latency trade-off than the FastViT-based model. It can be up to 3x faster for the same accuracy. Note that we had already shown that FastViT is significantly better than purely transformer-based or convolutional-based encoders.</p>
<figure id="figure4" aria-labelledby="figure-figure4-caption"><div><h3>Pareto-Optimal Curve by Model Size</h3><div><div id="default-toggler-panel-0" role="tabpanel" aria-labelledby="default-toggler-0"></div></div></div><figcaption id="figure-figure4-caption" aria-hidden="false">Figure 4: Comparison of FastViT and FastViT-HD backbones paired with LLMs of varying sizes and different image resolutions. Dashed lines show Pareto-optimal curve for both vision backbones. Note that the x-axis is in log scale. Avg-5 is the average performance of the model on GQA, TextVQA, DocVQA, SeedBench and POPE benchmarks.</figcaption></figure>
<h2>FastVLM: a New VLM Based on FastViTHD</h2>
<p>FastViTHD is a hybrid convolutional-transformer architecture comprising a convolutional stem, three convolutional stages, and two subsequent stages of transformer blocks. Each stage is preceded by a patch embedding layer that reduces the spatial dimensions of the input tensor by a factor of two. Using FastViTHD as the vision encoder, we built FastVLM, with a simple Multi-Layer Perceptron (MLP) module to project visual tokens to the embedding space of LLM, as shown in <a href="#figure5">Figure 5</a>.</p>
<figure id="figure5" aria-labelledby="figure-figure5-caption"><p><a href="https://mlr.cdn-apple.com/media/fast_vlm_fig5_cd0aa7ebc1.png" aria-label="Figure 5: Overview of the FastVLM architecture." tabindex="-1" target="_blank"><img src="https://mlr.cdn-apple.com/media/fast_vlm_fig5_cd0aa7ebc1.png" alt="Figure 5: Overview of the FastVLM architecture." loading="lazy"/></a></p><figcaption id="figure-figure5-caption" aria-hidden="false">Figure 5: Overview of the FastVLM architecture. FastVLM features our novel vision encoder, FastViT-HD, which incorporates multi-scale pooling, additional self-attention layers, and downsampling to generate 4× fewer tokens than FastViT and 16× fewer tokens than ViT-L/14 at a resolution of 336.</figcaption></figure>
<h2>FastVLM Outperforms Token Pruning and Merging Methods</h2>
<p>Prior research works in accelerating VLMs have employed complex merging or pruning techniques to reduce visual token counts to speed up LLM prefilling (and thus reduce the time to first token). As shown in <a href="#figure6">Figure 6</a> below, FastVLM achieves higher overall accuracy across different visual token counts (corresponding to different input resolutions) compared to these approaches. This is due to the high-quality visual tokens from its FastViTHD encoder, and because FastVLM does not require complicated token pruning or merging, it is simpler to deploy.</p>
<figure id="figure6" aria-labelledby="figure-figure6-caption"><div><h3>Performance Comparison</h3></div><figcaption id="figure-figure6-caption" aria-hidden="false">Figure 6:  Comparison of FastVLM’s average performance at different input image resolutions, corresponding to varying numbers of visual tokens, and different token pruning and merging methods. Y-axis is the average performance of the model on GQA, TextVQA, ScienceQA, SeedBench and POPE benchmarks.</figcaption></figure>
<h2>FastVLM and Dynamic Tiling</h2>
<p>As noted earlier, VLM accuracy increases with input resolution, particularly for tasks requiring understanding fine-grain details. Dynamic tiling (for example, in <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html" target="_blank" aria-label="AnyRes - Opens in a new window" rel="noopener nofollow">AnyRes</a>) is a popular way to handle very high-resolution images. This approach divides an image into smaller tiles, processes each tile separately through the vision encoder, and then sends all tokens to the LLM, as in <a href="#figure7">Figure 7</a> shown below.</p>
<figure id="figure7" aria-labelledby="figure-figure7-caption"><p><a href="https://mlr.cdn-apple.com/media/fast_vlm_fig7_530b99fbfc.png" aria-label="Figure 7: Dynamic tiling approach." tabindex="-1" target="_blank"><img src="https://mlr.cdn-apple.com/media/fast_vlm_fig7_530b99fbfc.png" alt="Figure 7: Dynamic tiling approach." loading="lazy"/></a></p><figcaption id="figure-figure7-caption" aria-hidden="false">Figure 7: AnyRes tiling encodes different sub-regions of the image (tiles) separately, along with a lower-resolution version of the full image, passing all tokens to the LLM.</figcaption></figure>
<p>Since FastVLM naturally handles high-resolution images, we explored if combining FastVLM with dynamic tiling improves its accuracy-latency tradeoff. <a href="#figure8">Figure 8</a> below shows that FastVLM without tiling (blue curve) achieves a better accuracy-latency trade-off compared to dynamic tiling (pink points), up to very high image resolutions, at which point combing FastVLM and AnyRes can be beneficial.</p>
<figure id="figure8" aria-labelledby="figure-figure8-caption"><div><h3>Tiling Effect on Performance</h3><div><div id="default-toggler-panel-0" role="tabpanel" aria-labelledby="default-toggler-0"></div></div></div><figcaption id="figure-figure8-caption" aria-hidden="false">Figure 8: Dynamic tiling (AnyRes) for FastVLM is only optimal at the highest resolution and when using fewer tiles (2×2). The tile grid size is specified in parentheses. Note that the x-axis is in log scale. Avg-5 is the average performance of the model on GQA, TextVQA, DocVQA, SeedBench and POPE benchmarks.</figcaption></figure>
<h2>FastVLM is Faster and More Accurate Than Popular VLMs of the Same Size</h2>
<p>Finally, we compared FastVLM with other popular VLMs. In <a href="#figure9">Figure 9</a> below, we show two curves for FastVLM: one with AnyRes (to achieve the highest accuracy) and one without tiling (for the best accuracy-latency tradeoff), each tested with three different LLM sizes. FastVLM is significantly faster and more accurate than popular models of the same size as indicated by the arrows: it is 85x faster than <a href="https://arxiv.org/abs/2408.03326" target="_blank" aria-label="LLava-OneVision - Opens in a new window" rel="noopener nofollow">LLava-OneVision</a>(0.5B LLM), 5.2x faster than <a href="https://arxiv.org/abs/2504.05299v1" target="_blank" aria-label="SmolVLM - Opens in a new window" rel="noopener nofollow">SmolVLM</a>(~0.5B LLM), and 21x faster than <a href="https://arxiv.org/abs/2406.16860" target="_blank" aria-label="Cambrian-1 - Opens in a new window" rel="noopener nofollow">Cambrian-1</a>(7B LLM).</p>
<figure id="figure9" aria-labelledby="figure-figure9-caption"><div><h3>Performance Comparison by Model Size</h3></div><figcaption id="figure-figure9-caption" aria-hidden="false">Figure 9: Comparison of FastVLM with popular VLMs. Arrows indicate comparisons with similarly sized VLMs, highlighting FastVLM’s superior accuracy and significantly faster performance. Y-axis is the average performance of the model on ChartQA, TextVQA, DocVQA, OCRBench, AI2D, MMMU and ScienceQA benchmarks.</figcaption></figure>
<p>To further show the on-device efficiency of FastVLM, we released an <a href="https://github.com/apple/ml-fastvlm/tree/main/app" target="_blank" aria-label="iOS/macOS demo app - Opens in a new window" rel="noopener nofollow">iOS/macOS demo app</a> based on MLX. <a href="#figure10">Figure 10</a> shows examples of FastVLM running locally on an iPhone GPU. FastVLM’s near real-time performance can enable new on-device features and experiences.</p>
<figure id="figure10" aria-labelledby="figure-figure10-caption"><video poster="https://mlr.cdn-apple.com/media/fast_vlm_fig10_poster_de35d7b380.png" autoplay="" loop="" muted="" controls="" data-testid="video"><source src="https://mlr.cdn-apple.com/video/fast_vlm_fig10_36ff61a056.mp4" type="video/mp4"/></video><figcaption id="figure-figure10-caption" aria-hidden="false">Figure 10: Demo app running FastVLM 0.5B model on iPhone 16 Pro. Time to first token is shown on the screen, highlighting near real-time performance.</figcaption></figure>
<h2>Conclusion</h2>
<p>By combining visual and textual understanding, VLMs can power a range of useful applications. Because the accuracy of these models generally corresponds to the resolution of input images, there has often been a performance tradeoff between accuracy and efficiency, which has limited the value of VLMs for applications that require both high accuracy and great efficiency.</p>
<p>FastVLM addresses this tradeoff by leveraging a hybrid-architecture vision encoder built for high-resolution images, FastViTHD. With a simple design, FastVLM outperforms prior approaches in both accuracy and efficiency, enabling on-device visual query processing suitable for real-time on-device applications.</p></div></section><section><div><div><div data-testid="card-fastvlm-efficient-vision-encoding"><div><p>Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing…</p><p><a href="https://machinelearning.apple.com/research/fastvlm-efficient-vision-encoding" aria-label="Read more about FastVLM: Efficient Vision encoding for Vision Language Models">Read more</a></p></div></div><div data-testid="card-on-device-scene-analysis"><div><p>Scene analysis is an integral core technology that powers many features and experiences in the Apple ecosystem. From visual content search to powerful memories marking special occasions in one’s life, outputs (or &#34;signals&#34;) produced by scene analysis are critical to how users interface with the photos on their devices. Deploying dedicated models for each of these individual features is inefficient as many of these models can benefit from sharing resources. We present how we developed Apple Neural Scene Analyzer (ANSA), a unified backbone to build and maintain scene analysis workflows in production. This was an important step towards enabling Apple to be among the first in the industry to deploy fully client-side scene analysis in 2016.</p><p><a href="https://machinelearning.apple.com/research/on-device-scene-analysis" aria-label="Read more about A Multi-Task Neural Architecture for On-Device Scene Analysis">Read more</a></p></div></div></div></div></section><section></section></div></div>
  </body>
</html>
