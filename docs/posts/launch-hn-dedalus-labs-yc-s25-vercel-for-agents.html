<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=45054040">Original</a>
    <h1>Launch HN: Dedalus Labs (YC S25) – Vercel for Agents</h1>
    
    <div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN! We are Windsor and Cathy of Dedalus Labs (<a href="https://www.dedaluslabs.ai/" rel="nofollow">https://www.dedaluslabs.ai/</a>), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools – local or hosted by us. No Dockerfiles or YAML configs required.</p><p>Here’s a demo: <a href="https://youtu.be/s2khf1Monho?si=yiWnZh5OP4HQcAwL&amp;t=11" rel="nofollow">https://youtu.be/s2khf1Monho?si=yiWnZh5OP4HQcAwL&amp;t=11</a></p><p>Last October, I was trying to build a stateful code execution sandbox in the cloud that LLMs could tool-call into. This was before MCP was released, and let’s just say it was super annoying to build… I was thinking to myself the entire time “Why can’t I just pass in `tools=code_execution` to the model and just have it…work?</p><p>Even with MCP, you’re stuck running local servers and handwiring API auth and formatting across OpenAI, Anthropic, Google, etc. before you can ship anything. Every change means redeploys, networking configs, and hours lost wrangling AWS. Hours of reading docs and wrestling with cloud setup is not what you want when building your product!</p><p>Dedalus simplifies this to just one API endpoint, so what used to take 2 weeks of setup can take 5 minutes. We allow you to upload streamable HTTP MCP servers to our platform. Once deployed, we offer OpenAI-compatible SDKs that you can drop into your codebase to use MCP-powered LLMs. The idea is to let anyone, anywhere, equip their LLMs with powerful tools for function calling.</p><p>The code you write looks something like this:</p><pre><code>  python
  client = Dedalus()
  runner = DedalusRunner(client)
  
  result = runner.run(
    input=prompt,
    tools=[tool_1, tool_2],
    mcp_servers=[&#34;author/server-1”, “author/server-2”],
    model=[&#34;openai/gpt-4.1”, “anthropic/claude-sonnet-4-20250514”],  # Defaults to first model in list
    stream=True,
  )
  stream_sync(result)  # Streams result, supports tool calling too
</code></pre><p>
Our docs start at <a href="https://docs.dedaluslabs.ai" rel="nofollow">https://docs.dedaluslabs.ai</a>. Here’s a simple Hello World example: <a href="https://docs.dedaluslabs.ai/examples/01-hello-world" rel="nofollow">https://docs.dedaluslabs.ai/examples/01-hello-world</a>. For basic tool execution, see <a href="https://docs.dedaluslabs.ai/examples/02-basic-tools" rel="nofollow">https://docs.dedaluslabs.ai/examples/02-basic-tools</a>. There are lots more examples on the site, including more complex ones like using the Open Meteo MCP to do weather forecasts: <a href="https://docs.dedaluslabs.ai/examples/use-case/weather-forecaster" rel="nofollow">https://docs.dedaluslabs.ai/examples/use-case/weather-foreca...</a>.</p><p>There are still a bunch of issues in the MCP landscape, no doubt. One big one is authentication (we joke that the “S” in MCP stands for “security”). MCP servers right now are expected to act as both the authentication server <i>and</i> the resource server. That is too much to ask of server writers. People just want to expose a resource endpoint and be done.</p><p>Still, we are bullish on MCP. Current shortcomings are not irrecoverable, and we expect future amendments to resolve them. We think that useful AI agents are bound to be habitual tool callers, and MCP is a pretty decent way to equip models with tools.</p><p>We aren’t <i>quite</i> yet at the stateful code execution sandbox that I wanted last October, but we’re getting there! Shipping secure and stateful MCP servers is high on our priority list, and we’ll be launching our auth solution next month. We’re also working on an MCP marketplace, so people can monetize their tools, while we handle billing and rev-share.</p><p>We’re big on open sourcing things and have these SDKs so far (MIT licensed):</p><p><a href="https://github.com/dedalus-labs/dedalus-sdk-python" rel="nofollow">https://github.com/dedalus-labs/dedalus-sdk-python</a></p><p><a href="https://github.com/dedalus-labs/dedalus-sdk-typescript" rel="nofollow">https://github.com/dedalus-labs/dedalus-sdk-typescript</a></p><p><a href="https://github.com/dedalus-labs/dedalus-sdk-go" rel="nofollow">https://github.com/dedalus-labs/dedalus-sdk-go</a></p><p><a href="https://github.com/dedalus-labs/dedalus-openapi" rel="nofollow">https://github.com/dedalus-labs/dedalus-openapi</a></p><p>We would love feedback on what you guys think are the biggest barriers that keep you from integrating MCP servers or using tool calling LLMs into your current workflow.</p><p>Thanks HN!</p></div></td></div></div>
  </body>
</html>
