<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://lwn.net/SubscriberLink/930705/4bbe9c26d7884277/">Original</a>
    <h1>Faster CPython at PyCon, part one</h1>
    
    <div id="readability-page-1" class="page"><div>
<!-- $Id: slink-none,v 1.2 2005-11-04 22:11:18 corbet Exp $ -->
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>
Two members of the Faster
CPython team, which was <a href="https://lwn.net/Articles/857754/">put together at Microsoft at the behest of Guido
van Rossum</a> to work on major performance improvements for CPython, came
to <a href="https://us.pycon.org/2023/">PyCon 2023</a> to report on what the
team has been working on—and its plans for the future.  <a href="https://peps.python.org/pep-0659/">PEP 659</a> (&#34;Specializing
Adaptive Interpreter&#34;) describes the foundation of the current work, some
of which
has already been released as part of Python 3.11.  Brandt Bucher, who
gave a 
popular <a href="https://lwn.net/Articles/893193/">talk on structural pattern matching</a>
at last year&#39;s PyCon, was up first, with a talk on what &#34;adaptive&#34; and
&#34;specializing&#34; mean in the context of Python, which we cover here in part
one. Mark Shannon, whose <a href="https://lwn.net/Articles/840248/">proposed plan
for performance improvements</a> in 2020 was a major impetus for this work,
presented on the past, present, and future of the Python performance
enhancements, 
which will be covered in part two.
</p>

<p>
Bucher started out by talking a bit about Python bytecode, but said that
developers do not need to know anything about it to get faster code:
&#34;just upgrade to 3.11 and you&#39;ll probably see a performance
improvement&#34;.  In order to show how the team sped up the interpreter,
though, it would help to look inside the Python code.  He put up an
example <tt>Point</tt> class that has two floating point attributes for its
position (<tt>x</tt>, <tt>y</tt>) and a single <tt>shifted()</tt>
method that
takes two offsets to 
apply to the position of an instance, and returns a new instance of the
class at the shifted position.  He focused on two lines from
the method:
</p><pre>    y = self.y + dy
    cls = type(self)
</pre><p>
The first line applies the offset for the </p><tt>y</tt><p> axis, while the second
gets a reference to the </p><tt>Point</tt><p> class (in preparation for returning
a new instance by calling </p><tt>cls(x, y)</tt><p>).  He used the </p><a href="https://docs.python.org/3/library/dis.html"><tt>dis</tt> module</a><p>
to disassemble the method; the relevant piece of bytecode is as follows:
</p><pre>    # part of the output of dis.dis(Point.shifted)
    LOAD_FAST      (dy)
    LOAD_FAST      (self)
    LOAD_ATTR      (y)
    BINARY_OP      (+)
    STORE_FAST     (y)

    LOAD_GLOBAL    (type)
    LOAD_FAST      (self)
    PRECALL        (1)
    CALL           (1)
    STORE_FAST     (cls)
</pre><p>
Some of the binary opcodes (and some operations, <a href="https://github.com/faster-cpython/ideas/discussions/210">such as method
calls</a>) have changed from those in Python 3.10 and earlier, so your
output may be different than what he showed.  The
bytecode is 
a set of instructions for the Python stack-based virtual machine.
In the first part above, the </p><tt>dy</tt><p> and </p><tt>self</tt><p> values are
pushed onto the stack,
then the </p><tt>LOAD_ATTR</tt><p> instruction retrieves the </p><tt>y</tt><p> attribute
from the value popped from the 
stack 
(</p><tt>self</tt><p>), then pushes it.  Next, the two values (</p><tt>self.y</tt><p> and
</p><tt>dy</tt><p>) on top of the stack 
are added (and the result is pushed) and that result is popped to be
stored in the variable </p><tt>y</tt><p>.  In the second part, </p><tt>type</tt><p> and
</p><tt>self</tt><p> are pushed, then </p><tt>PRECALL</tt><p>
and </p><tt>CALL</tt><p> are two separate steps to perform the </p><tt>type(self)</tt><p>
call; its result is popped to store into </p><tt>cls</tt><p>. 
</p>

<h4>Adaptive instructions</h4>

<p>
If that code is run more than a handful of times, Bucher said, it becomes a
candidate for optimization in 3.11.  The first step is something
called &#34;quickening&#34;, which the PEP calls &#34;<q>the process of replacing slow
instructions with faster variants</q>&#34;.  &#34;Superinstructions&#34; that combine
two related instructions into a single instruction are substituted into the
code.  For example, the first two <tt>LOAD_FAST</tt> instructions can be
replaced with a single one:
</p><pre>    LOAD_FAST__LOAD_FAST   (dy, self)
</pre><p>
It is a simple change that results in a real performance boost, he said.
The more interesting change is to replace some instructions with their
adaptive counterparts.
During the quickening process, five bytecodes are replaced with adaptive
versions:
</p><pre>    # part of the output of dis.dis(Point.shifted, adaptive=True)
    LOAD_FAST__LOAD_FAST   (dy, self)
    LOAD_ATTR_<b>ADAPTIVE</b>     (y)
    BINARY_OP_<b>ADAPTIVE</b>     (+)
    STORE_FAST             (y)

    LOAD_GLOBAL_<b>ADAPTIVE</b>   (type)
    LOAD_FAST              (self)
    PRECALL_<b>ADAPTIVE</b>       (1)
    CALL_<b>ADAPTIVE</b>          (1)
    STORE_FAST             (cls)
</pre>


<p>
The adaptive versions perform the same operation as their counterpart
except that they can specialize themselves depending on how they are being
used.  For example, loading an attribute is &#34;actually surprisingly complex&#34;,
because the attribute could come
from a number of places.  It could be a name from a module, a class
variable from a class, a 
method of a class, and so on, so the standard load-attribute code needs to
be prepared for those possibilities.  The simplest case is getting an
attribute from an instance dictionary, which is exactly what is being done here.
</p>

<p><a href="https://lwn.net/Articles/931195/">
<img src="https://static.lwn.net/images/2023/pycon-bucher-sm.png" alt="[Brandt Bucher]" title="Brandt Bucher" width="257" height="260"/>
</a></p><p>
The <tt>LOAD_ATTR_ADAPTIVE</tt> operation can recognize that it is in
the simple case and can ignore all of the rest of possibilities, so the
adaptive instruction changes to <tt>LOAD_ATTR_INSTANCE_VALUE</tt>, which is
a specialized instruction that only contains the code fast path for this
common case.  
</p>

<p>
The specialized instruction will then check to see if the
class is unchanged from the last time this attribute was accessed and if
the keys in 
the object&#39;s <tt>__dict__</tt> are the same.  Those much faster checks can
be done in lieu of two dictionary lookups (on the class dictionary for a
descriptor and on the object&#39;s <tt>__dict__</tt> for the name); &#34;dictionary
lookups are fast, but they still cost something&#34;, while the other two
checks are trivial. 
If the conditions
hold true, which is the normal situation, the code can simply return the
entry from the <tt>__dict__</tt> at the same offset that was used
previously; it does not need to do any hashing or collision resolution that
comes when doing a dictionary lookup.
</p>

<p>
If either of the checks fails, the code falls back to the regular load
attribute operation.  Python is a dynamic language and the new interpreter
needs to respect that, but the dynamic features are not being used all of
the time.  The idea is to not pay the cost of dynamism when it is not being
used, which is rather frequent in many Python programs.
</p>

<p>
Similarly, the <tt>BINARY_OP_ADAPTIVE</tt> instruction specializes to
<tt>BINARY_OP_ADD_FLOAT</tt> because floating-point values are being used.
That operation checks that both operands are of type <tt>float</tt> and
falls back to the (also surprisingly complex) machinery for an add
operation if they are not.  Otherwise, it can just add the raw floating
point values together in C.
</p>

<p>
Normally, when a global name is being loaded, it requires two dictionary
lookups;  for example, when <tt>type()</tt> is being loaded, the global
dictionary must be checked in case the function has been shadowed, if not
(which is likely), then it must be looked up in the builtins dictionary.
So <tt>LOAD_GLOBAL_ADAPTIVE</tt> takes a page from the attribute-loading
specialization to check if the global dictionary or builtins dictionary
have changed; if not, it simply grabs the value at the same index it used
before. 
</p>

<p>
It turns out that <tt>type()</tt> is called often enough that it gets its
own specialized bytecode.  It checks that the code for <tt>type()</tt> has not
changed (by way of a monkey patch or similar) and, if not, simply returns the
argument&#39;s class.  There is a call in the C API to do so and &#34;it&#39;s
much much cheaper than making the call&#34;.
</p>

<p>
If the specialized instructions fail their tests enough times, they will
revert back to the adaptive versions in order to change course.  For example,
if the <tt>Point</tt> class starts being used with integer values,
<tt>BINARY_OP_ADD_FLOAT</tt> will revert to <tt>BINARY_OP_ADAPTIVE</tt>,
which would replace itself with <tt>BINARY_OP_ADD_INT</tt> after a few
uses.  &#34;This is what 
we mean by specializing adaptive interpreter&#34;.
</p>

<p>
It may seem like a &#34;bunch of small localized changes&#34;—and it is—but they add
up to something substantial.  For example, the <tt>shifted()</tt> method is
nearly twice as fast in 3.11 versus 3.10.  He obviously chose
the example because it specializes well; a 2x performance increase is
probably at the upper end of what can be expected.  But it does show how
replacing the generalized versions of these Python bytecode operations with
their more specialized counterparts can lead to large improvements.
</p>

<p>
Bucher said that the various pieces of information that are being
stored (e.g. the index of a dictionary entry) are actually being placed
into the bytecode itself.  He called those &#34;inline caches&#34; and they can be
seen using the <tt>show_caches=True</tt> parameter to <tt>dis.dis()</tt>.
But Python programmers should not really need to look at the caches, or
even at the adaptive instructions, because all of that should not matter.
The idea is that the interpreter will still do what it always did, but it
can now do it faster in many cases.
</p>

<p>
For those who do want to dig under the covers more, though, Bucher
recommended his <a href="https://pypi.org/project/specialist/">specialist</a> tool.  Running
some code with <tt>specialist</tt> will generate a web page with the source
code of the program color-coded to show where the interpreter is optimizing
well—and where it is not.
</p>

<h4>Coming soon</h4>

<p>
He then shifted gears to looking at things that will be coming in
CPython 3.12.  To start with, the dedicated quickening step has been
eliminated and the superinstructions are simply emitted at compile time.
In addition, there will be only a single call instruction rather than the
two-step dance in 3.11.  
</p>

<p>
Instead of having separate adaptive versions of
the operations, the standard bytecodes implement the adaptivity. A bytecode
only needs to be executed twice in order to become fully specialized.  That
is measured on individual bytecodes, rather than a function as a whole, so
a loop will specialize immediately even if the surrounding code only gets
executed once.
</p>

<p>
The team has also added more specialized instructions. 
&#34;We have gotten better at specializing dynamic attribute accesses&#34;, so
there are
specializations for calling an object&#39;s <a href="https://docs.python.org/3/reference/datamodel.html#object.__getattribute__"><tt>__getattribute__()</tt></a>
and for loading properties specified using the <a href="https://docs.python.org/3/library/functions.html#property"><tt>property()</tt>
builtin</a>.   There are specializations for iterating over the four
most-common objects in a <tt>for</tt> loop: list, tuple, range, and
generator.  There is also a single specialization for
<tt>yield from</tt> in 
generators and <tt>await</tt> in coroutines.  There are two others in the
works that may make it into CPython 3.12.
</p>

<p>
Meanwhile, the inline caches are being reduced.  Having more cached
information makes the bytecode longer, which means longer jumps and more
memory use, so shrinking the amount of cached data is welcome.  The team
has been able to eliminate 
some cache entries or reorganize the caches to make some significant
reductions. All of that is coming in Python 3.12—and beyond.
</p>

<p>
Stay tuned for our coverage of Shannon&#39;s talk, which came on the second day
of the conference.  It will be the subject of part two, which is coming
soon to an LWN near you.
</p>

<p>
[I would like to thank LWN subscribers for supporting my travel to Salt
Lake City for PyCon.] 
</p></div></div>
  </body>
</html>
