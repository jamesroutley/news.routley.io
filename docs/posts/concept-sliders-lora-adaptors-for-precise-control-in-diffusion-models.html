<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://sliders.baulab.info/">Original</a>
    <h1>Concept sliders: LoRA adaptors for precise control in diffusion models</h1>
    
    <div id="readability-page-1" class="page"><div>

<figure>
  <center><img src="https://sliders.baulab.info/images/paper/main.png"/></center>
  <figcaption> Concept Sliders can be trained on text prompts, image pairs, or StyleGAN stylespace neurons to identify targeted concept directions in diffusion models for precise attribute control.
  </figcaption>
</figure>

  <h2> Why allow concept control in diffusion models? </h2>
    <p>The ability to precisely modulate semantic concepts during image generation and editing unlocks new frontiers of creative expression for artists utilizing text-to-image diffusion models. As evidenced by recent discourse within artistic communities, <a href"https:="" www.reddit.com="" r="" stablediffusion="" comments="" 11hemmj="" spent_hours_trying_to_understand_how_to_use="" "="">limitations in concept control</a> hinder creators&#39; capacity to fully manifest their vision through these generative technologies. It is also expressed that sometimes these models generate <a href"https:="" www.reddit.com="" r="" stablediffusion="" comments="" ym37xi="" can_an_ai_draw_hands="" "="">blurry, distorted images</a></p>
 <p>  Modifying prompts tends to drastically alter image structure, making fine-tuned tweaks to match artistic preferences difficult. For example, an artist may spend hours crafting a prompt to generate a compelling scene, but lack ability to softly adjust lighter concepts like a subject&#39;s precise age or a storm&#39;s ambience to realize their creative goals. More intuitive, fine-grained control over textual and visual attributes would empower artists to tweak generations for nuanced refinement.

In contrast, our Concept Sliders enables nuanced, continuous editing of visual attributes by identifying interpretable latent directions tied to specific concepts. By simply tuning the slider, artists gain finer-grained control over the generative process and can better shape outputs to match their artistic intentions.</p>

<h2> How to control concepts in a model? </h2>
  <p>We propose two types of training - using text prompts alone and using image pairs.
  For concepts that are hard to describe in text or concepts that are not understood by the model, we propose using the image pair training. We first discuss training for Textual Concept Sliders. </p>

  <h4>Textual Concept Sliders </h4>
  <p>The idea is simple but powerful: the pretrained model
  <SPAN size="+1"><em>P</em><sub><em>θ</em>*</sub>(<em>x</em>)</SPAN>
  has some pre-existing probability distribution to generate a concept
  <SPAN size="+1"><em>t</em></SPAN>,
  so our goal is to learn some low-rank updates to the layers of the model, there by forming a new model
  <SPAN size="+1"><em>P</em><sub><em>θ</em></sub>(<em>x</em>)</SPAN>
  that reshapes its
  distribution by reducing the probability of an attribute <SPAN size="+1"><em>c</em><sub><em>-</em></sub></SPAN> and boost the probability of attribute <SPAN size="+1"><em>c</em><sub><em>+</em></sub></SPAN> in an image when conditioned on <SPAN size="+1"><em>t</em></SPAN>,
  according to the original pretrained model:</p>
  <center><img src="https://sliders.baulab.info/images/paper/prob.png"/></center>
  <p>This is similar to the motivation behind <a href="https://proceedings.neurips.cc/paper/2020/file/49856ed476ad01fcff881d57e161d73f-Paper.pdf">compositional energy-based models</a>.  In diffusion it leads to a
  straightforward fine-tuning scheme that modifies the noise prediction model by
  subtracting a component and adding an component conditioned on the concept to target:
  </p>
  <figure>
    <center><img src="https://sliders.baulab.info/images/paper/score.png"/></center>
    <figcaption>Our Concept Slider fine tunes a low rank adaptor using the
      conditioned scores obtained from the original frozen Stable Diffusion (SD)
      model, to guide the output away from an attribute and towards another for a target concept being edited.
    </figcaption>
  </figure>

 <p>
  We query the frozen pre-trained model to predict the noise for the given target prompt, and control attribute prompts,
  then we train the edited model to guide it in the opposite direction using the
  ideas of <a href="https://arxiv.org/pdf/2207.12598.pdf">classifier-free guidance</a> at training time rather than inference.
  We find that fine-tuning the slider weights with this objective is very effective,
  producing a plug-and-play adaptor that directly controls the attributes for the target concept
  </p>

  <p>
  In practice, we notice that the concepts are entangled with each other. For instance, when we try to control the age attribute of a person, their race changes during inference. To avoid such undesired interference, we propose using a small set of preservation prompts to find the direction. Instead of defining the attribute with one pair of words alone, we define it by using multiple text compositions, finding a direction that changes the target attribute while holding other attribute-to-preserve constant.
      </p>
    <figure>
    <center><img src="https://sliders.baulab.info/images/paper/disent-score.png"/></center>
    <figcaption>To avoid undesired interference with the edits and allow precise control, we propose finding directions that preserve a set of protected concepts. For example instead of finding the direction from &#34;young person&#34; to &#34;old person&#34;, we find a direction that preserves race by particularly mentioning a set of protected attributes to preserve, like &#34;Asian young person&#34; to &#34;Asian old person&#34;.
    </figcaption>
  </figure>
  <figure>
    <center><img src="https://sliders.baulab.info/images/paper/directions.png"/></center>
    <figcaption>The arrow in the red is the original age direction trained using just &#34;old&#34; and &#34;young&#34; prompts. However, the direction is entangled with race. Instead we build a new disentangled direction (in blue) using multiple prompts to exclusively make the new vector invariant in those directions. For example, &#34;asian old person&#34; and &#34;asian young person&#34;. We do that with all the races for race disentanglement. 
    </figcaption>
  </figure>
  
  <h4>Visual Concept Sliders</h4>
  <p> To train sliders for concepts that can not be described with text prompts alone, we propose image pair based training. We particularly train the image based on gradient difference. The sliders learn to capture the visual concept through the contrast between image pairs (<SPAN size="+1"><em>x</em><sub><em>A</em></sub></SPAN> , <SPAN size="+1"><em>x</em><sub><em>B</em></sub></SPAN> ). Our training process optimizes the LORA applied in both the negative and positive directions. We shall write <SPAN size="+1"><em>ε</em><sub><em>θ</em></sub><sub><em>+</em></sub></SPAN>  for the application of positive LoRA and <SPAN size="+1"><em>ε</em><sub><em>θ</em></sub><sub><em>-</em></sub></SPAN> for the negative case. Then we minimize the following loss:
  </p><center><img src="https://sliders.baulab.info/images/paper/image-score.png"/></center>


<h2> Why are Concept Sliders Low Rank and Disentangled? </h2>
  <p>
  We introduce low-rank constraints to our sliders for two main reasons. First, for efficiency in parameter count and computation. Second to precisely capture the edit direction with better generalization. The disentangled formulation helps isolating the edit from unwanted attributes. We show an ablation study to better understand the role of these two main components of our work.
  </p>
  <figure>
    <center><img src="https://sliders.baulab.info/images/paper/ablations.png"/></center>
    <figcaption> The disentanglement objective helps avoid undesired attribute changes like change in race or gender when editing age. The low-rank constraint is also essential for enabling a precise edit.
    </figcaption>
  </figure>


<h2>Sliders to Improve Image Quality</h2>
<p>One of the most interesting aspects of a large-scale generative model such as Stable Diffusion XL is that, although their image output can often suffer from distortions such as warped or blurry objects, the parameters of the model contains a latent capability to generate higher-quality output with fewer distortions than produced by default. Concept Sliders can unlock these abilities by identifying low-rank parameter directions that repair common distortions.</p>

 <figure>
        <center><img src="https://sliders.baulab.info/images/paper/repair_all.png"/></center>
        <figcaption> The repair slider enables the model to generate images that are more realistic and undistorted. The parameters under the control of this slider help the model correct some of the flaws in their generated outputs like distorted humans and pets in (a, b), unnatural objects in (b, c, d), and blurry natural images in (b,c)
        </figcaption>
      </figure>

<figure>
        <center><img src="https://sliders.baulab.info/images/paper/repair_large.png"/></center>
        <figcaption> We demonstrate the effect of our &#34;repair&#34; slider on fine details: it improves the rendering of densely arranged objects, it straightens architectural lines, and it avoids blurring and distortions at the edges of complex shapes.
        </figcaption>
      </figure>

 <figure>
        <center><img src="https://sliders.baulab.info/images/paper/fixhands.png"/></center>
        <figcaption> We demonstrate a slider for fixing hands in stable diffusion. We find a direction to steer hands to be more realistic and away from &#34;poorly drawn hands&#34;.
        </figcaption>
      </figure>


<h2>Controlling Textual Concepts</h2>
      <p>We study Textual Concept Sliders; our paper includes more quantitative analysis comparing previous image editing methods and text-based prompt editing methods.
      </p>
      <figure>
        <center><img src="https://sliders.baulab.info/images/paper/text1.png"/></center>
        <figcaption> By using a small set of textual descriptions of the attributes to control, Concept Sliders can be trained to allow finegrained control of generated images during inference. By scaling the slider factor, users can control the strength of the edit.
        </figcaption>
      </figure>

      <figure>
        <center><img src="https://sliders.baulab.info/images/paper/text_detail.png"/></center>
        <figcaption> We show how several attributes of an image can be controlled using different sliders. We note that due to the low-rank formulation, the parameters are light weight, easy to share, and plug.
        </figcaption>
      </figure>

      <figure>
        <center><img src="https://sliders.baulab.info/images/weather.png"/></center>
        <figcaption> We demonstrate weather sliders for &#34;delightful&#34;, &#34;dark&#34;, &#34;tropical&#34;, and &#34;winter&#34;. For delightful, we notice that the model sometimes make the weather bright or adds festive decorations. For tropical, it adds tropical plants and trees. Finally, for winter, it adds snow.
        </figcaption>
      </figure>

      <figure>
        <center><img src="https://sliders.baulab.info/images/styles.png"/></center>
        <figcaption> We demonstrate style sliders for &#34;pixar&#34;, &#34;realistic details&#34;, &#34;clay&#34;, and &#34;sculpture&#34;.
        </figcaption>
      </figure>

<h2>Controlling Visual Concepts</h2>
      <p>Nunanced visual concepts can be controlled using our Visual Sliders; our paper shows comparisons with customization methods and some quantitative evaluations.
      </p>

      <figure>
        <center><img src="https://sliders.baulab.info/images/paper/imageslider.png"/></center>
        <figcaption> Sliders can be created for concepts that can not be described in words. These sliders are created by artists by using 6-8 pairs of images.
        </figcaption>
      </figure>
      <p>StyleGAN latents, especially the stylespace latents, can be transferred to Stable Diffusion. We collect images from styleGAN and train sliders on those images. We find that diffusion models can learn disentangled stylespace neuron behavior enabling artists to control nuanced attributes that are present in styleGAN.
      </p>
      <figure>
        <center><img src="https://sliders.baulab.info/images/paper/gan.png"/></center>
        <figcaption> Stylespace latents can be transferred from styleGAN to Stable Diffusion XL.
        </figcaption>
      </figure>
<h2>Composing Multiple Sliders</h2>

          <p>A key advantage of our low-rank slider directions is composability - users can combine multiple sliders for nuanced control rather than being limited to one concept at a time. By downloading interesting slider sets, users can adjust multiple knobs simultaneously to steer complex generations</p>
      <figure>
        <center><img src="https://sliders.baulab.info/images/paper/2dfood1.png"/></center>
        <figcaption> We show blending &#34;cooked&#34; and &#34;fine dining&#34; food sliders to traverse this 2D concept space. It is interesting how the model makes portion sizes small for &#34;fine dining&#34;.
        </figcaption>
      </figure>

      <figure>
        <center><img src="https://sliders.baulab.info/images/paper/multi-comp.png"/></center>
        <figcaption> We qualitatively show the effects of composing multiple sliders progressively up to 50 sliders at a time. We use far greater than 77 tokens (the current context limit of SDXL) to create these 50 sliders. This showcases the power of our method that allows control beyond what is possible through prompt-based methods alone.
        </figcaption>
      </figure>

<h2>How to cite</h2>

<p>The preprint can be cited as follows.
</p>

<div>

<div>
<p>
Rohit Gandikota, Joanna Materzyńska, Tingrui Zhou, Antonio Torralba, David Bau. &#34;<em>Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</em>&#34; arXiv preprint <nobr><a href="https://arxiv.org/abs/2311.12092">arXiv:2311.12092</a> (2023).
<!--arXiv preprint <nobr><a href="https://arxiv.org/abs/2303.07345">arXiv:2303.07345</a> (2023).</nobr>-->
</nobr></p><nobr>
</nobr></div><nobr>

<div>
<pre>

@article{gandikota2023sliders,
  title={Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models},
  author={Rohit Gandikota and Joanna Materzy\&#39;nska and Tingrui Zhou and Antonio Torralba and David Bau},
  journal={arXiv preprint arXiv:2311.12092},
  year={2023}
}
<!--
@inproceedings{gandikota2023erasing,
  title={Erasing Concepts from Diffusion Models},
  author={Rohit Gandikota and Joanna Materzy\'nska and Jaden Fiotto-Kaufman and David Bau},
  booktitle={Proceedings of the 2023 IEEE International Conference on Computer Vision},
  year={2023}
} -->
</pre>
</div>
</nobr></div><nobr>


</nobr></div></div>
  </body>
</html>
