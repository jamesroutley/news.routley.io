<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/ggml-org/llama.cpp/discussions/19759">Original</a>
    <h1>Ggml.ai joins Hugging Face to ensure the long-term progress of Local AI</h1>
    
    <div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="9509596" data-target-translation-type="discussion">
        <tr>
    <td>
        <h2 dir="auto">Announcement</h2>
<p dir="auto">We are happy to announce that <a href="https://ggml.ai" rel="nofollow">ggml.ai</a> (the founding team of <code>llama.cpp</code>) are joining <a href="https://huggingface.co" rel="nofollow">Hugging Face</a> in order to keep future AI truly open.</p>
<p dir="auto">Georgi and team are joining HF with the goal of scaling and supporting the <code>ggml</code>/<code>llama.cpp</code> community as Local AI continues to make exponential progress in the coming years.</p>
<h3 dir="auto">Summary / Key-points</h3>
<ul dir="auto">
<li>The <a href="https://github.com/ggml-org">ggml-org</a> projects remain open and community driven as always</li>
<li>The ggml team continues to lead, maintain and support full-time the <code>ggml</code> and <code>llama.cpp</code> libraries and related open-source projects</li>
<li>The new partnership ensures long-term sustainability of the projects and will help foster new opportunities for users and contributors</li>
<li>Additional focus will be dedicated on improving user experience and integration with the Hugging Face <a href="https://github.com/huggingface/transformers">transformers</a> library for improved model support</li>
</ul>
<h3 dir="auto">Why this change?</h3>
<p dir="auto">Since its foundation in 2023, the core mission of ggml.ai has continuously been to support the development and the adoption of the <code>ggml</code> machine learning library. Over the past 3 years, the small team behind the company has been doing its best to grow the open-source developer community and to help establish <code>ggml</code> as the definitive standard for efficient local AI inference. This was achieved through strong collaboration with individual contributors, as well as with partnerships with model providers and independent hardware vendors. As a result, today <code>llama.cpp</code> has become the fundamental building block in countless projects and products, enabling private and easily-accessible AI on consumer hardware.</p>
<p dir="auto">Throughout this development, Hugging Face stood out as the strongest and most supportive partner of this initiative. During the course of the last couple of years, HF engineers (notably <a data-hovercard-type="user" data-hovercard-url="/users/ngxson/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/ngxson">@ngxson</a> and <a data-hovercard-type="user" data-hovercard-url="/users/allozaur/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/allozaur">@allozaur</a>) have:</p>
<ul dir="auto">
<li>Contributed several core functionalities to <code>ggml</code> and <code>llama.cpp</code></li>
<li>Built a solid inference server with polished user interface</li>
<li>Introduced multi-modal support to <code>llama.cpp</code></li>
<li>Integrated <code>llama.cpp</code> into the Hugging Face Inference Endpoints</li>
<li>Improved compatibility of the GGUF file format with the Hugging Face platform</li>
<li>Implemented multiple model architectures into <code>llama.cpp</code></li>
<li>Helped <code>ggml</code> projects with general maintenance, PR reviews and more</li>
</ul>
<p dir="auto">The teamwork between our teams has always been smooth and efficient. Both sides, as well as the community, have benefited from these joint efforts. It only makes sense to formalize this collaboration and make it stronger in the future.</p>
<h3 dir="auto">What will change for <code>ggml</code>/<code>llama.cpp</code>, the open source project and the community?</h3>
<p dir="auto">Not much – Georgi and team will continue to dedicate 100% of their time maintaining <code>ggml</code>/<code>llama.cpp</code>. The community will continue to operate fully autonomously and make technical and architectural decisions as usual. Hugging Face is providing the project with long-term sustainable resources, improving the chances of the project to grow and thrive. The project will continue to be 100% open-source and community driven as it is now. Expect your favorite quants to be supported even faster once a model is released.</p>
<h3 dir="auto">Technical focus</h3>
<p dir="auto">Going forward, our joint efforts will be geared towards the following objectives:</p>
<ul dir="auto">
<li>
<p dir="auto">Towards seamless “single-click” integration with the <a href="https://github.com/huggingface/transformers">transformers</a> library</p>
</li>
<li>
<p dir="auto">Better packaging and user experience of ggml-based software</p>
</li>
</ul>
<h3 dir="auto">Long term vision</h3>
<p dir="auto">Our shared goal is to provide the building blocks to make open-source superintelligence accessible to the world over the coming years. We will achieve this together with the growing Local AI community, as we continue to build the ultimate inference stack that runs as efficiently as possible on our devices.</p>
    </td>
  </tr>

    </tbody>
  </div></div>
  </body>
</html>
