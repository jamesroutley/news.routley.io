<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/skyzh/tiny-llm">Original</a>
    <h1>Tiny-LLM â€“ a course of serving LLM on Apple Silicon for systems engineers</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://github.com/skyzh/tiny-llm/actions/workflows/main.yml"><img src="https://github.com/skyzh/tiny-llm/actions/workflows/main.yml/badge.svg" alt="CI (main)"/></a></p>
<p dir="auto">Still WIP and in very early stage. A tutorial on LLM serving using MLX for system engineers. The codebase
is solely (almost!) based on MLX array/matrix APIs without any high-level neural network APIs, so that we
can build the model serving infrastructure from scratch and dig into the optimizations.</p>
<p dir="auto">The goal is to learn the techniques behind efficiently serving a large language model (i.e., Qwen2 models).</p>
<p dir="auto">Why MLX: nowadays it&#39;s easier to get a macOS-based local development environment than setting up an NVIDIA GPU.</p>
<p dir="auto">Why Qwen2: this was the first LLM I&#39;ve interacted with -- it&#39;s the go-to example in the vllm documentation. I spent some time looking at the vllm source code and built some knowledge around it.</p>

<p dir="auto">The tiny-llm book is available at <a href="https://skyzh.github.io/tiny-llm/" rel="nofollow">https://skyzh.github.io/tiny-llm/</a>. You can follow the guide and start building.</p>

<p dir="auto">You may join skyzh&#39;s Discord server and study with the tiny-llm community.</p>
<p dir="auto"><a href="https://skyzh.dev/join/discord" rel="nofollow"><img src="https://github.com/skyzh/tiny-llm/raw/main/book/src/discord-badge.svg" alt="Join skyzh&#39;s Discord Server"/></a></p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Week + Chapter</th>
<th>Topic</th>
<th>Code</th>
<th>Test</th>
<th>Doc</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1</td>
<td>Attention</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>1.2</td>
<td>RoPE</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
</tr>
<tr>
<td>1.3</td>
<td>Grouped Query Attention</td>
<td>âœ…</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>1.4</td>
<td>RMSNorm and MLP</td>
<td>âœ…</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>1.5</td>
<td>Transformer Block</td>
<td>âœ…</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>1.6</td>
<td>Load the Model</td>
<td>âœ…</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>1.7</td>
<td>Generate Responses (aka Decoding)</td>
<td>âœ…</td>
<td>âœ…</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>2.1</td>
<td>KV Cache</td>
<td>âœ…</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>2.2</td>
<td>Quantized Matmul and Linear - CPU</td>
<td>âœ…</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>2.3</td>
<td>Quantized Matmul and Linear - GPU</td>
<td>âœ…</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>2.4</td>
<td>Flash Attention and Other Kernels</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>2.5</td>
<td>Continuous Batching</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>2.6</td>
<td>Speculative Decoding</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>2.7</td>
<td>Prompt/Prefix Cache</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>3.1</td>
<td>Paged Attention - Part 1</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>3.2</td>
<td>Paged Attention - Part 2</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>3.3</td>
<td>Prefill-Decode Separation</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>3.4</td>
<td>Scheduler</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>3.5</td>
<td>Parallelism</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>3.6</td>
<td>AI Agent</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
<tr>
<td>3.7</td>
<td>Streaming API Server</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
<td>ğŸš§</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Other topics not covered: quantized/compressed kv cache</p>

</article></div></div>
  </body>
</html>
