<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2602.11988">Original</a>
    <h1>Evaluating AGENTS.md: are they helpful for coding agents?</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
                
    <p><a href="https://arxiv.org/pdf/2602.11988">View PDF</a>
    <a href="https://arxiv.org/html/2602.11988v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>A widespread practice in software development is to tailor coding agents to repositories using context files, such as <a href="http://AGENTS.md" rel="external noopener nofollow">this http URL</a>, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents&#39; task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.
</blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Thibaud Gloaguen [<a href="https://arxiv.org/show-email/aeee8fcd/2602.11988" rel="nofollow">view email</a>]      </p></div></div>
  </body>
</html>
