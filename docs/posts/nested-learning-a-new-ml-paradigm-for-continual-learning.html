<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/">Original</a>
    <h1>Nested Learning: A new ML paradigm for continual learning</h1>
    
    <div id="readability-page-1" class="page"><div data-gt-publish-date="20251107">
                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="1guni">The last decade has seen incredible progress in machine learning (ML), primarily driven by powerful neural network architectures and the algorithms used to train them. However, despite the success of large language models (LLMs), a few fundamental challenges persist, especially around continual learning, the ability for a model to actively acquire new knowledge and skills over time without forgetting old ones.</p><p data-block-key="9an83">When it comes to continual learning and self-improvement, the human brain is the gold standard. It adapts through neuroplasticity — the remarkable capacity to change its structure in response to new experiences, memories, and learning. Without this ability, a person is limited to immediate context (like <a href="https://en.wikipedia.org/wiki/Anterograde_amnesia" target="_blank" rel="noopener noreferrer">anterograde amnesia</a>). We see a similar limitation in current LLMs: their knowledge is confined to either the immediate context of their input window or the static information that they learn during pre-training.</p><p data-block-key="fh441">The simple approach, continually updating a model&#39;s parameters with new data, often leads to “<a href="https://en.wikipedia.org/wiki/Catastrophic_interference" target="_blank" rel="noopener noreferrer">catastrophic forgetting</a>” (CF), where learning new tasks sacrifices proficiency on old tasks. Researchers traditionally combat CF through architectural tweaks or better optimization rules. However, for too long, we have treated the model&#39;s architecture (the network structure) and the optimization algorithm (the training rule) as two separate things, which prevents us from achieving a truly unified, efficient learning system.</p><p data-block-key="9ifb6">In our paper, “<a href="http://abehrouz.github.io/files/NL.pdf" target="_blank" rel="noopener noreferrer">Nested Learning: The Illusion of Deep Learning Architectures</a>”, published at <a href="https://neurips.cc/virtual/2025/poster/116123" target="_blank" rel="noopener noreferrer">NeurIPS 2025</a>, we introduce Nested Learning, which bridges this gap. Nested Learning treats a single ML model not as one continuous process, but as a system of interconnected, multi-level learning problems that are optimized simultaneously. We argue that the model&#39;s architecture and the rules used to train it (i.e., the optimization algorithm) are fundamentally the same concepts; they are just different &#34;levels&#34; of optimization, each with its own internal flow of information (&#34;context flow&#34;) and update rate. By recognizing this inherent structure, Nested Learning provides a new, previously invisible dimension for designing more capable AI, allowing us to build learning components with deeper computational depth, which ultimately helps solve issues like catastrophic forgetting.</p><p data-block-key="cp75j">We test and validate Nested Learning through a proof-of-concept, self-modifying architecture that we call “Hope”, which achieves superior performance in language modeling and demonstrates better long-context memory management than existing state-of-the-art models.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>The Nested Learning paradigm</h2>
            
        
        
    </p>



    <p data-block-key="1guni">Nested Learning reveals that a complex ML model is actually a set of coherent, interconnected optimization problems nested within each other or running in parallel. Each of these internal problems has its own <i>context flow</i> — its own distinct set of information from which it is trying to learn.</p><p data-block-key="aog9">This perspective implies that existing deep learning methods work by essentially <i>compressing</i> their internal context flows. More importantly, Nested Learning reveals a new dimension for designing models, allowing us to build learning components with deeper computational depth.</p><p data-block-key="28ofj">To illustrate this paradigm, we look at the concept of <a href="https://en.wikipedia.org/wiki/Associative_memory_(psychology)" target="_blank" rel="noopener noreferrer">associative memory</a> — the ability to map and recall one thing based on another (like recalling a name when you see a face).</p><ul><li data-block-key="565p0">We show that the training process itself, specifically the <a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank" rel="noopener noreferrer">backpropagation</a> process, can be modeled as an associative memory. The model learns to map a given data point to the value of its local error, which serves as a measure of how &#34;surprising&#34; or unexpected that data point was.</li><li data-block-key="d9pfu">Similarly, following previous studies (e.g., <a href="https://arxiv.org/pdf/2504.13173" target="_blank" rel="noopener noreferrer">Miras</a>), key architectural components, such as the <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)" target="_blank" rel="noopener noreferrer">attention mechanism in transformers</a>, can also be formalized as simple associative memory modules that learn the mapping between tokens in a sequence.</li></ul>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <p data-block-key="1guni">By defining an update frequency rate, i.e., how often each component&#39;s weights are adjusted, we can order these interconnected optimization problems into &#34;levels.&#34; This ordered set forms the heart of the Nested Learning paradigm.</p>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Putting Nested Learning to work</h2>
            
        
        
    </p>



    <p data-block-key="1guni">The Nested Learning perspective immediately gives us principled ways to improve existing algorithms and architectures:</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Deep optimizers</h3>
            
        
        
    </p>



    <p data-block-key="1guni">Since Nested Learning views optimizers (e.g., momentum-based optimizers) as associative memory modules, it allows us to apply principles from associative memory perspective to them. We observed that many standard optimizers rely on simple <a href="https://math.stackexchange.com/questions/689022/how-does-the-dot-product-determine-similarity" target="_blank" rel="noopener noreferrer">dot-product similarity</a> (a measure of how alike two vectors are by calculating the sum of the products of their corresponding components) whose update doesn&#39;t account for how different data samples relate to each other. By changing the underlying objective of the optimizer to a more standard loss metric, such as <a href="https://en.wikipedia.org/wiki/Ridge_regression" target="_blank" rel="noopener noreferrer">L2 regression loss</a> (a common loss function in regression tasks that quantifies the error by summing the squares of the differences between predicted and true values), we derive new formulations for core concepts like momentum, making them more resilient to imperfect data.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Continuum memory systems</h3>
            
        
        
    </p>



    <p data-block-key="1guni">In a standard Transformer, the sequence model acts as a short-term memory, holding the immediate context, while the <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network" target="_blank" rel="noopener noreferrer">feedforward neural networks</a> act as long-term memory, storing pre-training knowledge. The Nested Learning paradigm extends this concept into what we call a “continuum memory system” (CMS), where memory is seen as a spectrum of modules, each updating at a different, specific frequency rate. This creates a much richer and more effective memory system for continual learning.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Hope: A self-modifying architecture with continuum memory</h2>
            
        
        
    </p>



    <p data-block-key="1guni">As a proof-of-concept, we used Nested Learning principles to design Hope, a variant of the <a href="https://arxiv.org/abs/2501.00663" target="_blank" rel="noopener noreferrer">Titans</a> architecture. Titans architectures are long-term memory modules that prioritize memories based on how surprising they are. Despite their powerful memory management, they only have two levels of parameters update, resulting in a first-order in-context learning. Hope, however, is a self-modifying recurrent architecture that can take advantage of unbounded levels of in-context learning and also is augmented with CMS blocks to scale to larger context windows. It can essentially optimize its own memory through a <a href="https://people.idsia.ch/~juergen/selfref1992.pdf" target="_blank" rel="noopener noreferrer">self-referential process</a>, creating an architecture with infinite, looped learning levels.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Experiments</h3>
            
        
        
    </p>



    <p data-block-key="1guni">We conducted experiments to evaluate the effectiveness of our deep optimizers and the performance of Hope on language modeling, long-context reasoning, continual learning, and knowledge incorporation tasks. The full results are available in our <a href="http://abehrouz.github.io/files/NL.pdf" target="_blank" rel="noopener noreferrer">paper</a>.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h3>Results</h3>
            
        
        
    </p>



    <p data-block-key="1guni">Our experiments confirm the power of Nested Learning, the design of continuum memory systems, and self-modifying Titans.</p><p data-block-key="ca57m">On a diverse set of commonly used and public language modeling and common-sense reasoning tasks, the Hope architecture demonstrates lower perplexity and higher accuracy compared to modern recurrent models and standard transformers.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <p data-block-key="1guni">Hope showcases superior memory management in long-context Needle-In-Haystack (NIAH) downstream tasks, proving that the CMSs offer a more efficient and effective way to handle extended sequences of information.</p>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Conclusion</h2>
            
        
        
    </p>



    <p data-block-key="1guni">The Nested Learning paradigm represents a step forward in our understanding of deep learning. By treating architecture and optimization as a single, coherent system of nested optimization problems, we unlock a new dimension for design, stacking multiple levels. The resulting models, like the Hope architecture, show that a principled approach to unifying these elements can lead to more expressive, capable, and efficient learning algorithms.</p><p data-block-key="ejnn7">We believe the Nested Learning paradigm offers a robust foundation for closing the gap between the limited, forgetting nature of current LLMs and the remarkable continual learning abilities of the human brain. We are excited for the research community to explore this new dimension and help us build the next generation of self-improving AI.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    


    <p>
        
            
                <h2>Acknowledgements</h2>
            
        
        
    </p>



    <p data-block-key="1guni"><i>This research was conducted by Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. We thank Praneeth Kacham and Corinna Cortes for reviewing the work and their valuable suggestions. We also thank Yuan Deng and Zeman Li. Finally, we thank Mark Simborg and Kimberly Schwede for their help in crafting this blog post.</i></p>
</div>

    </div>
</section>

                    
                </div></div>
  </body>
</html>
