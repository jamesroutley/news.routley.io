<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.matthieulc.com/posts/shoggoth-mini">Original</a>
    <h1>Show HN: Shoggoth Mini – A soft tentacle robot powered by GPT-4o and RL</h1>
    
    <div id="readability-page-1" class="page"><div id="content"><article><p>Over the past year, robotics has been catching up with the LLM era. Pi’s π0.5 <a href="https://www.physicalintelligence.company/blog/pi05">can clean unseen homes</a>. Tesla’s Optimus can follow <a href="https://x.com/Tesla_Optimus/status/1925047336256078302">natural language cooking instructions</a>. These systems are extremely impressive, but they feel stuck in a utilitarian mindset of robotic appliances. For these future robots to live with us, they must be expressive. Expressiveness communicates internal state such as intent, attention, and confidence. Beyond its functional utility as a communication channel, expressiveness makes interactions feel natural. Without it, you get the textbook <a href="https://en.wikipedia.org/wiki/Uncanny_valley">uncanny valley</a> effect.</p><p>Earlier this year, I came across Apple’s <a href="https://arxiv.org/pdf/2501.12493">ELEGNT</a> paper, which frames this idea rigorously through a Pixar-like lamp to show how posture and timing alone can convey intention. Around the same time, I discovered <a href="https://arxiv.org/pdf/2303.09861">SpiRobs</a>, a soft tentacle robot that feels oddly alive with just simple movements. One system was carefully designed to express intent while the other just moved, yet somehow felt like it had intent. That difference was interesting. I started building Shoggoth Mini as a way to explore it more directly. Not with a clear goal, but to see what would happen if I pushed embodiment into stranger territory. This post retraces that process, the happy accidents, and what I learned about building robots.</p><div><div><p><img src="https://www.matthieulc.com/posts/shoggoth-mini/elegnt.png"/></p></div><div><p><img src="https://www.matthieulc.com/posts/shoggoth-mini/spirobs.png"/></p></div></div><p>The first challenge was creating a testbed to explore the control of SpiRobs. I started very simple: a plate to hold three motors, and a dome to lift the tentacle above them. This setup wasn’t meant to be the final design, only a platform for quick experimentation. However, halfway through 3D printing, I ran out of black filament and had to finish the dome in grey. This made it look like the dome had a mouth. When my flatmate saw it sitting on my desk, he grabbed a marker and drew some eyes. It looked good: cute, weird, slightly unsettling. I used ChatGPT to explore renders, and decided that this accident would become the form factor.</p><div><p><img src="https://www.matthieulc.com/posts/shoggoth-mini/first_version.png"/></p><p><img src="https://www.matthieulc.com/posts/shoggoth-mini/chatgpt_render.png"/></p></div><p>Later, I mounted stereo cameras on the dome to track the tentacle. Robot eyes are eerie. You keep expecting movement, but nothing ever happens. That prediction error <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6556781/">focuses attention even more</a>.</p><p><img src="https://www.matthieulc.com/posts/shoggoth-mini/early_eyes.jpg"/></p><p>The original open-spool design relied on constant cable tension, but any slight perturbation (such as testing a buggy new policy) would make the cables leave the spool and tangle around the motor shafts. The process to fix it required untying the knot at the tip holding the cables together, and dismantling the whole robot. Adding simple spool covers eliminated most tangles and made iteration dramatically faster.</p><p>Another key step was adding a calibration script and pre-rolling extra wire length. This made it possible to:</p><ul><li>Unroll and reroll the cables to open the robot without having to untie the tip knot, speeding up iteration dramatically</li><li>Calibrate cable tension precisely and as often as needed</li><li>Give control policies slack to work with during motion</li></ul><div><video controls="" loop="" autoplay="" muted="">
<source src="/posts/shoggoth-mini/calibration.mp4" type="video/mp4"/></video><p>Rolling the cables back after working on Shoggoth Mini&#39;s insides</p></div><p>Finally, as you can see in the video, the standard 3-cable SpiRobs design sags under its own weight. This makes consistent behavior hard to reproduce. I had to thicken the spine just enough to prevent sag, but not so much that it would deform permanently under high load.</p><p><img src="https://www.matthieulc.com/posts/shoggoth-mini/spirobs_spine.png"/></p><p>You can explore the current CAD assembly here, with all STL files for 3D printing included in the <a href="https://github.com/mlecauchois/shoggoth-mini">repo</a>.</p><p>This is an interactive 3D model - click and drag to rotate, scroll to zoom</p><p>With the hardware ready, the next step was to feel how the tentacle moved. To simplify control, I reduced the tentacle’s three tendon lengths (a 3D space) down to two intuitive dimensions you can manipulate with a trackpad.</p><p>Concretely, each of the three tendons has a principal pulling direction in the 2D plane, forming a triangular basis that sums to zero. By projecting the 2D cursor control vector onto each tendon’s principal axis, you compute how much each tendon should shorten or lengthen to align with the desired direction.</p><p><img src="https://www.matthieulc.com/posts/shoggoth-mini/projection.png"/></p><p>This projection is linear:</p><p>$$
s_i = \mathbf{v}_i^\top \mathbf{c}
$$</p><p>where:</p><ul><li>$\mathbf{c}$ is the 2D cursor vector,</li><li>$\mathbf{v}_i$ is the principal axis of tendon $i$.</li></ul><p>Positive $s_i$ means shortening the tendon; negative means lengthening it. In practice, the cursor input is normalized to keep the motor commands in a reasonable range.</p><p>While this 2D mapping doesn’t expose the tentacle’s full configuration space (there are internal shapes it cannot reach), it is intuitive. Anyone can immediately move the tentacle by dragging on a trackpad, seeing the tip follow the cursor in the same direction.</p><p>Unexpectedly, this simple 2D-to-3D mapping became the backbone of the entire system. Later, all automated control policies, from hardcoded primitives to reinforcement learning, reused the same projection layer to output actions.</p><p>Trackpad control</p></article></div></div>
  </body>
</html>
