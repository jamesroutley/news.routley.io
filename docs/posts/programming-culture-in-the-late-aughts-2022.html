<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://morepablo.com/2022/11/programming-culture-in-the-late-aughts.html">Original</a>
    <h1>Programming culture in the late aughts (2022)</h1>
    
    <div id="readability-page-1" class="page"><div>
      
      <article>
        <header>
          
          <p>Monday, November 28, 2022 :: Tagged under: <a href="https://morepablo.com/tags/engineering.html">engineering</a>. ‚è∞ 8 minutes.</p>
          </header>
            
        <p><small>üéµ <em>The song for this post is <a href="https://www.youtube.com/watch?v=_Ij5JaEIcPc">an accordion and guitar cover of Bella Ciao</a>, by
Tobias Kemerich and Betto Malheiros.</em> üéµ</small></p>
<p><a href="https://medium.com/@ssg/how-is-computer-programming-different-today-than-20-years-ago-9d0154d1b6ce">Sedat Kopanoglu</a> did a write-up trying to answer the question &#34;how is
programming different than it was 20 years ago?&#34;, and I thought I&#39;d take a crack
at it as of ~12 years ago, when I entered the workforce as a professional
programmer. Some of this bleeds into earlier trends than 2010, since I was
still studying/eating as much programming material as I could in my last few
years in school. I agree with Sedat in a few places, you can see the <a href="https://lobste.rs/s/czzkxt/how_is_computer_programming_different">lobste.rs
discussion here</a>.</p>
<div>
<p><img src="https://morepablo.com/img/2022/11/saurya_graduation.jpg" alt="Saurya and I, looking very young, for graduation in 2010"/></p><p>My best
friend and roommate <a href="https://saurya.com/blog/">Saurya</a> and I when we graduated, in 2010. I don&#39;t know what other photos to post for this so I&#39;ll be indulgent and post
retro photos from The Time.</p>
</div>
<hr/>
<p><strong>Multicore still doesn&#39;t matter, while &#34;async&#34; somehow does.</strong> 12 years ago
everyone was wondering how we&#39;d program on multicore CPUs, since Moore&#39;s Law
days of automatic &#34;transister count&#34; increases were coming to an end. There was
a fair bit of academic ink spilled over how we were going to keep systems
performant; people were calling languages without great multicore like Python,
Ruby, PHP, and OCaml as having limited lifetimes vs. those that had better
stories for it (Java was a player, though Erlang was originally revived solving
<em>this</em> problem instead of its high-uptime or distribution stories).</p>
<p>Big evidence of this war was the differences in how to achieve concurrency
&#34;scalably&#34;: a major consensus was that &#34;locks don&#39;t scale,&#34; so everyone was
looking at ways around locks. <a href="http://dtrace.org/blogs/bmc/2008/11/03/concurrencys-shysters/">Brian Cantrill had a great rebuttal to some of
these techniques</a> that I don&#39;t fully agree with, but that article has a ton
of pointers to what other people were pitching.</p>
<p>&#34;Few problems are like raytracing&#34; which was considered &#34;embarassingly
parallel.&#34; At the time the worry was on using multiple cores on individual
workloads. You can Google &#34;lock free data structures&#34; to see the kind
of thing people were writing whitepapers on.</p>
<p>Oddly, while parallelism didn&#39;t turn out to matter much, concurrency <em>did</em>:
datacenters, cloud compute, explosion of clients via smartphones, and the
realities of cell phone networks meant that we still had to turn our computing
models inside out, but for the different use case of minimizing latency waits
from blocking IO. That said, the biggest use case for non-mobile computing
turned out to be something like raytracing: handling multiple, <em>independent</em>
requests. Doesn&#39;t require parallelism, strictly speaking: Node and <code>nginx</code> were
built on the wonders of a single-threaded event loop when they don&#39;t have to
share state.</p>
<hr/>
<p><strong>&#34;Full-stack&#34; now includes a lot more pancakes.</strong> We&#39;ve added layers and layers
of abstractions, tools, and professionalism into every stage of &#34;hosting a
site.&#34; This is in part due to real factors related to adding hundreds of
millions of people to the internet, giving them always-on supercomputers they
check at several dozen times a day, common infrastructure and cloud platforms
providing solutions for minutiae that used to gate out amateur players, and the
explosion of VC/zero-interest capital in the last decade meaning a ton of
companies were playing the &#34;eat the world or die&#34; game.</p>
<p>It <em>also</em>, in my opinion, is a result of the incentives on developers to brand
themselves as expensive professionals by creating or becoming an authority on
the Next Hot Thing, the predilection of people generally to believe simple
narratives without measuring (<a href="https://www.youtube.com/watch?v=b2F-DItXtZs">&#34;X is not web scale! Someone loudly said it on
the internet!&#34;</a>), programmers naturally liking problems for their own sake
and solving them even when they didn&#39;t have the problem in the first place, and
companies acting irrationally based on aspirations that don&#39;t reflect their
current reality (FAANG envy; think of how many Americans <a href="https://signalproblems.substack.com/p/what-i-learned-about-cars">buy giant cars who
don&#39;t need giant cars.</a>)</p>
<p>12 years ago, even without AWS, you could get a Linode, host an app on its
naked IP, and not made to feel too much like you weren&#39;t &#34;serious&#34; about hosting
your app. These days you almost always need to know a fair bit of Docker and
Docker Compose, a lot of people want to Kubernetes, ELBs got replaced by ALBs +
NLBs which you gotta manage in VPCs which you gotta manage through Security
Groups and your traffic gets routed through CDNs. Logs are <em>structured</em> and
passed to a SaaS which will have a custom search syntax that takes tens of
seconds to search them poorly, and they&#39;ll get lost in the noise of all those
components. You&#39;ll spend a non-trivial amount of time devising systems for
tracing requests through all these components.</p>
<hr/>
<p><strong>Rich client experiences became their own tech industry.</strong> A similar thing to
‚òùÔ∏è happened in the client world. Before, JavaScript wasn&#39;t as mature, so jQuery was
<em>cutting edge</em> and rich-experience web clients were only possible after a lot of
custom work. Examples were Google&#39;s &#34;rich Gmail&#34; and Google Wave. We all knew it
was conceptually possible to make projects that didn&#39;t follow the &#34;webpage -&gt;
link to another webpage&#34; paradigm, and act more like &#34;apps,&#34; but the amount of
JavaScript was unseemly, and server-side HTML was deeply ingrained in our
brains.</p>
<p>I attribute this to a few factors, many being in common with the ops explosion I
listed above (developer nature and incentives, explosion of clients, VC plays)
but I&#39;ll give special mention to the iPhone, growing influence of Apple and its
product/design culture, and subsequent smartphone proliferation.</p>
<hr/>
<div>
<p><img src="https://morepablo.com/img/2022/11/freshman_year.jpg" alt="Me at freshman year, in a dorm and bad lighting."/></p><p>Five years before <em>that</em>, freshman year in 2005. I was probably 3 lectures into <a href="https://cs.brown.edu/courses/cs015/">CS15,</a> which challenged me harder than any class I&#39;d ever taken before. I thought &#34;maybe‚Ä¶ I could study computers!&#34; I was reeling from disappointment that I probably couldn&#39;t take any real theatre classes that year, due to weirdness of Brown&#39;s program.</p>
</div>
<p><strong>Death of the protocol, then the API.</strong> Protocols were a lot more popular: yes
we had chat products like AIM and MSN and ICQ, but even with them it was way
morepossible to use <a href="https://adium.im/">Adium</a> or <a href="https://pidgin.im/">Pidgin</a> to speak to all of them. Remember:
email is still, somehow, a protocol that anyone can speak! When I was at Google
in 2012, one of the TGIF topics was the loss of support of <a href="https://en.wikipedia.org/wiki/CalDAV">CalDAV.</a>
Generally speaking, making your app speak via a protocol was a mark of good
computer citizenship, since it was understood that custom clients and giving
data to users was a path forward.</p>
<p>Eventually this got replaced by companies and proprietary APIs. Part of this was
that we were going beyond the &#34;computer basics&#34; world of chat and documents, so
there was less predecessors for protocols (what was the interoperable protocol
for maps? tweets? photo albums?) which led to an explosion of <a href="https://blog.axway.com/learning-center/apis/api-management/api-mashup">mashups</a> and
possibility. Protocols allowed for custom clients <em>and</em> servers; APIs kept
the servers and data extremely proprietary, but you could at least have
custom clients. Anil Dash wrote a lot about this time period with <a href="https://anildash.com/2012/12/13/the_web_we_lost/"><em>The Web We
Lost</em></a>.</p>
<p>Over time, API waned in power and popularity too. I suspect the main reason is
as tech companies became less about customer empowerment and more about being
financial instruments, it simply didn&#39;t make sense to have an API unless you had
to: it was expensive to run and maintain, it exposed your data, more clients
limited your control (Twitter clients, for example, didn&#39;t serve ads) and
increased your liability (Facebook and Cambridge Analytica).</p>
<p>Protocols also suffered extra death because they have trouble keeping up with
innovation: if Slack wants to add a feature, they can just add it. If you
want to add the same feature to IRC or XMPP, you have everyone else complaining
about how you did it, not updating their clients to add it, and locking out your
users.</p>
<hr/>
<p><strong>OOP vs. FP; Language semantics vs. language ecosystems.</strong> A bit of an
extension of the &#34;concurrency, parallelism, vs. async&#34; point above, FP vs. OOP
felt more like a battle that was being actively worked out, and people tended to
argue over language features and semantics instead of ecosystems and hiring
markets. <a href="http://www.paulgraham.com/avg.html">Beating the Averages</a> still had a ton of people believing That One
Great Language could be the game-changer, and there hadn&#39;t been enough
high-profile tech startups to succeed or fail to solidify a solid story around
language choice or paradigm.</p>
<p>The story that solidified is: <a href="https://mcfunley.com/choose-boring-technology">Use Boring Technology</a>. Use Java (or Python
or Ruby or, if you really must, JavaScript. I feel like Golang has now ascended
here). At the time, there was a lot more belief that A Different Language
might be good to use commercially; today that&#39;s largely viewed as a sign of poor
engineering leadership.</p>
<p>(Cards on the table: I think this narrative, while conventional, is bollocks.
ITA was written in Common Lisp. WhatsApp and Discord are written in Erlang and
Elixir. Twitter did great with Scala. And Ruby&#39;s weird semantics are credited
for how one person as able to use it to make a world-changing framework;
to call Ruby a &#34;boring&#34; choice now is a testament to how successful the right
weird tech can be! &#34;Boring&#34; has a place, but I don&#39;t think that&#39;s everywhere. <a href="https://morepablo.com/2022/04/against-boring.html">I
write more about this here</a>).</p>
<p>What changed this was a whole lot of things, IMO:</p>
<ul>
<li>
<p>Many &#34;weird tech&#34; shops died. As did many non-weird-tech shops; it&#39;s startups
after all. But when weird tech shops died, there was an easy scapegoat.
Mundane tech or unambitious tech strategy never gets pointed to when those
startups die.</p>
</li>
<li>
<p>Professionalization, for lack of a better word: tech workers became a flatter
workforce rather than a couple thousand creative weirdos. Imagine if all music
went from from high school garage bands (each with a name, personality, and
&#34;sound&#34;) to a flatter set of interchangeable studio musicians who graduated
from music schools with a structured curriculum. The latter probably produces
more predictable, very useful music, but certainly is less interested in
expressing themselves, and will take fewer risks.</p>
</li>
<li>
<p>Anecdotal, and I say this without judgement: a larger percentage of us
(because of ‚òùÔ∏è) just don&#39;t care to think about computers much. It&#39;s a bit
ungenerous, but it&#39;s my observation: for a lot of professionals in computing,
if you have to think or understand something, it&#39;s Worse than the thing you
can just intuit something which will be right 90% of the time. I hate when
people say <a href="https://www.dreamsongs.com/WorseIsBetter.html">&#34;Worse is Better&#34;</a> as if it&#39;s a law of nature and not a choice
you&#39;re actively making; but whatever underlies that principle is in play here
too. At least regarding language choice.</p>
</li>
</ul>
<p>This last point dovetails well into‚Ä¶</p>
<hr/>
<div>
<p><img src="https://morepablo.com/img/2022/11/sophomore_year.jpg" alt="Me at sophomore year. In a dorm and bad lighting, but nicer shirt."/></p><p>Sophomore
year. I quit my data structures and algos class, and figured CS was completely behind me. I picked it back up again in 2008, and picked it up <em>hard</em>. idk why I was allergic to looking normal.</p>
</div>
<p><strong>Stack Overflow, Experts-Exchange, and more available community knowledge.</strong>
Finding answers to your tech problems was way, way harder before Stack Overflow.
The previous name in the game was Experts-Exchange (that hyphen was <strong>very</strong>
important for the domain name), which had way fewer &#34;experts&#34; answering
questions, and critically, was a pay site. To see the answers or ask questions
of your own, you had to sign up. In college I learned to avoid the results in
Google because they were never actually useful, even if they were the most
promising and numerous.</p>
<p>You&#39;d have to trawl forum posts, mailing lists, and read man pages and manuals.
There was a lot more experimentation, trial-and-error, and downloard/reading
source. This sounds Cool and Hardcore, but it was usually pretty inconvenient üòÖ</p>
<p>Of all the products that shaped how we program computers, it&#39;s hard to
understate the impact of SO.</p>
<hr/>
<p>There are a few others, but I like where this list is!</p>
<!--
**Offline documentation (or activity of any kind)**

**Bloat, third-party software**
-->

        

        <section>
          <p><strong>Thanks for
          the read!</strong> Disagreed? <em>Violent agreement!?</em> Feel free
          to join my mailing list, drop me a line at <span></span>, or leave a comment below! I&#39;d love to
          hear from you üòÑ</p> <!-- Obfuscate for 2006-era scrapers -->

        
          

          
          
        </section>
      </article>

    </div></div>
  </body>
</html>
