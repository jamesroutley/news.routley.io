<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/DiscoGrad/DiscoGrad">Original</a>
    <h1>Show HN: Boldly go where Gradient Descent has never gone before with DiscoGrad</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<blockquote>
<p dir="auto">Trying to do gradient descent using automatic differentiation over branchy programs?
Or to combine them with neural networks for end-to-end training?
Then this might be interesting to you.</p>
</blockquote>
<p dir="auto">Automatic Differentiation (AD) is a popular method to obtain the gradients of computer programs, which are extremely useful for adjusting program parameters using gradient descent to solve optimization, control, and inference problems. Unfortunately, AD alone often yields unhelpful (zero-valued and/or biased) gradients for programs involving both parameter-dependent branching control flow such as if-else statements and randomness, including various types of simulations.</p>
<p dir="auto">DiscoGrad automatically transforms C++ programs to a version that efficiently calculates smoothed gradients <em>across</em> branches. Smoothing via external perturbations is supported, but is not required if the target program itself involves randomness. DiscoGrad includes several gradient estimation backends as well as the possibility to integrate neural networks via Torch. The tool supports basic C++ constructs, but is still a research prototype.</p>
<p dir="auto">The repository includes a number of sample applications from domains such as transportation, crowd management, and epidemiology.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description use_cases.mp4">use_cases.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/59713878/320225640-6419fccf-1e20-4a2c-8fef-854197824b15.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY4MTMxMzUsIm5iZiI6MTcxNjgxMjgzNSwicGF0aCI6Ii81OTcxMzg3OC8zMjAyMjU2NDAtNjQxOWZjY2YtMWUyMC00YTJjLThmZWYtODU0MTk3ODI0YjE1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MjclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTI3VDEyMjcxNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZiYzlmYjI0NzM0ZjM0NTE4ZjZmYmRkYmMyZDNiMDViNGQxMTZlMTFhMDkzZmJkMWJhM2RjYjIyODhjYWU4YmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.4YfoaBBWkeMbKbgUbiK9o7usDWU6SxyjuqYtqc9qwBs" data-canonical-src="https://private-user-images.githubusercontent.com/59713878/320225640-6419fccf-1e20-4a2c-8fef-854197824b15.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY4MTMxMzUsIm5iZiI6MTcxNjgxMjgzNSwicGF0aCI6Ii81OTcxMzg3OC8zMjAyMjU2NDAtNjQxOWZjY2YtMWUyMC00YTJjLThmZWYtODU0MTk3ODI0YjE1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MjclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTI3VDEyMjcxNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZiYzlmYjI0NzM0ZjM0NTE4ZjZmYmRkYmMyZDNiMDViNGQxMTZlMTFhMDkzZmJkMWJhM2RjYjIyODhjYWU4YmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.4YfoaBBWkeMbKbgUbiK9o7usDWU6SxyjuqYtqc9qwBs" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description banner.mp4">banner.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/59713878/320877940-4fc691f2-d760-441a-8155-0dc266d5853a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY4MTMxMzUsIm5iZiI6MTcxNjgxMjgzNSwicGF0aCI6Ii81OTcxMzg3OC8zMjA4Nzc5NDAtNGZjNjkxZjItZDc2MC00NDFhLTgxNTUtMGRjMjY2ZDU4NTNhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MjclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTI3VDEyMjcxNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMwNmU4Yjg0OTQ4N2I2NmM5ZTA4NTZhZWNmMGMwYzQ2YzYxOWQzYmUwOWYzMGNmZTdiMjJhNWE4ZmM0ZjEwY2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.jZKO_NzEECy9x5taxEksSirEUCNjgebN5ekDm2ATgBk" data-canonical-src="https://private-user-images.githubusercontent.com/59713878/320877940-4fc691f2-d760-441a-8155-0dc266d5853a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTY4MTMxMzUsIm5iZiI6MTcxNjgxMjgzNSwicGF0aCI6Ii81OTcxMzg3OC8zMjA4Nzc5NDAtNGZjNjkxZjItZDc2MC00NDFhLTgxNTUtMGRjMjY2ZDU4NTNhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MjclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTI3VDEyMjcxNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMwNmU4Yjg0OTQ4N2I2NmM5ZTA4NTZhZWNmMGMwYzQ2YzYxOWQzYmUwOWYzMGNmZTdiMjJhNWE4ZmM0ZjEwY2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.jZKO_NzEECy9x5taxEksSirEUCNjgebN5ekDm2ATgBk" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto">Tested on <code>Ubuntu 22.04.4 LTS</code>, <code>Arch Linux</code> and <code>Fedora 38 Workstation</code></p>
<p dir="auto">To compile the transformation code, you need the following packages (or their analogues provided by your Linux distribution):</p>
<ul dir="auto">
<li><code>clang</code>, <code>clang-devel</code> (version 13 or higher)</li>
<li><code>llvm</code>, <code>llvm-devel</code> (version 13 or higher)</li>
<li><code>cmake</code></li>
</ul>
<div data-snippet-clipboard-copy-content="cd transformation
cmake .
make -j"><pre><code>cd transformation
cmake .
make -j
</code></pre></div>

<p dir="auto">You can use the code contained in <code>programs/hello_world/hello_world.cpp</code> as a quickstart template and reference. The <code>programs</code> folder also contains a number of more complex programs.</p>
<p dir="auto">To compile the hello world example, which implements the Heaviside step function as shown in the video above:</p>
<div dir="auto" data-snippet-clipboard-copy-content="discograd$ ./smooth_compile programs/hello_world/hello_world.cpp"><pre>discograd$ ./smooth_compile programs/hello_world/hello_world.cpp</pre></div>
<p dir="auto"><code>smooth_compile</code> is a shell script that invokes the commands for transforming and compiling the code for the different backends. Here, it will create a binary for each backend in the <code>programs/hello_world</code> folder.</p>
<p dir="auto">AD on the original (crisp) C++ program yields a 0 derivative:</p>
<div dir="auto" data-snippet-clipboard-copy-content="DiscoGrad$ echo 0.0 | ./programs/hello_world/hello_world_crisp_ad --var 0.0 --ns 1
expectation: 1
derivative: 0"><pre>DiscoGrad$ <span>echo</span> 0.0 <span>|</span> ./programs/hello_world/hello_world_crisp_ad --var 0.0 --ns 1
expectation: 1
derivative: 0</pre></div>
<p dir="auto">Our estimator DiscoGrad Gradient Oracle (DGO) calculates a non-zero derivative useful for optimization:</p>
<div dir="auto" data-snippet-clipboard-copy-content="DiscoGrad$ echo 0.0 | ./programs/hello_world/hello_world_dgo --var 0.25 --ns 1000
expectation: 0.527
derivative: -0.7939109206"><pre>DiscoGrad$ <span>echo</span> 0.0 <span>|</span> ./programs/hello_world/hello_world_dgo --var 0.25 --ns 1000
expectation: 0.527
derivative: -0.7939109206</pre></div>
<p dir="auto">You can run <code>./programs/hello_world/hello_world_{crisp,dgo,pgo,reinforce} -h</code> for CLI usage information.</p>


<p dir="auto">The use of our API requires some boilerplate, as detailed below. Please refer to the <code>programs</code> folder for some example usages.</p>
<ol dir="auto">
<li>At the top of your source file, define how many inputs your program has and include the discograd header (in this order).</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="const int num_inputs = 1;
#include &#34;discograd.hpp&#34;"><pre><span>const</span> <span>int</span> num_inputs = <span>1</span>;
#<span>include</span> <span><span>&#34;</span>discograd.hpp<span>&#34;</span></span></pre></div>
<ol start="2" dir="auto">
<li>Implement your entry function, by prepending <code>_DiscoGrad_</code> to the name and using the differentiable type <code>adouble</code> as return value. An object of the type <code>aparams</code> holds the program inputs. As in traditional AD libraries, the type <code>adouble</code> represents a double precision floating point variable. In addition to differentiating through adoubles, DiscoGrad allows branching on (functions of) adoubles and generates gradients that reflect the dependence of the branch taken on the condition.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="adouble _DiscoGrad_my_function(DiscoGrad&amp; _dg, aparams p) {
  adouble inputs[num_inputs];
  for (int i = 0; i &lt; num_inputs; i++)
    inputs[i] = p[i];
  adouble output = 0.0;
  ... // calculations and conditional branching based on inputs
  return output;
}"><pre>adouble <span>_DiscoGrad_my_function</span>(DiscoGrad&amp; _dg, aparams p) {
  adouble inputs[num_inputs];
  <span>for</span> (<span>int</span> i = <span>0</span>; i &lt; num_inputs; i++)
    inputs[i] = p[i];
  adouble output = <span>0.0</span>;
  ... <span><span>//</span> calculations and conditional branching based on inputs</span>
  <span>return</span> output;
}</pre></div>
<ol start="3" dir="auto">
<li>In the main function, create an instance of the <code>DiscoGrad</code> class and a wrapper for your smooth function. Call <code>.estimate(func)</code> on the DiscoGrad instance to invoke the backend-specific gradient estimator.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="int main(int argc, char** argv) {
  // interface with backend and provide the CLI arguments, such as the variance
  DiscoGrad&lt;num_inputs&gt; dg(argc, argv);
  // create a wrapper for the smooth function
  DiscoGradFunc&lt;num_inputs&gt; func(_DiscoGrad_my_function);  
  // call the estimate function of the backend (chosen during compilation)
  dg.estimate(func);
}"><pre><span>int</span> <span>main</span>(<span>int</span> argc, <span>char</span>** argv) {
  <span><span>//</span> interface with backend and provide the CLI arguments, such as the variance</span>
  DiscoGrad&lt;num_inputs&gt; <span>dg</span>(argc, argv);
  <span><span>//</span> create a wrapper for the smooth function</span>
  DiscoGradFunc&lt;num_inputs&gt; <span>func</span>(_DiscoGrad_my_function);  
  <span><span>//</span> call the estimate function of the backend (chosen during compilation)</span>
  dg.<span>estimate</span>(func);
}</pre></div>

<p dir="auto">To compile a program in the folder <code>programs/my_program/my_program.cpp</code> with every backend:</p>
<div dir="auto" data-snippet-clipboard-copy-content="discograd$ ./smooth_compile programs/my_program/my_program.cpp"><pre>discograd$ ./smooth_compile programs/my_program/my_program.cpp</pre></div>
<p dir="auto">Custom compiler or linker flags can be set in the <code>smooth_compile</code> script.</p>
<p dir="auto">You can find a list of backends below.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Executing a Smoothed Program</h3><a id="user-content-executing-a-smoothed-program" aria-label="Permalink: Executing a Smoothed Program" href="#executing-a-smoothed-program"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">To run a smoothed program and compute its gradient, simply invoke the binary with the desired CLI arguments, for example</p>
<div dir="auto" data-snippet-clipboard-copy-content="discograd$ ./programs/my_program/my_program_dgo --var 0.25 --ns 100"><pre>discograd$ ./programs/my_program/my_program_dgo --var 0.25 --ns 100</pre></div>
<p dir="auto">if you want to use the DGO backend. Parameters are entered via <code>stdin</code>, for example by piping the output of <code>echo</code> as shown in the quickstart guide. The output to <code>stdout</code> after <code>expectation</code> and <code>derivative</code> will provide the smoothed output and partial derivatives.</p>

<p dir="auto">This is an overview of all the current backends. More detailed explanations can be found in the following sections.</p>
<table>
<thead>
<tr>
<th>ExecutableSuffix</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>crisp</td>
<td>The original program with optional input perturbations and AD</td>
</tr>
<tr>
<td>dgo</td>
<td>DiscoGrad Gradient Oracle, DiscoGrad&#39;s own gradient estimator based on automatic differentiation and Monte Carlo sampling.</td>
</tr>
<tr>
<td>pgo</td>
<td>Polyak&#39;s Gradient-Free Oracle presented by Polyak and further analysed by Nesterov et al.</td>
</tr>
<tr>
<td>reinforce</td>
<td>Application of REINFORCE to programs with artificially introduced Gaussian randomness.</td>
</tr>
</tbody>
</table>
<p dir="auto">Additionally, an implementation of gradient estimation via Chaudhuri and Solar-Lezama&#39;s method of Smooth Interpretation can be found in the branch &#39;discograd_ieee_access&#39;.</p>
<p dir="auto">Note: When all branches occur directly on discrete random variables drawn from distributions of known shape, <a href="https://github.com/gaurav-arya/StochasticAD.jl">StochasticAD</a> may be a well-suited alternative to the above estimators.</p>
<p dir="auto"><strong>References:</strong></p>
<ul dir="auto">
<li>Chaudhuri, Swarat, and Armando Solar-Lezama. &#34;Smooth interpretation.&#34; ACM Sigplan Notices 45.6 (2010): 279-291.</li>
<li>Boris T Polyak. &#34;Introduction to optimization.&#34; 1987. (Chapter 3.4.2)</li>
<li>Nesterov, Yurii, and Vladimir Spokoiny. &#34;Random gradient-free minimization of convex functions.&#34; Foundations of Computational Mathematics 17 (2017): 527-566.</li>
</ul>

<p dir="auto">This project is licensed under the MIT License.
The DiscoGrad tool includes some parts from third parties, which are licensed as follows:</p>
<ul dir="auto">
<li><code>backend/ankerl/unordered_dense.h</code>, MIT license</li>
<li><code>backend/genann.hpp</code>, zlib license</li>
<li><code>backend/discograd_gradient_oracle/kdepp.hpp</code>, MIT license</li>
<li><code>backend/args.{h,cpp}</code>, MIT license</li>
<li>Doxygen Awesome theme, MIT license</li>
</ul>

<div data-snippet-clipboard-copy-content="@article{kreikemeyer2023smoothing,
     title={Smoothing Methods for Automatic Differentiation Across Conditional Branches},
     author={Kreikemeyer, Justin N. and Andelfinger, Philipp},
     journal={IEEE Access},
     year={2023},
     publisher={IEEE},
     volume={11},
     pages={143190-143211},
     doi={10.1109/ACCESS.2023.3342136}
}"><pre><code>@article{kreikemeyer2023smoothing,
     title={Smoothing Methods for Automatic Differentiation Across Conditional Branches},
     author={Kreikemeyer, Justin N. and Andelfinger, Philipp},
     journal={IEEE Access},
     year={2023},
     publisher={IEEE},
     volume={11},
     pages={143190-143211},
     doi={10.1109/ACCESS.2023.3342136}
}
</code></pre></div>
</article></div></div>
  </body>
</html>
