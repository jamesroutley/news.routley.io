<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://generallyintelligent.substack.com/p/fine-tuning-mistral-7b-on-magic-the">Original</a>
    <h1>Fine Tuning Mistral 7B on Magic the Gathering Draft</h1>
    
    <div id="readability-page-1" class="page"><div class=""><div><div dir="auto"><p><span>In the last six months, I’ve </span><a href="https://generallyintelligent.substack.com/p/llama-2-time-to-fine-tune" rel="">written about fine tuning</a><span> </span><a href="https://generallyintelligent.substack.com/p/gpt-35-finetuning" rel="">a few times</a><span>. Fine tuning is such an enticing technology — promising to fill the gaps in GPT-4’s capabilities while also being faster and cheaper. For as often as fine tuning is discussed, though, I’ve found a surprisingly small amount of content out there that has helped me reason about </span><strong>how effective fine tuning is and how hard it is to successfully fine tune new capabilities into language models.</strong></p><p>So, I decided to take things into my own hands, dust off my ML chops, and find out for myself. </p><p><span>I was particularly interested in testing models’ ability to </span><em><strong>reason</strong><span> </span></em><span>(i.e., perform a somewhat complex task that requires high context understanding) about </span><em><strong>out-of-distribution</strong><span> (</span></em><span>i.e.,</span><em> </em><span>unseen</span><em>)</em><span> data. I ended up using a hobby of mine: </span><em><strong><a href="https://magic.wizards.com/en" rel="">Magic the Gathering</a><span> (specifically, draft)</span></strong></em><span>. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png" width="716" height="402.25824175824175" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:818,&#34;width&#34;:1456,&#34;resizeWidth&#34;:716,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d021374-2254-4544-8282-b68f72f01ae0_1588x892.png 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a><figcaption>Picking a card in a Magic Draft</figcaption></figure></div><p><span>For the unfamiliar: Magic: The Gathering is a strategic trading card game where players use decks of cards representing creatures and spells to battle against their opponents. One of the ways that players play Magic (and my personal favorite way) is </span><em><strong>draft</strong></em><span>, where players build their decks by selecting individual cards from a rotating pool of randomized cards passed among them.</span></p><p><em><strong>Draft </strong></em><span>fits my criteria pretty nicely:</span></p><ul><li><p><strong>Reasoning: </strong><span>choosing a card from a randomized pack is quite skill testing and often requires a cohesive understanding of the context (e.g., what cards have you picked so far, what cards are available in the current pack)</span></p></li><li><p><strong>Out-of-distribution: </strong><span>New Magic cards are released ~4-6 times a year, and the most recent cards are not found in the training corpus of LLMs.</span></p></li></ul><p><span>Another important piece: </span><strong>data. </strong><span>There’s an awesome service called </span><a href="https://www.17lands.com/" rel="">17lands</a><span> that has a huge trove of historical data — players use 17lands’ tracking service to track draft data from the digital Magic client. With that data, you can extract “ground truth” by looking at the draft picks made by the best players on the service (sorted by win rate). This is all a bit fuzzy (a lot of great Magic players debate about correct picks all the time), but it’s a good enough signal to test LLM’s ability to learn a new task.</span></p><p><em><span>If you’re curious about data details, </span><a href="https://gist.github.com/davidhershey/665d45999376c34eefbb71acbf67dafd" rel="">here’s an example of what 17lands data looks like when transformed into a prompt for an LLM</a><span>.</span></em></p><p>Let’s get straight to the results, then dig into some specific learnings and thoughts:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png" width="1200" height="567.3292999135696" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/ca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:false,&#34;imageSize&#34;:&#34;large&#34;,&#34;height&#34;:547,&#34;width&#34;:1157,&#34;resizeWidth&#34;:1200,&#34;bytes&#34;:88838,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca314ee1-ea26-4fcf-a48c-eba1f745cf83_1157x547.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><strong>Thoughts:</strong></p><ul><li><p><strong>A fine tuned 7B parameter model handily beat GPT-4 and came close to human-level (or at least author-level) performance on this task.</strong></p></li><li><p><strong> </strong><span>It looks like fine-tuned GPT-3.5 would be even better, but </span><strong>fine-tuning GPT-3.5 is really expensive!</strong><span> (~100x more expensive than fine-tuning Mistral on bare metal + a premium price for each inference). A fine-tuning run of GPT-3.5 equivalent to my largest run of Mistral-7b would have cost ~$500 — an expensive experiment.</span></p></li><li><p><strong>Fine tuning is still a bit of an art</strong><span> — I had hoped that this would feel more like engineering than science, but there was a lot of experimentation to be done. In particular, prompt engineering with the long feedback loop of fine-tuning is brutal. I’ll go into more details below. </span></p><ul><li><p><em><strong><span>When in doubt, </span><a href="https://github.com/OpenAccess-AI-Collective/axolotl" rel="">use axolotl</a></strong><a href="https://github.com/OpenAccess-AI-Collective/axolotl" rel=""> </a><strong><a href="https://github.com/OpenAccess-AI-Collective/axolotl" rel="">for fine tuning</a><span>. </span></strong><span>It will save you from missing out on a lot of little optimizations.</span></em></p></li></ul></li><li><p><strong>Even the small OSS models are huge by the standard of 5 years ago</strong><span>. It’s one thing to read “7 Billion Parameters”; it’s another to deal with fitting 7 billion parameters and all of the associated math onto a GPU. </span></p></li><li><p><span>I did one interesting experiment, fine tuning a model on one set of cards, then evaluating it on an unseen set of cards</span><strong>. </strong><span>The </span><strong>model seemed to generalize on the concept of drafting</strong><span>, not just memorizing which cards were good.</span></p></li></ul><p><strong>Building a text dataset: </strong><span>The 17lands draft dataset is actually a big CSV file that describes a series of draft picks made by users, roughly with the format of:</span></p><ul><li><p>The cards that were available in the current pack</p></li><li><p>The cards the drafter had picked so far</p></li><li><p>The card the drafter picked from that pack</p></li></ul><p>To make this data suitable for fine tuning a language model, you have to transform it into text — I ended up using the assistant format popularized by OpenAI:</p><div data-attrs="{&#34;innerHTML&#34;:&#34;&lt;div id=\&#34;gist126705539\&#34; class=\&#34;gist\&#34;&gt;\n    &lt;div class=\&#34;gist-file\&#34; translate=\&#34;no\&#34;&gt;\n      &lt;div class=\&#34;gist-data\&#34;&gt;\n        &lt;div class=\&#34;js-gist-file-update-container js-task-list-container file-box\&#34;&gt;\n  &lt;div id=\&#34;file-full_draft_prompt_chatml-txt\&#34; class=\&#34;file my-2\&#34;&gt;\n    \n    &lt;div itemprop=\&#34;text\&#34; class=\&#34;Box-body p-0 blob-wrapper data type-text  \&#34;&gt;\n\n        \n&lt;div class=\&#34;js-check-bidi js-blob-code-container blob-code-content\&#34;&gt;\n\n  &lt;template class=\&#34;js-file-alert-template\&#34;&gt;\n  &lt;div data-view-component=\&#34;true\&#34; class=\&#34;flash flash-warn flash-full d-flex flex-items-center\&#34;&gt;\n  &lt;svg aria-hidden=\&#34;true\&#34; height=\&#34;16\&#34; viewBox=\&#34;0 0 16 16\&#34; version=\&#34;1.1\&#34; width=\&#34;16\&#34; data-view-component=\&#34;true\&#34; class=\&#34;octicon octicon-alert\&#34;&gt;\n    &lt;path d=\&#34;M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\&#34;&gt;&lt;/path&gt;\n&lt;/svg&gt;\n    &lt;span&gt;\n      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.\n      &lt;a class=\&#34;Link--inTextBlock\&#34; href=\&#34;https://github.co/hiddenchars\&#34; target=\&#34;_blank\&#34;&gt;Learn more about bidirectional Unicode characters&lt;/a&gt;\n    &lt;/span&gt;\n\n\n  &lt;div data-view-component=\&#34;true\&#34; class=\&#34;flash-action\&#34;&gt;        &lt;a href=\&#34;{{ revealButtonHref }}\&#34; data-view-component=\&#34;true\&#34; class=\&#34;btn-sm btn\&#34;&gt;    Show hidden characters\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;&lt;/template&gt;\n&lt;template class=\&#34;js-line-alert-template\&#34;&gt;\n  &lt;span aria-label=\&#34;This line has hidden Unicode characters\&#34; data-view-component=\&#34;true\&#34; class=\&#34;line-alert tooltipped tooltipped-e\&#34;&gt;\n    &lt;svg aria-hidden=\&#34;true\&#34; height=\&#34;16\&#34; viewBox=\&#34;0 0 16 16\&#34; version=\&#34;1.1\&#34; width=\&#34;16\&#34; data-view-component=\&#34;true\&#34; class=\&#34;octicon octicon-alert\&#34;&gt;\n    &lt;path d=\&#34;M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0ZM9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0Z\&#34;&gt;&lt;/path&gt;\n&lt;/svg&gt;\n&lt;/span&gt;&lt;/template&gt;\n\n  &lt;table data-hpc class=\&#34;highlight tab-size js-file-line-container js-code-nav-container js-tagsearch-file\&#34; data-tab-size=\&#34;8\&#34; data-paste-markdown-skip data-tagsearch-lang=\&#34;Text\&#34; data-tagsearch-path=\&#34;full_draft_prompt_chatml.txt\&#34;&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L1\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;1\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC1\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;{&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L2\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;2\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC2\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;  &amp;quot;messages&amp;quot;: [&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L3\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;3\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC3\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;    {&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L4\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;4\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC4\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;      &amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;,&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L5\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;5\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC5\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;      &amp;quot;content&amp;quot;: &amp;quot;You are DraftGPT, a Magic the Gathering Hall of Famer and helpful AI assistant that helps players choose what card to pick during a draft. You are a master of the current draft set, and know every card well.\\n\\nWhen asked for a draft pick, respond with the card&amp;#39;s name first.&amp;quot;&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L6\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;6\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC6\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;    },&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L7\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;7\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC7\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;    {&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L8\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;8\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC8\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;      &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;,&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L9\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;9\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC9\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;      &amp;quot;content&amp;quot;: &amp;quot;In our Magic the Gathering draft, we&amp;#39;re on pack 2 pick 13. These are the contents of our pool so far:\\n-------------------------\\nEvolving Wilds --  (common)\\nRat Out -- {B} (common)\\nNot Dead After All -- {B} (common)\\nHopeless Nightmare -- {B} (common)\\nBarrow Naughty -- {1}{B} (common)\\nUnassuming Sage -- {1}{W} (common)\\nThe Witch&amp;#39;s Vanity -- {1}{B} (uncommon)\\nSpell Stutter -- {1}{U} (common)\\nMintstrosity -- {1}{B} (common)\\nWater Wings -- {1}{U} (common)\\nBarrow Naughty -- {1}{B} (common)\\nGadwick&amp;#39;s First Duel -- {1}{U} (uncommon)\\nBitter Chill -- {1}{U} (uncommon)\\nThe Princess Takes Flight -- {2}{W} (uncommon)\\nStockpiling Celebrant -- {2}{W} (common)\\nVoracious Vermin -- {2}{B} (common)\\nDevouring Sugarmaw // Have for Dinner -- {2}{B}{B} // {1}{W} (rare)\\nMisleading Motes -- {3}{U} (common)\\nJohann&amp;#39;s Stopgap -- {3}{U} (common)\\nBesotted Knight // Betroth the Beast -- {3}{W} // {W} (common)\\nThreadbind Clique // Rip the Seams -- {3}{U} // {2}{W} (uncommon)\\nTwining Twins // Swift Spiral -- {2}{U}{U} // {1}{W} (rare)\\nEriette&amp;#39;s Whisper -- {3}{B} (common)\\nFarsight Ritual -- {2}{U}{U} (rare)\\nTwisted Sewer-Witch -- {3}{B}{B} (uncommon)\\nInto the Fae Court -- {3}{U}{U} (common)\\n-------------------------\\n\\nTo keep track of what colors are open, you&amp;#39;ve counted how many cards of each color identity you&amp;#39;ve seen in the last 5 packs. Here is the breakdown:\\nW: 11\\nB: 6\\nG: 4\\nRW: 1\\nR: 2\\n\\nThese are the contents of the pack:\\n-------------------------\\nCut In -- {3}{R}\\nSorcery (common)\\nCut In deals 4 damage to target creature.\\nCreate a Young Hero Role token attached to up to one target creature you control. (If you control another Role on it, put that one into the graveyard. Enchanted creature has \\&amp;quot;Whenever this creature attacks, if its toughness is 3 or less, put a +1/+1 counter on it.\\&amp;quot;)\\n-------------------------\\nSkewer Slinger -- {1}{R}\\nCreature — Dwarf Knight (common)\\nReach\\nWhenever Skewer Slinger blocks or becomes blocked by a creature, Skewer Slinger deals 1 damage to that creature.\\n1/3\\n-------------------------\\n\\nWhat card would you pick from this pack?&amp;quot;&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L10\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;10\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC10\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;    },&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L11\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;11\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC11\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;    {&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L12\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;12\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC12\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;      &amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;,&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L13\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;13\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC13\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;      &amp;quot;content&amp;quot;: &amp;quot;Cut In&amp;quot;&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L14\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;14\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC14\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;    }&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L15\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;15\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC15\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;  ]&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-L16\&#34; class=\&#34;blob-num js-line-number js-code-nav-line-number js-blob-rnum\&#34; data-line-number=\&#34;16\&#34;&gt;&lt;/td&gt;\n          &lt;td id=\&#34;file-full_draft_prompt_chatml-txt-LC16\&#34; class=\&#34;blob-code blob-code-inner js-file-line\&#34;&gt;}&lt;/td&gt;\n        &lt;/tr&gt;\n  &lt;/table&gt;\n&lt;/div&gt;\n\n\n    &lt;/div&gt;\n\n  &lt;/div&gt;\n&lt;/div&gt;\n\n      &lt;/div&gt;\n      &lt;div class=\&#34;gist-meta\&#34;&gt;\n        &lt;a href=\&#34;https://gist.github.com/davidhershey/f57d0b19563fef86b117751dcbe6de20/raw/7bba77a61f6b698563157efe2a772954456cfe5e/full_draft_prompt_chatml.txt\&#34; style=\&#34;float:right\&#34; class=\&#34;Link--inTextBlock\&#34;&gt;view raw&lt;/a&gt;\n        &lt;a href=\&#34;https://gist.github.com/davidhershey/f57d0b19563fef86b117751dcbe6de20#file-full_draft_prompt_chatml-txt\&#34; class=\&#34;Link--inTextBlock\&#34;&gt;\n          full_draft_prompt_chatml.txt\n        &lt;/a&gt;\n        hosted with &amp;#10084; by &lt;a class=\&#34;Link--inTextBlock\&#34; href=\&#34;https://github.com\&#34;&gt;GitHub&lt;/a&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n&#34;,&#34;stylesheet&#34;:&#34;https://github.githubassets.com/assets/gist-embed-94178ea21d55.css&#34;}" data-component-name="GitgistToDOM"><div id="gist126705539"><div><div><div><div id="file-full_draft_prompt_chatml-txt"><div itemprop="text"><div><div data-view-component="true"><p><span>
  
    

    </span><span><span>
      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      </span><a href="https://github.co/hiddenchars" target="_blank" rel="">Learn more about bidirectional Unicode characters</a><span>
    </span></span><span>


  </span></p></div></div></div></div></div></div></div></div></div><p><strong>This very quickly exposes the most challenging piece of fine tuning</strong><span>: formatting the data for the right outcome is </span><em>challenging </em><span>and </span><em>fundamentally experimental</em><span>. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png" width="542" height="588.93039049236" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1280,&#34;width&#34;:1178,&#34;resizeWidth&#34;:542,&#34;bytes&#34;:1698572,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b1def04-beb3-461b-9581-8e0456acbebd_1178x1280.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p><span>By now, most folks are familiar with prompt engineering — the experimental process of modifying your prompt to get the best performance out of a language model. </span><strong>The prompt engineering process is 100x slower with fine tuning. </strong><span>You typically need to kick off a multiple-hour job to test a prompt. This bogs down the experimental workflow significantly and makes fine-tuning feel just as challenging as classical machine learning.</span></p><p>To illustrate with the Magic draft problem, I considered and tested the following:</p><ul><li><p>~5 prompt formats, in particular how much detail about each card to show</p></li><li><p>Adding additional context about the last few draft picks to have “memory”</p></li><li><p>Including training lines of “card trivia,” where the model is asked to remember details about the new cards</p></li></ul><p>I did ~ 40 hours of experiments and still don’t conclusively feel that I’ve answered questions about what prompt format is “best” for this task. There is a lot of room to experiment.</p><p><strong>Finding GPUs: </strong><span>doesn’t need to be said, but it sucks! Most places don’t have a lot of availability. I ended up renting an hourly GPU from </span><a href="https://www.runpod.io/" rel="">Runpod</a><span> (an RTX 4090 w/ 24GB of VRAM) for ~$0.7/hr. </span></p><p><strong>Fine tuning script: </strong><span>This isn’t my first ML rodeo, so my gut was to write my own training script with HuggingFace transformers + </span><a href="https://www.google.com/search?q=peft&amp;sourceid=chrome&amp;ie=UTF-8" rel="">PEFT</a><span>. Considering my limited GPU situation, </span><a href="https://arxiv.org/abs/2305.14314" rel="">QLoRA</a><span> seemed like the way to go.</span></p><p><span>It turns out that writing my own script was a bad idea! There are a whole bunch of finicky little optimizations and options that range from </span><a href="https://github.com/Dao-AILab/flash-attention" rel="">straightforward-if-you-know-about-them</a><span> to </span><a href="https://huggingface.co/docs/peft/conceptual_guides/lora#common-lora-parameters-in-peft" rel="">pretty obtuse without reading a research paper</a><span>. Nothing insurmountable, but it would take a long time to figure out yourself.</span></p><p><span>I ended up using </span><a href="https://github.com/OpenAccess-AI-Collective/axolotl" rel="">axolotl</a><span>, which implements a ton of those optimizations out of the box and was much easier to get running (and running quickly). Their documentation is actually pretty decent, and I think is the right starting point for most people to fine-tune LLMs.</span></p><p><strong>A note on the models: </strong><span>Holy crap, LLMs are seriously large! The last time I trained models regularly was ~ 2019, when Bert had ~110 million parameters; now, the “small” LLMs are 70 times bigger than that. Models this large are fundamentally cumbersome. Weights being ~16GB makes storage a real concern; GPU memory is challenging even with methods like QLora. No wonder the best researchers are such a hot commodity; this is seriously challenging work at the largest scale.</span></p><p><strong>Start with evaluation first</strong><span>: One lesson from ML of old that I don’t think has been adopted enough among the prompt engineering wizards: you should always build a good evaluation before starting your experiments. Here, evaluation was pretty easy (hold out some full drafts from the training data and check if the model picks the same card as the human on the holdout data), but having a good evaluation set made reasoning about fine-tuning much more straightforward.</span></p><p><strong>Some criteria for language models are hard to define: </strong><span>The “pick the right card” task is pretty easy to define for Magic drafts, but there are some fuzzier things that I would like the final model to do, too:</span></p><ul><li><p>When it makes different picks, they should be justifiable</p></li><li><p>It would be nice if the model could give a reasonable explanation for “why” it made a pick</p></li></ul><p><span>Each of those is much harder to define, and I ended up testing them with the “eye test” by going through a bunch of examples, but this was slow. FWIW, </span><strong>GPT-4 is better at making less “weird” picks and better at justifying its choices than the fine-tuned smaller models.</strong></p><p>My two biggest takeaways from this experiment:</p><ul><li><p>Fine tuning on new data can be remarkably effective, easily surpassing GPT-4 + in-context learning on both accuracy and cost. </p></li><li><p>Fine tuning is a fundamentally experimental process to get “right”, and doing it well is a specialized skillset (and in particular, a skillset that is harder to learn than prompt engineering).</p></li></ul><p>In terms of how the bots actually feel as drafters? Pretty good! </p><p>I wired up the draft pick model to the logs generated by Magic Arena, whipped up a quick electron app, and have done a few drafts with a “Magic Copilot”:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png" width="648" height="857.8077571669477" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1570,&#34;width&#34;:1186,&#34;resizeWidth&#34;:648,&#34;bytes&#34;:511921,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;topImage&#34;:false,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee2540-efad-4fa0-9c8a-3d682df9a567_1186x1570.png 1456w" sizes="100vw" loading="lazy"/></picture></div></a></figure></div><p>Some quirks:</p><ul><li><p>The pick is generated by a fine tuned model, but the commentary is generated by GPT-4. This works well most of the time, but occasionally GPT-4 disagrees with the fine tune and immediately contradicts it 😅</p></li><li><p>I’ve hooked up eight draft AIs to a simulated draft (i.e., all of the bots are drafting against each other). They have some quirky behavior when passing to each other — they have a pretty weird tendency to draft mono-colored decks. If there’s a human making other picks, they tend to converge into much more normal-looking decks.</p></li></ul><p>Overall, I would venture to guess this is probably one of the more powerful and humanlike draft AIs out there right now. Compared to the bots in Magic Arena’s quick draft feature, these are much more similar to a high-quality human drafter than a heuristic bot.</p><p><span>Wizards of the Coast — if you’re looking for excessively high fidelity and somewhat expensive to run draft AI, </span><strong>hit me up! </strong><span>I’m happy to send you some LLMs!</span></p></div></div></div></div>
  </body>
</html>
