<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://browse.arxiv.org/html/2312.08361v1">Original</a>
    <h1>Distributed Inference and Fine-Tuning of Large Language Models over the Internet</h1>
    
    <div id="readability-page-1" class="page"><article>


<div>
<h6>Abstract</h6>
<p id="id1.1">Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters.
However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers.
In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a <span id="id1.1.1">large enough</span> model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network.
This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers.
We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will.
In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput.
We showcase these algorithms in <span id="id1.1.2">Petals</span>
 — a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to <math alttext="10\times" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1b"><mn id="id1.1.m1.1.1">10</mn><mo id="id1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="id1.1.m1.1c">10\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">10 ×</annotation></semantics></math> faster than offloading for interactive generation.
We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.</p>
</div>
<section id="S1">
<h2>
<span>1 </span>Introduction</h2>
<p id="S1.p1.1">In recent years, the NLP community has found that pretrained language models greatly accelerated progress on many research problems through either fine-tuning <cite>(Radford et al., <a href="#bib.bib46" title="">2018</a>)</cite> or simple prompting <cite>(Brown et al., <a href="#bib.bib9" title="">2020</a>)</cite>. Their quality tends to improve as we increase model scale <cite>(Radford et al., <a href="#bib.bib47" title="">2019</a>; Kaplan et al., <a href="#bib.bib26" title="">2020</a>)</cite>. Following this trend, modern language models often have hundreds of billions of parameters <cite>(Brown et al., <a href="#bib.bib9" title="">2020</a>; Rae et al., <a href="#bib.bib48" title="">2021</a>; Zeng et al., <a href="#bib.bib67" title="">2021</a>; Kim et al., <a href="#bib.bib28" title="">2021</a>)</cite>.</p>
<p id="S1.p2.1">Most recently, several research groups open-sourced their pretrained LLMs with over 50B parameters <cite>(Zhang et al., <a href="#bib.bib68" title="">2022</a>; BigScience, <a href="#bib.bib5" title="">2022a</a>; Touvron et al., <a href="#bib.bib58" title="">2023a</a>, <a href="#bib.bib59" title="">b</a>)</cite>.
However, they are still difficult to use due to the sheer size in terms of parameters. For example, OPT-175B and BLOOM-176B need over 350 GB accelerator memory for inference and even more for fine-tuning. As a result, even basic inference for these LLMs requires multiple high-end GPUs or multi-node clusters. Recent studies propose algorithms for running large models with more affordable hardware <cite>(Pudipeddi et al., <a href="#bib.bib45" title="">2020</a>; Ren et al., <a href="#bib.bib51" title="">2021</a>)</cite>, e.g. by offloading parameters to RAM. However, as we show in Section <a href="#S3.SS1" title="3.1 Performance bottlenecks of LLM inference ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3.1</span></a>, these techniques are inefficient in many use cases, such as LLM-based chatbots and search engines.</p>
<figure id="S1.F1"><img alt="Refer to caption" height="232" id="S1.F1.g1" src="https://olu.online/24-hopes-for-2024/x1.png" width="830"/>
<figcaption><span>Figure 1: </span>A high-level overview of our system design. Servers store pretrained LLM layers and temporarily hold attention caches for inferencing. Clients hold embedding layers and learned prompts/adapters (if used).
Arrows denote temporary chains formed for inference.</figcaption>
</figure>
<p id="S1.p3.1">In this work, we search for a more cost-effective way of running pretrained LLMs in their main use cases: inference, in-context learning, and fine-tuning. We analyze latency and throughput for these use cases and determine which factors become dominant for very large models. Notably, for models with over 50B parameters, communicating activations over a slow network can be faster than swapping layers from local RAM or SSD. Based on these observations, it should be possible to run LLMs cost-effectively by pooling together commodity hardware over the Internet.</p>
<p id="S1.p4.1">However, existing LM algorithms are not designed to run inference with unreliable devices or high-latency networks. To bridge this gap, we formulate a novel algorithm for fault-tolerant distributed autoregressive inference of very large models. Using dual attention caches, this algorithm can quickly recover from a failed server and reassign the load to one or more replacement servers. Finally, to make sure that there are enough servers for every part of the model, we develop a decentralzied load-balancing algorithm that assigns transformer blocks to every server to maximize the total system throughput. The fully decentralized nature of these protocols allows participants to add or remove their devices at any point, making optimal use of GPU idle time.</p>
<div id="S1.p5">
<p id="S1.p5.1">We summarize the main contributions of this work as such:</p>
<ul id="S1.I1">
<li id="S1.I1.i1">
<span>•</span>
<p id="S1.I1.i1.p1.1">We analyze the problem of cost-efficient LLM inference and propose a novel algorithm that can inference large (50B+) language models on distributed unreliable devices. To the best of our knowledge, this is the first algorithm that can inference LLMs with 50B+ parameters in this setup.
</p>
</li>
<li id="S1.I1.i2">
<span>•</span>
<p id="S1.I1.i2.p1.1">Using this algorithm, we develop <span id="S1.I1.i2.p1.1.1">Petals</span> — a decentralized system for inferencing and fine-tuning LLMs over the Internet. The system allows users to run inference and fine-tuning over a swarm of unreliable devices with the same correctness guarantees as when running locally. The system runs persistently with the help of volunteers.</p>
</li>
<li id="S1.I1.i3">
<span>•</span>
<p id="S1.I1.i3.p1.1">We benchmark the performance of the proposed algorithms on Llama 2 (70B) <cite>(Touvron et al., <a href="#bib.bib59" title="">2023b</a>)</cite> and BLOOM (176B) <cite>(BigScience, <a href="#bib.bib5" title="">2022a</a>)</cite>. We run experiments in controlled conditions, with simulated network latency and server failures, and in the actual geo-distributed system spanning two continents. With realistic network speeds, our distributed algorithms perform autoregressive generation <math alttext="{\geq}10\times" display="inline" id="S1.I1.i3.p1.1.m1.1"><semantics id="S1.I1.i3.p1.1.m1.1a"><mrow id="S1.I1.i3.p1.1.m1.1b"><mo id="S1.I1.i3.p1.1.m1.1.1">≥</mo><mn id="S1.I1.i3.p1.1.m1.1.2">10</mn><mo id="S1.I1.i3.p1.1.m1.1.3" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.I1.i3.p1.1.m1.1c">{\geq}10\times</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i3.p1.1.m1.1d">≥ 10 ×</annotation></semantics></math> faster than local offloading.</p>
</li>
</ul>
</div>
</section>
<section id="S2">
<h2>
<span>2 </span>Background: efficient training and inference</h2>
<p id="S2.p1.1">There is a wide variety of methods optimizing training and inference for most deep learning workloads. Here, we focus on two areas relevant for our analysis: model parallelism and parameter offloading.</p>
<section id="S2.SS1">
<h3>
<span>2.1 </span>Model parallelism</h3>
<p id="S2.SS1.p1.1">Model parallelism is a family of distributed training algorithms that assigns each device to hold a subset of model parameters, run a subset of computations and communicate output activations. <span id="S2.SS1.p1.1.1">Tensor parallelism</span> assigns each device to compute a subset of each model layer (e.g., a subset of neurons), then communicate results between each other and proceed to the next layer <cite>(Krizhevsky et al., <a href="#bib.bib31" title="">2012</a>; Ben-Nun &amp; Hoefler, <a href="#bib.bib4" title="">2019</a>; Tang et al., <a href="#bib.bib56" title="">2020</a>)</cite>. Each device performs a symmetric computation, applied to a different slice of model weights, which makes tensor parallelism compatible with MPI-based communication. In turn, the main performance overhead of this strategy comes from all-to-all communication (and synchronization) after each layer <cite>(Krizhevsky, <a href="#bib.bib30" title="">2014</a>)</cite>.</p>
<p id="S2.SS1.p2.1"><span id="S2.SS1.p2.1.1">Pipeline parallelism</span> reduces the communication overhead by assigning each device with one or several full layers <cite>(Huang et al., <a href="#bib.bib24" title="">2019</a>; Narayanan et al., <a href="#bib.bib41" title="">2019</a>; Yang et al., <a href="#bib.bib63" title="">2019</a>)</cite>. During the forward pass, each stage applies its subset of layers to the inputs supplied by the previous stage, then sends the outputs of the last layer to the next stage. For the backward pass, this process is reversed, with each pipeline stage passing the gradients to the same device that previously supplied it with input activations. To better utilize the available devices, the pipeline must process multiple microbatches per step, allowing each stage to run in parallel on a different batch of inputs. Even with optimal execution, some of the pipeline stages will remain idle some of the time <cite>(Huang et al., <a href="#bib.bib24" title="">2019</a>)</cite>.</p>
<p id="S2.SS1.p3.1">Both of these strategies are actively used for training LLMs. Real-world distributed training systems usually combine multiple forms of parallelism depending on hardware and network type <cite>(Narayanan et al., <a href="#bib.bib42" title="">2021</a>; Rajbhandari et al., <a href="#bib.bib49" title="">2020</a>; Jia et al., <a href="#bib.bib25" title="">2019</a>)</cite>.
Tensor parallelism is typically used within a single multi-GPU server or closely interconnected TPU cores <cite>(Narayanan et al., <a href="#bib.bib42" title="">2021</a>; Shazeer et al., <a href="#bib.bib54" title="">2018</a>)</cite>. In turn, pipeline parallelism is used to connect multiple servers <cite>(Narayanan et al., <a href="#bib.bib42" title="">2021</a>)</cite>. Recent works demonstrate that model parallelism can be used for cost-efficient <span id="S2.SS1.p3.1.1">pre-training</span> of LLMs by pooling together idle GPU devices <cite>(Athlur et al., <a href="#bib.bib3" title="">2022</a>; Wang et al., <a href="#bib.bib61" title="">2022</a>; Kuszmaul, <a href="#bib.bib32" title="">2022</a>; Yuan et al., <a href="#bib.bib65" title="">2022</a>; Ryabinin et al., <a href="#bib.bib52" title="">2023</a>)</cite>.</p>
</section>
<section id="S2.SS2">
<h3>
<span>2.2 </span>Offloading</h3>
<p id="S2.SS2.p1.1">Parameter offloading relegates model parameters from accelerator memory to a slower but cheaper storage: typically RAM or SSD <cite>(Pudipeddi et al., <a href="#bib.bib45" title="">2020</a>; Ren et al., <a href="#bib.bib51" title="">2021</a>; Rajbhandari et al., <a href="#bib.bib50" title="">2021</a>)</cite>. When using the model, parameters are loaded to the accelerator just-in-time for computation, one or few layers at a time. In principle, this method allows running large models with a single low-end accelerator as long as there is enough RAM (or SSD) to store the model.</p>
<p id="S2.SS2.p2.1">The main drawback of this strategy is having to load and unload through all model parameters for each forward and backward pass, which can be time-consuming. This extra time can be amortized in workloads where model can do a lot of useful computations for each time a parameter is loaded.
In practice, using offloading to run a single token through the OPT-175B on one GPU in the best-case scenario of hardware and bandwidth would require 11 seconds per forward pass, or twice that for training. As we show in Section <a href="#S4" title="4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>4</span></a>, real-world performance is significantly slower.</p>
<p id="S2.SS2.p3.1"><cite>Pudipeddi et al. (<a href="#bib.bib45" title="">2020</a>)</cite> circumvents this by training with very large batches, and hence, increasing the computation. In turn, <cite>Ren et al. (<a href="#bib.bib51" title="">2021</a>); Rajbhandari et al. (<a href="#bib.bib50" title="">2021</a>)</cite> reduce the overhead by overlapping communication and computation, that is, doing useful computation for the current layer while waiting for the transfer of the next layer to finish. Some of these systems <cite>Ren et al. (<a href="#bib.bib51" title="">2021</a>)</cite> also partition offloaded parameters between devices. However, unlike model-parallel training, distributed offloading still requires each device to compute the full model.</p>
</section>
</section>
<section id="S3">
<h2>
<span>3 </span>Method</h2>
<div id="S3.p1">
<p id="S3.p1.1">Using pretrained large language models for NLP tasks consists of two main workloads: inference and fine-tuning. The inference workload typically consists of encoding an input text, then generating tokens autoregressively.
In turn, fine-tuning requires updating either all of the model’s parameters or (more commonly for large models) a small set of trainable weights (e.g., adapters or soft prompts) by backpropagation. These two workloads also cover more advanced use cases:</p>
<ul id="S3.I1">
<li id="S3.I1.i1">
<span>•</span>
<p id="S3.I1.i1.p1.1">Manually engineering prompts for a given task, then deploying the model with these prompts.</p>
</li>
<li id="S3.I1.i2">
<span>•</span>
<p id="S3.I1.i2.p1.1">Fine-tuning with adapters <cite>(Hu et al., <a href="#bib.bib23" title="">2021</a>; Houlsby et al., <a href="#bib.bib22" title="">2019</a>; Liu et al., <a href="#bib.bib37" title="">2022b</a>)</cite> or “soft” prompts <cite>(Liu et al., <a href="#bib.bib39" title="">2021b</a>; Lester et al., <a href="#bib.bib34" title="">2021</a>; Liu et al., <a href="#bib.bib38" title="">2021a</a>)</cite> and inferencing fine-tuned models.</p>
</li>
<li id="S3.I1.i3">
<span>•</span>
<p id="S3.I1.i3.p1.1">Distillation into a smaller task-specific model for faster inference <cite>(Schick &amp; Schütze, <a href="#bib.bib53" title="">2021</a>)</cite>.</p>
</li>
</ul>
</div>
<p id="S3.p2.1">Counter-intuitively, we found that inference is more challenging than fine-tuning for cost-efficient setups. To that end, we dedicate most of this section to inference-specific problems. As for fine-tuning, we describe a way to support arbitrary parameter-efficient fine-tuning in Section <a href="#S3.SS4" title="3.4 Parameter-efficient fine-tuning ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3.4</span></a>.</p>
<section id="S3.SS1">
<h3>
<span>3.1 </span>Performance bottlenecks of LLM inference</h3>
<p id="S3.SS1.p1.7">Unlike training, autoregressive LLM inference cannot be done with a single pass through the model. Instead, the model needs to process one token at a time, pass it through the entire model, then generate the next token and repeat the process. In case of model parallelism, training an <math alttext="n" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_n</annotation></semantics></math>-layer model on a sequence of <math alttext="t" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_t</annotation></semantics></math> tokens needs <math alttext="O(n)" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.2" xref="S3.SS1.p1.3.m3.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.2.2" xref="S3.SS1.p1.3.m3.1.2.2.cmml">O</mi><mo id="S3.SS1.p1.3.m3.1.2.1" xref="S3.SS1.p1.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS1.p1.3.m3.1.2.3.2" xref="S3.SS1.p1.3.m3.1.2.cmml"><mo id="S3.SS1.p1.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS1.p1.3.m3.1.2.cmml">(</mo><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">n</mi><mo id="S3.SS1.p1.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS1.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.2.cmml" xref="S3.SS1.p1.3.m3.1.2"><times id="S3.SS1.p1.3.m3.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.2.1"></times><ci id="S3.SS1.p1.3.m3.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.2.2">𝑂</ci><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_O ( italic_n )</annotation></semantics></math> communication rounds, while generating the same sequence needs <math alttext="O(n\cdot t)" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">O</mi><mo id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.4.m4.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.4.m4.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.2.cmml">n</mi><mo id="S3.SS1.p1.4.m4.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.3.cmml">t</mi></mrow><mo id="S3.SS1.p1.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><times id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2"></times><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑂</ci><apply id="S3.SS1.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1"><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1">⋅</ci><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.2">𝑛</ci><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">O(n\cdot t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_O ( italic_n ⋅ italic_t )</annotation></semantics></math> rounds, making it more susceptible to network latency. Similarly with parameter offloading, generating a sequence of <math alttext="t" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_t</annotation></semantics></math> tokens needs loading every layer <math alttext="t" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_t</annotation></semantics></math> times, which also takes <math alttext="O(n\cdot t)" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mrow id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml">O</mi><mo id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.7.m7.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.7.m7.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.7.m7.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.7.m7.1.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.1.1.1.2" xref="S3.SS1.p1.7.m7.1.1.1.1.1.2.cmml">n</mi><mo id="S3.SS1.p1.7.m7.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.SS1.p1.7.m7.1.1.1.1.1.3" xref="S3.SS1.p1.7.m7.1.1.1.1.1.3.cmml">t</mi></mrow><mo id="S3.SS1.p1.7.m7.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><times id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2"></times><ci id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3">𝑂</ci><apply id="S3.SS1.p1.7.m7.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1"><ci id="S3.SS1.p1.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1">⋅</ci><ci id="S3.SS1.p1.7.m7.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.2">𝑛</ci><ci id="S3.SS1.p1.7.m7.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">O(n\cdot t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_O ( italic_n ⋅ italic_t )</annotation></semantics></math> time.</p>
<p id="S3.SS1.p2.2">The other problem of autoregressive generation is dealing with attention for past tokens <cite>(Vaswani et al., <a href="#bib.bib60" title="">2017</a>)</cite>. During an inference step <math alttext="t" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_t</annotation></semantics></math>, each layer needs to attend to <math alttext="t-1" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">−</mo><mn id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><minus id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></minus><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑡</ci><cn id="S3.SS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">t-1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_t - 1</annotation></semantics></math> previous attention keys and values. Existing inference algorithms store past entries in accelerator memory. Caching half-precision activations of a 2048-token sequence for large models like GPT-3 <cite>(Brown et al., <a href="#bib.bib9" title="">2020</a>)</cite> or OPT-175B <cite>(Zhang et al., <a href="#bib.bib68" title="">2022</a>)</cite> (with 96 layers of 12288 units each) takes up 9.6 GB GPU memory <span id="S3.SS1.p2.2.1">for each sequence</span>. Offloading these cached values faces the same problems as offloading in general.</p>
<p id="S3.SS1.p3.2">An alternative solution is to recompute all previous tokens on every inference step, storing only one set of keys &amp; values at a time.
Naturally, this approach needs increasingly more computation with sequence length <math alttext="t" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_t</annotation></semantics></math>, for a total of <math alttext="O(t^{3})" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">O</mi><mo id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">(</mo><msup id="S3.SS1.p3.2.m2.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml">t</mi><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml">3</mn></msup><mo id="S3.SS1.p3.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><times id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2"></times><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">𝑂</ci><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2">𝑡</ci><cn id="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">O(t^{3})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_O ( italic_t start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT )</annotation></semantics></math> time for transformer-based models.Surprisingly, this approach is often more efficient than offloaded caching, especially for shorter sequences due to the overhead from loading and storing cache from RAM or SSD.</p>
<p id="S3.SS1.p4.1">Parameter offloading can still be efficient when generating <span id="S3.SS1.p4.1.1">large amounts of short sequences</span> in bulk. Each individual sequence still takes a long time to generate, but the system maintains high throughput by running many samples in parallel.
Unfortunately, this scenario does not cover many important LLM use cases. For instance, it is incompatible with in-context learning or prompt engineering, where the model needs to process long sequences of training examples <cite>(Brown et al., <a href="#bib.bib9" title="">2020</a>)</cite>. More importantly, it does not support “interactive” applications where LLM needs to quickly respond to a user input. This rules out many LLM applications such as conversation systems or input completion (e.g. ChatGPT or Smart Compose).</p>
<p id="S3.SS1.p5.1">Hence, we explore a new solution based on pipeline-parallelism. A related line of work <cite>(Aminabadi et al., <a href="#bib.bib2" title="">2022</a>)</cite> investigates model parallelism to inference LLMs in GPU clusters. However, their approach does not apply to our more affordable setups: cheap “preemptible” instances or connecting existing resources over the Internet. To operate in these conditions, an inference algorithm needs to deal with node preemption, network errors, and high latency.</p>
</section>
<section id="S3.SS2">
<h3>
<span>3.2 </span>Distributed generation with fault tolerance</h3>
<p id="S3.SS2.p1.1">In this section, we formulate an algorithm for inferencing LLMs in a fleet of unreliable geographically distributed devices connected over the Internet. Each device can act as a server, a client, or both. A <span id="S3.SS2.p1.1.1">client</span> is a node operated by the user, which runs inference or fine-tuning jobs through the swarm of servers. A client only holds input and output embeddings (<math alttext="&lt;3\%" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">&lt;</mo><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mn id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">3</mn><mo id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><lt id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">absent</csymbol><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1">percent</csymbol><cn id="S3.SS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">&lt;3\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">&lt; 3 %</annotation></semantics></math> of model weights for BLOOM-176B) and delegates running transformer blocks (the most expensive computations) to remote servers. A <span id="S3.SS2.p1.1.2">server</span> is a GPU-enabled node holding a set of consecutive transformer blocks and processing requests coming from client nodes.</p>
<p id="S3.SS2.p2.1">For simplicity, we assume that every block is hosted on several servers and examine this assumption in the next section. Following this notation, a fault-tolerant algorithm should allow each client to complete an inference job with reproducible results even if some remote servers fail during inference.</p>
<p id="S3.SS2.p3.1">As we discuss in Section <a href="#S3.SS1" title="3.1 Performance bottlenecks of LLM inference ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3.1</span></a>, autoregressive generation requires many sequential communication rounds, making it sensitive to network latency.
However, if every device stores its past attention cache, every round only transfers activations for a single token, i.e. several kilobytes of data. We use this model to directly minimize the inference time over possible pipeline configurations. As we show later in Section <a href="#S4.SS2" title="4.2 Experiments for Llama 2 (70B) and BLOOM (176B) ‣ 4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>4.2</span></a>, this allows efficient inference over a low-bandwidth Internet connection.</p>
<p id="S3.SS2.p4.3">A more challenging problem is how to recover from node and network failures. If a remote server shuts down, any cached attention keys stored on that server will be lost with it. There are two naïve solutions to this problem: restarting inference from scratch or recomputing past embeddings on every step. Restarting might be enough at a small scale. However, running 50B+ models may involve many unreliable devices, making it unlikely to generate long sequence without at least one failure. In turn recomputing past attention caches requires communicating past tokens on every communication round, resulting in <math alttext="O(n\cdot t^{2})" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">O</mi><mo id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p4.1.m1.1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS2.p4.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p4.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p4.1.m1.1.1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.1.1.1.2.cmml">n</mi><mo id="S3.SS2.p4.1.m1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1.cmml">⋅</mo><msup id="S3.SS2.p4.1.m1.1.1.1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p4.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.2.cmml">t</mi><mn id="S3.SS2.p4.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.3.cmml">2</mn></msup></mrow><mo id="S3.SS2.p4.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><times id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2"></times><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">𝑂</ci><apply id="S3.SS2.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1"><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.1">⋅</ci><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.2">𝑛</ci><apply id="S3.SS2.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.2">𝑡</ci><cn id="S3.SS2.p4.1.m1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">O(n\cdot t^{2})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_O ( italic_n ⋅ italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )</annotation></semantics></math> total data transferred, where <math alttext="n" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_n</annotation></semantics></math> is the number of pipeline layers and <math alttext="t" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mi id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_t</annotation></semantics></math> is the sequence length. In other words, both these solutions struggle to generate long sequences.</p>
<p id="S3.SS2.p5.1">We address this problem by maintaining two types of cache: <span id="S3.SS2.p5.1.1">server-side cache</span> holds past attention keys and values for their layers, like in existing inference algorithms, while <span id="S3.SS2.p5.1.2">client-side cache</span> holds past inputs sent to a given pipeline stage. If a server disconnects, a client can find another server with that pipeline stage and use client-side cache to restore the server state.</p>
<p id="S3.SS2.p6.1">The resulting procedure is described in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.2 Distributed generation with fault tolerance ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a>.
For every pipeline stage, the client maintains a heap (priority queue) of servers that hold this stage (and may hold additional stages). The servers in queue are ordered by the network latency, measured from past communication. These queues are maintained through the lifetime of a client. To begin generation, the client runs a beam-search-like procedure to find a sequence of servers that results in the least total inference time under our performance model. When running inference steps, a client keeps track of intermediate activations sent between pipeline stages. If a remote server fails or leaves, the client retrieves the next best server (or multiple servers) and requests it to restore the attention state from the client’s cached activations.</p>
<figure id="S3.SS2.fig1">
<div>
<div>
<figure id="alg1">
<figcaption><span><span id="alg1.2.1.1">Algorithm 1</span> </span> Generating sequence, client-side code</figcaption>
<div id="alg1.3">
<p><span><span id="alg1.l0.1.1.1">0:</span></span>  prefix_tokens, embeddings, known_servers

</p>
<p><span><span id="alg1.l1.1.1.1">1:</span></span>  generated_sequence = list()

</p>
<p><span><span id="alg1.l2.1.1.1">2:</span></span>  cache = dictionary()

</p>
<p><span><span id="alg1.l3.1.1.1">3:</span></span>  streams = dictionary()

</p>
<p><span><span id="alg1.l4.1.1.1">4:</span></span>  chain = find_best_chain(known_servers)

</p>
<p><span><span id="alg1.l5.1.1.1">5:</span></span>  <span id="alg1.l5.2">for</span> <math alttext="\text{server}\in\text{chain}" display="inline" id="alg1.l5.m1.1"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mtext id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2a.cmml">server</mtext><mo id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">∈</mo><mtext id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3a.cmml">chain</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><in id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1"></in><ci id="alg1.l5.m1.1.1.2a.cmml" xref="alg1.l5.m1.1.1.2"><mtext id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">server</mtext></ci><ci id="alg1.l5.m1.1.1.3a.cmml" xref="alg1.l5.m1.1.1.3"><mtext id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3">chain</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">\text{server}\in\text{chain}</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.1d">server ∈ chain</annotation></semantics></math> <span id="alg1.l5.3">do</span>
</p>
<p><span><span id="alg1.l6.1.1.1">6:</span></span>     streams[server] = <span id="alg1.l6.2">rpc_inference</span>(server)

</p>
<p><span><span id="alg1.l7.1.1.1">7:</span></span>     cache[server] = list()

</p>
<p><span><span id="alg1.l8.1.1.1">8:</span></span>  <span id="alg1.l8.2">end</span> <span id="alg1.l8.3">for</span>
</p>
<p><span><span id="alg1.l9.1.1.1">9:</span></span>  
</p>
<p><span><span id="alg1.l10.1.1.1">10:</span></span>  inputs = embeddings(prefix_tokens)

</p>
<p><span><span id="alg1.l11.1.1.1">11:</span></span>  <span id="alg1.l11.2">while</span> should_continue(generated_sequence) <span id="alg1.l11.3">do</span>
</p>
<p><span><span id="alg1.l12.1.1.1">12:</span></span>     tail_servers = copy(chain)

</p>
<p><span><span id="alg1.l13.1.1.1">13:</span></span>     <span id="alg1.l13.2">while</span> not empty(tail_servers) <span id="alg1.l13.3">do</span>
</p>
<p><span><span id="alg1.l14.1.1.1">14:</span></span>        server = tail_servers.pop_left()

</p>
<p><span><span id="alg1.l15.1.1.1">15:</span></span>        <span id="alg1.l15.2">try:</span>
</p>
<p><span><span id="alg1.l16.1.1.1">16:</span></span>              <math alttext="\triangleright" display="inline" id="alg1.l16.m1.1"><semantics id="alg1.l16.m1.1a"><mo id="alg1.l16.m1.1.1" xref="alg1.l16.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l16.m1.1b"><ci id="alg1.l16.m1.1.1.cmml" xref="alg1.l16.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l16.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l16.m1.1d">▷</annotation></semantics></math> Attempt normal inference

</p>
<p><span><span id="alg1.l17.1.1.1">17:</span></span>              outputs = streams[server].send(inputs)

</p>
<p><span><span id="alg1.l18.1.1.1">18:</span></span>              cache[server].append(inputs)

</p>
<p><span><span id="alg1.l19.1.1.1">19:</span></span>              inputs = outputs

</p>
<p><span><span id="alg1.l20.1.1.1">20:</span></span>        <span id="alg1.l20.2">catch</span> ServerFailed:

</p>
<p><span><span id="alg1.l21.1.1.1">21:</span></span>              <math alttext="\triangleright" display="inline" id="alg1.l21.m1.1"><semantics id="alg1.l21.m1.1a"><mo id="alg1.l21.m1.1.1" xref="alg1.l21.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l21.m1.1b"><ci id="alg1.l21.m1.1.1.cmml" xref="alg1.l21.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l21.m1.1d">▷</annotation></semantics></math> Replace the failed server

</p>
<p><span><span id="alg1.l22.1.1.1">22:</span></span>              streams.pop(server).close()

</p>
<p><span><span id="alg1.l23.1.1.1">23:</span></span>              past_inputs = cache.pop(server)

</p>
<p><span><span id="alg1.l24.1.1.1">24:</span></span>              new_servers = <span id="alg1.l24.2">replace_failed_server</span>(

</p>
<p><span><span id="alg1.l25.1.1.1">25:</span></span>                    server, past_inputs, cache,

</p>
<p><span><span id="alg1.l26.1.1.1">26:</span></span>                    streams, known_servers)

</p>
<p><span><span id="alg1.l27.1.1.1">27:</span></span>              chain.replace(server, new_servers)

</p>
<p><span><span id="alg1.l28.1.1.1">28:</span></span>              tail_servers.push_left(new_servers)

</p>
<p><span><span id="alg1.l29.1.1.1">29:</span></span>     <span id="alg1.l29.2">end</span> <span id="alg1.l29.3">while</span>
</p>
<p><span><span id="alg1.l30.1.1.1">30:</span></span>     
</p>
<p><span><span id="alg1.l31.1.1.1">31:</span></span>     logits = compute_logits(outputs, embeddings)

</p>
<p><span><span id="alg1.l32.1.1.1">32:</span></span>     next_token = choose_next(logits) {e.g. greedy}

</p>
<p><span><span id="alg1.l33.1.1.1">33:</span></span>     generated_sequence.append(next_token)

</p>
<p><span><span id="alg1.l34.1.1.1">34:</span></span>     inputs = embeddings(next_token)

</p>
<p><span><span id="alg1.l35.1.1.1">35:</span></span>  <span id="alg1.l35.2">end</span> <span id="alg1.l35.3">while</span>
</p>
<p><span><span id="alg1.l36.1.1.1">36:</span></span>  
</p>
<p><span><span id="alg1.l37.1.1.1">37:</span></span>  <span id="alg1.l37.2">for</span> <math alttext="\text{server}\in\text{chain}" display="inline" id="alg1.l37.m1.1"><semantics id="alg1.l37.m1.1a"><mrow id="alg1.l37.m1.1.1" xref="alg1.l37.m1.1.1.cmml"><mtext id="alg1.l37.m1.1.1.2" xref="alg1.l37.m1.1.1.2a.cmml">server</mtext><mo id="alg1.l37.m1.1.1.1" xref="alg1.l37.m1.1.1.1.cmml">∈</mo><mtext id="alg1.l37.m1.1.1.3" xref="alg1.l37.m1.1.1.3a.cmml">chain</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l37.m1.1b"><apply id="alg1.l37.m1.1.1.cmml" xref="alg1.l37.m1.1.1"><in id="alg1.l37.m1.1.1.1.cmml" xref="alg1.l37.m1.1.1.1"></in><ci id="alg1.l37.m1.1.1.2a.cmml" xref="alg1.l37.m1.1.1.2"><mtext id="alg1.l37.m1.1.1.2.cmml" xref="alg1.l37.m1.1.1.2">server</mtext></ci><ci id="alg1.l37.m1.1.1.3a.cmml" xref="alg1.l37.m1.1.1.3"><mtext id="alg1.l37.m1.1.1.3.cmml" xref="alg1.l37.m1.1.1.3">chain</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l37.m1.1c">\text{server}\in\text{chain}</annotation><annotation encoding="application/x-llamapun" id="alg1.l37.m1.1d">server ∈ chain</annotation></semantics></math> <span id="alg1.l37.3">do</span>
</p>
<p><span><span id="alg1.l38.1.1.1">38:</span></span>     streams[server].close()

</p>
<p><span><span id="alg1.l39.1.1.1">39:</span></span>  <span id="alg1.l39.2">end</span> <span id="alg1.l39.3">for</span>
</p>
<p><span><span id="alg1.l40.1.1.1">40:</span></span>  <span id="alg1.l40.2">return</span> generated_sequence

</p>
</div>
</figure>
</div>
<p><span id="S3.SS2.fig1.1">
<span id="alg2">
<span><span><span id="alg2.3.1.1">Algorithm 2</span> </span> <span id="alg2.4.2">rpc_inference</span>(server)</span>
<span id="alg2.5">
<span id="alg2.l0"><span><span id="alg2.l0.1.1.1">0:</span></span>  local_layers, stream

</span>
<span id="alg2.l1"><span><span id="alg2.l1.1.1.1">1:</span></span>  cache = dictionary()

</span>
<span id="alg2.l2"><span><span id="alg2.l2.1.1.1">2:</span></span>  <span id="alg2.l2.2">for</span> <math alttext="\text{layer}\in\text{local\_layers}" display="inline" id="alg2.l2.m1.1"><semantics id="alg2.l2.m1.1a"><mrow id="alg2.l2.m1.1.1" xref="alg2.l2.m1.1.1.cmml"><mtext id="alg2.l2.m1.1.1.2" xref="alg2.l2.m1.1.1.2a.cmml">layer</mtext><mo id="alg2.l2.m1.1.1.1" xref="alg2.l2.m1.1.1.1.cmml">∈</mo><mtext id="alg2.l2.m1.1.1.3" xref="alg2.l2.m1.1.1.3a.cmml">local_layers</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg2.l2.m1.1b"><apply id="alg2.l2.m1.1.1.cmml" xref="alg2.l2.m1.1.1"><in id="alg2.l2.m1.1.1.1.cmml" xref="alg2.l2.m1.1.1.1"></in><ci id="alg2.l2.m1.1.1.2a.cmml" xref="alg2.l2.m1.1.1.2"><mtext id="alg2.l2.m1.1.1.2.cmml" xref="alg2.l2.m1.1.1.2">layer</mtext></ci><ci id="alg2.l2.m1.1.1.3a.cmml" xref="alg2.l2.m1.1.1.3"><mtext id="alg2.l2.m1.1.1.3.cmml" xref="alg2.l2.m1.1.1.3">local_layers</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l2.m1.1c">\text{layer}\in\text{local\_layers}</annotation><annotation encoding="application/x-llamapun" id="alg2.l2.m1.1d">layer ∈ local_layers</annotation></semantics></math> <span id="alg2.l2.3">do</span>
</span>
<span id="alg2.l3"><span><span id="alg2.l3.1.1.1">3:</span></span>     cache[layer] = make_empty()

</span>
<span id="alg2.l4"><span><span id="alg2.l4.1.1.1">4:</span></span>  <span id="alg2.l4.2">end</span> <span id="alg2.l4.3">for</span>
</span>
<span id="alg2.l5"><span><span id="alg2.l5.1.1.1">5:</span></span>  <span id="alg2.l5.2">while</span> not stream.closed() <span id="alg2.l5.3">do</span>
</span>
<span id="alg2.l6"><span><span id="alg2.l6.1.1.1">6:</span></span>     inputs = stream.receive()

</span>
<span id="alg2.l7"><span><span id="alg2.l7.1.1.1">7:</span></span>     <span id="alg2.l7.2">for</span> <math alttext="\text{layer}\in\text{local\_layers}" display="inline" id="alg2.l7.m1.1"><semantics id="alg2.l7.m1.1a"><mrow id="alg2.l7.m1.1.1" xref="alg2.l7.m1.1.1.cmml"><mtext id="alg2.l7.m1.1.1.2" xref="alg2.l7.m1.1.1.2a.cmml">layer</mtext><mo id="alg2.l7.m1.1.1.1" xref="alg2.l7.m1.1.1.1.cmml">∈</mo><mtext id="alg2.l7.m1.1.1.3" xref="alg2.l7.m1.1.1.3a.cmml">local_layers</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg2.l7.m1.1b"><apply id="alg2.l7.m1.1.1.cmml" xref="alg2.l7.m1.1.1"><in id="alg2.l7.m1.1.1.1.cmml" xref="alg2.l7.m1.1.1.1"></in><ci id="alg2.l7.m1.1.1.2a.cmml" xref="alg2.l7.m1.1.1.2"><mtext id="alg2.l7.m1.1.1.2.cmml" xref="alg2.l7.m1.1.1.2">layer</mtext></ci><ci id="alg2.l7.m1.1.1.3a.cmml" xref="alg2.l7.m1.1.1.3"><mtext id="alg2.l7.m1.1.1.3.cmml" xref="alg2.l7.m1.1.1.3">local_layers</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l7.m1.1c">\text{layer}\in\text{local\_layers}</annotation><annotation encoding="application/x-llamapun" id="alg2.l7.m1.1d">layer ∈ local_layers</annotation></semantics></math> <span id="alg2.l7.3">do</span>
</span>
<span id="alg2.l8"><span><span id="alg2.l8.1.1.1">8:</span></span>        past_kv = cache[layer]

</span>
<span id="alg2.l9"><span><span id="alg2.l9.1.1.1">9:</span></span>        inputs, new_kv = forward(

</span>
<span id="alg2.l10"><span><span id="alg2.l10.1.1.1">10:</span></span>              layer, inputs, past_kv)

</span>
<span id="alg2.l11"><span><span id="alg2.l11.1.1.1">11:</span></span>        cache[layer].append(new_kv)

</span>
<span id="alg2.l12"><span><span id="alg2.l12.1.1.1">12:</span></span>     <span id="alg2.l12.2">end</span> <span id="alg2.l12.3">for</span>
</span>
<span id="alg2.l13"><span><span id="alg2.l13.1.1.1">13:</span></span>     stream.send(inputs)

</span>
<span id="alg2.l14"><span><span id="alg2.l14.1.1.1">14:</span></span>  <span id="alg2.l14.2">end</span> <span id="alg2.l14.3">while</span>
</span>
</span>
</span>
<span id="alg3">
<span><span><span id="alg3.3.1.1">Algorithm 3</span> </span> <span id="alg3.4.2">replace_failed_server</span>(…)</span>
<span id="alg3.5">
<span id="alg3.l0"><span><span id="alg3.l0.1.1.1">0:</span></span>  server, inputs, cache, streams, known_servers

</span>
<span id="alg3.l1"><span><span id="alg3.l1.1.1.1">1:</span></span>  known_servers.ban(server)

</span>
<span id="alg3.l2"><span><span id="alg3.l2.1.1.1">2:</span></span>  missing_layers = get_layers(server)

</span>
<span id="alg3.l3"><span><span id="alg3.l3.1.1.1">3:</span></span>  chains = select_by_layer(

</span>
<span id="alg3.l4"><span><span id="alg3.l4.1.1.1">4:</span></span>      known_servers, missing_layers)

</span>
<span id="alg3.l5"><span><span id="alg3.l5.1.1.1">5:</span></span>  chain = find_best_chain(chains)

</span>
<span id="alg3.l6"><span><span id="alg3.l6.1.1.1">6:</span></span>  replacements = list()

</span>
<span id="alg3.l7"><span><span id="alg3.l7.1.1.1">7:</span></span>  <span id="alg3.l7.2">while</span> not empty(chain) <span id="alg3.l7.3">do</span>
</span>
<span id="alg3.l8"><span><span id="alg3.l8.1.1.1">8:</span></span>      s = chain.pop_left()

</span>
<span id="alg3.l9"><span><span id="alg3.l9.1.1.1">9:</span></span>      <span id="alg3.l9.2">try:</span>
</span>
<span id="alg3.l10"><span><span id="alg3.l10.1.1.1">10:</span></span>        streams[s] = <span id="alg3.l10.2">rpc_inference</span>(s)

</span>
<span id="alg3.l11"><span><span id="alg3.l11.1.1.1">11:</span></span>        outputs = streams[s].send(inputs)

</span>
<span id="alg3.l12"><span><span id="alg3.l12.1.1.1">12:</span></span>        replacements.append(s)

</span>
<span id="alg3.l13"><span><span id="alg3.l13.1.1.1">13:</span></span>        cache[s] = inputs

</span>
<span id="alg3.l14"><span><span id="alg3.l14.1.1.1">14:</span></span>        missing_layers.pop(get_layers(s))

</span>
<span id="alg3.l15"><span><span id="alg3.l15.1.1.1">15:</span></span>        inputs = outputs

</span>
<span id="alg3.l16"><span><span id="alg3.l16.1.1.1">16:</span></span>      <span id="alg3.l16.2">catch</span> FailedRPC:

</span>
<span id="alg3.l17"><span><span id="alg3.l17.1.1.1">17:</span></span>        known_servers.ban(s)

</span>
<span id="alg3.l18"><span><span id="alg3.l18.1.1.1">18:</span></span>        chains = select_by_layer(

</span>
<span id="alg3.l19"><span><span id="alg3.l19.1.1.1">19:</span></span>          chains, missing_layers)

</span>
<span id="alg3.l20"><span><span id="alg3.l20.1.1.1">20:</span></span>        chain = find_best_chain(chains)

</span>
<span id="alg3.l21"><span><span id="alg3.l21.1.1.1">21:</span></span>  <span id="alg3.l21.2">end</span> <span id="alg3.l21.3">while</span>
</span>
<span id="alg3.l22"><span><span id="alg3.l22.1.1.1">22:</span></span>  <span id="alg3.l22.2">return</span> chain

</span>
</span>
</span></span></p>
</div>
</figure>
<p id="S3.SS2.p7.2">When servers fail, the algorithm needs to send <math alttext="O(t)" display="inline" id="S3.SS2.p7.1.m1.1"><semantics id="S3.SS2.p7.1.m1.1a"><mrow id="S3.SS2.p7.1.m1.1.2" xref="S3.SS2.p7.1.m1.1.2.cmml"><mi id="S3.SS2.p7.1.m1.1.2.2" xref="S3.SS2.p7.1.m1.1.2.2.cmml">O</mi><mo id="S3.SS2.p7.1.m1.1.2.1" xref="S3.SS2.p7.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p7.1.m1.1.2.3.2" xref="S3.SS2.p7.1.m1.1.2.cmml"><mo id="S3.SS2.p7.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS2.p7.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">t</mi><mo id="S3.SS2.p7.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS2.p7.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><apply id="S3.SS2.p7.1.m1.1.2.cmml" xref="S3.SS2.p7.1.m1.1.2"><times id="S3.SS2.p7.1.m1.1.2.1.cmml" xref="S3.SS2.p7.1.m1.1.2.1"></times><ci id="S3.SS2.p7.1.m1.1.2.2.cmml" xref="S3.SS2.p7.1.m1.1.2.2">𝑂</ci><ci id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">O(t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.1.m1.1d">italic_O ( italic_t )</annotation></semantics></math> data (in one round) for each failed server and compute only the stages held by the failed servers. This can be seen as an interpolation between naive and cached inference, depending on the server failure rate. If none of the servers fail, we recover <math alttext="O(n\cdot t)" display="inline" id="S3.SS2.p7.2.m2.1"><semantics id="S3.SS2.p7.2.m2.1a"><mrow id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml"><mi id="S3.SS2.p7.2.m2.1.1.3" xref="S3.SS2.p7.2.m2.1.1.3.cmml">O</mi><mo id="S3.SS2.p7.2.m2.1.1.2" xref="S3.SS2.p7.2.m2.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p7.2.m2.1.1.1.1" xref="S3.SS2.p7.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS2.p7.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS2.p7.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p7.2.m2.1.1.1.1.1" xref="S3.SS2.p7.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.p7.2.m2.1.1.1.1.1.2" xref="S3.SS2.p7.2.m2.1.1.1.1.1.2.cmml">n</mi><mo id="S3.SS2.p7.2.m2.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p7.2.m2.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.SS2.p7.2.m2.1.1.1.1.1.3" xref="S3.SS2.p7.2.m2.1.1.1.1.1.3.cmml">t</mi></mrow><mo id="S3.SS2.p7.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS2.p7.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><apply id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1"><times id="S3.SS2.p7.2.m2.1.1.2.cmml" xref="S3.SS2.p7.2.m2.1.1.2"></times><ci id="S3.SS2.p7.2.m2.1.1.3.cmml" xref="S3.SS2.p7.2.m2.1.1.3">𝑂</ci><apply id="S3.SS2.p7.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1.1.1"><ci id="S3.SS2.p7.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1.1.1.1.1">⋅</ci><ci id="S3.SS2.p7.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p7.2.m2.1.1.1.1.1.2">𝑛</ci><ci id="S3.SS2.p7.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p7.2.m2.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">O(n\cdot t)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.2.m2.1d">italic_O ( italic_n ⋅ italic_t )</annotation></semantics></math> communication, similarly to <cite>Aminabadi et al. (<a href="#bib.bib2" title="">2022</a>)</cite>. In turn, if all servers fail after one step, the algorithm effectively performs non-caching generation, which is the best option in that scenario.</p>
<p id="S3.SS2.p8.1">In the basic formulation, all communication between pipeline stages is routed through the client, i.e. the client receives the outputs of every pipeline stage, caches it and sends it to the subsequent stage. In practice, it is more efficient to let pipeline stages communicate directly: once the server obtains output activations, it sends them to both client and the subsequent stage. This reduces the total step time since both messages are a few kilobytes in size an can be sent in parallel. To verify that both client and the next pipeline stage received the same set of activations, they can verify the checksums (i.e. hash values) of the received activations asynchronously, without blocking computation.</p>
<p id="S3.SS2.p9.1">Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.2 Distributed generation with fault tolerance ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a> can support greedy inference or any sampling variants (including <cite>Holtzman et al. (<a href="#bib.bib21" title="">2020</a>)</cite>). However, it requires one more step to support search-based algorithms such as beam search: cache reordering. This allows a client to generate multiple continuations of the same input prefix by cloning its attention cache and dropping less likely hypotheses. We describe beam search in Appendix <a href="#A3" title="Appendix C Extension to beam search algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>C</span></a>.</p>
<p id="S3.SS2.p10.1"><span id="S3.SS2.p10.1.1">Shortest path routing.</span>
In the Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.2 Distributed generation with fault tolerance ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a>, the <span id="S3.SS2.p10.1.2">find_best_chain</span> function (line 4) selects a sequence of servers that can run the required layers in the least amount of time. To estimate this time we add up two factors: computation time, determined by server’s compute throughput (“GPU speed”) and the network latency between the client and that server. Servers measure their own compute throughput and share this information with the clients. In turn, clients measure the network latency between them and a given server by “pinging” the candidate servers during routing. If a server runs multiple consecutive blocks, we multiply the computation time by the number of blocks.</p>
<p id="S3.SS2.p11.1">To find the best chain of servers, clients find the shortest path between the first and last block, using a graph where edge weights correspond to server inference time, as described in the previous paragraph. To minimize overhead, we do not run pathfinding from scratch on each call to <span id="S3.SS2.p11.1.1">find_best_chain</span>. Instead, clients run lifelong pathfinding in the background and reuse it between inference calls. More specifically, we use the <math alttext="\text{D}^{*}" display="inline" id="S3.SS2.p11.1.m1.1"><semantics id="S3.SS2.p11.1.m1.1a"><msup id="S3.SS2.p11.1.m1.1.1" xref="S3.SS2.p11.1.m1.1.1.cmml"><mtext id="S3.SS2.p11.1.m1.1.1.2" xref="S3.SS2.p11.1.m1.1.1.2a.cmml">D</mtext><mo id="S3.SS2.p11.1.m1.1.1.3" xref="S3.SS2.p11.1.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p11.1.m1.1b"><apply id="S3.SS2.p11.1.m1.1.1.cmml" xref="S3.SS2.p11.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p11.1.m1.1.1.1.cmml" xref="S3.SS2.p11.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p11.1.m1.1.1.2a.cmml" xref="S3.SS2.p11.1.m1.1.1.2"><mtext id="S3.SS2.p11.1.m1.1.1.2.cmml" xref="S3.SS2.p11.1.m1.1.1.2">D</mtext></ci><times id="S3.SS2.p11.1.m1.1.1.3.cmml" xref="S3.SS2.p11.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p11.1.m1.1c">\text{D}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p11.1.m1.1d">D start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math> Lite <cite>(Koenig &amp; Likhachev, <a href="#bib.bib29" title="">2005</a>)</cite> algorithm because it allows clients to quickly adjust paths after a server is banned or leaves the network.</p>
</section>
<section id="S3.SS3">
<h3>
<span>3.3 </span>Automatic load balancing</h3>
<p id="S3.SS3.p1.1">In order to run inference or fine-tuning, each server needs to be assigned to a pipeline stage, then reassigned if other servers join or leave the network. For example, if we deploy an LLM on idle compute resources from several data centers or labs, the number of participants may change over time based on the demand. Moreover, servers may have different compute throughput, network bandwidth, and geographical location. To operate in these conditions efficiently, servers should automatically choose which model layers they should serve in a given situation.</p>
<p id="S3.SS3.p2.1">To that end, servers periodically run a load balancing procedure and switch to new blocks if necessary.
Formally, servers choose blocks so as to maximize the total system throughput (tokens per second).
Each server periodically announces its blocks and empirically measured throughput to a distributed hash table <cite>(Maymounkov &amp; Mazieres, <a href="#bib.bib40" title="">2002</a>)</cite>. When a new server joins, it uses this information to identify a contiguous interval of blocks that would increase the total system throughput the most.</p>
<p id="S3.SS3.p3.1">Since peers may leave or fail at any time, all nodes periodically check if launching a rebalancing procedure would significantly improve the overall throughput. If it is the case, they switch layers until the throughput becomes near-optimal. In particular, if all peers serving certain blocks suddenly leave the system, this procedure quickly redistributes the remaining resources to close the emerged gaps.</p>
<p id="S3.SS3.p4.1">We provide a detailed description of the load balancing algorithms in Appendix <a href="#A4" title="Appendix D Details of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>D</span></a> and validate their properties in experiments reported in Appendix <a href="#A5" title="Appendix E Evaluation of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>E</span></a>.</p>
</section>
<section id="S3.SS4">
<h3>
<span>3.4 </span>Parameter-efficient fine-tuning</h3>
<p id="S3.SS4.p1.1">While LLMs achieve high quality on many problems with simple prompt engineering <cite>(Brown et al., <a href="#bib.bib9" title="">2020</a>)</cite>, they often need training to achieve the best results. Traditionally, this is done by fine-tuning all model parameters on the downstream task.
However, for extremely large models, this strategy becomes impractical due to hardware requirements. For example, fine-tuning BLOOM-176B with Adam would require almost 3 TB of GPU memory to store the model, gradients, and optimizer states.</p>
<p id="S3.SS4.p2.1">Fortunately, <span id="S3.SS4.p2.1.1">parameter-efficient fine-tuning</span> methods have been developed that keep most of the pretrained model intact. Some of them choose a subset of existing parameters to update <cite>(Sung et al., <a href="#bib.bib55" title="">2021</a>; Guo et al., <a href="#bib.bib20" title="">2021</a>)</cite> while others augment the model with additional trainable weights <cite>(Hu et al., <a href="#bib.bib23" title="">2021</a>; Houlsby et al., <a href="#bib.bib22" title="">2019</a>; Liu et al., <a href="#bib.bib39" title="">2021b</a>; Lester et al., <a href="#bib.bib34" title="">2021</a>; Liu et al., <a href="#bib.bib38" title="">2021a</a>, <a href="#bib.bib37" title="">2022b</a>)</cite>. Despite their lower memory requirements, parameter-efficient approaches are often competitive with full model fine-tuning <cite>(Hu et al., <a href="#bib.bib23" title="">2021</a>; Liu et al., <a href="#bib.bib38" title="">2021a</a>; Yong &amp; Nikoulina, <a href="#bib.bib64" title="">2022</a>)</cite> and even outperform it in low-data regimes <cite>(Liu et al., <a href="#bib.bib36" title="">2022a</a>)</cite>. Another appealing property of these approaches for our use-case is that they allow rapidly switching a pretrained LLM between adapters.</p>
<p id="S3.SS4.p3.1">By focusing on parameter-efficient fine-tuning, we are able to simplify the system design by <span id="S3.SS4.p3.1.1">making clients responsible for storing their trainable parameters</span> (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a>). Servers can run backpropagation through their layers and return gradients with respect to activations, but they <span id="S3.SS4.p3.1.2">do not update the server-side parameters</span>.
Even when client communicates learned values (e.g. soft prompts) to a server, the server treats these values same as input activations. Thus, a server can simultaneously run different fine-tuning tasks without them interfering with one another.
This design choice also allows users to define custom adapters in simple PyTorch without having network engineering expertise.</p>
<p id="S3.SS4.p4.1">Unlike inference, fine-tuning forward and backward passes process the entire batch at one go and do not need to store past attention caches between successive client requests. Thus, in case of a failure, we can discard the incomplete forward/backward pass and just repeat the previous forward/backward pass request. This algorithm behaves similarly to the cache-less baseline from Section <a href="#S4.SS1" title="4.1 Inference with unreliable servers ‣ 4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>4.1</span></a>.</p>
</section>
<section id="S3.SS5">
<h3>
<span>3.5 </span>Implementation details</h3>
<p id="S3.SS5.p1.1">Since our main intended use-case is running on inexpensive low-end devices, we need to work around their capabilities.
In terms of raw FLOPs, even consumer-grade GPUs like GeForce RTX 3070 could run a complete inference step of BLOOM-176B in less than a second <cite>(NVIDIA, <a href="#bib.bib43" title="">2020</a>)</cite>. However, the GPU memory can only hold a small fraction of model layers: running naïvely would require 44 RTX 3070 GPUs and 44 communication rounds.
To make this more efficient, we use quantization to store more parameters per GPU, reducing the number of consecutive devices and communication rounds.</p>
<p id="S3.SS5.p2.1">One option for quantization is to use 8-bit mixed matrix decomposition for matrix multiplication to quantize the weights to 8-bit precision and reduce the memory footprint compared to 16-bit weights, as suggested in <cite>Dettmers et al. (<a href="#bib.bib11" title="">2022a</a>)</cite>. This decomposition separates hidden states and weights into two portions: about 0.1% of 16-bit outlier and 99.9% of 8-bit regular values, which roughly halves the memory footprint with negligible effect on the model quality (see evaluations in Appendix <a href="#A1" title="Appendix A Quality and efficiency of BLOOM with 8-bit quantization ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>A</span></a>). Another option is to use the 4-bit NormalFloat format <cite>(Dettmers et al., <a href="#bib.bib13" title="">2023</a>)</cite>.</p>
<p id="S3.SS5.p3.1">To send less data between subsequent pipeline stages, we apply dynamic blockwise quantization <cite>(Dettmers et al., <a href="#bib.bib12" title="">2022b</a>)</cite> to the hidden states before pipeline-parallel communication, which halves the bandwidth requirements without any noticeable effect on generation quality <cite>(Ryabinin et al., <a href="#bib.bib52" title="">2023</a>)</cite>.
During fine-tuning, we also take advantage of gradient checkpointing <cite>(Griewank &amp; Walther, <a href="#bib.bib19" title="">2000</a>; Chen et al., <a href="#bib.bib10" title="">2016</a>)</cite> and half precision to reduce VRAM usage — both are standard practice for large language models <cite>(Narayanan et al., <a href="#bib.bib42" title="">2021</a>; Brown et al., <a href="#bib.bib9" title="">2020</a>; Athlur et al., <a href="#bib.bib3" title="">2022</a>)</cite>. In experiments, we apply the same optimizations to baseline systems for a fair comparison.</p>
</section>
</section>
<section id="S4">
<h2>
<span>4 </span>Experiments</h2>
<section id="S4.SS1">
<h3>
<span>4.1 </span>Inference with unreliable servers</h3>
<p id="S4.SS1.p1.1">First, we conduct small-scale preliminary experiments to test the fault-tolerant generation algorithm described in Section <a href="#S3.SS2" title="3.2 Distributed generation with fault tolerance ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3.2</span></a>.
For these experiments, we use a smaller BLOOM model with 7.1 billion parameters <cite>(BigScience, <a href="#bib.bib6" title="">2022b</a>)</cite>. This model contains 30 transformer blocks with hidden size 4096. We compare our algorithm with baselines when generating a single sequence of length 512. For simplicity, we run all computations and communications in single precision and disregard word embeddings and logits for this set of experiments. We measure the time to run a certain number of tokens through all blocks and simulate failures by resetting pipeline stages at a certain rate.</p>
<div id="S4.SS1.p2">
<p id="S4.SS1.p2.1">We compare three inference strategies:</p>
<ol id="S4.I1">
<li id="S4.I1.i1">
<span>1.</span>
<p id="S4.I1.i1.p1.1"><span id="S4.I1.i1.p1.1.1">Caching with restarts</span>, which refers to standard inference with servers storing attention caches. On failure, it restarts the entire generation from scratch since the failed server’s caches are lost.</p>
</li>
<li id="S4.I1.i2">
<span>2.</span>
<p id="S4.I1.i2.p1.1"><span id="S4.I1.i2.p1.1.1">Cache-less inference</span>, which reruns past tokens on every step. On failure, it restarts only the last generation step.</p>
</li>
<li id="S4.I1.i3">
<span>3.</span>
<p id="S4.I1.i3.p1.1"><span id="S4.I1.i3.p1.1.1">Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.2 Distributed generation with fault tolerance ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a></span>, which is specifically designed for fault-tolerant inference.</p>
</li>
</ol>
</div>
<p id="S4.SS1.p3.1">All runs use four pipeline stages with (8, 7, 8, 7) model layers per pipeline stage. Each pipeline stage is served by a single GeForce 1080 Ti GPU; the four GPUs are running in a single system with dual Xeon Gold 6148 CPU, 12 DDR4 LRDIMM sticks with 64 GB each. The system has 16 dedicated PCIe Gen. 3 lanes per GPU in dual root configuration, without using PCIe switches. Each stage runs in an isolated Docker containers with virtual network interfaces, but there is no limit to communication bandwidth for this experiment. We repeat all experiments 50 times and report the average time. The adjusted standard deviation never exceeds 0.2%. We use the pipeline parallelism implementation from Megatron-DeepSpeed <cite>(BigScience et al., <a href="#bib.bib7" title="">2022</a>)</cite> for the cache-less baseline.</p>
<figure id="S4.T1">
<figcaption><span>Table 1: </span>Sequential inference speed (steps/second) of BLOOM (7.1B) with varying failure rates. A failure rate <math alttext="p" display="inline" id="S4.T1.3.m1.1"><semantics id="S4.T1.3.m1.1b"><mi id="S4.T1.3.m1.1.1" xref="S4.T1.3.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.T1.3.m1.1c"><ci id="S4.T1.3.m1.1.1.cmml" xref="S4.T1.3.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.m1.1e">italic_p</annotation></semantics></math> means that sending any set of activations to the next stage of the pipeline fails with probability <math alttext="p" display="inline" id="S4.T1.4.m2.1"><semantics id="S4.T1.4.m2.1b"><mi id="S4.T1.4.m2.1.1" xref="S4.T1.4.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.T1.4.m2.1c"><ci id="S4.T1.4.m2.1.1.cmml" xref="S4.T1.4.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.m2.1d">p</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.m2.1e">italic_p</annotation></semantics></math>. Missing values mean that the algorithm did not finish within 1 hour.</figcaption>
<table id="S4.T1.5">
<tbody>
<tr id="S4.T1.5.1.1">
<td id="S4.T1.5.1.1.1" rowspan="2"><span id="S4.T1.5.1.1.1.1">Inference Algorithm</span></td>
<td colspan="4" id="S4.T1.5.1.1.2"><span id="S4.T1.5.1.1.2.1">128 tokens, failure rate:</span></td>
<td colspan="4" id="S4.T1.5.1.1.3"><span id="S4.T1.5.1.1.3.1">1024 tokens, failure rate:</span></td>
</tr>
<tr id="S4.T1.5.2.2">
<td id="S4.T1.5.2.2.1">0</td>
<td id="S4.T1.5.2.2.2">1e-4</td>
<td id="S4.T1.5.2.2.3">1e-3</td>
<td id="S4.T1.5.2.2.4">1e-2</td>
<td id="S4.T1.5.2.2.5">0</td>
<td id="S4.T1.5.2.2.6">1e-4</td>
<td id="S4.T1.5.2.2.7">1e-3</td>
<td id="S4.T1.5.2.2.8">1e-2</td>
</tr>
<tr id="S4.T1.5.3.3">
<td id="S4.T1.5.3.3.1">Caching with restarts</td>
<td id="S4.T1.5.3.3.2">17.1</td>
<td id="S4.T1.5.3.3.3">16.7</td>
<td id="S4.T1.5.3.3.4">12</td>
<td id="S4.T1.5.3.3.5">0.18</td>
<td id="S4.T1.5.3.3.6">15.5</td>
<td id="S4.T1.5.3.3.7">11.8</td>
<td id="S4.T1.5.3.3.8">0.48</td>
<td id="S4.T1.5.3.3.9">–</td>
</tr>
<tr id="S4.T1.5.4.4">
<td id="S4.T1.5.4.4.1">Cache-less inference</td>
<td id="S4.T1.5.4.4.2">3.44</td>
<td id="S4.T1.5.4.4.3">3.44</td>
<td id="S4.T1.5.4.4.4">3.44</td>
<td id="S4.T1.5.4.4.5">3.44</td>
<td id="S4.T1.5.4.4.6">0.89</td>
<td id="S4.T1.5.4.4.7">0.89</td>
<td id="S4.T1.5.4.4.8">0.89</td>
<td id="S4.T1.5.4.4.9">0.89</td>
</tr>
<tr id="S4.T1.5.5.5">
<td id="S4.T1.5.5.5.1">Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.2 Distributed generation with fault tolerance ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a> (ours)</td>
<td id="S4.T1.5.5.5.2">11.4</td>
<td id="S4.T1.5.5.5.3">11.4</td>
<td id="S4.T1.5.5.5.4">10.6</td>
<td id="S4.T1.5.5.5.5">3.38</td>
<td id="S4.T1.5.5.5.6">10.7</td>
<td id="S4.T1.5.5.5.7">10.7</td>
<td id="S4.T1.5.5.5.8">7.76</td>
<td id="S4.T1.5.5.5.9">2.17</td>
</tr>
</tbody>
</table>
</figure>
<p id="S4.SS1.p4.1">We report performance measurements in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Inference with unreliable servers ‣ 4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a>. Unlike baselines, our algorithm provides reasonable performance <span id="S4.SS1.p4.1.1">in all tested conditions</span>, especially for higher failure rates (common for communicating over the Internet, using spot/preemptible instances or unreliable hardware). Caching with restarts is most efficient for inference without failures, with our algorithm being somewhat slower due to less mature implementation. Finally, the cache-less inference can be competitive for short sequences (128 tokens), but slows down considerably on 1024 tokens, which agrees with our intuition from <a href="#S3.SS1" title="3.1 Performance bottlenecks of LLM inference ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3.1</span></a>.</p>
<p id="S4.SS1.p5.1">We provide plots showing additional evaluations for a wider range of failure rates (up to 5%) and sequence lengths (up to 2048 tokens) in Appendix <a href="#A6" title="Appendix F Experiments with a wider range of failure rates ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>F</span></a> (Figure <a href="#A6.F3" title="Figure 3 ‣ Appendix F Experiments with a wider range of failure rates ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3</span></a>).</p>
</section>
<section id="S4.SS2">
<h3>
<span>4.2 </span>Experiments for Llama 2 (70B) and BLOOM (176B)</h3>
<figure id="S4.T3">
<figcaption><span>Table 2: </span>Performance of Llama 2 (70B) sequential inference steps and parallel forward passes. The network parameters refer to
bidirectional bandwidth and round-trip latency (RTT).</figcaption>
<figcaption><span>Table 3: </span>Performance of BLOOM (176B) sequential inference steps and parallel forward passes.</figcaption><div>
<div>

</div>

<div>

</div>
</div>
<figcaption><span>Table 3: </span>Performance of BLOOM (176B) sequential inference steps and parallel forward passes.</figcaption>
</figure>
<p id="S4.SS2.p1.1">In this section, we evaluate our system on more practical tasks of running Llama 2 (70B) <cite>(Touvron et al., <a href="#bib.bib59" title="">2023b</a>)</cite> and BLOOM (176B) <cite>(BigScience, <a href="#bib.bib5" title="">2022a</a>)</cite>.
First, we consider servers running in a network with controlled bandwidth and latency. We measure performance for <span id="S4.SS2.p1.1.1">(a)</span> Llama 2 distributed across 3 servers with a T4 GPU each, <span id="S4.SS2.p1.1.2">(b)</span> BLOOM distributed across 3 servers with an A100 (80 GB) GPU each, and <span id="S4.SS2.p1.1.3">(c)</span> BLOOM distributed across 10 servers with an RTX 3090 GPU each. We use 4-bit NormalFloat quantization <cite>(Dettmers et al., <a href="#bib.bib13" title="">2023</a>)</cite> for Llama 2 and 8-bit matrix decomposition <cite>(Dettmers et al., <a href="#bib.bib11" title="">2022a</a>)</cite> for BLOOM in all evaluations including the baselines below.</p>
<div id="S4.SS2.p2">
<p id="S4.SS2.p2.1">We report performance of:</p>
<ul id="S4.I2">
<li id="S4.I2.i1">
<span>•</span>
<p id="S4.I2.i1.p1.1"><span id="S4.I2.i1.p1.1.1">Sequential (autoregressive) inference</span> for batch size 1 (i.e., each step generates 1 token). It is measured in generation steps per second a client can do and shows the <span id="S4.I2.i1.p1.1.2">generation latency</span>.</p>
</li>
<li id="S4.I2.i2">
<span>•</span>
<p id="S4.I2.i2.p1.1"><span id="S4.I2.i2.p1.1.1">Parallel forward passes</span> for batches of 128-token sequences. It is measured in tokens per second a client can process. This shows the <span id="S4.I2.i2.p1.1.2">system’s throughput</span> during batch processing and fine-tuning.</p>
</li>
</ul>
</div>
<p id="S4.SS2.p3.1">Since the backward pass performance depends on a set of trainable weights, batch size, and other hyperparameters, we report its performance in different setups separately in Appendix <a href="#A7" title="Appendix G Performance of training-time forward and backward passes ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>G</span></a>.</p>
<section id="S4.SS2.SSS0.Px1">
<h4>Concurrent clients.</h4>
<p id="S4.SS2.SSS0.Px1.p1.1">We also investigate the effect of having concurrent clients. We assume that each server
belongs to a different person, and multiple people (possibly, all of them) are interested in running inference or fine-tuning at the same time. In order to do that, they run the client interacting with our distributed system. The client runs on the same machine, uses 8 CPU cores and no GPU.
We report the speed of sequential inference and parallel forward passes that <span id="S4.SS2.SSS0.Px1.p1.1.1">each client gets on average.</span></p>
</section>
<section id="S4.SS2.SSS0.Px2">
<h4>Offloading baseline.</h4>
<p id="S4.SS2.SSS0.Px2.p1.2">We also evaluate parameter offloading, where each user runs independently on a single GPU, swapping parameters from CPU memory.
First, we report the actual throughput of RAM offloading in case of DeepSpeed with default recommended parameters and enabled <span id="S4.SS2.SSS0.Px2.p1.2.1">pin_memory</span> (gives <math alttext="1.2{-}2\times" display="inline" id="S4.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS2.SSS0.Px2.p1.2.m2.1b"><mn id="S4.SS2.SSS0.Px2.p1.2.m2.1.1">1.2</mn><mo id="S4.SS2.SSS0.Px2.p1.2.m2.1.2">−</mo><mn id="S4.SS2.SSS0.Px2.p1.2.m2.1.3">2</mn><mo id="S4.SS2.SSS0.Px2.p1.2.m2.1.4" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.2.m2.1c">1.2{-}2\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px2.p1.2.m2.1d">1.2 - 2 ×</annotation></semantics></math> speedup).
Next, we report the <span id="S4.SS2.SSS0.Px2.p1.2.2">theoretical-best</span> throughput the offloading baseline can reach for BLOOM. It is calculated as a maximal throughput in the best hardware setup possible (CPU RAM offloading via PCIe 4.0 with 16 PCIe lanes), assuming infinite GPU performance. The calculations are detailed in Appendix <a href="#A2" title="Appendix B Estimating theoretical best throughput with RAM offloading ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>B</span></a>.</p>
</section>
<section id="S4.SS2.SSS0.Px3">
<h4>Local pipeline parallelism (NVLink).</h4>
<p id="S4.SS2.SSS0.Px3.p1.1">Next, we report performance for BLOOM running on a server with 3<math alttext="\times" display="inline" id="S4.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px3.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.1.m1.1b"><times id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.1.m1.1d">×</annotation></semantics></math> A100 (80 GB) GPUs. In this setup, a single server has enough GPU memory to load the entire model, which provides an <span id="S4.SS2.SSS0.Px3.p1.1.1">upper bound</span> for performance reachable with these GPUs. This setup runs pipeline-parallelism from DeepSpeed v0.7.7.</p>
</section>
<section id="S4.SS2.SSS0.Px4">
<h4>Heterogeneous servers.</h4>
<p id="S4.SS2.SSS0.Px4.p1.1">To validate that our system works on heterogeneous hardware, we simulate 12 heterogeneous devices by partitioning each A100 (80 GB) into several virtual servers (3 large and 1 small). We get 9 servers hosting 7 blocks each, one server with 3 blocks and two more servers with 2 blocks (70 blocks in total, as required for BLOOM). Additionally, we benchmark the system on real heterogeneous GPUs with diverse compute capabilities in the &#34;Real-world setup&#34; below.</p>
</section>
<section id="S4.SS2.SSS0.Px5">
<h4>Real-world setup.</h4>
<p id="S4.SS2.SSS0.Px5.p1.5">Finally, we benchmark BLOOM in a real-world setup with 14 smaller servers holding 2<math alttext="\times" display="inline" id="S4.SS2.SSS0.Px5.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px5.p1.1.m1.1a"><mo id="S4.SS2.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px5.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p1.1.m1.1b"><times id="S4.SS2.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px5.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px5.p1.1.m1.1d">×</annotation></semantics></math>RTX 3060, 4<math alttext="\times" display="inline" id="S4.SS2.SSS0.Px5.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px5.p1.2.m2.1a"><mo id="S4.SS2.SSS0.Px5.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px5.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p1.2.m2.1b"><times id="S4.SS2.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px5.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px5.p1.2.m2.1d">×</annotation></semantics></math>2080Ti, 2<math alttext="\times" display="inline" id="S4.SS2.SSS0.Px5.p1.3.m3.1"><semantics id="S4.SS2.SSS0.Px5.p1.3.m3.1a"><mo id="S4.SS2.SSS0.Px5.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px5.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p1.3.m3.1b"><times id="S4.SS2.SSS0.Px5.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px5.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px5.p1.3.m3.1d">×</annotation></semantics></math>3090, 2<math alttext="\times" display="inline" id="S4.SS2.SSS0.Px5.p1.4.m4.1"><semantics id="S4.SS2.SSS0.Px5.p1.4.m4.1a"><mo id="S4.SS2.SSS0.Px5.p1.4.m4.1.1" xref="S4.SS2.SSS0.Px5.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p1.4.m4.1b"><times id="S4.SS2.SSS0.Px5.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px5.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p1.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px5.p1.4.m4.1d">×</annotation></semantics></math>A4000, and 4<math alttext="\times" display="inline" id="S4.SS2.SSS0.Px5.p1.5.m5.1"><semantics id="S4.SS2.SSS0.Px5.p1.5.m5.1a"><mo id="S4.SS2.SSS0.Px5.p1.5.m5.1.1" xref="S4.SS2.SSS0.Px5.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p1.5.m5.1b"><times id="S4.SS2.SSS0.Px5.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS0.Px5.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p1.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px5.p1.5.m5.1d">×</annotation></semantics></math>A5000 GPUs. These are personal servers and servers from university labs, spread across Europe and North America and connected to the Internet at speeds of 100–1000 Mbit/s. Four of the servers operate from behind firewalls.</p>
</section>
<section id="S4.SS2.SSS0.Px6">
<h4>Analysis.</h4>
<p id="S4.SS2.SSS0.Px6.p1.1">We report the results for Llama 2 in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Experiments for Llama 2 (70B) and BLOOM (176B) ‣ 4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3</span></a> and for BLOOM in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Experiments for Llama 2 (70B) and BLOOM (176B) ‣ 4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3</span></a>. For inference, performance does not depend much on bandwidth or sequence length but degrades with higher latency. In turn, fine-tuning forward passes for large batches are affected by both bandwidth and latency.</p>
<p id="S4.SS2.SSS0.Px6.p2.1">We can see that the offloading baseline is about an order of magnitude slower than our system for inference, both in practice and in the theoretical-best setup assuming an infinite GPU performance. For parallel forward passes, offloading is competitive if networking is limited to 100 Mbit/s or has high latency. In other cases, our algorithm offers higher throughput than offloading for training.</p>
<p id="S4.SS2.SSS0.Px6.p3.1">Crucially, our system significantly outperforms offloading even when each GPU node runs its own client doing single-batch inference at the same time. Thus, <span id="S4.SS2.SSS0.Px6.p3.1.1">given the same hardware</span>, a group of researchers will get much better inference speed by collaborating over the Internet using our system compared to each of them running offloading independently.</p>
<p id="S4.SS2.SSS0.Px6.p4.1">Finally, the real-world setup turns out to be slower than the A100 benchmarks due to slower hardware. Still, our algorithm outperforms offloading even when communicating between different continents.</p>
</section>
<section id="S4.SS2.SSS0.Px7">
<h4>Additional experiments.</h4>
<p id="S4.SS2.SSS0.Px7.p1.1">We conduct two additional experiments to test individual components of our system. We evaluate the load balancing from <a href="#S3.SS3" title="3.3 Automatic load balancing ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3.3</span></a> in isolation in Appendix <a href="#A5" title="Appendix E Evaluation of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>E</span></a>. We also evaluate the performance of model compression from Section <a href="#S3.SS5" title="3.5 Implementation details ‣ 3 Method ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3.5</span></a> in Appendix <a href="#A1" title="Appendix A Quality and efficiency of BLOOM with 8-bit quantization ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>A</span></a>. To reiterate, for each model, we use the same compression strategy in our system and all baselines. Finally, we perform a qualitative evaluation of fault tolerance by shutting down random servers during inference and fine-tuning to verify that the algorithm produces correct outputs and gradients.</p>
</section>
</section>
</section>
<section id="S5">
<h2>
<span>5 </span>Conclusion</h2>
<p id="S5.p1.1">In this paper, we introduced a novel fault-tolerant algorithm for inferencing large language models. On top of it, we introduced a decentralized system for running LLMs on distributed unreliable devices connected over the Internet, which significantly outperforms other approaches to running inference on consumer-grade hardware. We demonstrated that the proposed system can scale to the largest publicly available language model with hundreds of billions of trainable parameters.</p>
<p id="S5.p2.1">While our work is focused on technical aspects, it is important to consider limitations of our approach, such as privacy of data processed by outside peers, as well as broader impact of making LLMs more accessible. We discuss these issues and outline directions for future work in Appendix <a href="#A8" title="Appendix H Limitations and broader impact ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>H</span></a>.</p>
</section>
<section id="bib">
<h2>References</h2>
<ul>
<li id="bib.bib1">
<span>AI (21)</span>
<span>
AI21.

</span>
<span>Jurassic-1 language models.

</span>
<span>&#34;<a href="https://studio.ai21.com/docs/jurassic1-language-models" title="">https://studio.ai21.com/docs/jurassic1-language-models</a>&#34;.

</span>
<span>Accessed: 2022-06-22.

</span>
</li>
<li id="bib.bib2">
<span>Aminabadi et al. (2022)</span>
<span>
Aminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A., Li, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase, O., et al.

</span>
<span>Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale.

</span>
<span><em id="bib.bib2.1.1">arXiv preprint arXiv:2207.00032</em>, 2022.

</span>
</li>
<li id="bib.bib3">
<span>Athlur et al. (2022)</span>
<span>
Athlur, S., Saran, N., Sivathanu, M., Ramjee, R., and Kwatra, N.

</span>
<span>Varuna: scalable, low-cost training of massive deep learning models.

</span>
<span>In <em id="bib.bib3.1.1">Proceedings of the Seventeenth European Conference on Computer Systems</em>, pp.  472–487, 2022.

</span>
</li>
<li id="bib.bib4">
<span>Ben-Nun &amp; Hoefler (2019)</span>
<span>
Ben-Nun, T. and Hoefler, T.

</span>
<span>Demystifying parallel and distributed deep learning: An in-depth concurrency analysis.

</span>
<span><em id="bib.bib4.1.1">ACM Comput. Surv.</em>, 52(4), aug 2019.

</span>
<span>ISSN 0360-0300.

</span>
<span>doi: <a href="https://olu.online/24-hopes-for-2024/10.1145/3320060" title="">10.1145/3320060</a>.

</span>
<span>URL <a href="https://doi.org/10.1145/3320060" title="">https://doi.org/10.1145/3320060</a>.

</span>
</li>
<li id="bib.bib5">
<span>BigScience (2022a)</span>
<span>
BigScience.

</span>
<span>BLOOM: a 176B-parameter open-access multilingual language model.

</span>
<span><em id="bib.bib5.1.1">ArXiv</em>, abs/2211.05100, 2022a.

</span>
</li>
<li id="bib.bib6">
<span>BigScience (2022b)</span>
<span>
BigScience.

</span>
<span>A version of BLOOM with 7.1 billion parameters.

</span>
<span><a href="https://huggingface.co/bigscience/bloom-7b1" title="">https://huggingface.co/bigscience/bloom-7b1</a>, 2022b.

</span>
</li>
<li id="bib.bib7">
<span>BigScience et al. (2022)</span>
<span>
BigScience, Microsoft, and NVIDIA.

</span>
<span>The fork of Megatron-LM and Megatron-DeepSpeed by BigScience.

</span>
<span><a href="https://github.com/bigscience-workshop/Megatron-DeepSpeed" title="">https://github.com/bigscience-workshop/Megatron-DeepSpeed</a>, 2022.

</span>
</li>
<li id="bib.bib8">
<span>Black et al. (2022)</span>
<span>
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L., Tow, J., Wang, B., and Weinbach, S.

</span>
<span>Gpt-neox-20b: An open-source autoregressive language model, 2022.

</span>
<span>URL <a href="https://arxiv.org/abs/2204.06745" title="">https://arxiv.org/abs/2204.06745</a>.

</span>
</li>
<li id="bib.bib9">
<span>Brown et al. (2020)</span>
<span>
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.

</span>
<span>Language models are few-shot learners.

</span>
<span><em id="bib.bib9.1.1">arXiv preprint arXiv:2005.14165</em>, 2020.

</span>
</li>
<li id="bib.bib10">
<span>Chen et al. (2016)</span>
<span>
Chen, T., Xu, B., Zhang, C., and Guestrin, C.

</span>
<span>Training deep nets with sublinear memory cost.

</span>
<span><em id="bib.bib10.1.1">arXiv preprint arXiv:1604.06174</em>, 2016.

</span>
</li>
<li id="bib.bib11">
<span>Dettmers et al. (2022a)</span>
<span>
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.

</span>
<span>LLM.int8(): 8-bit matrix multiplication for transformers at scale.

</span>
<span><em id="bib.bib11.1.1">ArXiv</em>, abs/2208.07339, 2022a.

</span>
</li>
<li id="bib.bib12">
<span>Dettmers et al. (2022b)</span>
<span>
Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L.

</span>
<span>8-bit optimizers via block-wise quantization.

</span>
<span><em id="bib.bib12.1.1">International Conference on Learning Representations (ICLR)</em>, 2022b.

</span>
</li>
<li id="bib.bib13">
<span>Dettmers et al. (2023)</span>
<span>
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.

</span>
<span>Qlora: Efficient finetuning of quantized llms.

</span>
<span><em id="bib.bib13.1.1">arXiv preprint arXiv:2305.14314</em>, 2023.

</span>
</li>
<li id="bib.bib14">
<span>Du et al. (2021)</span>
<span>
Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y., Chen, Z., and Cui, C.

</span>
<span>Glam: Efficient scaling of language models with mixture-of-experts.

</span>
<span><em id="bib.bib14.1.1">CoRR</em>, abs/2112.06905, 2021.

</span>
<span>URL <a href="https://arxiv.org/abs/2112.06905" title="">https://arxiv.org/abs/2112.06905</a>.

</span>
</li>
<li id="bib.bib15">
<span>Evans et al. (2018)</span>
<span>
Evans, D., Kolesnikov, V., Rosulek, M., et al.

</span>
<span>A pragmatic introduction to secure multi-party computation.

</span>
<span><em id="bib.bib15.1.1">Foundations and Trends in Privacy and Security</em>, 2(2-3):70–246, 2018.

</span>
</li>
<li id="bib.bib16">
<span>Face &amp; contributors (2020)</span>
<span>
Face, H. and contributors.

</span>
<span>Accelerate: Run your raw pytorch training script on any kind of device.

</span>
<span><em id="bib.bib16.1.1">GitHub. Note: https://github.com/huggingface/datasets</em>, 1, 2020.

</span>
</li>
<li id="bib.bib17">
<span>Fedus et al. (2021)</span>
<span>
Fedus, W., Zoph, B., and Shazeer, N.

</span>
<span>Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021.

</span>
</li>
<li id="bib.bib18">
<span>Gao et al. (2021)</span>
<span>
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.

</span>
<span>A framework for few-shot language model evaluation, September 2021.

</span>
<span>URL <a href="https://doi.org/10.5281/zenodo.5371628" title="">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li id="bib.bib19">
<span>Griewank &amp; Walther (2000)</span>
<span>
Griewank, A. and Walther, A.

</span>
<span>Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation.

</span>
<span><em id="bib.bib19.1.1">ACM Transactions on Mathematical Software (TOMS)</em>, 26(1):19–45, 2000.

</span>
</li>
<li id="bib.bib20">
<span>Guo et al. (2021)</span>
<span>
Guo, D., Rush, A. M., and Kim, Y.

</span>
<span>Parameter-efficient transfer learning with diff pruning.

</span>
<span>In <em id="bib.bib20.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</em>, 2021.

</span>
</li>
<li id="bib.bib21">
<span>Holtzman et al. (2020)</span>
<span>
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.

</span>
<span>The curious case of neural text degeneration.

</span>
<span>In <em id="bib.bib21.1.1">International Conference on Learning Representations</em>, 2020.

</span>
<span>URL <a href="https://openreview.net/forum?id=rygGQyrFvH" title="">https://openreview.net/forum?id=rygGQyrFvH</a>.

</span>
</li>
<li id="bib.bib22">
<span>Houlsby et al. (2019)</span>
<span>
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S.

</span>
<span>Parameter-efficient transfer learning for nlp.

</span>
<span>In <em id="bib.bib22.1.1">International Conference on Machine Learning</em>, pp.  2790–2799. PMLR, 2019.

</span>
</li>
<li id="bib.bib23">
<span>Hu et al. (2021)</span>
<span>
Hu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, L., and Chen, W.

</span>
<span>Lora: Low-rank adaptation of large language models, 2021.

</span>
</li>
<li id="bib.bib24">
<span>Huang et al. (2019)</span>
<span>
Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al.

</span>
<span>Gpipe: Efficient training of giant neural networks using pipeline parallelism.

</span>
<span>In <em id="bib.bib24.1.1">Advances in Neural Information Processing Systems</em>, pp.  103–112, 2019.

</span>
</li>
<li id="bib.bib25">
<span>Jia et al. (2019)</span>
<span>
Jia, Z., Zaharia, M., and Aiken, A.

</span>
<span>Beyond data and model parallelism for deep neural networks.

</span>
<span>In Talwalkar, A., Smith, V., and Zaharia, M. (eds.), <em id="bib.bib25.1.1">Proceedings of Machine Learning and Systems</em>, volume 1, pp.  1–13, 2019.

</span>
<span>URL <a href="https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf" title="">https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib26">
<span>Kaplan et al. (2020)</span>
<span>
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.

</span>
<span>Scaling laws for neural language models, 2020.

</span>
</li>
<li id="bib.bib27">
<span>Khrushchev et al. (2022)</span>
<span>
Khrushchev, M., Vasilev, R., Zinov, N., Petrov, A., and Yandex.

</span>
<span>Yalm 100b, 2022.

</span>
<span><a href="%22https://huggingface.co/yandex/yalm-100b%22" title="">&#34;https://huggingface.co/yandex/yalm-100b&#34;</a>.

</span>
</li>
<li id="bib.bib28">
<span>Kim et al. (2021)</span>
<span>
Kim, B., Kim, H., Lee, S., Lee, G., Kwak, D., Jeon, D. H., Park, S., Kim, S., Kim, S., Seo, D., Lee, H., Jeong, M., Lee, S., Kim, M., Ko, S., Kim, S., Park, T., Kim, J., Kang, S., Ryu, N., Yoo, K. M., Chang, M., Suh, S., In, S., Park, J., Kim, K., Kim, H., Jeong, J., Yeo, Y. G., Ham, D., Park, D., Lee, M. Y., Kang, J., Kang, I., Ha, J., Park, W., and Sung, N.

</span>
<span>What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers.

</span>
<span><em id="bib.bib28.1.1">CoRR</em>, abs/2109.04650, 2021.

</span>
<span>URL <a href="https://arxiv.org/abs/2109.04650" title="">https://arxiv.org/abs/2109.04650</a>.

</span>
</li>
<li id="bib.bib29">
<span>Koenig &amp; Likhachev (2005)</span>
<span>
Koenig, S. and Likhachev, M.

</span>
<span>Fast replanning for navigation in unknown terrain.

</span>
<span><em id="bib.bib29.1.1">IEEE Transactions on Robotics</em>, 21(3):354–363, 2005.

</span>
<span>doi: <a href="https://olu.online/24-hopes-for-2024/10.1109/TRO.2004.838026" title="">10.1109/TRO.2004.838026</a>.

</span>
</li>
<li id="bib.bib30">
<span>Krizhevsky (2014)</span>
<span>
Krizhevsky, A.

</span>
<span>One weird trick for parallelizing convolutional neural networks.

</span>
<span><em id="bib.bib30.1.1">CoRR</em>, abs/1404.5997, 2014.

</span>
<span>URL <a href="http://arxiv.org/abs/1404.5997" title="">http://arxiv.org/abs/1404.5997</a>.

</span>
</li>
<li id="bib.bib31">
<span>Krizhevsky et al. (2012)</span>
<span>
Krizhevsky, A., Sutskever, I., and Hinton, G. E.

</span>
<span>Imagenet classification with deep convolutional neural networks.

</span>
<span>In Pereira, F., Burges, C. J. C., Bottou, L., and Weinberger, K. Q. (eds.), <em id="bib.bib31.1.1">Advances in Neural Information Processing Systems 25</em>, pp.  1097–1105. Curran Associates, Inc., 2012.

</span>
</li>
<li id="bib.bib32">
<span>Kuszmaul (2022)</span>
<span>
Kuszmaul, J.

</span>
<span>Bamboo trimming revisited: Simple algorithms can do well too.

</span>
<span><em id="bib.bib32.1.1">arXiv preprint arXiv:2201.07350</em>, 2022.

</span>
</li>
<li id="bib.bib33">
<span>Lepikhin et al. (2020)</span>
<span>
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z.

</span>
<span>Gshard: Scaling giant models with conditional computation and automatic sharding.

</span>
<span><em id="bib.bib33.1.1">ArXiv</em>, abs/2006.16668, 2020.

</span>
</li>
<li id="bib.bib34">
<span>Lester et al. (2021)</span>
<span>
Lester, B., Al-Rfou, R., and Constant, N.

</span>
<span>The power of scale for parameter-efficient prompt tuning.

</span>
<span>In <em id="bib.bib34.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pp.  3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

</span>
<span>doi: <a href="https://olu.online/24-hopes-for-2024/10.18653/v1/2021.emnlp-main.243" title="">10.18653/v1/2021.emnlp-main.243</a>.

</span>
<span>URL <a href="https://aclanthology.org/2021.emnlp-main.243" title="">https://aclanthology.org/2021.emnlp-main.243</a>.

</span>
</li>
<li id="bib.bib35">
<span>libp2p (2022)</span>
<span>
libp2p.

</span>
<span>libp2p circuit relay.

</span>
<span><a href="https://docs.libp2p.io/concepts/nat/circuit-relay/" title="">https://docs.libp2p.io/concepts/nat/circuit-relay/</a>, 2022.

</span>
</li>
<li id="bib.bib36">
<span>Liu et al. (2022a)</span>
<span>
Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C.

</span>
<span>Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning, 2022a.

</span>
<span>URL <a href="https://arxiv.org/abs/2205.05638" title="">https://arxiv.org/abs/2205.05638</a>.

</span>
</li>
<li id="bib.bib37">
<span>Liu et al. (2022b)</span>
<span>
Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C.

</span>
<span>Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning, 2022b.

</span>
<span>URL <a href="https://arxiv.org/abs/2205.05638" title="">https://arxiv.org/abs/2205.05638</a>.

</span>
</li>
<li id="bib.bib38">
<span>Liu et al. (2021a)</span>
<span>
Liu, X., Ji, K., Fu, Y., Du, Z., Yang, Z., and Tang, J.

</span>
<span>P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.

</span>
<span><em id="bib.bib38.1.1">arXiv preprint arXiv:2110.07602</em>, 2021a.

</span>
</li>
<li id="bib.bib39">
<span>Liu et al. (2021b)</span>
<span>
Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J.

</span>
<span>Gpt understands, too.

</span>
<span><em id="bib.bib39.1.1">arXiv:2103.10385</em>, 2021b.

</span>
</li>
<li id="bib.bib40">
<span>Maymounkov &amp; Mazieres (2002)</span>
<span>
Maymounkov, P. and Mazieres, D.

</span>
<span>Kademlia: A peer-to-peer information system based on the xor metric.

</span>
<span>In <em id="bib.bib40.1.1">International Workshop on Peer-to-Peer Systems</em>, pp.  53–65. Springer, 2002.

</span>
</li>
<li id="bib.bib41">
<span>Narayanan et al. (2019)</span>
<span>
Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N. R., Ganger, G. R., Gibbons, P. B., and Zaharia, M.

</span>
<span>Pipedream: Generalized pipeline parallelism for dnn training.

</span>
<span>In <em id="bib.bib41.1.1">Proceedings of the 27th ACM Symposium on Operating Systems Principles</em>, SOSP ’19, pp.  1–15, New York, NY, USA, 2019. Association for Computing Machinery.

</span>
<span>ISBN 9781450368735.

</span>
<span>doi: <a href="https://olu.online/24-hopes-for-2024/10.1145/3341301.3359646" title="">10.1145/3341301.3359646</a>.

</span>
<span>URL <a href="https://doi.org/10.1145/3341301.3359646" title="">https://doi.org/10.1145/3341301.3359646</a>.

</span>
</li>
<li id="bib.bib42">
<span>Narayanan et al. (2021)</span>
<span>
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et al.

</span>
<span>Efficient large-scale language model training on gpu clusters.

</span>
<span><em id="bib.bib42.1.1">arXiv preprint arXiv:2104.04473</em>, 2021.

</span>
</li>
<li id="bib.bib43">
<span>NVIDIA (2020)</span>
<span>
NVIDIA.

</span>
<span>NVIDIA Ampere GA102 GPU architecture, 2020.

</span>
<span>URL <a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf" title="">https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf</a>.

</span>
</li>
<li id="bib.bib44">
<span>NVIDIA (2022)</span>
<span>
NVIDIA.

</span>
<span>Nvidia confidential computing.

</span>
<span><a href="https://www.nvidia.com/en-in/data-center/solutions/confidential-computing/" title="">https://www.nvidia.com/en-in/data-center/solutions/confidential-computing/</a>, 2022.

</span>
</li>
<li id="bib.bib45">
<span>Pudipeddi et al. (2020)</span>
<span>
Pudipeddi, B., Mesmakhosroshahi, M., Xi, J., and Bharadwaj, S.

</span>
<span>Training large neural networks with constant memory using a new execution algorithm.

</span>
<span><em id="bib.bib45.1.1">arXiv preprint arXiv:2002.05645</em>, 2020.

</span>
</li>
<li id="bib.bib46">
<span>Radford et al. (2018)</span>
<span>
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.

</span>
<span>Improving language understanding by generative pre-training.

</span>
<span>2018.

</span>
<span>URL <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" title="">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a>.

</span>
</li>
<li id="bib.bib47">
<span>Radford et al. (2019)</span>
<span>
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.

</span>
<span>Language models are unsupervised multitask learners.

</span>
<span>2019.

</span>
</li>
<li id="bib.bib48">
<span>Rae et al. (2021)</span>
<span>
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young, S., and et al.

</span>
<span>Scaling language models: Methods, analysis &amp; insights from training gopher.

</span>
<span><em id="bib.bib48.1.1">CoRR</em>, abs/2112.11446, 2021.

</span>
<span>URL <a href="https://arxiv.org/abs/2112.11446" title="">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li id="bib.bib49">
<span>Rajbhandari et al. (2020)</span>
<span>
Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y.

</span>
<span>Zero: Memory optimization towards training a trillion parameter models.

</span>
<span>In <em id="bib.bib49.1.1">SC</em>, 2020.

</span>
</li>
<li id="bib.bib50">
<span>Rajbhandari et al. (2021)</span>
<span>
Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S., and He, Y.

</span>
<span>Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning.

</span>
<span><em id="bib.bib50.1.1">arXiv preprint arXiv:2104.07857</em>, 2021.

</span>
</li>
<li id="bib.bib51">
<span>Ren et al. (2021)</span>
<span>
Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y.

</span>
<span>Zero-offload: Democratizing billion-scale model training, 2021.

</span>
</li>
<li id="bib.bib52">
<span>Ryabinin et al. (2023)</span>
<span>
Ryabinin, M., Dettmers, T., Diskin, M., and Borzunov, A.

</span>
<span>SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient.

</span>
<span>In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), <em id="bib.bib52.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em id="bib.bib52.2.2">Proceedings of Machine Learning Research</em>, pp.  29416–29440. PMLR, 23–29 Jul 2023.

</span>
<span>URL <a href="https://proceedings.mlr.press/v202/ryabinin23a.html" title="">https://proceedings.mlr.press/v202/ryabinin23a.html</a>.

</span>
</li>
<li id="bib.bib53">
<span>Schick &amp; Schütze (2021)</span>
<span>
Schick, T. and Schütze, H.

</span>
<span>Generating datasets with pretrained language models.

</span>
<span>pp.  6943–6951, November 2021.

</span>
<span>doi: <a href="https://olu.online/24-hopes-for-2024/10.18653/v1/2021.emnlp-main.555" title="">10.18653/v1/2021.emnlp-main.555</a>.

</span>
<span>URL <a href="https://aclanthology.org/2021.emnlp-main.555" title="">https://aclanthology.org/2021.emnlp-main.555</a>.

</span>
</li>
<li id="bib.bib54">
<span>Shazeer et al. (2018)</span>
<span>
Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., Sepassi, R., and Hechtman, B. A.

</span>
<span>Mesh-tensorflow: Deep learning for supercomputers.

</span>
<span><em id="bib.bib54.1.1">CoRR</em>, abs/1811.02084, 2018.

</span>
<span>URL <a href="http://arxiv.org/abs/1811.02084" title="">http://arxiv.org/abs/1811.02084</a>.

</span>
</li>
<li id="bib.bib55">
<span>Sung et al. (2021)</span>
<span>
Sung, Y.-L., Nair, V., and Raffel, C.

</span>
<span>Training neural networks with fixed sparse masks.

</span>
<span><em id="bib.bib55.1.1">Advances in Neural Information Processing Systems</em>, 2021.

</span>
</li>
<li id="bib.bib56">
<span>Tang et al. (2020)</span>
<span>
Tang, Z., Shi, S., Chu, X., Wang, W., and Li, B.

</span>
<span>Communication-efficient distributed deep learning: A comprehensive survey, 2020.

</span>
</li>
<li id="bib.bib57">
<span>Taylor et al. (2022)</span>
<span>
Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R.

</span>
<span>Galactica: A large language model for science.

</span>
<span>2022.

</span>
</li>
<li id="bib.bib58">
<span>Touvron et al. (2023a)</span>
<span>
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.

</span>
<span>Llama: Open and efficient foundation language models.

</span>
<span><em id="bib.bib58.1.1">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li id="bib.bib59">
<span>Touvron et al. (2023b)</span>
<span>
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.

</span>
<span>Llama 2: Open foundation and fine-tuned chat models.

</span>
<span><em id="bib.bib59.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li id="bib.bib60">
<span>Vaswani et al. (2017)</span>
<span>
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.

</span>
<span>Attention is all you need.

</span>
<span>In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), <em id="bib.bib60.1.1">Advances in Neural Information Processing Systems 30</em>, pp.  5998–6008. Curran Associates, Inc., 2017.

</span>
<span>URL <a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" title="">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a>.

</span>
</li>
<li id="bib.bib61">
<span>Wang et al. (2022)</span>
<span>
Wang, J., Yuan, B., Rimanic, L., He, Y., Dao, T., Chen, B., Re, C., and Zhang, C.

</span>
<span>Fine-tuning language models over slow networks using activation compression with guarantees, 2022.

</span>
<span>URL <a href="https://arxiv.org/abs/2206.01299" title="">https://arxiv.org/abs/2206.01299</a>.

</span>
</li>
<li id="bib.bib62">
<span>West et al. (2021)</span>
<span>
West, P., Bhagavatula, C., Hessel, J., Hwang, J. D., Jiang, L., Bras, R. L., Lu, X., Welleck, S., and Choi, Y.

</span>
<span>Symbolic knowledge distillation: from general language models to commonsense models.

</span>
<span><em id="bib.bib62.1.1">arXiv preprint arXiv:2110.07178</em>, 2021.

</span>
</li>
<li id="bib.bib63">
<span>Yang et al. (2019)</span>
<span>
Yang, B., Zhang, J., Li, J., Ré, C., Aberger, C. R., and Sa, C. D.

</span>
<span>Pipemare: Asynchronous pipeline parallel dnn training.

</span>
<span><em id="bib.bib63.1.1">ArXiv</em>, abs/1910.05124, 2019.

</span>
</li>
<li id="bib.bib64">
<span>Yong &amp; Nikoulina (2022)</span>
<span>
Yong, Z.-X. and Nikoulina, V.

</span>
<span>Adapting bigscience multilingual model to unseen languages, 2022.

</span>
<span>URL <a href="https://arxiv.org/abs/2204.04873" title="">https://arxiv.org/abs/2204.04873</a>.

</span>
</li>
<li id="bib.bib65">
<span>Yuan et al. (2022)</span>
<span>
Yuan, B., He, Y., Davis, J., Zhang, T., Dao, T., Chen, B., Liang, P. S., Re, C., and Zhang, C.

</span>
<span>Decentralized training of foundation models in heterogeneous environments.

</span>
<span><em id="bib.bib65.1.1">Advances in Neural Information Processing Systems</em>, 35:25464–25477, 2022.

</span>
</li>
<li id="bib.bib66">
<span>Zeng et al. (2022)</span>
<span>
Zeng, A., Liu, X., Du, Z., Ding, M., Zheng, Q., Lai, H., Wang, Z., Yang, Z., Yu, J., Zhang, X., Zheng, W., Xia, X., Xu, Y., Tam, W. L., Dong, Y., Ma, Z., He, J., Sun, Z., Zhai, J., Chen, W., Zeng, G., Han, X., Zhao, W., Liu, Z., Xue, Y., Wang, S., Shan, J., Jiang, H., Guo, Z., Zhang, P., and Tang, J.

</span>
<span>GLM-130B: An open bilingual pre-trained model, 2022.

</span>
<span>URL <a href="http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/" title="">http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/</a>.

</span>
</li>
<li id="bib.bib67">
<span>Zeng et al. (2021)</span>
<span>
Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., Jiang, X., Yang, Z., Wang, K., Zhang, X., Li, C., Gong, Z., Yao, Y., Huang, X., Wang, J., Yu, J., Guo, Q., Yu, Y., Zhang, Y., Wang, J., Tao, H., Yan, D., Yi, Z., Peng, F., Jiang, F., Zhang, H., Deng, L., Zhang, Y., Lin, Z., Zhang, C., Zhang, S., Guo, M., Gu, S., Fan, G., Wang, Y., Jin, X., Liu, Q., and Tian, Y.

</span>
<span>Pangu-<math alttext="\alpha" display="inline" id="bib.bib67.1.m1.1"><semantics id="bib.bib67.1.m1.1a"><mi id="bib.bib67.1.m1.1.1" xref="bib.bib67.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="bib.bib67.1.m1.1b"><ci id="bib.bib67.1.m1.1.1.cmml" xref="bib.bib67.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib67.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="bib.bib67.1.m1.1d">italic_α</annotation></semantics></math>: Large-scale autoregressive pretrained chinese language models with auto-parallel computation.

</span>
<span><em id="bib.bib67.2.1">CoRR</em>, abs/2104.12369, 2021.

</span>
<span>URL <a href="https://arxiv.org/abs/2104.12369" title="">https://arxiv.org/abs/2104.12369</a>.

</span>
</li>
<li id="bib.bib68">
<span>Zhang et al. (2022)</span>
<span>
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L.

</span>
<span>OPT: open pre-trained transformer language models, 2022.

</span>
<span>URL <a href="https://arxiv.org/abs/2205.01068" title="">https://arxiv.org/abs/2205.01068</a>.

</span>
</li>
</ul>
</section>

<section id="Ax1">
<h2>Appendix</h2>
</section>
<section id="A1">
<h2>
<span>Appendix A </span>Quality and efficiency of BLOOM with 8-bit quantization</h2>
<figure id="A1.T5">
<figcaption><span>Table 4: </span>Zero-shot accuracy for <span id="A1.T5.5.1">BLOOM-176B</span> and <span id="A1.T5.6.2">OPT-175B</span> with 8-bit and 16-bit weights.</figcaption>
<figcaption><span>Table 5: </span>Generation throughput (tokens/s) for BLOOM-176B with 8-bit and 16-bit weights on 8<math alttext="\times" display="inline" id="A1.T5.2.m1.1"><semantics id="A1.T5.2.m1.1a"><mo id="A1.T5.2.m1.1.1" xref="A1.T5.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T5.2.m1.1b"><times id="A1.T5.2.m1.1.1.cmml" xref="A1.T5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.m1.1d">×</annotation></semantics></math> A100 GPUs.</figcaption><div>
<div>

</div>

<div>

</div>
</div>
<figcaption><span>Table 5: </span>Generation throughput (tokens/s) for BLOOM-176B with 8-bit and 16-bit weights on 8<math alttext="\times" display="inline" id="A1.T5.2.m1.1"><semantics id="A1.T5.2.m1.1a"><mo id="A1.T5.2.m1.1.1" xref="A1.T5.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.T5.2.m1.1b"><times id="A1.T5.2.m1.1.1.cmml" xref="A1.T5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.m1.1d">×</annotation></semantics></math> A100 GPUs.</figcaption>
</figure>
<p id="A1.p1.1">As shown in Table <a href="#A1.T5" title="Table 5 ‣ Appendix A Quality and efficiency of BLOOM with 8-bit quantization ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>5</span></a>, this method has little effect on LLM quality for major benchmarks.
In terms of inference time, Table <a href="#A1.T5" title="Table 5 ‣ Appendix A Quality and efficiency of BLOOM with 8-bit quantization ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>5</span></a> demonstrates that quantization has about <math alttext="5\%" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mn id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">5</mn><mo id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><csymbol cd="latexml" id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1">percent</csymbol><cn id="A1.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.p1.1.m1.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">5\%</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">5 %</annotation></semantics></math> of overhead with batch size 1 (20 tokens), but becomes negligible for larger batches.</p>
</section>
<section id="A2">
<h2>
<span>Appendix B </span>Estimating theoretical best throughput with RAM offloading</h2>
<div id="A2.p1">
<p id="A2.p1.2">In this estimate, we use the best possible hardware setup for offloading: CPU RAM offloading via PCIe 4.0 with 16 PCIe lanes per GPU.
In 8-bit, the model uses 1 GB of memory per billion parameters, and PCIe 4.0 with 16 lanes has a throughput of 256 Gbit/s. We assume an offloading latency of zero in the upper bound estimation. As such, offloading 176B parameters takes at least:</p>
<table id="A2.Ex1">
<tbody><tr>
<td></td>
<td><math alttext="\frac{176\text{ GB}\cdot 8}{256\text{ Gbit/s}}=5.5\text{ seconds}" display="block" id="A2.Ex1.m1.1"><semantics id="A2.Ex1.m1.1a"><mrow id="A2.Ex1.m1.1.1" xref="A2.Ex1.m1.1.1.cmml"><mfrac id="A2.Ex1.m1.1.1.2" xref="A2.Ex1.m1.1.1.2.cmml"><mrow id="A2.Ex1.m1.1.1.2.2" xref="A2.Ex1.m1.1.1.2.2.cmml"><mrow id="A2.Ex1.m1.1.1.2.2.2" xref="A2.Ex1.m1.1.1.2.2.2.cmml"><mn id="A2.Ex1.m1.1.1.2.2.2.2" xref="A2.Ex1.m1.1.1.2.2.2.2.cmml">176</mn><mo id="A2.Ex1.m1.1.1.2.2.2.1" xref="A2.Ex1.m1.1.1.2.2.2.1.cmml">⁢</mo><mtext id="A2.Ex1.m1.1.1.2.2.2.3" xref="A2.Ex1.m1.1.1.2.2.2.3a.cmml"> GB</mtext></mrow><mo id="A2.Ex1.m1.1.1.2.2.1" lspace="0.222em" rspace="0.222em" xref="A2.Ex1.m1.1.1.2.2.1.cmml">⋅</mo><mn id="A2.Ex1.m1.1.1.2.2.3" xref="A2.Ex1.m1.1.1.2.2.3.cmml">8</mn></mrow><mrow id="A2.Ex1.m1.1.1.2.3" xref="A2.Ex1.m1.1.1.2.3.cmml"><mn id="A2.Ex1.m1.1.1.2.3.2" xref="A2.Ex1.m1.1.1.2.3.2.cmml">256</mn><mo id="A2.Ex1.m1.1.1.2.3.1" xref="A2.Ex1.m1.1.1.2.3.1.cmml">⁢</mo><mtext id="A2.Ex1.m1.1.1.2.3.3" xref="A2.Ex1.m1.1.1.2.3.3a.cmml"> Gbit/s</mtext></mrow></mfrac><mo id="A2.Ex1.m1.1.1.1" xref="A2.Ex1.m1.1.1.1.cmml">=</mo><mrow id="A2.Ex1.m1.1.1.3" xref="A2.Ex1.m1.1.1.3.cmml"><mn id="A2.Ex1.m1.1.1.3.2" xref="A2.Ex1.m1.1.1.3.2.cmml">5.5</mn><mo id="A2.Ex1.m1.1.1.3.1" xref="A2.Ex1.m1.1.1.3.1.cmml">⁢</mo><mtext id="A2.Ex1.m1.1.1.3.3" xref="A2.Ex1.m1.1.1.3.3a.cmml"> seconds</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.Ex1.m1.1b"><apply id="A2.Ex1.m1.1.1.cmml" xref="A2.Ex1.m1.1.1"><eq id="A2.Ex1.m1.1.1.1.cmml" xref="A2.Ex1.m1.1.1.1"></eq><apply id="A2.Ex1.m1.1.1.2.cmml" xref="A2.Ex1.m1.1.1.2"><divide id="A2.Ex1.m1.1.1.2.1.cmml" xref="A2.Ex1.m1.1.1.2"></divide><apply id="A2.Ex1.m1.1.1.2.2.cmml" xref="A2.Ex1.m1.1.1.2.2"><ci id="A2.Ex1.m1.1.1.2.2.1.cmml" xref="A2.Ex1.m1.1.1.2.2.1">⋅</ci><apply id="A2.Ex1.m1.1.1.2.2.2.cmml" xref="A2.Ex1.m1.1.1.2.2.2"><times id="A2.Ex1.m1.1.1.2.2.2.1.cmml" xref="A2.Ex1.m1.1.1.2.2.2.1"></times><cn id="A2.Ex1.m1.1.1.2.2.2.2.cmml" type="integer" xref="A2.Ex1.m1.1.1.2.2.2.2">176</cn><ci id="A2.Ex1.m1.1.1.2.2.2.3a.cmml" xref="A2.Ex1.m1.1.1.2.2.2.3"><mtext id="A2.Ex1.m1.1.1.2.2.2.3.cmml" xref="A2.Ex1.m1.1.1.2.2.2.3"> GB</mtext></ci></apply><cn id="A2.Ex1.m1.1.1.2.2.3.cmml" type="integer" xref="A2.Ex1.m1.1.1.2.2.3">8</cn></apply><apply id="A2.Ex1.m1.1.1.2.3.cmml" xref="A2.Ex1.m1.1.1.2.3"><times id="A2.Ex1.m1.1.1.2.3.1.cmml" xref="A2.Ex1.m1.1.1.2.3.1"></times><cn id="A2.Ex1.m1.1.1.2.3.2.cmml" type="integer" xref="A2.Ex1.m1.1.1.2.3.2">256</cn><ci id="A2.Ex1.m1.1.1.2.3.3a.cmml" xref="A2.Ex1.m1.1.1.2.3.3"><mtext id="A2.Ex1.m1.1.1.2.3.3.cmml" xref="A2.Ex1.m1.1.1.2.3.3"> Gbit/s</mtext></ci></apply></apply><apply id="A2.Ex1.m1.1.1.3.cmml" xref="A2.Ex1.m1.1.1.3"><times id="A2.Ex1.m1.1.1.3.1.cmml" xref="A2.Ex1.m1.1.1.3.1"></times><cn id="A2.Ex1.m1.1.1.3.2.cmml" type="float" xref="A2.Ex1.m1.1.1.3.2">5.5</cn><ci id="A2.Ex1.m1.1.1.3.3a.cmml" xref="A2.Ex1.m1.1.1.3.3"><mtext id="A2.Ex1.m1.1.1.3.3.cmml" xref="A2.Ex1.m1.1.1.3.3"> seconds</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.Ex1.m1.1c">\frac{176\text{ GB}\cdot 8}{256\text{ Gbit/s}}=5.5\text{ seconds}</annotation><annotation encoding="application/x-llamapun" id="A2.Ex1.m1.1d">divide start_ARG 176 GB ⋅ 8 end_ARG start_ARG 256 Gbit/s end_ARG = 5.5 seconds</annotation></semantics></math></td>
<td></td>
</tr></tbody>
</table>
<p id="A2.p1.1">This gives the upper bound of <math alttext="1/5.5\approx 0.18" display="inline" id="A2.p1.1.m1.1"><semantics id="A2.p1.1.m1.1a"><mrow id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mrow id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml"><mn id="A2.p1.1.m1.1.1.2.2" xref="A2.p1.1.m1.1.1.2.2.cmml">1</mn><mo id="A2.p1.1.m1.1.1.2.1" xref="A2.p1.1.m1.1.1.2.1.cmml">/</mo><mn id="A2.p1.1.m1.1.1.2.3" xref="A2.p1.1.m1.1.1.2.3.cmml">5.5</mn></mrow><mo id="A2.p1.1.m1.1.1.1" xref="A2.p1.1.m1.1.1.1.cmml">≈</mo><mn id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">0.18</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><approx id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1.1"></approx><apply id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2"><divide id="A2.p1.1.m1.1.1.2.1.cmml" xref="A2.p1.1.m1.1.1.2.1"></divide><cn id="A2.p1.1.m1.1.1.2.2.cmml" type="integer" xref="A2.p1.1.m1.1.1.2.2">1</cn><cn id="A2.p1.1.m1.1.1.2.3.cmml" type="float" xref="A2.p1.1.m1.1.1.2.3">5.5</cn></apply><cn id="A2.p1.1.m1.1.1.3.cmml" type="float" xref="A2.p1.1.m1.1.1.3">0.18</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">1/5.5\approx 0.18</annotation><annotation encoding="application/x-llamapun" id="A2.p1.1.m1.1d">1 / 5.5 ≈ 0.18</annotation></semantics></math> tokens/s for the inference speed.</p>
</div>
</section>
<section id="A3">
<h2>
<span>Appendix C </span>Extension to beam search algorithms</h2>
<p id="A3.p1.2">There are several variations of beam-search algorithm used for language model inference, including standard beam search, diverse beam search, constrained beam search, and more. A common thread between those algorithms is that they maintain a fixed number <math alttext="k" display="inline" id="A3.p1.1.m1.1"><semantics id="A3.p1.1.m1.1a"><mi id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><ci id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.p1.1.m1.1d">italic_k</annotation></semantics></math> of candidate sequences between steps. These sequences are informally referred to as the “beam”. On every step, these algorithms generate possible continuations of sequences in the previous beam, then use some fitness criterion to select <math alttext="k" display="inline" id="A3.p1.2.m2.1"><semantics id="A3.p1.2.m2.1a"><mi id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><ci id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.p1.2.m2.1d">italic_k</annotation></semantics></math> of these continuations for the next beam.</p>
<p id="A3.p2.4">From a computational point of view, this procedure is similar to simple “greedy” inference with a batch of <math alttext="k" display="inline" id="A3.p2.1.m1.1"><semantics id="A3.p2.1.m1.1a"><mi id="A3.p2.1.m1.1.1" xref="A3.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p2.1.m1.1b"><ci id="A3.p2.1.m1.1.1.cmml" xref="A3.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.p2.1.m1.1d">italic_k</annotation></semantics></math> sequences. However, there is one important difference: unlike batched inference, beam search algorithms can “shuffle” candidate sequences between steps. In other words, 3rd best sequence from time step <math alttext="t" display="inline" id="A3.p2.2.m2.1"><semantics id="A3.p2.2.m2.1a"><mi id="A3.p2.2.m2.1.1" xref="A3.p2.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="A3.p2.2.m2.1b"><ci id="A3.p2.2.m2.1.1.cmml" xref="A3.p2.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="A3.p2.2.m2.1d">italic_t</annotation></semantics></math> can produce 1st or 2nd (or any other) sequence on the next step. Furthermore, a single sequence on time step <math alttext="t" display="inline" id="A3.p2.3.m3.1"><semantics id="A3.p2.3.m3.1a"><mi id="A3.p2.3.m3.1.1" xref="A3.p2.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="A3.p2.3.m3.1b"><ci id="A3.p2.3.m3.1.1.cmml" xref="A3.p2.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="A3.p2.3.m3.1d">italic_t</annotation></semantics></math> can produce multiple sequences selected for step <math alttext="t+1" display="inline" id="A3.p2.4.m4.1"><semantics id="A3.p2.4.m4.1a"><mrow id="A3.p2.4.m4.1.1" xref="A3.p2.4.m4.1.1.cmml"><mi id="A3.p2.4.m4.1.1.2" xref="A3.p2.4.m4.1.1.2.cmml">t</mi><mo id="A3.p2.4.m4.1.1.1" xref="A3.p2.4.m4.1.1.1.cmml">+</mo><mn id="A3.p2.4.m4.1.1.3" xref="A3.p2.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.p2.4.m4.1b"><apply id="A3.p2.4.m4.1.1.cmml" xref="A3.p2.4.m4.1.1"><plus id="A3.p2.4.m4.1.1.1.cmml" xref="A3.p2.4.m4.1.1.1"></plus><ci id="A3.p2.4.m4.1.1.2.cmml" xref="A3.p2.4.m4.1.1.2">𝑡</ci><cn id="A3.p2.4.m4.1.1.3.cmml" type="integer" xref="A3.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.4.m4.1c">t+1</annotation><annotation encoding="application/x-llamapun" id="A3.p2.4.m4.1d">italic_t + 1</annotation></semantics></math>.</p>
<p id="A3.p3.3">Since different beam search variantions use different criteria for selecting top sequences, we need a generic algorithm that can fit any criterion. In our system, we implement this by allowing clients to reorder server-side attention cache after each step. Formally, a client can send a list of at most <math alttext="k" display="inline" id="A3.p3.1.m1.1"><semantics id="A3.p3.1.m1.1a"><mi id="A3.p3.1.m1.1.1" xref="A3.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p3.1.m1.1b"><ci id="A3.p3.1.m1.1.1.cmml" xref="A3.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.p3.1.m1.1d">italic_k</annotation></semantics></math> integers in range <math alttext="[1,k]" display="inline" id="A3.p3.2.m2.2"><semantics id="A3.p3.2.m2.2a"><mrow id="A3.p3.2.m2.2.3.2" xref="A3.p3.2.m2.2.3.1.cmml"><mo id="A3.p3.2.m2.2.3.2.1" stretchy="false" xref="A3.p3.2.m2.2.3.1.cmml">[</mo><mn id="A3.p3.2.m2.1.1" xref="A3.p3.2.m2.1.1.cmml">1</mn><mo id="A3.p3.2.m2.2.3.2.2" xref="A3.p3.2.m2.2.3.1.cmml">,</mo><mi id="A3.p3.2.m2.2.2" xref="A3.p3.2.m2.2.2.cmml">k</mi><mo id="A3.p3.2.m2.2.3.2.3" stretchy="false" xref="A3.p3.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.p3.2.m2.2b"><interval closure="closed" id="A3.p3.2.m2.2.3.1.cmml" xref="A3.p3.2.m2.2.3.2"><cn id="A3.p3.2.m2.1.1.cmml" type="integer" xref="A3.p3.2.m2.1.1">1</cn><ci id="A3.p3.2.m2.2.2.cmml" xref="A3.p3.2.m2.2.2">𝑘</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="A3.p3.2.m2.2c">[1,k]</annotation><annotation encoding="application/x-llamapun" id="A3.p3.2.m2.2d">[ 1 , italic_k ]</annotation></semantics></math>, where i-th index specifies which previous attention cache should be used when generating <math alttext="i" display="inline" id="A3.p3.3.m3.1"><semantics id="A3.p3.3.m3.1a"><mi id="A3.p3.3.m3.1.1" xref="A3.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A3.p3.3.m3.1b"><ci id="A3.p3.3.m3.1.1.cmml" xref="A3.p3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p3.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="A3.p3.3.m3.1d">italic_i</annotation></semantics></math>-th sequence of the next beam.</p>
<p id="A3.p4.2">For instance, when given indices <math alttext="[2,2,1,3,2]" display="inline" id="A3.p4.1.m1.5"><semantics id="A3.p4.1.m1.5a"><mrow id="A3.p4.1.m1.5.6.2" xref="A3.p4.1.m1.5.6.1.cmml"><mo id="A3.p4.1.m1.5.6.2.1" stretchy="false" xref="A3.p4.1.m1.5.6.1.cmml">[</mo><mn id="A3.p4.1.m1.1.1" xref="A3.p4.1.m1.1.1.cmml">2</mn><mo id="A3.p4.1.m1.5.6.2.2" xref="A3.p4.1.m1.5.6.1.cmml">,</mo><mn id="A3.p4.1.m1.2.2" xref="A3.p4.1.m1.2.2.cmml">2</mn><mo id="A3.p4.1.m1.5.6.2.3" xref="A3.p4.1.m1.5.6.1.cmml">,</mo><mn id="A3.p4.1.m1.3.3" xref="A3.p4.1.m1.3.3.cmml">1</mn><mo id="A3.p4.1.m1.5.6.2.4" xref="A3.p4.1.m1.5.6.1.cmml">,</mo><mn id="A3.p4.1.m1.4.4" xref="A3.p4.1.m1.4.4.cmml">3</mn><mo id="A3.p4.1.m1.5.6.2.5" xref="A3.p4.1.m1.5.6.1.cmml">,</mo><mn id="A3.p4.1.m1.5.5" xref="A3.p4.1.m1.5.5.cmml">2</mn><mo id="A3.p4.1.m1.5.6.2.6" stretchy="false" xref="A3.p4.1.m1.5.6.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.p4.1.m1.5b"><list id="A3.p4.1.m1.5.6.1.cmml" xref="A3.p4.1.m1.5.6.2"><cn id="A3.p4.1.m1.1.1.cmml" type="integer" xref="A3.p4.1.m1.1.1">2</cn><cn id="A3.p4.1.m1.2.2.cmml" type="integer" xref="A3.p4.1.m1.2.2">2</cn><cn id="A3.p4.1.m1.3.3.cmml" type="integer" xref="A3.p4.1.m1.3.3">1</cn><cn id="A3.p4.1.m1.4.4.cmml" type="integer" xref="A3.p4.1.m1.4.4">3</cn><cn id="A3.p4.1.m1.5.5.cmml" type="integer" xref="A3.p4.1.m1.5.5">2</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.1.m1.5c">[2,2,1,3,2]</annotation><annotation encoding="application/x-llamapun" id="A3.p4.1.m1.5d">[ 2 , 2 , 1 , 3 , 2 ]</annotation></semantics></math>, a server will use 2nd best sequence from step <math alttext="t" display="inline" id="A3.p4.2.m2.1"><semantics id="A3.p4.2.m2.1a"><mi id="A3.p4.2.m2.1.1" xref="A3.p4.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="A3.p4.2.m2.1b"><ci id="A3.p4.2.m2.1.1.cmml" xref="A3.p4.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="A3.p4.2.m2.1d">italic_t</annotation></semantics></math> to produce the new 1st, 3rd and 5th best sequences. Previous 1st and 3rd best sequences go to 3rd and 4th places, respectively. Finally, previous 4th and 5th sequences are discarded. From a technical point of view, servers implement this reordering by reordering attention cache with the specified indices (<span id="A3.p4.2.1">torch.gather</span> operation) immediately before performing an inference step.</p>
</section>
<section id="A4">
<h2>
<span>Appendix D </span>Details of the server load balancing algorithms</h2>
<section id="A4.SS0.SSS0.Px1">
<h4>Measuring throughput.</h4>
<p id="A4.SS0.SSS0.Px1.p1.1">Before joining for the first time, each server measures its Internet connection throughput (in tokens/second, using one of public web APIs for doing that) and GPU throughput (in tokens/second, using a small benchmark running several forward passes). The minimum of these values becomes the overall server throughput, which is then cached for future runs.</p>
</section>
<section id="A4.SS0.SSS0.Px2">
<h4>Initial block assignment.</h4>
<p id="A4.SS0.SSS0.Px2.p1.1">We assume that each server holds a segment of <span id="A4.SS0.SSS0.Px2.p1.1.1">consecutive</span> transformer blocks to minimize inference latency. Clients may request to perform a forward or backward pass for the whole segment of blocks or its subsegment, if necessary. Normally, each server loads as many blocks as it can fit in its GPU memory, unless a user limits the number of blocks to utilize the rest of memory for something else.</p>
<div id="A4.SS0.SSS0.Px2.p2">
<p id="A4.SS0.SSS0.Px2.p2.4">Before starting, each server calculates the values of <math alttext="t_{i}" display="inline" id="A4.SS0.SSS0.Px2.p2.1.m1.1"><semantics id="A4.SS0.SSS0.Px2.p2.1.m1.1a"><msub id="A4.SS0.SSS0.Px2.p2.1.m1.1.1" xref="A4.SS0.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px2.p2.1.m1.1.1.2" xref="A4.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml">t</mi><mi id="A4.SS0.SSS0.Px2.p2.1.m1.1.1.3" xref="A4.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p2.1.m1.1b"><apply id="A4.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A4.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="A4.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="A4.SS0.SSS0.Px2.p2.1.m1.1.1.2">𝑡</ci><ci id="A4.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="A4.SS0.SSS0.Px2.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p2.1.m1.1c">t_{i}</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p2.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> – the total throughput of servers currently holding the <math alttext="i" display="inline" id="A4.SS0.SSS0.Px2.p2.2.m2.1"><semantics id="A4.SS0.SSS0.Px2.p2.2.m2.1a"><mi id="A4.SS0.SSS0.Px2.p2.2.m2.1.1" xref="A4.SS0.SSS0.Px2.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p2.2.m2.1b"><ci id="A4.SS0.SSS0.Px2.p2.2.m2.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p2.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p2.2.m2.1d">italic_i</annotation></semantics></math>-th block or loading it (to start holding it in a few minutes). Then, to find the best segment of blocks to serve, the server looks for the most narrow bottleneck in the network. Formally, if the model has <math alttext="L" display="inline" id="A4.SS0.SSS0.Px2.p2.3.m3.1"><semantics id="A4.SS0.SSS0.Px2.p2.3.m3.1a"><mi id="A4.SS0.SSS0.Px2.p2.3.m3.1.1" xref="A4.SS0.SSS0.Px2.p2.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p2.3.m3.1b"><ci id="A4.SS0.SSS0.Px2.p2.3.m3.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p2.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p2.3.m3.1d">italic_L</annotation></semantics></math> blocks and the server can hold <math alttext="K" display="inline" id="A4.SS0.SSS0.Px2.p2.4.m4.1"><semantics id="A4.SS0.SSS0.Px2.p2.4.m4.1a"><mi id="A4.SS0.SSS0.Px2.p2.4.m4.1.1" xref="A4.SS0.SSS0.Px2.p2.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p2.4.m4.1b"><ci id="A4.SS0.SSS0.Px2.p2.4.m4.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p2.4.m4.1c">K</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p2.4.m4.1d">italic_K</annotation></semantics></math> of them in its GPU memory, we calculate:</p>
<table id="A4.E1">
<tbody><tr>
<td></td>
<td><math alttext="start=\underset{i=1}{\overset{L-K+1}{\arg\min}}\quad\mathrm{sorted}([t_{i},\ t%
_{i+1},\ \ldots,\ t_{i+K-1}])" display="block" id="A4.E1.m1.3"><semantics id="A4.E1.m1.3a"><mrow id="A4.E1.m1.3.3" xref="A4.E1.m1.3.3.cmml"><mrow id="A4.E1.m1.3.3.3" xref="A4.E1.m1.3.3.3.cmml"><mi id="A4.E1.m1.3.3.3.2" xref="A4.E1.m1.3.3.3.2.cmml">s</mi><mo id="A4.E1.m1.3.3.3.1" xref="A4.E1.m1.3.3.3.1.cmml">⁢</mo><mi id="A4.E1.m1.3.3.3.3" xref="A4.E1.m1.3.3.3.3.cmml">t</mi><mo id="A4.E1.m1.3.3.3.1a" xref="A4.E1.m1.3.3.3.1.cmml">⁢</mo><mi id="A4.E1.m1.3.3.3.4" xref="A4.E1.m1.3.3.3.4.cmml">a</mi><mo id="A4.E1.m1.3.3.3.1b" xref="A4.E1.m1.3.3.3.1.cmml">⁢</mo><mi id="A4.E1.m1.3.3.3.5" xref="A4.E1.m1.3.3.3.5.cmml">r</mi><mo id="A4.E1.m1.3.3.3.1c" xref="A4.E1.m1.3.3.3.1.cmml">⁢</mo><mi id="A4.E1.m1.3.3.3.6" xref="A4.E1.m1.3.3.3.6.cmml">t</mi></mrow><mo id="A4.E1.m1.3.3.2" xref="A4.E1.m1.3.3.2.cmml">=</mo><mrow id="A4.E1.m1.3.3.1.1" xref="A4.E1.m1.3.3.1.2.cmml"><munderover accent="true" accentunder="true" id="A4.E1.m1.2.2" xref="A4.E1.m1.2.2.cmml"><mrow id="A4.E1.m1.2.2.2.2" xref="A4.E1.m1.2.2.2.2.cmml"><mi id="A4.E1.m1.2.2.2.2.1" xref="A4.E1.m1.2.2.2.2.1.cmml">arg</mi><mo id="A4.E1.m1.2.2.2.2a" lspace="0.167em" xref="A4.E1.m1.2.2.2.2.cmml">⁡</mo><mi id="A4.E1.m1.2.2.2.2.2" xref="A4.E1.m1.2.2.2.2.2.cmml">min</mi></mrow><mrow id="A4.E1.m1.2.2.1" xref="A4.E1.m1.2.2.1.cmml"><mi id="A4.E1.m1.2.2.1.2" mathsize="142%" xref="A4.E1.m1.2.2.1.2.cmml">i</mi><mo id="A4.E1.m1.2.2.1.1" mathsize="142%" xref="A4.E1.m1.2.2.1.1.cmml">=</mo><mn id="A4.E1.m1.2.2.1.3" mathsize="142%" xref="A4.E1.m1.2.2.1.3.cmml">1</mn></mrow><mrow id="A4.E1.m1.2.2.2.1" xref="A4.E1.m1.2.2.2.1.cmml"><mrow id="A4.E1.m1.2.2.2.1.2" xref="A4.E1.m1.2.2.2.1.2.cmml"><mi id="A4.E1.m1.2.2.2.1.2.2" xref="A4.E1.m1.2.2.2.1.2.2.cmml">L</mi><mo id="A4.E1.m1.2.2.2.1.2.1" xref="A4.E1.m1.2.2.2.1.2.1.cmml">−</mo><mi id="A4.E1.m1.2.2.2.1.2.3" xref="A4.E1.m1.2.2.2.1.2.3.cmml">K</mi></mrow><mo id="A4.E1.m1.2.2.2.1.1" xref="A4.E1.m1.2.2.2.1.1.cmml">+</mo><mn id="A4.E1.m1.2.2.2.1.3" xref="A4.E1.m1.2.2.2.1.3.cmml">1</mn></mrow></munderover><mspace id="A4.E1.m1.3.3.1.1.2" width="1.167em" xref="A4.E1.m1.3.3.1.2.cmml"></mspace><mrow id="A4.E1.m1.3.3.1.1.1" xref="A4.E1.m1.3.3.1.1.1.cmml"><mi id="A4.E1.m1.3.3.1.1.1.3" xref="A4.E1.m1.3.3.1.1.1.3.cmml">sorted</mi><mo id="A4.E1.m1.3.3.1.1.1.2" xref="A4.E1.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="A4.E1.m1.3.3.1.1.1.1.1" xref="A4.E1.m1.3.3.1.1.1.cmml"><mo id="A4.E1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="A4.E1.m1.3.3.1.1.1.cmml">(</mo><mrow id="A4.E1.m1.3.3.1.1.1.1.1.1.3" xref="A4.E1.m1.3.3.1.1.1.1.1.1.4.cmml"><mo id="A4.E1.m1.3.3.1.1.1.1.1.1.3.4" stretchy="false" xref="A4.E1.m1.3.3.1.1.1.1.1.1.4.cmml">[</mo><msub id="A4.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">t</mi><mi id="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="A4.E1.m1.3.3.1.1.1.1.1.1.3.5" rspace="0.667em" xref="A4.E1.m1.3.3.1.1.1.1.1.1.4.cmml">,</mo><msub id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.cmml"><mi id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.2" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml">t</mi><mrow id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.cmml"><mi id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1.cmml">+</mo><mn id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow></msub><mo id="A4.E1.m1.3.3.1.1.1.1.1.1.3.6" rspace="0.667em" xref="A4.E1.m1.3.3.1.1.1.1.1.1.4.cmml">,</mo><mi id="A4.E1.m1.1.1" mathvariant="normal" xref="A4.E1.m1.1.1.cmml">…</mi><mo id="A4.E1.m1.3.3.1.1.1.1.1.1.3.7" rspace="0.667em" xref="A4.E1.m1.3.3.1.1.1.1.1.1.4.cmml">,</mo><msub id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml"><mi id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.2" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.2.cmml">t</mi><mrow id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.cmml"><mrow id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml"><mi id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.2" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.2.cmml">i</mi><mo id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.1" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.1.cmml">+</mo><mi id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.3" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.3.cmml">K</mi></mrow><mo id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.1" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.1.cmml">−</mo><mn id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml">1</mn></mrow></msub><mo id="A4.E1.m1.3.3.1.1.1.1.1.1.3.8" stretchy="false" xref="A4.E1.m1.3.3.1.1.1.1.1.1.4.cmml">]</mo></mrow><mo id="A4.E1.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="A4.E1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E1.m1.3b"><apply id="A4.E1.m1.3.3.cmml" xref="A4.E1.m1.3.3"><eq id="A4.E1.m1.3.3.2.cmml" xref="A4.E1.m1.3.3.2"></eq><apply id="A4.E1.m1.3.3.3.cmml" xref="A4.E1.m1.3.3.3"><times id="A4.E1.m1.3.3.3.1.cmml" xref="A4.E1.m1.3.3.3.1"></times><ci id="A4.E1.m1.3.3.3.2.cmml" xref="A4.E1.m1.3.3.3.2">𝑠</ci><ci id="A4.E1.m1.3.3.3.3.cmml" xref="A4.E1.m1.3.3.3.3">𝑡</ci><ci id="A4.E1.m1.3.3.3.4.cmml" xref="A4.E1.m1.3.3.3.4">𝑎</ci><ci id="A4.E1.m1.3.3.3.5.cmml" xref="A4.E1.m1.3.3.3.5">𝑟</ci><ci id="A4.E1.m1.3.3.3.6.cmml" xref="A4.E1.m1.3.3.3.6">𝑡</ci></apply><list id="A4.E1.m1.3.3.1.2.cmml" xref="A4.E1.m1.3.3.1.1"><apply id="A4.E1.m1.2.2.cmml" xref="A4.E1.m1.2.2"><apply id="A4.E1.m1.2.2.1.cmml" xref="A4.E1.m1.2.2.1"><eq id="A4.E1.m1.2.2.1.1.cmml" xref="A4.E1.m1.2.2.1.1"></eq><ci id="A4.E1.m1.2.2.1.2.cmml" xref="A4.E1.m1.2.2.1.2">𝑖</ci><cn id="A4.E1.m1.2.2.1.3.cmml" type="integer" xref="A4.E1.m1.2.2.1.3">1</cn></apply><apply id="A4.E1.m1.2.2.2.cmml" xref="A4.E1.m1.2.2"><apply id="A4.E1.m1.2.2.2.1.cmml" xref="A4.E1.m1.2.2.2.1"><plus id="A4.E1.m1.2.2.2.1.1.cmml" xref="A4.E1.m1.2.2.2.1.1"></plus><apply id="A4.E1.m1.2.2.2.1.2.cmml" xref="A4.E1.m1.2.2.2.1.2"><minus id="A4.E1.m1.2.2.2.1.2.1.cmml" xref="A4.E1.m1.2.2.2.1.2.1"></minus><ci id="A4.E1.m1.2.2.2.1.2.2.cmml" xref="A4.E1.m1.2.2.2.1.2.2">𝐿</ci><ci id="A4.E1.m1.2.2.2.1.2.3.cmml" xref="A4.E1.m1.2.2.2.1.2.3">𝐾</ci></apply><cn id="A4.E1.m1.2.2.2.1.3.cmml" type="integer" xref="A4.E1.m1.2.2.2.1.3">1</cn></apply><apply id="A4.E1.m1.2.2.2.2.cmml" xref="A4.E1.m1.2.2.2.2"><arg id="A4.E1.m1.2.2.2.2.1.cmml" xref="A4.E1.m1.2.2.2.2.1"></arg><min id="A4.E1.m1.2.2.2.2.2.cmml" xref="A4.E1.m1.2.2.2.2.2"></min></apply></apply></apply><apply id="A4.E1.m1.3.3.1.1.1.cmml" xref="A4.E1.m1.3.3.1.1.1"><times id="A4.E1.m1.3.3.1.1.1.2.cmml" xref="A4.E1.m1.3.3.1.1.1.2"></times><ci id="A4.E1.m1.3.3.1.1.1.3.cmml" xref="A4.E1.m1.3.3.1.1.1.3">sorted</ci><list id="A4.E1.m1.3.3.1.1.1.1.1.1.4.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3"><apply id="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.2">𝑡</ci><ci id="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.2">𝑡</ci><apply id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3"><plus id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1"></plus><ci id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="A4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="A4.E1.m1.1.1.cmml" xref="A4.E1.m1.1.1">…</ci><apply id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.1.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.2.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.2">𝑡</ci><apply id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3"><minus id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.1.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.1"></minus><apply id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2"><plus id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.1.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.1"></plus><ci id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.2.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.2">𝑖</ci><ci id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.3.cmml" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.3">𝐾</ci></apply><cn id="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml" type="integer" xref="A4.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3">1</cn></apply></apply></list></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E1.m1.3c">start=\underset{i=1}{\overset{L-K+1}{\arg\min}}\quad\mathrm{sorted}([t_{i},\ t%
_{i+1},\ \ldots,\ t_{i+K-1}])</annotation><annotation encoding="application/x-llamapun" id="A4.E1.m1.3d">italic_s italic_t italic_a italic_r italic_t = start_UNDERACCENT italic_i = 1 end_UNDERACCENT start_ARG start_OVERACCENT italic_L - italic_K + 1 end_OVERACCENT start_ARG roman_arg roman_min end_ARG end_ARG roman_sorted ( [ italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_i + italic_K - 1 end_POSTSUBSCRIPT ] )</annotation></semantics></math></td>
<td></td>
<td rowspan="1"><span>(1)</span></td>
</tr></tbody>
</table>
<p id="A4.SS0.SSS0.Px2.p2.6">Here, <math alttext="\arg\min" display="inline" id="A4.SS0.SSS0.Px2.p2.5.m1.1"><semantics id="A4.SS0.SSS0.Px2.p2.5.m1.1a"><mrow id="A4.SS0.SSS0.Px2.p2.5.m1.1.1" xref="A4.SS0.SSS0.Px2.p2.5.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px2.p2.5.m1.1.1.1" xref="A4.SS0.SSS0.Px2.p2.5.m1.1.1.1.cmml">arg</mi><mo id="A4.SS0.SSS0.Px2.p2.5.m1.1.1a" lspace="0.167em" xref="A4.SS0.SSS0.Px2.p2.5.m1.1.1.cmml">⁡</mo><mi id="A4.SS0.SSS0.Px2.p2.5.m1.1.1.2" xref="A4.SS0.SSS0.Px2.p2.5.m1.1.1.2.cmml">min</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p2.5.m1.1b"><apply id="A4.SS0.SSS0.Px2.p2.5.m1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.5.m1.1.1"><arg id="A4.SS0.SSS0.Px2.p2.5.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.5.m1.1.1.1"></arg><min id="A4.SS0.SSS0.Px2.p2.5.m1.1.1.2.cmml" xref="A4.SS0.SSS0.Px2.p2.5.m1.1.1.2"></min></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p2.5.m1.1c">\arg\min</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p2.5.m1.1d">roman_arg roman_min</annotation></semantics></math> compares the sorted arrays lexicographically and chooses the leftmost <math alttext="start" display="inline" id="A4.SS0.SSS0.Px2.p2.6.m2.1"><semantics id="A4.SS0.SSS0.Px2.p2.6.m2.1a"><mrow id="A4.SS0.SSS0.Px2.p2.6.m2.1.1" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.cmml"><mi id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.2" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.2.cmml">s</mi><mo id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1.cmml">⁢</mo><mi id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.3" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.3.cmml">t</mi><mo id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1a" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1.cmml">⁢</mo><mi id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.4" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.4.cmml">a</mi><mo id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1b" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1.cmml">⁢</mo><mi id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.5" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.5.cmml">r</mi><mo id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1c" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1.cmml">⁢</mo><mi id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.6" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.6.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p2.6.m2.1b"><apply id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1"><times id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.1"></times><ci id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.2.cmml" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.2">𝑠</ci><ci id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.3.cmml" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.3">𝑡</ci><ci id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.4.cmml" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.4">𝑎</ci><ci id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.5.cmml" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.5">𝑟</ci><ci id="A4.SS0.SSS0.Px2.p2.6.m2.1.1.6.cmml" xref="A4.SS0.SSS0.Px2.p2.6.m2.1.1.6">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p2.6.m2.1c">start</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p2.6.m2.1d">italic_s italic_t italic_a italic_r italic_t</annotation></semantics></math> in case of multiple minimums.</p>
</div>
<p id="A4.SS0.SSS0.Px2.p3.1">This way, the next joining server would always cover a block with the smallest <math alttext="t_{i}" display="inline" id="A4.SS0.SSS0.Px2.p3.1.m1.1"><semantics id="A4.SS0.SSS0.Px2.p3.1.m1.1a"><msub id="A4.SS0.SSS0.Px2.p3.1.m1.1.1" xref="A4.SS0.SSS0.Px2.p3.1.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px2.p3.1.m1.1.1.2" xref="A4.SS0.SSS0.Px2.p3.1.m1.1.1.2.cmml">t</mi><mi id="A4.SS0.SSS0.Px2.p3.1.m1.1.1.3" xref="A4.SS0.SSS0.Px2.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p3.1.m1.1b"><apply id="A4.SS0.SSS0.Px2.p3.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A4.SS0.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p3.1.m1.1.1">subscript</csymbol><ci id="A4.SS0.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="A4.SS0.SSS0.Px2.p3.1.m1.1.1.2">𝑡</ci><ci id="A4.SS0.SSS0.Px2.p3.1.m1.1.1.3.cmml" xref="A4.SS0.SSS0.Px2.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p3.1.m1.1c">t_{i}</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p3.1.m1.1d">italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. If there are multiple bottlenecks like this, the server will try to cover as many of them as possible (we choose to cover the minimums first because the overall throughput is the minimum of throughputs among model blocks). Among the remaining options, we choose a segment covering as many second minimums as possible, and so on.</p>
</section>
<section id="A4.SS0.SSS0.Px3">
<h4>Quality of block assignment.</h4>
<p id="A4.SS0.SSS0.Px3.p1.1">While we are not aware of the exact polynomial-time solution for the problem of assigning the segments optimally, we have conducted computational experiments and found out that this greedy algorithm (running in polynomial time) usually finds an assignment with total throughput of 90-100% of the optimal one (found by trying out all possible assignments in exponential time), given that the values of throughput are realistic to our setup.</p>
</section>
<section id="A4.SS0.SSS0.Px4">
<h4>Rebalancing.</h4>
<p id="A4.SS0.SSS0.Px4.p1.1">Since servers may leave at any time, each server also periodically checks if the current assignment is &#34;good enough&#34; compared to the throughput estimated by running the greedy solution for servers currently present in the network.</p>
<p id="A4.SS0.SSS0.Px4.p2.2">Formally, each server periodically looks for a segment of blocks that is more appropriate than the currently loaded blocks with respect to the <math alttext="\arg\min" display="inline" id="A4.SS0.SSS0.Px4.p2.1.m1.1"><semantics id="A4.SS0.SSS0.Px4.p2.1.m1.1a"><mrow id="A4.SS0.SSS0.Px4.p2.1.m1.1.1" xref="A4.SS0.SSS0.Px4.p2.1.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px4.p2.1.m1.1.1.1" xref="A4.SS0.SSS0.Px4.p2.1.m1.1.1.1.cmml">arg</mi><mo id="A4.SS0.SSS0.Px4.p2.1.m1.1.1a" lspace="0.167em" xref="A4.SS0.SSS0.Px4.p2.1.m1.1.1.cmml">⁡</mo><mi id="A4.SS0.SSS0.Px4.p2.1.m1.1.1.2" xref="A4.SS0.SSS0.Px4.p2.1.m1.1.1.2.cmml">min</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px4.p2.1.m1.1b"><apply id="A4.SS0.SSS0.Px4.p2.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p2.1.m1.1.1"><arg id="A4.SS0.SSS0.Px4.p2.1.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p2.1.m1.1.1.1"></arg><min id="A4.SS0.SSS0.Px4.p2.1.m1.1.1.2.cmml" xref="A4.SS0.SSS0.Px4.p2.1.m1.1.1.2"></min></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px4.p2.1.m1.1c">\arg\min</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px4.p2.1.m1.1d">roman_arg roman_min</annotation></semantics></math> rule (<a href="#A4.E1" title="1 ‣ Initial block assignment. ‣ Appendix D Details of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a>). If it finds one, it simulates how the rest of the servers would behave if we replace the current blocks with the new ones (how other servers would change their blocks afterwards). If the eventual throughput is at least <math alttext="p\%" display="inline" id="A4.SS0.SSS0.Px4.p2.2.m2.1"><semantics id="A4.SS0.SSS0.Px4.p2.2.m2.1a"><mrow id="A4.SS0.SSS0.Px4.p2.2.m2.1.1" xref="A4.SS0.SSS0.Px4.p2.2.m2.1.1.cmml"><mi id="A4.SS0.SSS0.Px4.p2.2.m2.1.1.2" xref="A4.SS0.SSS0.Px4.p2.2.m2.1.1.2.cmml">p</mi><mo id="A4.SS0.SSS0.Px4.p2.2.m2.1.1.1" xref="A4.SS0.SSS0.Px4.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px4.p2.2.m2.1b"><apply id="A4.SS0.SSS0.Px4.p2.2.m2.1.1.cmml" xref="A4.SS0.SSS0.Px4.p2.2.m2.1.1"><csymbol cd="latexml" id="A4.SS0.SSS0.Px4.p2.2.m2.1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p2.2.m2.1.1.1">percent</csymbol><ci id="A4.SS0.SSS0.Px4.p2.2.m2.1.1.2.cmml" xref="A4.SS0.SSS0.Px4.p2.2.m2.1.1.2">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px4.p2.2.m2.1c">p\%</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px4.p2.2.m2.1d">italic_p %</annotation></semantics></math> better, the server commits to the change and announces that it changes the blocks, then other servers do the rest of the changes (eventually increasing the total throughput).</p>
<p id="A4.SS0.SSS0.Px4.p3.2">We use <math alttext="p=20\%" display="inline" id="A4.SS0.SSS0.Px4.p3.1.m1.1"><semantics id="A4.SS0.SSS0.Px4.p3.1.m1.1a"><mrow id="A4.SS0.SSS0.Px4.p3.1.m1.1.1" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.2" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.2.cmml">p</mi><mo id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.1" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.1.cmml">=</mo><mrow id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.cmml"><mn id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.2" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.2.cmml">20</mn><mo id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.1" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px4.p3.1.m1.1b"><apply id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1"><eq id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.1"></eq><ci id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.2.cmml" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.2">𝑝</ci><apply id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.cmml" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3"><csymbol cd="latexml" id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.1.cmml" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.1">percent</csymbol><cn id="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.2.cmml" type="integer" xref="A4.SS0.SSS0.Px4.p3.1.m1.1.1.3.2">20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px4.p3.1.m1.1c">p=20\%</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px4.p3.1.m1.1d">italic_p = 20 %</annotation></semantics></math> since it gives a reasonable trade-off between the swarm throughput and the frequency of block replacements in our experiments (see Appendix <a href="#A5" title="Appendix E Evaluation of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>E</span></a>). Specifically, a lower value of <math alttext="p" display="inline" id="A4.SS0.SSS0.Px4.p3.2.m2.1"><semantics id="A4.SS0.SSS0.Px4.p3.2.m2.1a"><mi id="A4.SS0.SSS0.Px4.p3.2.m2.1.1" xref="A4.SS0.SSS0.Px4.p3.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px4.p3.2.m2.1b"><ci id="A4.SS0.SSS0.Px4.p3.2.m2.1.1.cmml" xref="A4.SS0.SSS0.Px4.p3.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px4.p3.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px4.p3.2.m2.1d">italic_p</annotation></semantics></math> leads to block replacements happening too often, which negatively affects the inference latency since each block replacement resets attention caches for this block.</p>
</section>
<section id="A4.SS0.SSS0.Px5">
<h4>Stability of the greedy algorithm.</h4>
<p id="A4.SS0.SSS0.Px5.p1.1">The rebalancing algorithm does not cause oscillations since a series of block replacements is executed only if it leads to eventually increasing throughput by at least <math alttext="p\%" display="inline" id="A4.SS0.SSS0.Px5.p1.1.m1.1"><semantics id="A4.SS0.SSS0.Px5.p1.1.m1.1a"><mrow id="A4.SS0.SSS0.Px5.p1.1.m1.1.1" xref="A4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px5.p1.1.m1.1.1.2" xref="A4.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml">p</mi><mo id="A4.SS0.SSS0.Px5.p1.1.m1.1.1.1" xref="A4.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px5.p1.1.m1.1b"><apply id="A4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px5.p1.1.m1.1.1"><csymbol cd="latexml" id="A4.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px5.p1.1.m1.1.1.1">percent</csymbol><ci id="A4.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="A4.SS0.SSS0.Px5.p1.1.m1.1.1.2">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px5.p1.1.m1.1c">p\%</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px5.p1.1.m1.1d">italic_p %</annotation></semantics></math>. Once a &#34;good enough&#34; throughput is achieved, servers do not change their blocks anymore (unless an essential number of servers join or leave). We verified this behavior computationally, simulating a network with thousands of servers with different throughputs.</p>
<p id="A4.SS0.SSS0.Px5.p2.1">To conclude, this greedy heuristic allows servers to quickly close the gaps if a substantial share (up to 100%) of servers holding certain blocks leave, but avoids excess block replacements otherwise.</p>
<figure id="A4.F2"><img alt="Refer to caption" height="437" id="A4.F2.g1" src="https://olu.online/24-hopes-for-2024/x2.png" width="830"/>
<figcaption><span>Figure 2: </span>Behavior of the load balancing algorithms evaluated in Appendix <a href="#A5" title="Appendix E Evaluation of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>E</span></a>.</figcaption>
</figure>
</section>
</section>
<section id="A5">
<h2>
<span>Appendix E </span>Evaluation of the server load balancing algorithms</h2>
<p id="A5.p1.2">In this section, we measure the effectiveness of the load balancing algorithm used in our system. We run all experiments using a fleet of 206 virtual instances that simulate participants. To keep experiment costs manageable, we do not use GPUs for this evaluation, instead simulating uneven server throughput programmatically. For each server, we sample its throughput from the uniform distribution <math alttext="t\sim\mathbb{U}[0,100]" display="inline" id="A5.p1.1.m1.2"><semantics id="A5.p1.1.m1.2a"><mrow id="A5.p1.1.m1.2.3" xref="A5.p1.1.m1.2.3.cmml"><mi id="A5.p1.1.m1.2.3.2" xref="A5.p1.1.m1.2.3.2.cmml">t</mi><mo id="A5.p1.1.m1.2.3.1" xref="A5.p1.1.m1.2.3.1.cmml">∼</mo><mrow id="A5.p1.1.m1.2.3.3" xref="A5.p1.1.m1.2.3.3.cmml"><mi id="A5.p1.1.m1.2.3.3.2" xref="A5.p1.1.m1.2.3.3.2.cmml">𝕌</mi><mo id="A5.p1.1.m1.2.3.3.1" xref="A5.p1.1.m1.2.3.3.1.cmml">⁢</mo><mrow id="A5.p1.1.m1.2.3.3.3.2" xref="A5.p1.1.m1.2.3.3.3.1.cmml"><mo id="A5.p1.1.m1.2.3.3.3.2.1" stretchy="false" xref="A5.p1.1.m1.2.3.3.3.1.cmml">[</mo><mn id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml">0</mn><mo id="A5.p1.1.m1.2.3.3.3.2.2" xref="A5.p1.1.m1.2.3.3.3.1.cmml">,</mo><mn id="A5.p1.1.m1.2.2" xref="A5.p1.1.m1.2.2.cmml">100</mn><mo id="A5.p1.1.m1.2.3.3.3.2.3" stretchy="false" xref="A5.p1.1.m1.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.2b"><apply id="A5.p1.1.m1.2.3.cmml" xref="A5.p1.1.m1.2.3"><csymbol cd="latexml" id="A5.p1.1.m1.2.3.1.cmml" xref="A5.p1.1.m1.2.3.1">similar-to</csymbol><ci id="A5.p1.1.m1.2.3.2.cmml" xref="A5.p1.1.m1.2.3.2">𝑡</ci><apply id="A5.p1.1.m1.2.3.3.cmml" xref="A5.p1.1.m1.2.3.3"><times id="A5.p1.1.m1.2.3.3.1.cmml" xref="A5.p1.1.m1.2.3.3.1"></times><ci id="A5.p1.1.m1.2.3.3.2.cmml" xref="A5.p1.1.m1.2.3.3.2">𝕌</ci><interval closure="closed" id="A5.p1.1.m1.2.3.3.3.1.cmml" xref="A5.p1.1.m1.2.3.3.3.2"><cn id="A5.p1.1.m1.1.1.cmml" type="integer" xref="A5.p1.1.m1.1.1">0</cn><cn id="A5.p1.1.m1.2.2.cmml" type="integer" xref="A5.p1.1.m1.2.2">100</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.2c">t\sim\mathbb{U}[0,100]</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.2d">italic_t ∼ blackboard_U [ 0 , 100 ]</annotation></semantics></math> tokens/second, then sample its memory size so it can hold <math alttext="b\sim\mathbb{U}[1,10]" display="inline" id="A5.p1.2.m2.2"><semantics id="A5.p1.2.m2.2a"><mrow id="A5.p1.2.m2.2.3" xref="A5.p1.2.m2.2.3.cmml"><mi id="A5.p1.2.m2.2.3.2" xref="A5.p1.2.m2.2.3.2.cmml">b</mi><mo id="A5.p1.2.m2.2.3.1" xref="A5.p1.2.m2.2.3.1.cmml">∼</mo><mrow id="A5.p1.2.m2.2.3.3" xref="A5.p1.2.m2.2.3.3.cmml"><mi id="A5.p1.2.m2.2.3.3.2" xref="A5.p1.2.m2.2.3.3.2.cmml">𝕌</mi><mo id="A5.p1.2.m2.2.3.3.1" xref="A5.p1.2.m2.2.3.3.1.cmml">⁢</mo><mrow id="A5.p1.2.m2.2.3.3.3.2" xref="A5.p1.2.m2.2.3.3.3.1.cmml"><mo id="A5.p1.2.m2.2.3.3.3.2.1" stretchy="false" xref="A5.p1.2.m2.2.3.3.3.1.cmml">[</mo><mn id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml">1</mn><mo id="A5.p1.2.m2.2.3.3.3.2.2" xref="A5.p1.2.m2.2.3.3.3.1.cmml">,</mo><mn id="A5.p1.2.m2.2.2" xref="A5.p1.2.m2.2.2.cmml">10</mn><mo id="A5.p1.2.m2.2.3.3.3.2.3" stretchy="false" xref="A5.p1.2.m2.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.2b"><apply id="A5.p1.2.m2.2.3.cmml" xref="A5.p1.2.m2.2.3"><csymbol cd="latexml" id="A5.p1.2.m2.2.3.1.cmml" xref="A5.p1.2.m2.2.3.1">similar-to</csymbol><ci id="A5.p1.2.m2.2.3.2.cmml" xref="A5.p1.2.m2.2.3.2">𝑏</ci><apply id="A5.p1.2.m2.2.3.3.cmml" xref="A5.p1.2.m2.2.3.3"><times id="A5.p1.2.m2.2.3.3.1.cmml" xref="A5.p1.2.m2.2.3.3.1"></times><ci id="A5.p1.2.m2.2.3.3.2.cmml" xref="A5.p1.2.m2.2.3.3.2">𝕌</ci><interval closure="closed" id="A5.p1.2.m2.2.3.3.3.1.cmml" xref="A5.p1.2.m2.2.3.3.3.2"><cn id="A5.p1.2.m2.1.1.cmml" type="integer" xref="A5.p1.2.m2.1.1">1</cn><cn id="A5.p1.2.m2.2.2.cmml" type="integer" xref="A5.p1.2.m2.2.2">10</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.2c">b\sim\mathbb{U}[1,10]</annotation><annotation encoding="application/x-llamapun" id="A5.p1.2.m2.2d">italic_b ∼ blackboard_U [ 1 , 10 ]</annotation></semantics></math> blocks (out of 70 blocks in total, as in BLOOM-176B).</p>
<p id="A5.p2.1">Each server follows a certain availability schedule, i.e. turns on and shuts down at the same predefined time across all experiments.
We assign these schedules such that the number of active servers follows a sine wave, simulating daily activity cycles.
The schedule has approximately 100–110 active servers during peak activity and 15–25 servers at its lowest points.
Note that each peak contains a different subset of 100–110 active servers out of 206 instances in total.</p>
<div id="A5.p3">
<p id="A5.p3.1">We evaluate the following approaches to load balancing:</p>
<ol id="A5.I1">
<li id="A5.I1.i1">
<span>1.</span>
<p id="A5.I1.i1.p1.1"><span id="A5.I1.i1.p1.1.1">No load balancing</span> – a baseline system where servers load a random contiguous interval of model blocks.
</p>
</li>
<li id="A5.I1.i2">
<span>2.</span>
<p id="A5.I1.i2.p1.1"><span id="A5.I1.i2.p1.1.1">Balancing new servers only</span> – a simplified load balancing where servers choose the optimal blocks when joining the swarm (using the rule (<a href="#A4.E1" title="1 ‣ Initial block assignment. ‣ Appendix D Details of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>1</span></a>) from Appendix <a href="#A4" title="Appendix D Details of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>D</span></a>) but never change them.</p>
</li>
<li id="A5.I1.i3">
<span>3.</span>
<p id="A5.I1.i3.p1.1"><span id="A5.I1.i3.p1.1.1">Full load balancing</span> – the full algorithm, where every minute each server checks if they need to replace their blocks. We use the efficiency threshold <math alttext="p" display="inline" id="A5.I1.i3.p1.1.m1.1"><semantics id="A5.I1.i3.p1.1.m1.1a"><mi id="A5.I1.i3.p1.1.m1.1.1" xref="A5.I1.i3.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.1.m1.1b"><ci id="A5.I1.i3.p1.1.m1.1.1.cmml" xref="A5.I1.i3.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.I1.i3.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="A5.I1.i3.p1.1.m1.1d">italic_p</annotation></semantics></math> (as described in Appendix <a href="#A4" title="Appendix D Details of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>D</span></a>) to avoid excess block replacements.</p>
</li>
<li id="A5.I1.i4">
<span>4.</span>
<p id="A5.I1.i4.p1.1"><span id="A5.I1.i4.p1.1.1">Upper bound</span> — the best-case throughput estimate that reassigns contiguous block segments to servers optimally every minute.</p>
</li>
</ol>
</div>
<p id="A5.p4.2">We report their behavior in Figure <a href="#A4.F2" title="Figure 2 ‣ Stability of the greedy algorithm. ‣ Appendix D Details of the server load balancing algorithms ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>2</span></a>. The full load balancing maintains connectivity throughout the experiment and achieves throughput close to the upper bound (staying within the 10–15% range most of the time). Higher thresholds <math alttext="p" display="inline" id="A5.p4.1.m1.1"><semantics id="A5.p4.1.m1.1a"><mi id="A5.p4.1.m1.1.1" xref="A5.p4.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A5.p4.1.m1.1b"><ci id="A5.p4.1.m1.1.1.cmml" xref="A5.p4.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="A5.p4.1.m1.1d">italic_p</annotation></semantics></math> perform slightly worse during peak times but require only relatively infrequent block replacements, unlike the case with <math alttext="p=1\%" display="inline" id="A5.p4.2.m2.1"><semantics id="A5.p4.2.m2.1a"><mrow id="A5.p4.2.m2.1.1" xref="A5.p4.2.m2.1.1.cmml"><mi id="A5.p4.2.m2.1.1.2" xref="A5.p4.2.m2.1.1.2.cmml">p</mi><mo id="A5.p4.2.m2.1.1.1" xref="A5.p4.2.m2.1.1.1.cmml">=</mo><mrow id="A5.p4.2.m2.1.1.3" xref="A5.p4.2.m2.1.1.3.cmml"><mn id="A5.p4.2.m2.1.1.3.2" xref="A5.p4.2.m2.1.1.3.2.cmml">1</mn><mo id="A5.p4.2.m2.1.1.3.1" xref="A5.p4.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.p4.2.m2.1b"><apply id="A5.p4.2.m2.1.1.cmml" xref="A5.p4.2.m2.1.1"><eq id="A5.p4.2.m2.1.1.1.cmml" xref="A5.p4.2.m2.1.1.1"></eq><ci id="A5.p4.2.m2.1.1.2.cmml" xref="A5.p4.2.m2.1.1.2">𝑝</ci><apply id="A5.p4.2.m2.1.1.3.cmml" xref="A5.p4.2.m2.1.1.3"><csymbol cd="latexml" id="A5.p4.2.m2.1.1.3.1.cmml" xref="A5.p4.2.m2.1.1.3.1">percent</csymbol><cn id="A5.p4.2.m2.1.1.3.2.cmml" type="integer" xref="A5.p4.2.m2.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.2.m2.1c">p=1\%</annotation><annotation encoding="application/x-llamapun" id="A5.p4.2.m2.1d">italic_p = 1 %</annotation></semantics></math>. Note that using the assignment leading to the upper bound is not possible in practice since it requires each server to load a different set of layers every minute, on top of solving the computationally expensive optimization problem.</p>
<p id="A5.p5.1">Curiously, the baseline running load balancing for <span id="A5.p5.1.1">new servers only</span> achieves reasonable throughput during periods where servers are actively joining. However, it quickly loses throughput when random servers leave, since this creates “bottlenecks” in the pipeline that require rebalancing of existing peers. Finally, the naive baseline with random layer assignment has zero throughput most of the time because it is unable to form a complete pipeline.</p>
</section>
<section id="A6">
<h2>
<span>Appendix F </span>Experiments with a wider range of failure rates</h2>
<p id="A6.p1.1">In this section, we follow the setup from Section <a href="#S4.SS1" title="4.1 Inference with unreliable servers ‣ 4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>4.1</span></a> and provide additional evaluations for a wider range of failure rates (up to 5%) and sequence lengths (up to 2048 tokens). The results are shown in Figure <a href="#A6.F3" title="Figure 3 ‣ Appendix F Experiments with a wider range of failure rates ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>3</span></a>. Unlike baselines, our algorithm provides reasonable performance <span id="A6.p1.1.1">in all tested conditions</span>, especially for higher failure rates common for communicating over the Internet, using spot/preemptible instances or unreliable hardware).</p>
<figure id="A6.F3"><img alt="Refer to caption" height="1079" id="A6.F3.g1" src="https://olu.online/24-hopes-for-2024/x3.png" width="727"/>
<figcaption><span>Figure 3: </span>Sequential inference speed (steps/s) for BLOOM (7.1B) with varying failure rates. The setup is the same as in Section <a href="#S4.SS1" title="4.1 Inference with unreliable servers ‣ 4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>4.1</span></a>. A failure rate <math alttext="p" display="inline" id="A6.F3.3.m1.1"><semantics id="A6.F3.3.m1.1b"><mi id="A6.F3.3.m1.1.1" xref="A6.F3.3.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A6.F3.3.m1.1c"><ci id="A6.F3.3.m1.1.1.cmml" xref="A6.F3.3.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F3.3.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="A6.F3.3.m1.1e">italic_p</annotation></semantics></math> means that sending a set of activations to the next pipeline stage fails with probability <math alttext="p" display="inline" id="A6.F3.4.m2.1"><semantics id="A6.F3.4.m2.1b"><mi id="A6.F3.4.m2.1.1" xref="A6.F3.4.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A6.F3.4.m2.1c"><ci id="A6.F3.4.m2.1.1.cmml" xref="A6.F3.4.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F3.4.m2.1d">p</annotation><annotation encoding="application/x-llamapun" id="A6.F3.4.m2.1e">italic_p</annotation></semantics></math>. Zero speed means that the baseline did not finish within 1 hour.</figcaption>
</figure>
</section>
<section id="A7">
<h2>
<span>Appendix G </span>Performance of training-time forward and backward passes</h2>
<p id="A7.p1.1">In this section, we evaluate throughput of training-time forward and backward passes and study factors that affect their performance. We will only consider BLOOM-176B and the “3<math alttext="\times" display="inline" id="A7.p1.1.m1.1"><semantics id="A7.p1.1.m1.1a"><mo id="A7.p1.1.m1.1.1" xref="A7.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A7.p1.1.m1.1b"><times id="A7.p1.1.m1.1.1.cmml" xref="A7.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A7.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A7.p1.1.m1.1d">×</annotation></semantics></math> A100, 1 Gbit/s” setup from Section <a href="#S4.SS2" title="4.2 Experiments for Llama 2 (70B) and BLOOM (176B) ‣ 4 Experiments ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>4.2</span></a> and focus on finetuning-specific hyperparameters, since the influence of network bandwidth and latency has already been discussed in the main paper.</p>
<section id="A7.SS0.SSS0.Px1">
<h4>Sequence classification.</h4>
<p id="A7.SS0.SSS0.Px1.p1.1">First, we consider fine-tuning the model on a binary classification task. We take BLOOM-176B, replace the logit layer with a trainable classification head (similar to <span id="A7.SS0.SSS0.Px1.p1.1.1">transformers.BloomForSequenceClassification</span>), and add trainable prompts before the input sequence, then train the model on batches of 128-token sequences. We try <span id="A7.SS0.SSS0.Px1.p1.1.2">(a)</span> both prompt tuning and prefix tuning (involving “deep” prompts), <span id="A7.SS0.SSS0.Px1.p1.1.3">(b)</span> two batch sizes (8 and 32), and <span id="A7.SS0.SSS0.Px1.p1.1.4">(c)</span> two prompt lengths (16 and 4). The client shares 8 CPU cores with one of the servers and does not use the GPU.</p>
<p id="A7.SS0.SSS0.Px1.p2.1">The results are provided in Table <a href="#A7.T6" title="Table 6 ‣ Language modeling. ‣ Appendix G Performance of training-time forward and backward passes ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>6</span></a>. The prefix tuning turns out to be slower, since it adds several times more trainable parameters. Increasing prompt length and decreasing batch size also make training slower. Notably, we observe that moving client-side computations to GPU does not visibly improve performance, since the client does not perform any heavy operations in this setup.</p>
</section>
<section id="A7.SS0.SSS0.Px2">
<h4>Language modeling.</h4>
<p id="A7.SS0.SSS0.Px2.p1.1">Next, we consider fine-tuning the model on a causal language modeling task. We take BLOOM-176B, keep the logit layer, and add trainable prompts before the input sequence. We explore the same hyperparameters as with sequence classification.</p>
<p id="A7.SS0.SSS0.Px2.p2.1">We observe that the throughput of the GPU-enabled client is similar (within 10% difference) to the throughput in case of sequence classification, reported in Table <a href="#A7.T6" title="Table 6 ‣ Language modeling. ‣ Appendix G Performance of training-time forward and backward passes ‣ Distributed Inference and Fine-tuning of Large Language Models Over The Internet"><span>6</span></a>. Indeed, the client performs only a small share of GPU computations in the forward and backward passes, and a particular model head and a loss function do not have decisive influence on the performance. However, performance of the CPU-only client turns out to be 5-10 times worse in this setup, since the client has to multiply the output embedding matrix to the hidden states of all tokens in the batch. This operation is too large to be efficiently computed on CPU.</p>
<figure id="A7.T6">
<figcaption><span>Table 6: </span>Throughput (tokens/sec) of forward and backward passes for different tasks, batch sizes, prefix lengths.</figcaption>

</figure>
</section>
</section>
<section id="A8">
<h2>
<span>Appendix H </span>Limitations and broader impact</h2>
<section id="A8.SS0.SSS0.Px1">
<h4>Privacy.</h4>
<p id="A8.SS0.SSS0.Px1.p1.1">A key limitation of our approach is that servers hosting the first model blocks may use their inputs to recover client data. Thus, users working with <span id="A8.SS0.SSS0.Px1.p1.1.1">sensitive</span> data should limit their clients to only use trusted servers or, alternatively, set up their own isolated network using our software. For example, if multiple research labs or small companies have access to a specific private dataset and want to process it with a large language model, they may set up an isolated distributed network hosting this model to get a better inference speed, compared to running the model independently.</p>
<p id="A8.SS0.SSS0.Px1.p2.1">In the future, this limitation may be addressed in future work using secure multi-party computing <cite>(Evans et al., <a href="#bib.bib15" title="">2018</a>)</cite> or privacy-preserving hardware <cite>(NVIDIA, <a href="#bib.bib44" title="">2022</a>)</cite>.</p>
</section>
<section id="A8.SS0.SSS0.Px2">
<h4>Motivating contributors.</h4>
<p id="A8.SS0.SSS0.Px2.p1.1">Since people using the client are not required to run a server, our system may experience an imbalance between supply (peers who dedicate GPUs to serve model layers) and demand (peers using the servers to perform inference or fine-tuning for their own needs).</p>
<p id="A8.SS0.SSS0.Px2.p2.1">One way to encourage users to serve model blocks would be to introduce a system of incentives: peers running servers would earn <span id="A8.SS0.SSS0.Px2.p2.1.1">reward points</span>, which can be spent on high-priority inference and fine-tuning or exchanged for other rewards. To implement this, we can run a few <span id="A8.SS0.SSS0.Px2.p2.1.2">validator peers</span> that periodically traverse all available servers and issue reward points to their owners.</p>
</section>
<section id="A8.SS0.SSS0.Px3">
<h4>Security.</h4>
<p id="A8.SS0.SSS0.Px3.p1.1">We assume that servers in our system are run by many independent parties. In practice, some of them may turn out to be faulty and return incorrect outputs instead of the actual results of forward and backward passes. This may happen due to a malicious intent to influence other people’s outputs or, when rewards are introduced (as described above), to earn a reward for serving layers without actually performing the calculations.</p>
<p id="A8.SS0.SSS0.Px3.p2.1">To address this issue, we can extend the validator peers, so that they periodically test servers with random requests of different types and ban them if they respond with incorrect outputs (possibly, revoking their rewards). The validator requests should be difficult to distinguish from requests of typical users, so that malicious servers cannot pretend to be honest to the validators but send wrong outputs to other peers. While this approach still leaves a chance of receiving wrong outputs, it allows to eventually expose and penalize the faulty servers.</p>
<p id="A8.SS0.SSS0.Px3.p3.1">Finally, clients may reduce the probability of getting faulty outputs by running their data through multiple disjoint chains of servers simultaneously and comparing the outputs against each other.</p>
</section>
<section id="A8.SS0.SSS0.Px4">
<h4>Broader impact.</h4>
<p id="A8.SS0.SSS0.Px4.p1.1">This work introduces a general-purpose algorithm for decentralized inference and fine-tuning of large models, aiming to simplify access to the latest research in deep learning and provide an alternative way to efficiently run LLMs without high-end hardware. We do not envision any direct negative impacts from our research, since models that can be hosted with our system are already widely available and may be used via APIs, offloading, or other means.</p>

</section>
</section>
</article></div>
  </body>
</html>
