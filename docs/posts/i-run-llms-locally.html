<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://abishekmuthian.com/how-i-run-llms-locally/">Original</a>
    <h1>I Run LLMs Locally</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>A HN user asked me<sup><a href="#sources">0</a></sup> how I run LLMs locally with some specific questions, I’m documenting it here for everyone.</p><p>Before I begin I would like to credit the thousands or millions of unknown artists, coders and writers upon whose work the Large Language Models(LLMs) are trained, often without due credit or compensation.</p><h2 id="get-started">Get Started</h2><p>r/LocalLLaMA subreddit<sup><a href="#sources">1</a></sup> &amp; Ollama blog<sup><a href="#sources">2</a></sup> are great places to get started with running LLMs locally.</p><h2 id="hardware">Hardware</h2><p>I have a laptop running Linux with core i9 (32threads) CPU, 4090 GPU (16GB VRAM) and 96 GB of RAM. Models which fit within the VRAM can generate more tokens/second, larger models will be offloaded to RAM (dGPU offloading) and thereby lower tokens/second. I will talk about models in a section below.</p><p>It&#39;s not necessary to have such beefy computer for running LLMs locally, smaller models would run fine in older GPUs or in CPU albeit slowly and with more hallucinations.</p><p>There are a number of high quality open-source tools which enable running LLMs locally. These are the tools I use regularly.</p><p>Ollama<sup><a href="#sources">3</a></sup> is a middleware with python, JavaScript libraries for llama.cpp<sup><a href="#sources">4</a></sup> which helps run LLMs. I use Ollama in docker<sup><a href="#sources">5</a></sup>.</p><p>Open WebUI<sup><a href="#sources">6</a></sup> is a frontend which offers familiar chat interface for text and image input and communicates with Ollama back-end and streams the output back to the user.</p><p>llamafile<sup><a href="#sources">7</a></sup> is a single executable file with LLM. It’s probably the easiest way to get started with local LLMs, but I’ve had issues with dGPU offloading in llamafile<sup><a href="#sources">8</a></sup>.</p><p>I’m not a big consumer of image/video generation models, but when needed, I use AUTOMATIC1111<sup><a href="#sources">9</a></sup> for images which require some customisation and Fooocus<sup><a href="#sources">10</a></sup> for simple image generation. For complex workflow automatons with image generation, there’s ComfyUI<sup><a href="#sources">11</a></sup>.</p><p>For code completion I use Continue<sup><a href="#sources">12</a></sup> in VSCode.</p><p>I use Smart Connections<sup><a href="#sources">13</a></sup> in Obsidian<sup><a href="#sources">14</a></sup> to query my notes using Ollama.</p><figure><img alt="Screenshot of Obsidian with chat smart connections extension showing the last journal I write" src="https://abishekmuthian.com/images/obsidian-smart-connections.jpg" width="100%" height="auto" layout="responsive"/><figcaption>I asked Smart Connections when I wrote my last journal, I hope to write my journal every day in 2025.</figcaption></figure><h2 id="models">Models</h2><p>I use Ollama models page<sup><a href="#sources">15</a></sup> to download latest LLMs. I use RSS on Thunderbird to keep track of the models. I use CivitAI<sup><a href="#sources">16</a></sup> to download image generation models for specific styles (e.g. Isometric for world building). But be advised that most models in CivitAI seem to be catered towards adult image generation.</p><p>I choose LLMs based on performance/ size. My current choice for LLMs changes frequently due to rapid advancement in LLMs.</p><pre><code>•	Llama3.2 for Smart Connections and generic queries.
•	Deepseek-coder-v2 for code completion in Continue.
•	Qwen2.5-coder for chatting about code in Continue.
•	Stable Diffusion for image generation in AUTOMATIC1111 or Fooocus.
</code></pre><h2 id="updation">Updation</h2><p>I update the docker containers using WatchTower<sup><a href="#sources">17</a></sup> and models from within the Open Web UI.</p><h2 id="fine-tuning-and-quantization">Fine-Tuning and Quantization</h2><p>I haven’t fine-tuned or quantized any models on my machine yet as my Intel CPU may have a manufacturing defect<sup><a href="#sources">18</a></sup> so I don’t want to push it to high temperatures for long durations during training.</p><h2 id="conclusion">Conclusion</h2><p>Running LLMs locally gives me total control over my data and lower latency for the responses. None of these would be possible without the open-source projects and <del>open-source</del> free models and original owners of the data upon which these models are trained.</p><p>I will update this post as and when I use newer tools/models.</p><p>[0] <a href="https://news.ycombinator.com/item?id=42537024" target="_blank">https://news.ycombinator.com/item?id=42537024</a></p><p>[1] <a href="https://www.reddit.com/r/LocalLLaMA/" target="_blank">https://www.reddit.com/r/LocalLLaMA/</a></p><p>[2] <a href="https://ollama.com/blog" target="_blank">https://ollama.com/blog</a></p><p>[3] <a href="https://ollama.com/download" target="_blank">https://ollama.com/download</a></p><p>[4] <a href="https://github.com/ggerganov/llama.cpp" target="_blank">https://github.com/ggerganov/llama.cpp</a></p><p>[5] <a href="https://hub.docker.com/r/ollama/ollama" target="_blank">https://hub.docker.com/r/ollama/ollama</a></p><p>[6] <a href="https://github.com/open-webui/open-webui" target="_blank">https://github.com/open-webui/open-webui</a></p><p>[7] <a href="https://github.com/Mozilla-Ocho/llamafile" target="_blank">https://github.com/Mozilla-Ocho/llamafile</a></p><p>[8] <a href="https://github.com/Mozilla-Ocho/llamafile/issues/611" target="_blank">https://github.com/Mozilla-Ocho/llamafile/issues/611</a></p><p>[9] <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui" target="_blank">https://github.com/AUTOMATIC1111/stable-diffusion-webui</a></p><p>[10] <a href="https://github.com/lllyasviel/Fooocus" target="_blank">https://github.com/lllyasviel/Fooocus</a></p><p>[11] <a href="https://github.com/comfyanonymous/ComfyUI" target="_blank">https://github.com/comfyanonymous/ComfyUI</a></p><p>[12] <a href="https://docs.continue.dev/getting-started/overview" target="_blank">https://docs.continue.dev/getting-started/overview</a></p><p>[13] <a href="https://github.com/brianpetro/obsidian-smart-connections" target="_blank">https://github.com/brianpetro/obsidian-smart-connections</a></p><p>[14] <a href="https://obsidian.md" target="_blank">https://obsidian.md</a></p><p>[15] <a href="https://ollama.com/search" target="_blank">https://ollama.com/search</a></p><p>[16] <a href="https://civitai.com/models/63376/isometric-chinese-style-architecture-lora" target="_blank">https://civitai.com/models/63376/isometric-chinese-style-architecture-lora</a></p><p>[17] <a href="https://containrrr.dev/watchtower/" target="_blank">https://containrrr.dev/watchtower/</a></p><p>[18] <a href="https://en.wikipedia.org/wiki/Raptor_Lake#Instability_and_degradation_issue" target="_blank">https://en.wikipedia.org/wiki/Raptor_Lake#Instability_and_degradation_issue</a></p></div><h3>Newsletter</h3><p>I strive to write low frequency, High quality content on Health, Product Development, Programming, Software Engineering, DIY, Security, Philosophy and other interests.
If you would like to receive them in your email inbox then please consider subscribing to my <a href="http://eepurl.com/if97pb" target="_blank">Newsletter</a>.</p></div></div>
  </body>
</html>
