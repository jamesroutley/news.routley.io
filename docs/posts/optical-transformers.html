<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://arxiv.org/abs/2302.10360">Original</a>
    <h1>Optical Transformers</h1>
    
    <div id="readability-page-1" class="page"><div id="content-inner">
  <div id="abs">
    
    
    
      
    
  
    <p><a href="https://arxiv.org/pdf/2302.10360">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  The rapidly increasing size of deep-learning models has caused renewed and
growing interest in alternatives to digital computers to dramatically reduce
the energy cost of running state-of-the-art neural networks. Optical
matrix-vector multipliers are best suited to performing computations with very
large operands, which suggests that large Transformer models could be a good
target for optical computing. To test this idea, we performed small-scale
optical experiments with a prototype accelerator to demonstrate that
Transformer operations can run on optical hardware despite noise and errors.
Using simulations, validated by our experiments, we then explored the energy
efficiency of optical implementations of Transformers and identified scaling
laws for model performance with respect to optical energy usage. We found that
the optical energy per multiply-accumulate (MAC) scales as $\frac{1}{d}$ where
$d$ is the Transformer width, an asymptotic advantage over digital systems. We
conclude that with well-engineered, large-scale optical hardware, it may be
possible to achieve a $100 \times$ energy-efficiency advantage for running some
of the largest current Transformer models, and that if both the models and the
optical hardware are scaled to the quadrillion-parameter regime, optical
computers could have a $&gt;8,000\times$ energy-efficiency advantage over
state-of-the-art digital-electronic processors that achieve 300 fJ/MAC. We
analyzed how these results motivate and inform the construction of future
optical accelerators along with optics-amenable deep-learning approaches. With
assumptions about future improvements to electronics and Transformer
quantization techniques (5$\times$ cheaper memory access, double the
digital--analog conversion efficiency, and 4-bit precision), we estimated that
optical computers&#39; advantage against current 300-fJ/MAC digital processors
could grow to $&gt;100,000\times$.

    </blockquote>

    <!--CONTEXT-->
    
  </div>
</div><div>
      <h2>Submission history</h2><p> From: Maxwell Anderson [<a href="https://arxiv.org/show-email/4300901b/2302.10360">view email</a>]
      </p></div></div>
  </body>
</html>
