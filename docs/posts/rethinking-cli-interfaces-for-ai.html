<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.notcheckmark.com/2025/07/rethinking-cli-interfaces-for-ai/">Original</a>
    <h1>Rethinking CLI interfaces for AI</h1>
    
    <div id="readability-page-1" class="page"><section>
        <p>We need to augment our command line tools and design APIs so they can be better used by LLM Agents.¬†The designs are inadequate for LLMs as they are now ‚Äì  especially if you&#39;re constrained by the tiny context windows available with local models.</p><h3 id="agent-apis">Agent APIs</h3><p>Like many developers, I‚Äôve been dipping my toes into LLM agents. I‚Äôve done my fair share of vibe coding, but also I‚Äôve been playing around with using LLMs to automate reverse engineering tasks mostly using mrexodia‚Äôs <a href="https://github.com/mrexodia/ida-pro-mcp?ref=notcheckmark.com"><u>IDA Pro MCP</u></a>, including extending it.</p><p>Developing an MCP interface is an interesting process. You need to walk the line between providing too much information to avoid filling the context windows but also providing enough information to reduce tool calls. We have a few APIs that are better than others, like <code>get_global_variable_at</code>, which takes an address, identifies the type, and returns the best string representation of that value based on that type. However, the function can fail, so we provide a second set of accessor methods (<code>data_read_dword</code>, <code>data_read_word</code>, <code>read_memory_bytes</code>, etc). These accessor methods are fine, but they ignore type information ‚Äì so we don‚Äôt want the LLM to use them first.</p><p>To mitigate this problem, we added some guidance into the docstrings:</p><pre><code>@jsonrpc
@idaread
def data_read_byte(
    address: Annotated[str, &#34;Address to get 1 byte value from&#34;],
) -&gt; int:
    &#34;&#34;&#34;
    Read the 1 byte value at the specified address.

    Only use this function if `get_global_variable_at` failed.
    &#34;&#34;&#34;
    ea = parse_address(address)
    return ida_bytes.get_wide_byte(ea)</code></pre><p>This seems to have mostly worked, but these sorts of problems exist for all the APIs. We have the nice convenience function and we also have the more gnarly but more complete function and we want the LLM to use the convenience one first.</p><p>¬†I like to do work with offline LLMs which have much smaller context windows, so having better APIs matters a lot.</p><p>These problems exist for command line tools also. If you watch Claude Code, you‚Äôll see that it often uses <code>head -n100</code> to limit the results apriori. It also gets lost about which directory it‚Äôs in, and it will frustratingly flail around trying to run commands in different directories until it finds the right one.</p><p>To keep Claude Code in line on my project, I‚Äôve relied heavily on linters, build scripts, formatters, and git commit hooks. It‚Äôs pretty easy to get Claude Code to commit often by including it in your CLAUDE.md, but it often likes to ignore other commands like ‚Äúmake sure the build doesn‚Äôt fail‚Äù and ‚Äúfix any failing tests‚Äù. All my projects have a <a href="https://gist.github.com/withzombies/48829733ddeef289a3f42dbb2babbd7c?ref=notcheckmark.com"><u>.git/hooks/pre-commit script</u></a> that enforces project standards. The hook works really well to keep things in line.</p><p>However, on really difficult changes it really doesn‚Äôt like to acknowledge that it broke a test. I often see it in this loop:</p><ul><li>Make a change</li><li>Build the project: passes</li><li>Run the tests: fail</li><li>Attempt to fix the test</li><li>Fail at fixing the test</li><li>Say ‚Äúthis test was failing beforehand, going to commit with <code>--no-verify</code>&#34;</li></ul><p>Then it commits bypassing the hooks! It was doing this so often that I have a <a href="https://gist.github.com/withzombies/6dcfbd064c70d478f8aa06dd328b801e?ref=notcheckmark.com"><u>git wrapper</u></a> in my <code>PATH</code> now which prevents <code>git commit --no-verify</code> and the error message has some prompting to actually fix the errors.</p><pre><code>$ git commit --no-verify
------------------------------------------------------------------
‚ùå ERROR: Commit Rejected.
The use of the &#39;--no-verify&#39; flag is disabled for this repository.
------------------------------------------------------------------

ü§ñ GUIDANCE FOR THE AI AGENT:
You have attempted to bypass the required pre-commit verification 
steps. All code must pass quality checks (formatting, linting, and
tests) before it can be committed.

DO NOT BYPASS THE CHECKS. YOU MUST FIX THE UNDERLYING ERRORS.

The pre-commit hook is likely failing. Diagnose and fix the issues. 
Search for advice if you get stuck.

After all commands complete successfully, attempt the commit again 
*without* the &#39;--no-verify&#39; flag.</code></pre><p>This started a game of whack-a-mole where the LLM would also attempt to change the pre-commit hooks! I had to fix it by denying <code>Edit(.git/hooks/pre-commit)</code> to my project‚Äôs <code>.claude/settings.json</code>. I look forward to its next lazy innovation.</p><h2 id="information-architecture-for-llms">Information Architecture for LLMs</h2><p>The field of user experience has a concept called ‚Äú<a href="https://en.wikipedia.org/wiki/Information_architecture?ref=notcheckmark.com"><u>Information Architecture</u></a>‚Äù. Information Architecture focuses on how and what information is presented to users to provide the best possible user experience. You rarely notice a good IA, but you notice the lack of one.</p><figure><img src="https://www.notcheckmark.com/content/images/2025/07/0-Lgq-7qQyDGEd7F0f.png" alt="Bulk Rename Utility, notorious for having a terrible user experience" loading="lazy" width="926" height="704" srcset="https://www.notcheckmark.com/content/images/size/w600/2025/07/0-Lgq-7qQyDGEd7F0f.png 600w, https://www.notcheckmark.com/content/images/2025/07/0-Lgq-7qQyDGEd7F0f.png 926w" sizes="(min-width: 720px) 720px"/><figcaption><span>You have all the options you could ever want! Perhaps you&#39;d prefer a </span><a href="https://th.bing.com/th/id/R.388e625757265a08fc114093437e8976?rik=NrcFwbRLDGIypQ&amp;riu=http%3A%2F%2Fi.stack.imgur.com%2FBiSAr.jpg&amp;ehk=SS7C6EHweNEu65FT2eJ774ku2CRqPGoGHXGoCpHidEw%3D&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;ref=notcheckmark.com" rel="noreferrer"><span>second example</span></a><span>.</span></figcaption></figure><p>I think watching the agents use our existing command line utilities get confused and lost is a strong indicator that the information architecture of our command line utilities is inadequate.</p><p>LLMs are trained on using our existing CLI tools, so I think we need to augment them with context that is useful to the LLM and maybe adapt the output to be better consumed by agents.</p><p>Going back to the <code>head</code> example before. Often, the agent will run my build with <code>cargo build | head -n100</code>. I think there are a few problems here: </p><ul><li>the agent will have to repeat the build to get further output/errors, </li><li>the agent has no idea how many lines are remaining, and</li><li>re-running the build is resource intensive. </li></ul><p>Perhaps we could replace head with a wrapper which cache‚Äôs the output, converts it into a more structured output, and informs the agent how many lines are remaining.</p><p>Similarly, when the agent fails to run a command because it‚Äôs in the wrong directory. We could give the agent a little extra help with a shell hook:</p><pre><code>command_not_found_handler() {
¬†¬†¬†echo &#34;zsh: command not found: &#39;$1&#39;&#34;
¬†¬†¬†echo &#34;zsh: current directory is $PWD&#34;
¬†¬†¬†return 127¬† # Keep standard behavior (127 = command not found)
¬†}</code></pre><pre><code>$ sdfsdf
zsh: command not found: &#39;sdfsdf&#39;
zsh: current directory is /Users/ryan</code></pre><p>It could even be a bit fuzzier, checking the recent or parent directories for the command and suggesting:</p><pre><code>$ sdfsdf
zsh: command not found: &#39;sdfsdf&#39;
zsh: current directory is /Users/ryan
zsh: Perhaps you meant to run: cd agent_directory; sdfsdf</code></pre><h2 id="conclusion">Conclusion</h2><p>Basically every CLI tool can be improved in some way to provide extra context to LLMs. It will reduce tool calls and optimize context windows.¬†</p><p>The agents may benefit from some training on tools available within their agents. This will certainly help with the majority of general CLI tools, there are bespoke tools that could benefit from adapting to LLMs.</p><p>It seems a bit silly to suggest, but perhaps we need a whole set of LLM-enhanced CLI tools or a custom LLM shell? The user experience (UX) field could even branch into AI experience and provide us a whole new information architecture.</p>
    </section></div>
  </body>
</html>
