<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://martinfowler.com/articles/building-boba.html">Original</a>
    <h1>Building Boba AI: Lessons learnt in building an LLM-powered application</h1>
    
    <div id="readability-page-1" class="page"><p>Some lessons and patterns learnt in building an LLM-powered
  generative application</p><div>
<p>Boba is an experimental AI co-pilot for product strategy &amp; generative ideation,
    designed to augment the creative ideation process. It’s an LLM-powered
    application that we are building to learn about:</p>

<ol>
<li>How to design and build generative experiences beyond chat, powered by
      LLMs</li>

<li>How to use AI to augment our product and strategy processes and craft</li>
</ol>

<p>An AI co-pilot refers to an artificial intelligence-powered assistant designed 
    to help users with various tasks, often providing guidance, support, and automation 
    in different contexts. Examples of its application include navigation systems, 
    digital assistants, and software development environments. We like to think of a co-pilot 
    as an effective partner that a user can collaborate with to perform a specific domain 
    of tasks.</p>

<p>Boba as an AI co-pilot is designed to augment the early stages of strategy ideation and
    concept generation, which rely heavily on rapid cycles of divergent
    thinking (also known as generative ideation). We typically implement generative ideation 
    by closely collaborating with our peers, customers and subject matter experts, so that we can
    formulate and test innovative ideas that address our customers’ jobs, pains and gains. 
    This begs the question, what if AI could also participate in the same process? What if we
    could generate and evaluate more and better ideas, faster in partnership with AI? Boba starts to
    enable this by using OpenAI’s LLM to generate ideas and answer questions
    that can help scale and accelerate the creative thinking process. For the first prototype of
    Boba, we decided to focus on rudimentary versions of the following capabilities:</p>

<p>1. <b>Research signals and trends:</b> Search the web for
    articles and news to help you answer qualitative research questions,
    like:</p>

<ul>
<li>How is the hotel industry using generative AI today?</li>

<li>What are the key challenges facing retailers in 2023 and beyond?</li>

<li>How are pharma companies using AI to accelerate drug discovery?</li>

<li>What were the key takeaways from Nike&#39;s latest earnings call?</li>

<li>How do people on Reddit feel about Lululemon&#39;s products?&#34;</li>
</ul>

<p>2. <b>Creative Matrix: </b>The creative matrix is a concepting method for
    sparking new ideas at the intersections of distinct categories or
    dimensions. This involves stating a strategic prompt, often as a “How might
    we” question, and then answering that question for each
    combination/permutation of ideas at the intersection of each dimension. For
    example:</p>

<ul>
<li>Strategic prompt: “How might we use generative AI to transform wealth
      management?”</li>

<li>Dimension 1 - Stages of the value chain: Client acquisition,
      financial planning, portfolio construction, investment execution, performance
      monitoring, risk management, reporting and communication</li>

<li>Dimension 2 - Different personas: For employees, for customers,
      for partners</li>
</ul>

<p>3. <b>Scenario building:</b> Scenario building is a process of
    generating future-oriented stories by researching signals of change in
    business, culture, and technology. Scenarios are used to socialize learnings
    in a contextualized narrative, inspire divergent product thinking, conduct
    resilience/desirability testing, and/or inform strategic planning. For
    example, you can prompt Boba with the following and get a set of future
    scenarios based on different time horizons and levels of optimism and
    realism:</p>

<ul>
<li>“Hotel industry uses generative AI to transform the guest
      experience”</li>

<li>“Pfizer accelerates drug discovery with the use of generative AI”</li>

<li>“Show me the future of payments 10 years from now”</li>
</ul>

<p>4. <b>Strategy ideation:</b> Using the Playing to Win strategy
    framework, brainstorm &#34;where to play&#34; and &#34;how to win&#34; choices
    based on a strategic prompt and possible future scenarios. For example you
    can prompt it with:</p>

<ul>
<li>How might Nike use generative AI to transform its business model?</li>

<li>How might the Globe &amp; Mail increase readership and engagement?</li>
</ul>

<p>5. <b>Concept generation:</b> Based on a strategic prompt, such as a &#34;how might we&#34; question, generate 
    multiple product or feature concepts, which include value proposition pitches and hypotheses to test.
    </p>

<ul>
<li>How might we make travel more convenient for senior citizens?</li>

<li>How might we make shopping more social?</li>
</ul>

<p>6. <b>Storyboarding:</b> Generate visual storyboards based on a simple
    prompt or detailed narrative based on current or future state scenarios. The
    key features are:</p>

<ul>
<li>Generate illustrated scenes to describe your customer journeys</li>

<li>Customize the styles and illustrations</li>

<li>Generate storyboards directly from generated scenarios</li>
</ul>

<section id="UsingBoba">
<h2>Using Boba</h2>

<p>Boba is a web application that mediates an interaction between a human
      user and a Large-Language Model, currently GPT 3.5. A simple web
      front-end to an LLM just offers the ability for the user to converse with
      the LLM. This is helpful, but means the user needs to learn how to
      effectively interact the LLM. Even in the short time that LLMs have seized
      the public interest, we&#39;ve learned that there is considerable skill to
      constructing the prompts to the LLM to get a useful answer, resulting in
      the notion of a “Prompt Engineer”. A co-pilot application like Boba adds
      a range of UI elements that structure the conversation. This allows a user
      to make naive prompts which the application can manipulate, enriching
      simple requests with elements that will yield a better response from the
      LLM.</p>





<p>Boba can help with a number of product strategy tasks. We won&#39;t
      describe them all here, just enough to give a sense of what Boba does and
      to provide context for the patterns later in the article.</p>

<p>When a user navigates to the Boba application, they see an initial
      screen similar to this</p>

<div id="use-initial-screen.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/use-initial-screen.png" width="960"/></p>
</div>



<p>The left panel lists the various product strategy tasks that Boba
      supports. Clicking on one of these changes the main panel to the UI for
      that task. For the rest of the screenshots, we&#39;ll ignore that task panel
      on the left.</p>

<p>The above screenshot looks at the scenario design task. This invites
      the user to enter a prompt, such as &#34;Show me the future of retail&#34;. </p>

<div id="use-future-retail-prompt.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/use-future-retail-prompt.png" width="800"/></p>
</div>



<p>The UI offers a number of drop-downs in addition to the prompt, allowing
      the user to suggest time-horizons and the nature of the prediction. Boba
      will then ask the LLM to generate scenarios, using <a href="#templated-prompt">Templated Prompt</a> to enrich the user&#39;s prompt
      with additional elements both from general knowledge of the scenario
      building task and from the user&#39;s selections in the UI.</p>

<p>Boba receives a <a href="#formatted-response">Structured Response</a> from the LLM and displays the
      result as set of UI elements for each scenario.</p>

<div id="use-scenario-cards.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/use-scenario-cards.png" width="800"/></p>
</div>



<p>The user can then take one of these scenarios and hit the explore
      button, bringing up a new panel with a further prompt to have a <a href="#conversation">Contextual Conversation</a> with Boba.</p>

<div id="use-explore-scenario.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/use-explore-scenario.png"/></p>
</div>



<p>Boba takes this prompt and enriches it to focus on the context of the
      selected scenario before sending it to the LLM.</p>

<p>Boba uses <a href="#carry-context">Select and Carry Context</a>
      to hold onto the various parts of the user&#39;s interaction
      with the LLM, allowing the user to explore in multiple directions without
      having to worry about supplying the right context for each interaction.</p>

<p>One of the difficulties with using an
      LLM is that it&#39;s trained only on data up to some point in the past, making
      them ineffective for working with up-to-date information. Boba has a
      feature called research signals that uses <a href="#embed-external">Embedded External Knowledge</a>
      to combine the LLM with regular search
      facilities. It takes the prompted research query, such as &#34;How is the
      hotel industry using generative AI today?&#34;, sends an enriched version of
      that query to a search engine, retrieves the suggested articles, sends
      each article to the LLM to summarize.</p>

<div id="use-research-signals.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/use-research-signals.png" width="800"/></p>
</div>



<p>This is an example of how a co-pilot application can handle
      interactions that involve activities that an LLM alone isn&#39;t suitable for. Not
      just does this provide up-to-date information, we can also ensure we
      provide source links to the user, and those links won&#39;t be hallucinations
      (as long as the search engine isn&#39;t partaking of the wrong mushrooms).</p>
</section>

<section id="SomePatternsForBuildingGenerativeCo-pilotApplications">
<h2>Some patterns for building generative co-pilot applications</h2>

<p>In building Boba, we learnt a lot about different patterns and approaches
      to mediating a conversation between a user and an LLM, specifically Open AI’s
      GPT3.5/4. This list of patterns is not exhaustive and is limited to the lessons 
      we&#39;ve learnt so far while building Boba.</p>

<section id="templated-prompt">
<h3>Templated Prompt</h3>

<p>Use a text template to enrich a prompt with context and structure</p>

<p>The first and simplest pattern is using a string templates for the prompts, also
        known as chaining. We use Langchain, a library that provides a standard
        interface for chains and end-to-end chains for common applications out of
        the box. If you’ve used a Javascript templating engine, such as Nunjucks,
        EJS or Handlebars before, Langchain provides just that, but is designed specifically for
        common prompt engineering workflows, including features for function input variables, 
        few-shot prompt templates, prompt validation, and more sophisticated composable chains of prompts.</p>

<p>For example, to brainstorm potential future scenarios in Boba, you can
        enter a strategic prompt, such as “Show me the future of payments” or even a
        simple prompt like the name of a company. The user interface looks like
        this:</p>

<div id="template-initial-screen.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/template-initial-screen.png" width="960"/></p>
</div>



<p>The prompt template that powers this generation looks something like
        this:</p>

<pre>You are a visionary futurist. Given a strategic prompt, you will create
{num_scenarios} futuristic, hypothetical scenarios that happen
{time_horizon} from now. Each scenario must be a {optimism} version of the
future. Each scenario must be {realism}.

Strategic prompt: {strategic_prompt}
</pre>

<p>As you can imagine, the LLM’s response will only be as good as the prompt
        itself, so this is where the need for good prompt engineering comes in.
        While this article is not intended to be an introduction to prompt
        engineering, you will notice some techniques at play here, such as starting
        by telling the LLM to <a href="https://platform.openai.com/docs/guides/gpt-best-practices/tactic-ask-the-model-to-adopt-a-persona">Adopt a
        Persona</a>,
        specifically that of a visionary futurist. This was a technique we relied on
        extensively in various parts of the application to produce more relevant and
        useful completions.</p>

<p>As part of our test-and-learn prompt engineering workflow, we found that
        iterating on the prompt directly in ChatGPT offers the shortest path from
        idea to experimentation and helps build confidence in our prompts quickly.
        Having said that, we also found that we spent way more time on the user
        interface (about 80%) than the AI itself (about 20%), specifically in
        engineering the prompts.</p>

<p>We also kept our prompt templates as simple as possible, devoid of
        conditional statements. When we needed to drastically adapt the prompt based
        on the user input, such as when the user clicks “Add details (signals,
        threats, opportunities)”, we decided to run a different prompt template
        altogether, in the interest of keeping our prompt templates from becoming
        too complex and hard to maintain.</p>
</section>

<section id="formatted-response">
<h3>Structured Response</h3>

<p>Tell the LLM to respond in a structured data format</p>

<p>Almost any application you build with LLMs will most likely need to parse
        the output of the LLM to create some structured or semi-structured data to
        further operate on on behalf of the user. For Boba, we wanted to work with
        JSON as much as possible, so we tried many different variations of getting
        GPT to return well-formed JSON. We were quite surprised by how well and
        consistently GPT returns well-formed JSON based on the instructions in our
        prompts. For example, here’s what the scenario generation response
        instructions might look like:</p>

<pre>You will respond with only a valid JSON array of scenario objects.
Each scenario object will have the following schema:
    &#34;title&#34;: &lt;string&gt;,       //Must be a complete sentence written in the past tense
    &#34;summary&#34;: &lt;string&gt;,   //Scenario description
    &#34;plausibility&#34;: &lt;string&gt;,  //Plausibility of scenario
    &#34;horizon&#34;: &lt;string&gt;
</pre>

<p>We were equally surprised by the fact that it could support fairly complex
        nested JSON schemas, even when we described the response schemas in pseudo-code.
        Here’s an example of how we might describe a nested response for strategy
        generation:</p>

<pre>You will respond in JSON format containing two keys, &#34;questions&#34; and &#34;strategies&#34;, with the respective schemas below:
    &#34;questions&#34;: [&lt;list of question objects, with each containing the following keys:&gt;]
      &#34;question&#34;: &lt;string&gt;,           
      &#34;answer&#34;: &lt;string&gt;             
    &#34;strategies&#34;: [&lt;list of strategy objects, with each containing the following keys:&gt;]
      &#34;title&#34;: &lt;string&gt;,               
      &#34;summary&#34;: &lt;string&gt;,             
      &#34;problem_diagnosis&#34;: &lt;string&gt;, 
      &#34;winning_aspiration&#34;: &lt;string&gt;,   
      &#34;where_to_play&#34;: &lt;string&gt;,        
      &#34;how_to_win&#34;: &lt;string&gt;,           
      &#34;assumptions&#34;: &lt;string&gt;          
</pre>

<p>An interesting side effect of describing the JSON response schema was that we
        could also nudge the LLM to provide more relevant responses in the output. For
        example, for the Creative Matrix, we want the LLM to think about many different
        dimensions (the prompt, the row, the columns, and each idea that responds to the
        prompt at the intersection of each row and column):</p>

<div id="template-matrix.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/template-matrix.png" width="960"/></p>
</div>



<p>By providing a few-shot prompt that includes a specific example of the output
        schema, we were able to get the LLM to “think” in the right context for each
        idea (the context being the prompt, row and column):</p>

<pre>You will respond with a valid JSON array, by row by column by idea. For example:

If Rows = &#34;row 0, row 1&#34; and Columns = &#34;column 0, column 1&#34; then you will respond
with the following:

[
  {{
    &#34;row&#34;: &#34;row 0&#34;,
    &#34;columns&#34;: [
      {{
        &#34;column&#34;: &#34;column 0&#34;,
        &#34;ideas&#34;: [
          {{
            &#34;title&#34;: &#34;Idea 0 title for prompt and row 0 and column 0&#34;,
            &#34;description&#34;: &#34;idea 0 for prompt and row 0 and column 0&#34;
          }}
        ]
      }},
      {{
        &#34;column&#34;: &#34;column 1&#34;,
        &#34;ideas&#34;: [
          {{
            &#34;title&#34;: &#34;Idea 0 title for prompt and row 0 and column 1&#34;,
            &#34;description&#34;: &#34;idea 0 for prompt and row 0 and column 1&#34;
          }}
        ]
      }},
    ]
  }},
  {{
    &#34;row&#34;: &#34;row 1&#34;,
    &#34;columns&#34;: [
      {{
        &#34;column&#34;: &#34;column 0&#34;,
        &#34;ideas&#34;: [
          {{
            &#34;title&#34;: &#34;Idea 0 title for prompt and row 1 and column 0&#34;,
            &#34;description&#34;: &#34;idea 0 for prompt and row 1 and column 0&#34;
          }}
        ]
      }},
      {{
        &#34;column&#34;: &#34;column 1&#34;,
        &#34;ideas&#34;: [
          {{
            &#34;title&#34;: &#34;Idea 0 title for prompt and row 1 and column 1&#34;,
            &#34;description&#34;: &#34;idea 0 for prompt and row 1 and column 1&#34;
          }}
        ]
      }}
    ]
  }}
]
</pre>

<p>We could have alternatively described the schema more succinctly and
        generally, but by being more elaborate and specific in our example, we
        successfully nudged the quality of the LLM’s response in the direction we
        wanted. We believe this is because LLMs “think” in tokens, and outputting (ie
        repeating) the row and column values before outputting the ideas provides more
        accurate context for the ideas being generated.</p>

<p>At the time of this writing, OpenAI has released a new feature called
        <a href="https://openai.com/blog/function-calling-and-other-api-updates">Function
        Calling</a>, which
        provides a different way to achieve the goal of formatting responses. In this
        approach, a developer can describe callable function signatures and their
        respective schemas as JSON, and have the LLM return a function call with the
        respective parameters provided in JSON that conforms to that schema. This is
        particularly useful in scenarios when you want to invoke external tools, such as
        performing a web search or calling an API in response to a prompt. Langchain
        also provides similar functionality, but I imagine they will soon provide native
        integration between their external tools API and the OpenAI function calling
        API.</p>
</section>

<section id="rt-progress">
<h3>Real-Time Progress</h3>

<p>Stream the response to the UI so users can monitor progress</p>

<p>One of the first few things you’ll realize when implementing a graphical
        user interface on top of an LLM is that waiting for the entire response to
        complete takes too long. We don’t notice this as much with ChatGPT because
        it streams the response character by character. This is an important user
        interaction pattern to keep in mind because, in our experience, a user can
        only wait on a spinner for so long before losing patience. In our case, we
        didn’t want the user to wait more than a few seconds before they started
        seeing a response, even if it was a partial one.</p>

<p>Hence, when implementing a co-pilot experience, we highly recommend
        showing real-time progress during the execution of prompts that take more
        than a few seconds to complete. In our case, this meant streaming the
        generations across the full stack, from the LLM back to the UI in real-time.
        Fortunately, the Langchain and OpenAI APIs provide the ability to do just
        that:</p>

<pre>const chat = new ChatOpenAI({
  temperature: 1,
  modelName: &#39;gpt-3.5-turbo&#39;,
  streaming: true,
  callbackManager: onTokenStream ?
    CallbackManager.fromHandlers({
      async handleLLMNewToken(token) {
        onTokenStream(token)
      },
    }) : undefined
});
</pre>

<p>This allowed us to provide the real-time progress needed to create a smoother
        experience for the user, including the ability to stop a generation
        mid-completion if the ideas being generated did not match the user’s
        expectations:</p>

<div id="realtime-progress.gif"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/realtime-progress.gif" width="600"/></p>
</div>



<p>However, doing so adds a lot of additional complexity to your application
        logic, especially on the view and controller. In the case of Boba, we also had
        to perform best-effort parsing of JSON and maintain temporal state during the
        execution of an LLM call. At the time of writing this, some new and promising
        libraries are coming out that make this easier for web developers. For example,
        the <a href="https://github.com/vercel-labs/ai">Vercel AI SDK</a> is a library for building
        edge-ready AI-powered streaming text and chat UIs.</p>
</section>

<section id="carry-context">
<h3>Select and Carry Context</h3>

<p>Capture and add relevant context information to subsequent action</p>

<p>One of the biggest limitations of a chat interface is that a user is
        limited to a single-threaded context: the conversation chat window. When
        designing a co-pilot experience, we recommend thinking deeply about how to
        design UX affordances for performing actions within the context of a
        selection, similar to our natural inclination to point at something in real
        life in the context of an action or description.</p>

<p><a href="#carry-context">Select and Carry Context</a> allows the user to narrow or broaden the scope of
        interaction to perform subsequent tasks - also known as the task context. This is typically
        done by selecting one or more elements in the user interface and then performing an action on them.
        In the case of Boba, for example, we use this pattern to allow the user to have
        a narrower, focused conversation about an idea by selecting it (eg a scenario, strategy or
        prototype concept), as well as to select and generate variations of a
        concept. First, the user selects an idea (either explicitly with a checkbox or implicitly by clicking a link):</p>

<div id="select-carry-select.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/select-carry-select.png" width="600"/></p>
</div>



<p>Then, when the user performs an action on the selection, the selected item(s) are carried over as context into the new task,
        for example as scenario subprompts for strategy generation when the user clicks &#34;Brainstorm strategies and questions for this scenario&#34;, 
        or as context for a natural language conversation when the user clicks Explore:</p>

<div id="select-carry-carry.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/select-carry-carry.png" width="600"/></p>
</div>



<p>Depending on the nature and length of the context
        you wish to establish for a segment of conversation/interaction, implementing
        <a href="#carry-context">Select and Carry Context</a> can be anywhere from very easy to very difficult. When
        the context is brief and can fit into a single LLM context window (the maximum
        size of a prompt that the LLM supports), we can implement it through prompt
        engineering alone. For example, in Boba, as shown above, you can click “Explore”
        on an idea and have a conversation with Boba about that idea. The way we
        implement this in the backend is to create a multi-message chat
        conversation:</p>

<pre>const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  HumanMessagePromptTemplate.fromTemplate(contextPrompt),
  HumanMessagePromptTemplate.fromTemplate(&#34;{input}&#34;),
]);
const formattedPrompt = await chatPrompt.formatPromptValue({
  input: input
})
</pre>

<p>Another technique of implementing <a href="#carry-context">Select and Carry Context</a> is to do so within
        the prompt by providing the context within tag delimiters, as shown below. In
        this case, the user has selected multiple scenarios and wants to generate
        strategies for those scenarios (a technique often used in scenario building and
        stress testing of ideas). The context we want to carry into the strategy
        generation is collection of selected scenarios:</p>

<pre>Your questions and strategies must be specific to realizing the following
potential future scenarios (if any)
  &lt;scenarios&gt;
    {scenarios_subprompt}
  &lt;/scenarios&gt;
</pre>

<p>However, when your context outgrows an LLM’s context window, or if you need
        to provide a more sophisticated chain of past interactions, you may have to
        resort to using external short-term memory, which typically involves using a
        vector store (in-memory or external). We’ll give an example of how to do
        something similar in <a href="#embed-external">Embedded External Knowledge</a>.</p>

<p>If you want to learn more about the effective use of selection and
        context in generative applications, we highly recommend a talk given by
        Linus Lee, of Notion, at the LLMs in Production conference: <a href="https://www.youtube.com/watch?v=rd-J3hmycQs">“Generative Experiences Beyond Chat”</a>.</p>
</section>

<section id="conversation">
<h3>Contextual Conversation</h3>

<p>Allow direct conversation with the LLM within a context.</p>

<p>This is a special case of <a href="#carry-context">Select and Carry Context</a>.
        While we wanted Boba to break out of the chat window interaction model
        as much as possible, we found that it is still very useful to provide the
        user a “fallback” channel to converse directly with the LLM. This allows us
        to provide a conversational experience for interactions we don’t support in
        the UI, and support cases when having a textual natural language
        conversation does make the most sense for the user.</p>

<p>In the example below, the user is chatting with Boba about a concept for
        personalized highlight reels provided by Rogers Sportsnet. The complete
        context is mentioned as a chat message (“In this concept, Discover a world of
        sports you love...&#34;), and the user has asked Boba to create a user journey for 
        the concept. The response from the LLM is formatted and rendered as Markdown:</p>

<div id="conversation-example.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/conversation-example.png" width="600"/></p>
</div>



<p>When designing generative co-pilot experiences, we highly recommend
        supporting contextual conversations with your application. Make sure to
        offer examples of useful messages the user can send to your application so
        they know what kind of conversations they can engage in. In the case of
        Boba, as shown in the screenshot above, those examples are offered as
        message templates under the input box, such as “Can you be more
        specific?”</p>
</section>

<section id="loud">
<h3>Out-Loud Thinking</h3>

<p>Tell LLM to generate intermediate results while answering</p>

<p>While LLMs don’t actually “think”, it’s worth thinking metaphorically
        about a phrase by Andrei Karpathy of OpenAI: <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">“LLMs ‘think’ in
        tokens.”</a> What he means by this
        is that GPTs tend to make more reasoning errors when trying to answer a
        question right away, versus when you give them more time (i.e. more tokens)
        to “think”. In building Boba, we found that using Chain of Thought (CoT)
        prompting, or more specifically, asking for a chain of reasoning before an
        answer, helped the LLM to reason its way toward higher-quality and more
        relevant responses.</p>

<p>In some parts of Boba, like strategy and concept generation, we ask the
        LLM to generate a set of questions that expand on the user’s input prompt
        before generating the ideas (strategies and concepts in this case).</p>

<div id="outloud-questions.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/outloud-questions.png" width="450"/></p>
</div>



<p>While we display the questions generated by the LLM, an equally effective
        variant of this pattern is to implement an internal monologue that the user is
        not exposed to. In this case, we would ask the LLM to think through their
        response and put that inner monologue into a separate part of the response, that
        we can parse out and ignore in the results we show to the user. A more elaborate
        description of this pattern can be found in OpenAI’s <a href="https://platform.openai.com/docs/guides/gpt-best-practices">GPT Best Practices
        Guide</a>, in the
        section <a href="https://platform.openai.com/docs/guides/gpt-best-practices/strategy-give-gpts-time-to-think">Give GPTs time to
        “think”</a></p>

<p>As a user experience pattern for generative applications, we found it helpful
        to share the reasoning process with the user, wherever appropriate, so that the
        user has additional context to iterate on the next action or prompt. For
        example, in Boba, knowing the kinds of questions that Boba thought of gives the
        user more ideas about divergent areas to explore, or not to explore. It also
        allows the user to ask Boba to exclude certain classes of ideas in the next
        iteration. If you do go down this path, we recommend creating a UI affordance
        for hiding a monologue or chain of thought, such as Boba’s feature to toggle
        examples shown above.</p>
</section>

<section id="iterate-response">
<h3>Iterative Response</h3>

<p>Provide affordances for the user to have a back-and-forth
        interaction with the co-pilot</p>

<p>LLMs are bound to either misunderstand the user’s intent or simply
        generate responses that don’t meet the user’s expectations. Hence, so is
        your generative application. One of the most powerful capabilities that
        distinguishes ChatGPT from traditional chatbots is the ability to flexibly
        iterate on and refine the direction of the conversation, and hence improve
        the quality and relevance of the responses generated.</p>

<p>Similarly, we believe that the quality of a generative co-pilot
        experience depends on the ability of a user to have a fluid back-and-forth
        interaction with the co-pilot. This is what we call the Iterate on Response
        pattern. This can involve several approaches:</p>

<ul>
<li>Correcting the original input provided to the application/LLM</li>

<li>Refining a part of the co-pilot’s response to the user</li>

<li>Providing feedback to nudge the application in a different direction</li>
</ul>

<p>One example of where we’ve implemented <a href="#iterate-response">Iterative Response</a>
        in
        Boba is in Storyboarding. Given a prompt (either brief or elaborate), Boba
        can generate a visual storyboard, which includes multiple scenes, with each
        scene having a narrative script and an image generated with Stable
        Diffusion. For example, below is a partial storyboard describing the experience of a
        “Hotel of the Future”:</p>

<div id="iterate-hotel.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/iterate-hotel.png" width="800"/></p>
</div>



<p>Since Boba uses the LLM to generate the Stable Diffusion prompt, we don’t
        know how good the images will turn out–so it’s a bit of a hit or miss with
        this feature. To compensate for this, we decided to provide the user the
        ability to iterate on the image prompt so that they can refine the image for
        a given scene. The user would do this by simply clicking on the image,
        updating the Stable Diffusion prompt, and pressing Done, upon which Boba
        would generate a new image with the updated prompt, while preserving the
        rest of the storyboard:</p>

<div id="iterate-sdprompt.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/iterate-sdprompt.png" width="400"/></p>
</div>



<p>Another example <a href="#iterate-response">Iterative Response</a> that we
        are currently working on is a feature for the user to provide feedback
        to Boba on the quality of ideas generated, which would be a combination
        of <a href="#carry-context">Select and Carry Context</a> and <a href="#iterate-response">Iterative Response</a>. One
        approach would be to give a thumbs up or thumbs down on an idea, and
        letting Boba incorporate that feedback into a new or next set of
        recommendations. Another approach would be to provide conversational
        feedback in the form of natural language. Either way, we would like to
        do this in a style that supports reinforcement learning (the ideas get
        better as you provide more feedback). A good example of this would be
        Github Copilot, which demotes code suggestions that have been ignored by
        the user in its ranking of next best code suggestions.</p>

<p>We believe that this is one of the most important, albeit
        generically-framed, patterns to implementing effective generative
        experiences. The challenging part is incorporating the context of the
        feedback into subsequent responses, which will often require implementing
        short-term or long-term memory in your application because of the limited
        size of context windows.</p>
</section>

<section id="embed-external">
<h3>Embedded External Knowledge</h3>

<p>Combine LLM with other information sources to access data beyond
        the LLM&#39;s training set</p>

<p>As alluded to earlier in this article, oftentimes your generative
        applications will need the LLM to incorporate external tools (such as an API
        call) or external memory (short-term or long-term). We ran into this
        scenario when we were implementing the Research feature in Boba, which
        allows users to answer qualitative research questions based on publicly
        available information on the web, for example “How is the hotel industry
        using generative AI today?”:</p>

<div id="embed-hotel.png"><p><img src="https://thevaluable.dev/find-cli-guide-examples/building-boba/embed-hotel.png" width="800"/></p>
</div>



<p>To implement this, we had to “equip” the LLM with Google as an external
        web search tool and give the LLM the ability to read potentially long
        articles that may not fit into the context window of a prompt. We also
        wanted Boba to be able to chat with the user about any relevant articles the
        user finds, which required implementing a form of short-term memory. Lastly,
        we wanted to provide the user with proper links and references that were
        used to answer the user’s research question.</p>

<p>The way we implemented this in Boba is as follows:</p>

<ol>
<li>Use a Google SERP API to perform the web search based on the user’s query
          and get the top 10 articles (search results)</li>

<li>Read the full content of each article using the Extract API</li>

<li>Save the content of each article in short-term memory, specifically an
          in-memory vector store. The embeddings for the vector store are generated using
          the OpenAI API, and based on chunks of each article (versus embedding the entire
          article itself).</li>

<li>Generate an embedding of the user’s search query</li>

<li>Query the vector store using the embedding of the search query</li>

<li>Prompt the LLM to answer the user’s original query in natural language,
          while prefixing the results of the vector store query as context into the LLM
          prompt.</li>
</ol>

<p>This may sound like a lot of steps, but this is where using a tool like
        Langchain can speed up your process. Specifically, Langchain has an
        end-to-end chain called VectorDBQAChain, and using that to perform the
        question-answering took only a few lines of code in Boba:</p>

<pre>const researchArticle = async (article, prompt) =&gt; {
  const model = new OpenAI({});
  const text = article.text;
  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
  const docs = await textSplitter.createDocuments([text]);
  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
  const chain = VectorDBQAChain.fromLLM(model, vectorStore);
  const res = await chain.call({
    input_documents: docs,
    query: prompt + &#34;. Be detailed in your response.&#34;,
  });
  return { research_answer: res.text };
};
</pre>

<p>The article text contains the entire content of the article, which may not
        fit within a single prompt. So we perform the steps described above. As you can
        see, we used an in-memory vector store called HNSWLib (Hierarchical Navigable
        Small World). HNSW graphs are among the top-performing indexes for vector
        similarity search. However, for larger scale use cases and/or long-term memory,
        we recommend using an external vector DB like Pinecone or Weaviate.</p>

<p>We also could have further streamlined our workflow by using Langchain’s
        external tools API to perform the Google search, but we decided against it
        because it offloaded too much decision making to Langchain, and we were getting
        mixed, slow and harder-to-parse results. Another approach to implementing
        external tools is to use Open AI’s recently released <a href="https://openai.com/blog/function-calling-and-other-api-updates">Function Calling
        API</a>, which we
        mentioned earlier in this article.</p>

<p>To summarize, we combined two distinct techniques to implement <a href="#embed-external">Embedded External Knowledge</a>:</p>

<ol>
<li>Use External Tool: Search and read articles using Google SERP and Extract
          APIs</li>

<li>Use External Memory: Short-term memory using an in-memory vector store
          (HNSWLib)</li>
</ol>
</section>
</section>

<section id="FuturePlansAndPatterns">
<h2>Future plans and patterns</h2>

<p>So far, we’ve only scratched the surface with the prototype of Boba and what a
      generative co-pilot for product strategy and generative ideation could entail. There is yet a lot 
      to learn and share about the art of building LLM-powered generative co-pilot applications, 
      and we hope to do so in the months to come. It’s an exciting time to work on this new
      class of applications and experiences, and we believe many of the principles,
      patterns and practices are yet to be discovered!</p>
</section>

<hr/>
</div></div>
  </body>
</html>
