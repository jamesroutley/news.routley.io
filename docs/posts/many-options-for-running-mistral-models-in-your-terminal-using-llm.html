<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://simonwillison.net/2023/Dec/18/mistral/">Original</a>
    <h1>Many options for running Mistral models in your terminal using LLM</h1>
    
    <div id="readability-page-1" class="page"><div>



<p>18th December 2023</p>

<p><a href="https://mistral.ai/">Mistral AI</a> is the most exciting AI research lab at the moment. They’ve now released two extremely powerful smaller Large Language Models under an Apache 2 license, and have a third much larger one that’s available via their API.</p>
<p>I’ve been trying out their models using my <a href="https://llm.datasette.io/">LLM command-line tool tool</a>. Here’s what I’ve figured out so far.</p>

<ul>
  <li><a href="https://simonwillison.net/2023/Dec/18/mistral/#mixtral-llama-cpp">Mixtral 8x7B via llama.cpp and llm-llama-cpp</a></li>
  <li><a href="https://simonwillison.net/2023/Dec/18/mistral/#mistral-7b-local">Mistral 7B via llm-llama-cpp or llm-gpt4all or llm-mlc</a></li>
  <li><a href="https://simonwillison.net/2023/Dec/18/mistral/#mistral-api">Using the Mistral API, which includes the new Mistral-medium</a></li>
  <li><a href="https://simonwillison.net/2023/Dec/18/mistral/#mistral-other-apis">Mistral via other API providers</a></li>
  <li><a href="https://simonwillison.net/2023/Dec/18/mistral/#llamafile-openai">Using Llamafile’s OpenAI API endpoint</a></li>
</ul>

<h4 id="mixtral-llama-cpp">Mixtral 8x7B via llama.cpp and llm-llama-cpp</h4>
<p>On Friday 8th December Mistral AI <a href="https://twitter.com/MistralAI/status/1733150512395038967">tweeted a mysterious magnet</a> (BitTorrent) link. This is the second time they’ve done this, the first was on September 26th when <a href="https://twitter.com/MistralAI/status/1706877320844509405">they released</a> their excellent Mistral 7B model, also as a magnet link.</p>
<p>The new release was an 87GB file containing Mixtral 8x7B—“a high-quality sparse mixture of experts model (SMoE) with open weights”, according to <a href="https://mistral.ai/news/mixtral-of-experts/">the article</a> they released three days later.</p>
<p>Mixtral is a <em>very</em> impressive model. GPT-4 has long been rumored to use a mixture of experts architecture, and Mixtral is the first truly convincing openly licensed implementation of this architecture I’ve seen. It’s already showing impressive benchmark scores.</p>
<p>This <a href="https://github.com/ggerganov/llama.cpp/pull/4406">PR for llama.cpp</a> added support for the new model. <a href="https://github.com/abetlen/llama-cpp-python">llama-cpp-python</a> updated to land that patch shortly afterwards.</p>
<p>Which means... you can now run Mixtral on a Mac (and other platforms too, though I haven’t tested them myself yet) using my <a href="https://github.com/simonw/llm-llama-cpp">llm-llama-cpp plugin</a>.</p>
<p>Here’s how to do that:</p>
<ol>
<li>
<a href="https://llm.datasette.io/en/stable/setup.html">Install LLM</a>:

</li>
<li>Install the plugin:
<div><pre>llm install llm-llama-cpp</pre></div>
</li>
<li>Install <code>llama-cpp-python</code>—this needs to be done manually because the best approach differs for different platforms. On an Apple Silicon Mac I recommend running:
<div><pre>CMAKE_ARGS=<span><span>&#34;</span>-DLLAMA_METAL=on<span>&#34;</span></span> FORCE_CMAKE=1 llm install llama-cpp-python</pre></div>
More details <a href="https://github.com/simonw/llm-llama-cpp">in the llm-llama-cpp README</a>
</li>
<li>Download a GGUF file for Mixtral 8X7B Instruct v0.1. TheBloke has <a href="https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/tree/main">8 different options</a> for this—I picked the 36GB <code>mixtral-8x7b-instruct-v0.1.Q6_K.gguf</code>:
<div><pre>curl -LO <span><span>&#39;</span>https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q6_K.gguf?download=true<span>&#39;</span></span></pre></div>
</li>
<li>Run the model like this:
<div><pre>llm -m gguf \
  -o path mixtral-8x7b-instruct-v0.1.Q6_K.gguf \
  <span><span>&#39;</span>[INST] Write a Python function that downloads a file from a URL[/INST]<span>&#39;</span></span></pre></div>
</li>
</ol>
<p>The <code>-m gguf</code> here says to use the <code>gguf</code> model that was installed by the <code>llm-llama-cpp</code> plugin—this is a model which requires you to pass a <code>path</code> option—with <code>-o path filename</code>—specifying which model you would like to run.</p>
<p>The <code>[INST]</code> part there is the format of prompt expected by the Instruct model. I need to update the <code>llm-llama-cpp</code> plugin to make it easier to automatically construct the correct prompt format, see <a href="https://github.com/simonw/llm-llama-cpp/issues/28">this issue</a>.</p>
<p>Here’s what I got for that prompt:</p>
<blockquote>
<p>Sure, here’s an example Python function that downloads a file from a URL using the <code>requests</code> library:</p>
<pre><span>import</span> <span>requests</span>

<span>def</span> <span>download_file</span>(<span>url</span>, <span>save_path</span>):
    <span>&#34;&#34;&#34;</span>
<span>    Download a file from a URL and save it to a local path.</span>
<span>
<span>    :param url: The URL of the file to download.</span>
<span>    :param save_path: The local path to save the file.</span>
<span>    &#34;&#34;&#34;</span>
    <span>response</span> <span>=</span> <span>requests</span>.<span>get</span>(<span>url</span>, <span>stream</span><span>=</span><span>True</span>)
    <span>if</span> <span>response</span>.<span>status_code</span> <span>==</span> <span>200</span>:
        <span>with</span> <span>open</span>(<span>save_path</span>, <span>&#39;wb&#39;</span>) <span>as</span> <span>f</span>:
            <span>for</span> <span>chunk</span> <span>in</span> <span>response</span>.<span>iter_content</span>(<span>1024</span>):
                <span>f</span>.<span>write</span>(<span>chunk</span>)
        <span>print</span>(<span>f&#34;File downloaded successfully at <span><span>{</span><span>save_path</span><span>}</span></span>&#34;</span>)
    <span>else</span>:
        <span>print</span>(<span>f&#34;Failed to download file from <span><span>{</span><span>url</span><span>}</span></span>. Status code: <span><span>{</span><span>response</span>.<span>status_code</span><span>}</span></span>&#34;</span>)</span></pre>
<p>To use this function, simply call it with the URL of the file you want to download and the local path where you want to save it. For example:</p>
<pre><span>download_file</span>(<span>&#39;https://example.com/myfile.txt&#39;</span>, <span>&#39;/path/to/save/myfile.txt&#39;</span>)</pre>
<p>Note that this function uses the <code>stream=True</code> parameter in the <code>requests.get()</code> method to download the file in chunks, which can help prevent memory issues when downloading large files. It also checks the HTTP status code of the response before saving the file, so you can handle any errors that might occur during the download.</p>
</blockquote>
<p>That’s a very solid reply!</p>
<h4 id="mistral-7b-local">Mistral 7B via llm-llama-cpp or llm-gpt4all or llm-mlc</h4>
<p>The smaller Mistral 7B model dropped back in September. It’s since established itself as the most capable model family of that size—a size which is very convenient for running on personal devices.</p>
<p>I’m even running Mistral 7B on my iPhone now, thanks to an update to the <a href="https://apps.apple.com/us/app/mlc-chat/id6448482937">MLC Chat iOS app</a> from a few days ago.</p>
<p>There are a bunch of different options for running this model and its variants locally using LLM on a Mac—and probably other platforms too, though I’ve not tested these options myself on Linux or Windows:</p>
<ul>
<li>Using <a href="https://github.com/simonw/llm-llama-cpp">llm-llama-cpp</a>: download one of <a href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF">these Mistral-7B-Instruct GGUF files</a> for the chat-tuned version, or <a href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/tree/main">one of these</a> for base Mistral, then follow the steps listed above</li>
<li>Using <a href="https://github.com/simonw/llm-gpt4all">llm-gpt4all</a>. This is the easiest plugin to install:

The model will be downloaded the first time you try to use it:
<div><pre>llm -m mistral-7b-instruct-v0 <span><span>&#39;</span>Introduce yourself<span>&#39;</span></span></pre></div>
</li>
<li>Using <a href="https://github.com/simonw/llm-mlc">llm-mlc</a>. Follow the instructions in the README to install it, then:
<div><pre><span><span>#</span> Download the model:</span>
llm mlc download-model https://huggingface.co/mlc-ai/mlc-chat-Mistral-7B-Instruct-v0.2-q3f16_1
<span><span>#</span> Run it like this:</span>
llm -m mlc-chat-Mistral-7B-Instruct-v0.2-q3f16_1 <span><span>&#39;</span>Introduce yourself<span>&#39;</span></span></pre></div>
</li>
</ul>
<p>Each of these options work, but I’ve not spent time yet comparing them in terms of output quality or performance.</p>
<h4 id="mistral-api">Using the Mistral API, which includes the new Mistral-medium</h4>
<p>Mistral also recently announced <a href="https://mistral.ai/news/la-plateforme/">La plateforme</a>, their early access API for calling hosted versions of their models.</p>
<p>Their new API renames Mistral 7B model “Mistral-tiny”, the new Mixtral model “Mistral-small”... and offers something called <strong>Mistral-medium</strong> as well:</p>
<blockquote>
<p>Our highest-quality endpoint currently serves a prototype model, that is currently among the top serviced models available based on standard benchmarks. It masters English/French/Italian/German/Spanish and code and obtains a score of 8.6 on MT-Bench.</p>
</blockquote>
<p>I got access to their API and used it to build a new plugin, <a href="https://github.com/simonw/llm-mistral">llm-mistral</a>. Here’s how to use that:</p>
<ol>
<li>Install it:

</li>
<li>Set your Mistral API key:
<div><pre>llm keys <span>set</span> mistral
<span><span>#</span> &lt;paste key here&gt;</span></pre></div>
</li>
<li>Run the models like this:
<div><pre>llm -m mistral-tiny <span><span>&#39;</span>Say hi<span>&#39;</span></span>
<span><span>#</span> Or mistral-small or mistral-medium</span>
cat mycode.py <span>|</span> llm -m mistral-medium -s <span><span>&#39;</span>Explain this code<span>&#39;</span></span></pre></div>
</li>
</ol>
<p>Here’s their comparison table pitching Mistral Small and Medium against GPT-3.5:</p>
<p><img src="https://static.simonwillison.net/static/2023/mistral-table.jpg" alt="MMLU (MCQ in 57 subjects): GPT - 3.5 scored 70%, Mistral Small scored 70.6%, Mistral Medium scored 75.3%. HellaSwag (10-shot): GPT - 3.5 scored 85.5%, Mistral Small scored 86.7%, Mistral Medium scored 88%. ARC Challenge (25-shot): GPT - 3.5 scored 85.2%, Mistral Small scored 85.8%, Mistral Medium scored 89.9%. WinoGrande (5-shot): GPT - 3.5 scored 81.6%, Mistral Small scored 81.2%, Mistral Medium scored 88%. MBPP (pass@1): GPT - 3.5 scored 52.2%, Mistral Small scored 60.7%, Mistral Medium scored 62.3%. GSM-8K (5-shot): GPT - 3.5 scored 57.1%, Mistral Small scored 58.4%, Mistral Medium scored 66.7%. MT Bench (for Instruct models): GPT - 3.5 scored 8.32, Mistral Small scored 8.30, Mistral Medium scored 8.61."/></p>
<p>These may well be cherry-picked, but note that Small beats GPT-3.5 on almost every metric, and Medium beats it on everything by a wider margin.</p>
<p>Here’s the <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">MT Bench leaderboard</a> which includes scores for GPT-4 and Claude 2.1:</p>
<p><img src="https://static.simonwillison.net/static/2023/mt-bench.jpg" alt="GPT-4-Turbo: Arena Elo rating 1217, MT-bench score 9.32. GPT-4-0613: Arena Elo rating 1152, MT-bench score 9.18. GPT-4-0314: Arena Elo rating 1201, MT-bench score 8.96. GPT-3.5-turbo-0613: Arena Elo rating 1112, MT-bench score 8.39. GPT-3.5-Turbo-1106: Arena Elo rating 1074, MT-bench score 8.32. Claude-2.1: Arena Elo rating 1118, MT-bench score 8.18."/></p>
<p>That 8.61 score for Medium puts it half way between GPT-3.5 and GPT-4.</p>
<p>Benchmark scores are no replacement for spending time with a model to get a feel for how well it behaves across a wide spectrum of tasks, but these scores are extremely promising. GPT-4 may not hold the best model crown for much longer.</p>
<h4 id="mistral-other-apis">Mistral via other API providers</h4>
<p>Since both Mistral 7B and Mixtral 8x7B are available under an Apache 2 license, there’s been something of a race to the bottom in terms of pricing from other LLM hosting providers.</p>
<p>This trend makes me a little nervous, since it actively disincentivizes future open model releases from Mistral and from other providers who are hoping to offer their own hosted versions.</p>
<p>LLM has plugins for a bunch of these providers already. The three that I’ve tried so far are Replicate, Anyscale Endpoints and OpenRouter.</p>
<p>For <a href="https://replicate.com/">Replicate</a>:</p>
<div><pre>llm install llm-replicate
llm keys <span>set</span> replicate
<span><span>#</span> &lt;paste API key here&gt;</span>
llm replicate add mistralai/mistral-7b-v0.1</pre></div>
<p>Then run prompts like this:</p>
<div><pre>llm -m replicate-mistralai-mistral-7b-v0.1 <span><span>&#39;</span>3 reasons to get a pet weasel:<span>&#39;</span></span></pre></div>
<p>This example is the non-instruct tuned model, so the prompt needs to be shaped such that the model can complete it.</p>
<p>For <a href="https://www.anyscale.com/endpoints">Anyscale Endpoints</a>:</p>
<div><pre>llm install llm-anyscale-endpoints
llm keys <span>set</span> anyscale-endpoints
<span><span>#</span> &lt;paste API key here&gt;</span></pre></div>
<p>Now you can run both the 7B and the Mixtral 8x7B models:</p>
<div><pre>llm -m mistralai/Mixtral-8x7B-Instruct-v0.1 \
  <span><span>&#39;</span>3 reasons to get a pet weasel<span>&#39;</span></span>
llm -m mistralai/Mistral-7B-Instruct-v0.1 \
  <span><span>&#39;</span>3 reasons to get a pet weasel<span>&#39;</span></span></pre></div>
<p>And for <a href="https://openrouter.ai/">OpenRouter</a>:</p>
<div><pre>llm install llm-openrouter
llm keys <span>set</span> openrouter
<span><span>#</span> &lt;paste API key here&gt;</span></pre></div>
<p>Then run the models like so:</p>
<div><pre>llm -m openrouter/mistralai/mistral-7b-instruct \
  <span><span>&#39;</span>2 reasons to get a pet dragon<span>&#39;</span></span>
llm -m openrouter/mistralai/mixtral-8x7b-instruct \
  <span><span>&#39;</span>2 reasons to get a pet dragon<span>&#39;</span></span></pre></div>
<p>OpenRouter are currently offering Mistral and Mixtral via their API for $0.00/1M input tokens—it’s free! Obviously not sustainable, so don’t rely on that continuing, but that does make them a great platform for running some initial experiments with these models.</p>
<h4 id="llamafile-openai">Using Llamafile’s OpenAI API endpoint</h4>
<p>I <a href="https://simonwillison.net/2023/Nov/29/llamafile/">wrote about Llamafile</a> recently, a fascinating option fur running LLMs where the LLM can be bundled up in an executable that includes everything needed to run it, on multiple platforms.</p>
<p>Justine Tunney released <a href="https://huggingface.co/jartine/Mixtral-8x7B-v0.1.llamafile/tree/main">llamafiles for Mixtral</a> a few days ago.</p>
<p>The <a href="https://huggingface.co/jartine/Mixtral-8x7B-v0.1.llamafile/blob/main/mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile">mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile</a> one runs an OpenAI-compatible API endpoints which LLM can talk to.</p>
<p>Here’s how to use that:</p>
<ol>
<li>Download the llamafile:
<div><pre>curl -LO https://huggingface.co/jartine/Mixtral-8x7B-v0.1.llamafile/resolve/main/mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile</pre></div>
</li>
<li>Start that running:
<div><pre>./mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile</pre></div>
You may need to <code>chmod 755 mixtral-8x7b-instruct-v0.1.Q5_K_M-server.llamafile</code> it first, but I found I didn’t need to.</li>
<li>Configure LLM to know about that endpoint, by adding the following to a file at <code>~/Library/Application Support/io.datasette.llm/extra-openai-models.yaml</code>:
<div><pre>- <span>model_id</span>: <span>llamafile</span>
  <span>model_name</span>: <span>llamafile</span>
  <span>api_base</span>: <span><span>&#34;</span>http://127.0.0.1:8080/v1<span>&#34;</span></span></pre></div>
This registers a model called <code>llamafile</code> which you can now call like this:
<div><pre>llm -m llamafile <span><span>&#39;</span>Say hello to the world<span>&#39;</span></span></pre></div>
</li>
</ol>
<p>Setting up that <code>llamafile</code> alias means you’ll be able to use the same CLI invocation for any llamafile models you run on that default 8080 port.</p>
<p>The same exact approach should work for other model hosting options that provide an endpoint that imitates the OpenAI API.</p>
<h4>This is LLM plugins working as intended</h4>
<p>When I <a href="https://simonwillison.net/2023/Jul/12/llm/">added plugin support to LLM</a> this was exactly what I had in mind: I want it to be as easy as possible to add support for new models, both local and remotely hosted.</p>
<p>The <a href="https://llm.datasette.io/en/stable/plugins/directory.html">LLM plugin directory</a> lists 19 plugins in total now.</p>
<p>If you want to build your own plugin—for a locally hosted model or for one exposed via a remote API—the <a href="https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html">plugin author tutorial</a> (plus reviewing code from the existing plugins) should hopefully provide everything you need.</p>
<p>You’re also welcome to join us in the <a href="https://datasette.io/discord-llm">#llm Discord channel</a> to talk about your plans for your project.</p>




</div></div>
  </body>
</html>
