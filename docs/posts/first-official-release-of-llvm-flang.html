<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.llvm.org/posts/2025-03-11-flang-new/">Original</a>
    <h1>First official release of LLVM Flang</h1>
    
    <div id="readability-page-1" class="page"><article><p>LLVM has included a Fortran compiler “Flang” since LLVM 11 in late 2020. However,
until recently the Flang binary was not <code>flang</code> (like <code>clang</code>) but instead
<code>flang-new</code>.</p><p>LLVM 20 ends the era of <code>flang-new</code>. The community has decided that Flang is
worthy of a new name.</p><p>The “new” name? You guessed it, <code>flang</code>.</p><p>A simple change that represents a major milestone for Flang.</p><p>This article will cover the almost 10 year journey of Flang. The first
concepts, multiple rewrites, the adoption of LLVM’s Multi Level Intermediate
Representation (MLIR) and Flang entering the LLVM Project.</p><p>If you want to try <code>flang</code> right now, you can
<a href="https://github.com/llvm/llvm-project/releases/tag/llvmorg-20.1.0">download</a>
it or try it in your browser using
<a href="https://godbolt.org/z/3hhYM37Kh">Compiler Explorer</a>.</p><p>Fortran was first created in the 1950s, and the name came from “Formula Translation”.
Fortran focused on the mathematics use case and freed programmers from writing
assembly code that could only run on specific machines.</p><p>Instead they could write code that looked like a formula. You expect this today
but for the time it was a revolution. This feature led to heavy use in scientific
computing: weather modelling, fluid dynamics and computational chemistry, just
to name a few.</p><blockquote><p>Whilst many alternative programming languages have come
and gone, it [Fortran] has regained its popularity for writing high
performance codes. Indeed, over 80% of the applications
running on ARCHER2, a 750,000 core Cray EX which is
the UK national supercomputer, are written in Fortran.</p></blockquote><ul><li><a href="https://arxiv.org/pdf/2308.13274">Fortran High-Level Synthesis: Reducing the barriers
to accelerating High Performance Computing (HPC) codes on FPGAs</a> (Gabriel Rodriguez-Canal et al., 2023)</li></ul><p>Fortran has had a <a href="https://ondrejcertik.com/blog/2021/03/resurrecting-fortran/">resurgence</a>
in recent years, gaining a <a href="https://fpm.fortran-lang.org/">package manager</a>, an unofficial
<a href="https://github.com/fortran-lang/stdlib">standard library</a> and <a href="https://lfortran.org/">LFortran</a>,
a compiler that supports interactive programming (LFortran also uses LLVM).</p><p>For the full history of Fortran, IBM has an excellent <a href="https://www.ibm.com/history/fortran">article</a>
on the topic and I encourage you to look at the
<a href="https://archive.computerhistory.org/resources/text/Fortran/102653987.05.01.acc.pdf">“Programmer’s Primer for Fortran”</a>
if you want to see the early form of Fortran.</p><p>If you want to learn the language, <a href="https://fortran-lang.org/">fortran-lang.org</a>
is a great place to start.</p><p>There are many Fortran compilers. Some are vendor specific such as the
<a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/fortran-compiler.html">Intel Fortran Compiler</a>
or NVIDIA’s <a href="https://developer.nvidia.com/hpc-compilers">HPC compilers</a>. Then
there are open source options like <a href="https://gcc.gnu.org/fortran/">GFortran</a>, which
supports many platforms.</p><p>Why build one more?</p><p>The two partners in the early days of Flang were the US National Labs and NVIDIA.</p><p>For Pat McCormick (Flang project lead at Los Alamos National Laboratory) preserving
the utility of Fortran code was imperative:</p><blockquote><p>These [Fortran] codes represent an essential capability that supports many
elements of our [The United States’] scientific mission and will continue to do
so for the foreseeable future. A fundamental risk facing these codes is the
absence of a long-term, non-proprietary support path for Fortran.</p></blockquote><p>GFortran might seem to counter that statement, but remember that a single project
is a single point of failures, incompatibilities and disagreements. Having multiple
implementations reduces that risk.</p><p>NVIDIA’s Gary Klimowicz <a href="https://www.youtube.com/watch?v=Fy68k5hHgLk">laid out</a>
their goals for Flang in a presentation to FortranCon in 2020:</p><ul><li>Use a permissive license like that of <a href="https://llvm.org/LICENSE.txt">LLVM</a>,
which is more palatable to commercial users and contributors.</li><li>Develop an active community of Fortran compiler developers that includes
companies and institutions.</li><li>Support Fortran tool development by basing Flang on existing LLVM frameworks.</li><li>Support Fortran language experimentation for future language standards proposals.</li></ul><p>Intentions echoed by Pat McCormick:</p><blockquote><p>The overarching goal was to establish an open-source, modern implementation and
simultaneously grow a community that spanned industry, academia, and federal
agencies at both the national and international levels.</p></blockquote><p>Fortran as a language also benefits from having many implementations. For C++
language features, it is common to implement them on top of Clang and GCC, to
prove the feature is viable and get feedback.</p><p>Implementing the feature multiple times in different compilers uncovers
assumptions that may be a problem for certain compilers, or certain groups of
compiler users.</p><p>In the same way, Flang and GFortran can provide that diversity.</p><p>However, even when features are standardised, standards can be ambiguous and
implementations do make mistakes. A new compiler is a chance to uncover these.</p><p>Jeff Hammond (NVIDIA) is very familiar with this, having tested Flang with many
existing applications. They had this to say on the motivations for Flang
and how users have reacted to it:</p><blockquote><p>The Fortran language has changed quite a bit over the past 30 years. Modern Fortran
deserves a modern compiler ecosystem, that’s not only capable of compiling all
the old codes and all the code written for the current standard, but also supports
innovation in the future.</p><p>Because it’s a huge amount of work to build a feature-complete modern Fortran compiler,
it’s useful to leverage the resources of the entire LLVM community for this effort.
NVIDIA and ARM play leading roles right now, with important contributions from IBM,
Fujitsu and LBNL [Lawrence Berkeley National Laboratory], e.g. related to test
suites and coarrays. We hope to see the developer community grow in the future.</p><p>Another benefit from the LLVM Fortran compiler is that users are more likely to
invest in supporting a new compiler when it has full language support and runs on
all the platforms. A broad developer base is critical to support all the platforms.</p><p>What I have seen so far interacting with our Fortran users is that they are very
excited about LLVM Flang and were willing to commit to supporting it in their
build systems and CI systems, which has driven quality improvements in both the
Flang compiler and the applications.</p><p>Like Clang did with C and C++ codes when it started to become popular, Flang
is helping to identify bugs in Fortran code that weren’t noticed before, which
is making the Fortran software ecosystem better.</p></blockquote><p>The story of Flang really starts in 2015, but the Portland Group (PGI) collaborated
with US National Labs prior to this. PGI would later become part of NVIDIA and
be instrumental to the Flang project.</p><ul><li><strong>1989</strong> The <a href="https://web.archive.org/web/19970628161656/http://www.pgroup.com/corp_home.html">Portland Group</a>
is formed. To provide C, Fortran 77 and C++ compilers for the Intel i860 market.</li><li><strong>1990</strong> Intel bundles PGI compilers with its iPSC/860 supercomputer.</li><li><strong>1996</strong> PGI
<a href="https://web.archive.org/web/20100528051556/http://www.sandia.gov/ASCI/Red/papers/Mattson/OVERVIEW.html">works with</a>
Sandia National Laboratories to provide compilers for the Accelerated Strategic Computing Initiative (ASCI) Option Red
supercomputer.</li><li><strong>December 2000</strong> PGI becomes a
<a href="https://www.electronicsweekly.com/news/archived/resources-archived/stmicroelectronics-to-acquire-pgi-2000-12/">wholly owned subsidiary</a> of
STMicroElectronics.</li><li><strong>August 2011</strong> Away from PGI, Bill Wendling <a href="https://github.com/llvm-fortran/fort/commit/af352bf765ecf3e55da38c34cb480b269a157894">starts</a>
an LLVM based Fortran compiler called “Flang” (later known as <a href="https://github.com/llvm-fortran/fort">“Fort”</a>).
Bill is joined by several collaborators a few months later.</li><li><strong>July 2013</strong> PGI is <a href="https://www.theregister.com/2013/07/30/nvidia_buys_the_portland_group/">sold to NVIDIA</a>.</li></ul><p>In late 2015 there were the first signs of what would become “Classic Flang”. Though
at the time it was just “Flang”, I will use “Classic Flang” here for clarity.</p><p>Development of what was to become “Fort” continued under the “Flang” name,
completely separate from the Classic Flang project.</p><ul><li><p><strong>November 2015</strong> NVIDIA joins the US Department of Energy
Exascale Computing Project. Including a commitment to create an open source
<a href="https://www.llnl.gov/article/41756/nnsa-national-labs-team-nvidia-develop-open-source-fortran-compiler-technology">Fortran compiler</a>.</p><blockquote><p>“The U.S. Department of Energy’s National Nuclear Security Administration and its
three national labs [Los Alamos, Lawrence Livermore and Sandia] have reached an
agreement with NVIDIA’s PGI division to adapt and open-source PGI’s Fortran
frontend, and associated Fortran runtime library, for contribution to the LLVM project.”</p></blockquote><p>(this news is also the first appearance of Flang in an issue of
<a href="https://llvmweekly.org/issue/98">LLVM Weekly</a>)</p></li><li><p><strong>May 2017</strong> The first release of Classic Flang as a separate
repository, outside of the LLVM Project. Composed of a PGI compiler frontend
and a new backend that generates LLVM Intermediate Representation (LLVM IR).</p></li><li><p><strong>August 2017</strong> The Classic Flang project is <a href="https://llvmweekly.org/issue/191">announced officially</a>
(according to LLVM Weekly’s report, the original mailing list is offline).</p></li></ul><p>During this time, plans were formed to propose moving Classic Flang into the LLVM
Project.</p><ul><li><p><strong>December 2017</strong> The original “Flang” is renamed to
<a href="https://github.com/llvm-fortran/fort/commit/0585746476e3c1abe8ab4109b9dd98483cabdf09">“Fort”</a>
so as not to compete with Classic Flang.</p></li><li><p><strong>April 2018</strong> Steve Scalpone (NVIDIA) <a href="https://www.youtube.com/watch?v=sFVRQDgKihY">announces</a>
at the European LLVM Developers’ Conference that the frontend of Classic Flang will be rewritten to address
feedback from the LLVM community. This new front end became known as “F18”.</p></li><li><p><strong>August 2018</strong> Eric Schweitz (NVIDIA) begins work on what would become
“Fortran Intermediate Representation”, otherwise known as “FIR”. This work would
later become the <code>fir-dev</code> branch.</p></li><li><p><strong>February 2019</strong> Steve Scalpone <a href="https://lists.llvm.org/pipermail/llvm-dev/2019-February/130497.html">proposes</a>
contributing F18 to the LLVM Project.</p></li><li><p><strong>April 2019</strong> F18 is <a href="https://discourse.llvm.org/t/f18-is-accepted-as-part-of-llvm-project/51719">approved</a>
for migration into the LLVM Project monorepo.</p><p>At this point F18 was only the early parts of the compiler, it could not generate
code (later <code>fir-dev</code> work addressed this). Despite that, it moved into <code>flang/</code>
in the monorepo, awaiting the completion of the rest of the work.</p></li><li><p><strong>June 2019</strong> Peter Waller (Arm) <a href="https://discourse.llvm.org/t/rfc-adding-a-fortran-mode-to-the-clang-driver-for-flang/52307">proposes</a>
adding a Fortran mode to the Clang compiler driver.</p></li><li><p><strong>August 2019</strong> The <a href="https://github.com/flang-compiler/f18/commit/b6c30284e7876f6ccd4bb024bd5f349128e99b7c">first appearance</a>
of the <code>flang.sh</code> driver wrapper script (more on this later).</p></li><li><p><strong>December 2019</strong> The <a href="https://discourse.llvm.org/t/flang-landing-in-the-monorepo/54022">plan</a>
for rewriting the F18 git history to fit into the LLVM project is announced.
This effort was led by Arm, with Peter Waller going so far as to write
a <a href="https://github.com/llvm/llvm-project/commit/55d5e6cbe2509a24132d056e1f361dc39312929b#diff-c389405236998090c7c8b9741506f01fb28abbd7da52e9566323c585ac0eb89cL910">custom tool</a>
to rewrite the history of F18.</p><p>Kiran Chandramohan (Arm) <a href="https://groups.google.com/a/tensorflow.org/g/mlir/c/SCerbBpoxng/m/bVqWTRY7BAAJ">proposes</a>
an OpenMP dialect for MLIR, with the intention of using it in Flang (discussion
continues on <a href="https://discourse.llvm.org/t/rfc-openmp-dialect-in-mlir/397">Discourse</a>
during the following January).</p></li><li><p><strong>February 2020</strong> The <a href="https://discourse.llvm.org/t/plan-for-landing-flang-in-monorepo/54546">plan</a>
for improvements to F18 to meet the standards required for inclusion in the
LLVM monorepo is announced by Richard Barton (Arm).</p></li><li><p><strong>April 2020</strong> Upstreaming of F18 into the LLVM monorepo is
<a href="https://github.com/llvm/llvm-project/commit/b98ad941a40c96c841bceb171725c925500fce6c">completed</a>.</p></li></ul><p>At this point what was in the LLVM monorepo was F18, the rewritten frontend of
Classic Flang. Classic Flang remained unchanged, still using the PGI based frontend.</p><p>Around this time work started in the Classic Flang repo on the <code>fir-dev</code> branch
that would enable code generation when using F18.</p><p>For the following events remember that Classic Flang was still in use. The Classic
Flang binary is named <code>flang</code>, just like the folder F18 now occupies in the LLVM Project.</p><p><strong>Note:</strong> Some LLVM changes referenced below will appear to have skipped an LLVM release.
This is because they were done after the release branch was created, but before
the first release from that branch was distributed.</p><ul><li><p><strong>April 2020</strong> The first attempt at adding a new compiler driver for Flang is
<a href="https://reviews.llvm.org/D79092">posted</a> for review. It used the name
<code>flang-tmp</code>. This change was later abandoned in favour of a different approach.</p></li><li><p><strong>September 2020</strong> Flang’s new compiler driver is <a href="https://reviews.llvm.org/D86089">added</a>
as an experimental option. This is the first appearance of the <code>flang-new</code> binary,
instead of <code>flang-tmp</code> as proposed before.</p><blockquote><p>The name was intended as temporary, but not the driver.</p></blockquote><ul><li>Andrzej Warzyński (Arm, Flang Driver Maintainer)</li></ul></li><li><p><strong>October 2020</strong> Flang is included in an LLVM release for the first time in
LLVM 11.0.0. There is an <code>f18</code> binary and the previously mentioned script
<code>flang.sh</code>.</p></li><li><p><strong>August 2021</strong> <code>flang-new</code> is no longer experimental and <a href="https://reviews.llvm.org/D105811">replaces</a>
the previous Flang compiler driver binary <code>f18</code>.</p></li><li><p><strong>October 2021</strong> LLVM 13.0.0 is the first release to include a <code>flang-new</code> binary
(alongside <code>f18</code>).</p></li><li><p><strong>March 2022</strong> LLVM 14.0.0 releases, with <code>flang-new</code> replacing <code>f18</code> as the Flang
compiler driver.</p></li><li><p><strong>April 2022</strong> NVIDIA <a href="https://discourse.llvm.org/t/nvidia-transition-from-fir-dev/61947">ceases development</a>
of the <code>fir-dev</code> branch in the Classic Flang project. Upstreaming of <code>fir-dev</code>
to the LLVM Project begins around this date.</p><p><code>flang-new</code> can now do <a href="https://reviews.llvm.org/D122008">code generation</a>
if the <code>-flang-experimental-exec</code> option is used. This change used work
originally done on the <code>fir-dev</code> branch.</p></li><li><p><strong>May 2022</strong> Kiran Chandramohan <a href="https://www.youtube.com/watch?v=FoIjafZGDdE">announces</a>
at the European LLVM Developers’ Meeting that Flang’s OpenMP 1.1 support is close to complete.</p><p>The <code>flang.sh</code> compiler driver script becomes <code>flang-to-external-fc</code>. It
allows the user to use <code>flang-new</code> to parse Fortran source code, then write it back
to a file to be compiled with an existing Fortran compiler.</p><p>The script can be put in place of an existing compiler to test Flang’s parser on
large projects.</p></li><li><p><strong>June 2022</strong> Brad Richardson (Berkeley Lab) <a href="https://reviews.llvm.org/D153379">changes</a>
<code>flang-new</code> to generate code by default, removing the <code>-flang-experimental-exec</code>
option.</p></li><li><p><strong>July 2022</strong> Valentin Clément (NVIDIA) <a href="https://discourse.llvm.org/t/nvidia-transition-from-fir-dev/61947/5">announces</a>
that upstreaming of <code>fir-dev</code> to the LLVM Project is complete.</p></li><li><p><strong>September 2022</strong> LLVM 15.0.0 releases, including Flang’s experimental code
generation option.</p></li><li><p><strong>September 2023</strong> LLVM 17.0.0 releases, with Flang’s code generation enabled
by default.</p></li></ul><p>At this point the LLVM Project contained Flang as it is known today. Sometimes
referred to as “LLVM Flang”.</p><p>“LLVM Flang” is the combination of the F18 frontend and MLIR-based code generation
from <code>fir-dev</code>. As opposed to “Classic Flang” that combines a PGI based frontend and
its own custom backend.</p><p>The initiative to upstream Classic Flang was in some sense complete. Though
with all of the compiler rewritten in the process, what landed in the LLVM Project
was very different to Classic Flang.</p><ul><li><p><strong>April 2024</strong> The <code>flang-to-external-fc</code> script is <a href="https://github.com/llvm/llvm-project/pull/88904">removed</a>.</p></li><li><p><strong>September 2024</strong> LLVM 19.1.0 releases. The first release of <code>flang-new</code>
as a standalone compiler.</p></li><li><p><strong>October 2024</strong> The community deems that Flang has met the criteria to not be
“new” and the name is changed. Goodbye <code>flang-new</code>, hello <code>flang</code>!</p></li><li><p><strong>November 2024</strong> AMD <a href="https://rocm.blogs.amd.com/ecosystems-and-partners/fortran-journey/README.html">announces</a>
its next generation Fortran compiler, based on LLVM Flang.</p><p>Arm <a href="https://developer.arm.com/Tools%20and%20Software/Arm%20Compiler%20for%20Linux#Downloads">releases</a> an experimental version
of its new Arm Toolchain for Linux product, which includes LLVM Flang
as the Fortan compiler.</p></li><li><p><strong>March 2025</strong> LLVM 20.1.0 releases. The first time the <code>flang</code> binary has been
included in a release.</p></li></ul><p>Renaming Flang was discussed a few times <a href="https://discourse.llvm.org/t/reviving-rename-flang-new-to-flang/68130/1">before</a>
the final proposal. It was always contentious, so for the final
<a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462">proposal</a>
Brad Richardson decided to use the <a href="https://github.com/llvm/llvm-www/blob/main/proposals/LP0001-LLVMDecisionMaking.md">LLVM proposal process</a>.
Rarely used, but specifically designed for these situations.</p><blockquote><p>After several rounds of back and forth, I thought the discussion was
devolving and there wasn’t much chance we’d come to a consensus without some
outside perspective.</p></blockquote><ul><li>Brad Richardson</li></ul><p>That outside perspective included Chris Lattner (co-founder of the LLVM Project),
who quickly
<a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462/25">identified</a>
a unique problem:</p><blockquote><p>We have a bit of an unprecedented situation where an LLVM project is taking
the name of an already established compiler [Classic Flang]. Everyone seems to
want the older flang [Classic Flang] to fade away, but flang-new is not as
mature and it isn’t clear when and what the criteria should be for that.</p></blockquote><p>Confusion about the <code>flang</code> name was a key motivation for Brad Richardson too:</p><blockquote><p>Part of my concern was that the name “flang-new” would get common usage
before we were able to change it. I think it’s now been demonstrated that that
concern was valid, because right now [November 2024] fpm [<a href="https://fpm.fortran-lang.org/">Fortran Package Manager</a>]
recognizes the compiler by that name.</p><p>My main goal at that point was just clear goals for when we would
make the name change.</p></blockquote><p>No single list of goals was agreed, but some came up many times:</p><ul><li>Known limitations and supported features should be documented.</li><li>As much as possible, work that was expected to fix known
bugs should be completed, to prevent duplicate bug reports.</li><li>Unimplemented language features should fail with a message saying that they are
unimplemented. Rather than with a confusing failure or by producing incorrect
code.</li><li>LLVM Flang should perform relatively well when compared to other Fortran
compilers.</li><li>LLVM Flang must have a reasonable pass rate with a large Fortran language test
suite, and results of that must be shown publicly.</li><li>All reasonable steps should be taken to prevent anyone using a pre-packaged
Classic Flang confusing it with LLVM Flang.</li></ul><p>You will see a lot of relative language in those, like “reasonable”. No
one could say exactly what that meant, but everyone agreed that it was
inevitable that one day it would all be true.</p><p>Paul T Robinson summarised the dilemma <a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462/15">early</a>
in the thread:</p><blockquote><blockquote><p>the plan is to replace Classic Flang with the new Flang in the future.</p></blockquote><p>I suppose one of the relevant questions here is: Has the future arrived?</p></blockquote><p>After that Steve Scalpone (NVIDIA) gave
<a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462/16">their perspective</a>
that it was not yet time to change the name.</p><p>So the community got to work on those goals:</p><ul><li>Many performance and correctness issues were addressed by the “High Level
Fortran Intermediate Representation” (HLFIR) (which this article will explain later).</li><li>A cross-company team including Arm, Huawei, Linaro, Nvidia and Qualcomm
<a href="https://github.com/orgs/llvm/projects/35/views/1">collaborated</a>
to make it possible to build the popular <a href="https://www.spec.org/cpu2017/">SPEC 2017</a>
benchmark with Flang.</li><li>Flang gained support for OpenMP up to version 2.5, and was able to compile OpenMP
specific benchmarks like <a href="https://www.spec.org/omp2012/">SPEC OMP</a> and the
<a href="https://www.nas.nasa.gov/software/npb.html">NAS Parallel Benchmarks</a>.</li><li>Linaro <a href="https://www.youtube.com/watch?v=Gua80XRPhyY">showed that</a> the performance
of Flang compared favourably with Classic Flang and was not far behind GFortran.</li><li>The GFortran test suite was added to the <a href="https://github.com/llvm/llvm-test-suite/tree/main/Fortran/gfortran">LLVM Test Suite</a>,
and Flang achieved good results.</li><li>Fujitsu’s <a href="https://github.com/fujitsu/compiler-test-suite">test suite</a> was made
public and tested with Flang. The process to make IBM’s Fortran test suite public
was started.</li></ul><p>With all that done, in October of 2024 <code>flang-new</code>
<a href="https://github.com/llvm/llvm-project/commit/06eb10dadfaeaadc5d0d95d38bea4bfb5253e077">became</a>
<code>flang</code>. The future had arrived.</p><blockquote><p>And it’s merged! It’s been a long (and sometimes contentious) process, but
thank you to everyone who contributed to the discussion.</p></blockquote><ul><li>Brad Richardson, <a href="https://discourse.llvm.org/t/proposal-rename-flang-new-to-flang/69462/86">closing out</a> the proposal.</li></ul><p>The goals the community achieved have certainly been worth it for Flang as a
compiler, but did Brad achieve their own goals?</p><blockquote><p>What did I hope to see as a result of the name change? I wanted it to be
easier for more people to try it out.</p></blockquote><p>So once you have finished reading this article,
<a href="https://github.com/llvm/llvm-project/releases/tag/llvmorg-20.1.0">download</a>
Flang or try it out on <a href="https://godbolt.org/z/3hhYM37Kh">Compiler Explorer</a>.
You know at least one person will appreciate it!</p><p>All compilers that use LLVM as a backend eventually produce code in the form of
the <a href="https://llvm.org/docs/LangRef.html">LLVM Intermediate Representation</a>
(LLVM IR).</p><p>A drawback of this is that LLVM IR does not include language specific information.
This means that for example, it cannot be used to optimise arrays in a way
specific to Fortran programs.</p><p>One solution to this has been to build a higher level IR that represents the
unique features of the language, optimise that, then convert the result into LLVM IR.</p><p>Eric Schweitz (NVIDIA) started to do that for Fortran in late 2018:</p><blockquote><p>FIR was originally conceived as a high-level IR that would interoperate with
LLVM but have a representation more friendly and amenable to Fortran
optimizations.</p></blockquote><p>Naming is hard but Eric did well here:</p><blockquote><p>FIR was a pun of sorts. Fortran IR and meant to be evocative of the trees
(Abstract Syntax Trees).</p></blockquote><p>We will not go into detail about this early FIR, because <a href="https://mlir.llvm.org/">MLIR</a>
was revealed soon after Eric started the project and they quickly adopted it.</p><blockquote><p>When MLIR was announced, I quickly switched gears from building data
structures for a new “intermediate IR” to porting my IR design to MLIR and
using that instead.</p><p>I believe FIR was probably the first “serious project” outside of Google to
start using MLIR.</p></blockquote><p>The FIR work continued to develop, with Jean Perier (NVIDIA) joining Eric on
the project. It became its own public branch <code>fir-dev</code>, which was later contributed
to the LLVM Project.</p><p>The following sections will go into detail on the intermediate representations
that Flang uses today.</p><p>The journey from Classic Flang to LLVM Flang involved a rewrite of the
entire compiler. This provided an opportunity to pick up new things from
the LLVM Project. Most notably MLIR.</p><p>“Multi-Level Intermediate Representation” (MLIR) was first
<a href="https://www.youtube.com/watch?v=qzljG6DKgic">introduced</a> to the LLVM
community in 2019, around the time that F18 was approved to move into the LLVM Project.</p><p>The problem that MLIR addresses is the same one that Eric Schweitz tackled with FIR:
It is difficult to map high level details of programming languages
into LLVM IR.</p><p>You either have to attach them to the IR as metadata, try to recover the
lost details later, or fight an uphill battle to add the details to
LLVM IR itself. These details are crucial for producing optimised code in certain
languages. (Fortran array optimisations were one use case referenced).</p><p>This led languages such as Swift and Rust to create their own IRs that include
information relevant to their own optimisations. After that IR has been optimised
it is converted into LLVM IR and goes through the normal compilation pipeline.</p><p>To implement these IRs they have to build a lot of infrastructure, but it cannot
be shared between the compilers. This is where MLIR comes in.</p><blockquote><p>The MLIR project aims to directly tackle these programming language design and
implementation challenges—by making it very cheap to define and introduce new
abstraction levels, and provide “in the box” infrastructure to solve common
compiler engineering problems.</p></blockquote><ul><li><a href="https://arxiv.org/abs/2002.11054">“MLIR: A Compiler Infrastructure for the End of Moore’s Law”</a>
(Chris Lattner, Mehdi Amini et al., 2020)</li></ul><h2 id="flang-and-mlir">Flang and MLIR</h2><p>The same year MLIR debuted, Eric Schweitz gave a talk at the later US
LLVM Developers’ meeting titled
<a href="https://www.youtube.com/watch?v=ff3ngdvUang">“An MLIR Dialect for High-Level Optimization of Fortran”</a>.
FIR by that point was implemented as an MLIR dialect.</p><blockquote><p>That [switching FIR to be based on MLIR] happened very quickly and I never
looked back.</p><p>MLIR, even in its infancy, was clearly solving many of the exact same problems
that we were facing building a new Fortran compiler.</p></blockquote><ul><li>Eric Schweitz</li></ul><p>The MLIR community were also happy to have Flang on board:</p><blockquote><p>It was fantastic to have very quickly in the early days of MLIR a non-ML [Machine Learning] frontend
to exercise features we built in MLIR in anticipation. It led us to course-correct
in some cases, and Flang was a motivating factor for many feature requests.
It contributed significantly to establishing and validating that MLIR had the right foundations.</p></blockquote><ul><li>Mehdi Amini</li></ul><p>Flang did not stop there, later adding another dialect
<a href="https://flang.llvm.org/docs/HighLevelFIR.html">“High Level Fortran Intermediate Representation”</a>
(HLFIR) which works at a higher level than FIR. A big target of HLFIR
was array optimisations, that were more complex to handle using FIR alone.</p><blockquote><p>FIR was a compromise on both ends to some degree. It wasn’t trying to capture
syntactic information from Fortran, and I assumed there would be work done on
an Abstract Syntax Tree. That niche would later be filled by “High Level FIR”
[HLFIR].</p></blockquote><ul><li>Eric Schweitz</li></ul><h2 id="irs-all-the-way-down">IRs All the Way Down</h2><p>The compilation process starts with Fortran source code.</p><div><pre tabindex="0"><code data-lang="fortran"><span><span><span>subroutine</span> example(a, b)
</span></span><span><span>  <span>real</span> <span>::</span> a(:), b(:)
</span></span><span><span>  a <span>=</span> b
</span></span><span><span><span>end</span> <span>subroutine</span>
</span></span></code></pre></div><p>(<a href="https://godbolt.org/z/8j3W46j3j">Compiler Explorer</a>)</p><p>The
<a href="https://fortran-lang.org/learn/quickstart/organising_code/">subroutine</a> <code>example</code>
assigns array <code>b</code> to array <code>a</code>.</p><p>It is tempting to think of the IRs in a “stack” where each one is converted
into the next. However, MLIR allows multiple “dialects” of MLIR to exist in the
same file.</p><p>(The steps shown here are the most important ones for Flang. In reality there
are many more between Fortran and LLVM IR.)</p><p>In the first step, Flang produces a file that is a mixture of HLFIR, FIR
and the built-in MLIR dialect <code>func</code> (function).</p><div><pre tabindex="0"><code data-lang="mlir"><span><span>module attributes {&lt;...&gt;} {
</span></span><span><span>  <span>func</span>.<span>func</span> <span>@_QPexample</span>(%arg0: !fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt; {fir.bindc_name = <span>&#34;a&#34;</span>}, %arg1: !fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt; {fir.bindc_name = <span>&#34;b&#34;</span>}) {
</span></span><span><span>    %0 = fir.dummy_scope : !fir.dscope
</span></span><span><span>    %1:2 = hlfir.declare %arg0 dummy_scope %0 {uniq_name = <span>&#34;_QFexampleEa&#34;</span>} : (!fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;, !fir.dscope) -&gt; (!fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;, !fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;)
</span></span><span><span>    %2:2 = hlfir.declare %arg1 dummy_scope %0 {uniq_name = <span>&#34;_QFexampleEb&#34;</span>} : (!fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;, !fir.dscope) -&gt; (!fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;, !fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;)
</span></span><span><span>    hlfir.assign %2#0 to %1#0 : !fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;, !fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;
</span></span><span><span>    <span>return</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>For example, the “dummy arguments” (the
<a href="https://fortran-lang.org/en/learn/quickstart/organising_code/#subroutines">arguments of a subroutine</a>)
are declared with <code>hlfir.declare</code> but their type is specified with <code>fir.array</code>.</p><p>As MLIR allows multiple dialects to exist in the same file, there is no need for
HLFIR to have a <code>hlfir.array</code> that duplicates <code>fir.array</code>, unless HLFIR wanted
to handle that differently.</p><p>The next step is to convert HLFIR into FIR:</p><div><pre tabindex="0"><code data-lang="mlir"><span><span>module attributes {&lt;...&gt;} {
</span></span><span><span>  <span>func</span>.<span>func</span> <span>@_QPexample</span>(&lt;...&gt;) {
</span></span><span><span>    &lt;...&gt;
</span></span><span><span>    %c3_i32 = arith.<span>constant</span> <span>3</span> : <span>i32</span>
</span></span><span><span>    %7 = fir.convert %0 : (!fir.ref&lt;!fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;&gt;) -&gt; !fir.ref&lt;!fir.box&lt;none&gt;&gt;
</span></span><span><span>    %8 = fir.convert %5 : (!fir.box&lt;!fir.array&lt;<span>?x</span><span>f32</span>&gt;&gt;) -&gt; !fir.box&lt;none&gt;
</span></span><span><span>    %9 = fir.convert %6 : (!fir.ref&lt;!fir.char&lt;<span>1</span>,<span>17</span>&gt;&gt;) -&gt; !fir.ref&lt;<span>i8</span>&gt;
</span></span><span><span>    %10 = fir.call <span>@_FortranAAssign</span>(%7, %8, %9, %c3_i32) : (!fir.ref&lt;!fir.box&lt;none&gt;&gt;, !fir.box&lt;none&gt;, !fir.ref&lt;<span>i8</span>&gt;, <span>i32</span>) -&gt; none
</span></span><span><span>    <span>return</span>
</span></span><span><span>  }
</span></span><span><span>&lt;...&gt;
</span></span><span><span>}
</span></span></code></pre></div><p>Then this bundle of MLIR dialects is converted into LLVM IR:</p><div><pre tabindex="0"><code data-lang="mlir"><span><span>define void <span>@example_</span>(ptr %0, ptr %1) {
</span></span><span><span>  &lt;...&gt;
</span></span><span><span>  store { ptr, <span>i64</span>, <span>i32</span>, <span>i8</span>, <span>i8</span>, <span>i8</span>, <span>i8</span>, [<span>1 x</span> [<span>3 x</span> <span>i64</span>]] } %37, ptr %3, align <span>8</span>
</span></span><span><span>  call void <span>@llvm.memcpy.p0.p0.i32</span>(ptr %5, ptr %4, <span>i32</span> <span>48</span>, <span>i1</span> false)
</span></span><span><span>  %38 = call {} <span>@_FortranAAssign</span>(ptr %5, ptr %3, ptr <span>@_QQclX2F6170702F6578616D706C652E66393000</span>, <span>i32</span> <span>3</span>)
</span></span><span><span>  ret void
</span></span><span><span>}
</span></span><span><span>&lt;...&gt;
</span></span></code></pre></div><p>This LLVM IR passes through the standard compilation pipeline that clang also uses.
Eventually being converted into target specific
<a href="https://llvm.org/docs/MIRLangRef.html">Machine IR</a> (MIR), into assembly and
finally into a binary program.</p><ul><li>Fortran</li><li>MLIR (including HLFIR and FIR)</li><li>MLIR (including FIR)</li><li>LLVM IR</li><li>MIR</li><li>Assembly</li><li>Binary</li></ul><p>At each stage, the optimisations most suited to that stage are done.
For example, while you have HLFIR you could optimise array accesses because at that
point you have the most information about how the Fortran treats arrays.</p><p>If Flang were to do this later on, in LLVM IR, it would be much more difficult.
Either the information would be lost or incomplete, or you would be at a stage in
the pipeline where you cannot assume that you started with a specific source
language.</p><p><strong>Note:</strong> Most of the points made in this section also apply to <a href="https://www.openacc.org/">OpenACC</a> support in Flang. In the interest of brevity, I
will only describe OpenMP in this article. You can find more about OpenACC
in this <a href="https://www.youtube.com/watch?v=vVmCLdSboWc">presentation</a>.</p><h2 id="openmp-basics">OpenMP Basics</h2><p><a href="https://www.openmp.org/">OpenMP</a> is a standardised API for adding
parallelism to C, C++ and Fortran programs.</p><p>Programmers mark parts of their code with “directives”. These directives
tell the compiler how the work of the program should be distributed.
Based on this, the compiler transforms the code and inserts calls to an
OpenMP runtime library for certain operations.</p><p>This is a Fortran example:</p><div><pre tabindex="0"><code data-lang="Fortran"><span><span><span>SUBROUTINE</span> SIMPLE(N, A, B)
</span></span><span><span>  <span>INTEGER</span> I, N
</span></span><span><span>  <span>REAL</span> B(N), A(N)
</span></span><span><span><span>!$OMP PARALLEL DO
</span></span></span><span><span><span></span>  <span>DO</span> I<span>=</span><span>2</span>,N
</span></span><span><span>    B(I) <span>=</span> (A(I) <span>+</span> A(I<span>-</span><span>1</span>)) <span>/</span> <span>2.0</span>
</span></span><span><span>  <span>ENDDO</span>
</span></span><span><span><span>END</span> <span>SUBROUTINE</span> SIMPLE
</span></span></code></pre></div><p>(from <a href="https://www.openmp.org/wp-content/uploads/openmp-examples-4.5.0.pdf">“OpenMP Application Programming Interface Examples”</a>, <a href="https://godbolt.org/z/chjzs3o6r">Compiler Explorer</a>)</p><p><strong>Note:</strong> Fortran arrays are <a href="https://fortran-lang.org/en/learn/quickstart/arrays_strings/">one-based</a> by default. So the first element is at index 1. This example reads the previous element as well, so it starts <code>I</code> at 2.</p><p><code>!$OMP PARALLEL DO</code> is a directive in the form of a Fortran comment (Fortran
comments start with <code>!</code>).<code>PARALLEL DO</code> starts a parallel “region” that
includes the code from <code>DO</code> to <code>ENDDO</code>.</p><p>This tells the compiler that the work in the <code>DO</code> loop should be shared amongst
all the threads available to the program.</p><p>Clang has <a href="https://blog.llvm.org/2015/05/openmp-support_22.html">supported OpenMP</a>
for many years now. The equivalent C++ code is:</p><div><pre tabindex="0"><code data-lang="C++"><span><span><span>void</span> <span>simple</span>(<span>int</span> n, <span>float</span> <span>*</span>a, <span>float</span> <span>*</span>b)
</span></span><span><span>{
</span></span><span><span>    <span>int</span> i;
</span></span><span><span>
</span></span><span><span>    <span>#pragma omp parallel for
</span></span></span><span><span><span></span>    <span>for</span> (i<span>=</span><span>1</span>; i<span>&lt;</span>n; i<span>++</span>)
</span></span><span><span>        b[i] <span>=</span> (a[i] <span>+</span> a[i<span>-</span><span>1</span>]) <span>/</span> <span>2.0</span>;
</span></span><span><span>}
</span></span></code></pre></div><p>(<a href="https://godbolt.org/z/Yh9jb8rKe">Compiler Explorer</a>)</p><p>For C++, the directive is in the form of a <code>#pragma</code> and attached
to the <code>for</code> loop.</p><p>LLVM IR does not know anything about OpenMP specifically, so Clang does all the
work of converting the intent of the directives into LLVM IR. The output from
Clang looks like this:</p><div><pre tabindex="0"><code data-lang="mlir"><span><span>define dso_local void <span>@simple</span>(int, float*, float*)
</span></span><span><span>  (<span>i32</span> noundef %n, ptr noundef %a, ptr noundef %b) &lt;...&gt; {
</span></span><span><span>entry:
</span></span><span><span>&lt;...&gt;
</span></span><span><span>  call void (&lt;...&gt;) <span>@__kmpc_fork_call</span>(<span>@simple</span> &lt;...&gt; (.omp_outlined) &lt;...&gt;)
</span></span><span><span>  ret void
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span>define internal void <span>@simple</span>(int, float*, float*) (.omp_outlined)
</span></span><span><span>  (ptr &lt;...&gt; %.global_tid.,
</span></span><span><span>   ptr &lt;...&gt; %.bound_tid.,
</span></span><span><span>   ptr &lt;...&gt; %n,
</span></span><span><span>   ptr &lt;...&gt; %b,
</span></span><span><span>   ptr &lt;...&gt; %a) {
</span></span><span><span>entry:
</span></span><span><span>&lt;...&gt;
</span></span><span><span>  call void <span>@__kmpc_for_static_init_4</span>(&lt;...&gt;)
</span></span><span><span>&lt;...&gt;
</span></span><span><span>omp.inner.for.body.i:
</span></span><span><span>&lt;...&gt;
</span></span><span><span>omp.loop.exit.i:
</span></span><span><span>  call void <span>@__kmpc_for_static_fini</span>(&lt;...&gt;)
</span></span><span><span>&lt;...&gt;
</span></span><span><span>  ret void
</span></span><span><span>}
</span></span></code></pre></div><p>(output edited for readability)</p><p>The body of <code>simple</code> no longer does all the work. Instead it uses
<code>__kmpc_fork_call</code> to tell the OpenMP
<a href="https://github.com/llvm/llvm-project/tree/main/openmp">runtime library</a>
to run another function, <code>simple (.omp_outlined)</code> to do the work.</p><p>This second function is referred to as a “micro task”. The runtime library
splits the work across many instances of the micro task and each time
the micro task function is called, it gets a different slice of the work.</p><p>The number of instances is only known at runtime, and can be controlled with
settings such as <a href="https://www.openmp.org/spec-html/5.0/openmpse50.html"><code>OMP_NUM_THREADS</code></a>.</p><p>The LLVM IR representation of <code>simple (.omp_outlined)</code> includes labels like
<code>omp.loop.exit.i</code>, but these are not specific to OpenMP. They are just normal LLVM IR
labels whose name includes <code>omp</code>.</p><h2 id="sharing-clangs-openmp-knowledge">Sharing Clang’s OpenMP Knowledge</h2><p>Shortly after Flang was approved to join the LLVM Project, it was proposed that
Flang should share OpenMP support code with Clang.</p><blockquote><p>This is an RFC for the design of the OpenMP front-ends under the LLVM
umbrella. It is necessary to talk about this now as Flang (aka. F18) is
maturing at a very promising rate and about to become a sub-project next
to Clang.</p><p>TLDR;
Keep AST nodes and Sema separated but unify LLVM-IR generation for
OpenMP constructs based on the (almost) identical OpenMP directive
level.</p></blockquote><ul><li>“[RFC] Proposed interplay of Clang &amp; Flang &amp; LLVM wrt. OpenMP”,
Johannes Doerfert (Lawrence Livermore National Laboratory), May 2019 (only one
<a href="https://discourse.llvm.org/t/rfc-proposed-interplay-of-clang-flang-llvm-wrt-openmp-flang-dev/51905">part</a>
of this still exists online, this quote is from a copy of the other part, which was provided to me).</li></ul><p>For our purposes, the “TLDR” means that although both compilers have different
internal representations of the OpenMP directives, they both have to produce
LLVM IR from that representation.</p><p>This proposal led to the creation of the <code>LLVMFrontendOpenMP</code> library in
<code>llvm</code>. By using the same class <code>OpenMPIRBuilder</code>, there is no need to repeat work in
both compilers, at least for this part of the OpenMP pipeline.</p><p>As you will see in the following sections, Flang has diverged from Clang for other
parts of OpenMP processing.</p><h2 id="bringing-openmp-to-mlir">Bringing OpenMP to MLIR</h2><p>Early in 2020, Kiran Chandramohan (Arm) <a href="https://discourse.llvm.org/t/rfc-openmp-dialect-in-mlir/397">proposed</a>
an MLIR dialect for OpenMP, for use by Flang.</p><blockquote><p>We started the work for the OpenMP MLIR dialect because of Flang.
… So, MLIR has an OpenMP dialect because of Flang.</p></blockquote><ul><li>Kiran Chandramohan</li></ul><p>This dialect would represent OpenMP specifically, unlike the generic LLVM IR
you get from Clang.</p><p>If you compile the original Fortran OpenMP example without OpenMP enabled, you
get this MLIR:</p><div><pre tabindex="0"><code data-lang="mlir"><span><span>module attributes {&lt;...&gt;} {
</span></span><span><span>  <span>func</span>.<span>func</span> <span>@_QPsimple</span>(&lt;...&gt; {
</span></span><span><span>    %1:2 = hlfir.declare %arg0 &lt;...&gt; {uniq_name = <span>&#34;_QFsimpleEn&#34;</span>} : &lt;...&gt;
</span></span><span><span>    %3:2 = hlfir.declare %2 &lt;...&gt; {uniq_name = <span>&#34;_QFsimpleEi&#34;</span>} : &lt;...&gt;
</span></span><span><span>    %10:2 = hlfir.declare %arg1(%9) &lt;...&gt; {uniq_name = <span>&#34;_QFsimpleEa&#34;</span>} : &lt;...&gt;
</span></span><span><span>    %17:2 = hlfir.declare %arg2(%16) &lt;...&gt; {uniq_name = <span>&#34;_QFsimpleEb&#34;</span>} : &lt;...&gt;
</span></span><span><span>    %22:2 = fir.do_loop &lt;...&gt; {
</span></span><span><span>      &lt;...&gt;
</span></span><span><span>      hlfir.assign %34 to %37 : <span>f32</span>, !fir.ref&lt;<span>f32</span>&gt;
</span></span><span><span>    }
</span></span><span><span>    fir.store %22#1 to %3#1 : !fir.ref&lt;<span>i32</span>&gt;
</span></span><span><span>    <span>return</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>(output edited for readability)</p><p>Notice that the <code>DO</code> loop has been converted into <code>fir.do_loop</code>. Now enable
OpenMP and compile again:</p><div><pre tabindex="0"><code data-lang="mlir"><span><span>module attributes {&lt;...&gt;} {
</span></span><span><span>  <span>func</span>.<span>func</span> <span>@_QPsimple</span>(&lt;...&gt; {
</span></span><span><span>    %1:2 = hlfir.declare %arg0 &lt;...&gt; {uniq_name = <span>&#34;_QFsimpleEn&#34;</span>} : &lt;...&gt;
</span></span><span><span>    %10:2 = hlfir.declare %arg1(%9) &lt;...&gt; {uniq_name = <span>&#34;_QFsimpleEa&#34;</span>} : &lt;...&gt;
</span></span><span><span>    %17:2 = hlfir.declare %arg2(%16) &lt;...&gt; {uniq_name = <span>&#34;_QFsimpleEb&#34;</span>} : &lt;...&gt;
</span></span><span><span>    omp.parallel {
</span></span><span><span>      %19:2 = hlfir.declare %18 {uniq_name = <span>&#34;_QFsimpleEi&#34;</span>} : &lt;...&gt;
</span></span><span><span>      omp.wsloop {
</span></span><span><span>        omp.loop_nest (%arg3) : i32 = (%c2_i32) to (%20) inclusive step (%c1_i32) {
</span></span><span><span>          hlfir.assign %32 to %35 : <span>f32</span>, !fir.ref&lt;<span>f32</span>&gt;
</span></span><span><span>          omp.yield
</span></span><span><span>        }
</span></span><span><span>      }
</span></span><span><span>      omp.terminator
</span></span><span><span>    }
</span></span><span><span>    <span>return</span>
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><p>(output edited for readability)</p><p>You will see that instead of <code>fir.do_loop</code> you have <code>omp.parallel</code>,
<code>omp.wsloop</code> and <code>omp.loop_nest</code>. <code>omp</code> is an MLIR dialect that describes
<a href="https://mlir.llvm.org/docs/Dialects/OpenMPDialect/">OpenMP</a>.</p><p>This translation of the <code>PARALLEL DO</code> directive is much more literal than
the LLVM IR produced by Clang for <code>parallel for</code>.</p><p>As the <code>omp</code> dialect is specifically made for OpenMP, it can represent
it much more naturally. This makes it easier to understand the code and to
write optimisations.</p><p>Of course Flang needs to produce LLVM IR eventually, and to do that it
uses the same <code>OpenMPIRBuilder</code> class that Clang does. From the
MLIR shown previously, <code>OpenMPIRBuilder</code> produces the following LLVM IR:</p><div><pre tabindex="0"><code data-lang="mlir"><span><span>define void <span>@simple_</span> &lt;...&gt; {
</span></span><span><span>entry:
</span></span><span><span>  call void (&lt;...&gt;) <span>@__kmpc_fork_call</span>( &lt;...&gt; <span>@simple_..omp_par</span> &lt;...&gt;)
</span></span><span><span>  ret void
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span>define internal void <span>@simple_..omp_par</span> &lt;...&gt; {
</span></span><span><span>omp.par.entry:
</span></span><span><span>  call void <span>@__kmpc_for_static_init_4u</span> &lt;...&gt;
</span></span><span><span>omp_loop.exit:
</span></span><span><span>  call void <span>@__kmpc_barrier</span>(&lt;...&gt;)
</span></span><span><span>  ret void
</span></span><span><span>omp_loop.body:
</span></span><span><span>  &lt;...&gt;
</span></span><span><span>}
</span></span></code></pre></div><p>The LLVM IR produced by Flang and Clang is superficially different, but
structurally very similar. Considering the differences in source language
and compiler passes, it is not surprising that they are not identical.</p><h2 id="clangir-and-the-future">ClangIR and the Future</h2><p>It is surprising that a compiler for a language as old as Fortran got ahead of
Clang (the most well known LLVM based compiler) when it came to adopting MLIR.</p><p>This is largely due to timing, MLIR is a recent invention and Clang existed
before MLIR arrived. Clang also has a legacy to protect, so it is unlikely to
migrate to a new technology right away.</p><p>The <a href="https://llvm.github.io/clangir/">ClangIR</a> project is working to change
Clang to use a new MLIR dialect, “Clang Intermediate Representation” (“CIR”).
Much like Flang and its HLFIR/FIR dialects, ClangIR will convert C and C++
into the CIR dialect.</p><p>Work on OpenMP support for ClangIR has already <a href="https://github.com/llvm/clangir/pull/382">started</a>,
using the <code>omp</code> dialect that was originally added for Flang.</p><p>Unfortunately at time of writing the <code>parallel</code> directive is not supported by
ClangIR. However, if you look at the CIR produced when OpenMP is disabled, you can
see the <code>cir.for</code> element that the OpenMP dialect might replace:</p><div><pre tabindex="0"><code data-lang="mlir"><span><span>module &lt;...&gt; attributes {&lt;...&gt;} {
</span></span><span><span>  cir.<span>func</span> <span>@_Z6simpleiPfS_</span>( &lt;...&gt; {
</span></span><span><span>    %1 = cir.alloca &lt;...&gt; [<span>&#34;a&#34;</span>, init] &lt;...&gt;
</span></span><span><span>    %2 = cir.alloca &lt;...&gt; [<span>&#34;b&#34;</span>, init] &lt;...&gt;
</span></span><span><span>    %3 = cir.alloca &lt;...&gt; [<span>&#34;i&#34;</span>] &lt;...&gt;
</span></span><span><span>    cir.scope {
</span></span><span><span>      cir.for :
</span></span><span><span>      cond { &lt;...&gt; }
</span></span><span><span>      body {
</span></span><span><span>        &lt;...&gt;
</span></span><span><span>        cir.yield <span>loc</span>(#loc13)
</span></span><span><span>      } step {
</span></span><span><span>        &lt;...&gt;
</span></span><span><span>        cir.yield <span>loc</span>(#loc36)
</span></span><span><span>      } <span>loc</span>(#loc36)
</span></span><span><span>    } <span>loc</span>(#loc36)
</span></span><span><span>    cir.<span>return</span> <span>loc</span>(#loc2)
</span></span><span><span>  } <span>loc</span>(#loc31)
</span></span><span><span>} <span>loc</span>(#loc)
</span></span></code></pre></div><p>(on <a href="https://godbolt.org/z/Yj9EKK7ao">Compiler Explorer</a>)</p><p><strong>Note:</strong> This section paraphrases material from
<a href="https://github.com/llvm/llvm-project/blob/main/flang/docs/FlangDriver.md">“Flang Drivers”</a>.
If you want more detail please refer to that document, or
<a href="https://fabiensanglard.net/dc/index.php">Driving Compilers</a>.</p><p>“Driver” in a compiler context means the part of the compiler that decides
how to handle a set of options. For instance, when you use the option <code>-march=armv8a+memtag</code>,
something in Flang knows that you want to compile for Armv8.0-a with the Memory
Tagging Extension enabled.</p><p><code>-march=</code> is an example of a “compiler driver” option. These options are what users
give to the compiler. There is actually a second driver after this, confusingly
called the “frontend” driver, despite being behind the scenes.</p><p>In Flang’s case the “compiler driver” is <code>flang</code> and the “frontend driver” is
<code>flang -fc1</code> (they are two separate tools, contained in the same binary).</p><p>They are separate tools so that the compiler driver can provide an interface
suited to compiler users, with stable options that do not change over time.
On the other hand, the frontend driver is suited to compiler developers, exposes
internal compiler details and does not have a stable set of options.</p><p>You can see the differences if you add <code>-###</code> to the compiler command:</p><pre tabindex="0"><code>$ ./bin/flang /tmp/test.f90 -march=armv8a+memtag -###
 &#34;&lt;...&gt;/flang&#34; &#34;-fc1&#34;
   &#34;-triple&#34; &#34;aarch64-unknown-linux-gnu&#34;
   &#34;-target-feature&#34; &#34;+v8a&#34;
   &#34;-target-feature&#34; &#34;+mte&#34;
 &#34;/usr/bin/ld&#34; \
   &#34;-o&#34; &#34;a.out&#34;
   &#34;-L/usr/lib/gcc/aarch64-linux-gnu/11&#34;
</code></pre><p>(output edited for readability)</p><p>The compiler driver has split the compilation into a job for the frontend
(<code>flang -fc1</code>) and the linker (<code>ld</code>). <code>-march=</code> has been converted into many
arguments to <code>flang -fc1</code>. This means that if compiler developers decided to
change how <code>-march=</code> was converted, existing <code>flang</code> commands would still work.</p><p>Another responsibility of the compiler driver is to know where to find libraries
and header files. This differs between operating systems and even
distributions of the same family of operating systems (for example Linux
distributions).</p><p>This created a problem when implementing the compiler driver for Flang. All these
details would take a long time to get right.</p><p>Luckily, by this time Flang was in the LLVM Project alongside Clang.
Clang already knew how to handle this and had been tested on all sorts of
systems over many years.</p><blockquote><p>The intent is to mirror clang, for both the driver and CompilerInvocation, as
much as makes sense to do so. The aim is to avoid re-inventing the wheel and
to enable people who have worked with either the clang or flang entry points,
drivers, and frontends to easily understand the other.</p></blockquote><ul><li><a href="https://discourse.llvm.org/t/rfc-adding-a-fortran-mode-to-the-clang-driver-for-flang/52307">Peter Waller</a> (Arm)</li></ul><p>Flang became the first in-tree project to use Clang’s compiler driver
library (<code>clangDriver</code>) to implement its own compiler driver.</p><p>This meant that Flang was able to handle all the targets and tools that Clang
could, without duplicating large amounts of code.</p><p>We are almost 10 years from the first announcement of what would become LLVM
Flang. In the LLVM monorepo alone there have been close to 10,000 commits
from around 400 different contributors. Undoubtedly more in Classic Flang before
that.</p><p>So it is time to hear from users, contributors, and supporters, past and
present, about their experiences with Flang.</p><blockquote><p>Collaborating with NVIDIA and PGI on Classic Flang was crucial in establishing
Arm in High Performance Computing. It has been an honour to continue investing
in Flang, helping it become an integral part of the LLVM project and a solid
foundation for building HPC toolchains.</p><p>We are delighted to see the project reach maturity, as this was the last step in
allowing us to remove all downstream code from our compiler. Look out for Arm
Toolchain for Linux 20, which will be a fully open source, freely available
compiler based on LLVM 20, available later this year.”</p></blockquote><ul><li>Will Lovett, Director Technology Management at Arm.</li></ul><p>(the following quote is presented in Japanese and English, in case of differences,
Japanese is the authoritative version)</p><blockquote><p>富士通は、我々の数十年にわたるHPCの経験を通じて培ったテストスイートを用いて、Flangの改善に貢献できたことを嬉しく思います。Flangの親切で協力的なコミュ
ニティに大変感銘を受けました。</p><p>富士通は、より高いパフォーマンスと使いやすさを実現し、我々のプロセッサを最大限に活用するために、引き続きFlangに取り組んでいきます。Flangが改善を続け、ユーザーを増やしていくことを強く願っています。</p><p>Fujitsu is pleased to have contributed to the improvement of Flang with our
test suite, which we have developed through our decades of HPC experience.
Flang’s helpful and collaborative community really impressed us.</p><p>Fujitsu will continue to work on Flang to achieve higher performance and
usability, to make the best of our processors. We hope that Flang will continue
to improve and gain users.</p></blockquote><ul><li>富士通株式会社 コンパイラ開発担当 マネージャー 鎌塚　俊 (Shun Kamatsuka, Manager of the Compiler Development Team at Fujitsu).</li></ul><blockquote><p>Collaboration between Linaro and Fujitsu on an active CI using Fujitsu’s
testsuite helped find several issues and make Flang more robust, in
addition to detecting any regressions early.</p><p>Linaro has been contributing to Flang development for two years now, fixing a
great number of issues found by the Fujitsu testsuite.</p></blockquote><ul><li>Carlos Seo, Tech Lead at Linaro.</li></ul><blockquote><p><a href="https://scipy.org/">SciPy</a> is a foundational Python package. It provides easy
access to scientific algorithms, many of which are written in Fortran.</p><p>This has caused a long stream of problems for packaging and shipping SciPy,
especially because users expect first-class support for Windows;
a platform that (prior to Flang) had no license-free Fortran compilers
that would work with the default platform runtime.</p><p>As maintainers of SciPy and redistributors in the <a href="https://conda-forge.org/">conda-forge</a>
ecosystem, we hoped for a solution to this problem for many years. In the end,
we switched to using Flang, and that <a href="https://labs.quansight.org/blog/building-scipy-with-flang">process</a>
was a minor miracle.</p><p>Huge thanks to the Flang developers for removing a major source of pain for us!</p></blockquote><ul><li>Axel Obermeier, Quantsight Labs.</li></ul><blockquote><p>At the Barcelona Supercomputing Center, like many other HPC centers, we cannot
ignore Fortran.</p><p>As part of our research activities, Flang has allowed us to apply our work in
long vectors for RISC-V to complex Fortran applications which we have been able
to run and analyze in our prototype systems. We have also used Flang to support
an in-house task-based directive-based programming model.</p><p>These developments have proved to us that Flang is a powerful infrastructure.</p></blockquote><ul><li>Roger Ferrer Ibáñez, Senior Research Engineer at the Barcelona Supercomputing Center (BSC).</li></ul><blockquote><p>I am thrilled to see the LLVM Flang project achieve this milestone. It is a unique
project in that it marries state of the art compiler technologies like MLIR with
the venerable Fortran language and its large community of developers focused on
high performance compute.</p><p>Flang has set the standard for LLVM frontends by adopting MLIR and C++17 features
earlier than others, and I am thrilled to see Clang and other frontends modernize
based on those experiences.</p><p>Flang also continues something very precious to me: the LLVM Project’s ability
to enable collaboration by uniting people with shared interests even if they
span organizations like academic institutions, companies, and other research groups.</p></blockquote><ul><li>Chris Lattner, serving member of the LLVM Board of Directors, co-founder of
the LLVM Project, the Clang C++ compiler and MLIR.</li></ul><blockquote><p>The need for a more modern Fortran compiler motivated the creation of the LLVM Flang
project and AMD fully supports that path.</p><p>In following with community trends, AMD’s Next-Gen Fortran Compiler will be a
downstream flavor of LLVM Flang and will in time supplant the current AMD Flang
compiler, a downstream flavor of “Classic Flang”.</p><p>Our mission is to allow anyone that is using and developing a Fortran HPC codebase
to directly leverage the power of AMD’s GPUs. AMD’s Next-Gen Fortran Compiler’s goal
is fulfilling our vision by allowing you to deploy and accelerate your Fortran codes
on AMD GPUs using OpenMP offloading, and to directly interface and invoke HIP and
ROCm kernels.</p></blockquote><ul><li>AMD, <a href="https://rocm.blogs.amd.com/ecosystems-and-partners/fortran-journey/README.html">“Introducing AMD’s Next-Gen Fortran Compiler”</a></li></ul><p>Flang might not be new anymore, but it is definitely still improving. If you
want to try Flang on your own projects, you can
<a href="https://github.com/llvm/llvm-project/releases/tag/llvmorg-20.1.0">download</a>
it right now.</p><p>If you want to contribute, there are many ways to do so. Bug reports,
code contributions, documentation improvements and so on. Flang follows the
<a href="https://llvm.org/docs/Contributing.html">LLVM contribution process</a> and you
can find links to the forums, community calls and anything else you
might need <a href="https://flang.llvm.org/docs/GettingInvolved.html">here</a>.</p><p>Thank you to the following people for their contributions to this article:</p><ul><li>Alex Bradbury (Igalia)</li><li>Andrzej Warzyński (Arm)</li><li>Axel Obermeier (Quansight Labs)</li><li>Brad Richardson (Lawrence Berkeley National Laboratory)</li><li>Carlos Seo (Linaro)</li><li>Daniel C Chen (IBM)</li><li>Eric Schweitz (NVIDIA)</li><li>Hao Jin</li><li>Jeff Hammond (NVIDIA)</li><li>Kiran Chandramohan (Arm)</li><li>Leandro Lupori (Linaro)</li><li>Luis Machado (Arm)</li><li>Mehdi Amini</li><li>Pat McCormick (Los Alamos National Laboratory)</li><li>Peter Waller (Arm)</li><li>Steve Scalpone (NVIDIA)</li><li>Tarun Prabhu (Los Alamos National Laboratory)</li></ul><ul><li><a href="https://fortran-lang.org/learn/">Learn Fortran</a></li><li><a href="https://labs.quansight.org/blog/building-scipy-with-flang">The ’eu’ in eucatastrophe – Why SciPy builds for Python 3.12 on Windows are a minor miracle</a></li><li><a href="https://ondrejcertik.com/blog/2021/03/resurrecting-fortran/">Resurrecting Fortran</a></li><li><a href="https://everythingfunctional.wordpress.com/2021/03/12/the-fortran-package-managers-first-birthday/">The Fortran Package Manager’s First Birthday</a></li><li><a href="https://www.youtube.com/watch?v=OvTiKWfhaho">How to write a new compiler driver? The LLVM Flang perspective</a></li><li><a href="https://www.exascaleproject.org/research-project/flang/">Flang in the Exascale Supercomputing Project</a></li></ul></article></div>
  </body>
</html>
