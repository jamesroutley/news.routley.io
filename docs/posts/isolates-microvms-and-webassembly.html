<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://notes.crmarsh.com/isolates-microvms-and-webassembly">Original</a>
    <h1>Isolates, microVMs, and WebAssembly</h1>
    
    <div id="readability-page-1" class="page"><div><article id="block-isolates-microvms-and-webassembly"><p><span><span>Iâ€™ve been thinking about WebAssembly a lot lately.</span></span></p><p><span><span>Iâ€™m mostly focused on two lines of inquiry. First: Whatâ€™s the </span><span><em>promise</em></span><span> of WebAssembly? </span><span><em>Why</em></span><span> is it exciting? What makes it </span><span><em>different</em></span><span>? (I get into some of this in </span><span><a href="https://notes.crmarsh.com/whats-webassembly"><em>Whatâ€™s WebAssembly?</em></a></span><span>) And second: Whatâ€™s the state of WebAssembly </span><span><em>today</em></span><span>?</span></span></p><p><span><span>For me, answering both of those questions required an exploration of Serverless more broadly, as WebAssembly could both compete with and complement emergent technologies like V8 Isolates (which powers Cloudflare Workers) and microVMs (like Firecracker, which powers AWS Lambda and Fly.io).</span></span></p><p><span><span><em>N.B. These are public notes and not intended as an opinionated essay. Feel free to </em></span><span><a href="https://twitter.com/charliermarsh" target="_blank" rel="noopener noreferrer"><em>DM me</em></a></span><span><em> any corrections.</em></span></span></p><h2 id="block-679b7286418d490a9e859ed3da8bf676"><span id="679b7286418d490a9e859ed3da8bf676"></span><span><span>Containers</span></span></h2><p><span><span>Imagine youâ€™re me (very naive), and youâ€™re building a Serverless [1] platform (like </span><span><a href="https://cloud.google.com/run" target="_blank" rel="noopener noreferrer">Cloud Run</a></span><span>) based on Docker containers. The idea: users give you Docker containers, and you provide them with HTTP endpoints in return. When a request comes in, you route it to one of </span><span><em>your</em></span><span> servers, which routes it to a container (spinning it up if needed), which services the request.</span></span></p><div id="block-65b354cff36e4182bb8cb5eec5449a4e"><p><span><span>This illustrates some of the problem-solving required to deliver a Serverless solution (taken from </span><span><a href="https://matt-rickard.com/isolates-and-containers" target="_blank" rel="noopener noreferrer">Matt Rickardâ€™s excellent write-up</a></span><span>):</span></span></p></div><ol type="1"><li id="block-0b3a604485bc430fb8a5d0f1a44f8d54"><span><span>ğŸ“¦Â </span><span><strong>Packaging:</strong></span><span> How do users give you their code? (In the above example, we chose to use Docker containers, but there are other options.)</span></span></li><li id="block-7a4fd3218c0d4085a59ca6c92bbad75f"><span><span>ğŸ§Š </span><span><strong>Cold starts:</strong></span><span> When your server receives a request, it needs to download and run the Docker image. How do you avoid lengthy delays for those initial requests? For future requests? (Do you keep that container running forever?)</span></span></li><li id="block-b3bd16ff60bb4b00a4a8a5db58d8f973"><span><span>ğŸ”’ </span><span><strong>Security boundaries:</strong></span><span> Not only are you running untrusted, user-provided code, but in order to make Serverless â€˜workâ€™, you need to be able to run code from many users on the same physical machine. Docker provides some isolation, which is good news for usâ€¦ but itâ€™s considered unsafe to run multiple Docker containers with the default configuration in a multi-tenant scenario (</span><span><a href="https://news.ycombinator.com/item?id=25883253" target="_blank" rel="noopener noreferrer">source</a></span><span>), which is, of course, bad news for us.</span></span></li><li id="block-bc044624fb9046db920de7e0579fdd1b"><span><span>ğŸ“ˆ </span><span><strong>Efficiency:</strong></span><span> How do you make the system as efficient as possible? You might have many, many containers running on each of your machines. How do you enable your system to run as many of those containers, as cheaply as possible? (Unlike the others, this is largely a concern of the </span><span><em>platform provider</em></span><span> as opposed to the user, though of course it will ultimately impact pricing.)</span></span></li></ol><p><span><span>There are lots of tradeoffs here. For example, if youâ€™re willing to keep the userâ€™s container running at all times, that will help with cold starts, at the cost of efficiency (resource limits). But with this lens, the primary upside of our system is packaging, as users can give us any Docker container, and we can run it.</span></span></p><p><span><span>The first downside (at least, as described above) is security. If we want a secure multi-tenant system, we canâ€™t run Docker directly. </span><span><a href="https://cloud.google.com/kubernetes-engine" target="_blank" rel="noopener noreferrer">GKE</a></span><span>, for example, uses a technology called gVisor, which effectively emulates Linux in Go to create a </span><span><a href="https://mobycast.fm/the-future-of-containers-whats-next/" target="_blank" rel="noopener noreferrer">â€œkernel sandboxâ€</a></span><span>, and thus avoids containers hitting the kernel directly (</span><span><a href="https://fly.io/blog/sandboxing-and-workload-isolation/" target="_blank" rel="noopener noreferrer">source</a></span><span>). So maybe weâ€™d have to do something like that.</span></span></p><p><span><span>The second downside is low resource efficiency (in a relative, fuzzy sense). Containers are typically seen as more efficient than virtual machines, since they can share a common operating system kernel, which reduces the resources required to run a bunch of them at once and greatly decreases startup time. In a multi-tenant environment, though, we need to enforce isolation between containers â€” so we have to use something like the gVisor system described above (or: </span><span><em><a href="https://matt-rickard.com/different-types-of-containers" target="_blank" rel="noopener noreferrer">Different Types of Software Containers</a></em></span><span>), which will cause a â€œa low-double-digits percentage hitâ€ in performance vis-Ã -vis that ideal (</span><span><a href="https://fly.io/blog/sandboxing-and-workload-isolation/" target="_blank" rel="noopener noreferrer">source</a></span><span>). Also: compared to a small JavaScript payload (as in V8 Isolates, below), weâ€™re asking users to give us an entire root filesystem by way of a container, which means weâ€™re moving around and storing more, bigger files.</span></span></p><p><span><span>We </span><span><em>may</em></span><span> also suffer from cold start times. </span><span><a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda</a></span><span>, for example, can only handle a single request at a time, so a new Lambda has to be cold-started whenever you get concurrent requests. As Lambda receives more and more requests, it has to create more of these containerized processes, which is expensive (compared to, e.g., Isolates, described below).</span></span></p><h2 id="block-0a251460870f4a9c8699de80aefda081"><span id="0a251460870f4a9c8699de80aefda081"></span><span><span>Isolates (Cloudflare Workers)</span></span></h2><p><span><span>Cloudflare Workers takes a different approach to Serverless.</span></span></p><p><span><span>Thereâ€™s no Docker, and no Containers. Instead, they leverage a technology called </span><span><a href="https://blog.cloudflare.com/cloud-computing-without-containers/" target="_blank" rel="noopener noreferrer">V8 Isolates</a></span><span>, originally designed to power JavaScript in the browser. Isolates are â€œlightweight contexts that group variables with the code allowed to mutate themâ€.</span></span></p><p><span><span>You can run thousands of Isolates in a single process. You spin up one JavaScript runtime, and then have essentially no overhead for startup time or memory consumption as you run more and more Isolates. Cold starts are non-existent, and the system itself isÂ extremely efficient. You can run this system on the edge in part because all it takes is a small amount of user code (JavaScript) â€” V8 already has the standard library and everything that user code needs to be useful.</span></span></p><p><span><span>The downsides of Isolates are:</span></span></p><ol type="1"><li id="block-e05f53b8d2ab4a03a6e6ca7e0e9a9d2d"><span><span>ğŸ”’Â </span><span><strong>Security:</strong></span><span> &#34;v8 has a large and complicated attack surfaceâ€ (</span><span><a href="https://matt-rickard.com/isolates-and-containers" target="_blank" rel="noopener noreferrer">source</a></span><span>). Kurt Mackey from </span><span><a href="http://Fly.io" target="_blank" rel="noopener noreferrer">Fly.io</a></span><span> argues that â€œUnder no circumstances should CloudFlare or anyone else be running multiple isolates in the same OS process. They need to be sandboxed in isolated processesâ€ (</span><span><a href="https://news.ycombinator.com/item?id=31740885" target="_blank" rel="noopener noreferrer">source</a></span><span>). In response, Kenton Varda (the tech lead for Cloudflare Workers) claims that thereâ€™s plenty they can do instead of process isolation, and that imposing strict process isolation â€œwould mean an order of magnitude more overhead, in terms of CPU and memory usageâ€ (</span><span><a href="https://news.ycombinator.com/item?id=31740885" target="_blank" rel="noopener noreferrer">source</a></span><span>). So, thereâ€™s at least some debate here around the level of isolation and hardening required to run V8 Isolates in this way.</span></span></li><li id="block-d69987c458e24d5da10004c11ed73da1"><span><span>ğŸ“¦Â </span><span><strong>Packaging:</strong></span><span> Originally, those running V8 Isolates could only really support JavaScript / TypeScript. This is changing and is part of why WebAssembly is exciting: V8 can run WebAssembly, so any language that can compile to WebAssembly can in theory run on V8 Isolates.</span></span></li></ol><p><span><span>Workers </span><span><em>does</em></span><span> support WebAssembly, but the cold start time is quite high in my experience due to the lack of shared modules (</span><span><a href="https://community.cloudflare.com/t/fixed-cloudflare-workers-slow-with-moderate-sized-webassembly-bindings/184668/12" target="_blank" rel="noopener noreferrer">source</a></span><span>). For Ruff (linked above), I see a cold start time of ~1.5 seconds on Workers; warm requests take &lt; 100 ms.</span></span></p><h2 id="block-2423c2af9005496bb1e8951b2e546082"><span id="2423c2af9005496bb1e8951b2e546082"></span><span><span>Firecracker / microVMs (Fly.io)</span></span></h2><p><span><span><a href="http://Fly.io" target="_blank" rel="noopener noreferrer">Fly.io</a></span><span> (and AWS Lambda) takes yet another, different approach.</span></span></p><p><span><span>With Fly.io, you </span><span><em>do</em></span><span> package your code with Docker, but they </span><span><em>donâ€™t</em></span><span> run it in a container. Instead, they use a technology called Firecracker to create â€œmicroVMsâ€. These microVMs are very efficient: â€œFirecracker can fit thousands of micro-VMs on a single server, paying less than 5MB per instance in memoryâ€ (</span><span><a href="https://fly.io/blog/the-serverless-server/" target="_blank" rel="noopener noreferrer">source</a></span><span>). So when a request comes in, they route it to your VM, and run your code on the VM directly.</span></span></p><p><span><span>The upside, from the userâ€™s perspective, is that packaging is much easier: you can ship anything, not just JavaScript (according to Kurt Mackey, </span><span><a href="http://Fly.io" target="_blank" rel="noopener noreferrer">Fly.io</a></span><span> started with a JavaScript runtime, like Cloudflare, but found that customers were better off â€œjust running Docker imagesâ€ (</span><span><a href="https://news.ycombinator.com/item?id=23966136" target="_blank" rel="noopener noreferrer">source</a></span><span>)). Itâ€™s also (arguably) more secure than Isolates (</span><span><a href="https://news.ycombinator.com/item?id=26629420" target="_blank" rel="noopener noreferrer">source</a></span><span>), while still being very fast. Kurt Mackey argues that if youâ€™re forking a process for every Isolate (he thinks you should), you might as well run Firecracker, which enables you to run a much wider range of applications (</span><span><a href="https://news.ycombinator.com/item?id=31740885" target="_blank" rel="noopener noreferrer">source</a></span><span>).</span></span></p><p><span><span>Flyâ€™s bet is that they can â€œmake launching VMs as quick as launching containers while retaining the great isolation benefits of a VMâ€ (</span><span><a href="https://news.ycombinator.com/item?id=25883253" target="_blank" rel="noopener noreferrer">source</a></span><span>). (They couple this with running your app in multiple locations, rather than a traditional single region datacenter.)</span></span></p><p><span><span>Note that </span><span><a href="http://Fly.io" target="_blank" rel="noopener noreferrer">Fly.io</a></span><span> is more of aâ€¦ PaaS? Itâ€™s priced based on the number and size of your VMs, so itâ€™s â€œmore like Fargate than Lambdaâ€ (</span><span><a href="https://news.ycombinator.com/item?id=23966136" target="_blank" rel="noopener noreferrer">source</a></span><span>). It does autoscale, but they donâ€™t scale down to zero. It has a different set of tradeoffs than Cloudflare Workers, which take your code and run it for you at â€˜anyâ€™ scale.</span></span></p><h2 id="block-105a3aa6eec24212b77f5a46864d515f"><span id="105a3aa6eec24212b77f5a46864d515f"></span><span><span>WebAssembly</span></span></h2><p><span><span>Given this context, where does WebAssembly fit in? And how does it align with the criteria we set out at the start?</span></span></p><p><span><span>WebAssembly is designed to be very fast, very portable, and very secure (</span><span><a href="https://bytecodealliance.org/articles/wasmtime-1-0-fast-safe-and-production-ready" target="_blank" rel="noopener noreferrer">source</a></span><span>). With WebAssembly, you can compile a variety of languages down to WebAssemblyâ€™s intermediate representation, then run that compiled code on any WebAssembly runtime, anywhere, in a highly sandboxed way, at near-native speeds.</span></span></p><p><span><span>I donâ€™t have great intuition for whether WebAssembly can be </span><span><em>as</em></span><span> fast and lightweight as â€˜JavaScript on V8 Isolatesâ€™, but one goal could be: </span><span><strong>a </strong></span><span><strong><a href="https://twitter.com/bernhardsson/status/1574096015141048321" target="_blank" rel="noopener noreferrer">â€œmuch faster runtime than VMsâ€</a></strong></span><span><strong>, but with the same security guarantees, and similarly broad language support.</strong></span><span> Something between Isolates and microVMs, maybe? Fast, lightweight, portable, secure, etc.</span></span></p><p><span><span>I could imagine a Serverless platform based on WebAssembly (but not necessarily on V8 Isolates). That platform would spin up a bunch of servers, each armed with a WebAssembly runtime like </span><span><a href="https://wasmtime.dev/" target="_blank" rel="noopener noreferrer">Wasmtime</a></span><span> or </span><span><a href="https://wasmedge.org/" target="_blank" rel="noopener noreferrer">WasmEdge</a></span><span> or </span><span><a href="https://wasmer.io/" target="_blank" rel="noopener noreferrer">Wasmer</a></span><span> (there are a lot of runtimes), then run usersâ€™ WebAssembly binaries directly on host machines â€” no need for containers, no need for VMs, no need for microVMs.</span></span></p><p><span><span>In theory, this could be super efficient. </span><span><a href="https://www.fermyon.com/" target="_blank" rel="noopener noreferrer">Fermyon</a></span><span>, for example, â€œhas found tens of thousands of WebAssembly binaries can run in a single Spin instance while keeping startup times under a millisecondâ€ (</span><span><a href="https://bytecodealliance.org/articles/wasmtime-1-0-fast-safe-and-production-ready" target="_blank" rel="noopener noreferrer">source</a></span><span>).</span></span></p><p><span><span>This platform could have other benefits too. For example, you could seamlessly mix and match code written in multiple languages, each of which compile to WebAssembly. Further, the only demand on the user would be that they provide a WebAssembly module â€” they donâ€™t have to construct a Docker Image or otherwise package their code (though some might view this as a weakness, not a strength).</span></span></p><h2 id="block-db90951fc2c749b9968257b08895f418"><span id="db90951fc2c749b9968257b08895f418"></span><span><span>WebAssembly in 2022</span></span></h2><p><span><span>In exploring this idea, and itâ€™s applicability to shipping software </span><span><em>today</em></span><span>, Iâ€™ve run into a few problems:</span></span></p><ul><li id="block-5048705e5de4436581e73c1111edb13c"><span><span>Thereâ€™s no standardized component model for WebAssembly right now, so in order to run on any sort of hosted WebAssembly platform, you have to implement module-side code (</span><span><a href="https://twitter.com/paulgb/status/1574032413725859840" target="_blank" rel="noopener noreferrer">source</a></span><span>). You see this with Fermyon, where you have to write your code using their </span><span><a href="https://github.com/fermyon/spin" target="_blank" rel="noopener noreferrer">Spin</a></span><span> framework. You also see this with Fastlyâ€™s <a href="https://notes.crmarsh.com/cdn-cgi/l/email-protection" data-cfemail="7c3f13110c0908193c39181b19">[emailÂ protected]</a>, where you have to implement </span><span><code>fastly_http_req</code></span><span>Â and </span><span><code>fastly_http_body</code></span><span> to make your WebAssembly code compatible with their interface (</span><span><a href="https://developer.fastly.com/learning/compute/custom/" target="_blank" rel="noopener noreferrer">source</a></span><span>). Maybe this is fine, I just expected something more generic.</span></span></li><li id="block-134710e3da094676b873a805f0f7ec1a"><span><span>Compared to running JavaScript on V8 Isolates, WebAssembly binaries are quite large (though compared to Docker Images, theyâ€™re very small). Deploying WebAssembly on Cloudflare Workers is </span><span><em>really</em></span><span> slick (see </span><span><a href="https://blog.cloudflare.com/announcing-wasi-on-workers/" target="_blank" rel="noopener noreferrer">this primer</a></span><span>, or my </span><span><a href="https://twitter.com/charliermarsh/status/1573726744158371842" target="_blank" rel="noopener noreferrer">example</a></span><span>), but (1) they have a 1MB limit on the size of the binary, and (2) cold starts can take ~1-2 seconds. (Fastlyâ€™s <a href="https://notes.crmarsh.com/cdn-cgi/l/email-protection" data-cfemail="1a5975776a6f6e7f5a5f7e7d7f">[emailÂ protected]</a> doesnâ€™t seem to suffer from these limitations.) The main issue seems to be that thereâ€™s no concept of shared modules in the WebAssembly world right now, so every binary has to ship with its languageâ€™s standard library. Kenton Varda, the tech lead for Cloudflare Workers, talks about it </span><span><a href="https://community.cloudflare.com/t/fixed-cloudflare-workers-slow-with-moderate-sized-webassembly-bindings/184668/12" target="_blank" rel="noopener noreferrer">here</a></span><span> (this comment is also interesting in that it demonstrates some hesitancy from Cloudflare to prioritize Wasm):</span></span></li></ul><p><span><span><img alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27693%27%20height=%27133%27/%3e"/></span><img alt="image" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic"/></span></p><ul><li id="block-7d4d54fe3f2a43ec8dc381e467b85056"><span><span>The early WebAssembly-native companies, like </span><span><a href="https://www.fermyon.com/" target="_blank" rel="noopener noreferrer">Fermyon</a></span><span>, donâ€™t yet have hosted solutions: </span><span><a href="https://github.com/fermyon/spin" target="_blank" rel="noopener noreferrer">Fermyonâ€™s Spin</a></span><span> can only be self-hosted right now (</span><span><a href="https://spin.fermyon.dev/deploying-to-fermyon/" target="_blank" rel="noopener noreferrer">source</a></span><span>). </span><span><a href="https://docs.suborbital.dev/compute/" target="_blank" rel="noopener noreferrer">Suborbitalâ€™s</a></span><span> product is focused on infrastructure for running user-defined code rather than as a generic Serverless solution. (Note: this is often cited as a â€˜killer use-caseâ€™ for WebAssembly. As a concrete example: rather than using webhooks, and calling out a userâ€™s HTTP endpoint whenever an event occurs, the user could just </span><span><em>give you code</em></span><span> that </span><span><em>you run directly</em></span><span> â€” made safe by WebAssemblyâ€™s sandboxing</span><span><em>.</em></span><span> But you can also do this with V8 Isolates.) Most other companies (like </span><span><a href="https://wasmer.io/" target="_blank" rel="noopener noreferrer">Wasmer</a></span><span>) seem to be focused on runtimes. It may just be too early!</span></span></li><li id="block-e3f83d756ece44eeb5f62cf9969e41f5"><span><span>There are a bunch of limitations to what you can </span><span><em>do</em></span><span> with WebAssembly right now. For Example, if youâ€™re using </span><span><a href="https://wasi.dev/" target="_blank" rel="noopener noreferrer">WASI</a></span><span> (the standardized WebAssembly system interface), thereâ€™s no way to make HTTP requests (</span><span><a href="https://github.com/bytecodealliance/wasmtime/issues/163" target="_blank" rel="noopener noreferrer">source</a></span><span>). It will be possible once the standard has been defined and implemented, but today, youâ€™re out of luck. (</span><span><a href="https://www.fermyon.com/blog/spin-webhooks" target="_blank" rel="noopener noreferrer">Spin</a></span><span> does support HTTP requests.)</span></span></li><li id="block-e40a7f9b4b18425e9f748905867e7e7e"><span><span>Today, you can package a WebAssembly module into a container and deploy it wherever you like, butâ€¦ why do that? Like with Firecracker, I think you need to run on bare metal to fulfill the promise of WebAssembly. Do we need a </span><span><a href="http://Fly.io" target="_blank" rel="noopener noreferrer">Fly.io</a></span><span> for WebAssembly? (That might end up being Fastly, or Cloudflare, or Fly.io, or another existing platform.)</span></span></li></ul><p><span><span>The service I want doesnâ€™t seem to exist (yet). What I want is a </span><span><em>hosted</em></span><span> service that lets meâ€¦</span></span></p><ul><li id="block-d4c576d5465145ef9e4ba9c44b1df574"><span><span>Compile my code into a </span><span><a href="https://wasi.dev/" target="_blank" rel="noopener noreferrer">WASI</a></span><span>-complaint WebAssembly binaryâ€¦</span></span></li><li id="block-f162b93afe144670933d31b547fe6425"><span><span>Run a single command to create an HTTP endpoint that runs the binary (piping POST body to stdin and returning stdout, like Cloudflare Workers)â€¦</span></span></li><li id="block-f1ff9fd0fde34f7bbd585716af2b44b2"><span><span>Reap all of the performance, latency, scalability, and security benefits that WebAssembly promisesâ€¦</span></span></li><li id="block-7b82eeb43504492cb09d1e70a43c857e"><span><span>Pay via a true serverless, usage-based pricing modelâ€¦</span></span></li></ul><p><span><span>This would be something like Cloudflare Workers, but without the 1MB code size limit, and without the expensive cold starts. Something thatâ€™s â€˜WebAssembly-nativeâ€™. The best solution Iâ€™ve found is Fastlyâ€™s <a href="https://notes.crmarsh.com/cdn-cgi/l/email-protection" data-cfemail="23604c4e535657466366474446">[emailÂ protected]</a>, with the one tradeoff being that it requires writing code atop Fastlyâ€™s web framework (or, at least, implementing a specific module interface). Fermyon also seems well-positioned to offer this, and I suspect itâ€™s their goal to provide a fully managed solution eventually.</span></span></p><p><span><span>[1] Serverless is a weird term, since on GCP, all of AppEngine, Cloud Run, and Cloud Functions could be considered serverless (</span><span><a href="https://www.datadoghq.com/state-of-serverless/" target="_blank" rel="noopener noreferrer">source</a></span><span>), despite serving very different use-cases and architectures. I like </span><span><a href="https://twitter.com/bernhardsson/status/1574095253191245824" target="_blank" rel="noopener noreferrer">Erikâ€™s claim</a></span><span> that serverless demands usage-based pricing.</span></span></p></article></div></div>
  </body>
</html>
