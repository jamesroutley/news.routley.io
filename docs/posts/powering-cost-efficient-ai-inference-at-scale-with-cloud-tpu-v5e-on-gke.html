<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cloud.google.com/blog/products/containers-kubernetes/cost-efficient-ai-inference-with-cloud-tpu-v5e-on-gke">Original</a>
    <h1>Powering cost-efficient AI inference at scale with Cloud TPU v5e on GKE</h1>
    
    <div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Google <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-in-ga">Cloud TPU v5e</a> is a purpose-built AI accelerator that brings the cost-efficiency and performance required for large-scale model training and inference. With Cloud TPUs on Google Kubernetes Engine (GKE), the leading Kubernetes service in the industry, customers can orchestrate AI workloads efficiently and cost effectively with best-in-class training and inference capabilities. GKE has long been a leader in supporting GPUs for AI workloads and we are excited to expand our support to include TPU v5e for large-scale inference capabilities.</p><h3>MLPerf™ 3.1 results</h3><p><a href="https://cloud.google.com/blog/products/compute/performance-per-dollar-of-gpus-and-tpus-for-ai-inference">As we announced in September</a>, Google Cloud submitted results for the MLPerf™ Inference 3.1 benchmark and achieved 2.7x higher performance per dollar compared to TPU v4.</p><p>Our <a href="https://mlcommons.org/en/inference-datacenter-31/" target="_blank">MLPerf™ Inference 3.1 submission results</a> demonstrated running the 6-billion parameter <a href="https://huggingface.co/EleutherAI/gpt-j-6b" target="_blank">GPT-J LLM</a> benchmark using <a href="https://github.com/google/saxml" target="_blank">Saxml</a>, a high-performance inference system, and <a href="https://www.tensorflow.org/xla/architecture" target="_blank">XLA</a>, Google’s AI compiler. Some of the key optimizations used include:</p><ul><li><a href="https://www.tensorflow.org/xla/architecture" target="_blank">XLA</a> optimizations and fusions of Transformer operators</li><li><a href="https://github.com/google/praxis/tree/main/praxis/layers/quantization" target="_blank">Post-training weight quantization</a> with INT8 precision</li><li>High-performance sharding across the 2x2 TPU node pool topology using <a href="https://blog.research.google/2021/12/general-and-scalable-parallelization.html" target="_blank">GSPMD</a></li><li>Bucketized execution of batches of prefix computation and decoding in <a href="https://github.com/google/saxml" target="_blank">Saxml</a></li><li>Dynamic batching in <a href="https://github.com/google/saxml" target="_blank">Saxml</a></li></ul><p>We achieved the same performance when running Cloud TPU v5e on GKE clusters, demonstrating that Cloud TPUs on GKE allow you to gain the scalability, orchestration, and operational benefits of GKE while maintaining the price-performance of TPU.</p><h3>Maximizing cost-efficiency with GKE and TPUs</h3><p>When building a production-ready, highly-scalable, and fault-tolerant managed application, GKE brings additional value by reducing your total cost of ownership (TCO) for inference on TPUs:</p><ul><li>Manage and deploy your AI workloads with a Kubernetes standard platform.</li><li>Minimize cost with autoscaling to ensure that resources automatically adjust to workload needs. GKE can automatically scale up and down TPU node pools based on traffic using the autoscaling, increasing cost efficiency and improved automation for inference.</li><li>Provision the necessary compute resources needed for your workloads: TPU node pools can be automatically provisioned based on TPU workload requirements with GKE’s <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning">node auto provisioning</a> capabilities.</li><li>Ensure high availability of your applications with built in <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/tpus#metrics">health monitoring for TPU VM node pools</a> on GKE. If TPU nodes become unavailable, GKE will perform <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/tpus#node-auto-repair">node auto repair</a> to avoid disruptions.</li><li>Minimize disruption from updates and hardware failures with GKE’s <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/tpus#node-auto-repair">proactive handling of maintenance events</a> and gracefully terminating workloads.</li><li>Gain full visibility into your TPU applications with GKE&#39;s mature and reliable <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/tpus#metrics">metrics</a> and <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/tpus#logging">logging</a> capabilities</li></ul><h3>GKE TPU Inference Reference Architecture</h3><p>To take advantage of all of the above benefits, we created a proof of concept to demonstrate TPU inference using the GPT-J 6B LLM model with a single-host Saxml model server.</p></span></section><section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Below: Saxml Workflow</p></span></section><section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>We created a GKE cluster with the following architecture:</p><ul><li>We created a GKE cluster with a TPU v5e (2x2) node pool</li><li>We enabled the <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api">Gateway API</a> which provides the ability to expose different HTTP endpoints and provide health checks.</li><li>We developed a simple HTTP server as frontend to Saxml. This server proxied requests from end users to Saxml.</li><li>We developed a Kubernetes deployment that served two containers, the Saxml deployment as well the HTTP server. This ensured that the HTTP server ran as a sidecar and scaled proportionally with Saxml.</li><li>We configured the necessary Gateway API configuration including HTTP route, healthcheck, and a backing k8s load balancer based service for the Deployment.</li><li>Lastly, we added a HorizontalPodAutoscaler which can dynamically scale the number of replicas of the Deployment based on traffic to the load balancer.</li></ul><p>This reference architecture demonstrates how to achieve optimal price-performance for large-scale AI inference when operationalizing TPU v5e through the use of GKE. Refer to the below demo to see the cluster in action!</p><h3>Demo</h3></span></section><section><section><div jsaction="rcuQ6b:npT2md" jscontroller="wJu6E" data-video-url="https://www.youtube.com/watch?v=RjV8v1zIz5Y"><div><picture><section><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/AI_inference_at_scale_with_Cloud_TPU_v5e_o.max-2000x2000_eRKhzXo.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/AI_inference_at_scale_with_Cloud_TPU_v5e_o.max-2000x2000_eRKhzXo.jpg" loading="lazy"/></section></picture></div></div></section></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Refer to <a href="https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/saxml-on-gke/single-host-inference/gptj" target="_blank">our Github</a> for more examples and details around the reference architecture described!</p><p>Try it out and if you have any questions about Saxml on GKE, you can <a href="https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/saxml-on-gke/single-host-inference/gptj" target="_blank">leave us a comment</a>. We look forward to your feedback!</p><p>Learn more about <a href="https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra">AI/ML Workloads on GKE</a>.</p></span></section><section></section><section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/containers-kubernetes" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/containers-kubernetes" track-metadata-module="tag list" track-metadata-module_headline="posted in">Containers &amp; Kubernetes</a></li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/ai-machine-learning" track-metadata-module="tag list" track-metadata-module_headline="posted in">AI &amp; Machine Learning</a></li></ul></section></section></div></div>
  </body>
</html>
