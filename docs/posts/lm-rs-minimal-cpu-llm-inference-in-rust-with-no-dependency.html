<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/samuel-vitorino/lm.rs">Original</a>
    <h1>Lm.rs: Minimal CPU LLM inference in Rust with no dependency</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
    <img alt="lmrs logo" src="https://olu.online/samuel-vitorino/lm.rs/raw/main/repo_cover.svg"/>
</picture></themed-picture>
<p dir="auto">lm.rs: run inference on Language Models locally on the CPU with Rust</p>

</div>
<hr/>
<p dir="auto"><strong>ðŸŒƒ Now supporting multimodality with PHI-3.5-vision model! PHI-3.5-mini text-only model also now supported.</strong></p>
<p dir="auto">Inspired by Karpathy&#39;s <a href="https://github.com/karpathy/llama2.c">llama2.c</a> and <a href="https://github.com/karpathy/llm.c">llm.c</a> I decided to create the most minimal code (not so minimal atm) that can perform full inference on Language Models on the CPU without ML libraries. Previously only Google&#39;s Gemma 2 models were supported, but I decided to add support for the new Llama 3.2 models, and more recently the option to use images with PHI-3.5. Image processing/encoding currently takes a bit, so it slows the first response, working on optimization now.</p>
<p dir="auto">Disclaimer: some of the code could be optimized and improved. This is just an excuse for me to write Rust for the first time. Isn&#39;t it incredible that in a few years, we could have AGI running in a few lines of poorly written Rust code?</p>

<p dir="auto">Some benchmarks and download links for the models and tokenizers. I recommend using Q8_0, Q4_0 quantization still being improved. Speed measured on a 16-core AMD Epyc.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-2b-it-q4_0-LMRS" rel="nofollow">Gemma 2 2B IT Q4_0</a></td>
<td>1.39G</td>
<td>20 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-2b-it-q8_0-LMRS" rel="nofollow">Gemma 2 2B IT Q8_0</a></td>
<td>2.66GB</td>
<td>18 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-9b-it-q4_0-LMRS" rel="nofollow">Gemma 2 9B IT Q4_0</a></td>
<td>4.91GB</td>
<td>7 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-9b-it-q8_0-LMRS" rel="nofollow">Gemma 2 9B IT Q8_0</a></td>
<td>9.53GB</td>
<td>8 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-LMRS" rel="nofollow">Llama 3.2 1B IT</a></td>
<td>4.94GB</td>
<td>20 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-Q8_0-LMRS" rel="nofollow">Llama 3.2 1B IT Q8_0</a></td>
<td>1.27GB</td>
<td>35 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q4_0-LMRS" rel="nofollow">Llama 3.2 3B IT Q4_0</a></td>
<td>1.71GB</td>
<td>17 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q8_0-LMRS" rel="nofollow">Llama 3.2 3B IT Q8_0</a></td>
<td>3.31GB</td>
<td>16 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Phi-3.5-vision-instruct-q8_0-LMRS" rel="nofollow">PHI 3.5 IT Vision Q8_0</a></td>
<td>4.28GB</td>
<td>15 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Phi-3.5-mini-instruct-q8_0-LMRS" rel="nofollow">PHI 3.5 IT Mini Q8_0</a></td>
<td>3.94GB</td>
<td>16 tok/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">You can download the prepared quantized model and tokenizer model files in the lmrs format from huggingface. If you&#39;d prefer to convert the models published by Google/Meta on huggingface yourself, please refer to the following section. Otherwise, you can skip ahead to the build section.</p>

<p dir="auto">Install additional python dependencies (assuming you already have pytorch installed) used in export.py and tokenizer.py:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">Download the <strong>.safetensors</strong> and <strong>config.json</strong> files from the original model&#39;s page on huggingface (So we don&#39;t have to clone the pytorch repo). For multimodal models (PHI3.5 Vision), we also need the CLIP <strong>.config</strong> <a href="https://huggingface.co/openai/clip-vit-large-patch14-336/blob/main/config.json" rel="nofollow">file</a>.</p>
<p dir="auto">Use the export.py script to convert the model bfloat16 weights into the LMRS format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]"><pre>python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]</pre></div>
<p dir="auto">To export the quantized version use the <strong>--quantize</strong> and <strong>--quantize-type</strong> flags. The int8 quantized model size should be 4X smaller (from ~9.8G to ~2.5G, depending on the group size). For multimodal models include the <strong>--vision-config</strong> argument.</p>
<p dir="auto">Use the tokenizer.py script to convert the tokenizer model into the LMRS tokenizer format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]"><pre>python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]</pre></div>

<p dir="auto">Compile the rust code with cargo (make sure to pass the target-cpu flag):</p>
<div dir="auto" data-snippet-clipboard-copy-content="RUSTFLAGS=&#34;-C target-cpu=native&#34; cargo build --release --bin chat"><pre><span>RUSTFLAGS</span>=<span><span>&#34;</span>-C target-cpu=native<span>&#34;</span></span> cargo build --release --bin chat</pre></div>
<p dir="auto">To enable multimodality, include the multimodal feature by passing the <strong>--features multimodal</strong> argument.</p>
<p dir="auto">And you are good to go:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./target/release/chat --model [model weights file]"><pre>./target/release/chat --model [model weights file]</pre></div>
<p dir="auto">Other arguments include tokenizer, temperature, top-p, show-metrics etc. To check available arguments run with --help. For multimodal models use the <strong>--image</strong> argument with the image path.</p>
<hr/>
<p dir="auto">To run the backend for the <a href="https://github.com/samuel-vitorino/lm.rs-webui">WebUI</a>, first compile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="RUSTFLAGS=&#34;-C target-cpu=native&#34; cargo build --release --features backend --bin backend"><pre><span>RUSTFLAGS</span>=<span><span>&#34;</span>-C target-cpu=native<span>&#34;</span></span> cargo build --release --features backend --bin backend</pre></div>
<p dir="auto">For multimodality enable the <strong>backend-multimodal</strong> feature.</p>
<p dir="auto">Then run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./target/release/backend --model [model weights file]"><pre>./target/release/backend --model [model weights file]</pre></div>
<p dir="auto">You can change the ip and port with --ip and --port. Other flags such as temperature, etc. are also available. For multimodal compatibility use the <strong>--multimodal</strong> flag. You can now connect via the web interface.</p>

<p dir="auto">Some things to do in the future:</p>
<ul>
<li> Add other sampling methods.</li>
<li> Test the 9B and 27B models (tested the 9B, 27B would be too slow).</li>
<li> Parallelize the multi head attention loop.</li>
<li> Add performance metrics.</li>
<li> Ability to give a system prompt</li>
<li> Quantization support (int8, int4).</li>
</ul>

<p dir="auto">MIT</p>
</article></div></div>
  </body>
</html>
