<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/xaskasdf/ntransformer">Original</a>
    <h1>Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">High-efficiency C++/CUDA LLM inference engine. Runs Llama 70B on a single RTX 3090 (24GB VRAM) by streaming model layers through GPU memory via PCIe, with optional NVMe direct I/O that bypasses the CPU entirely.</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Mode</th>
<th>Decode</th>
<th>VRAM</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama 3.1 8B Q8_0</td>
<td>Resident</td>
<td>48.9 tok/s</td>
<td>10.0 GB</td>
<td>All layers in VRAM</td>
</tr>
<tr>
<td>Llama 3.1 8B Q8_0</td>
<td>Tiered (auto)</td>
<td>48.8 tok/s</td>
<td>10.3 GB</td>
<td>32/32 layers auto-promoted to VRAM</td>
</tr>
<tr>
<td>Llama 3.1 70B Q6_K</td>
<td>Streaming (mmap)</td>
<td>0.006 tok/s</td>
<td>7.3 GB</td>
<td>Page cache thrashing (53 GB &gt; 48 GB RAM)</td>
</tr>
<tr>
<td>Llama 3.1 70B Q6_K</td>
<td>Tiered (auto)</td>
<td>0.2 tok/s</td>
<td>23.1 GB</td>
<td>26 VRAM + 54 RAM + 0 NVMe</td>
</tr>
<tr>
<td>Llama 3.1 70B Q4_K_M</td>
<td>Tiered (auto)</td>
<td>0.3 tok/s</td>
<td>22.9 GB</td>
<td>36 VRAM + 44 RAM (50% faster)</td>
</tr>
<tr>
<td>Llama 3.1 70B Q4_K_M</td>
<td><strong>Tiered + layer skip</strong></td>
<td><strong>0.5 tok/s</strong></td>
<td><strong>22.9 GB</strong></td>
<td><strong>36 VRAM + 44 RAM, 20 layers skipped</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>3-tier adaptive caching</strong> auto-sizes from hardware: VRAM-resident layers (zero I/O) + pinned RAM (H2D only) + NVMe/mmap fallback. Achieves <strong>83x speedup</strong> over mmap baseline for 70B on consumer hardware (RTX 3090 + 48 GB RAM).</p>
<p dir="auto">Bottleneck is PCIe H2D bandwidth at Gen3 x8 (~6.5 GB/s). Q4_K_M fits 10 more layers in VRAM (36 vs 26), reducing tier B transfers. Layer skip (cosine similarity calibration) eliminates 20/80 layers per token with minimal quality loss.</p>

<ul dir="auto">
<li><strong>Zero external dependencies</strong> beyond CUDA Toolkit (no PyTorch, no cuBLAS)</li>
<li><strong>GGUF model format</strong> with Q4_0, Q8_0, Q4_K_M, Q5_K, Q6_K, F16, F32 quantization</li>
<li><strong>3-Tier Adaptive Caching</strong>: auto-sized VRAM resident + pinned RAM + NVMe/mmap tiers</li>
<li><strong>SLEP streaming</strong>: double-buffered layer pipeline overlaps NVMe reads, PCIe DMA, and GPU compute</li>
<li><strong>gpu-nvme-direct backend</strong>: userspace NVMe driver reads model weights directly to pinned GPU-accessible memory</li>
<li><strong>Layer skip</strong>: cosine-similarity calibration skips redundant layers (20/80 skipped at threshold 0.98)</li>
<li><strong>Self-speculative decoding</strong>: VRAM-resident layers as draft model (no extra model needed)</li>
<li><strong>Four data paths</strong> (auto-selected): VRAM resident &gt; pinned RAM H2D &gt; mmap pinned &gt; CPU worker memcpy</li>
<li>Llama architecture: RoPE, GQA, SwiGLU, RMSNorm, KV cache</li>
</ul>

<ul dir="auto">
<li>Linux (tested on Ubuntu, kernel 6.17+)</li>
<li>CUDA Toolkit 13.1</li>
<li>gcc-14 / g++-14</li>
<li>NVIDIA GPU with Compute Capability 8.0+ (RTX 3090 tested)</li>
<li>CMake 3.24+</li>
<li>(Optional) NVMe SSD on separate PCIe slot + <a href="https://github.com/xaskasdf/gpu-nvme-direct">gpu-nvme-direct</a> library</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="# Build
mkdir build &amp;&amp; cd build
cmake .. -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_C_COMPILER=gcc-14 \
  -DCMAKE_CXX_COMPILER=g++-14 \
  -DCMAKE_CUDA_COMPILER=/usr/local/cuda-13.1/bin/nvcc
cmake --build . -j

# Run (resident mode — model fits in VRAM)
./ntransformer -m /path/to/llama-8b-q8_0.gguf -p &#34;Hello&#34; -n 128

# Run (streaming mode — model larger than VRAM)
./ntransformer -m /path/to/llama-70b-q6_k.gguf -p &#34;Hello&#34; -n 32 --streaming

# Run with layer skip (fastest for 70B)
./ntransformer -m /path/to/llama-70b-q4_k_m.gguf -p &#34;Hello&#34; -n 32 --streaming --skip-threshold 0.98

# Self-speculative decoding (VRAM layers as draft, no extra model)
./ntransformer -m /path/to/llama-70b-q6_k.gguf -p &#34;Hello&#34; -n 32 --self-spec --draft-k 3

# Chat mode
./ntransformer -m /path/to/model.gguf --chat

# Benchmark
./ntransformer -m /path/to/model.gguf --benchmark -n 64"><pre><span><span>#</span> Build</span>
mkdir build <span>&amp;&amp;</span> <span>cd</span> build
cmake .. -DCMAKE_BUILD_TYPE=Release \
  -DCMAKE_C_COMPILER=gcc-14 \
  -DCMAKE_CXX_COMPILER=g++-14 \
  -DCMAKE_CUDA_COMPILER=/usr/local/cuda-13.1/bin/nvcc
cmake --build <span>.</span> -j

<span><span>#</span> Run (resident mode — model fits in VRAM)</span>
./ntransformer -m /path/to/llama-8b-q8_0.gguf -p <span><span>&#34;</span>Hello<span>&#34;</span></span> -n 128

<span><span>#</span> Run (streaming mode — model larger than VRAM)</span>
./ntransformer -m /path/to/llama-70b-q6_k.gguf -p <span><span>&#34;</span>Hello<span>&#34;</span></span> -n 32 --streaming

<span><span>#</span> Run with layer skip (fastest for 70B)</span>
./ntransformer -m /path/to/llama-70b-q4_k_m.gguf -p <span><span>&#34;</span>Hello<span>&#34;</span></span> -n 32 --streaming --skip-threshold 0.98

<span><span>#</span> Self-speculative decoding (VRAM layers as draft, no extra model)</span>
./ntransformer -m /path/to/llama-70b-q6_k.gguf -p <span><span>&#34;</span>Hello<span>&#34;</span></span> -n 32 --self-spec --draft-k 3

<span><span>#</span> Chat mode</span>
./ntransformer -m /path/to/model.gguf --chat

<span><span>#</span> Benchmark</span>
./ntransformer -m /path/to/model.gguf --benchmark -n 64</pre></div>

<p dir="auto">Running ntransformer with NVMe direct I/O requires system-level modifications. An automated setup script handles all of them:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Full first-time setup (interactive, creates backups)
sudo ./scripts/setup_system.sh

# Check current system state (no changes)
sudo ./scripts/setup_system.sh --check

# NVMe-only (run after every reboot)
sudo ./scripts/setup_system.sh --nvme-only"><pre><span><span>#</span> Full first-time setup (interactive, creates backups)</span>
sudo ./scripts/setup_system.sh

<span><span>#</span> Check current system state (no changes)</span>
sudo ./scripts/setup_system.sh --check

<span><span>#</span> NVMe-only (run after every reboot)</span>
sudo ./scripts/setup_system.sh --nvme-only</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">What the script modifies and why</h3><a id="user-content-what-the-script-modifies-and-why" aria-label="Permalink: What the script modifies and why" href="#what-the-script-modifies-and-why"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Phase</th>
<th>What</th>
<th>Why</th>
<th>Risk</th>
<th>Rollback</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Installs gcc-14, cmake, kernel headers</td>
<td>CUDA 13.1 is incompatible with gcc-15 (Ubuntu 25.10 default)</td>
<td>Low — standard packages</td>
<td><code>apt remove</code></td>
</tr>
<tr>
<td>2</td>
<td>Adds <code>amd_iommu=off</code> to GRUB</td>
<td>AMD root complex drops GPU→NVMe P2P reads if IOMMU is on. Disabling IOMMU lets posted PCIe writes (doorbells) through</td>
<td><strong>Medium</strong> — removes hardware DMA isolation between all PCIe devices. Don&#39;t run on multi-tenant/server systems</td>
<td>Remove <code>amd_iommu=off</code> from <code>/etc/default/grub</code>, run <code>update-grub</code>, reboot</td>
</tr>
<tr>
<td>3</td>
<td>Patches NVIDIA DKMS (<code>os-mlock.c</code>)</td>
<td><code>follow_pfn()</code> was removed in kernel 6.12+. Without the patch, <code>cudaHostRegisterIoMemory</code> fails and the GPU can&#39;t map NVMe BAR0 for MMIO writes</td>
<td><strong>High</strong> — bad patch prevents GPU driver from loading (black screen on reboot). Backup <code>.orig</code> created automatically</td>
<td><code>cp os-mlock.c.orig os-mlock.c</code> in DKMS source dir, <code>dkms remove/install nvidia/VERSION</code></td>
</tr>
<tr>
<td>3b</td>
<td>Patches CUDA header (<code>math_functions.h</code>)</td>
<td>glibc 2.42+ (Ubuntu 25.10) declares <code>rsqrt()</code>/<code>rsqrtf()</code> with <code>noexcept</code>. CUDA 13.1 declares without, causing build failure</td>
<td>Low — only affects one header, backup created</td>
<td><code>cp math_functions.h.orig math_functions.h</code></td>
</tr>
<tr>
<td>4</td>
<td>Loads VFIO modules (<code>vfio</code>, <code>vfio-pci</code>)</td>
<td>NVMe must be bound to VFIO for userspace access. Consumer GPUs (GeForce) require <code>enable_unsafe_noiommu_mode=1</code></td>
<td>Low — modules unload on reboot. &#34;Unsafe noiommu&#34; means no IOMMU DMA protection for VFIO devices</td>
<td>Reboot (or <code>modprobe -r vfio-pci vfio</code>)</td>
</tr>
<tr>
<td>5</td>
<td>Unbinds NVMe from kernel, binds to VFIO</td>
<td>gpu-nvme-direct needs raw PCIe access. The NVMe disappears from <code>/dev/</code> while bound to VFIO</td>
<td><strong>High if wrong device</strong> — never run on your boot drive. Script auto-detects and refuses boot devices</td>
<td><code>sudo ./scripts/restore_nvme.sh</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<div dir="auto"><h3 tabindex="-1" dir="auto">BIOS settings (manual, before running the script)</h3><a id="user-content-bios-settings-manual-before-running-the-script" aria-label="Permalink: BIOS settings (manual, before running the script)" href="#bios-settings-manual-before-running-the-script"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><strong>Above 4G Decoding</strong>: ON (required for 64-bit BAR mapping)</li>
<li><strong>IOMMU</strong>: OFF (or leave on — the script adds the kernel parameter)</li>
<li><strong>Secure Boot</strong>: OFF (required for unsigned/patched kernel module loading)</li>
</ul>

<blockquote>
<p dir="auto"><strong>WARNING</strong>: This project performs low-level PCIe operations (GPU MMIO writes to NVMe controller
registers, userspace NVMe command submission, VFIO device passthrough). While tested extensively on
RTX 3090 + WD SN740, incorrect configuration or hardware incompatibilities could theoretically cause:</p>
<ul dir="auto">
<li><strong>NVMe link failure</strong> requiring power cycle (observed during development with GPU reads)</li>
<li><strong>Data loss</strong> on the NVMe device used for raw block storage</li>
<li><strong>System instability</strong> from disabled IOMMU or patched kernel modules</li>
</ul>
<p dir="auto"><strong>Never use your boot drive for NVMe direct I/O.</strong> Always use a dedicated secondary NVMe.
The authors are not responsible for hardware damage or data loss. Use at your own risk.</p>
</blockquote>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Script</th>
<th>Purpose</th>
<th>When to run</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>scripts/setup_system.sh</code></td>
<td>Full system configuration (7 phases)</td>
<td>First-time setup</td>
</tr>
<tr>
<td><code>scripts/setup_system.sh --nvme-only</code></td>
<td>VFIO + NVMe bind only</td>
<td>After every reboot</td>
</tr>
<tr>
<td><code>scripts/setup_system.sh --check</code></td>
<td>Verify system state</td>
<td>Debugging</td>
</tr>
<tr>
<td><code>scripts/setup_nvme.sh [BDF]</code></td>
<td>Bind single NVMe to VFIO</td>
<td>After reboot (standalone)</td>
</tr>
<tr>
<td><code>scripts/restore_nvme.sh [BDF]</code></td>
<td>Restore NVMe to kernel driver</td>
<td>When done with NVMe direct</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">For models that don&#39;t fit in VRAM, the NVMe backend eliminates the CPU from the data path:</p>
<div data-snippet-clipboard-copy-content="NVMe SSD → (DMA) → Pinned Staging → (PCIe H2D) → GPU Buffers → Compute"><pre><code>NVMe SSD → (DMA) → Pinned Staging → (PCIe H2D) → GPU Buffers → Compute
</code></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Build with NVMe support (requires gpu-nvme-direct library)
cmake .. -DCMAKE_BUILD_TYPE=Release -DUSE_GPUNVME=ON \
  -DCMAKE_C_COMPILER=gcc-14 -DCMAKE_CXX_COMPILER=g++-14 \
  -DCMAKE_CUDA_COMPILER=/usr/local/cuda-13.1/bin/nvcc
cmake --build . -j

# Write GGUF model to NVMe raw device
sudo ./scripts/restore_nvme.sh           # ensure kernel driver is bound
sudo dd if=model.gguf of=/dev/nvme0n1 bs=1M oflag=direct status=progress

# Bind NVMe to VFIO for userspace access
sudo ./scripts/setup_nvme.sh             # loads VFIO, forces D0, enables BusMaster

# Run with NVMe backend
sudo GPUNVME_PCI_BDF=0000:01:00.0 GPUNVME_GGUF_LBA=0 \
  ./build/ntransformer -m /path/to/model.gguf -p &#34;Hello&#34; -n 32 --streaming

# Restore NVMe to kernel driver when done
sudo ./scripts/restore_nvme.sh"><pre><span><span>#</span> Build with NVMe support (requires gpu-nvme-direct library)</span>
cmake .. -DCMAKE_BUILD_TYPE=Release -DUSE_GPUNVME=ON \
  -DCMAKE_C_COMPILER=gcc-14 -DCMAKE_CXX_COMPILER=g++-14 \
  -DCMAKE_CUDA_COMPILER=/usr/local/cuda-13.1/bin/nvcc
cmake --build <span>.</span> -j

<span><span>#</span> Write GGUF model to NVMe raw device</span>
sudo ./scripts/restore_nvme.sh           <span><span>#</span> ensure kernel driver is bound</span>
sudo dd if=model.gguf of=/dev/nvme0n1 bs=1M oflag=direct status=progress

<span><span>#</span> Bind NVMe to VFIO for userspace access</span>
sudo ./scripts/setup_nvme.sh             <span><span>#</span> loads VFIO, forces D0, enables BusMaster</span>

<span><span>#</span> Run with NVMe backend</span>
sudo GPUNVME_PCI_BDF=0000:01:00.0 GPUNVME_GGUF_LBA=0 \
  ./build/ntransformer -m /path/to/model.gguf -p <span><span>&#34;</span>Hello<span>&#34;</span></span> -n 32 --streaming

<span><span>#</span> Restore NVMe to kernel driver when done</span>
sudo ./scripts/restore_nvme.sh</pre></div>

<ol dir="auto">
<li>The GGUF model file is written to raw NVMe blocks via <code>dd</code></li>
<li><code>setup_nvme.sh</code> binds the NVMe to VFIO, forces PCIe D0 power state, enables BusMaster</li>
<li>gpu-nvme-direct initializes the NVMe controller from userspace (admin queues, I/O queues)</li>
<li>During inference, each layer (~670 MB for 70B Q6_K) is read via 670 NVMe commands in ~202 ms</li>
<li>Data lands in CUDA pinned staging memory, then async DMA to GPU compute buffers</li>
<li>Pipeline overlaps NVMe reads, H2D DMA, and GPU compute across double buffers</li>
</ol>

<div data-snippet-clipboard-copy-content="src/
├── core/           # Tensor, allocator, GPU device management
├── cuda/           # CUDA kernels: GEMV, RMSNorm, RoPE, SwiGLU, softmax
├── memory/         # SLEP layer streaming engine (NVMe + mmap backends)
├── model/          # Transformer: config, GGUF loader, attention, FFN, norms
├── inference/      # Tokenizer, sampler, engine
├── utils/          # Timer, logger
├── main.cpp        # CLI entry point
scripts/
├── setup_system.sh # Full system setup (GRUB, NVIDIA patch, CUDA patch, VFIO, NVMe)
├── setup_nvme.sh   # Bind NVMe to VFIO, configure for gpu-nvme-direct
├── restore_nvme.sh # Restore NVMe to kernel driver
tests/              # Unit tests (tensor, GEMM kernels, NVMe layer loader)"><pre><code>src/
├── core/           # Tensor, allocator, GPU device management
├── cuda/           # CUDA kernels: GEMV, RMSNorm, RoPE, SwiGLU, softmax
├── memory/         # SLEP layer streaming engine (NVMe + mmap backends)
├── model/          # Transformer: config, GGUF loader, attention, FFN, norms
├── inference/      # Tokenizer, sampler, engine
├── utils/          # Timer, logger
├── main.cpp        # CLI entry point
scripts/
├── setup_system.sh # Full system setup (GRUB, NVIDIA patch, CUDA patch, VFIO, NVMe)
├── setup_nvme.sh   # Bind NVMe to VFIO, configure for gpu-nvme-direct
├── restore_nvme.sh # Restore NVMe to kernel driver
tests/              # Unit tests (tensor, GEMM kernels, NVMe layer loader)
</code></pre></div>

<div data-snippet-clipboard-copy-content="forward_tiered() — hybrid pipeline:

Tier A (VRAM resident, layers 0..28):
  GPU Compute:  [layer 0][layer 1]...[layer 28]     (zero I/O, weights permanent)

Tier B (pinned RAM, layers 29..79, double-buffered):
  H2D DMA:     [L29→gpu0][L30→gpu1][L31→gpu0]...   (async from pinned RAM)
  GPU Compute: [         ][layer 29][layer 30]...    (overlapped with H2D)

Tier C (NVMe/mmap fallback, if needed):
  NVMe/memcpy: [read L→stg0][read L→stg1]...
  H2D DMA:     [            ][stg0→gpu0  ]...
  GPU Compute: [            ][            ][layer]..."><pre><code>forward_tiered() — hybrid pipeline:

Tier A (VRAM resident, layers 0..28):
  GPU Compute:  [layer 0][layer 1]...[layer 28]     (zero I/O, weights permanent)

Tier B (pinned RAM, layers 29..79, double-buffered):
  H2D DMA:     [L29→gpu0][L30→gpu1][L31→gpu0]...   (async from pinned RAM)
  GPU Compute: [         ][layer 29][layer 30]...    (overlapped with H2D)

Tier C (NVMe/mmap fallback, if needed):
  NVMe/memcpy: [read L→stg0][read L→stg1]...
  H2D DMA:     [            ][stg0→gpu0  ]...
  GPU Compute: [            ][            ][layer]...
</code></pre></div>
<p dir="auto">Tier sizes auto-computed from <code>cudaMemGetInfo()</code> + <code>/proc/meminfo</code> MemAvailable.</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Format</th>
<th>Bits/Weight</th>
<th>Block Size</th>
<th>Supported</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q4_0</td>
<td>4.5</td>
<td>32</td>
<td>Yes</td>
</tr>
<tr>
<td>Q8_0</td>
<td>8.5</td>
<td>32</td>
<td>Yes</td>
</tr>
<tr>
<td>Q4_K_M</td>
<td>4.5</td>
<td>256</td>
<td>Yes (mixed: Q4_K + Q5_K + Q6_K)</td>
</tr>
<tr>
<td>Q5_K</td>
<td>5.5</td>
<td>256</td>
<td>Yes</td>
</tr>
<tr>
<td>Q6_K</td>
<td>6.6</td>
<td>256</td>
<td>Yes</td>
</tr>
<tr>
<td>F16</td>
<td>16</td>
<td>1</td>
<td>Yes</td>
</tr>
<tr>
<td>F32</td>
<td>32</td>
<td>1</td>
<td>Yes</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<ul dir="auto">
<li><strong>Phase 1</strong> - Foundation (complete): Llama 8B Q8_0, custom CUDA kernels, 48.9 tok/s</li>
<li><strong>Phase 2</strong> - SLEP Streaming (complete): 70B on single GPU, 3-tier caching, 33x speedup</li>
<li><strong>Phase 3</strong> - Optimization (complete): Q4_K_M/Q5_K support, layer skip (0.5 tok/s), self-speculative decoding, F16 KV cache</li>
<li><strong>Phase 4</strong> - NVMe Direct: gpu-nvme-direct backend for tier C (GPU-initiated NVMe reads, 3.35 GB/s)</li>
<li><strong>Phase 5</strong> - Polish: speculative decoding with draft model, benchmarks, public C API</li>
</ul>

<p dir="auto">BSD-2-Clause</p>
</article></div></div>
  </body>
</html>
