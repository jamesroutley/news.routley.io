<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/microsoft/fara">Original</a>
    <h1>Fara-7B: An efficient agentic model for computer use</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<div dir="auto"><a id="user-content-fara-7b-an-efficient-agentic-model-for-computer-use" aria-label="Permalink: Fara-7B: An Efficient Agentic Model for Computer Use" href="#fara-7b-an-efficient-agentic-model-for-computer-use"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p><a target="_blank" rel="noopener noreferrer" href="http://tinylogger.com/microsoft/fara/blob/main/figures/model_accuracy_vs_cost_v2_glm_cost_updated.png"><img src="http://tinylogger.com/microsoft/fara/raw/main/figures/model_accuracy_vs_cost_v2_glm_cost_updated.png" alt="Fara-7B Performance" width="600"/></a></p><p dir="auto"><a href="https://aka.ms/msaif/fara" rel="nofollow"><img src="https://camo.githubusercontent.com/574f57b77832fd1f5d69a888107eb3277e18e3303126f19249ad2b30d580486f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6963726f736f66742d50726f6a6563742d3030373844343f6c6f676f3d6d6963726f736f6674" alt="Microsoft" data-canonical-src="https://img.shields.io/badge/Microsoft-Project-0078D4?logo=microsoft"/></a>
<a href="https://huggingface.co/microsoft/Fara-7b" rel="nofollow"><img src="https://camo.githubusercontent.com/ed9df35495f85af0083bca138e68732525f68d58b4ddbeb5630183bc6d3b69d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d4d6f64656c2d79656c6c6f77" alt="Hugging Face Model" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-Model-yellow"/></a>
<a href="https://aka.ms/foundry-fara-7b" rel="nofollow"><img src="https://camo.githubusercontent.com/ff19c70146434f8197bfa03a0efa2212bfdbb073213cf78fa3b8030a6b1a8764/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f417a7572652d466f756e6472792d303038394436" alt="Foundry" data-canonical-src="https://img.shields.io/badge/Azure-Foundry-0089D6"/></a>
<a href="https://huggingface.co/datasets/microsoft/WebTailBench" rel="nofollow"><img src="https://camo.githubusercontent.com/26ab73ace4d059a01c9888876b5ec3b98e969ba97b49bccbd420e15a3fa66d59/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d5765625461696c42656e6368253230446174617365742d6f72616e6765" alt="Dataset" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-WebTailBench%20Dataset-orange"/></a></p>
</div>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Fara-7B</strong> is Microsoft&#39;s first <strong>agentic small language model (SLM)</strong> designed specifically for computer use. With only 7 billion parameters, Fara-7B is an ultra-compact Computer Use Agent (CUA) that achieves state-of-the-art performance within its size class and is competitive with larger, more resource-intensive agentic systems.</p>
<p dir="auto">Try Fara-7B locally as follows (see <a href="##Installation">Installation</a> for detailed instructions):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# 1. Clone repository
git clone https://github.com/microsoft/fara.git
cd fara

# 2. Setup environment
python3 -m venv .venv 
source .venv/bin/activate
pip install -e .
playwright install"><pre><span><span>#</span> 1. Clone repository</span>
git clone https://github.com/microsoft/fara.git
<span>cd</span> fara

<span><span>#</span> 2. Setup environment</span>
python3 -m venv .venv 
<span>source</span> .venv/bin/activate
pip install -e <span>.</span>
playwright install</pre></div>
<p dir="auto">Then in one process, host the model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve &#34;microsoft/Fara-7B&#34; --port 5000 --dtype auto "><pre>vllm serve <span><span>&#34;</span>microsoft/Fara-7B<span>&#34;</span></span> --port 5000 --dtype auto </pre></div>
<p dir="auto">Then you can iterative query it with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fara-cli --task &#34;whats the weather in new york now&#34;"><pre>fara-cli --task <span><span>&#34;</span>whats the weather in new york now<span>&#34;</span></span></pre></div>
<p dir="auto">Hint: might need to do <code>--tensor-parallel-size 2</code> with vllm command if you run out of memory</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">What Makes Fara-7B Unique</h3><a id="user-content-what-makes-fara-7b-unique" aria-label="Permalink: What Makes Fara-7B Unique" href="#what-makes-fara-7b-unique"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Unlike traditional chat models that generate text-based responses, Fara-7B leverages computer interfaces—mouse and keyboard—to perform multi-step tasks on behalf of users. The model:</p>
<ul dir="auto">
<li><strong>Operates visually</strong> by perceiving webpages and taking actions like scrolling, typing, and clicking on directly predicted coordinates</li>
<li><strong>Uses the same modalities as humans</strong> to interact with computers—no accessibility trees or separate parsing models required</li>
<li><strong>Enables on-device deployment</strong> due to its compact 7B parameter size, resulting in reduced latency and improved privacy as user data remains local</li>
<li><strong>Completes tasks efficiently</strong>, averaging only ~16 steps per task compared to ~41 for comparable models</li>
</ul>
<p dir="auto">Fara-7B is trained using a novel synthetic data generation pipeline built on the <a href="https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/" rel="nofollow">Magentic-One</a> multi-agent framework, with 145K trajectories covering diverse websites, task types, and difficulty levels. The model is based on <a href="https://arxiv.org/abs/2502.13923" rel="nofollow">Qwen2.5-VL-7B</a> and trained with supervised fine-tuning.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Key Capabilities</h3><a id="user-content-key-capabilities" aria-label="Permalink: Key Capabilities" href="#key-capabilities"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Fara-7B can automate everyday web tasks including:</p>
<ul dir="auto">
<li>Searching for information and summarizing results</li>
<li>Filling out forms and managing accounts</li>
<li>Booking travel, movie tickets, and restaurant reservations</li>
<li>Shopping and comparing prices across retailers</li>
<li>Finding job postings and real estate listings</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Performance Highlights</h3><a id="user-content-performance-highlights" aria-label="Permalink: Performance Highlights" href="#performance-highlights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Fara-7B achieves state-of-the-art results across multiple web agent benchmarks, outperforming both comparable-sized models and larger systems:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Params</th>
<th>WebVoyager</th>
<th>Online-M2W</th>
<th>DeepShop</th>
<th>WebTailBench</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SoM Agents</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SoM Agent (GPT-4o-0513)</td>
<td>-</td>
<td>90.6</td>
<td>57.7</td>
<td>49.1</td>
<td>60.4</td>
</tr>
<tr>
<td>SoM Agent (o3-mini)</td>
<td>-</td>
<td>79.3</td>
<td>55.4</td>
<td>49.7</td>
<td>52.7</td>
</tr>
<tr>
<td>SoM Agent (GPT-4o)</td>
<td>-</td>
<td>65.1</td>
<td>34.6</td>
<td>16.0</td>
<td>30.8</td>
</tr>
<tr>
<td>GLM-4.1V-9B-Thinking</td>
<td>9B</td>
<td>66.8</td>
<td>33.9</td>
<td>32.0</td>
<td>22.4</td>
</tr>
<tr>
<td><strong>Computer Use Models</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>OpenAI computer-use-preview</td>
<td>-</td>
<td>70.9</td>
<td>42.9</td>
<td>24.7</td>
<td>25.7</td>
</tr>
<tr>
<td>UI-TARS-1.5-7B</td>
<td>7B</td>
<td>66.4</td>
<td>31.3</td>
<td>11.6</td>
<td>19.5</td>
</tr>
<tr>
<td><strong>Fara-7B</strong></td>
<td><strong>7B</strong></td>
<td><strong>73.5</strong></td>
<td><strong>34.1</strong></td>
<td><strong>26.2</strong></td>
<td><strong>38.4</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em>Table: Online agent evaluation results showing success rates (%) across four web benchmarks. Results are averaged over 3 runs.</em></p>
<div dir="auto"><h3 tabindex="-1" dir="auto">WebTailBench: A New Benchmark for Real-World Web Tasks</h3><a id="user-content-webtailbench-a-new-benchmark-for-real-world-web-tasks" aria-label="Permalink: WebTailBench: A New Benchmark for Real-World Web Tasks" href="#webtailbench-a-new-benchmark-for-real-world-web-tasks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We are releasing <strong><a href="https://huggingface.co/datasets/microsoft/WebTailBench" rel="nofollow">WebTailBench</a></strong>, a new evaluation benchmark focusing on 11 real-world task types that are underrepresented or missing in existing benchmarks. The benchmark includes 609 tasks across diverse categories, with the first 8 segments testing single skills or objectives (usually on a single website), and the remaining 3 evaluating more difficult multi-step or cross-site tasks.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">WebTailBench Detailed Results</h4><a id="user-content-webtailbench-detailed-results" aria-label="Permalink: WebTailBench Detailed Results" href="#webtailbench-detailed-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Task Segment</th>
<th>Tasks</th>
<th>SoM GPT-4o-0513</th>
<th>SoM o3-mini</th>
<th>SoM GPT-4o</th>
<th>GLM-4.1V-9B</th>
<th>OAI Comp-Use</th>
<th>UI-TARS-1.5</th>
<th><strong>Fara-7B</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Single-Site Tasks</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Shopping</td>
<td>56</td>
<td>62.5</td>
<td>71.4</td>
<td>38.1</td>
<td>31.0</td>
<td>42.3</td>
<td>41.1</td>
<td><strong>52.4</strong></td>
</tr>
<tr>
<td>Flights</td>
<td>51</td>
<td>60.1</td>
<td>39.2</td>
<td>11.1</td>
<td>10.5</td>
<td>17.6</td>
<td>10.5</td>
<td><strong>37.9</strong></td>
</tr>
<tr>
<td>Hotels</td>
<td>52</td>
<td>68.6</td>
<td>56.4</td>
<td>31.4</td>
<td>19.9</td>
<td>26.9</td>
<td>35.3</td>
<td><strong>53.8</strong></td>
</tr>
<tr>
<td>Restaurants</td>
<td>52</td>
<td>67.9</td>
<td>59.6</td>
<td>47.4</td>
<td>32.1</td>
<td>35.9</td>
<td>22.4</td>
<td><strong>47.4</strong></td>
</tr>
<tr>
<td>Activities</td>
<td>80</td>
<td>70.4</td>
<td>62.9</td>
<td>41.7</td>
<td>26.3</td>
<td>30.4</td>
<td>9.6</td>
<td><strong>36.3</strong></td>
</tr>
<tr>
<td>Ticketing</td>
<td>57</td>
<td>58.5</td>
<td>56.7</td>
<td>37.4</td>
<td>35.7</td>
<td>49.7</td>
<td>30.4</td>
<td><strong>38.6</strong></td>
</tr>
<tr>
<td>Real Estate</td>
<td>48</td>
<td>34.0</td>
<td>17.4</td>
<td>20.1</td>
<td>16.0</td>
<td>9.0</td>
<td>9.7</td>
<td><strong>23.6</strong></td>
</tr>
<tr>
<td>Jobs/Careers</td>
<td>50</td>
<td>49.3</td>
<td>44.0</td>
<td>32.7</td>
<td>22.7</td>
<td>20.7</td>
<td>20.7</td>
<td><strong>28.0</strong></td>
</tr>
<tr>
<td><strong>Multi-Step Tasks</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Shopping List (2 items)</td>
<td>51</td>
<td>66.0</td>
<td>62.7</td>
<td>17.0</td>
<td>7.8</td>
<td>34.0</td>
<td>20.9</td>
<td><strong>49.0</strong></td>
</tr>
<tr>
<td>Comparison Shopping</td>
<td>57</td>
<td>67.3</td>
<td>59.1</td>
<td>27.5</td>
<td>22.8</td>
<td>1.2</td>
<td>8.8</td>
<td><strong>32.7</strong></td>
</tr>
<tr>
<td>Compositional Tasks</td>
<td>55</td>
<td>51.5</td>
<td>39.4</td>
<td>26.7</td>
<td>17.0</td>
<td>10.3</td>
<td>9.1</td>
<td><strong>23.0</strong></td>
</tr>
<tr>
<td><strong>Overall</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Macro Average</td>
<td>609</td>
<td>59.7</td>
<td>51.7</td>
<td>30.1</td>
<td>22.0</td>
<td>25.3</td>
<td>19.9</td>
<td><strong>38.4</strong></td>
</tr>
<tr>
<td>Micro Average</td>
<td>609</td>
<td>60.4</td>
<td>52.7</td>
<td>30.8</td>
<td>22.4</td>
<td>25.7</td>
<td>19.5</td>
<td><strong>38.4</strong></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><em>Table: Breakdown of WebTailBench results across all 11 segments. Success rates (%) are averaged over 3 independent runs. Fara-7B achieves the highest performance among computer-use models across all task categories.</em></p>
<p dir="auto"><strong>Coming Soon:</strong></p>
<ul dir="auto">
<li>Task Verification pipeline for LLM-as-a-judge evaluation</li>
<li>Official human annotations of WebTailBench (in partnership with BrowserBase)</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Evaluation Infrastructure</h3><a id="user-content-evaluation-infrastructure" aria-label="Permalink: Evaluation Infrastructure" href="#evaluation-infrastructure"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Our evaluation setup leverages:</p>
<ol dir="auto">
<li><strong>Playwright</strong> - A cross-browser automation framework that replicates browser environments</li>
<li><strong>Abstract Web Agent Interface</strong> - Allows integration of any model from any source into the evaluation environment</li>
<li><strong>Fara-Agent Class</strong> - Reference implementation for running the Fara model</li>
</ol>
<blockquote>
<p dir="auto"><strong>Note:</strong> Fara-7B is an experimental release designed to invite hands-on exploration and feedback from the community. We recommend running it in a sandboxed environment, monitoring its execution, and avoiding sensitive data or high-risk domains.</p>
</blockquote>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Install the package using either UV or pip:</p>
<div dir="auto" data-snippet-clipboard-copy-content="uv sync --all-extras"><pre>uv sync --all-extras</pre></div>
<p dir="auto">or</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -e ."><pre>pip install -e <span>.</span></pre></div>
<p dir="auto">Then install Playwright browsers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="playwright install"><pre>playwright install</pre></div>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Hosting the Model</h2><a id="user-content-hosting-the-model" aria-label="Permalink: Hosting the Model" href="#hosting-the-model"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Recommended:</strong> The easiest way to get started is using Azure Foundry hosting, which requires no GPU hardware or model downloads. Alternatively, you can self-host with VLLM if you have GPU resources available.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Azure Foundry Hosting (Recommended)</h3><a id="user-content-azure-foundry-hosting-recommended" aria-label="Permalink: Azure Foundry Hosting (Recommended)" href="#azure-foundry-hosting-recommended"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Deploy Fara-7B on <a href="https://ai.azure.com/explore/models/Fara-7B/version/2/registry/azureml-msr" rel="nofollow">Azure Foundry</a> without needing to download weights or manage GPU infrastructure.</p>
<p dir="auto"><strong>Setup:</strong></p>
<ol dir="auto">
<li>Deploy the Fara-7B model on Azure Foundry and obtain your endpoint URL and API key</li>
<li>Add your endpoint details to the existing <code>endpoint_configs/</code> directory (example configs are already provided):</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Edit one of the existing config files or create a new one
# endpoint_configs/fara-7b-hosting-ansrz.json (example format):
{
    &#34;model&#34;: &#34;Fara-7B&#34;,
    &#34;base_url&#34;: &#34;https://your-endpoint.inference.ml.azure.com/&#34;,
    &#34;api_key&#34;: &#34;YOUR_API_KEY_HERE&#34;
}"><pre><span><span>#</span> Edit one of the existing config files or create a new one</span>
<span><span>#</span> endpoint_configs/fara-7b-hosting-ansrz.json (example format):</span>
{
    <span><span>&#34;</span>model<span>&#34;</span></span>: <span><span>&#34;</span>Fara-7B<span>&#34;</span></span>,
    <span><span>&#34;</span>base_url<span>&#34;</span></span>: <span><span>&#34;</span>https://your-endpoint.inference.ml.azure.com/<span>&#34;</span></span>,
    <span><span>&#34;</span>api_key<span>&#34;</span></span>: <span><span>&#34;</span>YOUR_API_KEY_HERE<span>&#34;</span></span>
}</pre></div>
<ol start="3" dir="auto">
<li>Run the Fara agent:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="fara-cli --task &#34;how many pages does wikipedia have&#34; --start_page &#34;https://www.bing.com&#34;"><pre>fara-cli --task <span><span>&#34;</span>how many pages does wikipedia have<span>&#34;</span></span> --start_page <span><span>&#34;</span>https://www.bing.com<span>&#34;</span></span></pre></div>
<p dir="auto">That&#39;s it! No GPU or model downloads required.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Self-hosting with VLLM</h3><a id="user-content-self-hosting-with-vllm" aria-label="Permalink: Self-hosting with VLLM" href="#self-hosting-with-vllm"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you have access to GPU resources, you can self-host Fara-7B using VLLM. This requires a GPU machine with sufficient VRAM.</p>
<p dir="auto">All that is required is to run the following command to start the VLLM server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vllm serve &#34;microsoft/Fara-7B&#34; --port 5000 --dtype auto "><pre>vllm serve <span><span>&#34;</span>microsoft/Fara-7B<span>&#34;</span></span> --port 5000 --dtype auto </pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Testing the Fara Agent</h3><a id="user-content-testing-the-fara-agent" aria-label="Permalink: Testing the Fara Agent" href="#testing-the-fara-agent"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Run the test script to see Fara in action:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fara-cli --task &#34;how many pages does wikipedia have&#34; --start_page &#34;https://www.bing.com&#34; --endpoint_config endpoint_configs/azure_foundry_config.json [--headful] [--downloads_folder &#34;/path/to/downloads&#34;] [--save_screenshots] [--max_rounds 100] [--browserbase]"><pre>fara-cli --task <span><span>&#34;</span>how many pages does wikipedia have<span>&#34;</span></span> --start_page <span><span>&#34;</span>https://www.bing.com<span>&#34;</span></span> --endpoint_config endpoint_configs/azure_foundry_config.json [--headful] [--downloads_folder <span><span>&#34;</span>/path/to/downloads<span>&#34;</span></span>] [--save_screenshots] [--max_rounds 100] [--browserbase]</pre></div>
<p dir="auto">In self-hosting scenario the <code>endpoint_config</code> points to <code>endpoint_configs/vllm_config.json</code> from the VLLM server above.</p>
<p dir="auto">If you set <code>--browserbase</code>, export environment variables for the API key and project ID.</p>
<div dir="auto"><h4 tabindex="-1" dir="auto">Expected Output</h4><a id="user-content-expected-output" aria-label="Permalink: Expected Output" href="#expected-output"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div data-snippet-clipboard-copy-content="Initializing Browser...
Browser Running... Starting Fara Agent...
##########################################
Task: how many pages does wikipedia have
##########################################
Running Fara...


Thought #1: To find the current number of Wikipedia pages, I&#39;ll search for the latest Wikipedia page count statistics.
Action #1: executing tool &#39;web_search&#39; with arguments {&#34;action&#34;: &#34;web_search&#34;, &#34;query&#34;: &#34;Wikipedia total number of articles&#34;}
Observation#1: I typed &#39;Wikipedia total number of articles&#39; into the browser search bar.

Thought #2: Wikipedia currently has 7,095,446 articles.
Action #2: executing tool &#39;terminate&#39; with arguments {&#34;action&#34;: &#34;terminate&#34;, &#34;status&#34;: &#34;success&#34;}
Observation#2: Wikipedia currently has 7,095,446 articles.

Final Answer: Wikipedia currently has 7,095,446 articles.

Enter another task (or press Enter to exit): "><pre><code>Initializing Browser...
Browser Running... Starting Fara Agent...
##########################################
Task: how many pages does wikipedia have
##########################################
Running Fara...


Thought #1: To find the current number of Wikipedia pages, I&#39;ll search for the latest Wikipedia page count statistics.
Action #1: executing tool &#39;web_search&#39; with arguments {&#34;action&#34;: &#34;web_search&#34;, &#34;query&#34;: &#34;Wikipedia total number of articles&#34;}
Observation#1: I typed &#39;Wikipedia total number of articles&#39; into the browser search bar.

Thought #2: Wikipedia currently has 7,095,446 articles.
Action #2: executing tool &#39;terminate&#39; with arguments {&#34;action&#34;: &#34;terminate&#34;, &#34;status&#34;: &#34;success&#34;}
Observation#2: Wikipedia currently has 7,095,446 articles.

Final Answer: Wikipedia currently has 7,095,446 articles.

Enter another task (or press Enter to exit): 
</code></pre></div>
<hr/>
<div dir="auto"><a id="user-content-reproducibility" aria-label="Permalink: Reproducibility" href="#reproducibility"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">We provide a framework in <code>webeval/</code> to reproduce our results on WebVoyager and OnlineMind2Web.
Agentic evaluations on live websites present unique challenges due to day-to-day changes. We implement several measures to ensure reliable and comparable evaluations:</p>
<p dir="auto"><strong>BrowserBase Integration</strong>
We employ BrowserBase to manage browser session hosting, enabling reliable browser instance management.</p>
<p dir="auto"><strong>Time-sensitive Task Updates</strong>
Tasks in benchmarks like WebVoyager can become stale or impossible. We:</p>
<ul dir="auto">
<li>Removed ~48 impossible tasks from the original WebVoyager benchmark</li>
<li>Updated ~50 tasks with future dates to keep them achievable</li>
<li>Example: <em>&#34;Search for a hotel in Bali from Jan 1 to Jan 4, 2024&#34;</em> → <em>&#34;Search for a hotel in Bali from Jan 1 to Jan 4, 2026&#34;</em></li>
<li>Our updated WebVoyager benchmark is available at <code>webeval/data/webvoyager/WebVoyager_data_08312025.jsonl</code></li>
</ul>
<p dir="auto"><strong>Environment Error Handling</strong>
Browser errors (connection drops, page timeouts) are handled robustly:</p>
<ul dir="auto">
<li>Trajectories are retried up to 5 times when environment errors occur</li>
<li>Complete yet incorrect trajectories are never retried</li>
<li>Each retry starts with a fresh browser session, with no retained state</li>
</ul>
<p dir="auto"><strong>Step Budget</strong>
Each trajectory is capped at a maximum of 100 actions across all online benchmarks. Trajectories exceeding this budget without choosing to stop are considered incorrect.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">WebEval Package Installation</h2><a id="user-content-webeval-package-installation" aria-label="Permalink: WebEval Package Installation" href="#webeval-package-installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="conda create --name fara_webeval python=3.12
conda activate fara_webeval

# Install fara package
pip install -e .

# Install autogen submodule
git submodule update --init --recursive
cd autogen/python/packages
pip install -e autogen-core
pip install -e autogen-ext

# Install webeval
cd webeval
pip install -e .

# Install playwright
playwright install"><pre>conda create --name fara_webeval python=3.12
conda activate fara_webeval

<span><span>#</span> Install fara package</span>
pip install -e <span>.</span>

<span><span>#</span> Install autogen submodule</span>
git submodule update --init --recursive
<span>cd</span> autogen/python/packages
pip install -e autogen-core
pip install -e autogen-ext

<span><span>#</span> Install webeval</span>
<span>cd</span> webeval
pip install -e <span>.</span>

<span><span>#</span> Install playwright</span>
playwright install</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">Running Evaluations</h2><a id="user-content-running-evaluations" aria-label="Permalink: Running Evaluations" href="#running-evaluations"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Navigate to the scripts directory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd webeval/scripts"><pre><span>cd</span> webeval/scripts</pre></div>
<p dir="auto">Make sure you set a valid OpenAI GPT-4o endpoint in <code>endpoint_configs_gpt4o/dev</code> in order to run the WebVoyager LLM-as-a-judge!</p>
<p dir="auto"><strong>Option 1: Self-hosted VLLM</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="python webvoyager.py --model_url /path/where/you/want/to/download/model/ --model_port 5000 --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --device_id 0,1 --processes 1 --run_id 1 --max_rounds 100"><pre>python webvoyager.py --model_url /path/where/you/want/to/download/model/ --model_port 5000 --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --device_id 0,1 --processes 1 --run_id 1 --max_rounds 100</pre></div>
<p dir="auto"><strong>Option 2: Azure Foundry Deployment</strong></p>
<p dir="auto">Deploy <a href="https://ai.azure.com/explore/models/Fara-7B/version/2/registry/azureml-msr" rel="nofollow">Fara-7B on Foundry endpoint(s)</a>, then place endpoint URLs and keys in JSONs under <code>endpoint_configs/</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python webvoyager.py --model_endpoint ../../endpoint_configs/ --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --processes 1 --run_id 1_endpoint --max_rounds 100"><pre>python webvoyager.py --model_endpoint ../../endpoint_configs/ --eval_oai_config ../endpoint_configs_gpt4o/dev/ --out_url /path/to/save/eval/files --processes 1 --run_id 1_endpoint --max_rounds 100</pre></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Notes</h3><a id="user-content-notes" aria-label="Permalink: Notes" href="#notes"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li>We use the same LLM-as-a-judge prompts and model (GPT-4o) as WebVoyager, hence the <code>--eval_oai_config</code> argument</li>
<li>Set <code>--browserbase</code> for browser session management (requires exported API key and project ID environment variables)</li>
<li>Avoid overloading a single VLLM deployment with more than ~10 concurrent processes due to known issues</li>
<li>See debugging output in <code>fara/webeval/scripts/stdout.txt</code></li>
</ul>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Analyzing Evaluation Results</h2><a id="user-content-analyzing-evaluation-results" aria-label="Permalink: Analyzing Evaluation Results" href="#analyzing-evaluation-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto"><h3 tabindex="-1" dir="auto">Evaluation Output Structure</h3><a id="user-content-evaluation-output-structure" aria-label="Permalink: Evaluation Output Structure" href="#evaluation-output-structure"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Evaluation results are stored under <code>--out_url</code> in folders organized by:</p>
<ul dir="auto">
<li>Model name</li>
<li>Dataset</li>
<li>Username</li>
<li>Run ID</li>
</ul>
<p dir="auto">Example path:</p>
<div data-snippet-clipboard-copy-content="/runs/WebSurfer-fara-100-max_n_images-3/fara-7b/&lt;username&gt;/WebVoyager_WebVoyager_data_08312025.jsonl/&lt;run_id&gt;"><pre><code>/runs/WebSurfer-fara-100-max_n_images-3/fara-7b/&lt;username&gt;/WebVoyager_WebVoyager_data_08312025.jsonl/&lt;run_id&gt;
</code></pre></div>
<p dir="auto">Each evaluation folder contains:</p>
<ul dir="auto">
<li><code>gpt_eval/</code> - LLM-as-a-judge evaluation results</li>
<li><code>traj/</code> - Per-task trajectory subdirectories containing:
<ul dir="auto">
<li><code>final_answer.json</code> (e.g., <code>Amazon--1_final_answer.json</code>) - <code>&lt;no_answer&gt;</code> indicates abortion or step budget exceeded</li>
<li><code>scores/gpt_eval.json</code> - LLM judge scores</li>
<li><code>web_surfer.log</code> - Action history and errors</li>
<li><code>screenshot_X.png</code> - Screenshots captured before each action X</li>
</ul>
</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Running Analysis</h3><a id="user-content-running-analysis" aria-label="Permalink: Running Analysis" href="#running-analysis"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Use the analysis notebook to compute metrics:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd webeval/scripts/analyze_eval_results/
jupyter notebook analyze.ipynb"><pre><span>cd</span> webeval/scripts/analyze_eval_results/
jupyter notebook analyze.ipynb</pre></div>
<p dir="auto">The script:</p>
<ul dir="auto">
<li>Identifies trajectories aborted mid-execution and diagnostic reasons</li>
<li>Computes average scores across non-aborted trajectories</li>
<li>Distinguishes between aborted trajectories (errors during sampling) and completed trajectories (with terminate() call or step budget exceeded)</li>
</ul>
<p dir="auto">To re-run failed tasks, execute the evaluation script again with the same <code>run_id</code> and <code>username</code> - it will skip non-aborted tasks.</p>
<details>
<summary>Example WebVoyager GPT Eval Result</summary>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;score&#34;: 1.0,
  &#34;gpt_response_text&#34;: &#34;To evaluate the task, we need to verify if the criteria have been met:\n\n1. **Recipe Requirement**: A vegetarian lasagna recipe with zucchini and at least a four-star rating.\n\n2. **Search and Results**:\n   - The screenshots show that the search term used was \&#34;vegetarian lasagna zucchini.\&#34;\n   - Among the search results, \&#34;Debbie&#39;s Vegetable Lasagna\&#34; is prominently featured.\n   \n3. **Evaluation of the Recipe**:\n   - Rating: \&#34;Debbie&#39;s Vegetable Lasagna\&#34; has a rating of 4.7, which satisfies the requirement of being at least four stars.\n   - The presence of zucchini in the recipe is implied through the search conducted, though the screenshots do not explicitly show the ingredients list. However, the result response confirms the match to the criteria.\n\nGiven the information provided, the task seems to have fulfilled the requirement of finding a vegetarian lasagna recipe with zucchini and a four-star rating or higher. \n\n**Verdict: SUCCESS**&#34;
}"><pre>{
  <span>&#34;score&#34;</span>: <span>1.0</span>,
  <span>&#34;gpt_response_text&#34;</span>: <span><span>&#34;</span>To evaluate the task, we need to verify if the criteria have been met:<span>\n\n</span>1. **Recipe Requirement**: A vegetarian lasagna recipe with zucchini and at least a four-star rating.<span>\n\n</span>2. **Search and Results**:<span>\n</span>   - The screenshots show that the search term used was <span>\&#34;</span>vegetarian lasagna zucchini.<span>\&#34;\n</span>   - Among the search results, <span>\&#34;</span>Debbie&#39;s Vegetable Lasagna<span>\&#34;</span> is prominently featured.<span>\n</span>   <span>\n</span>3. **Evaluation of the Recipe**:<span>\n</span>   - Rating: <span>\&#34;</span>Debbie&#39;s Vegetable Lasagna<span>\&#34;</span> has a rating of 4.7, which satisfies the requirement of being at least four stars.<span>\n</span>   - The presence of zucchini in the recipe is implied through the search conducted, though the screenshots do not explicitly show the ingredients list. However, the result response confirms the match to the criteria.<span>\n\n</span>Given the information provided, the task seems to have fulfilled the requirement of finding a vegetarian lasagna recipe with zucchini and a four-star rating or higher. <span>\n\n</span>**Verdict: SUCCESS**<span>&#34;</span></span>
}</pre></div>
</details>
<hr/>
<div dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">If you use Fara in your research, please cite our work:</p>

<hr/>
</article></div></div>
  </body>
</html>
