<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://linus.schreibt.jetzt/posts/qemu-9p-performance.html">Original</a>
    <h1>Digging into a QEMU problem of slow data copying</h1>
    
    <div id="readability-page-1" class="page"><article>
    
    <section>
        <p><em>The work on QEMU and this post was paid for by Determinate Systems, and the post was co-published on the <a href="https://linus.schreibt.jetzt/posts/todo">Determinate Systems blog</a>.</em></p>

<p><a href="https://nixos.org">NixOS</a> uses virtual machines based on <a href="https://www.qemu.org/">QEMU</a> extensively for running its test suite. In order to avoid generating a disk image for every test, the test driver usually boots using a <a href="http://man.cat-v.org/plan_9/5/intro">Plan 9 File Protocol (9p)</a> share (server implemented by QEMU) for the Nix store, which contains all the programs and config necessary for the test.</p>
<p>I was working on a VM test that copied a fairly large amount of data (~278k files totaling ~5.3GiB) out of the 9p-mounted Nix store, and was surprised at how long copying this data took. On NVMe devices, I would expect this to take a matter of seconds or minutes, but the test actually ended up taking over 2 <em>hours</em>, most of which was spent copying files from 9p. Since this is untenably long for incremental work, I decided to dig into it a bit, and was able to reduce the duration of the test to only 7 minutes. In this post, I‚Äôll describe the whole journey.</p>

<p>As a preface: I don‚Äôt have much experience in debugging performance issues! Most of what I used was novel to me. The first thing I wanted to do was find out where a lot of time was being spent. My guess was that it was in QEMU and not in the guest, though this guess being correct was a matter of pure luck. <a href="https://stackoverflow.com/questions/16999681/how-to-do-profiling-with-qemu">This stack overflow question</a> described a problem mostly but not entirely unlike mine. This led me down the wonky path of trying to use the <a href="http://poormansprofiler.org/">poor man‚Äôs profiler</a>, a little hack composed of gdb, some shell, and awk.</p>
<h2 id="stories-of-surprises-and-failure-poor-mans-profiler">Stories of surprises and failure: Poor man‚Äôs profiler</h2>
<p>I immediately ran into a minor roadblock with this approach. gdb said:</p>
<blockquote>
<p>warning: Target and debugger are in different PID namespaces; thread lists and other data are likely unreliable. Connect to gdbserver inside the container.</p>
</blockquote>
<p>Nix uses Linux namespaces to provide builds with some isolation from the system running the build in order to reduce the effects that the environment of a particular machine can have on the result of the build (‚Äùpurity‚Äù). This includes PID namespaces, which prevent processes within the namespace from touching any processes outside the namespace. gdb was unhappy with being in a different PID namespace from the process it was targeting! I first attempted to get my gdb inside the sandbox using <code>nsenter</code>. The first surprise I encountered here was that entering a PID namespace does <em>not</em> cause utilities from procps, such as <code>ps</code>, <code>pgrep</code> and <code>top</code>, to report only on processes inside the new namespace:</p>
<pre><code>[root@oak:~]# pgrep -a qemu
1678991 /nix/store/6shk4z9ip57p6vffm5n9imnkwiks9fsa-qemu-host-cpu-only-for-vm-tests-7.0.0/bin/qemu-kvm [...]

[root@oak:~]# nsenter --target 1678991 --pid

üó£ This spawned a new shell within the build&#39;s PID namespace
[root@oak:~]# ps -f 1
UID          PID    PPID  C STIME TTY      STAT   TIME CMD
root           1       0  0 Sep02 ?        Ss     1:24 /run/current-system/systemd/lib/systemd/systemd</code></pre>
<p>What!? That‚Äôs not the PID1 I‚Äôm expecting! And I certainly haven‚Äôt been running this build since the 2nd of September.</p>
<p>I‚Äôll omit the details of the hour of frustration that ensued, but it turns out that the <code>/proc</code> which <code>ps</code> and friends were reading from was still that of the root PID namespace ‚Äî even though they were no longer running in it! This might cause some funny unexpected behaviour when using tools like <code>pkill</code>‚Ä¶ But that‚Äôs a problem for another day.</p>
<p>With my newfound knowledge, I was able to work around this issue by also creating a new mount namespace and mounting the desired <code>proc</code> filesystem on top of the <code>/proc</code> we inherited from the root namespace.</p>
<pre><code>[root@oak:~]# nsenter -t 1684606 -p -- unshare -m

üó£ Now inside the build&#39;s PID namespace (through nsenter) and a new mount namespace (created by unshare)
[root@oak:~]# ps -f 1
UID          PID    PPID  C STIME TTY      STAT   TIME CMD
root           1       0  0 Sep02 ?        Ss     1:24 /run/current-system/systemd/lib/systemd/systemd

[root@oak:~]# mount -t proc proc /proc

[root@oak:~]# ps -f 1
UID          PID    PPID  C STIME TTY      STAT   TIME CMD
nixbld1        1       0  0 12:27 ?        Ss     0:00 bash -e /nix/store/9krlzvny65gdc8s7kpb6lkx8cd02c25b-default-builder.sh

[root@oak:~]# ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
nixbld1        1       0  0 12:27 ?        00:00:00 bash -e /nix/store/9krlzvny65gdc8s7kpb6lkx8cd02c25b-default-builder.sh
nixbld1        6       1  0 12:27 ?        00:00:00 /nix/store/pn7863n7s2p66b0gazcylm6cccdwpzaf-python3-3.9.13/bin/python3.9 /nix/store/kdi82vgfixayxaql77j3nj7
nixbld1        7       6 99 12:27 ?        00:04:00 /nix/store/6shk4z9ip57p6vffm5n9imnkwiks9fsa-qemu-host-cpu-only-for-vm-tests-7.0.0/bin/qemu-kvm -cpu max -na
root          46       0  0 12:29 pts/5    00:00:00 -bash
root          79      46  0 12:30 pts/5    00:00:00 ps -ef
üó£ That&#39;s better!

[root@oak:~]# pid=7 ./pprof
   1500 __futex_abstimed_wait_common,__new_sem_wait_slow64.constprop.1,qemu_sem_timedwait,worker_thread,qemu_thread_start,start_thread,clone3
    743 __lll_lock_wait,pthread_mutex_lock@@GLIBC_2.2.5,qemu_mutex_lock_impl,qemu_mutex_lock_iothread_impl,flatview_read_continue,flatview_read,address_space_rw,kvm_cpu_exec,kvm_vcpu_thread_fn,qemu_thread_start,start_thread,clone3
    100 syscall,qemu_event_wait,call_rcu_thread,qemu_thread_start,start_thread,clone3
     53 ioctl,kvm_vcpu_ioctl,kvm_cpu_exec,kvm_vcpu_thread_fn,qemu_thread_start,start_thread,clone3
     45 get_fid,v9fs_read,coroutine_trampoline,__correctly_grouped_prefixwc,??
     15 get_fid,v9fs_walk,coroutine_trampoline,__correctly_grouped_prefixwc,??
     11 alloc_fid,v9fs_walk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      8 get_fid,v9fs_getattr,coroutine_trampoline,__correctly_grouped_prefixwc,??
      5 clunk_fid,v9fs_xattrwalk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      5 alloc_fid,v9fs_xattrwalk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      4 __lll_lock_wait,pthread_mutex_lock@@GLIBC_2.2.5,qemu_mutex_lock_impl,qemu_mutex_lock_iothread_impl,flatview_write_continue,flatview_write,address_space_rw,kvm_cpu_exec,kvm_vcpu_thread_fn,qemu_thread_start,start_thread,clone3
      3 get_fid,v9fs_xattrwalk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      3 get_fid,v9fs_open,coroutine_trampoline,__correctly_grouped_prefixwc,??
      2 clunk_fid,v9fs_clunk,coroutine_trampoline,__correctly_grouped_prefixwc,??
      1 get_fid,v9fs_readlink,coroutine_trampoline,__correctly_grouped_prefixwc,??
      1 get_fid,v9fs_readdir,coroutine_trampoline,__correctly_grouped_prefixwc,??
      1 address_space_translate_internal,flatview_do_translate.isra,address_space_map,virtqueue_map_desc,virtqueue_split_pop,virtio_blk_handle_vq,virtio_queue_notify_vq.part,aio_dispatch_handler,aio_dispatch,aio_ctx_dispatch,g_main_context_dispatch,main_loop_wait,qemu_main_loop,main
      1</code></pre>
<p>The most common points in the code were:</p>
<ol type="1">
<li>Locking resources, at 1500 and 743 thread samples. Since the top result was on worker threads, I guessed that that wasn‚Äôt interesting and they were just waiting for work to become available.</li>
<li>Waiting for things to happen inside the VM, at 100 and 53 thread samples. That didn‚Äôt seem like a relevant issue though ‚Äî I‚Äôd expect a VM monitor to be waiting for its guest to need something most of the time.</li>
</ol>
<p>The rest of the numbers were small enough that I (erroneously!) considered them uninteresting too. At this point I became frustrated enough with this branch of my investigation that I gave up on it.</p>
<p>I moved on to <a href="https://github.com/yoshinorim/quickstack/">quickstack</a>, <a href="https://github.com/NixOS/nixpkgs/pull/190335">packaged it</a>, and observed that it couldn‚Äôt get me any information on threads, before going all the way back to the Stack Overflow question to follow <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">the other link</a> provided in the answer.</p>
<h2 id="flame-graphs-with-perf">Flame graphs with perf</h2>
<p>This was what really got me somewhere. After recording performance data using <code>perf record -F max -a -g -- sleep 20</code> while the build was running, I was able to generate a flame graph which made the source of the performance problems quite apparent. The following command:</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span>perf</span> script <span>|</span> <span>stackcollapse-perf.pl</span> <span>|</span>  <span># taken from the flamegraph page linked above</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span>grep</span> ^qemu <span>|</span> <span># We&#39;re only interested in the qemu process</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span>awk</span> <span>&#39;/unknown/ { gsub(&#34;(\\[unknown];){1,}&#34;, &#34;[unknown...];&#34;, $0) } { print }&#39;</span> <span>|</span> <span># Make the graph a lot less tall by collapsing multiple consecutive unknown stack frames together</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span>flamegraph.pl</span> <span>&gt;</span> flamegraph.svg <span># and generate a flamegraph</span></span></code></pre></div>
<p>produced this nifty interactive SVG graph:</p>

<p>The prevalence of ‚Äúfid‚Äù-related functions is quite apparent in this graph. So I jumped into the 9p docs and QEMU source code to find out that fids are numbers which refer to open files in a 9p connection, similar to file descriptors in the POSIX file API ‚Äî so there is one fid for every file that the guest has open.</p>
<p>Let‚Äôs look at the previous implementation of <code>get_fid</code>, which QEMU‚Äôs 9p server uses in implementations of <code>stat</code> (getting file metadata), <code>open</code> (opening a file), and <code>read</code> (reading data from an open file), amongst others:</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span>static</span> V9fsFidState <span>*</span>coroutine_fn get_fid<span>(</span>V9fsPDU <span>*</span>pdu<span>,</span> <span>int32_t</span> fid<span>)</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span>{</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span>int</span> err<span>;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    V9fsFidState <span>*</span>f<span>;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    V9fsState <span>*</span>s <span>=</span> pdu<span>-&gt;</span>s<span>;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span>// Blog note: I&#39;ve omitted some parts that are irrelevant to performance here.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    QSIMPLEQ_FOREACH<span>(</span>f<span>,</span> <span>&amp;</span>s<span>-&gt;</span>fid_list<span>,</span> next<span>)</span> <span>{</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span>if</span> <span>(</span>f<span>-&gt;</span>fid <span>==</span> fid<span>)</span> <span>{</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            <span>return</span> f<span>;</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span>}</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span>}</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span>return</span> NULL<span>;</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span>}</span></span></code></pre></div>
<p>QEMU iterates over <code>fid_list</code> to find the desired fid‚Äôs data. Finding an entry in a list by iterating over has a time complexity of <span><em>O</em>(<em>n</em>)</span> where <span><em>n</em></span> is the size of the list ‚Äî in this case, the list of <em>all the files the guest has open</em> ‚Äî so this is expensive! Moreover, some inspection of QSIMPLEQ (QEMU simple queue) reveals that it‚Äôs implemented as a linked list, a data structure which tends to exhibit poor cache locality.</p>
<p>Since my test copies many files from the 9p filesystem, as well as being booted from it, this lookup happens very frequently:</p>
<ul>
<li>One <code>stat</code> for getting a file‚Äôs metadata (permissions in particular)</li>
<li>One <code>open</code> to get a handle on a file</li>
<li>One <code>read</code> for small files, or many for larger files, to get the contents of the file</li>
</ul>
<p>That makes at least 3 operations which perform the lookup, for each of the 278000 files, which bring the inefficient lookup into a hot code path. <em>This</em> was the reason for the slowness.</p>

<p>What we really want is a structure where we can look up entries by fid more cheaply. We can‚Äôt just use an array-based vector, which would consistently give us <span><em>O</em>(1)</span> lookup, because fids are chosen by the client: we can‚Äôt rely on every newly allocated fid just being the smallest unoccupied one, and need to support arbitrary 32-bit integers. I opted for a hash table, as conveniently <a href="https://developer-old.gnome.org/glib/stable/glib-Hash-Tables.html">implemented by glib</a>, which QEMU already depends on. That provides us with <span><em>O</em>(1)</span> best-case complexity, while keeping the worst case at <span><em>O</em>(<em>n</em>)</span>. The exact real-world performance characteristics are significantly more complex than with the linked list, and there may be marginally more suitable data structures (or hash table implementations) out there, but we‚Äôre looking for a big easy win and not micro-optimisation here.</p>
<p>Rewriting the relevant code was surprisingly simple and uneventful, to the point that once I had it compiling, it just worked (an experience I‚Äôm not sure I‚Äôve ever had before with C!). The results were spectacular: my previously &gt;2h test finished in 7 minutes. It also reduces the build time of NixOS‚Äôs ZFS AWS image from 19 minutes to 1. It was pretty clear to me that this needed to go upstream.</p>

<p>QEMU uses an <a href="https://www.qemu.org/docs/master/devel/submitting-a-patch.html">email-based workflow</a>, where patches are sent as emails to a mailing list which maintainers are subscribed to. I hadn‚Äôt done this before, and it went somewhat chaotically, as you can see by looking at the threads
(<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg00472.html">v1</a>
<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg01266.html">v3</a>
<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg04051.html">v1a</a>
<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg04575.html">v5</a>
<a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-10/msg00370.html">v6</a>
) in the list archives. There‚Äôs a lot of space for mistakes in email-based patch submission, and I made several:</p>
<ul>
<li>Forgetting to reply-all when answering review comments, so that the reply is visible to everyone interested and not just the reviewer</li>
<li>Missing the version tag on a resubmission (in my case, this was because of <a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-09/msg01795.html">misleading docs</a>)</li>
<li>Sending a resubmission as a reply to the previous thread (that was just me failing to read the docs)</li>
<li>Losing a bunch of patch-documenting work while using git-publish because <code>git send-email</code> failed (nixpkgs does not enable Git‚Äôs email functionality in the default package, and git-publish unconditionally deletes the working files once the send command has run).</li>
</ul>
<p>But the reviewers, especially Christian Schoenebeck (thanks!) were helpful and patient, and in the end <a href="https://lists.nongnu.org/archive/html/qemu-devel/2022-10/msg00909.html">the patch made it</a> (though an apparently unrelated bug was discovered) and will soon be finding its way to a QEMU version near you!
If you can‚Äôt wait and you use Nix, you can pull the patch in with this overlay:</p>
<pre><code>final: prev: {
  qemu = prev.qemu.overrideAttrs (o: {
    patches = o.patches ++ [ (prev.fetchpatch {
      name = &#34;qemu-9p-performance-fix.patch&#34;;
      url = &#34;https://gitlab.com/lheckemann/qemu/-/commit/8ab70b8958a8f9cb9bd316eecd3ccbcf05c06614.patch&#34;;
      sha256 = &#34;sha256-PSOv0dhiEq9g6B1uIbs6vbhGr7BQWCtAoLHnk4vnvVg=&#34;;
    }) ];
  });
}</code></pre>

<p>While the core of what I changed wasn‚Äôt hugely complex, this adventure had many collateral (positive!) outcomes for me:</p>
<ul>
<li>I wrote Nix packages for <a href="https://github.com/NixOS/nixpkgs/pull/190335">quickstack</a> (including an upstream PR that makes the build more generic) and <a href="https://github.com/NixOS/nixpkgs/pull/190309">git-publish</a>.</li>
<li>I used <code>perf</code> to work on performance issues for the first time, learning how useful it is and basics of how to use it.</li>
<li>I learnt that different methods of profiling can yield wildly different results ‚Äî the results from perf painted a very different picture from the poor man‚Äôs profiler; I don‚Äôt understand why yet, but I‚Äôll spend some more time learning about them soon.</li>
<li>I submitted my first patch to QEMU, getting to know the code a little.</li>
<li>I submitted my second patch to QEMU (trying to fix the patch submission docs).</li>
<li>I learnt how to submit patches by email, and which mistakes to avoid.</li>
<li>I learnt about the 9p protocol.</li>
<li>My tests now run much faster!</li>
<li>And I got the warm fuzzy feeling of having made a valuable contribution.</li>
</ul>
<p>My takeaway from it? Digging into a frustrating problem ‚Äî even if I‚Äôm totally unfamiliar with the code involved and the technology I‚Äôm using ‚Äî can be hugely rewarding!</p>
<p>Not only have I made my own work on the tests a lot more pleasant by reducing turnaround time, but this will go out to <em>all</em> QEMU users, and significantly reduce the load generated by installer tests on NixOS‚Äôs build farm.
This is what I love about working on open source software: if something‚Äôs wrong, I have the means to go in there and fix it, and can pass the benefits on to everyone else who uses it.</p>
<p>It doesn‚Äôt always go as well as this, but this kind of experience can make a great many less successful adventures worth it.</p>
    </section>
</article></div>
  </body>
</html>
