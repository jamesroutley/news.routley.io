<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://os-world.github.io/">Original</a>
    <h1>OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computers</h1>
    
    <div id="readability-page-1" class="page">

  <nav role="navigation" aria-label="main navigation">
    
    
  </nav>


  <section>
    <div>
      <div>
        <div>
          <div>
            
            

            <p><span><sup>1</sup>The University of Hong Kong,</span>
              <span><sup>2</sup>Salesforce Research,</span>
              <span><sup>3</sup>Carnegie Mellon University,</span>
              <span><sup>4</sup>University of Waterloo</span>
            </p>

            
          </div>
        </div>
      </div>
    </div>
  </section>
  <section>
    
  </section>

  <section>
    <div>
      <!-- Main Figure. -->
      
      <div>
        <p><img src="https://os-world.github.io/static/images/task_demonstration.png" width="100%" alt="osworld task_demonstration"/></p><md-block>
          **OSWorld** is a first-of-its-kind scalable, real computer environment for multimodal agents,
          supporting task setup, execution-based evaluation, and interactive learning across operating systems.
          It can serve as a unified environment for evaluating open-ended computer tasks that involve arbitrary
          apps (e.g., task examples in the above Fig). We also create a benchmark of 369 real-world computer
          tasks in **OSWorld** with reliable, reproducible setup and evaluation scripts.
        </md-block>
      </div>
      <!--/ Main Figure. -->
    </div>
  </section>

  <section>
    <div>
      <!-- Abstract. -->
      <div>
        <div>
          <h2>Abstract</h2>
          <p>
            <md-block>
              Autonomous agents that accomplish complex computer tasks with minimal human
              interventions has the potential to transform human-computer interaction, significantly enhancing
              accessibility and productivity. However, existing benchmarks
              either lack an interactive environment or are limited to environments specific to
              certain applications or domains, failing to reflect the diverse and complex nature of real-world computer
              use, thereby limiting the scope of tasks and agent
              scalability. To address this issue, we introduce **OSWorld**, the first-of-its-kind
              scalable, real computer environment for multimodal agents, supporting task setup,
              execution-based evaluation, and interactive learning across various operating systems such as Ubuntu,
              Windows, and macOS. **OSWorld** can serve as a unified,
              integrated computer environment for assessing open-ended computer tasks that
              involve arbitrary applications. Building upon **OSWorld**, we create a benchmark
              of 369 computer tasks involving real web and desktop apps in open domains, OS
              file I/O, and workflows spanning multiple applications. Each task example is
              derived from real-world computer use cases and includes a detailed initial state
              setup configuration and a custom execution-based evaluation script for reliable,
              reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based
              agents on **OSWorld** reveals significant deficiencies in their ability to serve as
              computer assistants. While humans can accomplish over 72.36% of the tasks, the
              best model achieves only 12.24% success, primarily struggling with GUI grounding
              and operational knowledge. Comprehensive analysis using **OSWorld** provides
              valuable insights for developing multimodal generalist agents that were not possible with previous
              benchmarks.
            </md-block>
          </p>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section>
    <div>
      <!-- Environment Infrastructure. -->
      <h2>OSWorld Environment Infrastructure</h2>
      <div>
        <p><img src="https://os-world.github.io/static/images/env.png" width="100%" alt="environment infrastructure"/></p><md-block>The **OSWorld** environment uses a configuration file for
          initializing tasks *(highlighted in red)*, agent interaction, post-processing upon agent
          completion *(highlighted in orange)*, retrieving files and information *(highlighted in yellow)*, and
          executing the evaluation function *(highlighted in green)*. The corresponding configuration items are
          highlighted in colors that match their respective components within the environment. Environments
          can run in parallel on a single host machine for learning or evaluation purposes. Headless operation
          is supported.</md-block>
      </div>
      <!--/ Environment Infrastructure. -->
    </div>
  </section>

  <section>
    <div>
      <!-- Data Statistics. -->
      <h2>Data Statistics and Comparison</h2>
      <md-block>
        Below we present an overview of the main statistics of **OSWorld**, showcasing the outline and a broad spectrum
        of tasks. **OSWorld** contains a total of 369 tasks (and an additional 43 tasks on Windows for
        analysis).</md-block>
      <div>
        <div>
          <div>

            <h5>
              Key statistics of
              <b>OSWorld</b>.<br/>
            </h5><p>
            The “Supp. tasks” refers to the Windows-based tasks, that could
            only be used after
            activation due to copyright restrictions.</p></div>
        </div>
        <div>
          <div>
            <p><img src="https://os-world.github.io/static/images/pie_chart.png" alt="data-composition"/></p><p>
              Distribution of task instructions in <b>OSWorld</b></p>
          </div>
        </div>
      </div>
      <md-block>We make a comparison of **OSWorld** against some other different benchmarks for digital agents as
        presented below. </md-block><!--/ Data Statistics. -->
    </div>
  </section>

  <section>
    <p>

      <h2>Benchmark</h2>
      <md-block>We adopt state-of-the-art LLM and VLM from open-source representatives such as Mixtral and
        CogAgent, and closed-source ones from GPT, Gemini, and Claude families on **OSWorld**, as LLM and VLM agent
        baselines. We also explore methods such as the Set-of-Marks aided
        approach, which has been demonstrated to improve spatial capabilities for visual
        reasoning.
        **We are actively updating the benchmark with new LLMs, VLMs and methods. Pull requests welcomed!**
      </md-block>
    </p>
    <div id="contentCover">
      <!-- Baseline. -->
      <div>
        <div>
          <div>
            <div>
              <div>
                
                
                <div title="A11y tree" id="BoardPanel1">
                  <!-- <div class="content has-text-justified"> -->
                  <md-block>
                    Notice: t = temperature, top-p = top-p cutoff, len = max context length
                  </md-block>
                  <!-- </div> -->
                  <table>
                    <tbody><tr>
                      <th>Rank</th>
                      <th>Model</th>
                      <th>Details</th>
                      <th>Score</th>
                    </tr>

                    <tr>
                      <td>
                        <p>1</p>
                        <span>
                          Mar 20, 2024
                        </span>
                      </td>
                      <td>
                        GPT-4
                        <p>OpenAI</p>
                        <a href="https://arxiv.org/abs/2303.08774">OpenAI, &#39;23</a>
                      </td>
                      <td>
                        <p>t=1.0, top-p=0.9</p>
                        <p>len = 128k</p>
                      </td>
                      <td>12.24</td>
                    </tr>

                    <tr>
                      <td>
                        <p>2</p>
                        <span>
                          April 23, 2024
                        </span>
                      </td>
                      <td>
                        GPT-4 Vision (0409)
                        <p>OpenAI</p>
                        <a href="https://openai.com/research/gpt-4v-system-card">OpenAI, &#39;23</a>
                      </td>
                      <td>
                        <p>t=1.0, top-p=0.9</p>
                        <p>len = 32k</p>
                      </td>
                      <td>10.82</td>
                    </tr>

                    <tr>
                      <td>
                        <p>3</p>
                        <span>
                          Mar 20, 2024
                        </span>
                      </td>
                      <td>
                        Mixtral-8x7B
                        <p>MistralAI</p>
                        <a href="https://arxiv.org/abs/2401.04088">Jiang et al., &#39;24</a>
                      </td>
                      <td>
                        <p>t=1.0, top-p=0.9</p>
                        <p>len = 32k</p>
                      </td>
                      <td>2.98</td>
                    </tr>

                    <tr>
                      <td>
                        <p>4</p>
                        <span>
                          Mar 20, 2024
                        </span>
                      </td>
                      <td>
                        GPT-3.5
                        <p>OpenAI</p>
                        <a href="https://platform.openai.com/docs/models/gpt-3-5-turbo">OpenAI, &#39;23</a>
                      </td>
                      <td>
                        <p>t=1.0, top-p=0.9</p>
                        <p>len = 16,385</p>
                      </td>
                      <td>2.69</td>
                    </tr>

                    <tr>
                      <td>
                        <p>5</p>
                        <span>
                          Mar 20, 2024
                        </span>
                      </td>
                      <td>
                        Gemini-Pro
                        <p> Google</p>
                        <a href="https://arxiv.org/abs/2312.11805"> Gemini Team,
                          Google, &#39;23</a>
                      </td>
                      <td>
                        <p>t=1.0, top-p=0.9</p>
                        <p>len = 32k</p>
                      </td>
                      <td>2.37</td>
                    </tr>
                  </tbody></table>
                </div>

                

                

                

              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>

  <section>

    <div>
      <!-- Analysis. -->
      <h2>Analysis</h2>
      <md-block>We conduct a
        qualitative analysis in the aspect of models, methods, and human to find out factors influencing the
        performance of VLMs in digital
        agent tasks and their underlying behavioral logic. We investigate the impact of task attributes *(such as
        difficulty, feasibility, visual requirement, and GUI complexity)*, input measurements *(such
        as screenshot resolution, the influence of trajectory history, and the effect of UI layout)*, and explore
        whether there are patterns in the agent&#39;s performance across different operating systems. Here is an overview of
        our analysis outcome.</md-block>
      <!--/ Analysis. -->

    </div>
  </section>


  <section>
    <!-- Acknowledgement. -->
    <div>
      <div>
        <div>
          <h2>Acknowledgement</h2>


          <div>
            <p>
              We thank <a href="https://www.sidaw.xyz/">Sida Wang</a>, <a href="https://www.ptshaw.com/">Peter
                Shaw</a>, <a href="https://www.alanesuhr.com/">Alane Suhr</a>, <a href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a>,
              <a href="https://chenwu.io/">Chen Henry Wu</a>,
              <a href="https://pengcheng.in/">Pengcheng Yin</a>, <a href="https://ysymyth.github.io/">Shunyu Yao</a>,
              <a href="https://xinghanlu.com/">Xing Han Lu</a>, <a href="https://sivareddy.in/">Siva Reddy</a>, <a href="https://www.linkedin.com/in/ruoxi-sun-84a85457/">Ruoxi Sun</a>,
              <a href="https://zhiyuan-zeng.github.io/"> Zhiyuan Zeng</a>, and <a href="https://lilei-nlp.github.io/">Lei Li</a> for their helpful feedback on this work
            </p>
          </div>
        </div>
        <!--/ Acknowledgement. -->

      </div>
  </div></section>

  <section>
    <!-- FAQ. -->
    <div>
      <div>
        <div>
          <h2>FAQ</h2>

          <div>
            <h3>Q:</h3>
            <p>What are the running times and costs under different settings?</p>
            <h3>A:</h3>
            <table>
              <thead>
                <tr>
                  <th>Setting</th>
                  <th>Expected Time*</th>
                  <th>Budget Cost (Full Test Set/Small Test Set)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>GPT-4V (screenshot)</td>
                  <td>10h</td>
                  <td>$100 ($10)</td>
                </tr>
                <tr>
                  <td>Gemini-ProV (screenshot)</td>
                  <td>15h</td>
                  <td>0 (0)</td>
                </tr>
                <tr>
                  <td>Claude-3 Opus (screenshot)</td>
                  <td>15h</td>
                  <td>$150 ($15)</td>
                </tr>
                <tr>
                  <td>GPT-4V (a11y tree, SoM, etc.)</td>
                  <td>30h</td>
                  <td>$500 ($50)</td>
                </tr>
              </tbody>
            </table>
            <p>*No environment parallelism. Calculated in April 2024.</p>
          </div>
        </div>
      </div>
      <!--/ FAQ. -->

    </div>
  </section>


  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@misc{OSWorld,
      title={OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments},
      author={Tianbao Xie and Danyang Zhang and Jixuan Chen and Xiaochuan Li and Siheng Zhao and Ruisheng Cao and Toh Jing Hua and Zhoujun Cheng and Dongchan Shin and Fangyu Lei and Yitao Liu and Yiheng Xu and Shuyan Zhou and Silvio Savarese and Caiming Xiong and Victor Zhong and Tao Yu},
      year={2024},
      eprint={2404.07972},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}</code></pre>
    </div>
  </section>


  



</div>
  </body>
</html>
