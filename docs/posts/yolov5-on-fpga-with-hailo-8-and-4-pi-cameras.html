<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/">Original</a>
    <h1>YOLOv5 on FPGA with Hailo-8 and 4 Pi Cameras</h1>
    
    <div id="readability-page-1" class="page"><div role="main">
  <div>
    <div>
      <article role="main">
        <p><strong>See it live:</strong> The demo described in this post will be displayed live at the <a href="https://my.avnet.com/ebv/">EBV Elektronik</a> booth at <a href="https://www.embedded-world.de/">Embedded World 2024</a>
on April 9-11. I’ll be attending too, so get in touch if you’re keen to meet up.
</p>

<p>Over the last few months I’ve been lucky to work with two very talented people on an interesting project for multi-camera machine
vision applications: <a href="https://www.hackster.io/gianluca-filippini">Gianluca Filippini</a> and <a href="https://www.hackster.io/AlbertaBeef">Mario Bergeron</a>.
Back in 2022, I was contacted by Gianluca, an engineer from <a href="https://my.avnet.com/ebv/">EBV Elektronik</a>.
He was using our <a href="https://fpgadrive.com">FPGA Drive FMC</a>, an adapter for connecting Solid-state drives to FPGAs - but he wasn’t interested in
SSDs. He wanted to use it for a new and exciting AI accelerator from a company called <a href="https://hailo.ai/">Hailo</a>. The idea was to
take the Zynq UltraScale+, a powerful MPSoC geared for video applications, and arm it with a <a href="https://hailo.ai/products/ai-accelerators/hailo-8-ai-accelerator/">top performing AI
accelerator</a> to run intelligent vision algorithms on the edge.</p>
<p>It made a lot of sense. If you’ve had any experience with the
AMD Xilinx <a href="https://docs.xilinx.com/r/1.2-English/ug1414-vitis-ai/Deep-Learning-Processor-Unit-DPU">Deep Learning Processor Unit (DPU)</a>, then you know
about the challenges of putting a neural network in the FPGA. If you haven’t used the DPU then let me spoil the punchline: The DPU consumes a tonne of
FPGA resources and the performance is a little disappointing. With the current solutions on offer, it seems that implementing a neural network in the FPGA
fabric is not the way to get the most bang for your buck. A better solution is to externalize the
neural network and reserve the valuable FPGA resources for something it does spectacularly: image processing. The <a href="https://hailo.ai/products/ai-accelerators/hailo-8-m2-ai-acceleration-module/">Hailo-8</a> is a perfect fit; an external AI accelerator with
a high-throughput PCIe interface that runs AI models way faster than any FPGA-based solution, while also being more power efficient.</p>
<p>Months passed. I launched the
<a href="https://camerafmc.com/docs/rpi-camera-fmc/overview/">RPi Camera FMC</a> and Gianluca contacted me again. This time he wanted to run
YOLOv5 on 4x Raspberry Pi cameras simultaneously, accelerated by the <a href="https://hailo.ai/products/ai-accelerators/hailo-8-m2-ai-acceleration-module/">Hailo-8</a>.
This sounded like fun, so I had to get involved.
Over the next few months we worked between continents to build the demo that I’ll be describing in this post. We teamed up with embedded vision and machine learning expert Mario Bergeron,
who was already working on <a href="https://www.hackster.io/AlbertaBeef/supercharge-your-zuboard-with-the-hailo-8-ai-accelerator-79dd76">pairing the Hailo-8 with the ZUBoard</a>.
I learned heaps and had a lot of fun working with these guys. In the end, I hope we’ve created not only a great demo, but also a useful reference
platform for others who want to build similar systems. In this post I’m providing a detailed description of the design and instructions for building
and running it. I’ve also described any hurdles that we went through, and any lessons learned.</p>
<p><strong>Want to try it?</strong> If you’ve got the required hardware and you want to test this for yourself, you can
download the pre-built boot files <a href="https://download.opsero.com/boot/2022-1/zynqmp-hailo-ai_zcu106_petalinux-2022-1.zip">here</a> or you can
download the disk image for the SD card <a href="https://download.opsero.com/boot/2022-1/zynqmp-hailo-ai-zcu106-2022-1.zip">here</a>.
</p>


<p>The architecture of this system involves the following main elements:</p>
<ul>
<li>4x Raspberry Pi cameras each with an independent MIPI capture pipeline that writes to the DDR</li>
<li>Video Mixer based display pipeline that writes to the DisplayPort live interface of the ZynqMP</li>
<li>PCIe root port for interfacing with the Hailo-8</li>
<li>Image processing accelerator (VVAS MultiScaler)</li>
<li>Video Codec Unit (VCU)</li>
</ul>
<p>The block diagram below illustrates the key elements of this design and their interconnections.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-arch.png" alt="ZynqMP Hailo-8 hardware architecture"/></p>
<h2 id="capture-pipelines">Capture pipelines</h2>
<p>There are four main capture/input pipelines in this design, one for each of the 4x Raspberry Pi cameras. Note that it is also possible to connect
USB cameras and/or IP cameras to this design, but I’ll leave that for another post. The capture pipelines are composed of the following IP, implemented in the PL of the Zynq UltraScale+:</p>
<ul>
<li><a href="https://docs.xilinx.com/r/en-US/pg232-mipi-csi2-rx">MIPI CSI-2 Receiver Subsystem IP</a></li>
<li><a href="https://github.com/Xilinx/Vitis_Libraries/tree/main/vision/L3/examples/isppipeline">ISP Pipeline of the Vitis Libraries</a></li>
<li><a href="https://docs.xilinx.com/r/en-US/pg231-v-proc-ss">Video Processing Subsystem IP</a></li>
<li><a href="https://docs.xilinx.com/r/en-US/pg278-v-frmbuf">Frame Buffer Write IP</a></li>
</ul>
<p>The MIPI CSI-2 RX IP is the front of the pipeline and receives image frames from the Raspberry Pi camera over the 2-lane MIPI interface.
The MIPI IP generates an AXI-Streaming output of the frames in RAW10 format. The ISP Pipeline IP performs BPC (Bad Pixel Correction),
gain control, demosaicing and auto white balance, to output the image frames in RGB888 format. The Video Processing Subsystem IP performs
scaling and color space conversion (when needed). The Frame Buffer Write IP then writes the frame data to memory (DDR).
The image below illustrates the MIPI pipeline.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-mipi.png" alt="ZynqMP Hailo-8 MIPI pipeline"/></p>
<h2 id="display-pipeline">Display pipeline</h2>
<p>The display pipeline reads frames from memory (DDR) and sends them to the monitor. As this is a multi-camera design, and we
would like to display all four video streams on the monitor at the same time, the display pipeline needs to have the ability
to combine the four video streams into one. To achieve this, we used the <a href="https://docs.xilinx.com/r/en-US/pg243-v-mix">Video Mixer IP</a>
with four overlay inputs configured as memory mapped AXI4 interfaces - this allows the mixer to read the four video streams from
memory. The Video Mixer requires you to select a single video
format for each input layer, which in this design we selected YUY2 (to this Video Mixer this is YUYV8) . This is the video format
that was used to train the AI model, and by selecting it here we avoid having to add an extra format conversion to the pipeline.
The output of the mixer is set to an AXI-Streaming video interface with YUV 422 format, which is what the DisplayPort live interface
wants to see.</p>
<p>The <a href="https://docs.xilinx.com/r/en-US/pg085-axi4stream-infrastructure/AXI4-Stream-Subset-Converter?tocId=VUGADTVnQ8nDgI86Et2EkA">AXIS Subset Converter</a>
takes the 3-byte output of the Video Mixer and removes the MSB to produce a 2-byte output which is
fed to the <a href="https://docs.xilinx.com/v/u/en-US/pg044_v_axis_vid_out">AXIS to Video Out Converter</a>. The video output is then fed
to the DisplayPort live interface of the Zynq UltraScale+ which is then routed to the monitor.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-display.png" alt="ZynqMP Hailo-8 display pipeline"/></p>
<p>You might be interested to know that before using this display pipeline, we started this design using a different one. The original
display pipeline used the <a href="https://xilinx.github.io/VVAS/2.0/build/html/docs/common/Acceleration-Hardware.html#multiscaler-kernel">VVAS Multi-scaler kernel</a>
to combine the four video streams into one. This was
done with the <a href="https://xilinx.github.io/VVAS/2.0/build/html/docs/common/common_plugins.html#vvas-xcompositor">vvas-xcompositor GStreamer plugin</a>.
The Multi-scaler kernel would read the four 720p video streams from memory. It would combine the four video streams
into a single 2K video (in the FPGA fabric) and then write that 2K video back to memory. The combined video would then be read back from memory by the hardened DisplayPort
interface module and sent to the monitor. This display pipeline worked well, but it turned out to be a bottleneck on performance due to the
extra load it created on the DDR interface. By switching to the Video Mixer based pipeline (shown above), we are no longer writing the combined 2K video
to memory and reading it back for display. Instead, the combined video is kept in the FPGA fabric all the way to the DisplayPort live interface.</p>
<h2 id="vvas-accelerator">VVAS Accelerator</h2>
<p>We also added the <a href="https://xilinx.github.io/VVAS/2.0/build/html/docs/common/Acceleration-Hardware.html#multiscaler-kernel">VVAS Multi-scaler kernel</a>
to this design as a Vitis overlay. The kernel enables hardware accelerated resizing, color space conversion, combining of video frames and more.
The accelerator can be exploited through a set of <a href="https://xilinx.github.io/VVAS/2.0/build/html/docs/common/common_plugins.html">GStreamer plug-ins</a>
that you can access in PetaLinux through the installed VVAS packages.</p>
<p>Based on our experience with GStreamer on the Zynq UltraScale+, at any substantial resolution and frame rate, it is essential to use hardware acceleration
to perform all resizing, color conversions and any other image processing. That is to say, the APU (processor) of the Zynq UltraScale+ generally does not
perform well when it is used to do any of this kind of image processing at a useful frame rate and resolution. To make this clear, let me share
numbers from some of the benchmarking done by Mario Bergeron:</p>
<h3 id="cpu-in-the-pipeline">CPU in the pipeline</h3>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Input video</th>
<th>Output video</th>
<th>Frame rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>None (baseline)</td>
<td>720p YUV</td>
<td>720p YUY</td>
<td>31.5 fps</td>
</tr>
<tr>
<td>Scaling</td>
<td>720p YUV</td>
<td>1080p YUY</td>
<td>0.78 fps</td>
</tr>
<tr>
<td>Scaling + CSC</td>
<td>720p YUV</td>
<td>1080p RGB</td>
<td>0.51 fps</td>
</tr>
</tbody>
</table>
<ul>
<li>CSC: Color space conversion</li>
</ul>
<h3 id="vvas-in-the-pipeline">VVAS in the pipeline</h3>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Input video</th>
<th>Output video</th>
<th>Frame rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>None (baseline)</td>
<td>720p YUV</td>
<td>720p YUY</td>
<td>30.2 fps</td>
</tr>
<tr>
<td>Scaling</td>
<td>720p YUV</td>
<td>1080p YUY</td>
<td>28.9 fps</td>
</tr>
<tr>
<td>Scaling + CSC</td>
<td>720p YUV</td>
<td>1080p RGB</td>
<td>28.3 fps</td>
</tr>
</tbody>
</table>
<ul>
<li>CSC: Color space conversion</li>
</ul>
<p>When the CPU performs the scaling and color space conversion, we see a huge drop in performance. If we were to look at the CPU usage for those cases, we would
see close to 100% on all cores. When instead the scaling and color space conversion is hardware accelerated with VVAS, we see that frame rates only slightly drop - and
if we configured the kernel for a higher throughput (PPC) I’m confident that we wouldn’t see any drop for those operations at those frame rates and resolutions.</p>
<p>To make this a truly powerful reference
platform, we included the VVAS Multi-scaler kernel into this design, even though it is not used by the actual Hailo-8 demo script. I’ll include some demo
scripts to exploit the VVAS accelerator at a later time.</p>
<h2 id="vcu">VCU</h2>
<p>The Zynq UltraScale+ contains a hardened <a href="https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18842546/Xilinx+Zynq+UltraScale+MPSoC+Video+Codec+Unit">Video Codec Unit</a>
(VCU) that can be used to perform video encoding and decoding of multiple video standards. The VCU makes the Zynq UltraScale+ a real monster for video
applications, and it’s a hardened block so we couldn’t <em>not</em> put it in the design. Like the VVAS accelerator, we don’t actually
use the VCU in the Hailo-8 demo script, but it’s there to enable applications such as video streaming over the internet or decoding
video streams from sources such as IP cameras. I will include some demo scripts to exploit the VCU at a later time.</p>
<h2 id="hailo-8-pipeline">Hailo-8 pipeline</h2>
<p>In this design, the Hailo-8 AI accelerator connects to the Zynq UltraScale+ through a single lane PCIe interface. To implement the
PCIe root complex, we used the <a href="https://docs.xilinx.com/r/en-US/pg195-pcie-dma">DMA/Bridge Subsystem for PCI Express IP</a>.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-hailo.png" alt="ZynqMP Hailo-8 PCIe pipeline"/></p>
<p>The PetaLinux projects in this design include the recipes for various Hailo software packages including the
<a href="https://github.com/hailo-ai/hailort-drivers">PCIe driver</a> and <a href="https://github.com/hailo-ai/tappas">Hailo TAPPAS</a>. The beautiful thing
about the Hailo-8 AI accelerator is that <a href="https://hailo.ai">Hailo</a> has provided all of the drivers and software required. It’s seamlessly
integrated with GStreamer so once you have your model, it’s super easy to get it running.</p>

<p>The best way to understand what’s happening under the hood of this system is to look at the end-to-end pipeline. The diagram below
illustrates the flow of image frames from the source (Raspberry Pi cameras) to the sink (DP monitor). At each interface, I’ve labelled the pixel format
and the resolution of the frames going through. By the way, a diagram like this is also very useful for spotting bottlenecks - notice how many times the images have
to be written to memory and read back, and the throughput that requires.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-end-to-end.png" alt="ZynqMP Hailo-8 end-to-end pipeline"/></p>
<p>Here is a description of the flow of image frames through the system:</p>
<ol>
<li>The RPi cameras are configured to output 1080p images at 30 fps</li>
<li>The MIPI CSI2 RX IPs output RAW10 1080p</li>
<li>The ISP Pipeline IPs output RBG888 1080p</li>
<li>The Video Processing Subsystem IPs scale down the images to 720p YUY2 format, which is the resolution and format for which our AI model has been trained</li>
<li>The Hailo-8 reads the 4x video streams from the DDR via PCIe and outputs the detection metadata: bounding box coordinates, object classes and confidence
levels</li>
<li>The CPU buffers the 4x video streams, and uses the detection metadata to overlay the bounding boxes</li>
<li>The Video Mixer reads the 4x 720p video streams, combines them into a single 2K video stream and drives the DisplayPort live interface with the result</li>
</ol>
<p>If you’ve used Raspberry Pi cameras before, you might know that the IMX219 image sensor can support 720p natively. So why then are we requesting 1080p
from the cameras, only to scale them down to 720p with the Video Processing Subsystem? The reason is that
the <a href="https://github.com/Xilinx/linux-xlnx/blob/master/drivers/media/i2c/imx219.c">Linux driver for IMX219</a> does not support 720p resolution (not yet anyway),
so we have to start at 1080p and scale down to the resolution that our AI model was trained for.</p>

<p>The hardware that you will need to run this demo are listed below:</p>
<ul>
<li>1x <a href="https://www.xilinx.com/zcu106">ZCU106</a> Zynq UltraScale+ Evaluation Board</li>
<li>1x <a href="https://camerafmc.com/docs/rpi-camera-fmc/overview/">RPi Camera FMC</a> (Digi-Key: <a href="https://www.digikey.com/en/products/detail/opsero/OP068/19556699">OP068</a>)</li>
<li>4x <a href="https://www.raspberrypi.com/products/camera-module-v2/">Raspberry Pi Camera Module 2</a> (Digi-Key: <a href="https://www.digikey.ca/en/products/detail/raspberry-pi/SC0023/6152810">SC0023</a>)</li>
<li>1x <a href="https://www.fpgadrive.com/docs/fpga-drive-fmc-gen4/overview/">FPGA Drive FMC Gen4</a> (Digi-Key: <a href="https://www.digikey.com/en/products/detail/opsero/OP063/16798386">OP063</a>)</li>
<li>1x <a href="https://hailo.ai/order-starter-kit/">Hailo-8 AI Accelerator</a> Starter Kit M.2 Key M 2280 ET PN:HM218B1C2XAE (see image below)</li>
</ul>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-2.jpg" alt="Hailo-8 AI accelerator"/></p>
<h2 id="monitor-options">Monitor options</h2>
<p>To see the best performance of this demo, you should use a monitor with a resolution of at least 2560x1440. This resolution allows the four 720p
video streams to be displayed in the four quadrants of the monitor without any scaling. You can use either a DisplayPort monitor, or a HDMI monitor
with DisplayPort-to-HDMI adapter. If you decide to use a DisplayPort-to-HDMI adapter, be aware that the majority of adapters I tested <strong>did
not work</strong> with the ZCU106. AMD Xilinx is <a href="https://support.xilinx.com/s/question/0D52E00006iHmNkSAK/display-port-to-hdmi-adapters-do-not-work-at-zcu106-board?language=en_US">aware of this issue</a>
but at the time of writing this they have not provided a list of recommended adapters. I have named and linked below to the adapter that I found
to work. I have no affiliation with the company that makes the adapter or the monitor.</p>
<ul>
<li>1x <a href="https://www.amazon.ca/dp/B0CGV8NSWM">2K 2560x1440 resolution HDMI monitor</a></li>
<li>1x <a href="https://www.amazon.ca/dp/B07ZNNRYFL">Benfei Active DisplayPort to HDMI adapter 4K@60Hz</a></li>
</ul>

<p>In this project we use the <a href="https://camerafmc.com/docs/rpi-camera-fmc/overview/">RPi Camera FMC</a> and the <a href="https://www.fpgadrive.com/docs/fpga-drive-fmc-gen4/overview/">FPGA Drive FMC Gen4</a>, both connected to the FMC connectors on the <a href="https://www.xilinx.com/zcu106">ZCU106</a>. Each of these boards have
different VADJ working ranges, as shown in the table below:</p>
<table>
<thead>
<tr>
<th>FMC</th>
<th>VADJ</th>
<th>ZCU106 connector</th>
</tr>
</thead>
<tbody>
<tr>
<td>RPi Camera FMC</td>
<td>1.2V</td>
<td>HPC0</td>
</tr>
<tr>
<td>FPGA Drive FMC Gen4</td>
<td>1.2-3.3V</td>
<td>HPC1</td>
</tr>
</tbody>
</table>
<p>Both FMC boards have an EEPROM that is programmed with it’s specific voltage requirements, and the ZCU106 board is designed to read those EEPROMs
and settle on a voltage that they both can accept: 1.2VDC. In order to ensure that this process runs smoothly, there are two things that need to be
corrected first:</p>
<ol>
<li>Update the EEPROM binary of the FPGA Drive FMC Gen4</li>
<li>Adjust the fault limit of one of the voltage regulators on the ZCU106</li>
</ol>
<h2 id="1-eeprom-binary-update">1. EEPROM binary update</h2>
<p>If you are using FPGA Drive FMC Gen4 (OP063) with serial number in the range of 630000 to 630209, you will need to update it’s EEPROM contents with the
latest binary. This can be done using the latest <a href="https://www.fpgadrive.com/docs/fpga-drive-fmc-gen4/programming/#fmc-eeprom-tool">Opsero FMC EEPROM Tool</a>.
These earlier units were programmed to accept a voltage of 1.8V only. The update ensures that the FMC card will accept a voltage between 1.2V and 3.3V,
which is the entire operable range of the FMC card.</p>
<h2 id="2-voltage-limit-adjustment">2. Voltage limit adjustment</h2>
<p>We have to do a slight adjustment to one of the power controllers on the ZCU106 in order to allow it to supply a voltage of 1.2V to the FMC cards.
The reason is that the regulator (or more specifically the Digital POL Controller MAX15301, U63) that generates VADJ (the FMC card’s I/O voltage)
was configured by default with a “Power Good ON” limit of 1.511V, meaning that any voltage output below 1.511V would trigger a fault.</p>
<p>To change this limit, you need to do the following:</p>
<ol>
<li>Download and install the <a href="https://www.analog.com/en/resources/evaluation-hardware-and-software/software/software-download.html?swpart=SFW0008830C">Maxim PowerTool software</a>
on your PC</li>
<li>Get access to a Maxim PowerTool (PN:<a href="https://www.digikey.ca/en/products/detail/analog-devices-inc-maxim-integrated/MAXPOWERTOOL002/5086175">MAXPOWERTOOL002#</a>)</li>
<li>Connect the tool to the PMBUS header of the ZCU106 board (shown below)
<img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-1.jpg" alt="PMBUS header"/></li>
<li>Connect the tool to a USB port on a Windows machine</li>
<li>Launch the Maxim PowerTool software</li>
<li>Click the tab of the power controller with address <code>0x18</code>
<img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/maximtool-instructions-2.png" alt="VADJ tab"/></li>
<li>Click the “Faults Set” tab and find the “Power Good ON” fault setting. It should be RED as shown in the image below. Note that the “Vout Under Voltage Fault” might also be
RED, but this one does not affect the VADJ output, so we can leave that one alone (unless you want to fix it)
<img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/maximtool-instructions-6.png" alt="Fault Set tab"/></li>
<li>Change the “Power Good ON” fault level to 1.15V (below our desired 1.2VDC)
<img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/maximtool-instructions-7.png" alt="Power Good ON"/></li>
<li>Click on the “Tools” tab as shown below and select the “STORE_USER_ALL” command from the drop-down menu
<img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/maximtool-instructions-8.png" alt="Store User ALL"/></li>
<li>Click “Write” as shown below
<img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/maximtool-instructions-9.png" alt="Write settings"/></li>
</ol>
<p>Now when the board is power cycled, and both FMC cards are connected, you should see that VADJ is powered up at 1.2V on both FMC cards. You can test
this quickly by plugging in both FMC cards and powering up the board in JTAG mode (SW6 set to ON-ON-ON-ON). If you see the “POWER” LEDs of both FMC
cards light up, then you’re good to go. Note that the FPGA Drive FMC Gen4 has a RED power LED - don’t confuse this with a fault.</p>

<p>The following instructions will show you how to setup the hardware to be able to run this demo. I suggest you follow the instructions
in the order that they are written.</p>
<h2 id="camera-fmc-setup">Camera FMC setup</h2>
<p>To setup the <a href="https://camerafmc.com/docs/rpi-camera-fmc/overview/">RPi Camera FMC</a>, I suggest that you first attach the 4x RPi cameras as shown in the image below. Make sure that each of the flex cables are
firmly locked into place by pushing in the fastening tabs. Make sure that the blue sides of the flex cables are pointed upwards, just like in the image.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-4.jpg" alt="RPi Camera FMC"/></p>
<p>Carefully turn over the FMC card and plug it into the HPC0 connector of the ZCU106 board as shown below. I prefer to do it this way because it’s
more of a challenge to attach the cameras once the FMC card is plugged into the carrier.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-5.jpg" alt="RPi Camera FMC"/></p>
<h2 id="hailo-8-setup">Hailo-8 setup</h2>
<p>You will need to attach a heat sink to the Hailo-8. I have used a 20x20x10mm heatsink and an adhesive thermal pad from Digikey:</p>
<ul>
<li><a href="https://www.digikey.ca/en/products/detail/advanced-thermal-solutions-inc/ATS020020010-SF-7I/18711199">20x20x10mm heatsink PN:ATS020020010-SF-7I</a></li>
<li><a href="https://www.digikey.ca/en/products/detail/3m-tc/19MM-19MM-10-8810/2649858">19x19mm Adhesive thermal pad</a></li>
</ul>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-3.jpg" alt="Hailo-8 AI accelerator"/></p>
<p>Insert the Hailo-8 module into slot 1 (J2) of the <a href="https://www.fpgadrive.com/docs/fpga-drive-fmc-gen4/overview/">FPGA Drive FMC Gen4</a> as shown below.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-6.jpg" alt="Hailo-8 and FPGA Drive FMC Gen4"/></p>
<p>Carefully plug the FMC card into the HPC1 connector of the ZCU106 board as shown below. When doing this, make sure that you support the ZCU106
board from the underside while you apply downward pressure on the FMC card. You don’t want the ZCU106 PCB to bend as you apply pressure.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-7.jpg" alt="ZCU106 HPC0 and HPC1 loaded"/></p>
<h2 id="remaining-connections">Remaining connections</h2>
<p>Make the following remaining connections to the ZCU106 board:</p>
<ul>
<li>
<p>Connect the DisplayPort monitor (or HDMI monitor with DP-to-HDMI adapter as in our case)</p>
</li>
<li>
<p>Connect the USB-UART port to your computer (we use it as a PetaLinux terminal)</p>
</li>
<li>
<p>Connect the power supply</p>
</li>
<li>
<p>Optional: Connect the Ethernet port to your network router if you need network connectivity for SSH access or anything else</p>
</li>
</ul>
<p>The resulting setup should look like this.</p>
<p><img src="https://www.fpgadeveloper.com/multi-camera-yolov5-on-zynq-ultrascale-with-hailo-8-ai-acceleration/images/zynqmp-hailo-ai-8.jpg" alt="ZCU106, Hailo-8 and RPi Camera FMC setup"/></p>

<p>As this project is built on PetaLinux, you will need a Linux machine to build it. The machine should have the following tools installed:</p>
<ul>
<li>Vivado 2022.1</li>
<li>Vitis 2022.1</li>
<li>PetaLinux 2022.1</li>
</ul>
<p><strong>No time?</strong> If you prefer to skip the build process, you can download the pre-built boot files
<a href="https://download.opsero.com/boot/2022-1/zynqmp-hailo-ai_zcu106_petalinux-2022-1.zip">here</a> or you can
download the disk image <a href="https://download.opsero.com/boot/2022-1/zynqmp-hailo-ai-zcu106-2022-1.zip">here</a>.
</p>

<p>The source code for this design is managed in a <a href="https://github.com/fpgadeveloper/zynqmp-hailo-ai">Github repo</a>.
The repo contains submodules so remember to use the <code>--recursive</code> option when cloning. To clone the repo, run:</p>
<div><pre tabindex="0"><code data-lang="fallback">git clone --recursive https://github.com/fpgadeveloper/zynqmp-hailo-ai.git
</code></pre></div><p>Then source the Vivado and PetaLinux tools:</p>
<div><pre tabindex="0"><code data-lang="fallback">source &lt;path-to-petalinux&gt;/2022.1/settings.sh
source &lt;path-to-vivado&gt;/2022.1/settings64.sh
</code></pre></div><p>To build all parts of the project, including Vivado project, accelerator kernel and PetaLinux, run the following:</p>
<div><pre tabindex="0"><code data-lang="fallback">cd zynqmp-hailo-ai/PetaLinux
make petalinux TARGET=zcu106
</code></pre></div><p>The whole build process takes a couple hours on a good machine.</p>

<p>The PetaLinux project that we just built is configured to boot from the SD card, and to store the root filesystem on
the SD card. So we need to create two partitions on the SD card: one for the boot files and another for the root file system.
We then need to copy the boot files and root file system to the corresponding partitions.</p>
<ol>
<li>Plug the SD card into your computer and find it’s device name using the <code>dmesg</code> command. The SD card should be found at the
end of the log, and it’s device name should be something like <code>/dev/sdX</code>, where <code>X</code> is a letter such as a,b,c,d, etc. Note that
you should replace the <code>X</code> in the following instructions.</li>
</ol>
<p><strong>WARNING:</strong> If you cannot find the SD card’s device name, or you are not quite sure, do NOT proceed with the following steps because
you might choose the wrong disk (eg. your PC’s hard drive) and completely mess it up. Only follow the next steps when you are
absolutely sure that you have the device name of the SD card.
</p>

<ol start="2">
<li>
<p>Run <code>fdisk</code> by typing the command (replace <code>sdX</code> with the correct device name):</p>
</li>
<li>
<p>Make the <code>boot</code> partition: type <code>n</code> to create a new partition, then type <code>p</code> to make it primary, then use the default
partition number and first sector. For the last sector, type <code>+1G</code> to allocate 1GB to this partition.</p>
</li>
<li>
<p>Make the <code>boot</code> partition bootable by typing <code>a</code></p>
</li>
<li>
<p>Make the <code>root</code> partition: typing <code>n</code> to create a new partition, then type <code>p</code> to make it primary, then use the default
partition number, first sector and last sector.</p>
</li>
<li>
<p>Save the partition table by typing <code>w</code></p>
</li>
<li>
<p>Format the <code>boot</code> partition (FAT32) by typing:</p>
<div><pre tabindex="0"><code data-lang="bash">sudo mkfs.vfat -F <span>32</span> -n boot /dev/sdX1
</code></pre></div></li>
<li>
<p>Format the <code>root</code> partition (ext4) by typing:</p>
<div><pre tabindex="0"><code data-lang="bash">sudo mkfs.ext4 -L root /dev/sdX2
</code></pre></div></li>
<li>
<p>Copy the boot files to the <code>boot</code> partition of the SD
card: Assuming the boot partition was mounted to <code>/media/user/boot</code>, follow these instructions:</p>
<div><pre tabindex="0"><code data-lang="bash"><span>cd</span> /media/user/boot/
sudo cp &lt;git-repo-path&gt;/PetaLinux/zcu106/images/linux/BOOT.BIN .
sudo cp &lt;git-repo-path&gt;/PetaLinux/zcu106/images/linux/boot.scr .
sudo cp &lt;git-repo-path&gt;/PetaLinux/zcu106/images/linux/image.ub .
sudo cp &lt;git-repo-path&gt;/VitisAccel/zcu106_container/dpu.xclbin .
</code></pre></div><p>If you’re using the pre-built boot files, the relevant files are in the <code>boot</code> folder; just copy them over.</p>
</li>
<li>
<p>Create the root file system by extracting the <code>rootfs.tar.gz</code> file to the root partition. Assuming the root partition was
mounted to <code>/media/user/root</code>, follow these instructions:</p>
<div><pre tabindex="0"><code data-lang="bash"><span>cd</span> /media/user/root/
sudo cp &lt;git-repo-path&gt;/PetaLinux/zcu106/images/linux/rootfs.tar.gz .
sudo tar xvf rootfs.tar.gz -C .
sync
</code></pre></div><p>If you’re using the pre-built boot files, the <code>rootfs.tar.gz</code> file is in the <code>root</code> folder, so just copy it over
and extract it.</p>
</li>
</ol>
<p>Once the sync command returns, you will be able to eject the SD card from the machine and plug it into the ZCU106.</p>

<ol>
<li>Open up a terminal console such as <a href="https://www.putty.org/">Putty</a> and configure it for the UART port of the ZCU106 board.</li>
<li>Power up the board and wait for the PetaLinux boot sequence to complete.</li>
<li>Login to PetaLinux using the username <code>root</code> and password <code>root</code>.</li>
</ol>
<h3 id="display-all-cameras">Display all cameras</h3>
<p>To display all camera video streams on the monitor, run the command <code>displaycams.sh</code>. To end the demo, press <code>Ctrl-C</code>.</p>
<h3 id="run-the-hailo-8-demo">Run the Hailo-8 demo</h3>
<p>To run YOLOv5 on the Hailo-8, and display the results on the monitor, run the command <code>hailodemo.sh</code>. To end the demo, press <code>Ctrl-C</code>.</p>
<p>I’ve filmed the demo running with the four cameras pointed at different sections of a monitor playing a video of some street
scenes. The monitor to the right is displaying the four output video streams with detection results. I filmed the demo in this
way, so that you could get a feeling for the output frame rate and the end-to-end latency.</p>

<p>
  <iframe src="https://www.youtube.com/embed/eSylWeIFcro" allowfullscreen="" title="YouTube Video"></iframe>
</p>


<p>As I mentioned earlier, this demo will be displayed live at the EBV Elektronik booth at <a href="https://www.embedded-world.de/">Embedded World 2024</a>
on April 9-11. I’ll be there too, so come and say “hi”.</p>

<p>You might have noticed that the <a href="https://www.github.com/fpgadeveloper/zynqmp-hailo-ai/">Git repository</a> contains designs for the <a href="https://www.xilinx.com/zcu104">ZCU104</a> and the <a href="https://www.avnet.com/shop/us/products/avnet-engineering-services/aes-zuev-cc-g-3074457345635014226/">Avnet UltraZed-EV Carrier</a> -
both of which only have one FMC connector. To make this possible, I’ve come up with a new and exciting product and a way to get more
out of FMC. I’ll talk more about this in my next post.</p>


        

        
            <hr/>
            
        

        
          

          
        
      </article>

      
        
      


      
        
        
        
          
        
      

    </div>
  </div>
</div></div>
  </body>
</html>
