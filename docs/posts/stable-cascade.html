<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/Stability-AI/StableCascade">Original</a>
    <h1>Stable Cascade</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/collage_1.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/collage_1.jpg" width="800"/></a>
</p>
<p dir="auto">This is the official codebase for <strong>Stable Cascade</strong>. We provide training &amp; inference scripts, as well as a variety of different models you can use.
</p>
<p dir="auto">Moreover, Stable Cascade achieves impressive results, both visually and evaluation wise. According to our evaluation,
Stable Cascade performs best in both prompt alignment and aesthetic quality in almost all comparisons. The above picture
shows the results from a human evaluation using a mix of parti-prompts (link) and aesthetic prompts. Specifically,
Stable Cascade (30 inference steps) was compared against Playground v2 (50 inference steps), SDXL (50 inference steps),
SDXL Turbo (1 inference step) and WÃ¼rstchen v2 (30 inference steps).
<br/></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/comparison.png"><img height="300" src="https://github.com/Stability-AI/StableCascade/raw/master/figures/comparison.png"/></a>
</p>
<p dir="auto">Stable CascadeÂ´s focus on efficiency is evidenced through its architecture and a higher compressed latent space.
Despite the largest model containing 1.4 billion parameters more than Stable Diffusion XL, it still features faster
inference times, as can be seen in the figure below.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/comparison-inference-speed.jpg"><img height="300" src="https://github.com/Stability-AI/StableCascade/raw/master/figures/comparison-inference-speed.jpg"/></a>
</p>
<hr/>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/collage_2.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/collage_2.jpg" width="800"/></a>
</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-model-overview" aria-hidden="true" tabindex="-1" href="#model-overview"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Model Overview</h2>
<p dir="auto">Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images,
hence the name &#34;Stable Cascade&#34;.
Stage A &amp; B are used to compress images, similarly to what the job of the VAE is in Stable Diffusion.
However, as mentioned before, with this setup a much higher compression of images can be achieved. Furthermore, Stage C
is responsible for generating the small 24 x 24 latents given a text prompt. The following picture shows this visually.
Note that Stage A is a VAE and both Stage B &amp; C are diffusion models.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/model-overview.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/model-overview.jpg" width="600"/></a>
</p>
<p dir="auto">For this release, we are providing two checkpoints for Stage C, two for Stage B and one for Stage A. Stage C comes with
a 1 billion and 3.6 billion parameter version, but we highly recommend using the 3.6 billion version, as most work was
put into its finetuning. The two versions for Stage B amount to 700 million and 1.5 billion parameters. Both achieve
great results, however the 1.5 billion excels at reconstructing small and fine details. Therefore, you will achieve the
best results if you use the larger variant of each. Lastly, Stage A contains 20 million parameters and is fixed due to
its small size.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" tabindex="-1" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h2>
<p dir="auto">This section will briefly outline how you can get started with <strong>Stable Cascade</strong>.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-inference" aria-hidden="true" tabindex="-1" href="#inference"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference</h3>
<p dir="auto">Running the model can be done through the notebooks provided in the <a href="https://github.com/Stability-AI/StableCascade/blob/master/inference">inference</a> section. You will find more
details regarding downloading the models, compute requirements as well as some tutorials on how to use the models.
Specifically, there are four notebooks provided for the following use-cases:</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-text-to-image" aria-hidden="true" tabindex="-1" href="#text-to-image"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Text-to-Image</h4>
<p dir="auto">A compact <a href="https://github.com/Stability-AI/StableCascade/blob/master/inference/text_to_image.ipynb">notebook</a> that provides you with basic functionality for text-to-image,
image-variation and image-to-image.</p>
<ul dir="auto">
<li>Text-to-Image</li>
</ul>
<p dir="auto"><code>Cinematic photo of an anthropomorphic penguin sitting in a cafe reading a book and having a coffee.</code></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/text-to-image-example-penguin.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/text-to-image-example-penguin.jpg" width="800"/></a>
</p>
<ul dir="auto">
<li>Image Variation</li>
</ul>
<p dir="auto">The model can also understand image embeddings, which makes it possible to generate variations of a given image (left).
There was no prompt given here.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/image-variations-example-headset.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/image-variations-example-headset.jpg" width="800"/></a>
</p>
<ul dir="auto">
<li>Image-to-Image</li>
</ul>
<p dir="auto">This works just as usual, by noising an image up to a specific point and then letting the model generate from that
starting point. Here the left image is noised to 80% and the caption is: <code>A person riding a rodent.</code></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/image-to-image-example-rodent.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/image-to-image-example-rodent.jpg" width="800"/></a>
</p>
<p dir="auto">Furthermore, the model is also accessible in the diffusers ðŸ¤— library. You can find the documentation and usage <a href="https://huggingface.co/stabilityai/stable-cascade" rel="nofollow">here</a>.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-controlnet" aria-hidden="true" tabindex="-1" href="#controlnet"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ControlNet</h4>
<p dir="auto">This <a href="https://github.com/Stability-AI/StableCascade/blob/master/inference/controlnet.ipynb">notebook</a> shows how to use ControlNets that were trained by us or how to use one that
you trained yourself for Stable Cascade. With this release, we provide the following ControlNets:</p>
<ul dir="auto">
<li>Inpainting / Outpainting</li>
</ul>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/controlnet-paint.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/controlnet-paint.jpg" width="800"/></a>
</p>
<ul dir="auto">
<li>Face Identity</li>
</ul>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/controlnet-face.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/controlnet-face.jpg" width="800"/></a>
</p>
<p dir="auto"><strong>Note</strong>: The Face Identity ControlNet will be released at a later point.</p>
<ul dir="auto">
<li>Canny</li>
</ul>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/controlnet-canny.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/controlnet-canny.jpg" width="800"/></a>
</p>
<ul dir="auto">
<li>Super Resolution</li>
</ul>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/controlnet-sr.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/controlnet-sr.jpg" width="800"/></a>
</p>
<p dir="auto">These can all be used through the same notebook and only require changing the config for each ControlNet. More
information is provided in the <a href="https://github.com/Stability-AI/StableCascade/blob/master/inference">inference guide</a>.</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-lora" aria-hidden="true" tabindex="-1" href="#lora"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>LoRA</h4>
<p dir="auto">We also provide our own implementation for training and using LoRAs with Stable Cascade, which can be used to finetune
the text-conditional model (Stage C). Specifically, you can add and learn new tokens and add LoRA layers to the model.
This <a href="https://github.com/Stability-AI/StableCascade/blob/master/inference/lora.ipynb">notebook</a> shows how you can use a trained LoRA.
For example, training a LoRA on my dog with the following kind of training images:</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/fernando_original.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/fernando_original.jpg" width="800"/></a>
</p>
<p dir="auto">Lets me generate the following images of my dog given the prompt:
<code>Cinematic photo of a dog [fernando] wearing a space suit.</code></p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/fernando.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/fernando.jpg" width="800"/></a>
</p>
<h4 tabindex="-1" dir="auto"><a id="user-content-image-reconstruction" aria-hidden="true" tabindex="-1" href="#image-reconstruction"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Image Reconstruction</h4>
<p dir="auto">Lastly, one thing that might be very interesting for people, especially if you want to train your own text-conditional
model from scratch, maybe even with a completely different architecture than our Stage C, is to use the (Diffusion)
Autoencoder that Stable Cascade uses to be able to work in the highly compressed space. Just like people use Stable
Diffusion&#39;s VAE to train their own models (e.g. Dalle3), you could use Stage A &amp; B in the same way, while
benefiting from a much higher compression, allowing you to train and run models faster. </p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/original.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/original.jpg" width="800"/></a>
</p>
<p dir="auto">You can encode these images to a compressed size of <code>4 x 16 x 24 x 24</code>, giving you a spatial compression factor of
<code>1024 / 24 = 42.67</code>. Afterwards you can use Stage A &amp; B to decode the images back to <code>4 x 3 x 1024 x 1024</code>, giving you
the following output:</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Stability-AI/StableCascade/blob/master/figures/reconstructed.jpg"><img src="https://github.com/Stability-AI/StableCascade/raw/master/figures/reconstructed.jpg" width="800"/></a>
</p>
<p dir="auto">As you can see, the reconstructions are surprisingly close, even for small details. Such reconstructions are not
possible with a standard VAE etc. The <a href="https://github.com/Stability-AI/StableCascade/blob/master/inference/reconstruct_images.ipynb">notebook</a> gives you more information and easy code to try it out.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-training" aria-hidden="true" tabindex="-1" href="#training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training</h3>
<p dir="auto">We provide code for training Stable Cascade from scratch, finetuning, ControlNet and LoRA. You can find a comprehensive
explanation for how to do so in the <a href="https://github.com/Stability-AI/StableCascade/blob/master/train">training folder</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-remarks" aria-hidden="true" tabindex="-1" href="#remarks"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Remarks</h2>
<p dir="auto">The codebase is in early development. You might encounter unexpected errors or not perfectly optimized training and
inference code. We apologize for that in advance. If there is interest, we will continue releasing updates to it,
aiming to bring in the latest improvements and optimizations. Moreover, we would be more than happy to receive
ideas, feedback or even updates from people that would like to contribute. Cheers.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-citation" aria-hidden="true" tabindex="-1" href="#citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Citation</h2>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{pernias2023wuerstchen,
      title={Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models}, 
      author={Pablo Pernias and Dominic Rampas and Mats L. Richter and Christopher J. Pal and Marc Aubreville},
      year={2023},
      eprint={2306.00637},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre><span>@misc</span>{<span>pernias2023wuerstchen</span>,
      <span>title</span>=<span><span>{</span>Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Pablo Pernias and Dominic Rampas and Mats L. Richter and Christopher J. Pal and Marc Aubreville<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2023<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2306.00637<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>
}</pre></div>
</article></div></div>
  </body>
</html>
