<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ieeexplore.ieee.org/abstract/document/9996741">Original</a>
    <h1>Planting Undetectable Backdoors in Machine Learning Models</h1>
    
    <div id="readability-page-1" class="page"><div id="LayoutWrapper">
				<div>
					<div>
						








	
		
	

	






						
						








<meta name="cToken" content="eyJhbGciOiJIUzUxMiIsInppcCI6IkRFRiJ9.eNqqVkosKFCyUoooyMkvSlXSUcosLgZyK2Dc1AqgrKGZubmJgampiQFQPrEEJmBkYWJQCwAAAP__.C37TcO0-VeaUJgo36JYH88kyV93awpjjyjRKh_FfjD_IePdSaeMI3Wl_Ti0mC4Ek78u2qs0a6ZoN8bXXc_pyuA"/>

	
	
		<meta name="citation_conference" content="2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)"/>
	
	
	
		<meta name="citation_publisher" content="IEEE"/>
	
	
		<meta name="citation_author" content="Shafi Goldwasser"/>
		
			<meta name="citation_author_institution" content="[UC Berkeley and Simons Institute, Berkeley, CA]"/>
		
	
		<meta name="citation_author" content="Michael P. Kim"/>
		
			<meta name="citation_author_institution" content="[UC Berkeley, Berkeley, CA]"/>
		
	
		<meta name="citation_author" content="Vinod Vaikuntanathan"/>
		
			<meta name="citation_author_institution" content="[MIT, Cambridge, MA]"/>
		
	
		<meta name="citation_author" content="Or Zamir"/>
		
			<meta name="citation_author_institution" content="[IAS, Princeton, NJ]"/>
		
	
	
		<meta name="citation_title" content="Planting Undetectable Backdoors in Machine Learning Models : [Extended Abstract]"/>
	
	
		<meta name="citation_abstract" content="Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. Delegation of learning has clear benefits, and at the same time raises serious concerns of trust. This work studies possible abuses of power by untrusted learners.We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate “backdoor key,” the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees.•First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given query access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Moreover, even if the distinguisher can request backdoored inputs of its choice, they cannot backdoor a new input—a property we call non-replicability.•Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm (Rahimi, Recht; NeurIPS 2007). In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is “clean” or contains a backdoor. The backdooring algorithm executes the RFF algorithm faithfully on the given training data, tampering only with its random coins. We prove this strong guarantee under the hardness of the Continuous Learning With Errors problem (Bruna, Regev, Song, Tang; STOC 2021). We show a similar white-box undetectable backdoor for random ReLU networks based on the hardness of Sparse PCA (Berthet, Rigollet; COLT 2013).Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, by constructing undetectable backdoor for an “adversarially-robust” learning algorithm, we can produce a classifier that is indistinguishable from a robust classifier, but where every input has an adversarial example! In this way, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness."/>
	

			
		
		
			<meta name="citation_date" content="31 October 2022 - 03 November 2022"/>
		
		
		
	

	
		
		
		
			<meta name="citation_firstpage" content="931"/>
		
		
			<meta name="citation_lastpage" content="942"/>
		
	
	
		<meta name="citation_doi" content="10.1109/FOCS54457.2022.00092"/>
	
	<meta name="citation_abstract_html_url" content="https://ieeexplore.ieee.org/abstract/document/9996741"/>
		
	<meta name="citation_fulltext_html_url" content="https://ieeexplore.ieee.org/document/9996741"/>
	
	
		
		
			
				<meta name="citation_pdf_url" content="https://ieeexplore.ieee.org/iel7/9996589/9996592/09996741.pdf"/>
			
		
	
	
	
		<meta name="citation_issn" content="2575-8454"/>
	
		<meta name="citation_issn" content="1523-8288"/>
	
	
		<meta name="citation_isbn" content="978-1-6654-5519-0"/>
	
		<meta name="citation_isbn" content="978-1-6654-5520-6"/>
	
	<meta name="citation_language" content="English"/>
	
	<meta name="citation_keywords" content="[Machine learning algorithms, Computational modeling, Perturbation methods, Training data, Machine learning, Observers, Robustness];[computational complexity, digital signatures, learning (artificial intelligence), set theory];[backdoor key, backdoored classifier, backdoored model, backdooring algorithm, machine learning models, random Fourier features learning paradigm, random ReLU networks, RFF learning paradigm, undetectable backdoor planting, white-box undetectable backdoor];[machine learning, cryptography];"/>

	

	<section>

		<div>
			
			
				
				
					<p><strong>Abstract:</strong>
						Given the computational cost and technical expertise required to train machine learning models, users may delegate the task of learning to a service provider. Delegation of learning has clear benefits, and at the same time raises serious concerns of trust. This work studies possible abuses of power by untrusted learners.We show how a malicious learner can plant an undetectable backdoor into a classifier. On the surface, such a backdoored classifier behaves normally, but in reality, the learner maintains a mechanism for changing the classification of any input, with only a slight perturbation. Importantly, without the appropriate “backdoor key,” the mechanism is hidden and cannot be detected by any computationally-bounded observer. We demonstrate two frameworks for planting undetectable backdoors, with incomparable guarantees.•First, we show how to plant a backdoor in any model, using digital signature schemes. The construction guarantees that given query access to the original model and the backdoored version, it is computationally infeasible to find even a single input where they differ. This property implies that the backdoored model has generalization error comparable with the original model. Moreover, even if the distinguisher can request backdoored inputs of its choice, they cannot backdoor a new input—a property we call non-replicability.•Second, we demonstrate how to insert undetectable backdoors in models trained using the Random Fourier Features (RFF) learning paradigm (Rahimi, Recht; NeurIPS 2007). In this construction, undetectability holds against powerful white-box distinguishers: given a complete description of the network and the training data, no efficient distinguisher can guess whether the model is “clean” or contains a backdoor. The backdooring algorithm executes the RFF algorithm faithfully on the given training data, tampering only with its random coins. We prove this strong guarantee under the hardness of the Continuous Learning With Errors problem (Bruna, Regev, Song, Tang; STOC 2021). We show a similar white-box undetectable backdoor for random ReLU networks based on the hardness of Sparse PCA (Berthet, Rigollet; COLT 2013).Our construction of undetectable backdoors also sheds light on the related issue of robustness to adversarial examples. In particular, by constructing undetectable backdoor for an “adversarially-robust” learning algorithm, we can produce a classifier that is indistinguishable from a robust classifier, but where every input has an adversarial example! In this way, the existence of undetectable backdoors represent a significant theoretical roadblock to certifying adversarial robustness.
					</p>
				
				
				
			
			<!-- Need a place for list of files -->

			
		</div>
		
		
		
			
		
		
		<div>
			<div>

				<p><strong>Article #: </strong> 
				</p>
				
				
				
				

					<p><strong>Date of Conference: </strong>
						31 October 2022 - 03 November 2022
					</p>
					<p><strong>Date Added to IEEE Xplore: </strong>
						28 December 2022
					</p>
				


				
					<div>
						<p><strong> <span>ISBN Information:</span>
							</strong>
						</p>
						<div>
							
								<p><strong>Electronic ISBN:</strong> 978-1-6654-5519-0
								</p>
							
								<p><strong>Print on Demand(PoD) ISBN:</strong> 978-1-6654-5520-6
								</p>
							
						</div>
					</div>
				
				
					<div>
						<p><strong> <span>ISSN Information:</span>
							</strong>
						</p>
						<div>
							
								<p><strong>Electronic ISSN:</strong> 2575-8454
								</p>
							
								<p><strong>Print on Demand(PoD) ISSN:</strong> 1523-8288
								</p>
							
						</div>
					</div>
				
				

			</div>
			<div>

				<p><strong>INSPEC Accession Number: </strong>
					22448592
				</p>
				
					
				
				
				

					<div>
						<p><strong>Persistent Link: </strong>
						https://ieeexplore.ieee.org/servlet/opac?punumber=9996589</p></div>
				
				
				
					<p><strong>Publisher: </strong> IEEE
					</p>
				
				

			</div>
		</div>

		

	</section>



<!-- XPL-21560-Added as part of Universal CASA-Dev -->



<!--- This is a picture popup embed for mobilew view.  -->














						






						<!-- BEGIN: tealium in v2/common/template.jsp. We need to include tealiumAnalytics.js here since Angular 2+ app load if you load after commnon.js then tealium value will not be available in angular 2+ app  -->
						






		<!-- BEGIN: TealiumAnalytics.jsp -->
		
		
		
		
		
		
		
		
		
		
		
		
			
			
			
			
				
			
		
		
		
		
		
		
			
				
			
		
			
				
					
					
				
			
	
			
	
			
				
			
		
		

			


			

			
			


		
 		
		<!-- END: TealiumAnalytics.jsp -->
			 

						<!-- END: tealium in v2/common/template.jsp -->
						


























<!-- START OF Angular bundle assets -->




<!-- END OF Angular bundle assets -->

<!-- Usabilla Combicode for IEEE-->
<!-- Begin Usabilla for Websites embed code -->

<!-- end usabilla live embed code -->


<!-- START: Bombora tagging integration XPL-25079-->

<!-- END: Bombora Tagging Integration-->



	

		
		

		
		
		

		
			
		
		
		

		

		
		


		

		
		
		
		
		
		
		
		
		
		
	
	








<g:compress>








		
		
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
						








 



					
					
								
			
				
					
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
		
	



















</g:compress>








		
			
					
			
					
			
					
			
				
					








					
			
					
			
					
			
					
			
					
			
					
			
					
			
		
		
	





	<!--Begin Optional Configuration-->
	
	
	
	
	
	


	
	
	

	<!--End Optional Configuration-->




<!-- Removed due to network issues when loading in China -->
<!-- <script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#pubid=ra-5005a435228f9245" async="async"></script>-->

<!-- Load Mathjax and process the document for Mathjax characters -->


<!-- <script type="text/javascript" src="/xploreAssets/MathJax-274/MathJax.js?config=default"></script> -->






					</div>
				</div>

			
				
			
		</div></div>
  </body>
</html>
