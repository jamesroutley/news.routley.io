<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/">Original</a>
    <h1>A bug that taught me more about PyTorch than years of using it</h1>
    
    <div id="readability-page-1" class="page"><div> <p><code>Expected to fix: my hyperparameters. Actually had to fix: PyTorch backend.</code></p> <p>My training loss plateaued and wouldn‚Äôt budge. Obviously I‚Äôd screwed something up. I tried every hyperparameter combination, rewrote my loss function, spent days assuming I‚Äôd made some stupid mistake. Because it‚Äôs always user error.</p> <p>This time, it wasn‚Äôt. It was a niche PyTorch bug that forced me through layers of abstraction I normally never think about: optimizer internals, memory layouts, dispatch systems, kernel implementations. Taught me more about the framework than years of using it.</p> <p>I had a surprisingly fun time with this bug hunt and wrote up the whole investigation step-by-step, explaining framework internals as they become necessary to crack the case. If you enjoy debugging mysteries or find that tracking down bugs teaches you more than docs ever could, this might resonate. üïµÔ∏è‚Äç‚ôÄÔ∏è</p> <p>Debugging post-mortems sometimes make me worry I wouldn‚Äôt have been smart enough to figure them out myself. So I structured this walkthrough to show the reasoning behind each step: what clues suggested each move, why I tested that hypothesis, why certain results pointed where they did. While the investigation took time and persistence, it didn‚Äôt require any particular expertise or wizardry‚Äî just observation and willingness to keep digging. I‚Äôve included background knowledge exactly when you need it to understand the next step‚Äîthink of it as an excuse to learn (or re-learn) PyTorch internals through a real problem. If you‚Äôd prefer to jump straight to reproducing the bug yourself, check out the <a href="https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug" rel="external nofollow noopener" target="_blank">minimal reproduction script and walkthrough</a> on GitHub. Otherwise, join me on the investigation!</p> <p><strong>Table of Contents:</strong> ü§î <a href="#the-mystery-a-plateauing-loss">The Mystery: A Plateauing Loss</a>‚Ä¶‚Ä¶ üîé <a href="#isolating-the-problem">Isolating the Problem</a>‚Ä¶‚Ä¶ üíª <a href="#device-specific-differences">Device-Specific Differences</a>‚Ä¶‚Ä¶ ‚å∫ <a href="#tensor-memory-layouts">Tensor Memory Layouts</a>‚Ä¶‚Ä¶ üíî <a href="#identifying-the-broken-operations">Identifying the Broken Operations</a>‚Ä¶‚Ä¶. üçé <a href="#inside-the-kernel-implementation">Inside the Kernel Implementation</a>‚Ä¶‚Ä¶ üïµÔ∏è‚Äç‚ôÄÔ∏è <a href="#case-closed">Case Closed</a></p> <details> <summary><b>TL;DR - Just tell me the bug</b></summary> <div> <p><strong>The Bug:</strong> A PyTorch GPU kernel bug silently failed when writing to non-contiguous memory, causing my model‚Äôs encoder weights to freeze during training on Apple Silicon (MPS backend, PyTorch &lt;2.4).</p> <p><strong>The Technical Details:</strong> PyTorch‚Äôs MPS (Apple Silicon GPU) backend had a kernel bug where <code>addcmul_</code> and <code>addcdiv_</code> operations silently fail when writing to non-contiguous output tensors.</p> <p><strong>Why It Caused the Training Plateau:</strong></p> <ul> <li>Encoder weights initialized as transpose of decoder ‚Üí non-contiguous memory layout</li> <li>Adam‚Äôs state tensors inherited this layout (<code>exp_avg</code> and <code>exp_avg_sq</code> became non-contiguous)</li> <li>MPS kernels for <code>addcmul_</code>/<code>addcdiv_</code> don‚Äôt handle non-contiguous outputs correctly</li> <li>Results computed but written to temporary buffer instead of actual tensor</li> <li>For the non-contiguous encoder‚Äôs Adam parameters, <code>exp_avg_sq.addcmul_()</code> doesn‚Äôt update ‚Üí value stays zero, then the parameter update via <code>addcdiv_</code> also fails ‚Üí complete silent freeze</li> </ul> <p><strong>The Fix:</strong></p> <ul> <li> <strong>Adjust your code:</strong> Make weights contiguous at initialization</li> <li> <strong>Upgrade PyTorch:</strong> Upgrade to PyTorch ‚â•2.4 (fixes <code>addcmul_</code>/<code>addcdiv_</code>)</li> <li> <strong>(Complete fix) Upgrade your Operating System:</strong> Upgrade to macOS 15+ (native non-contiguous tensor support)</li> </ul> <p><strong>Current Status:</strong> Random operations (<code>normal_</code>, <code>uniform_</code>, etc.) still have this bug on macOS &lt; 15 as of PyTorch 2.10 (I submitted a <a href="https://github.com/pytorch/pytorch/pull/165267" rel="external nofollow noopener" target="_blank">PR</a> to fix this). Other MPS operations may be affected.</p> <p><strong>Reproduction:</strong> A minimal reproduction script &amp; walkthrough is available at <a href="https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug" rel="external nofollow noopener" target="_blank">https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug</a>.</p> </div> </details> <h2 id="the-mystery-a-plateauing-loss">The Mystery: A Plateauing Loss</h2> <div> <figure> <picture> <source srcset="/assets/img/the_bug_that_taught_me_pytorch_post/loss_plateau-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/loss_plateau-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/loss_plateau-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="https://elanapearl.github.io/assets/img/the_bug_that_taught_me_pytorch_post/loss_plateau.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p>Training loss plateaued way too early. This felt like a standard hyperparameter issue- but I‚Äôd trained this same architecture on similar data with similar hyperparameters countless times and hit much lower losses.</p> <p>What had changed? Those runs were months old. I tried reproducing them exactly, but couldn‚Äôt pin down the exact environment‚Äîthe codebase had evolved through multiple projects, refactors, and dependency updates. Without a clean ‚Äúbefore vs after,‚Äù I had to debug forward.</p> <p>The architecture itself is straightforward: a two-layer sparse autoencoder (encoder ‚Äì&gt; sparse hidden layer ‚Äì&gt; decoder). However, it has some training quirks the <em>could</em> be potential culprits: the hidden layer uses TopK sparsity, where only the k largest activations remain (others are zeroed); the training process includes some manual gradient adjustments (gradient clipping for stability and modifications to decoder weight gradients); there‚Äôs an auxiliary loss term to encourage feature activation.</p> <p>Even though I thought my initial hyperparameters were already well-tested, I tried everything: varied learning rates, tested different schedules, tried different k values and hidden dimensions, adjusted the auxiliary loss coefficients.</p> <p>Nothing made a difference.</p> <p>Meanwhile, my actual research sat on hold while I was stuck second-guessing everything: was my code broken? My data corrupted? And the creeping doubt- I‚Äôve been doing ML for years, why can‚Äôt I make a simple two-layer autoencoder train properly?</p> <p>The model was small enough that I was training on my MacBook (using the Apple Silicon GPU) and simple enough I could actually inspect every parameter. So after the standard checks turned up nothing, I started looking at the weights directly.</p> <p>I visualized the weights at initialization and after the first few training steps. The decoder weights were updating- values shifting, gradients being applied, nothing crazy. <strong>But the encoder weights‚Ä¶ weren‚Äôt updating at all.</strong> No NaNs, no suspicious patterns‚Ä¶ they just‚Ä¶ weren‚Äôt changing. They stayed exactly at their initialized values, down to the last decimal place.</p> <p>Both layers participate in the same forward and backward pass. Why would one update and the other freeze completely?</p> <h2 id="isolating-the-problem">Isolating the Problem</h2> <h3 id="are-gradients-flowing">Are Gradients Flowing?</h3> <p>First check: are gradients even making it back to the encoder? The TopK sparsity should make gradients sparse‚Äîonly the k activated features get gradients through backprop, the rest are zeroed. But maybe I messed up the implementation so that <em>no</em> encoder gradients flow at all? Or the manual gradient adjustments I was making somehow blocked everything?</p> <p>After <code>loss.backward()</code>, the gradient statistics were:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Encoder</strong></th> <th><strong>Decoder</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Max Grad</strong></td> <td>2.35e6</td> <td>6.64e6</td> </tr> <tr> <td><strong>Sparsity</strong></td> <td>88.5% zeros</td> <td>88.5% zeros</td> </tr> </tbody> </table> <p>The encoder gradients were there- and they were pretty big (as intended for my dataset)! And they were sparse (majority zeros) which was also expected, but there were still plenty of non-zero gradients. So gradients are definitely being calculated.</p> <h3 id="is-it-the-optimizer">Is It the Optimizer?</h3> <p>Since the gradients exist but weights aren‚Äôt updating, the optimizer must be doing something wrong. Testing with a simpler optimizer, stochastic gradient descent (SGD):</p> <div><div><pre><code><span># Manual SGD update
</span><span>with</span> <span>torch</span><span>.</span><span>no_grad</span><span>():</span>
    <span>model</span><span>.</span><span>encoder</span><span>.</span><span>weight</span> <span>-=</span> <span>0.001</span> <span>*</span> <span>model</span><span>.</span><span>encoder</span><span>.</span><span>weight</span><span>.</span><span>grad</span>
<span># Encoder weights change! ‚úì
</span>
<span># Torch SGD update
</span><span>sgd_optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>SGD</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>0.001</span><span>)</span>
<span>sgd_optimizer</span><span>.</span><span>step</span><span>()</span>
<span># Encoder weights change! ‚úì
</span>
<span># But with Adam...
</span><span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>0.001</span><span>)</span>
<span>optimizer</span><span>.</span><span>step</span><span>()</span>
<span># Encoder weights don&#39;t change! ‚úó
</span></code></pre></div></div> <div> <p>ü§î</p> <p> The issue is localized to Adam specifically! But why would Adam fail on the encoder but work perfectly on the decoder? </p> </div> <hr/> <h3 id="how-adam-works">How Adam Works</h3> <p>To understand what might be breaking, I need to understand what Adam actually does differently from simple gradient descent.</p> <details open=""> <summary><b>Understanding Adam&#39;s Algorithm (click to collapse if familiar)</b></summary> <div> <h3 id="problems-with-vanilla-sgd">Problems with Vanilla SGD</h3> <p>SGD updates all parameters the same way:</p> <div> <div><pre><code><span># SGD: one learning rate for everything
</span><span>param</span> <span>=</span> <span>param</span> <span>-</span> <span>learning_rate</span> <span>*</span> <span>gradient</span>
</code></pre></div> </div> <p>This has a few problems:<d-footnote>SGD has other problems too (hence all the optimizer research), but these are the ones Adam addresses.</d-footnote></p> <ol> <li> <p><strong>Different parameters need different learning rates.</strong> Some parameters might consistently get gradients around 1000 while others get 0.01. With SGD‚Äôs fixed learning rate, you‚Äôre stuck: either you move too slowly on small gradients or you overshoot wildly on large ones.</p> </li> <li> <p><strong>The learning rate needs to change over time.</strong> Early in training, you want big steps to explore the space. Later, you need tiny steps to settle into a minimum. SGD requires manually decaying the learning rate on a schedule.</p> </li> </ol> <h3 id="adams-solution-adaptive-learning-rates-via-gradient-magnitude-tracking">Adam‚Äôs Solution: Adaptive Learning Rates via Gradient Magnitude Tracking</h3> <p>Adam maintains two pieces of state per parameter and uses two hyperparameters to control how these states evolve:</p> <p><strong>State variables</strong> (initialized to zero for each parameter):</p> <ul> <li> <code>exp_avg</code>: Running average of gradients (first moment)</li> <li> <code>exp_avg_sq</code>: Running average of squared gradients (second moment)</li> </ul> <p><strong>Hyperparameters</strong> (typically beta_1=0.9, beta_2=0.999):</p> <ul> <li> <code>beta_1</code>: Decay rate for first moment (momentum)</li> <li> <code>beta_2</code>: Decay rate for second moment (gradient magnitude history)</li> </ul> <p><strong>Here‚Äôs the simplified algorithm:</strong></p> <p>Initialize state (done once per parameter)</p> <div> <div><pre><code><span>exp_avg</span> <span>=</span> <span>zeros_like</span><span>(</span><span>param</span><span>)</span>
<span>exp_avg_sq</span> <span>=</span> <span>zeros_like</span><span>(</span><span>param</span><span>)</span>
<span>step</span> <span>=</span> <span>0</span>
</code></pre></div> </div> <p>Each training step:</p> <div> <div><pre><code><span># Update moments with exponential moving averages
</span><span>exp_avg</span> <span>=</span> <span>beta_1</span> <span>*</span> <span>exp_avg</span> <span>+</span> <span>(</span><span>1</span> <span>-</span> <span>beta_1</span><span>)</span> <span>*</span> <span>grad</span>
<span>exp_avg_sq</span> <span>=</span> <span>beta_2</span> <span>*</span> <span>exp_avg_sq</span> <span>+</span> <span>(</span><span>1</span> <span>-</span> <span>beta_2</span><span>)</span> <span>*</span> <span>grad</span><span>**</span><span>2</span>

<span># Update step count
# (It effectively starts at 1 to avoid division by zero in bias correction)
</span><span>step</span> <span>+=</span> <span>1</span>

<span># Bias correction
</span><span>exp_avg_corrected</span> <span>=</span> <span>exp_avg</span> <span>/</span> <span>(</span><span>1</span> <span>-</span> <span>beta_1</span><span>**</span><span>step</span><span>)</span>
<span>exp_avg_sq_corrected</span> <span>=</span> <span>exp_avg_sq</span> <span>/</span> <span>(</span><span>1</span> <span>-</span> <span>beta_2</span><span>**</span><span>step</span><span>)</span>

<span># Adaptive parameter update
</span><span>param</span> <span>=</span> <span>param</span> <span>-</span> <span>lr</span> <span>*</span> <span>exp_avg_corrected</span> <span>/</span> <span>(</span><span>sqrt</span><span>(</span><span>exp_avg_sq_corrected</span><span>)</span> <span>+</span> <span>Œµ</span><span>)</span>
</code></pre></div> </div> <p><strong>What Each Moment Does:</strong></p> <ul> <li> <p><strong>First moment (<code>exp_avg</code>)</strong>: Smooths out noisy gradients by averaging recent directions‚Äîlike momentum in physics. When gradients oscillate (+10, -10, +8, -9‚Ä¶), the positive and negative values cancel out, revealing there‚Äôs no consistent direction. Beta_1=0.9 means ‚Äúkeep 90% of old momentum, add 10% of new gradient.‚Äù This smoothed momentum is what gets multiplied by the learning rate in the parameter update: <code>lr * exp_avg</code>.</p> </li> <li> <p><strong>Second moment (<code>exp_avg_sq</code>)</strong>: Tracks typical gradient <strong>magnitude</strong> for each parameter by averaging squared gradients. Squaring removes the +/- sign (both +10 and -10 become 100), preventing cancellation. Beta_2=0.999 means ‚Äúkeep 99.9% of magnitude history, add 0.1% of new squared gradient.‚Äù This magnitude normalizes the momentum-based update: <code>lr * exp_avg / sqrt(exp_avg_sq)</code>. Parameters with consistently large gradients get their updates scaled down (large denominator), while parameters with small gradients get boosted (small denominator). This is how Adam achieves <strong>adaptive per-parameter learning rates</strong>.</p> </li> <li> <p><strong>Epsilon (<code>Œµ=1e-8</code>)</strong>: Prevents division by zero.</p> </li> </ul> <p><strong>Bias Correction:</strong></p> <p>Both moments start at zero, causing early estimates to be biased toward zero. The correction factor <code>(1 - Œ≤**step)</code> provides a large boost early to counteract this, effectively ‚Äúwarming up‚Äù the optimizer over the first ~1000-3000 steps. As training progresses, the correction approaches 1 and has negligible effect.</p> <div> <figure> <picture> <source srcset="/assets/img/the_bug_that_taught_me_pytorch_post/bias_correction_early-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/bias_correction_early-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/bias_correction_early-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="https://elanapearl.github.io/assets/img/the_bug_that_taught_me_pytorch_post/bias_correction_early.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p>The second moment works similarly. Without correction, <code>exp_avg_sq</code> would be only 0.1% of gradient¬≤ at step 1, but bias correction restores it to the full value.</p> <p>For a deeper dive into Adam‚Äôs design and intuition, as well as other optimizers that use momentum and adaptive learning rates (RMSprop, AdaGrad, etc.), check out <a href="https://cs231n.github.io/neural-networks-3/#update" rel="external nofollow noopener" target="_blank">Stanford‚Äôs CS231n notes on optimization</a>.</p> </div> </details> <p>Knowing what Adam <em>should</em> be doing, let‚Äôs look at the state it‚Äôs maintaining (those <code>exp_avg</code> and <code>exp_avg_sq</code> tensors that track momentum and variance) to see what it‚Äôs <em>actually</em> doing.</p> <h3 id="examining-adams-state">Examining Adam‚Äôs State</h3> <p>For our frozen encoder, the maximum values in each state tensor were:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Encoder</strong></th> <th><strong>Decoder</strong></th> </tr> </thead> <tbody> <tr> <td><strong>exp_avg</strong></td> <td>1.96e+05</td> <td>1.70e+06</td> </tr> <tr> <td><strong>exp_avg_sq</strong></td> <td><span>0</span></td> <td>1.18e+11</td> </tr> </tbody> </table> <p>Wait, WHAT?! The encoder‚Äôs <code>exp_avg_sq</code> is zero despite having momentum accumulated in <code>exp_avg</code>.</p> <p>This feels mathematically impossible‚Ä¶ The second moment (<code>exp_avg_sq</code>) is zero despite non-zero gradients. Since <code>exp_avg_sq</code> stores squared gradients, it should NEVER be zero if gradients are non-zero.</p> <p>And if it truly were zero, we‚Äôd see massive weight updates.</p> <div><div><pre><code><span>param_update</span> <span>=</span> <span>lr</span> <span>*</span> <span>exp_avg</span> <span>/</span> <span>(</span><span>sqrt</span><span>(</span><span>exp_avg_sq</span><span>)</span> <span>+</span> <span>Œµ</span><span>)</span> 
             <span>=</span> <span>0.001</span> <span>*</span> <span>1.96e5</span> <span>/</span> <span>(</span><span>sqrt</span><span>(</span><span>0</span><span>)</span> <span>+</span> <span>1e-8</span><span>)</span>
             <span>=</span> <span>196</span> <span>/</span> <span>1e-8</span>
             <span>=</span> <span>1.96e10</span>  <span># &lt;-- HUGE!
</span></code></pre></div></div> <p>This would be <strong>huge</strong>! Yet we see NO updates‚Ä¶ this paradox points to a deeper issue.</p> <h3 id="testing-hypotheses">Testing Hypotheses</h3> <h4 id="could-it-be-bias-correction">Could it be bias correction?</h4> <p>Adam uses bias correction to counteract zero initialization. Having previously encountered subtle training issues due to Adam bias initialization bugs, I wondered if the correction might be broken here. <d-footnote>üí°If you haven&#39;t been hurt by a bias correction bug before, check out <a href="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for" rel="external nofollow noopener" target="_blank">these</a> <a href="https://stats.stackexchange.com/questions/237169/why-are-non-zero-centered-activation-functions-a-problem-in-backpropagation/237282#237282" rel="external nofollow noopener" target="_blank">examples</a> to learn the importance of getting this step right!</d-footnote></p> <p>Recall, the bias correction is simply making our effective beta values dependent on the step index, so if the issue has to do with bias correction, it might have some relation to our beta parameters or step index.</p> <p>I tested with different beta values, at different steps, and even beta_2=0 (which bypasses the exponential average entirely, making <code>exp_avg_sq = grad**2</code> directly). The encoder‚Äôs <code>exp_avg_sq</code> still stayed zero, making bias correction seem less likely as a culprit.</p> <p>Plus, <code>exp_avg</code> updated correctly despite using the same bias correction mechanism. So maybe something else is preventing <code>exp_avg_sq</code> from updating.</p> <h4 id="is-it-a-precision-issue">Is it a precision issue?</h4> <p>My largest gradients were big (1e6), and squared that‚Äôs 1e12. While that <em>is</em> quite large, it shouldn‚Äôt overflow in float32. However, I‚Äôve also been hurt by precision bugs before<d-footnote>Floating point precision issues have a fun habit of causing silent failures/degradations like this one (where it completes but produces incorrect values). Always worth checking, even when it seems unlikely.</d-footnote>, so I had to try it anyway.</p> <p>I moved everything to float64‚Ä¶ <strong>AND IT STARTED WORKING!</strong></p> <div> <p>üòµ‚Äçüí´</p> <p><span>Wait... how could this possibly be a precision issue?!</span></p><p> I asked Claude to help me understand the situation &amp; was told there are intermediate calculations in Adam that might overflow...</p> <p>...but I couldn&#39;t find these mysterious intermediates in the code. And how would an overflow produce exact zeros instead of inf/NaN? Maybe we divide by the inf somewhere? Or there&#39;s an error correction step? Or we&#39;re underflowing? But that shouldn&#39;t give ALL zeros?!?! </p> <p> ...Going to fp64 <em>DID</em> fix it though, and LLMs probably know PyTorch better than I do, so maybe I&#39;m missing something obvious? But where was this secret intermediate? I couldn&#39;t find it anywhere... </p> <p><em>so now what???</em> </p> </div> <p>After a few more minutes of spiraling<d-footnote> You&#39;re probably not reading this for the mid-debugging-self-doubt, but every debugging adventure has a spiraling moment (at least for me) so feels disingenuous to skip this step. And maybe one of these theories could&#39;ve actually been correct! </d-footnote>, I realized something: when I switched to float64, I <em>also</em> had to switch from MPS (Apple Silicon GPU) to CPU, since MPS doesn‚Äôt support float64. <strong>I‚Äôd changed two variables at once.</strong></p> <p>Testing with float32 on CPU‚Ä¶ <strong>the weights update!!</strong></p> <div> <p>üí°</p> <p> Turns out, precision wasn&#39;t the culprit, it was <code>device-specific</code>! The exact same float32 code updates weights on CPU but fails on MPS. This was progress: same code, same datatypes, but different devices meant different implementations‚Äîand different bugs. </p> </div> <p>Ôπ° This is progress!!</p> <p>Ôπ° Note to self‚Ä¶ simpler explanations are more likely correct- even (and especially!) when LLMs confidently assert complicated theories that are hard to understand / verify</p> <p>Ôπ° Now I just need to figure out why the bug only occurs with MPS</p> <h2 id="device-specific-differences">Device-Specific Differences</h2> <h3 id="why-the-same-operation-behaves-differently-on-different-chips">Why the Same Operation Behaves Differently on Different Chips</h3> <p>PyTorch‚Äôs device abstraction lets you write the same code and run it on CPUs, GPUs, and even Apple Silicon. It <em>feels</em> like the same computation is running everywhere ‚Äî but under the hood, each device has its own entirely separate implementation.</p> <p>When you call a tensor operation like <code>matmul</code>, PyTorch looks at the tensor‚Äôs metadata (e.g. device, dtype, shape) and dispatches to a <strong>specialized kernel</strong>: a device-specific, highly optimized implementation tailored for that particular hardware backend.</p> <details><summary><b>Understanding Apple&#39;s GPU Stack and &#34;Kernel&#34; Terminology</b></summary> <div> <p><strong>Apple‚Äôs GPU Stack:</strong></p> <ul> <li> <strong>Metal</strong> - Apple‚Äôs low-level graphics/compute API (like CUDA for NVIDIA)</li> <li> <strong>MPS (Metal Performance Shaders)</strong> - High-level optimized functions built on Metal (like cuDNN for CUDA)</li> <li> <strong>PyTorch‚Äôs MPS backend</strong> - PyTorch‚Äôs integration that uses both Metal directly and MPS functions</li> </ul> <p><strong>On ‚ÄúKernel‚Äù Terminology:</strong></p> <p>Typically, ‚Äúkernel‚Äù refers to low-level GPU code that runs directly on hardware: functions that explicitly manage parallelism across thousands of GPU cores, handle device memory allocation, and are written in chip-specific languages like CUDA or Metal Shading Language.</p> <p>However, PyTorch seems to also use ‚Äúkernel‚Äù to describe a higher-level abstraction: the framework‚Äôs implementation code (C++, Objective-C++, or CUDA files in the <code>native/</code> directory) that handles specific operations for specific backends. These PyTorch kernels sit above the hardware level- they might call optimized libraries like MPS or cuDNN (which then use those low-level GPU kernels underneath), or they might contain hand-written GPU code.</p> <p>In this post, we end up primarily exploring PyTorch kernels (e.g. the C++/Objective-C++ code in <code>BinaryOps.mm</code> that orchestrates MPS operations) rather than the Metal compute shaders executing on GPU cores beneath them.</p> <p>I was surprised these higher-level implementations are also called ‚Äúkernels‚Äù and maybe I have just confused my terminology here but I didn‚Äôt have a better name for them so I tried to mostly use ‚ÄúPyTorch kernel‚Äù or just ‚Äúoperation‚Äù to describe them, though the terminology does get blurry in places.</p> </div> </details> <p>So when you write something like <code>result = tensor_a @ tensor_b</code>, you‚Äôre not invoking a universal multiply function. PyTorch uses the tensors‚Äô metadata to select a device- and dtype-specific kernel that performs the actual computation.</p> <p>Multiplying two tensors on the CPU uses a completely different kernel than on MPS or CUDA. Even on the same device, changing the dtype or layout can trigger a different kernel. PyTorch maintains a large set of these implementations to support all the combinations.</p> <p>We‚Äôll see exactly how this dispatch system works in C++ later when we dive into the source code. For now, the important point is: <strong><em>even with identical Python code</em> different tensor metadata ‚Üí different kernel code ‚Üí different efficiency / bugs.</strong></p> <p>In my case, because I‚Äôm running this on my M3 MacBook Pro, I‚Äô m using MPS (Metal Performance Shaders), which is the GPU backend for Apple Silicon. While it feels a bit crazy to assume that my training plateau is due to an internal kernel-level bug, it‚Äôs a bit less unreasonable with MPS as it‚Äôs newer and less mature than the CPU and CUDA backends. (And honestly, most people training/debugging ML models are not doing it on their MacBooks.)</p> <h3 id="why-does-only-the-encoder-hit-this-bug">Why Does Only the Encoder Hit This Bug?</h3> <p>The Adam bug appears when working with the encoder on MPS. What makes the encoder different from the decoder that would trigger different behavior?</p> <p>I tested everything I could think of that might differentiate the two tensors:</p> <ul> <li>Different gradient scales</li> <li>Dense vs sparse gradient patterns</li> <li>Removing decoder-specific gradient transformations</li> <li>Making encoder and decoder gradients statistically identical</li> </ul> <p>Nothing helped. Even when both tensors had similar gradient statistics, only the encoder‚Äôs <code>exp_avg_sq</code> stayed frozen. The difference wasn‚Äôt in the <em>values</em> of the tensor - something else about the encoder tensor itself was triggering the bug.</p> <p><strong>What properties does a PyTorch tensor even have?</strong> I asked Claude what attributes could differ between two tensors and checked them one-by-one:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Encoder</strong></th> <th><strong>Decoder</strong></th> <th><strong>Same?</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Device</strong></td> <td>mps:0</td> <td>mps:0</td> <td>‚úì</td> </tr> <tr> <td><strong>Dtype</strong></td> <td>float32</td> <td>float32</td> <td>‚úì</td> </tr> <tr> <td><strong>Shape</strong></td> <td>[1536, 384]</td> <td>[384, 1536]</td> <td>‚ùå</td> </tr> <tr> <td><strong>Requires_grad</strong></td> <td>True</td> <td>True</td> <td>‚úì</td> </tr> <tr> <td><strong>Stride</strong></td> <td>(1, 1536)</td> <td>(1536, 1)</td> <td>‚ùå</td> </tr> <tr> <td><strong>Contiguous</strong></td> <td>False</td> <td>True</td> <td>‚ùå</td> </tr> </tbody> </table> <p>Three differences! The encoder and decoder have different shapes (they‚Äôre transposes of each other)<d-footnote>PyTorch&#39;s <code>nn.Linear</code> stores weights as [out_features, in_features], so the encoder (384‚Üí1536) has shape [1536, 384] and the decoder (1536‚Üí384) has shape [384, 1536].</d-footnote>, different stride patterns, and different contiguity. These properties are all related (more on that below).</p> <p>The shape difference itself can‚Äôt cause different behavior (PyTorch operations handle any shape). But contiguity? That‚Äôs a low-level memory detail that could be relevant. Maybe the MPS Adam bug only affects non-contiguous tensors? Worth a shot:</p> <div><div><pre><code><span>model</span><span>.</span><span>encoder</span><span>.</span><span>weight</span><span>.</span><span>data</span> <span>=</span> <span>model</span><span>.</span><span>encoder</span><span>.</span><span>weight</span><span>.</span><span>contiguous</span><span>()</span>
<span>optimizer</span><span>.</span><span>step</span><span>()</span>
<span># Encoder updates!! ‚úì
</span></code></pre></div></div> <p><strong>IT WORKS!</strong> But <em>why</em>?</p> <h2 id="tensor-memory-layouts">Tensor Memory Layouts</h2> <h3 id="what-does-contiguous-even-mean">What Does ‚ÄúContiguous‚Äù Even Mean?</h3> <p>Your computer‚Äôs memory is just a flat, 1D array of bytes, but tensors represent multi-dimensional grids. When you index <code>tensor[i, j]</code>, PyTorch needs to find that element in the flat memory. The tensor‚Äôs <strong>stride</strong> tells it how to do this conversion (and the exact amount you jump between elements depends on the dtype and how much memory each element takes up).</p> <p>Think of stride as <strong>navigation instructions</strong>: ‚Äúto get from one row to the next, skip this many elements.‚Äù By default, memory is stored row-wise‚Äîeach row is stored sequentially, then the next row comes after. If you read through a row, you skip over 1 element at a time; to go to the next row, you move row-length elements over. (This is why going across a row is faster than going down a column.)</p> <p>However, the memory layout doesn‚Äôt have to match the logical layout we use to think about the tensor. We can change how the user views the tensor without moving any data! For example, when we run transpose (<code>.T</code>), we don‚Äôt need to move around any data‚Äîwe just change the stride!</p>  <p>As we see in the images, reading all the elements row-by-row in the contiguous tensor is easy and linear, but the same row-wise pattern in the non-contiguous tensor is much jumpier. This jumping pattern makes the tensor ‚Äúnon-contiguous.‚Äù</p> <p>While there‚Äôs only one way for a tensor to be contiguous (the ‚Äúnatural‚Äù layout), there are many ways to become non-contiguous. By default, tensors are initialized as contiguous, but operations like slicing (<code>tensor[::2, :]</code>), reshaping, and dimension reordering (<code>permute</code>) can all create different non-contiguous stride patterns.</p> <p><strong>Why design tensors this way?</strong> Wouldn‚Äôt it be simpler to always keep data in the ‚Äúnatural‚Äù contiguous layout? The answer is performance: by just adjusting the tensor‚Äôs metadata, operations like transpose, slice, and reshape can be nearly <strong>instant</strong>‚Äî no data movement or memory allocation required. Keeping everything contiguous would mean expensive copying every time you reorganize dimensions.</p> <h3 id="how-my-encoder-became-non-contiguous">How My Encoder Became Non-Contiguous</h3> <p>Looking at the weight initialization code:</p> <div><div><pre><code><span>self</span><span>.</span><span>encoder</span><span>.</span><span>weight</span><span>.</span><span>data</span> <span>=</span> <span>self</span><span>.</span><span>decoder</span><span>.</span><span>weight</span><span>.</span><span>T</span><span>.</span><span>clone</span><span>()</span>
</code></pre></div></div> <p>The <code>.T</code> creates a non-contiguous view, and <code>.clone()</code> preserves the stride pattern.</p> <details> <summary><b>Why does <code>.clone()</code> preserve stride patterns?</b></summary> <div> <p>At first this felt counterintuitive to me- if we‚Äôre already paying the cost to copy the data (the whole point of non-contiguous layouts is to avoid copying), why not copy it into the ‚Äúbetter‚Äù contiguous layout?</p> <p>But this actually makes sense from a design perspective: <code>.clone()</code> should create an exact copy with all properties preserved, including memory layout. The tensor might be non-contiguous for a reason‚Äîmaybe you‚Äôre about to transpose it back, or the layout is optimized for some operation. Silently reorganizing memory would be surprising behavior. (The optional <a href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.memory_format" rel="external nofollow noopener" target="_blank"><code>torch.memory_format</code></a> argument, which defaults to <code>torch.preserve_format</code>, makes this choice explicit.)</p> <p>As a bonus, preserving the layout is also faster. Even though both include new memory allocation and moving data, reorganizing it still slows things down:</p> <div> <div><pre><code><span>x_t</span> <span>=</span> <span>x</span><span>.</span><span>T</span>  <span># Start with non-contiguous
</span><span>y_noncontig</span> <span>=</span> <span>x_t</span><span>.</span><span>clone</span><span>()</span>              <span># Preserves non-contiguous (1.919ms)
</span><span>y_contig</span> <span>=</span> <span>x_t</span><span>.</span><span>clone</span><span>(</span><span>memory_format</span><span>=</span><span>torch</span><span>.</span><span>contiguous_format</span><span>)</span>  <span># Force contiguous (4.401ms)
</span></code></pre></div> </div> </div> </details> <p><strong>Okay so we now know this initialization is why only the encoder is non-contiguous, and thus why only the encoder has training issues!</strong></p> <p><em>While I could just call <code>.contiguous()</code> on my encoder, declare victory, and get back to the research this bug was blocking me from doing‚Ä¶ I felt like I was just scratching the surface of this bug and I feared it would haunt me until I fully figured out WHAT happened and WHY.</em></p> <div> <p>üîé</p> <p> Why does a non-contiguous encoder weight cause a zero second moment and no parameter updates with Adam on MPS?? </p> </div> <h2 id="identifying-the-broken-operations">Identifying the Broken Operations</h2> <h3 id="what-operations-does-adam-use">What Operations Does Adam Use?</h3> <p>When Adam updates parameters, what operations does it perform? Let‚Äôs look at <a href="https://github.com/pytorch/pytorch/blob/main/torch/optim/adam.py" rel="external nofollow noopener" target="_blank">PyTorch‚Äôs Adam implementation</a>.</p> <p>Fair warning: this file is over 1000 lines! To find what we need, search for where <code>exp_avg</code> and <code>exp_avg_sq</code> are defined and updated.</p> <p>Here are the critical lines (<a href="https://github.com/pytorch/pytorch/blob/39901f229520a5256505ec24782f716ee7ddc843/torch/optim/adam.py#L101" rel="external nofollow noopener" target="_blank">lines 101, 391-407</a>):</p> <div><div><pre><code><span># State initialization (line 101)
</span><span>state</span><span>[</span><span>&#34;</span><span>exp_avg</span><span>&#34;</span><span>]</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>param</span><span>,</span> <span>memory_format</span><span>=</span><span>torch</span><span>.</span><span>preserve_format</span><span>)</span>
<span>state</span><span>[</span><span>&#34;</span><span>exp_avg_sq</span><span>&#34;</span><span>]</span> <span>=</span> <span>torch</span><span>.</span><span>zeros_like</span><span>(</span><span>param</span><span>,</span> <span>memory_format</span><span>=</span><span>torch</span><span>.</span><span>preserve_format</span><span>)</span>

<span># ... [300 lines of setup and parameter group handling] ...
</span>
<span># First moment update (line 391)
</span><span>exp_avg</span><span>.</span><span>lerp_</span><span>(</span><span>grad</span><span>,</span> <span>1</span> <span>-</span> <span>beta1</span><span>)</span>

<span># Second moment update (line 392)
</span><span>exp_avg_sq</span><span>.</span><span>mul_</span><span>(</span><span>beta2</span><span>).</span><span>addcmul_</span><span>(</span><span>grad</span><span>,</span> <span>grad</span><span>,</span> <span>value</span><span>=</span><span>1</span> <span>-</span> <span>beta2</span><span>)</span>

<span># ... [bias correction calculations] ...
</span>
<span># Parameter update (line 407)
</span><span>param</span><span>.</span><span>addcdiv_</span><span>(</span><span>exp_avg</span><span>,</span> <span>denom</span><span>,</span> <span>value</span><span>=-</span><span>step_size</span><span>)</span>
</code></pre></div></div> <p>Look at that initialization! <code>memory_format=torch.preserve_format</code> means the state tensors inherit their stride pattern from <code>param</code>. So when our encoder weight is non-contiguous, both <code>exp_avg</code> and <code>exp_avg_sq</code> are also non-contiguous.</p> <p>But they‚Äôre BOTH non-contiguous - so why does only one break?</p> <p>Well, while they both are computed via addition and multiplication, they don‚Äôt use the exact same operations to perform this. Any of these operations could be a suspect, so let‚Äôs test each one individually!</p> <p>For operations like <code>output.addcmul_(input1, input2)</code>, the <strong>output tensor</strong><d-footnote>In PyTorch, when a function name ends with an underscore (like <code>mul_</code>), that indicates that it is performing an <b>in-place operation</b> to modify a tensor directly in memory. Just as different devices can distinct kernels, so can distinctions like these!</d-footnote> is modified while <strong>input tensors</strong> are read from. In our case, we know the output tensor is non-contiguous, so let‚Äôs test if that is sufficient to cause our bug.</p> <h3 id="testing-the-broken-operations">Testing the Broken Operations</h3> <p>Testing each Adam operation with non-contiguous output tensors on MPS:</p> <table> <thead> <tr> <th><strong>Operation</strong></th> <th><strong>Function</strong></th> <th><strong>Result</strong></th> </tr> </thead> <tbody> <tr> <td>Linear interpolation</td> <td><code>lerp_()</code></td> <td>Updates ‚úì</td> </tr> <tr> <td>Scalar multiply</td> <td><code>mul_()</code></td> <td>Updates ‚úì</td> </tr> <tr> <td>Add + multiply</td> <td><code>addcmul_()</code></td> <td>Stays zero ‚úó</td> </tr> <tr> <td>Add + divide</td> <td><code>addcdiv_()</code></td> <td>Stays zero ‚úó</td> </tr> </tbody> </table> <div> <p>‚ÄºÔ∏è</p> <p> Found it! <code>addcmul_()</code> and <code>addcdiv_()</code> both fail silently when writing to non-contiguous outputs on MPS. </p> </div> <p>Interestingly, <em>input contiguity doesn‚Äôt matter</em>, only the output! Whether <code>grad</code>, <code>exp_avg</code>, or <code>denom</code> are contiguous makes no difference. The bug is purely in how these kernels write to <em>non-contiguous output buffers</em>.</p> <p>The broken operations aren‚Äôt producing zeros or NaNs. They‚Äôre simply not modifying the output tensor at all. This wasn‚Äôt immediately obvious since <code>exp_avg_sq</code> was initialized to zeros, making ‚Äústays at zero‚Äù and ‚Äúnever updates‚Äù look identical. But testing with a non-zero, non-contiguous output tensor confirms that after calling <code>addcmul_</code> or <code>addcdiv_</code>, the values remain unchanged. No update happens.</p> <p>Yet timing shows MPS <em>is</em> doing substantial work. Non-contiguous operations take &gt;2x longer than contiguous ones, proving the kernels are computing <em>something</em>, yet those results never make it to the output tensor. On CPU, each of these operations work correctly regardless of memory layout. This is purely a MPS-specific bug.</p> <p>With the broken operations identified, we can trace the complete chain of events that triggers our failure:</p> <h3 id="putting-the-pieces-together">Putting the Pieces Together</h3> <div> <figure> <picture> <source srcset="/assets/img/the_bug_that_taught_me_pytorch_post/complete_bug_chain-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/complete_bug_chain-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/complete_bug_chain-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="https://elanapearl.github.io/assets/img/the_bug_that_taught_me_pytorch_post/complete_bug_chain.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <details> <summary><b>Show the complete bug chain in code</b></summary> <div> <p><strong>Step 1: Initialization</strong></p> <div> <div><pre><code><span># Creates non-contiguous encoder weight (stride: 1, 1536)
</span><span>encoder</span><span>.</span><span>weight</span> <span>=</span> <span>decoder</span><span>.</span><span>weight</span><span>.</span><span>T</span><span>.</span><span>clone</span><span>()</span>
</code></pre></div> </div> <p><strong>Step 2: Adam State Creation</strong></p> <div> <div><pre><code><span># Both state tensors inherit non-contiguous layout from param
</span><span>state</span><span>[</span><span>&#34;</span><span>exp_avg</span><span>&#34;</span><span>]</span> <span>=</span> <span>zeros_like</span><span>(</span><span>param</span><span>,</span> <span>memory_format</span><span>=</span><span>torch</span><span>.</span><span>preserve_format</span><span>)</span>
<span>state</span><span>[</span><span>&#34;</span><span>exp_avg_sq</span><span>&#34;</span><span>]</span> <span>=</span> <span>zeros_like</span><span>(</span><span>param</span><span>,</span> <span>memory_format</span><span>=</span><span>torch</span><span>.</span><span>preserve_format</span><span>)</span>
</code></pre></div> </div> <p><strong>Step 3: Optimization Loop</strong></p> <p><em>First moment update:</em></p> <div> <div><pre><code><span>exp_avg</span><span>.</span><span>lerp_</span><span>(</span><span>grad</span><span>,</span> <span>1</span><span>-</span><span>beta_1</span><span>)</span>  <span># ‚úì Works fine
</span></code></pre></div> </div> <p><em>Second moment update:</em></p> <div> <div><pre><code><span>exp_avg_sq</span><span>.</span><span>mul_</span><span>(</span><span>beta_2</span><span>)</span>                        <span># ‚úì Works fine
</span><span>exp_avg_sq</span><span>.</span><span>addcmul_</span><span>(</span><span>grad</span><span>,</span> <span>grad</span><span>,</span> <span>1</span><span>-</span><span>beta_2</span><span>)</span>      <span># ‚úó No update - stays zero!
</span></code></pre></div> </div> <p><strong>Step 4: Parameter Update</strong></p> <div> <div><pre><code><span># Should update param, does nothing, leading to silent failure
</span><span>param</span><span>.</span><span>addcdiv_</span><span>(</span><span>exp_avg</span><span>,</span> <span>denom</span><span>,</span> <span>value</span><span>=-</span><span>step_size</span><span>)</span>  <span># ‚úó No update!
</span></code></pre></div> </div> </div> </details> <p>If <em>only</em> <code>exp_avg_sq.addcmul_()</code> failed, the zero <code>exp_avg_sq</code> would produce massive weight explosions (update = <code>lr √ó exp_avg / ‚àö(Œµ)</code>), making the bug immediately obvious. But <code>param.addcdiv_()</code> <em>also</em> failed, producing no updates at all!</p> <p>The second bug masked the first, creating a silent failure: the spookiest type of error. The model appeared to be learning (the decoder was training normally), but progress stalled because the encoder stayed frozen. A subtle plateau that looked exactly like a hyperparameter issue üôÉ</p> <details> <summary><b>Side note: Why did forward and backward passes work fine with non-contiguous weights?</b></summary> <div> <p>If non-contiguous tensors can cause operations to silently fail on MPS, why didn‚Äôt the forward pass or backward pass break?</p> <p>The forward and backward passes for <code>F.linear</code> use <code>matmul</code> for their matrix multiplications, which handle non-contiguous tensors correctly on MPS. Testing confirms that both <code>matmul</code> (the <code>@</code> operator) and <code>F.linear</code> work correctly with non-contiguous input tensors and non-contiguous weight matrices on MPS, including during the backward pass where gradients flow through non-contiguous weights without issues.</p> <p>The bug is specific to the fused in-place operations that Adam uses for state updates: <code>addcmul_</code> and <code>addcdiv_</code>. These operations fail silently when writing to non-contiguous output tensors, while other in-place operations like <code>lerp_</code> and <code>mul_</code> work correctly.</p> </div> </details> <p><strong>While we have made so much progress on this case, we‚Äôre still not done yet!!</strong></p> <div> <p>Remaining Question</p> <p>üîç</p> <p> Why do <code>addcmul_</code> and <code>addcdiv_</code> fail to update non-contiguous outputs while <code>mul_</code> and <code>lerp_</code> work fine? </p> </div> <h2 id="inside-the-kernel-implementation">Inside the Kernel Implementation</h2> <p>To understand why some operations work and others don‚Äôt, I needed to look at PyTorch‚Äôs source code for the buggy kernels.</p> <p>While I normally trace through a Python codebase by jumping to definitions in my IDE, that doesn‚Äôt work with <code>tensor.addcmul_()</code>. When you call this function, there‚Äôs no Python source code executing - instead, Python immediately jumps into compiled C++ code for performance. And since PyTorch ships this as a pre-compiled binary, I can‚Äôt see that C++ implementation.</p> <details> <summary><b>How can Python call C++ functions? (a brief aside on bindings)</b></summary> <div> <p>How can a Python tensor object have methods that execute C++ code? I skipped over this earlier but even though I know PyTorch isn‚Äôt the only framework to do this and everything is just machine code if you zoom in close enough‚Ä¶ it still feels a bit magical to casually call another language.</p> <p>The explanation is <strong>Python bindings</strong>.</p> <p>When you install PyTorch, you‚Äôre not just getting Python files. You‚Äôre also getting compiled C++ libraries (.so files on Linux/Mac, .dll on Windows) that contain the actual mathematical operations. The Python part is essentially a wrapper that:</p> <ol> <li>Takes your Python arguments (<code>tensor</code>, <code>other_tensor</code>, etc.)</li> <li>Converts them to C++ data structures</li> <li>Calls the appropriate C++ function</li> <li>Converts the C++ result back to a Python tensor</li> <li>Returns it to your Python code</li> </ol> <p>PyTorch uses <a href="https://pybind11.readthedocs.io/" rel="external nofollow noopener" target="_blank">pybind11</a> to automatically generate this wrapper code. For example, the C++ function signature:</p> <div> <div><pre><code><span>Tensor</span><span>&amp;</span> <span>addcmul_</span><span>(</span><span>Tensor</span><span>&amp;</span> <span>self</span><span>,</span> <span>const</span> <span>Tensor</span><span>&amp;</span> <span>tensor1</span><span>,</span> <span>const</span> <span>Tensor</span><span>&amp;</span> <span>tensor2</span><span>,</span> <span>const</span> <span>Scalar</span><span>&amp;</span> <span>value</span><span>)</span>
</code></pre></div> </div> <p>Gets automatically wrapped so you can call it from Python as:</p> <div> <div><pre><code><span>tensor</span><span>.</span><span>addcmul_</span><span>(</span><span>tensor1</span><span>,</span> <span>tensor2</span><span>,</span> <span>value</span><span>=</span><span>1.0</span><span>)</span>
</code></pre></div> </div> <p>This is why PyTorch operations are fast despite being called from Python - the heavy lifting happens in optimized C++ code, with Python just handling the interface.</p> </div> </details> <p>And as we discussed earlier, PyTorch dispatches based on tensor metadata, so there isn‚Äôt just <em>one</em> implementation - there are device-specific kernels for CPU, CUDA, MPS, etc. Since my PyTorch installation just has the compiled binary files, to investigate the actual implementations, we need to clone PyTorch‚Äôs repository.</p> <h3 id="pytorchs-dispatch-system">PyTorch‚Äôs Dispatch System</h3> <p>All kernels are listed in an <strong>operation registry</strong> - a YAML file that maps operation names (like <code>addcmul_</code>) to their tensor-specific C++ implementations. In practice, when PyTorch is compiled (normally done before you install it), this registry is used to automatically generate hundreds of scripts that do the actual dispatching based on the patterns described here, but if we just want to understand what kernel our tensor is calling, we can look through the registry.</p> <p>Searching for ‚Äúaddcmul_‚Äù in the registry <code>native_functions.yaml</code>:</p> <div><div><pre><code><span>-</span> <span>func</span><span>:</span> <span>addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor(a!)</span>
  <span># our addcmul_ function just points us to the yaml for addcmul.out</span>
  <span>structured_delegate</span><span>:</span> <span>addcmul.out</span>

<span># The function addcmul_ points to:</span>
<span>-</span> <span>func</span><span>:</span> <span>addcmul.out(...)</span>
  <span>dispatch</span><span>:</span>
    <span>CPU, CUDA</span><span>:</span> <span>addcmul_out</span>
    <span>MPS</span><span>:</span> <span>addcmul_out_mps</span>  <span># Different function for MPS!</span>
</code></pre></div></div> <p>Now that we have the device-specific operation names, we can search them in the PyTorch repo within the <a href="`https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/mps/`">mps implementations</a>, and we find our implementation for <code>addcmul_out_mps</code> in <a href="https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/mps/operations/PointwiseOps.mm" rel="external nofollow noopener" target="_blank"><code>PointwiseOps.mm</code></a>. Upon a first skim of the code, I realized I had no clue how to read the MPS codebase. There were too many unknown variables and constructs, and I wasn‚Äôt sure what to look for in this implementation. I‚Äôd written a CUDA kernel before, and was pretty good with C about a decade ago, but as turns out, neither of those helped here :(</p> <h3 id="comparing-broken-vs-working-implementations">Comparing Broken vs Working Implementations</h3> <p>Rather than trying to decode unfamiliar code in isolation, I‚Äôd find something similar that works correctly and compare the two. <code>mul_</code> was the perfect comparison since both are simple element-wise in-place operations. The registry pointed me to <code>binaryOpTensor</code> in <a href="https://github.com/pytorch/pytorch/blob/v2.2.1/aten/src/ATen/native/mps/operations/BinaryOps.mm" rel="external nofollow noopener" target="_blank"><code>BinaryOps.mm</code></a>.</p> <p>Now I had my comparison:</p> <ul> <li> <strong>Broken:</strong> <code>addc_mul_div_out_mps</code> in <code>PointwiseOps.mm</code> (used by <code>addcmul_</code>)</li> <li> <strong>Working:</strong> <code>binaryOpTensor</code> in <code>BinaryOps.mm</code> (used by <code>mul_</code>)</li> </ul> <p>I opened both side-by-side, scanning specifically for differences in how they handle the output tensor. My experiments had already narrowed the search: I knew both operations were computing <em>something</em> (timing proved that), so the bug had to be in how results get written back to non-contiguous outputs. Look for anything related to contiguity checks or special output handling.</p> <p><strong>Broken version (<code>addcmul_</code>):</strong></p> <div><div><pre><code><span>static</span> <span>void</span> <span>addc_mul_div_out_mps</span><span>(...,</span> <span>Tensor</span><span>&amp;</span> <span>output</span><span>,</span> <span>...)</span> <span>{</span>
  <span>// ... setup code ...</span>
  <span>Placeholder</span> <span>outputPlaceholder</span> <span>=</span> <span>Placeholder</span><span>(</span><span>output</span><span>);</span>
  <span>runMPSGraph</span><span>(...);</span>
  <span>// That&#39;s it - no additional handling</span>
<span>}</span>
</code></pre></div></div> <p><strong>Working version (<code>mul_</code>):</strong></p> <div><div><pre><code><span>static</span> <span>void</span> <span>binaryOpTensor</span><span>(...,</span> <span>Tensor</span><span>&amp;</span> <span>output</span><span>,</span> <span>...)</span> <span>{</span>
  <span>// ... setup code ...</span>
  
  <span>bool</span> <span>needsCopyToOutput</span> <span>=</span> <span>!</span><span>output</span><span>.</span><span>is_contiguous</span><span>();</span>
  <span>if</span> <span>(</span><span>needsCopyToOutput</span><span>)</span> <span>{</span>
    <span>// Create temporary contiguous tensor</span>
    <span>output</span> <span>=</span> <span>at</span><span>::</span><span>empty</span><span>(...);</span>
  <span>}</span>
  
  <span>Placeholder</span> <span>outputPlaceholder</span> <span>=</span> <span>Placeholder</span><span>(</span><span>output</span><span>);</span>
  <span>runMPSGraph</span><span>(...);</span>
  
  <span>if</span> <span>(</span><span>needsCopyToOutput</span><span>)</span> <span>{</span>
    <span>output_</span><span>.</span><span>copy_</span><span>(</span><span>output</span><span>);</span>  <span>// Copy results back!</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div> <p>The working version explicitly checks <code>!output.is_contiguous()</code> and adds extra handling: it creates a temporary contiguous tensor, runs the operation, then copies results back. The broken version just passes the output directly to <code>Placeholder</code> and calls it a day.</p> <p>But this raises a new question: if non-contiguous memory layouts need this kind of explicit handling, why doesn‚Äôt <code>addcmul</code> just crash or throw an error instead of silently failing?</p> <h3 id="the-memory-conversion-problem">The Memory Conversion Problem</h3> <p>The answer lies in understanding what <code>Placeholder</code> does. PyTorch tensors and Metal (Apple‚Äôs GPU framework) use different memory formats, so PyTorch needs a converter when running operations on Apple Silicon. <code>Placeholder</code> handles this conversion - it takes PyTorch tensors and wraps them in Metal-compatible buffers, handles different data types, manages memory layouts, and sets up the compute pipeline.</p> <p>For most tensors, this conversion is straightforward. But for non-contiguous tensors, Metal can‚Äôt work with the scattered memory layout directly. Looking at the Placeholder code:</p> <div><div><pre><code><span>if</span> <span>(</span><span>!</span><span>src</span><span>.</span><span>is_contiguous</span><span>())</span> <span>{</span>
    <span>_tensor</span> <span>=</span> <span>src</span><span>.</span><span>clone</span><span>(</span><span>MemoryFormat</span><span>::</span><span>Contiguous</span><span>);</span>  <span>// Create contiguous copy</span>
    <span>srcBuf</span> <span>=</span> <span>getMTLBufferStorage</span><span>(</span><span>_tensor</span><span>);</span>          <span>// Point Metal to the copy</span>
<span>}</span>
</code></pre></div></div> <p>When Placeholder encounters a non-contiguous tensor, it automatically creates a contiguous copy and points Metal to that copy instead. This happens transparently - the broken kernels have no idea they‚Äôre working with a temporary.</p> <p>This automatic copying is perfect for <strong>input tensors</strong> - Metal reads from the copy, computation proceeds normally, and nobody cares what happens to the temporary afterward.</p> <p>But it‚Äôs disastrous for <strong>output tensors</strong> where the goal is in-place editing. The computation succeeds and writes results to the temporary copy, but those results never make it back to the original tensor that‚Äôs supposed to be updated.</p> <details> <summary><b>Why is this MPS-Specific?</b></summary> <div> <p>If non-contiguous tensors are so problematic, why do CPU and CUDA backends handle them fine?</p> <p><strong>CPU:</strong> Can handle arbitrary strides natively. When iterating through a non-contiguous tensor, the CPU just follows the stride pattern‚Äîjumping around memory is slower than sequential access, but it works correctly.</p> <p><strong>CUDA:</strong> NVIDIA‚Äôs CUDA framework has always supported strided memory access in kernels. Operations can read/write to non-contiguous layouts directly, though with some performance penalty.</p> <p><strong>MPS:</strong> Apple‚Äôs Metal Performance Shaders framework initially didn‚Äôt support strided access. Kernels expected contiguous memory layouts, period. This forced PyTorch to implement the gather-scatter workaround pattern we saw in the working kernels.</p> <p>The bug occurred because some MPS operations implemented this workaround (like <code>mul_</code>), while others didn‚Äôt (like <code>addcmul_</code>). The abstraction (Placeholder) that was supposed to hide this complexity actually made it worse by silently copying outputs without a way to copy results back. Although as we‚Äôll learn later this has been improved in newer Mac Operating Systems.</p> </div> </details> <h3 id="the-complete-bug-mechanism">The Complete Bug Mechanism</h3> <div> <figure> <picture> <source srcset="/assets/img/the_bug_that_taught_me_pytorch_post/placeholder_bug_mechanism-480.webp 480w,/assets/img/the_bug_that_taught_me_pytorch_post/placeholder_bug_mechanism-800.webp 800w,/assets/img/the_bug_that_taught_me_pytorch_post/placeholder_bug_mechanism-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="https://elanapearl.github.io/assets/img/the_bug_that_taught_me_pytorch_post/placeholder_bug_mechanism.png" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();"/> </picture> </figure> </div> <p>The broken kernels work perfectly with contiguous tensors and silently fail with non-contiguous ones. The working kernels detect this situation and add an explicit copy-back step to move results from the temporary to the original tensor.</p> <h3 id="the-fix">The Fix</h3> <p>Understanding the bug made the solution clear - apply the same pattern that working kernels use:</p>  <div> <div> <p>+</p> <p>Tensor output = output_;</p> </div> <div> <p>+</p> <p>bool needsCopyToOutput = false;</p> </div>  <div> <p>+</p> <p>if (!output_.is_contiguous()) {</p> </div> <div> <p>+</p> <p> output = at::empty(...); // Create contiguous buffer WE manage</p> </div> <div> <p>+</p> <p> needsCopyToOutput = true;</p> </div>    <div> <p>2</p> <p> Placeholder outputPlaceholder = Placeholder(output);</p> </div>   <div> <p>-</p> <p>// No copy-back - results vanish when Placeholder dies</p> </div>  <div> <p>+</p> <p>if (needsCopyToOutput) {</p> </div> <div> <p>+</p> <p> output_.copy_(output); // Copy results back</p> </div>  </div> <p>I tested this locally and it worked! The encoder weights finally updated and the model trained successfully üéâüéâ</p> <p>You can see the complete reproduction, debugging experiments, fix at <a href="https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug" rel="external nofollow noopener" target="_blank">https://github.com/ElanaPearl/pytorch-mps-noncontiguous-bug</a>.</p> <h2 id="case-closed">Case Closed</h2> <h3 id="a-lesson-in-version-control">A Lesson in Version Control</h3> <p>While editing a Python package just involves installing your locally editable version of the code instead of the default package, to test my PyTorch fix, I had to re-build it all locally, which was more work than expected and <em>also</em> made me acutely aware that this whole time I was working on PyTorch v2.2.1<d-footnote>I was working on a research codebase with dependency conflicts that blocked upgrading PyTorch. Common enough situation, but lesson learned: always check versions early in debugging, even if you can&#39;t immediately update!</d-footnote> (as this fact made it difficult to build and I had to downgrade things like CMake and deal with weird version conflicts to even build this older PyTorch).</p> <p>Checking the latest version revealed the bug was already fixed in v2.4, patched by an ML engineer at Apple last year using almost the exact same approach I‚Äôd used.<d-footnote>The official fix uses slightly different syntax; but the same core pattern: detect non-contiguous output, create a contiguous temporary buffer, perform the computation, then copy results back to the original tensor.</d-footnote> This updated code even informed me that in macOS 15+, MPS now handles non-contiguous tensors natively! <d-footnote>In macOS 15, Apple added native strided array support to MPSGraph via the <code>arrayView</code> API (see <a href="https://developer.apple.com/videos/play/wwdc2024/10218/" rel="external nofollow noopener" target="_blank">WWDC 2024 session</a> at timestamp 13:41). Instead of the gather-scatter workaround, Metal can now read/write directly from non-contiguous memory using stride metadata. This means on macOS 15+, PyTorch can skip the manual copy workarounds entirely. The performance gap between contiguous and non-contiguous tensors is now much smaller, though contiguous is still faster due to better cache utilization.</d-footnote></p> <div> <p>ü§¶‚Äç‚ôÄÔ∏è</p> <div><p> While I now felt silly for diving so deep on an already-fixed bug, the process was still very fun, educational, and so worth the effort.</p></div> </div> <h3 id="the-pattern-strikes-again">The Pattern Strikes Again</h3> <p>While writing this up, I added some more tests for my kernel fix to confirm it really worked, and one of the tests failed! I looked into it more and realized I‚Äôd stumbled upon <strong>the same failure pattern</strong> in the <code>random_</code> operation (in the most up-to-date PyTorch this time!)</p> <p><strong>Turns out, all random in-place operations</strong> (<code>normal_</code>, <code>uniform_</code>, <code>exponential_</code>, <code>random_</code>, <code>bernoulli_</code>) <strong>silently fail when called on non-contiguous tensors on MPS</strong>.</p> <div><div><pre><code><span>x</span> <span>=</span> <span>torch</span><span>.</span><span>zeros</span><span>(</span><span>10</span><span>,</span> <span>10</span><span>).</span><span>T</span>  <span># Non-contiguous
</span><span>x</span><span>.</span><span>normal_</span><span>()</span>  <span># Should fill with random values
</span><span>print</span><span>(</span><span>x</span><span>.</span><span>max</span><span>())</span>  <span># Prints 0.0 - the operation silently failed!
</span></code></pre></div></div> <p>Yet again, the operations complete without error, but the tensor remains unchanged‚Äîthe kernel computes random values into a temporary contiguous buffer but never copies them back.</p> <p>Having just traced through this exact bug pattern, I recognized it immediately and knew exactly how to fix it. Filed an <a href="https://github.com/pytorch/pytorch/issues/165257" rel="external nofollow noopener" target="_blank">Issue</a> and made a <a href="https://github.com/pytorch/pytorch/pull/165267" rel="external nofollow noopener" target="_blank">PR</a> applying the same solution.</p> <p>I suspect there are other similar bugs lying around, as none of these fixes actually address the underlying quirk that <strong>the Placeholder abstraction itself is problematic when used with output tensors</strong>.</p> <p>The core issue: Placeholder‚Äôs constructor silently creates a temporary contiguous copy for non-contiguous tensors, but it has no way to know if it‚Äôs wrapping an input (where the copy is fine- we just read from it) or an output (where the copy is broken- results get written to it then lost). This means <strong>every single operation that uses Placeholder for outputs must manually implement the same workaround pattern</strong> or else it has this silent failure:</p> <div><div><pre><code><span>// Every MPS operation must remember to do this:</span>
<span>bool</span> <span>needsCopy</span> <span>=</span> <span>!</span><span>output</span><span>.</span><span>is_contiguous</span><span>();</span>
<span>Tensor</span> <span>temp</span> <span>=</span> <span>needsCopy</span> <span>?</span> <span>at</span><span>:</span><span>:</span><span>empty</span><span>(...)</span> <span>:</span> <span>output</span><span>;</span>
<span>@autoreleasepool</span> <span>{</span>
    <span>Placeholder</span> <span>p</span><span>(</span><span>temp</span><span>);</span>
    <span>runGraph</span><span>();</span>
<span>}</span>
<span>if</span> <span>(</span><span>needsCopy</span><span>)</span>
  <span>output</span><span>.</span><span>copy_</span><span>(</span><span>temp</span><span>);</span>
</code></pre></div></div> <p>This is a leaky abstraction<d-footnote>A &#34;leaky abstraction&#34; is when an abstraction that&#39;s supposed to hide implementation details forces you to understand and work around those details anyway. Placeholder is supposed to abstract Metal buffer management, but its internal copying leaks through, forcing every caller to manually handle non-contiguous outputs. See Joel Spolsky&#39;s <a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/" rel="external nofollow noopener" target="_blank">The Law of Leaky Abstractions</a> for the canonical explanation.</d-footnote>: the internal implementation detail that ‚ÄúPlaceholder makes temporary copies‚Äù has leaked out to every caller, making it each operation‚Äôs responsibility to work around. A better design would be:</p> <ul> <li>Placeholder knows input vs output: Pass a flag so Placeholder can handle the copy-back itself</li> <li>Separate abstractions: Different wrapper types for inputs (InputPlaceholder) and outputs (OutputPlaceholder)</li> <li>Make the temporary explicit: Don‚Äôt hide the copy inside Placeholder‚Äîmake callers explicitly create and manage contiguous temporaries (this is what I used in the fixes for addcmul_/addcdiv_/the random ops)</li> </ul> <p>The good news: macOS 15+ Metal now handles non-contiguous tensors natively, making this entire issue obsolete for newer systems. But for anyone on older macOS versions or maintaining PyTorch‚Äôs MPS backend, this abstraction continues to cause issues.</p> <p>So ideally, the Placeholder class would be redesigned to handle output tensors correctly by default, but given that the hardware is moving to handle this natively anyway, the pragmatic fix is probably just to audit and patch the remaining operations using the established pattern.</p> <h3 id="practical-takeaways-for-your-code">Practical Takeaways for Your Code</h3> <p><strong>Performance Considerations</strong></p> <p>Even with the code fixes, non-contiguous tensors on MPS involve: Allocate temporary buffer -&gt; Copy to contiguous layout -&gt; Compute -&gt; Copy back. Making tensors contiguous once at initialization avoids thousands of copies during training! And even if your OS can avoid making this temporary contiguous copy, it is still slower to operate on non-contiguous memory if you will be using it many times.</p> <p><strong>When to Call <code>.contiguous()</code></strong></p> <div><div><pre><code><span># When to call .contiguous() - General Principles
</span>
<span># 1. After operations that change memory layout:
</span><span>x</span> <span>=</span> <span>tensor</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>)</span>  <span># Non-contiguous
</span><span>x</span> <span>=</span> <span>tensor</span><span>.</span><span>view</span><span>(</span><span>-</span><span>1</span><span>)</span>          <span># Might fail if non-contiguous!
</span><span>x</span> <span>=</span> <span>x</span><span>.</span><span>contiguous</span><span>().</span><span>view</span><span>(</span><span>-</span><span>1</span><span>)</span>  <span># Safe
</span>
<span># 2. Before operations that might not handle strides:
# - Custom CUDA/Metal kernels  
# - Newer backend features
# - Operations that failed mysteriously on certain devices
</span>
<span># 3. For performance on repeated operations:
</span><span>weights</span> <span>=</span> <span>init_weights</span><span>().</span><span>T</span>   <span># Used in every forward pass
</span><span>weights</span> <span>=</span> <span>weights</span><span>.</span><span>contiguous</span><span>()</span>  <span># Pay copy cost once, not every iteration
</span>
<span># But don&#39;t overuse it!
</span><span>x</span> <span>=</span> <span>x</span> <span>+</span> <span>y</span>  <span># Creates new contiguous tensor anyway
</span><span>x</span> <span>=</span> <span>x</span><span>.</span><span>contiguous</span><span>()</span>  <span># Unnecessary copy!
</span></code></pre></div></div> <p><strong>For MPS specifically:</strong> If on macOS &lt;15, make sure all your parameters are contiguous!</p> <h3 id="what-i-learned">What I Learned</h3> <p><strong>Isolate to specific, measurable symptoms.</strong> The most standard advice and for such good reason. Everything got easier once I had a concrete target: ‚Äú<code>exp_avg_sq</code> stays at zero‚Äù is infinitely more debuggable than ‚Äúthe loss plateaus mysteriously.‚Äù Once I had a specific symptom, I could strip away components and test the minimal case that triggered it.</p> <p><strong>When debugging tensor issues, check metadata not just values.</strong> I was checking for NaNs, visualizing weights, inspecting gradients‚Äîall focused on the numbers inside tensors. The actual problem was the tensor‚Äôs <em>stride pattern</em>. Device, dtype, contiguity, memory layout‚Äîthese aren‚Äôt just performance details, they can cause silent correctness bugs. <code>tensor.is_contiguous()</code> is now part of my debugging checklist.</p> <p><strong>When I‚Äôm confused, I might have changed two things‚Äîor there might be two bugs.</strong> Switching to fp64 ‚Äúfixed‚Äù it, but I‚Äôd also switched from MPS to CPU. Untangling that revealed the real culprit. And <code>exp_avg_sq</code> staying zero <em>should</em> have caused explosions, but the parameter update <em>also</em> failed‚Äîone bug perfectly masked the other.</p> <p><strong>Documentation makes more sense when I need it.</strong> I‚Äôd skimmed PyTorch internals docs before and nothing stuck‚Äîdispatch systems, stride patterns, kernel implementations all felt overwhelming. But once I <em>had</em> to understand how <code>addcmul_</code> dispatches to MPS kernels, everything clicked. Now PyTorch feels less like a black box. And when I hit the random ops bug weeks later, I wasn‚Äôt intimidated‚ÄîI knew exactly how to trace through the source.</p> <p><strong>Explore the system before exploring the code.</strong> When I needed to debug <code>addcmul_out_mps</code> in unfamiliar MPS code, I ran experiments first: which operations fail? Do they run at all? What triggers the bug? By the time I opened the source, I knew to compare <code>addcmul_</code> (broken) against <code>mul_</code> (working) and scan specifically for differences in output handling. Without that context, I‚Äôd have been lost in Objective-C++ with no idea what mattered. Also LLMs were very helpful with unfamiliar constructs like <code>MPSGraphTensor</code> or <code>@autoreleasepool</code>, although they‚Äôre still less reliable with MPS than more documented frameworks.</p> <p><strong>Write post-mortems‚Äì even for yourself.</strong> Forcing myself to explain <em>why</em> I tried each debugging step was as educational as the original investigation. It‚Äôs like experience replay in RL: you explore many failed paths, find one that works, then replay that successful trajectory to reinforce the policy. Writing it down builds pattern recognition‚Äîwhen I‚Äôm in ‚Äúsituation A‚Äù, what hypotheses are worth trying? I‚Äôve written lower-effort debugging debriefs before, but making this one readable for an external audience forced me to articulate why each step made sense, deepening my understanding of what actually worked.</p> <p>What started as a frustrating research roadblock became a surprisingly fun &amp; educational detour. It forced a closer look at things normally taken for granted: Adam‚Äôs momentum mechanics, stride patterns, kernel dispatch. Understanding why each operation behaved differently revealed more about PyTorch‚Äôs architecture than typical usage ever does.</p> <hr/> <p>If you made it this far, thanks for joining! Hope you had fun and/or learned something &amp; happy debugging!</p> <p>Special thanks to <a href="https://x.com/nickevanjoseph" rel="external nofollow noopener" target="_blank">Nicholas Joseph</a>, <a href="https://www.benkuhn.net/" rel="external nofollow noopener" target="_blank">Ben Kuhn</a>, <a href="https://blog.nelhage.com/" rel="external nofollow noopener" target="_blank">Nelson Elhage</a> and <a href="https://www.alextamkin.com/" rel="external nofollow noopener" target="_blank">Alex Tamkin</a> for giving feedback on this üíú</p>  </div></div>
  </body>
</html>
