<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://pepijndevos.nl/2024/12/01/jpeg-compress-your-llm-weights.html">Original</a>
    <h1>JPEG compress your LLM weights</h1>
    
    <div id="readability-page-1" class="page"><div>
            
<article>
    
    <section>
    <p>So quantization is kinda bad lossy compression right? JPEG is good lossy compression.
This may sound stupid, and maybe it is, but hear me out.</p>

<p>I’ve read that LLM performance is usually constrained by memory bandwidth, and for us plebs also by memory size, and there is a precedent in for example <a href="https://www.zfshandbook.com/docs/advanced-zfs/compression/">ZFS compression</a> which has shown to <em>increase</em> disk performance because you’re IO constrained rather than compute constrained.
So it might be beneficial to decompress LLM parameters on the fly, and if you’re doing that you might want to use a good lossy compression algorithm instead of blunt quantization.
<a href="http://pepijndevos.nl/2023/07/15/chatlmza.html">It is said that compression is equivalent to general intelligence</a>, so in that sense lossy compression would be expected to reduce intelligence, so you’d want to get a good compression ratio with minimal loss.</p>

<p>The way JPEG works is basically</p>
<ul>
  <li>break down the pixels in chunks - after decompression chunk boundaries are visible as JPEG artifacts.</li>
  <li>Discrete Cosine Transform them - lossless transformation in the family of Fourier transforms</li>
  <li>quantize them - data loss happens here, creating longer runs</li>
  <li>Run Length Encode them - compression happens here</li>
</ul>

<p>RLE is a lossless compression technique, which gets turbocharged by discarding some data to create longer runs.
In the case of image data, the DCT concentrates most information in the low frequencies so you can quantize high frequencies with minor loss in image quality.
Now, I don’t expect LLM parameters to be “smooth” like image data, so naive JPEG compression of LLM weights is not likely to be effective.</p>

<p>BUT!</p>

<p>You can reorder the columns and rows of a matrix without affecting the result. It’s like \(a+b+c=d \rightarrow c+b+a=d\).
So you could reorder your rows and columns to maximize clustering of similar values.
Not sure how you’d do this, maybe just sort by vector sum, or some genetic algorithm, or <a href="https://www.mathworks.com/help/matlab/math/sparse-matrix-reordering.html">other cleverness</a>.</p>

<p>So my proposed LLM compression would work like this</p>
<ul>
  <li>reorder the matrices to improve value clustering</li>
  <li>break down the values in chunks</li>
  <li>DCT them</li>
  <li>quantize them</li>
  <li>RLE them</li>
</ul>

<p>And then inference would</p>
<ul>
  <li>RLE expand a chunk</li>
  <li>inverse DCT it</li>
  <li>perform the multiplications</li>
</ul>

<p>So the compressed data would exist in VRAM and be decompressed on the fly chunk by chunk to perform a matrix vector product.
It’d take more compute, <a href="http://pepijndevos.nl/2018/07/04/loefflers-discrete-cosine-transform-algorithm-in-futhark.html">11 multiplications to be precise</a>, but if you’re memory constrained it could be worth it.</p>

<p>I guess the real question is if you can obtain any useful clustering in LLM data.
In a sense the parameters are already compressed(=intelligence), but there is no information in their order, so reordering and transforming parameters could improve RLE compression without incurring extra quantization loss.</p>

    </section>
    
</article>




        </div></div>
  </body>
</html>
