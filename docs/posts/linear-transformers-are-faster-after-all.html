<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://manifestai.com/blogposts/faster-after-all/">Original</a>
    <h1>Linear transformers are faster after all</h1>
    
    <div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
  

<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">



<p><span>\[
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\sft}{\text{softmax}}
\newcommand{\List}{\text{List}}
\newcommand{\Seq}{\text{Seq}}
\newcommand{\SeqT}{\text{SeqT}}
\newcommand{\CSeqT}{\text{CSeqT}}
\newcommand{\Dist}{\text{Dist}}
\newcommand{\SM}{\text{SM}}
\newcommand{\Fn}{\text{Fn}}
\newcommand{\Tok}{\text{Tok}}
\newcommand{\Aij}{ A_{[i,j]}}
\]</span></p>
<p>It is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length <span data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice <span data-cites="Tay2022Efficient"><a href="#ref-Tay2022Efficient" role="doc-biblioref">[2]</a></span>.</p>
<p>At the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput <span data-cites="dao2023flashattention"><a href="#ref-dao2023flashattention" role="doc-biblioref">[3]</a></span>. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.</p>

<div><p><strong>Experimental setup.</strong> Each timing experiment was run on an H100 with batch size 1 and vocabulary size 50k. We start with an exact JAX replication of GPT2 (numerically tested against <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>), and modify only the self-attention layers. Our implementation also includes modern bells-and-whistles such as mixed-precision training.</p></div><p>The experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in <span data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.</p>


<div><p>As we increase the context size, we utilize more and more GPU DRAM, until eventually we encounter an out of memory (OOM) error and cannot run a training step anymore. Different algorithms have different memory requirements (for example, FlashAttention utilizes dramatically less memory than traditional attention), so the various algorithms we tested have different ranges on this plot.</p></div><p>But speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.</p>

<div><p><strong>Experimental setup.</strong> These experiments are each run on an 8xH100 node for 24 hours. For all runs, we use flash-attention. We use the chunked algorithm with selecting optimal chunk size (see Section 3) for linear transformer runs. The dataset used was c4 <span data-cites="c4"><a href="#ref-c4" role="doc-biblioref">[4]</a></span>, tokenized by <a href="https://github.com/openai/tiktoken">tiktoken</a>’s GPT2 encoder. We plot the train loss because the dataset is large enough that all training was done in the single-epoch setting, and so there is no difference between train and heldout loss.</p></div>
<p>These results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.</p>
<p>Other variants of linear transformers have been proposed that claim resolve these learning issues <span data-cites="choromanski2020rethinking qin2022cosformer peng2021random qin2022devil hua2022transformer sun2023retentive yang2023gated"><a href="#ref-choromanski2020rethinking" role="doc-biblioref">[5]</a>–<a href="#ref-yang2023gated" role="doc-biblioref">[11]</a></span>, but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling.</p>
<section id="linear-transformers">
<h2 data-anchor-id="linear-transformers">1. Linear Transformers</h2>
<p>The inputs to a transformer layer are sequences of <span>\(Q_i, K_i, V_i \in \R^d\)</span> of query, key and values, where <span>\(i\)</span> ranges from <span>\(1\)</span> to the sequence length <span>\(t\)</span>. The outputs are a sequence <span>\(Y_i\in \R^d\)</span>. The well-known formula for the transformer layer, first popularized by Vaswani et al <span data-cites="vaswani2017attention"><a href="#ref-vaswani2017attention" role="doc-biblioref">[12]</a></span>, is: <span>\[
Y_i^\text{Transformer} = \sum_{j=1}^i e^{Q^T_i K_j} V_j
\]</span> Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.</p>

<div><p>Even though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.</p></div><p>The formula for the <em>linear transformer</em> (LT) layer is quite similar: just change the term <span>\(e^{Q^T_i K_j} \to Q^T_i K_j\)</span> yielding <span>\[
Y_i^\text{LinearTransformer} = \sum_{j=1}^i Q^T_i K_j V_j
\]</span></p>

<div><p>All our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on <span data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>, we divide each <span>\(Y_i\)</span> by <span>\(\sum_{j=1}^i Q^T_i K_j\)</span> after eunsuring the sum is positive by making keys and queries live in the positive quadrant using <code>softplus</code>.</p></div><p>This layer is “linear” in that the outputs <span>\(Y\)</span> are linearly related to all of <span>\(Q\)</span>, <span>\(K\)</span>, and <span>\(V\)</span>.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> From now on, we will omit the superscript of <span>\(Y_i^\text{LinearTransformer}\)</span> and just write <span>\(Y_i\)</span>. To begin our exploration of the computational cost of linear transformers, consider the following implementation.</p>
<div id="cb1"><pre><code><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span>def</span> LT_attention(Q, K, V):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span>    Shapes of inputs are</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span>     Q: [t, d]  K: [t, d]  V: [t, d]</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span>    Shapes of outputs are</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span>     Y: [t, d]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    t, d <span>=</span> Q.shape</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    Y_list <span>=</span> []</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span>for</span> i <span>in</span> <span>range</span>(t):           <span># loop cost: O(t^2 d)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        Y_i <span>=</span> zeros(d)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        Q_i <span>=</span> Q[i]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span>for</span> j <span>in</span> <span>range</span>(i):       <span># loop cost: O(id)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>            A_ij <span>=</span> inner(K[j], Q_i)  <span># cost: O(d)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            Y_i <span>+=</span> A_ij <span>*</span> V[j]   <span># cost: O(d)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        Y_list.append(Y_i)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span>return</span> stack(Y_list)</span></code></pre></div>
<p>Anyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call <em>attention</em> formulation, has a FLOP cost of <span>\(O(t^2 d)\)</span>.</p>
<p>The key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: <span>\[
Y_i = \sum_{j=1}^i Q^T_i K_j V_j = \underbrace{ \left (  \sum_{j=1}^i V_j  K_j^T\right )}_{S_i} \; \; Q_i
\]</span> Written in this form, we notice that the term labeled <span>\(S_i \in \R^{d\times d}\)</span> can be thought of as a state summarizing all the relevant information up to time <span>\(i\)</span>. It’s easy to rewrite into the following recurrent equations <span>\[
Y_{i} = S_i Q_i
\;\;\;\;\;\;\;\;\;\;\;\;
S_i = S_{i-1} + V_i K_i^T
\]</span> where we assume <span>\(S_{0} = 0\in \R^{d\times d}\)</span>. Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the <em>state</em> formulation, and analyze the cost:</p>
<div id="cb2"><pre><code><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span>def</span> LT_state(Q, K, V):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span>    Shapes of inputs are</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span>     Q: [t, d]  K: [t, d]  V: [t, d]</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span>    Shapes of outputs are</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span>     Y: [t, d]</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    t, d <span>=</span> Q.shape</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    S_i <span>=</span> zeros(d, d) <span># shape [d,d]</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    Y_list <span>=</span> []</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span>for</span> i <span>in</span> <span>range</span>(t):        <span># loop cost: O(t d^2)</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        S_i <span>+=</span> outer(K[i], V[i]) <span># cost: O(d^2)</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        Y_i <span>=</span> S_i <span>@</span> Q[i]      <span># cost: O(d^2)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        Y_list.append(Y_i)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span>return</span> stack(Y_list)</span></code></pre></div>
<p>We see that the cost here is <span>\(O(t d^2)\)</span>.</p>
<p>So, while a standard transformer layer always has cost <span>\(O(t^2 d)\)</span>, linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from <span>\(O(t^2 d)\)</span> to <span>\(O(t d^2)\)</span>, trading a <span>\(t\)</span> term for a <span>\(d\)</span> term.</p>
</section>
<section id="parallel-implementations">
<h2 data-anchor-id="parallel-implementations">2. Parallel Implementations</h2>
<p>In general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.</p>
<p>First, let’s do this for attention. Our main technique is to compute the attention matrix <span>\(A\)</span>, which contains all the terms <code>outer(Q[i], K[j])</code> that appeared inside the for-loops of <code>LT_attention</code>, using a single heavyweight matrix multiply.</p>
<div id="cb3"><pre><code><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span>def</span> LT_attention_parallel_no_flash(Q, K, V):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span>    Shapes of inputs are</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span>     Q: [t, d]  K: [t, d]  V: [t, d]</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span>    Shapes of outputs are</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span>     Y: [t, d]</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    t <span>=</span> Q.shape[<span>0</span>]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    M <span>=</span> causal_mask(t)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    A_raw <span>=</span> Q <span>@</span> K.T  <span># cost O(t^2 d)</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    A <span>=</span> A_raw <span>*</span> M    <span># cost O(t^2)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    Y <span>=</span> A <span>@</span> V        <span># cost O(t^2 d)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Y</span></code></pre></div>
<p>This implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized <em>flash attention</em> kernels <span data-cites="dao2023flashattention"><a href="#ref-dao2023flashattention" role="doc-biblioref">[3]</a></span> have been used to get even further speedups by avoiding explicitly storing the attention matrix <span>\(A\)</span>, and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use <code>LT_attention_parallel</code> to refer to the flash attention implementation.</p>
<p>Next, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms <span>\(V_i K^T_i\)</span> in parallel, and then use a cumulative-sum, which can be <a href="https://en.wikipedia.org/wiki/Prefix_sum">parallelized</a>, to combine them.</p>
<div id="cb4"><pre><code><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span>def</span> LT_state_parallel(Q, K, V):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span>    Shapes of inputs are</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span>     Q: [t, d]  K: [t, d]  V: [t, d]</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span>    Shapes of outputs are</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span>     Y: [t, d]</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    P <span>=</span> V[:,:,<span>None</span>] <span>@</span> K[:,<span>None</span>,:]  <span># cost: O(t d^2)</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    S <span>=</span> cumsum(P, axis<span>=</span><span>0</span>)          <span># cost: O(log_2(t) t d^2)</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    Y <span>=</span> S <span>@</span> Q[:,:,<span>None</span>]            <span># cost: O(t d^2)</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Y[:,:<span>0</span>]</span></code></pre></div>
<p>The cost in FLOPs of this algorithm is <span>\(O(\log_2(t) t d^2)\)</span>.<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Now that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our <code>LT_attention_parallel</code> implementation, we use a custom linear self-attention flash kernel we implemented in Triton <span data-cites="tillet2019triton"><a href="#ref-tillet2019triton" role="doc-biblioref">[13]</a></span> based on <a href="https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html">OpenAI’s FlashAttention2 implementation</a>.</p>

<p>Here are some takeaways:</p>
<ul>
<li>As expected, the <code>attention</code> variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>). The <code>state</code> variants all have linear asymptotic cost (slope 1). <a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></li>
<li><code>LT_state_parallel</code> is an order-of-magnitude faster than <code>LT_state</code>.</li>
<li><code>LT_attention_parallel_no_flash</code> is two orders-of-magnitude faster than <code>LT_attention</code>.</li>
<li><code>LT_attention_parallel</code> seems to asymptotically stabilize into being an order-of-magnitude faster than <code>LT_attention_parallel_no_flash</code>.</li>
<li>For the majority of settings, <code>LT_attention_parallel</code> is the fastest. (This is the linear version of the algorithm used by the standard transformer.)</li>
<li>Parallel attention is the fastest algorithm for small context sizes. However, <code>LT_state_parallel</code> overcomes <code>LT_attention_parallel_no_flash</code> at around 13k context size, and overcomes <code>LT_attention_parallel</code> at around 100k.</li>
</ul>
<p>Overall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?</p>
</section>
<section id="chunked-formulation">
<h2 data-anchor-id="chunked-formulation">3. Chunked Formulation</h2>
<p>It’s evident that, for small context sizes, computing the <span>\(t\)</span> by <span>\(t\)</span> attention matrix is much more efficient than computing many <span>\(d\)</span> by <span>\(d\)</span> state matrices. But as <span>\(t\)</span> grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small <span>\(t\)</span> and that states are necessary for large <span>\(t\)</span> motivates doing one last reworking of the LT equation.</p>
<p>Let <span>\(c \in \N\)</span> be a positive integer that we’ll call the <em>chunk size</em>. For any <span>\(i\in \N\)</span> find the unique <span>\(n\in \Z\)</span> s.t. <span>\(cn &lt; i \le c(n+1)\)</span>. We can easily see that the following equations are equivalent to the previous ones. <span>\[
Y_{i} = S_{cn}Q_i + \sum_{j=cn+1}^i Q_i^T K_j V_j
\;\;\;\;\;\;\;\;\;\;\;\;
S_{c(n+1)} = S_{cn} + \sum_{j=cn+1}^{c(n+1)} V_j K_j^T
\]</span> The key idea is that we are only going to compute a subset of all states: <span>\(S_0, S_c, S_{2c}, \cdots\)</span>. Then, to compute each output <span>\(Y_i\)</span>, we need only to take into account the contribution via the most recent state <span>\(S_{cn}\)</span>, as well as the contribution (computed via attention) of all moments in time <span>\(j\)</span> in the range <span>\(cn &lt; j \le i\)</span>.</p>
<p>As pseudocode, this looks like:</p>
<div id="cb5"><pre><code><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span>def</span> LT_attention_with_initial_state(S, Q, K, V):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span>    Shapes of inputs are</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span>     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span>    Shapes of outputs are</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span>     Y: [c, d]</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    Y_state <span>=</span> Q <span>@</span> S                               <span># cost O(c d^2)</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    Y_attention <span>=</span> LT_attention_parallel(Q, K, V)  <span># cost O(c^2 d)</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    Y <span>=</span> Y_state <span>+</span> Y_attention                     <span># cost O(cd)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Y</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span>def</span> LT_chunked(Q, K, V, c):</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span>&#34;&#34;&#34;</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span>    Shapes of inputs are</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span>     Q: [t, d]  K: [t, d]  V: [t, d], c: int</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span>    Shapes of outputs are</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span>     Y: [t, d]</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span>    &#34;&#34;&#34;</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    t, d <span>=</span> Q.shape</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span>assert</span> t <span>%</span> c <span>==</span> <span>0</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    Q_, K_, V_ <span>=</span> [arr.reshape(t<span>//</span>c, c, d)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    `               <span>for</span> arr <span>in</span> [Q,K,V]]</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    P_ <span>=</span> K_.transpose([<span>0</span>,<span>2</span>,<span>1</span>]) <span>@</span> V_  <span># cost O(t d^2)</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    S_ <span>=</span> cumsum(P_, axis<span>=</span><span>0</span>) <span>-</span> P_     <span># cost O(log_2(t/c)(t/c)d^2)</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    Y_ <span>=</span> vmap(LT_attention_with_initial_state, axis<span>=</span><span>0</span>)(</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>                S_, Q_, K_, V_)      <span># cost O(td^2 + tcd)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span>return</span> Y_.reshape(t, d)</span></code></pre></div>
<p>The cost is <span>\(O\left(td^2 + tcd + \log_2(t/c)(t/c)d^2\right)\)</span>, once again avoiding a quadratic dependency on <span>\(t\)</span>. Also, note that this algorithm makes an inner call to <code>LT_attention_parallel</code>, so we can use a flash-attention kernel to do that part of the computation.</p>
<p>This algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.</p>

<p>We see <code>LT_chunked</code> gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (&amp; rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers.</p>
</section>
<section id="sampling">
<h2 data-anchor-id="sampling">4. Sampling</h2>
<p>When working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, <span>\(z_1 \cdots z_t\)</span>, and we want to sample the next token, <span>\(z_{t+1}\)</span>.</p>
<p>The most efficient algorithm to sample from traditional transformers is called the <em>KV-cache</em> algorithm <span data-cites="pope2023efficiently"><a href="#ref-pope2023efficiently" role="doc-biblioref">[14]</a></span>. This algorithm assumes that when we generate token <span>\(z_{t+1}\)</span>, we will have already computed and cached all the <span>\(K_i, V_i\)</span> for all <span>\(0 \le i \le t\)</span>. In order to compute the output of the attention layer at time <span>\(t+1\)</span> given this cached information, we can use <span>\[
Y_{t+1}^\text{Transformer} = \sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j
\]</span> It is easy to see that this is an <span>\(O(td)\)</span> operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> This is one of the major limitations of the classic transformer architecture.</p>
<p>With linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size <span>\(O(d^2)\)</span>. <span>\[
Y_{i} = S_i Q_i
\;\;\;\;\;\;\;\;\;\;\;\;
S_i = S_{i-1} + V_i K_i^T
\]</span> We can compare the time it takes to generate any particular token when sampling a sequence:</p>

<p>As expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
</section>
<section id="learning-performance">
<h2 data-anchor-id="learning-performance">5. Learning Performance</h2>
<p>Until now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.</p>
<p>In order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees <span>\(2^{19}\)</span> tokens.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> Importantly, for this set of experiments, we have used the dataset c4 <span data-cites="c4"><a href="#ref-c4" role="doc-biblioref">[4]</a></span>, which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.</p>
<p>First, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.</p>

<p>Both architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.</p>

<p>We see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.</p>
<p>The results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to <em>ignore</em> many more irrelevant tokens, explaining the slowed initial learning.<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>In contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.</p>
<p>Remedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning <span data-cites="choromanski2020rethinking qin2022cosformer peng2021random qin2022devil hua2022transformer sun2023retentive yang2023gated"><a href="#ref-choromanski2020rethinking" role="doc-biblioref">[5]</a>–<a href="#ref-yang2023gated" role="doc-biblioref">[11]</a></span>. A future post will include a thorough study of the impact of all of these choices.</p>
<p>Ultimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity.<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.</p>



</section>


<div id="quarto-appendix"><section id="acknowledgments"><h2>Acknowledgments</h2><p>We would like to thank: Jono Ridgway for helping to prepare the release; Eric Alcaide for introducing us to the associative scan algorithm; Jannis Fengler for working on the tooling to confirm our JAX GPT2 numerically replicates NanoGPT; Joel Einbinder and Aaron Mondal for their assistance in setting up our engineering stack; Warfa Jibril, Tony Pezzullo, and Desh Raj for feedback on a draft of the blog post. …</p></section><section role="doc-bibliography"><h2>References</h2><div id="refs" role="list">
<div id="ref-katharopoulos2020transformers" role="listitem">
<p>[1] </p><p>A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, <span>“Transformers are rnns: Fast autoregressive transformers with linear attention,”</span> in <em>International conference on machine learning</em>, PMLR, 2020, pp. 5156–5165.</p>
</div>
<div id="ref-Tay2022Efficient" role="listitem">
<p>[2] </p><p>Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, <span>“Efficient transformers: A survey,”</span> <em>ACM Computing Surveys</em>, vol. 55, no. 6, pp. 1–28, 2022, doi: <a href="https://doi.org/10.1145/3530811">10.1145/3530811</a>.</p>
</div>
<div id="ref-dao2023flashattention" role="listitem">
<p>[3] </p><p>T. Dao, <span>“Flashattention-2: Faster attention with better parallelism and work partitioning,”</span> <em>arXiv preprint arXiv:2307.08691</em>, 2023.</p>
</div>
<div id="ref-c4" role="listitem">
<p>[4] </p><p>C. Raffel <em>et al.</em>, <span>“Exploring the limits of transfer learning with a unified text-to-text transformer,”</span> <em>The Journal of Machine Learning Research</em>, vol. 21, no. 1, pp. 5485–5551, 2020.</p>
</div>
<div id="ref-choromanski2020rethinking" role="listitem">
<p>[5] </p><p>K. Choromanski <em>et al.</em>, <span>“Rethinking attention with performers,”</span> <em>arXiv preprint arXiv:2009.14794</em>, 2020.</p>
</div>
<div id="ref-qin2022cosformer" role="listitem">
<p>[6] </p><p>Z. Qin <em>et al.</em>, <span>“Cosformer: Rethinking softmax in attention,”</span> <em>arXiv preprint arXiv:2202.08791</em>, 2022.</p>
</div>
<div id="ref-peng2021random" role="listitem">
<p>[7] </p><p>H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, <span>“Random feature attention,”</span> <em>arXiv preprint arXiv:2103.02143</em>, 2021.</p>
</div>
<div id="ref-qin2022devil" role="listitem">
<p>[8] </p><p>Z. Qin <em>et al.</em>, <span>“The devil in linear transformer,”</span> <em>arXiv preprint arXiv:2210.10340</em>, 2022.</p>
</div>
<div id="ref-hua2022transformer" role="listitem">
<p>[9] </p><p>W. Hua, Z. Dai, H. Liu, and Q. Le, <span>“Transformer quality in linear time,”</span> in <em>International conference on machine learning</em>, PMLR, 2022, pp. 9099–9117.</p>
</div>
<div id="ref-sun2023retentive" role="listitem">
<p>[10] </p><p>Y. Sun <em>et al.</em>, <span>“Retentive network: A successor to transformer for large language models,”</span> <em>arXiv preprint arXiv:2307.08621</em>, 2023.</p>
</div>
<div id="ref-yang2023gated" role="listitem">
<p>[11] </p><p>S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim, <span>“Gated linear attention transformers with hardware-efficient training,”</span> <em>arXiv preprint arXiv:2312.06635</em>, 2023.</p>
</div>
<div id="ref-vaswani2017attention" role="listitem">
<p>[12] </p><p>A. Vaswani <em>et al.</em>, <span>“Attention is all you need,”</span> <em>Advances in neural information processing systems</em>, vol. 30, 2017.</p>
</div>
<div id="ref-tillet2019triton" role="listitem">
<p>[13] </p><p>P. Tillet, H.-T. Kung, and D. Cox, <span>“Triton: An intermediate language and compiler for tiled neural network computations,”</span> in <em>Proceedings of the 3rd ACM SIGPLAN international workshop on machine learning and programming languages</em>, 2019, pp. 10–19.</p>
</div>
<div id="ref-pope2023efficiently" role="listitem">
<p>[14] </p><p>R. Pope <em>et al.</em>, <span>“Efficiently scaling transformer inference,”</span> <em>Proceedings of Machine Learning and Systems</em>, vol. 5, 2023.</p>
</div>
<div id="ref-shen2021efficient" role="listitem">
<p>[15] </p><p>Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, <span>“Efficient attention: Attention with linear complexities,”</span> in <em>Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, 2021, pp. 3531–3539.</p>
</div>
</div></section><section><h2>Citation</h2><div><p>BibTeX citation:</p><pre><code>@misc{buckman2024,
  author = {Buckman, Jacob and Gelada, Carles},
  publisher = {Manifest AI},
  title = {Linear {Transformers} {Are} {Faster} {After} {All}},
  date = {2024-01-05},
  langid = {en}
}
</code></pre><p>For attribution, please cite this work as:</p><div id="ref-buckman2024" role="listitem">
<p>J.
Buckman and C. Gelada, <span>“Linear Transformers Are Faster After
All.”</span> Manifest AI, Jan. 05, 2024.</p>
</div></div></section></div></main> <!-- /main -->

</div></div>
  </body>
</html>
