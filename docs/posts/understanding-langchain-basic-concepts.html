<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.kevinhu.me/2023/07/10/hacking-langchain-for-fun-and-profit/">Original</a>
    <h1>Understanding LangChain Basic Concepts</h1>
    
    <div id="readability-page-1" class="page"><div id="wrap">
      
      <div>
        <section id="main"><article id="post-hacking-langchain-for-fun-and-profit" itemscope="" itemprop="blogPost">
  
  <div>
    
    
      <header>
        
  
    
  

      </header>
    
    <div itemprop="articleBody">
      
        
<p>Recently I’ve looked into the LangChain project and I was surprised by how it could be such a powerful and mature a project built in such short span of time. It covers many essential tools for creating your own LLM-driven projects, abstracting cumbersome steps with only a few lines of code.</p>
<p>I like where the project direction is going, and the development team has been proactively including and introducing new ideas of the latest LLM features in the project.</p>
<p>The path to understanding this new project weren’t really smooth. It has its own opinions for code organization and it could be unintuitive to guess how to hack your own projects for more than the tutorials. Many of the tutorials out there explains how to create a small application with LangChain but doesn’t cover how to intuitively comprehend the abstraction and design choices.</p>
<p>Hence I have taken the initiative to document my personal cognitive process throughout this journey. By doing so, I aim to clarify my own understanding while also providing assistance to y’all who are interested in hacking LangChain for fun and profit.</p>
<p>This blog post will dedicate to the overall understanding of all the concepts. I found it really helpful to start by understanding the concepts that directly interacts with the LLM, especially the core API interfaces. Once you have the mindmap of all the LangChain abstractions, it’s much more intuitive to hack and extend your own implementation.</p>
<p>I’ll be covering the very basic concepts around Chain and Agents:</p>
<ul>
<li>Chain</li>
<li>Tool</li>
<li>Template</li>
<li>Agent</li>
<li>AgentExecutor</li>
</ul>
<p>What I’m not writing in this blog, and they’ll be for another blog/discussion:</p>
<ul>
<li>Embedding</li>
<li>Memory</li>
<li>Document Loader</li>
<li>Vector Store</li>
</ul>
<blockquote>
<p>Pick the right ones, and programming will flow naturally from design;</p>
<p>Pick the wrong ones, and programming will be a series of nasty surprises.</p>
<p>– MIT Professor Daniel Jackson on Abstraction in Software, in his book “Software Abstractions”</p>
</blockquote>

<h2 id="21-chains"><a href="#21-chains"></a> 2.1 Chains</h2>
<p>Chains are the basic way of organizing actions, extending LLM capabilities, and integrate different Chain actions together. You can think of it as a “chain” of actions grouped together.</p>
<p>The interface of the <a target="_blank" rel="noopener" href="https://github.com/hwchase17/langchain/blob/master/langchain/chains/base.py#L33C10-L33C10">base Chain class</a>:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span><span>class</span> <span>Chain</span>:</span></span></pre></td></tr></tbody></table></figure>
<p>Once you understand this it’s pretty clear how to extend the Chain. You’ll need to define:</p>
<ul>
<li>The input parameters</li>
<li>The output parameters</li>
<li>The action to take when calling the Chain, by defining the abstract <code>_call()</code> method (or <code>_acall()</code> for asynchronous calling, but I’d like to skip those for now).</li>
</ul>
<p>The <code>__call__()</code>, and <code>run()</code> methods are really just wrappers around this core method that processes input parameters.</p>
<p>Sometimes it could be confusing that there are so many different ways of calling the same Chain. But think of:</p>
<ul>
<li><code>_call()</code> as the basic functionality you’ll need to define as the developer. It has nice, preprocessed input parameters.</li>
<li><code>__call__()</code> or <code>run()</code> as the interface for users of your project that takes in more flexible input.</li>
</ul>
<p>With this interface, you can extend the functionality by “chaining” them together in a link. The output from the previous chain will be the input keys to the next.</p>
<p>See examples in: <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/02-langchain-chains.ipynb">https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/02-langchain-chains.ipynb</a></p>
<h3 id="211-llmchain"><a href="#211-llmchain"></a> 2.1.1 LLMChain</h3>
<p>LLMChain is a special type of chain that wraps around the underlying LLM generative engine. And it’s the most commonly used Chains for direct use and extensibility. You can extend it to any special features you want, or even “chain” them up to perform a pipeline of actions with LLM.</p>
<p>The interface for LLMChain is simple. See more at <a target="_blank" rel="noopener" href="https://github.com/hwchase17/langchain/blob/master/langchain/chains/llm.py#L30">source code</a>.</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span><span>class</span> <span>LLMChain</span>:</span></span></pre></td></tr></tbody></table></figure>
<p>LLMChain extends the original Chain by defining:</p>
<ul>
<li>inputs: the same inputs that are required by the text template.</li>
<li>outputs: the “text” field, which is the output return of the LLM generation.</li>
</ul>
<p>You can either directly call it, or use it to build more specialized Chains.</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span>chain = LLMChain(llm=llm, template=tempate)</span></pre></td></tr></tbody></table></figure>
<p>See more at the “Template” section to understand prompt Template.</p>
<h3 id="212-extending-and-joining-chains"><a href="#212-extending-and-joining-chains"></a> 2.1.2 Extending and Joining Chains</h3>
<p>You can extend the Chain to accomplish anything that requires inputs and produces an output. Think of it as a task and you can use it for: e.g. text preprocessing, or even parsing.</p>
<p>A Chain doesn’t necessarily have to involve interacting with LLMs. It can be any task you find useful when implementing the whole task pipeline.</p>
<p>See examples in section “Generic Chains”:</p>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/02-langchain-chains.ipynb#scrollTo=f66a25a2">https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/02-langchain-chains.ipynb#scrollTo=f66a25a2</a></p>
<p>In the example, the TransformChain that does just regex transformations to remove white spaces. You can use it together with other Chains to create a pipeline of <code>transformation -&gt; rewrite</code> using <code>SequentialChain</code> to link them together.</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span>sequential_chain = SequentialChain(</span></pre></td></tr></tbody></table></figure>
<p><img src="https://blog.kevinhu.me/2023/07/10/hacking-langchain-for-fun-and-profit/langchain-sequentialchain.jpg" alt="Concept of Chains"/></p>
<p>Once you understand Chains, you can build powerful pipeline of chains in LangChain (hence the name). There are chains that:</p>
<ul>
<li>Calculates and run math operations.</li>
<li>Summarizes text.</li>
<li>Translates into a different language.</li>
<li>Come up with product names and slogans.</li>
<li>…</li>
</ul>
<p>See more Chain examples on Github: <a target="_blank" rel="noopener" href="https://github.com/hwchase17/langchain/tree/master/langchain/chains">https://github.com/hwchase17/langchain/tree/master/langchain/chains</a></p>
<h2 id="22-prompt-template"><a href="#22-prompt-template"></a> 2.2 Prompt Template</h2>
<p>I was quite baffled with the idea of prompt and templates when I’m first exposed to LangChain. But the idea is actually quite simple. It’s the same idea with any templates: you define a template text, and interpolate it with text variables.</p>
<p>The most common use case of prompt template is that it creates the outline of the input to the LLM, and you can customize the input by variables. That’s it. That’s how simple it is.</p>
<p>One common use case for Template is, as mentioned above, to format the final LLM prompt. It could be very useful in Agents, where you have multiple queries to the LLM, and you want to define the prompt with different intermediate steps at each iteration.</p>
<p>And let’s take a look at the concept of Agents.</p>
<h2 id="23-agent"><a href="#23-agent"></a> 2.3 Agent</h2>
<p>One of the most powerful applications for LLM is tool-use. Agent provides an abstraction to choose from a toolbox to solve more open and complicated questions.</p>
<p>According to LangChain’s official documentation:</p>
<blockquote>
<p>Some applications require a flexible chain of calls to LLMs and other tools based on user input. The <strong>Agent</strong> interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.</p>
</blockquote>
<p>See more at:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/">https://python.langchain.com/docs/modules/agents/</a></li>
<li><a target="_blank" rel="noopener" href="https://archive.pinecone.io/learn/langchain-agents/">https://archive.pinecone.io/learn/langchain-agents/</a></li>
</ul>
<h3 id="231-agent-interface"><a href="#231-agent-interface"></a> 2.3.1 Agent Interface</h3>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span><span>class</span> <span>Agent</span>:</span></span></pre></td></tr></tbody></table></figure>
<p>That’s it. That’s the interface for Agent.</p>
<p>First step of understanding the Agent is to strip away the complicated tool-use features etc and look at the interface.</p>
<p>Agent is an automatic actor that can make “plans” based on each step of the LLM output. You can add more features to create a full-featured, complete Agent that can run actions for you, e.g. Tools to use tools, PromptTemplates to build prompts, Parsers to parse output.</p>
<p>To create your own LangChain Agents, <strong>you’ll just need to worry about making the plans</strong> (e.g. handling input, creating prompts, parsing outputs, and returning outputs).</p>
<p>To illustrate the interface for Agents, I’ve created a very simple implementation of a dummy agent that executes whatever tool you define for exactly 3 times.</p>
<p>In this example, the <strong>plan</strong> is: return the <code>AgentAction</code> for 3 times with whatever tool that’s given, and then return the <code>AgentFinish</code>.</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span><span>class</span> <span>DummyAgent</span>(<span>BaseSingleActionAgent</span>):</span></span></pre></td></tr></tbody></table></figure>
<p>(See the <a target="_blank" rel="noopener" href="https://gist.github.com/hxy9243/8374ba05f1732e89300bd77bfe5a137d">snippet</a> on Gist.
Also, I’ve just started a small side project that hacks Agents. See more on <a target="_blank" rel="noopener" href="https://github.com/hxy9243/sepcial_agents/blob/main/special_agents/agents/dummy_agent.py">Github</a>.)</p>
<h3 id="232-tools"><a href="#232-tools"></a> 2.3.2 Tools</h3>
<p>Tool is an interface that interacts with other environments. The interface is real simple too, with <code>run</code> or asynchronous <code>arun</code>.</p>
<p>Tools can be any external actions to the LLM, e.g. calculators, search engines, SQL execution, document or data loaders, or anything with an API. It can also be any other Chains!</p>
<p>Its interface is also simple. Similarly, you’ll just need to define the inputs, outputs, and what to run.</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>from</span> langchain.tools.base <span>import</span> BaseTool</span></pre></td></tr></tbody></table></figure>
<p>Sometimes it could be confusing as it could be used with different ways of initializing:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span></span></pre></td></tr></tbody></table></figure>
<p>But the idea is the same. Remember, it’s but syntactic sugar to create a <code>Tool</code> with name, description, and the <code>_run</code> step is to call the func.</p>
<p>Tool’s function could be an API call (e.g. calculator, search, load text, …), or it could invoke other Chains. It’s flexible like that, and you can reuse Chains or even Agents as Tool function. So in this way, one <code>Agent</code> can invoke other <code>Agent</code>s.</p>
<h3 id="233-agentexecutor"><a href="#233-agentexecutor"></a> 2.3.3 AgentExecutor</h3>
<p>To get an idea of how Agents come about and some of the fundamental ideas on LLM task performing, see my other <a href="https://blog.kevinhu.me/2023/06/12/Paper-Readings-on-LLM-Tool-Performing/">blog on a list of papers I found useful in understanding LLM reasoning</a>.</p>
<p><strong>AgentExecutor is also also a Chain</strong>: it has the exact simple interface of a Chain: input, output, and the action - which is to wrap everything about Agents together.</p>
<p>There are quite a few syntactic sugars provided by the LangChain library to “initialize_agent”. But remember, it’s not returning an Agent, but an AgentExecutor, which has the interface of a “Chain”.</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span><span>from</span> langchain.agents <span>import</span> initialize_agent</span></pre></td></tr></tbody></table></figure>
<p>(Example from: <a target="_blank" rel="noopener" href="https://archive.pinecone.io/learn/langchain-agents/">https://archive.pinecone.io/learn/langchain-agents/</a>)</p>
<p>The <code>Agent</code> class abstract the most essential part of the agent behavior: how it “plans” each step based on input and intermediate results, and how it decides what actions to take, or whether to finalize the Agent execution.</p>
<p>The <code>_call()</code> implementation of an AgentExecutor Chain wraps it all up:</p>
<ul>
<li>Initialize the Agent by passing arguments to it.</li>
<li>Reads Agent output.</li>
<li>Runs the actions from the toolbox.</li>
<li>Finalize the execution by providing the output.</li>
<li>Other infrastructure code like timeout, iteration limits, output streaming, etc.</li>
</ul>
<p><img src="https://blog.kevinhu.me/2023/07/10/hacking-langchain-for-fun-and-profit/langchain-agent-executor.jpg" alt="AgentExecutor Workflow"/></p>
<p>These grunt works all implemented in AgentExecutor so that we can focus on the interesting part, which is the actual planning.</p>
<p>And typically we ignore these grunt work and only focuses on the interesting part, like creating a ReAct Agent that performs tasks based on the tools given.</p>
<p>And yes, <code>AgentExecutor</code> is a <code>Chain</code> and so it can be used with other Chains or as Tools to other agents.</p>
<p>See another example in the same Pinecone tutorial mentioned above:</p>
<figure><table><tbody><tr><td><pre><span>1</span></pre></td><td><pre><span></span></pre></td></tr></tbody></table></figure>
<p>llm_math is an AgentExecutor class that wraps the “llm_math” Agent, and it’s a Chain whose <code>run()</code> interface is a function to invoke the Agent.</p>
<p>Clear enough?</p>
<p>LangChain already provides a rich library of Agents that can perform interesting work, like reading CSV data, managing files, calling APIs, etc.</p>
<p>See: <a target="_blank" rel="noopener" href="https://github.com/hwchase17/langchain/tree/master/langchain/agents/agent_toolkits">https://github.com/hwchase17/langchain/tree/master/langchain/agents/agent_toolkits</a></p>

<p>Once you understand all these pieces, you can assemble everything together to make your own Agent.</p>
<p>There are two papers behind the implementations. I’ve also mentioned them in <a href="https://blog.kevinhu.me/2023/06/12/Paper-Readings-on-LLM-Tool-Performing/">my previous blog</a>:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.00445">MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</a> that describes the idea of combining LLM with external tools.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a> that describes the ideas of formatting the prompt to make LLMs reason with external tools and Chain-of-Thought reasoning.</li>
</ul>
<p>LangChain had its implementation of <a target="_blank" rel="noopener" href="https://github.com/hwchase17/langchain/blob/master/langchain/agents/mrkl/base.py">ZeroShotAgent</a></p>
<ol>
<li>Create a prompt to guide the workflow, create few shot examples to follow the pattern of:</li>
</ol>
<ul>
<li>Question:</li>
<li>Thought:</li>
<li>Action:</li>
<li>Action Input:</li>
<li>Observation: (use tools, Thought + Action + Observation loop can happen N times)</li>
<li>Final answer:</li>
</ul>
<ol start="2">
<li>Create Tools to the Agent</li>
<li>Parse the LLM output from the Thought, e.g. what tools to use, is it the final answer.</li>
<li>Invoke the tools and create “Observation”.</li>
<li>Create yet another prompt based on the output, and again feed it to LLM.</li>
<li>Repeat until getting final answer. Output.</li>
</ol>
<p>If you think this is helpful, I’ll keep exploring and write what I found about LangChain and NLP + LLM in general. Hope this helps your understanding!</p>

<ul>
<li>Harrison Chase’s presentation: <a target="_blank" rel="noopener" href="https://docs.google.com/presentation/d/1EDmM1R0AcstfjadCUpvoa8h7NGwNqE2dwtJXqRK1618/edit?pli=1#slide=id.g238c17053ef_0_73">https://docs.google.com/presentation/d/1EDmM1R0AcstfjadCUpvoa8h7NGwNqE2dwtJXqRK1618/edit?pli=1#slide=id.g238c17053ef_0_73</a></li>
<li>Pinecone’s LangChain Handbook: <a target="_blank" rel="noopener" href="https://archive.pinecone.io/learn/langchain/">https://archive.pinecone.io/learn/langchain/</a></li>
<li>LangChain official documentation: <a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/agents/">https://python.langchain.com/docs/modules/agents/</a></li>
<li>Streamli LangChain tutorial: <a target="_blank" rel="noopener" href="https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/">https://blog.streamlit.io/langchain-tutorial-1-build-an-llm-powered-app-in-18-lines-of-code/</a></li>
<li>Arize tutorial on LangChain: <a target="_blank" rel="noopener" href="https://arize.com/resource/langchain-tutorial/">https://arize.com/resource/langchain-tutorial/</a></li>
<li>Andrew Ng’s LangChain tutorial: <a target="_blank" rel="noopener" href="https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/">https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/</a></li>
</ul>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="https://blog.kevinhu.me/2023/06/12/Paper-Readings-on-LLM-Tool-Performing/" id="article-nav-older">
      <strong>Older</strong>
      <p>Paper Readings on LLM Task Performing</p>
    </a>
  
</nav>

  
</article>



</section>
        
          
        
      </div>
      
    </div></div>
  </body>
</html>
