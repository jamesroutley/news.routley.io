<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://bernsteinbear.com/blog/irs/?utm_source=rss">Original</a>
    <h1>What I talk about when I talk about IRs</h1>
    
    <div id="readability-page-1" class="page"><div>
            <p>I have a lot of thoughts about the design of compiler intermediate
representations (IRs). In this post I’m going to try and communicate some of
those ideas and why I think they are important.</p>

<p>The overarching idea is <em>being able to make decisions with only local
information.</em></p>

<p>That comes in a couple of different flavors. We’ll assume that we’re compiling
a method at a time, instead of a something more trace-like (tracing, tracelets,
basic block versioning, etc).</p>

<h2 id="control-flow-graphs">Control-flow graphs</h2>

<p>A function will normally have some control-flow: <code>if</code>, <code>while</code>, <code>for</code>, any
amount of jumping around within a function. Let’s look at an example function
in a language with advanced control-flow constructs:</p>

<div><div><pre><code><span>int</span> <span>sumto</span><span>(</span><span>int</span> <span>n</span><span>)</span> <span>{</span>
  <span>int</span> <span>result</span> <span>=</span> <span>0</span><span>;</span>
  <span>for</span> <span>(</span><span>result</span> <span>=</span> <span>0</span><span>;</span> <span>n</span> <span>&gt;=</span> <span>0</span><span>;</span> <span>n</span> <span>-=</span> <span>1</span><span>)</span> <span>{</span>
    <span>result</span> <span>+=</span> <span>n</span><span>;</span>
  <span>}</span>
  <span>return</span> <span>result</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>Most compilers will deconstruct this <code>for</code>, with its many nested expressions,
into simple comparisons and jumps. In order to resolve jump targets in your
compiler, you may have some notion of labels (in this case, words ending with a
colon):</p>

<div><div><pre><code>int sumto(int n) {
  entry:
    int result = 0
  header:
    if n &lt; 0, jumpto end
    result = result + n
    n = n - 1
    jumpto header
  end:
    return result
}
</code></pre></div></div>

<p>This looks kind of like a pseudo-assembly language. It has its high-level
language features decomposed into many smaller instructions. It also has
implicit fallthrough between labeled sections (for example, <code>entry</code> into
<code>header</code>).</p>

<p>I mention these things because they, like the rest of the ideas in this post,
are points in an IR design space. Representing code this way is an explicit
choice, not a given. For example, one could make the jumps explicit by adding a
<code>jumpto header</code> at the end of <code>entry</code>.</p>

<p>As soon as we add that instruction, the code becomes position-independent: as
long as we start with <code>entry</code>, the chunks of code between labels could be
ordered any which way: they are addressed by name and have no implicit ordering
requirements.</p>

<p>This may seem arbitrary, but it gives the optimizer more flexibility. If some
optimization rule decides, for example, that a branch to <code>block4</code> may rarely be
taken, it can freely re-order it toward the end of the function (or even on a
different page!) so that more hot code can be on a single cache line.</p>

<p>Explicit jumps and labels turn the code from a strictly linear assembly into a
<em>control-flow graph</em> (CFG). Each sequence of code without internal control-flow
is a called <em>basic block</em> and is a vertex in this graph. The directed edges
represent jumps between blocks. See for example this crude GraphViz
representation:</p>

<figure>
<!--[[[cog
dot("""
digraph G {
    entry -> header;
    header -> end;
    header -> header;

    entry [label="int result = 0"];
    header [label="if n < 0, jumpto end\nresult = result + n\nn = n - 1\njumpto header"];
    end [label="return result"];
}
""", "assets/img/cfg.svg", url="/assets/img/cfg.svg")
]]]-->

<!--[[[end]]]-->
<figcaption>Each basic block gets its own node in the graph, and directed edges
between nodes represent jumps.</figcaption>
</figure>

<p>We’re actually kind of looking at <em>extended basic blocks</em> (EBBs), which allow
for multiple control exits per block but only one control entry. A
strict basic block representation of the above code would look, in text
form, something like this:</p>

<div><div><pre><code>int sumto(int n) {
  entry:
    int result = 0
    condbranch n &lt; 0, iftrue: end, iffalse: header
  header:
    result = result + n
    n = n - 1
    condbranch n &lt; 0, iftrue: end, iffalse: header
  end:
    return result
}
</code></pre></div></div>

<p>Notice how each block has exactly one terminator (control-flow instruction),
with (in this case) 0 or 2 targets.</p>

<p>Opinions differ about the merits and issues of extended vs normal basic blocks.
Most compilers I see use normal basic blocks. In either case, bringing the IR
into a graph form gives us an advantage: thanks to Cousot and Cousot, our
favorite power couple, we know how to do abstract interpretation on graphs and
we can use this to build an advanced optimizer. See, for example, my <a href="https://bernsteinbear.com/blog/toy-abstract-interpretation/">intro to
abstract interpretation post</a>.</p>

<h2 id="register-based-irs">Register-based IRs</h2>

<p>Some IRs are stack based. For concatenative languages or some newer JIT
compilers, IRs are formatted in such a way that each opcode reads its operands
from a stack and writes its outputs to a stack. This is reminiscent of a
point-free coding style in languages such as Haskell or OCaml.</p>



<p>In this style, there is an implicit shared state: the stack. Dataflow is
explicit (pushes and pops) and instructions can only be rearranged if the stack
structure is preserved. This requires some non-local reasoning: to move an
instruction, one must also rearrange the stack.</p>



<p>By contrast, in a register-based IR, things are more explicit. Instructions
take named inputs (<code>v0</code>, <code>v12</code>, etc) and produce named outputs. Instructions
can be slightly more easily moved around (modulo effects) as long as inputs
remain defined. Local variables do not exist. The stack does not exist.
Everything is IR “variables”.</p>

<div><div><pre><code>v1 = CONST 1
v2 = CONST 2
v3 = ADD v1 v2
</code></pre></div></div>

<p>The constraints (names being defined) are <em>part of the IR</em>. This gets a little
bit tricky if it’s possible to define a name multiple times.</p>

<div><div><pre><code>v1 = CONST 1
v2 = CONST 2
x = ADD v1 v2
x = ADD x v2
v3 = MUL x 12
</code></pre></div></div>

<p>What does <code>x</code> mean in the instruction for <code>v3</code>? Which definition does it refer
to? In order to reason about the instruction <code>v3 = MUL x 12</code>, we have to keep
around some context. This is non-trivial: requiring compiler writers to
constantly truck around side-tables and update them as they do analysis is slow
and error-prone. Fortunately, if we enforce some interesting rules, we can push
that analysis work into one pass up-front…</p>

<!--
If some analysis pass
tells us that `x` has some value or some type, it may be tricky to
disambiguate. Let's take this locality property a little bit further.
-->

<h2 id="ssa">SSA</h2>

<p>Static single assignment (SSA) was introduced by a bunch of folks at IBM (see <a href="https://bernsteinbear.com/blog/ssa/">my blog
post</a> about the different implementations). In SSA-based IRs, each
variable can only be defined once. Put another way, a variable <em>is</em> its
defining instruction; alternately, a variable and its defining instruction are
addressed by the same name. The previous example is not valid SSA; <code>x</code> has two
definitions.</p>

<p>If we turn the previous example into SSA, we can now use a different name for
each instruction. This is related to the <a href="https://en.wikipedia.org/wiki/Unique_name_assumption">unique name assumption</a> or the
global names property: names do not depend on context.</p>

<div><div><pre><code>v1 = CONST 1
v2 = CONST 2
x0 = ADD v1 v2
x1 = ADD x0 v2
</code></pre></div></div>

<p>Now we can identify each different <code>ADD</code> instruction by the variable it
defines. This is useful in analysis…</p>

<!--
if an analysis tells us that the value of
`x0` is `5`, we can use that property throughout the program, completely
independent of the context (more in the next section).

SSA makes writing analyses and interpreting analyses easier. Great. But we
don't want to always do transformations inside the analysis pass; sometimes we
want to separate the phases. So I advocate for storing analysis results on the
IR nodes.
-->

<h2 id="type-information">Type information</h2>

<p>I took a class with Olin Shivers and he described abstract interpretation as
“automated theorem finding”. Unlike theorem <em>provers</em> such as Lean and Rocq,
where you have to manually prove the properties you want, static analysis finds
interesting facts that already exist in your program (and optimizers use them
to make your program faster).</p>

<p>Your static analysis pass(es) can annotate your IR nodes with little bits of
information such as:</p>

<ul>
  <li>has type <code>int</code></li>
  <li>has value <code>5</code></li>
  <li>is between <code>2</code> and <code>5</code></li>
  <li>is equivalent to the expression <code>y*z</code>
    <ul>
      <li>https://cfbolz.de/posts/record-known-result/</li>
    </ul>
  </li>
  <li>does not have side effects</li>
  <li>is loop invariant</li>
  <li>does not escape</li>
  <li>…</li>
</ul>

<p>If your static analysis is over SSA, then generally the static analysis is
easier and (potentially) storing facts is easier. This is due to this property
called <em>sparseness</em>. Where a static analysis over non-SSA programs has to store
facts about all variables at <em>all program points</em>, an analysis over SSA need
only store facts about all variables, independent of context.</p>

<p>I sometimes describe this as “pushing time through the IR” but I don’t know
that that makes a ton of sense.</p>

<p>Potentially more subtle here is that we <em>could</em> represent the above IR snippet
as a list of tuples, where instructions are related via some other table (say,
a “variable environment”):</p>

<div><div><pre><code><span>code</span> <span>=</span> <span>[</span>
  <span>(</span><span>&#34;v1&#34;</span><span>,</span> <span>&#34;CONST&#34;</span><span>,</span> <span>1</span><span>),</span>
  <span>(</span><span>&#34;v2&#34;</span><span>,</span> <span>&#34;CONST&#34;</span><span>,</span> <span>2</span><span>),</span>
  <span>(</span><span>&#34;x0&#34;</span><span>,</span> <span>&#34;ADD&#34;</span><span>,</span> <span>&#34;v1&#34;</span><span>,</span> <span>&#34;v2&#34;</span><span>),</span>
  <span>(</span><span>&#34;x1&#34;</span><span>,</span> <span>&#34;ADD&#34;</span><span>,</span> <span>&#34;x0&#34;</span><span>,</span> <span>&#34;v2&#34;</span><span>),</span>
<span>]</span>
</code></pre></div></div>

<p>Instead, though, we could allocate an object for each instruction and let them
refer to one another <strong>by pointer</strong> (or index, if using Rust or something).
Then they directly refer to one another (no need for a side-table), which might
be faster and more compact. We can re-create nice names as needed for printing.
Then, when optimizing, we look up the type information of an operand by
directly reading a field (<code>operands[N]-&gt;type</code> or similar).</p>

<p>Another thing to note: when you start adding type information to your IR,
you’re going to start asking type information questions in your analysis.
Questions such as “what type is this instruction?”, where “type” could span a
semilattice, and even refer to a specific run-time object by its pointer. In
that case, it’s important to ask <em>the right questions</em>. For example:</p>

<ul>
  <li><strong>wrong</strong>: Is instruction X a <code>Const</code> instruction?</li>
  <li><strong>right</strong>: Is the type of X known to be a specific object?</li>
</ul>

<p><code>Const</code> instructions are likely not the only opcodes that could produce
specific objects; if you have an instruction like <code>GuardIsObject</code>, for example,
that burns a specific expected pointer into the generated code, the type (and
therefore the pointer) will come from the <code>GuardIsObject</code> instruction.</p>

<p>The big idea is that types represent a different slice of your IR than the
opcodes and should be treated as such.</p>

<p>Anyway, SSA only stores type information about instructions and does not encode
information that we might later learn in the IR. With basic SSA, there’s not a
good way to encode refinements…</p>

<h2 id="ssi">SSI</h2>

<p>Static single information (SSI) form gives us new ways to encode metadata about
instructions (variables). It was introduced by C. Scott Ananian in 1999 in his
<a href="https://bernsteinbear.com/assets/img/ananian-thesis.pdf">MS thesis</a> (PDF). (I also discussed it briefly
in <a href="https://bernsteinbear.com/blog/scrapscript-ir/">the Scrapscript IR post</a>.) Consider the following
SSA program (represented as pseudo-Python):</p>

<div><div><pre><code><span># @0
</span><span>x</span> <span>=</span> <span>int</span><span>(...)</span>
<span># @1
</span><span>if</span> <span>x</span> <span>&gt;=</span> <span>0</span><span>:</span>
    <span># @2
</span>    <span>return</span> <span>x</span>
<span>else</span><span>:</span>
    <span># @4
</span>    <span>result</span> <span>=</span> <span>-</span><span>x</span>
    <span># @5
</span>    <span>return</span> <span>result</span>
</code></pre></div></div>

<p><code>x</code> is undefined at <code>@0</code>. <code>x</code> is defined <em>and an integer</em> at <code>@1</code>. But then we
do something interesting: we split control flow based on the run-time value of
<code>x</code>. We can take this split to add new and interesting information to <code>x</code>. For
non-sparse analysis, we can record some fact on the side. That’s fine.</p>

<div><div><pre><code><span># @0
</span><span>x</span> <span>=</span> <span>int</span><span>(...)</span>
<span># @1
</span><span>if</span> <span>x</span> <span>&gt;=</span> <span>0</span><span>:</span>
    <span># @2
</span>    <span>LearnFact</span><span>(</span><span>x</span><span>,</span> <span>nonnegative</span><span>)</span>
    <span># @3
</span>    <span>return</span> <span>x</span>
<span>else</span><span>:</span>
    <span># @4
</span>    <span>LearnFact</span><span>(</span><span>x</span><span>,</span> <span>negative</span><span>)</span>
    <span># @5
</span>    <span>result</span> <span>=</span> <span>-</span><span>x</span>
    <span># @6
</span>    <span>return</span> <span>result</span>
</code></pre></div></div>

<p>When doing a dataflow analysis, we can keep track of the fact that at <code>@3</code>, <code>x</code>
is nonnegative, and at <code>@5</code>, <code>x</code> is negative. This is neat: we can then
determine that all paths to this function return a positive integer.</p>

<p>Importantly, <code>LearnFact(x, nonnegative)</code> does not <em>override</em> the existing known
type of <code>x</code>. Instead, it is a refinement: a set intersection. A lattice meet.
The middle bit of a Venn diagram containing two overlapping circles, <code>Integer</code>
and <code>Nonnegative</code>.</p>

<p>If we want to keep our information sparse, though, we have to add a new
definition to the IR.</p>

<div><div><pre><code><span># @0
</span><span>x</span> <span>=</span> <span>int</span><span>(...)</span>
<span># @1
</span><span>if</span> <span>x</span> <span>&gt;=</span> <span>0</span><span>:</span>
    <span># @2
</span>    <span>newx0</span> <span>=</span> <span>LearnFact</span><span>(</span><span>x</span><span>,</span> <span>nonnegative</span><span>)</span>
    <span># @3
</span>    <span>return</span> <span>newx0</span>
<span>else</span><span>:</span>
    <span># @4
</span>    <span>newx1</span> <span>=</span> <span>LearnFact</span><span>(</span><span>x</span><span>,</span> <span>negative</span><span>)</span>
    <span># @5
</span>    <span>result</span> <span>=</span> <span>-</span><span>newx1</span>
    <span># @6
</span>    <span>return</span> <span>result</span>
</code></pre></div></div>

<p>This is complicated (choose which variables to split, replace all uses, to
maintain SSA, etc) but gives us new places to store information <em>inside the
IR</em>. It means that every time we refer to <code>newx0</code>, we know that it is
nonnegative and every time we refer to <code>newx1</code>, we know that it is negative.
This information is independent of context!</p>

<p>I should note that you can get a lot of the benefits of SSI without going “full
SSI”. There is no need to split every variable at every branch, nor add a
special new merge instruction.</p>

<p>Okay, so we can encode a lot of information very sparsely in the IR. That’s
neat. It’s powerful. But we should also be mindful that even in this very
sparse representation, we are encoding information implicitly that we may not
want to: execution order.</p>

<h2 id="sea-of-nodes">Sea of nodes</h2>

<p>In a traditional CFG representation, the instructions are already <em>scheduled</em>,
or ordered. Normally this comes from the programmer in the original source form
and is faithfully maintained. We get data use edges in an IR like SSA, but the
control information is left implicit. Some forms of IR, however, seek to reify
both data and control dependencies into the IR itself. One such IR design is
<em>sea of nodes</em> (SoN), which was originally designed by Cliff Click during his
PhD.</p>

<p>In sea of nodes, every instruction gets its own vertex in the graph.
Instructions have use edges to their operands, which can be either data or some
other ordering property (control, effects, etc). The main idea is that IR nodes
are <em>by default</em> unordered and are only ordered later, after effect analysis
has removed a bunch of use edges.</p>

<p>Per Vognsen also notes that there is another motivating example of sea of
nodes: in the previous SSI example, the <code>LearnFact(x, nonnegative)</code> cannot be
validly hoisted above the <code>n &gt;= 0</code> check. In a “normal” IR, this is implicit in
the ordering. In a sea of nodes world, this is explicitly marked with an edge
from the <code>LearnFact</code> to the <code>if n &gt;= 0</code>. I think Graal, for example, calls
these <code>LearnFact</code> nodes “Pi nodes”.</p>

<p>I think I need to re-read the original paper, read a modern implementation (I
get the feeling it’s not done <em>exactly</em> the same way anymore), and then go
write more about it later. For now, see
<a href="https://github.com/seaofnodes/simple">Simple</a>, by Cliff Click and friends. It
is an implementation in Java and a little book to go with it.</p>

<p>Design neighbors include value dependence graphs (VDG), value state dependence
graphs (VSDG), region value state dependence graphs (RVSDG), and program
dependence graphs (PDG).</p>

<!--
## Union-find

Optimizers rewrite instructions. It's kind of their whole shtick. Consider the
following snippet of some made-up IR:

```
x = INTMUL y, 8
```

If we're in a situation where both `x` and `y` are machine-width words, then
it's fairly commonplace to replace that instruction with a left shift. It's
semantically equivalent, but takes fewer cycles on modern processors.

```
x = INTLEFTSHIFT y, 3
```

Depending on how your IR nodes are allocated, your original and your
replacement instructions may take up different amounts of memory, so it may not
be possible to directly overwrite the original opcode and its operands. You
also might not *want* to if keeping the original IR around is valuable to you.

Thankfully, union-find allows you to keep a side-table of equivalent
operations. You can mark that the old instruction is equivalent to the
replacement instruction and be done with it...right?

## E-graphs

Attach equivalence classes to each IR node

Intrusive, don't shell out to another tool (egg, souffle)

-->

<h2 id="more">More?</h2>

<p>There’s probably more in this vein to be explored right now and probably more
to invent in the future, too. Some other potentially interesting concepts to
think about include:</p>

<ul>
  <li>union-find</li>
  <li>e-graphs</li>
  <li>Webs, for operating on connected components of IR nodes that are all
concerned about a thing (for example, call and function nodes are concerned
about the signature of the callee)</li>
  <li>High-level IRs and strength reduction of language semantics before low-level
stuff (no low-level concerns in my high-level IR, please)</li>
</ul>

<h2 id="thank-yous">Thank yous</h2>

<p>Thank you to <a href="https://cfallin.org/">Chris Fallin</a>, Hunter Goldstein, and Per
Vognsen for valuable feedback on drafts of this post.</p>

        </div></div>
  </body>
</html>
