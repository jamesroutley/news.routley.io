<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://decomposition.al/blog/2025/12/30/interpreters-everywhere/">Original</a>
    <h1>Interpreters everywhere!</h1>
    
    <div id="readability-page-1" class="page"><div>
      
        <header>
          
          

  <p>
    
      
      <span>
        <i aria-hidden="true"></i>
        
        <time datetime="2025-12-30T14:30:00+00:00">December 30, 2025</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section itemprop="text">
        
        



<p>Last month, I was thrilled to have the chance to give a colloquium talk, ‚ÄúInterpreters everywhere!‚Äù, at the Indiana University Computer Science Colloquium.  This post is more or less a transcript of my talk, not including the Q&amp;A at the end or the lovely introduction by Carlo Angiuli.  Those parts, however, are included in the video recording, which you can find <a href="https://www.youtube.com/watch?v=q8398PMcuTc">on YouTube</a> if you‚Äôre interested!</p>

<h2 id="introduction">Introduction</h2>

<!-- Interpreters everywhere! -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-0.png" alt="Interpreters everywhere! / Lindsey Kuper @ UC Santa Cruz / IU CS Colloquium / November 7, 2025 / featuring the work of some amazing students: Gan Shen, Shun Kashiwa, Jonathan Castello, Patrick Redmond"/>
</figure>

<p>I‚Äôm so happy to be giving this talk, so thank you, <a href="https://www.carloangiuli.com/">Carlo</a>, for hosting, and for that lovely introduction.  I am going to talk about research, but this is much more than just a routine research talk; there is personal significance for me to be giving this particular talk to this particular audience, and I am only sorry that I couldn‚Äôt be there in person to visit Bloomington, where it‚Äôs so beautiful in the fall.</p>

<p>I‚Äôm going to begin, as I so often do, with a little bit of storytelling to set the stage.  And the story has to begin with <a href="https://en.wikipedia.org/wiki/Daniel_P._Friedman">Dan</a>.</p>

<!-- Dan Friedman -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-1.png" alt="A photo of Dan wearing his trademark black fedora."/>
</figure>

<p>I was a PhD student here at IU from 2008 to 2014, and when I arrived in 2008, I took Dan Friedman‚Äôs programming languages course, which was fondly known as ‚ÄúIntro to Dan‚Äù.  In this course, you learn about what programming languages are made of by writing interpreters.  So that was how my PhD began, and it was how I got hooked on programming languages as a field.</p>

<p>I was at Indiana for six years doing a PL PhD, and that journey involved a lot of twists and turns. That would be enough for a whole other talk. But, long story short, eventually I was working with my advisor, Ryan, on programming language abstractions for writing shared-memory deterministic concurrent programs. And we published some papers about this line of work, using what we called monotonic data structures.</p>

<p>And it was all about shared memory ‚Äì or so I thought.  But then, in the last year or so of my PhD, people who were interested in distributed computing ‚Äì so, specifically <em>not</em> shared memory ‚Äì tracked me down and said, ‚ÄúHey, your work seems to have a lot to do with our work.  You really ought to read our work.‚Äù</p>

<!-- My PhD -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-3.png" alt="A screenshot of an email to me from Marc Shapiro about CRDTs, dated May 8, 2013."/>
</figure>

<p>As an example of that, here‚Äôs an email I got in 2013 from <a href="https://www.lip6.fr/Marc.Shapiro/">Marc Shapiro</a>.  He‚Äôs a well-known distributed systems researcher, and he‚Äôs one of the inventors of something called conflict-free replicated data types, or CRDTs, which are data structures that are used in distributed systems to ensure fault tolerance and high availability.  So I got this email from Marc near the end of my PhD, and you don‚Äôt have to read this, but this is essentially pointing out that I ought to be paying attention to his and his collaborators‚Äô work on CRDTs, ‚Äòcause it had a lot to do with my work.  And it turned out, of course, they were right.</p>

<p>Of course, to make sense of CRDTs, I had to do some background reading on distributed systems, and one thing kind of led to another, and, you know, when I started looking at the papers that these folks were recommending to me, it was kind of a revelation for me, since up until then, I had only really paid attention to programming languages as a subfield, and I didn‚Äôt really look beyond PL research to consider everything else out there.</p>

<!-- Wait...distributed systems are fascinating! -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-4.png" alt="Hey, wait a second... Distributed systems are fascinating! / CRDT verification (OOPSLA &#39;20) / Causal broadcast protocol verification (IFL &#39;22) / Library-level choreographic programming (ICFP &#39;23, PLDI &#39;25) / Causal separation diagrams (OOPSLA &#39;24) / CRDT emulation (ICFP &#39;25)"/>
</figure>

<p>But once I did finally look beyond PL, I realized: Wait a second! Distributed systems are actually fascinating! And the field of distributed computing is something that I had a lot to learn from. Not only is it immediately practically relevant to our work and our lives, but it‚Äôs also full of profound and beautiful results. And so that‚Äôs when I really began to study distributed systems.</p>

<p>Of course, I haven‚Äôt stopped being a PL person; PL is still my research home. But for these last, you know, ten to fifteen years, I‚Äôve attempted to study distributed systems from a PL person‚Äôs point of view.  I was going pretty slowly at first, but this learning process accelerated quite a bit when I got a job at UC Santa Cruz in 2018, and I was suddenly expected to teach distributed systems courses, so at that point it really meant I couldn‚Äôt fake it anymore; I had to actually deeply understand something about distributed systems, instead of being a dilettante like I had been.  And that in turn influenced what research problems I was interested in.  So, these last several years, that‚Äôs mostly where my mind has been, research-wise, and my students and I have been going around sneaking distributed systems papers into PL conferences.  These papers listed here are a few examples from the last six years, a couple of which I‚Äôll say more about later.</p>

<!-- Hey, WAIT a second...it was all about interpreters the whole time! -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-5.png" alt="Hey, WAIT a second...it was all about interpreters the whole time! ü§Ø"/>
</figure>

<p>But finally, then, after many years of this, at some point recently I had <em>another</em> revelation.  And this one is not going to come as a surprise, you know, to someone like Dan, or really to any of the people here who are part of the Dan Friedman intellectual tradition, which I like to call the Dan-aspora ‚Äì and of course that includes a lot of you.</p>

<p>And that revelation, of course, was that it was actually about interpreters the whole time!  So, despite having been on this sort of long side quest into distributed computing all these years, it turned out that a lot of what my students and I had been doing all this time was still all about interpreters and interpretation.</p>

<!-- Oops! All interpreters! -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-6.png" alt="The Oops! All Berries box with &#39;Interpreters&#39; instead of &#39;Berries&#39;, and with Dan instead of Cap&#39;n Crunch"/>
</figure>

<p>So, you know ‚Äì oops.</p>

<p>So in the rest of this talk, I‚Äôm going to discuss a couple of our group‚Äôs projects that exemplify this realization that I had. And both of these projects are about distributed systems, but I‚Äôm going to talk about them through the lens of interpreters.  So of course this obligates us to try to answer this question of ‚ÄúWhat is an interpreter?‚Äù</p>

<p>So by the way, I would like this to be interactive if possible, so, you know, for those of you who are on Zoom, feel free to put peanut gallery comments in the chat. They won‚Äôt be a distraction to me; in fact, I enjoy it.  So I‚Äôm interested in people‚Äôs takes on, you know, what exactly is an interpreter.</p>

<!-- What is an interpreter? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-11.png" alt="What is an interpreter? The dictionary definitions aren&#39;t particularly helpful."/>
</figure>

<p>So, what exactly is an interpreter?  Well, first let‚Äôs see what the dictionary says. And this is actually a shout out to <a href="https://homes.luddy.indiana.edu/ccshan/">Ken Shan</a>, who, when I asked on Mastodon what an interpreter is, he helpfully sent me this dictionary definition, which is ‚Äúsomeone or something that interprets‚Äù.</p>

<p>Well, okay, ‚Äúthanks‚Äù. That‚Äôs not actually very helpful at all.</p>

<p>But there‚Äôs another definition that‚Äôs more computing-specific!  Let‚Äôs see what that one says. There are actually two definitions here. One is ‚Äúa machine that prints on punch cards the symbols recorded in them by perforations‚Äù.  Hmm, okay.  And the other is ‚Äúa computer program that executes each of a set of high-level instructions before going to the next instruction‚Äù.</p>

<p>Okay, well, ‚Äúyou tried‚Äù.  I think this definition is pretty bad also.  I think we can do much better than this!</p>

<!-- My definition of "interpreter" -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-14.png" alt="My definition of an &#39;interpreter&#39;: something that takes syntax and produces semantics."/>
</figure>

<p>Okay, so how am <em>I</em> gonna define what an interpreter is? So, for the purposes of this talk I‚Äôm going to say that an interpreter is something that takes <em>syntax</em> and produces <em>semantics</em>.</p>

<p>I like the generality of this definition ‚Äì so it lets us plug in domain-specific notions of ‚Äúsyntax‚Äù and ‚Äúsemantics‚Äù for whatever domain we care about. And it doesn‚Äôt even have to be computing-specific.  For example ‚Äì and this analogy is due to my friend <a href="https://cfbolz.de/">CF</a> ‚Äì we can think of a choir as an interpreter.  I‚Äôm a choral singer myself, and this analogy makes a lot of sense to me.  It‚Äôs the choir‚Äôs job to interpret a musical score ‚Äì usually aided by a director, who, you know, for the purpose of this analogy, we can think of the director as part of the choir ‚Äì it‚Äôs their job to interpret the score and ultimately produce some audible music.</p>

<p>In computing, we might think of an interpreter as something that takes a <em>program</em> and produces a <em>value</em>.  So I‚Äôm using the word ‚Äúvalue‚Äù here, but I‚Äôm using it pretty loosely, because that sounds very pure and like there aren‚Äôt any side effects, but a program might well have some sort of effect on the world.  I‚Äôm not even going to attempt to define ‚Äúprogram‚Äù here, except to say that both this notion of ‚Äúprogram‚Äù and this notion of ‚Äúvalue‚Äù might be domain-specific.</p>

<!-- My definition of "interpreter" -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-15.png" alt="For the purposes of this talk, an interpreter could be a compiler: it could take a program and produce another program."/>
</figure>

<p>I‚Äôll also point out that value you produce here might even itself be a program!  In which case, what is this last arrow on this slide? Well, now it‚Äôs actually a compiler, right? Because you‚Äôre taking a program and you‚Äôre producing a program.  So, you know, at risk of annoying <a href="https://jsiek.github.io/home/">Jeremy</a> and the other compiler folks here, I‚Äôm going to argue that, for the purposes of this talk, a compiler can be thought of as just a special case of an interpreter. And I would also argue that every programming language is ‚Äúinterpreted‚Äù, because sooner or later your code will be executed by the CPU, and a CPU is itself an interpreter.  So it‚Äôs interpreters all the way down.</p>

<!-- My definition of "interpreter" -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-16.png" alt="We can think of a choir as an interpreter that interprets a musical score into audible sound. Importantly, different parts may be interpreted individually and their interpretations can be composed. This is possible if there&#39;s an obvious way to break down the input into subcomponents and then put them back together."/>
</figure>

<p>And there‚Äôs one last really important thing that I want to mention about what ‚Äúinterpretation‚Äù means. So ‚Äì if you‚Äôll continue to indulge me with this choir analogy ‚Äì so, a music score for a choir is typically going to have different parts, right, that different people sing.  So, if we say that syntax is <em>interpretable</em>, then that usually is going to connote something about the form that that syntax takes.  In particular, it‚Äôll say something about how the interpretation of a given input has to be determined by the interpretations of its subcomponents, and the result you get is compositional.  To be able to do this, there has to be an obvious way to break down the input into its subcomponents and put them back together.  In other words, it has to be inductively structured data ‚Äì you know, like an AST.</p>

<!-- Wait...distributed systems are fascinating! -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-17.png" alt="Hey, wait a second... Distributed systems are fascinating!"/>
</figure>

<p>So what I want to do next is dig into a couple of specific places where this idea of interpretation seems to have arisen in my own work in these last several years.</p>

<p>I‚Äôm going to focus on two projects from that list that I gave earlier, and those are <em>library-level choreographic programming</em> and <em>causal separation diagrams</em>. I‚Äôm going to talk about both of these things, and I won‚Äôt really have time to do more than just scratch the surface of either of them, but I‚Äôm really happy to dig into the details in Q&amp;A, or chat with folks later.  So, let‚Äôs jump in, and let‚Äôs talk about library-level choreographic programming first.</p>

<h2 id="library-level-choreographic-programming">Library-level choreographic programming</h2>

<p>Okay, so let‚Äôs talk distributed systems!</p>

<p>In the world of distributed computing, we‚Äôre usually concerned with message passing.  The idea here is that you have some independent nodes that communicate with each other via a network, and they do it by sending and receiving some kind of messages.  They don‚Äôt share memory, so the only way for them to coordinate with each other is by passing messages.  And you might be able to imagine having some sort of so-called distributed shared memory as an abstraction on top of this, but under the hood, it‚Äôs ultimately all message passing.</p>

<p>So, let‚Äôs suppose that our independent nodes are Alice and Carol. And there‚Äôs typically some kind of, you know, global behavior that we want here. And Alice and Carol have to together accomplish that, but they have to do it by taking only <em>local</em> actions, so the only thing that either of them could do is send messages, or receive messages, or take internal actions.</p>

<!-- How to accomplish something global by taking only local actions? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-19.png" alt="How to accomplish something global by taking only local actions?"/>
</figure>

<p>So, if we want to run a really, really simple protocol in which Alice sends a request to Carol and Carol responds to Alice, how do we program that?</p>

<p>Well, we can assume for now that we have some sort of <code>send</code> and <code>recv</code> functions that actually implement the message transport somehow, so it might be TCP, it might be HTTP, it might be pigeons flying around.  Let‚Äôs not worry about the details of how the transport layer is implemented; let‚Äôs just think about what Alice and Carol have to do assuming it exists.</p>

<p>So, for this protocol, Alice would run this node-local program here on the left, and it might look like this: send a request to Carol, and then wait to receive a response from Carol, which gets stored in this variable here.</p>

<p>Meanwhile, Carol is running her own node-local program, which might look like: receive request from Alice and store it in this variable ‚Äòrequest‚Äô, then do something to handle the request, and finally send a response.</p>

<p>So this is just about the simplest distributed program that I could could conceive of, but even in this really simple program, notice how Alice and Carol are utterly dependent on <em>each other</em> to faithfully follow the protocol. So, for instance, if Carol neglected to call <code>recv</code> over here, now that‚Äôs become Alice‚Äôs problem.  So Alice will wait forever to get a response, or what she might do is time out and report an error, depending on how <code>send</code> is implemented.  So in general, in a distributed system, nodes have to execute messages in this very careful dance where a message sent from a node has to be expected by some recipient, and vice versa.  So if somebody is running buggy code, then things go wrong for somebody else.</p>

<!-- How to accomplish something global by taking only local actions? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-20.png" alt="A hand-drawn Lamport diagram showing an interaction between Alice, Carol, and Bob, in which Alice interacts directly only with Carol, not with Bob. At the bottom is a quote from Leslie Lamport: &#39;A distributed system is a system in which the failure of a computer that you didn&#39;t even know existed can render your own computer unusable.&#39;"/>
</figure>

<p>And it only gets more complicated if we add nodes.  Let‚Äôs say that Alice is sending Carol a request, and then Carol is supposed to pass the request along to Bob, and then get an acknowledgment from Bob, and finally respond to Alice to acknowledge her request.  This is the sort of pattern that you might see, for instance, in primary-backup replication, where ‚Äì let‚Äôs say Alice is the client, Carol is the primary, and Carol gets a request from Alice which she has to pass on to Bob, Bob has to ack Carol, and Carol has to ack the client. In this scenario, Alice may not even know that Bob <em>exists</em>. But nevertheless, if Bob is misprogrammed, then that could mean bad news for Alice.</p>

<p>In fact this, example calls to mind this wonderful quote from Leslie Lamport in which he defines a distributed system as ‚Äúa system in which the failure of a computer that you didn‚Äôt even know existed can render your own computer unusable.‚Äù</p>

<p>So, the challenge, then, for programmers is to try to reason about the behavior of this distributed system ‚Äì the <em>implicit global</em> behavior of this system ‚Äì  while writing the <em>explicit local</em> programs ‚Äì which are just sends and receives and internal actions ‚Äì that actually run on the nodes.</p>

<!-- Choreographic programming -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-21.png" alt="How to accomplish something global by taking only local actions? Choreographic programming&#39;s answer: Make the implicit global behavior explicit in the code!"/>
</figure>

<p>So, this brings us to choreographic programming.  And the idea of choreographic programming is: let‚Äôs make that implicit global behavior actually <em>explicit</em> in the code! In choreographic programming, we don‚Äôt write the local programs at all.  Instead, we write a single, global program that‚Äôs called a choreography.  And then we generate a collection of local programs, one for every node, from the choreography, via a technique called <em>endpoint projection</em>.</p>

<p>So what we have on the left here is a choreography that expresses the protocol that we talked about earlier. So, every choreographic programming language is going to have some sort of operator like this highlighted squiggle-arrow here.  And this is usually pronounced ‚Äúcomm‚Äù, because it denotes communication between two parties.</p>

<p>So, looking at this first line here in the choreography, a comm expression is going to express a both send and a receive at once.  And what this means is that Alice is going to compute this value <code>request</code>, communicates it to Carol, Carol receives it and stores it on her end in the variable <code>x</code>. When this choreographic program gets projected, every time you have an occurrence of Alice on the left-hand side of a <code>~&gt;</code>, it‚Äôs going to turn into a <code>send</code> call on Alice‚Äôs node, and every time you have an occurrence of Carol on the right-hand side, that‚Äôs going to turn into a <code>recv</code> on Carol‚Äôs node.</p>

<p>Thanks to this way of writing code, as long as our endpoint projection is correct, we can rule out the possibility of a mismatched send and receive, and we can therefore rule out a certain kind of bugs that could occur in our collection of local programs that we end up with.  In a nutshell, that‚Äôs the benefit of choreographic programming.</p>

<p>I do want to acknowledge one of the elephants in the room, which is that with what we‚Äôve shown so far, choreographic programming per se doesn‚Äôt actually do anything to help us if one of these nodes should crash, or if one of these messages should be lost.  So, those fault tolerance problems were present in the non-choreographic version of the code, and they‚Äôre still present here.  So choreographic programming doesn‚Äôt do anything to fix that as such.</p>

<p>What it <em>does</em> do is rule out programming errors that stem from a programmer not calling <em>send</em> where they should or not calling <em>recv</em> where they should.  In a simple program like this, that might not seem like such a big deal.  But once more parties are involved, it becomes less simple, and it‚Äôs in that multi-party setting where choreographies really shine. So next let‚Äôs look at a setting with three participants and with some more interesting control flow.</p>

<!-- Choreographies with conditionals -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-23.png" alt="Choreographies with conditionals"/>
</figure>

<p>OK, so this is a slightly more interesting protocol.  Suppose that Alice wants to interact with Carol, but Bob stops her to check her credentials first.  So, this example is kind of like a sign-on through an third-party authentication service.</p>

<p>And by the way, this example is straight from the <a href="https://decomposition.al/zines/#communicating-chorrectly-with-a-choreography">choreographic programming zine</a> that my student Ali and I wrote last year, which you can get from my website if you want.  In the zine it‚Äôs that Alice wants to go to Carol‚Äôs birthday party but she‚Äôs stopped by the bouncer Bob.</p>

<p>In this protocol, Alice has to have her credentials verified by Bob before she can interact with Carol. So, Alice sends those credentials to Bob. Bob runs this <code>check</code> function on Alice‚Äôs communicated credentials, and then, depending on the outcome of that, Alice will either get an access token from Carol or not.  So this diagram represents two different ways that an execution might go.</p>

<p>So, how do we express this in a choreography? Well, in a choreography, we can have conditionals at the choreography level, and here‚Äôs what that looks like.  So, on the first line, here‚Äôs our squiggle-arrow again.  And this notation <code>Alice.credentials ~&gt; Bob.authRequest</code> means Alice is computing something called ‚Äúcredentials‚Äù and then sending it to Bob, who receives it and then stores it on his end in this variable called ‚ÄúauthRequest‚Äù.</p>

<p>And then we have a conditional.  So, the conditional check takes place locally on Bob‚Äôs node.  And then, depending on which branch we take, either Bob communicates ‚ÄúSuccess‚Äù to Carol, who then communicates the token to Alice, or Bob communicates ‚ÄúFailure‚Äù to Carol, and then Carol tells Alice that she can‚Äôt have the token.</p>

<p>So what I think is useful to point out about this execution is that even though the execution is spread out across these three different parties ‚Äì so this is a distributed execution ‚Äì if we think about the execution globally, this is actually a sequential program in which steps happen one after the other.  And we can see that from reading the choreography; the choreography just looks like straight-line code.  In fact, the choreography looks remarkably similar to our informal diagram that we drew.  So, if we were to write this code as a traditional distributed program, which is really three programs running on the three participants, then the control flow wouldn‚Äôt be as obvious. So having it as a choreography actually helps the readability of the code as well.</p>

<!-- Choreographies with conditionals -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-24.png" alt="A hand-drawn diagram showing a choreography with a conditional being projected to three endpoints"/>
</figure>

<p>OK, so to actually run this thing, we need to do endpoint projection on it.  So, endpoint projection will take this choreography and it‚Äôll project it to versions running on Alice, Bob, and Carol, and this is done automatically by the choreographic language implementation.  And I‚Äôve written some pseudocode to give an idea of what the projected code might look like.  And notice, like I said, in this projected code, the control flow isn‚Äôt as clear.  You might have to stare at it for a bit to figure out what‚Äôs supposed to happen first, what‚Äôs supposed to happen second.  So choreographies give us this readability advantage.</p>

<p>There‚Äôs some stuff that I‚Äôm leaving out here. One important side note here is that when you do endpoint projection, if the choreography had a conditional like this, then the parties that are involved need to be told what branch to take.  And this is called the ‚Äúknowledge-of-choice‚Äù problem.  And different choreographic languages have different ways of dealing with this.  For instance, if I hadn‚Äôt had code in my projected code that commmunicates the outcome ‚Äì or if I didn‚Äôt have code in my choreography, rather, that communicates the outcome of this <code>check(authRequest)</code> call, then some choreographic languages would then complain that the choreography is what we call ‚Äúunprojectable‚Äù.  And other languages would just insert the necessary communication code for you.  So that‚Äôs all an interesting problem in its own right, and if you‚Äôre interested in knowing more about that, then I‚Äôm happy to talk more about it later.</p>

<!-- 20 years of choreographies -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-25.png" alt="A timeline showing some advances in choreographic programming from 2005 to 2023"/>
</figure>

<p>OK, so now you know what choreographic programming is.  So, we didn‚Äôt invent any of this stuff; choreographies have been around for a long time.</p>

<p>Dating back to the early 2000s, there was something called the World Wide Web Consortium‚Äôs <a href="https://www.w3.org/2002/ws/chor/">Web Services Choreography Working Group</a>, and they developed <a href="https://www.w3.org/TR/ws-cdl-10/">a draft specification for a language called WS-CDL</a>, which stood for Web Services Choreography Description Language.  However, this wasn‚Äôt intended to be executable; it was only a specification language ‚Äì you couldn‚Äôt actually run these things.</p>

<p>So, fast forward to 2013, when there was a language called Chor developed by Marco Carbone and <a href="https://www.fabriziomontesi.com/">Fabrizio Montesi</a>.  And this pioneered the idea of an executable choreographic programming language that used endpoint projection to compile choreographies to these runnable node-local programs.</p>

<p>Then in 2020, Fabrizio Montesi teamed up with <a href="https://saveriogiallorenzo.com/">Saverio Giallorenzo</a> and <a href="https://marcoperessotti.com/">Marco Peresotti</a> to create the <a href="https://www.choral-lang.org/">Choral</a> programming language. And Choral is intended to be useful for realistic programming, and it‚Äôs notable for how it uses mainstream object-oriented programming abstractions, so it‚Äôs designed to be familiar to, say, Java programmers.  And the Choral compiler projects choreographies to executable Java code.</p>

<p>Then, in 2022, <a href="https://akhirsch.science/">Andrew Hirsch</a> and Deepak Garg created a choreographic language called <a href="https://dl.acm.org/doi/10.1145/3498684">Pirouette</a>, which combined choreographic programming with higher-order functional programming.  This paper made a splash in the functional programming research community, and it won a Distinguished Paper award at POPL that year, and this is how my students and I found out about choreographic programming for the first time.  And this paper, by the way, is also notable for its mechanized proof of deadlock freedom.</p>

<p>So, my student <a href="https://gshen42.github.io/">Gan Shen</a> read the Pirouette paper and decided he wanted to try implementing a choregraphic language himself.  And so, being who he is, the approach that seemed obvious to him was to do it as an embedded domain-specific language in the form of a Haskell library.  And the idea is really straightforward, and what‚Äôs funny about this is that Gan wasn‚Äôt even trying to do something novel ‚Äì he just wanted to learn about choreographies.  But it turned out, to our surprise, that this idea of implementing a choreographic language purely as a library had not been done before.  And this appeared <a href="https://dl.acm.org/doi/10.1145/3607849">at ICFP ‚Äò23</a>, where it also won a distinguished paper award. So for a couple years in there, every paper about choreographic programming in PACMPL was getting a distinguished paper award ‚Äì sadly, no longer true.</p>

<p>So let‚Äôs dig in a little bit and talk about HasChor and how it works.</p>

<!-- HasChor's contribution: library-level choreographic programming -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-27.png" alt="HasChor&#39;s contribution: library-level choreographic programming"/>
</figure>

<p>So I‚Äôm gonna contrast this with the traditional approach to choreographic programming, which is exemplified here by Choral. So, in Choral you write a choreography in the standalone Choral language and you compile it to executable endpoint code using a standalone compiler.  So Choral compiles to Java and then you can run those Java programs using the usual Java toolchain.  And this all works fine, but it means that you need this purpose-built language and this purpose-built compiler.</p>

<p>In HasChor, on the other hand ‚Äì and this is our framework ‚Äì choreographies are expressed as programs in an existing host language ‚Äì in this case it‚Äôs Haskell ‚Äì  and the choreographic language constructs and endpoint projection are all implemented entirely in this host-language library, so there‚Äôs no additional compiler support needed, no additional runtime support beyond what the host language can already do.</p>

<p>So in particular, our choreographic language is implemented in Haskell as a DSL using something called a <em>freer monad</em>, which separates the interface and implementation of effectful computations.  So by itself, this language doesn‚Äôt assign any meaning to choreographic operators like that <code>~&gt;</code> that I was talking about before. Instead, it just accumulates them as this big term, and then we dynamically interpret that term, based on the location where the code‚Äôs being run.  So, for instance, that <code>~&gt;</code> operator: if you‚Äôre a sender, that gets interpreted as <code>send</code>, if you‚Äôre a recipient, it gets interpreted as <code>recv</code>, and if you‚Äôre anyone else, then it‚Äôs a no-op.</p>

<p>All the usual benefits of embedded DSLs apply here.  So, because these programs in HasChor are really just Haskell programs, that means that a HasChor programmer can take advantage of the whole Haskell ecosystem.  They can use their existing Haskell tools for development and debugging, they can use other Haskell libraries, and they can compile it just like any other Haskell program.  Whereas, for something like Choral, it‚Äôs a whole new language, right?  You might not have any tooling unless the developers bother to implement it.</p>

<p>And there‚Äôs one other plus side, I think, to this embedded DSL approach, which is that it makes it easier to integrate choreographies into bigger non-choreographic systems, so there‚Äôs no need to change languages just to implement certain pieces as choreographies. And I think this is really important, because while choreographic programming may be a good fit for certain components of a program, there might be other parts of the program where trying to fit it into that choreographic paradigm just doesn‚Äôt make sense.  So maybe you have a program where almost all of the code happens locally, but maybe there‚Äôs like a short-lived network interaction, just to do authentication or something. So then, you know, it wouldn‚Äôt really make sense to have to write everything in a choreographic language just so you could get the benefit of choreographies for that one little piece.  So that‚Äôs an argument in favor of this library-level approach.</p>

<!-- Endpoint projection is just an interpreter -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-29.png" alt="Endpoint Projection is Just an Interpreter"/>
</figure>

<p>And one of the consequences of this is that endpoint projection actually happens dynamically, at run time.  So let‚Äôs take a look at the type of the endpoint projection function in HasChor, and I hope to convince you that it‚Äôs really just an interpreter.  I promise you that this is the only line of Haskell code that I‚Äôll make you look at in this whole talk.</p>

<p>So, endpoint projection is a function that takes two arguments.  The first argument is this <code>Choreo</code> thingie here, which is a choreography.  So, endpoint projection takes one of these choreographies, and the second argument it takes is a location, which is the place we want to project the choreography to, so, like, Alice or Bob.</p>

<p>And endpoint projection returns something called a <code>Network</code>, which you can think of as a low-level program in terms of <code>send</code>s and <code>recv</code>s that‚Äôs been specialized to the location you gave as an argument.</p>

<p>So for the Haskell enthusiasts here, both <code>Choreo</code> and <code>Network</code> are freer monads.  But you should think of them both as just being <em>programs</em>. And these choreographic operators that are in <code>Choreo</code> programs all get interpreted to things like <code>send</code>, <code>recv</code>, and <code>broadcast</code> in the <code>Network</code> programs.  So <code>Choreo</code> gives us a syntax.  On the <code>Network</code> side, we have ‚Äì I‚Äôve put ‚Äúsemantics‚Äù in quotes here, because this isn‚Äôt actually semantics, it‚Äôs still just syntax, and then <em>that</em> will get dynamically interpreted to the <em>actual</em> meaning of <code>send</code> and <code>recv</code> and so forth.  The <em>actual</em> meaning of <code>send</code> and <code>recv</code> and so forth is going to be supplied by a network backend, and HasChor comes with two of those that you can choose between. There‚Äôs an HTTP backend which uses HTTP for the network transport, and there‚Äôs a local backend which implements these <code>send</code> and <code>recv</code> functions as communication among threads on a single machine. So that‚Äôs endpoint projection: it‚Äôs just an interpreter!</p>

<!-- Tons of new CP libraries! -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-30.png" alt="Tons of new CP libraries!"/>
</figure>

<p>So, returning to our timeline of choreographic programming, it‚Äôs really exciting how since HasChor came out in 2023, a bunch more choreographic languages-as-libraries have popped up.  So there‚Äôs a bunch more new choreographic programming libraries in these last two years.  Some of them are from our group ‚Äì so, <a href="https://hackage.haskell.org/package/MultiChor">MultiChor</a> is a successor to HasChor, <a href="https://lsd-ucsc.github.io/ChoRus/">ChoRus</a> is a choreographic programming library for Rust, <a href="https://github.com/shumbo/choreography-ts">ChoreoTS</a> is a choreographic programming library for TypeScript ‚Äì but what‚Äôs even more exciting to me is that a bunch of these are <em>not</em> from our group.  So there‚Äôs <a href="https://icfp24.sigplan.org/details/ocaml-2024-papers/13/ChorCaml-Functional-Choreographic-Programming-in-OCaml">ChorCaml</a> for OCaml, there‚Äôs <a href="https://github.com/utahplt/chorex">Chorex</a> for Elixir, there‚Äôs <a href="https://github.com/lovrosdu/klor">Klor</a> for Clojure, there <a href="https://github.com/Foundations-of-Decentralization-Group/choret">Choret</a> for Racket, and there‚Äôs <a href="https://share.unison-lang.org/@kaychaks/unichorn/">UniChorn</a>, which is for a distributed programming language called Unison.  So all of these are doing dynamic endpoint projection, so endpoint projection at run time, just like HasChor, but they all do it using the idioms that make sense in those languages.  So, for instance, Choret is built on top of Racket, and in Choret it‚Äôs done using macros, which is super cool.  So all of these different approaches use the idioms that make sense in the languages that they‚Äôre implemented in, and I love that.</p>

<p>All right, and here I‚Äôve just given the GitHub URLs for a couple of our projects.  So <a href="https://github.com/gshen42/HasChor">here‚Äôs the HasChor repo</a> and <a href="https://github.com/lsd-ucsc/ChoRus">here‚Äôs the repo for ChoRus</a>, which again, is our choreographic programming framework for Rust.  So feel free to take a look at these and check them out.  This is work led by my students <a href="https://gshen42.github.io/">Gan Shen</a> and <a href="https://shun-k.dev/">Shun Kashiwa</a>, and then ChoRus was also ‚Äì the lead author on it was <a href="https://irvingstreet.me/">Mako Bates</a>, who is a recent PhD graduate from <a href="https://www.uvm.edu/~jnear/">Joe Near</a>‚Äôs group at the University of Vermont.</p>

<p>Okay! So, that‚Äôs it for the first line of work I wanted to talk about, on library-level choreographic programming.</p>

<h2 id="causal-separation-diagrams">Causal separation diagrams</h2>

<!-- Lamport diagrams are everywhere -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-32.png" alt="Lamport diagrams are everywhere"/>
</figure>

<p>And now I want to talk about causal separation diagrams. So, let‚Äôs talk diagrams!</p>

<p>So, people who work on concurrent and distributed programming been using diagrams like these ‚Äì I call them Lamport diagrams, but they‚Äôre also known as sequence diagrams and time diagrams and a bunch of other names ‚Äì people have been using diagrams like these since ‚Äì at least since the 1970s, as a way to try to wrap our heads around how concurrent and distributed systems work. I‚Äôve been using them throughout this talk.  And they‚Äôre pretty intuitive, you know?  I didn‚Äôt have to explain what these diagrams mean.  And you might have drawn one yourself on a whiteboard recently.</p>

<p>These diagrams depict the events of interest in a system ‚Äì usually things like when message are sent or received ‚Äì and the relationships between those events across space and time. And these diagrams are so intuitive that we see them at all levels of academia and industry.</p>

<p>But they are just pictures, right? They‚Äôre not formal objects ‚Äì but they <em>could</em> be formal objects!</p>

<!-- How to formalize executions? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-35.png" alt="How to formalize executions?"/>
</figure>

<p>So, if we want to reason <em>formally</em> about executions like the one that‚Äôs shown here, we need a formal <em>model</em> of executions. So, let‚Äôs talk about how to do that.  And I‚Äôll put the diagram on the left here, and this textual formalism on the right, but these are really expressing the same information.</p>

<p>So the first thing we would need is a collection of <em>processes</em>, right? These are modeling whatever locations are participating in the system.</p>

<p>And then, every process is going to have a sequence consisting of the <em>events</em> that occurred on it.  And events could be message sends, or they could be message receives, or they could be events that are internal to a process. So we have a sequence of events on each process that‚Äôs totally ordered.</p>

<p>And then, last, we‚Äôre going to have pairs of events that comprise ‚Äúmessages‚Äù, which is where information propagates from one place to another.  So here, for instance, \(a_1\) would be a local event on this process \(P_1\), \(a_2\) would be a message send event, and the message is being sent to \(P_2\), where the receive event is \(a_5\).</p>

<!-- How to formalize executions? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-36.png" alt="An execution with a causal loop: one message goes backward in time, to before where a message in its causal history was sent."/>
</figure>

<p>So what I‚Äôm showing here is a completely standard way to formalize executions, and it get used all the time, with good reason. But it does have some pitfalls. So, what are some of the problems with doing it this way?</p>

<p>One issue is that this representation, doesn‚Äôt stop you from representing something that‚Äôs actually physically impossible, like a causal loop. So here, <code>a_2</code> is the send event for this message, <code>a_5</code> is the receive event, and then later on process 2, we have a send event for a message that‚Äôs being sent backward in time, back to <code>P_1</code>.  So this is an execution that couldn‚Äôt actually physically happen, and the structure of our data doesn‚Äôt really help us avoid this situation.  The formalism will still let us write down this execution that couldn‚Äôt actually occur, and you would need to do some additional checking to make sure that your data is self-consistent.</p>

<!-- How to formalize executions? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-37.png" alt="An execution with consistent and inconsistent cuts."/>
</figure>

<p>The other problem is that this representation doesn‚Äôt stop you from representing what are called <em>inconsistent cuts</em>.</p>

<p>So, a <em>consistent</em> cut in an execution like this is something like this blue line with a happy face here.  It‚Äôs separating the events of the execution into a clear past and a clear future. So, here, the ‚Äúpast‚Äù is what‚Äôs above this blue line, and the ‚Äúfuture‚Äù is below it.  And if an event is on the ‚Äúpast‚Äù side of the cut, then if it‚Äôs a consistent cut, then everything that‚Äôs causally before that event is also on the ‚Äúpast‚Äù side of the cut.  And this blue cut is in fact a consistent cut.</p>

<p>An <em>inconsistent</em> cut, like the one represented by the red line here, fails to have that property.  So here, for instance, we have this event <code>a_5</code>, which is on the <em>past</em> side of this cut ‚Äì but <code>a_5</code> represents the reception of a message that was sent here at <code>a_2</code>, which is actually on the future side of the cut. So <code>a_2</code> should causally precede <code>a_5</code>, but <code>a_2</code> is on the <em>future</em> side of the cut.</p>

<p>So the traditional model doesn‚Äôt help us avoid inconsistent cuts.  And so, if I just take a prefix like this of the events on each process, well, it might be a consistent cut or it might be an inconsistent cut.  And the problem is that there‚Äôs no connection between the processes that we‚Äôre splitting into past and future, and the messages we need to respect.</p>

<p>So tou might have heard of this famous software development dictum ‚Äúmake illegal states unrepresentable‚Äù, which I think is due to Yaron Minsky.  As my student <a href="https://jonathan.com/">Jonathan</a> put it, this way of representing executions makes illegal states all too representable.</p>

<p>Okay, so what do we do about this? What should we do instead?</p>

<!-- How to formalize executions? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-39.png" alt="A traditional formalization of executions, with spatial boundaries prioritized."/>
</figure>

<p>To get a better idea of what‚Äôs problematic about the traditional approach to representing executions, let‚Äôs take a closer look at it.  So here I‚Äôve written down the same execution we were looking at before.  I‚Äôve turned the diagram on its side, and kind of flipped it around, and I‚Äôve removed the labels on events.  But it‚Äôs the same execution as before; it‚Äôs got the same structure.</p>

<p>The only difference is that I‚Äôve added some additional dotted gray and dashed blue lines to this diagram, which make the structure of the execution more explicit. So, in this traditional approach, the top-level structure of this execution is this collection of processes, so the first thing we have to do is visually separate these processes from each other.  And that‚Äôs what these dotted gray lines do.  And we can think of them as spatial boundaries between the processes.</p>

<p>And next, each of these individual processes has a sequence of events in time. And we can make that explicit too, by adding these dashed blue lines as temporal boundaries between events.</p>

<p>Unfortunately, we cannot really do the same thing with messages. Because we made the order of events within a process our top priority, messages are kind of a different kind of entity.</p>

<p>But notice how these message arrows always seem to cross these gray ‚Äúspatial‚Äù boundaries, which run horizontally.  They cross those rather than crossing the blue ‚Äútemporal‚Äù boundaries, which run vertically. So these process-local edges that cross over the dashed blue lines are inherently causally ordered by the structure of the data, but the edges that cross over the dotted spatial boundaries between processes are <em>not</em> inherently causally ordered by the structure of the data.  And that is what makes it possible to accidentally construct an inconsistent cut.</p>

<p>So, this happened because we prioritized the spatial boundaries first, instead of prioritizing the temporal boundaries.  What would happen if we did it the other way around?  What would happen if we laid down the temporal boundaries first, and then the spatial ones?</p>

<!-- How to formalize executions? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-40.png" alt="The traditional approach to formalizing executions, where spatial boundaries are prioritized, contrasted with the approach that CSDs take, where temporal boundaries are prioritized."/>
</figure>

<p>That‚Äôs exactly what our approach does.  So here‚Äôs that same execution again, but now represented using one of our diagrams, which we call ‚Äúcausal separation diagrams‚Äù.  So, it‚Äôs the same execution, but represented in a different way.</p>

<p>First, we‚Äôre laying down enough consistent cuts to separate every event.  That‚Äôs what these vertical dashed blue lines are.  So, instead of splitting up by processes first, we‚Äôre splitting up this execution into these <em>global time steps</em>, where by ‚Äútime‚Äù I mean logical time. And then, within each of these global time steps, we can add these spatial boundaries between each distinct location.</p>

<p>And we end up with this different way of decomposing our diagram. So the original way of decomposing it, we had this structure that consisted of these three separated sequences of events.  In our new decomposition, we have a single sequence, not of events but of ‚Äúglobal steps‚Äù. And then those global steps, which are these columns here, delimited by these dashed blue lines, those are each made up of ‚Äúlocal steps‚Äù delimited by the spatial boundaries.</p>

<p>So here we‚Äôve got four big global steps arranged from left to right, separated by the blue dashed lines, and then every global step happens to contain three local steps, separated by these gray dotted lines.</p>

<p>So, this choice, to prioritize the temporal boundaries over the spatial boundaries, is <em>the</em> fundamental difference between our approach ‚Äì CSDs ‚Äì and the traditional model. And everything else is just a consequence of that. And I‚Äôve been saying ‚Äútemporal‚Äù boundaries, but I could also just as well have called them <em>causal</em> boundaries, because this potential causality here is following the flow of time.  Hence the name ‚Äúcausal separation diagrams‚Äù.</p>

<p>What‚Äôs most interesting here is that both messages and processes now evolve over causal boundaries. Information <em>never</em> crosses directly across a spatial boundary. And as a consequence of that, a diagram that can be decomposed this way <em>necessarily</em> has no causal loops. In other words, if we can <em>construct</em> diagrams this way with a CSD, then they are necessarily going to be causal by construction. So we have made illegal states unrepresentable.</p>

<!-- How to formalize executions? -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-41.png" alt="CSDs are made of composable &#39;tiles&#39;"/>
</figure>

<p>Here are all the different kinds of local steps that we can build diagrams out of. In the first two here, a <code>tick</code> is just a local event on a process, and a <code>noop</code> is the absence of an event.</p>

<p>These last two ‚Äì <code>swap</code> and <code>assoc</code> here ‚Äì they don‚Äôt model any actual events. Instead, they just allow us to rearrange the wires through the diagram so that processes can communicate with more than just their immediate neighbors. You can think of these as sort of like ‚Äústructural‚Äù rules in a logic.</p>

<p>And the middle two ‚Äì <code>fork</code> and <code>join</code> ‚Äì these are modeling the two halves of a traditional message. But unlike in a traditional message, we have both the process and the message state that are coming out the right-hand side, rather than having the message go out the top or bottom.</p>

<p>So, in fact, our model doesn‚Äôt distinguish process state from message state at all! Nothing prevents a so-called ‚Äúprocess‚Äù from joining state with another process, or a local event from occurring on a message. So we‚Äôre referring to these inputs and outputs uniformly not by ‚Äúprocesses‚Äù or ‚Äúmessages‚Äù, but by ‚Äúsites‚Äù.  A site is just a place where state can exist.  This was my student <a href="https://jonathan.com/">Jonathan</a>‚Äôs innovation, and it took me a while to get my head around treating messages and processes as the same thing, but it actually makes sense to me now. So a process sending a message is just a site forking off a piece of its state out into the world as a new site, and a process receiving a message is just two sites joining together.</p>

<p>So now that we have these tiles, these composable tiles, we can stick them together however we like and get a consistent CSD.  And I‚Äôve left it out here for brevity, but each one of these tiles actually has a <em>type</em>, which says how we‚Äôre allowed to compose it with other tiles.  And we also have combinators for composing local steps into global steps, and for chaining global steps together into an execution.  And the type of each tile tells you how it can be composed with other tiles vertically and horizontally. And for the fancy type enthusiasts here: yes, these are dependent types!</p>

<!-- Inductively defined data -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-43.png" alt="inductively defined data"/>
</figure>

<p>So, okay, so what does all this have to do with interpreters, which is the topic of this talk?  So, earlier, when we were talking about what interpretation meant, I said that if we say that a syntax is <em>interpretable</em>, that usually means that the interpretation of a given input is determined by the interpretations of its subcomponents.  And to do that, we need to have an obvious way to break down the input into subcomponents and to put it back together.</p>

<p>The ‚Äúobvious way‚Äù to break things down and put them back together is what we call an induction principle.</p>

<p>So, for that traditional formalization of executions that I showed first, we unfortunately don‚Äôt have a nice induction principle for the thing on the left here; we just have a set of processes and a set of messages, and the structure of the data doesn‚Äôt tell us how we‚Äôre supposed to break things down and put them back together.  So executions that are represented in this traditional way on the left are not particularly amenable to interpretation ‚Äì not if we want to work in a tool that encourages us to inductively define data structures, especially like a mechanized proof assistant.</p>

<p>But on the other hand, executions that are represented as causal separation diagrams come with a lovely induction principle, since they‚Äôre actually just lists of global steps.  So, to take a CSD apart, you can just peel off one global step at a time; to put it back together you can just glue on one global step at a time. And this makes CSDs amenable to interpretation.</p>

<!-- CSDs are interpretable -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-48.png" alt="CSDs are interpretable"/>
</figure>

<p>So, CSDs give us a formal, graphical, inductively defined <em>syntax</em> for representing concurrent executions, and it‚Äôs something that we can straightforwardly write interpreters for. So that‚Äôs exactly what we do.</p>

<p>So, in our paper we talk about three ways to interpret CSDs ‚Äì that is, three ways of giving CSDs a semantics!</p>

<p>The first of these is an interpretation of CSDs into what we call <em>causal paths</em>.  Causal paths are nothing but a proof-relevant analogue of <a href="https://lamport.azurewebsites.net/pubs/time-clocks.pdf">the classic Lamport happens-before relation</a>. So, instead of just saying that two events are causally related or aren‚Äôt causally related, we can interpret a CSD into a dependent type of the causal paths through a diagram, and these paths give <em>evidence</em> of the causal relationship between events.</p>

<p>The second way that we interpret CSDs is into logical clock functions, which assign an event to its corresponding logical timestamp. So there are a lot of different kinds of logical clocks that get used in distributed system implementations: Lamport clocks, vector clocks, matrix clocks ‚Äì our interpretation is parametric in a choice of clock implementation, so you can instantiate the same machinery over many logical clocks, and we‚Äôve done that for a bunch of different flavors of clocks.</p>

<p>And the third way that we give to interpret CSDs is into proofs that clocks respect causal paths.  So a logical clock that respects causal paths is a clock that satisfies Lamport‚Äôs [clock condition](https://lamport.azurewebsites.net/pubs/time-clocks.pdf, which is the thing that makes a logical clock a logical clock. So using CSDs, we‚Äôre able to give a mechanized proof of the clock condition that‚Äôs parametric in the choice of clock.</p>

<p>This interpretation-based approach to verification of clocks would be really awkward and difficult if we didn‚Äôt have this inductive data structure that accounts for causality; but with CSDs, it just kind of falls out naturally.</p>

<p>So, to wrap up this part, I‚Äôll just mention that you can find <a href="https://github.com/lsd-ucsc/csds">our proof development on GitHub</a> ‚Äì this is done in Agda ‚Äì and my student Jonathan and I are actively looking for more things to use CSDs for, which I‚Äôd love to talk with folks about.</p>

<h2 id="conclusion">Conclusion</h2>

<!-- It was all interpreters all along! -->
<figure>
  <img src="https://decomposition.al/assets/images/2025-12-30-interpreters-everywhere-49.png" alt="it was all interpreters all along! / Thank you, Dan-aspora ‚ù§Ô∏è"/>
</figure>

<p>Okay, so, we‚Äôre almost at the end.  So, to sum up: I‚Äôve talked about two lines of work from my research group that turned out to be all about interpreters, all along.</p>

<p>So we talked about library-level choreographic programming, which lets programmers write distributed programs as single, unified programs in their favorite language, and then dynamically project them to multiple endpoints.</p>

<p>And we talked about causal separation diagrams, which are inductively defined data structures designed for elegant mechanized reasoning about happens-before relationships in concurrent executions.</p>

<p>But really, we could sum up these two lines of work as being about interpreters.  HasChor is just asking, ‚ÄúWhat if endpoint projection was an interpreter?‚Äù and CSDs are just asking, ‚ÄúWhat if you could write interpreters for Lamport diagrams?‚Äù</p>

<p>So the takeaway I have for you here is: you can‚Äôt go wrong by studying how interpreters and interpretation works.  And then go study something else, and you can reap the benefits of applying your knowledge to that new domain.</p>

<p>So that‚Äôs all I‚Äôve got, and I want to thank you for your attention!</p>

        
      </section>

      

      

      
  


    </div></div>
  </body>
</html>
