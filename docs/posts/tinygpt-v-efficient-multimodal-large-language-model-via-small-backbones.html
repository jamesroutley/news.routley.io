<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/DLYuanGod/TinyGPT-V">Original</a>
    <h1>TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><strong>TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones</strong></p>
<p dir="auto">Zhengqing Yuan❁, Zhaoxu Li❁, Lichao Sun❋</p>
<p dir="auto">❁Visiting Students at LAIR Lab, Lehigh University
❋Lehigh University</p>
<p dir="auto"> <a href="https://arxiv.org/abs/2312.16862" rel="nofollow"><img src="https://camo.githubusercontent.com/36622932abbbd4c66f324e4cc02e7046b72a8537858d95eaad23e7ea84f379b0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d41727869762d726564" data-canonical-src="https://img.shields.io/badge/Paper-Arxiv-red"/></a>  <a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V" rel="nofollow"><img src="https://camo.githubusercontent.com/e5f7577427133b911a9e8a4b5ad96a8b23f11c9fe148cabbf13e5f3161910dfa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d4d6f64656c2d626c7565" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue"/></a> <a href="https://huggingface.co/spaces/llizhx/TinyGPT-V" rel="nofollow"><img src="https://camo.githubusercontent.com/5762a687b24495afb299c2c0bc68674a2a7dfca9bda6ee444b9da7617d4223a6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a></p>

<h2 tabindex="-1" dir="auto"><a id="user-content-news" aria-hidden="true" tabindex="-1" href="#news"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>News</h2>
<p dir="auto">[Jan.03 2024] Welcome to Hugging Face online demo to try out our models (for Stage-3)!</p>
<p dir="auto">[Dec.28 2023] Breaking! We release the code of our TinyGPT-V.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-tinygpt-v-traning-process" aria-hidden="true" tabindex="-1" href="#tinygpt-v-traning-process"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TinyGPT-V Traning Process</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/examples/Training_S.png"><img src="https://jvns.ca/DLYuanGod/TinyGPT-V/raw/main/examples/Training_S.png" alt="Traning_Process"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-tinygpt-v-model-structure" aria-hidden="true" tabindex="-1" href="#tinygpt-v-model-structure"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TinyGPT-V Model Structure</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/examples/TinyGPT-V-ST.png"><img src="https://jvns.ca/DLYuanGod/TinyGPT-V/raw/main/examples/TinyGPT-V-ST.png" alt="Model"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-tinygpt-v-results" aria-hidden="true" tabindex="-1" href="#tinygpt-v-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>TinyGPT-V Results</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/examples/result.png"><img src="https://jvns.ca/DLYuanGod/TinyGPT-V/raw/main/examples/result.png" alt="Results"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-getting-started" aria-hidden="true" tabindex="-1" href="#getting-started"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Getting Started</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-installation" aria-hidden="true" tabindex="-1" href="#installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Installation</h3>
<p dir="auto"><strong>1. Prepare the code and the environment</strong></p>
<p dir="auto">Git clone our repository, creating a python environment and activate it via the following command</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/DLYuanGod/TinyGPT-V.git
cd TinyGPT-V
conda env create -f environment.yml
conda activate tinygptv"><pre>git clone https://github.com/DLYuanGod/TinyGPT-V.git
<span>cd</span> TinyGPT-V
conda env create -f environment.yml
conda activate tinygptv</pre></div>
<p dir="auto"><strong>2. Prepare the pretrained LLM weights</strong></p>
<p dir="auto"><strong>TinyGPT-V</strong> is based on Phi-2.
Download the corresponding LLM weights from the following huggingface space via clone the repository using git-lfs.</p>
<p dir="auto">Phi-2 2.7B: <a href="https://huggingface.co/susnato/phi-2" rel="nofollow">Download</a></p>
<p dir="auto">Then, set the variable <em>phi_model</em> in the model config file to the LLM weight path.</p>
<ul dir="auto">
<li>Set the LLM path <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/minigpt4/configs/models/minigpt_v2.yaml#L14">here</a> at Line 14, <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/minigpt4/configs/models/minigpt4_vicuna0.yaml#L18">here</a> at Line 18 and <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/minigpt4/conversation/conversation.py#L16">here</a> at Line 16.</li>
</ul>
<p dir="auto"><strong>3. Prepare the pretrained model checkpoints</strong></p>
<p dir="auto">Download the pretrained model checkpoints</p>
<table>
<thead>
<tr>
<th>After stage-1</th>
<th>After stage-2</th>
<th>After stage-3</th>
<th>After stage-4</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage1.pth" rel="nofollow">Download</a></td>
<td><a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage2.pth" rel="nofollow">Download</a></td>
<td><a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage3.pth" rel="nofollow">Download</a></td>
<td><a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage4.pth" rel="nofollow">Download</a></td>
</tr>
</tbody>
</table>
<p dir="auto">For <strong>TinyGPT-V</strong>, set the path to the pretrained checkpoint in the evaluation config file
in <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/eval_configs/tinygptv_stage1_2_3_eval.yaml#L8">tinygptv_stage1_2_3_eval.yaml</a> at Line 8 for Stage 1, 2 and 3 version or <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/eval_configs/tinygptv_stage4_eval.yaml#L8">tinygptv_stage4_eval.yaml</a> for Stage 4 version.</p>
<p dir="auto"><strong>4. Update the Phi-2 Modeling for transformers lib.</strong></p>
<p dir="auto">Linux system:</p>
<div data-snippet-clipboard-copy-content="cp modeling_phi.py /root/miniconda3/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/"><pre><code>cp modeling_phi.py /root/miniconda3/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/
</code></pre></div>
<p dir="auto">Windows system</p>
<p dir="auto">Find your conda yourself: conda_sit/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/ Replace modeling_phi.py in that directory with the one in TinyGPT-V/modeling_phi.py.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-launching-demo-locally" aria-hidden="true" tabindex="-1" href="#launching-demo-locally"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Launching Demo Locally</h3>
<p dir="auto">For Stage 4, run</p>
<div data-snippet-clipboard-copy-content="python demo_v2.py --cfg-path eval_configs/tinygptv_stage4_eval.yaml  --gpu-id 0"><pre><code>python demo_v2.py --cfg-path eval_configs/tinygptv_stage4_eval.yaml  --gpu-id 0
</code></pre></div>
<p dir="auto">For Stage 1, 2 and 3, run</p>
<div data-snippet-clipboard-copy-content="python demo.py --cfg-path eval_configs/tinygptv_stage1_2_3_eval.yaml  --gpu-id 0"><pre><code>python demo.py --cfg-path eval_configs/tinygptv_stage1_2_3_eval.yaml  --gpu-id 0
</code></pre></div>
<p dir="auto">To perfer more powerful model, LLMs loads as 16 bit by default. This configuration requires about 8G GPU memory.
To more save GPU memory, you can run the model
in 8 bit below 8G device by setting <code>low_resource</code> to <code>True</code> in the relevant config file:</p>
<ul dir="auto">
<li>
<p dir="auto">Stage 4 <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/eval_configs/tinygptv_stage4_eval.yaml#6">tinygptv_stage4_eval.yaml</a></p>
</li>
<li>
<p dir="auto">Stage 1, 2 and 3 <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/eval_configs/tinygptv_stage1_2_3_eval.yaml#6">tinygptv_stage1_2_3_eval.yaml</a></p>
</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="-Note: Stage 4 is currently a test version as it utilizes partial data for traing. Please use Stage 3 for the demo."><pre><span><span>-</span>Note: Stage 4 is currently a test version as it utilizes partial data for traing. Please use Stage 3 for the demo.</span></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-training" aria-hidden="true" tabindex="-1" href="#training"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Training</h3>
<p dir="auto">First you need to adjust all the updated weights in the LLM to be calculated with full precision:<a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/minigpt4%5Cmodels%5Cbase_model.py">Here</a>. Remove the comments from the following lines:</p>
<div data-snippet-clipboard-copy-content="                layer.self_attn.q_layernorm.weight.data = layer.self_attn.q_layernorm.weight.data.float()
                layer.self_attn.k_layernorm.weight.data = layer.self_attn.k_layernorm.weight.data.float()
                layer.post_layernorm.weight.data = layer.post_layernorm.weight.data.float()
                layer.input_layernorm.weight.data = layer.input_layernorm.weight.data.float()

                # Perform a similar operation for the bias item
                if layer.self_attn.q_layernorm.bias is not None:
                    layer.self_attn.q_layernorm.bias.data = layer.self_attn.q_layernorm.bias.data.float()
                if layer.self_attn.k_layernorm.bias is not None:
                    layer.self_attn.k_layernorm.bias.data = layer.self_attn.k_layernorm.bias.data.float()
                if layer.input_layernorm.bias is not None:
                    layer.input_layernorm.bias.data = layer.input_layernorm.bias.data.float()


            llama_model.model.model.final_layernorm.weight.requires_grad = True
            llama_model.model.model.final_layernorm.weight.data = llama_model.model.model.final_layernorm.weight.data.float()
            if llama_model.model.model.final_layernorm.bias is not None:
                llama_model.model.model.final_layernorm.bias.data = llama_model.model.model.final_layernorm.bias.float()"><pre><code>                layer.self_attn.q_layernorm.weight.data = layer.self_attn.q_layernorm.weight.data.float()
                layer.self_attn.k_layernorm.weight.data = layer.self_attn.k_layernorm.weight.data.float()
                layer.post_layernorm.weight.data = layer.post_layernorm.weight.data.float()
                layer.input_layernorm.weight.data = layer.input_layernorm.weight.data.float()

                # Perform a similar operation for the bias item
                if layer.self_attn.q_layernorm.bias is not None:
                    layer.self_attn.q_layernorm.bias.data = layer.self_attn.q_layernorm.bias.data.float()
                if layer.self_attn.k_layernorm.bias is not None:
                    layer.self_attn.k_layernorm.bias.data = layer.self_attn.k_layernorm.bias.data.float()
                if layer.input_layernorm.bias is not None:
                    layer.input_layernorm.bias.data = layer.input_layernorm.bias.data.float()


            llama_model.model.model.final_layernorm.weight.requires_grad = True
            llama_model.model.model.final_layernorm.weight.data = llama_model.model.model.final_layernorm.weight.data.float()
            if llama_model.model.model.final_layernorm.bias is not None:
                llama_model.model.model.final_layernorm.bias.data = llama_model.model.model.final_layernorm.bias.float()
</code></pre></div>
<p dir="auto"><strong>Stage 1 and 2:</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Datasets: <a href="https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_1_STAGE.md">first stage dataset preparation instruction</a></p>
</li>
<li>
<p dir="auto">Then run:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage1.yaml"><pre><code>torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage1.yaml
</code></pre></div>
<p dir="auto">You need to execute the above code 17 times to complete the first stage of training.</p>
<ul dir="auto">
<li>Then run:</li>
</ul>
<div data-snippet-clipboard-copy-content="torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage2.yaml"><pre><code>torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage2.yaml
</code></pre></div>
<p dir="auto"><strong>Stage 3:</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Datasets: <a href="https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_2_STAGE.md">stage 3 dataset preparation instruction</a></p>
</li>
<li>
<p dir="auto">Then run:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage3.yaml"><pre><code>torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage3.yaml
</code></pre></div>
<p dir="auto"><strong>Stage 4:</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Datasets: <a href="https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_MINIGPTv2_FINETUNE.md">stage 4 dataset preparation instruction</a> Please prepare all datasets except COCO captions and OCR-VQA.</p>
</li>
<li>
<p dir="auto">Then run:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage4.yaml"><pre><code>torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage4.yaml
</code></pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-evaluation" aria-hidden="true" tabindex="-1" href="#evaluation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Evaluation</h3>
<p dir="auto">For eval. details of TinyGPT-V, check <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/eval_scripts/EVAL_README.md">here</a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-star-history" aria-hidden="true" tabindex="-1" href="#star-history"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Star History</h2>
<p dir="auto"><a href="https://star-history.com/#DLYuanGod/TinyGPT-V&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/aa772434fd30f80b20396d361e266a098c65ce887b7045e3a8a703bc17e8bfce/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d444c5975616e476f642f54696e794750542d5626747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=DLYuanGod/TinyGPT-V&amp;type=Date"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-acknowledgement" aria-hidden="true" tabindex="-1" href="#acknowledgement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Acknowledgement</h2>
<ul dir="auto">
<li><a href="https://github.com/Vision-CAIR/MiniGPT-4">MiniGPT</a> A very versatile model of MLLMs.</li>
</ul>
<p dir="auto">If you&#39;re using TinyGPT-V in your research or applications, please cite using this BibTeX:</p>
<div dir="auto" data-snippet-clipboard-copy-content="
@misc{yuan2023tinygptv,
      title={TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones}, 
      author={Zhengqing Yuan and Zhaoxu Li and Lichao Sun},
      year={2023},
      eprint={2312.16862},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre><span>@misc</span>{<span>yuan2023tinygptv</span>,
      <span>title</span>=<span><span>{</span>TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Zhengqing Yuan and Zhaoxu Li and Lichao Sun<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2023<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2312.16862<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>
}</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-license" aria-hidden="true" tabindex="-1" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>License</h2>
<p dir="auto">This repository is under <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/LICENSE.md">BSD 3-Clause License</a>.
Many codes are based on <a href="https://github.com/salesforce/LAVIS">Lavis</a> with
BSD 3-Clause License <a href="https://jvns.ca/DLYuanGod/TinyGPT-V/blob/main/LICENSE_Lavis.md">here</a>.</p>
</article>
          </div></div>
  </body>
</html>
