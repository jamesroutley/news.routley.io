<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://transformerlab.ai/">Original</a>
    <h1>Transformer Lab</h1>
    
    <div id="readability-page-1" class="page"><div id="__docusaurus_skipToContent_fallback"><header><div><h2>100% Open Source Toolkit for Large Language Models: Train, Tune, Eval on your own Machine</h2><p><video width="100%" autoplay="" loop="" muted=""><source src="/assets/medias/TFLDemoMarch25_HQ_2-46e4140ebf8d50a8203529093c46f96d.mp4" type="video/mp4"/>Your browser does not support the video tag.</video></p><div><article><header></header>
<p>Transformer Lab is proud to be supported by Mozilla through the <a href="https://future.mozilla.org/builders/">Mozilla Builders Program</a></p>
<a href="https://future.mozilla.org/builders/"><img src="https://transformerlab.ai/assets/images/mozilla-builders-2024-568f56ecae634d0635c6c76e4615baa4.png" alt="Example banner"/></a>

<p>Transformer Lab is an open source platform that allows anyone to build, tune, &amp; run Large Language Models locally, without writing code.</p>
<p>We imagine a world where every software developer will incorporate large language models in their products. Transformer Lab allows users to do this without needing to know Python nor have previous experience with machine learning.</p>
<p><a href="https://transformerlab.ai/about">Learn more about our vision</a></p>
<h2 id="detailed-feature-list">Detailed Feature List</h2>
<ul>
<li>ğŸ’• One-click Download Hundreds of Popular Models:<!-- -->
<ul>
<li>Llama3, Phi3, Mistral, Mixtral, Gemma, Command-R, and dozens more</li>
</ul>
</li>
<li>â¬‡ Download any LLM from Huggingface</li>
<li>ğŸ¶ Finetune / Train Across Different Hardware<!-- -->
<ul>
<li>Finetune using MLX on Apple Silicon</li>
<li>Finetune using Huggingface on GPU</li>
</ul>
</li>
<li>âš–ï¸ RLHF and Preference Optimization<!-- -->
<ul>
<li>DPO</li>
<li>ORPO</li>
<li>SIMPO</li>
<li>Reward Modeling</li>
</ul>
</li>
<li>ğŸ§ GRPO Training</li>
<li>ğŸ’» Work with LLMs Across Operating Systems:<!-- -->
<ul>
<li>Windows App</li>
<li>MacOS App</li>
<li>Linux</li>
</ul>
</li>
<li>ğŸ’¬ Chat with Models<!-- -->
<ul>
<li>Chat</li>
<li>Completions</li>
<li>Preset (Templated) Prompts<!-- -->
<ul>
<li>Chat History</li>
<li>Tweak generation parameters</li>
</ul>
</li>
<li>Batch Inference</li>
</ul>
</li>
<li>ğŸš‚ Use Different Inference Engines<!-- -->
<ul>
<li>MLX on Apple Silicon</li>
<li>Huggingface Transformers</li>
<li>vLLM</li>
<li>Llama CPP</li>
</ul>
</li>
<li>ğŸ§‘â€ğŸ“ Evaluate models<!-- -->
<ul>
<li>Eleuther Harness</li>
<li>LLM as a Judge</li>
<li>Objective Metrics</li>
<li>Red Teaming Evals</li>
<li>Eval visualization and graphing</li>
</ul>
</li>
<li>ğŸ“– RAG (Retreival Augmented Generation)<!-- -->
<ul>
<li>Drag and Drop File UI</li>
<li>Works on Apple MLX, Transformers, and other engines</li>
</ul>
</li>
<li>ğŸ““ Build Datasets for Training<!-- -->
<ul>
<li>Pull from hundreds of common datasets available on HuggingFace</li>
<li>Provide your own dataset using drag and drop</li>
</ul>
</li>
<li>ğŸ”¢ Calculate Embeddings</li>
<li>ğŸ’ Full REST API</li>
<li>ğŸŒ© Run in the Cloud<!-- -->
<ul>
<li>You can run the user interface on your desktop/laptop while the engine runs on a remote or cloud machine</li>
<li>Or you can run everything locally on a single machine</li>
</ul>
</li>
<li>ğŸ”€ Convert Models Across Platforms<!-- -->
<ul>
<li>Convert from/to Huggingface, MLX, GGUF</li>
</ul>
</li>
<li>ğŸ”Œ Plugin Support<!-- -->
<ul>
<li>Easily pull from a library of existing plugins</li>
<li>Write your own plugins to extend functionality</li>
</ul>
</li>
<li>ğŸ“ Prompt Editing<!-- -->
<ul>
<li>Easily edit System Messages or Prompt Templates</li>
</ul>
</li>
<li>ğŸ“œ Inference Logs<!-- -->
<ul>
<li>While doing inference or RAG, view a log of the raw queries sent to the LLM</li>
</ul>
</li>
</ul></article></div></div></header><main></main></div></div>
  </body>
</html>
