<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://toon3d.studio/">Original</a>
    <h1>Toon3D: Seeing cartoons from a new perspective</h1>
    
    <div id="readability-page-1" class="page">

  <nav role="navigation" aria-label="main navigation">
    
  </nav>


  <section>
    <div>
      <div>
        <div>
          <div>
            
            

            <p><span>* Equal contribution,</span>
              <span><sup>1</sup><a href="https://www.teton.ai/">Teton.ai</a>,</span>
              <span><sup>2</sup>UC Berkeley</span>
            </p>

            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div>
      <div>
        <h2>
          <b>TLDR</b></h2>
        <p><img src="https://niccrane.com/posts/rc/static/images/teaser.png"/>
      </p></div>
    </div>
  </section>


  <section>
    <div>
      <div>
        <div>
          <p>
            <!-- We run <span class="my-method-name">Toon3D</span> on 12 popular cartoon scenes. -->
            <!-- We
            align backprojected monocular depth predictions with a piecewise-rigid deformation at 3D keypoints. We refine
            the aligned
            point clouds with Gaussian optimization. These scenes have sparse viewpoints as input,
            ranging from only 4 to as many as 12 hand-drawn images. Press the button to move the cameras between two
            viewpoints! -->
            Hand-drawn scenes are not 3D consistent, so we create <span>Toon3D</span> to recover
            camera poses and dense geometry! We do this with a piecewise-rigid deformation optimization at hand-labeled
            keypoints and using monocular depth as a prior. Now we can interpolate novel views never before seen! Press
            the button to move the cameras between two
            viewpoints! <i>Note that we reconstruct the scenes with more than two hand-drawn images, but this demo shows
              a smooth transition between just two of the inputs views.</i>
          </p>
          </div>
        
      </div>
    </div>
  </section>
  


  <section>
    <div>
      <!-- Abstract. -->
      <div>
        <div>
          <h2>Abstract</h2>
          <p>
              We propose <span>Toon3D</span>. In this work, we recover the underlying 3D
              structure of non-geometrically consistent scenes. We focus our analysis on hand-drawn images from cartoons
              and anime. Many cartoons are created by artists without a 3D rendering engine, which means that any new
              image of a scene is hand-drawn. The hand-drawn images are usually faithful representations of the world,
              but only in a qualitative sense, since it is difficult for humans to draw multiple perspectives of an
              object or scene 3D consistently. Nevertheless, people can easily perceive 3D scenes from inconsistent
              inputs! In this work, we correct for 2D drawing inconsistencies to recover a plausible 3D structure such
              that the newly warped drawings are consistent with each other. Our pipeline consists of a user-friendly
              annotation tool, camera pose estimation, and image deformation to recover a dense structure. Our method
              warps images to obey a perspective camera model, enabling our aligned results to be plugged into
              novel-view synthesis reconstruction methods to experience cartoons from viewpoints never drawn before.
            </p>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section>
    <div>
      <!-- Abstract. -->
      <div>
        <div>
          <h2>Cartoon Reconstruction</h2>
          <p>
              (Left) We first recover camera poses and aligned point clouds. (Right) Then we initialize Gaussians from
              our dense point
              cloud and optimize Gaussian Splatting with the recovered
              cameras. Our method has depth regularization and is built on <a href="https://docs.nerf.studio/">Nerfstudio</a>. Here we show fly-through renders of our scenes.
          </p>
          <video poster="" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
            <source src="./static/assets/bobs-burgers-side-by-side.mp4" type="video/mp4"/>
          </video>
        </div>
      </div>
      <div>
        <div>
          <p>Here is the gallery of all our scenes. Can you guess which is which? Click to reveal names.</p>
          </div>
        
      </div>
    </div>
    

  </section>

  <section>
    <div>
      <div>
        <div>
          <h2>Method</h2>
          <p>
              We first predict the depth of each image with <a href="https://github.com/prs-eth/Marigold">Marigold</a>
              and obtain candidate transient masks with <a href="https://segment-anything.com/">SAM</a>. We then label
              images with the <span>Toon3D
                Labeler</span> to obtain correspondences and mark transient regions. We optimize camera poses and warp
              images to obtain calibrated, perspective cameras. Finally, we can initialize Gaussians with the aligned
              dense point cloud and run refinement.
            </p>
        </div>
      </div>
      <div>
        <div>
          <h4>Overview</h4>
          <div>
            <p><img src="https://niccrane.com/posts/rc/static/images/method.png"/>
            </p>
          </div>
        </div>
        
      </div>
      <p>Here you can see the two major steps of our method. The sparse alignment video shows rough camera parameter
          estimation. The dense alignment video shows various layers used in the method (e.g., cameras, sparse
          correspondences, warping meshes, etc.) and how they align in 3D.</p>
      <!-- Columns container -->
      
    </div>
  </section>


  <section>
    <div>
      <div>
        <div>
          <h2>Explore Inside Rick and Morty&#39;s House</h2>
          <p>We reconstruct inside the Rick and Morty house by labeling between walls and ceilings to connect the
              rooms. In the first video, we show the point cloud &amp; cameras and our custom labeling interface. In the
              second video, you can scrub the slider to
              see a walkthrough inside the house! The closest camera&#39;s image is shown in the bottom right corner.</p>
          <!-- Columns container -->
          
        </div>
      </div>
    </div>

    

  </section>

  <section>
    <div>
      <div>
        <div>
          <h2>Point Clouds and Cameras</h2>
          <p>
              Here we show point clouds and recovered cameras for the 12 cartoon scenes in the <span>Toon3D Dataset</span>. Click the icons to explore our scenes!
            </p>
        </div>
      </div>
      <div>
        <div>
          <div>

            <p>
              Click a scene icon to start!
            </p>

            
            

            

            

          </div>
        </div>
      </div>
    </div>
  </section>


  <section>
    <div>
      <div>
        <div>
          <h2>Sparse-View Reconstruction</h2>
          <p>
              We can reconstruct scenes from few images and with large
              viewpoint changes. Where COLMAP may fail, we can intervene with the <span>Toon3D
                Labeler</span> to obtain human-labeled correspondences. Here we show a
              fly-through rendering for two rooms (&#34;Living room&#34; and &#34;Bedroom 2&#34;) of <a href="https://www.airbnb.com/rooms/833261990707199349">this Airbnb listing</a>.
            </p>
        </div>
      </div>
      
    </div>
  </section>

  <section>
    <div>
      <div>
        <div>
          <h2>Visualizing Inconsistencies</h2>
          <p>
              Cartoons are hand-drawn so we need to warp the images to be 3D consistent. The first item is a video that
              shows the warp taking place during alignment optimization.
              The next two items are images which show the original and warped drawings, as well as the overlap between
              the two. Blurry
              regions indicate where a lot of warp occured.
            </p>
        </div>
      </div>
      <div>
        <div>
          <div id="blender-carousel">
            <p>
              <h4>Video</h4>
              <video poster="" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
                <source src="./static/assets/visualizing-inconsistencies-3.mp4" type="video/mp4"/>
              </video>
            </p>
            <div>
              <h4>Image</h4>
              <p><img src="https://niccrane.com/posts/rc/static/assets/visualizing-inconsistencies-1.png"/>
            </p></div>
            <div>
              <h4>Image</h4>
              <p><img src="https://niccrane.com/posts/rc/static/assets/visualizing-inconsistencies-2.png"/>
            </p></div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Multi-View Consistent Inpainting. -->
  <section>
    <div>
      <!-- Abstract. -->
      <div>
        <div>
          <h2>Reconstructing Paintings</h2>
          <p>
              We can reconstruct paintings with <span>Toon3D</span> even though the paintings are
              hand-drawn. We predict the depth of each image, then align and warp point clouds. Finally we use Gaussian
              refinement to create the video shown below.
            </p>
        </div>
      </div>
      
    </div>
  </section>
  <!--/ Multi-View Consistent Inpainting. -->



  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <p>Please consider citing our work if you find it useful.</p>
      <pre><code>@inproceedings{weber2023toon3d,
  title = {Toon3D: Seeing Cartoons from a New Perspective},
  author = {Ethan Weber* and Riley Peterlinz* and Rohan Mathur and
    Frederik Warburg and Alexei A. Efros and Angjoo Kanazawa},
  booktitle = {arXiv},
  year = {2024},
}
</code></pre>
    </div>
  </section>

  <section>
    <div>

      <!-- Concurrent Work. -->
      <div>
        <div>
          <!-- <h2 class="title is-3">Acknowledgements</h2> -->

          <p>
              We would like to thank Qianqian Wang, Justin Kerr, Brent Yi, David McAllister, Matthew Tancik, Evonne Ng,
              Anjali Thakrar, Christian Foley,
              Abhishek Kar, Georgios Pavlakos, the Nerfstudio team, and the KAIR lab for discussions, feedback, and
              technical support. We also thank Ian Mitchell and Roland Jose for helping to label points.
            </p>
        </div>
      </div>
      <!--/ Concurrent Work. -->

    </div>
  </section>


  



</div>
  </body>
</html>
