<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://unum.cloud/post/2021-12-21-macbook/">Original</a>
    <h1>Apple to Apple Comparison: M1 Max vs. Intel</h1>
    
    <div id="readability-page-1" class="page"><div>
      <p>This will be a story about many things: about computers, about their (memory) speed limits, about workloads that can push computers to those limits and the subtle differences in <a href="https://en.wikipedia.org/wiki/Hash_table">Hash-Tables</a> (HT) designs.
But before we get in, here is a glimpse of what we are about to see.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-21-macbook/report_google.svg" alt="Google HashMaps"/></p>
<p>A friendly warning, the following article contains many technical terms and is intended for somewhat technical and hopefully curious readers.</p>
<hr/>
<p>A couple of months ago, I followed one of my oldest traditions, watching Apple present their new products at one of its regular conferences.
I have done it since the iPhone 4 days and haven‚Äôt missed a single one.
Needless to say, the last ten years were a rodeo.
With hours spent watching new animated <del>poop</del> emojis on iOS presentations, I would sometimes lose hope in that üí≤3T company.
After the 2012 MacBook Pro, every following Mac I bought was a downgrade.
The 2015 model, the 2017 and even the Intel Core i9 16&#34; 2019 version, if we consider performance per buck.
Until now.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-21-macbook/Apple-M1-Max-Overview.jpg" alt="Apple M1 Max Specs"/></p>
<p>The M1 Pro/Max seem vastly different from the M1 in the last years MacBook Airs. Apple hates sharing technical details, but here are <a href="https://www.apple.com/newsroom/2021/10/introducing-m1-pro-and-m1-max-the-most-powerful-chips-apple-has-ever-built/">some of the core aspects</a> we know:</p>
<ol>
<li>CPU has 8x power cores and 2x efficiency cores.</li>
<li>Cores have massive L2 blocks for a total of 28 MB of L2.</li>
<li>Printing was done via the 5N TSMC lithography standard.</li>
<li>Memory bus was upgraded to LPDDR5 and claims up to 400 GB/s of bandwidth.</li>
</ol>
<p>We have recently published a <a href="https://unum.cloud/post/2021-12-07-supercycle/">2736-word overview</a> of upcoming memory-bus innovations, so the last one looks exceptionally interesting. In the ‚Äúvon Neumann computer architecture‚Äù today, the biggest bottleneck is between memory and compute. It‚Äôs especially evident in big-data processing and neuromorphic workloads. Both are essential to us, so we invested some time and benchmarked memory-intensive applications on three machines:</p>
<ol>
<li>16&#34; Apple MacBook Pro with an 8-core <a href="https://ark.intel.com/content/www/us/en/ark/products/192990/intel-core-i99980hk-processor-16m-cache-up-to-5-00-ghz.html">Intel Core i9-9980HK</a> and 16 GB of DDR4 memory, purchased for around üí≤3&#39;000 in 2019.</li>
<li>16&#34; Apple MacBook Pro with a 10-core ARM-based <a href="https://www.apple.com/newsroom/2021/10/introducing-m1-pro-and-m1-max-the-most-powerful-chips-apple-has-ever-built/">M1 Max</a> CPU with 64 GB of LPDDR5 memory, purchased for around üí≤3&#39;000 in 2021.</li>
<li>A custom-built liquid-cooled server with an <a href="https://www.amd.com/en/products/cpu/amd-ryzen-threadripper-pro-3995wx">AMD Threadripper Pro 3995WX</a> with 1 TB of eight-channel DDR4 memory, purchased for over üí≤50&#39;000 in 2021.</li>
</ol>
<p>Sounds intriguing? Let‚Äôs jump in!</p>
<h2 id="copying-memory">Copying Memory</h2>
<p>Generally, when companies claim a number like 100 GB/s, they mean the combined theoretical throughput of all the RAM channels in the most basic possible workload - sequentially streaming a lot of data.
Probably the only such use case on a desktop is watching multiple 4K movies at once.
Your multimedia player will fetch a ton of frames from memory and put them into the current frame buffer, overwriting the previous content.
In the x86 world, you would achieve something like that by combining <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_stream_load_si256"><code>_mm256_stream_load_si256</code></a> and <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_stream_si256"><code>_mm256_stream_si256</code></a> intrinsics.
Or if you prefer something higher-level, there is a <a href="https://man7.org/linux/man-pages/man3/memcpy.3.html"><code>memcpy</code></a> in every <code>libc</code> distribution.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-21-macbook/report_memcpy.svg" alt="Memcpy"/></p>
<p>In this benchmark, we generated a big buffer with lots of synthetic random data in it, and then every core would fetch chunks of it in random order.
As we see, there is much non-uniformity in the results.
The evaluation speed may differ significantly depending on the copied chunk size (1 KB, 4 KB, 1 MB or 4 MB).
The expectation is that copies below 4 KB <a href="http://www.cs.rpi.edu/academics/courses/fall04/os/c12/">standard page size</a> won‚Äôt be efficient.
As you make chunks bigger, the numbers would generally increase until reaching about 200 GB/s in copied data on the newest hardware.
All of that bandwidth, of course, rarely affects real life.</p>
<ul>
<li>If you are watching videos - you are likely decoding ‚Üí bottlenecking on CPUs ALUs.</li>
<li>If you handle less than 20 MB at a time ‚Üí you may never be leaving CPUs L1/L2 caches.</li>
</ul>
<p>Is there a memory-intensive benchmark that can translate into real-world performance?</p>
<h2 id="hash-tables">Hash-Tables</h2>
<p>Meet <a href="https://en.wikipedia.org/wiki/Hash_table">Hash-Tables</a> (HT) - probably the most straightforward yet most relevant data structure in all computing.
Every CS freshman can implement it, but even the biggest software companies in the world spend years optimizing and polishing them.
Hell, you can build a <a href="https://redis.com/press/redis-labs-110-million-series-g-led-by-tiger-global/">üí≤2B software</a> startup like <a href="https://redis.com">Redis</a> on a good hash-table implementation.
That‚Äôs how essential HTs are!
In reality, HT is just key-value store imeplemented as a bunch of memory pockets and an arbitrary mathematical function that routes every key to a bucket in which that data belongs.
The problem here, is that generally a tiny change in input data results in a jump to a different memory region.
Once it happens, you can‚Äôt fetch the data from the CPU caches and have to go all the way to RAM!
So our logic is the following:</p>
<ul>
<li>Fast memory ‚Üí Fast Hash-Tables.</li>
<li>Fast Hash-Tables ‚Üí Faster Applications.</li>
<li>Faster Applications ‚Üí Rainbows üåà, Unicorns ü¶Ñ and Profit ü§ë!</li>
</ul>
<p>With this unquestionable reasoning in mind, let‚Äôs compare some hash-tables on Apple‚Äôs silicon!</p>
<h2 id="c-standard-templates">C++ Standard Templates</h2>
<p>The most popular high-performance computing language is C++, and it comes with a standard library.
A standard library with many zero-cost abstractions, including some of the fastest general-purpose data structures shipped in any programming ecosystem.
It‚Äôs the most common HT, the people will be using, but not the fastest.</p>
<p>We have configured it in all power-of-two sizes between 16 MB and 256 MB of content.
Such a broad range is selected to showcase speeds both within and outside of CPU caches of chosen machines.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-21-macbook/report_std.svg" alt="STL HashMaps"/></p>
<p>Wow! What we see here, is that ARM-powered MacBooks end-up fetching about 6 GB/s worth of data. That of course, is nowhere close to the 100 GB/s in bulk-copy-workloads or 400 GB/s on the slide, but remarkably close to a huge server and way ahead of the last gen Intel chip!</p>
<p>Implementation-wise, <a href="https://stackoverflow.com/a/31113618">STL <code>std::unordered_map</code></a> is an array of buckets, where every bucket is a <a href="https://en.wikipedia.org/wiki/Linked_list">linked list</a>.
Every node of those linked lists is allocated in a new place on the heap in the default configuration.
Once we get a query, we hash the key, determine the bucket, pick the head of the list and compare our query to every object in the list.
This already must sound wasteful in the number of entries we will traverse.</p>
<p>Then, a linked list in each bucket would require storing at least one extra eight-byte pointer per added entry.
If you are mapping integers to integers, you will be wasting at least half of the space in the ideal case.
Continuing the topic of memory efficiency, allocating nodes on-heap means having even more metadata about your HT stored in another tree data structure.
Let‚Äôs take the second most popular solution.</p>
<h2 id="google-hash-maps">Google Hash Maps</h2>
<p>Googles <a href="https://github.com/sparsehash/sparsehash">SparseHash</a> library as over ‚≠ê 1K on GitHub!
They have a sparse and a dense <code>google::dense_hash_map</code> HT, the latter one being generally better across the board.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-21-macbook/report_google.svg" alt="Google HashMaps"/></p>
<p>Googles design is based on the idea of open-addressing.
Instead of nesting linked-lists in every bucket, they keep only one plain memory buffer.
If a slot is busy, a linear <a href="https://en.wikipedia.org/wiki/Linear_probing">probing procedure</a> is called to find an alternative.
This methodology has a noticeable disadvantage.
Previously, with <code>std::unordered_map</code> we could allocate one entry at a time, making memory consumption higher but more predictable.
Here, we have a single big arena, and when it‚Äôs 70% populated, we generally create another one, double the size, and migrate the data in bulk.
Sometimes, for <code>7x</code> entries, we will use <code>30x</code> worth of memory over four times what we expected.</p>
<p>Furthermore, those bulk migrations don‚Äôt happen momentarily and will drastically increase the latency on each growth.
This HT performs better on average but lowers predictability and causes latency spikes.</p>
<p>Another disadvantage with this library is defining an ‚Äúempty key‚Äù.
If your integer keys are only 4 bytes in size, this means reserving and avoiding 1 in 4&#39;000&#39;000&#39;000 potential values.
A small functional sacrifice in most cases, but not always.</p>
<h2 id="tsl-library">TSL Library</h2>
<p>If you feel a little less mainstream and curious to try other things, <code>tsl::</code> is another interesting namespace to explore.
Its best solution also relies on open addressing but with a different mechanism of probing and deletion.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-21-macbook/report_tsl.svg" alt="TSL HashMaps"/></p>
<p>Best of all, the author <a href="https://tessil.github.io/2016/08/29/benchmark-hopscotch-map.html">provides numerous benchmarks</a>, comparing his <code>tsl::robin_map</code> to some of the other established names.</p>
<hr/>
<p>At Unum, however, we don‚Äôt use any third party HT variants.
In this article, we only benchmarked a single operation - a lookup.
A broader report would include: insertions, deletions and range-constructions, to name a few and would highlight broader tradeoffs between different pre-existing solutions.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-21-macbook/report_containers.svg" alt="Different workloads on all big size hish-tables"/></p>
<p>When those benchmarks are running, we disable multithreading, pin cores to the process, start a separate sub-process that queries the system on both software and hardware events, like total reserved memory volume and the number of cache misses.
Those benchmarks run on Intel, AMD and ARM CPU design, on machines ranging from 3 W to 3 kW power-consumption, running Linux and sometimes even MacOS, as we saw today.</p>
<p><img src="https://unum.cloud/assets/post/2021-12-21-macbook/Apple_M1-Pro-M1-Max_CPU-Performance_10182021.jpg" alt="M1 Performance Per Watt"/></p>
<p>In years of High-Performance Computing Research, you learn one thing - there are no absolute benchmarks.
Noise and fluctuations are always present in the results.
Still, with gaps this obvious, we are genuinely excited about Apple‚Äôs new laptops and can‚Äôt wait for the <a href="https://unum.cloud/post/2021-12-21-macbook/post/2021-12-07-supercycle/">next generation of servers</a> to adopt DDR5 technology!</p>
<hr/>
<blockquote>
<p>All the benchmarks can be reproduced with code available on <a href="https://github.com/unum-cloud/HashTableBenchmark">our GitHub page</a>.</p>
</blockquote>
<blockquote>
<p>After years of R&amp;D and with in-house alternatives for almost every piece of modern software, we have a lot more to share, so <a href="http://eepurl.com/hNLhXb">subscribe to our newsletter</a>!</p>
</blockquote>

    </div></div>
  </body>
</html>
