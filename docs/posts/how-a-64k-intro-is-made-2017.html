<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.lofibucket.com/articles/64k_intro.html">Original</a>
    <h1>How a 64k intro is made (2017)</h1>
    
    <div id="readability-page-1" class="page">
<p><a href="http://www.lofibucket.com"> back to lofibucket.com </a></p>
<p id="header">

<h2>Pekka Väänänen</h2>
<h3>May 21st, 2017</h3>
</p>

<h2 id="an-intro-to-intros">An intro to intros</h2>
<p>The demoscene is about producing cool real time works (as in “runs on your computer”) called <em>demos</em>. Some demos are really small, say 64 kilobytes or less, and these are called <em>intros</em>. The name comes from “crack intros”. So an intro is just a demo that’s small.</p>
<p>I’ve noticed many people have interest in demoscene productions but have no idea how they are actually made. This is a braindump/post-mortem of our recent 64k intro <em>Guberniya</em> and I hope that it will be interesting to newcomers and seasoned veterans alike. This article touches basically all techniques used in the demo and should give you an idea what goes into making one. I refer to people with their nick names in this article because that’s what <em>sceners</em> do.</p>
<h2 id="guberniya-in-a-nutshell">Guberniya in a nutshell</h2>
<iframe width="850" height="480" src="https://www.youtube.com/embed/yei3mJm33SQ" frameborder="0" allowfullscreen="">
</iframe>
<p>Windows binary download: <a href="ftp://untergrund.net/users/peisik/guberniya_final.zip">guberniya_final.zip (61.8 kB)</a> (somewhat broken on AMD cards)</p>
<p>It’s a 64k intro released at <a href="https://2017.revision-party.net/">Revision 2017 demo party</a>. Some numbers:</p>
<ul>
<li>C++ and OpenGL, <a href="https://github.com/ocornut/imgui">dear imgui</a> for GUI</li>
<li>62976 byte Windows executable, packed with <a href="http://www.farbrausch.de/~fg/kkrunchy/">kkrunchy</a></li>
<li>mostly raymarching</li>
<li>6 person team
<ul>
<li>one artist :)</li>
</ul></li>
<li>built in four months</li>
<li>~8300 lines of C++, library code and whitespace excluded</li>
<li>4840 lines of GLSL shaders</li>
<li>~350 git commits</li>
</ul>
<h2 id="development">Development</h2>
<p>Demos are usually released at a <em>demo party</em> where the audience watches demos submitted to a competition and then votes for the winner. Releasing at a party is a great way to get motivated since you have a hard deadline and an eager audience. In our case it was <a href="https://2017.revision-party.net/">Revision 2017</a>, a big traditional demo party during the Easter weekend. You can view <a href="http://tf.weimarnetz.de/revision2017/">some photos</a> to get an idea what the event is like.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/commits.png" alt="The number of commits per week. That big spike is us hacking away right before the deadline. The last two bars are commits for the final version released after the party."/></p><p>The number of commits per week. That big spike is us hacking away right before the deadline. The last two bars are commits for the final version released after the party.</p>
</div>
<p>We started working on the demo early in January and released it on the Easter weekend in April during the party. You can watch <a href="https://www.youtube.com/watch?v=1jv0woZONvc">a recording of the whole competition</a> online if you wish :)</p>
<p>We were a team of six: cce (me), <a href="https://twitter.com/martin_b_radev">varko</a>, <a href="https://twitter.com/nnnnoby">noby</a>, <a href="https://twitter.com/nicebranch">branch</a>, msqrt, and goatman.</p>
<h2 id="design-influences">Design &amp; influences</h2>
<p>The song was done pretty early on, so I tried to design things around it. It was clear we needed something big and cinematic with memorable set pieces.</p>
<p>My original visual ideas centered around wires and their usage. I really liked Viktor Antonov’s designs and my first sketches were pretty much a rip-off of Half-Life 2:</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/sketch2_thumb.jpg" alt="Early sketches of citadel towers and ambitious human characters. Full size."/></p><p>Early sketches of citadel towers and ambitious human characters. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/sketch2.jpg">Full size.</a></p>
</div>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/Citadel_walls.jpg" alt="Viktor Antonov’s concept art in Half-Life 2: Raising the Bar."/></p><p>Viktor Antonov’s concept art in <em>Half-Life 2: Raising the Bar</em>.</p>
</div>
<p>The similarities are quite obvious. In the landscape scenes I was also trying to capture the mood of <a href="http://anthonyscime.deviantart.com/art/Eldion-Passageway-304528786">Eldion Passageway</a> by Anthony Scimes.</p>
<p>The landscape was inspired by <a href="https://vimeo.com/189904045">this nice video of Iceland</a> and also Koyaanisqatsi, I guess. I also had big plans for the story that manifested itself as a storyboard:</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/story1.jpg" alt="The storyboard differs from the final intro. For example the brutalist architecture was dropped. The full storyboard."/></p><p>The storyboard differs from the final intro. For example the brutalist architecture was dropped. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/story12.jpg">The full storyboard.</a></p>
</div>
<p>If I’d do this again I’d just settle with a timeline with a couple of photos that set the mood. It’s less work and leaves more room for imagination. But at least drawing it forced me to organize my thoughts.</p>
<h3 id="the-ship">The Ship</h3>
<p>The spaceship was designed by <a href="https://twitter.com/nnnnoby">noby</a>. It is a combination of multiple Mandelbox fractals intersected with geometric primitives. The ship’s design was left a bit incomplete, but we felt it shouldn’t be further tampered with in the final version.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/ship.jpg" alt="The spaceship is a raymarched distance field, just like everything else."/></p><p>The spaceship is a raymarched distance field, just like everything else.</p>
</div>
<p>We had also another ship shader that didn’t get used. Now that I look at the design it’s also very cool and it’s a shame it didn’t find use in the intro.</p>

<h2 id="implementation">Implementation</h2>
<p>We started with a codebase built for our earlier intro <a href="http://www.pouet.net/prod.php?which=67435">Pheromone</a> (<a href="https://www.youtube.com/watch?v=BTZs8ppUtSQ">YouTube</a>). It had basic windowing and OpenGL boilerplate along with file system utility that packed files from a data directory to executable with <code>bin2h</code>.</p>
<h3 id="the-workflow">The workflow</h3>
<p>We used Visual Studio 2013 to compile the project since it wouldn’t compile on VS2015. Our standard library replacement didn’t work well with the updated compiler and produced amusing errors like this:</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/linkkeri.png" alt="Visual Studio 2015 didn’t play well with our codebase."/></p><p>Visual Studio 2015 didn’t play well with our codebase.</p>
</div>
<p>For some reason we stuck with VS2015 as an editor though and just compiled the project using the v120 platform toolkit.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/vs.jpg" alt="I made most of my work with the demo like this: shaders open in one window and the end result with console output in others. Full size."/></p><p>I made most of my work with the demo like this: shaders open in one window and the end result with console output in others. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/vs.png">Full size.</a></p>
</div>
<p>We had a simple global keyboard hook that reloaded all shaders when CTRL+S key combination was detected:</p>
<div><pre><code><span>// Listen to CTRL+S.</span>
<span>if</span> (GetAsyncKeyState(VK_CONTROL) &amp;&amp; GetAsyncKeyState(&#39;S&#39;))
{
    <span>// Wait for a while to let the file system finish the file write.</span>
    <span>if</span> (system_get_millis() - last_load &gt; <span>200</span>) {
        Sleep(<span>100</span>);
        reloadShaders();
    }
    last_load = system_get_millis();
}</code></pre></div>
<p>This worked really well and made live editing shaders much more fun. No need to have file system hooks or anything.</p>
<h3 id="gnu-rocket">GNU Rocket</h3>
<p>For animation and direction we used a <a href="https://github.com/rocket/rocket">GNU Rocket</a> fork <a href="https://github.com/edoreshef/ground-control">Ground Control</a>. Rocket is a program for editing animation curves and it connects to the demo via a TCP socket. The keyframes are sent over when requested by the demo. It’s very convenient because you can edit and recompile the demo while keeping the editor open without losing the sync position. For the final release the keyframes are exported to a binary format. It has some <a href="https://github.com/rocket/rocket/issues/63">annoying limitations</a> though.</p>
<h3 id="the-tool">The Tool</h3>
<video width="800" src="img/64k/tool.mp4" controls="control">
</video>
<p><em>Moving the viewpoint with mouse and keyboard is very handy for picking camera angles. Even a simple GUI helps a lot when tweaking values.</em></p>
<p>We didn’t have a demotool <a href="http://peisik.untergrund.net/engines/">unlike some people</a> so we had to build it as we went a long. The excellent <a href="https://github.com/ocornut/imgui">dear imgui</a> library allowed us to easily add features as we needed them.</p>
<p>For example adding some sliders to control some bloom parameters is as simple as adding these lines inside the rendering loop (<em>not</em> to separate GUI code):</p>
<div><pre><code>    imgui::Begin(<span>&#34;Postprocessing&#34;</span>);
    imgui::SliderFloat(<span>&#34;Bloom blur&#34;</span>, &amp;postproc_bloom_blur_steps, <span>1</span>, <span>5</span>);
    imgui::SliderFloat(<span>&#34;Luminance&#34;</span>, &amp;postproc_luminance, <span>0.0</span>, <span>1.0</span>, <span>&#34;</span><span>%.3f</span><span>&#34;</span>, <span>1.0</span>);
    imgui::SliderFloat(<span>&#34;Threshold&#34;</span>, &amp;postproc_threshold, <span>0.0</span>, <span>1.0</span>, <span>&#34;</span><span>%.3f</span><span>&#34;</span>, <span>3.0</span>);
    imgui::End();</code></pre></div>
<p>The end result:</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/imgui.png" alt="These sliders were easy to add."/></p><p>These sliders were easy to add.</p>
</div>
<p>The camera position can be saved by pressing <code>F6</code> to a <code>.cpp</code> file, so the next time the code is compiled it will be included. This avoids the need for a separate data format and the related serialization code, but this solution can also get pretty messy.</p>
<h3 id="making-small-binaries">Making small binaries</h3>
<p>The key to small executables is scrapping the default standard library and compressing the compiled binary. We used Mike_V’s <a href="https://www.codeproject.com/Articles/15156/Tiny-C-Runtime-Library">Tiny C Runtime Library</a> as a base for our own library implementation.</p>
<p>The binaries are compressed with <a href="http://www.farbrausch.de/~fg/kkrunchy/">kkrunchy</a>, which is a tool made for exactly this purpose. It operates on standalone executables so you can write your demo in C++, Rust, Object Pascal or whatever. To be honest, size wasn’t really a problem for us. We didn’t store much binary data like images so we had plenty of room to play with. We didn’t even remove comments from shaders!</p>
<h3 id="floating-points">Floating points</h3>
<p>Floating point code caused some headaches by producing calls to nonexistent standard library functions. Most of these were eliminated by disabling SSE vectorization with the <code>/arch:IA32</code> compiler switch and removing calls to <code>ftol</code> with the <code>/QIfst</code> flag that generates code that doesn’t save the FPU truncation mode flags. This is not a problem because you can set the floating point truncation mode at the start of your program with this snippet courtesy of <a href="http://www.musicdsp.org/showone.php?id=246">Peter Schoffhauzer</a>:</p>
<div><pre><code><span>// set rounding mode to truncate</span>
<span>//  from http://www.musicdsp.org/showone.php?id=246</span>
<span>static</span> <span>short</span> control_word;
<span>static</span> <span>short</span> control_word2;

<span>inline</span> <span>void</span> SetFloatingPointRoundingToTruncate()
{
    __asm
    {
        fstcw   control_word                <span>// store fpu control word</span>
        mov     dx, word ptr [control_word]
        or      dx, <span>0x0C00</span>                  <span>// rounding: truncate</span>
        mov     control_word2, dx
        fldcw   control_word2               <span>// load modfied control word</span>
    }
}</code></pre></div>
<p>You can read more about these things at <a href="http://www.benshoof.org/blog/minicrt/">benshoof.org</a>.</p>
<h4 id="pow">POW</h4>
<p>Calling <code>pow</code> still generated a call to <code>__CIpow</code> intrinsic function that didn’t exist. I couldn’t figure out its signature on my own but I found an implementation in <a href="https://github.com/wine-mirror/wine/blob/77887b474e60d21157cc1c59784b2b4c6c42dedf/dlls/ntdll/misc.c#L218">Wine’s <code>ntdll.dll</code></a> that revealed that it expects two double precision floats in registers. Now it was possible to make a wrapper that calls our own <code>pow</code> implementation:</p>
<div><pre><code><span>double</span> __cdecl _CIpow(<span>void</span>) {
    <span>// Load the values from registers to local variables.</span>
    <span>double</span> b, p;
    __asm {
        fstp qword ptr p
        fstp qword ptr b
    }

    <span>// Implementation: http://www.mindspring.com/~pfilandr/C/fs_math/fs_math.c</span>
    <span>return</span> fs_pow(b, p);
}</code></pre></div>
<p>If you know a nicer way to fix this, please let me know.</p>
<h3 id="winapi">WinAPI</h3>
<p>When you can’t depend on SDL or similar you need to use plain WinAPI to do the necessary plumbing to get a window on screen. If you are suffering through this, these might prove helpful:</p>
<ul>
<li><a href="https://gist.github.com/seece/1f67da14d69b9c8a75a7e6839abf8e72">WinAPI window creation example</a></li>
<li><a href="https://gist.github.com/seece/9f5f3069130c4fe642f4fd5e7375816a">OpenGL initialization example</a>, requires <a href="https://www.khronos.org/registry/OpenGL/api/GL/glext.h">glext.h</a> and <a href="https://www.khronos.org/registry/OpenGL/api/GL/wglext.h">wglext.h</a></li>
</ul>
<p>Note that we only load the function pointers for OpenGL functions that are actually used in the production in the latter example. It might be a good idea to automate this. The functions need to be queried with string identifiers that get stored in the executable, so loading as few functions as possible saves space. <a href="https://msdn.microsoft.com/en-us/library/0zza0de8.aspx">Whole Program Optimization</a> might eliminate all unreferenced string literals but we couldn’t use it because of <a href="http://stackoverflow.com/a/2945619">a problem with <code>memcpy</code></a>.</p>
<h3 id="rendering-techniques">Rendering techniques</h3>
<p>Rendering is mostly raymarching and we used the <a href="http://mercury.sexy/hg_sdf/">hg_sdf</a> library for convenience. <a href="http://iquilezles.org/www/">Íñigo Quílez</a> (from now on called just iq) has written lots about this and many of the techniques. If you’ve ever visited <a href="https://www.shadertoy.com/">ShaderToy</a> you should be familiar with this already.</p>
<p>Additionally, we had the raymarcher output a depth buffer value so we could intersect signed distance fields with rasterized geometry and also apply post-processing effects.</p>
<h3 id="shading">Shading</h3>
<p>We use standard Unreal Engine 4 shading (<a href="http://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_notes_v2.pdf">here’s a big pdf that explains it</a>) with a GGX lobe. It isn’t very visible but makes a difference in highlights. Early on our plan was to have an unified lighting pipeline for both raymarched and rasterized shapes. The idea was to use deferred rendering and shadow maps, but this didn’t work at all.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/constant_shadows_thumb.jpg" alt="An early experiment with shadow mapping. Note how both the towers and the wires cast a shadow on the raymarched terrain and also intersect correctly. Full size."/></p><p>An early experiment with shadow mapping. Note how both the towers and the wires cast a shadow on the raymarched terrain and also intersect correctly. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/constant_shadows.jpg">Full size.</a></p>
</div>
<p>Rendering huge terrains with shadow maps is super hard to get right because of the wildly varying screen-to-shadow-map-texel ratio and other accuracy problems. I wasn’t really in the mood to start experimenting with <a href="https://msdn.microsoft.com/en-us/library/windows/desktop/ee416307(v=vs.85).aspx">cascaded shadow maps</a> either. Also, raymarching the same scene from multiple points of view is <em>slow</em>. So we just decided to scrap the whole unified lighting thing. This proved to be a huge pain later when were trying to match the lighting of the rasterized wires and raymarched scene geometry.</p>
<h2 id="terrain">Terrain</h2>
<p>The terrain is raymarched <a href="http://www.iquilezles.org/www/articles/morenoise/morenoise.htm">value noise</a> with analytic derivatives.<a href="#fn1" id="fnref1"><sup>1</sup></a> The generated derivates are used for shading of course, but also to control ray stepping length to accelerate ray traversal on smooth regions, just like in iq’s examples. If you want to learn more you you can read more about this technique in <a href="http://iquilezles.org/www/articles/terrainmarching/terrainmarching.htm">this old article of his</a> or play around with his <a href="https://www.shadertoy.com/view/4ttSWf">awesome rainforest scene on ShaderToy</a>. The landscape heightmap became much more realistic after msqrt implemented <a href="http://jcgt.org/published/0004/02/01/">exponentially distributed noise</a>.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/terrain.png" alt="Early tests of my own value noise terrain implementation."/></p><p>Early tests of my own value noise terrain implementation.</p>
</div>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/terrain_green_thumb.jpg" alt="A terrain implemented by branch that wasn’t used. I can’t remember why. Full size."/></p><p>A terrain implemented by branch that wasn’t used. I can’t remember why. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/terrain_green.jpg">Full size.</a></p>
</div>
<p>The landscape effect is very slow because we do brute force shadows and reflections. The shadows use <a href="http://www.iquilezles.org/www/articles/rmshadows/rmshadows.htm">a soft shadow hack</a> in which the penumbra size is determined by the closest distance encountered during shadow ray traversal. They look <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/shadow.gif">pretty nice in action</a>. We also tried using bisection tracing to speed it up but it produced too many artifacts to be useful. <a href="http://mercury.sexy/">Mercury</a>’s (another demogroup) <a href="http://erleuchtet.org/~cupe/permanent/enhanced_sphere_tracing.pdf">raymarching tricks</a> on the other hand helped us to eke out some extra quality with the same speed.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/refinement.png" alt="Landscape rendering with fixed point iteration enhancement (left) and with regular raymarching (right). Note the nasty ripple artifacts in the picture on the right."/></p><p>Landscape rendering with fixed point iteration enhancement (left) and with regular raymarching (right). Note the nasty ripple artifacts in the picture on the right.</p>
</div>
<p>The sky is built using pretty much the same techniques as described by iq in <em><a href="http://www.iquilezles.org/www/material/function2009/function2009.pdf">behind elevated</a></em>, slide 43. Just some simple functions of the ray direction vector. The sun outputs pretty large values to the framebuffer (&gt;100) so it adds some natural bloom as well.</p>
<h2 id="the-alley-scene">The alley scene</h2>
<p>This is a view that was inspired by <a href="http://www.fanhophotography.com/hong-kong-yesterday.html">Fan Ho’s</a> photography. Our post-processing effects really make it come together even though the underlying geometry is pretty simple.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/alley1_thumb.jpg" alt="An ugly distance field with some repeated blocks. Full size."/></p><p>An ugly distance field with some repeated blocks. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/alley1.jpg">Full size.</a></p>
</div>

<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/alley3_thumb.jpg" alt="The wires make the scene more interesting and lifelike. Full size."/></p><p>The wires make the scene more interesting and lifelike. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/alley3.jpg">Full size.</a></p>
</div>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/alley4_thumb.jpg" alt="In the final version some noise was added to the distance field to give an impression of brickwalls. Full size."/></p><p>In the final version some noise was added to the distance field to give an impression of brickwalls. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/alley4.jpg">Full size.</a></p>
</div>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/alley5_thumb.jpg" alt="A color gradient, bloom, chromatic aberration and lens flares are added in post-processing. Full size."/></p><p>A color gradient, bloom, chromatic aberration and lens flares are added in post-processing. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/alley5.jpg">Full size.</a></p>
</div>
<h2 id="modelling-with-distance-fields">Modelling with distance fields</h2>
<p>The B-52 bombers are a good example of modelling with signed distance fields. They were much simpler in the party version, but we spiced ’em up for the final. They look pretty convincing from afar:</p>

<p>However they are just a bunch of capsules. Admittedly it would’ve been easier to just to make them in some 3D package but we didn’t have any kind of mesh packing pipeline set up so this was faster. Just for reference, this is how the distance field shader looks like: <a href="https://gist.github.com/seece/dcc4e1a6a68f0a911ad05e3be755588b"><code>bomber_sdf.glsl</code></a></p>

<h2 id="the-characters">The characters</h2>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/goats.png" alt="The first four frames of the goat animation."/></p><p>The first four frames of the goat animation.</p>
</div>
<p>The animated characters are just packed 1-bit bitmaps. During playback the frames are crossfaded from one to the next. They were contributed by a mysterious goatman.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/characters.jpg" alt="A goatherd with his friends."/></p><p>A goatherd with his friends.</p>
</div>
<h2 id="post-processing">Post-processing</h2>
<p>The post-processing effects were written by varko. The pipeline is:</p>
<ol>
<li>Apply shading from G-buffer</li>
<li>Calculate depth-of-field</li>
<li>Extract bright parts for bloom</li>
<li>Perform <span><em>N</em></span> separable Gaussian blurs</li>
<li>Calculate fake lens flares &amp; wide headlight flares</li>
<li>Composite all together</li>
<li>Smooth edges with FXAA (<a href="https://github.com/dolphin-emu/dolphin/blob/master/Data/Sys/Shaders/FXAA.glsl">thanks mudlord!</a>)</li>
<li>Color correction</li>
<li>Gamma correction and subtle film grain</li>
</ol>
<p>The lens flares follow pretty much the technique <a href="http://john-chapman-graphics.blogspot.fi/2013/02/pseudo-lens-flare.html">described by John Chapman</a>. They were sometimes hard to work with but in the end still delivered.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/bigdof_thumb.jpg" alt="We tried to use the depth of field effect with good taste. Full size."/></p><p>We tried to use the depth of field effect with good taste. <a href="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/bigdof.jpg">Full size.</a></p>
</div>
<p>The depth of field effect (based on <a href="https://www.slideshare.net/DICEStudio/five-rendering-ideas-from-battlefield-3-need-for-speed-the-run">DICE’s technique</a>) is made of three passes. The first one calculates the size of circle of confusion for each pixel and the two other passes apply two rotated box blurs each. We also do iterative refinement (i.e. apply multiple Gaussian blurs) when needed. This implementation worked really well for us and was fun to play with.</p>
<video width="800" src="img/64k/dof.mp4" controls="control">
</video>
<p><em>The depth of field effect in action. The red picture shows the calculated circle of confusion for the DOF blur.</em></p>
<h3 id="color-correction">Color correction</h3>
<p>There is an animated parameter <code>pp_index</code> in Rocket that is used to switch between color correction profiles. Each profile is just a different branch in a big switch statement in the final post-processing pass shader:</p>
<div><pre><code>vec3 cl = getFinalColor();

<span>if</span> (u_GradeId == <span>1</span>) {
    cl.gb *= UV.y * <span>0.7</span>;
    cl = pow(cl, vec3(<span>1.1</span>));
} <span>else</span> <span>if</span> (u_GradeId == <span>2</span>) {
    cl.gb *= UV.y * <span>0.6</span>;
    cl.g = <span>0.0+0.6</span>*smoothstep(-<span>0.05</span>,<span>0.9</span>,cl.g*<span>2.0</span>);
    cl = <span>0.005</span>+pow(cl, vec3(<span>1.2</span>))*<span>1.5</span>;
} <span>/* etc.. */</span></code></pre></div>
<p>It’s very simple but worked well enough.</p>
<h2 id="physics-simulation">Physics simulation</h2>
<p>There are two simulated systems in the demo: the wires and a flock. They were also written by varko.</p>
<h3 id="the-wires">The wires</h3>

<p>The wires are considered a series of springs. They are simulated on the GPU using compute shaders. We run multiple small steps of the simulation due to the instability of the Verlet integration method we use. The compute shader also outputs the wire geometry (a series of triangular prisms) to a vertex buffer. Sadly, the simulation doesn’t work on AMD cards for some reason.</p>
<h3 id="a-flock-of-birds">A flock of birds</h3>
<video width="800" src="img/64k/birds.mp4" controls="control">
</video>
<p><em>The birds give a sense of scale.</em></p>
<p>The flock simulation consists of 512 birds with the first 128 considered the leaders. The leaders move in a <a href="http://prideout.net/blog/?p=63">curl noise</a> pattern and the others follow. I think in real life birds consider the movement of their closest neighbours, but this simplification looks good enough. The flock is rendered as <code>GL_POINT</code>s whose size is modulated to give appearance of flapping wings. This rendering technique was also used in Half-Life 2, I think.</p>
<h2 id="music">Music</h2>
<p>The traditional way to make music for a 64k intro is to have a <a href="https://en.wikipedia.org/wiki/Virtual_Studio_Technology">VST-instrument plugin</a> that allows a musicians to use their regular tools to compose the music. Farbrausch’s <a href="http://www.pouet.net/prod.php?which=15073">V2 synthesizer</a> is a classic example of this approach.</p>
<p>This was a problem. I didn’t want to use any ready made synthesizer but I also knew from earlier failed experiments that making my own virtual instrument would be a lot work. I remember really liking the mood of <a href="https://www.youtube.com/watch?v=Sr5Jh5hqfAA">element/gesture 61%</a>, a demo by branch with a <a href="http://hypermammut.sourceforge.net/paulstretch/">paulstretched</a> ambient song. It got me thinking about implementing it in a 4k or 64k size.</p>
<h3 id="paulstretch">Paulstretch</h3>
<p>Paulstretch is a wonderful algorithm for really crazy time stretching. If you haven’t heard about it, you should definitely listen <a href="https://www.youtube.com/watch?v=Fai5s3Zn7b4">what it can make out of Windows 98’s startup sound</a>. Its inner workings are described <a href="http://www.microscopics.co.uk/blog/2010/paulstretch-an-interview-with-paul-nasca/">in this interview with the author</a>, and it’s also open source.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/paulstretch.png" alt="Original audio (top) and stretched audio (bottom) done with Audacity’s Paulstretch effect. Note how the frequencies also get smeared across the spectrum (y-axis)."/></p><p>Original audio (top) and stretched audio (bottom) done with Audacity’s Paulstretch effect. Note how the frequencies also get smeared across the spectrum (y-axis).</p>
</div>
<p>Basically, as it stretches the input it also scrambles its phases in frequency space so that instead of metallic artifacts you get ethereal echoes. This requires of course a Fourier transform and the original application uses the Kiss FFT library for this. I didn’t want to depend on an external library so in the end I implemented a naive <span>O(<em>N</em><sup>2</sup>)</span> Discrete Fourier Transform on the GPU. This took a long time to get right but in the end it was worth it. The GLSL shader implementation is very compact and runs pretty fast despite its brute-force nature.</p>
<h3 id="a-tracker-module">A tracker module</h3>
<p>Now it was possible to make swathes of ambient drone, given some reasonable input audio to stretch. So I decided to use some tried and tested technology: tracker music. It’s pretty much like MIDI<a href="#fn2" id="fnref2"><sup>2</sup></a> but with also samples packed in the file. For example elitegroup’s <a href="http://www.pouet.net/prod.php?which=374">kasparov</a> (<a href="https://www.youtube.com/watch?v=svkjjNt7J5Q">YouTube</a>) uses a module with additional reverb added. If it worked 17 years ago, why not now?</p>
<p>I used Windows’ built-in <code>gm.dls</code> MIDI soundbank file (again, a classic trick) to make a song with MilkyTracker in XM module format. This is the format that was used also for many MS-DOS demoscene productions back in the 90s.</p>
<div>
<p><img src="https://www.wildlondon.org.uk/blog/david-bradshaw/img/64k/milky.png" alt="I used MilkyTracker to compose the original song. The instrument sample data is stripped off the final module file and replaced with offsets and lengths in gm.dls."/></p><p>I used MilkyTracker to compose the original song. The instrument sample data is stripped off the final module file and replaced with offsets and lengths in <code>gm.dls</code>.</p>
</div>
<p>The catch with <code>gm.dls</code> is that the instruments, courtesy of Roland in 1996, sound very dated and cheesy. Turns out this is not a problem if you bathe them in tons of reverb! Here’s an example where a short test song is played first and a stretched version follows:</p>
<audio src="img/64k/huojuu.mp3" controls="control">
</audio>
<p>Surprisingly atmospheric, right? So yeah, I made a song imitating Hollywood songwriting and it turned out great. That’s pretty much all that’s going on the music side.</p>
<h2 id="thanks">Thanks</h2>
<p>Thanks to varko for help in some technical details of this article.</p>
<h2 id="extra-stuff">Extra stuff</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=woqksTHNbvk">Ferris of Logicoma showing off his 64k toolkit</a>
<ul>
<li>Make sure to first watch <a href="https://www.youtube.com/watch?v=GjuridCR2Fo">Engage</a>, their contestant in the same compo we took part in</li>
</ul></li>
<li><a href="https://github.com/laurentlb/Ctrl-Alt-Test/tree/master/">The source of some Ctrl-Alt-Test productions</a>
<ul>
<li>Has some 4k and 64k code.</li>
<li>They had an intro too: <a href="https://www.youtube.com/watch?v=HpAMtE4i8zg">H-Immersion</a></li>
</ul></li>
</ul>

<hr/>


</div>
  </body>
</html>
