<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://eli.thegreenplace.net/2025/cross-entropy-and-kl-divergence/">Original</a>
    <h1>Cross-Entropy and KL Divergence</h1>
    
    <div id="readability-page-1" class="page"><div>
                
                <p>Cross-entropy is widely used in modern ML to compute the loss for classification
tasks. This post is a brief overview of the math behind it and a related
concept called Kullback-Leibler (KL) divergence.</p>
<div id="information-content-of-a-single-random-event">
<h2>Information content of a single random event</h2>
<p>We&#39;ll start with a single event (<em>E</em>) that has probability <em>p</em>. The information
content (or &#34;degree of surprise&#34;) of this event occurring is defined as:</p>
<p>The base 2 here is used so that we can count the information in units of <em>bits</em>.
Thinking about this definition intuitively, imagine an event with probability
<em>p=1</em>; using the formula, the information we gain by observing this event
occurring is 0, which makes sense. On the other extreme, as the probability
<em>p</em> approaches 0, the information we gain is huge. An equivalent way to write
the formula is:</p>
<p>Some numeric examples: suppose we flip a fair coin and it comes out heads. The
probability of this event happening is 1/2, therefore:</p>
<p>Now suppose we roll a fair die and it lands on 4. The probability of this event
happening is 1/6, therefore:</p>
<p>In other words, the degree of surprise for rolling a 4 is higher than the degree
of surprise for flipping to heads - which makes sense, given the probabilities
involved.</p>
<p>Other than behaving correctly for boundary values, the logarithm function makes
sense for calculating the degree of surprise for another important reason: the
way it behaves for a combination of events.</p>
<p>Consider this: we flip a fair coin and roll a fair die; the coin comes out
heads, and the die lands on 4. What is the probability of this event happening?
Because the two events are independent, the probability is the product of the
probabilities of the individual events, so 1/12, and then:</p>
<p>Note that the entropy is the precise <em>sum</em> of the entropies of individual events.
This is to be expected - we need so many bits for one of the events, and so many
for the other; the total of the bits adds up. The logarithm function gives us
exactly this behavior for probabilities:</p>
</div>
<div id="entropy">
<h2>Entropy</h2>
<p>Given a random variable <em>X</em> with values  and associated
probabilities , the <em>entropy of X</em> is defined as the
expected value of information for <em>X</em>:</p>
<p>High entropy means high uncertainty, while low entropy means low uncertainty.
Let&#39;s look at a couple of examples:</p>
<p><img alt="distribution with single value at probability 1, others at 0" src="https://eli.thegreenplace.net/images/2025/distrib-1-0s.png"/></p><p>This is a random variable with 5 distinct values; the probability of 
is 1, and the rest is 0. The entropy here is 0, because 
and also   <a href="#footnote-1" id="footnote-reference-1">[1]</a>. We gain no
information by observing an event sampled from this distribution, because we
knew ahead of time what would happen.</p>
<p>Another example is a uniform distribution for the 5 possible outcomes:</p>
<p><img alt="distribution with uniform probabilities 0.2 per value" src="https://eli.thegreenplace.net/images/2025/distrib-uniform.png"/></p><p>The entropy for this distribution is:</p>
<p>Intuitively: we have 5 different values with equal probabilities, so we&#39;ll need
 bits to represent that. Note that entropy is always
non-negative, because
 and therefore  for all <em>j</em>
in a proper probability distribution.</p>
<p>It&#39;s not hard to show that the maximum possible entropy for a random variable
occurs for a uniform distribution. In all other distributions, some values are
more represented than others which makes the result somewhat less surprising.</p>
</div>
<div id="cross-entropy">
<h2>Cross-entropy</h2>
<p>Cross-entropy is an extension of the concept of entropy, when two different
probability distributions are present. The typical formulation useful for
machine learning is:</p>
<p>Where:</p>
<ul>
<li><em>P</em> is the actual observed data distribution</li>
<li><em>Q</em> is the predicted data distribution</li>
</ul>
<p>Similarly to entropy, cross-entropy is non-negative; in fact, it collapses to
the entropy formula when <em>P</em> and <em>Q</em> are the same:</p>
<p>An information-theoretic interpretation of cross-entropy is: the average number
of bits required to encode an actual probability distribution <em>P</em>, when we
assumed the data follows <em>Q</em> instead.</p>
<p>Here&#39;s a numeric example:</p>
<div><pre><span></span><span>p</span> <span>=</span> <span>[</span><span>0.1</span><span>,</span> <span>0.2</span><span>,</span> <span>0.4</span><span>,</span> <span>0.2</span><span>,</span> <span>0.1</span><span>]</span>
<span>q</span> <span>=</span> <span>[</span><span>0.2</span><span>,</span> <span>0.2</span><span>,</span> <span>0.2</span><span>,</span> <span>0.2</span><span>,</span> <span>0.2</span><span>]</span>
</pre></div>
<p>Plotted:</p>
<p><img alt="plotting p vs q" src="https://eli.thegreenplace.net/images/2025/pq-n-vs-uniform.png"/></p><p>The cross-entropy of these two distributions is 2.32</p>
<p>Now let&#39;s try a <em>Q</em> that&#39;s slightly closer to <em>P</em>:</p>
<div><pre><span></span><span>p</span> <span>=</span> <span>[</span><span>0.1</span><span>,</span> <span>0.2</span><span>,</span> <span>0.4</span><span>,</span> <span>0.2</span><span>,</span> <span>0.1</span><span>]</span>
<span>q</span> <span>=</span> <span>[</span><span>0.15</span><span>,</span> <span>0.175</span><span>,</span> <span>0.35</span><span>,</span> <span>0.175</span><span>,</span> <span>0.15</span><span>]</span>
</pre></div>
<p><img alt="plotting p vs q" src="https://eli.thegreenplace.net/images/2025/pq-n-vs-n2.png"/></p><p>The cross-entropy in these distributions is somewhat lower, 2.16; this is
expected, because they&#39;re more similar. In other words, the outcome of measuring
<em>P</em> when our model predicted <em>Q</em> is less surprising.</p>
</div>
<div id="kl-divergence">
<h2>KL divergence</h2>
<p>Cross-entropy is useful for tracking the training loss of a model (more on this
in the next section),
but it has some mathematical properties that make it less than ideal
as a statistical tool to compare two probability distributions. Specifically,
, which isn&#39;t (usually) zero; this is the lowest value
possible for cross-entropy. In other words, cross-entropy
always retains the inherent uncertainty of <em>P</em>.</p>
<p>The KL divergence fixes this by subtracting  from cross-entropy:</p>
<p>Manipulating the logarithms, we can also get these alternative formulations:</p>
<p>Thus, the KL divergence is more useful as a <a href="https://en.wikipedia.org/wiki/Divergence_(statistics)">measure of divergence</a>
between
two probability distributions, since . Note, however, that
it&#39;s not a true <a href="https://en.wikipedia.org/wiki/Metric_space">distance metric</a>
because it&#39;s not symmetric:</p>
</div>
<div id="uses-in-machine-learning">
<h2>Uses in machine learning</h2>
<p>In ML, we often have a model that makes a prediction and a set of training data
which defines a real-world probability distribution. It&#39;s natural to define
a loss function in terms of the difference between the two distributions (the
model&#39;s prediction and the real data).</p>
<p>Cross-entropy is very useful as a loss function because it&#39;s non-negative and
provides a single scalar number that&#39;s lower for similar distributions and
higher for dissimilar distributions. Moreover, if we think of cross-entropy
in terms of KL divergence:</p>
<p>We&#39;ll notice that  - the entropy of the real-world distribution - does
not depend on the model at all. Therefore, optimizing cross-entropy is equivalent
to optimizing the KL divergence. I wrote about concrete uses of cross-entropy
as a loss function in previous posts:</p>
<ul>
<li><a href="https://eli.thegreenplace.net/2016/logistic-regression/">Logistic regression</a></li>
<li><a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">Softmax for multiclass classification</a></li>
</ul>
<p>That said, the KL divergence is also sometimes useful more directly; for example
in the <a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">evidence lower bound</a>
used for <a href="https://en.wikipedia.org/wiki/Variational_autoencoder">Variational autoencoders</a>.</p>
</div>
<div id="relation-to-maximum-likelihood-estimation">
<h2>Relation to Maximum Likelihood Estimation</h2>
<p>There&#39;s an interesting relation between the concepts discussed in this post
and <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum Likelihood Estimation</a>.</p>
<p>Suppose we have a true probability distribution <em>P</em>, and a parameterized model
that predicts the probability distribution . <img alt="\theta" src="https://eli.thegreenplace.net/images/math/cb005d76f9f2e394a770c2562c2e150a413b3216.png"/>
stands for all the parameters of our model (e.g. all the weights of a deep
learning network).</p>
<p>The <em>likelihood</em> of observing a set of samples  drawn
from <em>P</em> is:</p>
<p>However, we don&#39;t really know <em>P</em>; what we do know is , so
we can calculate:</p>
<p>The idea is to find an optimal set of parameters 
such that this likelihood is maximized; in other words:</p>
<p>Working with products is inconvenient, however, so a logarithm is used instead
to convert a product to a sum (since  is a monotonically
increasing function, maximizing it is akin to maximizing <img alt="f(x)" src="https://eli.thegreenplace.net/images/math/3e03f4706048fbc6c5a252a85d066adf107fcc1f.png"/> itself):</p>
<p>This is the <em>maximal log-likelihood</em>.</p>
<p>Now a clever statistical trick is employed; first, we multiply the function
we&#39;re maximizing by the constant  - this doesn&#39;t affect the
maxima, of course:</p>
<p>The function inside the <em>argmax</em> is now the average across <em>n</em> samples obtained
from the true probability distribution <em>P</em>. The <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">Law of Large numbers</a>
states that with a large enough <em>n</em>, this average converges to the expected
value of drawing from this distribution:</p>
<p>This should start looking familiar; all that&#39;s left is to negate the sum and
minimize the negative instead:</p>
<p>The function we&#39;re now minimizing is the <em>cross-entropy</em> between <em>P</em> and
. We&#39;ve shown that maximum likelihood estimation is equivalent
to minimizing the cross-entropy between the true and and predicted data
distributions.</p>
<hr/>

</div>

            </div></div>
  </body>
</html>
