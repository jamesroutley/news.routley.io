<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://alexwlchan.net/2025/copying-sqlite-databases/">Original</a>
    <h1>A faster way to copy SQLite databases between computers</h1>
    
    <div id="readability-page-1" class="page"><div id="main" tabindex="-1"> <article>   <p>I store a lot of data in SQLite databases on remote servers, and I often want to copy them to my local machine for analysis or backup.</p> <p>When I’m starting a new project and the database is near-empty, this is a simple rsync operation:</p> <div><div><pre><code><span>$</span><span> </span>rsync <span>--progress</span> username@server:my_remote_database.db my_local_database.db
</code></pre></div></div> <p>As the project matures and the database grows, this gets slower and less reliable. Downloading a 250MB database from my web server takes about a minute over my home Internet connection, and that’s pretty small – most of my databases are multiple gigabytes in size.</p> <p>I’ve been trying to make these copies go faster, and I recently discovered a neat trick.</p> <p>What really slows me down is my indexes. I have a lot of indexes in my SQLite databases, which dramatically speed up my queries, but also make the database file larger and slower to copy. (In one database, there’s an index which single-handedly accounts for half the size on disk!)</p> <p>The indexes don’t store anything unique – they just duplicate data from other tables to make queries faster. Copying the indexes makes the transfer less efficient, because I’m copying the same data multiple times. I was thinking about ways to skip copying the indexes, and I realised that SQLite has built-in tools to make this easy.</p> <h2 id="dumping-a-database-as-a-text-file">Dumping a database as a text file</h2> <p>SQLite allows you to <a href="https://sqlite.org/cli.html#converting_an_entire_database_to_a_text_file">dump a database as a text file</a>. If you use the <code>.dump</code> command, it prints the entire database as a series of SQL statements. This text file can often be significantly smaller than the original database.</p> <p>Here’s the command:</p> <div><div><pre><code><span>$</span><span> </span>sqlite3 my_database.db .dump <span>&gt;</span> my_database.db.txt
</code></pre></div></div> <p>And here’s what the beginning of that file looks like:</p> <div><div><pre><code><span>PRAGMA</span> <span>foreign_keys</span><span>=</span><span>OFF</span><span>;</span>
<span>BEGIN</span> <span>TRANSACTION</span><span>;</span>
<span>CREATE</span> <span>TABLE</span> <span>IF</span> <span>NOT</span> <span>EXISTS</span> <span>&#34;tags&#34;</span> <span>(</span>
   <span>[</span><span>name</span><span>]</span> <span>TEXT</span> <span>PRIMARY</span> <span>KEY</span><span>,</span>
   <span>[</span><span>count_uses</span><span>]</span> <span>INTEGER</span> <span>NOT</span> <span>NULL</span>
<span>);</span>
<span>INSERT</span> <span>INTO</span> <span>tags</span> <span>VALUES</span><span>(</span><span>&#39;carving&#39;</span><span>,</span><span>260</span><span>);</span>
<span>INSERT</span> <span>INTO</span> <span>tags</span> <span>VALUES</span><span>(</span><span>&#39;grass&#39;</span><span>,</span><span>743</span><span>);</span>
<span>…</span>
</code></pre></div></div> <p>Crucially, this reduces the large and disk-heavy indexes into a single line of text – it’s an instruction to create an index, not the index itself.</p> <div><div><pre><code><span>CREATE</span> <span>INDEX</span> <span>[</span><span>idx_photo_locations</span><span>]</span>
    <span>ON</span> <span>[</span><span>photos</span><span>]</span> <span>([</span><span>longitude</span><span>],</span> <span>[</span><span>latitude</span><span>]);</span>
</code></pre></div></div> <p>This means that I’m only storing each value once, rather than the many times it may be stored across the original table and my indexes. This is how the text file can be smaller than the original database.</p> <p>If you want to reconstruct the database, you pipe this text file back to SQLite:</p> <div><div><pre><code><span>$</span><span> </span><span>cat </span>my_database.db.txt | sqlite3 my_reconstructed_database.db
</code></pre></div></div> <p>Because the SQL statements are very repetitive, this text responds well to compression:</p> <div><div><pre><code><span>$</span><span> </span>sqlite3 explorer.db .dump | <span>gzip</span> <span>-c</span> <span>&gt;</span> explorer.db.txt.gz
</code></pre></div></div> <p>To give you an idea of the potential savings, here’s the relative disk size for one of my databases.</p> <table id="sizes"> <tbody><tr> <th>File</th> <th>Size on disk</th> </tr> <tr> <td>original SQLite database</td> <td title="7,081,912 bytes">3.4 GB</td> </tr> <tr> <td>text file (<code>sqlite3 my_database.db .dump</code>)</td> <td title="2,785,408 bytes">1.3 GB</td> </tr> <tr> <td>gzip-compressed text (<code>sqlite3 my_database.db .dump | gzip -c</code>)</td> <td title="491,904 bytes">240 MB</td> </tr> </tbody></table> <p>The gzip-compressed text file is 14× smaller than the original SQLite database – that makes downloading the database much faster.</p> <h2 id="my-new-sshrsync-command">My new ssh+rsync command</h2> <p>Rather than copying the database directly, now I create a gzip-compressed text file on the server, copy that to my local machine, and reconstruct the database. Like so:</p> <div><div><pre><code><span># Create a gzip-compressed text file on the server</span>
ssh username@server <span>&#34;sqlite3 my_remote_database.db .dump | gzip -c &gt; my_remote_database.db.txt.gz&#34;</span>

<span># Copy the gzip-compressed text file to my local machine</span>
rsync <span>--progress</span> username@server:my_remote_database.db.txt.gz my_local_database.db.txt.gz

<span># Remove the gzip-compressed text file from my server</span>
ssh username@server <span>&#34;rm my_remote_database.db.txt.gz&#34;</span>

<span># Uncompress the text file</span>
<span>gunzip </span>my_local_database.db.txt.gz

<span># Reconstruct the database from the text file</span>
<span>cat </span>my_local_database.db.txt | sqlite3 my_local_database.db

<span># Remove the local text file</span>
<span>rm </span>my_local_database.db.txt
</code></pre></div></div> <h2 id="a-database-dump-is-a-stable-copy-source">A database dump is a stable copy source</h2> <p>This approach fixes another issue I’ve had when copying SQLite databases.</p> <p>If it takes a long time to copy a database and it gets updated midway through, rsync may give me an invalid database file. The first half of the file is pre-update, the second half file is post-update, and they don’t match. When I try to open the database locally, I get an error:</p> <div><div><pre><code>database disk image is malformed
</code></pre></div></div> <p>By creating a text dump before I start the copy operation, I’m giving rsync a stable copy source. That text dump isn’t going to change midway through the copy, so I’ll always get a complete and consistent text file.</p> <hr/> <p>This approach has saved me hours when working with large databases, and made my downloads both faster and more reliable. If you have to copy around large SQLite databases, give it a try.</p> </article> </div></div>
  </body>
</html>
