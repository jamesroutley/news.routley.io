<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.psu.edu/news/engineering/story/conversations-remotely-detected-cell-phone-vibrations-researchers-report">Original</a>
    <h1>Conversations remotely detected from cell phone vibrations, researchers report</h1>
    
    <div id="readability-page-1" class="page"><div><div><div><div id="text-content-container"><p>UNIVERSITY PARK, Pa. —  An emerging form of surveillance, “wireless-tapping,” explores the possibility of remotely deciphering conversations from the tiny vibrations produced by a cellphone’s earpiece. With the goal of protecting users’ privacy from potential bad actors, a team of computer science researchers at Penn State demonstrated that transcriptions of phone calls can be generated from radar measurements taken up to three meters, or about 10 feet, from a phone. While accuracy remains limited — around 60% for a vocabulary of up to 10,000 — the findings raise important questions about future privacy risks. </p>
<p>They published their research in <a href="https://dl.acm.org/doi/abs/10.1145/3734477.3734708" target="_blank">Proceedings of WiSec 2025: 18th ACM Conference on Security and Privacy in Wireless and Mobile Networks</a>. The work builds upon a <a href="https://news.engr.psu.edu/2022/gowda-mahanth-mobile-vibrations-eavesdrop-remotely.aspx" target="_blank">2022 project</a> in which the team used a radar sensor and voice recognition software to wirelessly identify 10 predefined words, letters and numbers with up to 83% accuracy. </p>
<p>“When we talk on a cellphone, we tend to ignore the vibrations that come through the earpiece and cause the whole phone to vibrate,” said first author Suryoday Basak, doctoral candidate in computer science. “If we capture these same vibrations using remote radars and bring in machine learning to help us learn what is being said, using context clues, we can determine whole conversations. By understanding what is possible, we can help the public be aware of the potential risks.” </p>
<p>Basak and his adviser, <a href="https://www.eecs.psu.edu/departments/directory-detail-g.aspx?q=mkg31" target="_blank">Mahanth Gowda</a>, associate professor of computer science and engineering, who co-authored the paper, used a millimeter-wave radar sensor — the same type of technology used in self-driving cars, motion detectors and 5G wireless networks — to explore the potential for compact, radar-based devices that could be miniaturized to fit inside everyday objects like pens. Their experimental setup is only for research purposes, the researchers said, developed in anticipation of what bad actors could potentially create. They then adapted “Whisper,” an open-source, large-scale speech recognition model powered by artificial intelligence (AI), to decode the vibrations into recognizable speech transcriptions.   </p>
<p>“Over the last three years, there’s been a huge explosion in AI capabilities and open-source speech recognition models,” Basak said. “We can use these models, but they are catered more toward clean speech or everyday use cases, so we have to adapt them to recognize low quality, ‘noisy’ radar data.”</p>
</div></div></div></div></div>
  </body>
</html>
