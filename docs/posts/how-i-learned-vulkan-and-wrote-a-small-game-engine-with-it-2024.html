<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://edw.is/learning-vulkan/">Original</a>
    <h1>How I learned Vulkan and wrote a small game engine with it (2024)</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
<article>
    <ul>
<li>
<p><a href="https://github.com/eliasdaler/edw.is/discussions/7">Comments (GitHub discussion)</a></p>
</li>
<li>
<p><a href="https://news.ycombinator.com/item?id=40595741">Comments (Hacker News)</a></p>
</li>
</ul>
<p><strong>tl;dr</strong>: I learned some Vulkan and made a game engine with two small game demos in 3 months.</p>
<p>The code for the engine and the games can be found here: <a href="https://github.com/eliasdaler/edbr">https://github.com/eliasdaler/edbr</a></p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/final.jpg"/>
</figure>

<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/mtp_screenshot2.png"/>
</figure>

<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/platformer_screenshot.png"/>
</figure>

<hr/>


<p>This article documents my experience of learning Vulkan and writing a small game/engine with it. It took me around 3 months to do it without any previous knowledge of Vulkan (I had previous OpenGL experience and some experience with making game engines, though).</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/mtp_screenshot1.jpg"/>
</figure>

<p>The engine wasn’t implemented as a general purpose engine, which is probably why it took me a few months (and not years) to achieve this. I started by making a small 3D game and separated reusable parts into the “engine” afterwards. I can recommend everyone to follow the same process to not get stuck in the weeds (see “Bike-shedding” section below for more advice).</p>
<h2 id="preface">Preface</h2>
<p>I’m a professional programmer, but I’m self-taught in graphics programming. I started studying graphics programming around 1.5 years ago by learning OpenGL and writing a 3D engine in it.</p>
<p>The engine I wrote in Vulkan is mostly suited for smaller level-based games. I’ll explain things which worked for me, but they might not be the most efficient. My implementation would probably still be a good starting point for many people.</p>
<blockquote>
  Hopefully, this article will help make some things about Vulkan clearer to you. But you also need to be patient. It took me <em>months</em> to implement what I have today and I did it by cutting corners in many places. But if a self-taught programmer like me can build something with Vulkan, then so can you!
</blockquote>

<h2 id="learning-graphics-programming">Learning graphics programming</h2>
<blockquote>
  This is a very high level overview of how I learned some graphics programming myself. If there’s interest, I might write another article with more resources and helpful guidelines.
</blockquote>

<p>If you haven’t done any graphics programming before, you should start with OpenGL. It’s much easier to learn it and not get overwhelmed by all the complexity that Vulkan has. A lot of your OpenGL and graphics programming knowledge will be useful when you start doing things with Vulkan later.</p>
<p>Ideally, you should at least get a textured model displayed on the screen with some simple Blinn-Phong lighting. I can also recommend doing some basic shadow mapping too, so that you learn how to render your scene from a different viewpoint and to a different render target, how to sample from depth textures and so on.</p>
<p>I can recommend using the following resources to learn OpenGL:</p>
<ul>
<li><a href="https://learnopengl.com/">https://learnopengl.com/</a></li>
<li><a href="https://capnramses.itch.io/antons-opengl-4-tutorials">Anton’s OpenGL 4 Tutorials book</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL8vNj3osX2PzZ-cNSqhA8G6C1-Li5-Ck8">Thorsten Thormählen’s lectures</a> lectures (watch the first 6 videos, the rest might be a bit too advanced)</li>
</ul>
<p>Sadly, most OpenGL resources don’t teach the latest OpenGL 4.6 practices. They make writing OpenGL a lot more enjoyable. If you learn them, transitioning to Vulkan will be much easier (I only learned about OpenGL 3.3 during my previous engine development, though, so it’s not a necessity).</p>
<p>Here are some resources which teach you the latest OpenGL practices:</p>
<ul>
<li><a href="https://juandiegomontoya.github.io/modern_opengl.html">https://juandiegomontoya.github.io/modern_opengl.html</a></li>
<li><a href="https://github.com/fendevel/Guide-to-Modern-OpenGL-Functions">https://github.com/fendevel/Guide-to-Modern-OpenGL-Functions</a></li>
</ul>
<blockquote>
  It’s also good to have some math knowledge, especially linear algebra: how to work with vectors, transformation matrices and quaternions. My favorite book about linear algebra/math is <a href="https://www.gamemath.com/book/intro.html">3D Math Primer for Graphics and Game Development by F. Dunn and I. Parbery</a>. You don’t need to read it all in one go - use it as a reference if some math in the OpenGL resources above doesn’t make sense to you.
</blockquote>

<h2 id="bike-shedding-and-how-to-avoid-it">Bike-shedding and how to avoid it</h2>
<p><a href="https://en.wikipedia.org/wiki/Law_of_triviality">https://en.wikipedia.org/wiki/Law_of_triviality</a></p>
<p>Ah, bike-shedding… Basically, it’s a harmful pattern of overthinking and over-engineering even the simplest things. It’s easy to fall into this trap when doing graphics programming (<em>especially</em> when doing Vulkan since you need to make many choices when implementing an engine with it).</p>
<ul>
<li>Always ask yourself “Do I <em>really</em> need this?”, “Will this thing ever become a bottleneck?”.</li>
<li>Remember that you can always rewrite any part of your game/engine later.</li>
<li>Don’t implement something unless you need it <strong>right now</strong>. Don’t think “Well, a good engine needs X, right…?”.</li>
<li>Don’t try to make a general purpose game engine. It’s probably even better to not think about “the engine” at first and write a simple game.</li>
<li>Make a small game first - a Breakout clone, for example. Starting your engine development by doing a Minecraft clone with multiplayer support is probably not a good idea.</li>
<li>Be wary of people who tend to suggest complicated solutions to simple problems.</li>
<li>Don’t look too much at what other people do. I’ve seen many over-engineered engines on GitHub - sometimes they’re that complex for a good reason (and there are <em>years</em> of work behind them). But you probably don’t need most of that complexity, especially for simpler games.</li>
<li>Don’t try to make magical wrappers around Vulkan interfaces prematurely, especially while you’re still learning Vulkan.</li>
</ul>
<p>Get it working first. Leave “TODO”/“FIXME” comments in some places. Then move on to the next thing. Try to fix “TODO”/“FIXME” places only when they really become problematic or bottleneck your performance. You’ll be surprised to see how many things won’t become a problem at all.</p>
<blockquote>
  Some of this advice only applies when you’re working alone on a hobby project. Of course, it’s much harder to rewrite something from scratch when others start to depend on it and a “temp hack” becomes a fundamental part of the engine which is very hard to change without breaking many things.
</blockquote>

<h2 id="why-vulkan">Why Vulkan?</h2>
<blockquote>
  <p>Ask yourself if you need to learn a graphics API at all. If your main goal is to make a game as soon as possible, then you might be better off using something like Godot or Unreal Engine.</p>
<p>However, there’s nothing wrong with reinventing the wheel or doing something from scratch. Especially if you do it just for fun, to get into graphics programming or to get an in-depth knowledge about how something works.</p>

</blockquote>

<p>The situation with graphic APIs in 2024 is somewhat complicated. It all depends on the use case: DirectX seems like the most solid choice for most AAA games. WebGL or WebGPU are the only two choices for doing 3D graphics on the web. Metal is the go-to graphics API on macOS and iOS (though you can still do Vulkan there via MoltenVK).</p>
<p>My use case is simple: I want to make small 3D games for desktop platforms (Windows and Linux mostly). I also love open source technology and open standards. So, it was a choice between OpenGL and Vulkan for me.</p>
<p>OpenGL is a good enough choice for many small games. But it’s very unlikely that it’ll get new versions in the future (so you can’t use some newest GPU capabilities like ray tracing), it’s deprecated on macOS and its future is uncertain.</p>
<p>WebGPU was also a possible choice. Before learning Vulkan, I <a href="https://github.com/eliasdaler/webgpu-learning">learned some of it</a>. It’s a pretty solid API, but I had some problems with it:</p>
<ul>
<li>It’s still not stable and there’s not a lot of tutorials and examples for it. <a href="https://eliemichel.github.io/LearnWebGPU/">This tutorial</a> is fantastic, though.</li>
<li>WGSL is an okay shading language, but I just find its syntax not as pleasant as GLSL’s (note that you can write in GLSL and then load compiled SPIR-V on WebGPU native).</li>
<li>On desktop, it’s essentially a wrapper around other graphic APIs (DirectX, Vulkan, Metal).This introduces additional problems for me:
<ul>
<li>It can’t do things some things that Vulkan or DirectX can do.</li>
<li>It has more limitations than native graphic APIs since it needs to behave similarly between them.</li>
<li>RenderDoc captures become confusing as they differ between the platforms (you can get DirectX capture on Windows and Vulkan capture on Linux) and you don’t have 1-to-1 mapping between WebGPU calls and native API calls.</li>
<li>Using Dawn and WGPU feels like using bgfx or sokol. You don’t get the same degree of control over the GPU and some of the choices/abstractions might not be the most pleasant for you.</li>
</ul>
</li>
<li>No bindless textures (WIP discussion <a href="https://github.com/gpuweb/gpuweb/issues/380">here</a>).</li>
<li>No push constants (WIP discussion <a href="https://github.com/gpuweb/gpuweb/issues/75">here</a>).</li>
</ul>
<p>Still, I think that WebGPU is a better API than OpenGL/WebGL and can be more useful to you than Vulkan in some use cases:</p>
<ul>
<li>Validation errors are much better than in OpenGL/WebGL and not having global state helps a lot.</li>
<li>It’s also kind of similar to Vulkan in many things, so learning a bit of it before diving into Vulkan also helped me a lot.</li>
<li>It requires a lot less boilerplate to get things on the screen (compared to Vulkan).</li>
<li>You don’t have to deal with explicit synchronization which makes things much simpler.</li>
<li>You can make your games playable inside the browser.</li>
</ul>
<h2 id="learning-vulkan">Learning Vulkan</h2>
<p>Learning Vulkan seemed like an impossible thing for me previously. It felt like you needed to have many years of AAA game graphics programming experience to be able to do things in it. You also hear people saying “you’re basically writing a graphics driver when writing in Vulkan” which also made Vulkan sounds like an incredibly complicated thing.</p>
<p>I have also checked out some engines written in Vulkan before and was further demotivated by seeing tons of scary abstractions and files named like <code>GPUDevice.cpp</code> or <code>GPUAbstraction.cpp</code> which had thousands of lines of scary C++ code.</p>
<p>The situation has changed over the years. Vulkan is not as complicated as it was before. First of all, Khronos realized that some parts of Vulkan were indeed very complex and introduced some newer features which made many things much simpler (for example, dynamic rendering). Secondly, some very useful libraries which reduce boilerplate were implemented. And finally, there are a lot of fantastic resources which make learning Vulkan much easier than it was before.</p>
<p>The best Vulkan learning resource which helped me get started was <a href="https://vkguide.dev/">vkguide</a>. If you’re starting from scratch, just go through it all (you might stop at “GPU driver rendering” chapter at first - many simple games probably won’t need this level of complexity)</p>
<p><a href="https://www.youtube.com/playlist?list=PLmIqTlJ6KsE1Jx5HV4sd2jOe3V1KMHHgn">Vulkan Lecture Series by TU Wien</a> also nicely teaches Vulkan basics (you can probably skip “Real-Time Ray Tracing” chapter for now). I especially found a lecture on synchronization very helpful.</p>
<p>Here are some more advanced Vulkan books that also helped me:</p>
<ul>
<li><strong>3D Graphics Rendering Cookbook by Sergey Kosarevsky and Viktor Latypov</strong>. There is the second edition in the writing and it’s promising to be better than the first one. The second edition is not released yet, but the source code for it can be found here: <a href="https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook-Second-Edition">https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook-Second-Edition</a></li>
<li><strong>Mastering Graphics Programming with Vulkan by Marco Castorina, Gabriel Sassone</strong>. Very advanced book which explains some of the “cutting edge” graphics programming concepts (I mostly read it to understand where to go further, but didn’t have time to implement most of it). The source code for it can be found here: <a href="https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan">https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan</a></li>
</ul>
<p>Here’s the result of my first month of learning Vulkan:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/mtp_screenshot_one_month.png"/>
</figure>

<p>By this point I had:</p>
<ul>
<li>glTF model loading</li>
<li>Compute skinning</li>
<li>Frustum culling</li>
<li>Shadow mapping and cascaded shadow maps</li>
</ul>
<p>Of course, doing it for the 3rd time (I had it implemented it all in OpenGL and WebGPU before) certainly helped. Once you get to this point, Vulkan won’t seem as scary anymore.</p>
<p>Let’s see how the engine works and some useful things I learned.</p>
<h2 id="engine-overview-and-frame-analysis">Engine overview and frame analysis</h2>
<p><a href="https://github.com/eliasdaler/edbr">https://github.com/eliasdaler/edbr</a></p>
<p>My engine is called EDBR (Elias Daler’s Bikeshed Engine) and was initially started as a project for learning Vulkan. It quickly grew into a somewhat usable engine which I’m going to use for my further projects.</p>
<blockquote>
  <p>At the time of writing this article, the source code line counts are as follows:</p>
<ul>
<li>Engine itself: 19k lines of code
<ul>
<li>6.7k LoC related to graphics,</li>
<li>2k LoC are light abstractions around Vulkan</li>
</ul>
</li>
<li>3D cat game: 4.6k LoC</li>
<li>2D platformer game: 1.2k LoC</li>
</ul>

</blockquote>

<p>I copy-pasted some non-graphics related stuff from my previous engine (e.g. input handling and audio system) but all of the graphics and many other core systems were rewritten from scratch. I feel like it was a good way to do it instead of trying to cram Vulkan into my old OpenGL abstractions.</p>
<blockquote>
  You can follow the commit history which shows how I started from clearing the screen, drawing the first triangle, drawing a textured quad and so on. It might be easier to understand the engine when it was simpler and smaller.
</blockquote>

<p>Let’s see how this frame in rendered:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/final.jpg"/>
</figure>

<blockquote>
  Most of the steps will be explained in more detail below.
</blockquote>

<ul>
<li>Skinning</li>
</ul>
<p>First, models with skeletal animations are skinned in the compute shader. The compute shader takes unskinned mesh and produces a buffer of vertices which are then used instead of the original mesh in later rendering steps. This allows me to treat static and skinned meshes similarly in shaders and not do skinning repeatedly in different rendering steps.</p>
<ul>
<li>CSM (Cascaded Shadow Mapping)</li>
</ul>
<p>I use a 4096x4096 depth texture with 3 slices for cascaded shadow mapping. The first slice looks like this:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/csm_slice.png"/>
</figure>

<ul>
<li>Geometry + shading</li>
</ul>
<p>All the models are drawn and shading is calculated using the shadow map and light info. I use a PBR model which is almost identical to the one described in <a href="https://google.github.io/filament/Filament.md.html">Physically Based Rendering in Filament</a>. The fragment shader is quite big and does calculation for all the lights affecting the drawn mesh in one draw call:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/geometry.jpg"/>
</figure>

<p>Everything is drawn into a multi-sampled texture. Here’s how it looks after resolve:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/geometry_resolve.jpg"/>
</figure>

<p>(Open the previous two screenshots in the next tab and flip between the tabs to see the difference more clearly)</p>
<ul>
<li>Depth resolve</li>
</ul>
<p>Depth resolve step is performed manually via a fragment shader. I just go through all the fragments of multi-sample depth texture and write the minimum value into the non-MS depth texture (it’ll be useful in the next step).</p>
<ul>
<li>Post FX</li>
</ul>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/post_fx.jpg"/>
</figure>

<p>Some post FX is applied - right now it’s only depth fog (I use “depth resolve” texture from the previous step here), afterwards tone-mapping and bloom will also be done here.</p>
<ul>
<li>UI</li>
</ul>
<p>Dialogue UI is drawn. Everything is done in one draw call (more is explained in “Drawing many sprites” section)</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/final.jpg"/>
</figure>

<p>And that’s it! It’s pretty basic right now and would probably become much more complex in the future (see “Future work” section).</p>
<h2 id="general-advice">General advice</h2>
<h3 id="recommended-vulkan-libraries">Recommended Vulkan libraries</h3>
<p>There are a couple of libraries which greatly improve the experience of writing Vulkan. Most of them are already used in vkguide, but I still want to highlight how helpful they were to me.</p>
<ul>
<li>vk-bootstrap - <a href="https://github.com/charles-lunarg/vk-bootstrap">https://github.com/charles-lunarg/vk-bootstrap</a></li>
</ul>
<p>vk-bootstrap simplifies a lot of Vulkan boilerplate: physical device selection, swapchain creation and so on.</p>
<p>I don’t like big wrappers around graphic APIs because they tend to be very opinionated. Plus, you need to keep a mental map of “wrapper function vs function in the API spec” in your head at all times.</p>
<p>Thankfully, vk-bootstrap is not like this. It mostly affects the initialization step of your program and doesn’t attempt to be a wrapper around every Vulkan function.</p>
<blockquote>
  When I was learning Vulkan, I started doing Vulkan from scratch, without using any 3rd party libraries. Replacing big amounts of the initialization code with vk-bootstrap was a joy. It’s really worth it.
</blockquote>

<ul>
<li>Vulkan Memory Allocator (VMA) - <a href="https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator">https://github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator</a></li>
</ul>
<p>I’ll be honest, I used VMA without even learning about how to allocate memory in Vulkan manually. I read about it in the Vulkan spec later - I’m glad that I didn’t have to do it on my own.</p>
<ul>
<li>volk</li>
</ul>
<p>Volk was very useful for me for simplifying extension function loading. For example, if you want to use very useful <code>vkSetDebugUtilsObjectNameEXT</code> for setting debug names for your objects (useful for RenderDoc captures and validation errors), you’ll need to do this if you don’t use volk:</p>
<pre><code>// store this pointer somewhere
PFN_vkSetDebugUtilsObjectNameEXT pfnSetDebugUtilsObjectNameEXT;

// during your game init
pfnSetDebugUtilsObjectNameEXT = (PFN_vkSetDebugUtilsObjectNameEXT)
    vkGetInstanceProcAddr(instance, &#34;vkSetDebugUtilsObjectNameEXT&#34;);

// and finally in your game code
pfnSetDebugUtilsObjectNameEXT(device, ...);
</code></pre>
<p>With volk, all the extensions are immediately loaded after you call <code>volkInitialize</code> and you don’t need to store these pointers everywhere. You just include <code>volk.h</code> and call <code>vkSetDebugUtilsObjectNameEXT</code> - beautiful!</p>
<h3 id="gfxdevice-abstraction">GfxDevice abstraction</h3>
<p>I have a <code>GfxDevice</code> class which encapsulates most of the commonly used functionality and stores many objects that you need for calling Vulkan functions (<code>VkDevice</code>, <code>VkQueue</code> and so on). A single <code>GfxDevice</code> instance is created on the startup and then gets passed around.</p>
<p>It handles:</p>
<ul>
<li>Vulkan context initialization.</li>
<li>Swapchain creation and management.</li>
<li><code>beginFrame</code> returns a new <code>VkCommandBuffer</code> which is later used in all the drawing steps.</li>
<li><code>endFrame</code> does drawing to the swapchain and does sync between the frames.</li>
<li>Image creation and loading textures from files.</li>
<li>Buffer creation.</li>
<li>Bindless descriptor set management (see “Bindless descriptors” section below).</li>
</ul>
<p>That’s… a lot of things. However, it’s not that big: <code>GfxDevice.cpp</code> is only 714 lines at the time of writing this article. It’s more convenient to pass one object into the function instead of many (<code>VkDevice</code>, <code>VkQueue</code>, <code>VmaAllocator</code> and so on).</p>
<h3 id="handling-shaders">Handling shaders</h3>
<p>In Vulkan, you can use any shading language which compiles to SPIR-V - that means that you can use GLSL, HLSL and others. I chose GLSL because I already knew it from my OpenGL experience.</p>
<p>You can pre-compile your shaders during the build step or compile them on the fly. I do it during the build so that my shader loading runtime code is simpler. I also don’t have an additional runtime dependency on the shader compiler. Also, shader errors are detected during the build step and I don’t get compile errors during the runtime.</p>
<p>I use glslc (from <a href="https://github.com/google/shaderc">shaderc</a> project, it’s included in Vulkan SDK) which allows you to specify a <code>DEPFILE</code> in CMake which is incredibly useful when you use shader includes. If you change a shader file, all files which include it are recompiled automatically. Without the <code>DEPFILE</code>, CMake won’t be able to see which files shader files need to be recompiled and will only recompile the file which was changed.</p>
<p>My CMake script for building shaders looks like this:</p>
<pre><code>function (target_shaders target shaders)
    set(SHADERS_BUILD_DIR &#34;${CMAKE_CURRENT_BINARY_DIR}/shaders&#34;)
    file(MAKE_DIRECTORY &#34;${SHADERS_BUILD_DIR}&#34;)
    foreach (SHADER_PATH ${SHADERS})
        get_filename_component(SHADER_FILENAME &#34;${SHADER_PATH}&#34; NAME)
        set(SHADER_SPIRV_PATH &#34;${SHADERS_BUILD_DIR}/${SHADER_FILENAME}.spv&#34;)
        set(DEPFILE &#34;${SHADER_SPIRV_PATH}.d&#34;)
        add_custom_command(
          COMMENT &#34;Building ${SHADER_FILENAME}&#34;
          OUTPUT &#34;${SHADER_SPIRV_PATH}&#34;
          COMMAND ${GLSLC} &#34;${SHADER_PATH}&#34; -o &#34;${SHADER_SPIRV_PATH}&#34; -MD -MF ${DEPFILE} -g
          DEPENDS &#34;${SHADER_PATH}&#34;
          DEPFILE &#34;${DEPFILE}&#34;
        )
        list(APPEND SPIRV_BINARY_FILES ${SHADER_SPIRV_PATH})
    endforeach()

    set(shaders_target_name &#34;${target}_build_shaders&#34;)
    add_custom_target(${shaders_target_name}
      DEPENDS ${SPIRV_BINARY_FILES}
    )
    add_dependencies(${target} ${shaders_target_name})
endfunction()
</code></pre>
<p>and then in the main CMakeLists file:</p>
<pre><code>set(SHADERS
    skybox.frag
    skinning.comp
    ... // etc
)

# prepend shaders directory path
get_target_property(EDBR_SOURCE_DIR edbr SOURCE_DIR)
set(EDBR_SHADERS_DIR &#34;${EDBR_SOURCE_DIR}/src/shaders/&#34;)
list(TRANSFORM SHADERS PREPEND &#34;${EDBR_SHADERS_DIR}&#34;)

target_shaders(game ${SHADERS})
</code></pre>
<p>Now, when you build a <code>game</code> target, shaders get built automatically and the resulting SPIR-V files are put into the binary directory.</p>
<h3 id="push-constants-descriptor-sets-and-bindless-descriptors">Push constants, descriptor sets and bindless descriptors</h3>
<p>Passing data to shaders in OpenGL is much simpler than it is in Vulkan. In OpenGL, you could just do this:</p>
<p>In shader:</p>
<pre><code>uniform float someFloat;
</code></pre>
<p>In C++ code:</p>
<pre><code>const auto loc = glGetUniformLocation(shader, &#34;someFloat&#34;);
glUseProgram(shader);
glUniform1f(loc, 42.f);
</code></pre>
<p>You can also use <a href="https://www.khronos.org/opengl/wiki/Uniform_(GLSL)/Explicit_Uniform_Location">explicit uniform location</a> like this.</p>
<p>In shader:</p>
<pre><code>layout(location = 20) uniform float someFloat;
</code></pre>
<p>In code:</p>
<pre><code>const auto loc = 20;
glUniform1f(loc, 42.f);
</code></pre>
<p>In Vulkan, you need to group your uniforms into “descriptor sets”:</p>
<pre><code>// set 0
layout (set = 0, binding = 0) uniform float someFloat;
layout (set = 0, binding = 1) uniform mat4 someMatrix;
// set 1
layout (set = 1, binding = 0) uniform float someOtherFloat;
... // etc.
</code></pre>
<p>Now, this makes things a lot more complicated, because you need to specify descriptor set layout beforehand, use descriptor set pools and allocate descriptor sets with them, do the whole <code>VkWriteDescriptorSet</code> + <code>vkUpdateDescriptorSets</code> thing, call <code>vkCmdBindDescriptorSets</code> for each descriptor set and so on.</p>
<p>I’ll explain later how I avoided using descriptor sets by using bindless descriptors and buffer device access. Basically, I only have one “global” descriptor set for bindless textures and samplers, and that’s it. Everything else is passed via push constants which makes everything much easier to handle.</p>
<h3 id="pipeline-pattern">Pipeline pattern</h3>
<p>I separate drawing steps into “pipeline” classes.</p>
<p>Most of them look like this:</p>
<pre><code>class PostFXPipeline {
public:
    void init(GfxDevice&amp; gfxDevice, VkFormat drawImageFormat);
    void cleanup(VkDevice device);

    void draw(
        VkCommandBuffer cmd,
        GfxDevice&amp; gfxDevice,
        const GPUImage&amp; drawImage,
        const GPUImage&amp; depthImage,
        const GPUBuffer&amp; sceneDataBuffer);

private:
    VkPipelineLayout pipelineLayout;
    VkPipeline pipeline;

    struct PushConstants {
        VkDeviceAddress sceneDataBuffer;
        std::uint32_t drawImageId;
        std::uint32_t depthImageId;
    };
};
</code></pre>
<ul>
<li><code>init</code> loads needed shaders and initializes <code>pipeline</code> and <code>pipelineLayout</code>:</li>
</ul>
<pre><code>void PostFXPipeline::init(GfxDevice&amp; gfxDevice, VkFormat drawImageFormat)
{
    const auto&amp; device = gfxDevice.getDevice();

    const auto pcRange = VkPushConstantRange{
        .stageFlags = VK_SHADER_STAGE_FRAGMENT_BIT,
        .offset = 0,
        .size = sizeof(PushConstants),
    };

    const auto layouts = std::array{gfxDevice.getBindlessDescSetLayout()};
    const auto pushConstantRanges = std::array{pcRange};
    pipelineLayout = vkutil::createPipelineLayout(device, layouts, pushConstantRanges);

    const auto vertexShader =
        vkutil::loadShaderModule(&#34;shaders/fullscreen_triangle.vert.spv&#34;, device);
    const auto fragShader =
        vkutil::loadShaderModule(&#34;shaders/postfx.frag.spv&#34;, device);
    pipeline = PipelineBuilder{pipelineLayout}
                   .setShaders(vertexShader, fragShader)
                   .setInputTopology(VK_PRIMITIVE_TOPOLOGY_TRIANGLE_LIST)
                   .setPolygonMode(VK_POLYGON_MODE_FILL)
                   .disableCulling()
                   .setMultisamplingNone()
                   .disableBlending()
                   .setColorAttachmentFormat(drawImageFormat)
                   .disableDepthTest()
                   .build(device);
    vkutil::addDebugLabel(device, pipeline, &#34;postFX pipeline&#34;);

    vkDestroyShaderModule(device, vertexShader, nullptr);
    vkDestroyShaderModule(device, fragShader, nullptr);
}
</code></pre>
<p>The <code>init</code> function is usually called once during the engine initialization. <code>PipelineBuilder</code> abstraction is described in vkguide <a href="https://vkguide.dev/docs/new_chapter_3/building_pipeline/">here</a>. I modified it a bit to use the Builder pattern to be able to chain the calls.</p>
<ul>
<li><code>cleanup</code> does all the needed cleanup. It usually simply destroys the pipeline and its layout:</li>
</ul>
<pre><code>void PostFXPipeline::cleanup(VkDevice device)
{
    vkDestroyPipeline(device, pipeline, nullptr);
    vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
}
</code></pre>
<ul>
<li><code>draw</code> is called each frame and all the needed inputs are passed as arguments. It’s assumed that the sync is performed outside of the <code>draw</code> call (see “Synchronization” section below). Some pipelines are only called once per frame - some either take <code>std::vector</code> of objects to draw or are called like this:</li>
</ul>
<pre><code>for (const auto&amp; mesh : meshes) {
    somePipeline.draw(cmd, gfxDevice, mesh, ...);
}
</code></pre>
<p>The typical <code>draw</code> function looks like this:</p>
<pre><code>void PostFXPipeline::draw(
    VkCommandBuffer cmd,
    GfxDevice&amp; gfxDevice,
    const GPUImage&amp; drawImage,
    const GPUImage&amp; depthImage,
    const GPUBuffer&amp; sceneDataBuffer)
{
    // Bind the pipeline
    vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_GRAPHICS, pipeline);

    // Bind the bindless descriptor set
    gfxDevice.bindBindlessDescSet(cmd, pipelineLayout);

    // Handle push constants
    const auto pcs = PushConstants{
        // BDA - explained below
        .sceneDataBuffer = sceneDataBuffer.address,
        // bindless texture ids - no need for desc. sets!
        // explained below
        .drawImageId = drawImage.getBindlessId(),
        .depthImageId = depthImage.getBindlessId(),
    };
    vkCmdPushConstants(
        cmd, pipelineLayout, VK_SHADER_STAGE_FRAGMENT_BIT, 0, sizeof(PushConstants), &amp;pcs);

    // Finally, do some drawing. Here we&#39;re drawing a fullscreen triangle
    // to do a full-screen effect.
    vkCmdDraw(cmd, 3, 1, 0, 0);
}
</code></pre>
<p>Note another thing: it’s assumed that <code>draw</code> is called between <code>vkCmdBeginRendering</code> and <code>vkCmdEndRendering</code> - the render pass itself doesn’t care what texture it renders to - the caller of <code>draw</code> is responsible for that. It makes things simpler and allows you to do several draws to the same render target, e.g.:</p>
<pre><code>// handy wrapper for creating VkRenderingInfo
const auto renderInfo = vkutil::createRenderingInfo({
    .renderExtent = drawImage.getExtent2D(),
    .colorImageView = drawImage.imageView,
    .colorImageClearValue = glm::vec4{0.f, 0.f, 0.f, 1.f},
    .depthImageView = depthImage.imageView,
    .depthImageClearValue = 0.f,
    // for MSAA
    .resolveImageView = resolveImage.imageView,
});

vkCmdBeginRendering(cmd, &amp;renderInfo.renderingInfo);

// draw meshes
for (const auto&amp; mesh : meshesToDraw) {
    meshPipeline.draw(cmd, gfxDevice, mesh, ...);
}
// draw sky
skyboxPipeline.draw(cmd, gfxDevice, camera);

vkCmdEndRendering(cmd);
</code></pre>
<blockquote>
  I use <code>VK_KHR_dynamic_rendering</code> everywhere. I don’t use Vulkan render passes and subpasses at all. I’ve heard that they’re more efficient on tile-based GPUs, but I don’t care about mobile support for now. <code>VK_KHR_dynamic_rendering</code> just makes everything much easier.
</blockquote>

<h3 id="using-programmable-vertex-pulling-pvp--buffer-device-address-bda">Using programmable vertex pulling (PVP) + buffer device address (BDA)</h3>
<p>I have one vertex type for all the meshes. It looks like this:</p>
<pre><code>struct Vertex {
    vec3 position;
    float uv_x;
    vec3 normal;
    float uv_y;
    vec4 tangent;
};
</code></pre>
<blockquote>
  Of course, you can greatly optimize it using various methods, but it’s good enough for me for now. The <code>uv_x</code>/<code>uv_y</code> separation comes from vkguide - I think it’s a nice idea to get good alignment and not waste any bytes
</blockquote>

<p>The vertices are accessed in the shader like this:</p>
<pre><code>layout (buffer_reference, std430) readonly buffer VertexBuffer {
    Vertex vertices[];
};

layout (push_constant, scalar) uniform constants
{
    VertexBuffer vertexBuffer;
    ... // other stuff
} pcs;

void main()
{
    Vertex v = pcs.vertexBuffer.vertices[gl_VertexIndex];
    ...
}
</code></pre>
<p>PVP frees you from having to define vertex format (no more VAOs like in OpenGL or <code>VkVertexInputBindingDescription</code> + <code>VkVertexInputAttributeDescription</code> in Vulkan). BDA also frees you from having to bind a buffer to a descriptor set - you just pass an address to your buffer which contains vertices in push constants and that’s it.</p>
<blockquote>
  Also note the <code>scalar</code> layout for push constants. I use it for all the buffers too. Compared to “std430” layout, it makes alignment a lot more easy to handle - it almost works the same as in C++ and greatly reduces the need for “padding” members in C++ structs.
</blockquote>

<h3 id="bindless-descriptors">Bindless descriptors</h3>
<p>Textures were painful to work with even in OpenGL - you had “texture slots” which were awkward to work with. You couldn’t just sample any texture from the shader if it wasn’t bound to a texture slot beforehand. <code>ARB_bindless_texture</code> changed that and made many things easier.</p>
<p>Vulkan doesn’t have the exact same functionality, but it has something similar. You can create big descriptor sets which look like this:</p>
<pre><code>// bindless.glsl
layout (set = 0, binding = 0) uniform texture2D textures[];
...
layout (set = 0, binding = 1) uniform sampler samplers[];
</code></pre>
<p>You’ll need to maintain a list of all your textures using some “image manager” and when a new texture is loaded, you need to insert it into the <code>textures</code> array. The index at which you inserted it becomes a bindless “texture id” which then can be used to sample it in shaders. Now you can pass these ids in your push constants like this:</p>
<pre><code>layout (push_constant, scalar) uniform constants
{
  uint textureId;
  ...
} pcs;
</code></pre>
<p>and then you can sample your texture in the fragment shader like this:</p>
<pre><code>// bindless.glsl
#define NEAREST_SAMPLER_ID 0
...

vec4 sampleTexture2DNearest(uint texID, vec2 uv) {
    return texture(nonuniformEXT(sampler2D(textures[texID], samplers[NEAREST_SAMPLER_ID])), uv);
}

// shader.frag
vec4 color = sampleTexture2DNearest(pcs.textureId, inUV);
</code></pre>
<p>Two things to note:</p>
<ol>
<li>I chose separate image samplers so that I could sample any texture using different samplers. Common samplers (nearest, linear with anisotropy, depth texture samplers) are created and put into <code>samplers</code> array on the startup.</li>
<li>The wrapper function makes the process of sampling a lot more convenient.</li>
</ol>
<blockquote>
  The placement of <code>nonuniformEXT</code> is somewhat tricky and is explained very well <a href="https://github.com/KhronosGroup/Vulkan-Samples/blob/ada3895613b90fe21cde2718dbbfc221b27c575e/shaders/descriptor_indexing/glsl/nonuniform-quads.frag#L31">here</a>.
</blockquote>

<p>I use bindless ids for the mesh material buffer which looks like this:</p>
<pre><code>struct MaterialData {
    vec4 baseColor;
    vec4 metallicRoughnessEmissive;
    uint diffuseTex;
    uint normalTex;
    uint metallicRoughnessTex;
    uint emissiveTex;
};

layout (buffer_reference, std430) readonly buffer MaterialsBuffer {
    MaterialData data[];
} materialsBuffer;
</code></pre>
<p>Now I can only pass material ID in my push constants and then sample texture like this in the fragment shader:</p>
<pre><code>MaterialData material = materials[pcs.materialID];
vec4 diffuse = sampleTexture2DLinear(material.diffuseTex, inUV);
...
</code></pre>
<p>Neat! No more bulky descriptor sets, just one int per material in the push constants.</p>
<p>You can also put different texture types into the same set like this (this is needed for being able to access textures of types other than <code>texture2D</code>):</p>
<pre><code>layout (set = 0, binding = 0) uniform texture2D textures[];
layout (set = 0, binding = 0) uniform texture2DMS texturesMS[];
layout (set = 0, binding = 0) uniform textureCube textureCubes[];
layout (set = 0, binding = 0) uniform texture2DArray textureArrays[];
</code></pre>
<p>And here’s how you can sample <code>textureCube</code> with a linear sampler (note that we use <code>textureCubes</code> here instead of <code>textures</code>):</p>
<pre><code>vec4 sampleTextureCubeLinear(uint texID, vec3 p) {
    return texture(nonuniformEXT(samplerCube(textureCubes[texID], samplers[NEAREST_SAMPLER_ID])), p);
}
</code></pre>
<p>Here’s a very good article on using bindless textures in Vulkan:</p>
<p><a href="https://jorenjoestar.github.io/post/vulkan_bindless_texture/">https://jorenjoestar.github.io/post/vulkan_bindless_texture/</a></p>
<h3 id="handling-dynamic-data-which-needs-to-be-uploaded-every-frame">Handling dynamic data which needs to be uploaded every frame</h3>
<p>I find it useful to pre-allocate big arrays of things and push stuff to them in every frame.
Basically, you can pre-allocate an array of N structs (or matrices) and then start at index 0 at each new frame and push things to it from the CPU. Then, you can access all these items in your shaders. For example, I have all joint matrices stored in one big <code>mat4</code> array and the skinning compute shader accesses joint matrices of a particular mesh using start index passed via push constants (more about it will be explained later).</p>
<p>Here are two ways of doing this:</p>
<ul>
<li>
<ol>
<li>Have N buffers on GPU and swap between them.</li>
</ol>
</li>
</ul>
<p>vkguide explains the concept of “in flight” frames pretty well. To handle this parallelism properly, you need to have one buffer for the “currently drawing” frame and one buffer for “currently recording new drawing commands” frame to not have races. (If you have more frames in flight, you’ll need to allocate more than 2 buffers)</p>
<p>This means that you need to preallocate 2 buffers on GPU. You write data from CPU to GPU to the first buffer during the first frame. While you record the second frame, GPU reads from the first buffer while you write new data to the second buffer. On the third frame, GPU reads from the second buffer and you write new info to the first buffer… and so on.</p>
<ul>
<li>
<ol start="2">
<li>One buffer on GPU and N “staging” buffers on CPU</li>
</ol>
</li>
</ul>
<p>This might be useful if you need to conserve some memory on the GPU.</p>
<p>Let’s see how it works in my engine:</p>
<pre><code>class NBuffer {
public:
    void init(
        GfxDevice&amp; gfxDevice,
        VkBufferUsageFlags usage,
        std::size_t dataSize,
        std::size_t numFramesInFlight,
        const char* label);

    void cleanup(GfxDevice&amp; gfxDevice);

    void uploadNewData(
        VkCommandBuffer cmd,
        std::size_t frameIndex,
        void* newData,
        std::size_t dataSize,
        std::size_t offset = 0);

    const GPUBuffer&amp; getBuffer() const { return gpuBuffer; }

private:
    std::size_t framesInFlight{0};
    std::size_t gpuBufferSize{0};
    std::vector&lt;GPUBuffer&gt; stagingBuffers;
    GPUBuffer gpuBuffer;
    bool initialized{false};
};

void NBuffer::init(
    GfxDevice&amp; gfxDevice,
    VkBufferUsageFlags usage,
    std::size_t dataSize,
    std::size_t numFramesInFlight,
    const char* label)
{
    ...

    gpuBuffer = gfxDevice.createBuffer(
        dataSize, usage | VK_IMAGE_USAGE_TRANSFER_DST_BIT, VMA_MEMORY_USAGE_AUTO_PREFER_DEVICE);
    vkutil::addDebugLabel(gfxDevice.getDevice(), gpuBuffer.buffer, label);

    for (std::size_t i = 0; i &lt; numFramesInFlight; ++i) {
        stagingBuffers.push_back(gfxDevice.createBuffer(
            dataSize, usage | VK_BUFFER_USAGE_TRANSFER_SRC_BIT, VMA_MEMORY_USAGE_AUTO_PREFER_HOST));
    }

    ...
}
</code></pre>
<p>Note how staging buffers are created using VMA’s <code>PREFER_HOST</code> flag and the “main” buffer from which we read in the shader is using the <code>PREFER_DEVICE</code> flag.</p>
<p>Here’s how new data is uploaded (<a href="https://github.com/eliasdaler/edbr/blob/bf09366f9ad7d82e2cba01e66518c185af55fd5b/edbr/src/Graphics/NBuffer.cpp#L49">full implementation</a>):</p>
<pre><code>void NBuffer::uploadNewData(
    VkCommandBuffer cmd,
    std::size_t frameIndex,
    void* newData,
    std::size_t dataSize,
    std::size_t offset) const
{
    assert(initialized);
    assert(frameIndex &lt; framesInFlight);
    assert(offset + dataSize &lt;= gpuBufferSize &amp;&amp; &#34;NBuffer::uploadNewData: out of bounds write&#34;);

    if (dataSize == 0) {
        return;
    }

    // sync with previous read
    ... // READ BARRIER CODE HERE

    auto&amp; staging = stagingBuffers[frameIndex];
    auto* mappedData = reinterpret_cast&lt;std::uint8_t*&gt;(staging.info.pMappedData);
    memcpy((void*)&amp;mappedData[offset], newData, dataSize);

    const auto region = VkBufferCopy2{
        .sType = VK_STRUCTURE_TYPE_BUFFER_COPY_2,
        .srcOffset = (VkDeviceSize)offset,
        .dstOffset = (VkDeviceSize)offset,
        .size = dataSize,
    };
    const auto bufCopyInfo = VkCopyBufferInfo2{
        .sType = VK_STRUCTURE_TYPE_COPY_BUFFER_INFO_2,
        .srcBuffer = staging.buffer,
        .dstBuffer = gpuBuffer.buffer,
        .regionCount = 1,
        .pRegions = &amp;region,
    };

    vkCmdCopyBuffer2(cmd, &amp;bufCopyInfo);

    // sync with write
    ... // WRITE BARRIER CODE HERE
}
</code></pre>
<p>I’d go with the first approach for most cases (more data on GPU, but no need for manual sync) unless you need to conserve GPU memory for some reason. I’ve found no noticeable difference in performance between two approaches, but it might matter if you are uploading huge amounts of data to GPU on each frame.</p>
<h3 id="destructors-deletion-queue-and-cleanup">Destructors, deletion queue and cleanup</h3>
<p>Now, this might be somewhat controversial… but I didn’t find much use of the deletion queue pattern used in vkguide. I don’t really need to allocated/destroy new objects on every frame.</p>
<p>Using C++ destructors for Vulkan object cleanup is not very convenient either. You need to wrap everything in custom classes, add move constructors and move <code>operator=</code>… It adds an additional layer of complexity.</p>
<p>In most cases, the cleanup of Vulkan objects happens in one place - and you don’t want to accidentally destroy some in-use object mid-frame by accidentally destroying some wrapper object.</p>
<p>It’s also harder to manage lifetimes when you have cleanup in happening in the destructor. For example, suppose you have a case like this:</p>
<pre><code>struct SomeClass {
    SomeOtherClass b;

    void init() {
        ...
    }

    void cleanup() {
        ...
    }
}
</code></pre>
<p>If you want to cleanup <code>SomeOtherClass</code> resources (e.g. the instance of <code>SomeOtherClass</code> has a <code>VkPipeline</code> object) during <code>SomeClass::cleanup</code>, you can’t do that if the cleanup of <code>SomeOtherClass</code> is performed in its destructor.</p>
<p>Of course, you can do this:</p>
<pre><code>struct SomeClass {
    std::unique_ptr&lt;SomeOtherClass&gt; b;

    void init() {
        b = std::make_unique&lt;SomeOtherClass&gt;();
        ...
    }

    void cleanup() {
        b.reset();
        ...
    }
}
</code></pre>
<p>… but I don’t like how it introduces a dynamic allocation and requires you to do write more code (and it’s not that much different from calling a <code>cleanup</code> function manually).</p>
<p>Right now, I prefer to clean up stuff directly, e.g.</p>
<pre><code>class SkyboxPipeline {
public:
    void cleanup(VkDevice device) {
        vkDestroyPipeline(device, pipeline, nullptr);
        vkDestroyPipelineLayout(device, pipelineLayout, nullptr);
    }

private:
    VkPipelineLayout pipelineLayout;
    VkPipeline pipeline;
    ...
}

// in GameRenderer.cpp:
void GameRenderer::cleanup(VkDevice device) {
    ...
    skyboxPipeline.cleanup(device);
    ...
}
</code></pre>
<p>This approach is not perfect - first of all, it’s easy to forget to call <code>cleanup</code> function, This is not a huge problem since you get a validation error in case you forget to cleanup some Vulkan resources on shutdown:</p>
<pre><code>Validation Error: [ VUID-vkDestroyDevice-device-05137 ] Object 0: handle = 0x4256c1000000005d, type = VK_OBJECT_TYPE_PIPELINE_LAYOUT; | MessageID = 0x4872eaa0 | vkCreateDevice():  OBJ ERROR : For VkDevice 0x27bd530[], VkPipelineLayout 0x4256c1000000005d[] has not been destroyed. The Vulkan spec states: All child objects created on device must have been destroyed prior to destroying device (https://vulkan.lunarg.com/doc/view/1.3.280.1/linux/1.3-extensions/vkspec.html#VUID-vkDestroyDevice-device-05137)
</code></pre>
<p>VMA also triggers asserts if you forget to free some buffer/image allocated with it.</p>
<p>I find it convenient to have all the Vulkan cleanup happening <em>explicitly</em> in one place. It makes it easy to track when the objects get destroyed.</p>
<h3 id="synchronization">Synchronization</h3>
<p>Synchronization in Vulkan is difficult. OpenGL and WebGPU do it for you - if you read from some texture/buffer, you know that it will have the correct data and you won’t get problems with data races. With Vulkan, you need to be explicit and this is usually where things tend to get complicated.</p>
<p>Right now I manage most of the complexities of sync manually in one place. I separate my drawing into “passes”/pipelines (as described above) and then insert barriers between them. For example, the skinning pass writes new vertex data into GPU memory. Shadow mapping pass reads this data to render skinned meshes into the shadow map. Sync in my code looks like this:</p>
<pre><code>// do skinning in compute shader
for (const auto&amp; mesh : skinnedMeshes) {
    skinningPass.doSkinning(gfxDevice, mesh);
}

{
    // Sync skinning with CSM
    // This is a &#34;fat&#34; barrier and you can potentially optimize it
    // by specifying all the buffers that the next pass will read from
    const auto memoryBarrier = VkMemoryBarrier2{
        .sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER_2,
        .srcStageMask = VK_PIPELINE_STAGE_2_COMPUTE_SHADER_BIT,
        .srcAccessMask = VK_ACCESS_2_SHADER_WRITE_BIT,
        .dstStageMask = VK_PIPELINE_STAGE_2_VERTEX_SHADER_BIT,
        .dstAccessMask = VK_ACCESS_2_MEMORY_READ_BIT,
    };
    const auto dependencyInfo = VkDependencyInfo{
        .sType = VK_STRUCTURE_TYPE_DEPENDENCY_INFO,
        .memoryBarrierCount = 1,
        .pMemoryBarriers = &amp;memoryBarrier,
    };
    vkCmdPipelineBarrier2(cmd, &amp;dependencyInfo);
}

// do shadow mapping
shadowMappingPass.draw(gfxDevice, ...);
</code></pre>
<p>Of course, this can be automated/simplified using render graphs. This is something that I might implement in the future. Right now I’m okay with doing manual sync. vkconfig’s “synchronization” validation layer also helps greatly in finding sync errors.</p>
<p>The following resources were useful for understanding synchronization:</p>
<ul>
<li>
<p><a href="https://themaister.net/blog/2019/08/14/yet-another-blog-explaining-vulkan-synchronization/">https://themaister.net/blog/2019/08/14/yet-another-blog-explaining-vulkan-synchronization/</a></p>
</li>
<li>
<p><a href="https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples">https://github.com/KhronosGroup/Vulkan-Docs/wiki/Synchronization-Examples</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=GiKbGWI4M-Y">Vulkan Lecture Series Ep. 7 on Vulkan Sync by TU Wien</a></p>
</li>
</ul>
<h2 id="more-implementation-notes">More implementation notes</h2>
<h3 id="drawing-many-sprites">Drawing many sprites</h3>
<p>With bindless textures, it’s easy to draw many sprites using one draw call without having to allocate vertex buffers at all.</p>
<p>First of all, you can emit vertex coordinates and UVs using <code>gl_VertexIndex</code> in your vertex shader like this:</p>
<pre><code>void main()
{
    uint b = 1 &lt;&lt; (gl_VertexIndex % 6);
    vec2 baseCoord = vec2((0x1C &amp; b) != 0, (0xE &amp; b) != 0);
    ...
}
</code></pre>
<p>This snippet produces this set of values:</p>
<table>
<thead>
<tr>
<th>gl_VertexIndex</th>
<th>baseCoord</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>(0,0)</td>
</tr>
<tr>
<td>1</td>
<td>(0,1)</td>
</tr>
<tr>
<td>2</td>
<td>(1,1)</td>
</tr>
<tr>
<td>3</td>
<td>(1,1)</td>
</tr>
<tr>
<td>4</td>
<td>(1,0)</td>
</tr>
<tr>
<td>5</td>
<td>(0,0)</td>
</tr>
</tbody>
</table>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/quad.png" alt="Two triangles form a quad"/><figcaption>
            <p>Two triangles form a quad</p>
        </figcaption>
</figure>

<p>All the sprite draw calls are combined into <code>SpriteDrawBuffer</code> which looks like this in GLSL:</p>
<pre><code>struct SpriteDrawCommand {
    mat4 transform; // could potentially be mat2x2...
    vec2 uv0; // top-left uv coord
    vec2 uv1; // bottom-right uv coord
    vec4 color; // color by which texture is multiplied
    uint textureID; // sprite texture
    uint shaderID; // explained below
    vec2 padding; // padding to satisfy &#34;scalar&#34; requirements
};

layout (buffer_reference, scalar) readonly buffer SpriteDrawBuffer {
    SpriteDrawCommand commands[];
};
</code></pre>
<p>On CPU/C++ side, it looks almost the same:</p>
<pre><code>struct SpriteDrawCommand {
    glm::mat4 transform;
    glm::vec2 uv0; // top-left uv coordinate
    glm::vec2 uv1; // bottom-right uv coodinate
    LinearColor color; // color by which texture is multiplied by
    std::uint32_t textureId; // sprite texture
    std::uint32_t shaderId; // explained below
    glm::vec2 padding; // padding
};

std::vector&lt;SpriteDrawCommand&gt; spriteDrawCommands;
</code></pre>
<p>I create two fixed size buffers on the GPU and then upload the contents of <code>spriteDrawCommands</code> (using techniques described above in the “Handling dynamic data” section).</p>
<p>The sprite renderer is used like this:</p>
<pre><code>// record commands
renderer.beginDrawing();
{
    renderer.drawSprite(sprite, pos);
    renderer.drawText(font, &#34;Hello&#34;);
    renderer.drawRect(...);
}
renderer.endDrawing();

// do actual drawing later:
renderer.draw(cmd, gfxDevice, ...);
</code></pre>
<blockquote>
  The same renderer also draws text, rectangles and lines in my engine. For example, the text is just N “draw sprite” commands for a string composed of N glyphs. Solid color rectangles and lines are achieved by using a 1x1 pixel white texture and multiplying it by <code>SpriteCommand::color</code> in the fragment shader.
</blockquote>

<p>And finally, here’s how the command to do the drawing looks like inside <code>SpriteRenderer::draw</code>:</p>
<pre><code>vkCmdDraw(cmd, 6, spriteDrawCommands.size(), 0, 0);
// 6 vertices per instance, spriteDrawCommands.size() instances in total
</code></pre>
<p>The complete sprite.vert looks like this:</p>
<pre><code>#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_buffer_reference : require

#include &#34;sprite_commands.glsl&#34;

layout (push_constant) uniform constants
{
    mat4 viewProj; // 2D camera matrix
    SpriteDrawBuffer drawBuffer; // where sprite draw commands are stored
} pcs;

layout (location = 0) out vec2 outUV;
layout (location = 1) out vec4 outColor;
layout (location = 2) flat out uint textureID;
layout (location = 3) flat out uint shaderID;

void main()
{
    uint b = 1 &lt;&lt; (gl_VertexIndex % 6);
    vec2 baseCoord = vec2((0x1C &amp; b) != 0, (0xE &amp; b) != 0);

    SpriteDrawCommand command = pcs.drawBuffer.commands[gl_InstanceIndex];

    gl_Position = pcs.viewProj * command.transform * vec4(baseCoord, 0.f, 1.f);
    outUV = (1.f - baseCoord) * command.uv0 + baseCoord * command.uv1;
    outColor = command.color;
    textureID = command.textureID;
    shaderID = command.shaderID;
}
</code></pre>
<p>All the parameters of the sprite draw command are self-explanatory, but <code>shaderID</code> needs a bit of clarification. Currently, I use it to branch inside the fragment shader:</p>
<pre><code>...

#define SPRITE_SHADER_ID 0
#define TEXT_SHADER_ID   1

void main()
{
    vec4 texColor = sampleTexture2DNearest(textureID, inUV);

    // text drawing is performed differently...
    if (shaderID == TEXT_SHADER_ID) {
        // glyph atlas uses single-channel texture
        texColor = vec4(1.0, 1.0, 1.0, texColor.r);
    }

    if (texColor.a &lt; 0.1) {
        discard;
    }

    outColor = inColor * texColor;
}
</code></pre>
<p>This allows me to draw sprites differently depending on this ID without having to change pipelines. Of course, it can be potentially bad for the performance. This can be improved by drawing sprites with the same shader ID in batches. You’ll only need to switch pipelines when you encounter a draw command with a different shader ID.</p>
<p>The sprite renderer is very efficient: it can draw 10 thousand sprites in just 315 microseconds.
</p><figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/many_sprites.jpg"/>
</figure>

<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/many_sprites_prof.png"/>
</figure>

<h3 id="compute-skinning">Compute skinning</h3>
<p>I do skinning for skeletal animation in a compute shader. This allows me to have the same vertex format for all the meshes.</p>
<p>Basically, I just take the mesh’s vertices (not skinned) and joint matrices and produce a new buffer of vertices which are used in later rendering stages.</p>
<p>Suppose you spawn three cats with identical meshes:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/mtp_screenshot_one_month.png"/>
</figure>

<p>All three of them can have different animations. They all have an identical “input” mesh. But the “output” vertex buffer will differ between them, which means that you need to pre-allocate a vertex buffer for each instance of the mesh.</p>
<p>Here’s how the skinning compute shader looks like:</p>
<pre><code>#version 460

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_buffer_reference : require

#include &#34;vertex.glsl&#34;

struct SkinningDataType {
    ivec4 jointIds;
    vec4 weights;
};

layout (buffer_reference, std430) readonly buffer SkinningData {
    SkinningDataType data[];
};

layout (buffer_reference, std430) readonly buffer JointMatrices {
    mat4 matrices[];
};

layout (push_constant) uniform constants
{
    JointMatrices jointMatrices;
    uint jointMatricesStartIndex;
    uint numVertices;
    VertexBuffer inputBuffer;
    SkinningData skinningData;
    VertexBuffer outputBuffer;
} pcs;

layout (local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

mat4 getJointMatrix(int jointId) {
    return pcs.jointMatrices.matrices[pcs.jointMatricesStartIndex + jointId];
}

void main()
{
    uint index = gl_GlobalInvocationID.x;
    if (index &gt;= pcs.numVertices) {
        return;
    }

    SkinningDataType sd = pcs.skinningData.data[index];
    mat4 skinMatrix =
        sd.weights.x * getJointMatrix(sd.jointIds.x) +
        sd.weights.y * getJointMatrix(sd.jointIds.y) +
        sd.weights.z * getJointMatrix(sd.jointIds.z) +
        sd.weights.w * getJointMatrix(sd.jointIds.w);

    Vertex v = pcs.inputBuffer.vertices[index];
    v.position = vec3(skinMatrix * vec4(v.position, 1.0));

    pcs.outputBuffer.vertices[index] = v;
}
</code></pre>
<ol>
<li>I store all joint matrices in a big array and populate it every frame (and also pass the starting index in the array for each skinned mesh, <code>jointMatricesStartIndex</code>).</li>
<li>Skinning data is not stored inside each mesh vertex, a separate buffer of <code>num_vertices</code> elements is used.</li>
</ol>
<p>After the skinning is performed, all the later rendering stages use this set of vertices Thee rendering process for static and skinned meshes becomes identical, thanks to that.</p>
<blockquote>
  <a href="https://capnramses.itch.io/antons-opengl-4-tutorials">Anton’s OpenGL 4 Tutorials</a> book has the best skinning implementation guide I’ve ever read. Game Engine Architecture by Jason Gregory has nice explanations about skinning/skeletal animation math as well.
</blockquote>

<h3 id="game--renderer-separation">Game / renderer separation</h3>
<p>I have a  game/renderer separation which uses a simple concept of “draw commands”. In the game logic, I use <a href="https://github.com/skypjack/entt">entt</a>, but the renderer doesn’t know anything about entities or “game objects”. It only knows about the lights, some scene parameters (like fog, which skybox texture to use etc) and meshes it needs to draw.</p>
<p>The renderer’s API looks like this in action:</p>
<pre><code>void Game::generateDrawList()
{
    renderer.beginDrawing();

    // Add lights
    const auto lights = ...; // get list of all active lights
    for (const auto&amp;&amp; [e, tc, lc] : lights.each()) {
        renderer.addLight(lc.light, tc.transform);
    }

    // Render static meshes
    const auto staticMeshes = ...; // list of entities with static meshes
    for (const auto&amp;&amp; [e, tc, mc] : staticMeshes.each()) {
        // Each &#34;mesh&#34; can have multiple submeshes similar to how
        // glTF separates each &#34;mesh&#34; into &#34;primitives&#34;.
        for (std::size_t i = 0; i &lt; mc.meshes.size(); ++i) {
            renderer.drawMesh(mc.meshes[i], tc.worldTransform, mc.castShadow);
        }
    }

    // Render meshes with skeletal animation
    const auto skinnedMeshes = ...; // list of entities with skeletal animations
    for (const auto&amp;&amp; [e, tc, mc, sc] : skinnedMeshes.each()) {
        renderer.drawSkinnedMesh(
            mc.meshes, sc.skinnedMeshes, tc.worldTransform,
            sc.skeletonAnimator.getJointMatrices());
    }

    renderer.endDrawing();
}
</code></pre>
<p>When you call <code>drawMesh</code> or <code>drawSkinnedMesh</code>, the renderer creates a mesh draw command and puts it in <code>std::vector&lt;MeshDrawCommand&gt;</code> which are then iterated through during the drawing process. The <code>MeshDrawCommand</code> looks like this:</p>
<pre><code>
struct SkinnedMesh {
    GPUBuffer skinnedVertexBuffer;
};

struct MeshDrawCommand {
    MeshId meshId;
    glm::mat4 transformMatrix;
    math::Sphere worldBoundingSphere;

    const SkinnedMesh* skinnedMesh{nullptr};
    std::uint32_t jointMatricesStartIndex;
    bool castShadow{true};
};
</code></pre>
<ul>
<li><code>meshId</code> is used for looking up static meshes in <code>MeshCache</code> - it’s a simple <code>std::vector</code> of references to vertex buffers on GPU.</li>
<li>If the mesh has a skeleton, <code>jointMatricesStartIndex</code> is used during compute skinning and <code>skinnedMesh-&gt;skinnedVertexBuffer</code> is used for all the rendering afterwards (instead of <code>meshId</code>)</li>
<li><code>worldBoundingSphere</code> is used for frustum culling.</li>
</ul>
<p>This separation is nice because the renderer is clearly separated from the game logic. You can also do something more clever as described <a href="https://realtimecollisiondetection.net/blog/?p=86">here</a> if sorting draw commands becomes a bottleneck.</p>
<h3 id="scene-loading-and-entity-prefabs">Scene loading and entity prefabs</h3>
<p>I use Blender as a level editor and export it as glTF. It’s easy to place objects, colliders and lights there. Here’s how it looks like:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/blender_level.jpg"/>
</figure>

<p>Writing your own level editor would probably take months (years!), so using Blender instead saved me quite a lot of time.</p>
<p>It’s important to mention how I use node names for spawning some objects. For example, you can see an object named <code>Interact.Sphere.Diary</code> selected in the screenshot above. The part before the first dot is the prefab name (in this case “Interact”). The “Sphere” part is used by the physics system to create a sphere physics body for the object (“Capsule” and “Box” can also be used, otherwise the physics shape is created using mesh vertices).</p>
<p>Some models are pretty complex and I don’t want to place them directly into the level glTF file as it’ll greatly increase each level’s size. I just place an “Empty-&gt;Arrows” object and name it something like “Cat.NearStore”. This will spawn “Cat” prefab and attach “NearStore” tag to it for runtime identification.</p>
<p>Prefabs are written in JSON and look like this:</p>
<pre><code>{
  &#34;scene&#34;: {
    &#34;scene&#34;: &#34;assets/models/cato.gltf&#34;
  },
  &#34;movement&#34;: {
    &#34;maxSpeed&#34;: [4, 4, 4]
  },
  &#34;physics&#34;: {
    &#34;type&#34;: &#34;dynamic&#34;,
    &#34;bodyType&#34;: &#34;virtual_character&#34;,
    &#34;bodyParams&#34;: {
        ...
    }
  }
}
</code></pre>
<p>During the level loading process, if the node doesn’t have a corresponding prefab, it’s loaded as-is and its mesh data is taken from the glTF file itself (this is mostly used for static geometry). If the node has a corresponding prefab loaded, it’s created instead. Its mesh data is loaded from the external glTF file - only transform is copied from the original glTF node (the one in the level glTF file).</p>
<blockquote>
  Once <a href="https://github.com/KhronosGroup/glTF-External-Reference">glTFX</a> is released and the support for it is added to Blender, things might be even easier to handle as you’ll be able to reference external glTF files with it.
</blockquote>

<h3 id="msaa">MSAA</h3>
<p>Using forward rendering allowed me to easily implement MSAA. Here’s a comparison of how the game looks without AA and with MSAA on:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/geometry.jpg" alt="No AA"/><figcaption>
            <p>No AA</p>
        </figcaption>
</figure>

<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/frame_analysis/geometry_resolve.jpg" alt="MSAA x8"/><figcaption>
            <p>MSAA x8</p>
        </figcaption>
</figure>

<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/no_aa_vs_aa.png"/>
</figure>

<p>MSAA is explained well here: <a href="https://vulkan-tutorial.com/Multisampling">https://vulkan-tutorial.com/Multisampling</a></p>
<p>Here’s another good article about MSAA: <a href="https://therealmjp.github.io/posts/msaa-overview/">https://therealmjp.github.io/posts/msaa-overview/</a> and potential problems you can have with it (especially with HDR and tone-mapping).</p>
<h3 id="ui">UI</h3>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/ui_options.png"/>
</figure>

<p>My UI system was inspired by Roblox’s UI API: <a href="https://create.roblox.com/docs/ui">https://create.roblox.com/docs/ui</a></p>
<p>Basically, the UI can calculate its own layout without me having to hard code each individual element’s size and position. Basically it relies on the following concepts:</p>
<ul>
<li>Origin is an anchor around which the UI element is positioned. If origin is <code>(0, 0)</code>, setting UI element’s position to be <code>(x,y)</code> will make its upper-left pixel have (x,y) pixel coordinate. If the origin is <code>(1, 1)</code>, then the element’s bottom-right corner will be positioned at <code>(x, y)</code>. If the origin is (0.5, 1) then it will be positioned using bottom-center point as the reference.
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/ui_origin.png"/>
</figure>
</li>
<li>Relative size makes the children’s be proportional to parent’s size. If (1,1) then the child element will have the same size as the parent element. If it’s (0.5, 0.5) then it’ll have half the size of the parent. If the parent uses children’s size as a guide, then if a child has (0.5, 0.25) relative size, the parent’s width will be 2x larger and the height will be 4x larger.</li>
<li>Relative position uses parent’s size as a guide for positioning. It’s useful for centering elements, for example if you have an element with (0.5, 0.5) origin and (0.5, 0.5) relative position, it’ll be centered inside its parent element.</li>
<li>You can also set pixel offsets for both position and size separately (they’re called <code>offsetPosition</code> and <code>offsetSize</code> in my codebase).</li>
<li>You can also set a fixed size for the elements if you don’t want them to ever be resized.</li>
<li>The label/image element size is determined using its content.</li>
</ul>
<p>Here are some examples of how it can be used to position child elements:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/ui_samples.png"/>
</figure>

<p>a) The child (yellow) has relative size (0.5, 1), relative position of (0.5, 0.5) and origin (0.5, 0.5) (alternatively, the relative position can be (0.5, 0.0) and origin at (0.5, 0.0) in this case). Its parent (green) will be two times wider, but will have the same height. The child element will be centered inside the parent.</p>
<p>b) The child (yellow) has origin (1, 1), fixed size (w,h) and absolute offset of (x,y) - this way, the item can be positioned relative to the bottom-right corner of its parent (green)</p>
<hr/>
<p>Let’s see how sizes and positions of UI elements are calculated (<a href="https://github.com/eliasdaler/edbr/blob/bf09366f9ad7d82e2cba01e66518c185af55fd5b/edbr/src/UI/Element.cpp#L32">implementation in EDBR</a>).</p>
<p>First, sizes of all elements are calculated recursively. Then positions are computed based on the previously computed sizes and specified offset positions. Afterwards all elements are drawn recursively - parent element first, then its children etc.</p>
<p>When calculating the size, most elements either have a “fixed” size (which you can set manually, e.g. you can set some button to always be 60x60 pixels) or their size is computed based on their content. For example, for label elements, their size is computed using the text’s bounding box. For image elements, their size equals the image size and so on.</p>
<p>If an element has an “Auto-size” property, it needs to specify which child will be used to calculate its size. For example, the menu nine-slice can have several text labels inside the “vertical layout” element - the bounding boxes will be calculated first, then their sizes will be summed up - then, the parent’s size is calculated.</p>
<p>Let’s take a look at a simple menu with bounding boxes displayed:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/ui_with_bb.png"/>
</figure>

<p>Here, root <code>NineSliceElement</code> is marked as “Auto-size”. To compute its size, it first computes the size of its child (<code>ListLayoutElement</code>). This recursively computes the sizes of each button, sums them up and adds some padding (<code>ListLayoutElement</code> also makes the width of each button the same based on the maximum width in the list).</p>
<h3 id="dear-imgui-and-srgb-issues">Dear ImGui and sRGB issues</h3>
<p>I love Dear ImGui. I used it to implement many useful dev and debug tools (open the image in a new tab to see them better):</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/dev_tools.png"/>
</figure>

<p>It <a href="https://github.com/ocornut/imgui/issues/578">has some problems with sRGB</a>, though. I won’t explain it in detail, but basically if you use sRGB framebuffer, Dear ImGui will look wrong in many ways, see the comparison:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/imgui_srgb.png" alt="Left - naive sRGB fix for Dear ImGui, right - proper fix"/><figcaption>
            <p>Left - naive sRGB fix for Dear ImGui, right - proper fix</p>
        </figcaption>
</figure>

<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/imgui_srgb2.png" alt="Left - naive sRGB fix for Dear ImGui, right - proper fix"/><figcaption>
            <p>Left - naive sRGB fix for Dear ImGui, right - proper fix</p>
        </figcaption>
</figure>

<p>Sometimes you can see people doing hacks by doing <code>pow(col, vec4(2.2))</code> with Dear ImGui’s colors but it still doesn’t work properly with alpha and produces incorrect color pickers.</p>
<p>I ended up writing my own Dear ImGui backend and implementing DilligentEngine’s workaround which is explained in detail <a href="https://github.com/ocornut/imgui/issues/578#issuecomment-1585432977">here</a> and <a href="https://github.com/DiligentGraphics/DiligentTools/blob/da116e30adff75ccdb33443d604ca6d153ee1589/Imgui/src/ImGuiDiligentRenderer.cpp#L39">here</a>.</p>
<blockquote>
  Writing it wasn’t as hard as I expected. I only need to write the <em>rendering</em> part, while “logic/OS interaction” part (input event processing, clipboard etc.) is still handled by default Dear ImGui SDL backend in my case.
</blockquote>

<p>There are some additional benefits of having my own backend:</p>
<ol>
<li>It supports bindless texture ids, so I can draw images by simply calling <code>ImGui::Image(bindlessTextureId, ...)</code>. Dear ImGui’s Vulkan backend requires you to “register” textures by calling <code>ImGui_ImplVulkan_AddTexture</code> for each texture before you can call <code>ImGui::Image</code>.</li>
<li>It can properly draw linear and non-linear images by passing their format into backend (so that sRGB images are not gamma corrected twice when they’re displayed)</li>
<li>Initializing and dealing with it is easier as it does Vulkan things in the same way as the rest of my engine.</li>
</ol>
<h3 id="other-stuff">Other stuff</h3>
<p>There are many parts of the engine not covered there because they’re not related to Vulkan. I still feel like it’s good to mention them briefly for the sake of completion.</p>
<ul>
<li>I use Jolt Physics for physics.</li>
</ul>
<video preload="metadata" controls="">
    <source src="resources/jolt_low_res.mp4#t=0.001" type="video/mp4"/>
    
</video>

<p>Integrating it into the engine was pretty easy. Right now I mostly use it for collision resolution and basic character movement.</p>
<p>The samples are <em>fantastic</em>. The docs are very good too.</p>
<p>I especially want to point out how incredible <code>JPH::CharacterVirtual</code> is. It handles basic character movement so well. I remember spending <em>days</em> trying to get proper slope movement in Bullet to work. With Jolt, it just worked “out of the box”.</p>
<p>Here’s how it basically works (explaining how it works properly would probably require me to write quite a big article):</p>
<ul>
<li>You add your shapes to Jolt’s world.</li>
<li>You run the simulation.</li>
<li>You get new positions of your physics objects and use these positions to render objects in their current positions.</li>
</ul>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/jolt_shapes.jpg" alt="I implemented Jolt physics shape debug renderer using im3d"/><figcaption>
            <p>I implemented Jolt physics shape debug renderer using im3d</p>
        </figcaption>
</figure>

<ul>
<li>I use <a href="https://github.com/skypjack/entt">entt</a> for the entity-component-system part.</li>
</ul>
<p>It has worked great for me so far. Previously I had my own ECS implementation, but decided to experiment with a 3rd party ECS library to have less code to maintain.</p>
<ul>
<li>I use <a href="https://github.com/kcat/openal-soft">openal-soft</a>, <a href="https://github.com/xiph/ogg">libogg</a> and <a href="https://github.com/xiph/vorbis">libvorbis</a> for audio.</li>
</ul>
<p>The audio system is mostly based on these articles: <a href="https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/">https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/</a></p>
<ul>
<li>I use <a href="https://github.com/wolfpld/tracy">Tracy</a> for profiling.</li>
</ul>
<p>Integrating it was very easy (read the PDF doc, it’s fantastic!) and it helped me avoid tons of bike-shedding by seeing how little time something, which I thought was “inefficient”, really took.</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/tracy.png"/>
</figure>

<h2 id="what-i-gained-from-switching-to-vulkan">What I gained from switching to Vulkan</h2>
<p>There are many nice things I got after switching to Vulkan:</p>
<ul>
<li>No more global state</li>
</ul>
<p>This makes abstractions a lot easier. With OpenGL abstractions/engines, you frequently see “shader.bind()” calls, state trackers, magic RAII, which automatically binds/unbinds objects and so on. There’s no need for that in Vulkan - it’s easy to write functions which take some objects as an input and produce some output - stateless, more explicit and easier to reason about.</p>
<ul>
<li>API is more pleasant to work with overall - I didn’t like “binding” things and the whole “global state machine” of OpenGL.</li>
<li>You need to write less abstractions overall.</li>
</ul>
<p>With OpenGL, you need to write a <em>lot</em> of abstractions to make it all less error-prone… Vulkan’s API requires a lot less of this, in my experience. And usually the abstractions that you write map closer to Vulkan’s “raw” functions, compared to OpenGL abstractions which hide manipulation of global state and usually call several functions (and might do some stateful things for optimization).</p>
<ul>
<li>Better validation errors</li>
</ul>
<p>Validation errors are very good in Vulkan. While OpenGL has <code>glDebugMessageCallback</code>, it doesn’t catch that many issues and you’re left wondering why your texture looks weird, why your lighting is broken and so on. Vulkan has more extensive validation which makes the debugging process much better.</p>
<ul>
<li>Debugging in RenderDoc</li>
</ul>
<p>I can now debug shaders in RenderDoc. It looks like this:</p>
<figure><img src="https://steveklabnik.com/writing/i-see-a-future-in-jj/resources/renderdoc_debug.png"/>
</figure>

<p>With OpenGL I had to output the values to some texture and color-pick them… which took a lot of time. But now I can debug vertex and fragment shaders easily.</p>
<ul>
<li>More consistent experience across different GPUs and OSes.</li>
</ul>
<p>With OpenGL, drivers on different GPUs and OSes worked differently from each other which made some bugs pop up only on certain hardware configurations. It made the process of debugging them hard. I still experienced some slight differences between different GPUs in Vulkan, but it’s much less prevalent compared to OpenGL.</p>
<ul>
<li>Ability to use better shading languages in the future</li>
</ul>
<p>GLSL is a fine shading language, but there are some new shading languages which promise to be more feature-complete, convenient and readable, for example:</p>
<ul>
<li><a href="https://github.com/shader-slang/slang">https://github.com/shader-slang/slang</a></li>
<li><a href="https://github.com/shady-gang/shady">https://github.com/shady-gang/shady</a></li>
</ul>
<p>I might explore them in the future and see if they offer me something that GLSL lacks.</p>
<ul>
<li>More control over every aspect of the graphics pipeline.</li>
<li>Second system effect, but good</li>
</ul>
<p>My first OpenGL engine was written during the process of learning graphics programming from scratch. Many abstractions were not that good and rewriting them with some graphics programming knowledge (and some help from vkguide) helped me implement a much cleaner system.</p>
<ul>
<li>Street cred</li>
</ul>
<p>And finally, it makes me proud to be able to say “I have a custom engine written in Vulkan and it works”. Sometimes people start thinking about you as a coding wizard and it makes me happy and proud of my work. :)</p>
<h2 id="future-work">Future work</h2>
<p>There are many things that I plan to do in the future, here’s a list of some of them:</p>
<ul>
<li>Sign-distance field font support (<a href="https://www.redblobgames.com/blog/2024-03-21-sdf-fonts/">good article</a> about implementing them)</li>
<li>Loading many images and generating mipmaps in parallel (or use image formats which already have mipmaps stored inside of them)</li>
<li>Bloom.</li>
<li>Volumetric fog.</li>
<li>Animation blending.</li>
<li>Render graphs.</li>
<li>Ambient occlusion.</li>
<li>Finishing the game? (hopefully…)</li>
</ul>
<p>Overall, I’m quite satisfied with what I managed to accomplish. Learning Vulkan was quite difficult, but it wasn’t as hard as I imagined. It taught me a lot about graphics programming and modern APIs and now I have a strong foundation to build my games with.</p>
</article>

</div></div>
  </body>
</html>
