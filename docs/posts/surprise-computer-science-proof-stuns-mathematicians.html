<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.quantamagazine.org/surprise-computer-science-proof-stuns-mathematicians-20230321/">Original</a>
    <h1>Surprise Computer Science Proof Stuns Mathematicians</h1>
    
    <div id="readability-page-1" class="page"><div><p>But more salient than that particular gap was the overall behavior of the two formulas. Plot the fraction of elements between 1 and <em>N </em>that each formula represents, and you’ll see Behrend’s number rapidly shrink to zero as <em>N </em>grows. Roth’s fraction, on the other hand, slides toward zero, but slowly and gently. The two curves are very different shapes, and the true proportion of elements lying in a set without arithmetic progressions could, so far as mathematicians knew, lie anywhere between them.</p>
<p>Beginning in the 1980s, “there was a long sequence of, in hindsight, fairly incremental improvements by a large number of really famous mathematicians,” Green said. Every once in a while, someone would nudge Roth’s upper limit down by a hair or two, and eventually it got considerably lower. Behrend’s lower bound, by contrast, didn’t budge for decades. Mathematicians began to think that Behrend might not have been far from the true answer, Bloom said.</p>

<p>Until Kelley and Meka’s paper arrived in early 2023, the maximum size of a progression-free set was penned in from below by Behrend’s formula, and from above by Bloom and Sisask’s. Bloom and Sisask’s paper from July 2020 had crossed the critical “logarithmic” threshold by showing that a progression-free set must have substantially fewer than <em>N</em>/(log <em>N</em>) elements. But their result still sat high above Behrend’s. Kelley and Meka’s new upper bound is drastically closer to the floor set by Behrend.</p>
<p>“Meka and Kelley have sort of leapfrogged all this incremental progress,” said Terence Tao, a prominent mathematician at UCLA.</p>
<p>Their formula is almost the same as Behrend’s, with only a few parameters tweaked. As <em>N</em> approaches infinity, a plot of Kelley and Meka’s formula will eventually settle into a curve that resembles the Behrend curve. “Any bound of that shape just seemed like an impossible dream before,” Bloom said.</p>
<p>“I was really just quite staggered that they had made such an improvement,” Green said.</p>
<h2><strong>A Different Tack</strong></h2>
<p><strong> </strong>Though Kelley and Meka had never fully ventured into pure mathematics research before, arithmetic progressions were familiar to them when they started. In general, computer scientists “are hungrily looking outward for techniques that would work to solve our problems,” Kelley said. The tools historically used to study the size of a progression-free set have become widely used in the computer science subfield of complexity theory. The problem of narrowing down the size of such a set is well-known to complexity theorists as a quintessential example of applying techniques that probe the inner structure of sets.</p>

<p>In late 2021, Kelley and Meka were analyzing the chances that a team of players in a certain cooperative game would be able to win, a standard type of computer science problem. It occurred to them that techniques from research on the size of progression-free sets might be helpful. But they found it easier to directly study those techniques than to apply them to the cooperative game. “My best idea for how to make progress on this problem [was] to actually improve the tool itself, not to use it in a more clever way,” Kelley said.</p>
<p>“At some point, we just decided to work on this question directly,” Meka recalled. Six months later, the two researchers had figured out their strategy and just needed to iron out how to apply their method to the problem at hand.</p>
<p>To see how they arrived at their new upper limit, take any set of numbers between 1 and <em>N</em>. Call it <em>A</em>. The density of <em>A </em>is the percentage of the numbers between 1 and <em>N</em> that it includes. Since there are a lot of possible arithmetic progressions between 1 and <em>N</em>, if you don’t choose the elements of <em>A</em> carefully, any <em>A</em> with high density will likely contain lots of arithmetic progressions.</p>
<p>In their proof, Kelley and Meka imagined that <em>A</em> had few or no arithmetic progressions, and they attempted to trace out the consequences. If <em>A</em> was dense enough, they showed that an absence of progressions necessitated a level of structure within <em>A</em> that would inevitably result in a contradiction, meaning that <em>A</em> must, after all, contain at least one progression.</p>
<p>To understand that structure, they considered the set <em>A</em> + <em>A</em>, which consists of all the numbers made by adding two elements of <em>A</em>. They noticed that if <em>A</em> contains comparatively few arithmetic progressions, this implies a redundancy among the elements of <em>A</em> + <em>A</em>: Different pairs of numbers from <em>A</em> often add up to the same number.</p>
</div></div>
  </body>
</html>
