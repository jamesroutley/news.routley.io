<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.patterns.app/blog/2022/12/21/finetune-llm-tech-support/">Original</a>
    <h1>Prompt engineering DaVinci-003 on our own docs for automated support (Part I)</h1>
    
    <div id="readability-page-1" class="page"><div id="post-content" itemprop="articleBody"><p><img loading="lazy" alt="Tech support as an LLM Slack bot" src="https://www.patterns.app/assets/images/techsupport1-c9b608d23095c613aa34ada385c8fe71.png" width="1041" height="367"/></p><p>One problem we’re working on at Patterns is how to scale our technical support. We have technical documentation, Slack channels, emails, and support tickets that all provide a way for us to interface with our customers. Like many folks,
we&#39;ve been playing around with the power and potential of new Large Language Models like ChatGPT, so we decided to see if we could help tackle our support problem with an LLM support bot.</p><p>We came in with somewhat low expectations -- we know these models are prone to common failure modes you&#39;d expect from a next-token optimizer -- so we were shocked when we saw the end result. Read on to learn how we did it and our experience building with LLMs. By the end of the post, we&#39;ll have built a full app that you can clone and customize to have your own version of <a href="https://studio.patterns.app/graph/4u2hexllqax7nuqyp8he/domain-expert" target="_blank" rel="noopener noreferrer">our bot</a> if you&#39;d like.</p><h3 id="game-plan">Game plan<a href="#game-plan" title="Direct link to heading">​</a></h3><p>Our plan was to build an automated slack bot that would respond to tech support questions with knowledge from our technical documentation, community slack channels, and previous support tickets.</p><p>Normally with LLMs you put all the necessary context in the prompt. We had far too much content to fit in the 2048 token limit for the normal prompt though. Luckily, many LLM providers like OpenAI provide a &#34;fine-tuning&#34; api where you can submit labeled example completions to fine-tune your own version of their LLM.</p><p>This fine-tuning workflow seemed like a good fit for our problem -- it could take our rich text corpus, fine-tune a model, and then provide completions via a Slack bot.</p><p>To serve this experience as a <a href="https://www.patterns.app" target="_blank" rel="noopener noreferrer">Patterns</a> app end-to-end we need to:</p><ol><li>Generate training data from our text corpus</li><li>Fine-tune our model (OpenAIs GPT-3 davinci-003 engine)</li><li>Serve the fine-tuned model as a Slack bot</li></ol><p>Let&#39;s build it!</p><h3 id="prerequisites">Prerequisites<a href="#prerequisites" title="Direct link to heading">​</a></h3><ul><li>OpenAI account, API key</li><li>A text corpus to train your bot, e.g. technical documentation or something similar</li></ul><h3 id="generating-training-data">Generating training data<a href="#generating-training-data" title="Direct link to heading">​</a></h3><p>Immediately we ran into a problem -- to fine-tune an OpenAI model requires a specific format of prompt-completion pairs:</p><div><div><pre tabindex="0"><code><span><span>{</span><span>&#34;prompt&#34;</span><span>:</span><span> </span><span>&#34;&lt;prompt text&gt;&#34;</span><span>,</span><span> </span><span>&#34;completion&#34;</span><span>:</span><span> </span><span>&#34;&lt;ideal generated text&gt;&#34;</span><span>}</span><span></span><br/></span><span><span></span><span>{</span><span>&#34;prompt&#34;</span><span>:</span><span> </span><span>&#34;&lt;prompt text&gt;&#34;</span><span>,</span><span> </span><span>&#34;completion&#34;</span><span>:</span><span> </span><span>&#34;&lt;ideal generated text&gt;&#34;</span><span>}</span><span></span><br/></span><span><span></span><span>{</span><span>&#34;prompt&#34;</span><span>:</span><span> </span><span>&#34;&lt;prompt text&gt;&#34;</span><span>,</span><span> </span><span>&#34;completion&#34;</span><span>:</span><span> </span><span>&#34;&lt;ideal generated text&gt;&#34;</span><span>}</span><br/></span></code></pre></div></div><p>Our free-form text corpus wouldn&#39;t work out of the box. We could manually generate the training data, but that&#39;s a lot of work. So we remembered the <strong>Golden Rule of LLMs: If it at first you don&#39;t succeed, send it through again with a new prompt</strong>.</p><p>So instead of manually labeling we come up with a series of prompts to automatically generate our labeled pairs from our corpus:</p><ol><li>Feed GPT a chunk of our corpus and ask it to generate three relevant questions for the chunk</li><li>Feed those questions back in with the chunk and ask GPT to specifically answer the generated question using the chunk </li><li>Take this output and pair with the original generated question for a labeled training example to feed to the fine-tuner</li></ol><p><img loading="lazy" alt="Training data" src="https://www.patterns.app/assets/images/training-data-gen-e04a98d230486e2fdfa00f1dbbeecd61.svg" width="742" height="1060"/></p><h3 id="import-training-data-into-patterns">Import training data into Patterns<a href="#import-training-data-into-patterns" title="Direct link to heading">​</a></h3><p>There are many ways to import our documentation into Patterns. To keep it simple for our example, we loaded ours into an <a href="https://airtable.com/shrwqKCuJs1tWZ0Hy" target="_blank" rel="noopener noreferrer">Airtable base</a> that we imported via the standard Airtable component in Patterns. An example of the technical documentation text contained:</p><div><div><pre tabindex="0"><code><span><span>Webhooks and Streaming</span><br/></span><span><span></span><br/></span><span><span>Patterns has native support for stream processing. A common automation is to have </span><br/></span><span><span>a webhook ingesting real-time events from some external system, processing these </span><br/></span><span><span>events with a series of python nodes, and then sending a response, either to a </span><br/></span><span><span>communication channel like Slack or Discord, or to trigger an action in another </span><br/></span><span><span>external system.</span><br/></span><span><span></span><br/></span></code></pre></div></div><h3 id="generate-questions-to-use-as-prompts-from-our-docs">Generate questions to use as prompts from our docs<a href="#generate-questions-to-use-as-prompts-from-our-docs" title="Direct link to heading">​</a></h3><p>With our text data in Patterns, we wrote a prompt to tease out three questions that would act as our prompts in training data. From the marketplace we cloned the OpenAI Completions component, added the following prompt, and ran it to generate completions (an example is shown in green below).</p><div><div><pre tabindex="0"><code><span><span>You are reviewing the documentation for a data company called Patterns. Write </span><br/></span><span><span>three questions based on the documentation below.</span><br/></span><span><span></span><br/></span><span><span>Documentation: </span><br/></span><span><span>Patterns has native support for stream processing. A common automation is to have </span><br/></span><span><span>a webhook ingesting real-time events from some external system, processing these </span><br/></span><span><span>events with a series of python nodes, and then sending a response, either to a </span><br/></span><span><span>communication channel like Slack or Discord, or to trigger an action in another </span><br/></span><span><span>external system.</span><br/></span><span><span></span><br/></span><span><span>Questions:</span><br/></span><span><span>1. What types of real-time events are supported by Patterns webhooks?</span><br/></span><span><span>2. How are python nodes used to process events?</span><br/></span><span><span>3. How are responses sent to external systems?</span><br/></span></code></pre></div></div><p>This is what it looks like in the UI. </p><p><img loading="lazy" alt="Foo" src="https://www.patterns.app/assets/images/prompt1-cd9da810c884c7ae467693451d7c3650.png" width="3639" height="2302"/></p><h3 id="generate-answers-from-the-questions-in-the-prior-step">Generate answers from the questions in the prior step<a href="#generate-answers-from-the-questions-in-the-prior-step" title="Direct link to heading">​</a></h3><p>With the three generated questions / prompts that are relevant to our task, we next generated answers for these using the documentation text. Again, we cloned the OpenAI Completions component and edited the prompt: </p><div><div><pre tabindex="0"><code><span><span>You are a smart and witty technical support agent for a data company called Patterns. </span><br/></span><span><span>Please generate answers to the following questions given the documentation. </span><br/></span><span><span></span><br/></span><span><span>Documentation: </span><br/></span><span><span>Patterns has native support for stream processing. A common automation is to have </span><br/></span><span><span>a webhook ingesting real-time events from some external system, processing these </span><br/></span><span><span>events with a series of python nodes, and then sending a response, either to a </span><br/></span><span><span>communication channel like Slack or Discord, or to trigger an action in another </span><br/></span><span><span>external system.</span><br/></span><span><span></span><br/></span><span><span>Questions: </span><br/></span><span><span>1. What types of real-time events are supported by Patterns webhooks?</span><br/></span><span><span>2. How are python nodes used to process events?</span><br/></span><span><span>3. How are responses sent to external systems?</span><br/></span><span><span></span><br/></span><span><span>Answers:</span><br/></span><span><span>1. Patterns webhooks support a wide range of real-time events, including webhooks for Slack, Discord, and other external systems. </span><br/></span><span><span>2. Python nodes are used to process the real-time events received via webhooks and transform them into the desired output format. </span><br/></span><span><span>3. Responses can be sent to external systems through a communication channel like Slack or Discord, or by triggering an action in the external system.</span><br/></span></code></pre></div></div><p>Next we prepped the data and formatted it for fine-tuning. There are many tricks here and OpenAI has a <a href="https://beta.openai.com/docs/guides/fine-tuning/cli-data-preparation-tool" target="_blank" rel="noopener noreferrer">CLI data preparation</a> tool to ensure the structure of your data prior to upload. </p><p><img loading="lazy" alt="Foo" src="https://www.patterns.app/assets/images/trainingdata-6f99734cfe7f010bb63d525a5a277d3d.png" width="3639" height="2302"/></p><h3 id="upload-training-data-to-openai-and-start-the-fine-tuning-job">Upload training data to OpenAI, and start the fine-tuning job<a href="#upload-training-data-to-openai-and-start-the-fine-tuning-job" title="Direct link to heading">​</a></h3><p>This resulted in prompt/completion pairs that we could structure and upload to OpenAI for fine-tuning. This involved first uploading our completions, which took a few seconds, and then kicking off the fine-tuning job on OpenAI.</p><p>Depending on the size of training data, and OpenAI&#39;s resource availability, fine-tuning can take anywhere from a few minutes to a few hours, for that reason we kept a state object in our fine-tuning script and check for updates until it&#39;s finished. </p><p>When the fine-tuning job completes successfully, we write out the model to another table named <code>finetuned_models</code>. </p><h3 id="configure-fine-tuned-model-as-a-slack-bot">Configure fine-tuned model as a Slack Bot<a href="#configure-fine-tuned-model-as-a-slack-bot" title="Direct link to heading">​</a></h3><p>With the fine-tuned model ready to go on OpenAIs servers, we could tackle the next step -- building a Slackbot to receive and respond to customer&#39;s questions. </p><p>To configure a Slack bot in Patterns that uses GPT-3 we set up the following flow:</p><ol><li>A webhook configured to receive messages from a Slack channel </li><li>A Python node to detect when the bot is mentioned</li><li>An OpenAI node, parameterized with the <code>model_name</code> from the fine-tuning process, that provides a completion to any prompt</li><li>A Python node that serves completion back to the Slack channel</li></ol><p>To do this, we replicated <a href="https://www.patterns.app/marketplace/apps?uid=kybe52ek5riu2qobghbk" target="_blank" rel="noopener noreferrer">Engineering Advice GPT-3 Slack Bot</a> into the same App and followed the configuration in that link. </p><h3 id="learnings-and-next-steps">Learnings and next steps<a href="#learnings-and-next-steps" title="Direct link to heading">​</a></h3><p>Overall, we were impressed with the bot&#39;s ability to generate answers to relevant questions that it&#39;s trained on. However, a lot of the time the bot just makes stuff up; this occurs for both prompts that do and don&#39;t have relevant samples in the training data. Making stuff up and being confidently wrong are well known side-effects of LLMs and there are many techniques to change this behavior. </p><p><img loading="lazy" alt="Foo" src="https://www.patterns.app/assets/images/promptdatabase-5e459385bd91a1b8371db7323658b744.png" width="1200" height="265"/>
<em>(totally made up URL, not even our domain)</em></p><p>In Part II of this post (coming soon), we will explore several avenues for improvement, including expanding the corpus via automated ingestion, using <a href="https://beta.openai.com/docs/guides/embeddings" target="_blank" rel="noopener noreferrer">embeddings</a> to encode documentation semantics to provide more robust links directly to documentation, and labeling some negative examples of when the bot should or should not respond.</p><p>If you have any questions about setting up a use case similar to this, please email me at <a href="mailto:chris@patterns.app" target="_blank" rel="noopener noreferrer">chris@patterns.app</a> or create a Slack channel from your dashboard and message us there. </p></div></div>
  </body>
</html>
