<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://monadmonkey.com/dnns-from-scratch-in-zig">Original</a>
    <h1>Deep Neural Networks from Scratch in Zig</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body">
<div id="post-body-inner">


<p>Zig is one of the new names in the world of system&#39;s programming. I&#39;ve been following its growth over the last few months, and have been looking for an excuse to program with it. Needing a break from my experiments with Chaos Networks, I thought this would be the perfect time to take a small detour and have some fun with Zig.</p>

<p>Today we will be exploring <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a>, neural networks with at least two layers. <a href="https://en.wikipedia.org/wiki/Deep_learning">DNNs</a> are typically written in high level Python libraries like <a href="https://www.tensorflow.org/">Tensorflow</a> utilizing more complex libraries written in c++ to handle the heavy compute pieces like automatic differentiation.</p>
<p>We will not be using any such libraries and instead write our own simple <a href="https://en.wikipedia.org/wiki/Deep_learning">DNN</a> from scratch using nothing but the Zig standard library.</p>
<p>By the end of this post our homemade <a href="https://en.wikipedia.org/wiki/Deep_learning">DNN</a> will be solving <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> with 96% accuracy!</p>

<p>In contrast to the majority of public libraries, we will not be utilizing the concepts of tensors and the associated idea of automatic differentiation as doing so would be outside the scope of a simple weekend project. Instead, we will manually compute the gradients for layers. This causes our architecture to shift from the standard abstraction with layer structs possessing only forward methods, to layer structs possessing both forward and backwards methods. In other words, each layer will be in charge of computing its own individual gradients and the gradients for its inputs (so we can chain layers) as will be shown later.</p>
<p>Our simple executable will consist of four modules:</p>
<ul>
<li>layer.zig</li>
<li>relu.zig</li>
<li>nll.zig</li>
<li>mnist.zig</li>
</ul>
<p>Let&#39;s break them down.</p>

<pre><code>const std = @import(&#34;std&#34;);

pub fn Layer(comptime I: usize, comptime O: usize) type {
    const LayerGrads = struct { 
        weight_grads: []f64, 
        input_grads: []f64,
        const Self = @This();

        pub fn destruct(self: Self, allocator: *std.mem.Allocator) void {
            allocator.free(self.weight_grads);
            allocator.free(self.input_grads);
        }
    };

    return struct {
        inputs: usize,
        outputs: usize,
        weights: *[I * O]f64,
        last_inputs: []f64,
        const Self = @This();

        pub fn forward(self: *Self, inputs: []f64, allocator: *std.mem.Allocator) ![]f64 {
            const batch_size = inputs.len / I;
            var outputs = try allocator.alloc(f64, batch_size * O);
            var b: usize = 0;
            while (b &lt; batch_size) : (b += 1) {
                var o: usize = 0;
                while (o &lt; O) : (o += 1) {
                    var sum: f64 = 0;
                    var i: usize = 0;
                    while (i &lt; I) : (i += 1) {
                        sum += inputs[b * I + i] * self.weights[O * i + o];
                    }
                    outputs[b * O + o] = sum;
                }
            }
            self.last_inputs = inputs;
            return outputs;
        }

        pub fn backwards(self: *Self, grads: []f64, allocator: *std.mem.Allocator) !LayerGrads {
            var weight_grads = try allocator.alloc(f64, I * O);

            const batch_size = self.last_inputs.len / I;
            var input_grads = try allocator.alloc(f64, batch_size * I);

            var b: usize = 0;
            while (b &lt; batch_size) : (b += 1) {
                var i: usize = 0;
                while (i &lt; I) : (i += 1) {
                    var o: usize = 0;
                    while (o &lt; O) : (o += 1) {
                        weight_grads[i * O + o] += (grads[b * O + o] * self.last_inputs[b * I + i]) / @intToFloat(f64, batch_size);
                        input_grads[b * I + i] += grads[b * O + o] * self.weights[i * O + o];
                    }
                }
            }
            return LayerGrads{ .weight_grads = weight_grads, .input_grads = input_grads };
        }

        pub fn applyGradients(self: *Self, grads: []f64) void {
            var i: usize = 0;
            while (i &lt; I * O): (i += 1) {
                self.weights[i] -= 0.01 * grads[i];
            }
        }

        pub fn init(allocator: *std.mem.Allocator) !Self {
            var memory = try allocator.alloc(f64, I * O);
            var weights = memory[0 .. I * O];
            var prng = std.rand.DefaultPrng.init(123);
            var w: usize = 0;
            while (w &lt; I * O) : (w += 1) {
                weights[w] = prng.random().floatNorm(f64) * 0.2;
            }
            return Self{
                .inputs = I,
                .outputs = O,
                .weights = weights,
                .last_inputs = undefined,
            };
        }

        pub fn destruct(self: *Self, allocator: *std.mem.Allocator) void {
            allocator.free(self.weights);
        }
    };
}
</code></pre>
<p>If you are not familiar with neural networks, the most important thing to know is the <span>forward</span> method is the act of passing <span>inputs</span> into the layer and receiving <span>outputs</span>. The method simply performs matrix multiplication, multiplying the <span>inputs</span> by the <span>weights</span> of the layer. There are more performant ways to accomplish the matrix multiplication in the <span>forward</span> method, but as this is simply an educational endeavor, the above is adequate. </p>
<p>Also note the <span>backwards</span> method. As stated above, our simple <a href="https://en.wikipedia.org/wiki/Deep_learning">DNN</a> is in charge of calculating its own gradient. The <span>backwards</span> method does just that, taking <span>grads</span> from further down the chain of execution, and returning the <span>grads</span> for its own <span>weights</span> and <span>inputs</span> to the <span>forward</span> method. We need the <span>grads</span> for the <span>inputs</span> so we can chain the layers together.</p>
<p>One of the very cool features of Zig is the <span>comptime</span> directive which allows us to write code that is interpreted at compile time. In this case we are taking advantage of the <span>comptime</span> feature to write a generic layer struct that takes dynamic <span>inputs</span> and <span>outputs</span> arguments. Using this, we can write code like the following.</p>
<pre><code>var allocator = std.heap.page_allocator;

var layer1 = try layer.Layer(784, 100).init(&amp;allocator);
var layer2 = try layer.Layer(100, 10).init(&amp;allocator);

const outputs1 = try layer1.forward(inputs, &amp;allocator);
const outputs2 = try layer2.forward(outputs2, &amp;allocator);

</code></pre>
<p>By hard coding the <span>forward</span> and <span>backward</span> methods at <span>comptime</span> we have some more comfort with the general correctness and expected errors we would receive if we passed in incorrectly shaped data at runtime. The end goal would be having complete <span>comptime</span> guarantees that if the executable compiles the structure of the neural network is working as expected. I did not accomplish this yet, and the <span>comptime</span> usage above is basically useless, but was a great exercise in my learning of Zig.</p>

<pre><code>const std = @import(&#34;std&#34;);

pub const Relu = struct {
    last_inputs: []f64,
    const Self = @This();

    pub fn new() Self {
        return Self {
            .last_inputs = undefined,
        };
    }

    pub fn forward(self: *Self, inputs: []f64, allocator: *std.mem.Allocator) ![]f64 {
        var outputs = try allocator.alloc(f64, inputs.len);
        var i: usize = 0;
        while (i &lt; inputs.len): (i += 1) {
            if (inputs[i] &lt; 0) {
                outputs[i] = 0.01 * inputs[i];
            } else {
                outputs[i] = inputs[i];
            }
        }
        self.last_inputs = inputs;
        return outputs;
    }

    pub fn backwards(self: *Self, grads: []f64, allocator: *std.mem.Allocator) ![]f64 {
        var outputs = try allocator.alloc(f64, grads.len);
        var i: usize = 0;
        while (i &lt; self.last_inputs.len): (i += 1) {
            if (self.last_inputs[i] &lt; 0) {
                grads[i] = 0.01 * grads[i];
            } else {
                outputs[i] = grads[i];
            }
        }
        return outputs;
    }
};
</code></pre>
<p>I should clarify this is actually the <a href="https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html">leaky relu</a> and not plain <a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">relu</a>.</p>
<p>Similar to the <span>Layer</span> struct above we have both <span>forward</span> and <span>backwards</span> methods as this struct is also in charge of managing its own <span>grads</span>.</p>

<pre><code>const std = @import(&#34;std&#34;);

pub fn NLL(comptime I: usize) type {
    const NLLOuput = struct {
        loss: []f64,
        input_grads: []f64,
        const Self = @This();

        pub fn destruct(self: Self, allocator: *std.mem.Allocator) void {
            allocator.free(self.loss);
            allocator.free(self.input_grads);
        }
    };

    return struct {
        pub fn nll(inputs: []f64, targets: []u8, allocator: *std.mem.Allocator) !NLLOuput {
            const batch_size = targets.len;
            var sum_e = try allocator.alloc(f64, batch_size);
            defer allocator.free(sum_e);
            var b: usize = 0;
            while (b &lt; batch_size) : (b += 1) {
                var sum: f64 = 0;
                var i: usize = 0;
                while (i &lt; I): (i += 1) {
                    sum += std.math.exp(inputs[b * I + i]);
                }
                sum_e[b] = sum;
            }

            var loss = try allocator.alloc(f64, batch_size);
            b = 0;
            while (b &lt; batch_size): (b += 1) {
                loss[b] = -1 * std.math.ln(std.math.exp(inputs[b * I + targets[b]]) / sum_e[b]);
            }

            var input_grads = try allocator.alloc(f64, batch_size * I);
            b = 0;
            while (b &lt; batch_size): (b += 1) {
                var i: usize = 0;
                while (i &lt; I): (i += 1) {
                    input_grads[b * I + i] = std.math.exp(inputs[b * I + i]) / sum_e[b];
                    if (i == targets[b]) {
                        input_grads[b * I + i] -= 1;
                    }
                }
            }

            return NLLOuput {
                .loss = loss,
                .input_grads = input_grads 
            };
        }
    };
}
</code></pre>
<p>Given some <span>inputs</span> and some <span>targets</span> the <span>nll</span> method performs <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html?highlight=nll#torch.nn.functional.nll_loss">negative log likelihood</a> loss, and returns the loss and the grads for the <span>inputs</span>. </p>
<p>Similar to the <span>Layer</span> struct, we are utilizing Zig&#39;s <span>comptime</span> feature, but as mentioned above, this really is unnecessary and done mostly for my own personal education on how <span>comptime</span> works.</p>

<pre><code>const std = @import(&#34;std&#34;);


const Data = struct {
    train_images: []f64,
    train_labels: []u8,
    test_images: []f64,
    test_labels: []u8,
    const Self = @This();

    pub fn destruct(self: Self, allocator: *std.mem.Allocator) void {
        allocator.free(self.train_images);
        allocator.free(self.train_labels);
        allocator.free(self.test_images);
        allocator.free(self.test_labels);
    }
};


pub fn readMnist(allocator: *std.mem.Allocator) !Data {
    const train_images_path: []const u8 = &#34;data/train-images-idx3-ubyte&#34;;
    const train_images_u8 = try readIdxFile(train_images_path, 16, allocator);
    defer allocator.free(train_images_u8);
    var train_images = try allocator.alloc(f64, 784 * 60000);
    var i: u32 = 0;
    while (i &lt; 784 * 60000): (i += 1) {
        const x: f64 = @intToFloat(f64, train_images_u8[i]);
        train_images[i] = x / 255;
    }

    const train_labels_path: []const u8 = &#34;data/train-labels-idx1-ubyte&#34;;
    const train_labels = try readIdxFile(train_labels_path, 8, allocator);

    const test_images_path: []const u8 = &#34;data/t10k-images-idx3-ubyte&#34;;
    const test_images_u8 = try readIdxFile(test_images_path, 16, allocator);
    defer allocator.free(test_images_u8);
    var test_images = try allocator.alloc(f64, 784 * 10000);
    i = 0;
    while (i &lt; 784 * 10000): (i += 1) {
        const x: f64 = @intToFloat(f64, test_images_u8[i]);
        test_images[i] = x / 255;
    }

    const test_labels_path: []const u8 = &#34;data/t10k-labels-idx1-ubyte&#34;;
    const test_labels = try readIdxFile(test_labels_path, 8, allocator);

    return Data {
        .train_images = train_images,
        .train_labels = train_labels,
        .test_images = test_images,
        .test_labels = test_labels
    };


}

pub fn readIdxFile(path: []const u8, skip_bytes: u8, allocator: *std.mem.Allocator) ![]u8 {
    const file = try std.fs.cwd().openFile(
        path,
        .{},
    );
    defer file.close();

    const reader = file.reader();
    try reader.skipBytes(skip_bytes, .{});
    const data = reader.readAllAlloc(
        allocator.*,
        1000000000,
    );
    return data;
}
</code></pre>
<p><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> is stored in the IDX file format which basically has some leading bits specifying the type and structure of data. As we know these two items already, we simply skip the leading bytes, load the data, and divide it by 255 to normalize everything between 0 and 1. </p>

<pre><code>const layer = @import(&#34;layer.zig&#34;);
const nll = @import(&#34;nll.zig&#34;);
const mnist = @import(&#34;mnist.zig&#34;);
const relu = @import(&#34;relu.zig&#34;);

const std = @import(&#34;std&#34;);

const INPUT_SIZE: u32 = 784;
const OUTPUT_SIZE: u32 = 10;
const BATCH_SIZE: u32 = 32;
const EPOCHS: u32 = 25;

pub fn main() !void {
    var allocator = std.heap.page_allocator;

    // Get MNIST data
    const mnist_data = try mnist.readMnist(&amp;allocator);

    // Prep loss function
    const loss_function = nll.NLL(OUTPUT_SIZE);

    // Prep NN
    var layer1 = try layer.Layer(INPUT_SIZE, 100).init(&amp;allocator);
    var relu1 = relu.Relu.new();
    var layer2 = try layer.Layer(100, OUTPUT_SIZE).init(&amp;allocator);

    // Do training
    var e: usize = 0;
    while (e &lt; EPOCHS) : (e += 1) {
        // Do training
        var i: usize = 0;
        while (i &lt; 60000 / BATCH_SIZE) : (i += 1) {
            // Prep inputs and targets
            const inputs = mnist_data.train_images[i * INPUT_SIZE * BATCH_SIZE .. (i + 1) * INPUT_SIZE * BATCH_SIZE];
            const targets = mnist_data.train_labels[i * BATCH_SIZE .. (i + 1) * BATCH_SIZE];

            // Go forward and get loss
            const outputs1 = try layer1.forward(inputs, &amp;allocator);
            const outputs2 = try relu1.forward(outputs1, &amp;allocator);
            const outputs3 = try layer2.forward(outputs2, &amp;allocator);
            const loss = try loss_function.nll(outputs3, targets, &amp;allocator);

            // Update network
            const grads1 = try layer2.backwards(loss.input_grads, &amp;allocator);
            const grads2 = try relu1.backwards(grads1.input_grads, &amp;allocator);
            const grads3 = try layer1.backwards(grads2, &amp;allocator);
            layer1.applyGradients(grads3.weight_grads);
            layer2.applyGradients(grads1.weight_grads);

            // Free memory
            allocator.free(outputs1);
            allocator.free(outputs2);
            allocator.free(outputs3);
            grads1.destruct(&amp;allocator);
            allocator.free(grads2);
            grads3.destruct(&amp;allocator);
            loss.destruct(&amp;allocator);
        }

        // Do validation
        i = 0;
        var correct: f64 = 0;
        const outputs1 = try layer1.forward(mnist_data.test_images, &amp;allocator);
        const outputs2 = try relu1.forward(outputs1, &amp;allocator);
        const outputs3 = try layer2.forward(outputs2, &amp;allocator);
        var b: usize = 0;
        while (b &lt; 10000) : (b += 1) {
            var max_guess: f64 = outputs3[b * OUTPUT_SIZE];
            var guess_index: usize = 0;
            for (outputs3[b * OUTPUT_SIZE .. (b + 1) * OUTPUT_SIZE]) |o, oi| {
                if (o &gt; max_guess) {
                    max_guess = o;
                    guess_index = oi;
                }
            }
            if (guess_index == mnist_data.test_labels[b]) {
                correct += 1;
            }
        }

        // Free memory
        allocator.free(outputs1);
        allocator.free(outputs2);
        allocator.free(outputs3);

        correct = correct / 10000;
        std.debug.print(&#34;Average Validation Accuracy: {}\n&#34;, .{correct});
    }

    layer1.destruct(&amp;allocator);
    layer2.destruct(&amp;allocator);
    mnist_data.destruct(&amp;allocator);
}
</code></pre>
<p>The above code is the entire main.zig file. Utilizing the modules we created, we can easily create a simple <a href="https://en.wikipedia.org/wiki/Deep_learning">DNN</a> that solves <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>. The majority of the code above is simply <span>allocator</span> frees and validation on the test set.</p>
<p>Note the chaining of the different <span>Layers</span> and <span>Relu</span>. While we aren&#39;t utilizing tensors or performing automatic differentiation in the classic sense, by chaining <span>Layer</span> and <span>Relu</span> structs together, we could have an infinitely large <a href="https://en.wikipedia.org/wiki/Deep_learning">DNN</a>.</p>
<p>In this case, two layers are more than enough to boast an excellent score on <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>. See the training results below: </p>
<pre><code>&gt; zig build run
Average Validation Accuracy: 8.782e-01
Average Validation Accuracy: 9.011e-01
Average Validation Accuracy: 9.103e-01
Average Validation Accuracy: 9.18e-01
Average Validation Accuracy: 9.239e-01
Average Validation Accuracy: 9.286e-01
Average Validation Accuracy: 9.318e-01
Average Validation Accuracy: 9.35e-01
Average Validation Accuracy: 9.384e-01
Average Validation Accuracy: 9.412e-01
Average Validation Accuracy: 9.435e-01
Average Validation Accuracy: 9.462e-01
Average Validation Accuracy: 9.476e-01
Average Validation Accuracy: 9.491e-01
Average Validation Accuracy: 9.501e-01
Average Validation Accuracy: 9.512e-01
Average Validation Accuracy: 9.516e-01
Average Validation Accuracy: 9.526e-01
Average Validation Accuracy: 9.539e-01
Average Validation Accuracy: 9.544e-01
Average Validation Accuracy: 9.553e-01
Average Validation Accuracy: 9.561e-01
Average Validation Accuracy: 9.572e-01
Average Validation Accuracy: 9.579e-01
Average Validation Accuracy: 9.586e-01
</code></pre>
<p>For our simple network, 95.86% accuracy is fantastic! If we let it train longer, it could probably do even better.</p>
<p>It does run fairly slowly and though I have not profiled it, I would guess the constant memory allocation and frees in the training loop are the bottleneck. This could be fixed easily by reusing the same buffers, but it is currently more than fast enough for our purposes.</p>
<p>Thank you so much for reading!</p>
<p><a href="https://github.com/SilasMarvin/dnns-from-scratch-in-zig/">the repo</a></p>
</div>
</div></div>
  </body>
</html>
