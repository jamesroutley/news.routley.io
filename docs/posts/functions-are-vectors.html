<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thenumb.at/Functions-are-Vectors/">Original</a>
    <h1>Functions are vectors</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p>Conceptualizing functions as infinite-dimensional vectors lets us apply the tools of linear algebra to a vast landscape of new problems, from image and geometry processing to curve fitting, light transport, and machine learning.</p>

<p><em>Prerequisites: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">introductory linear algebra</a>, <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">introductory calculus</a>, <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNPOjrT6KVlfJuKtYTftqH6">introductory differential equations</a>.</em></p>

<ul>
  <li><a href="#functions-as-vectors">Functions as Vectors</a>
    <ul>
      <li><a href="#vector-spaces">Vector Spaces</a></li>
      <li><a href="#linear-operators">Linear Operators</a></li>
      <li><a href="#diagonalization">Diagonalization</a></li>
      <li><a href="#inner-product-spaces">Inner Product Spaces</a></li>
      <li><a href="#the-spectral-theorem">The Spectral Theorem</a></li>
    </ul>
  </li>
  <li><a href="#applications">Applications</a>
    <ul>
      <li><a href="#fourier-series">Fourier Series</a></li>
      <li><a href="#image-compression">Image Compression</a></li>
      <li><a href="#geometry-processing">Geometry Processing</a></li>
      <li><a href="#further-reading">Further Reading</a></li>
    </ul>
  </li>
</ul>

<hr/>



<p>Vectors are often first introduced as lists of real numbers—i.e. the familiar notation we use for points, directions, and more.</p>

<div>
<p>
$$ \mathbf{v} = \begin{bmatrix}x\\y\\z\end{bmatrix} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/vector.svg"/>
</p>
</div>

<p>You may recall that this representation is only one example of an abstract <em>vector space</em>.
There are many other types of vectors, such as lists of <a href="https://en.wikipedia.org/wiki/Complex_number">complex numbers</a>, <a href="https://en.wikipedia.org/wiki/Cycle_space">graph cycles</a>, and even <a href="https://pdodds.w3.uvm.edu/files/papers/others/1980/ward1980a.pdf">magic squares</a>.</p>

<p>However, all of these vector spaces have one thing in common: a <em>finite</em> number of dimensions.
That is, each kind of vector can be represented as a collection of \(N\) numbers, though the definition of “number” varies.</p>

<p>If any \(N\)-dimensional vector is essentially a length-\(N\) list, we could also consider a vector to be a <em>mapping</em> from an index to a value.</p><p>

\[\begin{align*}
\mathbf{v}_1 &amp;= x\\
\mathbf{v}_2 &amp;= y\\
\mathbf{v}_3 &amp;= z
\end{align*}\ \iff\ \mathbf{v} = \begin{bmatrix}x \\ y \\ z\end{bmatrix}\]

</p><p>What does this perspective hint at as we increase the number of dimensions?</p>

<svg id="fvec" width="500" height="350"></svg>


<p>In higher dimensions, vectors start to look more like functions!</p>

<h2 id="countably-infinite-indices">Countably Infinite Indices</h2>

<p>Of course, a finite-length vector only specifies a value at a limited number of indices.
Could we instead define a vector that contains infinitely many values?</p>

<p>Writing down a vector representing a function on the natural numbers (\(\mathbb{N}\))—or any other <a href="https://en.wikipedia.org/wiki/Countable_set"><em>countably infinite</em></a> domain—is straightforward: just extend the list indefinitely.</p>

<div>
<p>
$$ \begin{align*}\mathbf{v}_1 &amp;= 1\\\mathbf{v}_2 &amp;= 2\\ &amp;\vdots \\ \mathbf{v}_i &amp;= i\end{align*}\ \iff\ \mathbf{v} = \begin{bmatrix}1 \\ 2 \\ 3 \\ \vdots \end{bmatrix} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/identity.svg"/>
</p>
</div>

<p>This vector could represent the function \(f(x) = x\), where \(x \in \mathbb{N}\).<sup id="fnref:polynomials" role="doc-noteref"><a href="#fn:polynomials" rel="footnote">1</a></sup></p>

<h2 id="uncountably-infinite-indices">Uncountably Infinite Indices</h2>

<p>Many interesting functions are defined on the real numbers (\(\mathbb{R}\)), so may not be representable as a countably infinite vector.
Therefore, we will have to make a larger conceptual leap: not only will our set of indices be infinite, it will be <a href="https://en.wikipedia.org/wiki/Uncountable_set"><em>uncountably infinite</em></a>.</p>

<p>That means we can’t write down vectors as lists at all—it is <a href="https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument">impossible</a> to assign an integer index to each element of an uncountable set.
So, how can we write down a vector mapping a <em>real</em> index to a certain value?</p>

<p>Now, a vector really is just an arbitrary function:</p>

<div>
<p>
$$ \mathbf{v}_{x} = x^2\ \iff\ \mathbf{v} = \begin{bmatrix} x \mapsto x^2 \end{bmatrix} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/quadratic.svg"/>
</p>
</div>

<p>Precisely defining how and why we can represent functions as infinite-dimensional vectors is the purview of <em>functional analysis</em>.
In this post, we won’t attempt to prove our results in infinite dimensions: we will focus on building intuition via analogies to finite-dimensional linear algebra.</p>

<hr/>





<p>Formally, a <em>vector space</em> is defined by choosing a set of vectors \(\mathcal{V}\), a scalar <a href="https://en.wikipedia.org/wiki/Field_(mathematics)">field</a> \(\mathbb{F}\), and a zero vector \(\mathbf{0}\).
The field \(\mathbb{F}\) is often the real numbers (\(\mathbb{R}\)), complex numbers (\(\mathbb{C}\)), or a finite field such as the integers modulo a prime (\(\mathbb{Z}_p\)).</p>

<p>Additionally, we must specify how to add two vectors and how to multiply a vector by a scalar.</p><p>

\[\begin{align*}
(+)\ &amp;:\ \mathcal{V}\times\mathcal{V}\mapsto\mathcal{V}\\
(\cdot)\ &amp;:\ \mathbb{F}\times\mathcal{V} \mapsto \mathcal{V}
\end{align*}\]

</p><p>To describe a vector space, our definitions must entail several <em>vector space axioms</em>.</p>

<h2 id="a-functional-vector-space">A Functional Vector Space</h2>

<p>In the following sections, we’ll work with the vector space of real functions.
To avoid ambiguity, square brackets are used to denote function application.</p>

<ul>
  <li>The scalar field \(\mathbb{F}\) is the real numbers \(\mathbb{R}\).</li>
  <li>The set of vectors \(\mathcal{V}\) contains functions from \(\mathbb{R}\) to \(\mathbb{R}\).<sup id="fnref:rtor" role="doc-noteref"><a href="#fn:rtor" rel="footnote">2</a></sup></li>
  <li>\(\mathbf{0}\) is the zero function, i.e. \(\mathbf{0}[x] = 0\).</li>
</ul>

<p>Adding functions corresponds to applying the functions separately and summing the results.</p>
<div>
<p>
$$ (f + g)[x] = f[x] + g[x] $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/fadd.svg"/>
</p>
</div>

<p>This definition generalizes the typical element-wise addition rule—it’s like adding the two values at each index.</p><p>

\[f+g = \begin{bmatrix}f_1 + g_1 \\ f_2 + g_2 \\ \vdots \end{bmatrix}\]

</p><p>Multiplying a function by a scalar corresponds to applying the function and scaling the result.</p>

<div>
<p>
$$ (\alpha f)[x] = \alpha f[x] $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/fscale.svg"/>
</p>
</div>

<p>This rule similarly generalizes element-wise multiplication—it’s like scaling the value at each index.</p><p>

\[\alpha f = \begin{bmatrix}\alpha f_1 \\ \alpha f_2 \\ \vdots \end{bmatrix}\]

</p><h2 id="proofs">Proofs</h2>

<p>Given these definitions, we can now prove all necessary vector space axioms.
We will illustrate the analog of each property in \(\mathbb{R}^2\), the familiar vector space of two-dimensional arrows.</p>

<div>
  
  <div>
  <div><p>
For all vectors <span>$$\mathbf{u}, \mathbf{v} \in \mathcal{V}$$</span>:

</p><div>
<p>
$$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/commut.svg"/>
</p>
</div><p>

Since real addition is commutative, this property follows directly from our definition of vector addition:

$$\begin{align*}
(f + g)[x] &amp;= f[x] + g[x]\\
&amp;= g[x] + f[x]\\
&amp;= (g + f)[x]
\end{align*}$$
  </p></div>
  </div>
</div>

<div>
  
  <div>
  <div><p>
For all vectors <span>$$\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$$</span>:

</p><div>
<p>
$$(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/assoc.svg"/>
</p>
</div><p>

This property also follows from our definition of vector addition:

$$\begin{align*}
((f + g) + h)[x] &amp;= (f + g)[x] + h[x]\\
&amp;= f[x] + g[x] + h[x]\\
&amp;= f[x] + (g[x] + h[x])\\
&amp;= f[x] + (g + h)[x]\\
&amp;= (f + (g + h))[x]
\end{align*}$$

  </p></div>
  </div>
</div>

<div>
  
  <div>
  <div><p>
For all vectors <span>$$\mathbf{u} \in \mathcal{V}$$</span>:

</p><div>
<p>
$$\mathbf{0} + \mathbf{u} = \mathbf{u} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/zidentity.svg"/>
</p>
</div><p>

This one is easy:

$$\begin{align*}
(\mathbf{0} + f)[x] &amp;= \mathbf{0}[x] + f[x]\\
&amp;= 0 + f[x]\\
&amp;= f[x]
\end{align*}$$

  </p></div>
  </div>
</div>

<div>
  
  <div>
  <div><p>
For all vectors <span>$$\mathbf{u} \in \mathcal{V}$$</span>, there exists a vector <span>$$-\mathbf{u} \in \mathcal{V}$$</span> such that:

</p><div>
<p>
$$\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/inverse.svg"/>
</p>
</div><p>

Negation is defined as applying <span>$$f$$</span> and negating the result: <span>$$(-f)[x] = -f[x]$$</span>.
Clearly, <span>$$-f$$</span> is also in <span>$$\mathcal{V}$$</span>.

$$\begin{align*}
(f + (-f))[x] &amp;= f[x] + (-f)[x]\\
&amp;= f[x] - f[x]\\
&amp;= 0\\
&amp;= \mathbf{0}[x]
\end{align*}$$

  </p></div>
  </div>
</div>

<div>
  
  <div>
  <div><p>
For all vectors <span>$$\mathbf{u} \in \mathcal{V}$$</span>:

</p><div>
<p>
$$1\mathbf{u} = \mathbf{u}$$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/midentity.svg"/>
</p>
</div><p>

Note that <span>$$1$$</span> is specified by the choice of <span>$$\mathbb{F}$$</span>.
In our case, it is simply the real number <span>$$1$$</span>.

$$\begin{align*}
(1 f)[x] &amp;= 1 f[x]\\
&amp;= f[x]
\end{align*}$$

  </p></div>
  </div>
</div>

<div>
  
  <div>
  <div><p>
For all vectors <span>$$\mathbf{u} \in \mathcal{V}$$</span> and scalars <span>$$\alpha, \beta \in \mathbb{F}$$</span>:

</p><div>
<p>
$$(\alpha \beta)\mathbf{u} = \alpha(\beta\mathbf{u})$$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/mult.svg"/>
</p>
</div><p>

This property follows from our definition of scalar multiplication:

$$\begin{align*}
((\alpha\beta) f)[x] &amp;= (\alpha\beta)f[x]\\
&amp;= \alpha(\beta f[x])\\
&amp;= \alpha(\beta f)[x]
\end{align*}$$

  </p></div>
  </div>
</div>

<div>
  
  <div>
  <div><p>
For all vectors <span>$$\mathbf{u}, \mathbf{v} \in \mathcal{V}$$</span> and scalars <span>$$\alpha \in \mathbb{F}$$</span>:

</p><div>
<p>
$$\alpha(\mathbf{u} + \mathbf{v}) = \alpha\mathbf{u} + \alpha\mathbf{v}$$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/distrib1.svg"/>
</p>
</div><p>

Again using our definitions of vector addition and scalar multiplication:

$$\begin{align*}
(\alpha (f + g))[x] &amp;= \alpha(f + g)[x]\\
&amp;= \alpha(f[x] + g[x])\\
&amp;= \alpha f[x] + \alpha g[x]\\
&amp;= (\alpha f)[x] + (\alpha g)[x]\\
&amp;= (\alpha f + \alpha g)[x]
\end{align*}$$

  </p></div>
  </div>
</div>

<div>
  
  <div>
  <div><p>
For all vectors <span>$$\mathbf{u} \in \mathcal{V}$$</span> and scalars <span>$$\alpha, \beta \in \mathbb{F}$$</span>:

</p><div>
<p>
 $$(\alpha + \beta)\mathbf{u} = \alpha\mathbf{u} + \beta\mathbf{u}$$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/sdistrib.svg"/>
</p>
</div><p>

Again using our definitions of vector addition and scalar multiplication:

$$\begin{align*}
((\alpha + \beta)f)[x] &amp;= (\alpha + \beta)f[x]\\
&amp;= \alpha f[x] + \beta f[x] \\
&amp;= (\alpha f)[x] + (\beta f)[x]
\end{align*}$$

  </p></div>
  </div>
</div>

<p>Therefore, we’ve built a vector space of functions!<sup id="fnref:otherspaces" role="doc-noteref"><a href="#fn:otherspaces" rel="footnote">3</a></sup>
It may not be immediately obvious why this result is useful, but bear with us through a few more definitions—we will spend the rest of this post exploring powerful techniques arising from this perspective.</p>

<h2 id="a-standard-basis-for-functions">A Standard Basis for Functions</h2>



<p>Unless specified otherwise, vectors are written down with respect to the <em>standard basis</em>.
In \(\mathbb{R}^2\), the standard basis consists of the two coordinate axes.</p>

<div>
<p>
$$ \mathbf{e}_1 = \begin{bmatrix}1 \\ 0\end{bmatrix},\,\, \mathbf{e}_2 = \begin{bmatrix}0 \\ 1\end{bmatrix} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/r2std.svg"/>
</p>
</div>

<p>Hence, vector notation is shorthand for a <em>linear combination</em> of the standard basis vectors.</p>

<div>
<p>
$$ \mathbf{u} = \begin{bmatrix}\alpha \\ \beta\end{bmatrix} = \alpha\mathbf{e}_1 + \beta\mathbf{e}_2 $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/r2stdv.svg"/>
</p>
</div>

<p>Above, we represented functions as vectors by assuming each dimension of an infinite-length vector contains the function’s result for that index.
This construction points to a natural generalization of the standard basis.</p>

<p>Just like the coordinate axes, each <em>standard basis function</em> contains a \(1\) at one index and \(0\) everywhere else.
More precisely, for every \(\alpha \in \mathbb{R}\),</p>

<p>
\[\mathbf{e}_\alpha[x] = \begin{cases} 1 &amp; \text{if } x = \alpha \\ 0 &amp; \text{otherwise} \end{cases}\]

</p>

<p>We can then express any real function \(f\) as a linear combination of these basis functions:</p><p>

\[\begin{align*} f[x] &amp;= f[\alpha]\mathbf{e}_\alpha[x] \\ &amp;= f[1]\mathbf{e}_1[x] + f[2]\mathbf{e}_2[x] + f[\pi]\mathbf{e}_\pi[x] + \dots \end{align*}\]

</p><p>If you evaluate this sum at \(x\), you’ll find that all terms are zero—except \(\mathbf{e}_x\), making the result \(f[x]\).</p>

<hr/>





<p>Now that we can manipulate functions as vectors, let’s start transferring the tools of linear algebra to the functional perspective.</p>

<p>One ubiquitous operation on finite-dimensional vectors is transforming them with matrices.
A matrix \(\mathbf{A}\) encodes a <em>linear transformation</em>, meaning multiplication preserves linear combinations.</p><p>

\[\mathbf{A}(\alpha \mathbf{x} + \beta \mathbf{y}) = \alpha \mathbf{A}\mathbf{x} + \beta \mathbf{A}\mathbf{y}\]

</p><p>Multiplying a vector by a matrix can be intuitively interpreted as defining a new set of coordinate axes from the matrix’s column vectors.
The result is a linear combination of the columns:</p>

<div>
<div>

<p>

\[\mathbf{Ax} = \begin{bmatrix} \vert &amp; \vert &amp; \vert \\ \mathbf{u} &amp; \mathbf{v} &amp; \mathbf{w} \\ \vert &amp; \vert &amp; \vert \end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix} = x_1\mathbf{u} + x_2\mathbf{v} + x_3\mathbf{w}\]

    </p>
<p>

\[\begin{align*}
\mathbf{Ax} &amp;= \begin{bmatrix} \vert &amp; \vert &amp; \vert \\ \mathbf{u} &amp; \mathbf{v} &amp; \mathbf{w} \\ \vert &amp; \vert &amp; \vert \end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix} \\ &amp;= x_1\mathbf{u} + x_2\mathbf{v} + x_3\mathbf{w}
\end{align*}\]

    </p>

</div>
<p><img src="https://thenumb.at/assets/functions-are-vectors/transform2.svg"/>
</p>
</div>

<p>When all vectors can be expressed as a linear combination of \(\mathbf{u}\), \(\mathbf{v}\), and \(\mathbf{w}\), the columns form a basis for the underlying vector space.
Here, the matrix \(\mathbf{A}\) transforms a vector from the \(\mathbf{uvw}\) basis into the standard basis.</p>

<p>Since functions are vectors, we could imagine transforming a function by a matrix.
Such a matrix would be infinite-dimensional, so we will instead call it a <em>linear operator</em> and denote it with \(\mathcal{L}\).</p>

<p>

\[\mathcal{L}f = \begin{bmatrix} \vert &amp; \vert &amp; \vert &amp; \\ \mathbf{f} &amp; \mathbf{g} &amp; \mathbf{h} &amp; \cdots \\ \vert &amp; \vert &amp; \vert &amp; \end{bmatrix} \begin{bmatrix}f_1\\ f_2 \\ f_3\\ \vdots\end{bmatrix} = f_1\mathbf{f} + f_2\mathbf{g} + f_3\mathbf{h} + \cdots\]

</p>
<p>

\[\begin{align*}
\mathcal{L}f &amp;= \begin{bmatrix} \vert &amp; \vert &amp; \vert &amp; \\ \mathbf{f} &amp; \mathbf{g} &amp; \mathbf{h} &amp; \cdots \\ \vert &amp; \vert &amp; \vert &amp; \end{bmatrix} \begin{bmatrix}f_1\\ f_2\\ f_3 \\ \vdots\end{bmatrix} \\ &amp;= f_1\mathbf{f} + f_2\mathbf{g} + f_3\mathbf{h} + \cdots
\end{align*}\]

</p>

<p>This visualization isn’t very accurate—we’re dealing with <em>uncountably</em> infinite-dimensional vectors, so we can’t actually write out an operator in matrix form.
Nonetheless, the structure is suggestive: each “column” of the operator describes a new basis function for our functional vector space.
Just like we saw with finite-dimensional vectors, \(\mathcal{L}\) represents a change of basis.</p>

<h2 id="differentiation">Differentiation</h2>



<p>So, what’s an example of a linear operator on functions?
You might recall that <em>differentiation</em> is linear:</p><p>

\[\frac{\partial}{\partial x} \left(\alpha f[x] + \beta g[x]\right) = \alpha\frac{\partial f}{\partial x} + \beta\frac{\partial g}{\partial x}\]

</p><p>It’s hard to visualize differentiation on general functions, but it’s feasible for the <a href="https://en.wikipedia.org/wiki/Linear_subspace">subspace</a> of <em>polynomials</em>, \(\mathcal{P}\).
Let’s take a slight detour to examine this smaller space of functions.</p><p>

\[\mathcal{P} = \{ p[x] = a + bx + cx^2 + dx^3 + \cdots \}\]

</p><p>We typically write down polynomials as a sequence of powers, i.e. \(1, x, x^2, x^3\), etc.
All polynomials are linear combinations of the functions \(\mathbf{e}_i[x] = x^i\), so they constitute a countably infinite basis for \(\mathcal{P}\).<sup id="fnref:polynatural" role="doc-noteref"><a href="#fn:polynatural" rel="footnote">4</a></sup></p>

<p>This basis provides a convenient vector notation:</p>

<p>

\[\begin{align*} p[x] &amp;= a + bx + cx^2 + dx^3 + \cdots \\ &amp;= a\mathbf{e}_0 + b\mathbf{e}_1 + c \mathbf{e}_2 + d\mathbf{e}_3 + \dots \end{align*}\ \iff\ \mathbf{p} = \begin{bmatrix}a\\ b\\ c\\ d\\ \vdots\end{bmatrix}\]

</p>
<p>

\[\begin{align*} p[x] &amp;= a + bx + cx^2 + dx^3 + \cdots \\ &amp;= a\mathbf{e}_0 + b\mathbf{e}_1 + c \mathbf{e}_2 + d\mathbf{e}_3 + \dots \\&amp; \iff\ \mathbf{p} = \begin{bmatrix}a\\ b\\ c\\ d\\ \vdots\end{bmatrix} \end{align*}\]

</p>

<p>Since differentiation is linear, we’re able to apply the rule \(\frac{\partial}{\partial x} x^n = nx^{n-1}\) to each term.</p>

<p>

\[\begin{align*}\frac{\partial}{\partial x}p[x] &amp;= \vphantom{\Bigg\vert}a\frac{\partial}{\partial x}1 + b\frac{\partial}{\partial x}x + c\frac{\partial}{\partial x}x^2 + d\frac{\partial}{\partial x}x^3 + \dots \\ &amp;= b + 2cx + 3dx^2 + \cdots\\ &amp;= b\mathbf{e}_0 + 2c\mathbf{e}_1 + 3d\mathbf{e}_2 + \dots\end{align*}  \ \iff\ \frac{\partial}{\partial x}\mathbf{p} = \begin{bmatrix}b\\ 2c\\ 3d\\ \vdots\end{bmatrix}\]

</p>
<p>

\[\begin{align*}\frac{\partial}{\partial x}p[x] &amp;= \vphantom{\Bigg\vert}a\frac{\partial}{\partial x}1 + b\frac{\partial}{\partial x}x + c\frac{\partial}{\partial x}x^2\, +\\ &amp; \phantom{=} d\frac{\partial}{\partial x}x^3 + \dots \\ &amp;= b + 2cx + 3dx^2 + \cdots\\ &amp;= b\mathbf{e}_0 + 2c\mathbf{e}_1 + 3d\mathbf{e}_2 + \dots  \\ &amp;\iff\ \frac{\partial}{\partial x}\mathbf{p} = \begin{bmatrix}b\\ 2c\\ 3d\\ \vdots\end{bmatrix}\end{align*}\]

</p>

<p>We’ve performed a linear transformation on the coefficients, so we can represent differentiation as a matrix!</p><p>

\[\frac{\partial}{\partial x}\mathbf{p} = \begin{bmatrix}0 &amp; 1 &amp; 0 &amp; 0 &amp; \cdots\\ 0 &amp; 0 &amp; 2 &amp; 0 &amp; \cdots\\ 0 &amp; 0 &amp; 0 &amp; 3 &amp; \cdots\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots \end{bmatrix}\begin{bmatrix}a\\ b\\ c\\ d\\ \vdots\end{bmatrix} = \begin{bmatrix}b\\ 2c\\ 3d\\ \vdots\end{bmatrix}\]

</p><p>Each column of the differentiation operator is itself a polynomial, so this matrix represents a change of basis.</p><p>

\[\frac{\partial}{\partial x} = \begin{bmatrix} \vert &amp; \vert &amp; \vert &amp; \vert &amp; \vert &amp;  \\ 0 &amp; 1 &amp; 2x &amp; 3x^2 &amp; 4x^3 &amp; \cdots \\ \vert &amp; \vert &amp; \vert &amp; \vert &amp; \vert &amp;  \end{bmatrix}\]

</p><p>As we can see, the differentiation operator simply maps each basis function to its derivative.</p>

<p>This result also applies to the larger space of <em>analytic</em> real functions, which includes polynomials, exponential functions, trigonometric functions, logarithms, and other familiar names.
By definition, an analytic function can be expressed as a Taylor series about \(0\):</p><p>

\[f[x] = \sum_{n=0}^\infty \frac{f^{(n)}[0]}{n!}x^n = \sum_{n=0}^\infty \alpha_n x^n\]

</p><p>Which is a linear combination of our polynomial basis functions.
That means a Taylor expansion is essentially a change of basis into the sequence of powers, where our differentiation operator is quite simple.<sup id="fnref:polybasis" role="doc-noteref"><a href="#fn:polybasis" rel="footnote">5</a></sup></p>

<hr/>





<p>Matrix decompositions are arguably the crowning achievement of linear algebra.
To get started, let’s review what diagonalization means for a \(3\times3\) real matrix \(\mathbf{A}\).</p>

<h2 id="eigenvectors">Eigenvectors</h2>

<p>A vector \(\mathbf{u}\) is an <em>eigenvector</em> of the matrix \(\mathbf{A}\) when the following condition holds:</p>

<div>
<p>
$$ \mathbf{Au} = \lambda \mathbf{u} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/eigenvector.svg"/>
</p>
</div>

<p>The <em>eigenvalue</em> \(\lambda\) may be computed by solving the <em>characteristic polynomial</em> of \(\mathbf{A}\).
Eigenvalues may be real or complex.</p>

<p>The matrix \(\mathbf{A}\) is <em>diagonalizable</em> when it admits three linearly independent eigenvectors, each with a corresponding <strong>real</strong> eigenvalue.
This set of eigenvectors constitutes an <em>eigenbasis</em> for the underlying vector space, indicating that we can express any vector \(\mathbf{x}\) via their linear combination.</p>

<div>
<p>
$$ \mathbf{x} = \alpha\mathbf{u}_1 + \beta\mathbf{u}_2 + \gamma\mathbf{u}_3 $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/eigenbasis.svg"/>
</p>
</div>

<p>To multiply \(\mathbf{x}\) by \(\mathbf{A}\), we just have to scale each component by its corresponding eigenvalue.</p>

<div>
<p>
$$ \begin{align*} \mathbf{Ax} &amp;= \alpha\mathbf{A}\mathbf{u}_1 + \beta\mathbf{A}\mathbf{u}_2 + \gamma\mathbf{A}\mathbf{u}_3 \\
&amp;= \alpha\lambda_1\mathbf{u}_1 + \beta\lambda_2\mathbf{u}_2 + \gamma\lambda_3\mathbf{u}_3 \end{align*} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/eigenscale.svg"/>
</p>
</div>

<p>Finally, re-combining the eigenvectors expresses the result in the standard basis.</p>

<p><img src="https://thenumb.at/assets/functions-are-vectors/eigenbasis2.svg"/></p>

<p>Intuitively, we’ve shown that multiplying by \(\mathbf{A}\) is equivalent to a change of basis, a scaling, and a change back.
That means we can write \(\mathbf{A}\) as the product of an invertible matrix \(\mathbf{U}\) and a diagonal matrix \(\mathbf{\Lambda}\).</p>

<p>

\[\begin{align*} \mathbf{A} &amp;= \mathbf{U\Lambda U^{-1}} \\
 &amp;= \begin{bmatrix}\vert &amp; \vert &amp; \vert \\ \mathbf{u}_1 &amp; \mathbf{u}_2 &amp; \mathbf{u}_3 \\ \vert &amp; \vert &amp; \vert \end{bmatrix}
 \begin{bmatrix}\lambda_1 &amp; 0 &amp; 0 \\ 0 &amp; \lambda_2 &amp; 0 \\ 0 &amp; 0 &amp; \lambda_3 \end{bmatrix}
 \begin{bmatrix}\vert &amp; \vert &amp; \vert \\ \mathbf{u}_1 &amp; \mathbf{u}_2 &amp; \mathbf{u}_3 \\ \vert &amp; \vert &amp; \vert \end{bmatrix}^{-1}
\end{align*}\]

</p>
<p>

\[\begin{align*} \mathbf{A} &amp;= \mathbf{U\Lambda U^{-1}} \\
 &amp;= \begin{bmatrix}\vert &amp; \vert &amp; \vert \\ \mathbf{u}_1 &amp; \mathbf{u}_2 &amp; \mathbf{u}_3 \\ \vert &amp; \vert &amp; \vert \end{bmatrix}
 \\ &amp; \phantom{=} \begin{bmatrix}\lambda_1 &amp; 0 &amp; 0 \\ 0 &amp; \lambda_2 &amp; 0 \\ 0 &amp; 0 &amp; \lambda_3 \end{bmatrix}
 \\ &amp; \phantom{=} \begin{bmatrix}\vert &amp; \vert &amp; \vert \\ \mathbf{u}_1 &amp; \mathbf{u}_2 &amp; \mathbf{u}_3 \\ \vert &amp; \vert &amp; \vert \end{bmatrix}^{-1}
\end{align*}\]

</p>

<p>Note that \(\mathbf{U}\) is invertible because its columns (the eigenvectors) form a basis for \(\mathbb{R}^3\).
When multiplying by \(\mathbf{x}\), \(\mathbf{U}^{-1}\) converts \(\mathbf{x}\) to the eigenbasis, \(\mathbf{\Lambda}\) scales by the corresponding eigenvalues, and \(\mathbf{U}\) takes us back to the standard basis.</p>

<p>In the presence of complex eigenvalues, \(\mathbf{A}\) may still be diagonalizable if we allow \(\mathbf{U}\) and \(\mathbf{\Lambda}\) to include complex entires.
In this case, the decomposition as a whole still maps real vectors to real vectors, but the intermediate values become complex.</p>

<h2 id="eigenfunctions">Eigenfunctions</h2>



<p>So, what does diagonalization mean in a vector space of functions?
Given a linear operator \(\mathcal{L}\), you might imagine a corresponding definition for <em>eigenfunctions</em>:</p><p>

\[\mathcal{L}f = \psi f\]

</p><p>The scalar \(\psi\) is again known as an eigenvalue.
Since \(\mathcal{L}\) is infinite-dimensional, it doesn’t have a characteristic polynomial—there’s not a straightforward method for computing \(\psi\).</p>

<p>Nevertheless, let’s attempt to diagonalize differentiation on analytic functions.
The first step is to find the eigenfunctions.
Start by applying the above condition to our differentiation operator in the power basis:</p><p>

\[\begin{align*}
&amp;&amp; \frac{\partial}{\partial x}\mathbf{p} = \psi \mathbf{p} \vphantom{\Big|}&amp; \\
&amp;\iff&amp; \begin{bmatrix}0 &amp; 1 &amp; 0 &amp; 0 &amp; \cdots\\ 0 &amp; 0 &amp; 2 &amp; 0 &amp; \cdots\\ 0 &amp; 0 &amp; 0 &amp; 3 &amp; \cdots\\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots \end{bmatrix}\begin{bmatrix}p_0\\ p_1\\ p_2\\ p_3\\ \vdots\end{bmatrix}
&amp;= \begin{bmatrix}\psi p_0\\ \psi p_1 \\ \psi p_2 \\ \psi p_3 \\ \vdots \end{bmatrix} \\
&amp;\iff&amp; \begin{cases} p_1 &amp;= \psi p_0 \\ p_2 &amp;= \frac{\psi}{2} p_1 \\ p_3 &amp;= \frac{\psi}{3} p_2 \\ &amp;\dots \end{cases} &amp;
\end{align*}\]

</p><p>This system of equations implies that all coefficients are determined solely by our choice of constants \(p_0\) and \(\psi\).
We can explicitly write down their relationship as \(p_i = \frac{\psi^i}{i!}p_0\).</p>

<p>Now, let’s see what this class of polynomials actually looks like.</p>

<p>

\[p[x] = p_0 + p_0\psi x + p_0\frac{\psi^2}{2}x^2 + p_0\frac{\psi^3}{6}x^3 + p_0\frac{\psi^4}{24}x^4 + \dots\]

</p>
<p>

\[\begin{align*}
p[x] &amp;= p_0 + p_0\psi x + p_0\frac{\psi^2}{2}x^2\, +\\ &amp;\phantom{=} p_0\frac{\psi^3}{6}x^3 + p_0\frac{\psi^4}{24}x^4 + \dots
\end{align*}\]

</p>

<p>Differentiation shows that this function is, in fact, an eigenfunction for the eigenvalue \(\psi\).</p>

<p>

\[\begin{align*} \frac{\partial}{\partial x} p[x] &amp;= 0 + p_0\psi + p_0 \psi^2 x + p_0\frac{\psi^3}{2}x^2 + p_0\frac{\psi^4}{6}x^3 + \dots \\
&amp;= \psi p[x] \end{align*}\]

</p>
<p>

\[\begin{align*}
\frac{\partial}{\partial x} p[x] &amp;= 0 + p_0\psi + p_0 \psi^2 x\, +\\ &amp;\phantom{=} p_0\frac{\psi^3}{2}x^2 + p_0\frac{\psi^4}{6}x^3 + \dots \\
&amp;= \psi p[x]
\end{align*}\]

</p>

<p>With a bit of algebraic manipulation, the definition of \(e^{x}\) pops out:</p>

<p>

\[\begin{align*} p[x] &amp;= p_0 + p_0\psi x + p_0\frac{\psi^2}{2}x^2 + p_0\frac{\psi^3}{6}x^3 + p_0\frac{\psi^4}{24}x^4 + \dots \\
&amp;= p_0\left((\psi x) + \frac{1}{2!}(\psi x)^2 + \frac{1}{3!}(\psi x)^3 + \frac{1}{4!}(\psi x)^4 + \dots\right) \\
&amp;= p_0 e^{\psi x} \end{align*}\]

</p>
<p>

\[\begin{align*}
p[x] &amp;= p_0 + p_0\psi x + p_0\frac{\psi^2}{2}x^2\, +\\ &amp;\phantom{=} p_0\frac{\psi^3}{6}x^3 + p_0\frac{\psi^4}{24}x^4 + \dots \\
&amp;= p_0\Big((\psi x) + \frac{1}{2!}(\psi x)^2\, +\\ &amp;\phantom{=p_0\Big((} \frac{1}{3!}(\psi x)^3 + \frac{1}{4!}(\psi x)^4 + \dots\Big) \\
&amp;= p_0 e^{\psi x}
\end{align*}\]

</p>

<p>Therefore, functions of the form \(p_0e^{\psi x}\) are eigenfunctions for the eigenvalue \(\psi\), including when \(\psi=0\).</p>

<h2 id="diagonalizing-differentiation">Diagonalizing Differentiation</h2>

<p>We’ve found the eigenfunctions of the derivative operator, but can we diagonalize it?
Ideally, we would express differentiation as the combination of an invertible operator \(\mathcal{L}\) and a diagonal operator \(\mathcal{D}\).</p>

<p>

\[\begin{align*} \frac{\partial}{\partial x} &amp;= \mathcal{L} \mathcal{D} \mathcal{L}^{-1}  \\
&amp;=
\begin{bmatrix} \vert &amp; \vert &amp;  &amp; \\ \alpha e^{\psi_1 x} &amp; \beta e^{\psi_2 x} &amp; \dots \\ \vert &amp; \vert &amp;  \end{bmatrix}
\begin{bmatrix} \psi_1 &amp; 0 &amp; \dots \\ 0 &amp; \psi_2 &amp; \dots \\ \vdots &amp; \vdots &amp; \ddots \end{bmatrix}
{\color{red} \begin{bmatrix} \vert &amp; \vert &amp;  &amp; \\ \alpha e^{\psi_1 x} &amp; \beta e^{\psi_2 x} &amp; \dots \\ \vert &amp; \vert &amp;  \end{bmatrix}^{-1} }
\end{align*}\]

</p>
<p>

\[\begin{align*} \frac{\partial}{\partial x} &amp;= \mathcal{L} \mathcal{D} \mathcal{L}^{-1}  \\
&amp;=
\begin{bmatrix} \vert &amp; \vert &amp;  &amp; \\ \alpha e^{\psi_1 x} &amp; \beta e^{\psi_2 x} &amp; \dots \\ \vert &amp; \vert &amp;  \end{bmatrix}
\\ &amp; \phantom{=} \begin{bmatrix} \psi_1 &amp; 0 &amp; \dots \\ 0 &amp; \psi_2 &amp; \dots \\ \vdots &amp; \vdots &amp; \ddots \end{bmatrix}
\\ &amp; \phantom{=} {\color{red} \begin{bmatrix} \vert &amp; \vert &amp;  &amp; \\ \alpha e^{\psi_1 x} &amp; \beta e^{\psi_2 x} &amp; \dots \\ \vert &amp; \vert &amp;  \end{bmatrix}^{-1} }
\end{align*}\]

</p>

<p>Diagonalization is only possible when our eigenfunctions form a basis.
This would be true if all analytic functions are expressible as a linear combination of exponentials.
However…</p>

<div>
  
  <div>
    <div>
      <p>First assume that \(f[x] = x\) can be represented as a linear combination of exponentials.
Since analytic functions have countably infinite dimensionality, we should only need a countably infinite sum:</p><p>

\[f[x] = x = \sum_{n=0}^\infty \alpha_n e^{\psi_n x}\]

      </p><p>Differentiating both sides:</p><p>

\[\begin{align*} f^{\prime}[x] &amp;= 1 = \sum_{n=0}^\infty \psi_n\alpha_n e^{\psi_n x} \\
 f^{\prime\prime}[x] &amp;= 0 = \sum_{n=0}^\infty \psi_n^2\alpha_n e^{\psi_n x} \end{align*}\]

      </p><p>Since \(e^{\psi_n x}\) and \(e^{\psi_m x}\) are linearly independent when \(n\neq m\), the final equation implies that all \(\alpha = 0\), except possibly the \(\alpha_\xi\) corresponding to \(\psi_\xi = 0\).
Therefore:</p><p>

\[\begin{align*}
1 &amp;= \sum_{n=0}^\infty \psi_n\alpha_n e^{\psi_n x}\\
&amp;= \psi_\xi \alpha_\xi + \sum_{n\neq \xi} 0\psi_n e^{\psi_n x} \\
&amp;= 0
\end{align*}\]

      </p><p>That’s a contradiction—the linear combination representing \(f[x] = x\) does not exist.</p>

      <p>A similar argument shows that we can’t represent any non-constant function whose \(n\)th derivative is zero, nor periodic functions like sine and cosine.</p>
    </div>
  </div>
</div>

<p>Real exponentials don’t constitute a basis, so we cannot construct an invertible \(\mathcal{L}\).</p>

<h3 id="the-laplace-transform">The Laplace Transform</h3>

<p>We previously mentioned that more matrices can be diagonalized if we allow the decomposition to contain complex numbers.
Analogously, more linear operators are diagonalizable in the larger vector space of functions from \(\mathbb{R}\) to \(\mathbb{C}\).</p>

<p>Differentiation works the same way in this space; we’ll still find that its eigenfunctions are exponential.</p><p>

\[\frac{\partial}{\partial x} e^{(a+bi)x} = (a+bi)e^{(a+bi)x}\]

</p><p>However, the new eigenfunctions have complex eigenvalues, so we still can’t diagonalize.
We’ll need to consider the still larger space of functions from \(\mathbb{C}\) to \(\mathbb{C}\).</p><p>

\[\frac{\partial}{\partial x} : (\mathbb{C}\mapsto\mathbb{C}) \mapsto (\mathbb{C}\mapsto\mathbb{C})\]

</p><p>In this space, differentiation can be diagonalized via the <a href="https://en.wikipedia.org/wiki/Laplace_transform">Laplace transform</a>.
Although useful for solving differential equations, the Laplace transform is non-trivial to invert, so we won’t discuss it further.
In the following sections, we’ll delve into an operator that can be easily diagonalized in \(\mathbb{R}\mapsto\mathbb{C}\): the Laplacian.</p>

<hr/>





<p>Before we get to the spectral theorem, we’ll need to understand one more topic: inner products.
You’re likely already familiar with one example of an inner product—the Euclidean dot product.</p><p>

\[\begin{bmatrix}x\\ y\\ z\end{bmatrix} \cdot \begin{bmatrix}a\\ b\\ c\end{bmatrix} = ax + by + cz\]

</p><p>An inner product describes how to measure a vector along another vector.
For example, \(\mathbf{u}\cdot\mathbf{v}\) is proportional to the length of the <em>projection</em> of \(\mathbf{u}\) onto \(\mathbf{v}\).</p>

<div>
<p>
$$ \mathbf{u} \cdot \mathbf{v} =\|\mathbf{u}\|\|\mathbf{v}\|\cos[\theta] $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/dotproduct.svg"/>
</p>
</div>

<p>With a bit of trigonometry, we can show that the dot product is equivalent to multiplying the vectors’ lengths with the cosine of their angle.
This relationship suggests that the product of a vector with itself produces the square of its length.</p><p>

\[\begin{align*} \mathbf{u}\cdot\mathbf{u} &amp;= \|\mathbf{u}\|\|\mathbf{u}\|\cos[0] \\
&amp;= \|\mathbf{u}\|^2
\end{align*}\]

</p><p>Similarly, when two vectors form a right angle (are <em>orthogonal</em>), their dot product is zero.</p>

<div>
<p>
$$ \begin{align*}  \mathbf{u} \cdot \mathbf{v} &amp;= \|\mathbf{u}\|\|\mathbf{v}\|\cos[90^\circ] \\ &amp;= 0 \end{align*} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/orthogonal.svg"/>
</p>
</div>

<p>Of course, the Euclidean dot product is only one example of an inner product.
In more general spaces, the inner product is denoted using angle brackets, such as \(\langle \mathbf{u}, \mathbf{v} \rangle\).</p>

<ul>
  <li>The length (also known as the <em>norm</em>) of a vector is defined as \(\|\mathbf{u}\| = \sqrt{\langle \mathbf{u}, \mathbf{u} \rangle}\).</li>
  <li>Two vectors are orthogonal if their inner product is zero: \(\ \mathbf{u} \perp \mathbf{v}\ \iff\ \langle \mathbf{u}, \mathbf{v} \rangle = 0\).</li>
</ul>

<p>A vector space augmented with an inner product is known as an inner product space.</p>

<h2 id="a-functional-inner-product">A Functional Inner Product</h2>

<p>We can’t directly apply the Euclidean dot product to our space of real functions, but its \(N\)-dimensional generalization is suggestive.</p><p>

\[\begin{align*} \mathbf{u} \cdot \mathbf{v} &amp;= u_1v_1 + u_2v_2 + \dots + u_Nv_N \\ &amp;= \sum_{i=1}^N u_iv_i \end{align*}\]

</p><p>Given countable indices, we simply match up the values, multiply them, and add the results.
When indices are uncountable, we can convert the discrete sum to its continuous analog: an integral!</p><p>

\[\langle f, g \rangle = \int_a^b f[x]g[x] \, dx\]

</p><p>When \(f\) and \(g\) are similar, multiplying them produces a larger function; when they’re different, they cancel out.
Integration measures their product over some domain to produce a scalar result.</p>

<div>
<p><img src="https://thenumb.at/assets/functions-are-vectors/innerprod_large.svg"/>
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/innerprod_small.svg"/>
</p>
</div>

<p>Of course, not all functions can be integrated.
Our inner product space will only contain functions that are <a href="https://en.wikipedia.org/wiki/Square-integrable_function">square integrable</a> over the domain \([a, b]\), which may be \([-\infty, \infty]\).
Luckily, the important properties of our inner product do not depend on the choice of integration domain.</p>

<h2 id="proofs-1">Proofs</h2>

<p>Below, we’ll briefly cover functions from \(\mathbb{R}\) to \(\mathbb{C}\).
In this space, our intuitive notion of similarity still applies, but we’ll use a slightly more general inner product:</p><p>

\[\langle f,g \rangle = \int_a^b f[x]\overline{g[x]}\, dx\]

</p><p>Where \(\overline{x}\) denotes <a href="https://en.wikipedia.org/wiki/Complex_conjugate">conjugation</a>, i.e. \(\overline{a + bi} = a - bi\).</p>

<p>Like other vector space operations, an inner product must satisfy several axioms:</p>

<div>
  
  <div>
  <p>
  For all vectors <span>$$\mathbf{u}, \mathbf{v} \in \mathcal{V}$$</span>:

  $$\langle \mathbf{u}, \mathbf{v} \rangle = \overline{\langle \mathbf{v}, \mathbf{u} \rangle}$$

  Conjugation may be taken outside the integral, making this one easy:

  $$\begin{align*} \langle f, g \rangle &amp;= \int_a^b f[x]\overline{g[x]} \, dx \\
  &amp;= \int_a^b \overline{g[x]\overline{f[x]}} \, dx \\
  &amp;= \overline{\int_a^b g[x]\overline{f[x]} \, dx} \\
  &amp;= \overline{\langle g, f \rangle}
  \end{align*}$$

  Note that we require <em>conjugate</em> symmetry because it implies <span>$$\langle\mathbf{u}, \mathbf{u}\rangle = \overline{\langle\mathbf{u}, \mathbf{u}\rangle}$$</span>, i.e. the inner product of a vector with itself is real.
  </p>
  </div>
</div>

<div>
  
  <div>
  <div><p>
  For all vectors <span>$$\mathbf{u}, \mathbf{v}, \mathbf{w} \in \mathcal{V}$$</span> and scalars <span>$$\alpha, \beta \in \mathbb{F}$$</span>:

  $$\langle \alpha \mathbf{u} + \beta \mathbf{v}, \mathbf{w} \rangle = \alpha\langle \mathbf{u}, \mathbf{w} \rangle + \beta\langle \mathbf{v}, \mathbf{w} \rangle $$

  The proof follows from linearity of integration, as well as our vector space axioms:

</p><p>

\[\begin{align*} \langle \alpha f + \beta g, h \rangle &amp;= \int_a^b (\alpha f + \beta g)[x]\overline{h[x]} \, dx \\
&amp;= \int_a^b (\alpha f[x] + \beta g[x])\overline{h[x]} \, dx \\
&amp;= \int_a^b \alpha f[x]\overline{h[x]} + \beta g[x]\overline{h[x]} \, dx \\
&amp;= \alpha\int_a^b f[x]\overline{h[x]}\, dx + \beta\int_a^b g[x]\overline{h[x]} \, dx \\
&amp;= \alpha\langle f, h \rangle + \beta\langle g, h \rangle
\end{align*}\]

      </p>
<p>

\[\begin{align*} &amp;\langle \alpha f + \beta g, h \rangle\\ &amp;= \int_a^b (\alpha f + \beta g)[x]\overline{h[x]} \, dx \\
&amp;= \int_a^b (\alpha f[x] + \beta g[x])\overline{h[x]} \, dx \\
&amp;= \int_a^b \alpha f[x]\overline{h[x]} + \beta g[x]\overline{h[x]} \, dx \\
&amp;= \alpha\int_a^b f[x]\overline{h[x]}\, dx\, +\\&amp;\hphantom{==} \beta\int_a^b g[x]\overline{h[x]} \, dx \\
&amp;= \alpha\langle f, h \rangle + \beta\langle g, h \rangle
\end{align*}\]

      </p><p>

  Given conjugate symmetry, an inner product is also <a href="https://en.wikipedia.org/wiki/Antilinear_map">antilinear</a> in the second argument.
  </p></div>
  </div>
</div>

<div>
  
  <div>
  <div><p>
  For all <span>$$\mathbf{u} \in \mathcal{V}$$</span>:

  $$ \begin{cases} \langle \mathbf{u}, \mathbf{u} \rangle = 0 &amp; \text{if } \mathbf{u} = \mathbf{0} \\ \langle \mathbf{u}, \mathbf{u} \rangle &gt; 0 &amp; \text{otherwise} \end{cases} $$

  By conjugate symmetry, we know <span>$$\langle f, f \rangle$$</span> is real, so we can compare it with zero.
  </p></div>
  </div>
</div>

<p>Along with the definition \(\|f\| = \sqrt{\langle f, f \rangle}\), these properties entail a variety of important results, including the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Cauchy–Schwarz</a> and <a href="https://en.wikipedia.org/wiki/Triangle_inequality">triangle</a> inequalities.</p>

<hr/>



<p>Diagonalization is already a powerful technique, but we’re building up to an even more important result regarding <em>orthonormal</em> eigenbases.
In an inner product space, an orthonormal basis must satisfy two conditions: each vector is unit length, and all vectors are mutually orthogonal.</p>

<div>
<p>
$$ \begin{cases} \langle\mathbf{u}_i,\mathbf{u}_i\rangle = 1 &amp; \forall i \\ \langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0 &amp; \forall i \neq j \end{cases} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/orthonormal.svg"/>
</p>
</div>

<p>A matrix consisting of orthonormal columns is known as an <em>orthogonal</em> matrix.
Orthogonal matrices represent <em>rotations</em> of the standard basis.</p>

<p>In an inner product space, matrix-vector multiplication computes the inner product of the vector with each <em>row</em> of the matrix.
Something interesting happens when we multiply an orthogonal matrix \(\mathbf{U}\) by its transpose:</p>

<p>

\[\begin{align*}
\mathbf{U}^T\mathbf{U} &amp;=
  \begin{bmatrix}\text{---} &amp; \mathbf{u}_1 &amp; \text{---} \\ \text{---} &amp; \mathbf{u}_2 &amp; \text{---} \\ \text{---} &amp; \mathbf{u}_3 &amp; \text{---} \end{bmatrix}
  \begin{bmatrix}\vert &amp; \vert &amp; \vert \\ \mathbf{u}_1 &amp; \mathbf{u}_2 &amp; \mathbf{u}_3 \\ \vert &amp; \vert &amp; \vert \end{bmatrix} \\
&amp;= \begin{bmatrix} \langle \mathbf{u}_1, \mathbf{u}_1 \rangle &amp; \langle \mathbf{u}_1, \mathbf{u}_2 \rangle &amp; \langle \mathbf{u}_1, \mathbf{u}_3 \rangle \\
                   \langle \mathbf{u}_2, \mathbf{u}_1 \rangle &amp; \langle \mathbf{u}_2, \mathbf{u}_2 \rangle &amp; \langle \mathbf{u}_2, \mathbf{u}_3 \rangle \\
                   \langle \mathbf{u}_3, \mathbf{u}_1 \rangle &amp; \langle \mathbf{u}_3, \mathbf{u}_2 \rangle &amp; \langle \mathbf{u}_3, \mathbf{u}_3 \rangle \end{bmatrix} \\
&amp;= \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \\
&amp;= \mathcal{I}
\end{align*}\]

</p>

<p>Since \(\mathbf{U}^T\mathbf{U} = \mathcal{I}\) (and \(\mathbf{U}\mathbf{U}^T = \mathcal{I}\)), we’ve found that the transpose of \(\mathbf{U}\) is equal to its inverse.</p>

<p>When diagonalizing \(\mathbf{A}\), we used \(\mathbf{U}\) to transform vectors from our <em>eigenbasis</em> to the standard basis.
Conversely, its inverse transformed vectors from the <em>standard basis</em> to our eigenbasis.
If \(\mathbf{U}\) happens to be orthogonal, transforming a vector \(\mathbf{x}\) into the eigenbasis is equivalent to <em>projecting</em> \(\mathbf{x}\) onto each eigenvector.</p>

<div>
<p>
$$ \mathbf{U}^{-1}\mathbf{x} = \mathbf{U}^T\mathbf{x} = \begin{bmatrix}\langle \mathbf{u}_1, \mathbf{x} \rangle \\ \langle \mathbf{u}_2, \mathbf{x} \rangle \\ \langle \mathbf{u}_3, \mathbf{x} \rangle \end{bmatrix} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/orthoproj.svg"/>
</p>
</div>

<p>Additionally, the diagonalization of \(\mathbf{A}\) becomes quite simple:</p>

<p>

\[\begin{align*} \mathbf{A} &amp;= \mathbf{U\Lambda U^T} \\
&amp;= \begin{bmatrix}\vert &amp; \vert &amp; \vert \\ \mathbf{u}_1 &amp; \mathbf{u}_2 &amp; \mathbf{u}_3 \\ \vert &amp; \vert &amp; \vert \end{bmatrix}
 \begin{bmatrix}\lambda_1 &amp; 0 &amp; 0 \\ 0 &amp; \lambda_2 &amp; 0 \\ 0 &amp; 0 &amp; \lambda_3 \end{bmatrix}
  \begin{bmatrix}\text{---} &amp; \mathbf{u}_1 &amp; \text{---} \\ \text{---} &amp; \mathbf{u}_2 &amp; \text{---} \\ \text{---} &amp; \mathbf{u}_3 &amp; \text{---} \end{bmatrix} \end{align*}\]

</p>
<p>

\[\begin{align*} \mathbf{A} &amp;= \mathbf{U\Lambda U^T} \\
&amp;= \begin{bmatrix}\vert &amp; \vert &amp; \vert \\ \mathbf{u}_1 &amp; \mathbf{u}_2 &amp; \mathbf{u}_3 \\ \vert &amp; \vert &amp; \vert \end{bmatrix}
  \\ &amp;\phantom{=} \begin{bmatrix}\lambda_1 &amp; 0 &amp; 0 \\ 0 &amp; \lambda_2 &amp; 0 \\ 0 &amp; 0 &amp; \lambda_3 \end{bmatrix}
  \\ &amp;\phantom{=}  \begin{bmatrix}\text{---} &amp; \mathbf{u}_1 &amp; \text{---} \\ \text{---} &amp; \mathbf{u}_2 &amp; \text{---} \\ \text{---} &amp; \mathbf{u}_3 &amp; \text{---} \end{bmatrix} \end{align*}\]

</p>

<p>Given an orthogonal diagonalization of \(\mathbf{A}\), we can deduce that \(\mathbf{A}\) must be <em>symmetric</em>, i.e. \(\mathbf{A} = \mathbf{A}^T\).</p><p>

\[\begin{align*} \mathbf{A}^T &amp;= (\mathbf{U\Lambda U}^T)^T \\
&amp;= {\mathbf{U}^T}^T \mathbf{\Lambda }^T \mathbf{U}^T \\
&amp;= \mathbf{U\Lambda U}^T \\
&amp;= \mathbf{A}
\end{align*}\]

</p><p>The <a href="https://en.wikipedia.org/wiki/Spectral_theorem">spectral theorem</a> states that the converse is also true: \(\mathbf{A}\) is symmetric if and only if it admits an orthonormal eigenbasis with real eigenvalues.
Proving this result is <a href="https://mast.queensu.ca/~br66/419/spectraltheoremproof.pdf">somewhat involved</a> in finite dimensions and <a href="https://www.diva-portal.org/smash/get/diva2:1598080/FULLTEXT01.pdf">very involved</a> in infinite dimensions, so we won’t reproduce the proofs here.</p>

<h2 id="self-adjoint-operators">Self-Adjoint Operators</h2>

<p>We can generalize the spectral theorem to our space of functions, where it states that a <em>self-adjoint</em> operator admits an orthonormal eigenbasis with real eigenvalues.<sup id="fnref:symmetric" role="doc-noteref"><a href="#fn:symmetric" rel="footnote">6</a></sup></p>

<p>Denoted as \(\mathbf{A}^{\hspace{-0.1em}\star\hspace{0.1em}}\), the <em>adjoint</em> of an operator \(\mathbf{A}\) is defined by the following relationship.</p><p>

\[\langle \mathbf{Ax}, \mathbf{y} \rangle = \langle \mathbf{x}, \mathbf{A}^{\hspace{-0.1em}\star\hspace{0.1em}}\mathbf{y} \rangle\]

</p><p>When \(\mathbf{A} = \mathbf{A}^\star\), we say that \(\mathbf{A}\) is <em>self-adjoint</em>.</p>

<p>The adjoint can be thought of as a generalized transpose—but it’s not obvious what that means in infinite dimensions.
We will simply use our functional inner product to determine whether an operator is self-adjoint.</p>

<h2 id="the-laplace-operator">The Laplace Operator</h2>



<p>Earlier, we weren’t able to diagonalize (real) differentiation, so it must not be self-adjoint.
Therefore, we will explore another fundamental operator, the <a href="https://en.wikipedia.org/wiki/Laplace_operator">Laplacian</a>.</p>

<p>There are <a href="https://www.youtube.com/watch?v=oEq9ROl9Umk">many equivalent definitions of the Laplacian</a>, but in our space of one-dimensional functions, it’s just the second derivative. We will hence restrict our domain to twice-differentiable functions.</p><p>

\[\Delta f = \frac{\partial^2 f}{\partial x^2}\]

</p><p>We may compute \(\Delta^\star\) using two integrations by parts:</p>

<p>

\[\begin{align*}
\left\langle \Delta f[x], g[x] \right\rangle &amp;= \int_a^b f^{\prime\prime}[x] g[x]\, dx \\
&amp;=  f^\prime[x]g[x]\Big|_a^b - \int_a^b f^{\prime}[x] g^{\prime}[x]\, dx \\
&amp;=  (f^\prime[x]g[x] - f[x]g^{\prime}[x])\Big|_a^b + \int_a^b f[x] g^{\prime\prime}[x]\, dx \\
&amp;=  \left\langle f[x], \Delta g[x] \right\rangle
\end{align*}\]

</p>
<p>

\[\begin{align*}
\left\langle \Delta f[x], g[x] \right\rangle &amp;= \int_a^b f^{\prime\prime}[x] g[x]\, dx \\
&amp;=  f^\prime[x]g[x]\Big|_a^b - \int_a^b f^{\prime}[x] g^{\prime}[x]\, dx \\
&amp;=  (f^\prime[x]g[x] - f[x]g^{\prime}[x])\Big|_a^b \\&amp;\hphantom{=}+ \int_a^b f[x] g^{\prime\prime}[x]\, dx \\
&amp;=  \left\langle f[x], \Delta g[x] \right\rangle
\end{align*}\]

</p>

<p>In the final step, we assume that \((f^\prime[x]g[x] - f[x]g^{\prime}[x])\big|_a^b = 0\), which is not true in general.
To make our conclusion valid, we will constrain our domain to only include functions satisfying this boundary condition.
Specifically, we will only consider <em>periodic</em> functions with period \(b-a\).
These functions have the same value and derivative at \(a\) and \(b\), so the additional term vanishes.</p>

<p>For simplicity, we will also assume our domain to be \([0,1]\).
For example:</p>

<p><img src="https://thenumb.at/assets/functions-are-vectors/periodic.png"/></p>

<p>Therefore, the Laplacian is self-adjoint…almost.
Technically, we’ve shown that the Laplacian is symmetric, not that \(\Delta = \Delta^\star\).
This is a subtle point, and it’s possible to prove self-adjointness, so we will omit this detail.</p>

<h2 id="laplacian-eigenfunctions">Laplacian Eigenfunctions</h2>



<p>Applying the spectral theorem tells us that the Laplacian admits an orthonormal eigenbasis.
Let’s find it.<sup id="fnref:thisspace" role="doc-noteref"><a href="#fn:thisspace" rel="footnote">7</a></sup></p>

<p>Since the Laplacian is simply the second derivative, real exponentials would still be eigenfunctions—but they’re not periodic, so we’ll have to exclude them.</p><p>

\[\color{red} \Delta e^{\psi x} = \psi^2 e^{\psi x}\]

</p><p>Luckily, a new class of periodic eigenfunctions appears:</p><p>

\[\begin{align*} \Delta \sin[\psi x] &amp;= -\psi^2 \sin[\psi x] \\
\Delta \cos[\psi x] &amp;= -\psi^2 \cos[\psi x] \end{align*}\]

</p><p>If we allow our diagonalization to introduce complex numbers, we can also consider functions from \(\mathbb{R}\) to \(\mathbb{C}\) .
Here, <em>purely complex</em> exponentials are eigenfunctions with <em>real</em> eigenvalues.</p>

<p>

\[\begin{align*} \Delta e^{\psi i x} &amp;= (\psi i)^2e^{\psi i x} \\
&amp;= -\psi^2 e^{\psi i x} \\
&amp;= -\psi^2 (\cos[\psi x] + i\sin[\psi x]) \tag{Euler&#39;s formula} \end{align*}\]

</p>
<p>

\[\begin{align*} \Delta e^{\psi i x} &amp;= (\psi i)^2e^{\psi i x} \\
&amp;= -\psi^2 e^{\psi i x} \\
&amp;= -\psi^2 (\cos[\psi x] + i\sin[\psi x]) \\&amp; \tag{Euler&#39;s formula} \end{align*}\]

</p>

<p>Using Euler’s formula, we can see that these two perspectives are equivalent: they both introduce \(\sin\) and \(\cos\) as eigenfunctions.
Either path can lead to our final result, but we’ll stick with the more compact complex case.</p>

<p>We also need to constrain the set of eigenfunctions to be periodic on \([0,1]\).
As suggested above, we can pick out the eigenvalues that are an integer multiple of \(2\pi\).</p><p>

\[e^{2\pi \xi i x} = \cos[2\pi \xi x] + i\sin[2\pi \xi x]\]

</p><p>Our set of eigenfunctions is therefore \(e^{2\pi \xi i x}\) for all integers \(\xi\).</p>

<h2 id="diagonalizing-the-laplacian">Diagonalizing the Laplacian</h2>

<p>Now that we’ve found suitable eigenfunctions, we can construct an orthonormal basis.</p>

<p>Our collection of eigenfunctions is linearly independent, as each one corresponds to a distinct eigenvalue.
Next, we can check for orthogonality and unit magnitude:</p>

<div>
<div>
    
    <div>
      <div>
        <p>Compute the inner product of \(e^{2\pi \xi_1 i x}\) and \(e^{2\pi \xi_2 i x}\) for \(\xi_1 \neq \xi_2\):</p>

        <p>

\[\begin{align*} \langle e^{2\pi\xi_1 i x}, e^{2\pi\xi_2 i x} \rangle &amp;= \int_0^1 e^{2\pi\xi_1 i x} \overline{e^{2\pi\xi_2 i x}}\, dx \\
&amp;= \int_0^1 (\cos[2\pi\xi_1 x] + i\sin[2\pi\xi_1 x])(\cos[2\pi\xi_2 x] - i\sin[2\pi\xi_2 x])\, dx \\
&amp;= \int_0^1 \cos[2\pi\xi_1 x]\cos[2\pi\xi_2 x] - i\cos[2\pi\xi_1 x]\sin[2\pi\xi_2 x] +\\ &amp;\phantom{= \int_0^1} i\sin[2\pi\xi_1 x]\cos[2\pi\xi_2 x] + \sin[2\pi\xi_1 x]\sin[2\pi\xi_2 x] \, dx \\
&amp;= \int_0^1 \cos[2\pi(\xi_1-\xi_2)x] + i\sin[2\pi(\xi_1-\xi_2)x]\, dx \\
&amp;= \frac{1}{2\pi(\xi_1-\xi_2)}\left(\sin[2\pi(\xi_1-\xi_2] x)\Big|_0^1 - i\cos[2\pi(\xi_1-\xi_2) x]\Big|_0^1\right) \\
&amp;= 0
\end{align*}\]

        </p>
        <p>

\[\begin{align*} &amp;\langle e^{2\pi\xi_1 i x}, e^{2\pi\xi_2 i x} \rangle \\ &amp;= \int_0^1 e^{2\pi\xi_1 i x} \overline{e^{2\pi\xi_2 i x}}\, dx \\
&amp;= \int_0^1 (\cos[2\pi\xi_1 x] + i\sin[2\pi\xi_1 x])\cdot\\ &amp;\hphantom{==\int_0^1}(\cos[2\pi\xi_2 x] - i\sin[2\pi\xi_2 x])\, dx \\
&amp;= \int_0^1 \cos[2\pi\xi_1 x]\cos[2\pi\xi_2 x]\, -\\ &amp;\hphantom{==\int_0^1} i\cos[2\pi\xi_1 x]\sin[2\pi\xi_2 x] +\\ &amp;\hphantom{== \int_0^1} i\sin[2\pi\xi_1 x]\cos[2\pi\xi_2 x]\, +\\ &amp;\hphantom{==\int_0^1} \sin[2\pi\xi_1 x]\sin[2\pi\xi_2 x] \, dx \\
&amp;= \int_0^1 \cos[2\pi(\xi_1-\xi_2)x]\, +\\ &amp;\hphantom{==\int_0^1} i\sin[2\pi(\xi_1-\xi_2)x]\, dx \\
&amp;= \frac{1}{2\pi(\xi_1-\xi_2)}\Big(\sin[2\pi(\xi_1-\xi_2] x)\Big|_0^1\, -\\ &amp;\hphantom{=== \frac{1}{2\pi(\xi_1-\xi_2)}} i\cos[2\pi(\xi_1-\xi_2) x]\Big|_0^1\Big) \\
&amp;= 0
\end{align*}\]

        </p>

        <p>Note that the final step is valid because \(\xi_1-\xi_2\) is a non-zero integer.</p>

        <p>This result also applies to any domain \([a,b]\), given functions periodic on \([a,b]\).</p>

        <p>It’s possible to further generalize to \([-\infty,\infty]\), but doing so requires a <a href="https://math.stackexchange.com/questions/394237/understanding-weighted-inner-product-and-weighted-norms">weighted</a> inner product.</p>
      </div>
    </div>
  </div>

<div>
    
    <div>
      <div>
        <p>It’s easy to show that all candidate functions have norm one:</p><p>

\[\begin{align*} \langle e^{2\pi\xi i x}, e^{2\pi\xi i x} \rangle &amp;= \int_0^1 e^{2\pi\xi i x}\overline{e^{2\pi\xi i x}} \\
&amp;= \int_0^1 e^{2\pi\xi i x}e^{-2\pi\xi i x}\, dx \\&amp;= \int_0^1 1\, dx\\ &amp;= 1 \end{align*}\]

        </p><p>With the addition of a constant factor \(\frac{1}{b-a}\), this result generalizes to any \([a,b]\).</p>

        <p>It’s possible to further generalize to \([-\infty,\infty]\), but doing so requires a <a href="https://math.stackexchange.com/questions/394237/understanding-weighted-inner-product-and-weighted-norms">weighted</a> inner product.</p>
      </div>
    </div>
  </div>
</div>

<p>The final step is to show that all functions in our domain can be represented by a linear combination of eigenfunctions.
To do so, we will find an invertible operator \(\mathcal{L}\) representing the proper change of basis.</p>

<p>Critically, since our eigenbasis is <em>orthonormal</em>, we can intuitively consider the inverse of \(\mathcal{L}\) to be its transpose.</p>

<p>

\[\mathcal{I} = \mathcal{L}\mathcal{L}^{-1} = \mathcal{L}\mathcal{L}^{T} = \begin{bmatrix} \vert &amp; \vert &amp;  &amp; \\ e^{2\pi\xi_1 i x} &amp; e^{2\pi\xi_2 i x} &amp; \dots \\ \vert &amp; \vert &amp;  \end{bmatrix}\begin{bmatrix} \text{---} &amp; e^{2\pi\xi_1 i x} &amp; \text{---} \\ \text{---} &amp; e^{2\pi\xi_2 i x} &amp; \text{---} \\ &amp; \vdots &amp; \end{bmatrix}\]

</p>
<p>

\[\begin{align*} \mathcal{I} &amp;= \mathcal{L}\mathcal{L}^{-1} \\&amp;= \mathcal{L}\mathcal{L}^{T} \\&amp;= \begin{bmatrix} \vert &amp; \vert &amp;  &amp; \\ e^{2\pi\xi_1 i x} &amp; e^{2\pi\xi_2 i x} &amp; \dots \\ \vert &amp; \vert &amp;  \end{bmatrix}\\ &amp; \phantom{=} \begin{bmatrix} \text{---} &amp; e^{2\pi\xi_1 i x} &amp; \text{---} \\ \text{---} &amp; e^{2\pi\xi_2 i x} &amp; \text{---} \\ &amp; \vdots &amp; \end{bmatrix} \end{align*}\]

</p>

<p>This visualization suggests that \(\mathcal{L}^Tf\) computes the inner product of \(f\) with each eigenvector.</p><p>

\[\mathcal{L}^Tf = \begin{bmatrix}\langle f, e^{2\pi\xi_1 i x} \rangle \\ \langle f, e^{2\pi\xi_2 i x}  \rangle \\ \vdots \end{bmatrix}\]

</p><p>Which is highly reminiscent of the finite-dimensional case, where we projected onto each eigenvector of an orthogonal eigenbasis.</p>

<p>This insight allows us to write down the product \(\mathcal{L}^Tf\) as an integer function \(\hat{f}[\xi]\).
Note that the complex inner product conjugates the second argument, so the exponent is negated.</p><p>

\[(\mathcal{L}^Tf)[\xi] = \hat{f}[\xi] = \int_0^1 f[x]e^{-2\pi\xi i x}\, dx\]

</p><p>Conversely, \(\mathcal{L}\) converts \(\hat{f}\) back to the standard basis.
It simply creates a linear combination of eigenfunctions.</p><p>

\[(\mathcal{L}\hat{f})[x] = f[x] = \sum_{\xi=-\infty}^\infty \hat{f}[\xi] e^{2\pi\xi i x}\]

</p><p>These operators are, in fact, inverses of each other, but a rigorous proof is beyond the scope of this post.
Therefore, we’ve diagonalized the Laplacian:</p>

<p>

\[\begin{align*} \Delta &amp;= \mathcal{L} \mathcal{D} \mathcal{L}^T  \\
&amp;=
\begin{bmatrix} \vert &amp; \vert &amp;  &amp; \\ e^{2\pi\xi_1 i x} &amp; e^{2\pi\xi_2 i x} &amp; \dots \\ \vert &amp; \vert &amp;  \end{bmatrix}
\begin{bmatrix} -(2\pi\xi_1)^2 &amp; 0 &amp; \dots \\ 0 &amp; -(2\pi\xi_2)^2 &amp; \dots \\ \vdots &amp; \vdots &amp; \ddots \end{bmatrix}
\begin{bmatrix} \text{---} &amp; e^{2\pi\xi_1 i x} &amp; \text{---} \\ \text{---} &amp; e^{2\pi\xi_2 i x} &amp; \text{---} \\ &amp; \vdots &amp; \end{bmatrix}
\end{align*}\]

</p>
<p>

\[\begin{align*} \Delta &amp;= \mathcal{L} \mathcal{D} \mathcal{L}^T  \\
&amp;=
\begin{bmatrix} \vert &amp; \vert &amp;  &amp; \\ e^{2\pi\xi_1 i x} &amp; e^{2\pi\xi_2 i x} &amp; \dots \\ \vert &amp; \vert &amp;  \end{bmatrix}
\\ &amp; \phantom{=} \begin{bmatrix} -(2\pi\xi_1)^2 &amp; 0 &amp; \dots \\ 0 &amp; -(2\pi\xi_2)^2 &amp; \dots \\ \vdots &amp; \vdots &amp; \ddots \end{bmatrix}
\\ &amp; \phantom{=} \begin{bmatrix} \text{---} &amp; e^{2\pi\xi_1 i x} &amp; \text{---} \\ \text{---} &amp; e^{2\pi\xi_2 i x} &amp; \text{---} \\ &amp; \vdots &amp; \end{bmatrix}
\end{align*}\]

</p>

<p>Although \(\mathcal{L}^T\) transforms our real-valued function into a complex-valued function, \(\Delta\) as a whole still maps real functions to real functions.
Next, we’ll see how \(\mathcal{L}^T\) is itself an incredibly useful transformation.</p>

<hr/>



<p>In this section, we’ll explore several applications in <a href="https://en.wikipedia.org/wiki/Signal_processing">signal processing</a>, each of which arises from diagonalizing the Laplacian on a new domain.</p>





<p>If you’re familiar with Fourier methods, you likely noticed that \(\hat{f}\) encodes the Fourier series of \(f\).
That’s because a Fourier transform is a change of basis into the Laplacian eigenbasis!</p>

<p>This basis consists of <em>waves</em>, which makes \(\hat{f}\) is a particularly interesting representation for \(f\).
For example, consider evaluating \(\hat{f}[1]\):</p>

<p>

\[\hat{f}[1] = \int_0^1 f[x] e^{-2\pi i x}\, dx = \int_0^1 f[x](\cos[2\pi x] - i\sin[2\pi x])\, dx\]

</p>
<p>

\[\begin{align*} \hat{f}[1] &amp;= \int_0^1 f[x] e^{-2\pi i x}\, dx \\&amp;= \int_0^1 f[x](\cos[2\pi x] - i\sin[2\pi x])\, dx \end{align*}\]

</p>

<p>This integral measures how much of \(f\) is represented by waves of frequency (positive) 1.
Naturally, \(\hat{f}[\xi]\) computes the same quantity for any integer frequency \(\xi\).</p>

<p>

\[\hphantom{aaa} {\color{#9673A6} \text{Real}\left[e^{2\pi i \xi x}\right]}\,\,\,\,\,\,\,\, {\color{#D79B00} \text{Complex}\left[e^{2\pi i \xi x}\right]}\]

</p><svg id="real_eigen" style="margin-top: 20px; margin-bottom: -20px;" width="500" height="250"></svg>


<p>Therefore, we say that \(\hat{f}\) expresses our function in the <em>frequency domain</em>.
To illustrate this point, we’ll use a Fourier series to decompose a piecewise linear function into a collection of waves.<sup id="fnref:vibecheck" role="doc-noteref"><a href="#fn:vibecheck" rel="footnote">8</a></sup>
Since our new basis is orthonormal, the transform is easy to invert by re-combining the waves.</p>

<p>Here, the \(\color{#9673A6}\text{purple}\) curve is \(f\); the \(\color{#D79B00}\text{orange}\) curve is a reconstruction of \(f\) from the first \(N\) coefficients of \(\hat{f}\).
Try varying the number of coefficients and moving the \(\color{#9673A6}\text{purple}\) dots to effect the results.</p>

<div><p>

$$ \hphantom{aaa} {\color{#9673A6} f[x]}\,\,\,\,\,\,\,\,\,\,\,\, {\color{#D79B00} \sum_{\xi=-N}^N \hat{f}[\xi]e^{2\pi i \xi x}} $$

</p><p>

Additionally, explore the individual basis functions making up our result:

</p><table>
<tbody><tr>
<td>$$\hat{f}[0]$$</td>
<td>$$\hat{f}[1]e^{2\pi i x}$$</td>
<td>$$\hat{f}[2]e^{4\pi i x}$$</td>
<td>$$\hat{f}[3]e^{6\pi i x}$$</td>
</tr>

<tr>
<td rowspan="2"><svg id="real_fourier_0" width="200" height="100"></svg></td>
<td><svg id="real_fourier_c0" width="200" height="100"></svg></td>
<td><svg id="real_fourier_c1" width="200" height="100"></svg></td>
<td><svg id="real_fourier_c2" width="200" height="100"></svg></td>
</tr>

<tr>
<td><svg id="real_fourier_s0" width="200" height="100"></svg></td>
<td><svg id="real_fourier_s1" width="200" height="100"></svg></td>
<td><svg id="real_fourier_s2" width="200" height="100"></svg></td>
</tr>
</tbody></table>
</div>

<p>Many interesting operations become easy to compute in the frequency domain.
For example, by simply dropping Fourier coefficients beyond a certain threshold, we can reconstruct a smoothed version of our function.
This technique is known as a <em>low-pass filter</em>—try it out above.</p>



<p>Computationally, Fourier series are especially useful for <em>compression</em>.
Encoding a function \(f\) in the standard basis takes a lot of space, since we store a separate result for each input.
If we instead express \(f\) in the Fourier basis, we only need to store a few coefficients—we’ll be able to approximately reconstruct \(f\) by re-combining the corresponding basis functions.</p>

<p>So far, we’ve only defined a Fourier transform for functions on \(\mathbb{R}\).
Luckily, the transform arose via diagonalizing the Laplacian, and the Laplacian is not limited to one-dimensional functions.
<strong>In fact, wherever we can define a Laplacian, we can find a corresponding Fourier transform.</strong><sup id="fnref:morelaplacians" role="doc-noteref"><a href="#fn:morelaplacians" rel="footnote">9</a></sup></p>

<p>For example, in two dimensions, the Laplacian becomes a sum of second derivatives.</p><p>

\[\Delta f[x,y] = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2}\]

</p><p>For the domain \([0,1]\times[0,1]\), we’ll find a familiar set of periodic eigenfunctions.</p>

<p>

\[e^{2\pi i(nx + my)} = \cos[2\pi(nx + my)] + i\sin[2\pi(nx + my)]\]

</p>
<p>

\[\begin{align*} e^{2\pi i(nx + my)} &amp;= \cos[2\pi(nx + my)]\, + \\ &amp;\phantom{=}\, i\sin[2\pi(nx + my)] \end{align*}\]

</p>

<p>Where \(n\) and \(m\) are both integers.
Let’s see what these basis functions look like:</p>

<div>
<div><p>
\[{\color{#9673A6} \text{Real}\left[e^{2\pi i(nx + my)}\right]}\]

    </p><canvas width="320" height="256" id="2d_eigen_re"></canvas>
  </div>
<div><p>
\[{\color{#D79B00} \text{Complex}\left[e^{2\pi i(nx + my)}\right]}\]

    </p><canvas width="320" height="256" id="2d_eigen_im"></canvas>
  </div>
</div>



<p>Just like the 1D case, the corresponding Fourier transform is a change of basis into the Laplacian’s orthonormal eigenbasis.
Above, we decomposed a 1D function into a collection of 1D waves—here, we equivalently decompose a 2D image into a collection of 2D waves.</p>

<div>
<div>
<p>
\[\phantom{\Bigg|} {\color{#9673A6} f[x,y]}\]

    </p>
<canvas width="256" height="256" id="2d_image_before"></canvas>
</div>
<div>
<p>
\[{\color{#D79B00} \sum_{n=-N}^N \sum_{m=-N}^N \hat{f}[n,m]e^{2\pi i(nx + my)}}\]

    </p>
<canvas width="256" height="256" id="2d_image_after"></canvas>
</div>
</div>



<p>A variant of the 2D Fourier transform is at the core of many image compression algorithms, including JPEG.</p>

<h2 id="spherical-harmonics">Spherical Harmonics</h2>

<p>Computer graphics is often concerned with functions on the unit sphere, so let’s see if we can find a corresponding Fourier transform.
In <a href="https://thenumb.at/Spherical-Integration">spherical coordinates</a>, the Laplacian can be defined as follows:</p>

<p>

\[\Delta f(\theta, \phi) = \frac{1}{\sin[\theta]}\frac{\partial}{\partial \theta}\left(\sin[\theta] \frac{\partial f}{\partial \theta}\right) + \frac{1}{\sin^2[\theta]}\frac{\partial^2 f}{\partial \phi^2}\]

</p>
<p>

\[\begin{align*}
\Delta f(\theta, \phi) &amp;= \frac{1}{\sin[\theta]}\frac{\partial}{\partial \theta}\left(\sin[\theta] \frac{\partial f}{\partial \theta}\right)\, + \\ &amp;\phantom{=} \frac{1}{\sin^2[\theta]}\frac{\partial^2 f}{\partial \phi^2}
\end{align*}\]

</p>

<p>We won’t go through the full derivation, but this Laplacian admits an orthornormal eigenbasis known as the <em>spherical harmonics</em>.<sup id="fnref:harmonics" role="doc-noteref"><a href="#fn:harmonics" rel="footnote">10</a></sup></p><p>

\[Y_\ell^m[\theta, \phi] = N_\ell^m P_\ell^m[\cos[\theta]] e^{im\phi}\]

</p><p>Where \(Y_\ell^m\) is the spherical harmonic of degree \(\ell \ge 0\) and order \(m \in [-\ell,\ell]\). Note that \(N_\ell^m\) is a constant and \(P_\ell^m\) are the <a href="https://en.wikipedia.org/wiki/Associated_Legendre_polynomials">associated Legendre polynomials</a>.<sup id="fnref:legendre" role="doc-noteref"><a href="#fn:legendre" rel="footnote">11</a></sup></p>

<p>

\[\hphantom{aa} {\color{#9673A6} \text{Real}\left[Y_\ell^m[\theta,\phi]\right]}\,\,\,\,\,\,\,\, {\color{#D79B00} \text{Complex}\left[Y_\ell^m[\theta,\phi]\right]}\]

</p><canvas id="spr_eigen" width="450" height="350"></canvas>


<p>As above, we define the spherical Fourier transform as a change of basis into the spherical harmonics.
In game engines, this transform is often used to compress diffuse <em>environment maps</em> (i.e. spherical images) and global illumination probes.</p>

<div>
<div>
<p>
\[\phantom{\Bigg|} {\color{#9673A6} f[\theta,\phi]}\]

    </p>
<canvas width="320" height="256" id="spr_image_before"></canvas>
</div>
<div>
<p>
\[{\color{#D79B00} \sum_{\ell=0}^N \sum_{m=-\ell}^\ell \hat{f}[\ell,m]\left( Y_\ell^m[\theta,\phi]e^{im\phi} \right)}\]

    </p>
<canvas width="320" height="256" id="spr_image_after"></canvas>
</div>
</div>



<p>You might also recognize spherical harmonics as electron orbitals—quantum mechanics is primarily concerned with the eigenfunctions of linear operators.</p>



<p>Representing functions as vectors underlies many modern algorithms—image compression is only one example.
In fact, because computers can do linear algebra so efficiently, applying linear-algebraic techniques to functions produces a powerful new computational paradigm.</p>

<p>The nascent field of <a href="https://www.youtube.com/watch?v=mas-PUA3OvA&amp;list=PL9_jI1bdZmz0hIrNCMQW1YmZysAiIYSSS">discrete</a> <a href="https://www.ams.org/publications/journals/notices/201710/rnoti-p1153.pdf">differential</a> <a href="http://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf">geometry</a> uses this perspective to build algorithms for three-dimensional <em>geometry processing</em>.
In computer graphics, functions on <em>meshes</em> often represent textures, unwrappings, displacements, or simulation parameters.
DDG gives us a way to faithfully encode such functions as vectors: for example, by associating a value with each vertex of the mesh.</p>

<div>
<p>
$$ \mathbf{f} = \begin{bmatrix}
{\color{#666666} f_1}\\
{\color{#82B366} f_2}\\
{\color{#B85450} f_3}\\
{\color{#6C8EBF} f_4}\\
{\color{#D79B00} f_5}\\
{\color{#9673A6} f_6}\\
{\color{#D6B656} f_7}\\
\end{bmatrix} $$
</p>
<p><img src="https://thenumb.at/assets/functions-are-vectors/mesh.png"/>
</p>
</div>



<p>One particularly relevant result is a Laplace operator for meshes.
A mesh Laplacian is a finite-dimensional matrix, so we can use <a href="https://en.wikipedia.org/wiki/Numerical_linear_algebra">numerical linear algebra</a> to find its eigenfunctions.</p>

<p>As with the continuous case, these functions generalize sine and cosine to a new domain.
Here, we visualize the real and complex parts of each eigenfunction, where the two colors indicate positive vs. negative regions.</p>

<p>

\[\hphantom{aa} {\color{#9673A6} \text{Rea}}{\color{#82B366}\text{l}\left[\psi_N\right]}\,\,\,\,\,\,\,\, {\color{#D79B00} \text{Comp}}{\color{#6C8EBF} \text{lex}\left[\psi_N\right]}\]

</p><canvas id="geom_eigen" width="600" height="350"></canvas>

<div>
<div>
    <div><p id="geom_eigen_N">$$4\text{th Eigenfunction}$$</p>
</div>
  </div></div>

<p>At this point, the implications might be obvious—this eigenbasis is useful for transforming and compressing functions on the mesh.
In fact, by interpreting the vertices’ positions as a function, we can even smooth or sharpen the geometry itself.</p>



<p>There’s far more to signal and geometry processing than we can cover here, let alone the many other applications in engineering, physics, and computer science.
We will conclude with an (incomplete, biased) list of topics for further exploration. See if you can follow the functions-are-vectors thread throughout:</p>

<ul>
  <li>Geometry: <a href="http://www.cs.cmu.edu/~kmcrane/Projects/HeatMethod/index.html">Distances</a>, <a href="https://nmwsharp.com/research/vector-heat-method/">Parallel Transport</a>, <a href="https://geometrycollective.github.io/boundary-first-flattening/">Flattening</a>, <a href="http://www.cs.cmu.edu/~kmcrane/Projects/NonmanifoldLaplace/index.html">Non-manifold Meshes</a>, and <a href="https://dl.acm.org/doi/pdf/10.1145/3386569.3392389">Polygonal Meshes</a></li>
  <li>Simulation: the <a href="https://en.wikipedia.org/wiki/Finite_element_method">Finite Element Method</a>, <a href="http://www.cs.cmu.edu/~kmcrane/Projects/MonteCarloGeometryProcessing/index.html">Monte Carlo PDEs</a>, <a href="https://cseweb.ucsd.edu/~alchern/projects/MinimalCurrent/">Minimal Surfaces</a>, and <a href="https://yhesper.github.io/fc23/fc23.html">Fluid Cohomology</a></li>
  <li>Light Transport: <a href="https://en.wikipedia.org/wiki/Radiosity_(computer_graphics)">Radiosity</a>, <a href="https://graphics.stanford.edu/papers/veach_thesis/thesis.pdf">Operator Formulation (Ch.4)</a>, <a href="https://bartwronski.com/2022/02/15/light-transport-matrices-svd-spectral-analysis-and-matrix-completion/">Low-Rank Approximation</a>, and <a href="https://rgl.epfl.ch/publications">Inverse Rendering</a></li>
  <li>Machine Learning: <a href="https://nmwsharp.com/research/diffusion-net/">DiffusionNet</a>, <a href="https://ranahanocka.github.io/MeshCNN/">MeshCNN</a>, <a href="https://nmwsharp.com/research/neural-physics-subspaces/">Kinematics</a>, <a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">Fourier</a> <a href="https://arxiv.org/pdf/2006.10739.pdf">Features</a>, and <a href="https://rgl.s3.eu-central-1.amazonaws.com/media/papers/Nicolet2021Large.pdf">Inverse Geometry</a></li>
  <li><a href="https://www.youtube.com/watch?v=jvPPXbo87ds">Splines</a>: <a href="http://www.cemyuksel.com/research/interpolating_curves/">C2 Interpolation</a>, <a href="https://ttnghia.github.io/posts/quadratic-approximation-of-cubic-curves/">Quadratic Approximation</a>, and <a href="https://raphlinus.github.io/curves/2023/04/18/bezpath-simplify.html">Simplification</a></li>
</ul>

<p>Thanks to <a href="https://hachiyuki8.github.io/">Joanna Y</a>, <a href="https://yhesper.github.io/">Hesper Yin</a>, and <a href="https://fanpu.io/">Fan Pu Zeng</a> for providing feedback on this post.</p>

<hr/>













































  </div></div>
  </body>
</html>
