<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://blog.salesforceairesearch.com/xgen/">Original</a>
    <h1>XGen-7B, a new 7B foundational model trained on up to 8K length for 1.5T tokens</h1>
    
    <div id="readability-page-1" class="page"><section>
    <div>
      <div>
         
        <div>
          
          <h2 id="tldr">TLDR</h2><p>We trained a series of <strong>7B LLMs named XGen-7B</strong> with standard dense attention on up to <strong>8K</strong> sequence length for up to <strong>1.5T tokens</strong>. We also fine tune the models on public-domain instructional data. The main take-aways are:</p><ul><li>On standard NLP benchmarks, XGen achieves comparable or better results when compared with state-of-the-art open-source LLMs (e.g. MPT, Falcon, LLaMA, Redpajama, OpenLLaMA) of similar model size.</li><li>Our targeted evaluation on long sequence modeling benchmarks show benefits of our 8K-seq models over 2K- and 4K-seq models.</li><li>XGen-7B archives equally strong results both in text (e.g., MMLU, QA) and code (HumanEval) tasks.</li><li>Training cost of $150K on 1T tokens under Google Cloud pricing for TPU-v4.</li></ul><p><strong>Codebase: </strong><a href="https://github.com/salesforce/xGen?ref=blog.salesforceairesearch.com">https://github.com/salesforce/xGen</a></p><hr/><h2 id="why-xgen-7b-with-8k-sequence-length">Why XGen-7B with 8K Sequence Length</h2><p>As LLMs become ubiquitous, their applications to long sequences have been a key focus, especially for applications like summarizing text (potentially interleaved with other data sources like tables and images), writing code, and predicting protein sequences, which require the model to effectively consider long distance structural dependencies. A large context allows a pre-trained LLM to look at customer data (e.g., documents the LLM did not use in training) and responds to useful information seeking queries.</p><p>Yet, most open-source LLMs (e.g., <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/?ref=blog.salesforceairesearch.com">LLaMA</a>, <a href="https://www.mosaicml.com/blog/mpt-7b?ref=blog.salesforceairesearch.com">MPT,</a> <a href="https://falconllm.tii.ae/?ref=blog.salesforceairesearch.com">Falcon</a>) have been trained with a maximum of 2K token sequence length, which is a key limitation in modeling long sequences. Inference time solutions such as <a href="https://arxiv.org/abs/2108.12409?ref=blog.salesforceairesearch.com">ALiBi</a> have yet to be evaluated for larger models (e.g. <a href="https://www.mosaicml.com/blog/mpt-7b?ref=blog.salesforceairesearch.com">MPT-7b-StoryWriter-65k+</a>). Recent <a href="https://arxiv.org/abs/2203.15556?ref=blog.salesforceairesearch.com">work</a> on model scaling has shown that for a given compute budget, the best performances are not necessarily achieved by the largest models, but by smaller models trained on more data (measured by number of tokens). A smaller model is also generally preferred for inference efficiency during serving including on-device serving. In light of this, we train a series of 7B LLMs named XGen with standard dense attention on up to 8K sequence length for up to 1.5T tokens. We also fine tune the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-7B-inst). </p><!--kg-card-begin: html--><table><colgroup><col width="135"/><col width="491"/></colgroup><tbody><tr><td><p dir="ltr"><span>Model</span></p></td><td><p dir="ltr"><span>Description </span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7B-4K-base</span></p></td><td><p dir="ltr"><span>We train for 800B tokens with a sequence length of 2k tokens first, then for another 400B tokens (</span><span>total </span><span>1.2T tokens</span><span>) with 4k. Released under Apache-2.0.</span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7B-8K-base</span></p></td><td><p dir="ltr"><span>Initialized with XGen-7B-4K-base and further trained for 300B more tokens (total </span><span>1.5T tokens</span><span>) with 8K sequence length. </span><span>Released under Apache-2.0.</span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7B-{4K,8K}-inst</span></p></td><td><p dir="ltr"><span>Supervised fine tuned on public domain instructional data including </span><a href="https://huggingface.co/datasets/databricks/databricks-dolly-15k?ref=blog.salesforceairesearch.com"><span>databricks-dolly-15k</span></a><span>, </span><a href="https://huggingface.co/datasets/OpenAssistant/oasst1?ref=blog.salesforceairesearch.com"><span>oasst1</span></a><span>, </span><a href="https://github.com/project-baize/baize-chatbot?ref=blog.salesforceairesearch.com"><span>Baize</span></a><span> and GPT-related datasets</span><span>. Released for research purpose only.</span></p></td></tr></tbody></table><!--kg-card-end: html--><hr/><h2 id="pre-training-data">Pre-training Data<br/></h2><p>We employ a two-stage training strategy, where each stage uses a different data mixture. </p><p><strong>First stage (1.37T tokens)</strong></p><!--kg-card-begin: html--><table><colgroup><col width="210"/><col width="110"/><col width="71"/><col width="152"/></colgroup><tbody><tr><td><p dir="ltr"><span>Dataset name</span></p><br/></td><td><p dir="ltr"><span>Effective number of tokens (B)</span></p></td><td><p dir="ltr"><span>Epochs</span></p></td><td><p dir="ltr"><span>Sampling prop. (%)</span></p></td></tr><tr><td><p dir="ltr"><span>RedPajama-CommonCrawl</span></p></td><td><p dir="ltr"><span>879.37</span></p></td><td><p dir="ltr"><span>1</span></p></td><td><p dir="ltr"><span>63.98</span></p></td></tr><tr><td><p dir="ltr"><span>RedPajama-GitHub</span></p></td><td><p dir="ltr"><span>62.44</span></p></td><td><p dir="ltr"><span>1</span></p></td><td><p dir="ltr"><span>4.54</span></p></td></tr><tr><td><p dir="ltr"><span>RedPajama-Books</span></p></td><td><p dir="ltr"><span>65.18</span></p></td><td><p dir="ltr"><span>2.5</span></p></td><td><p dir="ltr"><span>4.74</span></p></td></tr><tr><td><p dir="ltr"><span>RedPajama-ArXiv</span></p></td><td><p dir="ltr"><span>63.32</span></p></td><td><p dir="ltr"><span>2</span></p></td><td><p dir="ltr"><span>4.61</span></p></td></tr><tr><td><p dir="ltr"><span>RedPajama-StackExchange</span></p></td><td><p dir="ltr"><span>21.38</span></p></td><td><p dir="ltr"><span>1</span></p></td><td><p dir="ltr"><span>1.56</span></p></td></tr><tr><td><p dir="ltr"><span>C4 from 6 CC dumps</span></p></td><td><p dir="ltr"><span>191.5</span></p></td><td><p dir="ltr"><span>0.2</span></p></td><td><p dir="ltr"><span>13.93</span></p></td></tr><tr><td><p dir="ltr"><span>Wikipedia-English</span></p></td><td><p dir="ltr"><span>19.52</span></p></td><td><p dir="ltr"><span>4</span></p></td><td><p dir="ltr"><span>1.42</span></p></td></tr><tr><td><p dir="ltr"><span>Wikipedia-21 other languages</span></p></td><td><p dir="ltr"><span>62.04</span></p></td><td><p dir="ltr"><span>2</span></p></td><td><p dir="ltr"><span>4.51</span></p></td></tr><tr><td><p dir="ltr"><span>Pile_DM_Mathematics</span></p></td><td><p dir="ltr"><span>7.68</span></p></td><td><p dir="ltr"><span>2</span></p></td><td><p dir="ltr"><span>0.56</span></p></td></tr><tr><td><p dir="ltr"><span>Apex code from 6 CC dumps</span></p></td><td><p dir="ltr"><span>2.09</span></p></td><td><p dir="ltr"><span>1</span></p></td><td><p dir="ltr"><span>0.15</span></p></td></tr><tr><td><p dir="ltr"><span>Total</span></p></td><td><p dir="ltr"><span>1374.52</span></p></td><td><br/></td><td><p dir="ltr"><span>100</span></p></td></tr></tbody></table><!--kg-card-end: html--><p>For C4, we processed 6 Common Crawl dumps with C4 pipeline, and deduplicated the documents across different dumps by only keeping the newest timestamp for the documents with the same URL. We trained a linear model, which classifies the C4 data as a Wikipedia-like document vs. a random document. We then chose the top 20% Wikipedia-like documents. For Wikipedia, we cover <strong>22 languages:</strong> bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk, ja, zh, more than LLaMA (20 languages) and MPT (English only).</p><p><strong>Second stage (110B tokens)</strong> </p><p>To support code-generation tasks, in the second stage we mix more code data from <a href="https://github.com/bigcode-project/starcoder?ref=blog.salesforceairesearch.com">Starcoder</a> with the data from Stage 1.  </p><!--kg-card-begin: html--><table><colgroup><col width="186"/><col width="213"/><col width="160"/></colgroup><tbody><tr><td><p dir="ltr"><span>Dataset name</span></p><br/></td><td><p dir="ltr"><span>Number of tokens used (B)</span></p></td><td><p dir="ltr"><span>Sampling prop. (%)</span></p></td></tr><tr><td><p dir="ltr"><span>Data mentioned above</span></p></td><td><p dir="ltr"><span>55</span></p></td><td><p dir="ltr"><span>50%</span></p></td></tr><tr><td><p dir="ltr"><span>BigCode Starcoder</span></p></td><td><p dir="ltr"><span>55</span></p></td><td><p dir="ltr"><span>50%</span></p></td></tr></tbody></table><!--kg-card-end: html--><p>We use <a href="https://github.com/openai/tiktoken?ref=blog.salesforceairesearch.com">OpenAI’s tiktoken</a> to tokenize our data. We add additional tokens for consecutive whitespaces and tabs, as well as the special tokens described in the <a href="https://arxiv.org/abs/2305.06161?ref=blog.salesforceairesearch.com">Starcoder paper</a>.</p><hr/><h2 id="training-details">Training Details<br/></h2><p>The XGen-7b models are trained with our in-house library JaxFormer, which facilitates efficient training of LLMs under both data and model parallelism optimized for TPU-v4 hardware. The training recipe and model architecture follow LLaMA, while we conduct two additional explorations. First, we investigate the occurrence of so-called “loss spikes” [<a href="https://arxiv.org/abs/2204.02311?ref=blog.salesforceairesearch.com">PaLM</a>, <a href="https://arxiv.org/abs/2304.09871?ref=blog.salesforceairesearch.com">loss spikes</a>] during training, that is, the loss suddenly explodes temporarily while the root cause for these spikes is unknown. Second, the XGen models support sequence lengths up to 8,192 tokens (rather than the common 2,048) for which we introduce stage-wise training.</p><p><strong>Loss Spikes</strong></p><p>As models are scaled to larger sizes, the training itself is increasingly sensitive to instabilities, which cause poor model performance, if not addressed carefully. In our exploration, we have gathered evidence for several factors, which individually contribute to unstable training. These preliminary findings include “sequential over parallel circuits”, “swish-GLU over GeLU”, “RMS-Norm over Layer-norm”. Specifically, widely used parallel circuits, which parallelize the computation of self-attention and feed-forward as adopted in [<a href="https://github.com/kingoflolz/mesh-transformer-jax?ref=blog.salesforceairesearch.com#gpt-j-6b">GPT-J</a>, <a href="https://arxiv.org/abs/2204.02311?ref=blog.salesforceairesearch.com">PaLM</a>, <a href="https://github.com/salesforce/CodeGen?ref=blog.salesforceairesearch.com">CodeGen</a>] may affect the stability of training.</p><figure><img src="https://lh5.googleusercontent.com/7BVsbldNGDMKZlMhp-a9HnhLjcxu1-Wi61MoXJH37rAfCWNVjutCtDlgORjRXbTnA55ecmII-lm64EdQW1-nySwLVZtFsUpE6jpzLBL6vp9jkw8T4ZALDOcB7cFf_qWbZD6y62xGhXGNaFxL8Yqq2SA" alt="" loading="lazy" width="444" height="346"/></figure><p>The figure above displays the loss in terms of cross-entropy over time following the well-known scaling laws. Remarkably, the training does not suffer from any instabilities or loss spikes. The two loss spikes depicted in the figure are expected when extending the sequence length, say from 2k to 4k tokens, since the model needs to adapt to such longer sequences.</p><p>Training with longer sequences is computationally unproportionally costly as the complexity of self-attention is quadratic, that is, the training process is slow. To mitigate slow training, we introduce training in stages with increasing sequence length. First, 800B tokens with sequence length of 2k tokens are observed, then 400B tokens with 4k, finally, 300B tokens with 8k length.</p><figure><img src="https://lh6.googleusercontent.com/HVZ52gGXXhP3S1A6zmCNPCQjR_0zPJdEa0Piw34K92wPGhcNnmUkvvqmMivyqWtObaB8intWIo79fp9jYdeETOUtgfK96WpHL-X4_VVGSfuahAw-ef1FgqGCVe6fzHKWpJv-sQsPd5Ed4mBorBwg7fg" alt="" loading="lazy" width="326" height="254"/></figure><hr/><h2 id="results-on-standard-benchmarks">Results on Standard Benchmarks<br/></h2><h3 id="i-mmlu">(i) MMLU<br/></h3><p>We first consider the Measuring Massive Multitask Language Understanding benchmark (see examples <a href="https://huggingface.co/datasets/cais/mmlu/viewer/college_medicine/dev?row=0&amp;ref=blog.salesforceairesearch.com">here</a>), which is more recent than others due to which it is arguably less susceptible to data contamination as reported in recent studies (see page 32 of <a href="https://arxiv.org/pdf/2303.08774.pdf.?ref=blog.salesforceairesearch.com">GPT-4 paper</a> and a related discussion <a href="https://hitz-zentroa.github.io/lm-contamination/blog/?ref=blog.salesforceairesearch.com">here</a>), and has been used consistently as a held-out evaluation benchmark. Recently, however, inconsistencies in reporting MMLU scores have been reported, which resulted in wrong rankings in Hugginface’s <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?ref=blog.salesforceairesearch.com">Open LLM leaderboard</a>; In fact, Huggingface later had to write a  <a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard?ref=blog.salesforceairesearch.com">blog</a> to clarify this. In our work, we follow the original MMLU standard, which is consistent with the published results (i.e., in LLaMA). <br/></p><p><strong>MMLU 5-shot In-context Learning Results: </strong>We first show results on the original (and recommended) 5-shot evaluation setting, where the LLM is provided with 5 demonstrations. XGen achieves the best results in most categories, also in weighted average.</p><!--kg-card-begin: html--><table><colgroup><col width="134"/><col width="87"/><col width="69"/><col width="119"/><col width="64"/><col width="91"/></colgroup><tbody><tr><td><p dir="ltr"><span>Models</span></p></td><td><p dir="ltr"><span>Humanities</span></p></td><td><p dir="ltr"><span>STEM</span></p></td><td><p dir="ltr"><span>Social Sciences</span></p></td><td><p dir="ltr"><span>Other</span></p></td><td><p dir="ltr"><span>Weighted average</span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7b</span></p></td><td><p dir="ltr"><span>33.8</span></p></td><td><p dir="ltr"><span>30.7</span></p></td><td><p dir="ltr"><span>40.0</span></p></td><td><p dir="ltr"><span>41.5</span></p></td><td><p dir="ltr"><span>36.3</span></p></td></tr><tr><td><p dir="ltr"><span>LLaMA-7b</span></p></td><td><p dir="ltr"><span>33.9</span></p></td><td><p dir="ltr"><span>30.6</span></p></td><td><p dir="ltr"><span>38.2</span></p></td><td><p dir="ltr"><span>38.2</span></p></td><td><p dir="ltr"><span>35.1</span></p></td></tr><tr><td><p dir="ltr"><span>OpenLLaMA-7b</span></p></td><td><p dir="ltr"><span>28.1</span></p></td><td><p dir="ltr"><span>28.5</span></p></td><td><p dir="ltr"><span>31.2</span></p></td><td><p dir="ltr"><span>32.8</span></p></td><td><p dir="ltr"><span>29.9</span></p></td></tr><tr><td><p dir="ltr"><span>Falcon-7b</span></p></td><td><p dir="ltr"><span>26.5</span></p></td><td><p dir="ltr"><span>25.4</span></p></td><td><p dir="ltr"><span>29.2</span></p></td><td><p dir="ltr"><span>26.8</span></p></td><td><p dir="ltr"><span>26.9</span></p></td></tr><tr><td><p dir="ltr"><span>MPT-7b</span></p></td><td><p dir="ltr"><span>25.9</span></p></td><td><p dir="ltr"><span>26.2</span></p></td><td><p dir="ltr"><span>26.9</span></p></td><td><p dir="ltr"><span>28.1</span></p></td><td><p dir="ltr"><span>26.7</span></p></td></tr><tr><td><p dir="ltr"><span>Redpajama-7b</span></p></td><td><p dir="ltr"><span>26.1</span></p></td><td><p dir="ltr"><span>25.2</span></p></td><td><p dir="ltr"><span>27.4</span></p></td><td><p dir="ltr"><span>26.7</span></p></td><td><p dir="ltr"><span>26.3</span></p></td></tr><tr><td><p dir="ltr"><span>Cerebras-GPT-13b</span></p></td><td><p dir="ltr"><span>26.1</span></p></td><td><p dir="ltr"><span>26.5</span></p></td><td><p dir="ltr"><span>25.8</span></p></td><td><p dir="ltr"><span>26.6</span></p></td><td><p dir="ltr"><span>26.2</span></p></td></tr><tr><td><p dir="ltr"><span>Dolly-v2-12b</span></p></td><td><p dir="ltr"><span>26.9</span></p></td><td><p dir="ltr"><span>25.7</span></p></td><td><p dir="ltr"><span>25.3</span></p></td><td><p dir="ltr"><span>26.5</span></p></td><td><p dir="ltr"><span>26.2</span></p></td></tr><tr><td><p dir="ltr"><span>OPT-13b</span></p></td><td><p dir="ltr"><span>26.2</span></p></td><td><p dir="ltr"><span>24.3</span></p></td><td><p dir="ltr"><span>23.4</span></p></td><td><p dir="ltr"><span>26</span></p></td><td><p dir="ltr"><span>25.1</span></p></td></tr><tr><td><p dir="ltr"><span>GPT-J-6b</span></p></td><td><p dir="ltr"><span>25.9</span></p></td><td><p dir="ltr"><span>24.0</span></p></td><td><p dir="ltr"><span>24.0</span></p></td><td><p dir="ltr"><span>25.8</span></p></td><td><p dir="ltr"><span>25.1</span></p></td></tr></tbody></table><!--kg-card-end: html--><!--kg-card-begin: html--><table><colgroup><col width="148"/><col width="100"/><col width="71"/><col width="126"/><col width="80"/><col width="97"/></colgroup><tbody><tr><td><p dir="ltr"><span>Models</span></p></td><td><p dir="ltr"><span>Humanities</span></p></td><td><p dir="ltr"><span>STEM</span></p></td><td><p dir="ltr"><span>Social Sciences</span></p></td><td><p dir="ltr"><span>Other</span></p></td><td><p dir="ltr"><span>Weighted average</span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7b</span></p></td><td><p dir="ltr"><span>31.4</span></p></td><td><p dir="ltr"><span>27.8</span></p></td><td><p dir="ltr"><span>32.1</span></p></td><td><p dir="ltr"><span>37.2</span></p></td><td><p dir="ltr"><span>32.1</span></p></td></tr><tr><td><p dir="ltr"><span>LLaMA-7b</span></p></td><td><p dir="ltr"><span>32.3</span></p></td><td><p dir="ltr"><span>27.1</span></p></td><td><p dir="ltr"><span>31.3</span></p></td><td><p dir="ltr"><span>36.8</span></p></td><td><p dir="ltr"><span>32.0</span></p></td></tr><tr><td><p dir="ltr"><span>OpenLLaMA-7b</span></p></td><td><p dir="ltr"><span>28.0</span></p></td><td><p dir="ltr"><span>27.6</span></p></td><td><p dir="ltr"><span>28.9</span></p></td><td><p dir="ltr"><span>30.1</span></p></td><td><p dir="ltr"><span>28.6</span></p></td></tr><tr><td><p dir="ltr"><span>MPT-7b</span></p></td><td><p dir="ltr"><span>27.4</span></p></td><td><p dir="ltr"><span>25.2</span></p></td><td><p dir="ltr"><span>26.0</span></p></td><td><p dir="ltr"><span>30.7</span></p></td><td><p dir="ltr"><span>27.4</span></p></td></tr><tr><td><p dir="ltr"><span>Redpajama-7b</span></p></td><td><p dir="ltr"><span>27.5</span></p></td><td><p dir="ltr"><span>25.5</span></p></td><td><p dir="ltr"><span>24.2</span></p></td><td><p dir="ltr"><span>25.0</span></p></td><td><p dir="ltr"><span>25.8</span></p></td></tr><tr><td><p dir="ltr"><span>GPT-J-6b</span></p></td><td><p dir="ltr"><span>25.3</span></p></td><td><p dir="ltr"><span>24.5</span></p></td><td><p dir="ltr"><span>25.5</span></p></td><td><p dir="ltr"><span>27.6</span></p></td><td><p dir="ltr"><span>25.7</span></p></td></tr><tr><td><p dir="ltr"><span>Dolly-v2-12b</span></p></td><td><p dir="ltr"><span>26.2</span></p></td><td><p dir="ltr"><span>26.0</span></p></td><td><p dir="ltr"><span>24.0</span></p></td><td><p dir="ltr"><span>24.9</span></p></td><td><p dir="ltr"><span>25.4</span></p></td></tr><tr><td><p dir="ltr"><span>Cerebras-GPT-13b</span></p></td><td><p dir="ltr"><span>24.3</span></p></td><td><p dir="ltr"><span>25.0</span></p></td><td><p dir="ltr"><span>23.0</span></p></td><td><p dir="ltr"><span>26.0</span></p></td><td><p dir="ltr"><span>24.6</span></p></td></tr><tr><td><p dir="ltr"><span>OPT-13b</span></p></td><td><p dir="ltr"><span>26.3</span></p></td><td><p dir="ltr"><span>23.3</span></p></td><td><p dir="ltr"><span>23.6</span></p></td><td><p dir="ltr"><span>23.6</span></p></td><td><p dir="ltr"><span>24.4</span></p></td></tr><tr><td><p dir="ltr"><span>Falcon-7b</span></p></td><td><p dir="ltr"><span>24.8</span></p></td><td><p dir="ltr"><span>21.7</span></p></td><td><p dir="ltr"><span>24.0</span></p></td><td><p dir="ltr"><span>24.4</span></p></td><td><p dir="ltr"><span>23.9</span></p></td></tr></tbody></table><!--kg-card-end: html--><h3 id="ii-general-zero-shot-results">(ii) General Zero-shot Results<br/></h3><p>Next, we report general zero-shot results on general NLP tasks that involve common sense reasoning and QA.<br/></p><!--kg-card-begin: html--><table><colgroup><col width="139"/><col width="56"/><col width="51"/><col width="59"/><col width="82"/><col width="61"/><col width="55"/><col width="69"/><col width="85"/></colgroup><tbody><tr><td><p dir="ltr"><span>Models</span></p></td><td><p dir="ltr"><span>MMLU</span></p><p dir="ltr"><span>-wavg</span></p></td><td><p dir="ltr"><span>ARC_ch</span></p></td><td><p dir="ltr"><span>Hella Swag</span></p></td><td><p dir="ltr"><span>Winogrande</span></p></td><td><p dir="ltr"><span>TruthfulQA</span></p></td><td><p dir="ltr"><span>BoolQ</span></p></td><td><p dir="ltr"><span>PiQA</span></p></td><td><p dir="ltr"><span>OpenBookQA</span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7b</span></p></td><td><p dir="ltr"><span>32.1</span></p></td><td><p dir="ltr"><span>41.2</span></p></td><td><p dir="ltr"><span>74.2</span></p></td><td><p dir="ltr"><span>64.9</span></p></td><td><p dir="ltr"><span>39.1</span></p></td><td><p dir="ltr"><span>74.3</span></p></td><td><p dir="ltr"><span>75.5</span></p></td><td><p dir="ltr"><span>40.2</span></p></td></tr><tr><td><p dir="ltr"><span>LLaMA-7b</span></p></td><td><p dir="ltr"><span>32.0</span></p></td><td><p dir="ltr"><span>44.8</span></p></td><td><p dir="ltr"><span>76.2</span></p></td><td><p dir="ltr"><span>69.6</span></p></td><td><p dir="ltr"><span>34</span></p></td><td><p dir="ltr"><span>74.9</span></p></td><td><p dir="ltr"><span>78.7</span></p></td><td><p dir="ltr"><span>44.2</span></p></td></tr><tr><td><p dir="ltr"><span>Falcon-7b</span></p></td><td><p dir="ltr"><span>23.9</span></p></td><td><p dir="ltr"><span>43.4</span></p></td><td><p dir="ltr"><span>76.4</span></p></td><td><p dir="ltr"><span>67.2</span></p></td><td><p dir="ltr"><span>34.3</span></p></td><td><p dir="ltr"><span>73.8</span></p></td><td><p dir="ltr"><span>79.4</span></p></td><td><p dir="ltr"><span>44.0</span></p></td></tr><tr><td><p dir="ltr"><span>MPT-7b</span></p></td><td><p dir="ltr"><span>27.4</span></p></td><td><p dir="ltr"><span>41.7</span></p></td><td><p dir="ltr"><span>76.1</span></p></td><td><p dir="ltr"><span>68.6</span></p></td><td><p dir="ltr"><span>33.4</span></p></td><td><p dir="ltr"><span>74.1</span></p></td><td><p dir="ltr"><span>79.1</span></p></td><td><p dir="ltr"><span>41.8</span></p></td></tr><tr><td><p dir="ltr"><span>OpenLLaMA-7b</span></p></td><td><p dir="ltr"><span>28.6</span></p></td><td><p dir="ltr"><span>38.7</span></p></td><td><p dir="ltr"><span>71.8</span></p></td><td><p dir="ltr"><span>67.0</span></p></td><td><p dir="ltr"><span>35.2</span></p></td><td><p dir="ltr"><span>70.6</span></p></td><td><p dir="ltr"><span>76.0</span></p></td><td><p dir="ltr"><span>39.0</span></p></td></tr><tr><td><p dir="ltr"><span>Redpajama-7b</span></p></td><td><p dir="ltr"><span>25.8</span></p></td><td><p dir="ltr"><span>39.1</span></p></td><td><p dir="ltr"><span>70.3</span></p></td><td><p dir="ltr"><span>63.8</span></p></td><td><p dir="ltr"><span>33.3</span></p></td><td><p dir="ltr"><span>69.3</span></p></td><td><p dir="ltr"><span>76.9</span></p></td><td><p dir="ltr"><span>40.0</span></p></td></tr><tr><td><p dir="ltr"><span>GPT-neox-20b</span></p></td><td><p dir="ltr"><span>24.5</span></p></td><td><p dir="ltr"><span>41.1</span></p></td><td><p dir="ltr"><span>70.5</span></p></td><td><p dir="ltr"><span>66.1</span></p></td><td><p dir="ltr"><span>31.4</span></p></td><td><p dir="ltr"><span>64.9</span></p></td><td><p dir="ltr"><span>76.7</span></p></td><td><p dir="ltr"><span>38.8</span></p></td></tr><tr><td><p dir="ltr"><span>OPT-13b</span></p></td><td><p dir="ltr"><span>24.4</span></p></td><td><p dir="ltr"><span>35.8</span></p></td><td><p dir="ltr"><span>69.9</span></p></td><td><p dir="ltr"><span>64.7</span></p></td><td><p dir="ltr"><span>33.9</span></p></td><td><p dir="ltr"><span>65.0</span></p></td><td><p dir="ltr"><span>75.7</span></p></td><td><p dir="ltr"><span>39.8</span></p></td></tr><tr><td><p dir="ltr"><span>GPT-J-6b</span></p></td><td><p dir="ltr"><span>25.7</span></p></td><td><p dir="ltr"><span>36.3</span></p></td><td><p dir="ltr"><span>66.2</span></p></td><td><p dir="ltr"><span>64.5</span></p></td><td><p dir="ltr"><span>36.0</span></p></td><td><p dir="ltr"><span>65.4</span></p></td><td><p dir="ltr"><span>75.4</span></p></td><td><p dir="ltr"><span>38.2</span></p></td></tr><tr><td><p dir="ltr"><span>Dolly-v2-12b</span></p></td><td><p dir="ltr"><span>25.4</span></p></td><td><p dir="ltr"><span>39.6</span></p></td><td><p dir="ltr"><span>70.8</span></p></td><td><p dir="ltr"><span>61.8</span></p></td><td><p dir="ltr"><span>34.4</span></p></td><td><p dir="ltr"><span>56.3</span></p></td><td><p dir="ltr"><span>75.4</span></p></td><td><p dir="ltr"><span>39.2</span></p></td></tr><tr><td><p dir="ltr"><span>Cerebras-GPT-13b</span></p></td><td><p dir="ltr"><span>24.6</span></p></td><td><p dir="ltr"><span>32.4</span></p></td><td><p dir="ltr"><span>59.4</span></p></td><td><p dir="ltr"><span>60.8</span></p></td><td><p dir="ltr"><span>39.2</span></p></td><td><p dir="ltr"><span>61.1</span></p></td><td><p dir="ltr"><span>73.5</span></p></td><td><p dir="ltr"><span>35.8</span></p></td></tr><tr><td><p dir="ltr"><span>StableLM-alpha-7b</span></p></td><td><p dir="ltr"><span>24.4</span></p></td><td><p dir="ltr"><span>27.0</span></p></td><td><p dir="ltr"><span>40.7</span></p></td><td><p dir="ltr"><span>51.5</span></p></td><td><p dir="ltr"><span>41.7</span></p></td><td><p dir="ltr"><span>59.0</span></p></td><td><p dir="ltr"><span>65.8</span></p></td><td><p dir="ltr"><span>32.4</span></p></td></tr></tbody></table><!--kg-card-end: html--><h3 id="iii-results-on-code-generation">(iii) Results on Code Generation<br/></h3><p>To evaluate XGen’s code generation capability from natural language instructions (docstrings), we evaluate it on the well-known <a href="https://github.com/openai/human-eval?ref=blog.salesforceairesearch.com">HumanEval</a> benchmark. We set the sampling temperature to 0.2, p to 0.95 (for top-p sampling), and num_samples_per_task (n) to 200. We report the standard zero-shot results with pass@1 metric.<br/></p><!--kg-card-begin: html--><table><colgroup><col width="197"/><col width="443"/></colgroup><tbody><tr><td><p dir="ltr"><span>Models</span></p></td><td><p dir="ltr"><span>pass@1</span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7b</span></p></td><td><p dir="ltr"><span>14.20</span></p></td></tr><tr><td><p dir="ltr"><span>LLaMA-7b</span></p></td><td><p dir="ltr"><span>10.38</span></p></td></tr><tr><td><p dir="ltr"><span>OpenLLaMA-7b</span></p></td><td><p dir="ltr"><span>0 (Consecutive whitespaces are treated as one, breaking Python syntax)</span></p></td></tr><tr><td><p dir="ltr"><span>Falcon-7b</span></p></td><td><p dir="ltr"><span>0 (didn’t generate meaningful code)</span></p></td></tr><tr><td><p dir="ltr"><span>MPT-7b</span></p></td><td><p dir="ltr"><span>15.90</span></p></td></tr><tr><td><p dir="ltr"><span>Redpajama-7b</span></p></td><td><p dir="ltr"><span>5.24</span></p></td></tr></tbody></table><!--kg-card-end: html--><hr/><h2 id="results-on-long-sequence-generation-tasks">Results on Long Sequence Generation Tasks</h2><p>To further evaluate our XGen-7b 8k model in comparison to baselines which are limited to 2k inputs, we turn to long-form dialogue generation, text summarization and QA. All these tasks benefit from using processing and understanding a long context to generate a correct response. Note that for these tasks most of the <strong>base</strong> <strong>pre-trained</strong> models failed to generate a plausible response because of the task difficulty. We thus use instruction-tuned models.</p><h3 id="dialogue"></h3><p>To assess the long dialogue understanding and summarization capabilities, we report results on three dialogue summarization tasks: <a href="https://huggingface.co/datasets/knkarthick/AMI?ref=blog.salesforceairesearch.com">AMI meeting summarization</a>, ForeverDreaming (FD), and TVMegaSite (TMS) <a href="https://arxiv.org/pdf/2104.07091.pdf?ref=blog.salesforceairesearch.com">screenplay summarization</a>. The average source lengths of these datasets are approximately 5570, 6466, and 7653, respectively. We specifically evaluate samples that are less than 8K in length using various instruction-tuned models. Notably, when input truncation was not applied, both MPT-7b-inst and Alpaca-inst failed to perform well in this setting. Our model (XGen-7B-inst) achieved the highest ROUGE scores across all metrics.</p><!--kg-card-begin: html--><table><colgroup><col width="118"/><col width="60"/><col width="61"/><col width="63"/><col width="59"/><col width="58"/><col width="60"/><col width="58"/><col width="58"/><col width="58"/></colgroup><tbody><tr><td rowspan="2"><p dir="ltr"><span>Model</span></p></td><td colspan="3"><p dir="ltr"><span>AMI</span></p></td><td colspan="3"><p dir="ltr"><span>FD</span></p></td><td colspan="3"><p dir="ltr"><span>TMS</span></p></td></tr><tr><td><p dir="ltr"><span>R-1</span></p></td><td><p dir="ltr"><span>R-2</span></p></td><td><p dir="ltr"><span>R-L</span></p></td><td><p dir="ltr"><span>R-1</span></p></td><td><p dir="ltr"><span>R-2</span></p></td><td><p dir="ltr"><span>R-L</span></p></td><td><p dir="ltr"><span>R-1</span></p></td><td><p dir="ltr"><span>R-2</span></p></td><td><p dir="ltr"><span>R-L</span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7b-inst</span></p></td><td><p dir="ltr"><span>31.34</span></p></td><td><p dir="ltr"><span>8.25</span></p></td><td><p dir="ltr"><span>17.00</span></p></td><td><p dir="ltr"><span>29.34</span></p></td><td><p dir="ltr"><span>5.39</span></p></td><td><p dir="ltr"><span>16.43</span></p></td><td><p dir="ltr"><span>26.39</span></p></td><td><p dir="ltr"><span>3.94</span></p></td><td><p dir="ltr"><span>13.71</span></p></td></tr><tr><td><p dir="ltr"><span>Falcon-7b-inst</span></p></td><td><p dir="ltr"><span>14.89</span></p></td><td><p dir="ltr"><span>1.97</span></p></td><td><p dir="ltr"><span>9.28</span></p></td><td><p dir="ltr"><span>18.90</span></p></td><td><p dir="ltr"><span>1.80</span></p></td><td><p dir="ltr"><span>9.37</span></p></td><td><p dir="ltr"><span>18.90</span></p></td><td><p dir="ltr"><span>1.80</span></p></td><td><p dir="ltr"><span>9.37</span></p></td></tr><tr><td><p dir="ltr"><span>MPT-7b-inst</span></p></td><td><p dir="ltr"><span>11.95</span></p></td><td><p dir="ltr"><span>1.88</span></p></td><td><p dir="ltr"><span>8.10</span></p></td><td><p dir="ltr"><span>14.27</span></p></td><td><p dir="ltr"><span>1.40</span></p></td><td><p dir="ltr"><span>8.89</span></p></td><td><p dir="ltr"><span>19.80</span></p></td><td><p dir="ltr"><span>2.39</span></p></td><td><p dir="ltr"><span>10.23</span></p></td></tr><tr><td><p dir="ltr"><span>Alpaca-7b-inst</span></p></td><td><p dir="ltr"><span>9.69</span></p></td><td><p dir="ltr"><span>1.77</span></p></td><td><p dir="ltr"><span>6.43</span></p></td><td><p dir="ltr"><span>16.26</span></p></td><td><p dir="ltr"><span>1.56</span></p></td><td><p dir="ltr"><span>10.66</span></p></td><td><p dir="ltr"><span>12.26</span></p></td><td><p dir="ltr"><span>1.15</span></p></td><td><p dir="ltr"><span>7.30</span></p></td></tr></tbody></table><!--kg-card-end: html--><h3 id="long-form-qa">Long-form QA<br/></h3><p>Next, we evaluate our XGen-7b-inst on a long-form QA task that we have designed in-house. We ask ChatGPT to generate questions from (a) long Wikipedia documents spanning four domains: Physics, Engineering, History, and Entertainment, and (b) summaries of these documents.  Then we query the LLMs to generate answers for these questions. The answers are typically up to 256 tokens long. We use GPT-4 to evaluate the answer quality in terms of coherence (structure and organization) and relevance (relevance of generated answer to the question and the context document) on a scale of 0-3. From the results below, we see our model has higher scores in different aspects compared to the baselines considered.</p><!--kg-card-begin: html--><table><colgroup><col width="129"/><col width="93"/><col width="93"/><col width="103"/></colgroup><tbody><tr><td rowspan="2"><p dir="ltr"><span>Model</span></p></td><td colspan="3"><p dir="ltr"><span>Metrics</span></p></td></tr><tr><td><p dir="ltr"><span>Coherence</span></p></td><td><p dir="ltr"><span>Relevance</span></p></td><td><p dir="ltr"><span>Avg. Ratings</span></p></td></tr><tr><td><p dir="ltr"><span>XGen-7b-inst</span></p></td><td><p dir="ltr"><span>2.55</span></p></td><td><p dir="ltr"><span>2.52</span></p></td><td><p dir="ltr"><span>2.54</span></p></td></tr><tr><td><p dir="ltr"><span>MPT-7b-inst</span></p></td><td><p dir="ltr"><span>2.5</span></p></td><td><p dir="ltr"><span>2.45</span></p></td><td><p dir="ltr"><span>2.48</span></p></td></tr><tr><td><p dir="ltr"><span>Alpaca-7b-inst</span></p></td><td><p dir="ltr"><span>1.65</span></p></td><td><p dir="ltr"><span>1.91</span></p></td><td><p dir="ltr"><span>1.78</span></p></td></tr><tr><td><p dir="ltr"><span>Falcon-7b-inst</span></p></td><td><p dir="ltr"><span>2.26</span></p></td><td><p dir="ltr"><span>2.13</span></p></td><td><p dir="ltr"><span>2.19</span></p></td></tr></tbody></table><!--kg-card-end: html--><h3 id="summarization"></h3><p>Here, we evaluate our model on two text summarization datasets included in the <a href="https://www.scrolls-benchmark.com/?ref=blog.salesforceairesearch.com">SCROLLS Benchmark</a>, namely QMSum and GovReport. They cover two different domains -- meeting conversations and government reports. Additionally, QMSum data includes specific natural language queries which instruct the model about the key aspects of the source document that should be included in the summary. We see that our model XGen-7b outperforms other baselines on these tasks. <br/></p><!--kg-card-begin: html--><table><colgroup><col width="178"/><col width="53"/><col width="53"/><col width="54"/><col width="52"/><col width="57"/><col width="61"/></colgroup><thead><tr><th rowspan="2" scope="col"><p dir="ltr"><span>Model</span></p></th><th colspan="3" scope="col"><p dir="ltr"><span>QMSum</span></p></th><th colspan="3" scope="col"><p dir="ltr"><span>GovReports</span></p></th></tr><tr><th scope="col"><p dir="ltr"><span>R-1</span></p></th><th scope="col"><p dir="ltr"><span>R-2</span></p></th><th scope="col"><p dir="ltr"><span>R-L</span></p></th><th scope="col"><p dir="ltr"><span>R-1</span></p></th><th scope="col"><p dir="ltr"><span>R-2</span></p></th><th scope="col"><p dir="ltr"><span>R-L</span></p></th></tr></thead><tbody><tr><td><p dir="ltr"><span>XGen-7b-inst</span></p></td><td><p dir="ltr"><span>27.96</span></p></td><td><p dir="ltr"><span>5.66</span></p></td><td><p dir="ltr"><span>24.26</span></p></td><td><p dir="ltr"><span>21.28</span></p></td><td><p dir="ltr"><span>8.19</span></p></td><td><p dir="ltr"><span>20.08</span></p></td></tr><tr><td><p dir="ltr"><span>Falcon-7b-inst</span></p></td><td><p dir="ltr"><span>15.68</span></p></td><td><p dir="ltr"><span>2.81</span></p></td><td><p dir="ltr"><span>14.01</span></p></td><td><p dir="ltr"><span>17.8</span></p></td><td><p dir="ltr"><span>6.13</span></p></td><td><p dir="ltr"><span>16.66</span></p></td></tr><tr><td><p dir="ltr"><span>MPT-7b-inst</span></p></td><td><p dir="ltr"><span>21.75</span></p></td><td><p dir="ltr"><span>4.38</span></p></td><td><p dir="ltr"><span>19.29</span></p></td><td><p dir="ltr"><span>18.11</span></p></td><td><p dir="ltr"><span>6.96</span></p></td><td><p dir="ltr"><span>17.11</span></p></td></tr><tr><td><p dir="ltr"><span>Redpajama-7b-inst</span></p></td><td><p dir="ltr"><span>19.81</span></p></td><td><p dir="ltr"><span>2.66</span></p></td><td><p dir="ltr"><span>17.58</span></p></td><td><p dir="ltr"><span>19.63</span></p></td><td><p dir="ltr"><span>6.93</span></p></td><td><p dir="ltr"><span>18.48</span></p></td></tr></tbody></table><!--kg-card-end: html--><p>As we see encouraging results of our XGen-7b models on these long sequence tasks, we would like to note that since these models are not trained on the same instructional data, they are not strictly comparable.<br/></p><hr/><h2 id="note-on-potential-risks">Note on Potential Risks<br/></h2><p>Finally, despite our effort in addressing the risks of bias, toxicity and hallucinations both in pre-training and fine-tuning stages, like other LLMs, XGen-7b models are not free from such limitations. We hope our open-sourced codebase will help other researchers better understand these challenges and improve on these key limitations for making AI beneficial for everyone.</p>
        </div>
      </div>
    </div>
  </section></div>
  </body>
</html>
