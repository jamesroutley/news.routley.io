<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://dlants.me/agi-not-imminent.html">Original</a>
    <h1>Why I don&#39;t think AGI is imminent</h1>
    
    <div id="readability-page-1" class="page"><div id="root">
    <div><div><div><p>February 14, 2026</p><p>The CEOs of OpenAI and Anthropic have both claimed that human-level AI is just around the corner — and at times, that it&#39;s already here. These claims have generated enormous public attention. There has been some technical scrutiny of these claims, but critiques rarely reach the public discourse. This piece is a sketch of my own thinking about the boundary of transformer-based large language models and human-level cognition. I have an MS degree in Machine Learning from over a decade ago, and I don&#39;t work in the field of AI currently, but I am well-read on the underlying research. If you know more than I do about these topics, please <a href="https://dlants.me/about.html">reach out</a> and let me know, I would love to develop my thinking on this further.</p><p>Research in evolutionary neuroscience has identified a set of cognitive primitives that are <a href="https://pubmed.ncbi.nlm.nih.gov/39932126/">hardwired into vertebrate brains</a>: some of these are a sense of number, object permanence, causality, spatial navigation, and the ability to distinguish animate from inanimate motion. These capacities are <a href="https://www.nature.com/articles/s41598-024-64396-8">shared across vertebrates</a>, from fish to ungulates to primates, pointing to a common evolutionary origin hundreds of millions of years old.</p><p>Language evolved on top of these primitives — a tool for communication where both speaker and listener share the same cognitive foundation. Because both sides have always had these primitives, language takes them for granted and does not state them explicitly.</p><p>Consider the sentence &#34;Mary held a ball.&#34; To understand it, you need to know that Mary is an animate entity capable of intentional action, that the ball is a separate, bounded, inanimate object with continuous existence through time, that Mary is roughly human-sized and upright while the ball is small enough to fit in her hand, that her hand exerts an upward force counteracting gravity, that the ball cannot pass through her palm, that releasing her grip would cause the ball to fall, and that there is one Mary and one ball, each persisting as the same entity from moment to moment, each occupying a distinct region of three-dimensional space. All of that is what a human understands from four words, and none of it is in the text. Modern LLMs are now trying to reverse-engineer this cognitive foundation from language, which is an extremely difficult task.</p><p>I find this to be useful framing for understanding many of the observed limitations of current LLM architectures. For example, transformer-based language models <a href="https://arxiv.org/abs/2410.05229">can&#39;t reliably do multi-digit arithmetic</a> because they have no number sense, only statistical patterns over digit tokens. They <a href="https://arxiv.org/abs/2309.12288">can&#39;t generalize simple logical relationships</a> — a model trained on &#34;A is B&#34; can&#39;t infer &#34;B is A&#34; — because they lack the compositional, symbolic machinery.</p><p>One might object: modern AIs are now being trained on video, not just text. And it&#39;s true that video prediction can teach something like object permanence. If you want to predict the next frame, you need to model what happens when an object passes behind an occluder, which is something like a representation of persistence. But I think the reality is more nuanced. Consider a shell game: a marble is placed under one of three cups, and the cups are shuffled. A video prediction model might learn the statistical regularity that &#34;when a cup is lifted, a marble is usually there.&#34; But actually tracking the marble through the shuffling requires something deeper — a commitment to the marble as a <em>persistent entity</em> with a continuous trajectory through space. That&#39;s not merely a visual pattern.</p><p>The shortcomings of visual models align with this framing. Early GPT-based vision models failed at even basic spatial reasoning. Much of the recent progress has come from generating large swaths of synthetic training data. But even in this, we are trying to learn the physical and logical constraints of the real world from visual data. The results, predictably, are fragile. A model trained on synthetic shell game data could probably learn to track the marble. But I suspect that learning would not generalize to other situations and relations — it would be shell game tracking, not object permanence.</p><p>Developmental psychologist <a href="https://harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf">Elizabeth Spelke&#39;s research on &#34;core knowledge&#34;</a> has shown that infants — including blind infants — represent objects as bounded, cohesive, spatiotemporally continuous entities. This isn&#39;t a learned visual skill. It appears to be something deeper: a fundamental category of representation that the brain uses to organize all sensory input. Objects have identity. They persist. They can&#39;t teleport or merge. This &#34;object-ness&#34; likely predates vision itself — it&#39;s rooted in hundreds of millions of years of organisms needing to <em>interact with things in the physical world</em>, and I think this aspect of our evolutionary &#34;training environment&#34; is key to our robust cognitive primitives. Organisms don&#39;t merely observe reality to predict what happens next. They perceive in order to act, and they act in order to perceive. Object permanence allows you to track prey behind an obstacle. Number sense lets you estimate whether you&#39;re outnumbered. Logical composition enables tool construction and use. Spatial navigation helps you find your way home. Every cognitive primitive is directly linked to action in a rich, multisensory, physical world.</p><p>As Rodney Brooks <a href="https://rodneybrooks.com/why-todays-humanoids-wont-learn-dexterity/">has pointed out</a>, even human dexterity is a tight coupling of fine motor control and rich sensory feedback. Modern robots do not have nearly as rich of sensory information available to them. While LLMs have benefited from vast quantities of text, video, and audio available on the internet, we simply don&#39;t have large-scale datasets of rich, multisensory perception coupled to intentional action. Collecting or generating such data is extremely challenging.</p><h2 id="what-about-world-models"><a href="#what-about-world-models">What about world models?</a></h2><p>What if we built simulated environments where AIs could gather embodied experience? Would we be able to create learning scenarios where agents could learn some of these cognitive primitives, and could that generalize to improve LLMs? There are a few papers that I found that poke in this direction.</p><p>Google DeepMind&#39;s <a href="https://arxiv.org/abs/2512.04797">SIMA 2</a> is one. Despite the &#34;embodied agent&#34; branding, SIMA 2 is primarily trained through behavioral cloning: it watches human gameplay videos and learns to predict what actions they took. The reasoning and planning come from its base model (Gemini Flash-Lite), which was pretrained on internet text and images — not from embodied experience. There is an RL self-improvement stage where the agent does interact with environments, but this is secondary; the core intelligence is borrowed from language pretraining. SIMA 2 reaches near-human performance on many game tasks, but what it&#39;s really demonstrating is that a powerful language model can be taught to output keyboard actions.</p><p>Can insights from world-model training actually transfer to and improve language understanding? DeepMind&#39;s researchers explicitly frame this as a trade off between two competing objectives: &#34;embodied competence&#34; (acting effectively in 3D worlds) and &#34;general reasoning&#34; (the language and math abilities from pretraining). They found that baseline Gemini models, despite being powerful language models, achieved only 3-7% success rates on embodied tasks — demonstrating that embodied competence is not something that emerges from language pretraining. After fine-tuning on gameplay data, SIMA 2 achieved near-human performance on embodied tasks while showing &#34;only minor regression&#34; on language and math benchmarks. But notice the framing: the <em>best case</em> is that embodied training doesn&#39;t <em>hurt</em> language ability too much. There&#39;s no evidence that it <em>improves</em> it. The two capabilities sit in separate regions of the model&#39;s parameter space, coexisting but not meaningfully interacting. LLMs have billions of parameters, and there is plenty of room in those weights to predict language and to model a physical world <em>separately</em>. Bridging that gap — using physical understanding to actually improve language reasoning — remains undemonstrated.</p><p>DeepMind&#39;s <a href="https://arxiv.org/abs/2509.24527">Dreamer 4</a> also hints at this direction. Rather than borrowing intelligence from a language model, Dreamer 4 learns a world model from gameplay footage, then trains an RL agent within that world model through simulated rollouts where the agent takes actions, observes consequences provided by the world model, and updates its policy. This is genuinely closer to perception-action coupling: the agent learns <em>through</em> acting. However, the goal of this research is not general intelligence — it&#39;s sample-efficient control for robotics. The agent is trained and evaluated on predefined task milestones (get wood, craft pickaxe, find diamond), scored by a learned reward model. Nobody has tested whether the representations learned through this sort of training generalize to reasoning, language, or anything beyond the specific control tasks they were trained on. The gap between &#34;an agent that learns to get diamonds in Minecraft through simulated practice&#34; and &#34;embodied experience that produces transferable cognitive primitives&#34; is enormous and entirely unexplored.</p><p>As far as I understand, we don&#39;t know how to:</p><ul><li><p>embed an agent in a perception-action coupled training environment</p></li><li><p>create an objective and training process that leads it to learn cognitive primitives like spatial reasoning or object permanence</p></li><li><p>leverage this to improve language models or move closer to general artificial intelligence</p></li></ul><p>Recent benchmarking work underscores how far we are. Stanford&#39;s <a href="https://arxiv.org/abs/2511.20937">ENACT benchmark</a> (2025) tested whether frontier vision-language models exhibit signs of embodied cognition — things like affordance recognition, action-effect reasoning, and long-horizon memory. The results were stark: current models lag significantly behind humans, and the gap <em>widens</em> as tasks require longer interaction horizons.</p><p>In short: world models are a genuinely exciting direction, and they could be the path to learning foundational primitives like object permanence, causality, and affordance. But this work is still in the absolute earliest stages. Transformers were an incredible leap forward, which is why we can now have things like the ENACT benchmark which better illustrate the boundaries of cognition. I think this area is really promising, but research in this space could easily take decades.</p><p>I will also mention that the most prominent &#34;world model&#34; comes from Yann LeCun, who recently left Meta to start <a href="https://www.technologyreview.com/2026/01/22/1131661/yann-lecuns-new-venture-ami-labs/">AMI Labs</a>. His <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">Joint Embedding Predictive Architecture (JEPA)</a> is a representation learning method: it trains a Vision Transformer on video data, masking parts of the input and predicting their abstract representations rather than their raw pixels. The innovation is predicting in representation space rather than input space, which lets the model focus on high-level structure and ignore unpredictable low-level details. This is a genuine improvement over generative approaches for learning useful embeddings. But despite the &#34;world model&#34; branding, JEPA&#39;s actual implementations (I-JEPA, V-JEPA, V-JEPA 2) are still training on passively observed video — not on agents embedded in physics simulations. There is no perception-action coupling, no closed-loop interaction with an environment. JEPA is a more sophisticated way to learn from observation, but by the logic of the argument above, observation alone is unlikely to yield the cognitive primitives that emerge from acting in the world.</p><h2 id="benchmarking-the-gap"><a href="#benchmarking-the-gap">Benchmarking the gap</a></h2><p>The <a href="https://arcprize.org/">ARC-AGI benchmark</a> offers an important illustration of where these primitives show up. ARC tasks are grid-based visual puzzles that test abstract reasoning: spatial composition, symmetry, relational abstraction, and few-shot generalization. They require no world knowledge or language — just the ability to infer abstract rules from a handful of examples and apply them to novel cases. Humans solve these tasks trivially, usually in under two attempts. When <a href="https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025">ARC-AGI-2</a> launched in March 2025, pure LLMs scored 0% and frontier reasoning systems achieved only single-digit percentages. By the end of the year, <a href="https://arcprize.org/blog/arc-prize-2025-results-analysis">refinement-loop systems</a> — scaffolding that wraps a model in iterative generate-verify-refine cycles — pushed scores to <a href="https://poetiq.ai/posts/arcagi_verified/">54% on the semi-private eval</a> and as high as 75% on the public eval using GPT-5.2, surpassing the 60% human average. But the nature of this progress matters as much as the numbers.</p><p>The nature of this progress is telling: the top standalone model without <a href="https://arxiv.org/abs/2601.10904">refinement</a> scaffolding — Claude Opus 4.5 — scores 37.6%. It takes a <a href="https://poetiq.ai/posts/arcagi_verified/">refinement harness</a> running dozens of iterative generate-verify-refine cycles at $30/task to push that to 54%, and a combination of GPT-5.2&#39;s strongest reasoning mode plus such a harness to reach 75%. This is not behavior that comes out of the core transformer architecture — it is scaffolded brute-force search, with each percentage point requiring substantially more compute. The <a href="https://arcprize.org/competitions/2025/">ARC Prize Grand Prize</a> at 85% remains unclaimed.</p><p>ARC is important because it illustrates the kind of abstract reasoning that seems central to intelligence. For humans, these capabilities arose from embodied experience. It&#39;s conceivable that training methods operating in purely abstract or logical spaces could teach an agent similar primitives without embodiment. We simply don&#39;t know yet. Research in this direction is just beginning, catalyzed by benchmarks like ARC that are sharpening our understanding of the boundary between what LLMs do and what intelligence actually requires. Notably, the benchmark itself is evolving in this direction <a href="https://arxiv.org/abs/2601.10904">ARC-AGI-3</a> introduces interactive reasoning challenges requiring exploration, planning, memory, and goal acquisition — moving closer to the perception-action coupling that I argue is central to intelligence.</p><p>It&#39;s worth addressing a common counterargument here: AI models have saturated many benchmarks in recent years, and we have to keep introducing new ones. Isn&#39;t this just moving the goalposts? I don&#39;t think this framing is true - benchmark saturation is exactly how we learn what a benchmark was actually measuring. Creating different benchmarks in response is not goalpost-moving — it&#39;s the normal process of refining our instruments and understanding. The &#34;G&#34; in AGI stands for &#34;general&#34; — truly general intelligence should transfer from one reasoning task to another. If a model had genuinely learned abstract reasoning from saturating one benchmark, the next benchmark testing similar capabilities should be easy, not devastating. The fact that each new generation of benchmarks consistently exposes fundamental failures is itself evidence about the nature of the gap. The ARC benchmark series illustrates this well: the progression from ARC-AGI-1 to ARC-AGI-3 didn&#39;t require heroic effort to find tasks that stump AI while remaining easy for humans - it just required refining the understanding of where the boundary lies. Tasks that are trivially easy for humans but impossible for current models are <em>abundant</em> (see multi-digit arithmetic, above). The benchmark designers aren&#39;t hunting for exotic edge cases; they&#39;re mapping a vast territory of basic cognitive capability that AI simply doesn&#39;t have.</p><h2 id="addendum-gemini-3-deep-think-and-inference-time-co"><a href="#addendum-gemini-3-deep-think-and-inference-time-co">Addendum: Gemini 3 Deep Think and inference-time compute <em>added after initial publication</em></a></h2><p>I didn&#39;t realize while writing this piece that Google DeepMind released <a href="https://deepmind.google/technologies/gemini/deep-think/">Gemini 3 Deep Think</a> (February 12, 2026), which scored <a href="https://arcprize.org/blog/arc-prize-2025-results-analysis">84.6% on ARC-AGI-2</a> — just shy of the 85% Grand Prize threshold. For context, the base Gemini 3 Pro model scores 31.1%. The entire 53-point gap is inference-time compute: extended reasoning chains, parallel hypothesis exploration, and search.</p><p>This result is significant. The <a href="https://arcprize.org/blog/arc-prize-2025-results-analysis">ARC Prize team&#39;s analysis</a> identifies &#34;refinement loops&#34; — iterative generate-verify-refine cycles — as the central theme driving progress. The intelligence is coming from scaffolding and search rather than from the base model having learned general abstract reasoning. As the ARC Prize team put it:</p><blockquote><p>For the ARC-AGI-1/2 format, we believe the Grand Prize accuracy gap is now primarily bottlenecked by engineering while the efficiency gap remains bottlenecked by science and ideas. ARC Prize stands for open AGI progress, and, as we&#39;ve previously committed, we will continue to run the ARC-AGI-2 Grand Prize competition in 2026 to track progress towards a fully open and reproducible solution.</p></blockquote><blockquote><p>As good as AI reasoning systems are, they still exhibit many flaws and inefficiencies necessary for AGI. We still need new ideas, like how to separate knowledge and reasoning, among others. And we&#39;ll need new benchmarks to highlight when those new ideas arrive.</p></blockquote><p>I am now really curious about how the agents will fare with AGI-3, which comes out in March 2026. Are refinement loops /
search / extended CoT chains effective at general reasoning? My guess is that these techniques are specifically fitting to the geometric pattern format of AGI 1 and 2, and we&#39;ll see a big drop-off in performance on AGI-3.</p><p>The transformer architectures powering current LLMs are strictly feed-forward. Information flows from tokens through successive layers to the output, and from earlier tokens to later ones, but never backward. This is partly because backpropagation — the method used to train neural networks — <a href="https://www.offconvex.org/2016/12/20/backprop/">requires acyclic computation graphs</a>. But there&#39;s also a hard practical constraint: these models have hundreds of billions of parameters and are trained on trillions of tokens, and rely heavily on reusing computation. When processing token N+1, an LLM reuses all the computation from tokens 1 through N (a technique called KV caching). This is what makes training and inference tractable at scale. But it also means the architecture is locked into a one-directional flow — processing a new token can never revisit or revise the representations of earlier ones. Any architecture that allowed backward flow would compromise this caching, requiring novel computational techniques to make it tractable at scale.</p><p>Human brains function in a fundamentally different way. The brain is not a feed-forward pipeline. Activations reverberate through recurrent, bidirectional connections, eventually settling into stable patterns. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3864796/">For every feedforward connection in the visual cortex, there is a reciprocal feedback connection</a> carrying contextual information back to earlier processing stages. When you recognize a face, it&#39;s not the output of a single forward pass — it&#39;s the result of distributed activity that echoes back and forth between regions until the system converges on an interpretation.</p><p>This is not to say that the human brain architecture is <em>necessary</em> to reach general intelligence. But the contrast helps contextualize just how constrained current LLM architectures are. There&#39;s a growing body of <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00663/120983">peer-reviewed theoretical work</a> formalizing these constraints. Merrill and Sabharwal have <a href="https://arxiv.org/abs/2207.00729">shown</a> that fixed-depth transformers with realistic (log-precision) arithmetic fall within the complexity class TC⁰ — which means they provably cannot recognize even regular languages or determine whether two nodes in a graph are connected. These are formally simple problems, well within the reach of basic algorithms, that transformers provably cannot solve in a single forward pass. This isn&#39;t an engineering limitation to be overcome with more data or compute — it&#39;s a mathematical property of the architecture itself. And Merrill and Sabharwal go further, arguing that this is a consequence of the transformer&#39;s high parallelizability: any architecture that is as parallelizable — and therefore as scalable — will hit similar walls.</p><p>What might alternative architectures look like? Gary Marcus has long advocated for other approaches, like <a href="https://arxiv.org/abs/2002.06177">neurosymbolic AI</a> — hybrid systems that combine neural networks with explicit symbolic reasoning modules for logic, compositionality, and variable binding. I think that neural architectures with feedback connections — networks that are not strictly feed-forward but allow information to flow backward and settle into stable states — could learn to represent cognitive primitives. The challenge, as discussed above, is that such architectures break the computational shortcuts that make current transformers trainable and deployable at scale. In either case, getting neurosymbolic, recurrent or bidirectional neural networks to work at the scale of modern LLMs is an open engineering and research problem.</p><h2 id="revision-chain-of-thought-changes-this-picture-add"><a href="#revision-chain-of-thought-changes-this-picture-add">Revision: Chain of Thought changes this picture <em>added after initial publication</em></a></h2><p>A <a href="https://news.ycombinator.com/item?id=47030078">reader pointed out</a> that chain of thought effectively invalidates the feed-forward argument, since we are never doing a single feed-forward pass, but instead repeated passes where preceding tokens are fed back into the network. As such, the transformer can use its own context window as a working space to solve a more complex class of problems. After this, I found a <a href="https://arxiv.org/abs/2310.07923">follow-up paper by the same authors</a> (Merrill &amp; Sabharwal, ICLR 2024) that confirms this. While a single forward pass through a transformer is limited to TC⁰, allowing the model to generate intermediate &#34;chain of thought&#34; tokens — where each token is the output of a new forward pass conditioned on all previous tokens — fundamentally extends its computational power. Specifically, with a polynomial number of CoT steps, a transformer can solve any problem in P.</p><p>This matters because modern &#34;reasoning&#34; models (OpenAI&#39;s o-series, Anthropic&#39;s Claude with extended thinking, DeepSeek R1) do exactly this: they generate long chains of intermediate reasoning tokens before producing an answer. The theoretical result says that this approach, in principle, overcomes the TC⁰ barrier I described above.</p><p>I&#39;ll admit I was a victim of anti-AI media hype on this point. I was sold on the architecture argument after reading a <a href="https://www.wired.com/story/ai-agents-math-doesnt-add-up/">Wired article</a> and <a href="https://arxiv.org/pdf/2507.07505">an accompanying paper</a> that brushed off CoT&#39;s impact on complexity, arguing that the base operation still carries the limited complexity and that token budgets are too small. In hindsight, that doesn&#39;t really address the formal result.</p><p>That said, there are important caveats. First, the theoretical result is about expressive power — what a transformer with CoT <em>could</em> compute with the right weights — not about what models actually <em>learn</em> to do. As the authors themselves note: &#34;our lower bounds do not directly imply transformers can learn to use intermediate steps effectively.&#34; Whether current training methods (including reinforcement learning) can actually teach models to exploit this theoretical capacity is an open question.</p><p>Second, the P result works by showing that a transformer can encode the transitions of any <em>specific</em> Turing machine, with the CoT tokens serving as the tape. But AGI would require something more demanding: the feed-forward network would need to encode a <em>universal</em> Turing machine — one capable of reading a novel problem, constructing a solution strategy, and executing it. (Some smart) humans can do this. Whether a fixed-depth transformer can learn to do this through CoT, even in principle, is a much stronger claim than &#34;CoT reaches P.&#34;</p><p>Furthermore, the systems achieving the highest scores on ARC-AGI-2 — like Gemini 3 Deep Think at 84.6% — go beyond simple sequential chain of thought. They use parallel hypothesis exploration, search over candidate solutions, and iterative refinement loops. This is a genuine extension to the feed-forward architecture: the transformer is no longer operating alone but is embedded in a broader program that orchestrates multiple inference passes, evaluates their outputs, and steers the search. In the original version of this piece, I suggested that alternative architectures with feedback connections might be needed. What&#39;s actually emerging is something different — the feedback is happening <em>outside</em> the model, in scaffolding that wraps the transformer in a loop. Whether this external scaffolding can ultimately substitute for the kind of internal recurrence I was imagining remains to be seen, but the progress is harder to dismiss than I initially thought.</p><p>So the architecture argument is weaker than I originally stated, but it isn&#39;t entirely gone. The theoretical ceiling has been raised from TC⁰ to P, which is a significant expansion. Whether models can actually reach that ceiling through current training methods, and whether P is sufficient for the kind of flexible, general reasoning that characterizes intelligence, remain open questions.</p><p>Most people encounter AGI through CEO proclamations. Sam Altman <a href="https://openai.com/index/reflections/">claims</a> that OpenAI knows how to build superintelligent AI. Dario Amodei <a href="https://darioamodei.com/machines-of-loving-grace">writes</a> that AI could be &#34;smarter than a Nobel Prize winner across most relevant fields&#34; by 2026. These are marketing statements from people whose companies depend on continued investment in the premise that AGI is imminent. They are not technical arguments.</p><p>Meanwhile, the actual research community tells a different story. A <a href="https://aaai.org/wp-content/uploads/2025/03/AAAI-2025-PresPanel-Report-FINAL.pdf">2025 survey by the Association for the Advancement of Artificial Intelligence (AAAI)</a>, surveying 475 AI researchers, found that 76% believe scaling up current AI approaches to achieve AGI is &#34;unlikely&#34; or &#34;very unlikely&#34; to succeed. The researchers cited specific limitations: difficulties in long-term planning and reasoning, generalization beyond training data, causal and counterfactual reasoning, and embodiment and real-world interaction. This is an extraordinary disconnect.</p><p>Consider the <a href="https://ai-2027.com/">AI 2027</a> scenario, perhaps the most widely-discussed AGI forecast of 2025. The <a href="https://www.aifuturesmodel.com/">underlying model&#39;s</a> first step is automating coding, which is entirely based on an extrapolation of the <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">METR study</a> on coding time horizons. The METR study collects coding tasks that an AI can complete with a 50% success rate, and tracks how the duration of those tasks grows over time. But task duration is not a measure of task complexity. As the ARC-AGI benchmarks illustrate, there are classes of problems that take humans only seconds to solve but that require AI systems thousands of dollars of compute and dozens of iterative refinement cycles to approach — and even then, the 85% Grand Prize threshold remains unmet. The focus on common coding tasks strongly emphasizes <em>within distribution</em> tasks, which are well-represented within the AI training set. The 50% success threshold also allows one to ignore precisely the tricky, out of distribution, short tasks that agents may not be making any progress on at all. The second step within the 2027 modeling is agents developing &#34;research taste&#34;. My take is that research taste is going to rely heavily on the short-duration cognitive primitives that the ARC highlights but the METR metric does not capture.</p><p>I&#39;d encourage anyone interested in this topic to seek out technical depth. Understand what these systems actually can and can&#39;t do. The real story is fascinating - it&#39;s about the fundamental nature of intelligence, and how far we still have to go to understand it.</p><p>Betting against AI is difficult currently, due to the sheer amount of capital being thrown at it. One thing I&#39;ve spent a lot of time thinking about is — what if there&#39;s a lab somewhere out there that&#39;s about to crack this? Maybe there are labs — even within OpenAI and Anthropic themselves — that are already working on all of these problems and keeping them secret?</p><p>But the open questions described above are not the kind of problem a secret lab can solve. They are long-standing problems that span multiple different fields — embodied cognition, evolutionary neuroscience, architecture design and complexity theory, training methodology and generalizability. Solving problems like this requires a global research community working across disciplines over many years, with plenty of dead ends along the way. This is high-risk, low-probability-of-reward, researchers-tinkering-in-a-lab kind of work. It&#39;s not a sprint towards a finish line.</p><p>This also helps us frame what AI companies are actually doing. They&#39;re buying up GPUs, building data centers, expanding product surface area, securing more funding. They are scaling up the current paradigm, which doesn&#39;t really have bearing on the fundamental research that can make progress in the problems highlighted above.</p><p>I&#39;m not saying that AGI is impossible, or even that it won&#39;t come within our lifetime. I fully believe neural networks, using appropriate architectures and training methods, can represent cognitive primitives and reach superhuman intelligence. They can probably do this without repeating our long evolutionary history, by training in simulated logical / symbolic simulations that have little to do with the physical world. I am also not saying that LLMs aren&#39;t useful. Even the current technology is fundamentally transforming our society (see <a href="https://dlants.me/ai-mid.html">AI is not mid - a response to Dr. Cottom’s NYT Op-Ed</a>)</p><p>We have to remember though that neural networks have their origins in the 1950&#39;s. Modern backpropagation was popularized in 1986. Many of the advances that made modern GPTs possible were discovered gradually over the following decades:</p><ul><li><p>Long Short-Term Memory (LSTM) networks, which solved the vanishing gradient problem for sequence modeling — Hochreiter and Schmidhuber, 1997</p></li><li><p>Attention mechanisms, which allowed models to dynamically focus on relevant parts of their input — Bahdanau et al., 2014</p></li><li><p>Residual connections (skip layers), which made it possible to train networks hundreds of layers deep — He et al., 2015</p></li><li><p>The transformer architecture itself, which combined attention with parallelizable training to replace recurrent networks entirely — Vaswani et al., 2017</p></li></ul><p>Transformers have fundamental limitations. They are very powerful, and they have taught us a lot about what general intelligence is. We are gaining a more and more crisp understanding of where the boundaries lie. But solving these problems will require research, which is a non-linear processs full of dead ends and plateaus. It could take decades, and even then we might discover new and more nuanced issues.</p></div></div></div>
  </div></div>
  </body>
</html>
