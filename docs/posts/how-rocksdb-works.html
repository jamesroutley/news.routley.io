<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://artem.krylysov.com/blog/2023/04/19/how-rocksdb-works/">Original</a>
    <h1>How RocksDB Works</h1>
    
    <div id="readability-page-1" class="page"><article>
        <h2>How RocksDB works</h2>

        


        <div id="introduction">
<h3>Introduction<a href="#introduction" title="Permalink to this headline"> #</a></h3>
<p>Over the past years, the adoption of RocksDB increased dramatically. It became a standard for embeddable key-value stores.</p>
<p>Today RocksDB runs in production at Meta, <a href="https://blogs.bing.com/Engineering-Blog/october-2021/RocksDB-in-Microsoft-Bing" target="_blank">Microsoft</a>, <a href="https://netflixtechblog.com/application-data-caching-using-ssds-5bf25df851ef" target="_blank">Netflix</a>, <a href="https://eng.uber.com/cherami-message-queue-system/" target="_blank">Uber</a>. At <a href="https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/" target="_blank">Meta</a> RocksDB serves as a storage engine for the MySQL deployment powering the distributed graph database called TAO.</p>
<p>Big tech companies are not the only RocksDB users. Several startups were built around RocksDB - <a href="https://www.cockroachlabs.com/" target="_blank">CockroachDB</a>, <a href="https://www.yugabyte.com/" target="_blank">Yugabyte</a>, <a href="https://www.pingcap.com/" target="_blank">PingCAP</a>, <a href="https://rockset.com/" target="_blank">Rockset</a>.</p>
<p>I spent the past 4 years at Datadog building and running services on top of RocksDB in production. In this post, I&#39;ll give a high-level overview of how RocksDB works.</p>
</div>
<div id="what-is-rocksdb">
<h3>What is RocksDB<a href="#what-is-rocksdb" title="Permalink to this headline"> #</a></h3>
<p>RocksDB is an embeddable persistent key-value store. It&#39;s a type of database designed to store large amounts of unique keys associated with values. The simple key-value data model can be used to build search indexes, document-oriented databases, SQL databases, caching systems and message brokers.</p>
<p>RocksDB was forked off Google&#39;s <a href="https://github.com/google/leveldb" target="_blank">LevelDB</a> in 2012 and optimized to run on servers with SSD drives.
Currently, RocksDB is <a href="https://github.com/facebook/rocksdb" target="_blank">developed</a> and maintained by Meta.</p>
<p>RocksDB is written in C++, so additionally to C and C++, the ะก bindings allow embedding the library into applications written in other languages such as <a href="https://github.com/rust-rocksdb/rust-rocksdb" target="_blank">Rust</a>, <a href="https://github.com/linxGnu/grocksdb" target="_blank">Go</a> or <a href="https://github.com/facebook/rocksdb/tree/main/java" target="_blank">Java</a>.</p>
<p>If you ever used SQLite, then you already know what an embeddable database is! In the context of databases, and particularly in the context of RocksDB, &#34;embeddable&#34; means:</p>
<ul>
<li><p>The database doesn&#39;t have a standalone process, it&#39;s instead linked with your application.</p></li>
<li><p>It doesn&#39;t come with a built-in server that can be accessed over the network.</p></li>
<li><p>It is not distributed, meaning it does not provide fault tolerance, replication, or sharding mechanisms.</p></li>
</ul>
<p>It is up to the application to implement these features if necessary.</p>
<p>RocksDB stores data as a collection of key-value pairs. Both keys and values are not typed, they are just arbitrary byte arrays. The database provides a low-level interface with a few functions for modifying the state of the collection:</p>
<ul>
<li><p><span>put(key, value)</span>: stores a new key-value pair or updates an existing one</p></li>
<li><p><span>merge(key, value)</span>: combines the new value with the existing value for a given key</p></li>
<li><p><span>delete(key)</span>: removes a key-value pair from the collection</p></li>
</ul>
<p>Values can be retrieved with point lookups:</p>
<ul>
<li><p><span>get(key)</span></p></li>
</ul>
<p>An iterator enables &#34;range scans&#34; - seeking to a specific key and accessing subsequent key-value pairs in order:</p>
<ul>
<li><p><span>iterator.seek(key_prefix); <span>iterator.value();</span> iterator.next()</span></p></li>
</ul>
</div>
<div id="log-structured-merge-tree">
<h3>Log-structured merge-tree<a href="#log-structured-merge-tree" title="Permalink to this headline"> #</a></h3>
<p>The core data structure behind RocksDB is called the <em>Log-structured merge-tree</em> (LSM-Tree). It&#39;s a tree-like structure organized into multiple levels, with data on each level ordered by key. The LSM-tree was primarily designed for write-heavy workloads and was introduced in 1996 in a <a href="http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf" target="_blank">paper</a> under the same name.</p>
<p>The top level of the LSM-Tree is kept in memory and contains the most recently inserted data. The lower levels are stored on disk and are numbered from 0 to N. Level 0 (L0) stores data moved from memory to disk, Level 1 and below store older data. When a level becomes too large, it&#39;s merged with the next level, which is typically an order of magnitude larger than the previous one.</p>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-lsm.png"/></p><div>
<p>Note</p>
<p>I&#39;ll be talking specifically about RocksDB, but most of the concepts covered apply to many databases that uses LSM-trees under the hood (e.g. Bigtable, HBase, Cassandra, ScyllaDB, LevelDB, MongoDB WiredTiger).</p>
</div>
<p>To better understand how LSM-trees work, let&#39;s take a closer look at the write and read paths.</p>
</div>
<div id="write-path">
<h3>Write path<a href="#write-path" title="Permalink to this headline"> #</a></h3>
<div id="memtable">
<h4>MemTable<a href="#memtable" title="Permalink to this headline"> #</a></h4>
<p>The top level of the LSM-tree is known as the <em>MemTable</em>. It&#39;s an in-memory buffer that holds keys and values before they are written to disk. All inserts and updates always go through the memtable. This is also true for deletes - rather than modifying key-value pairs in-place, RocksDB marks deleted keys by inserting a tombstone record.</p>
<p>The memtable is configured to have a specific size in bytes. When the memtable becomes full, it is swapped with a new memtable, the old memtable becomes immutable.</p>
<div>
<p>Note</p>
<p>The default size of the memtable is 64MB.</p>
</div>
<p>Let&#39;s start by adding a few keys to the database:</p>
<pre><code><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;chipmunk&#34;</span><span>,</span><span> </span><span>&#34;1&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;cat&#34;</span><span>,</span><span> </span><span>&#34;2&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;raccoon&#34;</span><span>,</span><span> </span><span>&#34;3&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;dog&#34;</span><span>,</span><span> </span><span>&#34;4&#34;</span><span>)</span></code></pre>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-memtable.png"/></p><p>As you can see, the key-value pairs in the memtable are ordered by their key. Although <em>chipmunk</em> was inserted first, it comes after <em>cat</em> in the memtable due to the sorted order. The ordering is a requirement for supporting range scans and it makes some operations, which I will cover later more efficient.</p>
</div>
<div id="write-ahead-log">
<h4>Write-ahead log<a href="#write-ahead-log" title="Permalink to this headline"> #</a></h4>
<p>In the event of a process crash or a planned application restart, data stored in the process memory is lost. To prevent data loss and ensure that database updates are durable, RocksDB writes all updates to the <em>Write-ahead log</em> (WAL) on disk, in addition to the memtable. This way the database can replay the log and restore the original state of the memtable on startup.</p>
<p>The WAL is an append-only file, consisting of a sequence of records. Each record contains a checksum, a key-value pair, and a record type (Put/Merge/Delete). Unlike in the memtable, records in the WAL are not ordered by key. Instead, they are appended in the order in which they arrive.</p>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-wal.png"/>
</p></div>
<div id="flush">
<h4>Flush<a href="#flush" title="Permalink to this headline"> #</a></h4>
<p>RocksDB runs a dedicated background thread that persists immutable memtables to disk. As soon as the flush process is complete, the immutable memtable and the corresponding WAL are discarded. RocksDB starts writing to a new WAL and a new memtable. Each flush produces a single <em>SST</em> file on L0. The produced files are immutable - they are never modified once written to disk.</p>
<p>The default memtable implementation in RocksDB is based on a <a href="https://en.wikipedia.org/wiki/Skip_list" target="_blank">skip list</a>. The data structure is a linked list with additional layers of links that allow fast search and insertion in sorted order. The ordering makes the flush process efficient, allowing the memtable content to be written to disk sequentially by iterating the key-value pairs. Turning random inserts into sequential writes is one of the key ideas behind the LSM-tree design.</p>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-flush.png"/></p><div>
<p>Note</p>
<p>RocksDB is highly configurable. Like many other components, the memtable implementation can be swapped with an alternative. It&#39;s not uncommon to see self-balancing binary search trees used to implement memtables in other LSM-based databases.</p>
</div>
</div>
<div id="sst">
<h4>SST<a href="#sst" title="Permalink to this headline"> #</a></h4>
<p><em>SST</em> stands for Static Sorted Table (or Sorted String Table in some other databases). This is a block-based file format that organizes data into fixed-size blocks (4KB by default). Individual blocks can be compressed with various compression algorithms supported by RocksDB, such as Zlib, BZ2, Snappy, LZ4, or ZSTD. Similar to records in the WAL, blocks contain checksums to detect data corruptions. RocksDB verifies these checksums every time it reads from the disk.</p>
<p>Blocks in an SST file are divided into sections. The first section, the <em>data</em> section, contains an ordered sequence of key-value pairs. This ordering allows delta-encoding of keys, meaning that instead of storing full keys, we can store only the difference between adjacent keys.</p>
<p>To find a specific key, we could use binary search on the SST file blocks. RocksDB optimizes lookups by adding an index, which is stored in a separate section right after the data section. The index maps the last key in each data block to its corresponding offset on disk. Again, the keys in the index are ordered, allowing us to find a key quickly by performing a binary search. For example, if we are searching for <em>lynx</em>, the index tells us the key might be in the block 2 because <em>lynx</em> comes after <em>chipmunk</em>, but before <em>raccoon</em>.</p>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-sst.png"/></p><p>In reality, there is no <em>lynx</em> in the SST file above, but we had to read the block from disk and search it. RocksDB supports enabling a <a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank">bloom filter</a> - a space-efficient probabilistic data structure used to test whether an element is in a set. It&#39;s stored in an optional bloom filter section and makes searching for keys that don&#39;t exist faster.</p>
<p>Additionally, there are several other less interesting sections, like the metadata section.</p>
</div>
<div id="compaction">
<h4>Compaction<a href="#compaction" title="Permalink to this headline"> #</a></h4>
<p>What I described so far is already a functional key-value store. But there are a few challenges that would prevent using it in a production system: space and read amplification. <em>Space amplification</em> measures the ratio of storage space to the size of the logical data stored. Let&#39;s say, if a database needs 2MB of disk space to store key-value pairs that take 1MB, the space amplification is <em>2</em>. Similarly, <em>read amplification</em> measures the number of IO operations to perform a logical read operation. I&#39;ll let you figure out what <em>write amplification</em> is as a little exercise.</p>
<p>Now, let&#39;s add more keys to the database and remove a few:</p>
<pre><code><span>db</span><span>.</span><span>delete</span><span>(</span><span>&#34;chipmunk&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;cat&#34;</span><span>,</span> <span>&#34;5&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;raccoon&#34;</span><span>,</span> <span>&#34;6&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;zebra&#34;</span><span>,</span> <span>&#34;7&#34;</span><span>)</span><span>
</span><span>//</span> <span>Flush</span> <span>triggers</span><span>
</span><span>db</span><span>.</span><span>delete</span><span>(</span><span>&#34;raccoon&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;cat&#34;</span><span>,</span> <span>&#34;8&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;zebra&#34;</span><span>,</span> <span>&#34;9&#34;</span><span>)</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>&#34;duck&#34;</span><span>,</span> <span>&#34;10&#34;</span><span>)</span></code></pre>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-compaction1.png"/></p><p>As we keep writing, the memtables get flushed and the number of SST files on L0 keeps growing:</p>
<ul>
<li><p>The space taken by deleted or updated keys is never reclaimed. For example, the <em>cat</em> key has three copies, <em>chipmunk</em> and <em>raccoon</em> still take up space on the disk even though they&#39;re no longer needed.</p></li>
<li><p>Reads get slower as their cost grows with the number of SST files on L0. Each key lookup requires inspecting every SST file to find the needed key.</p></li>
</ul>
<p>A background process called <em>compaction</em> helps to reduce space and read amplification in exchange for increased write amplification. Compaction selects SST files on one level and merges them with SST files on a level below, discarding deleted and overwritten keys.</p>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-compaction2.png"/></p><p><em>Leveled Compaction</em> is the default compaction strategy in RocksDB. With Leveled Compaction, key ranges of SST files on L0 overlap. Levels 1 and below are organized to contain a single sorted key range partitioned into multiple SST files, ensuring that there is no overlap in key ranges within a level. Compaction picks files on a level and merges them with the overlapping range of files on the level below. For example, during an L0 to L1 compaction, if the input files on L0 span the entire key range, the compaction has to pick all files from L0 and all files from L1.</p>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-compaction3.png"/></p><p>For this L1 to L2 compaction below, the input file on L1 overlaps with two files on L2, so the compaction is limited only to a subset of files.</p>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-compaction4.png"/></p><p>Compaction is triggered when the number of SST files on L0 reaches a certain threshold (4 by default). For L1 and below, compaction is triggered when the size of the entire level exceeds the configured <em>target size</em>. When this happens, it may trigger an L1 to L2 compaction. This way, an L0 to L1 compaction may cascade all the way to the bottommost level. After the compaction ends, RocksDB updates its metadata and removes compacted files from disk.</p>
<div>
<p>Note</p>
<p>RocksDB provides other compaction strategies offering different tradeoffs between space, read and write amplification.</p>
</div>
<p>Remember that keys in SST files are ordered? The ordering guarantee allows merging multiple SST files incrementally with the help of the <a href="https://en.wikipedia.org/wiki/K-way_merge_algorithm" target="_blank">k-way merge algorithm</a>. <em>K-way merge</em> is a generalized version of the <em>two-way merge</em> that works similarly to the merge phase of the <a href="https://en.wikipedia.org/wiki/Merge_sort" target="_blank">merge sort</a>.</p>
</div>
</div>
<div id="read-path">
<h3>Read path<a href="#read-path" title="Permalink to this headline"> #</a></h3>
<p>With immutable SST files persisted on disk, the read path is less sophisticated than the write path. A key lookup traverses the LSM-tree from the top to the bottom. It starts with the active memtable, descends to L0, and continues to lower levels until it finds the key or runs out of SST files to check.</p>
<p>Here are the lookup steps:</p>
<ol>
<li><p>Search the active memtable.</p></li>
<li><p>Search immutable memtables.</p></li>
<li><p>Search all SST files on L0 starting from the most recently flushed.</p></li>
<li><p>For L1 and below, find a single SST file that may contain the key and search the file.</p></li>
</ol>
<p>Searching an SST file involves:</p>
<ol>
<li><p>(optional) Probe the bloom filter.</p></li>
<li><p>Search the index to find the block the key may belong to.</p></li>
<li><p>Read the block and try to find the key there.</p></li>
</ol>
<p>That&#39;s it!</p>
<p>Consider this LSM-tree:</p>
<p><img alt="" src="https://artem.krylysov.com/images/2023-rocksdb/rocksdb-lookup.png"/></p><p>Depending on the key, a lookup may end early at any step. For example, looking up <em>cat</em> or <em>chipmunk</em> ends after searching the active memtable. Searching for <em>raccoon</em>, which exists only on Level 1 or <em>manul</em>, which doesn&#39;t exist in the LSM-tree at all requires searching the entire tree.</p>
</div>
<div id="merge">
<h3>Merge<a href="#merge" title="Permalink to this headline"> #</a></h3>
<p>RocksDB provides another feature that touches both read and write paths: the <em>Merge</em> operation. Imagine you store a list of integers in a database. Occasionally you need to extend the list. To modify the list, you read the existing value from the database, update it in memory and then write back the updated value. This is called &#34;Read-Modify-Write&#34; loop:</p>
<pre><code><span>db</span><span> </span><span>=</span><span> </span><span>open_db</span><span>(</span><span>path</span><span>)</span><span>

</span><span>// Read</span><span>
</span><span>old_val</span><span> </span><span>=</span><span> </span><span>db</span><span>.</span><span>get</span><span>(</span><span>key</span><span>)</span><span> </span><span>// RocksDB stores keys and values as byte arrays. We need to deserialize the value to turn it into a list.</span><span>
</span><span>old_list</span><span> </span><span>=</span><span> </span><span>deserialize_list</span><span>(</span><span>old_val</span><span>)</span><span> </span><span>// old_list: [1, 2, 3]</span><span>

</span><span>// Modify</span><span>
</span><span>new_list</span><span> </span><span>=</span><span> </span><span>old_list</span><span>.</span><span>extend</span><span>([</span><span>4</span><span>,</span><span> </span><span>5</span><span>,</span><span> </span><span>6</span><span>])</span><span> </span><span>// new_list: [1, 2, 3, 4, 5, 6]</span><span>
</span><span>new_val</span><span> </span><span>=</span><span> </span><span>serialize_list</span><span>(</span><span>new_list</span><span>)</span><span>

</span><span>// Write</span><span>
</span><span>db</span><span>.</span><span>put</span><span>(</span><span>key</span><span>,</span><span> </span><span>new_val</span><span>)</span><span>

</span><span>db</span><span>.</span><span>get</span><span>(</span><span>key</span><span>)</span><span> </span><span>// deserialized value: [1, 2, 3, 4, 5, 6]</span></code></pre>
<p>The approach works, but has some flaws:</p>
<ul>
<li><p>It&#39;s not thread-safe - two different threads may try to update the same key overwriting each other&#39;s updates.</p></li>
<li><p>Write amplification - the cost of the update increases as the value gets larger. E.g., appending a single integer to a list of 100 requires reading 100 and writing back 101 integers.</p></li>
</ul>
<p>In addition to the <em>Put</em> and <em>Delete</em> write operations, RocksDB supports a third write operation, <em>Merge</em>, which aims to solve these problems. The Merge operation requires providing a <em>Merge Operator</em> - a user-defined function responsible for combining incremental updates into a single value:</p>
<pre><code><span>func</span><span> </span><span>merge_operator</span><span>(</span><span>existing_val</span><span>,</span><span> </span><span>updates</span><span>)</span><span> </span><span>{</span><span>
        </span><span>combined_list</span><span> </span><span>=</span><span> </span><span>deserialize_list</span><span>(</span><span>existing_val</span><span>)</span><span>
        </span><span>for</span><span> </span><span>op</span><span> </span><span>in</span><span> </span><span>updates</span><span> </span><span>{</span><span>
                </span><span>combined_list</span><span>.</span><span>extend</span><span>(</span><span>op</span><span>)</span><span>
        </span><span>}</span><span>
        </span><span>return</span><span> </span><span>serialize_list</span><span>(</span><span>combined_list</span><span>)</span><span>
</span><span>}</span><span>

</span><span>db</span><span> </span><span>=</span><span> </span><span>open_db</span><span>(</span><span>path</span><span>,</span><span> </span><span>{</span><span>merge_operator</span><span>:</span><span> </span><span>merge_operator</span><span>})</span><span>
</span><span>// key&#39;s value is [1, 2, 3]</span><span>

</span><span>list_update</span><span> </span><span>=</span><span> </span><span>serialize_list</span><span>([</span><span>4</span><span>,</span><span> </span><span>5</span><span>,</span><span> </span><span>6</span><span>])</span><span>
</span><span>db</span><span>.</span><span>merge</span><span>(</span><span>key</span><span>,</span><span> </span><span>list_update</span><span>)</span><span>

</span><span>db</span><span>.</span><span>get</span><span>(</span><span>key</span><span>)</span><span> </span><span>// deserialized value: [1, 2, 3, 4, 5, 6]</span></code></pre>
<p>The merge operator above combines incremental updates passed to the <em>Merge</em> calls into a single value. When <em>Merge</em> is called, RocksDB inserts only incremental updates into the memtable and the WAL. Later, during flush and compaction, RocksDB calls the merge operator function to combine the updates into a single large update or a single value whenever it&#39;s possible. On a <em>Get</em> call or an iteration, if there are any pending not-compacted updates, the same function is called to return a single combined value to the caller.</p>
<p>Merge is a good fit for write-heavy streaming applications that constantly need to make small updates to the existing values. So, where is the catch? Reads become more expensive - the work done on reads is not saved. Repetitive queries fetching the same keys have to do the same work over and over again until a flush and compaction are triggered. Like almost everything else in RocksDB, the behavior can be tuned by limiting the number of merge operands in the memtable or by reducing the number of SST files in L0.</p>
</div>
<div id="challenges">
<h3>Challenges<a href="#challenges" title="Permalink to this headline"> #</a></h3>
<p>If the performance is critical for your application, the most challenging aspect of using RocksDB is configuring it appropriately for a specific workload. RocksDB offers numerous configuration options, and tuning them often requires understanding the database internals and diving deep into the RocksDB source code:</p>
<blockquote>
<p>&#34;Unfortunately, configuring RocksDB optimally is not trivial. Even we as RocksDB developers don&#39;t fully understand the effect of each configuration change. If you want to fully optimize RocksDB for your workload, we recommend experiments and benchmarking, while keeping an eye on the three amplification factors.&#34;</p>
<p>โ<a href="https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide" target="_blank">Official RocksDB Tuning Guide</a></p>
</blockquote>
</div>
<div id="final-thoughts">
<h3>Final thoughts<a href="#final-thoughts" title="Permalink to this headline"> #</a></h3>
<p>Writing a production-grade key-value store from scratch is hard:</p>
<ul>
<li><p>Hardware and OS can betray you at any moment dropping or corrupting data.</p></li>
<li><p>Performance optimizations require a large time investment.</p></li>
</ul>
<p>RocksDB solves this allowing you to focus on the business logic instead. This makes RocksDB an excellent building block for databases.</p>
</div>

    </article><p>I&#39;m not a native English speaker, and I&#39;m trying to improve my language skills. Feel free to correct me if you spot any spelling or grammatical errors!</p></div>
  </body>
</html>
