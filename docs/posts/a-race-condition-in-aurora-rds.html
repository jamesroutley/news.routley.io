<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hightouch.com/blog/uncovering-a-race-condition-in-aurora-rds">Original</a>
    <h1>A race condition in Aurora RDS</h1>
    
    <div id="readability-page-1" class="page"><div><div><p>Much of the developer world is familiar with the AWS outage in <code>us-east-1</code> that occurred on October 20th <a target="_blank" href="https://aws.amazon.com/message/101925/">due to a race condition bug</a> inside a DNS management service. The backlog of events we needed to process from that outage on the 20th stretched our system to the limits, and so we decided to increase our headroom for event handling throughput. When we attempted that infrastructure upgrade on October 23rd, we ran into yet another race condition bug in Aurora RDS. This is the story of how we figured out it was an AWS bug (later confirmed by AWS) and what we learned.</p>
<h2 id="background">Background</h2>
<p>The Hightouch Events product enables organizations to gather and centralize user behavioral data such as page views, clicks, and purchases. Customers can setup syncs to load events into a cloud data warehouse for analytics or stream them directly to marketing, operational, and analytics tools to support real-time personalization use cases.</p>
<p>Here is the portion of Hightouch’s architecture dedicated to our events system:</p>
<p><img alt="A diagram showing the architecture of Hightouch&#39;s events system" loading="lazy" width="700" height="350" decoding="async" data-nimg="1" sizes="100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=16&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 16w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=32&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 32w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=48&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 48w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=64&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 64w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=96&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 96w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=128&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 128w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=256&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 256w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=384&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 384w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=640&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 640w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=750&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 750w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=828&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 828w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=1080&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1080w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=1200&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1200w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=1920&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1920w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=2048&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 2048w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=3840&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 3840w" src="https://hightouch.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F4dadaa8b93e8218c771acf2bd9b7b2124b9186a5-2048x938.webp&amp;w=3840&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9"/></p>
<p>Hightouch events system architecture</p>
<p>Our system scales on three levers: Kubernetes clusters that contain event collectors and batch workers, Kafka for event processing, and Postgres as our virtual queue metadata store.</p>
<p>When our pagers went off during the AWS outage on the 20th, we observed:</p>
<ul role="list"><li>Services were unable to connect to Kafka brokers managed by AWS MSK.</li><li>Services struggled to autoscale because we couldn’t provision new EC2 nodes.</li><li>Customer functions for realtime data transformation were unavailable due to AWS STS errors, which caused our retry queues to balloon in size.</li></ul>
<p>Kafka’s durability meant that no events were dropped once they were accepted by the collectors, but there was a massive backlog to process. Syncs with consistently high traffic or with enrichments that needed to call slower 3rd party services took longer to catch up and were testing the limits of our (small) Postgres instance’s ability to act as a queue for the batch metadata.</p>
<p>As an aside, at Hightouch, we <a target="_blank" href="https://www.amazingcto.com/postgres-for-everything/">start with Postgres where we can</a>. Postgres queues serve our non-events architecture well at ~1M syncs/day and for events scaled to 500K events per second at ~1s end-to-end latency on a small Aurora instance.</p>
<p>After observing the events on the 20th, We wanted to upsize the DB to give us more headroom. Given that <a target="_blank" href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_AuroraOverview.html">Aurora</a> supports <a target="_blank" href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Managing.html#:~:text=As%20an%20alternative,Amazon%20Aurora%20PostgreSQL">fast failovers</a> for scaling up instances, we decided to proceed with an upgrade on Oct 23rd without a scheduled maintenance window.</p>
<h3 id="aws-aurora-rds">AWS Aurora RDS</h3>
<p>The central datastore for real-time streaming and warehouse delivery of customer events uses <strong>Amazon Aurora PostgreSQL</strong>.</p>
<p>Aurora&#39;s architecture differs from traditional PostgreSQL in a crucial way: it separates compute from storage. An Aurora cluster consists of:</p>
<ul role="list"><li><strong>One primary writer instance</strong> that handles all write operations</li><li><strong>Multiple read replica instances</strong> that handle read-only queries</li><li><strong>A shared storage layer</strong> that all instances access, automatically replicated across multiple availability zones</li></ul>
<p>This architecture enables fast failovers and efficient read scaling, but as we&#39;d discover, it also introduces unique failure modes.</p>
<p>A <strong>failover</strong> is the process of promoting a read replica to become the new primary writer - typically done automatically when the primary fails, or manually triggered for maintenance operations like ours. When you trigger a failover in the AWS console:</p>
<ol role="list"><li>Aurora designates a read replica as the new primary</li><li>The storage layer grants write privileges to the new primary</li><li>The cluster endpoint points to the new writer</li><li>The old primary becomes a read replica (if it&#39;s still healthy)</li></ol>
<p>The diagram below explains how Hightouch Events uses Aurora.</p>
<p><img alt="A diagram showing how Hightouch Events uses Aurora" loading="lazy" width="700" height="350" decoding="async" data-nimg="1" sizes="100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=16&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 16w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=32&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 32w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=48&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 48w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=64&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 64w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=96&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 96w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=128&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 128w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=256&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 256w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=384&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 384w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=640&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 640w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=750&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 750w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=828&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 828w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=1080&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1080w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=1200&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1200w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=1920&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1920w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=2048&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 2048w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=3840&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 3840w" src="https://hightouch.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2F7e22d064a925d556fea415ffe6a932916f85ddf4-2000x935.webp&amp;w=3840&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9"/></p>
<p>How Hightouch Events uses Aurora</p>
<h2 id="the-plan">The Plan</h2>
<p>This was our upgrade plan:</p>
<ol role="list"><li>Add another read replica (instance #3) to maintain read capacity during the upgrade.</li><li>Upgrade the existing reader (instance #2) to the target size and give it the highest failover priority.</li><li>Trigger a failover to promote instance #2 as the new writer (expected downtime less than 15s, handled gracefully by our backend).</li><li>Upgrade the old writer (instance #1) to match the size and make it a reader.</li><li>Remove the temporary extra reader (instance #3).</li></ol>
<p>The AWS docs supported this approach and we had already tested the process successfully in a staging environment while performing a load test, so we were confident in the correctness of the procedure.</p>
<h2 id="the-upgrade-attempt">The Upgrade Attempt</h2>
<p>At 16:39 EDT on October 23, 2025, we triggered the failover to the newly-upgraded instance #2. The AWS Console showed the typical progression: parameter adjustments, instance restarts, the usual status updates.</p>
<p>Then the page refreshed. Instance #1 - the original writer was still the primary. The failover had <strong>reversed itself</strong>.</p>
<p>According to AWS everything was healthy. The cluster appeared healthy across the board. But our backend services couldn&#39;t execute write queries. Restarting the services cleared the errors and restored normal operation, but the upgrade had failed.</p>
<p>We tried again at 16:43. Same result: brief promotion followed by immediate reversal.</p>
<p><strong>Two failed failovers in five minutes</strong>. Nothing else had changed - no code updates, no unusual queries, no traffic spikes. We had successfully tested this exact procedure in a staging environment under load earlier in the day. We checked our process to see if we had made any mistakes. We searched online to see if anyone else had encountered this issue but found nothing. Nothing obvious could explain why Aurora was refusing to complete the failover in this cluster. We were perplexed.</p>
<h2 id="the-investigation">The Investigation</h2>
<p>We first checked database metrics for anything unusual. There was a spike in connection count, network traffic, and commit throughput to the read replica (instance #2) during the failover.</p>
<p>The higher commit throughput could have been due to replication or the execution of write queries. The other two metrics simply indicated a higher query volume.</p>
<p>We checked the read query traffic from the app (graph below), and found that there was no change during this period. This told us the extra traffic to instance #2 came from our backend services which are supposed to connect to the writer instance.</p>
<p><img alt="A graph showing the query traffic from the Hightouch app" loading="lazy" width="700" height="350" decoding="async" data-nimg="1" sizes="100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=16&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 16w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=32&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 32w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=48&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 48w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=64&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 64w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=96&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 96w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=128&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 128w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=256&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 256w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=384&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 384w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=640&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 640w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=750&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 750w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=828&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 828w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=1080&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1080w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=1200&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1200w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=1920&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1920w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=2048&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 2048w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=3840&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 3840w" src="https://hightouch.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Fd8def16525fcf710cef070fe26e28d0905006fab-2048x460.webp&amp;w=3840&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9"/></p>
<p>Query traffic from the Hightouch app</p>
<p>When we looked at the backend application logs, we found this error -<code>DatabaseError: cannot execute UPDATE in a read-only transaction</code> in <em><strong>some</strong></em> pods.</p>
<p><img alt="A list of backend application logs" loading="lazy" width="700" height="350" decoding="async" data-nimg="1" sizes="100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=16&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 16w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=32&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 32w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=48&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 48w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=64&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 64w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=96&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 96w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=128&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 128w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=256&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 256w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=384&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 384w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=640&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 640w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=750&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 750w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=828&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 828w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=1080&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1080w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=1200&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1200w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=1920&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 1920w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=2048&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 2048w, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=3840&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9 3840w" src="https://hightouch.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fpwmfmi47%2Fproduction%2Ff5a45ddc73fc57823b82e0330b3992f8b34c6d15-1898x524.webp&amp;w=3840&amp;q=75&amp;dpl=dpl_E7GwXRfyynqoSoURfMzP8GereaG9"/></p>
<p>Backend application logs</p>
<p>Our services do not connect directly to the writer instance, but rather to a cluster endpoint which points to the writer. This could mean one of 3 things:</p>
<ol role="list"><li>The pods did not get the signal that the writer had changed - i.e. the cluster did not terminate the connection.</li><li>The cluster endpoint incorrectly pointed to a reader instance.</li><li>The pod was connected to the writer, but the write operation was rejected at runtime.</li></ol>
<p>We did not find any evidence supporting or disproving #1 in the application logs. We had a strong suspicion it was either #2 or #3. We downloaded the database logs to take a closer look and found something interesting. In both the promoted reader and the original writer, we found the same sequence of logs:</p>
<pre><code><span>2025</span><span>-10</span><span>-23</span> <span>20</span>:<span>38</span>:<span>58</span> UTC::@:[<span>569</span>]:LOG:  starting PostgreSQL...
...
...
...
LOG:  database <span>system</span> <span>is</span> ready <span>to</span> accept connections
LOG:  server process (PID <span>799</span>) was terminated <span>by</span> signal <span>9</span>: Killed
DETAIL:  Failed process was <span>running</span>: <span>&lt;</span>write query <span>from</span> backend application<span>&gt;</span>
LOG:  terminating <span>any</span> other active server processes
FATAL:  Can<span>&#39;t handle storage runtime process crash
LOG:  database system is shut down
</span></code></pre>
<p>This led us to a hypothesis:</p>
<p>During the failover window, Aurora briefly allowed both instances to process writes. The distributed storage layer rejected the concurrent write operations, causing both instances to crash.</p>
<p>We expect Aurora’s failover orchestration to do something like this:</p>
<ol role="list"><li>Stop accepting new writes. Clients can expect connection errors until the failover completes.</li><li>Finish processing in-flight write requests.</li><li>Demote the writer and simultaneously promote the reader.</li><li>Accept new write requests on the new writer.</li></ol>
<p><strong>There was clearly a race condition between steps 3 &amp; 4.</strong></p>
<h2 id="testing-the-hypothesis">Testing the Hypothesis</h2>
<p>To validate the theory, we performed a controlled failover attempt. This time:</p>
<ol role="list"><li>We scaled down all services that write to the database</li><li>We triggered the failover again</li><li>We monitored for storage runtime crashes</li></ol>
<p>By eliminating <strong>concurrent writes</strong>, the failover completed successfully. This strongly reinforced the race-condition hypothesis.</p>
<h2 id="aws-confirms-the-root-cause">AWS Confirms the Root Cause</h2>
<p>We escalated the findings and log patterns to AWS. After an internal review, AWS confirmed that:</p>
<p>The root cause was due to an internal signaling issue in the demotion process of the old writer, resulting in the writer being unchanged after the failover.</p>
<p><strong>They also confirmed that there was nothing unique about our configuration or usage that would trigger the bug</strong>. The conditions that caused it were not under our control.</p>
<p>AWS has indicated a fix is on their roadmap, but as of now, the recommended mitigation aligns with our solution: use Aurora’s Failover feature on an as-needed basis and ensure that no writes are executed against the DB during the failover.</p>
<h2 id="final-state">Final State</h2>
<p>With the race condition understood and mitigated, we:</p>
<ul role="list"><li>Successfully upsized the cluster in <code>us-east-1</code></li><li>Updated our internal playbooks to <strong>pause writers before an intentional failover</strong></li><li>Added monitoring to detect any unexpected writer role advertisement flips</li></ul>
<h2 id="takeaways">Takeaways</h2>
<p>The following principles were reinforced during this experience:</p>
<ol role="list"><li>Prepare for the worst in any migration - you could end up in your desired end state, beginning state, or an in-between state - even for services you trust. Ensuring you’re ready to redirect traffic and handle brief outages in dependencies will minimize downtime.</li><li>The importance of good observability cannot be emphasized enough. The “brief writer advertisement” was only detectable because we were monitoring queries to each instance in Datadog and had access to database logs in RDS.</li><li>For large scale distributed systems, isolating the impact any single component can have on the system can help both uptime and maintenance. It helps a lot if the design allows for such events without completely shutting down the system.</li><li>Test setups are not always representative of production environments. Even though we practiced the upgrade process during a load test in a staging region, we could not reproduce the exact conditions that caused the race condition in Aurora. AWS confirmed that there was nothing specific about our traffic pattern that would trigger it.</li></ol>
<p>If challenges like this investigation sound interesting, we encourage you to check out our <a target="_blank" href="https://hightouch.com/careers#open-positions">careers page</a></p></div></div></div>
  </body>
</html>
