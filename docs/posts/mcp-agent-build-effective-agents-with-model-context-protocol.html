<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/lastmile-ai/mcp-agent">Original</a>
    <h1>Show HN: Mcp-Agent – Build effective agents with Model Context Protocol</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/25641935/407477454-6f4e40c4-dc88-47b6-b965-5856b69416d2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc0Nzc0NTQtNmY0ZTQwYzQtZGM4OC00N2I2LWI5NjUtNTg1NmI2OTQxNmQyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFjNGJiYjI4OTJiYWFjMTM5NzI3ODNkYTJjZjViMTI5M2E0ZjIwZjM1M2NhYmNiY2U0ODJkZjRjZjQ5YjBjYjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0._4VkM5EwJrDy8LyWs15OdAvltxUJ1hRagTEGGN9iFwQ"><img src="https://private-user-images.githubusercontent.com/25641935/407477454-6f4e40c4-dc88-47b6-b965-5856b69416d2.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc0Nzc0NTQtNmY0ZTQwYzQtZGM4OC00N2I2LWI5NjUtNTg1NmI2OTQxNmQyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFjNGJiYjI4OTJiYWFjMTM5NzI3ODNkYTJjZjViMTI5M2E0ZjIwZjM1M2NhYmNiY2U0ODJkZjRjZjQ5YjBjYjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0._4VkM5EwJrDy8LyWs15OdAvltxUJ1hRagTEGGN9iFwQ" alt="Logo" width="300"/></a>
</p>
<p dir="auto">
  <em>Build effective agents with Model Context Protocol using simple, composable patterns.</em>
</p><p dir="auto">
  <a href="https://github.com/lastmile-ai/mcp-agent/tree/main/examples"><strong>Examples</strong></a>
  |
  <a href="https://www.anthropic.com/research/building-effective-agents" rel="nofollow"><strong>Building Effective Agents</strong></a>
  |
  <a href="https://modelcontextprotocol.io/introduction" rel="nofollow"><strong>MCP</strong></a>
</p>
<p dir="auto">
<a href="https://pypi.org/project/mcp-agent/" rel="nofollow"><img src="https://camo.githubusercontent.com/c6cc256bdb95c7f9f06cdf6beabd507cddf53f9fa76677164669a92bf19773f2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d63702d6167656e743f636f6c6f723d253233333444303538266c6162656c3d70797069" data-canonical-src="https://img.shields.io/pypi/v/mcp-agent?color=%2334D058&amp;label=pypi"/></a>
<a href="https://github.com/lastmile-ai/mcp-agent/issues"><img src="https://camo.githubusercontent.com/56979dcb9a2862d0b2642d0ff9c6f7781dfd8007b9a88dba15ef41966e13d001/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6c6173746d696c652d61692f6d63702d6167656e74" data-canonical-src="https://img.shields.io/github/issues-raw/lastmile-ai/mcp-agent"/></a>
<a href="https://lmai.link/discord/mcp-agent" rel="nofollow"><img src="https://camo.githubusercontent.com/06a525960d21fc26ff845c6952a7f6c9bceb2492c79da56dc9a38203a792f7dd/68747470733a2f2f736869656c64732e696f2f646973636f72642f31303839323834363130333239393532333537" alt="discord" data-canonical-src="https://shields.io/discord/1089284610329952357"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/53fda9c4c71a37f38ba80a93557e03de300bea6b7c2fdd83f7b6745b62122740/68747470733a2f2f696d672e736869656c64732e696f2f706570792f64742f6d63702d6167656e743f6c6162656c3d70797069253230253743253230646f776e6c6f616473"><img alt="Pepy Total Downloads" src="https://camo.githubusercontent.com/53fda9c4c71a37f38ba80a93557e03de300bea6b7c2fdd83f7b6745b62122740/68747470733a2f2f696d672e736869656c64732e696f2f706570792f64742f6d63702d6167656e743f6c6162656c3d70797069253230253743253230646f776e6c6f616473" data-canonical-src="https://img.shields.io/pepy/dt/mcp-agent?label=pypi%20%7C%20downloads"/></a>
<a href="https://github.com/lastmile-ai/mcp-agent/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/b0a90e81f70acaa7f30c721b56686de63ce9795673928ad3d441628fa28fdd73/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f6d63702d6167656e74" data-canonical-src="https://img.shields.io/pypi/l/mcp-agent"/></a>
</p>

<p dir="auto"><strong><code>mcp-agent</code></strong> is a simple, composable framework to build agents using <a href="https://modelcontextprotocol.io/introduction" rel="nofollow">Model Context Protocol</a>.</p>
<p dir="auto"><strong>Inspiration</strong>: Anthropic announced 2 foundational updates for AI application developers:</p>
<ol dir="auto">
<li><a href="https://www.anthropic.com/news/model-context-protocol" rel="nofollow">Model Context Protocol</a> - a standardized interface to let any software be accessible to AI assistants via MCP servers.</li>
<li><a href="https://www.anthropic.com/research/building-effective-agents" rel="nofollow">Building Effective Agents</a> - a seminal writeup on simple, composable patterns for building production-ready AI agents.</li>
</ol>
<p dir="auto"><code>mcp-agent</code> puts these two foundational pieces into an AI application framework:</p>
<ol dir="auto">
<li>It handles the pesky business of managing the lifecycle of MCP server connections so you don&#39;t have to.</li>
<li>It implements every pattern described in Building Effective Agents, and does so in a <em>composable</em> way, allowing you to chain these patterns together.</li>
<li><strong>Bonus</strong>: It implements <a href="https://github.com/openai/swarm">OpenAI&#39;s Swarm</a> pattern for multi-agent orchestration, but in a model-agnostic way.</li>
</ol>
<p dir="auto">Altogether, this is the simplest and easiest way to build robust agent applications. Much like MCP, this project is in early development.
We welcome all kinds of <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md">contributions</a>, feedback and your help in growing this to become a new standard.</p>

<p dir="auto">We recommend using <a href="https://docs.astral.sh/uv/" rel="nofollow">uv</a> to manage your Python projects:</p>

<p dir="auto">Alternatively:</p>


<div dir="auto"><p dir="auto"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363 0-4 1.69-4 3.75 0 .984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75 0 0 1-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456 0 0 0-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863 0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751 0 0 1-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304 0-2.06-1.637-3.75-4-3.75ZM5.75 12h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5ZM6 15.25a.75.75 0 0 1 .75-.75h2.5a.75.75 0 0 1 0 1.5h-2.5a.75.75 0 0 1-.75-.75Z"></path></svg>Tip</p><p dir="auto">The <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples"><code>examples</code></a> directory has several example applications to get started with.
To run an example, clone this repo, then:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd examples/mcp_basic_agent # Or any other example
cp mcp_agent.secrets.yaml.example mcp_agent.secrets.yaml # Update API keys
uv run main.py"><pre><span>cd</span> examples/mcp_basic_agent <span><span>#</span> Or any other example</span>
cp mcp_agent.secrets.yaml.example mcp_agent.secrets.yaml <span><span>#</span> Update API keys</span>
uv run main.py</pre></div>
</div>
<p dir="auto">Here is a basic &#34;finder&#34; agent that uses the fetch and filesystem servers to look up a file, read a blog and write a tweet. <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_agent">Example link</a>:</p>
<details open="">
<summary>finder_agent.py</summary>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
import os

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

app = MCPApp(name=&#34;hello_world_agent&#34;)

async def example_usage():
    async with app.run() as mcp_agent_app:
        logger = agent_app.logger
        # This agent can read the filesystem or fetch URLs
        finder_agent = Agent(
            name=&#34;finder&#34;,
            instruction=&#34;&#34;&#34;You can read local files or fetch URLs.
                Return the requested information when asked.&#34;&#34;&#34;,
            server_names=[&#34;fetch&#34;, &#34;filesystem&#34;], # MCP servers this Agent can use
        )

        async with finder_agent:
            # Automatically initializes the MCP servers and adds their tools for LLM use
            tools = await finder_agent.list_tools()
            logger.info(f&#34;Tools available:&#34;, data=tools)

            # Attach an OpenAI LLM to the agent (defaults to GPT-4o)
            llm = await finder_agent.attach_llm(OpenAIAugmentedLLM)

            # This will perform a file lookup and read using the filesystem server
            result = await llm.generate_str(
                message=&#34;Show me what&#39;s in README.md verbatim&#34;
            )
            logger.info(f&#34;README.md contents: {result}&#34;)

            # Uses the fetch server to fetch the content from URL
            result = await llm.generate_str(
                message=&#34;Print the first two paragraphs from https://www.anthropic.com/research/building-effective-agents&#34;
            )
            logger.info(f&#34;Blog intro: {result}&#34;)

            # Multi-turn interactions by default
            result = await llm.generate_str(&#34;Summarize that in a 128-char tweet&#34;)
            logger.info(f&#34;Tweet: {result}&#34;)

if __name__ == &#34;main&#34;:
    asyncio.run(example_usage())
"><pre><span>import</span> <span>asyncio</span>
<span>import</span> <span>os</span>

<span>from</span> <span>mcp_agent</span>.<span>app</span> <span>import</span> <span>MCPApp</span>
<span>from</span> <span>mcp_agent</span>.<span>agents</span>.<span>agent</span> <span>import</span> <span>Agent</span>
<span>from</span> <span>mcp_agent</span>.<span>workflows</span>.<span>llm</span>.<span>augmented_llm_openai</span> <span>import</span> <span>OpenAIAugmentedLLM</span>

<span>app</span> <span>=</span> <span>MCPApp</span>(<span>name</span><span>=</span><span>&#34;hello_world_agent&#34;</span>)

<span>async</span> <span>def</span> <span>example_usage</span>():
    <span>async</span> <span>with</span> <span>app</span>.<span>run</span>() <span>as</span> <span>mcp_agent_app</span>:
        <span>logger</span> <span>=</span> <span>agent_app</span>.<span>logger</span>
        <span># This agent can read the filesystem or fetch URLs</span>
        <span>finder_agent</span> <span>=</span> <span>Agent</span>(
            <span>name</span><span>=</span><span>&#34;finder&#34;</span>,
            <span>instruction</span><span>=</span><span>&#34;&#34;&#34;You can read local files or fetch URLs.</span>
<span>                Return the requested information when asked.&#34;&#34;&#34;</span>,
            <span>server_names</span><span>=</span>[<span>&#34;fetch&#34;</span>, <span>&#34;filesystem&#34;</span>], <span># MCP servers this Agent can use</span>
        )

        <span>async</span> <span>with</span> <span>finder_agent</span>:
            <span># Automatically initializes the MCP servers and adds their tools for LLM use</span>
            <span>tools</span> <span>=</span> <span>await</span> <span>finder_agent</span>.<span>list_tools</span>()
            <span>logger</span>.<span>info</span>(<span>f&#34;Tools available:&#34;</span>, <span>data</span><span>=</span><span>tools</span>)

            <span># Attach an OpenAI LLM to the agent (defaults to GPT-4o)</span>
            <span>llm</span> <span>=</span> <span>await</span> <span>finder_agent</span>.<span>attach_llm</span>(<span>OpenAIAugmentedLLM</span>)

            <span># This will perform a file lookup and read using the filesystem server</span>
            <span>result</span> <span>=</span> <span>await</span> <span>llm</span>.<span>generate_str</span>(
                <span>message</span><span>=</span><span>&#34;Show me what&#39;s in README.md verbatim&#34;</span>
            )
            <span>logger</span>.<span>info</span>(<span>f&#34;README.md contents: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)

            <span># Uses the fetch server to fetch the content from URL</span>
            <span>result</span> <span>=</span> <span>await</span> <span>llm</span>.<span>generate_str</span>(
                <span>message</span><span>=</span><span>&#34;Print the first two paragraphs from https://www.anthropic.com/research/building-effective-agents&#34;</span>
            )
            <span>logger</span>.<span>info</span>(<span>f&#34;Blog intro: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)

            <span># Multi-turn interactions by default</span>
            <span>result</span> <span>=</span> <span>await</span> <span>llm</span>.<span>generate_str</span>(<span>&#34;Summarize that in a 128-char tweet&#34;</span>)
            <span>logger</span>.<span>info</span>(<span>f&#34;Tweet: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;main&#34;</span>:
    <span>asyncio</span>.<span>run</span>(<span>example_usage</span>())</pre></div>
</details>
<details>
<summary>mcp_agent.config.yaml</summary>
<div dir="auto" data-snippet-clipboard-copy-content="execution_engine: asyncio
logger:
  type: console
  level: debug

mcp:
  servers:
    fetch:
      command: &#34;uvx&#34;
      args: [&#34;mcp-server-fetch&#34;]
    filesystem:
      command: &#34;npx&#34;
      args:
        [
          &#34;-y&#34;,
          &#34;@modelcontextprotocol/server-filesystem&#34;,
          &#34;&lt;add_your_directories&gt;&#34;,
        ]

openai:
  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored
  default_model: gpt-4o"><pre><span>execution_engine</span>: <span>asyncio</span>
<span>logger</span>:
  <span>type</span>: <span>console</span>
  <span>level</span>: <span>debug</span>

<span>mcp</span>:
  <span>servers</span>:
    <span>fetch</span>:
      <span>command</span>: <span><span>&#34;</span>uvx<span>&#34;</span></span>
      <span>args</span>: <span>[&#34;mcp-server-fetch&#34;]</span>
    <span>filesystem</span>:
      <span>command</span>: <span><span>&#34;</span>npx<span>&#34;</span></span>
      <span>args</span>:
        <span>[</span>
          <span><span>&#34;</span>-y<span>&#34;</span></span><span>,</span>
          <span><span>&#34;</span>@modelcontextprotocol/server-filesystem<span>&#34;</span></span><span>,</span>
          <span><span>&#34;</span>&lt;add_your_directories&gt;<span>&#34;</span></span><span>,</span>
        <span>]</span>

<span>openai</span>:
  <span><span>#</span> Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored</span>
  <span>default_model</span>: <span>gpt-4o</span></pre></div>
</details>
<details>
<summary>Agent output</summary>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/25641935/407786779-eaa60fdf-bcc6-460b-926e-6fa8534e9089.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc3ODY3NzktZWFhNjBmZGYtYmNjNi00NjBiLTkyNmUtNmZhODUzNGU5MDg5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNhMWE3NTBjZTc1ZjIyOWVmMjg3NjhmZjJiOTU5OTNmOTBjYTdlZTEyMjZmNDM3Y2I2ZWYzNzk2OTg4MjhjZmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.-ICqiww2pPvN2-4QOH0l_6Do36kAuVR4-pPz9GrlYL4"><img width="2000" alt="Image" src="https://private-user-images.githubusercontent.com/25641935/407786779-eaa60fdf-bcc6-460b-926e-6fa8534e9089.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc3ODY3NzktZWFhNjBmZGYtYmNjNi00NjBiLTkyNmUtNmZhODUzNGU5MDg5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNhMWE3NTBjZTc1ZjIyOWVmMjg3NjhmZjJiOTU5OTNmOTBjYTdlZTEyMjZmNDM3Y2I2ZWYzNzk2OTg4MjhjZmEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.-ICqiww2pPvN2-4QOH0l_6Do36kAuVR4-pPz9GrlYL4"/></a>
</details>

<ul dir="auto">
<li><a href="#why-use-mcp-agent">Why use mcp-agent?</a></li>
<li><a href="#examples">Example Applications</a>
<ul dir="auto">
<li><a href="#claude-desktop">Claude Desktop</a></li>
<li><a href="#streamlit">Streamlit</a>
<ul dir="auto">
<li><a href="#gmail-agent">Gmail Agent</a></li>
<li><a href="#simple-rag-chatbot">RAG</a></li>
</ul>
</li>
<li><a href="#marimo">Marimo</a></li>
<li><a href="#python">Python</a>
<ul dir="auto">
<li><a href="#swarm">Swarm (CLI)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#core-components">Core Concepts</a></li>
<li><a href="#workflows">Workflows Patterns</a>
<ul dir="auto">
<li><a href="#augmentedllm">Augmented LLM</a></li>
<li><a href="#parallel">Parallel</a></li>
<li><a href="#router">Router</a></li>
<li><a href="#intentclassifier">Intent-Classifier</a></li>
<li><a href="#orchestrator-workers">Orchestrator-Workers</a></li>
<li><a href="#evaluator-optimizer">Evaluator-Optimizer</a></li>
<li><a href="#swarm-1">OpenAI Swarm</a></li>
</ul>
</li>
<li><a href="#advanced">Advanced</a>
<ul dir="auto">
<li><a href="#composability">Composing multiple workflows</a></li>
<li><a href="#signaling-and-human-input">Signaling and Human input</a></li>
<li><a href="#app-config">App Config</a></li>
<li><a href="#mcp-server-management">MCP Server Management</a></li>
</ul>
</li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#roadmap">Roadmap</a></li>
<li><a href="#faqs">FAQs</a></li>
</ul>

<p dir="auto">There are too many AI frameworks out there already. But <code>mcp-agent</code> is the only one that is purpose-built for a shared protocol - <a href="https://modelcontextprotocol.io/introduction" rel="nofollow">MCP</a>. It is also the most lightweight, and is closer to an agent pattern library than a framework.</p>
<p dir="auto">As <a href="https://github.com/punkpeye/awesome-mcp-servers">more services become MCP-aware</a>, you can use mcp-agent to build robust and controllable AI agents that can leverage those services out-of-the-box.</p>

<p dir="auto">Before we go into the core concepts of mcp-agent, let&#39;s show what you can build with it.</p>
<p dir="auto">In short, you can build any kind of AI application with mcp-agent: multi-agent collaborative workflows, human-in-the-loop workflows, RAG pipelines and more.</p>

<p dir="auto">You can integrate mcp-agent apps into MCP clients like Claude Desktop.</p>

<p dir="auto">This app wraps an mcp-agent application inside an MCP server, and exposes that server to Claude Desktop.
The app exposes agents and workflows that Claude Desktop can invoke to service of the user&#39;s request.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description mcp-agent-server-demo.mp4">mcp-agent-server-demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/25641935/407666611-7807cffd-dba7-4f0c-9c70-9482fd7e0699.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc2NjY2MTEtNzgwN2NmZmQtZGJhNy00ZjBjLTljNzAtOTQ4MmZkN2UwNjk5Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQxYTViZmRhZjYxOTk4ZWY5YWFmMDJmODFmODQwN2JjZWNlMDk3OGQyNWExYTcxNTAwY2FiMTQ1NjVlODQ4MDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.uBvRvE7NVPE_psJWq6DMAHNynAM4ULSPlrJ0isuF3pU" data-canonical-src="https://private-user-images.githubusercontent.com/25641935/407666611-7807cffd-dba7-4f0c-9c70-9482fd7e0699.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc2NjY2MTEtNzgwN2NmZmQtZGJhNy00ZjBjLTljNzAtOTQ4MmZkN2UwNjk5Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQxYTViZmRhZjYxOTk4ZWY5YWFmMDJmODFmODQwN2JjZWNlMDk3OGQyNWExYTcxNTAwY2FiMTQ1NjVlODQ4MDYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.uBvRvE7NVPE_psJWq6DMAHNynAM4ULSPlrJ0isuF3pU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">This demo shows a multi-agent evaluation task where each agent evaluates aspects of an input poem, and
then an aggregator summarizes their findings into a final response.</p>
<p dir="auto"><strong>Details</strong>: Starting from a user&#39;s request over text, the application:</p>
<ul dir="auto">
<li>dynamically defines agents to do the job</li>
<li>uses the appropriate workflow to orchestrate those agents (in this case the Parallel workflow)</li>
</ul>
<p dir="auto"><strong>Link to code</strong>: <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_agent_server">examples/mcp_agent_server</a></p>


<p dir="auto">You can deploy mcp-agent apps using Streamlit.</p>

<p dir="auto">This app is able to perform read and write actions on gmail using text prompts -- i.e. read, delete, send emails, mark as read/unread, etc.
It uses an MCP server for Gmail.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description Screen.Recording.2025-01-28.at.7.37.24.PM.mov">Screen.Recording.2025-01-28.at.7.37.24.PM.mov</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/25641935/407688177-54899cac-de24-4102-bd7e-4b2022c956e3.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc2ODgxNzctNTQ4OTljYWMtZGUyNC00MTAyLWJkN2UtNGIyMDIyYzk1NmUzLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThmZmIxYWE5ZjU4MGI2MjFkZTk4NDNhMmEzNTRkYzYyNTJjYzU0NjkxNGRiZTE2MzU3MzI2MmE5NDYwODBhMGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.1AUm6_TlLc2AlX-LX5kBQEtDILQKrmoh6o4NL23UbpY" data-canonical-src="https://private-user-images.githubusercontent.com/25641935/407688177-54899cac-de24-4102-bd7e-4b2022c956e3.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc2ODgxNzctNTQ4OTljYWMtZGUyNC00MTAyLWJkN2UtNGIyMDIyYzk1NmUzLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThmZmIxYWE5ZjU4MGI2MjFkZTk4NDNhMmEzNTRkYzYyNTJjYzU0NjkxNGRiZTE2MzU3MzI2MmE5NDYwODBhMGMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.1AUm6_TlLc2AlX-LX5kBQEtDILQKrmoh6o4NL23UbpY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Link to code</strong>: <a href="https://github.com/jasonsum/gmail-mcp-server/blob/add-mcp-agent-streamlit/streamlit_app.py">gmail-mcp-server</a></p>


<p dir="auto">This app uses a Qdrant vector database (via an MCP server) to do Q&amp;A over a corpus of text.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description streamlit-mcp-rag-agent-demo.mp4">streamlit-mcp-rag-agent-demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/25641935/407691567-f4dcd227-cae9-4a59-aa9e-0eceeb4acaf4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc2OTE1NjctZjRkY2QyMjctY2FlOS00YTU5LWFhOWUtMGVjZWViNGFjYWY0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5ZGQzZDY0NWVlMGRjMmNkMWRhODMxNmQ5NDE5NzA0ZDY4NmU1MzhhNzhlMDllYmI0MjlkNTUzMzBmNjZmM2QmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.2ZJkU5H5jU3wEufZ_0YB2gBwujZZPwC0qVxplMtnbWc" data-canonical-src="https://private-user-images.githubusercontent.com/25641935/407691567-f4dcd227-cae9-4a59-aa9e-0eceeb4acaf4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc2OTE1NjctZjRkY2QyMjctY2FlOS00YTU5LWFhOWUtMGVjZWViNGFjYWY0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5ZGQzZDY0NWVlMGRjMmNkMWRhODMxNmQ5NDE5NzA0ZDY4NmU1MzhhNzhlMDllYmI0MjlkNTUzMzBmNjZmM2QmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.2ZJkU5H5jU3wEufZ_0YB2gBwujZZPwC0qVxplMtnbWc" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Link to code</strong>: <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples/streamlit_mcp_rag_agent">examples/streamlit_mcp_rag_agent</a></p>


<p dir="auto"><a href="https://github.com/marimo-team/marimo">Marimo</a> is a reactive Python notebook that replaces Jupyter and Streamlit.
Here&#39;s the &#34;file finder&#34; agent from <a href="#quickstart">Quickstart</a> implemented in Marimo:</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/25641935/407694927-139a95a5-e3ac-4ea7-9c8f-bad6577e8597.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc2OTQ5MjctMTM5YTk1YTUtZTNhYy00ZWE3LTljOGYtYmFkNjU3N2U4NTk3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYzYzQ4ODFhZjg3MjA4N2M3NzUzMGNlODA2Y2E4OGI4M2E3ZTdhNjBjZDhmNjk0ZDFkYzljOGI5M2JjYmM2MDEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.xvrpDLKp852KAYDZvSyLCNCzBole5i1ikvyx1Rb5-7Q"><img src="https://private-user-images.githubusercontent.com/25641935/407694927-139a95a5-e3ac-4ea7-9c8f-bad6577e8597.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc2OTQ5MjctMTM5YTk1YTUtZTNhYy00ZWE3LTljOGYtYmFkNjU3N2U4NTk3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYzYzQ4ODFhZjg3MjA4N2M3NzUzMGNlODA2Y2E4OGI4M2E3ZTdhNjBjZDhmNjk0ZDFkYzljOGI5M2JjYmM2MDEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.xvrpDLKp852KAYDZvSyLCNCzBole5i1ikvyx1Rb5-7Q" width="400"/></a>
<p dir="auto"><strong>Link to code</strong>: <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples/marimo_mcp_basic_agent">examples/marimo_mcp_basic_agent</a></p>


<p dir="auto">You can write mcp-agent apps as Python scripts or Jupyter notebooks.</p>

<p dir="auto">This example demonstrates a multi-agent setup for handling different customer service requests in an airline context using the Swarm workflow pattern. The agents can triage requests, handle flight modifications, cancellations, and lost baggage cases.</p>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description swarm_flight_support.mov">swarm_flight_support.mov</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/25641935/407713050-b314d75d-7945-4de6-965b-7f21eb14a8bd.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc3MTMwNTAtYjMxNGQ3NWQtNzk0NS00ZGU2LTk2NWItN2YyMWViMTRhOGJkLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE2MjcyMGIwMGIwOGVmYjFmOWVlZGE4MzU3YTk0YjQzOWExMGU3MmUxMjA1YWI0YjNkZDUwNjM5YjY2Mzc5MmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.4aHMOIiQzIDZbc8CpP0weHKqmEUge56Eqtd6-9M0Anc" data-canonical-src="https://private-user-images.githubusercontent.com/25641935/407713050-b314d75d-7945-4de6-965b-7f21eb14a8bd.mov?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzgyMDc0MTIsIm5iZiI6MTczODIwNzExMiwicGF0aCI6Ii8yNTY0MTkzNS80MDc3MTMwNTAtYjMxNGQ3NWQtNzk0NS00ZGU2LTk2NWItN2YyMWViMTRhOGJkLm1vdj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTMwVDAzMTgzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE2MjcyMGIwMGIwOGVmYjFmOWVlZGE4MzU3YTk0YjQzOWExMGU3MmUxMjA1YWI0YjNkZDUwNjM5YjY2Mzc5MmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.4aHMOIiQzIDZbc8CpP0weHKqmEUge56Eqtd6-9M0Anc" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><strong>Link to code</strong>: <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples/workflow_swarm">examples/workflow_swarm</a></p>

<p dir="auto">The following are the building blocks of the mcp-agent framework:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/app.py">MCPApp</a></strong>: global state and app configuration</li>
<li><strong>MCP server management</strong>: <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/mcp/gen_client.py"><code>gen_client</code></a> and <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/mcp/mcp_connection_manager.py"><code>MCPConnectionManager</code></a> to easily connect to MCP servers.</li>
<li><strong><a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/agents/agent.py">Agent</a></strong>: An Agent is an entity that has access to a set of MCP servers and exposes them to an LLM as tool calls. It has a name and purpose (instruction).</li>
<li><strong><a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/workflows/llm/augmented_llm.py">AugmentedLLM</a></strong>: An LLM that is enhanced with tools provided from a collection of MCP servers. Every Workflow pattern described below is an <code>AugmentedLLM</code> itself, allowing you to compose and chain them together.</li>
</ul>
<p dir="auto">Everything in the framework is a derivative of these core capabilities.</p>

<p dir="auto">mcp-agent provides implementations for every pattern in Anthropic’s <a href="https://www.anthropic.com/research/building-effective-agents" rel="nofollow">Building Effective Agents</a>, as well as the OpenAI <a href="https://github.com/openai/swarm">Swarm</a> pattern.
Each pattern is model-agnostic, and exposed as an <code>AugmentedLLM</code>, making everything very composable.</p>

<p dir="auto"><a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/workflows/llm/augmented_llm.py">AugmentedLLM</a> is an LLM that has access to MCP servers and functions via Agents.</p>
<p dir="auto">LLM providers implement the AugmentedLLM interface to expose 3 functions:</p>
<ul dir="auto">
<li><code>generate</code>: Generate message(s) given a prompt, possibly over multiple iterations and making tool calls as needed.</li>
<li><code>generate_str</code>: Calls <code>generate</code> and returns result as a string output.</li>
<li><code>generate_structured</code>: Uses <a href="https://github.com/instructor-ai/instructor">Instructor</a> to return the generated result as a Pydantic model.</li>
</ul>
<p dir="auto">Additionally, <code>AugmentedLLM</code> has memory, to keep track of long or short-term history.</p>
<details>
<summary>Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM

finder_agent = Agent(
    name=&#34;finder&#34;,
    instruction=&#34;You are an agent with filesystem + fetch access. Return the requested file or URL contents.&#34;,
    server_names=[&#34;fetch&#34;, &#34;filesystem&#34;],
)

async with finder_agent:
   llm = await finder_agent.attach_llm(AnthropicAugmentedLLM)

   result = await llm.generate_str(
      message=&#34;Print the first 2 paragraphs of https://www.anthropic.com/research/building-effective-agents&#34;,
      # Can override model, tokens and other defaults
   )
   logger.info(f&#34;Result: {result}&#34;)

   # Multi-turn conversation
   result = await llm.generate_str(
      message=&#34;Summarize those paragraphs in a 128 character tweet&#34;,
   )
   logger.info(f&#34;Result: {result}&#34;)"><pre><span>from</span> <span>mcp_agent</span>.<span>agents</span>.<span>agent</span> <span>import</span> <span>Agent</span>
<span>from</span> <span>mcp_agent</span>.<span>workflows</span>.<span>llm</span>.<span>augmented_llm_anthropic</span> <span>import</span> <span>AnthropicAugmentedLLM</span>

<span>finder_agent</span> <span>=</span> <span>Agent</span>(
    <span>name</span><span>=</span><span>&#34;finder&#34;</span>,
    <span>instruction</span><span>=</span><span>&#34;You are an agent with filesystem + fetch access. Return the requested file or URL contents.&#34;</span>,
    <span>server_names</span><span>=</span>[<span>&#34;fetch&#34;</span>, <span>&#34;filesystem&#34;</span>],
)

<span>async</span> <span>with</span> <span>finder_agent</span>:
   <span>llm</span> <span>=</span> <span>await</span> <span>finder_agent</span>.<span>attach_llm</span>(<span>AnthropicAugmentedLLM</span>)

   <span>result</span> <span>=</span> <span>await</span> <span>llm</span>.<span>generate_str</span>(
      <span>message</span><span>=</span><span>&#34;Print the first 2 paragraphs of https://www.anthropic.com/research/building-effective-agents&#34;</span>,
      <span># Can override model, tokens and other defaults</span>
   )
   <span>logger</span>.<span>info</span>(<span>f&#34;Result: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)

   <span># Multi-turn conversation</span>
   <span>result</span> <span>=</span> <span>await</span> <span>llm</span>.<span>generate_str</span>(
      <span>message</span><span>=</span><span>&#34;Summarize those paragraphs in a 128 character tweet&#34;</span>,
   )
   <span>logger</span>.<span>info</span>(<span>f&#34;Result: <span><span>{</span><span>result</span><span>}</span></span>&#34;</span>)</pre></div>
</details>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c0614dadbbcee40b9cb94111644400106384ee6254e83275cd11091824197c1f/68747470733a2f2f7777772e616e7468726f7069632e636f6d2f5f6e6578742f696d6167653f75726c3d68747470732533412532462532467777772d63646e2e616e7468726f7069632e636f6d253246696d61676573253246347a727a6f76626225324677656273697465253246343036626230333263613030376664313632346632363161663731376437306536636138363238362d3234303178313030302e706e6726773d3338343026713d3735"><img src="https://camo.githubusercontent.com/c0614dadbbcee40b9cb94111644400106384ee6254e83275cd11091824197c1f/68747470733a2f2f7777772e616e7468726f7069632e636f6d2f5f6e6578742f696d6167653f75726c3d68747470732533412532462532467777772d63646e2e616e7468726f7069632e636f6d253246696d61676573253246347a727a6f76626225324677656273697465253246343036626230333263613030376664313632346632363161663731376437306536636138363238362d3234303178313030302e706e6726773d3338343026713d3735" alt="Parallel workflow (Image credit: Anthropic)" data-canonical-src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&amp;w=3840&amp;q=75"/></a></p>
<p dir="auto">Fan-out tasks to multiple sub-agents and fan-in the results. Each subtask is an AugmentedLLM, as is the overall Parallel workflow, meaning each subtask can optionally be a more complex workflow itself.</p>

<details>
<summary>Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="proofreader = Agent(name=&#34;proofreader&#34;, instruction=&#34;Review grammar...&#34;)
fact_checker = Agent(name=&#34;fact_checker&#34;, instruction=&#34;Check factual consistency...&#34;)
style_enforcer = Agent(name=&#34;style_enforcer&#34;, instruction=&#34;Enforce style guidelines...&#34;)

grader = Agent(name=&#34;grader&#34;, instruction=&#34;Combine feedback into a structured report.&#34;)

parallel = ParallelLLM(
    fan_in_agent=grader,
    fan_out_agents=[proofreader, fact_checker, style_enforcer],
    llm_factory=OpenAIAugmentedLLM,
)

result = await parallel.generate_str(&#34;Student short story submission: ...&#34;, RequestParams(model=&#34;gpt4-o&#34;))"><pre><span>proofreader</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;proofreader&#34;</span>, <span>instruction</span><span>=</span><span>&#34;Review grammar...&#34;</span>)
<span>fact_checker</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;fact_checker&#34;</span>, <span>instruction</span><span>=</span><span>&#34;Check factual consistency...&#34;</span>)
<span>style_enforcer</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;style_enforcer&#34;</span>, <span>instruction</span><span>=</span><span>&#34;Enforce style guidelines...&#34;</span>)

<span>grader</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;grader&#34;</span>, <span>instruction</span><span>=</span><span>&#34;Combine feedback into a structured report.&#34;</span>)

<span>parallel</span> <span>=</span> <span>ParallelLLM</span>(
    <span>fan_in_agent</span><span>=</span><span>grader</span>,
    <span>fan_out_agents</span><span>=</span>[<span>proofreader</span>, <span>fact_checker</span>, <span>style_enforcer</span>],
    <span>llm_factory</span><span>=</span><span>OpenAIAugmentedLLM</span>,
)

<span>result</span> <span>=</span> <span>await</span> <span>parallel</span>.<span>generate_str</span>(<span>&#34;Student short story submission: ...&#34;</span>, <span>RequestParams</span>(<span>model</span><span>=</span><span>&#34;gpt4-o&#34;</span>))</pre></div>
</details>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f24087710985a7d76000aab04f083eefcc7460fb965c9f2bac6ca0783329c5c3/68747470733a2f2f7777772e616e7468726f7069632e636f6d2f5f6e6578742f696d6167653f75726c3d68747470732533412532462532467777772d63646e2e616e7468726f7069632e636f6d253246696d61676573253246347a727a6f76626225324677656273697465253246356330633065396665346465663062353834633034643337383439393431646135356535653731632d3234303178313030302e706e6726773d3338343026713d3735"><img src="https://camo.githubusercontent.com/f24087710985a7d76000aab04f083eefcc7460fb965c9f2bac6ca0783329c5c3/68747470733a2f2f7777772e616e7468726f7069632e636f6d2f5f6e6578742f696d6167653f75726c3d68747470732533412532462532467777772d63646e2e616e7468726f7069632e636f6d253246696d61676573253246347a727a6f76626225324677656273697465253246356330633065396665346465663062353834633034643337383439393431646135356535653731632d3234303178313030302e706e6726773d3338343026713d3735" alt="Router workflow (Image credit: Anthropic)" data-canonical-src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&amp;w=3840&amp;q=75"/></a></p>
<p dir="auto">Given an input, route to the <code>top_k</code> most relevant categories. A category can be an Agent, an MCP server or a regular function.</p>
<p dir="auto">mcp-agent provides several router implementations, including:</p>
<ul dir="auto">
<li><a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/workflows/router/router_embedding.py"><code>EmbeddingRouter</code></a>: uses embedding models for classification</li>
<li><a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/workflows/router/router_llm.py"><code>LLMRouter</code></a>: uses LLMs for classification</li>
</ul>

<details>
<summary>Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="def print_hello_world:
     print(&#34;Hello, world!&#34;)

finder_agent = Agent(name=&#34;finder&#34;, server_names=[&#34;fetch&#34;, &#34;filesystem&#34;])
writer_agent = Agent(name=&#34;writer&#34;, server_names=[&#34;filesystem&#34;])

llm = OpenAIAugmentedLLM()
router = LLMRouter(
    llm=llm,
    agents=[finder_agent, writer_agent],
    functions=[print_hello_world],
)

results = await router.route( # Also available: route_to_agent, route_to_server
    request=&#34;Find and print the contents of README.md verbatim&#34;,
    top_k=1
)
chosen_agent = results[0].result
async with chosen_agent:
    ..."><pre><span>def</span> <span>print_hello_world</span>:
     <span>print</span>(<span>&#34;Hello, world!&#34;</span>)

<span>finder_agent</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;finder&#34;</span>, <span>server_names</span><span>=</span>[<span>&#34;fetch&#34;</span>, <span>&#34;filesystem&#34;</span>])
<span>writer_agent</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;writer&#34;</span>, <span>server_names</span><span>=</span>[<span>&#34;filesystem&#34;</span>])

<span>llm</span> <span>=</span> <span>OpenAIAugmentedLLM</span>()
<span>router</span> <span>=</span> <span>LLMRouter</span>(
    <span>llm</span><span>=</span><span>llm</span>,
    <span>agents</span><span>=</span>[<span>finder_agent</span>, <span>writer_agent</span>],
    <span>functions</span><span>=</span>[<span>print_hello_world</span>],
)

<span>results</span> <span>=</span> <span>await</span> <span>router</span>.<span>route</span>( <span># Also available: route_to_agent, route_to_server</span>
    <span>request</span><span>=</span><span>&#34;Find and print the contents of README.md verbatim&#34;</span>,
    <span>top_k</span><span>=</span><span>1</span>
)
<span>chosen_agent</span> <span>=</span> <span>results</span>[<span>0</span>].<span>result</span>
<span>async</span> <span>with</span> <span>chosen_agent</span>:
    ...</pre></div>
</details>

<p dir="auto">A close sibling of Router, the Intent Classifier pattern identifies the <code>top_k</code> Intents that most closely match a given input.
Just like a Router, mcp-agent provides both an <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/workflows/intent_classifier/intent_classifier_embedding.py">embedding</a> and <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/workflows/intent_classifier/intent_classifier_llm.py">LLM-based</a> intent classifier.</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/641fd9ec67275236720f633fd65d058b99593c9bdab7c5fff24c8a764da3703b/68747470733a2f2f7777772e616e7468726f7069632e636f6d2f5f6e6578742f696d6167653f75726c3d68747470732533412532462532467777772d63646e2e616e7468726f7069632e636f6d253246696d61676573253246347a727a6f76626225324677656273697465253246313466353165363430366363623239653639356461343862313730313765383939613631313963372d3234303178313030302e706e6726773d3338343026713d3735"><img src="https://camo.githubusercontent.com/641fd9ec67275236720f633fd65d058b99593c9bdab7c5fff24c8a764da3703b/68747470733a2f2f7777772e616e7468726f7069632e636f6d2f5f6e6578742f696d6167653f75726c3d68747470732533412532462532467777772d63646e2e616e7468726f7069632e636f6d253246696d61676573253246347a727a6f76626225324677656273697465253246313466353165363430366363623239653639356461343862313730313765383939613631313963372d3234303178313030302e706e6726773d3338343026713d3735" alt="Evaluator-optimizer workflow (Image credit: Anthropic)" data-canonical-src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&amp;w=3840&amp;q=75"/></a></p>
<p dir="auto">One LLM (the “optimizer”) refines a response, another (the “evaluator”) critiques it until a response exceeds a quality criteria.</p>

<details>
<summary>Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="optimizer = Agent(name=&#34;cover_letter_writer&#34;, server_names=[&#34;fetch&#34;], instruction=&#34;Generate a cover letter ...&#34;)
evaluator = Agent(name=&#34;critiquer&#34;, instruction=&#34;Evaluate clarity, specificity, relevance...&#34;)

llm = EvaluatorOptimizerLLM(
    optimizer=optimizer,
    evaluator=evaluator,
    llm_factory=OpenAIAugmentedLLM,
    min_rating=QualityRating.EXCELLENT, # Keep iterating until the minimum quality bar is reached
)

result = await eo_llm.generate_str(&#34;Write a job cover letter for an AI framework developer role at LastMile AI.&#34;)
print(&#34;Final refined cover letter:&#34;, result)"><pre><span>optimizer</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;cover_letter_writer&#34;</span>, <span>server_names</span><span>=</span>[<span>&#34;fetch&#34;</span>], <span>instruction</span><span>=</span><span>&#34;Generate a cover letter ...&#34;</span>)
<span>evaluator</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;critiquer&#34;</span>, <span>instruction</span><span>=</span><span>&#34;Evaluate clarity, specificity, relevance...&#34;</span>)

<span>llm</span> <span>=</span> <span>EvaluatorOptimizerLLM</span>(
    <span>optimizer</span><span>=</span><span>optimizer</span>,
    <span>evaluator</span><span>=</span><span>evaluator</span>,
    <span>llm_factory</span><span>=</span><span>OpenAIAugmentedLLM</span>,
    <span>min_rating</span><span>=</span><span>QualityRating</span>.<span>EXCELLENT</span>, <span># Keep iterating until the minimum quality bar is reached</span>
)

<span>result</span> <span>=</span> <span>await</span> <span>eo_llm</span>.<span>generate_str</span>(<span>&#34;Write a job cover letter for an AI framework developer role at LastMile AI.&#34;</span>)
<span>print</span>(<span>&#34;Final refined cover letter:&#34;</span>, <span>result</span>)</pre></div>
</details>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/953d1b88a05e93e1707bc955924cff7742b408515af0df34c4790c2a27f60131/68747470733a2f2f7777772e616e7468726f7069632e636f6d2f5f6e6578742f696d6167653f75726c3d68747470732533412532462532467777772d63646e2e616e7468726f7069632e636f6d253246696d61676573253246347a727a6f76626225324677656273697465253246383938356663363833666165343738306662333465616231333635616237386337653531626338652d3234303178313030302e706e6726773d3338343026713d3735"><img src="https://camo.githubusercontent.com/953d1b88a05e93e1707bc955924cff7742b408515af0df34c4790c2a27f60131/68747470733a2f2f7777772e616e7468726f7069632e636f6d2f5f6e6578742f696d6167653f75726c3d68747470732533412532462532467777772d63646e2e616e7468726f7069632e636f6d253246696d61676573253246347a727a6f76626225324677656273697465253246383938356663363833666165343738306662333465616231333635616237386337653531626338652d3234303178313030302e706e6726773d3338343026713d3735" alt="Orchestrator workflow (Image credit: Anthropic)" data-canonical-src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&amp;w=3840&amp;q=75"/></a></p>
<p dir="auto">A higher-level LLM generates a plan, then assigns them to sub-agents, and synthesizes the results.
The Orchestrator workflow automatically parallelizes steps that can be done in parallel, and blocks on dependencies.</p>

<details>
<summary>Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="finder_agent = Agent(name=&#34;finder&#34;, server_names=[&#34;fetch&#34;, &#34;filesystem&#34;])
writer_agent = Agent(name=&#34;writer&#34;, server_names=[&#34;filesystem&#34;])
proofreader = Agent(name=&#34;proofreader&#34;, ...)
fact_checker = Agent(name=&#34;fact_checker&#34;, ...)
style_enforcer = Agent(name=&#34;style_enforcer&#34;, instructions=&#34;Use APA style guide from ...&#34;, server_names=[&#34;fetch&#34;])

orchestrator = Orchestrator(
    llm_factory=AnthropicAugmentedLLM,
    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],
)

task = &#34;Load short_story.md, evaluate it, produce a graded_report.md with multiple feedback aspects.&#34;
result = await orchestrator.generate_str(task, RequestParams(model=&#34;gpt-4o&#34;))
print(result)"><pre><span>finder_agent</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;finder&#34;</span>, <span>server_names</span><span>=</span>[<span>&#34;fetch&#34;</span>, <span>&#34;filesystem&#34;</span>])
<span>writer_agent</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;writer&#34;</span>, <span>server_names</span><span>=</span>[<span>&#34;filesystem&#34;</span>])
<span>proofreader</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;proofreader&#34;</span>, ...)
<span>fact_checker</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;fact_checker&#34;</span>, ...)
<span>style_enforcer</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;style_enforcer&#34;</span>, <span>instructions</span><span>=</span><span>&#34;Use APA style guide from ...&#34;</span>, <span>server_names</span><span>=</span>[<span>&#34;fetch&#34;</span>])

<span>orchestrator</span> <span>=</span> <span>Orchestrator</span>(
    <span>llm_factory</span><span>=</span><span>AnthropicAugmentedLLM</span>,
    <span>available_agents</span><span>=</span>[<span>finder_agent</span>, <span>writer_agent</span>, <span>proofreader</span>, <span>fact_checker</span>, <span>style_enforcer</span>],
)

<span>task</span> <span>=</span> <span>&#34;Load short_story.md, evaluate it, produce a graded_report.md with multiple feedback aspects.&#34;</span>
<span>result</span> <span>=</span> <span>await</span> <span>orchestrator</span>.<span>generate_str</span>(<span>task</span>, <span>RequestParams</span>(<span>model</span><span>=</span><span>&#34;gpt-4o&#34;</span>))
<span>print</span>(<span>result</span>)</pre></div>
</details>

<p dir="auto">OpenAI has an experimental multi-agent pattern called <a href="https://github.com/openai/swarm">Swarm</a>, which we provide a model-agnostic reference implementation for in mcp-agent.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/openai/swarm/blob/main/assets/swarm_diagram.png?raw=true"><img src="https://github.com/openai/swarm/raw/main/assets/swarm_diagram.png?raw=true" width="500"/></a></p>
<p dir="auto">The mcp-agent Swarm pattern works seamlessly with MCP servers, and is exposed as an <code>AugmentedLLM</code>, allowing for composability with other patterns above.</p>

<details>
<summary>Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="triage_agent = SwarmAgent(...)
flight_mod_agent = SwarmAgent(...)
lost_baggage_agent = SwarmAgent(...)

# The triage agent decides whether to route to flight_mod_agent or lost_baggage_agent
swarm = AnthropicSwarm(agent=triage_agent, context_variables={...})

test_input = &#34;My bag was not delivered!&#34;
result = await swarm.generate_str(test_input)
print(&#34;Result:&#34;, result)"><pre><span>triage_agent</span> <span>=</span> <span>SwarmAgent</span>(...)
<span>flight_mod_agent</span> <span>=</span> <span>SwarmAgent</span>(...)
<span>lost_baggage_agent</span> <span>=</span> <span>SwarmAgent</span>(...)

<span># The triage agent decides whether to route to flight_mod_agent or lost_baggage_agent</span>
<span>swarm</span> <span>=</span> <span>AnthropicSwarm</span>(<span>agent</span><span>=</span><span>triage_agent</span>, <span>context_variables</span><span>=</span>{...})

<span>test_input</span> <span>=</span> <span>&#34;My bag was not delivered!&#34;</span>
<span>result</span> <span>=</span> <span>await</span> <span>swarm</span>.<span>generate_str</span>(<span>test_input</span>)
<span>print</span>(<span>&#34;Result:&#34;</span>, <span>result</span>)</pre></div>
</details>


<p dir="auto">An example of composability is using an <a href="#evaluator-optimizer">Evaluator-Optimizer</a> workflow as the planner LLM inside
the <a href="#orchestrator-workers">Orchestrator</a> workflow. Generating a high-quality plan to execute is important for robust behavior, and an evaluator-optimizer can help ensure that.</p>
<p dir="auto">Doing so is seamless in mcp-agent, because each workflow is implemented as an <code>AugmentedLLM</code>.</p>
<details>
<summary>Example</summary>
<div dir="auto" data-snippet-clipboard-copy-content="optimizer = Agent(name=&#34;plan_optimizer&#34;, server_names=[...], instruction=&#34;Generate a plan given an objective ...&#34;)
evaluator = Agent(name=&#34;plan_evaluator&#34;, instruction=&#34;Evaluate logic, ordering and precision of plan......&#34;)

planner_llm = EvaluatorOptimizerLLM(
    optimizer=optimizer,
    evaluator=evaluator,
    llm_factory=OpenAIAugmentedLLM,
    min_rating=QualityRating.EXCELLENT,
)

orchestrator = Orchestrator(
    llm_factory=AnthropicAugmentedLLM,
    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],
    planner=planner_llm # It&#39;s that simple
)

..."><pre><span>optimizer</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;plan_optimizer&#34;</span>, <span>server_names</span><span>=</span>[...], <span>instruction</span><span>=</span><span>&#34;Generate a plan given an objective ...&#34;</span>)
<span>evaluator</span> <span>=</span> <span>Agent</span>(<span>name</span><span>=</span><span>&#34;plan_evaluator&#34;</span>, <span>instruction</span><span>=</span><span>&#34;Evaluate logic, ordering and precision of plan......&#34;</span>)

<span>planner_llm</span> <span>=</span> <span>EvaluatorOptimizerLLM</span>(
    <span>optimizer</span><span>=</span><span>optimizer</span>,
    <span>evaluator</span><span>=</span><span>evaluator</span>,
    <span>llm_factory</span><span>=</span><span>OpenAIAugmentedLLM</span>,
    <span>min_rating</span><span>=</span><span>QualityRating</span>.<span>EXCELLENT</span>,
)

<span>orchestrator</span> <span>=</span> <span>Orchestrator</span>(
    <span>llm_factory</span><span>=</span><span>AnthropicAugmentedLLM</span>,
    <span>available_agents</span><span>=</span>[<span>finder_agent</span>, <span>writer_agent</span>, <span>proofreader</span>, <span>fact_checker</span>, <span>style_enforcer</span>],
    <span>planner</span><span>=</span><span>planner_llm</span> <span># It&#39;s that simple</span>
)

...</pre></div>
</details>
<div dir="auto"><h3 tabindex="-1" dir="auto">Signaling and Human Input</h3><a id="user-content-signaling-and-human-input" aria-label="Permalink: Signaling and Human Input" href="#signaling-and-human-input"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><strong>Signaling</strong>: The framework can pause/resume tasks. The agent or LLM might “signal” that it needs user input, so the workflow awaits. A developer may signal during a workflow to seek approval or review before continuing with a workflow.</p>
<p dir="auto"><strong>Human Input</strong>: If an Agent has a <code>human_input_callback</code>, the LLM can call a <code>__human_input__</code> tool to request user input mid-workflow.</p>
<details>
<summary>Example</summary>
<p dir="auto">The <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples/workflow_swarm/main.py">Swarm example</a> shows this in action.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from mcp_agent.human_input.handler import console_input_callback

lost_baggage = SwarmAgent(
    name=&#34;Lost baggage traversal&#34;,
    instruction=lambda context_variables: f&#34;&#34;&#34;
        {
        FLY_AIR_AGENT_PROMPT.format(
            customer_context=context_variables.get(&#34;customer_context&#34;, &#34;None&#34;),
            flight_context=context_variables.get(&#34;flight_context&#34;, &#34;None&#34;),
        )
    }\n Lost baggage policy: policies/lost_baggage_policy.md&#34;&#34;&#34;,
    functions=[
        escalate_to_agent,
        initiate_baggage_search,
        transfer_to_triage,
        case_resolved,
    ],
    server_names=[&#34;fetch&#34;, &#34;filesystem&#34;],
    human_input_callback=console_input_callback, # Request input from the console
)"><pre><span>from</span> <span>mcp_agent</span>.<span>human_input</span>.<span>handler</span> <span>import</span> <span>console_input_callback</span>

<span>lost_baggage</span> <span>=</span> <span>SwarmAgent</span>(
    <span>name</span><span>=</span><span>&#34;Lost baggage traversal&#34;</span>,
    <span>instruction</span><span>=</span><span>lambda</span> <span>context_variables</span>: <span>f&#34;&#34;&#34;</span>
<span>        <span><span>{</span></span></span>
<span><span>        <span>FLY_AIR_AGENT_PROMPT</span>.<span>format</span>(</span></span>
<span><span>            <span>customer_context</span><span>=</span><span>context_variables</span>.<span>get</span>(<span>&#34;customer_context&#34;</span>, <span>&#34;None&#34;</span>),</span></span>
<span><span>            <span>flight_context</span><span>=</span><span>context_variables</span>.<span>get</span>(<span>&#34;flight_context&#34;</span>, <span>&#34;None&#34;</span>),</span></span>
<span><span>        )</span></span>
<span><span>    <span>}</span></span><span>\n</span> Lost baggage policy: policies/lost_baggage_policy.md&#34;&#34;&#34;</span>,
    <span>functions</span><span>=</span>[
        <span>escalate_to_agent</span>,
        <span>initiate_baggage_search</span>,
        <span>transfer_to_triage</span>,
        <span>case_resolved</span>,
    ],
    <span>server_names</span><span>=</span>[<span>&#34;fetch&#34;</span>, <span>&#34;filesystem&#34;</span>],
    <span>human_input_callback</span><span>=</span><span>console_input_callback</span>, <span># Request input from the console</span>
)</pre></div>
</details>

<p dir="auto">Create an <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/schema/mcp-agent.config.schema.json"><code>mcp_agent.config.yaml</code></a> and a gitignored <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_basic_agent/mcp_agent.secrets.yaml.example"><code>mcp_agent.secrets.yaml</code></a> to define MCP app configuration. This controls logging, execution, LLM provider APIs, and MCP server configuration:</p>

<p dir="auto">mcp-agent makes it trivial to connect to MCP servers. Create an <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/schema/mcp-agent.config.schema.json"><code>mcp_agent.config.yaml</code></a> to define server configuration under the <code>mcp</code> section:</p>
<div dir="auto" data-snippet-clipboard-copy-content="mcp:
  servers:
    fetch:
      command: &#34;uvx&#34;
      args: [&#34;mcp-server-fetch&#34;]
      description: &#34;Fetch content at URLs from the world wide web&#34;"><pre><span>mcp</span>:
  <span>servers</span>:
    <span>fetch</span>:
      <span>command</span>: <span><span>&#34;</span>uvx<span>&#34;</span></span>
      <span>args</span>: <span>[&#34;mcp-server-fetch&#34;]</span>
      <span>description</span>: <span><span>&#34;</span>Fetch content at URLs from the world wide web<span>&#34;</span></span></pre></div>

<p dir="auto">Manage the lifecycle of an MCP server within an async context manager:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from mcp_agent.mcp.gen_client import gen_client

async with gen_client(&#34;fetch&#34;) as fetch_client:
    # Fetch server is initialized and ready to use
    result = await fetch_client.list_tools()

# Fetch server is automatically disconnected/shutdown"><pre><span>from</span> <span>mcp_agent</span>.<span>mcp</span>.<span>gen_client</span> <span>import</span> <span>gen_client</span>

<span>async</span> <span>with</span> <span>gen_client</span>(<span>&#34;fetch&#34;</span>) <span>as</span> <span>fetch_client</span>:
    <span># Fetch server is initialized and ready to use</span>
    <span>result</span> <span>=</span> <span>await</span> <span>fetch_client</span>.<span>list_tools</span>()

<span># Fetch server is automatically disconnected/shutdown</span></pre></div>
<p dir="auto">The gen_client function makes it easy to spin up connections to MCP servers.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Persistent server connections</h3><a id="user-content-persistent-server-connections" aria-label="Permalink: Persistent server connections" href="#persistent-server-connections"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">In many cases, you want an MCP server to stay online for persistent use (e.g. in a multi-step tool use workflow).
For persistent connections, use:</p>
<ul dir="auto">
<li><a href="https://github.com/lastmile-ai/mcp-agent/blob/main/(src/mcp_agent/mcp/gen_client.py)"><code>connect</code></a> and <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/mcp/gen_client.py"><code>disconnect</code></a></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="from mcp_agent.mcp.gen_client import connect, disconnect

fetch_client = None
try:
     fetch_client = connect(&#34;fetch&#34;)
     result = await fetch_client.list_tools()
finally:
     disconnect(&#34;fetch&#34;)"><pre><span>from</span> <span>mcp_agent</span>.<span>mcp</span>.<span>gen_client</span> <span>import</span> <span>connect</span>, <span>disconnect</span>

<span>fetch_client</span> <span>=</span> <span>None</span>
<span>try</span>:
     <span>fetch_client</span> <span>=</span> <span>connect</span>(<span>&#34;fetch&#34;</span>)
     <span>result</span> <span>=</span> <span>await</span> <span>fetch_client</span>.<span>list_tools</span>()
<span>finally</span>:
     <span>disconnect</span>(<span>&#34;fetch&#34;</span>)</pre></div>
<ul dir="auto">
<li><a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/mcp/mcp_connection_manager.py"><code>MCPConnectionManager</code></a>
For even more fine-grained control over server connections, you can use the MCPConnectionManager.</li>
</ul>
<details>
<summary>Example</summary>
```python
from mcp_agent.context import get_current_context
from mcp_agent.mcp.mcp_connection_manager import MCPConnectionManager
<p dir="auto">context = get_current_context()
connection_manager = MCPConnectionManager(context.server_registry)</p>
<p dir="auto">async with connection_manager:
fetch_client = await connection_manager.get_server(&#34;fetch&#34;) # Initializes fetch server
result = fetch_client.list_tool()
fetch_client2 = await connection_manager.get_server(&#34;fetch&#34;) # Reuses same server connection</p>

<div data-snippet-clipboard-copy-content="
&lt;/details&gt;

### MCP Server Aggregator

[`MCPAggregator`](src/mcp_agent/mcp/mcp_aggregator.py) acts as a &#34;server-of-servers&#34;.
It provides a single MCP server interface for interacting with multiple MCP servers.
This allows you to expose tools from multiple servers to LLM applications.

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
from mcp_agent.mcp.mcp_aggregator import MCPAggregator

aggregator = await MCPAggregator.create(server_names=[&#34;fetch&#34;, &#34;filesystem&#34;])

async with aggregator:
   # combined list of tools exposed by &#39;fetch&#39; and &#39;filesystem&#39; servers
   tools = await aggregator.list_tools()

   # namespacing -- invokes the &#39;fetch&#39; server to call the &#39;fetch&#39; tool
   fetch_result = await aggregator.call_tool(name=&#34;fetch-fetch&#34;, arguments={&#34;url&#34;: &#34;https://www.anthropic.com/research/building-effective-agents&#34;})

   # no namespacing -- first server in the aggregator exposing that tool wins
   read_file_result = await aggregator.call_tool(name=&#34;read_file&#34;, arguments={})"><pre><code>
&lt;/details&gt;

### MCP Server Aggregator

[`MCPAggregator`](src/mcp_agent/mcp/mcp_aggregator.py) acts as a &#34;server-of-servers&#34;.
It provides a single MCP server interface for interacting with multiple MCP servers.
This allows you to expose tools from multiple servers to LLM applications.

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
from mcp_agent.mcp.mcp_aggregator import MCPAggregator

aggregator = await MCPAggregator.create(server_names=[&#34;fetch&#34;, &#34;filesystem&#34;])

async with aggregator:
   # combined list of tools exposed by &#39;fetch&#39; and &#39;filesystem&#39; servers
   tools = await aggregator.list_tools()

   # namespacing -- invokes the &#39;fetch&#39; server to call the &#39;fetch&#39; tool
   fetch_result = await aggregator.call_tool(name=&#34;fetch-fetch&#34;, arguments={&#34;url&#34;: &#34;https://www.anthropic.com/research/building-effective-agents&#34;})

   # no namespacing -- first server in the aggregator exposing that tool wins
   read_file_result = await aggregator.call_tool(name=&#34;read_file&#34;, arguments={})
</code></pre></div>
</details>

<p dir="auto">We welcome any and all kinds of contributions. Please see the <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/CONTRIBUTING.md">CONTRIBUTING guidelines</a> to get started.</p>

<p dir="auto">There have already been incredible community contributors who are driving this project forward:</p>
<ul dir="auto">
<li><a href="https://github.com/StreetLamb">Jerron Lim (@StreetLamb)</a> -- who has contributed countless hours and excellent examples, and great ideas to the project.</li>
<li><a href="https://github.com/jasonsum">Jason Summer (@jasonsum)</a> -- for identifying several issues and adapting his Gmail MCP server to work with mcp-agent</li>
</ul>

<p dir="auto">We will be adding a detailed roadmap (ideally driven by your feedback). The current set of priorities include:</p>
<ul dir="auto">
<li><strong>Durable Execution</strong> -- allow workflows to pause/resume and serialize state so they can be replayed or be paused indefinitely. We are working on integrating <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/src/mcp_agent/executor/temporal.py">Temporal</a> for this purpose.</li>
<li><strong>Memory</strong> -- adding support for long-term memory</li>
<li><strong>Streaming</strong> -- Support streaming listeners for iterative progress</li>
<li><strong>Additional MCP capabilities</strong> -- Expand beyond tool calls to support:
<ul dir="auto">
<li>Resources</li>
<li>Prompts</li>
<li>Notifications</li>
</ul>
</li>
</ul>

<div dir="auto"><h3 tabindex="-1" dir="auto">What are the core benefits of using mcp-agent?</h3><a id="user-content-what-are-the-core-benefits-of-using-mcp-agent" aria-label="Permalink: What are the core benefits of using mcp-agent?" href="#what-are-the-core-benefits-of-using-mcp-agent"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">mcp-agent provides a streamlined approach to building AI agents using capabilities exposed by <strong>MCP</strong> (Model Context Protocol) servers.</p>
<p dir="auto">MCP is quite low-level, and this framework handles the mechanics of connecting to servers, working with LLMs, handling external signals (like human input) and supporting persistent state via durable execution. That lets you, the developer, focus on the core business logic of your AI application.</p>
<p dir="auto">Core benefits:</p>
<ul dir="auto">
<li>🤝 <strong>Interoperability</strong>: ensures that any tool exposed by any number of MCP servers can seamlessly plug in to your agents.</li>
<li>⛓️ <strong>Composability &amp; Cutstomizability</strong>: Implements well-defined workflows, but in a composable way that enables compound workflows, and allows full customization across model provider, logging, orchestrator, etc.</li>
<li>💻 <strong>Programmatic control flow</strong>: Keeps things simple as developers just write code instead of thinking in graphs, nodes and edges. For branching logic, you write <code>if</code> statements. For cycles, use <code>while</code> loops.</li>
<li>🖐️ <strong>Human Input &amp; Signals</strong>: Supports pausing workflows for external signals, such as human input, which are exposed as tool calls an Agent can make.</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Do you need an MCP client to use mcp-agent?</h3><a id="user-content-do-you-need-an-mcp-client-to-use-mcp-agent" aria-label="Permalink: Do you need an MCP client to use mcp-agent?" href="#do-you-need-an-mcp-client-to-use-mcp-agent"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">No, you can use mcp-agent anywhere, since it handles MCPClient creation for you. This allows you to leverage MCP servers outside of MCP hosts like Claude Desktop.</p>
<p dir="auto">Here&#39;s all the ways you can set up your mcp-agent application:</p>

<p dir="auto">You can expose mcp-agent applications as MCP servers themselves (see <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples/mcp_agent_server">example</a>), allowing MCP clients to interface with sophisticated AI workflows using the standard tools API of MCP servers. This is effectively a server-of-servers.</p>

<p dir="auto">You can embed mcp-agent in an MCP client directly to manage the orchestration across multiple MCP servers.</p>

<p dir="auto">You can use mcp-agent applications in a standalone fashion (i.e. they aren&#39;t part of an MCP client). The <a href="https://github.com/lastmile-ai/mcp-agent/blob/main/examples"><code>examples</code></a> are all standalone applications.</p>

<p dir="auto">I debated naming this project <em>silsila</em> (سلسلہ), which means chain of events in Urdu. mcp-agent is more matter-of-fact, but there&#39;s still an easter egg in the project paying homage to silsila.</p>
</article></div></div>
  </body>
</html>
