<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thephd.dev/the-cost-of-a-closure-in-c-c2y-followup">Original</a>
    <h1>The Cost of a Closure in C: The Rest</h1>
    
    <div id="readability-page-1" class="page"><article>
    <header id="main">
        
    </header>

    <section>
    
            <p>The last article checked the landscape of various C and C extension implementations of Closures for their performance capabilities. But, there were a few tweaks and extra things we could do to check the performance of other techniques. At the time,<!--more--> we ignored such techniques because they were so common, but it helps to quantify their performance relative to everything else, so we re-ran the benchmarks with a few new categories!</p>



<p>If you want an introduction to what is going on, there‚Äôs a gentle description with some 10,000 foot overview in <a href="https://thephd.dev/the-cost-of-a-closure-in-c-c2y">the previous article</a>. Additionally, if you‚Äôd like to learn more about specific kinds of Closures as they exist in C and/or C++, you can read <a href="https://thephd.dev/lambdas-nested-functions-block-expressions-oh-my">a much older article</a> or read the entire introduction in <a href="https://thephd.dev/_vendor/future_cxx/papers/C%20-%20Functions%20with%20Data%20-%20Closures%20in%20C.html">this work-in-progress C proposal</a>. The much older article is a much gentler introduction; the work-in-progress C proposal goes through a lot of the technical and design nitty-gritty and why things work or do not work very well.</p>

<p>The purpose of this article will, once again, be performance and deducing the performance characteristics of various designs. Much of this was covered in the previous article, so we‚Äôre going to focus on the new additions to the Benchmarks since then and the important takeaways.</p>

<p>As always, the implementations of my benchmarks are publicly available<sup id="fnref:idk-benchmarks-closures" role="doc-noteref"><a href="#fn:idk-benchmarks-closures" rel="footnote">1</a></sup>.</p>



<p>The only thing that changed from the last time we did this was to use 150 repetitions of the whole 100,000+ sample iterations benchmarks rather than just 50 or 100 repetitions. You can find the full, detailed explanation <a href="#methodology">at the bottom of this article</a>.</p>



<p>The new benchmarking categories reflected in the new bar graphs explicitly track the performance of a few different kinds of ‚ÄúPlain C‚Äù testing.</p>

<ul>
  <li>Normal Functions: regular C functions which add an extra argument to the function call in order to pass more data. Somewhat similar in representation to rewriting <code>qsort</code> to <code>qsort_r</code>/<code>qsort_s</code> to pass a user data pointer.</li>
  <li>Normal Functions (Rosetta Code): regular C functions which add an extra argument to the function call in order to pass more data. Taken directly from the Rosetta Code weekly, and uses a pointer <code>int* k</code> to refer to an already-existing value of <code>k</code> during a series of recursive calls.</li>
  <li>Normal Functions (Static): regular C function which uses a <code>static</code> variable to pass the specific context to the next function. Not thread safe. Does not modify the function call signature.</li>
  <li>Normal Functions (Thread Local): same as ‚ÄúNormal Functions (Static)‚Äù but using a <code>thread_local</code> variable instead of a static variable. Obviously thread safe. Does not modify the function call signature.</li>
</ul>

<p>These are different from the ‚ÄúNormal Functions‚Äù in small but important ways, and ‚Äì critically ‚Äì two of them do not modify the signature of the function call, meaning they can be used with the old-style of <code>qsort</code> APIs that do not take a <code>void* user_data</code> parameter. In particular, rather than taking an extra or dummy argument like <code>arg*</code> in:</p>

<div><div><pre><code><span>int</span> <span>f0</span><span>(</span><span>arg</span><span>*</span> <span>unused</span><span>)</span> <span>{</span>
	<span>(</span><span>void</span><span>)</span><span>unused</span><span>;</span>
	<span>return</span> <span>0</span><span>;</span>
<span>}</span>

<span>int</span> <span>f1</span><span>(</span><span>arg</span><span>*</span> <span>unused</span><span>)</span> <span>{</span>
	<span>(</span><span>void</span><span>)</span><span>unused</span><span>;</span>
	<span>return</span> <span>1</span><span>;</span>
<span>}</span>

<span>int</span> <span>f_1</span><span>(</span><span>arg</span><span>*</span> <span>unused</span><span>)</span> <span>{</span>
	<span>(</span><span>void</span><span>)</span><span>unused</span><span>;</span>
	<span>return</span> <span>-</span><span>1</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>It instead preserves the initial interface, without the (potentially unused) argument. This is important for Foreign Function Interfaces (FFI) and other shenanigans that gets used with closure-style code. Thus, rather than needing to write new functions with an extra argument, the <code>return 1</code>, <code>return -1</code>, and <code>return 0</code> helpers can be written in the normal, plain, usual way:</p>

<div><div><pre><code><span>int</span> <span>f0</span><span>()</span> <span>{</span>
	<span>return</span> <span>0</span><span>;</span>
<span>}</span>

<span>int</span> <span>f1</span><span>()</span> <span>{</span>
	<span>return</span> <span>1</span><span>;</span>
<span>}</span>

<span>int</span> <span>f_1</span><span>()</span> <span>{</span>
	<span>return</span> <span>-</span><span>1</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>One would imagine that such a change would not actually have any meaningful performance impact, and that using something like <code>static</code> variables or global variables to shuttle that data over into whatever function that needed it wouldn‚Äôt cause any measurable performance difference.</p>



<p>Of course, if it were true that there was no performance difference, I wouldn‚Äôt be forced to write about it! So, here we are, the cost or non-cost for the various kinds of ‚ÄúNormal Functions‚Äù usages, as compared to all the others:</p>

<p><img src="https://thephd.dev/assets/img/2025/12/manorboy2-appleclang17-linear.png" alt="See the paragraph for a text description of this bar chart image."/></p>

<p><img src="https://thephd.dev/assets/img/2025/12/manorboy2-gcc15-linear.png" alt="See the paragraph for a text description of this bar chart image."/></p>

<p><sub><a href="https://thephd.dev/assets/img/2025/12/manorboy2-gcc15-linear.png.txt">For the vision-impaired, a text description is available.</a></sub></p>

<p>As shown in the last article, performance is <strong>SO TERRIBLE</strong> for some solutions that it completely crowds out any useful visual from the linear graphs. So, we need to swap to the logarithmic graphs to get a better picture:</p>

<p><img src="https://thephd.dev/assets/img/2025/12/manorboy2-appleclang17-logarithmic.png" alt="See the paragraph for a text description of this bar chart image."/></p>

<p><img src="https://thephd.dev/assets/img/2025/12/manorboy2-gcc15-logarithmic.png" alt="See the paragraph for a text description of this bar chart image."/></p>

<p><sub><a href="https://thephd.dev/assets/img/2025/12/manorboy2-gcc15-logarithmic.png.txt">For the vision-impaired, a text description is available.</a></sub></p>

<p>Still, the logarithmic graphs render things like the black error bars on each bar graph completely useless. So, we swap back to linear this time, but with the caveat that we remove some of the worst ‚Äúoutliers‚Äù (e.g., the things that had the most awful performance metrics). This, effectively, means cutting out the ‚ÄúLambda (Rosetta Code)‚Äù category and bar graph. This gives us the following linearly-scaled graph:</p>

<p><img src="https://thephd.dev/assets/img/2025/12/manorboy2-appleclang17-linear-focused.png" alt="See the paragraph for a text description of this bar chart image."/></p>

<p><img src="https://thephd.dev/assets/img/2025/12/manorboy2-gcc15-linear-focused.png" alt="See the paragraph for a text description of this bar chart image."/></p>

<p><sub><a href="https://thephd.dev/assets/img/2025/12/manorboy2-gcc15-linear-focused.png.txt">For the vision-impaired, a text description is available.</a></sub></p>

<p>There, that‚Äôs much better and easier to read! It also gives us a more precise look at the faster-performing functions, and lets us talk about it much more clearly!</p>



<p>There are quite a few insights here that are important to elaborate on. We will start first with the obvious DRASTIC improvements we need from the original code contained in the previous article to where are are now: ‚ÄúNormal Functions (Rosetta Code)‚Äù to ‚ÄúNormal Functions‚Äù.</p>

<h2 id="becoming-the-most-normal-function">Becoming the Most Normal Function</h2>

<p>The only difference between this and ‚ÄúNormal Functions (Rosetta Code)‚Äù is us not holding onto a pointer. Specifically, the <code>all</code> structure in the Normal Functions is just:</p>

<div><div><pre><code><span>typedef</span> <span>struct</span> <span>all</span> <span>{</span>
	<span>int</span> <span>(</span><span>*</span><span>B</span><span>)(</span><span>struct</span> <span>all</span><span>*</span><span>);</span>
	<span>int</span> <span>k</span><span>;</span>
	<span>struct</span> <span>all</span> <span>*</span><span>x1</span><span>,</span> <span>*</span><span>x2</span><span>,</span> <span>*</span><span>x3</span><span>,</span> <span>*</span><span>x4</span><span>,</span> <span>*</span><span>x5</span><span>;</span>
<span>}</span> <span>all</span><span>;</span>

<span>static</span> <span>int</span> <span>A</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>all</span><span>*</span> <span>x1</span><span>,</span> <span>all</span><span>*</span> <span>x2</span><span>,</span> <span>all</span><span>*</span> <span>x3</span><span>,</span> <span>all</span><span>*</span> <span>x4</span><span>,</span> <span>all</span><span>*</span> <span>x5</span><span>);</span>

<span>static</span> <span>int</span> <span>B</span><span>(</span><span>all</span><span>*</span> <span>self</span><span>)</span> <span>{</span>
	<span>return</span> <span>A</span><span>(</span><span>--</span><span>self</span><span>-&gt;</span><span>k</span><span>,</span> <span>self</span><span>,</span> <span>self</span><span>-&gt;</span><span>x1</span><span>,</span> <span>self</span><span>-&gt;</span><span>x2</span><span>,</span> <span>self</span><span>-&gt;</span><span>x3</span><span>,</span> <span>self</span><span>-&gt;</span><span>x4</span><span>);</span>
<span>}</span>

<span>static</span> <span>int</span> <span>A</span><span>(</span><span>int</span> <span>k</span><span>,</span> <span>all</span><span>*</span> <span>x1</span><span>,</span> <span>all</span><span>*</span> <span>x2</span><span>,</span> <span>all</span><span>*</span> <span>x3</span><span>,</span> <span>all</span><span>*</span> <span>x4</span><span>,</span> <span>all</span><span>*</span> <span>x5</span><span>)</span> <span>{</span>
	<span>if</span> <span>(</span><span>k</span> <span>&lt;=</span> <span>0</span><span>)</span> <span>{</span>
		<span>return</span> <span>x4</span><span>-&gt;</span><span>B</span><span>(</span><span>x4</span><span>)</span> <span>+</span> <span>x5</span><span>-&gt;</span><span>B</span><span>(</span><span>x5</span><span>);</span>
	<span>}</span>
	<span>else</span> <span>{</span>
		<span>all</span> <span>y</span> <span>=</span> <span>{</span> <span>.</span><span>B</span> <span>=</span> <span>B</span><span>,</span> <span>.</span><span>k</span> <span>=</span> <span>k</span><span>,</span> <span>.</span><span>x1</span> <span>=</span> <span>x1</span><span>,</span> <span>.</span><span>x2</span> <span>=</span> <span>x2</span><span>,</span> <span>.</span><span>x3</span> <span>=</span> <span>x3</span><span>,</span> <span>.</span><span>x4</span> <span>=</span> <span>x4</span><span>,</span> <span>.</span><span>x5</span> <span>=</span> <span>x5</span> <span>};</span>
		<span>return</span> <span>B</span><span>(</span><span>&amp;</span><span>y</span><span>);</span>
	<span>}</span>
<span>}</span>
</code></pre></div></div>

<p>The only change here is that instead of using <code>int* k</code> like in the <code>arg</code> structure of Rosetta Code we use <code>int k</code> directly:</p>

<div><div><pre><code><span>typedef</span> <span>struct</span> <span>arg</span> <span>{</span>
	<span>int</span> <span>(</span><span>*</span><span>fn</span><span>)(</span><span>struct</span> <span>arg</span><span>*</span><span>);</span>
	<span>int</span><span>*</span> <span>k</span><span>;</span>
	<span>struct</span> <span>arg</span> <span>*</span><span>x1</span><span>,</span> <span>*</span><span>x2</span><span>,</span> <span>*</span><span>x3</span><span>,</span> <span>*</span><span>x4</span><span>,</span> <span>*</span><span>x5</span><span>;</span>
<span>}</span> <span>arg</span><span>;</span>

<span>static</span> <span>int</span> <span>f_1</span><span>(</span><span>arg</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>-</span><span>1</span><span>;</span>
<span>}</span>

<span>static</span> <span>int</span> <span>f0</span><span>(</span><span>arg</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>0</span><span>;</span>
<span>}</span>

<span>static</span> <span>int</span> <span>f1</span><span>(</span><span>arg</span><span>*</span> <span>_</span><span>)</span> <span>{</span>
	<span>return</span> <span>1</span><span>;</span>
<span>}</span>

<span>// --- helper</span>
<span>static</span> <span>int</span> <span>eval</span><span>(</span><span>arg</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>return</span> <span>a</span><span>-&gt;</span><span>fn</span><span>(</span><span>a</span><span>);</span>
<span>}</span>

<span>static</span> <span>int</span> <span>A</span><span>(</span><span>arg</span><span>*</span><span>);</span>

<span>// --- functions</span>
<span>static</span> <span>int</span> <span>B</span><span>(</span><span>arg</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>int</span> <span>k</span>    <span>=</span> <span>*</span><span>a</span><span>-&gt;</span><span>k</span> <span>-=</span> <span>1</span><span>;</span>
	<span>arg</span> <span>args</span> <span>=</span> <span>{</span> <span>B</span><span>,</span> <span>&amp;</span><span>k</span><span>,</span> <span>a</span><span>,</span> <span>a</span><span>-&gt;</span><span>x1</span><span>,</span> <span>a</span><span>-&gt;</span><span>x2</span><span>,</span> <span>a</span><span>-&gt;</span><span>x3</span><span>,</span> <span>a</span><span>-&gt;</span><span>x4</span> <span>};</span>
	<span>return</span> <span>A</span><span>(</span><span>&amp;</span><span>args</span><span>);</span>
<span>}</span>

<span>static</span> <span>int</span> <span>A</span><span>(</span><span>arg</span><span>*</span> <span>a</span><span>)</span> <span>{</span>
	<span>return</span> <span>*</span><span>a</span><span>-&gt;</span><span>k</span> <span>&lt;=</span> <span>0</span> <span>?</span> <span>eval</span><span>(</span><span>a</span><span>-&gt;</span><span>x4</span><span>)</span> <span>+</span> <span>eval</span><span>(</span><span>a</span><span>-&gt;</span><span>x5</span><span>)</span> <span>:</span> <span>B</span><span>(</span><span>a</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>It turns out needing to do that indirect load to get at <code>int* k</code> cost us a LOT more than any of us could hope for. This is surprising, given that the lambda uses a single default capture of <code>&amp;</code> and references the <code>k</code> it was made with transparently. In essence: it works actually like the poorly-performing ‚ÄúNormal Functions (Rosetta Code)‚Äù example, and yet the compiler is able to outperform this in comparison to the structure passed as an explicit argument.</p>

<p>The problem is that the indirect load through both (a) the <code>int* k</code> and (b) the <code>all*</code>/<code>arg*</code> structure are actually impeding compiler optimization and slowing us down. In C, we like to imagine that doing in-place modification and operations directly on a given piece of memory can generally be better and faster than other techniques. This applies for big data sets and huge arrays, but for smaller work like what is in the Man or Boy test, it‚Äôs actually the opposite: pointers to smaller pieces of data are a big waste of time.</p>

<p>The good news is that removing the <code>int* k</code> only means we have one level of indirection to deal with, and that really boosts performance compared to the original, bad Rosetta Code Wiki example that this benchmark is based on. Unfortunately, despite getting a huge boost from its old performance‚Ä¶</p>

<h2 id="lambdas-are-still-peak">Lambdas Are Still Peak</h2>

<p>It is the encapsulation and the preservation of type information without hiding it behind an additional structures that keeps the performance lean. This means that the design of lambdas ‚Äì a unique object with its own type that is not immediately hoisted or erased like it is in Apple Blocks, GNU Nested Functions, and other compiler techniques ‚Äì is actually the <strong>leanest possible implementation</strong>.</p>

<p>The drawback of this that is especially egregious in C, unfortunately, is that unlike C++ there are no templates in C. There‚Äôs no ‚Äúfake‚Äù recursion parameter we can add to limit an infinity-spiral of self-calls. This means that unique typings ‚Äì while an unrestricted boon in C++ ‚Äì is actually a bit of a drawback in C! In terms of passing arguments around and returning them, there‚Äôs no type-generics at compile-time that can help with this.</p>

<p>So either all the code interacting with it has to be macros (EWWWW), OR we need to develop at <strong>least</strong> one layer of indirection so we can prevent things like infinite recursion or realistically handle lots of data types. The much more sadder conclusion is that a programming language like C, unless you drop down to assembly or hand-unroll loops with your own selection of manually-crafted strong types, you will lose out on some degree of performance. This is not normally something anybody would be able to say about C, but it turns out that needing to do type-erasure imposes a cost. If the compiler cannot unroll that cost for any number of reasons, you will end up paying for it in performance. (But you can still get pretty good code size, so that part is nice at least.)</p>

<h2 id="the-next-tier-up-very-small-amounts-of-type-erasure">The Next Tier Up: Very Small Amounts of Type Erasure</h2>

<p>While Lambdas are the best and standalone in what they are capable of, they are only the best under C++-ish, template-ish circumstances (like C macro generics). When you have to ditch the templates and the perfect type information, C++-style Lambdas lose a good bit of their competitive edge. Primarily, any amount of lean type erasure adds an non-negotiable impact to performance over the base case, as shown by ‚ÄúNormal Functions‚Äù, ‚ÄúCustom C++ Class‚Äù, ‚ÄúLambdas <code>std::function_ref</code>‚Äù, and ‚ÄúNormal Functions (Statics)‚Äù.</p>

<p>I put ‚ÄúNormal Functions (Statics)‚Äù into this group despite it clearly having very bad performance implications from how GCC implements it that actually make it slightly wore than the others. It‚Äôs also surprising that passing a variable by <code>static</code> variable ‚Äì a solution touted by many C developers and often said to be ‚Äújust as good‚Äù as being able to hijack the function signature and add a new parameter ‚Äì is actually strictly worse than ‚ÄúNormal Functions‚Äù. One can imagine that a <code>static</code> variable in charge of doing transportation is inevitably going to have to pay for the cost of loads and stores for each function call, and that compilers have to try to contest with that differently.</p>

<h3 id="slightly-worse-thread_local">Slightly Worse: <code>thread_local</code></h3>

<p>No surprise that no matter the setup, using the <code>thread_local</code> keyword instead of the <code>static</code> keyword adds more overhead. I was, again, surprised by exactly how much assigning into it once and then reading it a single time once inside the function could have on the performance metrics, but it turns out that this is not free either.</p>

<p>It goes to show that having what the Closures WIP ISO C proposal asks for both C++-style Lambdas and C-style ‚ÄúCapture Functions‚Äù (nested functions that do not have the design, ABI issues, and Implementation Baggage of regular GNU Nested Functions)<sup id="fnref:capture-functions" role="doc-noteref"><a href="#fn:capture-functions" rel="footnote">2</a></sup> <strong>along with</strong> a Wide Function Pointer type would be better than trying to figure out a magic <code>static</code> or magic <code>thread_local</code> style of implementation.</p>

<p>We are not sure what to think of the Local Functions and Function Literals proposals<sup id="fnref:local-literal-functions" role="doc-noteref"><a href="#fn:local-literal-functions" rel="footnote">3</a></sup>, because neither of them try to allow you to access local variables. Which is 90%<sup id="fnref:stat-90" role="doc-noteref"><a href="#fn:stat-90" rel="footnote">4</a></sup> of the reason anyone uses Nested Functions to begin with!</p>

<h2 id="what-is-going-on-with-gnu-nested-functions">What Is Going On With GNU Nested Functions???</h2>

<p>Honestly, I do NOT know at this point.</p>

<p>It‚Äôs worth saying that I almost had to cut out GNU Nested Functions because of how god-awfully the were performing in the GCC graphs. It made it exponentially harder to get a good, zoomed-in look at the rest of the entries. While some have talked about standardizing just GNU Nested Functions, I do not think that ISO C could standardize an extension like this in any kind of Good Faith and still call itself a language concerned about low-level code and speed. Its existing implementations are so performance-deleting it‚Äôs a wonder why the decades-old code generated for it hasn‚Äôt been improved or touched up. I can only hope that the forthcoming <code>-ftrampoline-impl=heap</code> code from GCC puts it more in-line with the ‚ÄúNormal Functions (Static)‚Äù or ‚ÄúNormal Functions (Thread Local)‚Äù category, but if the performance of the new trampoline is just as awful as the current one I‚Äôd consider GNU Nested Functions to be dead-on-arrival for a lot of use cases.</p>

<p>This sort of awful performance also retroactively justifies Clang‚Äôs public and open decision to never, ever implement GNU Nested Functions. On top of the security issues the typical stack-based trampoline creates, the performance qualities are so egregious that just asking everyone to use <code>-fblocks</code> and the Apple Blocks extension for this functionality is probably the lesser of two evils. It also brings into question whether a ‚Äúlean‚Äù approach that grabs the ‚Äúenvironment pointer‚Äù or the ‚Äústack frame‚Äù pointer directly, as in n3654<sup id="fnref:n3654" role="doc-noteref"><a href="#fn:n3654" rel="footnote">5</a></sup> is a good idea to start with.</p>

<p>But, it‚Äôs premature to condemn n3654 because it‚Äôs unknown if the problem is the fact that the use of accessing variables through what is effectively <code>__builtin_stack_address</code> and a trampoline is why performance sucks so bad, or if it‚Äôs the way the trampoline screws with the stack. There are many compounding reasons why GNU Nested Functions as they exist today do so poorly, and more investigation is needed to make sure the approach in n3654 of accessing the ‚ÄúContext‚Äù of a nested function isn‚Äôt actually a huge performance footgun.</p>



<p>Now that we have thoroughly evaluated the solution space for C, including many of the home-cooked favorite solutions written in plain C, I think the safe conclusions I can draw are:</p>

<ul>
  <li>Lambdas (and the proposed Capture Functions<sup id="fnref:capture-functions:1" role="doc-noteref"><a href="#fn:capture-functions" rel="footnote">2</a></sup>) are the best for performance, so long as perfect information is retained.</li>
  <li>A type-preserving closure (e.g. Lambdas or Capture Functions) combined with the smallest, thinnest possible type erasure (a Wide Function Pointer type) would bring immediate performance gains over existing C extensions and plain C code that does not modify the function signature.</li>
  <li>Both Apple Blocks and GNU Nested Functions have parts of their designs and implementations that are deeply problematic for integration into normal compilers.</li>
  <li>It is unclear if making what is effectively access to the function frame / ‚Äúenvironment‚Äù through a pointer is an advisable course of action for the future of the C ecosystem.</li>
  <li>C users writing typical C code will, at some point, suffer some degree of performance loss in complex scenarios due to necessary type erasure to work with complex, compiler-generated closure types. Type-generic macro programming can help here, but the tradeoff for code size versus speed should be considered on whether to use a normal, type-erased interface versus an entirely (macro-)generic set of function calls.</li>
</ul>

<p>Finally, both <code>static</code> and <code>thread_local</code> have performance cost, moreso on GCC than on Clang. I‚Äôd be interested to run the MSVC numbers too as more than just a quick ‚Äúthis works on the damn compiler‚Äù check, but I think these numbers are more than enough to draw general conclusions about the viability of the various approaches.</p>

<p>Happy New Year, and until next weird niche performance bit. üíö</p>

<ul>
  <li>Banner and Title Photo by <a href="https://www.pexels.com/photo/person-holding-black-card-holder-928181/">Lukas, from Pexels</a></li>
</ul>



<h2 id="methodology">Methodology</h2>

<p>The tests were ran on a 13-inch 2020 MacBook Pro M1. It has 16 GB of RAM and is on MacOS 15.7.2 Sequoia at the time the test was taken, using the stock MacOS AppleClang Compiler and the stock brew install gcc compiler in order to produce the numbers seen on December 28th, 2025.</p>

<p>The experimental setup used the Man or Boy test, but with the given <code>k</code> value loaded by calling a function in a DLL / Shared Object. The expected <code>k</code> value that the Man or Boy test is supposed to yield is also loaded from a DLL / Shared Object. This prevents optimizing out all recursion and doing enough ahead-of-time computation to simply collapse the benchmarked code into a constant-time, translation-time calculation. It ensures the benchmark is actually measuring the actual performance characteristics of the technique used, as all of them are computing from the same initial k value and all of them are expected to produce the same <code>expected_k</code> answer.</p>

<p>There 2 measures being conducted: Real (‚Äúwall clock‚Äù) Time and CPU Time. The time is gathered by running a single iteration of the code within a for loop. That loop runs anywhere from a couple thousand to hundreds of thousands of times to produce confidence in that run of the benchmark, and each loop run is considered an individual iteration. The iterations are then averaged to produce the first point after there is confidence that the measurement is accurate and the benchmark is warm. The iteration process to produce a single mean was then repeated 150 times. All 150 means are used as the points for the values (shown as transparent dots) on the bar graph, and the average of all of those 150 means is then used as the height of a bar in a bar graph.</p>

<p>The bars are presented side-by-side as a horizontal bar chart with various categories of C or C++ code being measured. The 13 total categories of C and C++ code are:</p>

<ul>
  <li>no-op: Literally doing nothing. It‚Äôs just there to test environmental noise and make sure none of our benchmarks are so off-base that we‚Äôre measuring noise rather than computation. Helps keep us grounded in reality.</li>
  <li>Normal Functions: regular C functions which add an extra argument to the function call in order to pass more data. Somewhat similar in representation to rewriting qsort to qsort_r/qsort_s to pass a user data pointer.</li>
  <li>Normal Functions (Static): regular C function which uses a <code>static</code> variable to pass the specific context to the next function. Not thread safe.</li>
  <li>Normal Functions (Thread Local): same as ‚ÄúNormal Functions (Static)‚Äù but using a <code>thread_local</code> variable instead of a <code>static</code> variable. Obviously thread safe.</li>
  <li>Lambdas (No Function Helpers): a solution using C++-style lambdas. Rather than using helper functions like <code>f0</code>, <code>f1</code>, and <code>f_1</code>, we compute a raw lambda that stores the value meant to be returned for the Man-or-Boy test (with a body of just return i;) in the lambda itself and then pass that uniquely-typed lambda to the core of the test. The entire test is templated and uses a fake recursion template parameter to halt the translation-time recursion after a certain depth.</li>
  <li>Lambdas: The same as above but actually using int f0(void), etc. helper functions at the start rather than lambdas. Tries to reduce optimizer pressure by using ‚Äúnormal‚Äù types which do not add to the generated number of lambda-typed, recursive, templated function calls.</li>
  <li>Lambdas (std::function_ref): The same as above, but rather than using a function template to handle each uniquely-typed lambda like a precious baby bird, it instead erases the lambda behind a <code>std::function_ref&lt;int(void)&gt;</code>. This allows the recursive function to retain exactly one signature.</li>
  <li>Lambdas (std::function): The same as above, but replaces <code>std::function_ref&lt;int(void)&gt;</code> with <code>std::function&lt;int(void)&gt;</code>. This is an allocating, C++03-style type.</li>
  <li>Lambdas (Rosetta Code): The code straight out of the C++11 Rosetta Code Lambda section on the Man-or-Boy Rosetta Code implementation.</li>
  <li>Apple Blocks: Uses Apple Blocks to implement the test, along with the <code>__block</code> specifier to refer directly to certain variables on the stack.</li>
  <li>GNU Nested Functions (Rosetta Code): The code straight out of the C Rosetta Code section on the Man-or-Boy Rosetta Code implementation.</li>
  <li>GNU Nested Functions: GNU Nested Functions similar to the Rosetta Code implementation, but with some slight modifications in a hope to potentially alleviate some stack pressure if possible by using regular helper functions like <code>f0</code>, <code>f1</code>, and <code>f_1</code>.</li>
  <li>Custom C++ Class: A custom-written C++ class using a discriminated union to decide whether it‚Äôs doing a straight function call or attempting to engage in the Man-or-Boy recursion.</li>
  <li>C++03 shared_ptr (Rosetta Code): A C++ class using <code>std::enable_shared_from_this</code> and <code>std::shared_ptr</code> with a virtual function call to invoke the ‚Äúright‚Äù function call during recursion.</li>
</ul>

<p>Each bar graph has a black error bar at the end, representing the standard error of the measurements performed. At 150 iterations, the error bars (which are most easily understood and read in the linear graphs) are a decent visual approximation of whether or not two solutions are within a statistical threshold of one another.</p>

<p>The two compilers tested are Apple Clang 17 and GCC 15. There are two graph images for each kind of measurement (linear, logarithmic, and linear-but-with-outliers-removed) because one is for Apple Clang and the other is for GCC. This is particularly important because neither compiler implements the other‚Äôs closure extension (Clang does Apple Blocks but not Nested Functions, while GCC does Nested Functions in exclusively its C frontend but does not implement Apple Blocks).</p>

<p>MSVC was not tested because MSVC implements none of the extensions being tested, and we do not expect that its performance characteristics would be wildly different than what GCC or Clang are capable of. (In fact, we expect it might be a bit worse in all untested, non-scientific honesty.)</p>







    
    </section>

    <!-- Social media shares -->
    






    <!-- Category and Tag list -->
    

</article></div>
  </body>
</html>
