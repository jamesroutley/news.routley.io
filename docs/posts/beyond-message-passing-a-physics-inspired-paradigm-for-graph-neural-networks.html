<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://thegradient.pub/graph-neural-networks-beyond-message-passing-and-weisfeiler-lehman/">Original</a>
    <h1>Beyond message passing: A physics-inspired paradigm for graph neural networks</h1>
    
    <div id="readability-page-1" class="page"><div>
          <p>The message-passing paradigm has been the ‚Äúbattle horse‚Äù of deep learning on graphs for several years, making graph neural networks a big success in a wide range of applications, from particle physics to protein design. From a theoretical viewpoint, it established the link to the Weisfeiler-Lehman hierarchy, allowing to analyse the expressive power of GNNs. We argue that the ‚Äúnode and edge-centric‚Äù mindset of current graph deep learning schemes imposes strong limitations that hinder future progress in the field. As an alternative, we propose physics-inspired ‚Äúcontinuous‚Äù learning models that open up a new trove of tools from the fields of differential geometry, algebraic topology, and differential equations so far largely unexplored in graph ML. </p><p>Graphs are a convenient way to abstract complex systems of relations and interactions. The increasing prominence of graph-structured data from social networks to high-energy physics to chemistry, and a series of high-impact successes have made deep learning on graphs one of the hottest topics in machine learning research [1]. Graph Neural Networks (GNNs) are by far the most common among graph ML methods and the most popular neural network architectures overall [2].</p><figure><img src="https://lh3.googleusercontent.com/PvNQrlvrBpgCm4ff_W9OVcKhNwUl9E8NVjfHRFqL8VRiGdEhZtBbDTPderyY8_u8g6GRgTpz-T2jRAT1NYiDymu5Cd7xxZ69Nuv2tMZAj4gKG6GrRpo_1cTlSMTNre5sny8pMN4D" alt=""/><figcaption>Graphs abstract complex systems of relations and interactions. Shown left to right: molecular graph (representing chemical bonds between atoms forming a molecule), social network (representing relations and interactions between users), recommender system (representing associations between users and products).¬†</figcaption></figure><h2 id="how-do-gnns-work">How do GNNs work? </h2><p>Graph neural networks take as input a graph endowed with node and edge features and compute a function that depends both on the features and the graph structure. Message-passing type GNNs, also called Message Passing Neural Networks (MPNN) [3], propagate node features by exchanging information between adjacent nodes. A typical MPNN architecture has several propagation layers, where each node is updated based on the aggregation of its neighbours‚Äô features. Aggregation functions are typically parametric, and come in three different ‚Äúflavours‚Äù of graph neural networks [4]:</p><ul><li><strong>convolutional:</strong> linear combination of neighbour features where weights depend only on the structure of the graph</li><li><strong>attentional: </strong>linear combination, where weights are computed based on the features</li><li><strong>message passing: </strong>a general nonlinear function dependent on the features of two nodes sharing an edge.</li></ul><p>Of these three flavours, the latter is the most general and the two others can be seen as special cases of message passing. <br/></p><figure><img src="https://lh4.googleusercontent.com/4oFmw_1uZHF-nNZyEV-X79zZdhPJJpjCU44H53YfOaC4k4zZNH2CqqTmf5K1UqfgsCI_QV1peaVcRICsuOjZPkzhi1vMIE1ZpwHiMYZRR1xSC41IN3h3hiNIni1uWR1VFYIPdrRZ" alt=""/><figcaption>Three ‚Äúflavours‚Äù of GNNs, left-to-right: convolutional, attentional, and general nonlinear message passing flavours. All are forms of message passing. Figure adapted from P. Veliƒçkoviƒá.</figcaption></figure><p>The parameters of propagation layers are learned based on the downstream tasks. Typical use cases include:</p><ul><li><em>node</em> <em>embedding: </em>representing each node as a point in a vector space in such a way that the proximity of these points recovers the connectivity of the original graph, a task known as ‚Äúlink prediction‚Äù</li><li><em>node-wise</em> classification or regression: for example inferring attributes of social network users</li><li><em>graph-wise</em> classification or regression: for example, predicting chemical properties of molecular graphs </li></ul><h2 id="what-s-wrong-with-message-passing-gnns">What‚Äôs wrong with message passing GNNs?</h2><p>It is fair to say that the predominant model underlying current GNNs is message passing. We believe it is this very ‚Äúnode-and-edge-centric‚Äù mindset that hinders future progress in the field due to several important limitations:</p><h3 id="the-weisfeiler-lehman-analogy-is-limiting">The Weisfeiler-Lehman analogy is limiting </h3><p>An appropriate choice of the local aggregation function like summation [5] makes message passing equivalent to the Weisfeiler-Lehman graph isomorphism test [6], allowing graph neural networks to discover some structure of the graph from the way information propagates on it. This important link to graph theory [7] has produced multiple theoretical results on the expressive power of GNNs, determining whether certain functions on a graph can be computed by means of message passing or not [8]. However, these results are not generally informative of the <em>efficiency</em> of such representations<em> </em>(i.e., how many layers are required to compute a function) [9], nor about the <em>generalisation</em> capabilities of GNNs. </p><figure><img src="https://lh4.googleusercontent.com/lWoUHcM6jgektuf8d2N2k-70hOYzRoKeVZRZ1R71tGUfT7LCyX_lzoOQw3MAeehwoSL8n4_h1jrNwrU-xKROMW4wVFh4WrkDtz5egRDYmLPTZlxyB79-d9FCaIphKN4S_i0eluIT" alt=""/><figcaption>Dominique Beaini suggests a metaphor for the Weisfeiler-Lehman test as trying to understand the structure of a maze by walking in it without a map. Positional encoding provides a ‚Äúmap‚Äù of the maze, while rewiring gives a ladder to go over the ‚Äúwalls‚Äù. <span>‚Äå‚Äå</span></figcaption></figure><p>The inability of the Weisfeiler-Lehman algorithm to detect even simple graph structures such as triangles is astonishingly disappointing for practitioners trying to use message passing neural networks for molecular graphs. In organic chemistry, for example, structures such as rings are abundant and play an important role in the way molecules behave.</p><figure><img src="https://lh3.googleusercontent.com/dZM0WBgjJJpQZX5v9bFJuj_mlSxmjtcycgB2ZFUr2V4GrA395Xcav7MM2PJsuLUD9GQMLwEMmPxOp2kC_4_51lDHht-xy-yYw8cW0NYV4drxAi4A6n_WR_FldX9LLXw7iVznrCow" alt=""/><figcaption>Examples of two molecular graphs of decalin (left) and bicyclopentyl (right), which are structurally different (one has 6-cycles, another one 5-cycles) but indistinguishable by the Wesifeiler-Lehman test.¬†</figcaption></figure><p>Several approaches have been proposed to improve the expressiveness of GNN models. These include higher-dimensional isomorphism tests in the Weisfeiler-Lehman hierarchy [10]. This approach comes at the expense of higher computational and memory complexity, and lack of locality. Applying the Wesifeiler-Lehman test to a collection of subgraphs [11], or positional- or structural encoding [12] that &#34;colours&#39;&#39; graph nodes to help break the regularities that ‚Äúconfuse‚Äù the Weisfeiler-Lehman algorithm. Positional encoding is by far the most common technique borrowed from Transformers [13], it is now ubiquitous in GNNs. While multiple positional encodings exist, it is fair to say that a particular choice is application-dependent and rather empirical. </p><figure><img src="https://lh5.googleusercontent.com/xQVOlsX8k6-1zxIZnKY_YyedHcKT8i9qAtpSnCgSUn7OYEgK2djU2A3il7Tn8lb93Ry6TBHLhCo7MvN75J47MAFVI3SXoGJF6v8rW4T-E4A0m-pwWKkCZ9KjxK9BFOXwWltJcTqU" alt=""/><figcaption>[FIGURE: positional encoding examples] Shown left-to-right: random features, Laplacian eigenvectors (analogy of sinusoids used in Transformers), structural features (counts of triangles and rectangles)<span>‚Äå‚Äå</span></figcaption></figure><h3 id="graph-rewiring-breaks-the-theoretical-foundations-of-gnns">Graph rewiring breaks the theoretical foundations of GNNs </h3><p>One important and somewhat subtle difference between GNNs and Convolutional Neural Networks (CNNs) is that the graph is both <em>part of the input</em> and the <em>computational structure</em>. Traditional GNNs use the input graph to propagate information, thus obtaining a representation that reflects both the structure of the graph and its features. However, some graphs turn out to be less ‚Äúfriendly‚Äù for information propagation due to certain structural characteristics. These characteristics are called ‚Äúbottlenecks‚Äù, and result in information from too many nodes to be ‚Äúsqueezed‚Äù into a single node representation ‚Äì a phenomenon referred to as ‚Äúoversquashing‚Äù [14].</p><figure><img src="https://lh5.googleusercontent.com/Xd5VZNrM8LNZ2cZBuu1h80Wa2JyCPiPfaQ20sLixx80Np5sYmQA3RbjmaFUNo0-oL7OVIgHlLr8WvC2cCE6OHS6tJ5yRfEbmdxijcTCLWbYNkNmEei_YHNpPdYpCnGMNEjcveZdG" alt=""/><figcaption>Different graph rewiring techniques used in GNNs. Left-to-right: original graph, neighbourhood sampling (e.g. GraphSAGE [15]), attention (e.g. GAT [20]), and connectivity evolution (e.g. DIGL [17]).¬†</figcaption></figure><p>This is bad news for theoretical analyses relying on the equivalence of message passing and the Weisfeiler-Lehman test to obtain a description of the graph from the way information propagates on it. Rewiring breaks this theoretical link, creating a situation not uncommon in machine learning: models that can be analysed in theory are not the same as those used in practice. </p><h3 id="the-geometry-of-graphs-is-too-poor">The ‚Äúgeometry‚Äù of graphs is too poor </h3><p>Graph neural networks can be obtained as an instance of the &#34;Geometric Deep Learning blueprint&#34; [22], a group-theoretical framework allowing to derive deep learning architectures from the symmetry of the domain underlying the data. In graphs, this symmetry is <em>node permutation</em>, as graphs do not have a canonical node ordering. Because of this structural characteristic, MPNNs operating locally on the graph must rely on <em>permutation-invariant</em> feature aggregation functions. This implies that graphs have no straightforward notion of &#34;direction&#34;, and that the propagation of information is <em>isotropic</em> [23]. This situation is strikingly different from learning on continuous domains, grids, or even meshes [24], and has been one of the early criticisms of GNNs arguing that isotropic filters are not very useful [25]. </p><figure><img src="https://lh5.googleusercontent.com/ilVYn7volQIQ35r6joo962So2yU7T71NiDfoqlS0x0CroFSLjw8q7xCHOuip5xJP9lWkONeHKQ75IQrK8RYDXpAZkJu8yLrdsbkczL5ilzpGSN799zhLZGSdjXmum7wQaXtZJkKr" alt=""/><figcaption>Left: meshes are discrete manifolds with a locally Euclidean structure. The neighbour nodes are defined up to a rotation, allowing a notion of ‚Äúdirection‚Äù. Right: graphs have less structure, and neighbour nodes are defined up to a permutation.¬†</figcaption></figure><h3 id="the-geometry-of-graphs-is-too-rich">The ‚Äúgeometry‚Äù of graphs is too rich </h3><p>On the other hand, graphs might have structures that are not compatible with standard spaces used for their representations. When building node embeddings, the distance between the vector representations of the nodes is used to capture the connectivity of the graph. In short, nodes close in the embedding space are expected to be connected by an edge in the graph. Typical use cases are recommender systems, where graph embeddings are used to create associations (edges) between entities represented by the nodes (e.g., whom to follow on Twitter). </p><p>The quality of graph embeddings and their ability to convey the structure of the graph crucially depends on the geometry of the embedding space and its compatibility with the &#34;geometry&#34; of the graph. Euclidean spaces, a staple of representation learning and by far the simplest and most convenient to work with, can be a poor choice for many natural graphs. One of the reasons is that the volume growth of Euclidean metric balls is polynomial in the radius but exponential in the dimension [26]. In contrast, many real-world graphs manifest exponential volume growth [27]. As a result, embeddings become &#34;too crowded&#34;, forcing the ¬†use of high dimensional embedding spaces that come with higher computational and space complexity. </p><p>A recently popular alternative is to use hyperbolic (negatively-curved) spaces [28] that have exponential volume growth more compatible with that of graphs [29]. The use of hyperbolic geometry typically leads to lower embedding dimensions, resulting in more compact node representations. However, this is not enough: graphs often tend to be <em>heterogeneous</em>, and some parts may look like trees and others as cliques, with very different volume growth properties. On the other hand, hyperbolic and Euclidean embedding spaces are <em>homogeneous</em>, meaning that every point has the same geometry. </p><p>Furthermore, it is generally impossible to exactly represent the metric structure of a general graph in an embedding space [30], even using non-Euclidean geometry. Thus, graph embeddings are inevitably <em>approximate</em>. Moreover, since embeddings are constructed with &#34;link prediction&#34; in mind, the distortion of higher-order structures (triangles, rectangles, etc.) can be uncontrollably large [31]. In some applications, including social and biological networks, such structures play an important role as they capture complex non-pairwise interactions and motifs [32]. </p><figure><img src="https://lh5.googleusercontent.com/_yS2s-fIaYZH2CHXfpdU1hVtNXTny9beiVmF2XQhyWquJdYw_WhA8x556gOHt9xwOaZ4LsbiBz2rV4dKLQhvIrVRqFjrYY-hQGcNIoa6YZkI5WZza3hw8FH4glxY5zRxwMR8N8VX" alt=""/><figcaption>Graph motifs are examples of higher-order structures. Such structures have been observed in graphs modelling many biological phenomena. Figure: Milo et al.</figcaption></figure><h3 id="gnns-struggle-when-data-structure-is-not-compatible-with-the-underlying-graph-structure">GNNs struggle when data structure is not compatible with the underlying ¬†graph structure</h3><p>Many graph learning datasets and benchmarks make the ¬†tacit assumption that the features or labels of adjacent nodes are similar, a property called <em>homophily</em>. In this setting, even simple low-pass filtering on the graph (e.g., taking the neighbour average) tends to work well. Early benchmarks, including the still ubiquitous Cora dataset, were graphs with a high level of homophily. Many models however show disappointing results when dealing with heterophilic data, where more nuanced aggregation is required ¬†[33]. Typically the model then either ignores neighbour information altogether, or node representations become smoother with every layer of GNN, eventually collapsing into a single point [34]. The latter phenomenon also occurs in homophilic datasets and appears to be a more fundamental plight of some types of MPNNs, making it <a href="https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59?sk=8daa06935676e78bdb229017d3c4bac9">hard to implement deep graph learning models</a> [35]. ¬†</p><figure><img src="https://lh4.googleusercontent.com/YjFaOc1GvRjwf6B-KlDbEjbuYDPcog-aIG_fNu5sOxwmagz0mqQRZDItyPv3U1_re8-6Lgvrp5CsTRGpDxKBd32ZJ9VU93iUGb6ioFUEhQjnLWBGCsmdOXoJxRjhhknkCJVLJXr4" alt=""/><figcaption>Homophilic (left) and heterophilic (right) datasets. In the former case, the structure of the node features or labels is compatible with that of the graph (i.e., a node is ‚Äúsimilar‚Äù to its neighbours). Similar nodes denotes by matching colour.</figcaption></figure><h3 id="gnns-are-hard-to-interpret">GNNs are hard to interpret</h3><p>Several recent works try to mitigate this issue by providing interpretation methods for GNN-based models. These can take ¬†the form of compact subgraph structures, or a small subset of node features that have a crucial role in GNN&#39;s prediction [36]. Restricting generic message-passing functions helps ruling out implausible outputs and ensures that what the GNN learns suits domain-specific applications. In particular, it is possible to endow message passing with additional ‚Äúinternal‚Äù data symmetries that better fit the underlying problem [37]. A perfect example is E(3)-equivariant message passing that correctly treats the atomic coordinates in molecular graphs and has recently contributed to the breakthroughs ¬†in protein structure prediction with architectures such as AlphaFold [38] and RosettaFold [39]. </p><p>Another example is the work coauthored by Miles and Kyle Cranmer (no family relation) [40]. In this work, message-passing functions learned on a multi-body dynamical system were replaced by symbolic formulae. Allowing the model to learn physical equations. Finally, attempts were made to link GNNs to causal inference [41]. Here one tries to build a graph explaining the causal relations between different variables. Overall, this is still a research domain in its infancy, and much remains to be discovered. </p><figure><img src="https://lh5.googleusercontent.com/X6Kqworg5YmacA-QKzPJUPDPUjty1Nm-MmEZ-c3GC4xklN1LlVCIPiBfD6Pvxa0IOZgRkyd4mD5LtjF7nTny0MDLCoNEgQgXyh5x7zMcv822l2lmGkJPb_y6Ks6UtN4tv3ExwU3K" alt=""/><figcaption>Different interpretable GNN models (left-to-right): Graph Explainer [36] (showing the chemical structure responsible for mutagenicity), latent graph learning [21], and equivariant message passing.¬†</figcaption></figure><h3 id="lack-of-gnn-specific-hardware">Lack of GNN specific hardware</h3><p>The majority of today&#39;s GNNs rely on GPU implementation and tacitly assume that the data fits into memory. This assumption is often wrong when dealing with large graphs such as biological and social networks. In these cases, understanding the limitations of the underlying hardware such as different bandwidth and latency of the memory hierarchy is of crucial importance.. There might be an order of magnitude difference in the cost of message passing (in terms of latency and power, for example) between two nodes in the same physical memory and two nodes on different chips. Making GNNs friendly to existing hardware is an important and frequently overlooked problem. Developing novel graph-centric hardware is an even bigger challenge, given the time and effort it takes to design a new chip, and the speed at which machine learning state-of-the-art moves.</p><h2 id="reimagining-gnns-as-continuous-physical-processes">Reimagining GNNs as continuous physical processes </h2><p>An emerging and promising alternative to current GNN paradigms ¬†are continuous learning models that are inspired by physical systems and open up a new trove of tools from the fields of differential geometry, algebraic topology, and differential equations so far largely unexplored in graph ML. </p><p>A plethora of different physically-inspired processes can be drawn from both classical and quantum systems. In a series of papers [42‚Äì44], we have shown that many existing GNNs can be <a href="https://towardsdatascience.com/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774?sk=cf541fa43f94587bfa81454a98533e00">related to diffusion processes</a>, which are probably the most natural way of propagating information, but other more exotic ways, such as systems of coupled oscillators [45] are also possible and may offer certain advantages. </p><figure><img src="https://lh3.googleusercontent.com/5-44BQzjIGY504arSxQbfJs9cce1R9wnCcQn9pF5oRbJTa3pL6a2ewhbCSHZFFP8YxQiPUWQFSG8f6PpAi4kwmPeVq2OBGC6uigEyYAfZNuGIuQjOaxxdGqQx-2DC5_4e3HtrsFp" alt=""/><figcaption>The dynamics of a system of graph-coupled oscillators [45].</figcaption></figure><p>A continuous system can be discretised both in time and space. The spatial discretisation takes the form of a graph connecting nearby points on the continuous domain, and can change throughout both time and space. Such a learning paradigm is a radical departure from the traditional Weisfeiler-Lehman scheme that is rigidly bound by the assumption of the underlying input graph. More importantly, it brings a new set of tools allowing, at least in principle, to address important questions that are presently out of reach of the existing graph-theoretical frameworks.</p><figure><img src="https://lh4.googleusercontent.com/R4xo5hS-yBhzN71I3SE-G5QImP7VzshwvdviCYdHGB-52GW6epPdj9EJEzdrY85ENO4EotB-dnEkNkDzMVB927oCt809Yx26FK0iMIsQcy8HMwBKiqJ4KnL1kgqU9xAGhGV9dR8b" alt=""/><figcaption>Different discretisations of the 2D Laplacian operator.¬†</figcaption></figure><h3 id="learning-as-an-optimal-control-problem">Learning as an optimal control problem</h3><p>The space of all possible states of a ¬†process at a given time can be regarded as a <em>hypothesis class</em> of functions to be represented. Framed this way, learning is posed as an optimal control problem [46]. The goal becomes ¬†to control the process, by choosing a trajectory in the parameter space, to reach a certain desirable state. Using optimal control terminology, expressive power can be formulated as how well a given function can be reached by a trajectory in the parameter space. Efficiency is related to how long it takes to reach a certain state. Finally, generalisation is related to the stability of this process. <br/></p><figure><img src="https://lh3.googleusercontent.com/_2Yq8wCarEaphvllNPu_2eIIDuMePX8Re_tZtxN7C8loz3DEHj-mk-OXsTpeP7kjtXRqFuZYoGQzDOlsgfHTmUf7qmqpK_ovsUT8wO-VsBt125p6_KKwmo2KIFsWpPiuVkmGEANl" alt=""/><figcaption>Learning as a control problem: the airplane is a metaphor of a physical system whose xyz coordinates (‚Äúsystem state‚Äù) are controlled by manipulating the thrust, ailerons, and rudder (‚Äúparameter space‚Äù).</figcaption></figure><h3 id="gnns-can-be-derived-from-discretised-differential-equations">GNNs can be derived from discretised differential equations </h3><p>The behaviour of physical systems is typically governed by differential equations whose solutions produce the state of the system. In some cases, such solutions can be derived in a closed form [47], but more generally, one has to resort to numerical solutions based on appropriate discretisations. The wealth of different iterative solvers accumulated in the numerical analysis literature after over a century of research offers potentially completely new architectures for deep learning on graphs. ¬† </p><p>For example, we showed in [42] that attentional flavours of GNNs can be interpreted as discretised diffusion PDEs with learnable diffusivity solved using explicit numerical schemes. In this case, each iteration of the solver (discrete steps in time) corresponds to a layer of a GNN. More sophisticated solvers, like using adaptive step size or multi-step schemes do not currently have an immediate analogy with current ¬†GNN architectures, and could lead to future architectural insights. Implicit schemes, requiring the solution of a linear system at each iteration, can be interpreted as ‚Äúmulti-hop‚Äù filters [48]. Furthermore, numerical schemes come with stability and convergence guarantees, providing conditions when they work and an understanding when they fail [49].</p><h3 id="numerical-solvers-can-be-hardware-friendly">Numerical solvers can be hardware-friendly</h3><p>Iterative solvers are older than computers, and from the very beginning of the digital computers era, the former had to be aware of the latter. Large-scale problems in scientific computing often have to be solved on clusters of computers, where such issues are of crucial importance. </p><p>The continuous approach to deep learning on graphs allows discretising the underlying differential equations in ways that are compatible with the hardware on which they are simulated, potentially leveraging vast literature from the supercomputing community such as <a href="https://en.wikipedia.org/wiki/Domain_decomposition_methods">domain decomposition techniques</a> [50]. In particular, graph rewiring and adaptive iterative solvers can be made aware of memory hierarchy. They can, for example, perform fewer infrequent steps of information transfer across nodes in different physical locations and more frequent steps across nodes in the same physical memory. </p><p>Analysing the limit cases of such flows is a standard exercise that provides deep insight on the behaviour of the model that would be difficult to gain otherwise. For example, in our recent paper [44] we showed that traditional GNNs (analogous to isotropic diffusion equations) necessarily lead to oversmoothing and have separation power only under the assumption of homophily; better separation power can be achieved by using additional structure on the graph (a cellular sheaf) [52]. In another recent paper [45], we showed that an oscillatory system avoids oversmoothing in the limit. Such results can explain why certain undesirable phenomena occur in some GNN architectures, and how to design architectures to avoid them. Furthermore, linking the limit case of the flow to separation reveals bounds on the expressive power of the model. </p><h3 id="graphs-can-be-endowed-with-richer-structures">Graphs can be endowed with richer structures</h3><p>We already mentioned that graphs may be ‚Äútoo poor‚Äù (i.e., fail to capture more complex phenomena like non-pairwise relations) and ‚Äútoo rich‚Äù (i.e., hard to be represented in a homogeneous space) at the same time. A way to deal with the former problem is to ‚Äúenrich‚Äù the graph with additional structures. A good example is molecular graphs containing rings (cycles in graph terminology) that chemists regard as single entities rather than just a collection of atoms and bonds (nodes and edges). </p><p>In a series of works [53‚Äì54] we showed that graphs can be &#34;lifted&#34; to higher-dimensional topological structures called<em> simplicial-</em> and <em>cellular complexes</em>, on which one can design a more complex message passing scheme allowing to propagate information not only between nodes like in traditional GNNs, but also between structures like rings (&#34;cells&#34;). Appropriate construction of the lifting operation makes such models more expressive than the traditional Weisfeiler-Lehman test [55]. </p><figure><img src="https://lh6.googleusercontent.com/BW7kYnKeKQd5g431DLsTknYoWRF0rjp5IelnWJz66Skq5BZGWE8oN1KV5NyD8Dol9-pWQ70Clwp3NqhZ47qAB2zCsCHDxzpRGWE4N3lawtPbd5pscGA3TOJ7tmhHslB1oKfLsgxd" alt=""/><figcaption>Lifting graphs to higher topological structures</figcaption></figure><p>In another recent paper [44], we showed that graphs can be equipped with an additional &#34;geometric&#34; structure called a <em>cellular sheaf</em> by assigning vector spaces and linear maps to nodes and edges. Traditional GNNs implicitly assume a graph with a trivial underlying sheaf, which is reflected in the properties of the associated diffusion equation and the structure of the graph Laplacian operator. Making the sheaf non-trivial (and learnable from the data) produces richer diffusion processes and gives greater control over their asymptotic behaviour compared to traditional GNNs. For example, diffusion equations on an appropriately selected sheaf structure can separate in the limit multiple classes even in heterophilic settings [52]. ¬†</p><p>From the geometric standpoint, the sheaf structure is analogous to a <em>connection</em>, a notion from differential geometry describing the parallel transport of vectors on manifolds [56]. In this sense, one can regard sheaf learning as a way of evolving the &#34;geometric&#34; structure of the graph depending on the downstream task. We show that interesting insights can be gained by restricting the structure group of the sheaf. Using the jargon of algebraic topology, <em>restriction maps</em> can be applied to the special orthogonal group, allowing the node feature vectors to rotate only.</p><figure><img src="https://lh4.googleusercontent.com/z6NSKtjGWG_G8pFIxlH_fgr3yjRjN5DBu_vOcfGVaQ1BUcwMX4Xe82whrT79ksH29--hiUiFVY4G4vp5eWVMQf-b-xFApQ85GIBwNdke6ktWW-fhfzXy3Eqv7B3IIoJdrV_JKX3p" alt=""/><figcaption>Cellular sheaf built upon a graph consists of vector spaces attached to each node and linear ‚Äúrestriction maps‚Äù connecting between them. In geometric terms, this can be thought of as endowing the graph with ‚Äúgeometry‚Äù, and restriction maps analogous to a connection. <span>‚Äå‚Äå</span></figcaption></figure><p>Another example of geometric structures on graphs are discrete analogues of <em>curvature</em>. A standard tool from the field of differential geometry to describe the local behaviour of manifolds. In [18], we showed that negative graph Ricci curvature produces information flow bottlenecks on the graph and resulting in the oversquashing phenomenon in GNNs [14]. Discrete Ricci curvature accounts for higher-order structures (triangles and rectangles) that are important in many applications. Such structures are &#34;too rich&#34; for traditional graph embeddings, since graphs are heterogeneous (have non-constant curvature), whereas the spaces commonly used for embeddings, even non-Euclidean ones, are homogeneous (constant curvature). </p><p>In another recent paper [57], we showed a construction of <em>heterogeneous</em> embedding spaces with controllable Ricci curvature that can be chosen to match that of the graph, allowing to better represent not only the neighbourhood (distance) structure but also the higher-order structures such as triangles and rectangles. Such spaces are constructed as products of homogeneous and rotationally-symmetric manifolds and allow for efficient optimisation using standard Riemannian gradient descent methods [58]. </p><figure><img src="https://lh6.googleusercontent.com/howr_a1u_v4gyc8fPubg8pc9fzGCY5HKyjjXIKEKPJBnsNGK0AXblkvYTb7TImnPMXUwz6ELj0BHznzTyKn6QetdY-9IyPQA6aoj99sN551P1M8kIpC3cJZwQF2t9iBC5-_DTVKQ" alt=""/><figcaption>Left: space-forms (sphere, plane, and hyperboloid) with constant positive, zero, and negative Ricci curvature and their graph analogies (clique, grid, and tree) with the corresponding discrete Forman curvature. Middle: a product manifold (cylinder can be thought of as a product of a circle and a line). Right: a heterogeneous manifold with varying curvature and its graph analogy. See [57] for details. Figure: Francesco di Giovanni.</figcaption></figure><h3 id="positional-encoding-can-be-regarded-as-part-of-the-domain">Positional encoding can be regarded as part of the domain</h3><p>Thinking of ¬†a graph as a discretisation of a continuous manifold, one can consider the node positional and feature coordinates as different dimensions of the same space. The graph in this case can be used to represent a discrete analogue of a Riemannian metric induced by such an embedding. ¬†The harmonic energy associated with the embedding is a non-Euclidean extension of the Dirichlet energy, known as the Polyakov functional in string theory [59]. The gradient flow of this energy is a diffusion-type equation that evolves both the positional and feature coordinates [43]. Building a graph upon the positions of the nodes is a form of task-specific graph rewiring that changes across the iterations (layers) of the diffusion. </p><figure><img src="https://lh3.googleusercontent.com/h7wk00bwB_TxNR_Ap723BwGd7e4_5IAQr49Rc0OgIWgycBwbk2rQpdWGDnAITP8IkEcWE-Ma3Vey8m9HkBXYZjh02HaVuIdmyO0TSHr6cSe_t-I8LI75ZLP0qWii3CL_qV1I_4SD" alt=""/><figcaption>Evolution of positional and feature components of the Cora graph by the Beltrami flow with rewiring (colours represent feature vectors). Animation: James Rowbottom.¬†</figcaption></figure><h3 id="evolution-of-the-domain-replaces-graph-rewiring">Evolution of the domain replaces graph rewiring</h3><p>Diffusion equations can also be applied to the connectivity of the graph as a pre-processing step aimed at improving information flow and avoiding oversquashing. Klicpera et al. [17] proposed one such algorithm based on personalised page rank, a form of graph diffusion embedding. In [18], we analysed this process pointing to its problematic behaviour in heterogeneous settings and proposed an alternative graph rewiring scheme by a process inspired by the Ricci flow. Such a rewiring reduces the effect of negatively-curved edges responsible for graph bottlenecks. <em>Ricci flows</em> are geometric evolution equations for manifolds very roughly resembling the diffusion equations applied to the Riemannian metric, and are a popular and powerful technique in differential geometry, including the celebrated proof of the Poincar√© conjecture. More generally, instead of performing graph rewiring as a pre-processing step, one can consider a coupled system of evolution processes: one evolving the features and another one evolving the domain (graph). </p><figure><img src="https://lh3.googleusercontent.com/9rtrNzg9UNbstZQ-fCzAkKVggGaGXTwCkv1OPmYI7frl4-a-XWr3ceneaGEoTvpyCmTcPdLjvXVFAyPcWgbbJmuSRIhISUDAyM-ANdO_xUD7tOQ37q8Jlf9U2B1HwOgZhzQTAmY7" alt=""/><figcaption>Top: a dumbbell-shaped Riemannian manifold with a negatively-curved bottleneck (negative curvature coded by cold colours) undergoing curvature-based metric evolution becomes ‚Äúrounder‚Äù and less ‚Äúbottlenecked‚Äù. Bottom: an analogous process of curvature-based graph rewiring, reducing the bottlenecks and making the graph friendlier for message passing. See <a href="https://towardsdatascience.com/over-squashing-bottlenecks-and-graph-ricci-curvature-c238b7169e16?sk=f5cf01cbd57b4fee8fb3a89d447e56a9">our blog post</a> for additional details.¬†</figcaption></figure><p>Is this really new? One of the traps we must avoid falling into is basing new models on fancy mathematical formalism, with the underlying models still doing roughly what the previous generation of GNNs did. For example, one can argue that sheaf diffusion studied in [44] is a special case of message passing. The insightful theoretical results that were made possible with tools from differential geometry [18,43] and algebraic topology [44,53‚Äì54] suggest that it is unlikely. However, how far can the new theoretical framework take us, and whether it will be able to address the currently unanswered questions in the field is still an open question. </p><p><strong>Will these methods be actually used in practice? </strong>For practitioners, a crucial question would be whether these methods will result in better architectures, or remain a theoretical apparatus detached from real-world applications used solely to prove theorems. ¬†We believe that theoretical insights obtained with topological and geometric tools would allow for better decision making regarding architectural choices and innovation. Our framework could help answer central questions about how and when to restrict message passing functions. </p><p>However, one can hypothetically use ¬†other physical systems [62] or computing paradigms such as analogue electronics or photonics [63]. Mathematically, the solution of the underlying differential equations may sometimes be given in a closed form: for example, the solution of the isotropic diffusion equation is a convolution with a Gaussian kernel. In this case, the influence of the ‚Äúneighbours‚Äù is absorbed within the structure of the kernel and no actual ‚Äúmessage passing‚Äù happens. Therefore, the continuous nature of this model calls for a more appropriate term, such as &#34;spatial coupling&#34; or &#34;information propagation&#34; in the absence of better ideas at this time. Nonetheless,, message passing, as computational primitive, is and will likely remain useful, when applied synergistically with additional paradigms such as those presented here. </p><figure><img src="https://lh5.googleusercontent.com/VpH6I5Dxkj5yYu32FyGCRN3o3lLx43mRqRXJY26l47xcKhl70GDe5vO0r6LMpy_Fo33egQ_PQVYVCpHYgh52P-Nt5N7tSAqAHH0kCZefyt4EVp6J0Y-d6pSr2VN5NUbSxZiMehon" alt=""/><figcaption>Deep learning by backpropagation through real physical systems. Image: Wright et al [61].<span>‚Äå‚Äå</span></figcaption></figure><hr/><h3 id="acknowledgments">Acknowledgments</h3><p>I am grateful to Dominique Beaini, Cristian Bodnar, Giorgos Bouritsas, Ben Chamberlain, Xiaowen Dong, Francesco Di Giovanni, Nils Hammerla, Haggai Maron, Sid Mishra, Christopher Morris, James Rowbottom, and Petar Veliƒçkoviƒá for very fruitful discussions and comments, some of which were so substantial that amounted to ‚Äúcollective editing‚Äù. </p><p>Michael Bronstein is the DeepMind Professor of AI at the University of Oxford and Head of Graph Learning Research at Twitter. He was previously a professor at Imperial College London and held visiting appointments at Stanford, MIT, and Harvard, and has also been affiliated with three Institutes for Advanced Study (at TUM as a Rudolf Diesel Fellow (2017-2019), at Harvard as a Radcliffe fellow (2017-2018), and at Princeton as a short-time scholar (2020)). Michael received his PhD from the Technion in 2007. He is the recipient of the Royal Society Wolfson Research Merit Award, Royal Academy of Engineering Silver Medal, five ERC grants, two Google Faculty Research Awards, and two Amazon AWS ML Research Awards. He is a Member of the Academia Europaea, Fellow of IEEE, IAPR, BCS, and ELLIS, ACM Distinguished Speaker, and World Economic Forum Young Scientist. In addition to his academic career, Michael is a serial entrepreneur and founder of multiple startup companies, including Novafora, Invision (acquired by Intel in 2012), Videocites, and Fabula AI (acquired by Twitter in 2019).</p><h3 id="citation">Citation</h3><p>For attribution in academic contexts or books, please cite this work as</p><blockquote>Michael Bronstein, &#34;Beyond Message Passing, a Physics-Inspired Paradigm for Graph Neural Networks&#34;, The Gradient, 2022.</blockquote><p><em>BibTeX citation:</em></p><blockquote>@article{michaelbeyond2022,</blockquote><p>[1] See e.g. <a href="https://twitter.com/chrmanning/status/1332725903470706688?s=20&amp;t=jkQ8nc6YO5JzLlU4Lj3WVw">ICLR 2021</a> statistics. </p><p>[2] I am counting Transformers as an <a href="https://thegradient.pub/transformers-are-graph-neural-networks/">instance of GNNs</a>.</p><p>[3] I use the term ‚ÄúGNN‚Äù to refer to an arbitrary graph neural network architecture and ‚ÄúMPNN‚Äù for architectures based on local message passing. ¬†</p><p>[4] Typical representatives of the convolutional flavour are ‚Äúspectral‚Äù graph neural networks that emerged from the domain of graph signal processing such as M. Defferrard et al. <a href="https://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf">Convolutional neural networks on graphs with fast localized spectral filtering</a> (2016) NIPS and T. Kipf and M. Welling, <a href="https://arxiv.org/pdf/1609.02907.pdf">Semi-supervised classification with graph convolutional networks</a> (2017) ICLR. The attentional flavour was introduced by Petar Veliƒçkoviƒá in [16], though one can consider the Monet architecture developed by F. Monti et al., Geometric deep learning on graphs and manifolds using mixture model CNNs (2017) CVPR, as a form of attention on positional coordinates. A general form of message passing and the name MPNN were used by J. Gilmer et al., Neural message passing for quantum chemistry (2017) ICML.</p><p>The naming of the flavours, which we adopted in our ‚Äúproto-book‚Äù M. M. Bronstein et al., <a href="https://arxiv.org/abs/2104.13478">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</a> (2021) arXiv:2104.13478, is due to Petar Veliƒçkoviƒá. In my earlier versions of this dichotomy (such as the <a href="https://www.dropbox.com/s/z6lh612kpphx8g8/MLSS%202019.pdf?dl=0">tutorial at MLSS Moscow 2019</a>) I used the terms <em>‚Äúlinear‚Äù</em> (where the feature computation has the form <strong>Y</strong>=<strong>AX </strong>with <strong>A</strong> typically being some form of the graph adjacency matrix), <em>‚Äúlinear feature dependent‚Äù</em> (<strong>Y</strong>=<strong>A</strong>(<strong>X</strong>)<strong>X</strong>) and <em>‚Äúnonlinear‚Äù</em> (<strong>Y</strong>=<strong>ùíú</strong>(<strong>X</strong>)). This is less general than the notation adopted in the book, as it assumes sum-based aggregation. </p><p>[5] The local aggregation function must be injective. See K. Xu et al. <a href="https://arxiv.org/abs/1810.00826">How powerful are graph neural networks?</a> (2019) ICLR. </p><p>[6] B. Weisfeiler and A. Lehman, The reduction of a graph to canonical form and the algebra which appears therein (1968). Nauchno-Technicheskaya Informatsia 2(9):12‚Äì16.</p><p>[7] See C. Morris et al., Weisfeiler and Leman go Machine Learning: The Story so far (2021) arXiv:2112.09992 for historic notes and an in-depth review.</p><p>[8] Z. Chen et al., On the equivalence between graph isomorphism testing and function approximation with GNNs (2019) NeurIPS, showed the equivalence between graph isomorphism testing and the approximation of permutation-invariant graph functions. </p><p>[9] For example, N. Dehmamy, A.-L. Barab√°si, R. Yu, <a href="https://papers.nips.cc/paper/9675-understanding-the-representation-power-of-graph-neural-networks-in-learning-graph-topology.pdf">Understanding the representation power of graph neural networks in learning graph topology</a> (2019) NeurIPS shows that learning certain functions on graphs (graph moments of certain order) requires GNNs of certain minimum depth. See also F. Geerts and J. L. Reutter, Expressiveness and Approximation Properties of Graph Neural Networks (2022) ICLR. </p><p>[10] The hierarchy of so-called ‚Äúk-WL tests‚Äù of strictly increasing power. L√°szl√≥ Babai credits the invention of the <em>k</em>-WL tests to himself with <a href="https://www.cs.toronto.edu/dcs/people-faculty-combin.html">Rudolf Mathon</a> and independently, <a href="https://en.wikipedia.org/wiki/Neil_Immerman">Neil Immerman</a> and <a href="https://en.wikipedia.org/wiki/Eric_Lander">Eric Lander</a>. See L. Babai, <a href="https://arxiv.org/abs/1512.03547">Graph isomorphism in quasipolynomial time</a> (2015), arXiv:1512.03547 p. 27.</p><p>[11] There have been several papers proposing to apply GNNs on a collection of subgraphs, including our own B. Bevilacqua et al., <a href="https://arxiv.org/abs/2110.02910">Equivariant Subgraph Aggregation Networks</a> (2021) arXiv:2110.02910. See <a href="https://towardsdatascience.com/using-subgraphs-for-more-expressive-gnns-8d06418d5ab?sk=8806ffcd9ecf74c440d40df53528c1c7">our post on this topic</a>. ¬†</p><p>[12] Structural encoding by counting local substructures was proposed in the context of GNNs by G. Bouritsas, F. Frasca et al. <a href="https://arxiv.org/abs/2006.09252">Improving graph neural network expressivity via subgraph isomorphism counting</a> (2020). arXiv:2006.09252. See also P. Barcel√≥ et al., ¬†Graph Neural Networks with Local Graph Parameters (2021) arXiv:2106.06707.</p><p>[13] A. Vaswani et al., <a href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all you need</a> (2017) NIPS.</p><p>[14] The oversquashing phenomenon is not unique to GNNs and has been previously observed also in seq2seq models. However, it can become particularly acute in graphs with exponential volume growth. See U. Alon and E. Yahav, <a href="https://arxiv.org/pdf/2006.05205.pdf">On the bottleneck of graph neural networks and its practical implications</a> (2020). arXiv:2006.05205 and our paper [14] for the description and analysis of this phenomenon. </p><p>[15] W. Hamilton et al., <a href="https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf">Inductive representation learning on large graphs</a> (2017) NIPS used neighbourhood sampling to cope with scalability.</p><p>[16] P. W. Battaglia et al., Relational inductive biases, deep learning, and graph networks (2018), arXiv:1806.01261.</p><p>[17] J. Klicpera et al., <a href="https://proceedings.neurips.cc/paper/2019/file/23c894276a2c5a16470e6a31f4618d73-Paper.pdf">Diffusion improves graph learning</a> (2019) NeurIPS used rewiring through personalised page rank embedding (a form of ‚Äúconnectivity diffusion‚Äù).</p><p>[18] J. Topping, F. Di Giovanni et al., <a href="https://arxiv.org/pdf/2111.14522.pdf">Understanding over-squashing and bottlenecks on graphs via curvature</a> (2022) ICLR. </p><p>[19] Dropout can be done on edges or nodes and also leads to better expressive power (for reasons similar to Subgraph GNNs), see e.g. Y. Rong et al., <a href="https://openreview.net/pdf?id=Hkx1qkrKPr">DropEdge: Towards deep graph convolutional networks on node classification</a> (2020) ICLR and P. A. Papp et al., <a href="https://arxiv.org/pdf/2111.06283.pdf">DropGNN: Random dropouts increase the expressiveness of Graph Neural Networks</a> (2021) arXiv:2111.06283.</p><p>[20] P. Veliƒçkoviƒá et al., <a href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a> (2018) <em>ICLR</em>.</p><p>[21] One of the first ‚Äúlatent graph learning‚Äù architectures was our work Y. Wang et al. <a href="https://arxiv.org/pdf/1801.07829.pdf">Dynamic graph CNN for learning on point clouds</a> (2019) ACM Trans. Graphics 38(5):146 on point clouds, where the initial ‚Äúpositional encoding‚Äù (xyz coordinates of the points) represent the geometry of the data, and the graph is used to propagate information between the relevant parts thereof. A more general case was considered by A. Kazi et al., <a href="https://arxiv.org/pdf/2002.04999.pdf">Differentiable Graph Module (DGM) for graph convolutional networks</a> (2020) arXiv:2002.04999, where there is no obvious data geometry and the appropriate ‚Äúpositional encoding‚Äù is learned for the task at hand. See also my <a href="https://towardsdatascience.com/manifold-learning-2-99a25eeb677d?sk=1c855a020f09b72edfa50a8aba5f24a0">blog post on the topic</a>. </p><p>[22] More on this in my <a href="https://youtu.be/w6Pw4MOzMuo">ICLR keynote talk</a>.</p><p>[23] There have been several attempts to make ‚Äúanisotropic‚Äù propagation in graphs, a recent one by D. Beaini et al., Directional Graph Networks (2020), arXiv:2010.02863 and an old one in our paper F. Monti, K. Otness, M. M. Bronstein, MotifNet: a motif-based Graph Convolutional Network for directed graphs (2018), arXiv:1802.01572. Some form of external information must be provided, e.g. gradients of the graph Laplacian eigenvectors in the case of DGNNs and some chosen substructures in the case of MotifGNNs. </p><p>[24] D. Boscaini et al., Learning shape correspondence with anisotropic convolutional neural networks (2016) NIPS. </p><p>[25] See for instance <a href="https://www.inference.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/">Ferenc Huszar‚Äôs blog post</a>.</p><p>[26] Recall the classical formulae for 2- and 3-dimensional volumes, œÄ<em>r</em>¬≤ and 4œÄ<em>r</em>¬≥/3: these are polynomials in <em>r</em>. The general formula for a 2<em>n</em>-dimensional ball is V=œÄ<em>‚Åø</em> <em>r</em>¬≤<em>‚Åø</em> / <em>n</em>!, showing an exponential dependence on <em>n</em>. </p><p>[27] Real-world graphs such as social and biological networks exhibit hierarchical structure and a power-law degree distribution that is linked to negative curvature.</p><p>[28] Q. Liu, M. Nickel, D. Kiela, Hyperbolic Graph Neural Networks (2019) NeurIPS.</p><p>[29] See e.g. M. Bogu√±√° et al., <a href="https://arxiv.org/abs/2001.03241">Network geometry</a> (2021) Nature Reviews Physics 3:114‚Äì135.</p><p>[38] J. Jumper et al., Highly accurate protein structure prediction with AlphaFold, Nature 596:583‚Äì589, 2021</p>
        </div></div>
  </body>
</html>
