<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://auerstack.substack.com/p/what-chatgpt-cant-do">Original</a>
    <h1>What ChatGPT can&#39;t do</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p>ChatGPT is truly amazing. I’ve never been one to mince words about how astonishing AI’s gains have been in the last 20 years. Machine learning’s ability to achieve human or near-human performance on well-defined tasks, enabled by (1) the voluminous amounts of data collected on the global internet and (2) continually exponential increases in processing power, went far beyond what I can recall anyone predicting in the 1990s. At the same time, there is a definite wall to these achievements—perhaps scalable, but indisputably present.</p><p>But AIs human mimicry abilities grow ever more uncanny, and the ability of ChatGPT to approximate human performance in increasing numbers of scenarios poses some serious problems even if it falls down in many more. </p><p><span>I don’t think ChatGPT can be said to </span><em>understand </em><span>what it’s talking about in any meaningful sense of the word—in the sense of being able to justify and explain itself coherently. Alas, too often, real people also can’t justify or explain themselves coherently, so to the extent that people can behave like modestly creative regurgitators of received ideas, ChatGPT looks pretty good.</span></p><p>Here I asked ChatGPT to tell a Jewish joke, and it gave a performance at least some people I know found uncannily convincing:</p><blockquote><p><strong><span>Please tell me a Jewish joke.</span><br/></strong></p></blockquote><p>I’ve been studying this stuff for long enough that I likely have a more intuitive sense of where the seams are in a performance like this than most people do. I can imagine where ChatGPT might be sourcing various phrases from and how it’s mashing them together. As a technical achievement it’s impressive—stunningly impressive in fact—but it doesn’t haunt me.</p><p>Asking ChatGPT to explain itself further undermines the illusion:</p><blockquote><p><strong>Why is that joke funny?</strong></p></blockquote><p><span>ChatGPT offers the </span><em>structure </em><span>of an explanation, but falls into tautologies and truisms. It doesn’t know what the “unexpected twist” is, and it certainly has no idea where the humor actually lies. It takes a stab by arguing that there’s some double meaning to “compromise,” and it’s not entirely wrong, but the crux that only one partner is compromising completely eludes it.</span></p><p><span>For me the seams show, and the barrier seems far wider than it does to some. To that end, here’s another example of ChatGPT really falling all over itself. There’s a phenomenon called “</span><a href="https://arxiv.org/abs/2201.11903" rel="nofollow ugc noopener">chain of thought</a><span>” where large AI language models perform much, </span><em>much</em><span> better if given prompts like “Think about it carefully and show your steps.” Mathematical word problems that trip up ChatGPT normally get conquered if such a prompt is added.</span></p><p>Except when they don’t. Normally, ChatGPT gets the question “Is the number of letters in the word ‘prime’ prime?” wrong, claiming that five isn’t prime. (In most circumstances, naturally, it does “know” and claim that five is prime.) Telling it to think carefully, however, yields this bit of tortuous reasoning: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg" width="795" height="1014" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/d539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:1014,&#34;width&#34;:795,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:&#34;No photo description available.&#34;,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true,&#34;internalRedirect&#34;:null}" alt="No photo description available." title="No photo description available." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd539c725-7783-4644-aa25-b785a9bca0e7_795x1014.jpeg 1456w" sizes="100vw" loading="lazy"/></picture></div></a><figcaption>ChatGPT trips over itself</figcaption></figure></div><p>In the same answer, ChatGPT claims 5 is and is not prime, getting the right answer before contradicting itself and saying that the right answer was the right answer to the wrong question. In particular, it produces this head-spinning sentence:</p><blockquote><p>The question asks whether the number of letters in the word &#34;prime&#34; is prime, not whether the word &#34;prime&#34; has a prime number of letters.</p></blockquote><p>I had to go over this twice to be certain this was a distinction without a difference. I couldn’t ask ChatGPT what distinction it thought it was making. ChatGPT can’t account for itself a good percentage of the time.</p><p>I hope this is enough to show that ChatGPT doesn’t actually have a grasp on the concepts behind the words it’s using. Perhaps you could argue that ChatGPT transiently understands such concepts when its neural networks happen to be conditioned in certain ways. I wouldn’t go that far, but there may be a way forward in reifying those cases where ChatGPT does get it right. Even so, I still suspect there’s a long way to go before reaching actual understanding.</p><p>And yet…a lot of the time it does not matter. If ChatGPT or similar networks can generate convincing essays and research papers enough of the time, philosophical arguments against true understanding will mean nothing. From a brutally pragmatic point of view, it will have succeeded. Convincing mimicry is good enough.</p></div></div></div></article></div></div></div>
  </body>
</html>
