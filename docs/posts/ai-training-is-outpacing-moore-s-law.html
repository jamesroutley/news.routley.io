<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://spectrum.ieee.org/ai-training-mlperf">Original</a>
    <h1>AI training is outpacing Moore’s Law</h1>
    
    <div id="readability-page-1" class="page"><div><template>
                    <p>
	Deep neural networks (DNNs), systems that learn how to respond to new queries when they&#39;re trained with the right answers to very similar queries, have enabled these new capabilities. DNNs are the primary driver behind the rapidly growing global market for AI hardware, software, and services, valued at 
	<a href="https://www.idc.com/getdoc.jsp?containerId=prUS47482321" rel="noopener noreferrer" target="_blank">US $327.5 billion this year and expected to pass $500 billion in 2024, according to the International Data Corporation</a>.
</p><p>
	<a href="https://www.ibm.com/cloud/learn/convolutional-neural-networks" rel="noopener noreferrer" target="_blank">Convolutional neural networks</a> first fueled this revolution by providing superhuman image-recognition capabilities. In the last decade, new DNN models for natural-language processing, speech recognition, reinforcement learning, and recommendation systems have enabled many other commercial applications.
</p><p>
	But it&#39;s not just the number of applications that&#39;s growing. The size of the networks and the data they need are growing, too. DNNs are inherently scalable—they provide more reliable answers as they get bigger and as you train them with more data. But doing so comes at a cost. The number of computing operations needed to train the best DNN models 
	<a href="https://openai.com/blog/ai-and-compute/" rel="noopener noreferrer" target="_blank">grew 1 billionfold</a> between 2010 and 2018, meaning a huge increase in energy consumption And while each use of an already-trained DNN model on new data—termed inference—requires much less computing, and therefore less energy, than the training itself, the sheer volume of such inference calculations is enormous and increasing. If it&#39;s to continue to change people&#39;s lives, AI is going to have to get more efficient.
</p><p>
	We think changing from digital to analog computation might be what&#39;s needed. Using nonvolatile memory devices and two fundamental physical laws of electrical engineering, simple circuits can implement a version of deep learning&#39;s most basic calculations that requires mere thousandths of a trillionth of a joule (a femtojoule). There&#39;s a great deal of engineering to do before this tech can take on complex AIs, but we&#39;ve already made great strides and mapped out a path forward.
</p>
                    


<div id="rebelltitem3" data-id="3" data-is-image="False" data-href="https://spectrum.ieee.org/analog-ai/particle-3" data-published-at="1637002553" data-basename="particle-3" data-post-id="2655552523"><p>The biggest time and energy costs in most computers occur when lots of data has to move between external memory and computational resources such as CPUs and GPUs. This is the &#34;von Neumann bottleneck,&#34; named after the <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture" target="_blank">classic computer architecture</a> that separates memory and logic. One way to greatly reduce the power needed for deep learning is to avoid moving the data—to do the computation out where the data is stored.</p><p>DNNs are composed of layers of artificial neurons. Each layer of neurons drives the output of those in the next layer according to a pair of values—the neuron&#39;s &#34;activation&#34; and the synaptic &#34;weight&#34; of the connection to the next neuron.</p><p>Most DNN computation is made up of what are called vector-matrix-multiply (VMM) operations—in which a vector (a one-dimensional array of numbers) is multiplied by a two-dimensional array. At the circuit level these are composed of many multiply-accumulate (MAC) operations. For each downstream neuron, all the upstream activations must be multiplied by the corresponding weights, and these contributions are then summed.</p><p>Most useful neural networks are too large to be stored within a processor&#39;s internal memory, so weights must be brought in from external memory as each layer of the network is computed, each time subjecting the calculations to the dreaded von Neumann bottleneck. This leads digital compute hardware to favor DNNs that move fewer weights in from memory and then aggressively reuse these weights.</p><p>A radical new approach to energy-efficient DNN hardware occurred to us at <a href="https://research.ibm.com/" target="_blank">IBM Research</a> back in 2014. Together with other investigators, we had been working on <a href="https://ieeexplore.ieee.org/abstract/document/5388605" target="_blank">crossbar arrays of nonvolatile memory (NVM) devices</a>. Crossbar arrays are constructs where devices, memory cells for example, are built in the vertical space between two perpendicular sets of horizontal conductors, the so-called bitlines and the wordlines. We realized that, with a few slight adaptations, our memory systems would be ideal for DNN computations, particularly those for which existing weight-reuse tricks work poorly. We refer to this opportunity as &#34;<a href="https://ieeexplore.ieee.org/abstract/document/8490883" target="_blank">analog AI</a>,&#34; although other researchers doing similar work also use terms like &#34;processing-in-memory&#34; or &#34;compute-in-memory.&#34;</p><p>There are several varieties of NVM, and each stores data differently. But data is retrieved from all of them by measuring the device&#39;s resistance (or, equivalently, its inverse—conductance). Magnetoresistive RAM (MRAM) uses electron spins, and flash memory uses trapped charge. <a href="https://ieeexplore.ieee.org/abstract/document/6193402" target="_blank">Resistive RAM (RRAM)</a> devices store data by creating and later disrupting conductive filamentary defects within a tiny metal-insulator-metal device. <a href="https://avs.scitation.org/doi/abs/10.1116/1.3301579" target="_blank">Phase-change memory (PCM)</a> uses heat to induce rapid and reversible transitions between a high-conductivity crystalline phase and a low-conductivity amorphous phase.</p><p>Flash, RRAM, and PCM offer the low- and high-resistance states needed for conventional digital data storage, plus the intermediate resistances needed for analog AI. But only RRAM and PCM can be readily placed in a crossbar array built in the wiring above silicon transistors in high-performance logic, to minimize the distance between memory and logic.</p><p>We organize these NVM memory cells in a two-dimensional array, or &#34;tile.&#34; Included on the tile are transistors or other devices that control the reading and writing of the NVM devices. For memory applications, a read voltage addressed to one row (the wordline) creates currents proportional to the NVM&#39;s resistance that can be detected on the columns (the bitlines) at the edge of the array, retrieving the stored data.</p><p>To make such a tile part of a DNN, each row is driven with a voltage for a duration that encodes the activation value of one upstream neuron. Each NVM device along the row encodes one synaptic weight with its conductance. The resulting read current is effectively performing, through Ohm&#39;s Law (in this case expressed as &#34;current equals voltage times conductance&#34;), the multiplication of excitation and weight. The individual currents on each bitline then add together according to Kirchhoff&#39;s Current Law. The charge generated by those currents is integrated over time on a capacitor, producing the result of the MAC operation.</p><p>These same analog in-memory summation techniques can also be performed using flash and even SRAM cells, which can be made to store multiple bits but not analog conductances. But we can&#39;t use Ohm&#39;s Law for the multiplication step. Instead, we use a technique that can accommodate the one- or two-bit dynamic range of these memory devices. However, this technique is highly sensitive to noise, so we at IBM have stuck to analog AI based on PCM and RRAM.</p><p>Unlike conductances, DNN weights and activations can be either positive or negative. To implement signed weights, we use a pair of current paths—one adding charge to the capacitor, the other subtracting. To implement signed excitations, we allow each row of devices to swap which of these paths it connects with, as needed.</p></div>



<p><strong data-redactor-tag="strong" data-verified="redactor">​Phase-change memory</strong>&#39;s conductance is set by the transition between a crystalline and an amorphous state in a chalcogenide glass. In resistive RAM, conductance depends on the creation and destruction of conductive filaments in an insulator. </p>



<p><img id="afb92" data-rm-shortcode-id="7f47d752d1cb1218728d55da104c9c52" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="data:image/svg+xml,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; viewBox=&#39;0 0 752 835&#39;%3E%3C/svg%3E" data-runner-src="https://spectrum.ieee.org/media-library/three-rows-of-color-with-the-middle-tan-layer-having-white-dots-labeled-u201cvacancy-u201d.png?id=27986575&amp;width=980" width="752" height="835" alt="Three rows of color with the middle tan layer having white dots labeled \u201cVacancy.\u201d"/></p>

<p>In <strong>resistive RAM</strong>, conductance depends on the creation and destruction of conductive filaments in an insulator. </p>



<p><img id="f1bc0" data-rm-shortcode-id="e58a6e927892968d8b680f8a108092cc" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="data:image/svg+xml,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; viewBox=&#39;0 0 1207 838&#39;%3E%3C/svg%3E" data-runner-src="https://spectrum.ieee.org/media-library/rows-of-colors-with-red-dots.png?id=27986587&amp;width=980" width="1207" height="838" alt="Rows of colors with red dots."/></p>

<p><strong>Flash memory</strong> stores data as charge trapped in a &#34;floating gate.&#34; The presence or absence of that charge modifies conductances across the device. </p>



<p><img id="8ad16" data-rm-shortcode-id="9ceed60db1b206719e40f6af438361dc" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="data:image/svg+xml,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; viewBox=&#39;0 0 934 838&#39;%3E%3C/svg%3E" data-runner-src="https://spectrum.ieee.org/media-library/rows-of-colors-with-plus-and-minus-icons-and-a-label-that-says-u201celectrochemical-ram-u201d.png?id=27986603&amp;width=980" width="934" height="838" alt="Rows of colors with plus and minus icons and a label that says \u201cElectrochemical RAM\u201d"/></p>

<p><strong>Electrochemical RAM</strong> acts like a miniature battery. Pulses of voltage on a gate electrode modulate the conductance between the other two terminals by the exchange of ions through a solid electrolyte.</p>

<div id="rebelltitem5" data-id="5" data-is-image="False" data-href="https://spectrum.ieee.org/analog-ai/particle-5" data-published-at="1637002553" data-basename="particle-5" data-post-id="2655552523"><p>With each column performing one MAC operation, the tile does an entire vector-matrix multiplication in parallel. For a tile with 1,024 × 1,024 weights, this is 1 million MACs at once.</p><p>In systems we&#39;ve designed, we expect that all these calculations can take as little as 32 nanoseconds. Because each MAC performs a computation equivalent to that of two digital operations (one multiply followed by one add), performing these 1 million analog MACs every 32 nanoseconds represents 65 trillion operations per second.</p><p><a href="https://ieeexplore.ieee.org/document/9073978/" target="_blank">We&#39;ve built tiles</a> that manage this feat using just 36 femtojoules of energy per operation, the equivalent of 28 trillion operations per joule. Our latest tile designs reduce this figure to less than 10 fJ, making them 100 times as efficient as commercially available hardware and 10 times better than the system-level energy efficiency of the <a href="https://ieeexplore.ieee.org/document/9365791" target="_blank">latest custom digital accelerators</a>, even those that aggressively sacrifice precision for energy efficiency.</p><p>It&#39;s been important for us to make this per-tile energy efficiency high, because a full system consumes energy on other tasks as well, such as moving activation values and supporting digital circuitry.</p><p>There are significant challenges to overcome for this analog-AI approach to really take off. First, deep neural networks, by definition, have multiple layers. To cascade multiple layers, we must process the VMM tile&#39;s output through an artificial neuron&#39;s activation—a nonlinear function—and convey it to the next tile. The nonlinearity could potentially be performed with analog circuits and the results communicated in the duration form needed for the next layer, but most networks require other operations beyond a simple cascade of VMMs. That means we need efficient analog-to-digital conversion (ADC) and modest amounts of parallel digital compute between the tiles. Novel, high-efficiency ADCs can help keep these circuits from affecting the overall efficiency too much. Recently, we unveiled a <a href="https://ieeexplore.ieee.org/document/9508706" target="_blank">high-performance PCM-based tile</a> using a new kind of ADC that helped the tile achieve better than 10 trillion operations per watt.</p><p>A second challenge, which has to do with the behavior of NVM devices, is more troublesome. Digital DNNs have proven accurate even when their weights are described with fairly low-precision numbers. The 32-bit floating-point numbers that CPUs often calculate with are overkill for DNNs, which <a href="https://spectrum.ieee.org/ibms-new-doitall-deep-learning-chip" target="_self">usually work just fine and with less energy</a> when using 8-bit floating-point values or even 4-bit integers. This provides hope for analog computation, so long as we can maintain a similar precision.</p><p>Given the importance of conductance precision, writing conductance values to NVM devices to <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/aelm.201900026" target="_blank">represent weights in an analog neural network</a> needs to be done slowly and carefully. Compared with traditional memories, such as SRAM and DRAM, PCM and RRAM are already slower to program and wear out after fewer programming cycles. Fortunately, for inference, weights don&#39;t need to be frequently reprogrammed. So analog AI can use time-consuming write-verification techniques to boost the precision of programming RRAM and PCM devices without any concern about wearing the devices out.</p><p>That boost is much needed because nonvolatile memories have an inherent level of programming noise. RRAM&#39;s conductivity depends on the movement of just a few atoms to form filaments. PCM&#39;s conductivity depends on the random formation of grains in the polycrystalline material. In both, this randomness poses challenges for writing, verifying, and reading values. Further, in most NVMs, conductances change with temperature and with time, as the amorphous phase structure in a PCM device drifts, or the filament in an RRAM relaxes, or the trapped charge in a flash memory cell leaks away.</p><p>There are some ways to finesse this problem. Significant improvements in weight programming can be obtained by using two conductance pairs. Here, one pair holds most of the signal, while the other pair is used to correct for programming errors on the main pair. Noise is reduced because it gets averaged out across more devices.</p><p>We tested this approach recently in a <a href="https://ieeexplore.ieee.org/document/9566604" target="_blank" rel="noopener noreferrer">multitile PCM-based chip</a>, using both one and two conductance pairs per weight. With it, we demonstrated excellent accuracy on several DNNs, even on a recurrent neural network, a type that&#39;s typically sensitive to weight programming errors.</p></div>





                    <p>
	Different techniques can help ameliorate noise in reading and drift effects. But because drift is predictable, perhaps the simplest is to amplify the signal during a read with a time-dependent gain that can offset much of the error. Another approach is to use the same techniques that have been developed to train DNNs for low-precision digital inference. These adjust the neural-network model to match the noise limitations of the underlying hardware.
	<br/>
</p><p>
	 As we mentioned, networks are becoming larger. In a digital system, if the network doesn&#39;t fit on your accelerator, you bring in the weights for each layer of the DNN from external memory chips. But NVM&#39;s writing limitations make that a poor decision. Instead, multiple analog AI chips should be ganged together, with each passing the intermediate results of a partial network from one chip to the next. This scheme incurs some additional communication latency and energy, but it&#39;s far less of a penalty than moving the weights themselves.
</p><p>
	<strong>Until now,</strong> we&#39;ve only been talking about inference—where an already-trained neural network acts on novel data. But there are also opportunities for analog AI to help train DNNs.<br/>
</p><p>
	DNNs are trained using the backpropagation algorithm. This combines the usual forward inference operation with two other important steps—error backpropagation and weight update. Error backpropagation is like running inference in reverse, moving from the last layer of the network back to the first layer; weight update then combines information from the original forward inference run with these backpropagated errors to adjust the network weights in a way that makes the model more accurate.
</p><p>
	The backpropagation step can be done in place on the tiles but in the opposite manner of inferencing—applying voltages to the columns and integrating current along rows. Weight update is then performed by driving the rows with the original activation data from the forward inference, while driving the columns with the error signals produced during backpropagation.
</p><p>
	Training involves numerous small weight increases and decreases that must cancel out properly. That&#39;s difficult for two reasons. First, recall that NVM devices wear out with too much programming. Second, the same voltage pulse applied with opposite polarity to an NVM may not change the cell&#39;s conductance by the same amount; its response is asymmetric. But symmetric behavior is critical for backpropagation to produce accurate networks. This is only made more challenging because the magnitude of the conductance changes needed for training approaches the level of inherent randomness of the materials in the NVMs.
</p><p>
	There are several approaches that can help here. For example, there are various ways to aggregate weight updates across multiple training examples, and then transfer these updates onto NVM devices periodically during training. A novel algorithm we developed at IBM, called Tiki-Taka, uses such techniques to train DNNs successfully even with highly asymmetric RRAM devices. Finally, we are developing a device called electrochemical random-access memory (ECRAM) that can offer not just symmetric but highly linear and gradual conductance updates.
</p><p>
	<strong>The success of</strong> analog AI will depend on achieving high density, high throughput, low latency, and high energy efficiency—simultaneously. Density depends on how tightly the NVMs can be integrated into the wiring above a chip&#39;s transistors. Energy efficiency at the level of the tiles will be limited by the circuitry used for analog-to-digital conversion.<br/>
</p><p>
	But even as these factors improve and as more and more tiles are linked together, Amdahl&#39;s Law—an argument about the limits of parallel computing—will pose new challenges to optimizing system energy efficiency. Previously unimportant aspects such as data communication and the residual digital computing needed between tiles will incur more and more of the energy budget, leading to a gap between the peak energy efficiency of the tile itself and the sustained energy efficiency of the overall analog-AI system. Of course, that&#39;s a problem that eventually arises for every AI accelerator, analog or digital.
</p><p>
	The path forward is necessarily different from digital AI accelerators. Digital approaches can bring precision down until accuracy falters. But analog AI must first increase the signal-to-noise ratio (SNR) of the internal analog modules until it is high enough to demonstrate accuracy equivalent to that of digital systems. Any subsequent SNR improvements can then be applied toward increasing density and energy efficiency.
</p><p>
	These are exciting problems to solve, and it will take the coordinated efforts of materials scientists, device experts, circuit designers, system architects, and DNN experts working together to solve them. There is a strong and continued need for higher energy-efficiency AI acceleration, and a shortage of other attractive alternatives for delivering on this need. Given the wide variety of potential memory devices and implementation paths, it is quite likely that some degree of analog computation will find its way into future AI accelerators. 
	<span></span>
</p><p>
	<em>This article appears in the December 2021 print issue as &#34;Ohm&#39;s Law + Kirchhoff&#39;s Current Law = Better AI.&#34;</em>
</p></template></div></div>
  </body>
</html>
