<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://www.kolaayonrinde.com/blog/2023/10/20/soft-moe.html">Original</a>
    <h1>From Sparse To Soft Mixtures of Experts</h1>
    
    <div id="readability-page-1" class="page"><div itemprop="articleBody"><p>Mixture of Expert (MoE) models have recently emerged as an ML architecture
offering efficient scaling and practicality in both training and inference <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<h3 id="what-are-sparse-moes">What Are Sparse MoEs?</h3>

<p>In traditional <a href="https://arxiv.org/pdf/2101.03961.pdf">Sparse MoEs</a>, we swap out
the <code>MLP layers</code> of the vanilla transformer for an <code>Expert Layer</code>. The Expert
Layer is made up of multiple MLPs referred to as Experts. For each input one
expert is selected to send that input to. A dynamic routing mechanism decides
how to map tokens to Experts. Importantly, though this is less mentioned, MoEs
are more modular and hence more naturally interpretable than vanilla
transformers.</p>

<div>
  <figure>
    <img src="http://jsxgraph.uni-bayreuth.de/blog/images/softmoe/moe.png" width="800" alt="Sparse MoE"/>
    <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption>
    </figure>
</div>

<h3 id="introducing-soft-moes">Introducing Soft MoEs</h3>

<p>The Soft MoE paradigm was introduced by Google researchers in the paper
<a href="https://arxiv.org/pdf/2308.00951.pdf">From Sparse To Soft Mixtures of Experts</a>.
Unlike Sparse MoEs, Soft MoEs don’t send a <em>subset</em> of the input tokens to
experts. Instead, each expert receives a <em>linear combination</em> of all the input
tokens. The weights for these combinations are determined by the same dynamic
routing mechanism as in Sparse MoEs.</p>

<div>
  <figure>
    <img src="http://jsxgraph.uni-bayreuth.de/blog/images/softmoe/duck.png" width="500" alt="Soft MoE"/>
    <figcaption>In Soft MoEs each expert processes linear combinations of image patches. </figcaption>
    </figure>
</div>

<p>The discrete routing that makes Sparse MoEs so effective also makes them not
inherently fully differentiable and can cause training issues. The Soft MoE
approach solves these issues, are better suited to GPU hardware and in general
outperform Sparse MoEs.</p>

<p>The paper abstract reads:</p>

<blockquote>
  <p>Sparse mixture of expert architectures (MoEs) scale model capacity without
large increases in training or inference costs. Despite their success, MoEs
suffer from a number of issues: training instability, token dropping,
inability to scale the number of experts, or ineffective finetuning. In this
work, we propose Soft MoE, a fully-differentiable sparse Transformer that
addresses these challenges, while maintaining the benefits of MoEs. Soft MoE
performs an implicit soft assignment by passing different weighted
combinations of all input tokens to each expert. As in other MoE works,
experts in Soft MoE only process a subset of the (combined) tokens, enabling
larger model capacity at lower inference cost. In the context of visual
recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and
popular MoE variants (Tokens Choice and Experts Choice). For example, Soft
MoE-Base/16 requires 10.5× lower inference cost (5.7× lower wall-clock time)
than ViT-Huge/14 while matching its performance after similar training. Soft
MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has
over 40× more parameters than ViT Huge/14, while inference time cost grows by
only 2%, and it performs substantially better.</p>
</blockquote>

<h3 id="links-to-talk-and-slides">Links to Talk and Slides</h3>

<p>I recently gave a talk at <a href="https://www.eleuther.ai">EleutherAI</a>, the open-source
AI research lab, about Soft MoEs.</p>

<p>You can watch the talk back on YouTube
<a href="https://youtu.be/xCKdBC5dh_g?si=uDH8vLVII7l_X8_L">here</a> <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup> or view the slides
<a href="https://docs.google.com/presentation/d/12Sw4wRQJr3sxcJR91_UM_dlYgYxeAbf9t8es54bAYUM/edit#slide=id.p">here</a>.</p>

<p>I’m very excited about research ideas working on expanding the SoftMoE paradigm
to autoregressive (GPT-style) models, which is currently an open problem
described in the above talk. Feel free to reach out if you’re interested in or
are currently researching in this area. </p>

<hr/>




</div><div>
    <h3>If you&#39;d like to cite this article, please use:</h3>
    <pre>  @misc{kayonrinde2023softmoe,
    author = &#34;Kola Ayonrinde&#34;,
    title = &#34;From Sparse To Soft Mixtures of Experts&#34;,
    year = 2023,
    howpublished = &#34;Blog post&#34;,
    url = &#34;http://www.kolaayonrinde.com/2023/10/20/soft-moe.html&#34;
  }
    </pre>
  </div></div>
  </body>
</html>
