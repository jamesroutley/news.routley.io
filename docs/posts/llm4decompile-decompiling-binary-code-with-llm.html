<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/albertan017/LLM4Decompile">Original</a>
    <h1>LLM4Decompile: Decompiling Binary Code with LLM</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">Reverse Engineering: Decompiling Binary Code with Large Language Models</p>
<p dir="auto">For more details check out the <a href="https://arxiv.org/abs/2403.05286" rel="nofollow">paper</a>.</p>

<p dir="auto"><strong>2023.03.16</strong> Add <a href="https://huggingface.co/arise-sustech/llm4decompile-6.7b-uo" rel="nofollow">llm4decompile-6.7b-uo</a> model which is trained without prior knowledge of the optimization levels (O0~O3), the average re-executability is arond 0.21.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">1. Introduction of LLM4Decompile and Decompile-Eval</h2><a id="user-content-1-introduction-of-llm4decompile-and-decompile-eval" aria-label="Permalink: 1. Introduction of LLM4Decompile and Decompile-Eval" href="#1-introduction-of-llm4decompile-and-decompile-eval"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Our objective is to create and release the first open-source LLM dedicated to decompilation, and to assess its capabilities by constructing the first decompilation benchmark focused on re-compilability and re-executable.</p>
<p dir="auto">We start by compiling a million C code samples from <a href="https://github.com/brenocfg/AnghaBench">AnghaBench</a> into assembly code using GCC with different configurations, forming a dataset of assembly-source pairs in 4 billion tokens. We then finetune the DeepSeek-Coder model, a leading-edge code LLM, using this dataset. Followed by constructing the evaluation benchmark, Decompile-Eval, based on HumanEval questions and test samples. Specifically, we formulate the evaluation from two perspectives: whether the decompiled code can recompile successfully, and whether it passes all assertions in the test cases.</p>
<p dir="auto">Figure 1 presents the steps involved in our decompilation evaluation. First, the source code (denoted as src) is compiled by the GCC compiler with specific parameters, such as optimization levels, to produce the executable binary. This binary is then disassembled into assembly language (asm) using the objdump tool. The assembly instructions are subsequently decompiled to reconstruct the source code in a format that&#39;s readable to humans (noted as src&#39;). To assess the quality of the decompiled code (src&#39;), it is tested for its ability to be recompiled with the original GCC compiler (re-compilability) and for its functionality through test assertions (re-executability).</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/albertan017/LLM4Decompile/blob/main/samples/pipeline.png"><img src="https://github.com/albertan017/LLM4Decompile/raw/main/samples/pipeline.png" alt="image" width="300" height="auto"/></a>
</p>


<p dir="auto"><strong>Re-compilability</strong> and <strong>re-executability</strong> serve as critical indicators in validating the effectiveness of a decompilation process. When decompiled code can be recompiled, it provides strong evidence of syntactic integrity. It ensures that the decompiled code is not just readable, but also adheres to the structural and syntactical standards expected by the compiler.
However, syntax alone does not guarantee semantic equivalence to the original pre-compiled program. Re-executability provides this critical measure of semantic correctness. By re-compiling the decompiled output and running the test cases, we assess if the decompilation preserved the program logic and behavior.
Together, re-compilability and re-executability indicate syntax recovery and semantic preservation - both essential for usable and robust decompilation.</p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/albertan017/LLM4Decompile/blob/main/samples/results_decompile.png"><img src="https://github.com/albertan017/LLM4Decompile/raw/main/samples/results_decompile.png" alt="Alt text"/></a></p>

<p dir="auto">Our LLM4Decompile includes models with sizes between 1.3 billion and 33 billion parameters, and we have made these models available on Hugging Face.</p>
<p dir="auto"><a href="https://huggingface.co/arise-sustech/llm4decompile-1.3b" rel="nofollow">llm4decompile-1.3b</a></p>
<p dir="auto"><a href="https://huggingface.co/arise-sustech/llm4decompile-6.7b" rel="nofollow">llm4decompile-6.7b</a></p>
<p dir="auto"><a href="https://huggingface.co/arise-sustech/llm4decompile-33b" rel="nofollow">llm4decompile-33b</a></p>
<p dir="auto"><a href="https://huggingface.co/arise-sustech/llm4decompile-6.7b-nsp" rel="nofollow">llm4decompile-6.7b-nsp</a></p>
<p dir="auto"><a href="https://huggingface.co/arise-sustech/llm4decompile-6.7b-uo" rel="nofollow">llm4decompile-6.7b-uo</a></p>
<p dir="auto">Note: The NSP model is trained with assembly code, the average re-executability is arond 0.17.</p>
<p dir="auto">Note: The unified optimization (UO) model is trained without prior knowledge of the optimization levels (O0~O3), the average re-executability is arond 0.21. The pre-processing of UO model is slightly different (no prior knowledge of the On), please check the <a href="https://huggingface.co/arise-sustech/llm4decompile-6.7b-uo#3-how-to-use" rel="nofollow">model page</a>.</p>
<p dir="auto">Here give an example of how to use our model.</p>
<p dir="auto"><strong>Preprocessing:</strong> compile the C code into binary, disassemble the binary into assembly instructions.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import subprocess
import os
import re

digit_pattern = r&#39;\b0x[a-fA-F0-9]+\b&#39;# binary codes in Hexadecimal
zeros_pattern = r&#39;^0+\s&#39;#0s
OPT = [&#34;O0&#34;, &#34;O1&#34;, &#34;O2&#34;, &#34;O3&#34;]
fileName = &#39;path/to/file&#39;
with open(fileName+&#39;.c&#39;,&#39;r&#39;) as f:#original file
    c_func = f.read()
for opt_state in OPT:
    output_file = fileName +&#39;_&#39; + opt_state
    input_file = fileName+&#39;.c&#39;
    compile_command = f&#39;gcc -c -o {output_file}.o {input_file} -{opt_state} -lm&#39;#compile the code with GCC on Linux
    subprocess.run(compile_command, shell=True, check=True)
    compile_command = f&#39;objdump -d {output_file}.o &gt; {output_file}.s&#39;#disassemble the binary file into assembly instructions
    subprocess.run(compile_command, shell=True, check=True)
    
    input_asm = &#39;&#39;
    with open(output_file+&#39;.s&#39;) as f:#asm file
        asm= f.read()
    asm = asm.split(&#39;Disassembly of section .text:&#39;)[-1].strip()
    for tmp in asm.split(&#39;\n&#39;):
        tmp_asm = tmp.split(&#39;\t&#39;)[-1]#remove the binary code
        tmp_asm = tmp_asm.split(&#39;#&#39;)[0].strip()#remove the comments
        input_asm+=tmp_asm+&#39;\n&#39;
    input_asm = re.sub(zeros_pattern, &#39;&#39;, input_asm)
    before = f&#34;# This is the assembly code with {opt_state} optimization:\n&#34;#prompt
    after = &#34;\n# What is the source code?\n&#34;#prompt
    input_asm_prompt = before+input_asm.strip()+after
    with open(fileName +&#39;_&#39; + opt_state +&#39;.asm&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;) as f:
        f.write(input_asm_prompt)"><pre><span>import</span> <span>subprocess</span>
<span>import</span> <span>os</span>
<span>import</span> <span>re</span>

<span>digit_pattern</span> <span>=</span> <span>r&#39;\b0x[a-fA-F0-9]+\b&#39;</span><span># binary codes in Hexadecimal</span>
<span>zeros_pattern</span> <span>=</span> <span>r&#39;^0+\s&#39;</span><span>#0s</span>
<span>OPT</span> <span>=</span> [<span>&#34;O0&#34;</span>, <span>&#34;O1&#34;</span>, <span>&#34;O2&#34;</span>, <span>&#34;O3&#34;</span>]
<span>fileName</span> <span>=</span> <span>&#39;path/to/file&#39;</span>
<span>with</span> <span>open</span>(<span>fileName</span><span>+</span><span>&#39;.c&#39;</span>,<span>&#39;r&#39;</span>) <span>as</span> <span>f</span>:<span>#original file</span>
    <span>c_func</span> <span>=</span> <span>f</span>.<span>read</span>()
<span>for</span> <span>opt_state</span> <span>in</span> <span>OPT</span>:
    <span>output_file</span> <span>=</span> <span>fileName</span> <span>+</span><span>&#39;_&#39;</span> <span>+</span> <span>opt_state</span>
    <span>input_file</span> <span>=</span> <span>fileName</span><span>+</span><span>&#39;.c&#39;</span>
    <span>compile_command</span> <span>=</span> <span>f&#39;gcc -c -o <span><span>{</span><span>output_file</span><span>}</span></span>.o <span><span>{</span><span>input_file</span><span>}</span></span> -<span><span>{</span><span>opt_state</span><span>}</span></span> -lm&#39;</span><span>#compile the code with GCC on Linux</span>
    <span>subprocess</span>.<span>run</span>(<span>compile_command</span>, <span>shell</span><span>=</span><span>True</span>, <span>check</span><span>=</span><span>True</span>)
    <span>compile_command</span> <span>=</span> <span>f&#39;objdump -d <span><span>{</span><span>output_file</span><span>}</span></span>.o &gt; <span><span>{</span><span>output_file</span><span>}</span></span>.s&#39;</span><span>#disassemble the binary file into assembly instructions</span>
    <span>subprocess</span>.<span>run</span>(<span>compile_command</span>, <span>shell</span><span>=</span><span>True</span>, <span>check</span><span>=</span><span>True</span>)
    
    <span>input_asm</span> <span>=</span> <span>&#39;&#39;</span>
    <span>with</span> <span>open</span>(<span>output_file</span><span>+</span><span>&#39;.s&#39;</span>) <span>as</span> <span>f</span>:<span>#asm file</span>
        <span>asm</span><span>=</span> <span>f</span>.<span>read</span>()
    <span>asm</span> <span>=</span> <span>asm</span>.<span>split</span>(<span>&#39;Disassembly of section .text:&#39;</span>)[<span>-</span><span>1</span>].<span>strip</span>()
    <span>for</span> <span>tmp</span> <span>in</span> <span>asm</span>.<span>split</span>(<span>&#39;<span>\n</span>&#39;</span>):
        <span>tmp_asm</span> <span>=</span> <span>tmp</span>.<span>split</span>(<span>&#39;<span>\t</span>&#39;</span>)[<span>-</span><span>1</span>]<span>#remove the binary code</span>
        <span>tmp_asm</span> <span>=</span> <span>tmp_asm</span>.<span>split</span>(<span>&#39;#&#39;</span>)[<span>0</span>].<span>strip</span>()<span>#remove the comments</span>
        <span>input_asm</span><span>+=</span><span>tmp_asm</span><span>+</span><span>&#39;<span>\n</span>&#39;</span>
    <span>input_asm</span> <span>=</span> <span>re</span>.<span>sub</span>(<span>zeros_pattern</span>, <span>&#39;&#39;</span>, <span>input_asm</span>)
    <span>before</span> <span>=</span> <span>f&#34;# This is the assembly code with <span><span>{</span><span>opt_state</span><span>}</span></span> optimization:<span>\n</span>&#34;</span><span>#prompt</span>
    <span>after</span> <span>=</span> <span>&#34;<span>\n</span># What is the source code?<span>\n</span>&#34;</span><span>#prompt</span>
    <span>input_asm_prompt</span> <span>=</span> <span>before</span><span>+</span><span>input_asm</span>.<span>strip</span>()<span>+</span><span>after</span>
    <span>with</span> <span>open</span>(<span>fileName</span> <span>+</span><span>&#39;_&#39;</span> <span>+</span> <span>opt_state</span> <span>+</span><span>&#39;.asm&#39;</span>,<span>&#39;w&#39;</span>,<span>encoding</span><span>=</span><span>&#39;utf-8&#39;</span>) <span>as</span> <span>f</span>:
        <span>f</span>.<span>write</span>(<span>input_asm_prompt</span>)</pre></div>
<p dir="auto"><strong>Decompilation:</strong> use LLM4Decompile to translate the assembly instructions into C:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = &#39;arise-sustech/llm4decompile-1.3b&#39;
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.bfloat16).cuda()

with open(fileName +&#39;_&#39; + opt_state +&#39;.asm&#39;,&#39;r&#39;) as f:#original file
    asm_func = f.read()
inputs = tokenizer(asm_func, return_tensors=&#34;pt&#34;).to(model.device)
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=500)
c_func_decompile = tokenizer.decode(outputs[0][len(inputs[0]):-1])"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoTokenizer</span>, <span>AutoModelForCausalLM</span>
<span>import</span> <span>torch</span>

<span>model_path</span> <span>=</span> <span>&#39;arise-sustech/llm4decompile-1.3b&#39;</span>
<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_path</span>)
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_path</span>,<span>torch_dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>).<span>cuda</span>()

<span>with</span> <span>open</span>(<span>fileName</span> <span>+</span><span>&#39;_&#39;</span> <span>+</span> <span>opt_state</span> <span>+</span><span>&#39;.asm&#39;</span>,<span>&#39;r&#39;</span>) <span>as</span> <span>f</span>:<span>#original file</span>
    <span>asm_func</span> <span>=</span> <span>f</span>.<span>read</span>()
<span>inputs</span> <span>=</span> <span>tokenizer</span>(<span>asm_func</span>, <span>return_tensors</span><span>=</span><span>&#34;pt&#34;</span>).<span>to</span>(<span>model</span>.<span>device</span>)
<span>with</span> <span>torch</span>.<span>no_grad</span>():
    <span>outputs</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>inputs</span>, <span>max_new_tokens</span><span>=</span><span>500</span>)
<span>c_func_decompile</span> <span>=</span> <span>tokenizer</span>.<span>decode</span>(<span>outputs</span>[<span>0</span>][<span>len</span>(<span>inputs</span>[<span>0</span>]):<span>-</span><span>1</span>])</pre></div>
<div dir="auto"><h2 tabindex="-1" dir="auto">4. How to use Decompile-Eval</h2><a id="user-content-4-how-to-use-decompile-eval" aria-label="Permalink: 4. How to use Decompile-Eval" href="#4-how-to-use-decompile-eval"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Data are stored in <code>llm4decompile/decompile-eval/decompile-eval.json</code>, using JSON list format. There are 164*4 (O0, O1, O2, O3) samples, each with five keys:</p>
<ul dir="auto">
<li><code>task_id</code>: indicates the ID of the problem.</li>
<li><code>type</code>: the optimization stage, is one of [O0, O1, O2, O3].</li>
<li><code>c_func</code>: C solution for HumanEval problem.</li>
<li><code>c_test</code>: C test assertions.</li>
<li><code>input_asm_prompt</code>: assembly instructions with prompts, can be derived as in our <a href="https://github.com/albertan017/LLM4Decompile/blob/main/README.md#3-how-to-use-the-model">preprocessing example</a>.</li>
</ul>
<p dir="auto">To run the evaluation on single GPU and single process:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd LLM4Decompile
python ./evaluation/run_evaluation_llm4decompile_singleGPU.py"><pre><span>cd</span> LLM4Decompile
python ./evaluation/run_evaluation_llm4decompile_singleGPU.py</pre></div>
<p dir="auto">To run the evaluation using TGI (10x faster, support multiple GPUs and multi-process):
First, please install the text-generation-inference following the official <a href="https://github.com/huggingface/text-generation-inference">link</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/albertan017/LLM4Decompile.git
cd LLM4Decompile
pip install -r requirements.txt

# Before run the evaluation script, plase update the model_path to your local mdoel path.
bash ./scripts/run_evaluation_llm4decompile.sh"><pre>git clone https://github.com/albertan017/LLM4Decompile.git
<span>cd</span> LLM4Decompile
pip install -r requirements.txt

<span><span>#</span> Before run the evaluation script, plase update the model_path to your local mdoel path.</span>
bash ./scripts/run_evaluation_llm4decompile.sh</pre></div>

<p dir="auto">LLM4Binary: We plan to include larger dataset to pre-train the model with assembly code and C code.</p>
<p dir="auto">Decompiler-ALL: Support popular languages/platforms and settings (e.g., decompile multiple functions).</p>

<p dir="auto">This code repository is licensed under the MIT License.</p>

<p dir="auto">If you have any questions, please raise an issue.</p>

<p dir="auto">The conversation about the language model decompiler that took place on <a href="https://www.reddit.com/r/MachineLearning/comments/123asbg/d_can_we_train_a_decompiler/" rel="nofollow">Reddit</a> roughly a year ago was quite fascinating to us.</p>

<div data-snippet-clipboard-copy-content="@misc{tan2024llm4decompile,
      title={LLM4Decompile: Decompiling Binary Code with Large Language Models}, 
      author={Hanzhuo Tan and Qi Luo and Jing Li and Yuqun Zhang},
      year={2024},
      eprint={2403.05286},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}"><pre><code>@misc{tan2024llm4decompile,
      title={LLM4Decompile: Decompiling Binary Code with Large Language Models}, 
      author={Hanzhuo Tan and Qi Luo and Jing Li and Yuqun Zhang},
      year={2024},
      eprint={2403.05286},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}
</code></pre></div>
</article></div></div>
  </body>
</html>
