<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/microsoft/LMOps">Original</a>
    <h1>General technology for enabling AI capabilities with LLMs and Generative models</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">

<p dir="auto">LMOps is a research initiative on fundamental research and technology for building AI products w/ foundation models, especially on the general technology for enabling AI capabilities w/ LLMs and Generative AI models.</p>
<ul dir="auto">
<li>Better Prompts: <a href="https://arxiv.org/abs/2212.09611" rel="nofollow">Promptist</a>, <a href="https://arxiv.org/abs/2212.00616" rel="nofollow">Extensible prompts</a></li>
<li>Longer Context: <a href="https://arxiv.org/abs/2212.06713" rel="nofollow">Structured prompting</a>, <a href="https://arxiv.org/abs/2212.10554" rel="nofollow">Length-Extrapolatable Transformers</a></li>
<li>Knowledge Augmentation (TBA)</li>
<li>Fundamentals</li>
</ul>
<h2 dir="auto"><a id="user-content-links" aria-hidden="true" href="#links"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Links</h2>
<ul dir="auto">
<li><a href="https://github.com/microsoft/unilm">microsoft/unilm</a>: Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities</li>
<li><a href="https://github.com/microsoft/torchscale">microsoft/torchscale</a>: Transformers at (any) Scale</li>
</ul>
<h2 dir="auto"><a id="user-content-news" aria-hidden="true" href="#news"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>News</h2>
<ul dir="auto">
<li>[Paper Release] Dec, 2022: <a href="https://arxiv.org/abs/2212.10559" rel="nofollow">Why Can GPT Learn In-Context? Language Models Secretly Perform Finetuning as Meta Optimizers</a></li>
<li>[Paper&amp;Model&amp;Demo Release] Dec, 2022: <a href="https://aka.ms/promptist" rel="nofollow">Optimizing Prompts for Text-to-Image Generation</a></li>
<li>[Paper&amp;Code Release] Dec, 2022: <a href="https://arxiv.org/abs/2212.06713" rel="nofollow">Structured Prompting: Scaling In-Context Learning to 1,000 Examples</a></li>
<li>[Paper Release] Nov, 2022: <a href="https://arxiv.org/abs/2212.00616" rel="nofollow">Extensible Prompts for Language Models</a></li>
</ul>
<h2 dir="auto"><a id="user-content-prompt-intelligence" aria-hidden="true" href="#prompt-intelligence"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Prompt Intelligence</h2>
<p dir="auto">Advanced technologies facilitating prompting language models.</p>
<h3 dir="auto"><a id="user-content-promptist-reinforcement-learning-for-automatic-prompt-optimization" aria-hidden="true" href="#promptist-reinforcement-learning-for-automatic-prompt-optimization"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Promptist: reinforcement learning for automatic prompt optimization</h3>
<p dir="auto">[Paper] <a href="https://arxiv.org/abs/2212.09611" rel="nofollow">Optimizing Prompts for Text-to-Image Generation</a></p>
<blockquote>
<ul dir="auto">
<li>Language models serve as a prompt interface that optimizes user input into model-preferred prompts.</li>
</ul>
</blockquote>
<blockquote>
<ul dir="auto">
<li>Learn a language model for automatic prompt optimization via reinforcement learning.</li>
</ul>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1070872/207856962-02f08d92-f2bf-441a-b1c3-efff1a4b6187.png"><img src="https://user-images.githubusercontent.com/1070872/207856962-02f08d92-f2bf-441a-b1c3-efff1a4b6187.png" alt="image"/></a></p>
<h3 dir="auto"><a id="user-content-structured-prompting-consume-long-sequence-prompts-in-an-efficient-way" aria-hidden="true" href="#structured-prompting-consume-long-sequence-prompts-in-an-efficient-way"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Structured Prompting: consume long-sequence prompts in an efficient way</h3>
<p dir="auto">[Paper] <a href="https://arxiv.org/abs/2212.06713" rel="nofollow">Structured Prompting: Scaling In-Context Learning to 1,000 Examples</a></p>
<ul dir="auto">
<li>Example use cases:</li>
</ul>
<blockquote>
<ol dir="auto">
<li>Prepend (many) retrieved (long) documents as context in GPT.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" dir="auto">
<li>Scale in-context learning to many demonstration examples.</li>
</ol>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1070872/207856629-2bb0c933-c27b-4177-9e10-e397622ae79b.png"><img src="https://user-images.githubusercontent.com/1070872/207856629-2bb0c933-c27b-4177-9e10-e397622ae79b.png" alt="image"/></a></p>
<h3 dir="auto"><a id="user-content-x-prompt-extensible-prompts-beyond-nl-for-descriptive-instructions" aria-hidden="true" href="#x-prompt-extensible-prompts-beyond-nl-for-descriptive-instructions"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>X-Prompt: extensible prompts beyond NL for descriptive instructions</h3>
<p dir="auto">[Paper] <a href="https://arxiv.org/abs/2212.00616" rel="nofollow">Extensible Prompts for Language Models</a></p>
<blockquote>
<ul dir="auto">
<li>Extensible interface allowing prompting LLMs beyond natural language for fine-grain specifications</li>
</ul>
</blockquote>
<blockquote>
<ul dir="auto">
<li>Context-guided imaginary word learning for general usability</li>
</ul>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1070872/207856788-5409d04d-c406-4b29-ae7b-2732e727d4cc.png"><img src="https://user-images.githubusercontent.com/1070872/207856788-5409d04d-c406-4b29-ae7b-2732e727d4cc.png" alt="Extensible Prompts for Language Models"/></a></p>
<h2 dir="auto"><a id="user-content-fundamental-understanding-of-llms" aria-hidden="true" href="#fundamental-understanding-of-llms"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Fundamental Understanding of LLMs</h2>
<h3 dir="auto"><a id="user-content-understanding-in-context-learning" aria-hidden="true" href="#understanding-in-context-learning"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Understanding In-Context Learning</h3>
<p dir="auto">[Paper] <a href="https://arxiv.org/abs/2212.10559" rel="nofollow">Why Can GPT Learn In-Context? Language Models Secretly Perform Finetuning as Meta Optimizers</a></p>
<blockquote>
<ul dir="auto">
<li>According to the demonstration examples, GPT produces meta gradients for In-Context Learning (ICL) through forward computation. ICL works by applying these meta gradients to the model through attention.</li>
</ul>
</blockquote>
<blockquote>
<ul dir="auto">
<li>The meta optimization process of ICL shares a dual view with finetuning that explicitly updates the model parameters with back-propagated gradients.</li>
</ul>
</blockquote>
<blockquote>
<ul dir="auto">
<li>We can translate optimization algorithms (such as SGD with Momentum) to their corresponding Transformer architectures.</li>
</ul>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/1070872/208835096-54407f5f-d136-4747-9629-3219988df5d4.png"><img src="https://user-images.githubusercontent.com/1070872/208835096-54407f5f-d136-4747-9629-3219988df5d4.png" alt="image"/></a></p>
<h2 dir="auto"><a id="user-content-hiring-akamsnlpagi" aria-hidden="true" href="#hiring-akamsnlpagi"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Hiring: <a href="https://aka.ms/nlpagi" rel="nofollow">aka.ms/nlpagi</a></h2>
<p dir="auto">We are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models (aka large-scale pre-trained models) and AGI, NLP, MT, Speech, Document AI and Multimodal AI, please send your resume to <a href="mailto:fuwei@microsoft.com"></a><a href="mailto:fuwei@microsoft.com">fuwei@microsoft.com</a>.</p>
<h2 dir="auto"><a id="user-content-license" aria-hidden="true" href="#license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>License</h2>
<p dir="auto">This project is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>
<p dir="auto"><a href="https://opensource.microsoft.com/codeofconduct" rel="nofollow">Microsoft Open Source Code of Conduct</a></p>
<h3 dir="auto"><a id="user-content-contact-information" aria-hidden="true" href="#contact-information"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a>Contact Information</h3>
<p dir="auto">For help or issues using the pre-trained models, please submit a GitHub issue.
For other communications, please contact <a href="http://gitnlp.org/" rel="nofollow">Furu Wei</a> (<code>fuwei@microsoft.com</code>).</p>
</article>
          </div></div>
  </body>
</html>
