<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.gwern.net/Google-shutdowns">Original</a>
    <h1>Predicting Google Closures (2013)</h1>
    
    <div id="readability-page-1" class="page"><div id="page-metadata">
        <p>Analyzing predictors of Google abandoning products; predicting future shutdowns</p>
        
        
      </div><div id="markdownBody">
        <div>
          <blockquote>
            <p>Prompted by the shutdown of Google Reader, I ponder the evanescence of online services and wonder what is the risk of them disappearing. I collect data on <a href="#sources">350 Google products</a> launched before March‚ÄÖ2013, looking for <a href="#variables">variables predictive of mortality</a> (web hits, service vs software, commercial vs free, FLOSS, social networking, and internal vs acquired). Shutdowns are unevenly distributed over the calendar year or Google‚Äôs history. I use logistic regression &amp; survival analysis (which can deal with right-censorship) to <a href="#modeling">model the risk of shutdown over time</a> and examine correlates. The logistic regression indicates socialness, acquisitions, and lack of web hits predict being shut down, but the results may not be right. The survival analysis finds a median lifespan of 2824 days with a roughly Type III survival curve (high early-life mortality); a Cox regression finds similar results as the logistic - socialness, free, acquisition, and long life predict lower mortality. Using the best model, I <a href="#predictions">make predictions</a> about probability of shutdown of the most risky and least risky services in the next 5 years (up to March‚ÄÖ2018). (All data &amp; R source code is provided.)</p>
          </blockquote>
        </div>
        <p>Google has occasionally shut down services I use, and not always with serious warning (many tech companies are like that - here one day and gone the next - though Google is one of the least-worst); this is frustrating and tedious.</p>
        <p>Naturally, we are preached at by apologists that Google owes us nothing and if it‚Äôs a problem then it‚Äôs all our fault and we should have prophesied the future better (and too bad about the ordinary people who may be screwed over or the unique history<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> or data casually destroyed).</p>
        <p>But how can we have any sort of rational expectation if we lack any data or ideas about how long Google will run anything or why or how it chooses to do what it does? So in the following essay, I try to get an idea of the risk, and hopefully the results are interesting, useful, or both.</p>
        <section id="a-glance-back">
          
          <div>
            <blockquote>
              <p>‚ÄúThis is something that literature has always been very keen on, that technology never gets around to acknowledging. The cold wind moaning through the empty stone box. When are you gonna own up to it? Where are the Dell PCs? This is Austin, Texas. Michael Dell is the biggest tech mogul in central Texas. Why is he not here? Why is he not at least not selling his wares? Where are the dedicated gaming consoles you used to love? Do you remember how important those were? I could spend all day here just reciting the names of the causalities in your line of work. It‚Äôs always the electronic frontier. Nobody ever goes back to look at the electronic forests that were cut down with chainsaws and tossed into the rivers. And then there‚Äôs this empty pretense that these innovations make the world ‚Äòbetter‚Äô‚Ä¶Like: ‚ÄòIf we‚Äôre not making the world better, then why are we doing this at all?‚Äô Now, I don‚Äôt want to claim that this attitude is hypocritical. Because when you say a thing like that at South By: ‚ÄòOh, we‚Äôre here to make the world better‚Äô‚Äîyou haven‚Äôt even <em>reached</em> the level of hypocrisy. You‚Äôre stuck at the level of childish naivete.‚Äù</p>
              <p><a href="https://en.wikipedia.org/wiki/Bruce_Sterling" data-link-icon="wikipedia" data-link-icon-type="svg">Bruce Sterling</a>‚Å†, <a href="https://www.wired.com/beyond_the_beyond/2013/04/text-of-sxsw2013-closing-remarks-by-bruce-sterling/" data-link-icon="wired" data-link-icon-type="svg">‚ÄúText of SXSW2013 closing remarks‚Äù</a></p>
            </blockquote>
          </div>
          <p>The shutdown of the popular service <a href="https://en.wikipedia.org/wiki/Google_Reader" data-link-icon="wikipedia" data-link-icon-type="svg">Google Reader</a>‚Å†, announced on <a href="https://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html" data-link-icon="google" data-link-icon-type="svg" title="A second spring of cleaning">2013-03-13</a>‚Å†, has brought home to many people that some products they rely on exist only at Google‚Äôs sufferance: it provides the products for reasons that are difficult for outsiders to divine, may have little commitment to a product<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a><a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a><a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>‚Å†, may not include their users‚Äô best interests, may choose to withdraw the product at any time for any reason<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> (especially since most of the products are services<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> &amp; not <a href="https://en.wikipedia.org/wiki/Free_and_open-source_software" data-link-icon="wikipedia" data-link-icon-type="svg">FLOSS</a> in any way, and may be too tightly coupled with the Google infrastructure<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> to be spun off or sold, so when the CEO turns against it &amp; <a href="https://ntietz.com/docs/www/www.buzzfeednews.com/f379f2ca2e86edbb4013cb522af779f16482c2cf.html" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://www.buzzfeednews.com/article/mattlynley/google-reader-died-because-no-one-would-run-it" title="(Original URL: https://www.buzzfeednews.com/article/mattlynley/google-reader-died-because-no-one-would-run-it )">no Googlers</a> are willing to waste their careers championing it‚Ä¶), and users have no voice<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> - <a href="https://en.wikipedia.org/wiki/Exit,_Voice,_and_Loyalty" data-link-icon="wikipedia" data-link-icon-type="svg">only exit</a> as an option.</p>
          <p><a href="https://nitter.hu/waxpancake" data-link-icon="twitter" data-link-icon-type="svg">Andy Baio</a> (<a href="https://ntietz.com/docs/www/medium.com/cf2c0933040157f906a8f298c08fc06b9d518892.html" data-link-icon="ùêå" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://medium.com/message/never-trust-a-corporation-to-do-a-librarys-job-f58db4673351" title="(Original URL: https://medium.com/message/never-trust-a-corporation-to-do-a-librarys-job-f58db4673351 )">‚ÄúNever Trust a Corporation to do a Library‚Äôs Job‚Äù</a>) summarizes Google‚Äôs track record:</p>
          <blockquote>
            <blockquote>
              <p>‚ÄúGoogle‚Äôs mission is to organize the world‚Äôs information and make it universally accessible and useful.‚Äù</p>
            </blockquote>
            <p>For years, Google‚Äôs mission included the <em>preservation of the past</em>. In‚ÄÖ2001, Google made their first acquisition, the Deja archives. The largest collection of <a href="https://en.wikipedia.org/wiki/Usenet" data-link-icon="wikipedia" data-link-icon-type="svg">Usenet</a> archives, Google relaunched it as <em>Google Groups</em>, supplemented with archived messages going back to 1981. In‚ÄÖ2004, <em>Google Books</em> signaled the company‚Äôs intention to scan every known book, partnering with libraries and developing its own book scanner capable of digitizing 1,000 pages per hour. In‚ÄÖ2006, <em>Google News Archive</em> launched, with historical news articles dating back 200 years. In‚ÄÖ2008, they expanded it to include their own digitization efforts, scanning newspapers that were never online. In the last five years, starting around 2010, the shifting priorities of Google‚Äôs management left these archival projects in limbo, or abandoned entirely. After a series of redesigns, Google Groups is effectively dead for research purposes. The archives, while still online, have no means of searching by date. Google News Archives are dead, killed off in 2011, now <a href="https://web.archive.org/web/20150211211107/https://support.google.com/news/answer/1638638" data-link-icon="google" data-link-icon-type="svg">directing searchers</a> to just use Google. Google Books is still online, but <a href="https://www.chronicle.com/article/Google-Begins-to-Scale-Back/131109/">curtailed their scanning efforts</a> in recent years, likely discouraged by a decade of legal wrangling <a href="https://ntietz.com/docs/www/gigaom.com/59b3336a70ad3701c4ce327bc8d9fcd614655e44.html" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://gigaom.com/2014/12/03/in-google-books-appeal-judges-focus-on-profit-and-security/" title="(Original URL: https://gigaom.com/2014/12/03/in-google-books-appeal-judges-focus-on-profit-and-security/ )">still in appeal</a>‚Å†. The <a href="https://ntietz.com/docs/www/booksearch.blogspot.com/a895ebdfd9986db8a5cd80e92e777a19700dce69.html" rel="archived alternate nofollow" data-url-original="http://booksearch.blogspot.com/" title="(Original URL: http://booksearch.blogspot.com/ )">official blog</a> stopped updating in 2012 and the <a href="https://nitter.hu/googlebooks" data-link-icon="google" data-link-icon-type="svg">Twitter account‚Äôs</a> been dormant since February‚ÄÖ2013. Even Google Search, their flagship product, stopped focusing on the history of the web. In‚ÄÖ2011, Google <a href="https://readwrite.com/2011/11/11/google_kills_its_own_timeline_feature/" data-link-icon="google" data-link-icon-type="svg">removed</a> the Timeline view letting users filter search results by date, while a series of <a href="https://readwrite.com/2011/11/03/armed_with_social_signals_google_moves_back_toward/" data-link-icon="google" data-link-icon-type="svg">major changes</a> to their search ranking algorithm increasingly favored freshness over older pages from established sources. (To the <a href="https://matt.haughey.com/on-the-future-of-metafilter-941d15ec96f0">detriment of some</a>‚Å†.)‚Ä¶As it turns out, organizing the world‚Äôs information isn‚Äôt always profitable. Projects that preserve the past for the public good aren‚Äôt really a big profit center. Old Google knew that, but didn‚Äôt seem to care.</p>
          </blockquote>
          <p>In the case of Reader, while Reader destroyed the original RSS reader market, there still exist some usable alternatives; the consequence is a shrinkage in the RSS audience as inevitably many users choose not to invest in a new reader or give up or interpret it as a deathblow to RSS, and an irreversible loss of Reader‚Äôs uniquely comprehensive RSS archives back to 2005. Although to be fair, I should mention 2 major points in favor of Google:</p>
          <ol type="1">
            <li>a reason I did and still do use Google services is that, with a few lapses like <a href="https://ntietz.com/AB-testing#max-width" id="gwern-ab-testing-max-width">Website Optimizer</a> aside, they are almost unique in enabling users to back up their data via the work of the <a href="https://en.wikipedia.org/wiki/Google_Data_Liberation_Front" data-link-icon="wikipedia" data-link-icon-type="svg">Google Data Liberation Front</a> and have been far more proactive than many companies in encouraging users to back up data from dead services - for example, in automatically copying Buzz users‚Äô data to their Google Drive.
            </li>
            <li>Google‚Äôs practices of undercutting all market incumbents with free services <em>also</em> has very large benefits<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>‚Å†, so we shouldn‚Äôt focus just on <a href="https://ntietz.com/docs/www/bastiat.org/a591113821e904b34d98283579d2c35976218317.html" rel="archived alternate nofollow" data-url-original="http://bastiat.org/en/twisatwins.html" title="&#39;That Which is Seen, and That Which is Not Seen&#39;, Frederic Bastiat (1850) (Original URL: http://bastiat.org/en/twisatwins.html )">the seen</a>‚Å†.
            </li>
          </ol>
          <p>But nevertheless, every shutdown still hurts its users to some degree, even if we - currently<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> - can rule out the most devastating possible shutdowns, like Gmail. It would be interesting to see if shutdowns are to some degree predictable, whether there are any patterns, whether common claims about relevant factors can be confirmed, and what the results might suggest for the future.</p>
        </section>
        <section id="data">
          
          <section id="sources">
            <h2><a href="#sources" title="Link to section: ¬ß &#39;Sources&#39;">Sources</a></h2>
            <section id="dead-products">
              <h3><a href="#dead-products" title="Link to section: ¬ß &#39;Dead products&#39;">Dead Products</a></h3>
              <div>
                <blockquote>
                  <p>‚ÄúThe summer grasses‚Äî
                  </p>
                  <p>Basho</p>
                </blockquote>
              </div>
              <p>I begin with a list of services/‚ÄãAPIs/‚Äãprograms that Google has shutdown or abandoned taken from the <em>Guardian</em> article <a href="https://www.theguardian.com/technology/2013/mar/22/google-keep-services-closed" data-link-icon="google" data-link-icon-type="svg">‚ÄúGoogle Keep? It‚Äôll probably be with us until March‚ÄÖ2017 - on average: The closure of Google Reader has got early adopters and developers worried that Google services or APIs they adopt will just get shut off. An analysis of 39 shuttered offerings says how long they get‚Äù</a> by Charles Arthur. Arthur‚Äôs list seemed relatively complete, but I‚Äôve added in &gt;300 items he missed based on the <a href="https://www.slate.com/articles/technology/map_of_the_week/2013/03/google_reader_joins_graveyard_of_dead_google_products.html" data-link-icon="google" data-link-icon-type="svg">Slate graveyard</a>‚Å†, Weber‚Äôs <a href="https://thenextweb.com/news/google-fails" data-link-icon="google" data-link-icon-type="svg">‚ÄúGoogle Fails 36% Of The Time‚Äù</a><a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a>‚Å†, the <a href="https://en.wikipedia.org/wiki/Category:Google_acquisitions" data-link-icon="wikipedia" data-link-icon-type="svg">Wikipedia category</a>‚Å†/‚Äã<a href="https://en.wikipedia.org/wiki/List_of_mergers_and_acquisitions_by_Google" data-link-icon="wikipedia" data-link-icon-type="svg">list</a> for Google acquisitions, the <a href="https://en.wikipedia.org/wiki/Category:Discontinued_Google_services" data-link-icon="wikipedia" data-link-icon-type="svg">Wikipedia category</a>‚Å†/‚Äã<a href="https://en.wikipedia.org/wiki/List_of_Google_products#Discontinued_products_and_services" data-link-icon="wikipedia" data-link-icon-type="svg">list</a>‚Å†, and finally the official <a href="https://about.google/" data-link-icon="google" data-link-icon-type="svg">Google History</a>‚Å†. (The additional shutdowns include many shutdowns predating 2010, suggesting that Arthur‚Äôs list was biased towards recent shutdowns.)</p>
              <p>In a few cases, the start dates are well-informed guesses (eg. <a href="https://plus.google.com/u/0/103530621949492999968/posts/fqxuM2SBRQ5" data-link-icon="google" data-link-icon-type="svg">Google Translate</a>) and dates of abandonment/‚Äãshut-down are even harder to get due to the lack of attention paid to most (Joga Bonito) and so I infer the date from archived pages on the <a href="https://en.wikipedia.org/wiki/Internet_Archive" data-link-icon="wikipedia" data-link-icon-type="svg">Internet Archive</a>‚Å†, news reports, blogs such as <a href="https://ntietz.com/docs/www/googlesystem.blogspot.com/5f91362c44599386882507f87d49181b2cd42426.html" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="http://googlesystem.blogspot.com/" title="(Original URL: http://googlesystem.blogspot.com/ )">Google Operating System</a>‚Å†, the dates of press releases, the shutdown of closely related services (eReader Play based on Reader), source code repositories (AngularJS) etc; some are listed as discontinued (Google Catalogs) but are still supported or were merged into other software (Spreadsheets, Docs, Writely, News Archive) or sold/‚Äãgiven to third parties (Flu Shot Finder, App Inventor, Body) or active effort has ceased but the content remains and so I do not list those as dead; for cases of acquired software/‚Äãservices that were shutdown, I date the start from Google‚Äôs purchase.</p>
            </section>
            <section id="live-products">
              <h3><a href="#live-products" title="Link to section: ¬ß &#39;Live products&#39;">Live Products</a></h3>
              <div>
                <blockquote>
                  <p>‚Äú‚Ä¶He often lying broad awake, and yet / Remaining from the body, and apart / In intellect and power and will, hath heard / Time flowing in the middle of the night, / And all things creeping to a day of doom.‚Äù</p>
                  <p><a href="https://en.wikipedia.org/wiki/Alfred,_Lord_Tennyson" data-link-icon="wikipedia" data-link-icon-type="svg">Lord Alfred Tennyson</a>‚Å†, <a href="https://ntietz.com/docs/www/www.blackcatpoems.com/01c6fc6313eac0e337a80967ff4be726b23c8017.html" rel="archived alternate nofollow" data-url-original="http://www.blackcatpoems.com/t/the_mystic.html" title="(Original URL: http://www.blackcatpoems.com/t/the_mystic.html )">‚ÄúThe Mystic‚Äù</a>‚Å†, <em>Poems, Chiefly Lyrical</em></p>
                </blockquote>
              </div>
              <p>A major criticism of Arthur‚Äôs post was that it was fundamentally using the wrong data: if you have a dataset of all Google products which have been shutdown, you can make statements like ‚Äúthe average dead Google product lived 1459 days‚Äù, but you can‚Äôt infer very much about a live product‚Äôs life expectancy - because you don‚Äôt know if it will join the dead products. If, for example, only 1% of products ever died, then 1459 days would lead to a massive underestimates of the average lifespan of all currently living products. With his data, you can only make inferences conditional on a product eventually dying, you cannot make an unconditional inference. Unfortunately, the unconditional question ‚Äúwill it die?‚Äù is the real question any Google user wants answered!</p>
              <p>So drawing on the same sources, I have compiled a second list of <em>living</em> products; the ratio of living to dead gives a base rate for how likely a randomly selected Google product is to be canceled within the 1997-2013 window, and with the date of the founding of each living product, we can also do a simple right-censored <a href="https://en.wikipedia.org/wiki/Survival_analysis" data-link-icon="wikipedia" data-link-icon-type="svg">survival analysis</a> which will let us make better still predictions by extracting concrete results like mean time to shutdown. Some items are dead in the most meaningful sense since they have been closed to new users (Sync), lost major functionality (FeedBurner, Meebo), degraded severely due to neglect (eg. <a href="https://ntietz.com/Google-Alerts" id="gwern-google-alerts" title="&#39;Alerts Over Time&#39;, Branwen‚ÄÖ2013">Google Alerts</a>), or just been completely neglected for a decade or more (Google Group‚Äôs Usenet archive) - but haven‚Äôt actually died or closed yet, so I list them as alive.</p>
            </section>
          </section>
          <section id="variables">
            <h2><a href="#variables" title="Link to section: ¬ß &#39;Variables&#39;">Variables</a></h2>
            
            <p>Simply collecting the data is useful since it allows us to make some estimates like overall death-rates or median lifespan. But maybe we can do better than just base rates and find characteristics which let us crack open the Google black box a tiny bit. So finally, for all products, I have collected several covariates which I thought might help predict longevity:</p>
            <ul>
              <li>
                <p><code>Hits</code>: the number of Google hits for a service</p>
                <p>While number of Google hits is a very crude measure, at best, for underlying variables like ‚Äúpopularity‚Äù or ‚Äúnumber of users‚Äù or ‚Äúprofitability‚Äù, and clearly biased towards recently released products (there aren‚Äôt going to be as many hits for, say, ‚ÄúGoogle Answers‚Äù as there would have been if we had searched for it in 2002), it may add some insight.</p>
                <p>There do not seem to be any other free quality sources indicating either historical or contemporary traffic to a product URL/‚Äãhomepage which could be used in the analysis - services like Alexa or Google Ad Planner either are commercial, for domains only, or simply do not cover many of the URLs. (After I finished data collection, it was pointed out to me that while Google‚Äôs Ad Planner may not be useful, Google‚Äôs AdWords <em>does</em> yield a count of global searches for a particular query that month, which would have worked albeit it would only indicate current levels of interest and nothing about historical levels.)</p>
              </li>
              <li>
                <p><code>Type</code>: a categorization into ‚Äúservice‚Äù/‚Äã‚Äúprogram‚Äù/‚Äã‚Äúthing‚Äù/‚Äã‚Äúother‚Äù</p>
                <ol type="1">
                  <li>
                    <p>A <em>service</em> is anything primarily accessed through a web browser or API or the Internet; so Gmail or a browser loading fonts from a Google server, but not a Gmail notification program one runs on one‚Äôs computer or a FLOSS font available for download &amp; distribution.</p>
                  </li>
                  <li>
                    <p>A <em>program</em> is anything which is an application, plugin, library, framework, or all of these combined; some are very small (Authenticator) and some are very large (Android). This does include programs which require Internet connections or Google APIs as well as programs for which the source code has not been released, so things in the program category are not immune to shutdown and may be useful only as long as Google supports them.</p>
                  </li>
                  <li>
                    <p>A <em>thing</em> is anything which is primarily a physical object. A cellphone running Android or a Chromebook would be an example.</p>
                    <p>In retrospect, I probably should have excluded this category entirely: there‚Äôs no reason to expect cellphones to follow the same lifecycle as a service or program, it leads to even worse classification problems (when does an Android cellphone ‚Äòdie‚Äô? should one even be looking at individual cellphones or laptops rather than entire product lines?), there tend to be many iterations of a product and they‚Äôre all hard to research, etc.</p>
                  </li>
                  <li>
                    <p><em>Other</em> is the catch-all category for things which don‚Äôt quite seem to fit. Where does a Google think-tank, charity, conference, or venture capital fund fit in? They certainly aren‚Äôt software, but they don‚Äôt seem to be quite services either.</p>
                  </li>
                </ol>
              </li>
              <li>
                <p><code>Profit</code>: whether Google <em>directly</em> makes money off a product</p>
                <p>This is a tricky one. Google excuses many of its products by saying that anything which increases Internet usage benefits Google and so by this logic, every single one of its services could potentially increase profit; but this is a little stretched, the truth very hard to judge by an outsider, and one would expect that products without direct monetization are more likely to be killed.</p>
                <p>Generally, I classify as for profit any Google product directly relating to producing/‚Äãdisplaying advertising, paid subscriptions, fees, or purchases (AdWords, Gmail, Blogger, Search, shopping engines, surveys); but many do not seem to have any form of monetization related to them (Alerts, Office, Drive, Gears, Reader<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a>). Some services like Voice charge (for international calls) but the amounts are minor enough that one might wonder if classifying them as for profit is really right. While it might make sense to define every feature added to, say, Google Search (eg. Personalized Search, or Search History) as being ‚Äòfor profit‚Äô since Search lucratively displays ads, I have chosen to classify these secondary features as being not for profit.</p>
              </li>
              <li>
                <p><code>FLOSS</code>: whether the source code was released or Google otherwise made it possible for third parties to continue the service or maintain the application.</p>
                <blockquote>
                  <p>In the long run, the utility of all non-Free software approaches zero. All non-Free software is a dead end.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
                </blockquote>
                <p>Android, AngularJS, and Chrome are all examples of software products where Google losing interest would not be fatal; services spun off to third parties would also count. Many of the codebases rely on a proprietary Google API or service (especially the mobile applications), which means that this variable is not as meaningful and laudable as one might expect, so in the minority of cases where this variable is relevant, I code <code>Dead</code> &amp; <code>Ended</code> as related to whether &amp; when Google abandoned it, regardless of whether it was then picked up by third parties or not. (Example: App Inventor for Android is listed as dying in December‚ÄÖ2011, though it was then half a year later handed over to MIT, who has supported it since.) It‚Äôs important to not naively believe that simply because source code is available, Google support doesn‚Äôt matter.</p>
              </li>
              <li>
                <p><code>Acquisition</code>: whether it was related to a purchase of a company or licensing, or internally developed.</p>
                <p>This is useful for investigating the so-called <a href="https://slate.com/technology/2008/08/the-google-black-hole.html" data-link-icon="google" data-link-icon-type="svg" title="The Google Black Hole: Sergey and Larry just bought my company. Uh oh.">‚ÄúGoogle black hole‚Äù</a>: Google has bought many startups (DoubleClick, Dodgeball, Android, Picasa), or technologies/‚Äãdata licensed (SYSTRAN for Translate, Twitter data for Real-Time Search), but it‚Äôs claimed many stagnate &amp; wither (Jaiku, JotSpot, Dodgeball, <a href="https://ntietz.com/docs/www/www.businessinsider.com/5966e0a48e2178cd9d4f6f99fc7157330af509c3.html" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://www.businessinsider.com/google-zagat-story-2013-6" title="MISERY AT GOOGLE: You&#39;d Never Expect NSFW Graffiti Like This On Google&#39;s Bathroom Walls (Original URL: https://www.businessinsider.com/google-zagat-story-2013-6 )">Zagat</a>). So we‚Äôll include this. If a closely related product is developed and released after purchase, like a mobile application, I do not class it as an acquisition; just products that were in existence when the company was purchased. I do not include products that Google dropped immediately on purchase (Apture, fflick, Sparrow, Reqwireless, PeakStream, Wavii) or where products based on them have not been released (BumpTop).</p>
              </li>
            </ul>
            <section id="hits">
              <h3><a href="#hits" title="Link to section: ¬ß &#39;Hits&#39;">Hits</a></h3>
              <p>Ideally we would have Google hits from the day before a product was officially killed, but the past is, alas, no longer accessible to us, and we only have hits from searches I conducted 2013-04-01‚Äì2013-04-05. There are three main problems with the Google hits metric:</p>
              <ol type="1">
                <li>the Web keeps growing, so 1 million hits in 2000 are not equivalent to 1 million hits in 2013</li>
                <li>services which are not killed live longer and can rack up more hits</li>
                <li>and the longer ago a product‚Äôs hits came into existence, the more likely the relevant hits may be to have disappeared themselves.</li>
              </ol>
              <p>We can partially compensate by looking at hits averaged by lifespan; 100k hits means much less for something that lived for a decade than 100k hits means for something that lived just 6 months. What about the growth objection? We can estimate the size of Google‚Äôs index at any period and interpret the current hits as a fraction of the index when the service died (example: suppose Answers has 1 million hits, died in 2006, and in 2006 the index held 1 billion URLs, then we‚Äôd turn our 1m hit figure into 1/‚Äã1000 or 0.001); this gives us our ‚Äúdeflated hits‚Äù. We‚Äôll deflate the hits by first estimating the size of the index by fitting an exponential to the rare public reports and third-party estimates of the size of the Google index. The data points with the best linear fit:</p>
              <figure>
                <img alt="Estimating Google WWW index size over time" decoding="async" height="406" loading="lazy" sizes="(max-width: 768px) 100vw, 1127px" src="https://ntietz.com/images/google/www-index-model.png" srcset="/images/google/www-index-model.png-768px.png 768w, /images/google/www-index-model.png 1127w" width="1127"/>
                <figcaption aria-hidden="true">
                  Estimating Google WWW index size over time
                </figcaption>
              </figure>
              <p>It fits reasonably well. (A sigmoid might fit better, but maybe not, given the large disagreements towards the end.) With this we can then average over days as well, giving us 4 indices to use. We‚Äôll look closer at the hit variables later.</p>
            </section>
          </section>
          <section id="processing">
            <h2><a href="#processing" title="Link to section: ¬ß &#39;Processing&#39;">Processing</a></h2>
            <p>If a product has not ended, the end-date is defined as 2013-04-01 (which is when I stopped compiling products); then the total lifetime is simply the end-date minus the start-date. The final CSV is available at <a href="https://ntietz.com/docs/statistics/survival-analysis/2013-google.csv" data-link-icon="google" data-link-icon-type="svg"><code>2013-google.csv</code></a>‚Å†. (I welcome corrections from Googlers or Xooglers about any variables like launch or shutdown dates or products directly raising revenue.)</p>
          </section>
        </section>
        <section id="analysis">
          
          
          <section id="descriptive">
            <h2><a href="#descriptive" title="Link to section: ¬ß &#39;Descriptive&#39;">Descriptive</a></h2>
            <p>Loading up our hard-won data and looking at an R summary (for full source code reproducing all graphs and analyses below, see the <a href="#source-code">appendix</a>‚Å†; I welcome statistical corrections or elaborations if accompanied by equally reproducible R source code), we can see we have a lot of data to look at:</p>
            <div id="cb1">
              <pre><code><span id="cb1-1">    Dead            Started               Ended                 Hits               Type</span>
<span id="cb1-2">#  Mode :logical   Min.   :1997-09-15   Min.   :2005-03-16   Min.   :2.04e+03   other  : 14</span>
<span id="cb1-3">#  FALSE:227       1st Qu.:2006-06-09   1st Qu.:2012-04-27   1st Qu.:1.55e+05   program: 92</span>
<span id="cb1-4">#  TRUE :123       Median :2008-10-18   Median :2013-04-01   Median :6.50e+05   service:234</span>
<span id="cb1-5">#                  Mean   :2008-05-27   Mean   :2012-07-16   Mean   :5.23e+07   thing  : 10</span>
<span id="cb1-6">#                  3rd Qu.:2010-05-28   3rd Qu.:2013-04-01   3rd Qu.:4.16e+06</span>
<span id="cb1-7">#                  Max.   :2013-03-20   Max.   :2013-11-01   Max.   :3.86e+09</span>
<span id="cb1-8">#    Profit          FLOSS         Acquisition       Social             Days         AvgHits</span>
<span id="cb1-9">#  Mode :logical   Mode :logical   Mode :logical   Mode :logical   Min.   :   1   Min.   :      1</span>
<span id="cb1-10">#  FALSE:227       FALSE:300       FALSE:287       FALSE:305       1st Qu.: 746   1st Qu.:    104</span>
<span id="cb1-11">#  TRUE :123       TRUE :50        TRUE :63        TRUE :45        Median :1340   Median :    466</span>
<span id="cb1-12">#                                                                  Mean   :1511   Mean   :  29870</span>
<span id="cb1-13">#                                                                  3rd Qu.:2112   3rd Qu.:   2980</span>
<span id="cb1-14">#                                                                  Max.   :5677   Max.   :3611940</span>
<span id="cb1-15">#   DeflatedHits    AvgDeflatedHits  EarlyGoogle      RelativeRisk    LinearPredictor</span>
<span id="cb1-16">#  Min.   :0.0000   Min.   :-36.57   Mode :logical   Min.   : 0.021   Min.   :-3.848</span>
<span id="cb1-17">#  1st Qu.:0.0000   1st Qu.: -0.84   FALSE:317       1st Qu.: 0.597   1st Qu.:-0.517</span>
<span id="cb1-18">#  Median :0.0000   Median : -0.54   TRUE :33        Median : 1.262   Median : 0.233</span>
<span id="cb1-19">#  Mean   :0.0073   Mean   : -0.95                   Mean   : 1.578   Mean   : 0.000</span>
<span id="cb1-20">#  3rd Qu.:0.0001   3rd Qu.: -0.37                   3rd Qu.: 2.100   3rd Qu.: 0.742</span>
<span id="cb1-21">#  Max.   :0.7669   Max.   :  0.00                   Max.   :12.556   Max.   : 2.530</span>
<span id="cb1-22">#  ExpectedEvents   FiveYearSurvival</span>
<span id="cb1-23">#  Min.   :0.0008   Min.   :0.0002</span>
<span id="cb1-24">#  1st Qu.:0.1280   1st Qu.:0.1699</span>
<span id="cb1-25">#  Median :0.2408   Median :0.3417</span>
<span id="cb1-26">#  Mean   :0.3518   Mean   :0.3952</span>
<span id="cb1-27">#  3rd Qu.:0.4580   3rd Qu.:0.5839</span>
<span id="cb1-28">#  Max.   :2.0456   Max.   :1.3443</span></code></pre>
            </div><!-- Better yet, a stacked line plot of start/end intervals? something like https://stackoverflow.com/questions/9871043/increasing-the-performance-of-visualising-overlapping-segments? -->
            <section id="shutdowns-over-time">
              <h3><a href="#shutdowns-over-time" title="Link to section: ¬ß &#39;Shutdowns over time&#39;">Shutdowns over Time</a></h3>
              <div>
                <blockquote>
                  <p><strong>Google Reader</strong>: ‚ÄúWho is it in the blogs that calls on me? / I hear a tongue shriller than all the YouTubes / Cry ‚ÄòReader!‚Äô Speak, Reader is turn‚Äôd to hear.‚Äù</p>
                  <p><strong>Dataset</strong>: ‚ÄúBeware the <a href="https://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html" data-link-icon="google" data-link-icon-type="svg" title="&#39;A second spring of cleaning&#39;, 2013-03-13">ideas of March</a>‚Å†.‚Äù</p>
                  <p><a href="https://en.wikipedia.org/wiki/Julius_Caesar_(play)" data-link-icon="wikipedia" data-link-icon-type="svg"><em>Google Reader</em></a> Act‚ÄÖ1, scene 2, 15-19; with apologies.</p>
                </blockquote>
              </div>
              <p>An interesting aspect of the shutdowns is they are unevenly distributed by month as we can see with a chi-squared test (<em>p</em> = 0.014) and graphically, with a major spike in September and then March/‚ÄãApril<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a>:</p>
              <figure>
                <img alt="Shutdowns binned by month of year, revealing peaks in September, March, and April" decoding="async" height="361" loading="lazy" src="https://ntietz.com/images/google/shutdownsbymonth.png" width="690"/>
                <figcaption aria-hidden="true">
                  Shutdowns binned by month of year, revealing peaks in September, March, and April
                </figcaption>
              </figure>
              <p>As befits a company which has grown enormously since 1997, we can see other imbalances over time: eg. Google launched very few products from 1997-2004, and many more from 2005 and on:</p>
              <figure>
                <img alt="Starts binned by year" decoding="async" height="443" loading="lazy" src="https://ntietz.com/images/google/startsbyyear.png" width="642"/>
                <figcaption aria-hidden="true">
                  Starts binned by year
                </figcaption>
              </figure>
              <p>We can plot lifetime against shut-down to get a clearer picture:</p>
              <figure>
                <img alt="All products scatter-plotted date of opening vs lifespan" decoding="async" height="480" loading="lazy" src="https://ntietz.com/images/google/openedvslifespan.png" width="720"/>
                <figcaption aria-hidden="true">
                  All products scatter-plotted date of opening vs lifespan
                </figcaption>
              </figure>
              <p>That clumpiness around 2009 is suspicious. To emphasize this bulge of shutdowns in late 2011-2012, we can plot the histogram of dead products by year and also a kernel density:</p>
              <figure>
                <img alt="Shutdown density binned by year" decoding="async" height="442" loading="lazy" src="https://ntietz.com/images/google/shutdownsbyyear.png" width="676"/>
                <figcaption aria-hidden="true">
                  Shutdown density binned by year
                </figcaption>
              </figure>
              <figure>
                <img alt="Equivalent kernel density (default bandwidth)" decoding="async" height="444" loading="lazy" src="https://ntietz.com/images/google/shutdownsbyyear-kernel.png" width="398"/>
                <figcaption aria-hidden="true">
                  Equivalent kernel density (default bandwidth)
                </figcaption>
              </figure>
              <p>The kernel density brings out an aspect of shutdowns we might have missed before: there seems to be an absence of recent shut downs. There are 4 shut downs scheduled for 2013 but the last one is scheduled for November, suggesting that we have seen the last of the 2013 casualties and that any future shut downs may be for 2014.</p>
              <p>What explains such graphs over time? One candidate is the 2011-04-04 accession of Larry Page to CEO, replacing Eric Schmidt who had been hired to provide ‚Äúadult supervision‚Äù for pre-IPO Google. He respected <a href="https://en.wikipedia.org/wiki/Steve_Jobs" data-link-icon="wikipedia" data-link-icon-type="svg">Steve Jobs</a> greatly (he and Brin suggested, before meeting Schmidt, that their CEO be Jobs). <a href="https://www.amazon.com/Steve-Jobs-Walter-Isaacson/dp/1442369051?tag=gwernnet-20" data-link-icon="amazon" data-link-icon-type="svg">Isaacon‚Äôs <em>Steve Jobs</em></a> records that before his death, Jobs had strongly advised Page to ‚Äúfocus‚Äù, and asked ‚ÄúWhat are the five products you want to focus on?‚Äù, saying ‚ÄúGet rid of the rest, because they‚Äôre dragging you down.‚Äù And on <a href="https://plus.google.com/+LarryPage/posts/dRtqKJCbpZ7" data-link-icon="google" data-link-icon-type="svg">2011-07-14</a> Page posted:</p>
              <blockquote>
                <p>‚Ä¶Greater focus has also been another big feature for me this quarter ‚Äì more wood behind fewer arrows. Last month, for example, we announced that we will be closing Google Health and Google PowerMeter. We‚Äôve also done substantial internal work simplifying and streamlining our product lines. While much of that work has not yet become visible externally, I am very happy with our progress here. Focus and prioritization are crucial given our amazing opportunities.</p>
              </blockquote>
              <p>While some have <a href="https://thenextweb.com/news/larry-page-did-well-to-ignore-steve-jobs" title="Larry Page ignored Steve Jobs&#39;s deathbed advice, and Google is doing great">tried to disagree</a>‚Å†, it‚Äôs hard not to conclude that indeed, a wall of shutdowns followed in late 2011 and 2012. But this sound very much like an one-time purge: if one has a new focus on focus, then one may not be starting up as many services as before and the services which one does start up should be more likely to survive.</p>
            </section>
          </section>
          <section id="modeling">
            <h2><a href="#modeling" title="Link to section: ¬ß &#39;Modeling&#39;">Modeling</a></h2>
            <section id="logistic-regression">
              <h3><a href="#logistic-regression" title="Link to section: ¬ß &#39;Logistic regression&#39;">Logistic Regression</a></h3>
              <p>A first step in predicting when a product will be shutdown is predicting whether it will be shutdown. Since we‚Äôre predicting a binary outcome (a product living or dying), we can use the usual: an ordinary <a href="https://en.wikipedia.org/wiki/Logistic_regression" data-link-icon="wikipedia" data-link-icon-type="svg">logistic regression</a>‚Å†. Our first look uses the main variables plus the total hits:</p>
              <div id="cb2">
                <pre><code><span id="cb2-1"># Coefficients:</span>
<span id="cb2-2">#                 Estimate Std. Error z value Pr(&gt;|z|)</span>
<span id="cb2-3"># (Intercept)       2.3968     1.0680    2.24    0.025</span>
<span id="cb2-4"># Typeprogram       0.9248     0.8181    1.13    0.258</span>
<span id="cb2-5"># Typeservice       1.2261     0.7894    1.55    0.120</span>
<span id="cb2-6"># Typething         0.8805     1.1617    0.76    0.448</span>
<span id="cb2-7"># ProfitTRUE       -0.3857     0.2952   -1.31    0.191</span>
<span id="cb2-8"># FLOSSTRUE        -0.1777     0.3791   -0.47    0.639</span>
<span id="cb2-9"># AcquisitionTRUE   0.4955     0.3434    1.44    0.149</span>
<span id="cb2-10"># SocialTRUE        0.7866     0.3888    2.02    0.043</span>
<span id="cb2-11"># log(Hits)        -0.3089     0.0567   -5.45  5.1e-08</span></code></pre>
              </div>
              <p>In <a href="https://en.wikipedia.org/wiki/Log_odds" data-link-icon="wikipedia" data-link-icon-type="svg">log odds</a>‚Å†, &gt;0 increases the chance of an event (shutdown) and &lt;0 decreases it. So looking at the coefficients, we can venture some interpretations:</p>
              <ul>
                <li>
                  <p>Google has a past history of screwing up social and then killing them</p>
                  <p>This is interesting for confirming the general belief that Google has handled badly its social properties in the past, but I‚Äôm not sure how useful this is for predicting the future: since Larry Page became obsessed with social in 2009, a we might expect anything to do with ‚Äúsocial‚Äù would be either merged into Google+ or otherwise be kept on life support longer than it would before</p>
                </li>
                <li>
                  <p>Google is deprecating software products in favor of web services</p>
                  <p>A lot of Google‚Äôs efforts with Firefox and then Chromium was for improving web browsers as a platform for delivering applications. As efforts like HTML5 mature, there is less incentive for Google to release and support standalone software.</p>
                </li>
                <li>
                  <p>But apparently not its FLOSS software</p>
                  <p>This seems due to a number of its software releases being picked up by third-parties (Wave, Etherpad, Refine), designed to be integrated into existing communities (Summer of Code projects), or apparently serving a <em>strategic</em> role (Android, Chromium, Dart, Go, Closure Tools, VP Codecs) in which we could summarize as ‚Äòbuilding up a browser replacement for operating systems‚Äô. (Why? <a href="https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/" id="spolsky-2002" title="&#39;Joel on Software: Strategy Letter V&#39;, Spolsky‚ÄÖ2002">‚ÄúCommoditize your complements.‚Äù</a>)</p>
                </li>
                <li>
                  <p>things which charge or show advertising are more likely to survive</p>
                  <p>We expect this, but it‚Äôs good to have confirmation (if nothing else, it partially validates the data).</p>
                </li>
                <li>
                  <p>Popularity as measured by Google hits seems to matter</p>
                  <p>‚Ä¶Or does it? This variable seems particularly treacherous and susceptible to reverse-causation issues (does lack of hits diagnose failure, or does failing cause lack of hits when I later searched?)</p>
                </li>
              </ul>
              <section id="use-of-hits-data">
                <h4><a href="#use-of-hits-data" title="Link to section: ¬ß &#39;Use of hits data&#39;">Use of Hits Data</a></h4>
                <p>Is our popularity metric - or any of the 4 - trustworthy? All this data has been collected after the fact, sometimes many years; what if the data have been contaminated by the fact that something shutdown? For example, by a burst of publicity about an obscure service shutting down? (Ironically, this page is contributing to the inflation of hits for any dead service mentioned.) Are we just seeing <a href="https://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf" id="kaufman-al-2011" data-link-icon="pdf" data-link-icon-type="svg" title="&#39;Leakage in Data Mining: Formulation, Detection, and Avoidance&#39;, Kaufman‚ÄÖet‚ÄÖal‚ÄÖ2011">information ‚Äúleakage‚Äù</a>? Leakage can be subtle, as I <a href="#leakage">learned for myself</a> doing this analysis.</p>
                <p>Investigating further, hits by themselves do matter:</p>
                <div id="cb3">
                  <pre><code><span id="cb3-1">#             Estimate Std. Error z value Pr(&gt;|z|)</span>
<span id="cb3-2"># (Intercept)   3.4052     0.7302    4.66  3.1e-06</span>
<span id="cb3-3"># log(Hits)    -0.3000     0.0549   -5.46  4.7e-08</span></code></pre>
                </div>
                <p>Average hits (hits over the product‚Äôs lifetime) turns out to be even more important:</p>
                <div id="cb4">
                  <pre><code><span id="cb4-1">#              Estimate Std. Error z value Pr(&gt;|z|)</span>
<span id="cb4-2"># (Intercept)    -2.297      1.586   -1.45    0.147</span>
<span id="cb4-3"># log(Hits)       0.511      0.209    2.44    0.015</span>
<span id="cb4-4"># log(AvgHits)   -0.852      0.217   -3.93  8.3e-05</span></code></pre>
                </div>
                <p>This is more than a little strange; the higher the average hits, the less likely to be killed makes perfect sense but then, surely the higher the hits, the less likely as well? But no. The mystery deepens as we bring in the third hit metric we developed:</p>
                <div id="cb5">
                  <pre><code><span id="cb5-1">#                   Estimate Std. Error z value Pr(&gt;|z|)</span>
<span id="cb5-2"># (Intercept)        -21.589     11.955   -1.81   0.0709</span>
<span id="cb5-3"># log(Hits)            2.054      0.980    2.10   0.0362</span>
<span id="cb5-4"># log(AvgHits)        -1.921      0.708   -2.71   0.0067</span>
<span id="cb5-5"># log(DeflatedHits)   -0.456      0.277   -1.64   0.1001</span></code></pre>
                </div>
                <p>And sure enough, if we run all 4 hit variables, 3 of them turn out to be <a href="https://en.wikipedia.org/wiki/Statistical_significance" data-link-icon="wikipedia" data-link-icon-type="svg">statistically-significant</a> and large:</p>
                <div id="cb6">
                  <pre><code><span id="cb6-1">#                   Estimate Std. Error z value Pr(&gt;|z|)</span>
<span id="cb6-2"># (Intercept)       -24.6898    12.4696   -1.98   0.0477</span>
<span id="cb6-3"># log(Hits)           2.2908     1.0203    2.25   0.0248</span>
<span id="cb6-4"># log(AvgHits)       -2.0943     0.7405   -2.83   0.0047</span>
<span id="cb6-5"># log(DeflatedHits)  -0.5383     0.2914   -1.85   0.0647</span>
<span id="cb6-6"># AvgDeflatedHits    -0.0651     0.0605   -1.08   0.2819</span></code></pre>
                </div>
                <p>It‚Äôs not that the hit variables are somehow summarizing or proxying for the others, because if we toss in all the non-hits predictors and penalize parameters based on adding complexity without increasing fit, we still wind up with the 3 hit variables:</p>
                <div id="cb7">
                  <pre><code><span id="cb7-1">#                   Estimate Std. Error z value Pr(&gt;|z|)</span>
<span id="cb7-2"># (Intercept)        -23.341     12.034   -1.94   0.0524</span>
<span id="cb7-3"># AcquisitionTRUE      0.631      0.350    1.80   0.0712</span>
<span id="cb7-4"># SocialTRUE           0.907      0.394    2.30   0.0213</span>
<span id="cb7-5"># log(Hits)            2.204      0.985    2.24   0.0252</span>
<span id="cb7-6"># log(AvgHits)        -2.068      0.713   -2.90   0.0037</span>
<span id="cb7-7"># log(DeflatedHits)   -0.492      0.280   -1.75   0.0793</span>
<span id="cb7-8"># ...</span>
<span id="cb7-9"># AIC: 396.9</span></code></pre>
                </div>
                <p>Most of the predictors were removed as not helping a lot, 3 of the 4 hit variables survived (but not the both averaged &amp; deflated hits, suggesting it wasn‚Äôt adding much in combination), and we see two of the better predictors from earlier survived: whether something was an acquisition and whether it was social.</p>
                <p>The original hits variable has the wrong sign, as expected of data leakage; now the average and deflated hits have the predicted sign (the higher the hit count, the lower the risk of death), but this doesn‚Äôt put to rest my concerns: the average hits has the right sign, yes, but now the <a href="https://en.wikipedia.org/wiki/Effect_sizes" data-link-icon="wikipedia" data-link-icon-type="svg">effect size</a> seems way too high - we reject the hits with a log-odds of +2.1 as contaminated and a correlation almost 4 times larger than one of the known-good correlations (being an acquisition), but the average hits is -2 &amp; almost as big a log odds! The only variable which seems trustworthy is the deflated hits: it has the right sign and is a more plausible 5x smaller. I‚Äôll use just the deflated hits variable (although I will keep in mind that I‚Äôm still not sure it is free from data leakage).</p>
              </section>
            </section>
            <section id="survival-curve">
              <h3><a href="#survival-curve" title="Link to section: ¬ß &#39;Survival curve&#39;">Survival Curve</a></h3>
              <p>The logistic regression helped winnow down the variables but is limited to the binary outcome of shutdown or not; it can‚Äôt use the potentially very important variable of how many days a product has survived for the simple reason that <em>of course</em> mortality will increase with time! (‚ÄúBut this long run is a misleading guide to current affairs. In the long run we are all dead.‚Äù)</p>
              <p>For looking at survival over time, <a href="https://en.wikipedia.org/wiki/Survival_analysis" data-link-icon="wikipedia" data-link-icon-type="svg">survival analysis</a> might be an useful elaboration. Not being previously familiar with the area, I drew on Wikipedia, <a href="https://socialsciences.mcmaster.ca/jfox/Books/Companion-2E/appendix/Appendix-Cox-Regression.pdf" data-link-icon="pdf" data-link-icon-type="svg">Fox &amp; Weisberg‚Äôs appendix</a>‚Å†, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1065034/" id="bewick-et-al-2004" data-link-icon="nlm-ncbi" data-link-icon-type="svg" title="Statistics review 12: Survival analysis"><span> <span>et al</span> <span>2004</span></span></a>‚Å†, <a href="https://ntietz.com/docs/www/citeseerx.ist.psu.edu/11b669b9ade34da88858b1a09fdd381f7ef3b90a.pdf" data-link-icon="pdf" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.551.3600&amp;rep=rep1&amp;type=pdf" title="Use Software R to do Survival Analysis and Simulation. A tutorial (Original URL: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.551.3600&amp;rep=rep1&amp;type=pdf )">Zhou‚Äôs tutorial</a>‚Å†, and Hosmer &amp; Lemeshow‚Äôs <a href="https://www.amazon.com/Applied-Survival-Analysis-Regression-Probability/dp/0471754994/?tag=gwernnet-20" data-link-icon="amazon" data-link-icon-type="svg"><em>Applied Survival Analysis</em></a> for the following results using the <code>survival</code> library (see also <a href="https://cran.r-project.org/web/views/Survival.html" data-link-icon="R" data-link-icon-type="text">CRAN Task View: Survival Analysis</a>‚Å†, and the taxonomy of survival analysis methods in <a href="https://ntietz.com/docs/www/arxiv.org/6ecfb2f70d4fba3492e57801908f61eca0d06d04.pdf" id="wang-et-al-2017" data-link-icon="ùõò" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/1708.04649?fallback=original" title="Machine Learning for Survival Analysis: A Survey (Original URL: https://arxiv.org/abs/1708.04649 )"><span><span title="et al">Wang</span> <span>et al</span> <span>2017</span></span></a>). Any errors are mine.</p>
              <p>The initial characterization gives us an optimistic median of 2824 days (note that this is higher than Arthur‚Äôs mean of 1459 days because it addressed the conditionality issue discussed earlier by including products which were never canceled, and I made a stronger effort to collect pre-2009 products), but the lower bound is not tight and too little of the sample has died to get an upper bound:</p>
              <div id="cb8">
                <pre><code><span id="cb8-1"># records   n.max n.start  events  median 0.95LCL 0.95UCL</span>
<span id="cb8-2">#     350     350     350     123    2824    2095      NA</span></code></pre>
              </div>
              <p>Our overall <a href="https://en.wikipedia.org/wiki/Kaplan-Meier_estimator" data-link-icon="wikipedia" data-link-icon-type="svg">Kaplan-Meier</a> <a href="https://en.wikipedia.org/wiki/Survivorship_curve" data-link-icon="wikipedia" data-link-icon-type="svg">survivorship curve</a> looks a bit interesting:</p>
              <figure>
                <img alt="Shutdown cumulative probability as a function of time" decoding="async" height="402" loading="lazy" src="https://ntietz.com/images/google/overall-survivorship-curve.png" width="604"/>
                <figcaption aria-hidden="true">
                  Shutdown cumulative probability as a function of time
                </figcaption>
              </figure>
              <p>If there were constant mortality of products at each day after their launch, we would expect a ‚Äútype II‚Äù curve where it looks like a straight line, and if the hazard <a href="https://www.lesswrong.com/posts/GytPrQ9cT46k9etoz/living-forever-is-hard-or-the-gompertz-curve" data-link-icon="LW" data-link-icon-type="text" title="Living Forever is Hard, or, The Gompertz Curve">increased with age like with humans</a> we would see a ‚Äútype I‚Äù graph in which the curve nose-dives; but in fact it looks like there‚Äôs a sort of ‚Äúleveling off‚Äù of deaths, suggesting a ‚Äútype III‚Äù curve; per Wikipedia:</p>
              <blockquote>
                <p>‚Ä¶the greatest mortality is experienced early on in life, with relatively low rates of death for those surviving this bottleneck. This type of curve is characteristic of species that produce a large number of offspring (see <a href="https://en.wikipedia.org/wiki/R/K_selection_theory" data-link-icon="wikipedia" data-link-icon-type="svg">r‚Ää/‚Ää‚Ää‚ÄãK selection theory</a>).</p>
              </blockquote>
              <p>Very nifty: the survivorship curve is consistent with tech industry or startup philosophies of doing lots of things, iterating fast, and throwing things at the wall to see what sticks. (More pleasingly, it suggests that my dataset is not biased against the inclusion of short-lived products: if I had been failing to find a lot of short-lived products, then we would expect to see the true survivorship curve distorted into something of a type II or type I curve and not a type III curve where a lot of products are early deaths; so if there were a data collection bias against short-lived products, then the true survivorship curve must be even more extremely type III.)</p>
              <p>However, it looks like the mortality only starts decreasing around 2000 days, so any product that far out must have been founded around or before 2005, which is when we previously noted that Google started pumping out a lot of products and may also have changed its shutdown-related behaviors; this could violate a basic assumption of <a href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator" data-link-icon="wikipedia" data-link-icon-type="svg">Kaplan-Meier</a>‚Å†, that the underlying survival function isn‚Äôt itself changing over time.</p>
              <p>Our next step is to fit a Cox <a href="https://en.wikipedia.org/wiki/Proportional_hazards_model" data-link-icon="wikipedia" data-link-icon-type="svg">proportional hazards model</a> to our covariates:</p>
              <div id="cb9">
                <pre><code><span id="cb9-1"># ...n= 350, number of events= 123</span>
<span id="cb9-2">#</span>
<span id="cb9-3">#                     coef exp(coef) se(coef)     z Pr(&gt;|z|)</span>
<span id="cb9-4"># AcquisitionTRUE    0.130     1.139    0.257  0.51    0.613</span>
<span id="cb9-5"># FLOSSTRUE          0.141     1.151    0.293  0.48    0.630</span>
<span id="cb9-6"># ProfitTRUE        -0.180     0.836    0.231 -0.78    0.438</span>
<span id="cb9-7"># SocialTRUE         0.664     1.943    0.262  2.53    0.011</span>
<span id="cb9-8"># Typeprogram        0.957     2.603    0.747  1.28    0.200</span>
<span id="cb9-9"># Typeservice        1.291     3.638    0.725  1.78    0.075</span>
<span id="cb9-10"># Typething          1.682     5.378    1.023  1.64    0.100</span>
<span id="cb9-11"># log(DeflatedHits) -0.288     0.749    0.036 -8.01  1.2e-15</span>
<span id="cb9-12">#</span>
<span id="cb9-13">#                   exp(coef) exp(-coef) lower .95 upper .95</span>
<span id="cb9-14"># AcquisitionTRUE       1.139      0.878     0.688     1.884</span>
<span id="cb9-15"># FLOSSTRUE             1.151      0.868     0.648     2.045</span>
<span id="cb9-16"># ProfitTRUE            0.836      1.197     0.531     1.315</span>
<span id="cb9-17"># SocialTRUE            1.943      0.515     1.163     3.247</span>
<span id="cb9-18"># Typeprogram           2.603      0.384     0.602    11.247</span>
<span id="cb9-19"># Typeservice           3.637      0.275     0.878    15.064</span>
<span id="cb9-20"># Typething             5.377      0.186     0.724    39.955</span>
<span id="cb9-21"># log(DeflatedHits)     0.749      1.334     0.698     0.804</span>
<span id="cb9-22">#</span>
<span id="cb9-23"># Concordance= 0.726  (se = 0.028 )</span>
<span id="cb9-24"># Rsquare= 0.227   (max possible= 0.974 )</span>
<span id="cb9-25"># Likelihood ratio test= 90.1  on 8 df,   p=4.44e-16</span>
<span id="cb9-26"># Wald test            = 79.5  on 8 df,   p=6.22e-14</span>
<span id="cb9-27"># Score (logrank) test = 83.5  on 8 df,   p=9.77e-15</span></code></pre>
              </div>
              <p>And then we can also test whether any of the covariates are suspicious; in general they seem to be fine:</p>
              <div id="cb10">
                <pre><code><span id="cb10-1">#                       rho  chisq     p</span>
<span id="cb10-2"># AcquisitionTRUE   -0.0252 0.0805 0.777</span>
<span id="cb10-3"># FLOSSTRUE          0.0168 0.0370 0.848</span>
<span id="cb10-4"># ProfitTRUE        -0.0694 0.6290 0.428</span>
<span id="cb10-5"># SocialTRUE         0.0279 0.0882 0.767</span>
<span id="cb10-6"># Typeprogram        0.0857 0.9429 0.332</span>
<span id="cb10-7"># Typeservice        0.0936 1.1433 0.285</span>
<span id="cb10-8"># Typething          0.0613 0.4697 0.493</span>
<span id="cb10-9"># log(DeflatedHits) -0.0450 0.2610 0.609</span>
<span id="cb10-10"># GLOBAL                 NA‚ÄÖ2.5358 0.960</span></code></pre>
              </div>
              <p>My suspicion lingers, though, so I threw in another covariate (<code>EarlyGoogle</code>): whether a product was released before or after 2005. Does this add predictive value above and over simply knowing that a product is really old, and does the regression still pass the proportional assumption check? Apparently yes to both:</p>
              <div id="cb11">
                <pre><code><span id="cb11-1">#                      coef exp(coef) se(coef)     z Pr(&gt;|z|)</span>
<span id="cb11-2"># AcquisitionTRUE    0.1674    1.1823   0.2553  0.66    0.512</span>
<span id="cb11-3"># FLOSSTRUE          0.1034    1.1090   0.2922  0.35    0.723</span>
<span id="cb11-4"># ProfitTRUE        -0.1949    0.8230   0.2318 -0.84    0.401</span>
<span id="cb11-5"># SocialTRUE         0.6541    1.9233   0.2601  2.51    0.012</span>
<span id="cb11-6"># Typeprogram        0.8195    2.2694   0.7472  1.10    0.273</span>
<span id="cb11-7"># Typeservice        1.1619    3.1960   0.7262  1.60    0.110</span>
<span id="cb11-8"># Typething          1.6200    5.0529   1.0234  1.58    0.113</span>
<span id="cb11-9"># log(DeflatedHits) -0.2645    0.7676   0.0375 -7.06  1.7e-12</span>
<span id="cb11-10"># EarlyGoogleTRUE   -1.0061    0.3656   0.5279 -1.91    0.057</span>
<span id="cb11-11"># ...</span>
<span id="cb11-12"># Concordance= 0.728  (se = 0.028 )</span>
<span id="cb11-13"># Rsquare= 0.237   (max possible= 0.974 )</span>
<span id="cb11-14"># Likelihood ratio test= 94.7  on 9 df,   p=2.22e-16</span>
<span id="cb11-15"># Wald test            = 76.7  on 9 df,   p=7.2e-13</span>
<span id="cb11-16"># Score (logrank) test = 83.8  on 9 df,   p=2.85e-14</span></code></pre>
              </div>
              <div id="cb12">
                <pre><code><span id="cb12-1">#                        rho   chisq     p</span>
<span id="cb12-2"># ...</span>
<span id="cb12-3"># EarlyGoogleTRUE   -0.05167 0.51424 0.473</span>
<span id="cb12-4"># GLOBAL                  NA‚ÄÖ2.52587 0.980</span></code></pre>
              </div>
              <p>As predicted, the pre-2005 variable does indeed correlate to less chance of being shutdown, is the third-largest predictor, and almost reaches a random<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a> level of statistical-significance - but it doesn‚Äôt trigger the assumption tester, so we‚Äôll keep using the Cox model.</p>
              <p>Now let‚Äôs interpret the model. The covariates tell us that to reduce the risk of shutdown, you want to:</p>
              <ol type="1">
                <li>Not be an acquisition</li>
                <li>Not be FLOSS</li>
                <li>Be directly making money</li>
                <li>Not be related to social networking</li>
                <li>Have lots of Google hits relative to lifetime</li>
                <li>Have been launched early in Google‚Äôs lifetime</li>
              </ol>
              <p>This all makes sense to me. I find particularly interesting the profit and social effects, but the odds are a little hard to understand intuitively; if being social increases the odds of shutdown by 1.9233 and not being directly profitable increases the odds by 1.215, what do those <em>look</em> like? We can graph pairs of survivorship curves, splitting the full dataset (omitting the <a href="https://en.wikipedia.org/wiki/Confidence_interval" data-link-icon="wikipedia" data-link-icon-type="svg">confidence intervals</a> for legibility, although they do overlap), to get a grasp of what these numbers mean:</p>
              <figure>
                <img alt="All products over time, split by Profit variable" decoding="async" height="403" loading="lazy" src="https://ntietz.com/images/google/profit-survivorship-curve.png" width="603"/>
                <figcaption aria-hidden="true">
                  All products over time, split by <code>Profit</code> variable
                </figcaption>
              </figure>
              <figure>
                <img alt="All products over time, split by Social variable" decoding="async" height="402" loading="lazy" src="https://ntietz.com/images/google/social-survivorship-curve.png" width="603"/>
                <figcaption aria-hidden="true">
                  All products over time, split by <code>Social</code> variable
                </figcaption>
              </figure>
            </section>
            <section id="random-forests">
              <h3><a href="#random-forests" title="Link to section: ¬ß &#39;Random forests&#39;">Random Forests</a></h3>
              <p>Because I can, I was curious how <a href="https://en.wikipedia.org/wiki/Random_forests" data-link-icon="wikipedia" data-link-icon-type="svg">random forests</a> (<a href="https://ntietz.com/docs/ai/2001-breiman.pdf" data-link-icon="pdf" data-link-icon-type="svg" title="Random Forests"><span><span>Breiman</span><span>2001</span></span></a>) might stack up to the logistic regression and against a base-rate predictor (that nothing was shut down, since ~65% of the products are still alive).</p>
              <p>With <a href="https://cran.r-project.org/web/packages/randomForest/index.html" data-link-icon="R" data-link-icon-type="text"><code>randomForest</code></a>‚Å†, I trained a <a href="https://en.wikipedia.org/wiki/Random_forest" data-link-icon="wikipedia" data-link-icon-type="svg">random forest</a> as a classifier, yielding reasonable looking error rates:</p>
              <div id="cb13">
                <pre><code><span id="cb13-1">#                Type of random forest: classification</span>
<span id="cb13-2">#                      Number of trees: 500</span>
<span id="cb13-3"># No. of variables tried at each split: 2</span>
<span id="cb13-4">#</span>
<span id="cb13-5">#         OOB estimate of  error rate: 31.71%</span>
<span id="cb13-6"># Confusion matrix:</span>
<span id="cb13-7">#       FALSE TRUE class.error</span>
<span id="cb13-8"># FALSE   216   11     0.04846</span>
<span id="cb13-9"># TRUE    100   23     0.81301</span></code></pre>
              </div>
              <p>To compare the random forest accuracy with the logistic model‚Äôs accuracy, I interpreted the logistic estimate of shutdown odds &gt;1 as predicting shutdown and &lt;1 as predicting not shutdown; I then compared the full sets of predictions with the actual shutdown status. (This is not a <a href="https://en.wikipedia.org/wiki/Proper_scoring_rule" data-link-icon="wikipedia" data-link-icon-type="svg">proper scoring rule</a> like those I employed in grading forecasts of the <a href="https://ntietz.com/2012-election-predictions" id="gwern-2012-election-predictions" title="&#39;2012 election predictions&#39;, Branwen‚ÄÖ2012">2012 American elections</a>‚Å†, but this should be an intuitively understandable way of grading models‚Äô predictions.)</p>
              <p>The base-rate predictor got 65% right by definition, the logistic managed to score 68% correct (<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" data-link-icon="wikipedia" data-link-icon-type="svg">bootstrap</a><a href="#fn17" id="fnref17" role="doc-noteref"><sup>17</sup></a> 95% CI: 66-72%), and the random forest similarly got 68% (67-78%). These rates are not quite as bad as they may seem: I excluded the lifetime length (<code>Days</code>) from the logistic and random forests because unless one is handling it specially with survival analysis, <a href="#leakage">it leaks information</a>‚Å†; so there‚Äôs predictive power being left on the table. A fairer comparison would use lifetimes.</p>
              <section id="random-survival-forests">
                <h4><a href="#random-survival-forests" title="Link to section: ¬ß &#39;Random survival forests&#39;">Random Survival Forests</a></h4>
                <p>The next step is to take into account lifetime length &amp; estimated survival curves. We can do that using <a href="https://ntietz.com/docs/www/arxiv.org/b460859e28ae22db03ade292fee0baf0ddf0476b.pdf" id="ishwaran-et-al-2008" data-link-icon="ùõò" data-link-icon-type="text" rel="archived alternate nofollow" data-url-original="https://ar5iv.labs.arxiv.org/html/0811.1645?fallback=original" title="Ishwaran‚ÄÖet‚ÄÖal‚ÄÖ2008 (Original URL: https://arxiv.org/abs/0811.1645 )">‚ÄúRandom survival forests‚Äù</a> (see also <a href="https://www.jstatsoft.org/index.php/jss/article/download/v050i11/634" title="Evaluating Random Forests for Survival Analysis: Using Prediction Error Curves">‚Äú<span><span title="et al">Mogensen</span> <span>et al</span> <span>2012</span></span>‚Äù</a>), implemented in <a href="https://cran.r-project.org/web/packages/randomForestSRC/index.html" data-link-icon="R" data-link-icon-type="text"><code>randomForestSRC</code></a> (successor to Ishwaran‚Äôs original library <a href="https://cran.r-project.org/web/packages/randomSurvivalForest/index.html" data-link-icon="R" data-link-icon-type="text"><code>randomSurvivalForest</code></a>). This initially seems very promising:</p>
                <div id="cb14">
                  <pre><code><span id="cb14-1">#                          Sample size: 350</span>
<span id="cb14-2">#                     Number of deaths: 122</span>
<span id="cb14-3">#                      Number of trees: 1000</span>
<span id="cb14-4">#           Minimum terminal node size: 3</span>
<span id="cb14-5">#        Average no. of terminal nodes: 61.05</span>
<span id="cb14-6"># No. of variables tried at each split: 3</span>
<span id="cb14-7">#               Total no. of variables: 7</span>
<span id="cb14-8">#                             Analysis: Random Forests [S]RC</span>
<span id="cb14-9">#                               Family: surv</span>
<span id="cb14-10">#                       Splitting rule: logrank *random*</span>
<span id="cb14-11">#        Number of random split points: 1</span>
<span id="cb14-12">#               Estimate of error rate: 35.37%</span></code></pre>
                </div>
                <p>and even gives us a cute plot of how accuracy varies with how big the forest is (looks like we don‚Äôt need to tweak it) and how important each variable is as a predictor:</p>
                <figure>
                  <img alt="Visual comparison of the average usefulness of each variable to decision trees" decoding="async" height="393" loading="lazy" src="https://ntietz.com/images/google/rsf-importance.png" width="694"/>
                  <figcaption aria-hidden="true">
                    Visual comparison of the average usefulness of each variable to decision trees
                  </figcaption>
                </figure>
                <p>Estimating the error rate for this random survival forest like we did previously, we‚Äôre happy to see a 78% error rate. Building a predictor based on the Cox model, we get a lesser (but still better than the non-survival models) 72% error rate.</p>
                <p>How do these models perform when we check their robustness via the bootstrap? Not so great. The random survival forest collapses to 57-64% (95% on 200 replicates), but the Cox model just to 68-73%. This suggests to me that something is going wrong with the random survival forest model (overfitting? programming error?) and there‚Äôs no real reason to switch to the more complex random forests, so here too we‚Äôll stick with the ordinary Cox model.</p>
              </section>
            </section>
          </section>
          <section id="predictions">
            <h2><a href="#predictions" title="Link to section: ¬ß &#39;Predictions&#39;">Predictions</a></h2>
            <p>Before making explicit predictions of the future, let‚Äôs look at the <a href="https://en.wikipedia.org/wiki/Relative_risks" data-link-icon="wikipedia" data-link-icon-type="svg">relative risks</a> for products which haven‚Äôt been shutdown. What does the Cox model consider the 10 most at risk and likely to be shutdown products?</p>
            <p>It lists (in decreasingly risky order):</p>
            <div>
              <ol type="1">
                <li>Schemer</li>
                <li>Boutiques</li>
                <li>Magnifier</li>
                <li>Hotpot</li>
                <li>Page Speed Online API</li>
                <li>WhatsonWhen</li>
                <li>Unofficial Guides</li>
                <li>WDYL search engine</li>
                <li>Cloud Messaging</li>
                <li>Correlate</li>
              </ol>
            </div>
            <p>These all seem like reasonable products to signal out (as much as I love Correlate for making it <a href="https://slatestarcodex.com/2013/02/16/google-correlate-does-not-imply-google-causation/" data-link-icon="google" data-link-icon-type="svg">easier than ever to demonstrate ‚Äúcorrelation ‚â† causation‚Äù</a>‚Å†, I‚Äôm surprised it or Boutiques still exist), except for Cloud Messaging which seems to be a key part of a lot of Android. And likewise, the list of the 10 <em>least</em> risky (increasingly risky order):</p>
            <div>
              <ol type="1">
                <li>Search</li>
                <li>Translate</li>
                <li>AdWords</li>
                <li>Picasa</li>
                <li>Groups</li>
                <li>Image Search</li>
                <li>News</li>
                <li>Books</li>
                <li>Toolbar</li>
                <li>AdSense</li>
              </ol>
            </div>
            <p>One can‚Äôt imagine flagship products like Search or Books ever being shut down, so this list is good as far as it goes; I am skeptical about the actual unriskiness of Picasa and Toolbar given their general neglect and old-fashionedness, though I understand why the model favors them (both are pre-2005, proprietary, many hits, and advertising-supported). But let‚Äôs get more specific; looking at still alive services, what predictions do we make about the odds of a selected batch surviving the next, say, 5 years? We can derive a survival curve for each member of the batch adjusted for each subject‚Äôs covariates (and they visibly differ from each other):</p>
            <figure>
              <img alt="Estimated curves for 15 interesting products (AdSense, Scholar, Voice, etc)" decoding="async" height="442" loading="lazy" src="https://ntietz.com/images/google/15-predicted-survivorship-curves.png" width="666"/>
              <figcaption aria-hidden="true">
                Estimated curves for 15 interesting products (AdSense, Scholar, Voice, etc)
              </figcaption>
            </figure>
            <p>But these are the curves for hypothetical populations all like the specific product in question, starting from Day 0. Can we extract specific estimates assuming the product has survived to today (as by definition these live services have done)? Yes, but extracting them turns out to be a pretty gruesome hack to extract predictions from survival curves; anyway, I derive the following 5-year estimates and as commentary, register my own best guesses as well (I‚Äôm <a href="https://ntietz.com/Prediction-markets" id="gwern-prediction-markets" title="&#39;Prediction Markets&#39;, Branwen‚ÄÖ2009">not too bad</a> at making predictions):</p>
            <div>
              <table>
                <thead>
                  <tr>
                    <th>Product</th>
                    <th>5-year survival</th>
                    <th>Personal guess</th>
                    <th>Relative risk vs average (lower = better)</th>
                    <th>Survived (March‚ÄÖ2018)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>AdSense</td>
                    <td>100%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17897" data-link-icon="?" data-link-icon-type="text,sans,bold">99%</a>
                    </td>
                    <td>0.07</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Blogger</td>
                    <td>100%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17900" data-link-icon="?" data-link-icon-type="text,sans,bold">80%</a>
                    </td>
                    <td>0.32</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Gmail</td>
                    <td>96%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17904" data-link-icon="?" data-link-icon-type="text,sans,bold">99%</a>
                    </td>
                    <td>0.08</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Search</td>
                    <td>96%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17912" data-link-icon="?" data-link-icon-type="text,sans,bold">100%</a>
                    </td>
                    <td>0.05</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Translate</td>
                    <td>92%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17913" data-link-icon="?" data-link-icon-type="text,sans,bold">95%</a>
                    </td>
                    <td>0.78</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Scholar</td>
                    <td>92%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17906" data-link-icon="?" data-link-icon-type="text,sans,bold">85%</a>
                    </td>
                    <td>0.10</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Alerts</td>
                    <td>89%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17898" data-link-icon="?" data-link-icon-type="text,sans,bold">70%</a>
                    </td>
                    <td>0.21</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Google+</td>
                    <td>79%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17905" data-link-icon="?" data-link-icon-type="text,sans,bold">85%</a>
                    </td>
                    <td>0.36</td>
                    <td>
                      Yes<a href="#fn18" id="fnref18" role="doc-noteref"><sup>18</sup></a>
                    </td>
                  </tr>
                  <tr>
                    <td>Analytics</td>
                    <td>76%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17899" data-link-icon="?" data-link-icon-type="text,sans,bold">97%</a>
                    </td>
                    <td>0.24</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Chrome</td>
                    <td>70%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17916" data-link-icon="?" data-link-icon-type="text,sans,bold">95%</a>
                    </td>
                    <td>0.24</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Calendar</td>
                    <td>66%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17901" data-link-icon="?" data-link-icon-type="text,sans,bold">95%</a>
                    </td>
                    <td>0.36</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Docs</td>
                    <td>63%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17902" data-link-icon="?" data-link-icon-type="text,sans,bold">95%</a>
                    </td>
                    <td>0.39</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>
                      Voice<a href="#fn19" id="fnref19" role="doc-noteref"><sup>19</sup></a>
                    </td>
                    <td>44%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17907" data-link-icon="?" data-link-icon-type="text,sans,bold">50%</a>
                    </td>
                    <td>0.78</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>FeedBurner</td>
                    <td>43%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17903" data-link-icon="?" data-link-icon-type="text,sans,bold">35%</a>
                    </td>
                    <td>0.66</td>
                    <td>Yes</td>
                  </tr>
                  <tr>
                    <td>Project Glass</td>
                    <td>37%</td>
                    <td>
                      <a href="https://predictionbook.com/predictions/17911" data-link-icon="?" data-link-icon-type="text,sans,bold">50%</a>
                    </td>
                    <td>0.10</td>
                    <td>No</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <p>One immediately spots that some of the model‚Äôs estimates seem questionable in the light of our greater knowledge of Google.</p>
            <p>I am more pessimistic about the <a href="https://ntietz.com/Google-Alerts" id="gwern-google-alerts" title="&#39;Alerts Over Time&#39;, Branwen‚ÄÖ2013">much-neglected Alerts</a>‚Å†. And I think it‚Äôs absurd to give any serious credence Analytics or Calendar or Docs being at risk (Analytics is a key part of the advertising infrastructure, and Calendar a sine qua non of any business software suite - much less the core of said suite, Docs!). The Glass estimate is also interesting: I don‚Äôt know if I agree with the model, given how famous Glass is and how much Google is pushing it - could its future really be so chancy? On the other hand, many tech fads have come and go without a trace, hardware is always tricky, the more intimate a gadget the more design matters (Glass seems like the sort of thing Apple could make a blockbuster, but can Google?), Glass has already received a <a href="https://ntietz.com/docs/www/www.businessinsider.com/43fcb0f83881ce4d6ed63b5307a7ac77ce2950be.html" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://www.businessinsider.com/nobody-really-likes-google-glass-2013-5" title="(Original URL: https://www.businessinsider.com/nobody-really-likes-google-glass-2013-5 )">hefty helping of criticism</a>‚Å†, particularly the man most experienced with such HUDs (<a href="https://en.wikipedia.org/wiki/Steve_Mann_(inventor)" data-link-icon="wikipedia" data-link-icon-type="svg">Steve Mann</a>) <a href="https://spectrum.ieee.org/steve-mann-my-augmediated-life" data-link-icon="IEEE" data-link-icon-type="text,mono,quad" title="Steve Mann: My &#39;Augmediated&#39; Life: What I&#39;ve learned from 35 years of wearing computerized eyewear">has criticized Glass</a> as being ‚Äúmuch less ambitious‚Äù than the state of the art and worries that ‚ÄúGoogle and certain other companies are neglecting some important lessons. Their design decisions could make it hard for many folks to use these systems. Worse, poorly configured products might even damage some people‚Äôs eyesight and set the movement back years. My concern comes from direct experience.‚Äù</p>
            <p>But some estimates are more forgivable - Google <em>does</em> have a bad track record with social media so some level of skepticism about Google+ seems warranted (and indeed, in October‚ÄÖ2018 Google <a href="https://blog.google/technology/safety-security/project-strobe/" data-link-icon="google" data-link-icon-type="svg" title="Project Strobe: Protecting your data, improving our third-party APIs, and sunsetting consumer Google+">quietly announced public Google+ would be shut down &amp; henceforth only an enterprise product</a>) - and on FeedBurner or Voice, I agree with the model that their future is cloudy. The extreme optimism about Blogger is interesting since before I began this project, I thought it was slowly dying and would inevitably shut down in a few years; but as I researched the timelines for various Google products, I noticed that Blogger seems to be favored in some ways: such as getting exclusive access to a few otherwise shutdown things (eg. Scribe &amp; Friend Connect); it was the ground zero for Google‚Äôs Dynamic Views skin redesign which was applied globally; and Google is still heavily using Blogger for all its official announcements even into the Google+ era.</p>
            <p>Overall, these are pretty sane-sounding estimates.</p>
          </section>
        </section>
        <section id="followups">
          
          <div>
            <blockquote>
              <p>‚ÄúShow me the person who doesn‚Äôt die‚Äî
              </p>
              <p>Han-Shan, #50</p>
            </blockquote>
          </div>
          <p>It seems like it might be worthwhile to continue compiling a database and do a followup analysis in 5 years (2018), by which point we can judge how my predictions stacked up against the model, and also because ~100 products may have been shut down (going by the &gt;30 casualties of 2011 and 2012) and the survival curve &amp; covariate estimates rendered that much sharper. So to compile updates, I‚Äôve:</p>
          <ul>
            <li>
              <p>set up 2 Google Alerts searches:</p>
              <ul>
                <li><code>google (&#34;shut down&#34; OR &#34;shut down&#34; &#34;shutting&#34; OR &#34;closing&#34; OR &#34;killing&#34; OR &#34;abandoning&#34; OR &#34;leaving&#34;)</code></li>
                <li><code>google (launch OR release OR announce)</code></li>
              </ul>
            </li>
            <li>
              <p>and subscribed to the aforementioned Google Operating System blog</p>
            </li>
          </ul>
          <p>These sources yielded ~64 candidates over the following year before I shut down additions 2014-06-04.</p>
        </section>
        <section id="see-also">
          
          
        </section>
        <section id="external-links">
          
          <ul>
            <li>
              <p><a href="https://www.lemonde.fr/pixels/visuel/2015/03/06/google-memorial-le-petit-musee-des-projets-google-abandonnes_4588392_4408996.html" data-link-icon="google" data-link-icon-type="svg">‚ÄúGoogle Memorial, le petit mus√©e des projets Google abandonn√©s‚Äù</a></p>
            </li>
            <li>
              <p><a href="https://ntietz.com/docs/www/killedbygoogle.com/ed702ecb27bfb6421bbe76b9c067e5149d0b7c46.html" data-link-icon="google" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://killedbygoogle.com/" title="(Original URL: https://killedbygoogle.com/ )">Killed by Google</a>‚Å†; <a href="https://killedbymicrosoft.info/" data-link-icon="MS" data-link-icon-type="text,sans,italic">Killed by Microsoft</a></p>
            </li>
            <li>
              <p><a href="https://en.wikipedia.org/wiki/Archive_Team" data-link-icon="wikipedia" data-link-icon-type="svg">Archive Team</a> (<a href="https://wiki.archiveteam.org/index.php?title=ArchiveTeam_Warrior" data-link-icon="internetarchive" data-link-icon-type="svg">ArchiveTeam Warrior</a>: <a href="https://wiki.archiveteam.org/index.php?title=Google_Reader" data-link-icon="internetarchive" data-link-icon-type="svg">Reader</a>)</p>
            </li>
            <li>
              <p><a href="https://arstechnica.com/gadgets/2019/04/googles-constant-product-shutdowns-are-damaging-its-brand/" data-link-icon="google" data-link-icon-type="svg">‚ÄúGoogle‚Äôs constant product shutdowns are damaging its brand: Google‚Äôs product support has become a joke, and the company should be very concerned‚Äù</a></p>
            </li>
            <li>
              <p><a href="https://www.theverge.com/2021/6/21/22538240/google-chat-allo-hangouts-talk-messaging-mess-timeline" data-link-icon="google" data-link-icon-type="svg">‚ÄúA very brief history of every Google messaging app: What a decade and a half of confusing services can tell us about the future‚Äù</a>‚Å†; <a href="https://arstechnica.com/gadgets/2021/08/a-decade-and-a-half-of-instability-the-history-of-google-messaging-apps/" data-link-icon="google" data-link-icon-type="svg">‚ÄúA decade and a half of instability: The history of Google messaging apps: 16 years after the launch of Google Talk, Google messaging is still a mess‚Äù</a></p>
            </li>
            <li>
              <p>Comments:</p>
              <ul>
                <li>
                  <a href="https://ntietz.com/docs/www/news.ycombinator.com/4f43f8d48f544da5f2d70ca264036ff0a1139ea5.html" data-link-icon="hn" data-link-icon-type="svg" rel="archived alternate nofollow" data-url-original="https://news.ycombinator.com/item?id=5653748" title="(Original URL: https://news.ycombinator.com/item?id=5653748 )">Hacker News</a>
                </li>
                <li>
                  <a href="https://www.metafilter.com/127712/In-a-few-cases-the-start-dates-are-wellinformed-guesses" data-link-icon="MF" data-link-icon-type="text,sans,italic">Metafilter</a>
                </li>
              </ul>
            </li>
            <li>
              <p>Article coverage:</p>
              <ul>
                <li>
                  <p><a href="https://arstechnica.com/information-technology/2013/05/google-services-survive-if-they-make-money-arent-social/" data-link-icon="google" data-link-icon-type="svg" title="Google services survive if they make money, aren&#39;t social: Statistical analysis of Google products finds a shutdown rate of 35 percent"><em>Ars Technica</em></a> (<a href="https://arstechnica.com/information-technology/2013/05/google-services-survive-if-they-make-money-arent-social/?comments=1" data-link-icon="google" data-link-icon-type="svg">comments</a>)</p>
                </li>
                <li>
                  <p><a href="https://www.theatlantic.com/technology/archive/2013/05/interesting-software-follow-up-scrivener-googles-orphans/275563/" data-link-icon="google" data-link-icon-type="svg">‚Äú‚ÄòInteresting‚Äô Software Follow-Up: Scrivener, Google‚Äôs Orphans: A new, free guide to an exceptional program‚Äù</a> (James Fallows, <em>Atlantic</em>)</p>
                </li>
                <li>
                  <p><em>Forbes</em>:</p>
                  <ul>
                    <li>
                      <a href="https://www.forbes.com/sites/haydnshaughnessy/2013/05/08/what-is-driving-the-google-stock-price-up/" data-link-icon="google" data-link-icon-type="svg">‚ÄúIs This One Emotion Driving The Google Stock Price Up?‚Äù</a>
                    </li>
                    <li>
                      <a href="https://www.forbes.com/sites/haydnshaughnessy/2013/05/09/google-glass-has-only-a-37-chance-of-going-five-years-lessons/" data-link-icon="google" data-link-icon-type="svg">‚ÄúGoogle Glass - Is It Meant To Last?‚Äù</a>
                    </li>
                  </ul>
                </li>
                <li>
                  <p><a href="https://bgr.com/general/google-services-shut-down-study/" data-link-icon="google" data-link-icon-type="svg" title="Statistical analysis finds Google shuts down 35% of its services"><em>Boy Genius Report</em></a></p>
                </li>
              </ul>
            </li>
            <li>
              <p>Acquisition background:</p>
              <ul>
                <li>
                  <p><a href="http://www.paulgraham.com/yahoo.html" data-link-icon="hn" data-link-icon-type="svg">‚ÄúWhat Happened to Yahoo‚Äù</a>‚Å†, Paul Graham</p>
                </li>
                <li>
                  <p>37signals‚Äô <a href="https://www.google.com/search?q=%22Exit+Interview%22&amp;sitesearch=37signals.com/svn/posts/" data-link-icon="google" data-link-icon-type="svg">‚ÄúExit Interview‚Äù series</a>:</p>
                  <ol type="1">
                    <li>
                      <a href="https://signalvnoise.com/posts/2883-exit-interview-jaikus-jyri-engestrm">‚ÄúExit interview: Jaiku‚Äôs Jyri Engestr√∂m‚Äù</a>
                    </li>
                    <li>
                      <a href="https://signalvnoise.com/posts/2942-exit-interview-founders-look-back-at-acquisitions-by-google-aol-microsoft-and-more" data-link-icon="google" data-link-icon-type="svg">‚ÄúExit Interview: Founders look back at acquisitions by Google, AOL, Microsoft, and more‚Äù</a>
                    </li>
                    <li>
                      <a href="https://signalvnoise.com/posts/2806-exit-interview-ask-jeeves-acquisition-of-bloglines">‚ÄúExit Interview: Ask Jeeves‚Äô acquisition of Bloglines‚Äù</a>
                    </li>
                    <li>
                      <a href="https://signalvnoise.com/posts/2777-what-happens-after-yahoo-acquires-you">‚ÄúWhat happens after Yahoo acquires you‚Äù</a>
                    </li>
                  </ol>
                </li>
              </ul>
            </li>
          </ul><!--
How to handle updates? I guess we'll store it here. Idea: new covariate for 'business' vs 'consumer' vs 'both'. New data:

"Product","Dead","Started","Ended","Hits","Type","Profit","FLOSS","Acquisition","Social"
"Inactive Account Manager",FALSE,2013-04-11,NA,175000,service,FALSE,FALSE,FALSE,FALSE
"Affiliate Network",TRUE,2007-04-13,2013-04-16,3480000,service,TRUE,FALSE,TRUE,FALSE http://googleaffiliatenetwork-blog.blogspot.in/2013/04/an-update-on-google-affiliate-network.html
Behavio's FunF http://www.sfgate.com/technology/businessinsider/article/Google-Just-Bought-A-Cool-SXSW-Startup-Behavio-4430716.php supposedly Google will continue it but it may be killed too quickly to count for this list
"AdWords Pay Per Action",TRUE,2007-03-20,2008-08-31,144000,service,TRUE,FALSE,TRUE,FALSE
"AdSense Referrals",TRUE,2007-04-05,2008-08-31,25000,service,TRUE,FALSE,FALSE,FALSE
"Quick View in Search",TRUE,2009-10-07,2013-04-24,2460000,service,FALSE,FALSE,FALSE,FALSE http://googleblog.blogspot.com/2009/10/quickly-view-formatted-pdfs-in-your.html http://googlesystem.blogspot.com/2013/04/no-more-quick-view-in-google-search.html exact search query: "google search" "quick view"
WhatsonWhen: dead? site down after January‚ÄÖ2013, but no apparent announcements. "google whatsonwhen" 48,400 results
Free Zone: http://www.nation.lk/edition/biz-news/item/17466-dialog-axiata-and-google-launch-free-zone.html 'google "free zone"' 3,030,000 results
Meebo Bar: http://www.pcmag.com/article2/0,2817,2418268,00.asp 'google "meebo bar"' 160,000 results killed 2013-06-06 http://gigaom.com/2013/06/06/a-year-after-google-acquistion-meebo-bar-will-discontinue-as-team-settles-in-with-google/ social acquisition ad-supported
'paid channels'? https://www.wired.com/gadgetlab/2013/05/youtube-adds-paid-channel-subscriptions/ http://youtube-global.blogspot.com/2013/05/yt-pc-2013.html
SMS down 2013-05-09:  http://productforums.google.com/forum/#!msg/websearch/yKG7BGro7QQ/ntAXQWWKj70J https://news.ycombinator.com/item?id=5695086 "google sms" 579,000 other results.
Scratchpad free program non-social non-acquisition non-floss  "google scratchpad" 965 results  dead 2012-11-02 http://www.omgchrome.com/google-stopping-development-of-scratchpad-web-app/ general description: https://support.google.com/chrome_webstore/answer/183097 merged into Keep? "* * Scratchpad is moving to Google Keep. * * If you have unsynced Scratchpad notes, please open the app and follow instructions to save your notes. You can add Google Keep here:  https://chrome.google.com/webstore/detail/google-keep/hmjkmjkepdijhoojdojkdfohbdgmmhki" when was it launched? best guess, January‚ÄÖ2011 http://voices.yahoo.com/scratchpad-notepad-application-google-chrome-7581750.html?cat=15
Cloud Messaging for Chrome http://developer.chrome.com/apps/cloudMessaging.html "service",FALSE,FALSE,FALSE,FALSE launch: 2013-05-10 search 'Google "Cloud Messaging for Chrome"' 23,300 results
Google Play Music‚ÄÖ15 05 2013 paid service non-acquisition non-FLOSS social? ("You can share songs on your google+") '"google play music"' About 37,300,000 results (0.46 seconds)
google Music: was actually dead 2012-10-19! https://www.reuters.com/article/2012/09/21/net-us-google-china-music-idUSBRE88K07120120921
Google+ Games, launched 11 08 2011 http://googleblog.blogspot.com/2011/08/games-in-google-fun-that-fits-your.html Dead, 2013-06-30 https://support.google.com/plus/answer/3123176?p=plus_games&rd=1 non-FLOSS, non-acquisition profit social, '"google+ games"' 142000 hits
Checkout Gadget: dead, see http://googlecommerce.blogspot.com/2013/05/an-update-to-google-checkout-for.html https://support.google.com/checkout/sell/answer/3080449
is checkout proper dead?
"On November‚ÄÖ20th, 2013, Google will shut down its Checkout product. Here's how this may affect you:

    Merchants selling digital goods may transition to Google Wallet for digital goods
    Merchants selling through Google-hosted marketplaces (eg. Google Play) will be unaffected
    Merchants selling physical goods will need to switch to third-party alternatives (see below)"
https://news.ycombinator.com/item?id=5740447 (also https://news.ycombinator.com/item?id=6579812 / https://news.ycombinator.com/item?id=6581906 )
"> 'Wait, so what's the difference between this and Google Wallet?' With Checkout, Google is a credit card processor. With Wallet, Google isn't a credit card processor, they are partnered with Bankcorp Bank who is issuing virtual cards which are funded either by transferring funds from the users bank account or by charging credit cards."
Wallet dead March‚ÄÖ2, 2015. https://support.google.com/wallet/business/answer/6107573

04:57:29 < quanticle> gwern: When you get this, would you mind giving me your opinion on the continued long-term viability of Google Tasks? My prediction is that it won't last much longer; it seems to be a massively underdeveloped part of GMail/Google Calendar, and the task-list/to-do-list market is fiercely competitive.
05:11:25 < harrow> old Closure Library codebase, guy who wrote it quit, no tie-in with G+
Tasks LIVE, service non-profit non-social non-acquisition '"Google Tasks"' 641000 hits launched: 2008-12-08 http://gmailblog.blogspot.com/2008/12/new-in-labs-tasks.html
http://googlesystem.blogspot.com/2013/06/googles-caldav-and-carddav-apis-for.html CalDav API given a reprieve?
what happened to Waze? http://www.globes.co.il/serveen/globes/docview.asp?did=1000850934&fid=1725 apparently decided to keep Waze app running, so it enters the listings: https://www.wired.com/gadgetlab/2013/06/google-waze-acquisition/ http://googleblog.blogspot.com/2013/06/google-maps-and-waze-outsmarting.html
also check these: "Google acquired personalized Website gadget developer Labpixies for $25 million and interactive video-clip developer Quiksee for $10 million. Both acquisitions were in 2010."
google Chrome Frame https://en.wikipedia.org/wiki/Google_Chrome_Frame "chrome frame" 2,830,000 "google chrome frame" 28,200,000 plugin "According to Alex Russell - who came to Google specifically to start the Chrome Frame project - the plug-in will officially be retired early in 2014, and the company is beginning to warn the consumers and businesses who use the tool. "We've gotten to the point now where the trend lines indicate the need for Chrome Frame is going to expire early next year, so we're giving consumers and enterprises a lot of heads up that Chrome Frame is going to be going away," he says." https://www.wired.com/wiredenterprise/2013/06/chrome-frame-ends/ http://www.techrepublic.com/blog/google-in-the-enterprise/google-chrome-frame-is-leaving-the-picture/ https://blog.chromium.org/2013/06/retiring-chrome-frame.html January‚ÄÖ2014
AdWhirl bought with AdMob, died 2013-09-30 http://vator.tv/news/2013-06-14-google-shutting-down-adwhirl-at-the-end-of-september for profit open-source https://code.google.com/p/adwhirl/ "google adwhirl" 101,000 results
"Google Mine", Pinterest competitor http://googlesystem.blogspot.com/2013/06/google-mine.html unreleased as of June‚ÄÖ21, 2013
"Google Latitude" dead 2013-08-09; search 'Google Latitude' pulls up 1,150,000 hits on 10 July ANN: https://support.google.com/gmm/answer/3001634 https://plus.google.com/+jlapenna/posts/deBP7kj7rMi  (Google+'s location stuff is apparently not a full replacement)
Google Shopping for Suppliers 'Google "Shopping for Suppliers"' 67,300  2013-07-11; profit www.google.com/shopping/suppliers/  started 2013-01-28
Alfred,  dead 2013-07-19, for-profit acquisition (2011-12-14) https://techcrunch.com/2013/07/11/google-will-shut-down-alfred-the-local-recommendations-app-july-19/ http://www.ubergizmo.com/2013/07/google-to-shut-down-alfred-on-july-19th/ http://www.engadget.com/2011/12/14/google-buys-alfred-maker-clever-sense-brings-us-closer-to-perso/ "google alfred"  46,100   2013-07-17
Chromecast product for-profit non-social? non-FLOSS  341,000,000 hits 'google chromecast' 2013-07-29  http://www.n3rdabl3.co.uk/2013/07/google-announce-chromecast-a-new-way-to-share-on-your-tv/
Google Shopper: dead. 2013-08-30 http://www.fiercemobilecontent.com/story/google-shopper-price-comparison-app-shutting-down-aug-30/2013-07-29 "google shopper", 297000 hits as of 2013-08-2
Google Earth Tour Builder  http://googlesystem.blogspot.com/2013/08/google-earth-tour-builder.html https://tourbuilder.withgoogle.com/about/faq About‚ÄÖ28,500 results (0.33 seconds) as of 2013-09-1
`google "tour builder"` About‚ÄÖ24,800 results (2013-09-19) non-profit software FLOSS https://code.google.com/p/tour-builder/
Google Lime Scholarship 'google lime scholarship' About‚ÄÖ1,030,000 results (0.23 seconds) (2013-09-19) started 2009-04-15 http://googleforstudents.blogspot.com/2011/04/2011-google-lime-scholars-announced.html http://googleforstudents.blogspot.com/2012/03/announcing-2012-google-lime-scholars.html  http://google.about.com/b/2009/04/15/google-lime-scholarship.htm http://www.limeconnect.com/opportunities/page/google-lime-scholarship-program https://www.google.com/edu/students/scholarships.html
Course Builder non-profit FLOSS https://code.google.com/p/course-builder/ program 2012-09-11 '"google course builder"' About‚ÄÖ264,000 results  (2013-09-19) (sunsetted by Open edX)
Open edX 2013-09-10;  '"Open edX"' About‚ÄÖ18,900 results (2013-09-19) http://googleresearch.blogspot.com/2013/09/we-are-joining-open-edx-platform.html FLOSS? program? service?
Bump start: 2013-09-17; acquisition: http://www.iol.co.za/scitech/technology/business/google-buys-bump-app-1.1578671 https://techcrunch.com/2013/09/16/bump-mobile-contact-sharing-app-acquired-by-google-will-stay-alive-for-now/ application non-profit non-FLOSS '"google bump"' About‚ÄÖ116,000 results Dead: 2014-01-31 http://blog.bu.mp/post/71781606704/all-good-things http://www.hngn.com/articles/20977/20140102/google-shutting-down-bump-flock-apps-end-january.htm
Flock start: 2013-09-17; acquisition: http://www.iol.co.za/scitech/technology/business/google-buys-bump-app-1.1578671 https://techcrunch.com/2013/09/16/bump-mobile-contact-sharing-app-acquired-by-google-will-stay-alive-for-now/ application non-profit non-FLOSS 'google flock' About 34,200,000 results Dead: 2014-01-31
Google Web Designer: 2013-09-30 ' 1,790,000 results ' "google web designer"' https://news.ycombinator.com/item?id=6470426 for-profit program non-FLOSS (see https://support.google.com/webdesigner/answer/3413931 ) non-acquisition
Google Shopping Express: 2013-09-25 service for-profit non-acquisition non-floss http://www.siliconbeat.com/2013/09/25/public-launch-of-google-shopping-express-a-challenge-to-amazon-ebay/  '"google shopping express"' 1,480,000 results  (2013-10-01)
Flutter https://flutterapp.com/home/ https://arstechnica.com/gadgets/2013/10/hands-on-with-googles-latest-acquisition-flutter-a-webcam-gesture-app/ 2013-10-02  "Flutter users will be able to continue to use the app, and stay tuned for future updates." https://flutterapp.com/ closed-source non-profit? flutter,  18,800,000;  '"google flutter"'  3,980
Google Tag Manager alive released Oct‚ÄÖ1, 2012; https://www.google.com/tagmanager/ for-profit service non-acquisition  262,000 results  'google "tag manager"' 2013-10-10
Google TV killed? '"google tv"' 7,860,000 2013-10-17 http://www.hngn.com/articles/14778/20131013/google-tv-shuts-down-three-years-shifting-focus-android.htm http://gigaom.com/2013/10/10/google-tv-rebranded-android-tv/
Google WiFi Passport http://googlesystem.blogspot.com/2013/10/google-wifi-passport.html launched 2013-10-16 https://arstechnica.com/civis/viewtopic.php?f=9&t=1221867 for-profit service non-floss  'google "wifi passport"' 2013-10-26 1,090 results
'google helpouts' 2013-11-04 https://blogs.wsj.com/digits/2013/11/04/google-to-launch-helpouts-on-monday/ service, for-profit non-acquisition '"google helpouts"' 140,000 results; killed 2015-04-20 https://techcrunch.com/2015/02/13/google-pulls-its-helpouts-mobile-applications-from-the-app-stores/
Ingress‚ÄÖ2013-12-14 http://venturebeat.com/2013/11/04/googles-niantic-labs-to-formally-launch-massive-ingress-augmented-reality-game-on-dec-14/ application closed-source for-profit (sponsorship)
Google Trader: dead 2013-12-10, google.com.ng; '"Google Trader"' 2013-11-14: 101k results http://niyitabiti.net/2013/11/google-trader-nigeria-shut-down-website/
"Offer Extensions"  2013-02-22; 2013-11-14: 'google "Offer Extensions"' 35.6k hits dead 2013-11-01 http://searchengineland.com/adwords-offer-extensions-get-shut-down-in-favor-of-google-offers-176566
Google Newsstand http://www.pcmag.com/article2/0,2817,2427413,00.asp http://play.google.com/about/newsstand/ Android app 2013-11-20 '"Google Newsstand"' 71500 for-profit closed-source
Google Partners‚ÄÖ2013-11-28 '"Google Partners"' 289k 2013-12-03;  http://articles.timesofindia.indiatimes.com/2013-11-28/services-apps/44545708_1_google-india-google-partners-agencies https://www.google.com/partners/ http://www.zdnet.com/in/launch-of-google-partners-to-help-smes-in-india-7000023740/ organization, for-profit
Android Gallery, dead Dec‚ÄÖ2013? http://www.androidcentral.com/stock-android-gallery-app-no-more-new-google-play-edition-devices http://www.techhive.com/article/2079353/google-is-killing-the-android-gallery-app-so-that-google-plus-may-live.html 'google "android gallery"' 2013-12-13 516,000 results
Open Gallery, 2013-12-10 https://nitter.hu/digitalfay/status/410361346522611712 https://www.google.com/opengallery https://support.google.com/opengallery/ closed-source service non-profit 'google "open gallery"' 1,110,000 2013-12-17
LiquidFun‚ÄÖ2013-12-11 FLOSS software non-profit http://google-opensource.blogspot.ca/2013/12/liquidfun-rigid-body-physics-library.html http://www.i-programmer.info/news/144-graphics-and-games/6711-googles-liquidfun-for-fluid-simulation-.html http://google.github.io/liquidfun/ https://github.com/google/liquidfun  'google liquidfun' 2013-12-14 174000 results
Google+ Auto Backup ios/android application closed-source non-profit start: 2013-06-24? http://www.vlogg.com/10149/how-to-disable-auto-backup-of-photos-in-android/ https://support.google.com/plus/answer/1647509 http://www.techrepublic.com/blog/google-in-the-enterprise/quick-tip-back-up-your-photos-to-google-plus/ http://googlesystem.blogspot.com/2013/12/google-auto-backup-for-desktop.html http://www.gplusexpertise.com/2013/12/google-photos-auto-backup-now-in-picasa.html 'Google+ Auto Backup' 8,830,000 results  2013-12-28
2014-02-07 Schemer http://googlesystem.blogspot.com/2013/12/schemer-to-be-discontinued.html to be shut down in 2014? confirmed: "All your schemes are available for download until February 7, 2014, after which all data will be permanently deleted." https://plus.google.com/+Schemer/posts/AhXkhvzRtek
Google Talk Windows client? http://www.tbreak.ae/news/google-killing-google-talk-windows-client "Connections from the Google Talk client to Google Talk will continue to operate for the first two months of 2014. Users may continue to see the deprecation warning in the client during this time. After this period has elapsed the client will be unable to connect to the Google Talk service."  http://tech.firstpost.com/news-analysis/google-to-shut-down-gtalk-today-will-force-users-to-switch-to-hangouts-253637.html "However, this week, Google has made it quite clear that GTalk will no longer be operational starting Feb‚ÄÖ16 [2015]." https://gsuiteupdates.googleblog.com/2017/03/updates-in-g-suite-to-streamline-hangouts-and-gmail.html
Timely‚ÄÖ2014-01-04 closed-source application Bitspin acquisition non-profit (since Google made the paid Android Timely app free) http://www.bitspin.ch/google http://www.pcworld.com/article/2084140/google-makes-timely-buy-of-swiss-app-maker.html "For new and existing users, Timely will continue to work as it always has." '"google timely"' 2014-01-08: 25,400 results
Nest: "Nest Learning Thermostat", "Protect"  2013-01-13 https://investor.google.com/releases/2014/0113.html https://techcrunch.com/2014/01/13/google-just-bought-connected-device-company-nest-for-3-2b-in-cash/ https://nest.com/blog/2014/01/13/nest-google-and-you/ acquisition thing for-profit "google nest"  'About 87,300,000 results (0.42 seconds) '
Google Currents: dead 2014-02-19 http://www.androidpolice.com/2014/02/19/google-currents-is-officially-dead-with-latest-update-transitions-users-to-play-newsstand-and-disappears-for-good/
Wildfire‚ÄÖ2012-07-31 http://wildfireapp.blogspot.com/2012/07/wildfire-is-joining-google.html acquisition social for-profit Maintenance stops 2014-09-01 "A source tells us that Wildfire has begun contacting its clients - which have included Cisco, Amazon, DQ, Gap, Jamba Juice, and McCann - to tell them that their business would no longer be serviced after the end of 2015."; shutdown: http://wildfireapp.blogspot.com/2014/03/accelerating-our-wildfire-integration.html
Know Your Candidate http://www.google.co.in/elections/ed/in/districts http://www.businessinsider.in/Indian-Election-Fever-Has-Hit-Google-As-They-Launch-KnowYour-Candidate-Tool/articleshow/33444019.cms 2014-04-08 website nonprofit non-FLOSS 'google "know your candidate"' 2014-04-25: 2,190,000 results
TODO: G+'s Hangout, Photo
2014-05-06 http://www.digitaljournal.com/pr/1903697 for-profit acquisition service https://www.google.com/search?num=100&q=google%20ezanga 2014-05-14 '34,400 results'
Google Zavers https://www.google.com/get/zavers/ coupon business for-profit acquisition service launched January‚ÄÖ2013 death leaked 2014-06-02 http://recode.net/2014/06/02/google-will-kill-off-its-digital-coupon-business-zavers/ '"google zavers"' 2014-06-05:  1,540 hits ('zavers': 51,500)
Orkut: dead 2014-09-30 http://en.blog.orkut.com/2014/06/tchau-orkut.html
QuickOffice dead http://www.slashgear.com/quickoffice-closing-down-replaced-by-google-drive-30335806/ http://googleappsupdates.blogspot.sg/2014/06/removal-of-quickoffice-from-google-play.html 'upcoming weeks'
Google Sync, dead August‚ÄÖ1 2014 / 2014-08-01 free service https://support.google.com/a/answer/2716936 (already in original compilation)
followup window closed 2014-06-04: no more new entries unless they came into existence before then! too much work
Freebase https://groups.google.com/forum/#!msg/freebase-discuss/s_BPoL92edc/Y585r7_2E1YJ dead: June 30, 2015
Glass https://www.bbc.com/news/technology-30831128 https://news.ycombinator.com/item?id=8894168 'January‚ÄÖ19 [2015] will be the last day to get the Glass Explorer Edition.' https://plus.google.com/+GoogleGlass/posts/9uiwXY42tvc
Google Maps Coordinate dead 2016-01-21 http://www.engadget.com/2015/01/22/google-maps-coordinate-closing/
Notifier http://googlesystem.blogspot.de/2014/01/no-more-google-notifier.html  2014-01-31
Google Code Search dead 2013-03-06, started October 5, 2006
Google Code "Beginning today, we have disabled new project creation on Google Code. We will be shutting down the service about 10 months from now on January‚ÄÖ25th, 2016." http://google-opensource.blogspot.com/2015/03/farewell-to-google-code.html
Google Moderator dead 2015-06-30 https://www.google.com/moderator/ https://news.ycombinator.com/item?id=9335082
Page Speed Service dead 2015-08-03 https://techcrunch.com/2015/05/06/google-shuts-down-pagespeed-service-for-accelerating-websites/ https://developers.google.com/speed/pagespeed/service/Deprecation
Panoramio remaining features killed: 2018-01-29 https://productforums.google.com/forum/#!topic/maps/H4VObBFsIyA
Google Surveys, 2012-03-29--2022-11-01 https://news.googleblog.com/2012/03/new-way-to-access-quality-content.html https://support.google.com/surveys/answer/12626240
Google Stadia: 2019-11-19--2023-0118 https://blog.google/products/stadia/message-on-stadia-streaming-strategy/
-->
        </section>
        <section id="appendix">
          
          <section id="source-code">
            <h2><a href="#source-code" title="Link to section: ¬ß &#39;Source code&#39;">Source Code</a></h2>
            <p>Run as <code>R --slave --file=google.r</code>:</p>
            <div id="cb15">
              <pre><code><span id="cb15-1"><span>set.seed</span>(<span>7777</span>) <span># for reproducible numbers</span></span>
<span id="cb15-2"></span>
<span id="cb15-3"><span>library</span>(survival)</span>
<span id="cb15-4"><span>library</span>(randomForest)</span>
<span id="cb15-5"><span>library</span>(boot)</span>
<span id="cb15-6"><span>library</span>(randomForestSRC)</span>
<span id="cb15-7"><span>library</span>(prodlim) <span># for &#39;sindex&#39; call</span></span>
<span id="cb15-8"><span>library</span>(rms)</span>
<span id="cb15-9"></span>
<span id="cb15-10"># Generate Google corpus model for use in main analysis</span>
<span id="cb15-11"># Load the data, fit, and plot:</span>
<span id="cb15-12">index <span>&lt;-</span> <span>read.csv</span>(<span>&#34;https://www.gwern.net/docs/statistics/2013-google-index.csv&#34;</span>,</span>
<span id="cb15-13">                   <span>colClasses=</span><span>c</span>(<span>&#34;Date&#34;</span>,<span>&#34;double&#34;</span>,<span>&#34;character&#34;</span>))</span>
<span id="cb15-14"># an exponential doesn&#39;t fit too badly:</span>
<span id="cb15-15">model1 <span>&lt;-</span> <span>lm</span>(<span>log</span>(Size) <span>~</span> Date, <span>data=</span>index); <span>summary</span>(model1)</span>
<span id="cb15-16"># plot logged size data and the fit:</span>
<span id="cb15-17"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/www-index-model.png&#34;</span>, <span>width =</span> <span>3</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-18"><span>plot</span>(<span>log</span>(index<span>$</span>Size) <span>~</span> index<span>$</span>Date, <span>ylab=</span><span>&#34;WWW index size&#34;</span>, <span>xlab=</span><span>&#34;Date&#34;</span>)</span>
<span id="cb15-19"><span>abline</span>(model1)</span>
<span id="cb15-20"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-21"></span>
<span id="cb15-22"># Begin actual data analysis</span>
<span id="cb15-23">google <span>&lt;-</span> <span>read.csv</span>(<span>&#34;https://www.gwern.net/docs/statistics/survival-analysis/2013-google.csv&#34;</span>,</span>
<span id="cb15-24">                    <span>colClasses=</span><span>c</span>(<span>&#34;character&#34;</span>,<span>&#34;logical&#34;</span>,<span>&#34;Date&#34;</span>,<span>&#34;Date&#34;</span>,<span>&#34;double&#34;</span>,<span>&#34;factor&#34;</span>,</span>
<span id="cb15-25">                                 <span>&#34;logical&#34;</span>,<span>&#34;logical&#34;</span>,<span>&#34;logical&#34;</span>,<span>&#34;logical&#34;</span>, <span>&#34;integer&#34;</span>,</span>
<span id="cb15-26">                                 <span>&#34;numeric&#34;</span>, <span>&#34;numeric&#34;</span>, <span>&#34;numeric&#34;</span>, <span>&#34;logical&#34;</span>, <span>&#34;numeric&#34;</span>,</span>
<span id="cb15-27">                                 <span>&#34;numeric&#34;</span>, <span>&#34;numeric&#34;</span>, <span>&#34;numeric&#34;</span>))</span>
<span id="cb15-28"># google$Days &lt;- as.integer(google$Ended - google$Started)</span>
<span id="cb15-29"># derive all the Google index-variables</span>
<span id="cb15-30">## hits per day to the present</span>
<span id="cb15-31"># google$AvgHits &lt;- google$Hits / as.integer(as.Date(&#34;2013-04-01&#34;) - google$Started)</span>
<span id="cb15-32">## divide total hits for each product by total estimated size of Google index when that product started</span>
<span id="cb15-33"># google$DeflatedHits &lt;- log(google$Hits / exp(predict(model1, newdata=data.frame(Date = google$Started))))</span>
<span id="cb15-34">## Finally, let&#39;s combine the two strategies: deflate and then average.</span>
<span id="cb15-35"># google$AvgDeflatedHits &lt;- log(google$AvgHits) / google$DeflatedHits</span>
<span id="cb15-36"># google$DeflatedHits &lt;- log(google$DeflatedHits)</span>
<span id="cb15-37"></span>
<span id="cb15-38"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Overview of data:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-39"><span>print</span>(<span>summary</span>(google[<span>-</span><span>1</span>]))</span>
<span id="cb15-40"></span>
<span id="cb15-41">dead <span>&lt;-</span> google[google<span>$</span>Dead,]</span>
<span id="cb15-42"></span>
<span id="cb15-43"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/openedvslifespan.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-44"><span>plot</span>(dead<span>$</span>Days <span>~</span> dead<span>$</span>Ended, <span>xlab=</span><span>&#34;Shutdown&#34;</span>, <span>ylab=</span><span>&#34;Total lifespan&#34;</span>)</span>
<span id="cb15-45"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-46"></span>
<span id="cb15-47"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/shutdownsbyyear.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-48"><span>hist</span>(dead<span>$</span>Ended, <span>breaks=</span><span>seq.Date</span>(<span>as.Date</span>(<span>&#34;2005-01-01&#34;</span>), <span>as.Date</span>(<span>&#34;2014-01-01&#34;</span>), <span>&#34;years&#34;</span>),</span>
<span id="cb15-49">                   <span>main=</span><span>&#34;shutdowns per year&#34;</span>, <span>xlab=</span><span>&#34;Year&#34;</span>)</span>
<span id="cb15-50"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-51"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/shutdownsbyyear-kernel.png&#34;</span>, <span>width =</span> <span>1</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-52"><span>plot</span>(<span>density</span>(<span>as.numeric</span>(dead<span>$</span>Ended)), <span>main=</span><span>&#34;Shutdown kernel density over time&#34;</span>)</span>
<span id="cb15-53"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-54"></span>
<span id="cb15-55"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/startsbyyear.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-56"><span>hist</span>(google<span>$</span>Started, <span>breaks=</span><span>seq.Date</span>(<span>as.Date</span>(<span>&#34;1997-01-01&#34;</span>), <span>as.Date</span>(<span>&#34;2014-01-01&#34;</span>), <span>&#34;years&#34;</span>),</span>
<span id="cb15-57">     <span>xlab=</span><span>&#34;total products released in year&#34;</span>)</span>
<span id="cb15-58"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-59"></span>
<span id="cb15-60"># extract the month of each kill</span>
<span id="cb15-61">m <span>=</span> <span>months</span>(dead<span>$</span>Ended)</span>
<span id="cb15-62"># sort by chronological order, not alphabetical</span>
<span id="cb15-63">m_fac <span>=</span> <span>factor</span>(m, <span>levels =</span> month.name)</span>
<span id="cb15-64"># count by month</span>
<span id="cb15-65">months <span>&lt;-</span> <span>table</span>(<span>sort</span>(m_fac))</span>
<span id="cb15-66"># shutdowns by month are imbalanced:</span>
<span id="cb15-67"><span>print</span>(<span>chisq.test</span>(months))</span>
<span id="cb15-68"># and visibly so:</span>
<span id="cb15-69"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/shutdownsbymonth.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-70"><span>plot</span>(months)</span>
<span id="cb15-71"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-72"></span>
<span id="cb15-73"><span>cat</span>(<span>&#34;</span><span>\n</span><span>First logistic regression:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-74"><span>print</span>(<span>summary</span>(<span>glm</span>(Dead <span>~</span> Type <span>+</span> Profit <span>+</span> FLOSS <span>+</span> Acquisition <span>+</span> Social <span>+</span> <span>log</span>(Hits),</span>
<span id="cb15-75">                  <span>data=</span>google, <span>family=</span><span>&#34;binomial&#34;</span>)))</span>
<span id="cb15-76"></span>
<span id="cb15-77"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Second logistic regression, focusing on treacherous hit data:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-78"><span>print</span>(<span>summary</span>(<span>glm</span>(Dead <span>~</span> <span>log</span>(Hits), <span>data=</span>google,<span>family=</span><span>&#34;binomial&#34;</span>)))</span>
<span id="cb15-79"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Total + average:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-80"><span>print</span>(<span>summary</span>(<span>glm</span>(Dead <span>~</span> <span>log</span>(Hits) <span>+</span> <span>log</span>(AvgHits), <span>data=</span>google,<span>family=</span><span>&#34;binomial&#34;</span>)))</span>
<span id="cb15-81"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Total, average, deflated:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-82"><span>print</span>(<span>summary</span>(<span>glm</span>(Dead <span>~</span> <span>log</span>(Hits) <span>+</span> <span>log</span>(AvgHits) <span>+</span> DeflatedHits, <span>data=</span>google,<span>family=</span><span>&#34;binomial&#34;</span>)))</span>
<span id="cb15-83"><span>cat</span>(<span>&#34;</span><span>\n</span><span>All:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-84"><span>print</span>(<span>summary</span>(<span>glm</span>(Dead <span>~</span> <span>log</span>(Hits) <span>+</span> <span>log</span>(AvgHits) <span>+</span> DeflatedHits <span>+</span> AvgDeflatedHits,</span>
<span id="cb15-85">                  <span>data=</span>google, <span>family=</span><span>&#34;binomial&#34;</span>)))</span>
<span id="cb15-86"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Stepwise regression through possible logistic regressions involving the hit variables:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-87"><span>print</span>(<span>summary</span>(<span>step</span>(<span>glm</span>(Dead <span>~</span> Type <span>+</span> Profit <span>+</span> FLOSS <span>+</span> Acquisition <span>+</span> Social <span>+</span></span>
<span id="cb15-88">                        <span>log</span>(Hits) <span>+</span> <span>log</span>(AvgHits) <span>+</span> DeflatedHits <span>+</span> AvgDeflatedHits,</span>
<span id="cb15-89">                       <span>data=</span>google, <span>family=</span><span>&#34;binomial&#34;</span>))))</span>
<span id="cb15-90"></span>
<span id="cb15-91"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Entering survival analysis section:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-92"></span>
<span id="cb15-93"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Unconditional Kaplan-Meier survival curve:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-94">surv <span>&lt;-</span> <span>survfit</span>(<span>Surv</span>(google<span>$</span>Days, google<span>$</span>Dead, <span>type=</span><span>&#34;right&#34;</span>) <span>~</span> <span>1</span>)</span>
<span id="cb15-95"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/overall-survivorship-curve.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-96"><span>plot</span>(surv, <span>xlab=</span><span>&#34;Days&#34;</span>, <span>ylab=</span><span>&#34;Survival Probability function with 95% CI&#34;</span>)</span>
<span id="cb15-97"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-98"></span>
<span id="cb15-99"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Cox:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-100">cmodel <span>&lt;-</span> <span>coxph</span>(<span>Surv</span>(Days, Dead) <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span> DeflatedHits,</span>
<span id="cb15-101">                <span>data =</span> google)</span>
<span id="cb15-102"><span>print</span>(<span>summary</span>(cmodel))</span>
<span id="cb15-103"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Test proportional assumption:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-104"><span>print</span>(<span>cox.zph</span>(cmodel))</span>
<span id="cb15-105"></span>
<span id="cb15-106"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Primitive check for regime change (re-regress &amp; check):</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-107">google<span>$</span>EarlyGoogle <span>&lt;-</span> (<span>as.POSIXlt</span>(google<span>$</span>Started)<span>$</span>year<span>+</span><span>1900</span>) <span>&lt;</span> <span>2005</span></span>
<span id="cb15-108">cmodel <span>&lt;-</span> <span>coxph</span>(<span>Surv</span>(Days, Dead) <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span></span>
<span id="cb15-109">                                   DeflatedHits <span>+</span> EarlyGoogle,</span>
<span id="cb15-110">                <span>data =</span> google)</span>
<span id="cb15-111"><span>print</span>(<span>summary</span>(cmodel))</span>
<span id="cb15-112"><span>print</span>(<span>cox.zph</span>(cmodel))</span>
<span id="cb15-113"></span>
<span id="cb15-114"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Generating intuitive plots of social &amp; profit;</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-115"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Plot empirical survival split by profit...</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-116"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/profit-survivorship-curve.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-117">smodel1 <span>&lt;-</span> <span>survfit</span>(<span>Surv</span>(Days, Dead) <span>~</span> Profit, <span>data =</span> google);</span>
<span id="cb15-118"><span>plot</span>(smodel1, <span>lty=</span><span>c</span>(<span>1</span>, <span>2</span>), <span>xlab=</span><span>&#34;Days&#34;</span>, <span>ylab=</span><span>&#34;Fraction surviving by Day&#34;</span>);</span>
<span id="cb15-119"><span>legend</span>(<span>&#34;bottomleft&#34;</span>, <span>legend=</span><span>c</span>(<span>&#34;Profit = no&#34;</span>, <span>&#34;Profit = yes&#34;</span>), <span>lty=</span><span>c</span>(<span>1</span> ,<span>2</span>), <span>inset=</span><span>0.02</span>)</span>
<span id="cb15-120"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-121"></span>
<span id="cb15-122"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Split by social...</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-123">smodel2 <span>&lt;-</span> <span>survfit</span>(<span>Surv</span>(Days, Dead) <span>~</span> Social, <span>data =</span> google)</span>
<span id="cb15-124"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/social-survivorship-curve.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-125"><span>plot</span>(smodel2, <span>lty=</span><span>c</span>(<span>1</span>, <span>2</span>), <span>xlab=</span><span>&#34;Days&#34;</span>, <span>ylab=</span><span>&#34;Fraction surviving by Day&#34;</span>)</span>
<span id="cb15-126"><span>legend</span>(<span>&#34;bottomleft&#34;</span>, <span>legend=</span><span>c</span>(<span>&#34;Social = no&#34;</span>, <span>&#34;Social = yes&#34;</span>), <span>lty=</span><span>c</span>(<span>1</span> ,<span>2</span>), <span>inset=</span><span>0.02</span>)</span>
<span id="cb15-127"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-128"></span>
<span id="cb15-129"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Train some random forests for prediction:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-130">lmodel <span>&lt;-</span> <span>glm</span>(Dead <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span></span>
<span id="cb15-131">                     DeflatedHits <span>+</span> EarlyGoogle <span>+</span> Days,</span>
<span id="cb15-132">                   <span>data=</span>google, <span>family=</span><span>&#34;binomial&#34;</span>)</span>
<span id="cb15-133">rf <span>&lt;-</span> <span>randomForest</span>(<span>as.factor</span>(Dead) <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span></span>
<span id="cb15-134">                                     Type <span>+</span> DeflatedHits <span>+</span> EarlyGoogle,</span>
<span id="cb15-135">                   <span>importance=</span><span>TRUE</span>, <span>data=</span>google)</span>
<span id="cb15-136"><span>print</span>(rf)</span>
<span id="cb15-137"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Variables by importance for forests:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-138"><span>print</span>(<span>importance</span>(rf))</span>
<span id="cb15-139"></span>
<span id="cb15-140"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Base-rate predictor of ~65% products alive:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-141"><span>print</span>(<span>sum</span>(<span>FALSE</span> <span>==</span> google<span>$</span>Dead) <span>/</span> <span>nrow</span>(google))</span>
<span id="cb15-142"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Logistic regression&#39;s correct predictions:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-143"><span>print</span>(<span>sum</span>((<span>exp</span>(<span>predict</span>(lmodel))<span>&gt;</span><span>1</span>) <span>==</span> google<span>$</span>Dead) <span>/</span> <span>nrow</span>(google))</span>
<span id="cb15-144"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Random forest&#39;s correct predictions:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-145"><span>print</span>(<span>sum</span>((<span>as.logical</span>(<span>predict</span>(rf))) <span>==</span> google<span>$</span>Dead) <span>/</span> <span>nrow</span>(google))</span>
<span id="cb15-146"></span>
<span id="cb15-147"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Begin bootstrap test of predictive accuracy...</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-148"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Get a subsample, train logistic regression on it, test accuracy on original Google data:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-149">logisticPredictionAccuracy <span>&lt;-</span> <span>function</span>(gb, indices) {</span>
<span id="cb15-150">  g <span>&lt;-</span> gb[indices,] <span># allows boot to select subsample</span></span>
<span id="cb15-151">  <span># train new regression model on subsample</span></span>
<span id="cb15-152">  lmodel <span>&lt;-</span> <span>glm</span>(Dead <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span></span>
<span id="cb15-153">                       DeflatedHits <span>+</span> EarlyGoogle <span>+</span> Days,</span>
<span id="cb15-154">                   <span>data=</span>g, <span>family=</span><span>&#34;binomial&#34;</span>)</span>
<span id="cb15-155">  <span>return</span>(<span>sum</span>((<span>exp</span>(<span>predict</span>(lmodel, <span>newdata=</span>google))<span>&gt;</span><span>1</span>) <span>==</span> google<span>$</span>Dead) <span>/</span> <span>nrow</span>(google))</span>
<span id="cb15-156">}</span>
<span id="cb15-157">lbs <span>&lt;-</span> <span>boot</span>(<span>data=</span>google, <span>statistic=</span>logisticPredictionAccuracy, <span>R=</span><span>20000</span>, <span>parallel=</span><span>&#34;multicore&#34;</span>, <span>ncpus=</span><span>4</span>)</span>
<span id="cb15-158"><span>print</span>(<span>boot.ci</span>(lbs, <span>type=</span><span>&#34;norm&#34;</span>))</span>
<span id="cb15-159"></span>
<span id="cb15-160"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Ditto for random forests:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-161">randomforestPredictionAccuracy <span>&lt;-</span> <span>function</span>(gb, indices) {</span>
<span id="cb15-162">  g <span>&lt;-</span> gb[indices,]</span>
<span id="cb15-163">  rf <span>&lt;-</span> <span>randomForest</span>(<span>as.factor</span>(Dead) <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span></span>
<span id="cb15-164">                       DeflatedHits <span>+</span> EarlyGoogle <span>+</span> Days,</span>
<span id="cb15-165">                   <span>data=</span>g)</span>
<span id="cb15-166">  <span>return</span>(<span>sum</span>((<span>as.logical</span>(<span>predict</span>(rf))) <span>==</span> google<span>$</span>Dead) <span>/</span> <span>nrow</span>(google))</span>
<span id="cb15-167">}</span>
<span id="cb15-168">rfbs <span>&lt;-</span> <span>boot</span>(<span>data=</span>google, <span>statistic=</span>randomforestPredictionAccuracy, <span>R=</span><span>20000</span>, <span>parallel=</span><span>&#34;multicore&#34;</span>, <span>ncpus=</span><span>4</span>)</span>
<span id="cb15-169"><span>print</span>(<span>boot.ci</span>(rfbs, <span>type=</span><span>&#34;norm&#34;</span>))</span>
<span id="cb15-170"></span>
<span id="cb15-171"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Fancier comparison: random survival forests and full Cox model with bootstrap</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-172">rsf <span>&lt;-</span> <span>rfsrc</span>(<span>Surv</span>(Days, Dead) <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span> DeflatedHits <span>+</span> EarlyGoogle,</span>
<span id="cb15-173">             <span>data=</span>google, <span>nsplit=</span><span>1</span>)</span>
<span id="cb15-174"><span>print</span>(rsf)</span>
<span id="cb15-175"></span>
<span id="cb15-176"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/rsf-importance.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-177"><span>plot</span>(rsf)</span>
<span id="cb15-178"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-179"></span>
<span id="cb15-180"># calculate cumulative hazard function; adapted from Mogensen‚ÄÖet‚ÄÖal‚ÄÖ2012 (I don&#39;t understand this)</span>
<span id="cb15-181">predictSurvProb.rsf <span>&lt;-</span> <span>function</span> (object, newdata, times, ...) {</span>
<span id="cb15-182">    N <span>&lt;-</span> <span>NROW</span>(newdata)</span>
<span id="cb15-183">    <span># class(object) &lt;- c(&#34;rsf&#34;, &#34;grow&#34;)</span></span>
<span id="cb15-184">    S <span>&lt;-</span> <span>exp</span>(<span>-</span><span>predict.rfsrc</span>(object, <span>test =</span> newdata)<span>$</span>chf)</span>
<span id="cb15-185">    <span>if</span>(N <span>==</span> <span>1</span>) S <span>&lt;-</span> <span>matrix</span>(S, <span>nrow =</span> <span>1</span>)</span>
<span id="cb15-186">    Time <span>&lt;-</span> object<span>$</span>time.interest</span>
<span id="cb15-187">    p <span>&lt;-</span> <span>cbind</span>(<span>1</span>, S)[, <span>1</span> <span>+</span> <span>sindex</span>(Time, times),drop <span>=</span> <span>FALSE</span>]</span>
<span id="cb15-188">    <span>if</span>(<span>NROW</span>(p) <span>!=</span> <span>NROW</span>(newdata) <span>||</span> <span>NCOL</span>(p) <span>!=</span> <span>length</span>(times))</span>
<span id="cb15-189">     <span>stop</span>(<span>&#34;Prediction failed&#34;</span>)</span>
<span id="cb15-190">    p</span>
<span id="cb15-191">}</span>
<span id="cb15-192">totals <span>&lt;-</span> <span>as.integer</span>(<span>as.Date</span>(<span>&#34;2013-04-01&#34;</span>) <span>-</span> google<span>$</span>Started)</span>
<span id="cb15-193">randomSurvivalPredictionAccuracy <span>&lt;-</span> <span>function</span>(gb, indices) {</span>
<span id="cb15-194">    g <span>&lt;-</span> gb[indices,]</span>
<span id="cb15-195">    rsfB <span>&lt;-</span> <span>rfsrc</span>(<span>Surv</span>(Days, Dead) <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span></span>
<span id="cb15-196">                                     DeflatedHits <span>+</span> EarlyGoogle,</span>
<span id="cb15-197">                 <span>data=</span>g, <span>nsplit=</span><span>1</span>)</span>
<span id="cb15-198"></span>
<span id="cb15-199">    predictionMatrix <span>&lt;-</span> <span>predictSurvProb.rsf</span>(rsfB, google, totals)</span>
<span id="cb15-200">    <span>rm</span>(predictions)</span>
<span id="cb15-201">    <span>for</span> (i <span>in</span> <span>1</span><span>:</span><span>nrow</span>(google)) { predictions[i] <span>&lt;-</span> predictionMatrix[i,i] }</span>
<span id="cb15-202"></span>
<span id="cb15-203">    <span>return</span>(<span>sum</span>((predictions<span>&lt;</span><span>0.50</span>) <span>==</span> google<span>$</span>Dead) <span>/</span> <span>nrow</span>(google))</span>
<span id="cb15-204">}</span>
<span id="cb15-205"># accuracy on full Google dataset</span>
<span id="cb15-206"><span>print</span>(<span>randomSurvivalPredictionAccuracy</span>(google, <span>1</span><span>:</span><span>nrow</span>(google)))</span>
<span id="cb15-207"># check this high accuracy using bootstrap</span>
<span id="cb15-208">rsfBs <span>&lt;-</span> <span>boot</span>(<span>data=</span>google, <span>statistic=</span>randomSurvivalPredictionAccuracy, <span>R=</span><span>200</span>, <span>parallel=</span><span>&#34;multicore&#34;</span>, <span>ncpus=</span><span>4</span>)</span>
<span id="cb15-209"><span>print</span>(rsfBs)</span>
<span id="cb15-210"><span>print</span>(<span>boot.ci</span>(rsfBs, <span>type=</span><span>&#34;perc&#34;</span>))</span>
<span id="cb15-211"></span>
<span id="cb15-212">coxProbability <span>&lt;-</span> <span>function</span>(cm, d, t) {</span>
<span id="cb15-213">    x <span>&lt;-</span> <span>survfit</span>(cm, <span>newdata=</span>d)</span>
<span id="cb15-214">    p <span>&lt;-</span> x<span>$</span>surv[<span>Position</span>(<span>function</span>(a) a<span>&gt;</span>t, x<span>$</span>time)]</span>
<span id="cb15-215">    <span>if</span> (<span>is.null</span>(p)) { <span>coxProbability</span>(d, (t<span>-1</span>)) } <span>else</span> {<span>if</span> (<span>is.na</span>(p)) p <span>&lt;-</span> <span>0</span>}</span>
<span id="cb15-216">    p</span>
<span id="cb15-217">    }</span>
<span id="cb15-218">randomCoxPredictionAccuracy <span>&lt;-</span> <span>function</span>(gb, indices) {</span>
<span id="cb15-219">    g <span>&lt;-</span> gb[indices,]</span>
<span id="cb15-220">    cmodel <span>&lt;-</span> cmodel <span>&lt;-</span> <span>coxph</span>(<span>Surv</span>(Days, Dead) <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span> DeflatedHits,</span>
<span id="cb15-221">                <span>data =</span> g)</span>
<span id="cb15-222"></span>
<span id="cb15-223">    <span>rm</span>(predictions)</span>
<span id="cb15-224">    <span>for</span> (i <span>in</span> <span>1</span><span>:</span><span>nrow</span>(google)) { predictions[i] <span>&lt;-</span> <span>coxProbability</span>(cmodel, google[i,], totals[i]) }</span>
<span id="cb15-225"></span>
<span id="cb15-226">    <span>return</span>(<span>sum</span>((predictions<span>&lt;</span><span>0.50</span>) <span>==</span> google<span>$</span>Dead) <span>/</span> <span>nrow</span>(google))</span>
<span id="cb15-227">    }</span>
<span id="cb15-228"><span>print</span>(<span>randomCoxPredictionAccuracy</span>(google, <span>1</span><span>:</span><span>nrow</span>(google)))</span>
<span id="cb15-229">coxBs <span>&lt;-</span> <span>boot</span>(<span>data=</span>google, <span>statistic=</span>randomCoxPredictionAccuracy, <span>R=</span><span>200</span>, <span>parallel=</span><span>&#34;multicore&#34;</span>, <span>ncpus=</span><span>4</span>)</span>
<span id="cb15-230"><span>print</span>(coxBs)</span>
<span id="cb15-231"><span>print</span>(<span>boot.ci</span>(coxBs, <span>type=</span><span>&#34;perc&#34;</span>))</span>
<span id="cb15-232"></span>
<span id="cb15-233"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Ranking products by Cox risk ratio...</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-234">google<span>$</span>RiskRatio <span>&lt;-</span> <span>predict</span>(cmodel, <span>type=</span><span>&#34;risk&#34;</span>)</span>
<span id="cb15-235">alive <span>&lt;-</span> google[<span>!</span>google<span>$</span>Dead,]</span>
<span id="cb15-236"></span>
<span id="cb15-237"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Extract the 10 living products with highest estimated relative risks:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-238"><span>print</span>(<span>head</span>(alive[<span>order</span>(alive<span>$</span>RiskRatio, <span>decreasing=</span><span>TRUE</span>),], <span>n=</span><span>10</span>)<span>$</span>Product)</span>
<span id="cb15-239"></span>
<span id="cb15-240"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Extract the 10 living products with lowest estimated relative risk:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-241"><span>print</span>(<span>head</span>(alive[<span>order</span>(alive<span>$</span>RiskRatio, <span>decreasing=</span><span>FALSE</span>),], <span>n=</span><span>10</span>)<span>$</span>Product)</span>
<span id="cb15-242"></span>
<span id="cb15-243"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Begin calculating specific numerical predictions about remaining lifespans..</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-244">cpmodel <span>&lt;-</span> <span>cph</span>(<span>Surv</span>(Days, Dead) <span>~</span> Acquisition <span>+</span> FLOSS <span>+</span> Profit <span>+</span> Social <span>+</span> Type <span>+</span></span>
<span id="cb15-245">                                  DeflatedHits <span>+</span> EarlyGoogle,</span>
<span id="cb15-246">               <span>data =</span> google, <span>x=</span><span>TRUE</span>, <span>y=</span><span>TRUE</span>, <span>surv=</span><span>TRUE</span>)</span>
<span id="cb15-247">predictees <span>&lt;-</span> <span>subset</span>(google, Product <span>%in%</span> <span>c</span>(<span>&#34;Alerts&#34;</span>,<span>&#34;Blogger&#34;</span>,<span>&#34;FeedBurner&#34;</span>,<span>&#34;Scholar&#34;</span>,</span>
<span id="cb15-248">                                            <span>&#34;Book Search&#34;</span>,<span>&#34;Voice&#34;</span>,<span>&#34;Gmail&#34;</span>,<span>&#34;Analytics&#34;</span>,</span>
<span id="cb15-249">                                            <span>&#34;AdSense&#34;</span>,<span>&#34;Calendar&#34;</span>,<span>&#34;Alerts&#34;</span>,<span>&#34;Google+&#34;</span>,<span>&#34;Docs&#34;</span>,</span>
<span id="cb15-250">                                            <span>&#34;Search&#34;</span>, <span>&#34;Project Glass&#34;</span>, <span>&#34;Chrome&#34;</span>, <span>&#34;Translate&#34;</span>))</span>
<span id="cb15-251"># seriously ugly hack</span>
<span id="cb15-252">conditionalProbability <span>&lt;-</span> <span>function</span> (d, followupUnits) {</span>
<span id="cb15-253">    chances <span>&lt;-</span> <span>rep</span>(<span>NA</span>, <span>nrow</span>(d)) <span># stash results</span></span>
<span id="cb15-254"></span>
<span id="cb15-255">    <span>for</span> (i <span>in</span> <span>1</span><span>:</span><span>nrow</span>(d)) {</span>
<span id="cb15-256"></span>
<span id="cb15-257">        <span># extract chance of particular subject surviving as long as it has:</span></span>
<span id="cb15-258">        beginProb <span>&lt;-</span> <span>survest</span>(cpmodel, d[i,], <span>times=</span>(d[i,]<span>$</span>Days))<span>$</span>surv</span>
<span id="cb15-259">        <span>if</span> (<span>length</span>(beginProb)<span>==</span><span>0</span>) { beginProb <span>&lt;-</span> <span>1</span> } <span># set to a default</span></span>
<span id="cb15-260"></span>
<span id="cb15-261">        tmpFollowup <span>&lt;-</span> followupUnits <span># reset in each for loop</span></span>
<span id="cb15-262">        <span>while</span> (<span>TRUE</span>) {</span>
<span id="cb15-263">            <span># extract chance of subject surviving as long as it has + an arbitrary additional time-units</span></span>
<span id="cb15-264">            endProb <span>&lt;-</span> <span>survest</span>(cpmodel, d[i,], <span>times=</span>(d[i,]<span>$</span>Days <span>+</span> tmpFollowup))<span>$</span>surv</span>
<span id="cb15-265">            <span># survival curve may not reach that far! &#39;survexp returns &#39;numeric(0)&#39; if it doesn&#39;t;</span></span>
<span id="cb15-266">            <span># so we shrink down 1 day and try again until &#39;survexp&#39; *does* return an usable answer</span></span>
<span id="cb15-267">            <span>if</span> (<span>length</span>(endProb)<span>==</span><span>0</span>) { tmpFollowup <span>&lt;-</span> tmpFollowup <span>-</span> <span>1</span>} <span>else</span> { <span>break</span> }</span>
<span id="cb15-268">        }</span>
<span id="cb15-269"></span>
<span id="cb15-270">        <span># if 50% of all subjects survive to time t, and 20% of all survive to time t+100, say, what chance</span></span>
<span id="cb15-271">        <span># does a survivor - at exactly time t - have of making it to time t+100? 40%: 0.20 / 0.50 = 0.40</span></span>
<span id="cb15-272">        chances[i] <span>&lt;-</span> endProb <span>/</span> beginProb</span>
<span id="cb15-273">    }</span>
<span id="cb15-274">    <span>return</span>(chances)</span>
<span id="cb15-275">}</span>
<span id="cb15-276">## the risks and survival estimate have been stashed in the original CSV to save computation</span>
<span id="cb15-277"># google$RelativeRisk &lt;- predict(cmodel, newdata=google, type=&#34;risk&#34;)</span>
<span id="cb15-278"># google$LinearPredictor &lt;- predict(cmodel, newdata=google, type=&#34;lp&#34;)</span>
<span id="cb15-279"># google$ExpectedEvents &lt;- predict(cmodel, newdata=google, type=&#34;expected&#34;)</span>
<span id="cb15-280"># google$FiveYearSurvival &lt;- conditionalProbability(google, 5*365.25)</span>
<span id="cb15-281"></span>
<span id="cb15-282"># graphs survival curves for each of the 15</span>
<span id="cb15-283"><span>png</span>(<span>file=</span><span>&#34;~/wiki/images/google/15-predicted-survivorship-curves.png&#34;</span>, <span>width =</span> <span>1.5</span><span>*</span><span>480</span>, <span>height =</span> <span>1</span><span>*</span><span>480</span>)</span>
<span id="cb15-284"><span>plot</span>(<span>survfit</span>(cmodel, <span>newdata=</span>predictees),</span>
<span id="cb15-285">     <span>xlab =</span> <span>&#34;time&#34;</span>, <span>ylab=</span><span>&#34;Survival&#34;</span>, <span>main=</span><span>&#34;Survival curves for 15 selected Google products&#34;</span>)</span>
<span id="cb15-286"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb15-287"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Predictions for the 15 and also their relative risks:</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-288">ps <span>&lt;-</span> <span>conditionalProbability</span>(predictees, <span>5</span><span>*</span><span>365.25</span>)</span>
<span id="cb15-289"><span>print</span>(<span>data.frame</span>(predictees<span>$</span>Product, ps<span>*</span><span>100</span>))</span>
<span id="cb15-290"><span>print</span>(<span>round</span>(<span>predict</span>(cmodel, <span>newdata=</span>predictees, <span>type=</span><span>&#34;risk&#34;</span>), <span>digits=</span><span>2</span>))</span>
<span id="cb15-291"></span>
<span id="cb15-292"># Analysis done</span>
<span id="cb15-293"></span>
<span id="cb15-294"><span>cat</span>(<span>&#34;</span><span>\n</span><span>Optimizing the generated graphs by cropping whitespace &amp; losslessly compressing them...</span><span>\n</span><span>&#34;</span>)</span>
<span id="cb15-295"><span>system</span>(<span>paste</span>(<span>&#39;cd ~/wiki/images/google/ &amp;&amp;&#39;</span>,</span>
<span id="cb15-296">             <span>&#39;for f in *.png; do convert &#34;$f&#34; -crop&#39;</span>,</span>
<span id="cb15-297">             <span>&#39;`nice convert &#34;$f&#34; -virtual-pixel edge -blur 0x5 -fuzz 10% -trim -format&#39;</span>,</span>
<span id="cb15-298">             <span>&#39;</span><span>\&#39;</span><span>%wx%h%O</span><span>\&#39;</span><span> info:` +repage &#34;$f&#34;; done&#39;</span>))</span>
<span id="cb15-299"><span>system</span>(<span>&#34;optipng -o9 -fix ~/wiki/images/google/*.png&#34;</span>, <span>ignore.stdout =</span> <span>TRUE</span>)</span></code></pre>
            </div>
          </section>
          <section id="leakage">
            <h2><a href="#leakage" title="Link to section: ¬ß &#39;Leakage&#39;">Leakage</a></h2>
            <p>While the hit-counts are a possible form of leakage, I accidentally caused a clear case of leakage while seeing how random forests would do in predicting shutdowns.</p>
            <p>One way to get data leakage is if we include the end-date; early on in my analysis I removed the <code>Dead</code> variable but it didn‚Äôt occur to me to remove the <code>Ended</code> date factor. The random forest would predict correctly <em>every single shutdown</em> except for 8, for an error-rate of 2%. How did it turn in this nearly-omniscient set of predictions and why did it get those 8 wrong? Because the 8 products are correctly marked in the original dataset as ‚Äúdead‚Äù because their shutdown had been announced by Google, but had been scheduled by Google to die <em>after</em> the day I was running the code. So it turned out that the random forests were just emitting ‚Äòdead‚Äô for ‚Äòanything with an end date before 2013-04-04‚Äô, and alive for everything thereafter!</p>
            <div id="cb16">
              <pre><code><span id="cb16-1"><span>library</span>(randomForest)</span>
<span id="cb16-2">rf <span>&lt;-</span> <span>randomForest</span>(<span>as.factor</span>(Dead) <span>~</span> ., <span>data=</span>google[<span>-</span><span>1</span>])</span>
<span id="cb16-3">google[rf<span>$</span>predicted <span>!=</span> google<span>$</span>Dead,]</span>
<span id="cb16-4">#                      Product Dead    Started      Ended     Hits    Type Profit FLOSS Acquisition</span>
<span id="cb16-5"># 24 Gmail Exchange ActiveSync TRUE‚ÄÖ2009-02-09 2013-07-01   637000 service  FALSE FALSE       FALSE</span>
<span id="cb16-6"># 30  CalDAV support for Gmail TRUE‚ÄÖ2008-07-28 2013-09-16   245000 service  FALSE FALSE       FALSE</span>
<span id="cb16-7"># 37                    Reader TRUE‚ÄÖ2005-10-07 2013-07-01 79100000 service  FALSE FALSE       FALSE</span>
<span id="cb16-8"># 38               Reader Play TRUE‚ÄÖ2010-03-10 2013-07-01    43500 service  FALSE FALSE       FALSE</span>
<span id="cb16-9"># 39                   iGoogle TRUE‚ÄÖ2005-05-01 2013-11-01 33600000 service   TRUE FALSE       FALSE</span>
<span id="cb16-10"># 74            Building Maker TRUE‚ÄÖ2009-10-13 2013-06-01  1730000 service  FALSE FALSE       FALSE</span>
<span id="cb16-11"># 75             Cloud Connect TRUE‚ÄÖ2011-02-24 2013-04-30   530000 program  FALSE FALSE       FALSE</span>
<span id="cb16-12"># 77   Search API for Shopping TRUE‚ÄÖ2011-02-11 2013-09-16   217000 service   TRUE FALSE       FALSE</span>
<span id="cb16-13">#    Social Days  AvgHits DeflatedHits AvgDeflatedHits</span>
<span id="cb16-14"># 24  FALSE‚ÄÖ1603   419.35    9.308e-06         -0.5213</span>
<span id="cb16-15"># 30  FALSE‚ÄÖ1876   142.86    4.823e-06         -0.4053</span>
<span id="cb16-16"># 37   TRUE‚ÄÖ2824 28868.61    7.396e-03         -2.0931</span>
<span id="cb16-17"># 38  FALSE‚ÄÖ1209    38.67    3.492e-07         -0.2458</span>
<span id="cb16-18"># 39   TRUE 3106 11590.20    4.001e-03         -1.6949</span>
<span id="cb16-19"># 74  FALSE‚ÄÖ1327  1358.99    1.739e-05         -0.6583</span>
<span id="cb16-20"># 75  FALSE  796   684.75    2.496e-06         -0.5061</span>
<span id="cb16-21"># 77  FALSE  948   275.73    1.042e-06         -0.4080</span>
<span id="cb16-22"># rf</span>
<span id="cb16-23"># ...</span>
<span id="cb16-24">#     Type of random forest: classification</span>
<span id="cb16-25">#                      Number of trees: 500</span>
<span id="cb16-26"># No. of variables tried at each split: 3</span>
<span id="cb16-27">#</span>
<span id="cb16-28">#         OOB estimate of  error rate: 2.29%</span>
<span id="cb16-29"># Confusion matrix:</span>
<span id="cb16-30">#       FALSE TRUE class.error</span>
<span id="cb16-31"># FALSE   226    0     0.00000</span>
<span id="cb16-32"># TRUE      8  115     0.06504</span></code></pre>
            </div><!--
http://blog.detectify.com/post/82370846588/how-we-got-read-access-on-googles-production-servers
13:03:04 <@gwern> '... And this is why you want to discontinue products and services your engineers can't be motivated to maintain. Amazing. This should scare
                  anyone who has ever left an old side project running; I could see a lot of companies doing a product/service portfolio review based on this
                  as a case study.' https://news.ycombinator.com/item?id=7571942 no kidding
-->
          </section>
        </section>
        
        <section id="further-reading">
          <a id="backlinks" href="https://ntietz.com/metadata/annotations/backlinks/%252FGoogle-shutdowns.html" title="Reverse citations/backlinks for this page (the list of other pages which link to this page).">backlinks</a>
        </section>
        <section id="link-bibliography">
          <a id="link-bibliography-link-footer-transclusion" href="https://ntietz.com/docs/link-bibliography/Google-shutdowns#link-bibliography" title="Bibliography of links cited in this page (forward citations). Lazily-transcluded version at footer of page for easier scrolling.">Link Bibliography</a>
        </section>
      </div></div>
  </body>
</html>
