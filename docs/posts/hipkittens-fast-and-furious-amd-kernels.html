<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://hazyresearch.stanford.edu/blog/2025-11-09-hk">Original</a>
    <h1>HipKittens: Fast and furious AMD kernels</h1>
    
    <div id="readability-page-1" class="page"><div><p><strong>Team</strong>: William Hu, Drew Wadsworth, Sean Siddens, Stanley Winata, Daniel Fu, Ryan Swann, Muhammad Osama, Christopher Ré, Simran Arora</p>
<p><strong>AI is gated by hardware. We think that opening up AI’s compute landscape is one of the most important problems to be working on right now.</strong> Building towards this goal, we present HipKittens: SoTA AMD kernels and a collection of opinionated programming primitives to make AMD kernel dev easier!</p>
<p><em>Named after AMD&#39;s CUDA equivalent, called <a href="https://www.google.com/url?q=https://rocm.docs.amd.com/projects/HIP/en/docs-develop/what_is_hip.html&amp;sa=D&amp;source=docs&amp;ust=1762797198251657&amp;usg=AOvVaw0nJDptNvO-LI1BHu_fKZMT">HIP</a>.</em></p>
<h3>Building towards multi-silicon AI systems</h3>
<p>While AI has largely <a href="https://hc2023.hotchips.org/assets/program/conference/day2/Keynote%202/Keynote-NVIDIA_Hardware-for-Deep-Learning.pdf">used a single hardware vendor</a> to get to its current stage, AMD GPU hardware now offers state-of-the-art peak compute and memory bandwidth. However, this performance is locked away from AI workflows due to the lack of mature AMD software.</p>
<table><thead><tr><th>Spec</th><th>NVIDIA B200 SXM5</th><th>AMD MI355X OAM</th></tr></thead><tbody><tr><td>BF16 matrix / tensor</td><td>2.2 PFLOPs</td><td>2.5 PFLOPs</td></tr><tr><td>MXFP8 matrix / tensor</td><td>4.5 PFLOPs</td><td>5.0 PFLOPs</td></tr><tr><td>MXFP6 matrix / tensor</td><td>4.5 PFLOPs</td><td>10.1 PFLOPs</td></tr><tr><td>MXFP4 matrix / tensor</td><td>9.0 PFLOPs</td><td>10.1 PFLOPs</td></tr><tr><td>Memory capacity</td><td>180 GB</td><td>288 GB</td></tr><tr><td>Memory bandwidth</td><td>8.0 TB/s</td><td>8.0 TB/s</td></tr></tbody></table>
<figcaption><p>Table 1: <strong>Hardware overview.</strong> Peak memory and compute speeds for the latest generation GPU platforms.</p></figcaption>
<p>The AMD software ecosystem includes <a href="https://github.com/ROCm/aiter">AITER</a>, a high performance AI kernel library; PyTorch and a few compilers (<a href="https://github.com/ROCm/triton">Triton</a>, <a href="https://github.com/modular/modular">Mojo</a>, <a href="https://github.com/tile-ai/tilelang">TileLang</a>); and <a href="https://github.com/ROCm/composable_kernel">Composable Kernel (CK)</a>, AMD&#39;s C++ based programming model for writing kernels. However, despite <a href="https://openai.com/index/openai-amd-strategic-partnership/">gigawatt-scale AMD deployments</a>, the software remains brittle.</p>
<ol>
<li><strong>The existing software offerings fail to consistently achieve peak performance.</strong> CK kernels frequently underperform (see our evaluations below). AITER and PyTorch are volatile; for instance, AITER and PyTorch SDPA Llama GQA backwards kernels achieve just 30% and 24% of SoTA performance respectively on AMD MI355X GPUs. And the compilers currently significantly sacrifice performance and have not yet demonstrated reusable programming primitives for AMD. Further, we find that some critical aspects of hardware functionality around bank conflict avoidance are undocumented in the <a href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-cdna4-instruction-set-architecture.pdf">CDNA ISA</a>, discussed in <a href="https://hazyresearch.stanford.edu/blog/2025-11-09-amd-brr">our technical deep dive blog</a>.</li>
</ol>
<details><summary>Details: expand to learn more about our current compiler observations</summary><p>We expand on a few observations about current compilers:</p><ul>
<li>
<p>Mojo&#39;s <a href="https://github.com/modular/modular/tree/main/max/kernels/benchmarks/gpu">MHA forwards kernel suffers from bank conflicts</a> and offers just 50% of peak performance on the MI355X (e.g., 430 TFLOPs at B=16, H=16, N=2048, D=128).</p>
</li>
<li>
<p>TileLang <a href="https://openreview.net/pdf?id=Jb1WkNSfUB">is currently limited to CDNA3</a> and the MHA kernel <a href="https://openreview.net/pdf?id=Jb1WkNSfUB">“is competitive with PyTorch”</a>, which is amongst the slowest baselines (see results below). TileLang lacks many features that we believe are important for AMD kernels--multiple <a href="https://github.com/tile-ai/tilelang/blob/85218bd97309deeff5a7ad5dc1f111d5366244d9/src/tl_templates/hip/gemm.h#L12">matrix core shapes like 32x32x16</a>, <a href="https://github.com/tile-ai/tilelang/blob/85218bd97309deeff5a7ad5dc1f111d5366244d9/src/tl_templates/hip/copy.h#L69">buffer_load_dwordx4</a>, XCD chiplet swizzling--and also has <a href="https://github.com/tile-ai/tilelang/blob/85218bd97309deeff5a7ad5dc1f111d5366244d9/src/tl_templates/hip/reduce.h#L15">dependencies on Composable Kernel</a>, adding complexity to the library.</p>
</li>
<li>
<p>On AMD, Triton struggles with register lifetime tracking and lowering memory accesses to the most performant intrinsics. For example, it may fail to <a href="https://github.com/ROCm/tritonBLAS/blob/98c7d7d43ce831b30240389687f0417ff4b8fcc4/include/tritonblas/internal/streamk_matmul.py#L206-L265">reclaim registers</a> or <a href="https://github.com/ROCm/iris/blob/ec2dd919536ff56ce731020ac879a32430983291/iris/iris.py#L1456-L1462">lower vectorized loads</a>. In our evaluations, we find that Triton kernels from AMD’s <a href="https://github.com/ROCm/triton">ROCm/Triton</a> tend to underperform (even on a vanilla BF16 GEMM).</p>
</li>
</ul><p>While the Pythonic interface and portability of such compilers is helpful, our goal is to identify the principles that lead to peak performance as well.</p></details>
<ol start="2">
<li><strong>As a result, AMD&#39;s most performant AI kernels need to be hand-optimized by experts <strong>in raw assembly</strong>.</strong> It is very difficult to scale to the breadth of AI workloads and as a result, most widely used AI workloads are unsupported/under-optimized on AMD (e.g., we see this on some attention problem shapes, non-causal gqa backwards pass, memory bound kernels).</li>
</ol>
<figure><img src="https://www.vesto.me/static/posts/2025-11-09-hk/asm.png"/><figcaption><p>Figure: what is raw assembly? can&#39;t understand it? that&#39;s the point!</p></figcaption></figure>
<p><strong>With all of this, it remains up in the air what the best path forwards is for multi-silicon kernel development!</strong></p>
<p>As a result, the AI community says that there’s a CUDA moat in AI software: <a href="https://x.com/jxmnop/status/1887898002188103796">tweet #1</a>, <a href="https://x.com/SemiAnalysis_/status/1960070677379133949">tweet #2</a>, <a href="https://www.linkedin.com/posts/appenz_flash-attention-4-was-announced-today-at-activity-7365856788727939072-QI5B">tweet #3</a> and <a href="https://x.com/search?q=cuda%20moat&amp;src=typed_query">many more</a>.</p>
<figure><img src="https://www.vesto.me/static/posts/2025-11-09-hk/moat.png"/></figure>
<p>But that being said, developing performant NVIDIA kernels was also painstakingly tedious a few years ago. Using low level CUDA/CUTLASS, it took two years between the H100 GPU’s release and the release of <a href="https://arxiv.org/abs/2407.08608">peak performance open-source attention kernels</a>.  Compilers and <a href="https://arxiv.org/abs/2502.10517">LLMs-for-kernel-development</a> on NVIDIA have so far sacrificed performance for simplicity and <a href="https://x.com/difficultyang/status/1968027448622379387">struggled to quickly support new hardware features</a>.</p>
<p><strong>Opinionated primitives are simplifying the process of writing performant NVIDIA kernels!</strong>  Amazingly through the community’s effort on all these DSLs and AI assistants, NVIDIA kernel development is starting to get easier! Last year, we shared our opinionated take on kernel DSLs — keep the familiar PyTorch feel, but make the primitives C++ embedded to get peak performance, simplicity and extensibility to hardware platforms and AI workloads. In May 2024, we shared <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-quick-tk">ThunderKittens (TK)</a>, and have been excited to see its ideas used in a wave of frameworks this year like <a href="https://docs.nvidia.com/cutlass/media/docs/pythonDSL/cute_dsl.html">CuTe DSL in Sept 2025</a>, <a href="https://x.com/__tinygrad__/status/1976084605141909845">Tiny Grad “tinykittens”</a>, <a href="https://arxiv.org/abs/2504.17577">TileLang in April 2025</a> and <a href="https://github.com/triton-lang/triton/tree/main/python/tutorials/gluon">Gluon in June 2025</a>. It’s been fun to see TK used in companies like <a href="https://www.together.ai/blog/thunderkittens">Together AI</a>, Jump Trading, and <a href="https://cursor.com/en-US/blog/kernels">Cursor</a> and in academic research.</p>
<p><strong>So then we were curious whether entirely new programming primitives are needed to simplify AMD kernel development, or whether existing primitives suffice.</strong> It wasn&#39;t obvious to us where this exploration would end up; <a href="https://arxiv.org/abs/2407.08608">most</a> <a href="https://openreview.net/pdf?id=fGgQS5VW09">modern</a> <a href="https://arxiv.org/pdf/2412.19437">kernels</a> are designed around NVIDIA-specific hardware features. AMD hardware differs meaningfully (no <em>wgmma/tcgen05</em> pipelined async matmuls, no <em>tma</em>, no <em>mbarriers</em>, no register reallocation, smaller shared memory, chiplet instead of monolithic, etc.) and we weren&#39;t sure where performance would end up nor how different the primitives might look compared to NVIDIA frameworks.</p>
<p>Our exploration resulted in HipKittens, a minimal, opinionated collection of C++ embedded programming primitives for fast AMD kernels. We find:</p>
<ol>
<li><strong>The tile abstraction generalizes across architectures.</strong> The core tile-based primitives we identified as effective on NVIDIA GPUs—including tile types, PyTorch-like bulk compute operators over tiles, and composable load/store interfaces—translate naturally to AMD.</li>
<li><strong>Backend implementations are architecture-specific.</strong> The underlying memory access patterns (e.g., swizzling schemes, register scheduling) that realize the tile interface differ between AMD and NVIDIA due to hardware differences.</li>
<li><strong>Scheduling strategies adapt to hardware constraints.</strong> The scheduling patterns both within a processor and across processors differ on AMD compared to NVIDIA, reflecting fundamental architectural distinctions. Wave specialization underperforms on CDNA3 and CDNA4. However, reasoning about schedules at the tile granularity—rather than at the level of individual registers or memory transactions—continues to simplify development, maintain code readability, and enable peak performance.</li>
</ol>
<p>Ultimately, we see that tile-based abstractions remain general across architectures, providing evidence that a unified, performant programming model for AI accelerators is achievable. The key insight is separating the interface (tiles and operations on tiles) from the implementation (how tiles map to hardware), allowing the same high-level programming model to target diverse GPU architectures.</p>
<h3>Climbing out of the CUDA moat: Introducing HipKittens</h3>
<p>We first explored <a href="https://github.com/HazyResearch/ThunderKittens">ThunderKittens for NVIDIA</a>, then <a href="https://hazyresearch.stanford.edu/blog/2024-11-28-tk-mlx">ThunderMittens on Apple Silicon</a> and now we’re excited to share <a href="https://github.com/HazyResearch/HipKittens/tree/main">HipKittens (HK) for AMD</a>!</p>
<figure><img src="https://www.vesto.me/static/posts/2025-11-09-hk/cinematic.png"/><figcaption><p>Figure: The Kittens cinematic universe! Towards multi-silicon AI!</p></figcaption></figure>
<p>HK kernels are performant, while remaining easy to read and modify! We might not need raw assembly for peak performance AMD kernels any more! Life is good!</p>
<figure><img src="https://www.vesto.me/static/posts/2025-11-09-hk/hk_wave.png"/><figcaption><p>Figure: HipKittens riding the ~wave~ (not warp).</p></figcaption></figure>
<p>Let&#39;s go through the results:</p>
<ol>
<li>Our attention forwards kernels are written in <a href="https://github.com/HazyResearch/HipKittens/blob/ec9fd64195d10bf5fa06d2170fd5c68a457a77ab/kernels/attn/gqa/kernel.cpp#L102">~500 lines of code</a> and outperform all of AMDs baselines on average, including the AITER kernels which are written in hand-optimized assembly! We show different head dimensions (<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>D</span></span></span></span></span>) and sequence lengths <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>N</span></span></span></span></span>, for both causal and non-causal settings.</li>
</ol>
<figure><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_gqa_causal_fwd_attn_plot.png"/><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_d64_gqa_causal_fwd_attn_d64_plot.png"/><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_gqa_non_causal_fwd_attn_plot.png"/><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_d64_gqa_non_causal_fwd_attn_d64_plot.png"/></figure>
<ol start="2">
<li>Our GEMM kernel features a <a href="https://github.com/HazyResearch/HipKittens/blob/ec9fd64195d10bf5fa06d2170fd5c68a457a77ab/kernels/gemm/bf16fp32/mi350x/256_256_64_32_with16x32.cpp#L113">hot loop</a> that&#39;s <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">&lt;100</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>&lt;</span><span></span></span><span><span></span><span>100</span></span></span></span></span> lines of code and achieves peak performance. Again, AITER and HipBLASLT kernels are programmed in ... raw assembly!</li>
</ol>
<figure><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_bf16_gemm_plot.png"/><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_fp8_gemm_plot.png"/></figure>
<ol start="3">
<li>We also get speedy attention backwards pass, rotary, and fused dropout-residual-layernorm kernels compared to the strongest available baselines! These results use head dimension <span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn></mrow><annotation encoding="application/x-tex">128</annotation></semantics></math></span><span aria-hidden="true"><span><span></span><span>128</span></span></span></span></span> and we vary the sequence length.</li>
</ol>
<figure><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_gqa_non_causal_bkwd_plot.png"/><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_gqa_causal_bkwd_plot.png"/><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_norm_plot.png"/><img src="https://www.vesto.me/static/posts/2025-11-09-hk/mi355x_rotary_plot.png"/></figure>
<h3>Multi-silicon AI is coming!</h3>
<p>Realizing AI&#39;s full potential requires diverse, open hardware.<sup id="fnref-1"><a href="#fn-1">1</a></sup> Today, that means making AMD GPUs truly accessible.</p>
<p>We want more AI in the world. AI has relied on and innovated on a single hardware provider, but we need to be able to use and experiment with all the kinds of compute we can. We need to be able to use the fastest hardware out there. We’re happy to help address these problems with HipKittens!</p>
<p>Checkout <a href="https://hazyresearch.stanford.edu/blog/2025-11-09-amd-brr">part two</a> for a technical deep dive on HK.</p>
<p><strong>Links</strong>: <a href="https://arxiv.org/abs/2511.08083">Arxiv</a> | <a href="https://github.com/HazyResearch/HipKittens">Code</a></p>
</div></div>
  </body>
</html>
