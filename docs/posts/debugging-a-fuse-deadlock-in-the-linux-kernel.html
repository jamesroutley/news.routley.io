<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://netflixtechblog.com/debugging-a-fuse-deadlock-in-the-linux-kernel-c75cd7989b6d?gi=f4c590ed3fe6">Original</a>
    <h1>Debugging a FUSE deadlock in the Linux kernel</h1>
    
    <div id="readability-page-1" class="page"><section><div><div><div><div><div><div><div><div><div><div><div><a href="https://netflixtechblog.medium.com/?source=post_page-----c75cd7989b6d--------------------------------" rel="noopener follow"><div><div aria-hidden="false"><div><div><p><img alt="Netflix Technology Blog" src="https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" width="44" height="44" loading="lazy"/></p></div></div></div></div></a><a href="https://netflixtechblog.com/?source=post_page-----c75cd7989b6d--------------------------------" rel="noopener  ugc nofollow"><div><div><div aria-hidden="false"><div><div><p><img alt="Netflix TechBlog" src="https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png" width="24" height="24" loading="lazy"/></p></div></div></div></div></div></a></div></div></div></div></div></div></div><p id="939c"><a href="https://tycho.pizza" rel="noopener ugc nofollow" target="_blank">Tycho Andersen</a></p><p id="97e4">The Compute team at Netflix is charged with managing all AWS and containerized workloads at Netflix, including autoscaling, deployment of containers, issue remediation, etc. As part of this team, I work on fixing strange things that users report.</p><p id="adb6">This particular issue involved a custom internal <a href="https://www.kernel.org/doc/html/latest/filesystems/fuse.html" rel="noopener ugc nofollow" target="_blank">FUSE filesystem</a>: <a rel="noopener ugc nofollow" target="_blank" href="https://www.datadoodad.com/netflix-drive-a607538c3055">ndrive</a>. It had been festering for some time, but needed someone to sit down and look at it in anger. This blog post describes how I poked at <code>/proc</code>to get a sense of what was going on, before posting the issue to the kernel mailing list and getting schooled on how the kernel’s wait code actually works!</p><p id="1e38">We had a stuck docker API call:</p><pre><span id="91cc">goroutine 146 [select, 8817 minutes]:</span></pre><p id="abe6">Here, our management engine has made an HTTP call to the Docker API’s unix socket asking it to kill a container. Our containers are configured to be killed via <code>SIGKILL</code>. But this is strange. <code>kill(SIGKILL)</code> should be relatively fatal, so what is the container doing?</p><pre><span id="4458">$ docker exec -it 6643cd073492 bash</span></pre><p id="9510">Hmm. Seems like it’s alive, but <code>setns(2)</code> fails. Why would that be? If we look at the process tree via <code>ps awwfux</code>, we see:</p><pre><span id="e7da">\_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/6643cd073492ba9166100ed30dbe389ff1caef0dc3d35</span></pre><p id="484b">Ok, so the container’s init process is still alive, but it has one zombie child. What could the container’s init process possibly be doing?</p><pre><span id="2bb2"># cat /proc/1528591/stack</span></pre><p id="5c89">It is in the process of exiting, but it seems stuck. The only child is the ndrive process in Z (i.e. “zombie”) state, though. Zombies are processes that have successfully exited, and are waiting to be reaped by a corresponding <code>wait()</code> syscall from their parents. So how could the kernel be stuck waiting on a zombie?</p><pre><span id="b55b"># ls /proc/1544450/task</span></pre><p id="5d90">Ah ha, there are two threads in the thread group. One of them is a zombie, maybe the other one isn’t:</p><pre><span id="1cc3"># cat /proc/1544574/stack</span></pre><p id="f377">Indeed it is not a zombie. It is trying to become one as hard as it can, but it’s blocking inside FUSE for some reason. To find out why, let’s look at some kernel code. If we look at <code><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/kernel/pid_namespace.c?h=v5.19#n166" rel="noopener ugc nofollow" target="_blank">zap_pid_ns_processes()</a></code>, it does:</p><pre><span id="ee96">/*</span></pre><p id="b65e">which is where we are stuck, but before that, it has done:</p><pre><span id="73e6">/* Don&#39;t allow any more processes into the pid namespace */</span></pre><p id="3eed">which is why docker can’t <code>setns()</code> — the <em>namespace</em> is a zombie. Ok, so we can’t <code>setns(2)</code>, but why are we stuck in <code>kernel_wait4()</code>? To understand why, let’s look at what the other thread was doing in FUSE’s <code><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/fs/fuse/dev.c?h=v5.19#n407" rel="noopener ugc nofollow" target="_blank">request_wait_answer()</a></code>:</p><pre><span id="ea99">/*</span></pre><p id="df37">Ok, so we’re waiting for an event (in this case, that userspace has replied to the FUSE flush request). But <code>zap_pid_ns_processes()</code>sent a <code>SIGKILL</code>! <code>SIGKILL</code> should be very fatal to a process. If we look at the process, we can indeed see that there’s a pending <code>SIGKILL</code>:</p><pre><span id="8bc8"># grep Pnd /proc/1544574/status</span></pre><p id="3a27">Viewing process status this way, you can see <code>0x100</code> (i.e. the 9th bit is set) under <code>SigPnd</code>, which is the signal number corresponding to <code>SIGKILL</code>. Pending signals are signals that have been generated by the kernel, but have not yet been delivered to userspace. Signals are only delivered at certain times, for example when entering or leaving a syscall, or when waiting on events. If the kernel is currently doing something on behalf of the task, the signal may be pending. Signals can also be blocked by a task, so that they are never delivered. Blocked signals will show up in their respective pending sets as well. However, <code>man 7 signal</code> says: “The signals <code>SIGKILL</code> and <code>SIGSTOP</code> cannot be caught, blocked, or ignored.” But here the kernel is telling us that we have a pending <code>SIGKILL</code>, aka that it is being ignored even while the task is waiting!</p><p id="03f8">Well that is weird. The wait code (i.e. <code>include/linux/wait.h</code>) is used everywhere in the kernel: semaphores, wait queues, completions, etc. Surely it knows to look for <code>SIGKILL</code>s. So what does <code>wait_event()</code> actually do? Digging through the macro expansions and wrappers, the meat of it is:</p><pre><span id="7675">#define ___wait_event(wq_head, condition, state, exclusive, ret, cmd)           \</span></pre><p id="2343">So it loops forever, doing <code>prepare_to_wait_event()</code>, checking the condition, then checking to see if we need to interrupt. Then it does <code>cmd</code>, which in this case is <code>schedule()</code>, i.e. “do something else for a while”. <code>prepare_to_wait_event()</code> looks like:</p><pre><span id="ec85">long prepare_to_wait_event(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state)</span></pre><p id="7e53">It looks like the only way we can break out of this with a non-zero exit code is if <code>signal_pending_state()</code> is true. Since our call site was just <code>wait_event()</code>, we know that state here is <code>TASK_UNINTERRUPTIBLE</code>; the definition of <code>signal_pending_state()</code> looks like:</p><pre><span id="bc4b">static inline int signal_pending_state(unsigned int state, struct task_struct *p)</span></pre><p id="45d6">Our task is not interruptible, so the first if fails. Our task should have a signal pending, though, right?</p><pre><span id="bc82">static inline int signal_pending(struct task_struct *p)</span></pre><p id="d7ba">As the comment notes, <code>TIF_NOTIFY_SIGNAL</code> isn’t relevant here, in spite of its name, but let’s look at <code>task_sigpending()</code>:</p><pre><span id="88bf">static inline int task_sigpending(struct task_struct *p)</span></pre><p id="962f">Hmm. Seems like we should have that flag set, right? To figure that out, let’s look at how signal delivery works. When we’re shutting down the pid namespace in <code>zap_pid_ns_processes()</code>, it does:</p><pre><span id="ac63">group_send_sig_info(SIGKILL, SEND_SIG_PRIV, task, PIDTYPE_MAX);</span></pre><p id="a015">which eventually gets to <code>__send_signal_locked()</code>, which has:</p><pre><span id="68ae">pending = (type != PIDTYPE_PID) ? &amp;t-&gt;signal-&gt;shared_pending : &amp;t-&gt;pending;</span></pre><p id="a0ed">Using <code>PIDTYPE_MAX</code> here as the type is a little weird, but it roughly indicates “this is very privileged kernel stuff sending this signal, you should definitely deliver it”. There is a bit of unintended consequence here, though, in that <code>__send_signal_locked()</code> ends up sending the <code>SIGKILL</code> to the shared set, instead of the individual task’s set. If we look at the <code>__fatal_signal_pending()</code> code, we see:</p><pre><span id="0a6e">static inline int __fatal_signal_pending(struct task_struct *p)</span></pre><p id="ba1a">But it turns out this is a bit of a red herring (<a href="https://lore.kernel.org/all/YuGUyayVWDB7R89i@tycho.pizza/" rel="noopener ugc nofollow" target="_blank">although</a> <a href="https://lore.kernel.org/all/20220728091220.GA11207@redhat.com/" rel="noopener ugc nofollow" target="_blank">it</a> <a href="https://lore.kernel.org/all/871qu6bjp3.fsf@email.froward.int.ebiederm.org/" rel="noopener ugc nofollow" target="_blank">took</a> <a href="https://lore.kernel.org/all/8735elhy4u.fsf@email.froward.int.ebiederm.org/" rel="noopener ugc nofollow" target="_blank">a</a> <a href="https://lore.kernel.org/all/87pmhofr1q.fsf@email.froward.int.ebiederm.org/" rel="noopener ugc nofollow" target="_blank">while</a> for me to understand that).</p><p id="8339">To understand what’s really going on here, we need to look at <code>complete_signal()</code>, since it unconditionally adds a <code>SIGKILL</code> to the task’s pending set:</p><pre><span id="5c53">sigaddset(&amp;t-&gt;pending.signal, SIGKILL);</span></pre><p id="4553">but why doesn’t it work? At the top of the function we have:</p><pre><span id="3636">/*</span></pre><p id="07ad">but as <a href="https://lore.kernel.org/all/877d4jbabb.fsf@email.froward.int.ebiederm.org/" rel="noopener ugc nofollow" target="_blank">Eric Biederman described</a>, basically every thread can handle a <code>SIGKILL</code> at any time. Here’s <code>wants_signal()</code>:</p><pre><span id="d689">static inline bool wants_signal(int sig, struct task_struct *p)</span></pre><p id="04f2">So… if a thread is already exiting (i.e. it has <code>PF_EXITING</code>), it doesn’t want a signal. Consider the following sequence of events:</p><p id="f18d">1. a task opens a FUSE file, and doesn’t close it, then exits. During that exit, the kernel dutifully calls <code>do_exit()</code>, which does the following:</p><pre><span id="07d1">exit_signals(tsk); /* sets PF_EXITING */</span></pre><p id="2fd3">2. <code>do_exit()</code> continues on to <code>exit_files(tsk);</code>, which flushes all files that are still open, resulting in the stack trace above.</p><p id="ded2">3. the pid namespace exits, and enters <code>zap_pid_ns_processes()</code>, sends a <code>SIGKILL</code> to everyone (that it expects to be fatal), and then waits for everyone to exit.</p><p id="8eb7">4. this kills the FUSE daemon in the pid ns so it can never respond.</p><p id="2db9">5. <code>complete_signal()</code> for the FUSE task that was already exiting ignores the signal, since it has <code>PF_EXITING</code>.</p><p id="b28c">6. Deadlock. Without manually aborting the FUSE connection, things will hang forever.</p><p id="16d8">It doesn’t really make sense to wait for flushes in this case: the task is dying, so there’s nobody to tell the return code of <code>flush()</code> to. It also turns out that this bug can happen with several filesystems (anything that calls the kernel’s wait code in <code>flush()</code>, i.e. basically anything that talks to something outside the local kernel).</p><p id="0106">Individual filesystems will need to be patched in the meantime, for example the fix for FUSE is <a href="https://github.com/torvalds/linux/commit/14feceeeb012faf9def7d313d37f5d4f85e6572b" rel="noopener ugc nofollow" target="_blank">here</a>, which was released on April 23 in Linux 6.3.</p><p id="ea5e">While this blog post addresses FUSE deadlocks, there are definitely issues in the nfs code and elsewhere, which we have not hit in production yet, but almost certainly will. You can also see it as a <a href="https://lore.kernel.org/all/20230512225414.GE3223426@dread.disaster.area/" rel="noopener ugc nofollow" target="_blank">symptom of other filesystem bugs</a>. Something to look out for if you have a pid namespace that won’t exit.</p><p id="6e54">This is just a small taste of the variety of strange issues we encounter running containers at scale at Netflix. Our team is hiring, so please reach out if you also love red herrings and kernel deadlocks!</p></div></div></div></div></section></div>
  </body>
</html>
