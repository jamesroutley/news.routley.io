<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.gilesthomas.com/2025/10/llm-from-scratch-22-finally-training-our-llm">Original</a>
    <h1>Writing an LLM from scratch, part 22 â€“ training our LLM</h1>
    
    <div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest(&#39;.dropdown&#39;)) {
                        let targetId = event.target.closest(&#39;.dropdown&#39;).dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? &#39;&#39; : targetId;
                        event.stopPropagation();
                    }">

                

                <div>
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

                
                
                
            </div>

            

    

    

    <p>This post wraps up my notes on chapter 5 of <a href="https://sebastianraschka.com/">Sebastian Raschka</a>&#39;s book
&#34;<a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Build a Large Language Model (from Scratch)</a>&#34;.
Understanding <a href="https://www.gilesthomas.com/2025/10/llm-from-scratch-20-starting-training-cross-entropy-loss">cross entropy loss</a> and
<a href="https://www.gilesthomas.com/2025/10/llm-from-scratch-21-perplexed-by-perplexity">perplexity</a> were the hard bits for
me in this chapter -- the remaining 28 pages were more a case of plugging bits together and
running the code, to see what happens.</p>

<p>The shortness of this post almost feels like a damp squib.  After writing so much
in the last 22 posts, there&#39;s really not all that much to say -- but that hides the fact that
this part of the book is probably the most exciting to work through.  All these pieces
developed with such care, and with so much to learn, over the preceding 140 pages,
with not all that much to show -- and suddenly, we have a codebase that we can let
rip on a training set -- and our model starts talking to us!</p>

<p>I trained my model on the sample dataset that we use in the book, the 20,000
characters of &#34;The Verdict&#34; by Edith Wharton, and then ran it to predict next tokens after &#34;Every effort
moves you&#34;.  I got:</p>

<pre><code>Every effort moves you in,&#34; was down surprise a was one of lo &#34;I quote.
</code></pre>

<p>Not bad for a model trained on such a small amount of data (in just over ten seconds).</p>

<p>The next step was to download the weights for the original 124M-parameter version of
GPT-2 from OpenAI, following the instructions in the book, and then to load them
into my model.  With those weights, against the same prompt, I got this:</p>

<pre><code>Every effort moves you as far as the hand can go until the end of your turn unless something interrupts your control flow. As you may observe I
</code></pre>

<p>That&#39;s amazingly cool.  Coherent enough that you could believe it&#39;s part of the instructions for a game.</p>

<p>Now, I won&#39;t go through the remainder of the chapter in detail -- as I said, it&#39;s essentially
just plugging together the various bits that we&#39;ve gone through so far, even though the results
are brilliant.  In this post I&#39;m
just going to make a few brief notes on the things that I found interesting.</p>


    
        <h3 id="randomness-and-seeding">Randomness and seeding</h3>

<p>One thing I really do recommend to anyone working through the book is that you type
in all of the code, and run it yourself -- it really will help you remember
how stuff fits together.</p>

<p>There is one slight issue I found with that, however:
the book has a number of examples where you get output from code that uses randomness -- for
example, where you take a look at the loss it has on some sample text before you
start training, or make it generate samples during the train.</p>

<p>Now, in theory, because Raschka puts <code>torch.manual_seed</code> calls before all of these,
the results you get should be exactly the same as the outputs in the book.  However,
the amount of code we&#39;re working with at this stage is quite large -- we have various
helper functions that were created in earlier sections, for example.  And some of these
use randomness.</p>

<p>That means that to get the same results as the ones in the book, you would need to ensure
that all of the code that uses randomness was running in exactly the same order as it was
when Raschka did it for the book.  That turns out to be surprisingly hard!</p>

<p>My instinct is that it doesn&#39;t actually matter all that much.  So long as the loss numbers
that you see are in the same ballpark as the ones in the book, and the outputs you see
are roughly equally incoherent (before training) and become more coherent at what feels like
the same kind of rate, you&#39;re fine.  Probably the most important one to look out for
is when the training run starts -- you should see loss on the training set decreasing steadily,
just like in the book, and likewise as in the book, the validation loss should plateau out pretty early.</p>

<h3 id="optimisers">Optimisers</h3>

<p>When I have built simple backpropagation through neural networks in the past, I&#39;ve
generally updated parameters by multiplying the gradients by a small number, the
<em>learning rate</em>, and then subtracting them from their respective parameters to get
updated ones -- classic <em>stochastic gradient descent</em>.</p>

<p>Non-trivial ML uses optimisers; I&#39;d come across them while <a href="https://www.gilesthomas.com/fine-tuning">fine-tuning LLMs</a>,
and also used one in the RNN code I wrote <a href="https://www.gilesthomas.com/2025/10/revisiting-karpathy-unreasonable-effectiveness-rnns">last week</a>.
Instead of updating the parameters yourself, you ask the optimiser to do it for you, by
calling its <code>step</code> function.  <a href="https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html">AdamW</a> appears to be the default optimiser in most textbooks,
though <a href="https://kellerjordan.github.io/posts/muon/">Muon</a> seems to be the most popular
in use, if my AI X/Twitter feed is to be believed.</p>

<p>I don&#39;t understand how optimisers work in any detail, and I&#39;m going to have to dig into that in the future.  However, my
high-level simplified picture right now is that they dynamically adjust the learning
rate over time, so that it&#39;s easier to take big &#34;jumps&#34; downwards on the gradients when
you start, and then smaller ones later.  I believe they can also sometimes avoid local
minima in the loss landscape -- a nice metaphor I read somewhere (lost the source, sadly)
was that simple gradient descent was like rolling a ball down a hill, but (some?) optimisers give the ball a bit
of momentum so that it can coast over a small uphill portion, so long as the general
slope is downwards.</p>

<p>Anyway, more investigation needed later.</p>

<p>In practice, with AdamW, you initialise it at the start of your training loop,
with a learning rate (which I imagine is similar to the one my older code used, a
scaling factor for gradients) and a weight decay (:shrug:).  You also provide it with the parameters
it&#39;s going to be managing.</p>

<p>In the training loop, at the start of each input batch, you tell it to zero out the gradients it&#39;s managing
with <code>optimizer.zero_grad()</code>, run the data through your model and calculate your loss, and then after
calling <code>loss.backward()</code> to get your gradients,
you just call <code>optimizer.step()</code>, and that does the parameter update.</p>

<p>Again, I want to dig into how optimisers work in more detail in the future.  But
for now, I think that&#39;s all I need to know.</p>

<h3 id="speed-and-the-cost-of-training">Speed, and the cost of training</h3>

<p>The book tells you how to train on a public domain book, &#34;The Verdict&#34; by Edith Wharton.
Full training on the hardware that people are likely to have to hand would be extremely
expensive, so we just train on that short example, then later on learn how to download
and use the weights that OpenAI made available for their GPT-2 models.</p>

<p>But there was something that surprised me a little.  When talking about the training
run on &#34;The Verdict&#34;, Raschka says that it takes &#34;about 5 minutes to complete on a MacBook
Air&#34;.</p>

<p>On my machine using CUDA on an RTX 3090, it took just less than eleven seconds.</p>

<p>This makes perfect sense, of course -- there&#39;s a really good reason why AI training
is normally done on GPUs or custom hardware, and the MacBook Air would presumably
be training on the CPU.  But I was a little surprised at how huge the difference was
in this simple example!</p>

<p>Now, while the book mentions that Llama 2 probably cost hundreds of thousands of dollars to train,
I must admit that I do wonder how much it really would cost to train a 124M parameter
model on my own hardware -- or, indeed, on the machines with 8x 80GiB A100 GPUs that I rented
from Lambda Labs during my fine-tuning experiments.</p>

<p>Andrej Karpathy was able to <a href="https://github.com/karpathy/llm.c/discussions/481">train a 124M GPT-2 model for $20</a>,
using his hand-written C/CUDA LLM system <code>llm.c</code>.  That is undoubtedly more efficient than the
PyTorch code that we&#39;re working on in this book.  But it really would be interesting
to find out whether it would be doable for me at all!  The training data he used
is the 10B-token version of the <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">FineWeb</a> collection, which
is freely available. </p>

<p>I think I have a good candidate for a next project when I&#39;ve finished the book;
see how many tokens/second I can train on locally -- that will allow me to estimate
how long it would take to train one epoch over the whole training set.  I imagine
that will be longer than I&#39;m willing to leave my desktop machine tied up doing this,
but then I can try mixing in the lessons I learned doing fine-tuning, and see if I can
get it up and running on Lambda Labs.  If the cost is in the tens of dollars, or even a hundred or so, I really
think it would be worthwhile!</p>

<h3 id="memorisation-temperature-and-top-k-sampling">&#34;Memorisation&#34;, temperature and top-k sampling</h3>

<p>One thing I found a little confusing in this chapter -- and this is very much a nit -- was the section on preventing
&#34;memorisation&#34;; I think this was due to a mismatch in the meaning I attach to the word,
and the way it&#39;s used here.</p>

<p>To me, memorisation is something that the model does during training -- if you keep
training a 124M-parameter model on a 20,000-character file, as we&#39;re doing here, then whatever
happens the model is going to memorise it -- it&#39;s unavoidable.  The only way to reduce
memorisation in this sense would be to increase the amount of training data (and even
then, as the findings in the <a href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html">lawsuit</a>
by the New York Times against OpenAI show, some stuff would be memorised).</p>

<p>In the book, &#34;memorisation&#34; is being used to mean something more like what I&#39;d call &#34;parroting&#34; --
issues with the model just repeating the stuff that it has memorised, because it was always
choosing the most-probable next word.  Avoiding this is super-important, of course!  It&#39;s
just the framing that confused me a little.</p>

<p>The techniques are nifty, anyway.  The first cut -- just use the softmaxed logits
as a probability distribution and sample from it -- is obvious enough.  Temperature
is a clever trick on top of that -- just divide the logits by some number greater than
one before softmax, and you can make the distribution that comes out flatter (or you can
make it more &#34;pointy&#34; by dividing by a number less than 1).  The
graphs in the book showing how that works are great, but I asked Claude to knock together a
<a href="https://www.gilesthomas.com/post-assets/llm-from-scratch-22-finally-training-our-llm/temperature-playground.html">temperature playground</a>
website, which I found made things even clearer to me.</p>

<p>And finally, the top-k technique -- only consider the <em>k</em> most probable tokens, and
then do the temperature/softmax calculations -- was a sensible addition to add on top
of that.  The code is clever: identify the top k logits, get the value of the lowest one
of them, and then replace every logit less than that with minus infinity.  When you
run that through softmax, you get zeros for the ones that were replaced, and the probability
distribution is based on the remainder.</p>

<p>So: excellent stuff, and very well explained in the book -- it just didn&#39;t feel like
preventing &#34;memorisation&#34; specifically was what it was doing, at least based on what I
take the word to mean.</p>

<h3 id="downloading-the-openai-weights">Downloading the OpenAI weights</h3>

<p>At the end of the chapter, we download the weights for the original GPT-2 model
that OpenAI produced from their site, and load them into our own model.</p>

<p>The code to download weights is (thankfully) something that you don&#39;t need to type
in, as it&#39;s downloadable from GitHub.  And in one specific related case, I&#39;ll also contradict what I said earlier
about typing stuff in yourself -- I definitely recommend that you copy the
<code>load_weights_into_gpt</code> that copies the downloaded weights into our own model
from GitHub too.  I did actually type it all in and I don&#39;t think I gained anything
from doing that.</p>

<p>One thing I did notice while going through that section was that I&#39;d been making a
mistake as I wrote up this series; I&#39;d thought that all GPT-2 models had 768 embedding
dimensions.  It turns out that this is only true of the 124M model in that series, and
the larger ones have more.  That makes a lot of sense -- and I&#39;ve updated the older
posts to reflect it.</p>

<h3 id="wrapping-up">Wrapping up</h3>

<p>That&#39;s all I really have to add to what is in the rest of chapter 5.  Like I said at
the start, it feels almost like a let-down to be writing so little about a section
of the book that has such amazing results!  But now we have a working LLM, and
at least the foundations that might allow us to train our own from scratch if we had
the resources.</p>

<p>Next up: using it to classify text.  Will this be quick and easy?  Or will it lead down
another fascinating rabbit hole?  Time will tell...</p>



    

    
        
    

    



            
        </div></div>
  </body>
</html>
