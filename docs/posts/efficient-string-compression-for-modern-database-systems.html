<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://cedardb.com/blog/string_compression/">Original</a>
    <h1>Efficient String Compression for Modern Database Systems</h1>
    
    <div id="readability-page-1" class="page"><div><section id="post-hero" data-component="post-hero"><header><p><span data-anim="lines">January 29, 2026</span> <span data-anim="lines">•</span> <span data-anim="lines">17 minutes</span></p><p data-anim="lines" data-anim-delay="0.4">Since version v2026-01-22, CedarDB applies a quite novel compression scheme called FSST to its text columns, allowing to halve their storage size while making queries faster. In this blog post, we cover how we achieved this by sharing implementation details and insights into the trade-offs and implications of integrating FSST.</p></header></section></div><div><section><h2 id="why-you-should-care-about-strings">Why you should care about strings</h2><p>If dealing with strings wasn’t important, why bother thinking about compressing them?
Whether you like it or not, “strings are everywhere”<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. They are by far the most prominent data type in the real world. In fact, roughly 50% of data is <strong>stored</strong> as strings.
This is largely because strings are flexible and convenient: you can store almost anything as a string. As a result, they’re often used even when better alternatives exist.
For example, have you ever found yourself storing enum-like values in a text column? Or, even worse, using a text column for UUIDs? Guilty as charged, I’ve done it too, and it turns out this is actually very common.<sup id="fnref1:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><p>Recently, Snowflake published insights into one of their analytical workloads<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>. They found that string columns where not only the most common data type, but also the most frequently used in filters.</p><p>This means two things: First, it is important to store these strings efficiently, i.e., in a way that does not waste resources, including money.
Second, they must be stored in a way that allows to answer queries efficiently, because that is what users want: Fast responses to their queries.</p><h2 id="why-you-want-to-compress-your-strings">Why you want to compress (your strings)</h2><p>Compression (usually) reduces the size of your data, and therefore the resources your data consumes.
That can mean real money if you’re storing data in cloud object stores where you pay per GB stored, or simply less space used on your local storage device.
However, in database systems, size reduction isn’t the only reason to compress. As Prof. Thomas Neumann always used to say in one of my foundational database courses (paraphrased):
“In database systems, you don’t primarily compress data to reduce size, but to improve query performance.”
As compression also reduces the data’s memory footprint, your data might all of a sudden fit into CPU caches where it previously only fit into RAM, cutting the access time by more than 10x.
And because data must travel through bandwidth-limited physical channels from disk to RAM to the CPU registers, smaller data also means reading more information in the same amount of time,
and thus better bandwidth-utilization.</p><h2 id="string-compression-in-cedardb">String Compression in CedarDB</h2><p>Before diving deeper into FSST, it makes sense to take a brief detour and look at CedarDB’s current compression suite for text columns, as this helps explain some design decisions and better contextualize the impact of FSST.
Until the 22nd January 2026, CedarDB supported following compression schemes for strings:</p><ul><li>Uncompressed</li><li>Single Value</li><li>Dictionary</li></ul><p>The first two of the schemes are special cases, and we choose them either when compression is not worth it, as all the strings are very short (<code>Uncompressed</code>), or if there is a single value in the domain (<code>Single Value</code>).
Since dictionary compression will play an important role later in this post, let’s first take a closer look at how CedarDB builds dictionaries and the optimizations they enable.</p><p>To illustrate string compression in CedarDB, let’s consider the following string data throughout the remainder of this blog post:</p><div><pre tabindex="0"><code data-lang="text"><span><span>https://www.cit.tum.de/
</span></span><span><span>https://cedardb.com/
</span></span><span><span>https://www.wikipedia.org/
</span></span><span><span>https://www.vldb.org/
</span></span><span><span>https://cedardb.com/
</span></span><span><span>…
</span></span></code></pre></div><h3 id="dictionary-compression">Dictionary Compression</h3><p>Dictionary compression is a well-known and widely applied technique.
The basic idea is that you store all the unique input values within a dictionary, and then you compress the input by
substituting the input values with smaller fixed-size integer keys that act as offsets into the dictionary.
Building a CedarDB dictionary on our input data and compressing the data would look like this:</p><p><img src="https://cedardb.com/blog/string_compression/dictionary.svg" alt=""/></p><p>The attentive reader may have noticed two things.
First, we store the offsets to the strings in our dictionary. This is necessary because we want to enable efficient random access to our dictionary.
Efficient random access means that, given a key, we can directly jump to the correct string using the stored offset.
Without storing the offset, we would need to read the dictionary from beginning to end until we find the desired string. Also, since strings have variable sizes, some kind of length information must be stored somewhere.</p><p>Second, our dictionary data is lexicographically ordered.
Performing insertions and deletions in such an ordered dictionary would be quite costly. CedarDB already treats compressed data as immutable<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>, sidestepping this issue.
An ordered dictionary provides interesting properties that will be useful when evaluating queries on the compressed representation.
For example, if <code>str1</code> &lt; <code>str2</code>, then <code>key_str1</code> &lt; <code>key_str2</code> in the compressed representation. You’ll understand later why this comes in handy.</p><p>Additionally, we might be able to further compress the keys. Since the value range of the keys depends on the number of values in the dictionary,
we might be able to represent the keys with one or two bytes instead of four bytes.
This technique, called <strong>truncation</strong>, is part of our compression repertoire for integers and floats.</p><h4 id="evaluating-queries-on-dictionary-compressed-data">Evaluating Queries on Dictionary-Compressed Data</h4><p>No matter the scheme, decompressing data is always more CPU-intensive than not decompressing data.
Thus, we want to keep the data compressed for as long as possible. CedarDB evaluates filters directly on the compressed representation when possible.</p><p>Consider the following query on our input data:</p><p><code>SELECT * FROM hits WHERE url=&#39;https://cedardb.com/&#39;;</code></p><p>Our ordered dictionary comes in handy now. Instead of comparing against every value in the dictionary, we can perform a binary search on the dictionary values to find the key representing our search string “<a href="https://cedardb.com/%22">https://cedardb.com/&#34;</a>.
This is because the order of the keys matches the order of the uncompressed strings in the dictionary.</p><p>If we don’t find a match, we already know that none of the compressed data will equal our search string; therefore, we can stop here.
If we find a match, then we know the compressed representation of the string, i.e., the index in the dictionary where the match was found.
Note that we perform this procedure only once per compressed block, so the operation is amortized across 2¹⁸ tuples (the size of our compressed blocks).</p><p><img src="https://cedardb.com/blog/string_compression/bns.svg" alt=""/></p><p>Now that we have found the key, we can perform cheap integer comparisons on the compressed keys.
These small, fixed-size integer keys allow us to perform comparisons more efficiently since we can leverage modern processor features, such as SIMD (vectorized instructions),
that enable us to perform multiple comparisons with a single instruction. Note that using SIMD for string comparisons is also possible. However, the variable size of strings makes these comparisons more involved and thus less efficient.</p><p><img src="https://cedardb.com/blog/string_compression/dictionary-result.svg" alt=""/></p><h4 id="why-not-stop-here">Why not stop here?</h4><p>Hopefully, all of what I just described sounds interesting, and you also get an intuition for how this can accelerate query processing.
If dictionaries were the ultimate solution for all problems without any drawbacks, thus the best compression scheme for strings, we would stop here.
Unfortunately, dictionaries also have drawbacks.
For one, they only perform well with data that contains few distinct values. After all, they force us to store every single distinct string in full. While real-world data is usually highly repetitive, we can’t rely on that: The number of possible distinct strings is boundless.</p><div><p><img src="https://cedardb.com/blog/string_compression/input-patterns.svg" alt=""/></p></div><p>As you can see, and as the colors indicate, the strings share many common patterns.
From an information theoretical perspective, the strings have fairly low entropy, meaning they are more predictable than completely random strings.
Another compression scheme could exploit this predictability; and this is where FSST comes in!</p><h2 id="fsst">FSST</h2><p>FSST (Fast Static Symbol Table)<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> operates similarly to a <a href="https://en.wikipedia.org/wiki/Large_language_model#Tokenization">tokenizer</a> in that it replaces frequently occurring substrings with short, fixed-size tokens.
In FSST-Lingo, substrings are called “symbols” and can be up to eight bytes long. Tokens are called “codes” and are one byte long, meaning there can be a maximum of 256.
These 1-byte codes are useful because they allow work on byte boundaries during compression and decompression.
Since there can be only 256 different symbols, all the symbols easily fit into the first-level cache of modern processors (L1), allowing for very fast access (~1 ns).
The table that stores the symbols is static, i.e. it won’t change after construction, and FSST’s design goals are to support fast compression and decompression; thus, the name “Fast Static Symbol Table.”
Let’s look at it in a bit more detail.</p><h3 id="fsst-compression">FSST compression</h3><p>FSST compresses in two phases. First, it creates a symbol table using a sample of the input data. Then, it uses the symbol table to tokenize the input.
Working on a sample accelerates the compression process. Intuitively, sampling should work well because symbols that appear frequently in the input data will also appear frequently in a randomly selected sample.
To illustrate the compression process, consider the following sample of our input data:</p><div><pre tabindex="0"><code data-lang="text"><span><span>https://www.wikipedia.org/
</span></span><span><span>https://cedardb.com/
</span></span><span><span>…
</span></span></code></pre></div><p>During construction of the symbol table, the algorithm iteratively modifies an initially empty table.
First, it compresses the sample using the existing table and, while doing that, count the appearances of the table’s symbols and individual bytes in the sample.
To extend existing symbols, it also counts the occurrences of combinations of two symbols. Then, in a second step, it selects the 255 symbols that yield the best compression gain.
The compression gain of a symbol is the number of bytes that would be eliminated from the input by having this symbol in the table.
It is calculated as follows: <code>gain(s1) = frequency(s1) * size(s1)</code>. This process is illustrated below using our sample.</p><p><img src="https://cedardb.com/blog/string_compression/fsst-compression.svg" alt=""/></p><p>If you’ve been paying close attention, you might have noticed that only 255 symbols are picked, even though a single byte can represent 256 values.
The reason for this is that code 255 is reserved for the “escape code”. This code is necessary because the symbol table cannot
(or rather, should not) hold all 256 individual byte values. Otherwise, FSST would not compress at all since one-byte symbols would be substituted by one-byte codes.
Consequently, the input may contain a byte that cannot be represented by one of the codes.
The “escape code” tells the decoder to interpret the next byte literally.</p><p>Now that the Symbol Table is constructed, it can be used to compress the input.
This is done by scanning each string in the input and looking for the longest symbol at the current string offset.
When such a symbol is found, its code is written to the output buffer and the input buffer is advanced by the length of the symbol.
If there is no symbol for the current string offset byte, the escape code and the literal byte are written to the output buffer.</p><div><p><img src="https://cedardb.com/blog/string_compression/fsst-compression2.svg" alt=""/></p></div><h3 id="fsst-decompression">FSST decompression</h3><p>Decompression is straightforward. For each code in the compressed input, the symbol table is consulted to find the corresponding symbol, which is then written to the output buffer. The output buffer is advanced by the length of the symbol.</p><div><p><img src="https://cedardb.com/blog/string_compression/fsst-decompression.svg" alt=""/></p></div><p>Note that the first “c” is different from the second “c.” The first “c” is prepended by the escape code “ESC” and is therefore interpreted literally. The second “c,” however, is a real code that refers to the symbol at index “c” (99 in ASCII) in the symbol table.</p><h3 id="integrating-fsst-into-a-database-system">Integrating FSST into a Database System</h3><p>Until now, we have discussed FSST compression and decompression, but not how to integrate it into a modern database that optimizes for very low query latencies, like CedarDB.
Specifically, we have not discussed what compressed FSST data looks like when written to persistent storage so that it can be decompressed efficiently upon re-reading, so let’s do that now.</p><p>Obviously, you want to serialize the symbol table with your data, i.e., the symbols and their lengths.
Without it, you won’t be able to decode the compressed strings and will lose all your data.
To support efficient random access to individual strings in our compressed corpus, we also store the offsets to the compressed strings.
To decompress the third string in our input, for example, we first look at the third value in the offset array.
Since both the symbol table and the offsets have a fixed size, we always know where the offset is in our compressed corpus.
The offset tells us the location of our string in the compressed strings. Finally, we use the symbol table to decompress the string.</p><p><img src="https://cedardb.com/blog/string_compression/fsst-serialized.svg" alt=""/></p><p>We planned to use the above layout in the beginning. However, it has significant drawbacks when it comes to query processing, i.e., evaluating predicates on the data.
To illustrate this, let’s evaluate our query on the above layout:</p><p><code>SELECT * FROM hits WHERE url=&#39;https://cedardb.com/&#39;;</code></p><p>One naive way to evaluate this query on the data would be to decompress each compressed string and then perform a string comparison. This process is illustrated below:</p><p><img src="https://cedardb.com/blog/string_compression/fsst-old-query-evaluation.svg" alt=""/></p><p>This is quite slow. Alternatively, one could be smarter and use the compressed representation of the search string as soon as the first match is found for further comparisons.
Another option is to directly compress the search string and then compare the compressed strings. Note that this only works for equality comparisons, not for greater than or less than comparisons,
as our symbols are not lexicographically sorted. Even if we sorted them, we’d ultimately end up with string comparisons because FSST-compressed strings are still strings, meaning they’re variable-size byte sequences.
As already mentioned, this makes comparing them more difficult and slower than comparing small fixed-size integers, for example.
Wait, does comparing small fixed-size integers ring a bell? This is exactly what dictionaries allowed us to do before! So is it possible to combine the advantages of a dictionary and FSST? Yes, it is!</p><h4 id="combining-fsst-with-a-dictionary">Combining FSST with a Dictionary</h4><p>The key idea is to create a dictionary from the input data and then use FSST to compress only the dictionary.
This allows for efficient filtering of the compressed representation (i.e., the dictionary keys), as illustrated previously,
while achieving better compression ratios than regular dictionary compression, as FSST allows us to leverage common patterns in the data for compression.
Combining FSST and dictionary compression would look like this:</p><p><img src="https://cedardb.com/blog/string_compression/dictfsst.svg" alt=""/></p><p>Evaluating predicates on this data works the same way as it does for dictionary compression. Decompression works the same way as it does for FSST.
However, accessing the strings involves an additional level of indirection due to the dictionary keys.</p><p>Note that combining FSST with a dictionary is nothing new, DuckDB integrated <code>DICT_FSST</code> half a year ago<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>, which is a very similar approach.</p><h4 id="deciding-when-to-choose-fsst">Deciding when to choose FSST</h4><p>We’re almost there, but not quite yet. The open question is still: How do we decide when to apply FSST to a text column?
Before CedarDB supported FSST, it chose a scheme by compressing the input and selecting the one scheme that produced the smallest size (though there are some edge cases).
However, applying FSST just to save one byte is not worth it since decompressing a FSST-compressed string is much more expensive than decompressing a dictionary-compressed string.
Thus, we introduced a penalty, <strong>X</strong>, such that the FSST-compressed data must be <strong>X</strong>% smaller than the second-smallest scheme to be chosen.</p><p>We evaluated multiple penalty values by contrasting storage size and query runtime on some popular benchmarks (see the Benchmarks section). In the end, we chose a penalty of 40%.
A more detailed discussion of the trade-off will follow in the next section.</p><h2 id="benchmarks">Benchmarks</h2><p>As CedarDB is rooted in research, let’s stop the talking and look at some data to empirically validate my claims by examining benchmark results.
We’ll take a look at two popular analytical benchmarks, <em>ClickBench</em> and <em>TPC-H</em>. Why these two? TPC-H is an industry standard, but its data is artificially generated, while ClickBench is based on real-world data.</p><h3 id="storage-size">Storage Size</h3><p>As one of the reasons for compressing data is reducing its storage size, let’s have a look at the impact of activating FSST in CedarDB on the data size:</p><div><p><img src="https://cedardb.com/blog/string_compression/compression_comparison.svg" alt=""/></p></div><p>Enabling FSST reduces the storage size of <em>ClickBench</em> by roughly 6GB, corresponding to a <strong>20% reduction</strong> in total data size and a <strong>35% reduction</strong> in string data size.
For <em>TPC-H</em>, the effect is even more pronounced: total storage size is reduced by <strong>over 40%</strong>, while string data size shrinks <strong>by almost 60%</strong>. This is likely because the data is artificially generated and therefore contains patterns that are more easily captured by the symbol table.</p><h3 id="query-runtime">Query Runtime</h3><p>As previously mentioned, we also aim to compress data to improve query performance.
To measure this, we ran all queries from <em>ClickBench</em> and <em>TPC-H</em> on CedarDB with FSST active, and compared them to runs without FSST, normalizing the results by the runtime without FSST.
To provide a clearer picture, we differentiate between <strong>cold runs</strong>, where the data is stored on disk and the query is executed for the first time, and <strong>hot runs</strong>, where the data is already cached in memory.</p><h4 id="cold-runs">Cold Runs</h4><p><img src="https://cedardb.com/blog/string_compression/cold_runs_cb.svg" alt=""/>
<img src="https://cedardb.com/blog/string_compression/cold_runs_tpch.svg" alt=""/></p><p>As shown, activating FSST has a positive effect on cold runs for both ClickBench and TPC-H.
For <em>ClickBench</em>, query runtimes improve by up to 40% for queries that operate mainly on FSST-compressed data, such as queries 21, 22, 23, 24, and 28.
On my machine, this 40% reduction corresponds to an absolute runtime decrease of over a second, which is a substantial impact.
For <em>TPC-H</em>, the effect is less pronounced, likely because the queries are more complex, i.e. there is a lot of other stuff to be done, and thus loading data from disk is less of a bottleneck compared to the mostly simple queries in ClickBench.
Nevertheless, we still observe a speedup of up to 10% for query 13.
Moreover, not only is the impact on individual TPC-H queries smaller, but activating FSST also affects fewer queries. This is because fewer filters are applied to FSST-compressed string columns compared to <em>ClickBench</em>.</p><h4 id="hot-runs">Hot Runs</h4><p>Looking at the hots runs, the effect is quite the opposite.</p><p><img src="https://cedardb.com/blog/string_compression/hot_runs_cb.svg" alt=""/>
<img src="https://cedardb.com/blog/string_compression/hot_runs_tpch.svg" alt=""/></p><p>For the <em>ClickBench</em> queries 21,22,23,24 and 28, which showed the largest runtime improvements in the <strong>cold runs</strong>, the runtime for hot runs is higher by up to 2.8x. Since the data is already cached in memory, loading the data from disk is no longer the
bottleneck; instead, decompressing the FSST-compressed strings becomes the limiting factor. This is because all these queries need to decompress most of the strings in the text columns to evaluate the <code>LIKE</code> (or <code>length</code> in the case of Q28) predicates. And since decompressing FSST is more expensive
than simple dictionary lookups, this results in slower execution. Note that this is really the worst-case scenario for compression in database systems: very simple queries that require decompressing nearly all the data to produce a result.
As they say, there’s no free lunch, you can’t beat physics.</p><p>Queries that can operate on FSST-compressed columns without full decompression, such as query 31, continue to benefit in the hot runs, in this case achieving a notable <strong>25% speedup</strong>.
Once again, the effect is less pronounced for TPC-H. For query 13, the runtime is <strong>2.5×</strong> higher because a <code>LIKE</code> predicate is applied on almost all values of an FSST-compressed column.
Note that, although the relative runtime difference is higher than in the <strong>cold runs</strong>, the absolute difference is an order of magnitude smaller. For example, for query 22, being 2.8x slower corresponds to only about 100ms.
This is because reading data from memory is much faster than reading it from disk.</p><p>As another idea for those wanting to integrate this into their system, one way to improve the performance of simple, decompression-heavy queries might be to cache decompressed strings in the buffer manager.
This way, subsequent queries touching the same data can read the decompressed strings directly without repeated decompression.
However, decompressed strings take up more space than compressed strings, so there is less space in the buffer manager for other things that could help answer queries efficiently, ultimately making it a trade-off again.
I don’t have any numbers on that yet because I haven’t had the chance to implement this. Let me know if you do.</p><p>As you might have noticed, compressing data is always a trade-off between storage size and decompression speed. After careful consideration, we decided to activate FSST to reduce CedarDB’s storage footprint and improve loading times.
While some queries may not benefit, or even slightly suffer from FSST, working on better-compressed strings improves overall resource usage and query performance, making it a net win.
If you’ve read this far, I hope I’ve given you some insight into string compression schemes and FSST, as well as what it takes to integrate such a scheme into a database system, along with the potential implications.
If you have any questions about the topics covered in this post, feel free to reach out at <a href="mailto:contact@cedardb.com">contact@cedardb.com</a> or contact me directly at <a href="mailto:nicolas@cedardb.com">nicolas@cedardb.com</a>.
To see how well CedarDB compresses your own data and how this affects query performance, download the community version from our <a href="https://cedardb.com/">website</a> and examine the compression schemes applied to your data using the <a href="https://cedardb.com/docs/compatibility/system_table/#system-tables">system table</a> <code>cedardb_compression_infos</code>.</p><h2 id="sources">Sources</h2></section></div></div>
  </body>
</html>
