<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.pedronasc.com/articles/lessons-building-ai-data-analyst">Original</a>
    <h1>Lessons from building an AI data analyst</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><img alt="Malloy" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" srcset="/_next/image?url=%2Fimages%2Fmalloy.png&amp;w=16&amp;q=75 1x, /_next/image?url=%2Fimages%2Fmalloy.png&amp;w=32&amp;q=75 2x" src="https://www.pedronasc.com/_next/image?url=%2Fimages%2Fmalloy.png&amp;w=32&amp;q=75"/>Malloy</p></div><h2>TL;DR</h2><ul><li><strong>Text-to-SQL is not enough.</strong> Answering real user questions requires going the extra mile like multi-step plans, external tools (coding) and external context.</li><li><strong>Context is the product.</strong> A semantic layer (we use <a href="https://www.malloydata.dev/" target="_blank" rel="noopener noreferrer">Malloy ⎋</a>) encodes business meaning and sharply reduces SQL complexity.</li><li><strong>Use a multi-agent, research-oriented system.</strong> Break problems down using context / domain knowledge, retrieve precisely, write code, interact with the environment and learn from it.</li><li><strong>Retrieval is a recommendation problem.</strong> Mix keyword, embeddings, and a fine-tuned reranker; optimise for precision, recall, and latency.</li><li><strong>Benchmarks ≠ production.</strong> Users expect human-level answers, drill-downs, and defensible reasoning, not just pass@k.</li><li><strong>Latency and quality are a tight bar.</strong> Route between fast and reasoning models; cache aggressively; keep contexts short. Continuous model evaluation is needed to avoid drifts as new models are launched.</li></ul><h2>The short story</h2><p>I spent years on ML for Analytics and Knowledge Discovery at Google and Twitter. For the past 3 years I&#39;ve been building an AI data analyst at Findly (<a href="https://findly.ai" target="_blank" rel="noopener noreferrer">findly.ai ⎋</a>). We entered Y Combinator with a different idea, but quickly realised the real problem for most teams wasn&#39;t &#34;lack of data&#34; — it was <strong>data discovery and use</strong>.</p><p>We started the company as <em>Conversion Pattern</em>, tackling post-iOS 14 attribution and the privacy-driven collapse of cookie-based measurement. What we kept seeing: our customers <strong>already had most of the data they needed</strong>. They either didn&#39;t know it existed or couldn&#39;t stitch it together to answer business questions. The job wasn&#39;t to generate new data; it was to <em>unlock</em> the value of existing data.</p><p>We started with a <em>toy</em> problem — <code>text-to-SQL</code> — and then let users pull us forward. The product evolved into a generative BI platform: it generates SQL, draws charts, writes Python for complex calculations, grounds itself in enterprise context, and pulls in external sources (web, PDFs) when the data story demands it.</p><h2>Why <code>text-to-SQL</code> isn&#39;t enough</h2><p>Real questions rarely map to a single query:</p><ul><li>&#34;Give me a study on the crude oil market.&#34;</li><li>&#34;Create a trading strategy….&#34;</li><li>&#34;Compare these cohorts over the last four releases and explain the variance.&#34;</li></ul><p>You can sometimes force these into one monstrous SQL statement, but it&#39;s brittle and hard for current models. In practice, the system should run a multi-step workflow:</p><div><ol><li><p><span>01</span></p><div><p>Plan the analysis by breaking down the problem, defining the required tools/capabilities.</p></div></li><li><p><span>02</span></p><div><p>Issue targeted SQL queries.</p></div></li><li><p><span>03</span></p><div><p>Join/transform in Python (safer merges, custom calcs, charting).</p></div></li><li><p><span>04</span></p><div><p>Validate assumptions with checks &amp; sanity tests.</p></div></li><li><p><span>05</span></p><div><p>Visualise and explain the result.</p></div></li><li><p><span>06</span></p><div><p>Offer drill-downs and next questions.</p></div></li></ol></div><div><svg viewBox="0 0 16 16" aria-hidden="true"><circle cx="8" cy="8" r="8" stroke-width="0"></circle><path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M6.75 7.75h1.5v3.5"></path><circle cx="8" cy="4" r=".5" fill="none"></circle></svg><p><strong>Bottom line:</strong> Text-to-SQL is a capability. The product is end-to-end analysis that stands up to scrutiny.</p></div><h2>Context Engineering &amp; Semantic Metadata</h2><p>When building AI-powered data tools, context and metadata can mean the difference between the right and wrong answer. We invest heavily in a semantic layer for our data because it brings several critical benefits:</p><ul><li><strong>Encodes business meaning:</strong> All the important context – <em>dimensions</em>, <em>measures</em>, <em>relationships</em>, and <em>constraints</em> – lives in a maintained semantic model instead of being buried inside prompts. Business logic (like how &#34;revenue&#34; is calculated or what qualifies as a &#34;customer&#34;) is explicitly defined in one place and can be reused everywhere, rather than re-explained in every prompt. This also allows faster prototyping and testing.</li><li><strong>Shrinks the search space:</strong> By providing structured context, our LLM-based planner avoids guesswork with ambiguous table or column names. The model knows exactly which fields are relevant and won&#39;t wander off into nonexistent or irrelevant data. This drastically improves the reliability of generated SQL, because the AI isn&#39;t brainstorming schema details – it&#39;s selecting from a known set.</li><li><strong>Enables compile-time checks:</strong> Because the LLM works against a defined schema and semantic model, we can validate its output before execution. If it tries to use a field that doesn’t exist or apply a metric incorrectly, the semantic layer’s compiler catches it early. This leads to fewer silent failures and much more predictable behavior when the SQL or code runs, allowing us to self-correct intermediary steps along the process.</li></ul><h3>Our Choice: Malloy for Semantic Modeling</h3><p>To implement this semantic layer, we chose <a href="https://www.malloydata.dev/" target="_blank" rel="noopener noreferrer">Malloy ⎋</a>, an open-source semantic modeling language. Malloy lets us model our data relationships as a graph of sources (tables) and joins, then define <strong>metrics (measures) and dimensions</strong> in that graph. We express complex queries at the semantic level, and Malloy&#39;s compiler translates them into optimized SQL with strong guarantees of correctness – essentially acting as our &#34;knowledge graph plus compiler&#34;. In other words, Malloy serves as a single source of truth for business logic that ensures consistent, accurate SQL generation across the board.</p><div><svg viewBox="0 0 16 16" aria-hidden="true"><circle cx="8" cy="8" r="8" stroke-width="0"></circle><path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M6.75 7.75h1.5v3.5"></path><circle cx="8" cy="4" r=".5" fill="none"></circle></svg><p>If your organization has already invested in a different semantic layer, the same principles apply. Snowflake&#39;s recently introduced <a href="https://www.snowflake.com/en/engineering-blog/native-semantic-views-ai-bi/" target="_blank" rel="noopener noreferrer">Native Semantic Views ⎋</a> are one attempt to bake a semantic layer directly into their platform, and Google&#39;s Looker provides a semantic modeling solution on top of BigQuery. We&#39;ll explore these alternatives for the AI era in a future post. Regardless of the tool, the key is to make your business logic explicit and shareable.</p></div><p>Another advantage of Malloy is the ability to attach rich metadata and documentation directly to the model. We annotate each <em>measure</em> and <em>dimension</em> with human-readable descriptions and tags (units, currencies, etc.) right alongside its definition. For example, we might tag a metric with its unit and add a description:</p><p>In the snippet above, the <code>total_revenue</code> measure is clearly defined as the sum of <code>price</code> × <code>quantity</code>, and it&#39;s annotated with a description as well as a currency tag. <a href="https://docs.malloydata.dev/blog/2025-06-16-annotations-and-tags/" target="_blank" rel="noopener noreferrer">Malloy&#39;s flexible annotation system</a> allows us to store arbitrary metadata like this (in this case, noting that <code>total_revenue</code> is in USD). These descriptions and facts are not just for show – they are programmatically accessible. Our application can retrieve them and pass them into the LLM&#39;s context, so the model knows, for instance, that &#34;<code>total_revenue</code>&#34; means &#34;sum of <code>price</code>×<code>quantity</code> in USD&#34; without having to infer it purely from the name. We can also have operations like casting or even case statements to create maps that are directly related to the business logic.</p><h3>Integrating the Semantic Layer with LLMs (Functions &amp; RAG)</h3><p>How do we actually feed this context to the LLM? We employ a combination of <strong>retrieval-augmented generation (RAG)</strong> and the LLM&#39;s <strong>function-calling capabilities</strong> to integrate Malloy&#39;s semantic layer into our AI workflow. Instead of dumping the entire data schema into every prompt, we maintain a lightweight knowledge base of the semantic model. When a user asks a question, we first retrieve the relevant model fragments (e.g. the definitions of any <em>measures</em> or <em>dimensions</em> that the question mentions) and include only those in the prompt. This keeps prompts concise and focused. The LLM sees only the pertinent pieces of context, which dramatically narrows its search space to the correct solution.</p><p>Furthermore, we define a set of <strong>tools</strong> that the LLM can invoke as needed. Using <code>function-calling API</code> the model can ask for more info or actions. For example, if it needs additional detail about a field, it can call something like <code>get_definition(&#34;trading_day_window&#34;)</code> and our system will return the stored description/metadata for that term. Or the LLM might decide to call <code>run_query(model, params)</code> to execute a Malloy-defined query plan and retrieve some data. This way, the LLM doesn&#39;t have to guess or hallucinate schema details – it can query the semantic layer directly for clarification. After gathering the needed context via these function calls, we can generate the final code (SQL or Python) with much higher confidence.</p><p>It&#39;s worth noting that this semantic context benefits Python code generation as much as SQL. Because our model includes things like <em>unit conversions</em> and <em>custom calendar logic</em>, the LLM can produce Python code that&#39;s aware of those definitions. For instance, if certain measures are tagged as currency in USD or a <code>trading_day</code> dimension delineates business days vs. weekends, the assistant can incorporate that knowledge (maybe by calling a <code>convert_currency()</code> helper or using a pre-defined <code>trading-days</code> list) in the Python code it writes. By making the model more focused with a well-defined semantic layer, we get code that is not only plausible but also <strong>correct and aligned with our business rules</strong>.</p><h3>Example: Malloy Semantic Layer in Action</h3><p>Let’s tie it all together with a concrete example using Malloy. Suppose we have an e-commerce dataset with orders and customers. We define a semantic model as follows:</p><p>Here&#39;s a more complete example of our Malloy semantic model:</p><p>Here we&#39;ve explicitly modeled the relationships (linking orders to customers) and defined a metric <code>total_revenue</code> with a clear meaning and unit. Now imagine a user asks: &#34;What was our total revenue by region last quarter?&#34; Our system will:</p><div><ol><li><p><span>01</span></p><div><p>Retrieve context</p><p>Recognize that the question involves the total_revenue measure and the region dimension. It pulls their definitions from the Malloy model (including the knowledge that total_revenue is price × quantity in USD, and that region comes from the customers table related to orders).</p></div></li><li><p><span>02</span></p><div><p>Provide context to the LLM</p><p>Construct a prompt that includes the user&#39;s question along with the retrieved semantic definitions. This might look like a short snippet of Malloy model info or a brief text explanation for each relevant field, injected before asking the LLM to formulate an answer.</p></div></li><li><p><span>03</span></p><div><p>Generate code via function call</p><p>The LLM analyzes the question with the given context and decides on a plan. It might output a structured function call such as generate_sql(query_params…) rather than a raw answer. For example, it could produce a call like generate_sql(model=&#34;orders&#34;, measure=&#34;total_revenue&#34;, dimension=&#34;customers.region&#34;, filter=&#34;order_date in last_quarter&#34;). This is the signal to take over and produce the actual query.</p></div></li><li><p><span>04</span></p><div><p>Compile and validate</p><p>Our backend function receives that structured request and uses Malloy to compile the corresponding query. Malloy knows about the orders→customers join and the definitions of each field, so it can generate the correct SQL. If the LLM&#39;s request referenced something incorrectly (say an undefined field), Malloy would throw an error here – catching the issue before execution.</p></div></li><li><p><span>05</span></p><div><p>Execute or return code</p><p>Once the query is successfully compiled, we execute it on the database. In our example, Malloy would produce a SQL that joins the orders and customers tables, filters to last quarter&#39;s dates, groups by region, and sums the total_revenue. The end result might be a neat table of regions with their respective revenue, or the SQL code for it – either way, it&#39;s guaranteed to be using the right tables, joins, and formulas.</p></div></li></ol></div><div><svg viewBox="0 0 16 16" aria-hidden="true"><circle cx="8" cy="8" r="8" stroke-width="0"></circle><path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M6.75 7.75h1.5v3.5"></path><circle cx="8" cy="4" r=".5" fill="none"></circle></svg><p>Note: This is a very simple example, the schema in practice for enterprises is much more complex. It usually even requires multiple calls to the system in order to get disjoint tables etc.</p></div><p>The heavy lifting of &#34;knowing the data&#34; is handled by the semantic layer, and not left to the LLM. By making the business logic explicit and shareable, we ensure that both AI and humans are always speaking the same language – and that language is formally defined (in Malloy, in our case). The outcome is answers and code that are not just plausible, but <strong>correct, maintainable, and aligned with the business&#39;s reality</strong>.</p><h2>Python code generation (and why it matters)</h2><p>A lot of business analysis is post-SQL computation: statistical tests, time-series transforms, strategy backtests, data quality checks. We run these in a sandboxed Python environment with pre-installed libraries tuned to the customer&#39;s domain. Two big benefits:</p><ul><li><strong>Fewer tokens, more leverage.</strong> Libraries abstract tools and capabilities, allowing the model to just recall them instead of creating them from scratch.</li><li><strong>Better generalisation.</strong> The model composes short, readable Python blocks instead of over‑fitting giant prompts. Pre-built, well-tested functions encode the general solution and its edge cases (missing timestamps, time zones, irregular sampling, NaNs, etc.). The model only has to compose these building blocks, so the resulting code is shorter, clearer, and behaves correctly across many datasets—i.e., it generalizes.</li></ul><p>A simple pattern that works well:</p><p>Store facts with reasoning traces and explanations. When generating code, retrieve the relevant traces and let the model adapt them.</p><p>Treat these snippets as natural‑language programs: general, succinct, and reusable. Reasoning models are good at recombining past programs (in the form of CoTs) — it’s how they’re trained. Storing traces in your business context “reminds” the model of the right approach and narrows the search space. <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/" target="_blank" rel="noopener noreferrer">AlphaEvolve ⎋</a> and <a href="https://arxiv.org/abs/2507.15855" target="_blank" rel="noopener noreferrer">Gemini 2.5 Pro Capable of Winning Gold at IMO 2025 ⎋</a> are good examples on how &#34;hints&#34; can significantly increase the results from the models.</p><div><svg viewBox="0 0 16 16" aria-hidden="true"><circle cx="8" cy="8" r="8" stroke-width="0"></circle><path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M6.75 7.75h1.5v3.5"></path><circle cx="8" cy="4" r=".5" fill="none"></circle></svg><p>Also: treat prompts and reasoning traces as <strong>company assets</strong>. Store them, version them, test them.</p></div><h2>Multi-agent planning, memory, and grounding</h2><p>Complex requests benefit from <strong>decomposition</strong>. Our architecture uses cooperating agents that:</p><div><ol><li><p><span>01</span></p><div><p>Plan the analysis</p><p>Decompose tasks, choose tools, define checks.</p></div></li><li><p><span>02</span></p><div><p>Retrieve precisely</p><p>See next section, iterating when gaps are detected.</p></div></li><li><p><span>03</span></p><div><p>Generate SQL/Python</p><p>Run it in sandboxes.</p></div></li><li><p><span>04</span></p><div><p>Validate</p><p>With unit checks / sanity tests.</p></div></li><li><p><span>05</span></p><div><p>Explain results</p><p>And propose next questions.</p></div></li></ol></div><p>This reduces hallucinations and ambiguity, sharpens accountability by having more self-contained problems, and makes debugging possible. Memory (short- and long-term) keeps the system grounded in prior decisions and user preferences.</p><h2>Retrieval systems: treat RAG like recommendations</h2><p><img src="https://www.pedronasc.com/images/multi-stage-diagram.jpg" alt="Multi-agent planning"/></p><p><em>Figure 1. Multi-stage retrieval as a recommendation pipeline: cheap candidate generation → instruction-tuned reranking → minimal, high-quality context. Click the image to zoom.</em></p><p>LLM speed suffers as context grows (transformer attention is ~quadratic in input length), so good retrieval is non-negotiable. The shorter and well curated the data you put in the LLM is, the better and faster the results will be. Think of it as a recommendation pipeline:</p><ul><li><strong>Candidate generation:</strong> keyword search for internal acronyms and exact terms; embeddings for semantic matches. People should look more at their &#34;<code>RAG</code>&#34; systems as a full recommendation system. You can always improve the latency, precision and recall of the system.</li><li><strong>Reranking:</strong> a fine-tuned instruction-following reranker optimises for the current question style. (Off-the-shelf rerankers underperform without this.) Fine-tuning the reranker is important, otherwise it won&#39;t perform as well as needed. You can see a lot of companies have been releasing better instruction following reranker models — this is quite important as the LLMs will be doing the queries.</li><li><strong>Multi-stage ranking:</strong> keep the early stages cheap; spend budget late where it matters. Aim for both precision (fewer irrelevant docs) and recall (don&#39;t miss the key one). This helps to keep latency and cost in check.</li><li><strong>Query rewriting:</strong> LLMs can write long, precise queries — use that to drive search, not just retrieval.</li><li><strong>Chunking and keys:</strong> design retrieval keys to match how analysts think (e.g., metric → dimension → time), not how files are stored.</li></ul><p>Current search and recommendation systems are heavily optimized for humans: LLMs search is different from human search. LLMs are able to write complex, precise and verbose queries. This information needs to be used to make the search systems as precise as possible as part of a multi-agent framework. This also helps to reduce the information needed to be passed as context to the LLM.</p><div><svg viewBox="0 0 16 16" aria-hidden="true"><circle cx="8" cy="8" r="8" stroke-width="0"></circle><path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M6.75 7.75h1.5v3.5"></path><circle cx="8" cy="4" r=".5" fill="none"></circle></svg><p>Note: the specific <code>top@k</code> thresholds and models used in an AI data analytics system really depends on the specific tasks being solved. Some problems require bigger models specially at the later stages, other ones not that much. Some of them might even require LLMs using a <code>map-reduce</code> / <code>divide-and-conquer</code> approach to get the right accuracy.</p></div><p>Some of the companies that provide good reranking models:</p><ul><li>Open source: <code>Qwen3</code> reranker - <a href="https://qwenlm.github.io/blog/qwen3-embedding/" target="_blank" rel="noopener noreferrer">https://qwenlm.github.io/blog/qwen3-embedding/</a></li><li><code>Voyage AI</code> - <a href="https://blog.voyageai.com/2025/08/11/rerank-2-5/" target="_blank" rel="noopener noreferrer">https://blog.voyageai.com/2025/08/11/rerank-2-5/</a></li><li><code>Contextual AI</code> - <a href="https://contextual.ai/blog/introducing-instruction-following-reranker/" target="_blank" rel="noopener noreferrer">https://contextual.ai/blog/introducing-instruction-following-reranker/</a></li><li><code>Cohere</code> - <a href="https://cohere.com/blog/rerank-3pt5" target="_blank" rel="noopener noreferrer">https://cohere.com/blog/rerank-3pt5</a></li></ul><p>The latest <code>Voyage</code>, <code>Contextual</code> and <code>Qwen3</code> models all emphasize the instruction following capabilities, showing the need for it on multi-agent systems.</p><p>The picture below shows the improvements given by the instructions following models from <code>Voyage AI</code>.</p><p><img src="https://www.pedronasc.com/images/reranker-comparison.jpg" alt="Reranker comparison"/></p><p><em>Figure 2. Accuracy gains from instruction-following: +8.13% (rerank-2.5) and +7.55% (rerank-2.5-lite) across 24 datasets in 7 domains.</em></p><p>I am excited about the usage of <a href="https://groq.com/" target="_blank" rel="noopener noreferrer">specialized hardware ⎋</a> and diffusion LLM language models (e.g., the ones by <a href="https://inceptionlabs.ai/" target="_blank" rel="noopener noreferrer">Inception Labs ⎋</a>) for rerankers. While most people focus a lot on the top-tier LLMs, having strong and extremely low-latency rerankers might be a much better performance improvement for AI systems than the top model itself.</p><h2>Different LLM choices</h2><p>Reasoning-style models are already excellent for <code>text-to-SQL</code>. They handle ambiguous or very hard questions well, and outright hallucinations are now uncommon. The trade-off is latency (and often cost), so you can&#39;t run them end-to-end across every step of a real-time pipeline.</p><p><strong>Key takeaways:</strong></p><ul><li><strong>Hallucinations aren&#39;t the main risk anymore.</strong> Modern reasoning models rarely fabricate facts outright.</li><li><strong>Context is the real failure mode.</strong> Missing schema details, vague user intent, or unclear join paths lead to wrong queries.</li><li><strong>Context engineering matters most.</strong> Invest in precise retrieval, schema selection, examples/constraints, and clear problem framing.</li></ul><p><strong>Recommendation.</strong> If you&#39;re building an AI data-analyst workflow, use top-tier reasoning models for the SQL generation + schema reasoning step (e.g., <code>Gemini 2.5 Pro</code>, <code>o4-mini</code>, <code>Claude 4 Sonnet</code>). <code>O3</code> and <code>Claude 4 Opus</code> are extremely strong, but their latency and cost typically make them impractical for interactive production use.</p><p>A practical pattern is a hybrid setup: route easy or routine requests to a faster model, and automatically escalate the hard/ambiguous ones to a reasoning model. This preserves quality where it matters without blowing up response times.</p><h2>Common failure modes (and fixes)</h2><ul><li><strong>Ambiguous tables/joins</strong> — push grain/joins into the semantic layer; add compile-time checks.</li><li><strong>Over-long contexts</strong> — narrow retrieval keys; teach query rewriting; cache partial results.</li><li><strong>Quiet wrong answers</strong> — add validators and reconciliation tests; require citations.</li><li><strong>Latency spikes</strong> — stage reranking; cap tokens; route early to fast paths.</li><li><strong>Brittle prompts</strong> — store &amp; version traces; test against real user questions.</li></ul><h2>What is next</h2><ul><li><strong>Adaptive models</strong> that switch between fast and reasoning modes and know how much to think. This will lead to faster models on par with human expectations on how long a task should take given the difficulty.</li><li><strong>More agentic systems</strong> that explore alternative plans, fill knowledge gaps, and critique their own outputs.</li><li><strong>Automated knowledge extraction</strong> that continuously harvests and organises metadata and business logic. With curated knowledge, today&#39;s multi-agent systems can already tackle surprisingly complex tasks.</li></ul><p>In the next posts I will dig more on the specifics about semantic layer choices, what is missing to make enterprise program synthesis better, etc.</p><div><div><form><p>Was this article helpful?</p></form></div></div></div></div>
  </body>
</html>
