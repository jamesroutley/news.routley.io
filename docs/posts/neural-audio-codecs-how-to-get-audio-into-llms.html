<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://kyutai.org/next/codec-explainer">Original</a>
    <h1>Neural audio codecs: how to get audio into LLMs</h1>
    
    <div id="readability-page-1" class="page"><div><div><div>
<div><p><em>Václav Volhejn</em></p><p><em>Thank you for the valuable feedback on the drafts: Chung-Ming Chien, Moritz Boehle, Richard Hladík, Eugene Kharitonov, Patrick Perez, and Tom Sláma.</em>
<em>I’d also like to thank the rest of the Kyutai team for the the research discussions without which this article could not exist.</em></p></div>
<figure><div><div><video src="assets/codec-explainer/codecIntro.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption><p>The plan: sandwich a language model in an audio encoder/decoder pair (=neural
audio codec), allowing it to predict audio continuations.</p></figcaption></figure>
<p>As of October 2025, speech LLMs suck. Many LLMs have voice interfaces, but they usually work by transcribing your speech, generating the answer in text, and using text-to-speech to read the response out loud. That’s perfectly fine in many cases (see <a href="https://unmute.sh/" target="_blank" rel="noopener noreferrer">Unmute</a>), but it’s a wrapper, not <em>real</em> speech understanding. The model can’t hear the frustration in your voice and respond with empathy, it can’t emphasize important words in its answer, it cannot sense sarcasm, and so on.</p>
<p>Yes, there <em>are</em> LLMs (<a href="https://blog.google/technology/google-deepmind/gemini-2-5-native-audio/" target="_blank" rel="noopener noreferrer">Gemini</a>, <a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer">ChatGPT</a>’s Advanced Voice Mode, <a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen</a>, <a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a>) that understand and generate speech natively. But in practice, they’re either not as smart, or they behave like text model wrappers. Try asking any of them “Am I speaking in a low voice or a high voice?” in a high-pitched voice, and they won’t be able to tell you.</p>
<p>Clearly, speech LLMs lag behind text LLMs. But why? For text, we found out a few years ago that if you take a lot of text data, a big Transformer, and a lot of GPUs, you’ll get some pretty damn good text continuation models. Why can’t we just replace text with audio and get pretty damn good speech continuation models?</p>
<p>As a teaser, here’s what happens when you try to do that naively (warning, loud):</p>

<p>We’ll have a look at why audio is harder to model than text and how we can make it easier with <em>neural audio codecs</em>, the de-facto standard way of getting audio into and out of LLMs. With a codec, we can turn audio into larger discrete <em>tokens</em>, train models to predict continuations for these tokens, and then decode those back into audio: see animation above.</p>
<p>Kyutai folks have done a lot of work in this space, which is part of the reason I chose to cover this topic. We’ll start from the basics and build up all the way to <a href="https://huggingface.co/kyutai/mimi" target="_blank" rel="noopener noreferrer">Mimi</a>, our neural audio codec. It was originally developed for <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a> and later adopted by others for their models, notably <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">Sesame’s CSM</a>.</p>
<h2 id="text-is-easy">Text is easy</h2>
<p>To <a href="https://platform.openai.com/docs/concepts/tokens#tokens" target="_blank" rel="noopener noreferrer">tokenize</a> text, everybody uses a technique called byte-pair encoding and rarely changes the tokenizer: OpenAI has been using <a href="https://github.com/openai/tiktoken/blob/2ab6d3706d557b560b200be48e6a32324682c9a3/tiktoken/model.py#L8-L16C17" target="_blank" rel="noopener noreferrer">the same tokenizer</a> since GPT-4o, an ancient model if you count in LLM years.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image.png" alt=""/></p><figcaption><p>A random text from Wikipedia tokenized via the GPT-4o tokenizer</p></figcaption></figure>
<p>You can even get decent results without tokenizing text at all, just predicting individual
characters. One of the first posts that got me excited about machine learning was
Andrej Karpathy’s <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener noreferrer">RNN effectiveness</a>
blog post from 2015. Karpathy trains a three-layer LSTM on a single GPU and gets
it to generate decent-looking code and LaTeX:</p>
<p><img src="https://kyutai.org/next/assets/codec-explainer/rnns-code.png" alt=""/><img src="https://kyutai.org/next/assets/codec-explainer/rnns-latex.png" alt=""/></p>
<p>Remember this was ten years ago, back when we didn’t even know that <a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need" target="_blank" rel="noopener noreferrer">attention is all we need</a>.
Now compare Karpathy’s results to a sample from <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/" target="_blank" rel="noopener noreferrer">WaveNet</a>, a model DeepMind published a year later:</p>

<p>Purely acoustically, the audio sounds good, but it rarely even manages to produce a single correct English word. We can’t be too hard on WaveNet, though. The samples from Karpathy’s RNNs are only a few thousand characters long, but this 10-second audio consists of 160k audio samples, and WaveNet creates it by painstakingly predicting sample-by-sample.</p>
<figure><div><div><video src="assets/codec-explainer/wavenet-audio.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption><p>A single second of audio consists of tens of thousands of samples, although it
corresponds to just a few words. Animation from the <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/" target="_blank" rel="noopener noreferrer">WaveNet blog
post</a>.</p></figcaption></figure>
<p>It’s difficult to build models that are coherent over time scales this long, and the model also takes very long to run for so many steps.</p>
<p>So instead of running the model to predict the samples one-by-one directly, we’d like to train a model to compress the audio into a more manageable size. We could compress the audio, use an LLM to predict a continuation in the compressed representation, and then decompress the result.</p>
<h2 id="sample-by-sample">Sample by sample</h2>
<p>But first, let’s get a baseline model by generating audio sample by sample, like WaveNet does. <strong>The code for all of these experiments is open-source! Check it out <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">here</a>.</strong> I forked Andrej Karpathy’s <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT</a> repo, a simple implementation of GPT-2.</p>
<p>Text and audio are kind of the same from the perspective of the language model: it’s just tokens in, tokens out. The only thing we need to do is to quantize the continuous values of the samples into discrete buckets. Like WaveNet, we’ll use the <a href="https://en.wikipedia.org/wiki/%CE%9C-law_algorithm" target="_blank" rel="noopener noreferrer">&#34;μ-law algorithm&#34;</a> to get 256 buckets. We’ll treat those as 256 possible tokens.</p>
<p>Let’s train a language model on audio tokenized like this. For the dataset, we’ll use the <a href="https://ai.meta.com/tools/libri-light/" target="_blank" rel="noopener noreferrer">Libri-Light</a> dataset, following <a href="https://arxiv.org/abs/2209.03143" target="_blank" rel="noopener noreferrer">AudioLM</a> (with Neil Zeghidour, Eugene Kharitonov). Its train split contains 50k hours in total, but we’ll go with 1000 hours for this experiment. With this sample-by-sample tokenization, we end up with a dataset of 53 GB.</p>
<p>We train a small-ish transformer of 151.28M parameters, about the size of the <a href="https://openai.com/index/better-language-models/" target="_blank" rel="noopener noreferrer">smallest GPT-2 variant</a>. When we sample from the model, it makes babbling sounds (warning, loud at times!):</p>

<p>Often, it goes into a “crackling mode” that it can’t seem to get out of:</p>

<p>I also trained a smaller model, which I teased at the beginning. It’s prone to generate nightmare fuel screeches (loud!):</p>

<p>As you can tell, we’re not AGI yet. It sounds speech-like, but you can’t make out a single word and the voice keeps changing. No wonder: the context size of the model is 2048, which, for 16 kHz audio, translates to 128ms, not even a the length of one word. Also, these 10-second examples took 30 minutes to generate on an H100, so we’re a few orders of magnitude away from being real-time.</p>
<p>So let’s build a neural audio codec to compress the audio. The hope is that if we reduce the sampling rate 100x, the model will also become “100x more coherent”. An old idea in machine learning is to do this using an <em>autoencoder:</em> a model that takes an input, compresses it into a smaller “latent space”, and then tries to reconstruct the original input.</p>
<p>In our case, we’ll want an autoencoder whose latent space is quantized so that we can feed the latents into a language model and produce continuations. (You <em>can</em> generate continuations with unquantized latents, but it’s trickier – see the <a href="#further-reading">Further reading</a> section.)</p>
<h2 id="autoencoders-with-vector-quantization-vq-vae">Autoencoders with vector quantization (VQ-VAE)</h2>
<p>Bear with me, because we’ll take a detour from audio: let’s build a quantized autoencoder on images from <a href="https://arxiv.org/abs/1708.07747" target="_blank" rel="noopener noreferrer">Fashion MNIST</a>. We’ll take a subset with the first three classes: t-shirt/top, trouser, and pullover.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/fashion-mnist-3.png" alt=""/></p><figcaption><p><a href="https://www.researchgate.net/figure/The-FashionMNIST-dataset-consists-of-10-classes-of-monochrome-clothing-items-and-is_fig1_373046669" target="_blank" rel="noopener noreferrer">image
source</a></p></figcaption></figure>
<p>First, let’s train a regular autoencoder to encode the images into two-dimensional space:</p>
<figure><div><div><video src="assets/codec-explainer/vq_images_unquantized_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption><p>Training a regular autoencoder on Fashion MNIST</p></figcaption></figure>
<p>Each frame shows one batch of training, with some batches skipped. The little images are the autoencoder’s reconstructions for the images in the batch. I’ve added colors for the three classes (t-shirt/top=blue trousers=green, pullover=yellow), but the autoencoder doesn’t get a class as input – the space just naturally clusters by class. Let&#39;s zoom in on a few reconstructions:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-without-quantization-v4.png" alt=""/></p><figcaption><p>Original images (top) and their reconstructed versions (bottom)</p></figcaption></figure>
<p>As you can tell, the reconstruction quality is not great. The images are blurry and the first two images are reconstructed to nearly the same thing. But we used a tiny network (4 fully connected layers for the encoder and decoder each) and projected into a mere two dimensions, so we can’t expect too much of our model.</p>
<p>Now let’s quantize these embeddings using a clustering. We’ll do something like <a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="noopener noreferrer">k-means</a>: we’ll maintain a list of the positions of the cluster centers. We initialize the positions randomly. For each training batch, we look at which embeddings would go to each cluster. (We don’t modify the embeddings, we just look at the assignment). Then we’ll nudge each cluster center towards the average position of these embeddings.</p>
<p>Also, if a center is unused for a while, we teleport it to a random embedding from the batch, because otherwise it has no way to get unstuck from its current position.</p>
<figure><div><div><video src="assets/codec-explainer/vq_images_unquantized_with_clustering_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption><p>Quantizing by fitting a clustering on top of the autoencoder</p></figcaption></figure>
<p>You can see the reconstructions of the cluster centers getting refined over time.</p>
<p>Next, we’ll make the encoder and decoder themselves better at handling quantized embeddings during training, because currently, we’re just fitting the clustering on top of an autoencoder that is not “aware” it’s being quantized. We’d like the autoencoder to adapt to the quantization as we train it. Currently, we’re doing this:</p>
<pre><code>x = get_batch()
z = encoder(x)

x_reconstructed = decoder(z)

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>Instead of feeding the unquantized embedding into the decoder, we’ll first move it to the closest cluster:</p>
<pre><code>x = get_batch()
z = encoder(x)

z_quantized = to_nearest_cluster(z)     
x_reconstructed = decoder(z_quantized)  

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>There is a snag: if we do this, we won’t be able to train the autoencoder any more, because the quantization operation is not differentiable, meaning there is no gradient flowing from the loss to the weights of the encoder. Essentially, we’re no longer able to answer the question: “if I want the loss to decrease a bit, in which direction should I nudge the encoder’s weights?”</p>
<p>We’ll fix this problem by pretending it doesn’t exist. Yes, really. We’ll think of <code>z_quantized</code> as <code>z</code> moved by an arbitrary vector that doesn’t affect the gradient. That will make the gradient of <code>z</code> equal to that of <code>z_quantized</code>, which is why this is also known as the <em>straight-through estimator</em> of the gradient.</p>
<pre><code>x = get_batch()
z = encoder(x)

residual = z - to_nearest_cluster(z)

z_quantized = z - residual.detach()
x_reconstructed = decoder(z_quantized)

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>In the forward pass, <code>z_quantized</code> is set to the same value as before, but importantly, the gradient of <code>z</code> is now equal to that of <code>z_quantized</code> rather than just being 0 because of the non-differentiable <code>to_nearest_cluster(z)</code> operation.</p>
<p>There is a price to pay for this lie. When training, the encoder’s weights will be updated to improve the reconstruction loss, but they’re updated as if the quantization didn’t happen, so they won’t move in the optimal direction. But as long as the embeddings stick close to their cluster centers, the gradient direction will still be mostly correct.</p>
<p>We can actually encourage the encoder to make embeddings that are easily quantizable by adding a <em>commitment loss</em>: a penalty for each point based on how far it is from its cluster center. The gradient of this loss will push the points closer to their cluster centers.</p>
<p>By quantizing at training time and adding a commitment loss, it’s no longer just a clustering being fit on top of the embeddings. The model itself is trained to be good for quantization.</p>
<figure><div><div><video src="assets/codec-explainer/vq_images_balanced_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption><p>An autoencoder trained explicitly to be easy to quantize</p></figcaption></figure>
<p>You’ll notice that the training dynamics look different: the commitment loss adds a certain “stiffness” that doesn’t allow the embeddings to move around as easily.</p>
<p>Here’s what the reconstructions look like when we use the quantized representations:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-1-level-v4.png" alt=""/></p><figcaption></figcaption></figure>
<p>Notice how the first two images are reconstructed to <em>exactly</em> the same image. That’s simply because their embeddings got assigned to the same cluster and therefore quantized to the same value.</p>
<p>The model described here is known as a “<a href="https://arxiv.org/abs/1711.00937" target="_blank" rel="noopener noreferrer">VQ-VAE</a>”: a vector-quantized variational autoencoder. The word “variational” here is just a vestigial leftover that doesn’t mean anything anymore.</p>
<h2 id="residual-vector-quantization">Residual vector quantization</h2>
<p>To improve the reconstruction fidelity, we can just increase the number of cluster centers. But keeping track of too many centers can get prohibitively expensive in terms of compute and memory required, so we’ll do a clever trick: if we want 2^20 (~1M) possible values, we won’t create 2^20 clusters directly. Instead, we’ll use two separate quantizers with 2^10=1024 clusters and combine their result. Each embedding will then be quantized to a tuple of two integers in [0..1023], yielding 2^20 possible combinations.</p>
<p>Ok, but how? Well, recall the <code>residual</code> variable we used in the straight-through estimator, defined as <code>z - to_nearest_cluster(z)</code> the shift from the quantized embedding to the unquantized one. It represents the part of the original vector <code>z</code> that we didn’t manage to take into account when quantizing to <code>to_nearest_cluster(z)</code>.</p>
<p>So for each embedding in the batch, we have a corresponding residual vector. The solution is obvious: we’ll quantize these residuals exactly the same way we did with the original embeddings, by training another vector quantizer.</p>
<p>This time, the 2D positions for a single quantizer don’t define images because we need to combine the two quantizers, so we’ll just visualize everything as dots:</p>
<figure><div><div><video src="assets/codec-explainer/rvq_fmnist.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption><p>Two-level quantization by fitting a quantizer on top of the
“residuals”, aka the errors of the first quantizer</p></figcaption></figure>
<p>Each image is then represented as the index of the cluster of the embedding and that of the residual. Let’s try to reconstruct a few images with this two-level quantizer:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-2-level-v4.png" alt=""/></p><figcaption><p>Original images (top), one-level reconstruction (middle), two-level
reconstruction (bottom). These images are encoded as (4, 3), (4, 5), (16, 21),
and (30, 3).</p></figcaption></figure>
<p>The reconstructions of the first two images are similar, but no longer the exact same: the first image is represented as (4, 3) and the second as (4, 5). In other words, they share the same token for the first level, but differ in how the residual is quantized. The differences are quite subtle, so here’s a comparison between the one-level and two-level reconstructions:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-2-level-diff-v3.png" alt=""/></p><figcaption><p>Difference between one-level and two-level reconstructions</p></figcaption></figure>
<p>I’d like to emphasize that the second quantization level makes modifications to the embedding, not the output pixels directly. This can be seen by the fact that the leftmost and rightmost image are encoded as (4, 3) and (30, 3) respectively. So they have the same residual code, 3, but it modifies the two reconstructed images in different ways.</p>
<p>Clearly, the reconstructions are still not very accurate. The upper bound on the quality is the reconstruction from unquantized embeddings, so if your autoencoder is bad (and ours is), improving the quantization won’t save you.</p>
<p>We’ll stop here, but a natural extension to this idea is to go beyond two levels. Just take the residuals of the two-level reconstruction and quantize those, and so on. This generalized Residual Vector Quantization algorithm looks like this:</p>
<pre><code><span>def</span> <span>rvq_quantize</span>(<span>z</span>):
    residual = z
    codes = []

    <span>for</span> level <span>in</span> <span>range</span>(levels):
        quantized, cluster_i = to_nearest_cluster(level, residual)
        residual -= quantized
        codes.append(cluster_i)

    <span>return</span> codes
</code></pre>
<p>Residual vector quantization was first applied to neural audio codecs in <a href="https://arxiv.org/abs/2107.03312" target="_blank" rel="noopener noreferrer">SoundStream</a>, but the idea <a href="https://ieeexplore.ieee.org/document/1171604" target="_blank" rel="noopener noreferrer">has been around since the 80s</a>.</p>
<h2 id="now-lets-tokenize-audio">Now let’s tokenize audio</h2>
<p>Applying RVQ to audio is fairly straightforward. As our autoencoder, we’ll use a convolutional neural network (CNN) similar to <a href="https://github.com/openai/jukebox/blob/08efbbc1d4ed1a3cef96e08a931944c8b4d63bb3/jukebox/vqvae/encdec.py" target="_blank" rel="noopener noreferrer">what Jukebox uses</a>. The details of the architecture aren’t too important here. What’s important is that it’s a network that takes an audio with <em>t</em> samples and converts it to a vector of shape <em>(t/128, 32)</em>. In other words, it downsamples by a factor of 128 and gives us 32-dimensional float representations. The decoder then takes the <em>(t/128, 32)</em> embeddings and decodes them back into <em>t</em> samples.</p>
<pre><code>audio = get_batch()               
z = encoder(audio)                
audio_reconstructed = decoder(z)  
</code></pre>
<p>As before, we’ll add an RVQ after the encoder. The only difference from the image case is that for each audio sample, we have <em>t/128</em> embedding vectors, not just a single one as we did for images. We just quantize these independently (even though the encoder “sees” more audio than what corresponds to that one vector). During training, we also have a batch dimension, so our model now looks like this:</p>
<pre><code>audio = get_batch()                         
z = encoder(audio)                          


z = rearrange(                              
    z, <span>&#34;b t_emb d -&gt; (b t_emb) d&#34;</span>
)

codes = rvq_quantize(z)           
z_quantized = codes_to_embeddings(codes)    
z_quantized = rearrange(                    
    z, <span>&#34;(b t_emb) d -&gt; b t_emb d&#34;</span>
)

audio_reconstructed = decoder(z_quantized)  
</code></pre>
<figure><div><div><video src="assets/codec-explainer/codecWithRvq.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption></figcaption></figure>
<p>The last missing piece before we can train our first neural audio codec is a loss function. There’s a whole rabbit hole we could go into about which one to choose, but we’ll avoid it and just use a very simple one. We’ll compute the log amplitude spectrogram of the original and reconstructed audio, and take their difference. The loss is the mean square of this difference between spectrograms.</p>
<p>To make it harder for the model to overfit to this loss, we take the spectrogram with three different parameters for the short-time Fourier transform, and let our loss be the mean between the three sub-losses. This is called the <em>multi-scale spectral loss</em>.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%202.png" alt=""/></p><figcaption><p>Image from Evan Radkoff’s excellent <a href="https://www.soundsandwords.io/audio-loss-functions/" target="_blank" rel="noopener noreferrer">blog
post</a> about loss
functions in audio ML. Check it out if you want to go down the loss function
rabbit hole.</p></figcaption></figure>
<p>Finally, let’s train some codecs! We’ll look at how varying the number of RVQ levels affects the reconstruction quality. As we expected, increasing the number of levels helps, decreasing the spectral loss:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%203.png" alt=""/></p><figcaption></figcaption></figure>
<p>Let’s hear what the codecs sound like. We’ll use the three codecs to reconstruct this audio from the <a href="https://speechbot.github.io/expresso/" target="_blank" rel="noopener noreferrer">Expresso dataset</a>:</p>

<p>And the reconstructions:</p>

<p>Clearly, the audio gets better as we add more RVQ levels.</p>
<p>Even with 16 levels, there is some crackling, the audio sounds muffled, and there is a constant high-pitched noise. Later we’ll discuss how we could improve the codec further, but for demonstration purposes, this will do.</p>
<h2 id="why-care-about-audio">Why care about audio</h2>
<p>So now we have a neural audio codec: we can turn audio into LLM-friendly tokens and back. Codec just means a tokenizer for audio, but we say <em>codec</em> because that’s the term used for classic compression like MP3. I’ll be using codec and tokenizer interchangeably.</p>
<p>Let’s come back to what we wanted to do in the first place: modeling audio. Specifically, we’ll make a model that can take an audio prefix and generate a plausible continuation for it.</p>
<p>Just as a reminder, we want to train good audio LLMs so that we have models that understand and produce speech natively, understanding emotion, emphasis, and so on. They could also be fine-tuned into <a href="https://kyutai.org/next/tts" target="_blank" rel="noopener noreferrer">text-to-speech</a>, <a href="https://kyutai.org/next/stt" target="_blank" rel="noopener noreferrer">speech-to-text</a>, or <a href="https://arxiv.org/abs/2502.03382" target="_blank" rel="noopener noreferrer">translation models</a>, among others.</p>
<p>So now that you’re convinced that audio LLMs are the path to AGI, let’s train a few.</p>
<p>For our dataset, we’ll use <a href="https://ai.meta.com/tools/libri-light/" target="_blank" rel="noopener noreferrer">Libri-Light</a>, like we did for our sample-by-sample model earlier. This time we’ll use 10000h of audio instead of 1000h. It’s a dataset of public-domain audiobooks, so if we have a good model for it, maybe we’ll be able to generate more stories. (Don’t get your hopes up too much.) All we need to do is to convert the audio dataset into a sequence of discrete tokens so that we can feed it into an LLM.</p>
<h2 id="dealing-with-multiple-levels">Dealing with multiple levels</h2>
<p>We’ll do that using our 8-level RVQ codec. From an audio with <em>t</em> samples, we’ll get an array of tokens of shape <em>(t/128, 8)</em>. But now there’s an issue: how to deal with the fact that for each time step, there’s not one but eight tokens? This is not a problem we have to deal with in text LLMs, where we have a single sequence of tokens.</p>
<p>We’ll do the simplest thing possible and just flatten the array into 1D of shape <em>(t/128 * 8)</em>, and have our LLM predict the eight levels in separate time steps.</p>
<figure><div><div><video src="assets/codec-explainer/flattenRvq.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption><p>Flattening a three-level RVQ to allow it to be fed into a language model</p></figcaption></figure>
<p>The big disadvantage is that we lose some of our temporal compression. We downsampled the audio 128x, but now we’re inflating it 8x again by flattening the levels. That makes inference less efficient, and possibly worse quality because the effective context size decreases. We&#39;ll be using the 8 RVQ codec rather than the 16 RVQ one to avoid making the compression even worse.</p>
<p>You could also predict all RVQ levels for a single step at once (”parallel pattern”), but it also makes things harder for the model because it has to decide on all levels at once. There are a bunch of other schemes people have tried to balance compression and quality. Here are a few tried out in MusicGen:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%204.png" alt=""/></p><figcaption><p>Figure taken from <a href="https://arxiv.org/abs/2306.05284" target="_blank" rel="noopener noreferrer">MusicGen</a></p></figcaption></figure>
<p>Interestingly, as of 2025, there is no single solution that “won”: every paper does something different, and the schemes can get quite involved. Just look at this diagram from <a href="https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf" target="_blank" rel="noopener noreferrer">MiMo-Audio</a>, a model released in September 2025:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%205.png" alt=""/></p><figcaption><p>Ways to deal with multiple RVQ levels can get quite involved</p></figcaption></figure>
<h2 id="finally-lets-train">Finally, let&#39;s train</h2>
<p>Time to finally train a codec-wrapped language model! As I’ve mentioned, <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">our code</a> is based on Andrej Karpathy’s <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT codebase</a> for training text LLMs. We just need to modify it to accept audio as input. But that’s easy, because LLMs don’t care about what kind of tokens you’re feeding in – it’s all just numbers. Once we’ve tokenized the dataset and flattened it into a 1D sequence, we’re good to go. Tokenized this way, our 10000 hours of audio take up 134 GB. For comparison, storing this much data as uncompressed audio would take over 1 TB.</p>
<p>We’re going to use the exact same model architecture and hyperparameters as for the sample-by-sample model: the only difference is in the tokenization. We also have a 10x bigger dataset, but the sample-by-sample model can’t even fit the dataset with 1k hours, so more data wouldn’t save it.</p>
<p>I trained the model on 8 H100s for about 5 days. To get some samples, I decided to prompt the model with a sample of Libri-Light reading of two lines from <a href="https://www.theotherpages.org/poems/field02.html" target="_blank" rel="noopener noreferrer">Michael Field’s poem July</a>. (As I learned when working on this, Michael Field is a pen name of Katherine Harris and Edith Emma Cooper.) Let’s see what kind of poetry we can get from our model:</p>

<p>There are some signs of life, but we don’t have a poet yet. It sounds like somebody speaking behind a curtain. You can’t really make out what it’s saying, but the intonation is there: it sounds like somebody reading from a book, which is indeed what the model was trained on.</p>
<p>It also maintains a coherent voice, until it decides for the last few seconds to switch to a different one. That is also consistent with the data: we sample the training data from a concatenation of all the audiobooks chopped up into segments and mixed together, so the model does encounter boundaries between different speakers.</p>
<h2 id="how-far-can-a-codec-get-us">How far can a codec get us?</h2>
<p>Our codec was deliberately simplistic, which explains why the results aren&#39;t great—but there&#39;s been a good amount of research on neural audio codecs in the last four years that we could leverage.
We won’t implement all the improvements here, but instead we’ll look at what happens when we use <a href="https://huggingface.co/kyutai/mimi" target="_blank" rel="noopener noreferrer">Mimi</a> as the tokenizer.</p>
<p>Mimi is a modern neural audio codec built here at Kyutai for <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a>, our audio language model. It’s since been used as the tokenizer for other models as well, like <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">Sesame CSM</a>, <a href="https://herimor.github.io/voxtream/" target="_blank" rel="noopener noreferrer">VoXtream</a>, and <a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model" target="_blank" rel="noopener noreferrer">LFM2-Audio</a>.</p>
<p>Unsurprisingly, Mimi sounds a lot better than the homemade codec we trained earlier.</p>
<p>Instead of the multi-scale spectral loss, Mimi uses an adversarial loss, like a GAN. There’s a discriminator network that tries to classify audios as being original or reconstructed by the codec, and the goal of the codec is to fool this discriminator.</p>
<p>Another improvement Mimi adds is using RVQ dropout: it uses 32 RVQ levels but during training, the reconstruction is sometimes randomly truncated to a lower number of levels. That allows us to run Mimi for a lower number of RVQ levels at inference time and still get decent results, because it doesn’t rely on all levels being present. For our codec, we had to train separately.</p>
<p>Let’s hear our example audio reconstructed with Mimi:</p>
<p>Original</p>


<p>For our purposes, a variant with fewer levels might have the advantage of being easier to model because it’s more compressed. Let’s train models with 8- and 32-level Mimi and compare the results.</p>
<p>I trained the exact same model architecture as before, the only thing that changes is the tokenizer. It’s 10k hours from Libri-Light as the dataset, just like when we used our simple codec. Mimi has a sample rate of 24 kHz but Libri-Light uses 16 kHz, which puts a cap on how good it can sound, since we lose the higher frequencies of the audio.</p>
<p>Mimi downsamples the audio a lot more aggressively, too: its sample rate is 12.5 frames per second, whereas we used 125 frames per second for our codec – 10x higher! This means the dataset is also smaller on disk. With our codec, it took 134 GB, but for Mimi it’s “just” 54 GB.</p>
<p>Here’s a poem generated with the model trained on Mimi-tokenized data. I prompted it with two lines from the poem, as before:</p>

<p>Here is my best attempt at a transcription:</p>
<blockquote>
<p><em>When the grass is gone</em></p>
</blockquote>
<p>A tad too surrealist for my taste, but maybe Lewis Carroll would like it.</p>
<h2 id="semantic-tokens">Semantic tokens</h2>
<p>I have a confession to make: I lied to you just now. But just a bit, and for didactic purposes. In fact, the model above was trained on audio from a 31-level Mimi, where I omitted the very first level, which contains the “semantic token”.</p>
<p>The role of this token is to represent semantic information of the audio, without necessarily aiding reconstruction. I won’t go into how these work, but in one sentence, Mimi’s semantic tokens are distilled from <a href="https://arxiv.org/abs/2110.13900" target="_blank" rel="noopener noreferrer">WavLM</a>, which you can think of as a <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT</a> for speech.</p>
<p>To get a feeling for what information semantic tokens encode, let’s take this example audio, passed through Mimi:</p>

<p>Now let’s train a language model trained on the full Mimi, including semantic tokens. We’re going to run the model in a way where we keep the semantic tokens from the original audio but we discard the others, and let the model predict them. That means the information from the semantic tokens is fixed (”teacher-forced”), but the model is free to decide the others according to what continuations it finds plausible.</p>
<figure><div><div><video src="assets/codec-explainer/regenerateWithSemantic.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div></div><figcaption><p>We can get an idea of what information is contained in semantic tokens by
keeping them fixed and letting the model regenerate the rest.</p></figcaption></figure>
<p>Listen to two different reconstructions we obtain this way:</p>


<p>The voice is completely different, but it’s saying the same thing! This means the semantic tokens encode what the person is saying, but are invariant to the voice. That’s useful because it helps the model focus on <em>what</em> to say, not <em>how</em> to say it. In that regard, they’re closer to text tokens, which also don’t contain information about the voice, intonation, timing, or emotion.</p>
<h2 id="making-poetry-semantic">Making poetry semantic</h2>
<p>Now let’s take the model trained on semantic Mimi and ask it to complete the poem:</p>

<blockquote>
<p><em>When grass is gone</em></p>
</blockquote>
<p>It still makes up words and the sentences are not too coherent, but clearly, the proportion of real words is much higher; the model is “more semantic”. The acoustic quality is the same, which is what we’d expect.</p>
<p>Let’s listen to a second poem:</p>

<blockquote>
<p><em>When grass is gone</em></p>
</blockquote>
<p>Indeed, the whom?</p>
<h2 id="semanticacoustic-tradeoff">Semantic–acoustic tradeoff</h2>
<p>We can sacrifice some acoustic quality to improve the semantics by reducing the number of RVQ levels. Let’s do 8. That way, we get higher audio compression, and a proportionally higher part of the loss comes from the semantic token, since now it’s 1/8 tokens and not just 1/32.</p>
<p>One of the first things I noticed about this model is that it learned to memorize the Librivox notice, so it sometimes generates things like:</p>

<blockquote>
<p>Chapter 6 of The Founday, by R. Auclair.</p>
</blockquote>
<p>Repeating the training data is generally not what you want, but in our case it’s a great sign of life, because the previous models couldn’t even manage that. It also makes up the book, author, and reader, so there is still novelty here.</p>
<p>Now let’s try to make some more poetry:</p>

<blockquote>
<p><em>When grass is gone</em></p>
<p>end of poem.</p>
<p>This recording is in the public domain.</p>
<p>[different voice]</p>
</blockquote>
<p>This is great. There are several signs of the model being better than the previous ones. I love that it makes up the word “mollity” and then repeats it in the next line. Also, it realizes that it’s reciting a poem and ends the section with “end of poem”. Then it decides it’s the end of the chapter/section and it ends with the “This recording is in the public domain.” disclaimer. After that, it changes the voice and continues talking. That makes sense, since the clips from various audiobooks are just shuffled and concatenated during training, so here the model simulated a clip boundary.</p>
<p>We might get even better results by weighing the loss of the semantic tokens higher than the acoustic tokens, to make the model focus more on the meaning than the sound – in fact, Moshi uses a semantic loss factor of 100x! But we have to stop somewhere.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We’ve managed to use neural audio codecs to make an audio language model that generates somewhat coherent speech. Obviously, that’s not where the state of the art is in 2025 (and we’re not trying to reach it here) but keep in mind that by using the <em>exact same model</em> without neural audio codecs gives us this:</p>

<p>Of course, still a long way to go to match text models! Currently, there seems to be a trade-off between speech understanding and reasoning abilities. At the beginning, I mentioned that the speech-native models (<a href="https://blog.google/technology/google-deepmind/gemini-2-5-native-audio/" target="_blank" rel="noopener noreferrer">Gemini</a>, ChatGPT’s <a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer">Advanced Voice Mode</a>, <a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen</a>, <a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a>) aren’t able to tell you whether you’re speaking in a high or low voice, despite the fact that they’re trained to natively understand audio. This is likely because they’re trained on a lot of data generated synthetically with text-to-speech and/or because understanding the tone of the voice (apparently) doesn’t help the models make more accurate predictions.</p>
<p>Kyutai took a stab at creating a voice chat based on an audio language model with Moshi (<a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">demo</a>, <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">paper</a>), released in July 2024. Moshi might not be the AI you’d pick to do your homework for you, but cut it some slack: it was the first end-to-end voice AI, released even before OpenAI’s Advanced Voice Mode.</p>
<p>Moshi models an “inner monologue” text stream in parallel with audio streams for itself and the user. The text stream is helps it plan what it’s going to say, and ablations showed that the text stream helps the model massively. At the same time, it’s a bit sad: most of the reasoning seems to be delegated to the text stream and the audio streams are just there to provide an integrated speech-to-text and text-to-speech.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/moshi-figure-1.png" alt=""/></p><figcaption><p><a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a> models two audio streams and a text
stream in parallel</p></figcaption></figure>
<p>It’s not just Moshi: as the “am I speaking in a high voice” experiment shows, this over-reliance on text in favor of audio is an issue for all audio LLMs. And that’s even though the dominant modeling approach is somewhat different than Moshi’s: interleaving text and audio tokens instead of modeling them in parallel streams.</p>
<p>Over a year after Moshi, audio models still lag behind text LLMs. But why? To me, this mysterious unsolved “modality gap” makes audio ML an exciting field to work on.</p>
<p><strong>Thank you for reading! The code for the experiments is <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">here</a>, and for the animations <a href="https://github.com/kyutai-labs/neural-audio-codecs-anims" target="_blank" rel="noopener noreferrer">here</a>.</strong></p>
<h2 id="further-reading">Further reading</h2>
<p>Here are some papers to check out if you&#39;d like to learn more. This list is naturally Kyutai-centric because that&#39;s the school of thought I&#39;m exposed to; my goal is not to do a complete review of the field.</p>
<p>van den Oord et al., 2016. <a href="https://arxiv.org/abs/1609.03499" target="_blank" rel="noopener noreferrer">WaveNet: A Generative Model for Raw Audio</a></p>
<ul>
<li>The OG, sample-by-sample audio continuation model.</li>
</ul>
<p>Mehri et al., 2016. <a href="https://arxiv.org/abs/1612.07837" target="_blank" rel="noopener noreferrer">SampleRNN: An Unconditional End-to-End Neural Audio Generation Model</a></p>
<p>van den Oord et al., 2017. <a href="https://arxiv.org/abs/1711.10433" target="_blank" rel="noopener noreferrer">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</a></p>
<p>Kumar et al., 2019. <a href="https://arxiv.org/abs/1910.06711" target="_blank" rel="noopener noreferrer">MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis</a></p>
<p>Kong et al., 2020. <a href="https://arxiv.org/abs/2010.05646" target="_blank" rel="noopener noreferrer">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</a></p>
<ul>
<li>Various pre-codec improvements over WaveNet, mainly focused on efficiency.</li>
</ul>
<p>van den Oord et al., 2017. <a href="https://arxiv.org/abs/1711.00937" target="_blank" rel="noopener noreferrer">Neural Discrete Representation Learning</a></p>
<ul>
<li>Introduces VQ-VAE, originally for images.</li>
</ul>
<p>Esser et al., 2020. <a href="https://arxiv.org/abs/2012.09841" target="_blank" rel="noopener noreferrer">Taming Transformers for High-Resolution Image Synthesis</a></p>
<ul>
<li>VQGAN, successfully applies a similar two-stage recipe to what we showed here with audio. A VQ-VAE generates quantized image representations, and a transformer predicts them autoregressively, building the image row-by-row.</li>
</ul>
<p>Lakhotia et al., 2021. <a href="https://arxiv.org/abs/2102.01192" target="_blank" rel="noopener noreferrer">On Generative Spoken Language Modeling from Raw Audio</a></p>
<ul>
<li>The first paper to train a language model on discretized speech. As a tokenizer, it uses k-means to quantize latents from pre-trained speech representation models.</li>
</ul>
<p>Zeghidour et al., 2021. <a href="https://arxiv.org/abs/2107.03312" target="_blank" rel="noopener noreferrer">SoundStream: An End-to-End Neural Audio Codec</a></p>
<ul>
<li>Introduces RVQ for neural audio codecs.</li>
</ul>
<p>Lee et al., 2022. <a href="https://arxiv.org/abs/2203.01941" target="_blank" rel="noopener noreferrer">Autoregressive Image Generation using Residual Quantization</a></p>
<ul>
<li>Combines VQGAN with residual vector quantization.</li>
</ul>
<p>Défossez et al., 2022. <a href="https://arxiv.org/abs/2210.13438" target="_blank" rel="noopener noreferrer">High Fidelity Neural Audio Compression</a></p>
<ul>
<li>EnCodec, an early <a href="https://github.com/facebookresearch/encodec?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">open-source</a> neural audio codec. One interesting point is that they try out the <a href="https://arxiv.org/abs/1611.01144" target="_blank" rel="noopener noreferrer">Gumbel-Softmax</a>, which is a different way of dealing with the fact that quantization is non-differentiable.</li>
</ul>
<p>Hsu et al., 2021. <a href="https://arxiv.org/abs/2106.07447" target="_blank" rel="noopener noreferrer">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></p>
<ul>
<li>A masked speech prediction model used to create semantic tokens in Mimi.</li>
</ul>
<p>Défossez et al., 2024. <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi: a speech-text foundation model for real-time dialogue</a></p>
<ul>
<li>Moshi, Kyutai&#39;s audio-native model. Models the user and assistant audio as parallel audio streams, and includes an assistant text stream to help guide the generation.
The paper also introduces the neural audio codec Mimi.</li>
</ul>
<p>Dieleman, 2025. <a href="https://sander.ai/2025/04/15/latents.html" target="_blank" rel="noopener noreferrer">Generative modelling in latent space</a></p>
<ul>
<li>A more high-level blog post about the general idea of using an encoder + generative model + decoder, where the (encoder, decoder) pair is trained separately from the generative model. A great read about where the field is and might be going.</li>
</ul>
<p>Peng et al., 2025. <a href="https://arxiv.org/abs/2508.19205" target="_blank" rel="noopener noreferrer">VibeVoice Technical Report</a></p>
<p>Rouard et al., 2025. <a href="https://arxiv.org/abs/2509.06926" target="_blank" rel="noopener noreferrer">Continuous Audio Language Models</a></p>
<ul>
<li>These works bypass the need for discrete tokens altogether by using diffusion or consistency models respectively, representing a promising alternative to RVQ.</li>
</ul>

<p>Here are some modern LLMs (as of October 2025) that natively support audio. Again, I&#39;m not trying to maintain a complete list here, and I&#39;m not including models without any published technical details.</p>
<p><a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a> (Kyutai, 2023): the online demo of Moshi, Kyutai&#39;s audio language model – see above.</p>
<p><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">CSM</a> (Sesame, 2025): a natural-sounding voice chat, based on Llama + Mimi.</p>
<p><a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen3-Omni</a> (Alibaba, 2025): Alibaba&#39;s multimodal LLM. The audio output is created by a &#34;talker&#34; model whose outputs are not fed back into, which, as far as I can tell, basically makes it a text model with an integrated text-to-speech.</p>
<p><a href="https://github.com/XiaomiMiMo/MiMo-Audio" target="_blank" rel="noopener noreferrer">MiMo-Audio</a> (Xiaomi, 2025): an audio-only language model that shows promising few-shot capabilities, similar to what GPT-2 did for text.</p>
<p><a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model" target="_blank" rel="noopener noreferrer">LFM2-Audio</a> (Liquid AI, 2025): audio/text language model, uses Mimi as the codec.</p></div></div></div></div>
  </body>
</html>
