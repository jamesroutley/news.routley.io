<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gal.hagever.com/posts/my-node-js-is-a-bit-rusty">Original</a>
    <h1>My Node.js is a bit Rusty</h1>
    
    <div id="readability-page-1" class="page"><article><p><span role="img" aria-hidden="true">üëÄ</span><span>This blog post has been residing in &#34;draft&#34; mode for quite a while now. I&#39;ve finally decided to just publish it. As such, a few things might be a bit off, but... that&#39;s life, isn&#39;t it? I&#39;m eager to hear what you think.</span></p><p>Back in 2020, my team at Wix launched a new internal product called CDN Stats. As the name suggests, CDN Stats is a platform that displays and aggregates data and statistics derived from Wix&#39;s CDN.</p><p>That same year, I delivered a presentation about an experiment I conducted. This experiment, which involved rewriting a single module into a native Node.js add-on using Rust, resulted in a staggering 25x performance improvement.</p><p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/99be2ad5-1edf-4cac-968f-9e7866e506bf/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230908T071157Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=15c611cc4759571cda5b0b249092dd5b84227b3b01ceb31174a2b0296fb493be&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject" alt="CDN stats screenshot from the talk."/><span>CDN stats screenshot from the talk.</span></p><p>This platform empowers front-end developers at Wix by providing real-time data on the usage of their compiled assets by our customers, including:</p><ul><li>Platforms</li><li>Downloads (per platform)</li><li>Response size (transfer size)</li></ul><p>These metrics allow front-end developers to identify which bundles need optimization and the urgency of these optimizations. It helped us rectify a critical issue that we might have otherwise overlooked: due to a human error, one of our primary JavaScript assets ballooned to 33MB(!) because it inlined uncompressed SVG without externalizing it or even dynamically importing it.</p><p>This project seems fairly straightforward, doesn&#39;t it? All we&#39;re doing is counting and summing up some values! Toss in some indices in the database, present it with a data table, and voila. But is it really that simple?</p><h2>How does that work?</h2><p>In order to populate the data table, we need to sift through the CDN logs. These are tab-separated values files (TSV) stored in Amazon S3. We&#39;re talking about ~290k files per day, which can amount to 200GB.</p><p>So, every day, we download the TSV files from the previous day, parse them into meaningful information, and store this data in our database. We accomplish this by enqueuing each log file into a job queue. This strategy allows us to parallelize the process and utilize <em>25 instances</em> to parse the log files within approximately <em>3 hours</em> each day.</p><p>Then, every few hours, we generate an aggregation <em>for the previous week</em>. This process takes around ~1 hour and is primarily executed in the database using a MongoDB aggregation.</p><p>The MongoDB query was <a href="https://twitter.com/galstar/status/1384874486814724096">pretty intense</a>.</p><p><span role="img" aria-hidden="true">ü§î</span><span>You may be thinking, &#34;Isn&#39;t that a problem AWS Athena can solve?&#34; and you&#39;d be absolutely right. However, let&#39;s set that aside for now. It simply wasn&#39;t an option we were able to utilize at the time.</span></p><h2>So What&#39;s the Problem?</h2><p>Parsing gigabytes of TSV files might seem like a tedious task. Investing so much time and resources on it... well, it&#39;s not exactly a joy. To be brutally honest: having 25 Node.js instances running for three hours to parse 200GB of data seems like an indication that something isn&#39;t quite right.</p><p>The solution that eventually worked was not our first attempt. Initially, we tried using the internal Wix Serverless framework. We processed all log files concurrently (and aggregated them using <code>Promise.all</code>, isn&#39;t that delightful?). However, to our surprise, we soon ran into out-of-memory issues, even when using utilities like <a href="https://www.npmjs.com/package/p-limit"><code>p-limit</code></a> to limit ourselves to just two jobs in parallel. So, it was back to the drawing board for us.</p><p>Our second approach involved migrating the same code, verbatim, to the Wix Node.js platform, which runs on Docker containers in a Kubernetes cluster. We still encountered out-of-memory issues, leading us to reduce the number of files processed in parallel. Eventually, we got it to work‚Äîhowever, it was disappointingly slow. Processing a single day&#39;s worth of data took more than a day. Clearly, this was not a scalable solution!</p><p>So, we decided to try the job queue pattern. By scaling our servers to 25 containers, we managed to achieve a more reasonable processing time. But, can we truly consider running 25 instances to be <em>reasonable</em>?</p><p><strong>Perhaps it&#39;s time to consider that JavaScript may be the problem.</strong> There might be certain issues that JavaScript simply can&#39;t solve efficiently, especially when the work is primarily CPU and memory bound‚Äîareas where Node.js doesn&#39;t excel.</p><p>But why is that so?</p><h2>Node.js is a Garbage Collected VM</h2><p>Node.js is a remarkable piece of technology. It fosters innovation and enables countless developers to accomplish tasks and deliver products. Unlike languages such as C and C++, JavaScript does not require explicit memory management. Everything is handled by the runtime‚Äîin this case, a VM known as V8.</p><p>V8 determines when to free up memory. This is how many languages are designed to optimize the developer experience. However, for certain applications, explicit memory usage is unavoidable.</p><p>Let&#39;s analyze what (approximately) transpires when we execute our simplified TSV parsing code:</p><pre data-shiki="true"><code><span><span>for</span><span> </span><span>await</span><span> (</span><span>const</span><span> </span><span>line</span><span> </span><span>of</span><span> readlineStream) {</span></span>
<span><span>  </span><span>const</span><span> </span><span>fields</span><span> </span><span>=</span><span> </span><span>line</span><span>.split</span><span>(</span><span>&#39;\t&#39;</span><span>);</span></span>
<span><span>  </span><span>const</span><span> </span><span>httpStatus</span><span> </span><span>=</span><span> </span><span>Number</span><span>(fields[</span><span>5</span><span>]);</span></span>
<span><span>  </span><span>if</span><span> (httpStatus </span><span>&lt;</span><span> </span><span>200</span><span> </span><span>||</span><span> httpStatus </span><span>&gt;</span><span> </span><span>299</span><span>) </span><span>continue</span><span>;</span></span>
<span><span>  </span><span>records</span><span>.push</span><span>({</span></span>
<span><span>    pathname</span><span>:</span><span> fields[</span><span>7</span><span>]</span><span>,</span></span>
<span><span>    referrer</span><span>:</span><span> fields[</span><span>8</span><span>]</span><span>,</span></span>
<span><span>    </span><span>// ...</span></span>
<span><span>  });</span></span>
<span><span>}</span></span>
<span></span></code></pre><p>This code is quite straightforward. By using <code>readline</code>, we iterate through lines in the file in a stream to circumvent memory bottlenecks caused by reading the entire file into memory. For every line, we split it by the tab character and push an item to an array of results. But what&#39;s happening behind the scenes in terms of memory?</p><p>When <code>line.split(&#39;\t&#39;)</code> is invoked, we get a <em>newly allocated array</em> containing multiple items. Each item is a <em>newly allocated string that occupies memory</em>. This is the way <code>Array#split</code> works: it creates a new array and strings every time.</p><p>The intriguing part here is that <code>line</code> and <code>fields</code> won&#39;t be cleared from memory until the garbage collector determines it&#39;s time to do so. We find ourselves with RAM filled with redundant strings and arrays that need cleaning. It&#39;s hardly surprising that our computer wasn&#39;t thrilled with this situation.</p><h2>Finding Comfort in Rust</h2><p>As an intermediate Rustacean (I do have a fairly successful open-source project written in Rust, <a href="https://github.com/schniz/fnm">called fnm</a>), I&#39;m well aware that memory management is one of Rust&#39;s key strengths. The fact that Rust does not require a runtime garbage collector makes it suitable for embedding within other languages and virtual machines, positioning it as an ideal candidate for implementing such features.</p><p>I&#39;ve shared a few tweets expressing my beliefs. Building applications in Rust is powerful, but <strong>the ability to embed Rust in a battle-tested VM</strong> without altering the rest of your application <strong>is a true game-changer</strong>. Creating a service in Node.js and only using Rust when necessary can help you accelerate development and replace modules with more efficient ones as and when required.</p><p>Using Node.js as the entry point of our application enabled me to leverage Wix‚Äôs standard storage/database connectors, error reporting and logging, and configuration management, without the need to re-implement these on the Rust side. This allowed me to focus <em>solely</em> on what mattered most: optimizing the slow part.</p><p>Fortunately, embedding Rust in Node.js is a breeze, thanks to <a href="https://napi.rs/">https://napi.rs</a>: an excellent Rust library that facilitates seamless integration between Rust and Node.js. You simply write a Rust library, use some fancy macros, and voila! You have a native Node.js module written in Rust.</p><h3>The Rust code</h3><p>In Rust, I can begin by declaring my structs as the relevant data structures I want to parse the lines into‚Äîa &#34;type-driven development&#34; workflow. I started with <code>Record</code>, the most basic line I want to parse from a TSV record set. Instead of designating every part of the Record as <code>String</code> (an owned string), I can force it to be a reference, <code>&amp;&#39;a str</code>, which lives for the lifetime of <code>&#39;a</code>:</p><pre data-shiki="true"><code><span><span>struct</span><span> </span><span>Record</span><span>&lt;&#39;</span><span>a</span><span>&gt; {</span></span>
<span><span>  pathname</span><span>:</span><span> </span><span>&amp;</span><span>&#39;</span><span>a</span><span> </span><span>str</span><span>,</span></span>
<span><span>  referrer</span><span>:</span><span> </span><span>&amp;</span><span>&#39;</span><span>a</span><span> </span><span>str</span><span>,</span></span>
<span><span>  // ...</span></span>
<span><span>}</span></span></code></pre><p><span role="img" aria-hidden="true">üìù</span><span><strong>Note:</strong> By making <code>Record</code> contain slices of strings (with a <code>&#39;a</code> lifetime), we&#39;re essentially stating that <code>Record</code> is a derived data structure that doesn&#39;t alter the original data. This implies that <code>Record</code> cannot outlive <code>line</code>‚Äîan advantageous feature. It aids us in developing our application while being mindful of optimal memory usage.</span></p><p>So, what exactly is <code>Record</code>? It&#39;s a struct that organizes the raw data in a structured manner. However, merely deriving the raw data isn&#39;t sufficient for our use case. To maximize the capabilities of browser caching, static assets are deployed with a content hash as part of the filename. Instead of having a <code>/artifact/file.js</code>, we have <code>/artifact/file.abcdef0123456.js</code> files, allowing the browser to cache them indefinitely.</p><p>For our aggregations, we want to remove this content hash. Furthermore, we want to infer the artifact name from the pathname. This is why we introduced <code>EnhancedRow</code>:</p><pre data-shiki="true"><code><span><span>struct</span><span> </span><span>EnhancedRow</span><span>&lt;&#39;</span><span>a</span><span>&gt; {</span></span>
<span><span>  pathname</span><span>:</span><span> </span><span>&amp;</span><span>&#39;</span><span>a</span><span> </span><span>str</span><span>,</span></span>
<span><span>  referrer</span><span>:</span><span> </span><span>&amp;</span><span>&#39;</span><span>a</span><span> </span><span>str</span><span>,</span></span>
<span><span>  // ...</span></span>
<span><span>  artifact</span><span>:</span><span> </span><span>&amp;</span><span>&#39;</span><span>a</span><span> </span><span>str</span><span>,</span><span> // This is a slice of pathname</span></span>
<span><span>                     // =&gt; a slice of a slice =&gt; a slice</span></span>
<span><span>  filename</span><span>:</span><span> </span><span>Cow</span><span>&lt;&#39;</span><span>a</span><span>, </span><span>str</span><span>&gt;,</span></span>
<span><span>  // ...</span></span>
<span><span>}</span></span>
<span></span></code></pre><p>As we can see, <code>EnhancedRow&lt;&#39;a&gt;</code> incorporates everything that <code>Record&lt;&#39;a&gt;</code> does, but it enhances it with a few values:</p><ul><li><code>artifact</code> is a <code>&amp;&#39;a str</code>, as it refers to a part of the <code>pathname</code> slice. No cloning is necessary to obtain this information.</li><li><code>filename</code> is a <code>Cow&lt;&#39;a, str&gt;</code>, or &#34;copy on write&#34;. As we just mentioned, we <em>attempt</em> to remove the content hash from the file. If we don&#39;t find a content hash, there&#39;s no need to copy the string. If we do find it, we copy the string. This type allows us to be incredibly explicit about our intentions.</li></ul><p>After defining these two structs, we implemented <code>TryFrom&lt;&amp;&#39;a str&gt; for Record&lt;&#39;a&gt;</code> and <code>TryFrom&lt;Record&lt;&#39;a&gt;&gt; for EnhancedRecord&lt;&#39;a&gt;</code>, enabling us to safely parse a borrowed string into a Record, and then into an <code>EnhancedRecord</code>, with minimal cloning involved.</p><p>Next, we created a new struct called <code>ResourceCounter</code>, which performed the aggregation for the given files. It was a simple wrapper around <code>HashMap</code>. Our key was composed of <code>(artifact, file_name)</code>, and our value had a counter for requests, based on <code>Platform</code>s.</p><p><code>ResourceCounter</code> allows us to provide an <code>EnhancedRecord&lt;&#39;a&gt;</code>, and it only clones data if necessary. So, if the artifact/filename had never been encountered before, it would clone the data.</p><p>With this, our code was complete. However, since we were implementing a performance optimization feature, we knew we couldn&#39;t call it a day without benchmarking it against the current JavaScript implementation. This benchmarking would require us to use the Node.js module (rather than using Rust as a CLI or similar) to ensure we included the overhead of the JS and Rust communication.</p><p>I fired up our internal JavaScript benchmarking tool (called Perfer) and wrote a simple benchmark:</p><pre data-shiki="true"><code><span><span>import</span><span> { benchmark } </span><span>from</span><span> </span><span>&#39;@wix/perfer&#39;</span><span>;</span></span>
<span><span>import</span><span> </span><span>*</span><span> </span><span>as</span><span> jsParser </span><span>from</span><span> </span><span>&#39;@wix/cdn-stats-js-parser&#39;</span><span>;</span></span>
<span><span>import</span><span> </span><span>*</span><span> </span><span>as</span><span> nativeParser </span><span>from</span><span> </span><span>&#39;@wix/cdn-stats-native-parser&#39;</span><span>;</span></span>
<span><span>import</span><span> fs </span><span>from</span><span> </span><span>&#39;fs&#39;</span><span>;</span></span>
<span><span>import</span><span> assert </span><span>from</span><span> </span><span>&#39;assert&#39;</span><span>;</span></span>
<span></span>
<span><span>benchmark</span><span>.node</span><span>(</span><span>&#39;native parser&#39;</span><span>,</span><span> </span><span>async</span><span> () </span><span>=&gt;</span><span> {</span></span>
<span><span>  </span><span>const</span><span> </span><span>values</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>nativeParser</span><span>.runAsync</span><span>([</span><span>&#39;../SOME_BIG_FIXTURE&#39;</span><span>]);</span></span>
<span><span>  </span><span>assert</span><span>.ok</span><span>(</span><span>values</span><span>.</span><span>length</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>)</span></span>
<span><span>});</span></span>
<span></span>
<span><span>benchmark</span><span>.node</span><span>(</span><span>&#39;js parser&#39;</span><span>,</span><span> </span><span>async</span><span> () </span><span>=&gt;</span><span> {</span></span>
<span><span>  </span><span>const</span><span> </span><span>stream</span><span> </span><span>=</span><span> </span><span>fs</span><span>.createReadStream</span><span>(</span><span>&#39;../SOME_BIG_FIXTURE&#39;</span><span>);</span></span>
<span><span>  </span><span>const</span><span> </span><span>values</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>jsParser</span><span>.runAsync</span><span>([stream]);</span></span>
<span><span>  </span><span>assert</span><span>.ok</span><span>(</span><span>values</span><span>.</span><span>length</span><span> </span><span>&gt;</span><span> </span><span>0</span><span>)</span></span>
<span><span>});</span></span></code></pre><p>and ran it. The results were impressive:</p><p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/cc580a60-25ed-46cf-a1e1-f1f094c90b0e/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230908T071157Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=e8ed6287ac6f6de769ff9e113b4ffc5c91db2f36cf375a6e6d286ac094b7d96f&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject" alt="a screenshot from the CLI output of the benchmarking tool"/><span>a screenshot from the CLI output of the benchmarking tool</span></p><p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/866c4ca6-29a1-420b-9983-290179f1f58b/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230908T071157Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=77a2d03a8c12c33784f04fc5d3d7b55944cc24e2e41cbdc42d87a0796a3abd30&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject" alt="a screenshot from the web UI of the benchmarking tool"/><span>a screenshot from the web UI of the benchmarking tool</span></p><p>The results in plain text were as follows:</p><ul><li>JavaScript parser<ul><li>5878ms of overall runtime</li><li>6.49sec CPU time</li><li><em>381mb of memory usage</em></li></ul></li><li>Rust parser<ul><li>1145ms of overall runtime</li><li>3.25sec CPU time</li><li><em>1.75mb of memory usage</em></li></ul></li></ul><p>While there is a significant difference in times, the most glaring disparity lies in memory usage. This difference enables us to process <em>more files in parallel</em>. And that&#39;s a huge advantage.</p><h2>Previewing a Deployment</h2><p>The Wix deployment preview model is based on ephemeral Kubernetes pods that can be manually requested for a specific commit. The ingress proxy can then direct traffic to these special pods given a specific header. This is great for testing something as innovative as this project, so we decided to try running a preview deployment using the Rust parser instead of the JavaScript one.</p><p>We ran a benchmark a few times. We processed the same 190,000 log files, using the following infrastructure:</p><ul><li>JavaScript implementation with 25 instances, which finished in ~3 hours.</li><li>Rust implementation with one deploy preview instance, completing in ~2.5 hours.</li></ul><p>This does not include any database I/O (since the deploy preview instance is read-only), but:</p><ul><li>There are far fewer insertions in the Rust implementation because we can process more files simultaneously. This enhances our aggregation and reduces the number of records to be pushed to the database.</li><li>We could modify our implementation to store the data directly in a simple storage like S3, and then have a separate job that transfers the data directly to the database. This would allow us to separate computational work from network work.</li></ul><p>Assuming the times were the same, the Rust implementation used only 1/25 of the resources available to the JavaScript implementation. That&#39;s a 2500% performance boost! Even if we do need to add the MongoDB insertions, which may take some extra time, we would still be looking at a boost of 2000%? 1900%? Either way, it&#39;s a significant win.</p><p><img src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/853061dd-45dd-4788-8670-0becf29831d2/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230908%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20230908T071157Z&amp;X-Amz-Expires=3600&amp;X-Amz-Signature=6b9439be7f0e09091f2cba3dcb8d44b9c6940391b2a24639d7d594ed77ddff96&amp;X-Amz-SignedHeaders=host&amp;x-id=GetObject" alt="Figure #0"/></p><h2>So, What&#39;s the Takeaway?</h2><p>It seems that choosing the right tool for the job is essential. Some tasks are well-suited to JavaScript, while others are not. Although Rust is a language that&#39;s more challenging to master, we can still encapsulate the logic and distribute it in such a way that users don&#39;t mind using it. We can see this trend across the JavaScript ecosystem: Next.js replaced their use of Babel with SWC and may soon use Turbopack by default (fingers crossed). Esbuild, a popular choice for bundling and transforming TypeScript or modern JS to JavaScript, is written in Golang. It&#39;s simply faster, which is always beneficial.</p><p>So, I suppose my takeaway here is... use whatever makes you happy, and if something is too slow, profile it first, then encapsulate it, and then‚Äîperhaps‚Äîconsider rewriting it in Rust. üòà</p></article></div>
  </body>
</html>
