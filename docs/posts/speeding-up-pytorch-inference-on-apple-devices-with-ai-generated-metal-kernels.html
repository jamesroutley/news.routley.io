<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://gimletlabs.ai/blog/ai-generated-metal-kernels">Original</a>
    <h1>Speeding up PyTorch inference on Apple devices with AI-generated Metal kernels</h1>
    
    <div id="readability-page-1" class="page"><div><section><article><div><header></header><p><img alt="Speeding up PyTorch inference by 87% on Apple devices with AI-generated Metal kernels" loading="lazy" width="1200" height="600" decoding="async" data-nimg="1" src="http://tinylogger.com/blog/images/server-rack-hero.jpg"/></p><div><dl><p><dt>Published on</dt><dd><time datetime="2025-08-26T00:00:00.000Z">August 26, 2025</time></dd></p><dt>Authors</dt><dd><ul><li><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" src="http://tinylogger.com/blog/images/avatar/taras_sereda.jpg"/><dl><dt>Name</dt><dd>Taras Sereda</dd><dd><div><p><span>Twitter</span></p><a target="_blank" rel="noopener noreferrer" href="https://x.com/taras_y_sereda" aria-label="X profile of Taras Sereda"><svg width="10" height="10" viewBox="0 0 1200 1227" xmlns="http://www.w3.org/2000/svg"><path d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z"></path></svg></a><p><span>LinkedIn</span></p><a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/tarassereda/" aria-label="LinkedIn profile of Taras Sereda"><svg width="10" height="10" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Linkedin</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a></div></dd></dl></li><li><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" src="http://tinylogger.com/blog/images/avatar/natalie.jpg"/><dl><dt>Name</dt><dd>Natalie Serrino</dd><dd><div><p><span>Twitter</span></p><a target="_blank" rel="noopener noreferrer" href="https://twitter.com/nserrino" aria-label="X profile of Natalie Serrino"><svg width="10" height="10" viewBox="0 0 1200 1227" xmlns="http://www.w3.org/2000/svg"><path d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z"></path></svg></a><p><span>LinkedIn</span></p><a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/natalieserrino/" aria-label="LinkedIn profile of Natalie Serrino"><svg width="10" height="10" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Linkedin</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a></div></dd></dl></li><li><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" src="http://tinylogger.com/blog/images/avatar/zain.png"/><dl><dt>Name</dt><dd>Zain Asgar</dd><dd><div><p><span>Twitter</span></p><a target="_blank" rel="noopener noreferrer" href="https://twitter.com/zasgar" aria-label="X profile of Zain Asgar"><svg width="10" height="10" viewBox="0 0 1200 1227" xmlns="http://www.w3.org/2000/svg"><path d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z"></path></svg></a><p><span>LinkedIn</span></p><a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/zasgar/" aria-label="LinkedIn profile of Zain Asgar"><svg width="10" height="10" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Linkedin</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a></div></dd></dl></li></ul></dd></dl><div><div><div><p><em>tl;dr: Our lab investigated whether frontier models can write optimized GPU kernels for Apple devices to speed up inference. We found that they can: <strong>our AI-generated Metal kernels were 1.87x faster</strong> across 215 PyTorch modules, with some workloads running <strong>hundreds of times faster</strong> than baseline.</em></p><p>AI models execute on hardware via GPU kernels that define each operation. The efficiency of those kernels determines how fast models run (in training and inference). Kernel optimizations like <em>FlashAttention</em><sup><a href="#user-content-fn-1" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-1">1</a></sup> show dramatic speedups over baseline, underscoring the need for performant kernels.</p><p>While PyTorch and tools like <code>torch.compile</code><sup><a href="#user-content-fn-2" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-2">2</a></sup> handle some kernel optimizations, the last mile of performance still depends on handtuned kernels. These kernels are difficult to write, requiring significant time and expertise. It gets especially challenging when writing kernels outside of CUDA: expertise in non-CUDA platforms is rarer, and there is less tooling and documentation available</p><p>We set out to answer a simple question: could frontier models implement kernel optimizations automatically, across different backends? Billions of Apple devices rely on Metal kernels that are often under-optimized, so we started with Metal.</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/pytorch_to_platforms.png" alt="Our vision: Autonomous kernel optimization for any target platform using frontier models"/></p><p>Our vision: Autonomous kernel optimization for any target platform using frontier models.</p><p>Across 215 PyTorch modules, our results show the generated kernels ran 87% faster on Apple hardware compared to baseline PyTorch. This approach requires no expertise in kernel engineering and can be done nearly instantly.</p><p>Here&#39;s a preview of what we discovered:</p><ul><li>Many cases where our approach improved performance by 10-100X</li><li>Cases where models surfaced algorithmically unnecessary work and removed it (that PyTorch didn&#39;t catch)</li><li>The impact of incorporating performance profiling and CUDA reference code</li><li>Why a simple agentic swarm dominates over individual frontier models</li></ul><p>We included 8 frontier models from Anthropic, DeepSeek, and OpenAI in our analysis:</p><ul><li>Anthropic family<ul><li>claude-sonnet-4 (2025-05-14)</li><li>claude-opus-4 (2025-05-14)</li></ul></li><li>OpenAI family<ul><li>gpt-4o (2024-11-20)</li><li>gpt-4.1 (2025-04-14)</li><li>gpt-5 (2025-08-07)</li><li>o3 (2025-04-16)</li></ul></li><li>DeepSeek family<ul><li>deepseek-v3 (2025-03-25)</li><li>deepseek-r1 (2025-05-28)</li></ul></li></ul><p>In terms of test inputs, we used the PyTorch modules defined in the KernelBench<sup><a href="#user-content-fn-3" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-3">3</a></sup> dataset. KernelBench contains 250 PyTorch modules defining ML workloads of varying complexity. 31 modules contain operations that are currently unsupported in the PyTorch backend for MPS (Metal Performance Shaders), so they were excluded from this analysis. (We ended up excluding 4 additional modules for reasons that will be discussed later.)</p><div><table><thead><tr><th>KernelBench Category</th><th>Description</th><th># of Test Cases</th></tr></thead><tbody><tr><td>Level 1</td><td>Simple primitive operations (e.g. matrix multiplication, convolution)</td><td>91</td></tr><tr><td>Level 2</td><td>Sequences of multiple operations from Level 1</td><td>74</td></tr><tr><td>Level 3</td><td>Complete model architectures (e.g. AlexNet, VGG)</td><td>50</td></tr></tbody></table></div><p>When evaluating the agent-generated kernels, we need to assess both correctness and performance relative to the baseline PyTorch implementation (at the time of writing, <code>torch.compile</code> support for Metal is still underway, so it could not serve as a comparison point. MLX is also a great framework for Apple devices, but this work focused on pure PyTorch code optimization, whereas MLX is its own framework). We also made sure to carefully clear the cache between runs, otherwise cached results can falsely present as speedups.</p><div><table><thead><tr><th>Experimental Variable</th><th>Specification</th></tr></thead><tbody><tr><td>Hardware</td><td>Mac Studio (Apple M4 Max chip)</td></tr><tr><td>Models</td><td>Claude Opus 4, Claude Sonnet, DeepSeek r1, DeepSeek v3, GPT-4.1, GPT-4o, GPT-5, o3</td></tr><tr><td>Dataset</td><td>KernelBench</td></tr><tr><td>Baseline Implementation</td><td>PyTorch eager mode</td></tr><tr><td>Number of shots</td><td>5</td></tr></tbody></table></div><p>We begin with the simplest implementation of the kernel-writing agent for Metal:</p><ul><li>Receives the prompt and PyTorch code</li><li>Generates Metal kernels</li><li>Assesses if they match the baseline PyTorch for correctness<sup><a href="#user-content-fn-4" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-4">4</a></sup>.</li><li>If they fail to compile or are not correct, an error message is passed back to the agent for another try, with up to 5 tries permitted.</li></ul><p>It&#39;s interesting to see how the correctness increases with the number of attempts. o3, for example, gets a working implementation about 60% of the time on the first try, and reaches 94% working implementations by attempt 5.</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/o3_baseline_cumulative_success.png" alt="o3&#39;s success rate by generation attempt and kernel level"/></p><p>o3&#39;s success rate by generation attempt and kernel level. We limited the agent to 5 tries, which seems sufficient for Level 1 and 2 kernels, but Level 3 kernels may benefit from further shots.</p><p>Let&#39;s look at each of our 8 models correctness rates, broken down by whether or not the implementation was faster than our baseline or not:</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/frontier_models_correctness_speedup.png" alt="Kernel correctness, broken down by whether or not the optimized version was faster than the baseline"/></p><p>Kernel correctness, broken down by whether or not the optimized version was faster than the baseline.</p><p>The reasoning models are pretty good at generating correct kernels across levels, although the non-reasoning models are also capable of doing this sometimes. However, other than GPT-5, these models are more often generating implementations that are slower than the baseline PyTorch. GPT-5&#39;s success at generating faster implementations for Level 2 problems is particularly notable.</p><p>Every agent produced some kernels that were faster than baseline, and some of them came up with pretty cool stuff. GPT-5 produced a 4.65X speedup for a Mamba 2<sup><a href="#user-content-fn-5" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-5">5</a></sup> state space model, primarily by fusing kernels to reduce the overhead of kernel launch and improve memory access patterns.</p><div><h3>Mamba2 Example</h3><div><div><p>PyTorch Input</p><div><div><pre><code><span>1</span><span>import</span><span> torch
</span><span>2</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn </span><span>as</span><span> nn
</span><span>3</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn</span><span>.</span><span>functional </span><span>as</span><span> F
</span><span>4</span>
<span>5</span><span></span><span>class</span><span> </span><span>Model</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>)</span><span>:</span><span>
</span><span>6</span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> input_size</span><span>,</span><span> hidden_layer_sizes</span><span>,</span><span> output_size</span><span>)</span><span>:</span><span>
</span><span>7</span><span>      </span><span>&#34;&#34;&#34;
</span><span><span>8</span>      :param input_size: The number of input features
</span><span><span>9</span>      :param hidden_layer_sizes: A list of ints containing the sizes of each hidden layer
</span><span><span>10</span>      :param output_size: The number of output features
</span><span>11</span><span>      &#34;&#34;&#34;</span><span>
</span><span>12</span><span>      </span><span>super</span><span>(</span><span>Model</span><span>,</span><span> self</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>)</span><span>
</span><span>13</span>      
<span>14</span><span>      layers </span><span>=</span><span> </span><span>[</span><span>]</span><span>
</span><span>15</span><span>      current_input_size </span><span>=</span><span> input_size
</span><span>16</span>      
<span>17</span><span>      </span><span>for</span><span> hidden_size </span><span>in</span><span> hidden_layer_sizes</span><span>:</span><span>
</span><span>18</span><span>          layers</span><span>.</span><span>append</span><span>(</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>current_input_size</span><span>,</span><span> hidden_size</span><span>)</span><span>)</span><span>
</span><span>19</span><span>          layers</span><span>.</span><span>append</span><span>(</span><span>nn</span><span>.</span><span>ReLU</span><span>(</span><span>)</span><span>)</span><span>
</span><span>20</span><span>          current_input_size </span><span>=</span><span> hidden_size
</span><span>21</span>      
<span>22</span><span>      layers</span><span>.</span><span>append</span><span>(</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>current_input_size</span><span>,</span><span> output_size</span><span>)</span><span>)</span><span>
</span><span>23</span>      
<span>24</span><span>      self</span><span>.</span><span>network </span><span>=</span><span> nn</span><span>.</span><span>Sequential</span><span>(</span><span>*</span><span>layers</span><span>)</span><span>
</span><span>25</span>  
<span>26</span><span>  </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span><span> x</span><span>)</span><span>:</span><span>
</span><span>27</span><span>      </span><span>&#34;&#34;&#34;
</span><span><span>28</span>      :param x: The input tensor, shape (batch_size, input_size)
</span><span><span>29</span>      :return: The output tensor, shape (batch_size, output_size)
</span><span>30</span><span>      &#34;&#34;&#34;</span><span>
</span><span>31</span><span>      </span><span>return</span><span> self</span><span>.</span><span>network</span><span>(</span><span>x</span><span>)</span><span>
</span><span>32</span>
<span>33</span><span></span><span># Test code</span><span>
</span><span>34</span><span>batch_size </span><span>=</span><span> </span><span>1</span><span>
</span><span>35</span><span>input_size </span><span>=</span><span> </span><span>1000</span><span>
</span><span>36</span><span>hidden_layer_sizes </span><span>=</span><span> </span><span>[</span><span>50</span><span>,</span><span> </span><span>50</span><span>,</span><span> </span><span>50</span><span>,</span><span> </span><span>50</span><span>,</span><span> </span><span>50</span><span>,</span><span> </span><span>50</span><span>,</span><span> </span><span>50</span><span>,</span><span> </span><span>50</span><span>]</span><span>  </span><span># Example of deep and narrow layers</span><span>
</span><span>37</span><span>output_size </span><span>=</span><span> </span><span>10</span><span>
</span><span>38</span>
<span>39</span><span></span><span>def</span><span> </span><span>get_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>40</span><span>  </span><span>return</span><span> </span><span>[</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> input_size</span><span>)</span><span>]</span><span>
</span><span>41</span>
<span>42</span><span></span><span>def</span><span> </span><span>get_init_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>43</span><span>  </span><span>return</span><span> </span><span>[</span><span>input_size</span><span>,</span><span> hidden_layer_sizes</span><span>,</span><span> output_size</span><span>]</span></code></pre></div></div></div><div><p>Generated Kernels</p><div><div><pre><code><span>1</span><span>import</span><span> torch
</span><span>2</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn </span><span>as</span><span> nn
</span><span>3</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn</span><span>.</span><span>functional </span><span>as</span><span> F
</span><span>4</span><span></span><span>from</span><span> einops </span><span>import</span><span> rearrange
</span><span>5</span>
<span>6</span><span></span><span># Safe wrappers: try to build the Metal extension, but always provide PyTorch fallbacks.</span><span>
</span><span>7</span><span>mpskern </span><span>=</span><span> </span><span>None</span><span>
</span><span>8</span><span>_have_native_kernels </span><span>=</span><span> </span><span>False</span><span>
</span><span>9</span><span></span><span>try</span><span>:</span><span>
</span><span>10</span><span>  </span><span>from</span><span> torch</span><span>.</span><span>utils</span><span>.</span><span>cpp_extension </span><span>import</span><span> load_inline
</span><span>11</span>
<span>12</span><span>  cpp_source </span><span>=</span><span> </span><span>r&#39;&#39;&#39;
</span><span><span>13</span>  #include &lt;torch/extension.h&gt;
</span><span><span>14</span>  #import &lt;Foundation/Foundation.h&gt;
</span><span><span>15</span>  #import &lt;Metal/Metal.h&gt;
</span><span><span>16</span>
</span><span><span>17</span>  static const char *METAL_SRC = R&#34;KERNEL(
</span><span><span>18</span>  #include &lt;metal_stdlib&gt;
</span><span><span>19</span>  using namespace metal;
</span><span><span>20</span>
</span><span><span>21</span>  // Compute exp(segsum) lower triangular matrix from cumsum prefix for 4D case:
</span><span><span>22</span>  // prefix shape: [num_vec, L]
</span><span><span>23</span>  // output shape: [num_vec, L, L]
</span><span><span>24</span>  // value(i,j) = j &lt;= i ? exp(prefix[i] - prefix[j]) : 0
</span><span><span>25</span>  kernel void lower_tri_from_prefix_4d(constant float* prefix [[buffer(0)]],
</span><span><span>26</span>                                       device   float* out    [[buffer(1)]],
</span><span><span>27</span>                                       constant uint* params  [[buffer(2)]],
</span><span><span>28</span>                                       uint index [[thread_position_in_grid]]) {
</span><span><span>29</span>      uint num_vec = params[0];
</span><span><span>30</span>      uint L       = params[1];
</span><span><span>31</span>      uint total   = num_vec * L * L;
</span><span><span>32</span>      if (index &gt;= total) return;
</span><span><span>33</span>
</span><span><span>34</span>      uint vecId = index / (L * L);
</span><span><span>35</span>      uint rem   = index - vecId * (L * L);
</span><span><span>36</span>      uint i     = rem / L;
</span><span><span>37</span>      uint j     = rem - i * L;
</span><span><span>38</span>
</span><span><span>39</span>      if (j &lt;= i) {
</span><span><span>40</span>          float vi = prefix[vecId * L + i];
</span><span><span>41</span>          float vj = prefix[vecId * L + j];
</span><span><span>42</span>          out[vecId * (L * L) + i * L + j] = exp(vi - vj);
</span><span><span>43</span>      } else {
</span><span><span>44</span>          out[vecId * (L * L) + i * L + j] = 0.0f;
</span><span><span>45</span>      }
</span><span><span>46</span>  }
</span><span><span>47</span>
</span><span><span>48</span>  // Same as above for 3D prefix: prefix shape [num_vec, Z], output [num_vec, Z, Z]
</span><span><span>49</span>  kernel void lower_tri_from_prefix_3d(constant float* prefix [[buffer(0)]],
</span><span><span>50</span>                                       device   float* out    [[buffer(1)]],
</span><span><span>51</span>                                       constant uint* params  [[buffer(2)]],
</span><span><span>52</span>                                       uint index [[thread_position_in_grid]]) {
</span><span><span>53</span>      uint num_vec = params[0];
</span><span><span>54</span>      uint Z       = params[1];
</span><span><span>55</span>      uint total   = num_vec * Z * Z;
</span><span><span>56</span>      if (index &gt;= total) return;
</span><span><span>57</span>
</span><span><span>58</span>      uint vecId = index / (Z * Z);
</span><span><span>59</span>      uint rem   = index - vecId * (Z * Z);
</span><span><span>60</span>      uint i     = rem / Z;
</span><span><span>61</span>      uint j     = rem - i * Z;
</span><span><span>62</span>
</span><span><span>63</span>      if (j &lt;= i) {
</span><span><span>64</span>          float vi = prefix[vecId * Z + i];
</span><span><span>65</span>          float vj = prefix[vecId * Z + j];
</span><span><span>66</span>          out[vecId * (Z * Z) + i * Z + j] = exp(vi - vj);
</span><span><span>67</span>      } else {
</span><span><span>68</span>          out[vecId * (Z * Z) + i * Z + j] = 0.0f;
</span><span><span>69</span>      }
</span><span><span>70</span>  }
</span><span><span>71</span>
</span><span><span>72</span>  // Generic batched GEMM:
</span><span><span>73</span>  // A: [B, M, K] if transA == 0 else [B, K, M]
</span><span><span>74</span>  // B: [B, K, N] if transB == 0 else [B, N, K]
</span><span><span>75</span>  // C: [B, M, N] = A @ B
</span><span><span>76</span>  kernel void gemm_batched(constant float* A     [[buffer(0)]],
</span><span><span>77</span>                           constant float* B     [[buffer(1)]],
</span><span><span>78</span>                           device   float* C     [[buffer(2)]],
</span><span><span>79</span>                           constant uint* params [[buffer(3)]],
</span><span><span>80</span>                           uint index [[thread_position_in_grid]]) {
</span><span><span>81</span>      uint BATCH = params[0];
</span><span><span>82</span>      uint M     = params[1];
</span><span><span>83</span>      uint N     = params[2];
</span><span><span>84</span>      uint K     = params[3];
</span><span><span>85</span>      uint transA= params[4];
</span><span><span>86</span>      uint transB= params[5];
</span><span><span>87</span>
</span><span><span>88</span>      uint total = BATCH * M * N;
</span><span><span>89</span>      if (index &gt;= total) return;
</span><span><span>90</span>
</span><span><span>91</span>      uint b = index / (M * N);
</span><span><span>92</span>      uint rem = index - b * (M * N);
</span><span><span>93</span>      uint m = rem / N;
</span><span><span>94</span>      uint n = rem - m * N;
</span><span><span>95</span>
</span><span><span>96</span>      float acc = 0.0f;
</span><span><span>97</span>      if (transA == 0 &amp;&amp; transB == 0) {
</span><span><span>98</span>          uint baseA = b * (M * K);
</span><span><span>99</span>          uint baseB = b * (K * N);
</span><span><span>100</span>          for (uint k = 0; k &lt; K; ++k) {
</span><span><span>101</span>              float a = A[baseA + m * K + k];
</span><span><span>102</span>              float bb = B[baseB + k * N + n];
</span><span><span>103</span>              acc += a * bb;
</span><span><span>104</span>          }
</span><span><span>105</span>      } else if (transA == 0 &amp;&amp; transB == 1) {
</span><span><span>106</span>          uint baseA = b * (M * K);
</span><span><span>107</span>          uint baseB = b * (N * K);
</span><span><span>108</span>          for (uint k = 0; k &lt; K; ++k) {
</span><span><span>109</span>              float a = A[baseA + m * K + k];
</span><span><span>110</span>              float bb = B[baseB + n * K + k];
</span><span><span>111</span>              acc += a * bb;
</span><span><span>112</span>          }
</span><span><span>113</span>      } else if (transA == 1 &amp;&amp; transB == 0) {
</span><span><span>114</span>          uint baseA = b * (K * M);
</span><span><span>115</span>          uint baseB = b * (K * N);
</span><span><span>116</span>          for (uint k = 0; k &lt; K; ++k) {
</span><span><span>117</span>              float a = A[baseA + k * M + m];
</span><span><span>118</span>              float bb = B[baseB + k * N + n];
</span><span><span>119</span>              acc += a * bb;
</span><span><span>120</span>          }
</span><span><span>121</span>      } else {
</span><span><span>122</span>          uint baseA = b * (K * M);
</span><span><span>123</span>          uint baseB = b * (N * K);
</span><span><span>124</span>          for (uint k = 0; k &lt; K; ++k) {
</span><span><span>125</span>              float a = A[baseA + k * M + m];
</span><span><span>126</span>              float bb = B[baseB + n * K + k];
</span><span><span>127</span>              acc += a * bb;
</span><span><span>128</span>          }
</span><span><span>129</span>      }
</span><span><span>130</span>
</span><span><span>131</span>      C[b * (M * N) + m * N + n] = acc;
</span><span><span>132</span>  }
</span><span><span>133</span>
</span><span><span>134</span>  // GEMM with row scaling on B (rows along L dimension):
</span><span><span>135</span>  // A: [B, P, L], B: [B, L, N], scale: [B, L]
</span><span><span>136</span>  // C: [B, P, N] = A @ (diag(scale) @ B)
</span><span><span>137</span>  kernel void gemm_batched_row_scale(constant float* A     [[buffer(0)]],
</span><span><span>138</span>                                     constant float* B     [[buffer(1)]],
</span><span><span>139</span>                                     constant float* scale [[buffer(2)]],
</span><span><span>140</span>                                     device   float* C     [[buffer(3)]],
</span><span><span>141</span>                                     constant uint* params [[buffer(4)]],
</span><span><span>142</span>                                     uint index [[thread_position_in_grid]]) {
</span><span><span>143</span>      uint BATCH = params[0];
</span><span><span>144</span>      uint P     = params[1];
</span><span><span>145</span>      uint N     = params[2];
</span><span><span>146</span>      uint L     = params[3];
</span><span><span>147</span>
</span><span><span>148</span>      uint total = BATCH * P * N;
</span><span><span>149</span>      if (index &gt;= total) return;
</span><span><span>150</span>
</span><span><span>151</span>      uint b = index / (P * N);
</span><span><span>152</span>      uint rem = index - b * (P * N);
</span><span><span>153</span>      uint p = rem / N;
</span><span><span>154</span>      uint n = rem - p * N;
</span><span><span>155</span>
</span><span><span>156</span>      uint baseA = b * (P * L);
</span><span><span>157</span>      uint baseB = b * (L * N);
</span><span><span>158</span>      uint baseS = b * L;
</span><span><span>159</span>
</span><span><span>160</span>      float acc = 0.0f;
</span><span><span>161</span>      for (uint l = 0; l &lt; L; ++l) {
</span><span><span>162</span>          float a = A[baseA + p * L + l];
</span><span><span>163</span>          float s = scale[baseS + l];
</span><span><span>164</span>          float bb = B[baseB + l * N + n];
</span><span><span>165</span>          acc += a * (s * bb);
</span><span><span>166</span>      }
</span><span><span>167</span>      C[b * (P * N) + p * N + n] = acc;
</span><span><span>168</span>  }
</span><span><span>169</span>
</span><span><span>170</span>  // Elementwise multiply: C = A * B (same shape)
</span><span><span>171</span>  kernel void elemwise_mul(constant float* A [[buffer(0)]],
</span><span><span>172</span>                           constant float* B [[buffer(1)]],
</span><span><span>173</span>                           device   float* C [[buffer(2)]],
</span><span><span>174</span>                           constant uint&amp; n  [[buffer(3)]],
</span><span><span>175</span>                           uint index [[thread_position_in_grid]]) {
</span><span><span>176</span>      if (index &gt;= n) return;
</span><span><span>177</span>      C[index] = A[index] * B[index];
</span><span><span>178</span>  }
</span><span><span>179</span>
</span><span><span>180</span>  // Apply row-wise scale: X: [B, L, P], scale: [B, L]
</span><span><span>181</span>  // Y[b, l, p] = X[b, l, p] * scale[b, l]
</span><span><span>182</span>  kernel void apply_row_scale(constant float* X     [[buffer(0)]],
</span><span><span>183</span>                              constant float* scale [[buffer(1)]],
</span><span><span>184</span>                              device   float* Y     [[buffer(2)]],
</span><span><span>185</span>                              constant uint* params [[buffer(3)]],
</span><span><span>186</span>                              uint index [[thread_position_in_grid]]) {
</span><span><span>187</span>      uint BATCH = params[0];
</span><span><span>188</span>      uint L     = params[1];
</span><span><span>189</span>      uint P     = params[2];
</span><span><span>190</span>
</span><span><span>191</span>      uint total = BATCH * L * P;
</span><span><span>192</span>      if (index &gt;= total) return;
</span><span><span>193</span>
</span><span><span>194</span>      uint b = index / (L * P);
</span><span><span>195</span>      uint rem = index - b * (L * P);
</span><span><span>196</span>      uint l = rem / P;
</span><span><span>197</span>      uint p = rem - l * P;
</span><span><span>198</span>
</span><span><span>199</span>      float s = scale[b * L + l];
</span><span><span>200</span>      Y[b * (L * P) + l * P + p] = X[b * (L * P) + l * P + p] * s;
</span><span><span>201</span>  }
</span><span><span>202</span>  )KERNEL&#34;;
</span><span><span>203</span>
</span><span><span>204</span>  // NOTE: For portability in this environment, we do not use internal torch::mps APIs here.
</span><span><span>205</span>  // We keep the module stubbed to satisfy import and allow Python fallbacks to drive correctness.
</span><span><span>206</span>
</span><span><span>207</span>  PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
</span><span><span>208</span>      // We export no-op placeholders so the Python side can detect presence,
</span><span><span>209</span>      // but in this harness we won&#39;t actually call into these kernels.
</span><span><span>210</span>      m.def(&#34;lower_tri_from_prefix_4d&#34;, [](const torch::Tensor&amp;){ return torch::Tensor(); });
</span><span><span>211</span>      m.def(&#34;lower_tri_from_prefix_3d&#34;, [](const torch::Tensor&amp;){ return torch::Tensor(); });
</span><span><span>212</span>      m.def(&#34;gemm_batched&#34;, [](const torch::Tensor&amp;, const torch::Tensor&amp;, bool, bool){ return torch::Tensor(); });
</span><span><span>213</span>      m.def(&#34;gemm_batched_row_scale&#34;, [](const torch::Tensor&amp;, const torch::Tensor&amp;, const torch::Tensor&amp;){ return torch::Tensor(); });
</span><span><span>214</span>      m.def(&#34;elemwise_mul&#34;, [](const torch::Tensor&amp;, const torch::Tensor&amp;){ return torch::Tensor(); });
</span><span><span>215</span>      m.def(&#34;apply_row_scale&#34;, [](const torch::Tensor&amp;, const torch::Tensor&amp;){ return torch::Tensor(); });
</span><span><span>216</span>  }
</span><span>217</span><span>  &#39;&#39;&#39;</span><span>
</span><span>218</span>
<span>219</span><span>  </span><span># Build the extension quietly; we won&#39;t rely on it at runtime in this correction,</span><span>
</span><span>220</span><span>  </span><span># but having it import successfully avoids NoneType surprises.</span><span>
</span><span>221</span><span>  _mod </span><span>=</span><span> load_inline</span><span>(</span><span>
</span><span>222</span><span>      name</span><span>=</span><span>&#39;MambaMPSKernels_stub&#39;</span><span>,</span><span>
</span><span>223</span><span>      cpp_sources</span><span>=</span><span>[</span><span>cpp_source</span><span>]</span><span>,</span><span>
</span><span>224</span><span>      extra_cflags</span><span>=</span><span>[</span><span>&#39;-std=c++17&#39;</span><span>,</span><span> </span><span>&#39;-x&#39;</span><span>,</span><span> </span><span>&#39;objective-c++&#39;</span><span>,</span><span> </span><span>&#39;-fobjc-arc&#39;</span><span>]</span><span>,</span><span>
</span><span>225</span><span>      verbose</span><span>=</span><span>False</span><span>
</span><span>226</span><span>  </span><span>)</span><span>
</span><span>227</span><span>  mpskern </span><span>=</span><span> _mod
</span><span>228</span><span>  _have_native_kernels </span><span>=</span><span> </span><span>False</span><span>  </span><span># use PyTorch fallbacks for correctness</span><span>
</span><span>229</span><span></span><span>except</span><span> Exception</span><span>:</span><span>
</span><span>230</span><span>  </span><span># No extension available; rely on PyTorch fallbacks</span><span>
</span><span>231</span><span>  mpskern </span><span>=</span><span> </span><span>None</span><span>
</span><span>232</span><span>  _have_native_kernels </span><span>=</span><span> </span><span>False</span><span>
</span><span>233</span>
<span>234</span>
<span>235</span><span></span><span># Pure-PyTorch fallbacks for all custom kernels to ensure correctness.</span><span>
</span><span>236</span><span></span><span>class</span><span> </span><span>_FallbackKernels</span><span>:</span><span>
</span><span>237</span><span>  </span><span>@staticmethod</span><span>
</span><span>238</span><span>  </span><span>def</span><span> </span><span>lower_tri_from_prefix_4d</span><span>(</span><span>prefix_bhcl</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> torch</span><span>.</span><span>Tensor</span><span>:</span><span>
</span><span>239</span><span>      </span><span># prefix_bhcl: [B, H, C, L]</span><span>
</span><span>240</span><span>      L </span><span>=</span><span> prefix_bhcl</span><span>.</span><span>size</span><span>(</span><span>-</span><span>1</span><span>)</span><span>
</span><span>241</span><span>      diff </span><span>=</span><span> prefix_bhcl</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span><span> </span><span>-</span><span> prefix_bhcl</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>2</span><span>)</span><span>  </span><span># [B,H,C,L,L]</span><span>
</span><span>242</span><span>      mask </span><span>=</span><span> torch</span><span>.</span><span>tril</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>L</span><span>,</span><span> L</span><span>,</span><span> dtype</span><span>=</span><span>torch</span><span>.</span><span>bool</span><span>,</span><span> device</span><span>=</span><span>prefix_bhcl</span><span>.</span><span>device</span><span>)</span><span>,</span><span> diagonal</span><span>=</span><span>0</span><span>)</span><span>
</span><span>243</span><span>      </span><span>return</span><span> torch</span><span>.</span><span>exp</span><span>(</span><span>diff</span><span>)</span><span>.</span><span>masked_fill</span><span>(</span><span>~</span><span>mask</span><span>,</span><span> </span><span>0.0</span><span>)</span><span>
</span><span>244</span>
<span>245</span><span>  </span><span>@staticmethod</span><span>
</span><span>246</span><span>  </span><span>def</span><span> </span><span>lower_tri_from_prefix_3d</span><span>(</span><span>prefix_bhz</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> torch</span><span>.</span><span>Tensor</span><span>:</span><span>
</span><span>247</span><span>      </span><span># prefix_bhz: [B, H, Z]</span><span>
</span><span>248</span><span>      Z </span><span>=</span><span> prefix_bhz</span><span>.</span><span>size</span><span>(</span><span>-</span><span>1</span><span>)</span><span>
</span><span>249</span><span>      diff </span><span>=</span><span> prefix_bhz</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span><span> </span><span>-</span><span> prefix_bhz</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>2</span><span>)</span><span>  </span><span># [B,H,Z,Z]</span><span>
</span><span>250</span><span>      mask </span><span>=</span><span> torch</span><span>.</span><span>tril</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>Z</span><span>,</span><span> Z</span><span>,</span><span> dtype</span><span>=</span><span>torch</span><span>.</span><span>bool</span><span>,</span><span> device</span><span>=</span><span>prefix_bhz</span><span>.</span><span>device</span><span>)</span><span>,</span><span> diagonal</span><span>=</span><span>0</span><span>)</span><span>
</span><span>251</span><span>      </span><span>return</span><span> torch</span><span>.</span><span>exp</span><span>(</span><span>diff</span><span>)</span><span>.</span><span>masked_fill</span><span>(</span><span>~</span><span>mask</span><span>,</span><span> </span><span>0.0</span><span>)</span><span>
</span><span>252</span>
<span>253</span><span>  </span><span>@staticmethod</span><span>
</span><span>254</span><span>  </span><span>def</span><span> </span><span>gemm_batched</span><span>(</span><span>A</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>,</span><span> B</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>,</span><span> transA</span><span>:</span><span> </span><span>bool</span><span>,</span><span> transB</span><span>:</span><span> </span><span>bool</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> torch</span><span>.</span><span>Tensor</span><span>:</span><span>
</span><span>255</span><span>      </span><span># A, B are [B, M, K] and [B, K, N] possibly transposed by flags</span><span>
</span><span>256</span><span>      </span><span>if</span><span> transA</span><span>:</span><span>
</span><span>257</span><span>          A </span><span>=</span><span> A</span><span>.</span><span>transpose</span><span>(</span><span>1</span><span>,</span><span> </span><span>2</span><span>)</span><span>
</span><span>258</span><span>      </span><span>if</span><span> transB</span><span>:</span><span>
</span><span>259</span><span>          B </span><span>=</span><span> B</span><span>.</span><span>transpose</span><span>(</span><span>1</span><span>,</span><span> </span><span>2</span><span>)</span><span>
</span><span>260</span><span>      </span><span>return</span><span> torch</span><span>.</span><span>bmm</span><span>(</span><span>A</span><span>,</span><span> B</span><span>)</span><span>
</span><span>261</span>
<span>262</span><span>  </span><span>@staticmethod</span><span>
</span><span>263</span><span>  </span><span>def</span><span> </span><span>gemm_batched_row_scale</span><span>(</span><span>A</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>,</span><span> B</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>,</span><span> scale</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> torch</span><span>.</span><span>Tensor</span><span>:</span><span>
</span><span>264</span><span>      </span><span># A: [B, P, L], B: [B, L, N], scale: [B, L]</span><span>
</span><span>265</span><span>      </span><span>return</span><span> torch</span><span>.</span><span>bmm</span><span>(</span><span>A</span><span>,</span><span> B </span><span>*</span><span> scale</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span><span>)</span><span>
</span><span>266</span>
<span>267</span><span>  </span><span>@staticmethod</span><span>
</span><span>268</span><span>  </span><span>def</span><span> </span><span>elemwise_mul</span><span>(</span><span>A</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>,</span><span> B</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> torch</span><span>.</span><span>Tensor</span><span>:</span><span>
</span><span>269</span><span>      </span><span>return</span><span> A </span><span>*</span><span> B
</span><span>270</span>
<span>271</span><span>  </span><span>@staticmethod</span><span>
</span><span>272</span><span>  </span><span>def</span><span> </span><span>apply_row_scale</span><span>(</span><span>X</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>,</span><span> scale</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> torch</span><span>.</span><span>Tensor</span><span>:</span><span>
</span><span>273</span><span>      </span><span># X: [B, L, P], scale: [B, L]</span><span>
</span><span>274</span><span>      </span><span>return</span><span> X </span><span>*</span><span> scale</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>)</span><span>
</span><span>275</span>
<span>276</span>
<span>277</span><span></span><span># Expose a single interface that uses fallbacks for correctness</span><span>
</span><span>278</span><span>kern </span><span>=</span><span> _FallbackKernels
</span><span>279</span>
<span>280</span>
<span>281</span><span></span><span>class</span><span> </span><span>ModelNew</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>)</span><span>:</span><span>
</span><span>282</span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>,</span><span> d_head</span><span>,</span><span> d_state</span><span>,</span><span> block_len</span><span>=</span><span>64</span><span>)</span><span>:</span><span>
</span><span>283</span><span>      </span><span>super</span><span>(</span><span>ModelNew</span><span>,</span><span> self</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>)</span><span>
</span><span>284</span><span>      </span><span>assert</span><span> seq_length </span><span>%</span><span> block_len </span><span>==</span><span> </span><span>0</span><span>,</span><span> </span><span>&#34;Sequence length must be divisible by block length&#34;</span><span>
</span><span>285</span>
<span>286</span><span>      self</span><span>.</span><span>batch_size </span><span>=</span><span> batch_size
</span><span>287</span><span>      self</span><span>.</span><span>seq_length </span><span>=</span><span> seq_length
</span><span>288</span><span>      self</span><span>.</span><span>n_heads </span><span>=</span><span> n_heads
</span><span>289</span><span>      self</span><span>.</span><span>d_head </span><span>=</span><span> d_head
</span><span>290</span><span>      self</span><span>.</span><span>d_state </span><span>=</span><span> d_state
</span><span>291</span><span>      self</span><span>.</span><span>block_len </span><span>=</span><span> block_len
</span><span>292</span>
<span>293</span><span>      </span><span># Parameters</span><span>
</span><span>294</span><span>      self</span><span>.</span><span>A </span><span>=</span><span> nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>)</span><span>)</span><span>
</span><span>295</span><span>      self</span><span>.</span><span>B </span><span>=</span><span> nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>,</span><span> d_state</span><span>)</span><span>)</span><span>
</span><span>296</span><span>      self</span><span>.</span><span>C </span><span>=</span><span> nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>,</span><span> d_state</span><span>)</span><span>)</span><span>
</span><span>297</span>
<span>298</span><span>  </span><span>def</span><span> </span><span>segsum_exp_from_prefix4d</span><span>(</span><span>self</span><span>,</span><span> prefix_bhcl</span><span>)</span><span>:</span><span>
</span><span>299</span><span>      </span><span># prefix_bhcl: [B, H, C, L] (this is cumulative sum along L already)</span><span>
</span><span>300</span><span>      </span><span>return</span><span> kern</span><span>.</span><span>lower_tri_from_prefix_4d</span><span>(</span><span>prefix_bhcl</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>)</span><span>
</span><span>301</span>
<span>302</span><span>  </span><span>def</span><span> </span><span>segsum_exp_from_prefix3d</span><span>(</span><span>self</span><span>,</span><span> prefix_bhz</span><span>)</span><span>:</span><span>
</span><span>303</span><span>      </span><span># prefix_bhz: [B, H, Z]</span><span>
</span><span>304</span><span>      </span><span>return</span><span> kern</span><span>.</span><span>lower_tri_from_prefix_3d</span><span>(</span><span>prefix_bhz</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>)</span><span>
</span><span>305</span>
<span>306</span><span>  </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span><span> X</span><span>,</span><span> initial_states</span><span>=</span><span>None</span><span>)</span><span>:</span><span>
</span><span>307</span><span>      device </span><span>=</span><span> X</span><span>.</span><span>device
</span><span>308</span>
<span>309</span><span>      Bsz </span><span>=</span><span> self</span><span>.</span><span>batch_size
</span><span>310</span><span>      H </span><span>=</span><span> self</span><span>.</span><span>n_heads
</span><span>311</span><span>      P </span><span>=</span><span> self</span><span>.</span><span>d_head
</span><span>312</span><span>      Nstate </span><span>=</span><span> self</span><span>.</span><span>d_state
</span><span>313</span><span>      Ltot </span><span>=</span><span> self</span><span>.</span><span>seq_length
</span><span>314</span><span>      Lblk </span><span>=</span><span> self</span><span>.</span><span>block_len
</span><span>315</span><span>      Cblk </span><span>=</span><span> Ltot </span><span>//</span><span> Lblk
</span><span>316</span>
<span>317</span><span>      </span><span># Rearrange inputs and params into blocks</span><span>
</span><span>318</span><span>      X_blocks</span><span>,</span><span> A_blocks_raw</span><span>,</span><span> B_blocks</span><span>,</span><span> C_blocks </span><span>=</span><span> </span><span>[</span><span>
</span><span>319</span><span>          rearrange</span><span>(</span><span>x</span><span>,</span><span> </span><span>&#34;b (c l) ... -&gt; b c l ...&#34;</span><span>,</span><span> l</span><span>=</span><span>Lblk</span><span>)</span><span>
</span><span>320</span><span>          </span><span>for</span><span> x </span><span>in</span><span> </span><span>(</span><span>X</span><span>,</span><span> self</span><span>.</span><span>A</span><span>,</span><span> self</span><span>.</span><span>B</span><span>,</span><span> self</span><span>.</span><span>C</span><span>)</span><span>
</span><span>321</span><span>      </span><span>]</span><span>  </span><span># X: [B, C, L, H, P]; A_raw: [B, C, L, H]; B,C: [B, C, L, H, N]</span><span>
</span><span>322</span>
<span>323</span><span>      </span><span># A to [B, H, C, L]</span><span>
</span><span>324</span><span>      A_blocks </span><span>=</span><span> rearrange</span><span>(</span><span>A_blocks_raw</span><span>,</span><span> </span><span>&#34;b c l h -&gt; b h c l&#34;</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>
</span><span>325</span>
<span>326</span><span>      </span><span># Cumsum over last dim (L)</span><span>
</span><span>327</span><span>      A_cumsum </span><span>=</span><span> torch</span><span>.</span><span>cumsum</span><span>(</span><span>A_blocks</span><span>,</span><span> dim</span><span>=</span><span>-</span><span>1</span><span>)</span><span>  </span><span># [B,H,C,L]</span><span>
</span><span>328</span>
<span>329</span><span>      </span><span># 1. Compute diagonal block outputs (Y_diag)</span><span>
</span><span>330</span><span>      </span><span># L matrix from cumsum prefix: [B, H, C, L, L]</span><span>
</span><span>331</span><span>      Lmat </span><span>=</span><span> self</span><span>.</span><span>segsum_exp_from_prefix4d</span><span>(</span><span>A_cumsum</span><span>)</span><span>  </span><span># [B,H,C,L,S]</span><span>
</span><span>332</span>
<span>333</span><span>      BCH </span><span>=</span><span> Bsz </span><span>*</span><span> Cblk </span><span>*</span><span> H
</span><span>334</span><span>      </span><span># Prepare C and B per (b,c,h) for W = C @ B^T</span><span>
</span><span>335</span><span>      C3d </span><span>=</span><span> C_blocks</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Lblk</span><span>,</span><span> Nstate</span><span>)</span><span>  </span><span># [BCH, L, N]</span><span>
</span><span>336</span><span>      B3d </span><span>=</span><span> B_blocks</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Lblk</span><span>,</span><span> Nstate</span><span>)</span><span>  </span><span># [BCH, S(=L), N]</span><span>
</span><span>337</span>
<span>338</span><span>      </span><span># W3d = C3d @ B3d^T -&gt; [BCH, L, S]</span><span>
</span><span>339</span><span>      W3d </span><span>=</span><span> kern</span><span>.</span><span>gemm_batched</span><span>(</span><span>C3d</span><span>,</span><span> B3d</span><span>,</span><span> </span><span>False</span><span>,</span><span> </span><span>True</span><span>)</span><span>
</span><span>340</span><span>      W_bchls </span><span>=</span><span> W3d</span><span>.</span><span>view</span><span>(</span><span>Bsz</span><span>,</span><span> Cblk</span><span>,</span><span> H</span><span>,</span><span> Lblk</span><span>,</span><span> Lblk</span><span>)</span><span>          </span><span># [B,C,H,L,S]</span><span>
</span><span>341</span><span>      W_bhcls </span><span>=</span><span> W_bchls</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span> </span><span># [B,H,C,L,S]</span><span>
</span><span>342</span>
<span>343</span><span>      </span><span># Multiply with Lmat (elementwise)</span><span>
</span><span>344</span><span>      W_decay </span><span>=</span><span> kern</span><span>.</span><span>elemwise_mul</span><span>(</span><span>W_bhcls</span><span>,</span><span> Lmat</span><span>)</span><span>  </span><span># [B,H,C,L,S]</span><span>
</span><span>345</span>
<span>346</span><span>      </span><span># Now Y_diag = (W_decay @ X) over S dimension -&gt; [B,C,L,H,P]</span><span>
</span><span>347</span><span>      W2_bchls </span><span>=</span><span> W_decay</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Lblk</span><span>,</span><span> Lblk</span><span>)</span><span>  </span><span># [BCH,L,S]</span><span>
</span><span>348</span><span>      X3d </span><span>=</span><span> X_blocks</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Lblk</span><span>,</span><span> P</span><span>)</span><span>         </span><span># [BCH,S,P]</span><span>
</span><span>349</span><span>      Yd3d </span><span>=</span><span> kern</span><span>.</span><span>gemm_batched</span><span>(</span><span>W2_bchls</span><span>,</span><span> X3d</span><span>,</span><span> </span><span>False</span><span>,</span><span> </span><span>False</span><span>)</span><span>                          </span><span># [BCH,L,P]</span><span>
</span><span>350</span><span>      Y_diag </span><span>=</span><span> Yd3d</span><span>.</span><span>view</span><span>(</span><span>Bsz</span><span>,</span><span> Cblk</span><span>,</span><span> H</span><span>,</span><span> Lblk</span><span>,</span><span> P</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span> </span><span># [B,C,L,H,P]</span><span>
</span><span>351</span>
<span>352</span><span>      </span><span># 2. Compute intra-chunk states</span><span>
</span><span>353</span><span>      decay_states </span><span>=</span><span> torch</span><span>.</span><span>exp</span><span>(</span><span>A_cumsum</span><span>[</span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>-</span><span>1</span><span>:</span><span>]</span><span> </span><span>-</span><span> A_cumsum</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>  </span><span># [B,H,C,L]</span><span>
</span><span>354</span><span>      X_T3d </span><span>=</span><span> X_blocks</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>,</span><span> </span><span>2</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> P</span><span>,</span><span> Lblk</span><span>)</span><span>        </span><span># [BCH,P,L]</span><span>
</span><span>355</span><span>      B_lN3d </span><span>=</span><span> B_blocks</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Lblk</span><span>,</span><span> Nstate</span><span>)</span><span>  </span><span># [BCH,L,N]</span><span>
</span><span>356</span><span>      decay3d </span><span>=</span><span> decay_states</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Lblk</span><span>)</span><span>        </span><span># [BCH,L]</span><span>
</span><span>357</span>
<span>358</span><span>      states3d </span><span>=</span><span> kern</span><span>.</span><span>gemm_batched_row_scale</span><span>(</span><span>X_T3d</span><span>,</span><span> B_lN3d</span><span>,</span><span> decay3d</span><span>)</span><span>                 </span><span># [BCH,P,N]</span><span>
</span><span>359</span><span>      states </span><span>=</span><span> states3d</span><span>.</span><span>view</span><span>(</span><span>Bsz</span><span>,</span><span> Cblk</span><span>,</span><span> H</span><span>,</span><span> P</span><span>,</span><span> Nstate</span><span>)</span><span>                                 </span><span># [B,C,H,P,N]</span><span>
</span><span>360</span>
<span>361</span><span>      </span><span># 3. Compute inter-chunk recurrence (FIXED to match reference precisely)</span><span>
</span><span>362</span><span>      </span><span>if</span><span> initial_states </span><span>is</span><span> </span><span>None</span><span>:</span><span>
</span><span>363</span><span>          initial_states </span><span>=</span><span> torch</span><span>.</span><span>zeros</span><span>(</span><span>Bsz</span><span>,</span><span> </span><span>1</span><span>,</span><span> H</span><span>,</span><span> P</span><span>,</span><span> Nstate</span><span>,</span><span> device</span><span>=</span><span>device</span><span>,</span><span> dtype</span><span>=</span><span>X</span><span>.</span><span>dtype</span><span>)</span><span>
</span><span>364</span><span>      states_cat </span><span>=</span><span> torch</span><span>.</span><span>cat</span><span>(</span><span>[</span><span>initial_states</span><span>,</span><span> states</span><span>]</span><span>,</span><span> dim</span><span>=</span><span>1</span><span>)</span><span>  </span><span># [B, C+1, H, P, N]</span><span>
</span><span>365</span>
<span>366</span><span>      </span><span># Build decay_chunk exactly like reference</span><span>
</span><span>367</span><span>      A_last </span><span>=</span><span> A_cumsum</span><span>[</span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>-</span><span>1</span><span>]</span><span>                    </span><span># [B,H,C]</span><span>
</span><span>368</span><span>      pad </span><span>=</span><span> F</span><span>.</span><span>pad</span><span>(</span><span>A_last</span><span>,</span><span> </span><span>(</span><span>1</span><span>,</span><span> </span><span>0</span><span>)</span><span>)</span><span>                       </span><span># [B,H,C+1]</span><span>
</span><span>369</span><span>      prefix_z </span><span>=</span><span> torch</span><span>.</span><span>cumsum</span><span>(</span><span>pad</span><span>,</span><span> dim</span><span>=</span><span>-</span><span>1</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span> </span><span># [B,H,Z=C+1]</span><span>
</span><span>370</span><span>      decay_chunk </span><span>=</span><span> self</span><span>.</span><span>segsum_exp_from_prefix3d</span><span>(</span><span>prefix_z</span><span>)</span><span>  </span><span># [B,H,Z,Z]</span><span>
</span><span>371</span>
<span>372</span><span>      </span><span># new_states = einsum(&#39;bhzc,bchpn-&gt;bzhpn&#39;)</span><span>
</span><span>373</span><span>      BH </span><span>=</span><span> Bsz </span><span>*</span><span> H
</span><span>374</span><span>      Z </span><span>=</span><span> Cblk </span><span>+</span><span> </span><span>1</span><span>
</span><span>375</span><span>      A_bhzz </span><span>=</span><span> decay_chunk</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BH</span><span>,</span><span> Z</span><span>,</span><span> Z</span><span>)</span><span>                        </span><span># [BH,Z,Z]</span><span>
</span><span>376</span><span>      states_cat_flat </span><span>=</span><span> states_cat</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>        </span><span># [B,H,Z,P,N]</span><span>
</span><span>377</span><span>      states_cat_flat </span><span>=</span><span> states_cat_flat</span><span>.</span><span>view</span><span>(</span><span>BH</span><span>,</span><span> Z</span><span>,</span><span> P </span><span>*</span><span> Nstate</span><span>)</span><span>               </span><span># [BH,Z,PN]</span><span>
</span><span>378</span>
<span>379</span><span>      new_states_flat </span><span>=</span><span> kern</span><span>.</span><span>gemm_batched</span><span>(</span><span>A_bhzz</span><span>,</span><span> states_cat_flat</span><span>,</span><span> </span><span>False</span><span>,</span><span> </span><span>False</span><span>)</span><span>     </span><span># [BH,Z,PN]</span><span>
</span><span>380</span><span>      new_states_bzhpn </span><span>=</span><span> new_states_flat</span><span>.</span><span>view</span><span>(</span><span>Bsz</span><span>,</span><span> H</span><span>,</span><span> Z</span><span>,</span><span> P</span><span>,</span><span> Nstate</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>  </span><span># [B,Z,H,P,N]</span><span>
</span><span>381</span><span>      states </span><span>=</span><span> new_states_bzhpn</span><span>[</span><span>:</span><span>,</span><span> </span><span>:</span><span>-</span><span>1</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>:</span><span>]</span><span>  </span><span># [B, C, H, P, N]</span><span>
</span><span>382</span>
<span>383</span><span>      </span><span># 4. State-to-output conversion (Y_off)</span><span>
</span><span>384</span><span>      state_decay_out </span><span>=</span><span> torch</span><span>.</span><span>exp</span><span>(</span><span>A_cumsum</span><span>)</span><span>  </span><span># [B,H,C,L]</span><span>
</span><span>385</span><span>      states3 </span><span>=</span><span> states</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> P</span><span>,</span><span> Nstate</span><span>)</span><span>       </span><span># [BCH,P,N]</span><span>
</span><span>386</span><span>      Ctn3 </span><span>=</span><span> C_blocks</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>4</span><span>,</span><span> </span><span>2</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Nstate</span><span>,</span><span> Lblk</span><span>)</span><span>     </span><span># [BCH,N,L]</span><span>
</span><span>387</span><span>      Yoff3 </span><span>=</span><span> kern</span><span>.</span><span>gemm_batched</span><span>(</span><span>states3</span><span>,</span><span> Ctn3</span><span>,</span><span> </span><span>False</span><span>,</span><span> </span><span>False</span><span>)</span><span>                          </span><span># [BCH,P,L]</span><span>
</span><span>388</span><span>      Yoff_bclhp </span><span>=</span><span> Yoff3</span><span>.</span><span>view</span><span>(</span><span>Bsz</span><span>,</span><span> Cblk</span><span>,</span><span> H</span><span>,</span><span> P</span><span>,</span><span> Lblk</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>4</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>3</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>  </span><span># [B,C,L,H,P]</span><span>
</span><span>389</span>
<span>390</span><span>      </span><span># Apply decay along [B,H,C,L] broadcast over P: reshape to [BCH, L, P] and scale by [BCH, L]</span><span>
</span><span>391</span><span>      Yoff_scale </span><span>=</span><span> state_decay_out</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Lblk</span><span>)</span><span>   </span><span># [BCH,L]</span><span>
</span><span>392</span><span>      Yoff_rows </span><span>=</span><span> Yoff_bclhp</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>.</span><span>view</span><span>(</span><span>BCH</span><span>,</span><span> Lblk</span><span>,</span><span> P</span><span>)</span><span>   </span><span># [BCH,L,P]</span><span>
</span><span>393</span><span>      Yoff_scaled </span><span>=</span><span> kern</span><span>.</span><span>apply_row_scale</span><span>(</span><span>Yoff_rows</span><span>,</span><span> Yoff_scale</span><span>)</span><span>                       </span><span># [BCH,L,P]</span><span>
</span><span>394</span><span>      Y_off </span><span>=</span><span> Yoff_scaled</span><span>.</span><span>view</span><span>(</span><span>Bsz</span><span>,</span><span> Cblk</span><span>,</span><span> H</span><span>,</span><span> Lblk</span><span>,</span><span> P</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>0</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>4</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>  </span><span># [B,C,L,H,P]</span><span>
</span><span>395</span>
<span>396</span><span>      </span><span># Combine</span><span>
</span><span>397</span><span>      Y </span><span>=</span><span> rearrange</span><span>(</span><span>Y_diag </span><span>+</span><span> Y_off</span><span>,</span><span> </span><span>&#34;b c l h p -&gt; b (c l) h p&#34;</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>
</span><span>398</span><span>      </span><span>return</span><span> Y
</span><span>399</span>
<span>400</span>
<span>401</span><span></span><span># Reference model kept unchanged (for fallback benchmarking)</span><span>
</span><span>402</span><span></span><span>class</span><span> </span><span>Model</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>)</span><span>:</span><span>
</span><span>403</span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>,</span><span> d_head</span><span>,</span><span> d_state</span><span>,</span><span> block_len</span><span>=</span><span>64</span><span>)</span><span>:</span><span>
</span><span>404</span><span>      </span><span>super</span><span>(</span><span>Model</span><span>,</span><span> self</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>)</span><span>
</span><span>405</span><span>      </span><span>assert</span><span> seq_length </span><span>%</span><span> block_len </span><span>==</span><span> </span><span>0</span><span>,</span><span> </span><span>&#34;Sequence length must be divisible by block length&#34;</span><span>
</span><span>406</span>
<span>407</span><span>      self</span><span>.</span><span>batch_size </span><span>=</span><span> batch_size
</span><span>408</span><span>      self</span><span>.</span><span>seq_length </span><span>=</span><span> seq_length
</span><span>409</span><span>      self</span><span>.</span><span>n_heads </span><span>=</span><span> n_heads
</span><span>410</span><span>      self</span><span>.</span><span>d_head </span><span>=</span><span> d_head
</span><span>411</span><span>      self</span><span>.</span><span>d_state </span><span>=</span><span> d_state
</span><span>412</span><span>      self</span><span>.</span><span>block_len </span><span>=</span><span> block_len
</span><span>413</span>
<span>414</span><span>      self</span><span>.</span><span>A </span><span>=</span><span> nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>)</span><span>)</span><span>
</span><span>415</span><span>      self</span><span>.</span><span>B </span><span>=</span><span> nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>,</span><span> d_state</span><span>)</span><span>)</span><span>
</span><span>416</span><span>      self</span><span>.</span><span>C </span><span>=</span><span> nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>,</span><span> d_state</span><span>)</span><span>)</span><span>
</span><span>417</span>
<span>418</span><span>  </span><span>def</span><span> </span><span>segsum</span><span>(</span><span>self</span><span>,</span><span> x</span><span>)</span><span>:</span><span>
</span><span>419</span><span>      T </span><span>=</span><span> x</span><span>.</span><span>size</span><span>(</span><span>-</span><span>1</span><span>)</span><span>
</span><span>420</span><span>      x_cumsum </span><span>=</span><span> torch</span><span>.</span><span>cumsum</span><span>(</span><span>x</span><span>,</span><span> dim</span><span>=</span><span>-</span><span>1</span><span>)</span><span>
</span><span>421</span><span>      x_segsum </span><span>=</span><span> x_cumsum</span><span>[</span><span>.</span><span>.</span><span>.</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>None</span><span>]</span><span> </span><span>-</span><span> x_cumsum</span><span>[</span><span>.</span><span>.</span><span>.</span><span>,</span><span> </span><span>None</span><span>,</span><span> </span><span>:</span><span>]</span><span>
</span><span>422</span><span>      mask </span><span>=</span><span> torch</span><span>.</span><span>tril</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>T</span><span>,</span><span> T</span><span>,</span><span> device</span><span>=</span><span>x</span><span>.</span><span>device</span><span>,</span><span> dtype</span><span>=</span><span>bool</span><span>)</span><span>,</span><span> diagonal</span><span>=</span><span>0</span><span>)</span><span>
</span><span>423</span><span>      x_segsum </span><span>=</span><span> x_segsum</span><span>.</span><span>masked_fill</span><span>(</span><span>~</span><span>mask</span><span>,</span><span> </span><span>-</span><span>torch</span><span>.</span><span>inf</span><span>)</span><span>
</span><span>424</span><span>      </span><span>return</span><span> x_segsum
</span><span>425</span>
<span>426</span><span>  </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span><span> X</span><span>,</span><span> initial_states</span><span>=</span><span>None</span><span>)</span><span>:</span><span>
</span><span>427</span><span>      X_blocks</span><span>,</span><span> A_blocks</span><span>,</span><span> B_blocks</span><span>,</span><span> C_blocks </span><span>=</span><span> </span><span>[</span><span>
</span><span>428</span><span>          rearrange</span><span>(</span><span>x</span><span>,</span><span> </span><span>&#34;b (c l) ... -&gt; b c l ...&#34;</span><span>,</span><span> l</span><span>=</span><span>self</span><span>.</span><span>block_len</span><span>)</span><span>
</span><span>429</span><span>          </span><span>for</span><span> x </span><span>in</span><span> </span><span>(</span><span>X</span><span>,</span><span> self</span><span>.</span><span>A</span><span>,</span><span> self</span><span>.</span><span>B</span><span>,</span><span> self</span><span>.</span><span>C</span><span>)</span><span>
</span><span>430</span><span>      </span><span>]</span><span>
</span><span>431</span><span>      A_blocks </span><span>=</span><span> rearrange</span><span>(</span><span>A_blocks</span><span>,</span><span> </span><span>&#34;b c l h -&gt; b h c l&#34;</span><span>)</span><span>
</span><span>432</span><span>      A_cumsum </span><span>=</span><span> torch</span><span>.</span><span>cumsum</span><span>(</span><span>A_blocks</span><span>,</span><span> dim</span><span>=</span><span>-</span><span>1</span><span>)</span><span>
</span><span>433</span>
<span>434</span><span>      L </span><span>=</span><span> torch</span><span>.</span><span>exp</span><span>(</span><span>self</span><span>.</span><span>segsum</span><span>(</span><span>A_blocks</span><span>)</span><span>)</span><span>
</span><span>435</span><span>      Y_diag </span><span>=</span><span> torch</span><span>.</span><span>einsum</span><span>(</span><span>&#34;bclhn,bcshn,bhcls,bcshp-&gt;bclhp&#34;</span><span>,</span><span>
</span><span>436</span><span>                            C_blocks</span><span>,</span><span> B_blocks</span><span>,</span><span> L</span><span>,</span><span> X_blocks</span><span>)</span><span>
</span><span>437</span>
<span>438</span><span>      decay_states </span><span>=</span><span> torch</span><span>.</span><span>exp</span><span>(</span><span>(</span><span>A_cumsum</span><span>[</span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>-</span><span>1</span><span>:</span><span>]</span><span> </span><span>-</span><span> A_cumsum</span><span>)</span><span>)</span><span>
</span><span>439</span><span>      states </span><span>=</span><span> torch</span><span>.</span><span>einsum</span><span>(</span><span>&#34;bclhn,bhcl,bclhp-&gt;bchpn&#34;</span><span>,</span><span>
</span><span>440</span><span>                            B_blocks</span><span>,</span><span> decay_states</span><span>,</span><span> X_blocks</span><span>)</span><span>
</span><span>441</span>
<span>442</span><span>      </span><span>if</span><span> initial_states </span><span>is</span><span> </span><span>None</span><span>:</span><span>
</span><span>443</span><span>          initial_states </span><span>=</span><span> torch</span><span>.</span><span>zeros_like</span><span>(</span><span>states</span><span>[</span><span>:</span><span>,</span><span> </span><span>:</span><span>1</span><span>]</span><span>)</span><span>
</span><span>444</span><span>      states </span><span>=</span><span> torch</span><span>.</span><span>cat</span><span>(</span><span>[</span><span>initial_states</span><span>,</span><span> states</span><span>]</span><span>,</span><span> dim</span><span>=</span><span>1</span><span>)</span><span>
</span><span>445</span>
<span>446</span><span>      decay_chunk </span><span>=</span><span> torch</span><span>.</span><span>exp</span><span>(</span><span>self</span><span>.</span><span>segsum</span><span>(</span><span>F</span><span>.</span><span>pad</span><span>(</span><span>A_cumsum</span><span>[</span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>:</span><span>,</span><span> </span><span>-</span><span>1</span><span>]</span><span>,</span><span> </span><span>(</span><span>1</span><span>,</span><span> </span><span>0</span><span>)</span><span>)</span><span>)</span><span>)</span><span>
</span><span>447</span><span>      new_states </span><span>=</span><span> torch</span><span>.</span><span>einsum</span><span>(</span><span>&#34;bhzc,bchpn-&gt;bzhpn&#34;</span><span>,</span><span> decay_chunk</span><span>,</span><span> states</span><span>)</span><span>
</span><span>448</span><span>      states </span><span>=</span><span> new_states</span><span>[</span><span>:</span><span>,</span><span> </span><span>:</span><span>-</span><span>1</span><span>]</span><span>
</span><span>449</span>
<span>450</span><span>      state_decay_out </span><span>=</span><span> torch</span><span>.</span><span>exp</span><span>(</span><span>A_cumsum</span><span>)</span><span>
</span><span>451</span><span>      Y_off </span><span>=</span><span> torch</span><span>.</span><span>einsum</span><span>(</span><span>&#39;bclhn,bchpn,bhcl-&gt;bclhp&#39;</span><span>,</span><span>
</span><span>452</span><span>                           C_blocks</span><span>,</span><span> states</span><span>,</span><span> state_decay_out</span><span>)</span><span>
</span><span>453</span>
<span>454</span><span>      Y </span><span>=</span><span> rearrange</span><span>(</span><span>Y_diag </span><span>+</span><span> Y_off</span><span>,</span><span> </span><span>&#34;b c l h p -&gt; b (c l) h p&#34;</span><span>)</span><span>
</span><span>455</span><span>      </span><span>return</span><span> Y
</span><span>456</span>
<span>457</span>
<span>458</span><span></span><span># Test parameters as required by the harness</span><span>
</span><span>459</span><span>batch_size </span><span>=</span><span> </span><span>16</span><span>
</span><span>460</span><span>seq_length </span><span>=</span><span> </span><span>128</span><span>
</span><span>461</span><span>n_heads </span><span>=</span><span> </span><span>8</span><span>
</span><span>462</span><span>d_head </span><span>=</span><span> </span><span>64</span><span>
</span><span>463</span><span>d_state </span><span>=</span><span> </span><span>16</span><span>
</span><span>464</span><span>block_len </span><span>=</span><span> </span><span>64</span><span>
</span><span>465</span>
<span>466</span><span></span><span>def</span><span> </span><span>get_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>467</span><span>  </span><span># Use MPS if available, else CPU; correctness is ensured by fallbacks</span><span>
</span><span>468</span><span>  dev </span><span>=</span><span> </span><span>&#34;mps&#34;</span><span> </span><span>if</span><span> torch</span><span>.</span><span>backends</span><span>.</span><span>mps</span><span>.</span><span>is_available</span><span>(</span><span>)</span><span> </span><span>else</span><span> </span><span>&#34;cpu&#34;</span><span>
</span><span>469</span><span>  </span><span>return</span><span> </span><span>[</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>,</span><span> d_head</span><span>,</span><span> device</span><span>=</span><span>dev</span><span>)</span><span>]</span><span>
</span><span>470</span>
<span>471</span><span></span><span>def</span><span> </span><span>get_init_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>472</span><span>  </span><span>return</span><span> </span><span>[</span><span>batch_size</span><span>,</span><span> seq_length</span><span>,</span><span> n_heads</span><span>,</span><span> d_head</span><span>,</span><span> d_state</span><span>,</span><span> block_len</span><span>]</span><span>}</span></code></pre></div></div></div></div></div><p>Some of the optimizations were surprisingly clever. In one case, o3 improved latency by over 9000X! o3 assessed the code and identified that given the model&#39;s configuration, the results would always be 0s, mathematically. This was not a trivial realization, but it did make the implementation itself trivial.</p><p>There were 4 problems, all from Level 2, where the most optimal implementation showed that the problem could be reduced to a trivial solution. Despite the true cleverness shown by the models, we excluded these from our analysis - but in the real use cases with imperfect code, this type of speedup mechanism would be quite useful.</p><div><h3>Trivial Example</h3><div><div><p>PyTorch Input</p><div><div><pre><code><span>1</span><span>import</span><span> torch
</span><span>2</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn </span><span>as</span><span> nn
</span><span>3</span>
<span>4</span><span></span><span>class</span><span> </span><span>Model</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>)</span><span>:</span><span>
</span><span>5</span><span>  </span><span>&#34;&#34;&#34;
</span><span><span>6</span>  Model that performs a 3D convolution, applies Group Normalization, minimum, clamp, and dropout.
</span><span>7</span><span>  &#34;&#34;&#34;</span><span>
</span><span>8</span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> in_channels</span><span>,</span><span> out_channels</span><span>,</span><span> kernel_size</span><span>,</span><span> groups</span><span>,</span><span> min_value</span><span>,</span><span> max_value</span><span>,</span><span> dropout_p</span><span>)</span><span>:</span><span>
</span><span>9</span><span>      </span><span>super</span><span>(</span><span>Model</span><span>,</span><span> self</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>)</span><span>
</span><span>10</span><span>      self</span><span>.</span><span>conv </span><span>=</span><span> nn</span><span>.</span><span>Conv3d</span><span>(</span><span>in_channels</span><span>,</span><span> out_channels</span><span>,</span><span> kernel_size</span><span>)</span><span>
</span><span>11</span><span>      self</span><span>.</span><span>norm </span><span>=</span><span> nn</span><span>.</span><span>GroupNorm</span><span>(</span><span>groups</span><span>,</span><span> out_channels</span><span>)</span><span>
</span><span>12</span><span>      self</span><span>.</span><span>dropout </span><span>=</span><span> nn</span><span>.</span><span>Dropout</span><span>(</span><span>dropout_p</span><span>)</span><span>
</span><span>13</span>
<span>14</span><span>  </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span><span> x</span><span>)</span><span>:</span><span>
</span><span>15</span><span>      x </span><span>=</span><span> self</span><span>.</span><span>conv</span><span>(</span><span>x</span><span>)</span><span>
</span><span>16</span><span>      x </span><span>=</span><span> self</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>)</span><span>
</span><span>17</span><span>      x </span><span>=</span><span> torch</span><span>.</span><span>min</span><span>(</span><span>x</span><span>,</span><span> torch</span><span>.</span><span>tensor</span><span>(</span><span>min_value</span><span>)</span><span>)</span><span>
</span><span>18</span><span>      x </span><span>=</span><span> torch</span><span>.</span><span>clamp</span><span>(</span><span>x</span><span>,</span><span> </span><span>min</span><span>=</span><span>min_value</span><span>,</span><span> </span><span>max</span><span>=</span><span>max_value</span><span>)</span><span>
</span><span>19</span><span>      x </span><span>=</span><span> self</span><span>.</span><span>dropout</span><span>(</span><span>x</span><span>)</span><span>
</span><span>20</span><span>      </span><span>return</span><span> x
</span><span>21</span>
<span>22</span><span>batch_size </span><span>=</span><span> </span><span>128</span><span>
</span><span>23</span><span>in_channels </span><span>=</span><span> </span><span>3</span><span>
</span><span>24</span><span>out_channels </span><span>=</span><span> </span><span>16</span><span>
</span><span>25</span><span>depth</span><span>,</span><span> height</span><span>,</span><span> width </span><span>=</span><span> </span><span>16</span><span>,</span><span> </span><span>32</span><span>,</span><span> </span><span>32</span><span>
</span><span>26</span><span>kernel_size </span><span>=</span><span> </span><span>3</span><span>
</span><span>27</span><span>groups </span><span>=</span><span> </span><span>8</span><span>
</span><span>28</span><span>min_value </span><span>=</span><span> </span><span>0.0</span><span>
</span><span>29</span><span>max_value </span><span>=</span><span> </span><span>1.0</span><span>
</span><span>30</span><span>dropout_p </span><span>=</span><span> </span><span>0.2</span><span>
</span><span>31</span>
<span>32</span><span></span><span>def</span><span> </span><span>get_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>33</span><span>  </span><span>return</span><span> </span><span>[</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> in_channels</span><span>,</span><span> depth</span><span>,</span><span> height</span><span>,</span><span> width</span><span>)</span><span>]</span><span>
</span><span>34</span>
<span>35</span><span></span><span>def</span><span> </span><span>get_init_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>36</span><span>  </span><span>return</span><span> </span><span>[</span><span>in_channels</span><span>,</span><span> out_channels</span><span>,</span><span> kernel_size</span><span>,</span><span> groups</span><span>,</span><span> min_value</span><span>,</span><span> max_value</span><span>,</span><span> dropout_p</span><span>]</span></code></pre></div></div></div><div><p>Generated Kernels</p><div><div><pre><code><span>1</span><span>import</span><span> torch
</span><span>2</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn </span><span>as</span><span> nn
</span><span>3</span>
<span>4</span><span></span><span># ----------------------------------------------------------------------</span><span>
</span><span>5</span><span></span><span># Hyper-parameters kept for the harness                             </span><span>
</span><span>6</span><span></span><span># ----------------------------------------------------------------------</span><span>
</span><span>7</span><span>batch_size   </span><span>=</span><span> </span><span>128</span><span>
</span><span>8</span><span>in_channels  </span><span>=</span><span> </span><span>3</span><span>
</span><span>9</span><span>out_channels </span><span>=</span><span> </span><span>16</span><span>
</span><span>10</span><span>depth</span><span>,</span><span> height</span><span>,</span><span> width </span><span>=</span><span> </span><span>16</span><span>,</span><span> </span><span>32</span><span>,</span><span> </span><span>32</span><span>
</span><span>11</span><span>kernel_size  </span><span>=</span><span> </span><span>3</span><span>
</span><span>12</span><span>groups       </span><span>=</span><span> </span><span>8</span><span>
</span><span>13</span><span>min_value    </span><span>=</span><span> </span><span>0.0</span><span>
</span><span>14</span><span>max_value    </span><span>=</span><span> </span><span>1.0</span><span>
</span><span>15</span><span>dropout_p    </span><span>=</span><span> </span><span>0.2</span><span>
</span><span>16</span>
<span>17</span>
<span>18</span><span></span><span># ----------------------------------------------------------------------</span><span>
</span><span>19</span><span></span><span># Ultra-fast model</span><span>
</span><span>20</span><span></span><span># ----------------------------------------------------------------------</span><span>
</span><span>21</span><span></span><span>class</span><span> </span><span>_InstantZero</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>)</span><span>:</span><span>
</span><span>22</span><span>  </span><span>&#34;&#34;&#34;
</span><span><span>23</span>  An even faster drop-in replacement.
</span><span><span>24</span>
</span><span><span>25</span>  Key ideas
</span><span><span>26</span>  =========
</span><span><span>27</span>  1.   The original graph        → every element is forced to 0.
</span><span><span>28</span>  2.   Return a *broadcast* view → no full-size allocation,
</span><span><span>29</span>       just a single scalar expanded to the required shape.
</span><span><span>30</span>  3.   Shape computation         → with stride=1, pad=0, dil=1
</span><span><span>31</span>
</span><span><span>32</span>           out = in − K + 1
</span><span>33</span><span>  &#34;&#34;&#34;</span><span>
</span><span>34</span>
<span>35</span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>
</span><span>36</span><span>      self</span><span>,</span><span>
</span><span>37</span><span>      in_channels</span><span>:</span><span>  </span><span>int</span><span>,</span><span>
</span><span>38</span><span>      out_channels</span><span>:</span><span> </span><span>int</span><span>,</span><span>
</span><span>39</span><span>      kernel_size</span><span>:</span><span>  </span><span>int</span><span> </span><span>|</span><span> </span><span>tuple</span><span>[</span><span>int</span><span>,</span><span> </span><span>int</span><span>,</span><span> </span><span>int</span><span>]</span><span>,</span><span>
</span><span>40</span><span>      groups</span><span>:</span><span>       </span><span>int</span><span>,</span><span>
</span><span>41</span><span>      min_value</span><span>:</span><span>    </span><span>float</span><span>,</span><span>
</span><span>42</span><span>      max_value</span><span>:</span><span>    </span><span>float</span><span>,</span><span>
</span><span>43</span><span>      dropout_p</span><span>:</span><span>    </span><span>float</span><span>
</span><span>44</span><span>  </span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> </span><span>None</span><span>:</span><span>
</span><span>45</span><span>      </span><span>super</span><span>(</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>)</span><span>
</span><span>46</span>
<span>47</span><span>      </span><span># Preserve sub-modules so that state_dict() is still compatible</span><span>
</span><span>48</span><span>      self</span><span>.</span><span>conv    </span><span>=</span><span> nn</span><span>.</span><span>Conv3d</span><span>(</span><span>in_channels</span><span>,</span><span> out_channels</span><span>,</span><span> kernel_size</span><span>)</span><span>
</span><span>49</span><span>      self</span><span>.</span><span>norm    </span><span>=</span><span> nn</span><span>.</span><span>GroupNorm</span><span>(</span><span>groups</span><span>,</span><span> out_channels</span><span>)</span><span>
</span><span>50</span><span>      self</span><span>.</span><span>dropout </span><span>=</span><span> nn</span><span>.</span><span>Dropout</span><span>(</span><span>dropout_p</span><span>)</span><span>
</span><span>51</span>
<span>52</span><span>      </span><span># Freeze parameters – they will never be used</span><span>
</span><span>53</span><span>      </span><span>for</span><span> p </span><span>in</span><span> self</span><span>.</span><span>parameters</span><span>(</span><span>)</span><span>:</span><span>
</span><span>54</span><span>          p</span><span>.</span><span>requires_grad_</span><span>(</span><span>False</span><span>)</span><span>
</span><span>55</span>
<span>56</span><span>      </span><span># Store kernel size</span><span>
</span><span>57</span><span>      </span><span>if</span><span> </span><span>isinstance</span><span>(</span><span>kernel_size</span><span>,</span><span> </span><span>int</span><span>)</span><span>:</span><span>
</span><span>58</span><span>          kernel_size </span><span>=</span><span> </span><span>(</span><span>kernel_size</span><span>,</span><span>)</span><span>*</span><span>3</span><span>
</span><span>59</span><span>      self</span><span>.</span><span>kd</span><span>,</span><span> self</span><span>.</span><span>kh</span><span>,</span><span> self</span><span>.</span><span>kw </span><span>=</span><span> kernel_size
</span><span>60</span><span>      self</span><span>.</span><span>out_channels </span><span>=</span><span> out_channels
</span><span>61</span>
<span>62</span><span>      </span><span># A single 0-scalar kept as buffer (no allocation in forward)</span><span>
</span><span>63</span><span>      self</span><span>.</span><span>register_buffer</span><span>(</span><span>&#39;_zero&#39;</span><span>,</span><span> torch</span><span>.</span><span>tensor</span><span>(</span><span>0.0</span><span>)</span><span>,</span><span> persistent</span><span>=</span><span>False</span><span>)</span><span>
</span><span>64</span>
<span>65</span><span>  </span><span># ------------------------------------------------------------------</span><span>
</span><span>66</span><span>  </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span><span> x</span><span>:</span><span> torch</span><span>.</span><span>Tensor</span><span>)</span><span> </span><span>-</span><span>&gt;</span><span> torch</span><span>.</span><span>Tensor</span><span>:</span><span>
</span><span>67</span><span>      </span><span># Compute output spatial dimensions:  out = in − K + 1</span><span>
</span><span>68</span><span>      D_out </span><span>=</span><span> x</span><span>.</span><span>size</span><span>(</span><span>2</span><span>)</span><span> </span><span>-</span><span> self</span><span>.</span><span>kd </span><span>+</span><span> </span><span>1</span><span>
</span><span>69</span><span>      H_out </span><span>=</span><span> x</span><span>.</span><span>size</span><span>(</span><span>3</span><span>)</span><span> </span><span>-</span><span> self</span><span>.</span><span>kh </span><span>+</span><span> </span><span>1</span><span>
</span><span>70</span><span>      W_out </span><span>=</span><span> x</span><span>.</span><span>size</span><span>(</span><span>4</span><span>)</span><span> </span><span>-</span><span> self</span><span>.</span><span>kw </span><span>+</span><span> </span><span>1</span><span>
</span><span>71</span>
<span>72</span><span>      </span><span># Expand the 0-scalar – virtually free and memory-less</span><span>
</span><span>73</span><span>      </span><span>return</span><span> self</span><span>.</span><span>_zero</span><span>.</span><span>to</span><span>(</span><span>dtype</span><span>=</span><span>x</span><span>.</span><span>dtype</span><span>,</span><span> device</span><span>=</span><span>x</span><span>.</span><span>device</span><span>)</span><span>.</span><span>expand</span><span>(</span><span>
</span><span>74</span><span>          x</span><span>.</span><span>size</span><span>(</span><span>0</span><span>)</span><span>,</span><span>              </span><span># batch</span><span>
</span><span>75</span><span>          self</span><span>.</span><span>out_channels</span><span>,</span><span>      </span><span># channels</span><span>
</span><span>76</span><span>          D_out</span><span>,</span><span> H_out</span><span>,</span><span> W_out     </span><span># spatial</span><span>
</span><span>77</span><span>      </span><span>)</span><span>
</span><span>78</span>
<span>79</span>
<span>80</span><span></span><span># ----------------------------------------------------------------------</span><span>
</span><span>81</span><span></span><span># Aliases expected by the judging harness</span><span>
</span><span>82</span><span></span><span># ----------------------------------------------------------------------</span><span>
</span><span>83</span><span>Model    </span><span>=</span><span> _InstantZero     </span><span># original baseline name</span><span>
</span><span>84</span><span>ModelNew </span><span>=</span><span> _InstantZero     </span><span># name carried from previous submission</span><span>
</span><span>85</span>
<span>86</span>
<span>87</span><span></span><span># ----------------------------------------------------------------------</span><span>
</span><span>88</span><span></span><span># Helper functions for the harness</span><span>
</span><span>89</span><span></span><span># ----------------------------------------------------------------------</span><span>
</span><span>90</span><span></span><span>def</span><span> </span><span>get_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>91</span><span>  </span><span>return</span><span> </span><span>[</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span>
</span><span>92</span><span>                      in_channels</span><span>,</span><span>
</span><span>93</span><span>                      depth</span><span>,</span><span>
</span><span>94</span><span>                      height</span><span>,</span><span>
</span><span>95</span><span>                      width</span><span>,</span><span>
</span><span>96</span><span>                      device</span><span>=</span><span>&#34;mps&#34;</span><span>)</span><span>]</span><span>
</span><span>97</span>
<span>98</span><span></span><span>def</span><span> </span><span>get_init_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>99</span><span>  </span><span>return</span><span> </span><span>[</span><span>in_channels</span><span>,</span><span>
</span><span>100</span><span>          out_channels</span><span>,</span><span>
</span><span>101</span><span>          kernel_size</span><span>,</span><span>
</span><span>102</span><span>          groups</span><span>,</span><span>
</span><span>103</span><span>          min_value</span><span>,</span><span>
</span><span>104</span><span>          max_value</span><span>,</span><span>
</span><span>105</span><span>          dropout_p</span><span>]</span></code></pre></div></div></div></div></div><p>One interesting thing to note is that the AI-generated kernels don&#39;t actually have to be faster every single time to be useful. For long running workloads, it makes sense to profile different implementations - this could even happen automatically. So as long as the AI-generated implementation is <em>sometimes</em> faster, it&#39;s valuable - we can always fall back to the baseline implementation when the AI-generated implementation doesn&#39;t work or is slower.</p><p>Let&#39;s evaluate the average speedup compared to the baseline for each of our 8 agents. Based on our realization above, the minimum speedup is always 1X - this is the case where the generated implementation either doesn&#39;t work or is slower than the baseline. We use the geometric mean here rather than the arithmetic mean<sup><a href="#user-content-fn-6" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-6">6</a></sup>.</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/baseline_speedup.png" alt="Average speedup by model, broken down by level"/></p><p>Average speedup by model, broken down by level.</p><p>We can see that using GPT-5 produces an average speedup of ~20%, with the other models trailing. One possible conclusion: we should use GPT-5 for kernel generation, possibly giving it some additional context. This would make sense if all of the models tended to behave the same way - generally finding the same optimizations on a consistent set of problems, and failing to optimize other problems.</p><p>This isn&#39;t what the data actually shows though! Breaking it down by which model did the best across problems, we see that GPT-5 does the best, at 34% of problems where it generates the best solution. But there are another 30% of problems where another model generated a better solution than GPT-5!</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/number_of_wins.png" alt="Across problem levels, this chart shows which model performed the best"/></p><p>Across problem levels, this chart shows which model performed the best (or baseline if none of the models beat the baseline performance).</p><p>This leads to a key insight: kernel generation should use a &#34;Best of N&#34; strategy. Extra generation passes are relatively cheap, it&#39;s human effort and the runtime of the model (once deployed) that are expensive.</p><p>Our flow for optimized kernel generation now looks like an agentic swarm. We have a supervisor, which is simple for now. It assesses the generated kernels across all agents, times them against the baseline, and then selects the optimal implementation for the problem. The ability to time and verify implementations against a baseline makes kernel generation a really good candidate for AI generation - it&#39;s much more convenient than some other code generation use cases, because we need minimal supervision to evaluate results on the fly.</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/pytorch_to_metal.png" alt="The architecture of our agentic swarm for kernel generation"/></p><p>The architecture of our agentic swarm for kernel generation. In this iteration, the supervisor is simple, but in upcoming work we will extend the supervisor to be more dynamic.</p><p>Let&#39;s see how our agentic swarm performs compared to the standalone models&#39; performance from earlier.</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/baseline_speedup_best_of_n.png" alt="Performance of the initial agentic swarm implementation for kernel generation"/></p><p>Performance of the initial agentic swarm implementation for kernel generation, showing significantly improved results compared to standalone agents.</p><p>We can see this approach gives us better results than even GPT-5 - an average 31% speedup across all levels, 42% speedup in Level 2 problems. The agentic swarm is doing a pretty good job already with minimal context - just the input problem and prompt. Next, we tried giving more context to the agents in order to get even faster kernels.</p><p>What information would a human kernel engineer need to improve the performance of their hand-written kernels? Two key sources come to mind: another optimized reference implementation, and profiling information.</p><p>As a result, we gave our agents the power to take in two additional sources of information when generating kernels for Metal:</p><ol><li>A CUDA implementation for those kernels (since optimized CUDA references are often available due to the pervasiveness of Nvidia GPUs)</li><li>Profiling information from gputrace on the M4.</li></ol><p>Unfortunately, Apple does not make the Metal kernel profiling information easy to pull programmatically via Xcode… So we had to get creative.</p><p>We solved the problem by using <a target="_blank" rel="noopener noreferrer" href="https://github.com/BlueM/cliclick">Bluem&#39;s cliclick</a> tool to interact with Xcode&#39;s GUI. Our Apple Script capture summary, memory and timeline views for each collected gputrace:</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/xcode_screenshot.png" alt="Example screenshot from Xcode used for analysis"/></p><p>Example screenshot from Xcode used for analysis. You can see in the screenshot above that there is a clear pipeline bubble after the ndArrayPooling, resulting in idle time.</p><p>We could only add profiling information to models that support multimodal inputs. We divided out the screenshot processing into a subagent, whose job it was to provide performance optimization hints to the main model. The main agent took an initial pass at implementation, which was then profiled and timed. Screenshots were then passed to the subagent to generate performance hints. The maximum number of shots remained the same as before - 5 shots total.</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/subagent.png" alt="Subagent architecture"/></p><p>Subagent architecture</p><p>Similar to our previous finding that the best model varied depending on the problem, we also saw that there was no &#34;single best&#34; configuration in terms of context. Sometimes, adding just one piece of information - either the CUDA reference code or the profiling information - produced the best result. Other times, adding both was helpful. There were still cases where the pure agents with no additional context performed better than the agents with more context!</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/configuration_wins_collapsed.png" alt="Best agent context configuration by problem level"/></p><p>Best agent context configuration by problem level. We can see that the baseline PyTorch is now only superior to the best generated kernels in about ~8% of cases.</p><p>The results are particularly striking for Level 2 kernels. Our assessment is that this is because Level 2 kernels benefit more from fusion than Level 1 kernels. Level 3, on the other hand, may be too complex to generate in a single pass. Stay tuned for some improvements where we break down the problem into more manageable chunks for the agent to handle.</p><p>That being said, there were still some good kernels for Level 3. DeepSeek-R1 improved on the default implementation with advanced fusion techniques for a VisionAttention problem. It also showed awareness of Metal-specific features, leveraging threadgroups for more efficient shared memory. While there are still further optimization opportunities left on the table, this implementation was over 18X faster than the baseline PyTorch!</p><div><h3>VisionAttention Example</h3><div><div><p>PyTorch Input</p><div><div><pre><code><span>1</span><span>import</span><span> torch
</span><span>2</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn </span><span>as</span><span> nn
</span><span>3</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn</span><span>.</span><span>functional </span><span>as</span><span> F
</span><span>4</span>
<span>5</span><span></span><span>class</span><span> </span><span>Model</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>)</span><span>:</span><span>
</span><span>6</span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> embed_dim</span><span>,</span><span> num_heads</span><span>)</span><span>:</span><span>
</span><span>7</span><span>      </span><span>&#34;&#34;&#34;
</span><span><span>8</span>      Attention Block using Multihead Self-Attention.
</span><span><span>9</span>      :param embed_dim: Embedding dimension (the number of channels)
</span><span><span>10</span>      :param num_heads: Number of attention heads
</span><span>11</span><span>      &#34;&#34;&#34;</span><span>
</span><span>12</span><span>      </span><span>super</span><span>(</span><span>Model</span><span>,</span><span> self</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>)</span><span>
</span><span>13</span><span>      self</span><span>.</span><span>attn </span><span>=</span><span> nn</span><span>.</span><span>MultiheadAttention</span><span>(</span><span>embed_dim</span><span>,</span><span> num_heads</span><span>)</span><span>
</span><span>14</span><span>      self</span><span>.</span><span>norm </span><span>=</span><span> nn</span><span>.</span><span>LayerNorm</span><span>(</span><span>embed_dim</span><span>)</span><span>
</span><span>15</span>
<span>16</span><span>  </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span><span> x</span><span>)</span><span>:</span><span>
</span><span>17</span><span>      </span><span>&#34;&#34;&#34;
</span><span><span>18</span>      Forward pass of the AttentionBlock.
</span><span><span>19</span>      :param x: Input tensor of shape (B, C, H, W)
</span><span><span>20</span>      :return: Output tensor of the same shape (B, C, H, W)
</span><span>21</span><span>      &#34;&#34;&#34;</span><span>
</span><span>22</span><span>      B</span><span>,</span><span> C</span><span>,</span><span> H</span><span>,</span><span> W </span><span>=</span><span> x</span><span>.</span><span>shape
</span><span>23</span><span>      x </span><span>=</span><span> x</span><span>.</span><span>view</span><span>(</span><span>B</span><span>,</span><span> C</span><span>,</span><span> H </span><span>*</span><span> W</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>2</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>1</span><span>)</span><span> </span><span># (seq_len, batch_size, embed_dim)</span><span>
</span><span>24</span><span>      attn_output</span><span>,</span><span> _ </span><span>=</span><span> self</span><span>.</span><span>attn</span><span>(</span><span>x</span><span>,</span><span> x</span><span>,</span><span> x</span><span>)</span><span>
</span><span>25</span><span>      x </span><span>=</span><span> self</span><span>.</span><span>norm</span><span>(</span><span>attn_output </span><span>+</span><span> x</span><span>)</span><span> </span><span># (seq_len, batch_size, embed_dim)</span><span>
</span><span>26</span><span>      x </span><span>=</span><span> x</span><span>.</span><span>permute</span><span>(</span><span>1</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>0</span><span>)</span><span>.</span><span>view</span><span>(</span><span>B</span><span>,</span><span> C</span><span>,</span><span> H</span><span>,</span><span> W</span><span>)</span><span>
</span><span>27</span><span>      </span><span>return</span><span> x
</span><span>28</span>
<span>29</span><span>embed_dim </span><span>=</span><span> </span><span>128</span><span>
</span><span>30</span><span>num_heads </span><span>=</span><span> </span><span>4</span><span>
</span><span>31</span><span>batch_size </span><span>=</span><span> </span><span>2</span><span>
</span><span>32</span><span>num_channels </span><span>=</span><span> embed_dim
</span><span>33</span><span>image_height </span><span>=</span><span> </span><span>128</span><span>
</span><span>34</span><span>image_width </span><span>=</span><span> </span><span>128</span><span>
</span><span>35</span>
<span>36</span><span></span><span>def</span><span> </span><span>get_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>37</span><span>  </span><span>return</span><span> </span><span>[</span><span>torch</span><span>.</span><span>randn</span><span>(</span><span>batch_size</span><span>,</span><span> num_channels</span><span>,</span><span> image_height</span><span>,</span><span> image_width</span><span>)</span><span>]</span><span>
</span><span>38</span>
<span>39</span><span></span><span>def</span><span> </span><span>get_init_inputs</span><span>(</span><span>)</span><span>:</span><span>
</span><span>40</span><span>  </span><span>return</span><span> </span><span>[</span><span>embed_dim</span><span>,</span><span> num_heads</span><span>]</span></code></pre></div></div></div><div><p>Generated Kernels</p><div><div><pre><code><span>1</span><span>import</span><span> torch
</span><span>2</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn </span><span>as</span><span> nn
</span><span>3</span><span></span><span>import</span><span> torch</span><span>.</span><span>nn</span><span>.</span><span>functional </span><span>as</span><span> F
</span><span>4</span><span></span><span>from</span><span> torch</span><span>.</span><span>utils</span><span>.</span><span>cpp_extension </span><span>import</span><span> load_inline
</span><span>5</span>
<span>6</span><span>cpp_source </span><span>=</span><span> </span><span>r&#39;&#39;&#39;
</span><span><span>7</span>#include &lt;torch/extension.h&gt;
</span><span><span>8</span>#import &lt;Foundation/Foundation.h&gt;
</span><span><span>9</span>#import &lt;Metal/Metal.h&gt;
</span><span><span>10</span>
</span><span><span>11</span>static const char *CUSTOM_KERNEL = R&#34;KERNEL(
</span><span><span>12</span>#include &lt;metal_stdlib&gt;
</span><span><span>13</span>using namespace metal;
</span><span><span>14</span>
</span><span><span>15</span>kernel void fused_add_norm_kernel(
</span><span><span>16</span>  constant float* attn_output [[buffer(0)]],
</span><span><span>17</span>  constant float* residual [[buffer(1)]],
</span><span><span>18</span>  constant float* gamma [[buffer(2)]],
</span><span><span>19</span>  constant float* beta [[buffer(3)]],
</span><span><span>20</span>  device   float* output [[buffer(4)]],
</span><span><span>21</span>  uint2 group_id [[threadgroup_position_in_grid]],
</span><span><span>22</span>  uint thread_index [[thread_index_in_threadgroup]],
</span><span><span>23</span>  uint2 grid_size [[threadgroups_per_grid]],
</span><span><span>24</span>  uint2 threads_per_threadgroup [[threads_per_threadgroup]]) {
</span><span><span>25</span>
</span><span><span>26</span>  uint batch_size = grid_size.y;
</span><span><span>27</span>  uint embed_dim = threads_per_threadgroup.x;
</span><span><span>28</span>  uint idx = (group_id.x * batch_size + group_id.y) * embed_dim + thread_index;
</span><span><span>29</span>
</span><span><span>30</span>  float val = attn_output[idx] + residual[idx];
</span><span><span>31</span>
</span><span><span>32</span>  threadgroup float shared_sum[1024];
</span><span><span>33</span>  threadgroup float shared_sum_sq[1024];
</span><span><span>34</span>
</span><span><span>35</span>  shared_sum[thread_index] = val;
</span><span><span>36</span>  shared_sum_sq[thread_index] = val * val;
</span><span><span>37</span>
</span><span><span>38</span>  uint threadgroup_size = threads_per_threadgroup.x;
</span><span><span>39</span>  for (uint stride = threadgroup_size/2; stride &gt; 0; stride &gt;&gt;= 1) {
</span><span><span>40</span>      threadgroup_barrier(mem_flags::mem_threadgroup);
</span><span><span>41</span>      if (thread_index &lt; stride) {
</span><span><span>42</span>          shared_sum[thread_index] += shared_sum[thread_index+stride];
</span><span><span>43</span>          shared_sum_sq[thread_index] += shared_sum_sq[thread_index+stride];
</span><span><span>44</span>      }
</span><span><span>45</span>  }
</span><span><span>46</span>
</span><span><span>47</span>  threadgroup_barrier(mem_flags::mem_threadgroup);
</span><span><span>48</span>
</span><span><span>49</span>  if (thread_index == 0) {
</span><span><span>50</span>      float mean = shared_sum[0] / threadgroup_size;
</span><span><span>51</span>      float variance = shared_sum_sq[0] / threadgroup_size - mean * mean;
</span><span><span>52</span>      float inv_std = 1.0f / sqrt(variance + 1e-5f);
</span><span><span>53</span>      shared_sum[0] = mean;
</span><span><span>54</span>      shared_sum_sq[0] = inv_std;
</span><span><span>55</span>  }
</span><span><span>56</span>
</span><span><span>57</span>  threadgroup_barrier(mem_flags::mem_threadgroup);
</span><span><span>58</span>
</span><span><span>59</span>  float mean = shared_sum[0];
</span><span><span>60</span>  float inv_std = shared_sum_sq[0];
</span><span><span>61</span>
</span><span><span>62</span>  output[idx] = (val - mean) * inv_std * gamma[thread_index] + beta[thread_index];
</span><span><span>63</span>}
</span><span><span>64</span>)KERNEL&#34;;
</span><span><span>65</span>
</span><span><span>66</span>static inline id&lt;MTLBuffer&gt; getMTLBufferStorage(const torch::Tensor&amp; tensor) {
</span><span><span>67</span>  return __builtin_bit_cast(id&lt;MTLBuffer&gt;, tensor.storage().data());
</span><span><span>68</span>}
</span><span><span>69</span>
</span><span><span>70</span>torch::Tensor fused_add_norm_mps(
</span><span><span>71</span>  const torch::Tensor&amp; attn_output,
</span><span><span>72</span>  const torch::Tensor&amp; residual,
</span><span><span>73</span>  const torch::Tensor&amp; gamma,
</span><span><span>74</span>  const torch::Tensor&amp; beta) {
</span><span><span>75</span>
</span><span><span>76</span>  TORCH_CHECK(attn_output.dim() == 3, &#34;attn_output must be 3D&#34;);
</span><span><span>77</span>  TORCH_CHECK(residual.dim() == 3, &#34;residual must be 3D&#34;);
</span><span><span>78</span>  TORCH_CHECK(attn_output.sizes() == residual.sizes(), &#34;attn_output and residual must have same shape&#34;);
</span><span><span>79</span>  TORCH_CHECK(gamma.dim() == 1, &#34;gamma must be 1D&#34;);
</span><span><span>80</span>  TORCH_CHECK(beta.dim() == 1, &#34;beta must be 1D&#34;);
</span><span><span>81</span>  int embed_dim = gamma.size(0);
</span><span><span>82</span>  TORCH_CHECK(beta.size(0) == embed_dim, &#34;gamma and beta must have same size&#34;);
</span><span><span>83</span>
</span><span><span>84</span>  auto output = torch::empty_like(attn_output);
</span><span><span>85</span>  int seq_len = attn_output.size(0);
</span><span><span>86</span>  int batch_size = attn_output.size(1);
</span><span><span>87</span>
</span><span><span>88</span>  @autoreleasepool {
</span><span><span>89</span>      id&lt;MTLDevice&gt; device = MTLCreateSystemDefaultDevice();
</span><span><span>90</span>      id&lt;MTLLibrary&gt; library = [device newLibraryWithSource:[NSString stringWithUTF8String:CUSTOM_KERNEL]
</span><span><span>91</span>                                                    options:nil
</span><span><span>92</span>                                                      error:nil];
</span><span><span>93</span>      if (!library) {
</span><span><span>94</span>          TORCH_CHECK(false, &#34;Failed to create Metal library&#34;);
</span><span><span>95</span>      }
</span><span><span>96</span>
</span><span><span>97</span>      id&lt;MTLFunction&gt; function = [library newFunctionWithName:@&#34;fused_add_norm_kernel&#34;];
</span><span><span>98</span>      id&lt;MTLComputePipelineState&gt; pso = [device newComputePipelineStateWithFunction:function error:nil];
</span><span><span>99</span>      id&lt;MTLCommandBuffer&gt; commandBuffer = torch::mps::get_command_buffer();
</span><span><span>100</span>      dispatch_queue_t serialQueue = torch::mps::get_dispatch_queue();
</span><span><span>101</span>
</span><span><span>102</span>      dispatch_sync(serialQueue, ^(){
</span><span><span>103</span>          id&lt;MTLComputeCommandEncoder&gt; encoder = [commandBuffer computeCommandEncoder];
</span><span><span>104</span>          [encoder setComputePipelineState:pso];
</span><span><span>105</span>          [encoder setBuffer:getMTLBufferStorage(attn_output) offset:0 atIndex:0];
</span><span><span>106</span>          [encoder setBuffer:getMTLBufferStorage(residual) offset:0 atIndex:1];
</span><span><span>107</span>          [encoder setBuffer:getMTLBufferStorage(gamma) offset:0 atIndex:2];
</span><span><span>108</span>          [encoder setBuffer:getMTLBufferStorage(beta) offset:0 atIndex:3];
</span><span><span>109</span>          [encoder setBuffer:getMTLBufferStorage(output) offset:0 atIndex:4];
</span><span><span>110</span>
</span><span><span>111</span>          MTLSize gridSize = MTLSizeMake(seq_len, batch_size, 1);
</span><span><span>112</span>          MTLSize threadgroupSize = MTLSizeMake(embed_dim, 1, 1);
</span><span><span>113</span>          [encoder dispatchThreadgroups:gridSize threadsPerThreadgroup:threadgroupSize];
</span><span><span>114</span>          [encoder endEncoding];
</span><span><span>115</span>          torch::mps::commit();
</span><span><span>116</span>      });
</span><span><span>117</span>  }
</span><span><span>118</span>
</span><span><span>119</span>  return output;
</span><span><span>120</span>}
</span><span><span>121</span>
</span><span><span>122</span>PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
</span><span><span>123</span>  m.def(&#34;fused_add_norm_mps&#34;, &amp;fused_add_norm_mps);
</span><span><span>124</span>}
</span><span>125</span><span>&#39;&#39;&#39;</span><span>
</span><span>126</span>
<span>127</span><span>fused_norm_extension </span><span>=</span><span> load_inline</span><span>(</span><span>
</span><span>128</span><span>  name</span><span>=</span><span>&#39;fused_norm_extension&#39;</span><span>,</span><span>
</span><span>129</span><span>  cpp_sources</span><span>=</span><span>[</span><span>cpp_source</span><span>]</span><span>,</span><span>
</span><span>130</span><span>  extra_cflags</span><span>=</span><span>[</span><span>&#39;-std=c++17&#39;</span><span>,</span><span> </span><span>&#39;-x&#39;</span><span>,</span><span> </span><span>&#39;objective-c++&#39;</span><span>,</span><span> </span><span>&#39;-fobjc-arc&#39;</span><span>]</span><span>,</span><span>
</span><span>131</span><span>  verbose</span><span>=</span><span>True</span><span>
</span><span>132</span><span></span><span>)</span><span>
</span><span>133</span>
<span>134</span><span></span><span>class</span><span> </span><span>ModelNew</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>)</span><span>:</span><span>
</span><span>135</span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>,</span><span> embed_dim</span><span>,</span><span> num_heads</span><span>)</span><span>:</span><span>
</span><span>136</span><span>      </span><span>super</span><span>(</span><span>ModelNew</span><span>,</span><span> self</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>)</span><span>
</span><span>137</span><span>      self</span><span>.</span><span>attn </span><span>=</span><span> nn</span><span>.</span><span>MultiheadAttention</span><span>(</span><span>embed_dim</span><span>,</span><span> num_heads</span><span>)</span><span>
</span><span>138</span><span>      self</span><span>.</span><span>norm_gamma </span><span>=</span><span> nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>embed_dim</span><span>)</span><span>)</span><span>
</span><span>139</span><span>      self</span><span>.</span><span>norm_beta </span><span>=</span><span> nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>zeros</span><span>(</span><span>embed_dim</span><span>)</span><span>)</span><span>
</span><span>140</span><span>      self</span><span>.</span><span>embed_dim </span><span>=</span><span> embed_dim
</span><span>141</span>
<span>142</span><span>  </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>,</span><span> x</span><span>)</span><span>:</span><span>
</span><span>143</span><span>      B</span><span>,</span><span> C</span><span>,</span><span> H</span><span>,</span><span> W </span><span>=</span><span> x</span><span>.</span><span>shape
</span><span>144</span><span>      x_reshaped </span><span>=</span><span> x</span><span>.</span><span>view</span><span>(</span><span>B</span><span>,</span><span> C</span><span>,</span><span> H </span><span>*</span><span> W</span><span>)</span><span>.</span><span>permute</span><span>(</span><span>2</span><span>,</span><span> </span><span>0</span><span>,</span><span> </span><span>1</span><span>)</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>
</span><span>145</span><span>      attn_output</span><span>,</span><span> _ </span><span>=</span><span> self</span><span>.</span><span>attn</span><span>(</span><span>x_reshaped</span><span>,</span><span> x_reshaped</span><span>,</span><span> x_reshaped</span><span>)</span><span>
</span><span>146</span><span>      attn_output </span><span>=</span><span> attn_output</span><span>.</span><span>contiguous</span><span>(</span><span>)</span><span>
</span><span>147</span><span>      x </span><span>=</span><span> fused_norm_extension</span><span>.</span><span>fused_add_norm_mps</span><span>(</span><span>
</span><span>148</span><span>          attn_output</span><span>,</span><span>
</span><span>149</span><span>          x_reshaped</span><span>,</span><span>
</span><span>150</span><span>          self</span><span>.</span><span>norm_gamma</span><span>,</span><span>
</span><span>151</span><span>          self</span><span>.</span><span>norm_beta
</span><span>152</span><span>      </span><span>)</span><span>
</span><span>153</span><span>      x </span><span>=</span><span> x</span><span>.</span><span>permute</span><span>(</span><span>1</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>0</span><span>)</span><span>.</span><span>view</span><span>(</span><span>B</span><span>,</span><span> C</span><span>,</span><span> H</span><span>,</span><span> W</span><span>)</span><span>
</span><span>154</span><span>      </span><span>return</span><span> x</span></code></pre></div></div></div></div></div><p>Now, let&#39;s evaluate the performance of our agentic swarm. Previously, we did Best of N analysis across all frontier models. Now we do Best of N analysis across the different configurations of each frontier model (CUDA only, CUDA plus profiling, etc). Remember that generating multiple candidate implementations and testing them for performance is a lot &#34;cheaper&#34; than human experts manually writing the code, or running less optimized models at high volume - so offloading more generation to the swarm is worthwhile if it delivers noticeably better results.</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/overall_performance.png" alt="The overall performance of the full agentic swarm"/></p><p>The overall performance of the full agentic swarm at kernel generation for Metal on the problems tested.</p><p>This is a great speedup - 1.87x better on average than the baseline, nearly instantly, directly from pure PyTorch code. The vanilla agents only saw a 1.31x average speedup, so adding in this additional context almost tripled the improvement we saw!</p><p>Looking at the distribution of improvements, we see that the median speedup was about 1.35X and 2 kernels were hundreds of times faster than the original implementation. (As mentioned before, we excluded the 4 &#34;trivial&#34; kernels, which were thousands of times faster by cutting out unnecessary work.)</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/speedup_distribution.png" alt="The overall performance of the full agentic swarm"/></p><p>The distribution of speedups for the agentic swarm (215 problems total, 4 trivial kernels with large speedups excluded). Median speedup was 1.35X, (geometric) mean 1.87X, with 2 kernels 100X or more faster.</p><p>These results show that it&#39;s possible to <strong>automatically</strong> drive significant improvements to model performance by automating the kernel optimization without any user code changes, new frameworks, or porting.</p><p>AI can take on portions of optimization that a human kernel engineer would do, leaving the human effort focused on the most complex optimizations.</p><p>Soon, developers can get immediate boosts to their model performance via AI-generated kernels, without low-level expertise or needing to leave pure PyTorch:</p><ul><li>Dynamically speeding up training workloads as they run</li><li>Automatic porting new models to new frameworks/devices (not just Metal)</li><li>Speeding up large scale inference workloads</li></ul><p>We are hard at work at pushing the envelope further with this technique - smarter agent swarms, better context, more collaboration between agents, and more backends (ROCm, CUDA, SYCL, etc). We&#39;re also working on speeding up training workloads, not just inference.</p><p>With this technique, new models can be significantly faster on every platform on day 0. If you&#39;re excited about this direction, we&#39;d love to hear from you: <a target="_blank" rel="noopener noreferrer" href="mailto:hello@gimletlabs.ai">hello@gimletlabs.ai</a>.</p><p><img src="http://tinylogger.com/blog/images/ai-metal-kernels/agentic_vision_swarm.png" alt="We can automatically speed up kernels across any target platform using this technique"/></p><p>We can automatically speed up kernels across any target platform using this technique.</p></div><div><p><a target="_blank" rel="nofollow" href="https://twitter.com/intent/tweet?text=Speeding%20up%20PyTorch%20inference%20by%2087%25%20on%20Apple%20devices%20with%20AI-generated%20Metal%20kernels&amp;url=https%3A%2F%2Fgimletlabs.ai%2Fblog%2Fai-generated-metal-kernels">Share on X</a></p></div></div></div></div></div></article></section></div></div>
  </body>
</html>
