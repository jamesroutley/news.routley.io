<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://riskmusings.substack.com/p/openais-war-of-succession-and-whats">Original</a>
    <h1>OpenAI’s War of Succession - and What’s Next</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><em>I’m sated with Thanksgiving leftovers, catching up on sleep, and I’ll just say: the OpenAI drama this past week gives me massive empathy for people who enjoy keeping up with the Kardashians. If only the stakes weren’t so high! Onward…. </em></p><p><span>OpenAI CEO Sam Altman woke up Friday, November 17th, expecting a different day than the one he got. Also, most of us probably live in a world that is fundamentally different than we think it is. We won’t know </span><em>how </em><span>it’s different for months or even perhaps years, but OpenAI will probably play a role.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024" width="512" height="512" data-attrs="{&#34;src&#34;:&#34;https://substack-post-media.s3.amazonaws.com/public/images/1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024&#34;,&#34;srcNoWatermark&#34;:null,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:&#34;normal&#34;,&#34;height&#34;:512,&#34;width&#34;:512,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:false,&#34;topImage&#34;:true,&#34;internalRedirect&#34;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1975fae8-dab0-42a0-a493-eb9383fd899b_1024x1024 1456w" sizes="100vw" fetchpriority="high"/></picture></div></a></figure></div><p>A brief rundown of the main events that engulfed OpenAI last weekend (if you know all this stuff already, skip to the next section): </p><ul><li><p><strong>Friday, November 17th:</strong><span> Something triggered OpenAI’s board of directors, including chief scientist Ilya Sutskever, to fire Altman and remove company president Greg Brockman from his board chair role. CTO Mira Murati was named interim CEO. Brockman resigned his role as president in response to Altman’s firing and his own demotion.</span></p></li><li><p><strong>Over the weekend:</strong><span> Altman and Brockman teamed up and met with Satya Nadella while also entertaining overtures from OpenAI management (notably </span><em>not </em><span>from</span><em> </em><span>the board) to return to their former roles. By the end of the weekend, the board had instead installed a new interim CEO named Emmett Shear, while Altman and Brockman had agreed in principle to move to Microsoft and work on AI there, possibly along with senior employees who had also resigned in solidarity. </span></p></li><li><p><strong>Overnight Sunday into Monday:</strong><span> the rest of the OpenAI employees displayed staggering solidarity, too, with more than 500 (and ultimately more than 700) signing a letter saying they would resign if the board did not itself resign and reinstate Altman and Brockman. Nadella said Microsoft would support whatever Sam and Greg decided, and chief scientist Ilya Sutskever publicly changed his mind and signed the letter. The board hung on for another day before conceding to a reorganization late Tuesday night. </span></p></li><li><p><strong>As of Wednesday, November 22nd:</strong><span> Altman and Brockman are back. The board now consists of Bret Taylor, Larry Summers, and Adam D’Angelo (the sole holdover from the old board). Former board members Helen Toner and Tasha McCauley are out, and Ilya Sutskever is still chief scientist but is not on the board (and neither are Altman and Brockman). Mira Murati is still CTO. </span></p></li></ul><p><strong>So: meet the new boss, same as the old boss? </strong></p><p>I very much doubt it. This shuffling has interesting implications. </p><p><span>First and probably foremost, the board’s power was mainly based on the </span><em>threat</em><span> that it could do what it did: fire Sam Altman. But it shot its shot, and </span><a href="https://www.kansascity.com/sports/spt-columns-blogs/for-petes-sake/article279607534.html" rel="">unlike with Travis Kelce and Taylor Swift</a><span>, it didn’t work out, not even a little. We now find ourselves in much the same situation as before, but somewhat worse from a risk management perspective. </span></p><p><span>Will the board ever be able to pull that maneuver again? My personal view is probably not, barring major malfeasance, </span><a href="https://www.cnbc.com/2023/11/18/openai-exec-addresses-ceo-sam-altmans-firing-in-note-to-employees-read-the-full-memo.html" rel="">which a widely reported internal OpenAI memo said did not take place this time</a><span> … so why did they pull the trigger? Whatever the board’s plan was, as I wrote last weekend, </span><a href="https://riskmusings.substack.com/p/where-plans-and-playbooks-meet-reality" rel="">no plan survives contact with reality intact</a><span>. This one detonated on impact. </span></p><p><span>One big takeaway is that the board’s silence after issuing their initial statement was </span><em>not</em><span> golden. Honest explanations would have helped a lot. This is often true with corporate and political machinations: in the absence of a clear message with provable rationales, rumors fly, which can get employees and investors up in arms and set the stage for chaos to flourish. (As a side note, this was also true for actions taken to stabilize markets in the 2008 financial crisis! I understand the impulse not to spark panic, but honest explanations of, “We are doing X to shore up Y, not to give handouts to Z for their own sake,” could have gone a long way. I’m not a fan of secrecy at the expense of stability. Secrecy exists to </span><em>maintain </em><span>stability sometimes, but taken too far, it can have the </span><em>opposite </em><span>effect. People can cope with a lot more information than power-brokers think they can.)</span></p><p><strong>Lingering questions</strong></p><p>Given the still-somewhat-cloudy view-from-the-outside of what happened, I do wonder why Ilya Sutskever initially agreed to the ouster, then changed his mind. I think he did not expect the magnitude of the backlash, and he decided he wanted to keep working on transformative AI, and also decided that it could be more dangerous to have Altman, Brockman, and several hundred all-in employees working in perhaps less-constrained ways on transformative AI at Microsoft. But those are just my speculations. </p><p><span>To be fair, this is not to say Altman is necessarily full-speed-ahead. OpenAI </span><a href="https://promptengineering.org/gpt-4-10-crucial-insights-and-their-implications-on-our-future/" rel="">has held back</a><span> </span><a href="https://arxiv.org/abs/2303.08774" rel="">on releasing advances</a><span> in the past if they felt controls weren’t ready</span><span>, </span><a href="https://www.cnn.com/2023/05/16/tech/sam-altman-openai-congress/index.html" rel="">Altman has been open publicly about the various potential risks of AI advancement, and he is seemingly open to reasonable regulation</a><span>. These past behaviors are heartening factors.</span></p><p>Most importantly at this point, where do we go from here? In the future, humans cannot make decisions involving artificial general intelligence (AGI) this way, by careening from chaos to chaos, and expect it to go at all well. Humans need to do better. </p><p><span>Otherwise, we really are just shuffling deck chairs on the </span><em>Titanic</em><span>.</span></p><p><strong>Speaking of (weak) artificial general intelligence….</strong></p><p>Yes. As a meme circulating on Twitter goes: </p><p><a href="https://twitter.com/AviSchiffmann/status/1726439597113287108" rel="">What</a><span> </span><a href="https://twitter.com/apples_jimmy/status/1726459275885817938" rel="">did</a><span> </span><a href="https://twitter.com/pmarca/status/1727928602417914318" rel="">Ilya</a><span> </span><a href="https://twitter.com/parmy/status/1727438112643797417" rel="">see</a><span>? </span></p><p><a href="https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/" rel="">Reuters broke a story on the eve of Thanksgiving about a possible capabilities breakthrough earlier this year at OpenAI</a><span>, called Q* (Q star). Nothing is confirmed about Q*, but the Reuters article stated that it possibly can reason better about math problems than current LLMs. If this capability: a.) exists at all; and b.) scales, it could potentially pave the way for AI that’s less of a remixer and more of a discoverer. </span></p><p><span>Think of </span><a href="https://www.youtube.com/watch?v=lAl28d6tbko" rel="">a turbo-powered blender</a><span> (worth clicking for humor), which chops up raw ingredients and serves them up in a new form, as an analogy for current AI LLMs. Think of </span><a href="https://en.wikipedia.org/wiki/Alexander_Fleming" rel="">Alexander Fleming’s discovery and the subsequent industrial production of penicillin</a><span> as an analog for possible future AI breakthroughs. These are only analogies and are not perfect, but they hopefully convey the main difference.</span></p><p>I find the Q* theory appealing because it could potentially explain several things, particularly the board’s sudden and seemingly radical action followed by silence, and Ilya Sutskever’s involvement and subsequent turnaround when it seemed the company might evaporate. It is also unsettling. If even weak AGI has entered our world, how will things change? </p><p><span>Evidence is circumstantial. Yes, back in early October Sam Altman posted on Reddit for the first time in more than 5 years, writing, “</span><a href="https://www.reddit.com/r/singularity/comments/16sdu6w/comment/k2aroaw/" rel="">agi has been achieved internally</a><span>”. He then walked it back as a joke, but I can absolutely imagine Altman and a </span><a href="https://en.wikipedia.org/wiki/Reddit" rel="">Reddit</a><span> co-founder in the Y-Combinator orbit making a deal ages ago about how if OpenAI ever achieved AGI, it would be posted first on Reddit. If that did happen, it would be very Silicon Valley.</span></p><p>Somewhat more concretely, there’s also these quotes from not even two weeks ago. At the Asia-Pacific Economic Cooperation event on November 16th (yes, one day before the board’s maneuver), Altman said (at around 13:22 in the video below): “Four times now in the history of OpenAI—the most recent time was just in the last couple of weeks—I’ve gotten to be in the room when we … push the veil of ignorance back and the frontier of discovery forward.”</p><div id="youtube2-ZFFvqRemDv8" data-attrs="{&#34;videoId&#34;:&#34;ZFFvqRemDv8&#34;,&#34;startTime&#34;:null,&#34;endTime&#34;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/ZFFvqRemDv8?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><span>Later, during the same event, </span><a href="https://www.youtube.com/watch?v=ZFFvqRemDv8" rel="">Altman said in response to a question about AI in 2024 (at around 50:33 in the video)</a><span>: “The model capability will have taken such a leap forward that no one expected … no one expected that much progress. It’s different to expectation. I think people have, like, in their mind how much better the model will be next year, and it’ll be remarkable how much different it is.”</span></p><p><strong>Takeaways </strong></p><p>What does all this mean? I don’t know. Maybe nothing; maybe the maneuvering really was just a case of overblown politics and insufficient planning. Or maybe something; maybe we’ll look back on this moment in a year or two with greater understanding and context and realize everything has changed. </p><p><span>One thing’s for sure: risk management is still important. I refuse to polarize into a camp and declare war on another camp. “AI acceleration versus AI deceleration” shouting matches will probably produce bad outcomes if they devolve into a culture war, because </span><em>the base case for culture wars is that they produce bad outcomes</em><span>.</span><span> There’s room for AI regulation and AI progress: understanding the differences among various use cases; considering all arguments, data, and implications, including knock-on (nth-order) effects; simulating various outcomes; and making informed decisions about capabilities and controls. This is what people in critical industries do.</span></p><p>AI could produce massively beneficial breakthroughs in medicine, in education, in energy. It could also send our civilization hurtling off the rails. Both of those sentences are true. Open discussion and lively debate are vital going forward. We need to be the humans we want to be when we envision a transformative future. Last week was not it. </p><p>So, what’s next? We’ll have to live it to find out. I just hope we do it with wisdom, grace, responsibility, and enough conscientiousness that the pro-wrestling vibes of pre-Thanksgiving fade into a brighter history. </p></div></div></div></article></div></div></div>
  </body>
</html>
