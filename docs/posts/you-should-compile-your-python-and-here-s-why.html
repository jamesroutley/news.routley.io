<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://glyph.twistedmatrix.com/2022/04/you-should-compile-your-python-and-heres-why.html">Original</a>
    <h1>You Should Compile Your Python and Here’s Why</h1>
    
    <div id="readability-page-1" class="page"><div>
  <p>In this post I’d like to convince you that you should be running
<a href="https://mypyc.readthedocs.io/en/latest/">Mypyc</a> over your code<sup id="fnref:1:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:1:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:1">1</a></sup> —
especially if your code is a library you upload to PyPI — for both your own
benefit and that of the Python ecosystem at large.</p>
<p>But first, let me give you some background.</p>
<h2 id="python-is-slow-and-thats-fine-because-its-fast-enough">Python is Slow, And That’s Fine, Because It’s Fast Enough</h2>
<p>A common narrative about Python’s value proposition, from the very earliest
days of the language<sup id="fnref:2:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:2:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:2">2</a></sup>, often recited in response to a teammate saying
“shouldn’t we just write this in <code>$HIGHER_PERFORMANCE_LANGUAGE</code> instead?” goes
something like this:</p>
<blockquote>
<p>Sure, Python is slow.</p>
<p>But that’s okay, because it saves you so much time over implementing your
code in <code>$HIGHER_PERFORMANCE_LANGUAGE</code> that you’ll have so much more time for
optimizing those critical hot-spots where performance is really critical.</p>
<p>And if the language’s primitives are too slow to micro-optimize those
hot-spots enough, that’s okay too, because you can always re-write just those
small portions of the program as a C extension module.</p>
<p>Python’s got you covered!</p>
</blockquote>
<p>There is some truth to this narrative, and I’ve quoted from it myself on many
occasions.  When I did so, I was not quoting it as some facile, abstract
hypothetical, either.  I had a few projects, particularly very early in my
Python career, where I replaced performance-critical C++ code with a one tenth
the number of lines of Python, and improved performance by orders of magnitude
in the process<sup id="fnref:3:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:3:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:3">3</a></sup>.</p>
<p>When you have algorithmically interesting, performance-sensitive code that can
benefit from a high-level expressive language, and the resources to invest in
making it fast, this process <em>can</em> be counterintuitively more efficient than
other, “faster” tools.  If you’re working on massively multiplayer online
games<sup id="fnref:4:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:4:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:4">4</a></sup> or something equally technically challenging, Python can be a
<em>surprisingly</em> good idea.</p>
<h2 id="but-is-it-fine-though">But… <em>Is</em> It Fine, Though?</h2>
<p>This little nugget of folk wisdom does sound a <em>bit</em> defensive, doesn’t it?  If
Python were just <em>fast</em>, you could just <em>use</em> it, you wouldn’t need this litany
of rationalizations.  Surely if we <em>believed</em> that <a href="https://blog.nelhage.com/post/reflections-on-performance/">performance is
important</a> in our
own Python code, we wouldn’t try to wave away the performance of Python itself.</p>
<p>Most projects are not massively multiplayer online games.  On many
straightforward business automation projects, this sort of staged approach to
performance is impractical.</p>
<p><a href="https://blog.nelhage.com/post/reflections-on-performance/#performance-isnt-just-about-hot-spots">Not all performance problems are hot
spots</a>.
Some programs have to be fast all the way through.  This is true of some
complex problems, like compilers and type checkers, but is also often the case
in many kinds of batch processing; there are just a lot of numbers, and you
have to add them all up.</p>
<p>More saliently for the vast majority of average software projects, optimization
just isn’t in the budget. You do your best on your first try and hope that none
of those hot spots get too hot, because as long as the system works within a
<em>painfully</em> generous time budget, the business doesn’t care if it’s slow.</p>
<p>The progression from “idiomatic Python” to “optimized Python” to “C” is a
<em>one-way process</em> that gradually loses the advantages that brought us to Python
in the first place.</p>
<p>The difficult-to-reverse nature of each step means that once you have
prototyped out a reasonably optimized data structure or algorithm, you need to
quasi-<em>permanently</em> commit to it in order to squeeze out more straight-line
performance of the implementation.</p>
<p>Plus, the process of optimizing Python often destroys its readability, for a
few reasons:</p>
<ol>
<li>Optimized Python relies on knowledge of unusual tricks.  Things like “use
   the <code>array</code> module instead of lists”, and “<a href="https://stackoverflow.com/questions/40714555/python-string-formatting-is-more-efficient-than-format-function">use <code>%</code> instead of
   <code>.format</code></a>”.</li>
<li>Optimized Python requires you to <em>avoid the things that make Python code
   nicely organized</em>:<ol>
<li>method lookups are slow so you should use functions.</li>
<li>object attribute accesses are slow so you should use tuples with
   hard-coded numeric offsets.</li>
<li>function calls are slow so you should copy/paste and inline your logic</li>
</ol>
</li>
<li>Optimized Python requires very specific knowledge of where it’s going to be
   running, so you lose the flexibility of how to run it: making your code fast
   on CPython might make it much slower on PyPy, for example.  Native extension
   modules can make your code faster, but might also make it fail to run inside
   a browser, or add a ton of work to get it set up on a new operating system.</li>
</ol>
<p>Maintaining good performance is part of your software’s development lifecycle,
not just a thing you do once and stop.  So by moving into this increasingly
arcane dialect of “fast” python, and then into another programming language
entirely with a C rewrite, you end up having to maintain C code anyway.  Not to
mention the fact that rewriting large amounts of code in C is both ludicrously
difficult (particularly if your team primarily knows Python) and also
<a href="https://twitter.com/lazyfishbarrel">catastrophically dangerous</a>.  In recent
years, <a href="https://pyo3.rs/v0.16.4/">safer tools such as PyO3 have become
available</a>, but they still involve switching
programming languages and rewriting all your code as soon as you care about
speed<sup id="fnref:5:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:5:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:5">5</a></sup>.</p>
<p>So, for Python to be a truly general-purpose language, we need some way to
<em>just write Python</em>, and have it be fast.</p>
<p>It would benefit every user of Python for there to be an <em>easy, widely-used</em>
way to make idiomatic, simple Python that just does stuff like adding numbers,
calling methods, and formatting strings in a straight line go really fast —
exactly the sorts of things that are the slowest in Python, but are also the
most common, particularly before you’ve had an opportunity to cleverly
optimize.</p>
<h2 id="weve-been-able-to-at-least-make-do">We’ve Been Able To At Least Make Do</h2>
<p>There are also a number of tools that have long been in use for addressing this
problem: PyPy, Pyrex, Cython, Numba, and Numpy to name a few.  Their
maintainers all deserve <em>tremendous</em> amounts of credit, and I want to be very
clear that this post is <em>not</em> intended to be critical of anyone’s work here.
These tools have drawbacks, but many of those drawbacks make them much better
suited to specialized uses beyond the more general 80% case I’m talking about
in this post, for which Mypyc would not be suitable.</p>
<p>Each one of these tools impose limitations on either the way that you write
code or where you can deploy it.</p>
<p>Cython and Numba aren’t really “Python” any more, because they require
special-purpose performance-oriented annotations.  Cython has long supported
pure-Python type annotations, but you won’t get any benefit from telling it
that your variable is an <code>int</code>, only a <code>cython.int</code>. It can’t optimize a
<code>@dataclass</code>, only a <code>@cython.cclass</code>.  And so on.</p>
<p>PyPy gets the closest — it’s definitely regular Python — but its strategy has
important limitations.  Primarily, despite the phenomenal and heroic effort
that went into
<a href="https://morepypy.blogspot.com/2018/09/inside-cpyext-why-emulating-cpython-c.html"><code>cpyext</code></a>,
it seems like <a href="https://towardsdatascience.com/pypy-is-faster-than-python-but-at-what-cost-12739bf2b8e9">there’s always just <em>one</em> PyPy-incompatible
library</a>
in every large, existing project’s dependency list which makes it impossible to
just drop in PyPy without doing a bunch of arcane debugging first.</p>
<p>PyPy <em>might</em> make your program magically much faster, but if it doesn’t work,
you have to read the tea leaves on the JIT’s behavior in a profiler which
practically requires an online component that <a href="https://vmprof.com">doesn’t even work any
more</a>.  So mostly you just simplify your code to use more
straightforward data structures and remove CPython-specific tricks that might
trip up the JIT, and hope for the best.</p>
<p>PyPy also introduces platform limitations.  It’s always — understandably, since
they have to catch up after the fact — lagging a bit behind the most recently
released version of CPython, so there’s always some nifty language feature that
you have to refrain from using for at least one more release cycle.</p>
<p>It also has architectural limitations.  For example, it performs quite poorly
on an M1 Mac since it still runs under x86_64 emulation on that platform.  And
due to iOS forbidding 3rd-party JITs, it won’t ever be able to provide better
performance in one of the more constrained environments that needs it more that
other places.  So you might need to rely on CPython on those platforms anyway…
and you just removed all your CPython-specific hacks to try to please the JIT
on the <em>other</em> platforms you support.</p>
<p>So while I would encourage everyone to at least <em>try</em> their code on PyPy — if
you’re running a web-based backend, it might save you half your hardware
budget<sup id="fnref:6:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:6:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:6">6</a></sup> — it’s not going to solve “python is slow” in the general case.</p>
<h2 id="itll-eventually-be-all-right">It’ll Eventually Be All Right</h2>
<p>This all sounds pretty negative, so I would be remiss if I did not also point
out that <a href="https://www.theregister.com/2021/05/13/guido_van_rossum_cpython_3_11/">the core team is well aware that Python’s default performance needs
to be better, and Guido van Rossum literally came out of retirement for <em>one
last
job</em></a> to
fix it, and
<a href="https://twitter.com/pyblogsal/status/1433411699235774466">we’ve already seen a bunch of benefits from that effort</a>.</p>
<p>But there are some fundamental limitations on the long-term strategy for these
optimizations; one of the big upcoming improvements is a JIT, which suffers
from some (but not all) of the <a href="https://github.com/markshannon/faster-cpython/blob/314eec8f554c5d93a6a69e0c824aaa0d183986d4/plan.md#platforms-that-prohibit-runtime-code-generation">same limitations as
PyPy</a>,
and the late-bound, freewheeling nature of Python inherently comes with some
performance tradeoffs.</p>
<p>So it would still behoove us to have a strategy for production-ized code that
gives good, portable, ahead-of-time performance.</p>
<h2 id="but-what-about-right-now">But What About Right Now?</h2>
<p><a href="https://mypyc.readthedocs.io/en/latest/">Mypyc</a> takes the annotations meant
for Mypy and generates C with them, potentially turning your code into a much
more efficient extension module.  As part of Mypy itself, it does this with
your existing Python type-hints, the kind you’d already use Mypy with to check
for correctness, so it doesn’t entail much in the way of <em>additional</em> work.</p>
<p>I’d been curious about this since <a href="https://mypy-lang.blogspot.com/2019/01/mypy-0660-released.html">it was initially
released</a>, but
I still haven’t had a hard real-world performance problem to really put it
through its paces.</p>
<p>So when I learned about the <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/">High Throughput Fizzbuzz
Challenge</a>
via its <em>impressive</em> <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/236630#236630">assembler
implementation</a>
that achieves 56GiB/s, and I saw even <em>heavily</em>-optimized Python
implementations sitting well below the performance of a totally naïve C
reference implementation, I thought this would be an interesting miniature
experiment to use to at least approximate practical usage.</p>
<h2 id="in-which-i-design-a-completely-unfair-fight-which-i-will-then-handily-win">In Which I Design A Completely Unfair Fight Which I Will Then Handily Win</h2>
<p>The dizzying heights of cycle-counting hand-tuned assembler implementations of
this benchmark are squarely out of our reach, but I wanted to see if I could
beat the performance of this very naïve C implementation with Python that was
optimized, but at least, somewhat idiomatic and readable.</p>
<p>I am about to compare a totally naïve C implementation with a fairly optimized
hand-tuned Python one, which might seem like an unfair fight.  But what I’m
trying to approximate here is a micro-instance of the real-world
development-team choice that looks like this:</p>
<blockquote>
<p>Since Python is more productive, but slower, the effort to deliver each of
the following is similar:</p>
<ol>
<li>a basic, straightforward implementation of our solution in C</li>
<li>a moderately optimized Python implementation of our solution</li>
</ol>
<p>and we need to choose between them.</p>
</blockquote>
<p>This is why I’ll just be showing naïve C and not unrolling any loops; I’ll use
<code>-O3</code> because any team moderately concerned with performance would at least
turn on the most basic options, but nothing further.</p>
<p>Furthermore, our hypothetical team also has this constraint, which really every
reasonable team should:</p>
<blockquote>
<p>We can trade off <strong>some</strong> readability for efficiency, but it’s important that
our team be able to maintain this code going forward.</p>
</blockquote>
<p>This is why I’m doing a <em>bit</em> of optimizing in Python but not going all out by
calling <code>mmap</code> or pulling in <code>numpy</code> or attempting to use something super
esoteric like a SIMD library to emulate what the assembler implementations do.
The goal is that this is normal Python code with a reasonable level of
systems-level understanding (i.e. accounting for the fact that pipes have
buffers in the kernel and approximately matching their size maximizes
throughput).</p>
<p>If you want to see FizzBuzz pushed to its limit, you can go <a href="https://codegolf.stackexchange.com/questions/215216/high-throughput-fizz-buzz/">check out the
challenge
itself</a>.
Although I think I do coincidentally beat the performance of the Python
versions they currently have on there, that’s not what I’m setting out to do.</p>
<p>So with that elaborate framing of this slightly odd experiment out of the way,
here’s our naïve C version:</p>
<table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span></pre></div></td><td><div><pre><span></span><code><span>#include &lt;stdio.h&gt;</span>

<span>int</span> <span>main</span><span>()</span> <span>{</span>
    <span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>1</span><span>;</span> <span>i</span> <span>&lt;</span> <span>1000000000</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>if</span> <span>((</span><span>i</span> <span>%</span> <span>3</span> <span>==</span> <span>0</span><span>)</span> <span>&amp;&amp;</span> <span>(</span><span>i</span> <span>%</span> <span>5</span> <span>==</span> <span>0</span><span>))</span> <span>{</span>
            <span>printf</span><span>(</span><span>&#34;FizzBuzz</span><span>\n</span><span>&#34;</span><span>);</span>
        <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>i</span> <span>%</span> <span>3</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
            <span>printf</span><span>(</span><span>&#34;Fizz</span><span>\n</span><span>&#34;</span><span>);</span>
        <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>i</span> <span>%</span> <span>5</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
            <span>printf</span><span>(</span><span>&#34;Buzz</span><span>\n</span><span>&#34;</span><span>);</span>
        <span>}</span> <span>else</span> <span>{</span>
            <span>printf</span><span>(</span><span>&#34;</span><span>%d</span><span>\n</span><span>&#34;</span><span>,</span> <span>i</span><span>);</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>
</td></tr></tbody></table>
<p>First, let’s do a quick head-to-head comparison with a naïve Python
implementation of the algorithm:</p>
<table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span></pre></div></td><td><div><pre><span></span><code><span>def</span> <span>fizzbuzz</span><span>()</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>for</span> <span>counter</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>1000000000</span><span>):</span>
        <span>fizz</span> <span>=</span> <span>counter</span> <span>%</span> <span>3</span> <span>==</span> <span>0</span>
        <span>buzz</span> <span>=</span> <span>counter</span> <span>%</span> <span>5</span> <span>==</span> <span>0</span>
        <span>if</span> <span>fizz</span><span>:</span>
            <span>print</span><span>(</span><span>&#34;Fizz&#34;</span><span>,</span> <span>end</span><span>=</span><span>&#34;&#34;</span><span>)</span>
        <span>if</span> <span>buzz</span><span>:</span>
            <span>print</span><span>(</span><span>&#34;Buzz&#34;</span><span>,</span> <span>end</span><span>=</span><span>&#34;&#34;</span><span>)</span>
        <span>if</span> <span>not</span> <span>(</span><span>fizz</span> <span>or</span> <span>buzz</span><span>):</span>
            <span>print</span><span>(</span><span>counter</span><span>,</span> <span>end</span><span>=</span><span>&#34;&#34;</span><span>)</span>
        <span>print</span><span>()</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span><span>:</span>
    <span>fizzbuzz</span><span>()</span>
</code></pre></div>
</td></tr></tbody></table>
<p>Running both of these on my M1 Max MacBook, the naïve C implementation yields
127 MiB/s of Fizzbuzz output.  But, as I said, although we’re not going to have
time for testing a more complex optimized C version, we would want to at
<em>least</em> build it with the performance benefits we get for free with the <code>-O3</code>
compiler option. It turns out that yields us a 27 MiB/s speedup.  So 154 MiB/s
is the number we have to beat.<sup id="fnref:7:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:7:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:7">7</a></sup></p>
<p>The naïve Python version achieves a dismal 24.3 MiB/s, due to a few
issues. First of all, although it’s idiomatic, <code>print()</code> is doing a lot of
unnecessary work here.  Among other things, we are encoding Unicode, which the
C version isn’t.  Still, our equivalent of adding the <code>-O3</code> option for C is
running <code>mypyc</code> without changing anything, and that yields us a 6.8MiB/s
speedup immediately.  We still aren’t achieving comparable performance, but a
roughly 25% performance improvement for no work at all is a promising start!<sup id="fnref:8:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:8:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:8">8</a></sup></p>
<p>In keeping with the “some optimizations, but not so much that it’s illegible”
constraint described above, the specific optimizations I’ve chosen to pursue
here are:</p>
<ol>
<li>switch to using <code>bytes</code> objects and <code>sys.stdout.buffer</code> to avoid encoding overhead</li>
<li>take advantage of the repeating nature of the pattern in FizzBuzz output and
   pre-generate a template rather than computing each line independently</li>
<li>fill out the buffer with the relevant integers from a sequence as we go</li>
<li>tune the repetition of that template to a size that roughly fills a pipe
   buffer on my platform of choice</li>
</ol>
<p>Hopefully, with that explanation, this isn’t too bad:</p>
<table><tbody><tr><td><div><pre><span> 1</span>
<span> 2</span>
<span> 3</span>
<span> 4</span>
<span> 5</span>
<span> 6</span>
<span> 7</span>
<span> 8</span>
<span> 9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span>
<span>16</span>
<span>17</span>
<span>18</span>
<span>19</span>
<span>20</span>
<span>21</span>
<span>22</span>
<span>23</span>
<span>24</span>
<span>25</span>
<span>26</span>
<span>27</span>
<span>28</span>
<span>29</span>
<span>30</span>
<span>31</span>
<span>32</span>
<span>33</span>
<span>34</span>
<span>35</span>
<span>36</span>
<span>37</span>
<span>38</span>
<span>39</span>
<span>40</span>
<span>41</span>
<span>42</span>
<span>43</span>
<span>44</span></pre></div></td><td><div><pre><span></span><code><span>from</span> <span>sys</span> <span>import</span> <span>stdout</span>
<span>from</span> <span>typing</span> <span>import</span> <span>Tuple</span><span>,</span> <span>Iterable</span>


<span>def</span> <span>precompute_template</span><span>()</span> <span>-&gt;</span> <span>Iterable</span><span>[</span><span>bytes</span><span>]:</span>
    <span>for</span> <span>counter</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>16</span><span>):</span>
        <span>fizz</span> <span>=</span> <span>counter</span> <span>%</span> <span>3</span> <span>==</span> <span>0</span>
        <span>buzz</span> <span>=</span> <span>counter</span> <span>%</span> <span>5</span> <span>==</span> <span>0</span>
        <span>if</span> <span>fizz</span><span>:</span>
            <span>yield</span> <span>b</span><span>&#34;Fizz&#34;</span>
        <span>if</span> <span>buzz</span><span>:</span>
            <span>yield</span> <span>b</span><span>&#34;Buzz&#34;</span>
        <span>if</span> <span>not</span> <span>(</span><span>fizz</span> <span>or</span> <span>buzz</span><span>):</span>
            <span>yield</span> <span>b</span><span>&#34;</span><span>%d</span><span>&#34;</span>
        <span>yield</span> <span>b</span><span>&#34;</span><span>\n</span><span>&#34;</span>


<span>chunk_copies</span> <span>=</span> <span>4</span>
<span>precomputed_template_chunks</span> <span>=</span> <span>list</span><span>(</span><span>precompute_template</span><span>())</span>
<span>format_string</span> <span>=</span> <span>b</span><span>&#34;&#34;</span><span>.</span><span>join</span><span>(</span><span>precomputed_template_chunks</span><span>)</span>
<span>number_indexes</span> <span>=</span> <span>[</span>
    <span>number_index</span>
    <span>for</span> <span>number_index</span><span>,</span> <span>line_content</span> <span>in</span> <span>enumerate</span><span>(</span><span>format_string</span><span>.</span><span>split</span><span>(</span><span>b</span><span>&#34;</span><span>\n</span><span>&#34;</span><span>))</span>
    <span>if</span> <span>line_content</span> <span>==</span> <span>b</span><span>&#34;</span><span>%d</span><span>&#34;</span>
<span>]</span>
<span>format_string</span> <span>*=</span> <span>chunk_copies</span>


<span>def</span> <span>fizzbuzz</span><span>()</span> <span>-&gt;</span> <span>None</span><span>:</span>
    <span>num</span><span>:</span> <span>int</span> <span>=</span> <span>1</span>
    <span>output</span> <span>=</span> <span>stdout</span><span>.</span><span>buffer</span><span>.</span><span>write</span>
    <span>for</span> <span>num</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>1000000001</span><span>,</span> <span>15</span> <span>*</span> <span>chunk_copies</span><span>):</span>
        <span>t</span><span>:</span> <span>Tuple</span><span>[</span><span>int</span><span>,</span> <span>...</span><span>]</span> <span>=</span> <span>tuple</span><span>(</span>
            <span>(</span>
                <span>x</span> <span>+</span> <span>number_index</span>
                <span>for</span> <span>x</span> <span>in</span> <span>range</span><span>(</span><span>num</span><span>,</span> <span>num</span> <span>+</span> <span>(</span><span>15</span> <span>*</span> <span>chunk_copies</span><span>),</span> <span>15</span><span>)</span>
                <span>for</span> <span>number_index</span> <span>in</span> <span>number_indexes</span>
            <span>)</span>
        <span>)</span>
        <span>output</span><span>(</span><span>format_string</span> <span>%</span> <span>t</span><span>)</span>


<span>if</span> <span>__name__</span> <span>==</span> <span>&#34;__main__&#34;</span><span>:</span>
    <span>fizzbuzz</span><span>()</span>
</code></pre></div>
</td></tr></tbody></table>
<p>Running this optimized version actually gets us within the ballpark of the
naïve C version, even beating it by a hair; my measurement was 159 MiB/s, a
small improvement even over <code>-O3</code>.  So, per the “litany against C” from the
beginning of this post, algorithmic optimization of Python really does help a
lot; it’s not <em>just</em> a rationalization.  This is a much bigger boost than our
original no-effort Mypyc run, giving us more like an <em>85%</em> speedup; definitely
bigger than 25%.</p>
<p>But clearly we’re still being slowed down by Python’s function call overhead,
object allocations for small integers, and so on, so Mypyc should help us out
here: and indeed it does.  On my machine, it nets a whopping <em>233 MiB/s</em>.  Now
that we are accounting for performance and optimizing a bit, Mypyc’s relative
advantage has doubled to a <em>50% improvement</em> in performance on both the
optimized-but-interpreted Python and naïve C versions.</p>
<p>It’s worth noting that the technique I used to produce the extension modules to
test was <em>literally</em> <code>pip install mypy; mypyc .../module.py</code>, then <code>python -c
“import module”</code>.  I did already have a C compiler installed, but other than
that, there was no setup.</p>
<p>I just wrote Python, and it <em>just worked</em>.</p>

<p>Here’s what I want you to take away from all this:</p>
<ol>
<li>Python can be fast.</li>
<li>More importantly, <strong>your</strong> Python can be fast.</li>
<li>For a fairly <em>small investment of effort</em>, your Python code can be made
   <em>meaningfully faster</em>.</li>
</ol>
<p>Unfortunately, due to the limitations and caveats of existing powerful
performance tools like Cython and PyPy, over the last few years in the Python
community a passive consensus has emerged.  For most projects, in most cases,
it’s just not worth it to bother to focus on performance.  Everyone just uses
the standard interpreter, and only fixes the worst performance regressions.</p>
<p>We should, of course, be glad that the standard interpreter is <a href="https://www.theregister.com/2021/05/13/guido_van_rossum_cpython_3_11/">reliably
getting faster all the time
now</a>,
but we shouldn’t be basing our individual libraries’ and applications’
performance strategies on that alone.</p>
<p>The projects that care the most about performance <em>have</em> made the effort to use
some of these tools, and they have often invested huge amounts of effort to
good effect, but often they care about performance <em>too</em> much.  They make the
problem look even <em>harder</em> for everyone else, by essentially stipulating that
step 1 is to do something extreme like <a href="https://numpy.org/doc/stable/user/c-info.python-as-glue.html">give up and use Fortran for all the
interesting
stuff</a>.</p>
<p>My goal with this post is to challenge that status quo, spark interest in
revisiting the package ecosystem’s baseline performance expectations, and to
get more projects — particularly <em>libraries on PyPI</em> — to pick up Mypyc and
start giving Python a deserved reputation for being surprisingly fast.</p>
<h2 id="the-last-piece-of-the-puzzle">The Last Piece of the Puzzle</h2>
<p>One immediate objection you might be thinking of is the fact that, under the
hood, Mypyc <em>is</em> emitting some C code and building it, and so this might create
a problem for deployment: if you’ve got a Linux machine but 30% of your users
are on Windows, moving from pure-Python to this hybrid workflow might create
installation difficulties for them, or at least they won’t see the benefits.</p>
<p>Luckily a separate tool should make that a non-issue:
<a href="https://cibuildwheel.readthedocs.io/en/stable/"><code>cibuildwheel</code></a>. “CI Build
Wheel”, as its name suggests, lets you build your wheels in your continuous
integration system, and upload those builds automatically upon tagging a
release.</p>
<p>Often, the bulk of the work in using it is dealing with the additional
complexities involved in setting up your build environment in CI to make sure
you’re appropriately bundling in any native libraries you depend upon, and
linking to them in the correct way.  Mypyc’s limitation relative to Cython is a
huge advantage here: it doesn’t <em>let you</em> link to other native libraries, so
you can always skip the worst step here.</p>
<p>So, for maintainers, you don’t need to maintain a pile of janky VMs on your
personal development machine in order to serve your users.  For users, nobody
needs to deal with the nightmare of setting up the right C compiler on their
windows machine, because the wheels are prebuilt.  Even users without a
compiler who want to <em>contribute new code</em> or debug it can run it with the
interpreter locally, and let the cloud handle the complicated compilation steps
later.  Once again, the fact that you can’t require additional, external C
libraries here is a big advantage; it prevents you from making the user’s
experience inadvertently worse.</p>
<p><code>cibuildwheel</code> supports all major operating systems and architectures, and
supported versions of Python, and even lets you build wheels for PyPy while
you’re at it.<sup id="fnref:9:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:9:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:9">9</a></sup></p>
<h2 id="putting-it-all-together">Putting It All Together</h2>
<p>Using Mypyc and <code>cibuildwheel</code>, we, as PyPI package maintainers, can
potentially produce an ecosystem of much faster out-of-the-box experiences via
prebuilt extension modules, written entirely in Python, which would make the
average big Python application with plenty of dependencies <em>feel</em> snappier than
expected.  This doesn’t have to come with the pain that we have <a href="https://www.reddit.com/r/Python/comments/6z7j5z/there_goes_my_weekend_stuck_in_compile_hell_boost/">unfortunately
come to
expect</a>
from C extensions, either as maintainers or users.</p>
<p>Another nice thing is that this is not an all-or-nothing proposition. If you
try PyPy and it blows up in some obscure way on your code, you have to give up
on it unless you want to fully investigate what’s happening. But if you trip
over a bug in Mypyc, you can report the bug, drop the module where you’re
having the problem from the list of things you’re trying to compile, and move
on. You don’t even have to start out by trying to jam your whole project
through it; just pick a few key modules to get started, and gradually expand
that list over time, as it makes sense for your project.</p>
<p>In a future post, I’ll try to put all of this together myself, and hopefully
it’s not going to be embarrassingly difficult and make me eat my words.</p>
<p>Despite not having done that yet, I wanted to put this suggestion out now, to
get other folks thinking about getting started with it.  For older
projects<sup id="fnref:10:you-should-compile-your-python-and-heres-why-2022-4"><a href="#fn:10:you-should-compile-your-python-and-heres-why-2022-4" id="fnref:10">10</a></sup>, retrofitting all the existing infrastructure to put Mypyc in
place might be a bit of a challenge.  But for <em>new</em> projects starting today,
putting this in place when there’s very little code might be as simple as
adding a couple of lines to <code>pyproject.toml</code> and copy-pasting <a href="https://cibuildwheel.readthedocs.io/en/stable/setup/#github-actions">some
YAML</a> into
a Github workflow.</p>
<p>If you’re thinking about making some new open source Python, give Mypyc a try,
and see if you can delight some users with lightning speed right out of the
box.  If you do, <a href="mailto:fast-python-082161@glyph.im">let me know how it turns
out</a>.</p>

<p>Thanks to Donald Stufft, Moshe Zadka, Nelson Elhage, Itamar Turner-Trauring,
and David Reid for extensive feedback on this post.  As always, any errors or
inaccuracies remain my own.</p>

</div></div>
  </body>
</html>
