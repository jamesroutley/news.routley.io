<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://austinsnerdythings.com/2025/11/24/worlds-most-stable-raspberry-pi-81-better-ntp-with-thermal-management/">Original</a>
    <h1>Most Stable Raspberry Pi? Better NTP with Thermal Management</h1>
    
    <div id="readability-page-1" class="page"><div>

		<div>

			
<p>I’ve written before about building <a href="https://austinsnerdythings.com/2021/04/19/microsecond-accurate-ntp-with-a-raspberry-pi-and-pps-gps/">microsecond-accurate NTP servers with Raspberry Pi and GPS PPS</a>, and more recently about <a href="https://austinsnerdythings.com/2025/02/14/revisiting-microsecond-accurate-ntp-for-raspberry-pi-with-gps-pps-in-2025/">revisiting the setup in 2025</a>. Both posts focused on the hardware setup and basic configuration to achieve sub-microsecond time synchronization using GPS Pulse Per Second (PPS) signals.</p>



<p>But there was a problem. Despite having a stable PPS reference, my NTP server’s frequency drift was exhibiting significant variation over time. After months (years) of monitoring the system with Grafana dashboards, I noticed something interesting: the frequency oscillations seemed to correlate with CPU temperature changes. The frequency would drift as the CPU heated up during the day and cooled down at night, even though the PPS reference remained rock-solid.</p>



<p>Like clockwork (no pun intended), I somehow get sucked back into trying to improve my setup every 6-8 weeks. This post is the latest on that never-ending quest.</p>



<p>This post details how I achieved an <strong>81% reduction in frequency variability</strong> and <strong>77% reduction in frequency standard deviation</strong> through a combination of CPU core pinning and thermal stabilization. Welcome to Austin’s Nerdy Things, where we solve problems that 99.999% of people (and 99% of datacenters) don’t have.</p>



<h2>The Problem: Thermal-Induced Timing Jitter</h2>



<p>Modern CPUs, including those in Raspberry Pis, use dynamic frequency scaling to save power and manage heat. When the CPU is idle, it runs at a lower frequency (and voltage). When load increases, it scales up. This is great for power efficiency, but terrible for precision timekeeping.</p>



<p>Why? Because timekeeping (with NTP/chronyd/others) relies on a stable system clock to discipline itself against reference sources. If the CPU frequency is constantly changing, the system clock’s tick rate varies, introducing jitter into the timing measurements. Even though my PPS signal was providing a mostly perfect 1-pulse-per-second reference, the CPU’s frequency bouncing around made it harder for chronyd to maintain a stable lock.</p>



<p>But here’s the key insight: <strong>the system clock is ultimately derived from a crystal oscillator</strong>, and crystal oscillator frequency is temperature-dependent. The oscillator sits on the board near the CPU, and as the CPU heats up and cools down throughout the day, so does the crystal. Even a few degrees of temperature change can shift the oscillator’s frequency by parts per million – exactly what I was seeing in my frequency drift graphs. The CPU frequency scaling was one factor, but the underlying problem was that temperature changes were affecting the crystal oscillator itself. By stabilizing the CPU temperature, I could stabilize the thermal environment for the crystal oscillator, keeping its frequency consistent.</p>



<p>Looking at my Grafana dashboard, I could see the frequency offset wandering over a range of about 1 PPM (parts per million) as the Pi warmed up and cooled down throughout the day. The RMS offset was averaging around 86 nanoseconds, which isn’t terrible (it’s actually really, really, really good), but I knew it could be better.</p>



<h2>The Discovery</h2>



<p>After staring at graphs for longer than I’d like to admit, I had an idea: what if I could keep the CPU at a constant temperature? If the temperature (and therefore the frequency) stayed stable, maybe the timing would stabilize too.</p>



<p>The solution came in two parts:</p>



<p>1. <strong>CPU core isolation</strong> – Dedicate CPU 0 exclusively to timing-critical tasks (chronyd and PPS interrupts) 2. <strong>Thermal stabilization</strong> – Keep the other CPUs busy to maintain a constant temperature, preventing frequency scaling</p>



<p>Here’s what happened when I turned on the thermal stabilization system on November 17, 2025 at 09:10 AM:</p>



<figure><img decoding="async" width="2082" height="880" src="https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1.png" alt="NTP Frequency Stability" srcset="https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1.png 2082w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-300x127.png 300w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-800x338.png 800w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-768x325.png 768w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-1536x649.png 1536w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-2048x866.png 2048w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-1200x507.png 1200w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_stability-1-1980x837.png 1980w" sizes="(max-width: 2082px) 100vw, 2082px"/></figure>



<p>Same ish graph but with CPU temp also plotted:</p>



<figure><a href="https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature.png"><img decoding="async" width="800" height="354" src="https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature-800x354.png" alt="" srcset="https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature-800x354.png 800w, https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature-300x133.png 300w, https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature-768x340.png 768w, https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature-1536x680.png 1536w, https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature-2048x907.png 2048w, https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature-1200x532.png 1200w, https://austinsnerdythings.com/wp-content/uploads/2025/11/raspberry-pps-ntp-time_burner-constant-temperature-1980x877.png 1980w" sizes="(max-width: 800px) 100vw, 800px"/></a></figure>



<p>That vertical red line marks on the first plot when I activated the “time burner” process. Notice how the frequency oscillations immediately dampen and settle into a much tighter band? Let’s dive into how this works.</p>



<p>EDIT: 2025-11-25 I didn’t expect to wake up and see this at #2 on Hacker News – <a href="https://news.ycombinator.com/item?id=46042946">https://news.ycombinator.com/item?id=46042946</a></p>



<h2>The Solution Part 1: CPU Core Pinning and Real-Time Priority</h2>



<p>The first step is isolating timing-critical operations onto a dedicated CPU core. On a Raspberry Pi (4-core ARM), this means:</p>



<ul>
<li>CPU 0: Reserved for chronyd and PPS interrupts</li>



<li>CPUs 1-3: Everything else, including our thermal load</li>
</ul>



<p>I had AI (probably Claude Sonnet 4 ish, maybe 4.5) create a boot optimization script that runs at system startup:</p>



<pre><code>#!/bin/bash
# PPS NTP Server Performance Optimization Script
# Sets CPU affinity, priorities, and performance governor at boot

set -e

echo &#34;Setting up PPS NTP server performance optimizations...&#34;

# Wait for system to be ready
sleep 5

# Set CPU governor to performance mode
echo &#34;Setting CPU governor to performance...&#34;
cpupower frequency-set -g performance

# Pin PPS interrupt to CPU0 (may fail if already pinned, that&#39;s OK)
echo &#34;Configuring PPS interrupt affinity...&#34;
echo 1 &gt; /proc/irq/200/smp_affinity 2&gt;/dev/null || echo &#34;PPS IRQ already configured&#34;

# Wait for chronyd to start
echo &#34;Waiting for chronyd to start...&#34;
timeout=30
while [ $timeout -gt 0 ]; do
    chronyd_pid=$(pgrep chronyd 2&gt;/dev/null || echo &#34;&#34;)
    if [ -n &#34;$chronyd_pid&#34; ]; then
        echo &#34;Found chronyd PID: $chronyd_pid&#34;
        break
    fi
    sleep 1
    ((timeout--))
done

if [ -z &#34;$chronyd_pid&#34; ]; then
    echo &#34;Warning: chronyd not found after 30 seconds&#34;
else
    # Set chronyd to real-time priority and pin to CPU 0
    echo &#34;Setting chronyd to real-time priority and pinning to CPU 0...&#34;
    chrt -f -p 50 $chronyd_pid
    taskset -cp 0 $chronyd_pid
fi

# Boost ksoftirqd/0 priority
echo &#34;Boosting ksoftirqd/0 priority...&#34;
ksoftirqd_pid=$(ps aux | grep &#39;\[ksoftirqd/0\]&#39; | grep -v grep | awk &#39;{print $2}&#39;)
if [ -n &#34;$ksoftirqd_pid&#34; ]; then
    renice -n -10 $ksoftirqd_pid
    echo &#34;ksoftirqd/0 priority boosted (PID: $ksoftirqd_pid)&#34;
else
    echo &#34;Warning: ksoftirqd/0 not found&#34;
fi

echo &#34;PPS NTP optimization complete!&#34;

# Log current status
echo &#34;=== Current Status ===&#34;
echo &#34;CPU Governor: $(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor)&#34;
echo &#34;PPS IRQ Affinity: $(cat /proc/irq/200/effective_affinity_list 2&gt;/dev/null || echo &#39;not readable&#39;)&#34;
if [ -n &#34;$chronyd_pid&#34; ]; then
    echo &#34;chronyd Priority: $(chrt -p $chronyd_pid)&#34;
fi
echo &#34;======================&#34;</code></pre>



<p><strong>What this does:</strong></p>



<ol>
<li><strong>Performance Governor</strong>: Forces all CPUs to run at maximum frequency, disabling frequency scaling</li>



<li><strong>PPS IRQ Pinning</strong>: Ensures PPS interrupt (IRQ 200) is handled exclusively by CPU 0</li>



<li><strong>Chronyd Real-Time Priority</strong>: Sets chronyd to SCHED_FIFO priority 50, giving it preferential CPU scheduling</li>



<li>C<strong>hronyd CPU Affinity</strong>: Pins chronyd to CPU 0 using <code>taskset</code></li>



<li><strong>ksoftirqd Priority Boost</strong>: Improves priority of the kernel softirq handler on CPU 0</li>
</ol>



<p>This script can be added to <code>/etc/rc.local</code> or as a systemd service to run at boot.</p>



<h2>The Solution Part 2: PID-Controlled Thermal Stabilization</h2>



<p>Setting the performance governor helps, but on a Raspberry Pi, even at max frequency, the CPU temperature will still vary based on ambient conditions and load. Temperature changes affect the CPU’s actual operating frequency due to thermal characteristics of the silicon.</p>



<p>The solution? Keep the CPU at a constant temperature using a PID-controlled thermal load. I call it the “time burner” (inspired by CPU burn-in tools, but with precise temperature control).</p>



<p>As a reminder of what we’re really doing here: <strong>we’re maintaining a stable thermal environment for the crystal oscillator</strong>. The RPi 3B’s 19.2 MHz oscillator is physically located near the CPU on the Raspberry Pi board, so by actively controlling CPU temperature, we’re indirectly controlling the oscillator’s temperature. Since the oscillator’s frequency is temperature-dependent (this is basic physics of quartz crystals), keeping it at a constant temperature means keeping its frequency stable – which is exactly what we need for precise timekeeping.</p>



<p>Here’s how it works:</p>



<ol>
<li><strong>Read CPU temperature</strong> from <code>/sys/class/thermal/thermal_zone0/temp</code> </li>



<li><strong>PID controller</strong> calculates how much CPU time to burn to maintain target temperature (I chose 54°C) </li>



<li><strong>Three worker processes</strong> run on CPUs 1, 2, and 3 (avoiding CPU 0) </li>



<li><strong>Each worker</strong> alternates between busy-loop (MD5 hashing) and sleeping based on PID output </li>



<li><strong>Temperature stabilizes</strong> at the setpoint, preventing thermal drift</li>
</ol>



<p>Here’s the core implementation (simplified for readability):</p>



<pre><code>#!/usr/bin/env python3
import time
import argparse
import multiprocessing
import hashlib
import os
from collections import deque

class PIDController:
    &#34;&#34;&#34;Simple PID controller with output clamping and anti-windup.&#34;&#34;&#34;
    def __init__(self, Kp, Ki, Kd, setpoint, output_limits=(0, 1), sample_time=1.0):
        self.Kp = Kp
        self.Ki = Ki
        self.Kd = Kd
        self.setpoint = setpoint
        self.output_limits = output_limits
        self.sample_time = sample_time
        self._last_time = time.time()
        self._last_error = 0.0
        self._integral = 0.0
        self._last_output = 0.0

    def update(self, measurement):
        &#34;&#34;&#34;Compute new output of PID based on measurement.&#34;&#34;&#34;
        now = time.time()
        dt = now - self._last_time

        if dt &lt; self.sample_time:
            return self._last_output

        error = self.setpoint - measurement

        # Proportional
        P = self.Kp * error

        # Integral with anti-windup
        self._integral += error * dt
        I = self.Ki * self._integral

        # Derivative
        derivative = (error - self._last_error) / dt if dt &gt; 0 else 0.0
        D = self.Kd * derivative

        # Combine and clamp
        output = P + I + D
        low, high = self.output_limits
        output = max(low, min(high, output))

        self._last_output = output
        self._last_error = error
        self._last_time = now

        return output

def read_cpu_temperature(path=&#39;/sys/class/thermal/thermal_zone0/temp&#39;):
    &#34;&#34;&#34;Return CPU temperature in Celsius.&#34;&#34;&#34;
    with open(path, &#39;r&#39;) as f:
        temp_str = f.read().strip()
    return float(temp_str) / 1000.0

def burn_cpu(duration):
    &#34;&#34;&#34;Busy-loop hashing for &#39;duration&#39; seconds.&#34;&#34;&#34;
    end_time = time.time() + duration
    m = hashlib.md5()
    while time.time() &lt; end_time:
        m.update(b&#34;burning-cpu&#34;)

def worker_loop(worker_id, cmd_queue, done_queue):
    &#34;&#34;&#34;
    Worker process:
    - Pins itself to CPUs 1, 2, or 3 (avoiding CPU 0)
    - Burns CPU based on commands from main process
    &#34;&#34;&#34;
    available_cpus = [1, 2, 3]
    cpu_to_use = available_cpus[worker_id % len(available_cpus)]
    os.sched_setaffinity(0, {cpu_to_use})
    print(f&#34;Worker {worker_id} pinned to CPU {cpu_to_use}&#34;)

    while True:
        cmd = cmd_queue.get()
        if cmd is None:
            break

        burn_time, sleep_time = cmd
        burn_cpu(burn_time)
        time.sleep(sleep_time)
        done_queue.put(worker_id)

# Main control loop (simplified)
def main():
    target_temp = 54.0  # degrees Celsius
    control_window = 0.20  # 200ms cycle time

    pid = PIDController(Kp=0.05, Ki=0.02, Kd=0.0,
                        setpoint=target_temp,
                        sample_time=0.18)

    # Start 3 worker processes
    workers = []
    cmd_queues = []
    done_queue = multiprocessing.Queue()

    for i in range(3):
        q = multiprocessing.Queue()
        p = multiprocessing.Process(target=worker_loop, args=(i, q, done_queue))
        p.start()
        workers.append(p)
        cmd_queues.append(q)

    try:
        while True:
            # Measure temperature
            current_temp = read_cpu_temperature()

            # PID control: output is fraction of time to burn (0.0 to 1.0)
            output = pid.update(current_temp)

            # Convert to burn/sleep times
            burn_time = output * control_window
            sleep_time = control_window - burn_time

            # Send command to all workers
            for q in cmd_queues:
                q.put((burn_time, sleep_time))

            # Wait for workers to complete
            for _ in range(3):
                done_queue.get()

            print(f&#34;Temp={current_temp:.2f}C, Output={output:.2f}, &#34;
                  f&#34;Burn={burn_time:.2f}s&#34;)

    except KeyboardInterrupt:
        for q in cmd_queues:
            q.put(None)
        for p in workers:
            p.join()

if __name__ == &#39;__main__&#39;:
    main()</code></pre>



<p>The full implementation includes a temperature filtering system to smooth out sensor noise and command-line arguments for tuning the PID parameters.</p>



<p><strong>PID Tuning Notes:</strong></p>



<ul>
<li><strong>Kp=0.05</strong>: Proportional gain – responds to current error</li>



<li><strong>Ki=0.02</strong>: Integral gain – eliminates steady-state error</li>



<li><strong>Kd=0.0</strong>: Derivative gain – set to zero because temperature changes slowly</li>
</ul>



<p>The target temperature of 54°C was chosen empirically – high enough to keep the CPU from idling down, but low enough to avoid thermal throttling (which starts around 80°C on Raspberry Pi).</p>



<h2>The Results: Numbers Don’t Lie</h2>



<p>The improvement was immediately visible. Here are the statistics comparing performance before and after the optimization:</p>



<p><strong>A note on ambient conditions:</strong> The Raspberry Pi lives in a project enclosure in our master bedroom (chosen for its decent GPS reception and ADS-B coverage for a new <a href="https://skyspottr.com/map/">aircraft AR overlay app idea I’m working on</a> also running on this Pi). While the time burner maintains the CPU die temperature at 54°C, the enclosure is still subject to ambient temperature swings. Room temperature cycles from a low of 66°F (18.9°C) at 5:15 AM to a peak of 72°F (22.2°C) at 11:30 AM – a 6°F daily swing from our heating schedule. The fact that we see such dramatic frequency stability improvements <em>despite</em> this ambient variation speaks to how effective the thermal control is. The CPU’s active heating overwhelms the environmental changes, maintaining consistent silicon temperature where it matters most.</p>



<h3>Frequency Stability</h3>



<figure><img loading="lazy" decoding="async" width="2082" height="880" src="https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability.png" alt="Frequency Variability" srcset="https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability.png 2082w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-300x127.png 300w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-800x338.png 800w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-768x325.png 768w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-1536x649.png 1536w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-2048x866.png 2048w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-1200x507.png 1200w, https://austinsnerdythings.com/wp-content/uploads/2025/11/ntp_frequency_variability-1980x837.png 1980w" sizes="auto, (max-width: 2082px) 100vw, 2082px"/></figure>



<figure><table><thead><tr><th>Metric</th><th>Before</th><th>After</th><th>Improvement</th></tr></thead><tbody><tr><td><strong>Mean RMS Offset</strong></td><td>85.44 ns</td><td>43.54 ns</td><td><strong>49.0% reduction</strong></td></tr><tr><td><strong>Median RMS Offset</strong></td><td>80.13 ns</td><td>37.93 ns</td><td><strong>52.7% reduction</strong></td></tr></tbody></table></figure>



<p>The RMS offset is chronyd’s estimate of the timing uncertainty. Cutting this nearly in half means the system is maintaining significantly better time accuracy.</p>



<h2>Setup Instructions</h2>



<p>Want to replicate this? Here’s the step-by-step process:</p>



<h3>Prerequisites</h3>



<p>You need a working GPS PPS NTP server setup. If you don’t have one yet, follow my <a href="https://austinsnerdythings.com/2025/02/14/revisiting-microsecond-accurate-ntp-for-raspberry-pi-with-gps-pps-in-2025/">2025 NTP guide</a> first.</p>



<h3>Step 0: Install Required Tools</h3>



<pre><code>sudo apt-get update
sudo apt-get install linux-cpupower python3 util-linux</code></pre>



<h3>Step 1: Create the Boot Optimization Script</h3>



<p>Save the optimization script from earlier as <code>/usr/local/bin/pps-optimize.sh</code>:</p>



<pre><code>sudo nano /usr/local/bin/pps-optimize.sh
# Paste the script content
sudo chmod +x /usr/local/bin/pps-optimize.sh</code></pre>



<h3>Step 2: Create Systemd Service for Boot Script</h3>



<p>Create <code>/etc/systemd/system/pps-optimize.service</code>:</p>



<pre><code>[Unit]
Description=PPS NTP Performance Optimization
After=chronyd.service
Requires=chronyd.service

[Service]
Type=oneshot
ExecStart=/usr/local/bin/pps-optimize.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target</code></pre>



<p>Enable it:</p>



<pre><code>sudo systemctl enable pps-optimize.service</code></pre>



<h3>Step 3: Install the Time Burner Script</h3>



<p>Save the time burner Python script as <code>/usr/local/bin/time_burner.py</code>:</p>



<pre><code>sudo nano /usr/local/bin/time_burner.py
# Paste the full time burner script
sudo chmod +x /usr/local/bin/time_burner.py</code></pre>



<h3>Step 4: Create Systemd Service for Time Burner</h3>



<p>Create <code>/etc/systemd/system/time-burner.service</code>:</p>



<pre><code>[Unit]
Description=CPU Thermal Stabilization for NTP
After=network.target

[Service]
Type=simple
User=root
ExecStart=/usr/bin/python3 /usr/local/bin/time_burner.py -t 54.0 -n 3
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target</code></pre>



<p>Enable and start it:</p>



<pre><code>sudo systemctl enable time-burner.service
sudo systemctl start time-burner.service</code></pre>



<h3>Step 5: Verify the Setup</h3>



<p>Check that everything is running:</p>



<pre><code># Verify CPU governor
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
# Should output: performance

# Check chronyd CPU affinity and priority
ps -eo pid,comm,psr,ni,rtprio | grep chronyd
# Should show psr=0 (CPU 0) and rtprio=50

# Check time burner processes
ps aux | grep time_burner
# Should show 4 processes (1 main + 3 workers)

# Monitor NTP performance
chronyc tracking</code></pre>



<p>Example output from <code>chronyc tracking</code>:</p>



<pre><code>Reference ID    : 50505300 (PPS)
Stratum         : 1
Ref time (UTC)  : Sun Nov 24 16:45:23 2025
System time     : 0.000000038 seconds fast of NTP time
Last offset     : -0.000000012 seconds
RMS offset      : 0.000000035 seconds
Frequency       : 1.685 ppm slow
Residual freq   : -0.001 ppm
Skew            : 0.002 ppm
Root delay      : 0.000000001 seconds
Root dispersion : 0.000010521 seconds
Update interval : 16.0 seconds
Leap status     : Normal</code></pre>



<p>Notice the RMS offset of 35 nanoseconds – this is the kind of accuracy you can achieve with thermal stabilization.</p>



<h3>Step 6: Monitor Over Time</h3>



<p>(Topic for a future post)</p>



<p>Set up Grafana dashboards to monitor:</p>



<ul>
<li>Frequency offset (PPM)</li>



<li>RMS offset (nanoseconds)</li>



<li>CPU temperature</li>



<li>System time offset</li>
</ul>



<p>You’ll see the frequency stabilize within a few hours as the PID controller locks onto the target temperature.</p>



<h2>Monitoring and Troubleshooting</h2>



<h3>Real-Time Monitoring</h3>



<p>Watch chronyd tracking in real-time:</p>



<pre><code>watch -n 1 &#34;chronyc tracking&#34;</code></pre>



<p>Check time burner status:</p>



<pre><code>sudo systemctl status time-burner.service</code></pre>



<p>View time burner output:</p>



<pre><code>sudo journalctl -u time-burner.service -f</code></pre>



<h3>Common Issues</h3>



<p><strong>Temperature overshoots or oscillates:</strong></p>



<ul>
<li>Adjust PID gains – reduce Kp if oscillating, increase Ki if steady-state error</li>



<li>Try different target temperatures (50-60°C range)</li>
</ul>



<p><strong>High CPU usage (obviously):</strong></p>



<ul>
<li>This is intentional – the time burner uses ~90% of 3 cores</li>



<li>Not suitable for Pis running other workloads</li>
</ul>



<p><strong>Chronyd not pinned to CPU 0:</strong></p>



<ul>
<li>Check that the optimization script runs after chronyd starts</li>



<li>Adjust the timing in the systemd service dependencies</li>
</ul>



<h2>Trade-offs and Considerations</h2>



<p>Let’s be honest about the downsides:</p>



<h3>Power Consumption</h3>



<p>The time burner keeps 3 cores at ~30% average utilization. My Pi now draws about 3-4W continuously (vs 1-2W idle). Over a year, that’s an extra 15-25 kWh, or about $2-3 in electricity (depending on your rates).</p>



<h3>Heat</h3>



<p>Running at 54°C means the Pi is warm to the touch. This is well within safe operating temperature (thermal throttling doesn’t start until 80°C), but you might want to ensure adequate ventilation. I added a small heatsink just to be safe.</p>



<h3>CPU Resources</h3>



<p>You’re dedicating 3 of 4 cores to burning cycles. This is fine for a dedicated NTP server, but not suitable if you’re running other services on the same Pi. That said, I am also running the feeder to my new <a href="https://skyspottr.com/map/">ADS-B aircraft visualization app</a> on it. My readsb instance regularly gets to 1200 msg/s with 200+ aircraft.</p>



<h3>Is It Worth It?</h3>



<p>For 99.999% of use cases: <strong>absolutely not</strong>.</p>



<p>Most applications don’t need better than millisecond accuracy, let alone the 35-nanosecond RMS offset I’m achieving. Even for distributed systems, microsecond-level accuracy is typically overkill.</p>



<p><strong>When this might make sense:</strong></p>



<ul>
<li><strong>Precision timing applications</strong> (scientific instrumentation, radio astronomy)</li>



<li><strong>Distributed systems research</strong> requiring tight clock synchronization</li>



<li><strong>Network testing</strong> where timing precision affects results</li>



<li><strong>Because you can</strong> (the best reason for any homelab project)</li>
</ul>



<p>For me, this falls squarely in the “because you can” category. I had the monitoring infrastructure in place, noticed the thermal correlation, and couldn’t resist solving the problem. Plus, I learned a lot about PID control, CPU thermal characteristics, and Linux real-time scheduling.</p>



<h2>Future Improvements</h2>



<p>Some ideas I’m considering:</p>



<h3>Adaptive PID Tuning</h3>



<p>The current PID gains are hand-tuned for a specific ambient temperature range. The fairly low P value is to avoid spikes when some load on the Pi kicks up the temp. The I is a balance to keep long term “burn” relatively consistent. Implementing an auto-tuning algorithm (like Ziegler-Nichols) or adaptive PID could handle seasonal temperature variations better.</p>



<h3>Hardware Thermal Control</h3>



<p>Instead of software thermal control, I could add an actively cooled heatsink with PWM fan control. This might achieve similar temperature stability while using less power overall.</p>



<h3>Oven-Controlled Crystal Oscillator (OCXO)</h3>



<p>For the ultimate in frequency stability, replacing the Pi’s crystal with a temperature-controlled OCXO would eliminate thermal drift at the source. This is how professional timing equipment works. I do have a BH3SAP GPSDO sitting next to me (subject to a future post)… Then again, I’m the person who just wrote 4000 words about optimizing a $50 time server, so who am I kidding?</p>



<h2>Conclusions</h2>



<p>Through a combination of CPU core isolation and PID-controlled thermal stabilization, I achieved:</p>



<ul>
<li><strong>81% reduction</strong> in frequency variability</li>



<li><strong>77% reduction</strong> in frequency standard deviation</li>



<li><strong>74% reduction</strong> in frequency range</li>



<li><strong>49% reduction</strong> in RMS offset</li>
</ul>



<p>The system now maintains 38-nanosecond median RMS offset from the GPS PPS reference, with frequency drift that’s barely detectable in the noise. The CPU runs at a constant 54°C, and in steady state, the frequency offset stays within a tight ±0.14 PPM band (compared to ±0.52 PPM before optimization).</p>



<p>Was this necessary? No. Did I learn a bunch about thermal management, PID control, and Linux real-time scheduling? Yes. Would I do it again? Absolutely.</p>



<h3>Resource</h3>



<p>I did come across a “burn” script that was the basis for this thermal management. I can’t find it at the moment, but when I do I’ll link it here.</p>



<h3>Related Posts</h3>



<ul>
<li><a href="https://austinsnerdythings.com/2021/04/19/microsecond-accurate-ntp-with-a-raspberry-pi-and-pps-gps/">Microsecond-Accurate NTP with a Raspberry Pi and PPS GPS (2021)</a></li>



<li><a href="https://austinsnerdythings.com/2025/02/14/revisiting-microsecond-accurate-ntp-for-raspberry-pi-with-gps-pps-in-2025/">Revisiting Microsecond-Accurate NTP for Raspberry Pi in 2025</a></li>
</ul>



<h3>Further Reading</h3>



<ul>
<li><a href="https://chrony.tuxfamily.org/documentation.html">Chrony Documentation</a></li>



<li><a href="https://en.wikipedia.org/wiki/PID_controller">PID Control Theory</a></li>



<li><a href="https://www.kernel.org/doc/html/latest/scheduler/sched-rt-group.html">Linux Real-Time Scheduling</a></li>
</ul>



<p>Have questions or suggestions? Drop a comment below. I’m particularly interested to hear if anyone has tried alternative thermal management approaches or has experience with OCXO modules for Raspberry Pi timing applications.</p>



<p>Thanks for reading, and happy timekeeping!</p>
<p><span></span> <span>Post Views:</span> <span>19,076</span>
			</p>
		</div><!-- .entry-content -->

	</div></div>
  </body>
</html>
