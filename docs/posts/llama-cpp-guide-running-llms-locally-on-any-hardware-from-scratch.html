<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://steelph0enix.github.io/posts/llama-cpp-guide/">Original</a>
    <h1>Llama.cpp guide – Running LLMs locally on any hardware, from scratch</h1>
    
    <div id="readability-page-1" class="page"><div><div><p><em>No LLMs were harmed during creation of this post.</em></p><h2 id="so-i-started-playing-with-llms">so, i started playing with LLMs…<a href="#so-i-started-playing-with-llms" arialabel="Anchor">#</a></h2><p>…and it’s pretty fun.
I was very skeptical about the AI/LLM “boom” back when it started.
I thought, like many other people, that they are just mostly making stuff up, and generating uncanny-valley-tier nonsense.
Boy, was i wrong.
I’ve used ChatGPT once or twice, to test the waters - it made a pretty good first impression, despite hallucinating a bit.
That was back when GPT3.5 was the top model. We came a pretty long way since then.</p><p>However, despite ChatGPT not disappointing me, i was still skeptical.
Everything i’ve wrote, and every piece of response was fully available to OpenAI, or whatever other provider i’d want to use.
This is not a big deal, but it tickles me in a wrong way, and also means i can’t use LLMs for any work-related non-open-source stuff.
Also, ChatGPT is free only to a some degree - if i’d want to go full-in on AI, i’d probably have to start paying.
Which, obviously, i’d rather avoid.</p><p>At some point i started looking at open-source models.
I had no idea how to use them, but the moment i saw the sizes of “small” models, like Llama 2 7B, i’ve realized that my RTX 2070 Super with mere 8GB of VRAM would probably have issues running them (i was wrong on that too!), and running them on CPU would probably yield very bad performance.
And then, i’ve bought a new GPU - RX 7900 XT, with 20GB of VRAM, which is definitely more than enough to run small-to-medium LLMs.
Yay!</p><p>Now my issue was finding some software that could run an LLM on that GPU.
CUDA was the most popular back-end - but that’s for NVidia GPUs, not AMD.
After doing a bit of research, i’ve found out about ROCm and found <a href="https://lmstudio.ai/">LM Studio</a>.
And this was exactly what i was looking for - at least for the time being.
Great UI, easy access to many models, and the quantization - that was the thing that absolutely sold me into self-hosting LLMs.
Existence of quantization made me realize that you don’t need powerful hardware for running LLMs!
You can even run <a href="https://www.reddit.com/r/raspberry_pi/comments/1ati2ki/how_to_run_a_large_language_model_llm_on_a/">LLMs on RaspberryPi’s</a> at this point (with <code>llama.cpp</code> too!)
Of course, the performance will be <em>abysmal</em> if you don’t run the LLM with a proper backend on a decent hardware, but the bar is currently not very high.</p><p>If you came here with intention of finding some piece of software that will allow you to <strong>easily run popular models on most modern hardware for non-commercial purposes</strong> - grab <a href="https://lmstudio.ai/">LM Studio</a>, read the <a href="https://steelph0enix.github.io/posts/llama-cpp-guide/#but-first---some-disclaimers-for-expectation-management">next section</a> of this post, and go play with it.
It fits this description very well, just make sure to use appropriate back-end for your GPU/CPU for optimal performance.</p><p>However, if you:</p><ul><li>Want to learn more about <code>llama.cpp</code> (which LM Studio uses as a back-end), and LLMs in general</li><li>Want to use LLMs for commercial purposes (<a href="https://lmstudio.ai/terms">LM Studio’s terms</a> forbid that)</li><li>Want to run LLMs on exotic hardware (LM Studio provides only the most popular backends)</li><li>Don’t like closed-source software (which LM Studio, unfortunately, is) and/or don’t trust anything you don’t build yourself</li><li>Want to have access to latest features and models as soon as possible</li></ul><p>you should find the rest of this post pretty useful!</p><h2 id="but-first---some-disclaimers-for-expectation-management">but first - some disclaimers for expectation management<a href="#but-first---some-disclaimers-for-expectation-management" arialabel="Anchor">#</a></h2><p>Before i proceed, i want to make some stuff clear.
This “FAQ” answers some questions i’d like to know answers to before getting into self-hosted LLMs.</p><h3 id="do-i-need-rtx-2070-superrx-7900-xt-ot-similar-midhigh-end-gpu-to-do-what-you-did-here">Do I need RTX 2070 Super/RX 7900 XT ot similar mid/high-end GPU to do what you did here?<a href="#do-i-need-rtx-2070-superrx-7900-xt-ot-similar-midhigh-end-gpu-to-do-what-you-did-here" arialabel="Anchor">#</a></h3><p>No, you don’t.
I’ll elaborate later, but you can run LLMs with no GPU at all.
As long as you have reasonably modern hardware (by that i mean <em>at least</em> a decent CPU with AVX support) - you’re <em>compatible</em>.
<strong>But remember - your performance may vary.</strong></p><h3 id="what-performance-can-i-expect">What performance can I expect?<a href="#what-performance-can-i-expect" arialabel="Anchor">#</a></h3><p>This is a very hard question to answer directly.
The speed of text generation depends on multiple factors, but primarily</p><ul><li>matrix operations performance on your hardware</li><li>memory bandwidth</li><li>model size</li></ul><p>I’ll explain this with more details later, but you can generally get reasonable performance from the LLM by picking model small enough for your hardware.
If you intend to use GPU, and it has enough memory for a model with it’s context - expect real-time text generation.
In case you want to use both GPU and CPU, or only CPU - you should expect much lower performance, but real-time text generation is possible with small models.</p><h3 id="what-quality-of-responses-can-i-expect">What quality of responses can I expect?<a href="#what-quality-of-responses-can-i-expect" arialabel="Anchor">#</a></h3><p>That heavily depends on your usage and chosen model.
I can’t answer that question directly, you’ll have to play around and find out yourself.
A rule of thumb is “larger the model, better the response” - consider the fact that size of SOTA (state-of-the-art) models, like GPT-4 or Claude, is usually measured in hundreds of billions of parameters.
Unless you have multiple GPUs or unreasonable amount of RAM and patience - you’ll most likely be restricted to models with less than 20 billion parameters.
From my experience, 7-8B models are pretty good for generic purposes and programming - and they are not <em>very</em> far from SOTA models like GPT-4o or Claude in terms of raw quality of generated responses, but the difference is definitely noticeable.
Keep in mind that the choice of a model is only a part of the problem - providing proper context and system prompt, or fine-tuning LLMs can do wonders.</p><h3 id="can-i-replace-chatgptclaudeinsert-online-llm-provider-with-that">Can i replace ChatGPT/Claude/[insert online LLM provider] with that?<a href="#can-i-replace-chatgptclaudeinsert-online-llm-provider-with-that" arialabel="Anchor">#</a></h3><p>Maybe. In theory - yes, but in practice - it depends on your tools.
<code>llama.cpp</code> provides OpenAI-compatible server.
As long as your tools communicate with LLMs via OpenAI API, and you are able to set custom endpoint, you will be able to use self-hosted LLM with them.</p><h2 id="prerequisites">prerequisites<a href="#prerequisites" arialabel="Anchor">#</a></h2><ul><li>Reasonably modern CPU.
If you’re rocking any Ryzen, or Intel’s 8th gen or newer, you’re good to go, but all of this should work on older hardware too.</li><li>Optimally, a GPU.
More VRAM, the better.
If you have at least 8GB of VRAM, you should be able to run 7-8B models, i’d say that it’s reasonable minimum.
Vendor doesn’t matter, llama.cpp supports NVidia, AMD and Apple GPUs (not sure about Intel, but i think i saw a backend for that - if not, Vulkan should work).</li><li>If you’re not using GPU or it doesn’t have enough VRAM, you need RAM for the model.
As above, at least 8GB of free RAM is recommended, but more is better.
Keep in mind that when only GPU is used by llama.cpp, RAM usage is very low.</li></ul><p>In the guide, i’ll assume you’re using either Windows or Linux.
I can’t provide any support for Mac users, so they should follow Linux steps and consult the llama.cpp docs wherever possible.</p><p>Some context-specific formatting is used in this post:</p><blockquote><p>Parts of this post where i’ll write about Windows-specific stuff will have this background.
You’ll notice they’re much longer than Linux ones - Windows is a PITA.
Linux is preferred. I will still explain everything step-by-step for Windows, but in case of issues - try Linux.</p></blockquote><blockquote><p>And parts where i’ll write about Linux-specific stuff will have this background.</p></blockquote><h2 id="building-the-llama">building the llama<a href="#building-the-llama" arialabel="Anchor">#</a></h2><p>In <a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md"><code>docs/build.md</code></a>, you’ll find detailed build instructions for all the supported platforms.
By default, <code>llama.cpp</code> builds with auto-detected CPU support.
We’ll talk about enabling GPU support later, first - let’s try building it as-is, because it’s a good baseline to start with, and it doesn’t require any external dependencies.
To do that, we only need a C++ toolchain, <a href="https://cmake.org/">CMake</a> and <a href="https://ninja-build.org/">Ninja</a>.</p><blockquote><p>If you are <strong>very</strong> lazy, you can download a release from Github and skip building steps.
Make sure to download correct version for your hardware/backend.
If you have troubles picking, i recommend following the build guide anyway - it’s simple enough and should explain what you should be looking for.
Keep in mind that release won’t contain Python scripts that we’re going to use, so if you’ll want to quantize models manually, you’ll need to get them from repository.</p></blockquote><p>On Windows, i recommend using <a href="https://www.msys2.org/">MSYS</a> to setup the environment for building and using <code>llama.cpp</code>.
<a href="https://visualstudio.microsoft.com/downloads/">Microsoft Visual C++</a> is supported too, but trust me on that - you’ll want to use MSYS instead (it’s still a bit of pain in the ass, Linux setup is much simpler).
Follow the guide on the main page to install MinGW for x64 UCRT environment, which you probably should be using.
CMake, Ninja and Git can be installed in UCRT MSYS environment like that:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>pacman -S git mingw-w64-ucrt-x86_64-cmake mingw-w64-ucrt-x86_64-ninja
</span></span></code></pre></div><p>However, if you’re using any other toolchain (MSVC, or non-MSYS one), you should install CMake, Git and Ninja via <code>winget</code>:</p><div><pre tabindex="0"><code data-lang="powershell"><span><span>winget install cmake git.git ninja-build.ninja
</span></span></code></pre></div><p>You’ll also need Python, which you can get via winget.
Get the latest available version, at the time of writing this post it’s 3.13.</p><p><strong>DO NOT USE PYTHON FROM MSYS, IT WILL NOT WORK PROPERLY DUE TO ISSUES WITH BUILDING <code>llama.cpp</code> DEPENDENCY PACKAGES!</strong>
<strong>We’re going to be using MSYS only for <em>building</em> <code>llama.cpp</code>, nothing more.</strong></p><p><strong>If you’re using MSYS, remember to add it’s <code>/bin</code> (<code>C:\msys64\ucrt64\bin</code> by default) directory to PATH, so Python can use MinGW for building packages.</strong>
<strong>Check if GCC is available by opening PowerShell/Command line and trying to run <code>gcc --version</code>.</strong>
Also; check if it’s <em>correct</em> GCC by running <code>where.exe gcc.exe</code> and seeing where the first entry points to.
Reorder your PATH if you’ll notice that you’re using wrong GCC.</p><p><strong>If you’re using MSVC - ignore this disclaimer, it should be “detectable” by default.</strong></p><div><pre tabindex="0"><code data-lang="powershell"><span><span>winget install python.python.3.13
</span></span></code></pre></div><p>I recommend installing/upgrading <code>pip</code>, <code>setuptools</code> and <code>wheel</code> packages before continuing.</p><div><pre tabindex="0"><code data-lang="powershell"><span><span>python -m pip install --upgrade pip wheel setuptools
</span></span></code></pre></div><p>On Linux, GCC is recommended, but you should be able to use Clang if you’d prefer by setting <code>CMAKE_C_COMPILER=clang</code> and <code>CMAKE_CXX_COMPILER=clang++</code> variables.
You should have GCC preinstalled (check <code>gcc --version</code> in terminal), if not - get latest version for your distribution using your package manager.
Same applies to CMake, Ninja, Python 3 (with <code>setuptools</code>, <code>wheel</code> and <code>pip</code>) and Git.</p><p>Let’s start by grabbing a copy of <a href="https://github.com/ggerganov/llama.cpp"><code>llama.cpp</code> source code</a>, and moving into it.</p><blockquote><p>Disclaimer: this guide assumes all commands are ran from user’s home directory (<code>/home/[yourusername]</code> on Linux, <code>C:/Users/[yourusername]</code> on Windows).
You can use any directory you’d like, just keep in mind that if “starting directory” is not explicitly mentioned, start from home dir/your chosen one.</p></blockquote><blockquote><p><strong>If you’re using MSYS</strong>, remember that MSYS home directory is different from Windows home directory. Make sure to use <code>cd</code> (without arguments) to move into it after starting MSYS.</p></blockquote><p><em>(If you have previously configured SSH auth w/ GitHub, use <code>git@github.com:ggerganov/llama.cpp.git</code> instead of the URL below)</em></p><div><pre tabindex="0"><code data-lang="sh"><span><span>git clone https://github.com/ggerganov/llama.cpp.git
</span></span><span><span>cd llama.cpp
</span></span><span><span>git submodule update --init --recursive
</span></span></code></pre></div><p>Now we’ll use CMake to generate build files, build the project, and install it.
Run the following command to generate build files in <code>build/</code> subdirectory:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE<span>=</span>Release -DCMAKE_INSTALL_PREFIX<span>=</span>/your/install/dir -DLLAMA_BUILD_TESTS<span>=</span>OFF -DLLAMA_BUILD_EXAMPLES<span>=</span>ON -DLLAMA_BUILD_SERVER<span>=</span>ON
</span></span></code></pre></div><p>There’s a lot of CMake variables being defined, which we could ignore and let llama.cpp use it’s defaults, but we won’t:</p><ul><li><code>CMAKE_BUILD_TYPE</code> is set to release for obvious reasons - we want maximum performance.</li><li><a href="https://cmake.org/cmake/help/latest/variable/CMAKE_INSTALL_PREFIX.html"><code>CMAKE_INSTALL_PREFIX</code></a> is where the <code>llama.cpp</code> binaries and python scripts will go. Replace the value of this variable, or remove it’s definition to keep default value.<ul><li>On Windows, default directory is <code>c:/Program Files/llama.cpp</code>.
As above, you’ll need admin privileges to install it, and you’ll have to add the <code>bin/</code> subdirectory to your <code>PATH</code> to make llama.cpp binaries accessible system-wide.
I prefer installing llama.cpp in <code>$env:LOCALAPPDATA/llama.cpp</code> (<code>C:/Users/[yourusername]/AppData/Local/llama.cpp</code>), as it doesn’t require admin privileges.</li><li>On Linux, default directory is <code>/usr/local</code>.
You can ignore this variable if that’s fine with you, but you’ll need superuser permissions to install the binaries there.
If you don’t have them, change it to point somewhere in your user directory and add it’s <code>bin/</code> subdirectory to <code>PATH</code>.</li></ul></li><li><code>LLAMA_BUILD_TESTS</code> is set to <code>OFF</code> because we don’t need tests, it’ll make the build a bit quicker.</li><li><code>LLAMA_BUILD_EXAMPLES</code> is <code>ON</code> because we’re gonna be using them.</li><li><code>LLAMA_BUILD_SERVER</code> - see above. Note: Disabling <code>LLAMA_BUILD_EXAMPLES</code> unconditionally disables building the server, both must be <code>ON</code>.</li></ul><p>Now, let’s build the project.
Replace <code>X</code> with amount of cores your CPU has for faster compilation.
In theory, Ninja should automatically use all available cores, but i still prefer passing this argument manually.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cmake --build build --config Release -j X
</span></span></code></pre></div><p>Building should take only a few minutes.
After that, we can install the binaries for easier usage.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cmake --install build --config Release
</span></span></code></pre></div><p>Now, after going to <code>CMAKE_INSTALL_PREFIX/bin</code> directory, we should see a list of executables and Python scripts:</p><div><pre tabindex="0"><code data-lang="text"><span><span>/c/Users/phoen/llama-build/bin
</span></span><span><span>❯ l
</span></span><span><span>Mode  Size Date Modified Name
</span></span><span><span>-a--- 203k  7 Nov 16:14  convert_hf_to_gguf.py
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-batched-bench.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-batched.exe
</span></span><span><span>-a--- 3.4M  7 Nov 16:18  llama-bench.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-cli.exe
</span></span><span><span>-a--- 3.2M  7 Nov 16:18  llama-convert-llama2c-to-ggml.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-cvector-generator.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-embedding.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-eval-callback.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-export-lora.exe
</span></span><span><span>-a--- 3.0M  7 Nov 16:18  llama-gbnf-validator.exe
</span></span><span><span>-a--- 1.2M  7 Nov 16:18  llama-gguf-hash.exe
</span></span><span><span>-a--- 3.0M  7 Nov 16:18  llama-gguf-split.exe
</span></span><span><span>-a--- 1.1M  7 Nov 16:18  llama-gguf.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-gritlm.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-imatrix.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-infill.exe
</span></span><span><span>-a--- 4.2M  7 Nov 16:18  llama-llava-cli.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-lookahead.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-lookup-create.exe
</span></span><span><span>-a--- 1.2M  7 Nov 16:18  llama-lookup-merge.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-lookup-stats.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-lookup.exe
</span></span><span><span>-a--- 4.1M  7 Nov 16:18  llama-minicpmv-cli.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-parallel.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-passkey.exe
</span></span><span><span>-a--- 4.0M  7 Nov 16:18  llama-perplexity.exe
</span></span><span><span>-a--- 3.0M  7 Nov 16:18  llama-quantize-stats.exe
</span></span><span><span>-a--- 3.2M  7 Nov 16:18  llama-quantize.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-retrieval.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-save-load-state.exe
</span></span><span><span>-a--- 5.0M  7 Nov 16:19  llama-server.exe
</span></span><span><span>-a--- 3.0M  7 Nov 16:18  llama-simple-chat.exe
</span></span><span><span>-a--- 3.0M  7 Nov 16:18  llama-simple.exe
</span></span><span><span>-a--- 3.9M  7 Nov 16:18  llama-speculative.exe
</span></span><span><span>-a--- 3.1M  7 Nov 16:18  llama-tokenize.exe
</span></span></code></pre></div><p>Don’t feel overwhelmed by the amount, we’re only going to be using few of them.
You should try running one of them to check if the executables have built correctly, for example - try <code>llama-cli --help</code>.
We can’t do anything meaningful yet, because we lack a single critical component - a model to run.</p><h2 id="getting-a-model">getting a model<a href="#getting-a-model" arialabel="Anchor">#</a></h2><p>The main place to look for models is <a href="https://huggingface.co/">HuggingFace</a>.
You can also find datasets and other AI-related stuff there, great site.</p><p>We’re going to use <a href="https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9"><code>SmolLM2</code></a> here, a model series created by HuggingFace and published fairly recently (1st November 2024).
The reason i’ve chosen this model is the size - as the name implies, it’s <em>small</em>.
Largest model from this series has 1.7 billion parameters, which means that it requires approx. 4GB of system memory to run in <em>raw, unquantized</em> form (excluding context)!
There are also 360M and 135M variants, which are even smaller and should be easily runnable on RaspberryPi or a smartphone.</p><p>There’s but one issue - <code>llama.cpp</code> cannot run “raw” models directly.
What is usually provided by most LLM creators are original weights in <code>.safetensors</code> or similar format.
<code>llama.cpp</code> expects models in <code>.gguf</code> format.
Fortunately, there is a very simple way of converting original model weights into <code>.gguf</code> - <code>llama.cpp</code> provides <code>convert_hf_to_gguf.py</code> script exactly for this purpose!
Sometimes the creator provides <code>.gguf</code> files - for example, two variants of <code>SmolLM2</code> are provided by HuggingFace in this format.
This is not a very common practice, but you can also find models in <code>.gguf</code> format uploaded there by community.
However, i’ll ignore the existence of pre-quantized <code>.gguf</code> files here, and focus on quantizing our models by ourselves here, as it’ll allow us to experiment and adjust the quantization parameters of our model without having to download it multiple times.</p><p>Grab the content of <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct/tree/main">SmolLM2 1.7B Instruct</a> repository (you can use <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct/tree/main">360M Instruct</a> or <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main">135M Instruct</a> version instead, if you have less than 4GB of free (V)RAM - or use any other model you’re already familiar with, if it supports <code>transformers</code>), but omit the LFS files - we only need a single one, and we’ll download it manually.</p><blockquote><p>Why <em>Instruct</em>, specifically?
You might have noticed that there are two variants of all those models - <em>Instruct</em> and <em>the other one without a suffix</em>.
<em>Instruct</em> is trained for chat conversations, base model is only trained for text completion and is usually used as a base for further training.
This rule applies to most LLMs, but not all, so make sure to read the model’s description before using it!</p></blockquote><p>If you’re using Bash/ZSH or compatible shell:</p><p><em>MSYS uses Bash by default, so it applies to it too.</em>
<em>From now on, assume that Linux commands work on MSYS too, unless i explicitly say otherwise.</em></p><div><pre tabindex="0"><code data-lang="sh"><span><span>GIT_LFS_SKIP_SMUDGE<span>=</span><span>1</span> git clone https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct
</span></span></code></pre></div><p>If you’re using PowerShell:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>$env:GIT_LFS_SKIP_SMUDGE<span>=</span><span>1</span>
</span></span><span><span>git clone https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct
</span></span></code></pre></div><p>If you’re using cmd.exe (VS Development Prompt, for example):</p><div><pre tabindex="0"><code data-lang="sh"><span><span>set GIT_LFS_SKIP_SMUDGE<span>=</span><span>1</span>
</span></span><span><span>git clone https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct
</span></span></code></pre></div><p>HuggingFace also supports Git over SSH. You can look up the <code>git clone</code> command for every repo here:
<img alt="huggingface - where to find git clone button" src="https://steelph0enix.github.io/img/llama-cpp/clone-hf-button.png"/></p><p>After cloning the repo, <strong>download the <code>model.safetensors</code> file from HuggingFace manually.</strong>
The reason why we used <code>GIT_LFS_SKIP_SMUDGE</code> is because there’s many other large model files hidden in repo, and we don’t need them.
Also, downloading very large files manually is faster, because Git LFS sucks in that regard.</p><p>After downloading everything, our local copy of the SmolLM repo should look like this:</p><div><pre tabindex="0"><code data-lang="text"><span><span>PS D:\LLMs\repos\SmolLM2-1.7B-Instruct&gt; l
</span></span><span><span>Mode  Size Date Modified Name
</span></span><span><span>-a---  806  2 Nov 15:16  all_results.json
</span></span><span><span>-a---  888  2 Nov 15:16  config.json
</span></span><span><span>-a---  602  2 Nov 15:16  eval_results.json
</span></span><span><span>-a---  139  2 Nov 15:16  generation_config.json
</span></span><span><span>-a--- 515k  2 Nov 15:16  merges.txt
</span></span><span><span>-a--- 3.4G  2 Nov 15:34  model.safetensors
</span></span><span><span>d----    -  2 Nov 15:16  onnx
</span></span><span><span>-a---  11k  2 Nov 15:16  README.md
</span></span><span><span>d----    -  2 Nov 15:16  runs
</span></span><span><span>-a---  689  2 Nov 15:16  special_tokens_map.json
</span></span><span><span>-a--- 2.2M  2 Nov 15:16  tokenizer.json
</span></span><span><span>-a--- 3.9k  2 Nov 15:16  tokenizer_config.json
</span></span><span><span>-a---  240  2 Nov 15:16  train_results.json
</span></span><span><span>-a---  89k  2 Nov 15:16  trainer_state.json
</span></span><span><span>-a---  129  2 Nov 15:16  training_args.bin
</span></span><span><span>-a--- 801k  2 Nov 15:16  vocab.json
</span></span></code></pre></div><p>We’re gonna (indirectly) use only four of those files:</p><ul><li><code>config.json</code> contains configuration/metadata of our model</li><li><code>model.safetensors</code> contains model weights</li><li><code>tokenizer.json</code> contains tokenizer data (mapping of text tokens to their ID’s, and other stuff).
Sometimes this data is stored in <code>tokenizer.model</code> file instead.</li><li><code>tokenizer_config.json</code> contains tokenizer configuration (for example, special tokens and chat template)</li></ul><p>i’m leaving this sentence here as anti-plagiarism token.
If you’re not currently reading this on my blog, which is @ steelph0enix.github.io, someone probably stolen that article without permission</p><h3 id="converting-huggingface-model-to-gguf">converting huggingface model to GGUF</h3><p>In order to convert this raw model to something that <code>llama.cpp</code> will understand, we’ll use aforementioned <code>convert_hf_to_gguf.py</code> script that comes with <code>llama.cpp</code>.
For all our Python needs, we’re gonna need a virtual environment.
I recommend making it outside of <code>llama.cpp</code> repo, for example - in your home directory.</p><p>To make one on Linux, run this command (tweak the path if you’d like):</p><div><pre tabindex="0"><code data-lang="sh"><span><span>python -m venv ~/llama-cpp-venv
</span></span></code></pre></div><p>If you’re using PowerShell, this is the equivalent:</p><div><pre tabindex="0"><code data-lang="powershell"><span><span>python -m venv $env:USERPROFILE/llama-cpp-venv
</span></span></code></pre></div><p>If you’re using cmd.exe, this is the equivalent:</p><div><pre tabindex="0"><code data-lang="batch"><span><span>python -m venv %USERPROFILE%/llama-cpp-venv
</span></span></code></pre></div><p>Then, we need to activate it.</p><p>On Linux:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>source ~/llama-cpp-venv/bin/activate
</span></span></code></pre></div><p>With PowerShell:</p><div><pre tabindex="0"><code data-lang="powershell"><span><span>. $env:USERPROFILE/llama-cpp-venv/Scripts/Activate.ps1
</span></span></code></pre></div><p>With cmd.exe:</p><div><pre tabindex="0"><code data-lang="cmd"><span><span><span>call</span> %USERPROFILE%/llama-cpp-venv/Scripts/activate.bat
</span></span></code></pre></div><p>After that, let’s make sure that our virtualenv has all the core packages up-to-date.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>python -m pip install --upgrade pip wheel setuptools
</span></span></code></pre></div><p>Next, we need to install prerequisites for the llama.cpp scripts.
Let’s look into <code>requirements/</code> directory of our <code>llama.cpp</code> repository.
We should see something like this:</p><div><pre tabindex="0"><code data-lang="text"><span><span>❯ l llama.cpp/requirements
</span></span><span><span>Mode  Size Date Modified Name
</span></span><span><span>-a---  428 11 Nov 13:57  requirements-all.txt
</span></span><span><span>-a---   34 11 Nov 13:57  requirements-compare-llama-bench.txt
</span></span><span><span>-a---  111 11 Nov 13:57  requirements-convert_hf_to_gguf.txt
</span></span><span><span>-a---  111 11 Nov 13:57  requirements-convert_hf_to_gguf_update.txt
</span></span><span><span>-a---   99 11 Nov 13:57  requirements-convert_legacy_llama.txt
</span></span><span><span>-a---   43 11 Nov 13:57  requirements-convert_llama_ggml_to_gguf.txt
</span></span><span><span>-a---   96 11 Nov 13:57  requirements-convert_lora_to_gguf.txt
</span></span><span><span>-a---   48 11 Nov 13:57  requirements-pydantic.txt
</span></span><span><span>-a---   13 11 Nov 13:57  requirements-test-tokenizer-random.txt
</span></span></code></pre></div><p>As we can see, there’s a file with deps for our script!
To install dependencies from it, run this command:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>python -m pip install --upgrade -r llama.cpp/requirements/requirements-convert_hf_to_gguf.txt
</span></span></code></pre></div><p>If <code>pip</code> failed during build, make sure you have working C/C++ toolchain in your <code>PATH</code>.</p><blockquote><p>If you’re using MSYS for that, don’t. Go back to PowerShell/cmd, install Python via winget and repeat the setup.
As far as i’ve tested it, Python deps don’t detect the platform correctly on MSYS and try to use wrong build config.
This is what i warned you about earlier.</p></blockquote><p>Now we can use the script to create our GGUF model file.
Start with printing the help and reading the options.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>python llama.cpp/convert_hf_to_gguf.py --help
</span></span></code></pre></div><p>If this command printed help, you can continue.
Otherwise make sure that python’s virtualenv is active and dependencies are correctly installed, and try again.
To convert our model we can simply pass the path to directory with model’s repository and, optionally, path to output file.
We don’t need to tweak the quantization here, for maximum flexibility we’re going to create a floating-point GGUF file which we’ll then quantize down.
That’s because <code>llama-quantize</code> offers much more quantization options, and this script picks optimal floating-point format by default.</p><p>To create GGUF file from our downloaded HuggingFace repository with SmolLM2 (Replace <code>SmolLM2-1.7B-Instruct</code> with your path, if it’s different) run this command:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>python llama.cpp/convert_hf_to_gguf.py SmolLM2-1.7B-Instruct --outfile ./SmolLM2.gguf
</span></span></code></pre></div><p>If everything went correctly, you should see similar output:</p><div><pre tabindex="0"><code data-lang="text"><span><span>INFO:hf-to-gguf:Loading model: SmolLM2-1.7B-Instruct
</span></span><span><span>INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
</span></span><span><span>INFO:hf-to-gguf:Exporting model...
</span></span><span><span>INFO:hf-to-gguf:gguf: loading model part &#39;model.safetensors&#39;
</span></span><span><span>INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --&gt; F16, shape = {2048, 49152}
</span></span><span><span>INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = {2048}
</span></span><span><span>...
</span></span><span><span>INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = {2048, 2048}
</span></span><span><span>INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = {2048, 2048}
</span></span><span><span>INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --&gt; F32, shape = {2048}
</span></span><span><span>INFO:hf-to-gguf:Set meta model
</span></span><span><span>INFO:hf-to-gguf:Set model parameters
</span></span><span><span>INFO:hf-to-gguf:gguf: context length = 8192
</span></span><span><span>INFO:hf-to-gguf:gguf: embedding length = 2048
</span></span><span><span>INFO:hf-to-gguf:gguf: feed forward length = 8192
</span></span><span><span>INFO:hf-to-gguf:gguf: head count = 32
</span></span><span><span>INFO:hf-to-gguf:gguf: key-value head count = 32
</span></span><span><span>INFO:hf-to-gguf:gguf: rope theta = 130000
</span></span><span><span>INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05
</span></span><span><span>INFO:hf-to-gguf:gguf: file type = 1
</span></span><span><span>INFO:hf-to-gguf:Set model tokenizer
</span></span><span><span>INFO:gguf.vocab:Adding 48900 merge(s).
</span></span><span><span>INFO:gguf.vocab:Setting special token type bos to 1
</span></span><span><span>INFO:gguf.vocab:Setting special token type eos to 2
</span></span><span><span>INFO:gguf.vocab:Setting special token type unk to 0
</span></span><span><span>INFO:gguf.vocab:Setting special token type pad to 2
</span></span><span><span>INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0][&#39;role&#39;] != &#39;system&#39; %}{{ &#39;&lt;|im_start|&gt;system
</span></span><span><span>You are a helpful AI assistant named SmolLM, trained by Hugging Face&lt;|im_end|&gt;
</span></span><span><span>&#39; }}{% endif %}{{&#39;&lt;|im_start|&gt;&#39; + message[&#39;role&#39;] + &#39;
</span></span><span><span>&#39; + message[&#39;content&#39;] + &#39;&lt;|im_end|&gt;&#39; + &#39;
</span></span><span><span>&#39;}}{% endfor %}{% if add_generation_prompt %}{{ &#39;&lt;|im_start|&gt;assistant
</span></span><span><span>&#39; }}{% endif %}
</span></span><span><span>INFO:hf-to-gguf:Set model quantization version
</span></span><span><span>INFO:gguf.gguf_writer:Writing the following files:
</span></span><span><span>INFO:gguf.gguf_writer:SmolLM2.gguf: n_tensors = 218, total_size = 3.4G
</span></span><span><span>Writing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.42G/3.42G [00:15&lt;00:00, 215Mbyte/s]
</span></span><span><span>INFO:hf-to-gguf:Model successfully exported to SmolLM2.gguf
</span></span></code></pre></div><h3 id="quantizing-the-model">quantizing the model<a href="#quantizing-the-model" arialabel="Anchor">#</a></h3><p>Now we can finally quantize our model!
To do that, we’ll use <code>llama-quantize</code> executable that we previously compiled with other <code>llama.cpp</code> executables.
First, let’s check what quantizations we have available.</p><p>As of now, <code>llama-quantize --help</code> shows following types:</p><div><pre tabindex="0"><code data-lang="text"><span><span>
</span></span><span><span>usage: llama-quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights] [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--override-kv] model-f32.gguf [model-quant.gguf] type [nthreads]
</span></span><span><span>
</span></span><span><span>...
</span></span><span><span>
</span></span><span><span>Allowed quantization types:
</span></span><span><span>   2  or  Q4_0    :  4.34G, +0.4685 ppl @ Llama-3-8B
</span></span><span><span>   3  or  Q4_1    :  4.78G, +0.4511 ppl @ Llama-3-8B
</span></span><span><span>   8  or  Q5_0    :  5.21G, +0.1316 ppl @ Llama-3-8B
</span></span><span><span>   9  or  Q5_1    :  5.65G, +0.1062 ppl @ Llama-3-8B
</span></span><span><span>  19  or  IQ2_XXS :  2.06 bpw quantization
</span></span><span><span>  20  or  IQ2_XS  :  2.31 bpw quantization
</span></span><span><span>  28  or  IQ2_S   :  2.5  bpw quantization
</span></span><span><span>  29  or  IQ2_M   :  2.7  bpw quantization
</span></span><span><span>  24  or  IQ1_S   :  1.56 bpw quantization
</span></span><span><span>  31  or  IQ1_M   :  1.75 bpw quantization
</span></span><span><span>  36  or  TQ1_0   :  1.69 bpw ternarization
</span></span><span><span>  37  or  TQ2_0   :  2.06 bpw ternarization
</span></span><span><span>  10  or  Q2_K    :  2.96G, +3.5199 ppl @ Llama-3-8B
</span></span><span><span>  21  or  Q2_K_S  :  2.96G, +3.1836 ppl @ Llama-3-8B
</span></span><span><span>  23  or  IQ3_XXS :  3.06 bpw quantization
</span></span><span><span>  26  or  IQ3_S   :  3.44 bpw quantization
</span></span><span><span>  27  or  IQ3_M   :  3.66 bpw quantization mix
</span></span><span><span>  12  or  Q3_K    : alias for Q3_K_M
</span></span><span><span>  22  or  IQ3_XS  :  3.3 bpw quantization
</span></span><span><span>  11  or  Q3_K_S  :  3.41G, +1.6321 ppl @ Llama-3-8B
</span></span><span><span>  12  or  Q3_K_M  :  3.74G, +0.6569 ppl @ Llama-3-8B
</span></span><span><span>  13  or  Q3_K_L  :  4.03G, +0.5562 ppl @ Llama-3-8B
</span></span><span><span>  25  or  IQ4_NL  :  4.50 bpw non-linear quantization
</span></span><span><span>  30  or  IQ4_XS  :  4.25 bpw non-linear quantization
</span></span><span><span>  15  or  Q4_K    : alias for Q4_K_M
</span></span><span><span>  14  or  Q4_K_S  :  4.37G, +0.2689 ppl @ Llama-3-8B
</span></span><span><span>  15  or  Q4_K_M  :  4.58G, +0.1754 ppl @ Llama-3-8B
</span></span><span><span>  17  or  Q5_K    : alias for Q5_K_M
</span></span><span><span>  16  or  Q5_K_S  :  5.21G, +0.1049 ppl @ Llama-3-8B
</span></span><span><span>  17  or  Q5_K_M  :  5.33G, +0.0569 ppl @ Llama-3-8B
</span></span><span><span>  18  or  Q6_K    :  6.14G, +0.0217 ppl @ Llama-3-8B
</span></span><span><span>   7  or  Q8_0    :  7.96G, +0.0026 ppl @ Llama-3-8B
</span></span><span><span>  33  or  Q4_0_4_4 :  4.34G, +0.4685 ppl @ Llama-3-8B
</span></span><span><span>  34  or  Q4_0_4_8 :  4.34G, +0.4685 ppl @ Llama-3-8B
</span></span><span><span>  35  or  Q4_0_8_8 :  4.34G, +0.4685 ppl @ Llama-3-8B
</span></span><span><span>   1  or  F16     : 14.00G, +0.0020 ppl @ Mistral-7B
</span></span><span><span>  32  or  BF16    : 14.00G, -0.0050 ppl @ Mistral-7B
</span></span><span><span>   0  or  F32     : 26.00G              @ 7B
</span></span><span><span>          COPY    : only copy tensors, no quantizing
</span></span></code></pre></div><p>Let’s decode this table.
From the left, we have IDs and names of quantization types - you can use either when calling <code>llama-quantize</code>.
After the <code>:</code>, there’s a short description that in most cases shows either the example model’s size and perplexity, or the amount of bits per tensor weight (bpw) for that specific quantization.
Perplexity is a metric that describes how certain the model is about it’s predictions.
We can think about it like that: lower perplexity -&gt; model is more certain about it’s predictions -&gt; model is more accurate.
This is a daily reminder that LLMs are nothing more than overcomplicated autocompletion algorithms.
The “bits per weight” metric tells us the average size of quantized tensor’s weight.
You may think it’s strange that those are floating-point values, but we’ll see the reason for that soon.</p><p>Now, the main question that needs to be answered is “which quantization do we pick?”.
And the answer is “it depends”.
My rule of thumb for picking quantization type is “the largest i can fit in my VRAM, unless it’s too slow for my taste” and i recommend this approach if you don’t know where to start!
Obviously, if you don’t have/want to use GPU, replace “VRAM” with “RAM” and “largest i can fit” with “largest i can fit without forcing the OS to move everything to swap”.
That creates another question - “what is the largest quant i can fit in my (V)RAM”?
And this depends on the original model’s size and encoding, and - obviously - the amount of (V)RAM you have.
Since we’re using SmolLM2, our model is relatively small.
GGUF file of 1.7B-Instruct variant in BF16 format weights 3.4GB.
Most models you’ll encounter will be encoded in either BF16 or FP16 format, rarely we can find FP32-encoded LLMs.
That means most of models have 16 bits per weight by default.
We can easily approximate the size of quantized model by multiplying the original size with approximate ratio of bits per weight.
For example, let’s assume we want to know how large will SmolLM2 1.7B-Instruct be after Q8_0 quantization.
Let’s assume Q8_0 quant uses 8 bits per word, which means the ratio is simply 1/2, so our model should weight ~1.7GB.
Let’s check that!</p><p>The first argument is source model, second - target file.
Third is the quantization type, and last is the amount of cores for parallel processing.
Replace <code>N</code> with amount of cores in your system and run this command:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>llama-quantize SmolLM2.gguf SmolLM2.q8.gguf Q8_0 N
</span></span></code></pre></div><p>You should see similar output:</p><div><pre tabindex="0"><code data-lang="text"><span><span>main: build = 4200 (46c69e0e)
</span></span><span><span>main: built with gcc (GCC) 14.2.1 20240910 for x86_64-pc-linux-gnu
</span></span><span><span>main: quantizing &#39;SmolLM2.gguf&#39; to &#39;SmolLM2.q8.gguf&#39; as Q8_0 using 24 threads
</span></span><span><span>llama_model_loader: loaded meta data with 37 key-value pairs and 218 tensors from SmolLM2.gguf (version GGUF V3 (latest))
</span></span><span><span>llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
</span></span><span><span>llama_model_loader: - kv   0:                       general.architecture str              = llama
</span></span><span><span>llama_model_loader: - kv   1:                               general.type str              = model
</span></span><span><span>llama_model_loader: - kv   2:                               general.name str              = SmolLM2 1.7B Instruct
</span></span><span><span>llama_model_loader: - kv   3:                           general.finetune str              = Instruct
</span></span><span><span>llama_model_loader: - kv   4:                           general.basename str              = SmolLM2
</span></span><span><span>llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
</span></span><span><span>llama_model_loader: - kv   6:                            general.license str              = apache-2.0
</span></span><span><span>llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
</span></span><span><span>llama_model_loader: - kv   8:                  general.base_model.0.name str              = SmolLM2 1.7B
</span></span><span><span>llama_model_loader: - kv   9:          general.base_model.0.organization str              = HuggingFaceTB
</span></span><span><span>llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/HuggingFaceTB/...
</span></span><span><span>llama_model_loader: - kv  11:                               general.tags arr[str,4]       = [&#34;safetensors&#34;, &#34;onnx&#34;, &#34;transformers...
</span></span><span><span>llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [&#34;en&#34;]
</span></span><span><span>llama_model_loader: - kv  13:                          llama.block_count u32              = 24
</span></span><span><span>llama_model_loader: - kv  14:                       llama.context_length u32              = 8192
</span></span><span><span>llama_model_loader: - kv  15:                     llama.embedding_length u32              = 2048
</span></span><span><span>llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 8192
</span></span><span><span>llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32
</span></span><span><span>llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 32
</span></span><span><span>llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 130000.000000
</span></span><span><span>llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
</span></span><span><span>llama_model_loader: - kv  21:                          general.file_type u32              = 32
</span></span><span><span>llama_model_loader: - kv  22:                           llama.vocab_size u32              = 49152
</span></span><span><span>llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 64
</span></span><span><span>llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2
</span></span><span><span>llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = smollm
</span></span><span><span>llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49152]   = [&#34;&lt;|endoftext|&gt;&#34;, &#34;&lt;|im_start|&gt;&#34;, &#34;&lt;|...
</span></span><span><span>llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
</span></span><span><span>llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48900]   = [&#34;Ġ t&#34;, &#34;Ġ a&#34;, &#34;i n&#34;, &#34;h e&#34;, &#34;Ġ Ġ...
</span></span><span><span>llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1
</span></span><span><span>llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 2
</span></span><span><span>llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0
</span></span><span><span>llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 2
</span></span><span><span>llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
</span></span><span><span>llama_model_loader: - kv  34:            tokenizer.ggml.add_space_prefix bool             = false
</span></span><span><span>llama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = false
</span></span><span><span>llama_model_loader: - kv  36:               general.quantization_version u32              = 2
</span></span><span><span>llama_model_loader: - type  f32:   49 tensors
</span></span><span><span>llama_model_loader: - type bf16:  169 tensors
</span></span><span><span>ggml_vulkan: Found 1 Vulkan devices:
</span></span><span><span>ggml_vulkan: 0 = AMD Radeon RX 7900 XT (AMD open-source driver) | uma: 0 | fp16: 1 | warp size: 64
</span></span><span><span>[   1/ 218]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
</span></span><span><span>[   2/ 218]                    token_embd.weight - [ 2048, 49152,     1,     1], type =   bf16, converting to q8_0 .. size =   192.00 MiB -&gt;   102.00 MiB
</span></span><span><span>[   3/ 218]                  blk.0.attn_k.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span><span>[   4/ 218]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
</span></span><span><span>[   5/ 218]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span><span>[   6/ 218]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span><span>[   7/ 218]                  blk.0.attn_v.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span><span>...
</span></span><span><span>[ 212/ 218]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span><span>[ 213/ 218]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span><span>[ 214/ 218]                 blk.23.attn_v.weight - [ 2048,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB -&gt;     4.25 MiB
</span></span><span><span>[ 215/ 218]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
</span></span><span><span>[ 216/ 218]               blk.23.ffn_gate.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
</span></span><span><span>[ 217/ 218]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
</span></span><span><span>[ 218/ 218]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB -&gt;    17.00 MiB
</span></span><span><span>llama_model_quantize_internal: model size  =  3264.38 MB
</span></span><span><span>llama_model_quantize_internal: quant size  =  1734.38 MB
</span></span><span><span>
</span></span><span><span>main: quantize time =  2289.97 ms
</span></span><span><span>main:    total time =  2289.97 ms
</span></span></code></pre></div><p>And yeah, we were more-or-less correct, it’s 1.7GB!
<strong>BUT</strong> this is only the model’s size, we also have to consider the memory requirements for the context.
Fortunately, context doesn’t require massive amounts of memory - i’m not really sure how much exactly it eats up, but it’s safe to assume that we should have at least 1GB memory for it.
So, taking all that in account, we can approximate that to load this model to memory with the context, we need at least 3GB of free (V)RAM.
This is not so bad, and most modern consumer GPUs have at least this amount (even the old GTX 1060 3GB should be able to run this model in Q8_0 quant).
However, if that’s still too much, we can easily go lower!
Reducing the amount of bits per weight via quantization not only reduces the model’s size, but also increases the speed of data generation.
Unfortunately, it also makes the model more stupid.
The change is gradual, you may not notice it when going from Q8_0 to Q6_K, but going below Q4 quant can be noticeable.
I strongly recommend experimenting on your own with different models and quantization types, because your experience may be different from mine!</p><blockquote><p>Oh, by the way - remember that right now our <code>llama.cpp</code> build will use CPU for calculations, so the model will reside in RAM.
Make sure you have at least 3GB of free RAM before trying to use the model, if you don’t - quantize it with smaller quant, or get a smaller version.</p></blockquote><p>Anyway, we got our quantized model now, we can <strong>finally</strong> use it!</p><h2 id="running-llamacpp-server">running llama.cpp server<a href="#running-llamacpp-server" arialabel="Anchor">#</a></h2><p>If going through the first part of this post felt like pain and suffering, don’t worry - i felt the same writing it.
That’s why it took a month to write.
But, at long last we can do something fun.</p><p>Let’s start, as usual, with printing the help to make sure our binary is working fine:</p><p>You should see a lot of options.
Some of them will be explained here in a bit, some of them you’ll have to research yourself.
For now, the only options that are interesting to us are:</p><div><pre tabindex="0"><code data-lang="text"><span><span>-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`
</span></span><span><span>                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)
</span></span><span><span>                                        (env: LLAMA_ARG_MODEL)
</span></span><span><span>--host HOST                             ip address to listen (default: 127.0.0.1)
</span></span><span><span>                                        (env: LLAMA_ARG_HOST)
</span></span><span><span>--port PORT                             port to listen (default: 8080)
</span></span><span><span>                                        (env: LLAMA_ARG_PORT)
</span></span></code></pre></div><p>Run <code>llama-server</code> with model’s path set to quantized SmolLM2 GGUF file.
If you don’t have anything running on <code>127.0.0.1:8080</code>, you can leave the host and port on defaults.
Notice that you can also use environmental variables instead of arguments, so you can setup your env and just call <code>llama-server</code>.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>llama-server -m SmolLM2.q8.gguf
</span></span></code></pre></div><p>You should see similar output after running this command:</p><div><pre tabindex="0"><code data-lang="text"><span><span>build: 4182 (ab96610b) with cc (GCC) 14.2.1 20240910 for x86_64-pc-linux-gnu
</span></span><span><span>system info: n_threads = 12, n_threads_batch = 12, total_threads = 24
</span></span><span><span>
</span></span><span><span>system_info: n_threads = 12 (n_threads_batch = 12) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 |
</span></span><span><span>
</span></span><span><span>main: HTTP server is listening, hostname: 127.0.0.1, port: 8080, http threads: 23
</span></span><span><span>main: loading model
</span></span><span><span>srv    load_model: loading model &#39;SmolLM2.q8.gguf&#39;
</span></span><span><span>llama_model_loader: loaded meta data with 37 key-value pairs and 218 tensors from SmolLM2.q8.gguf (version GGUF V3 (latest))
</span></span><span><span>llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
</span></span><span><span>llama_model_loader: - kv   0:                       general.architecture str              = llama
</span></span><span><span>llama_model_loader: - kv   1:                               general.type str              = model
</span></span><span><span>llama_model_loader: - kv   2:                               general.name str              = SmolLM2 1.7B Instruct
</span></span><span><span>llama_model_loader: - kv   3:                           general.finetune str              = Instruct
</span></span><span><span>llama_model_loader: - kv   4:                           general.basename str              = SmolLM2
</span></span><span><span>llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
</span></span><span><span>llama_model_loader: - kv   6:                            general.license str              = apache-2.0
</span></span><span><span>llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
</span></span><span><span>llama_model_loader: - kv   8:                  general.base_model.0.name str              = SmolLM2 1.7B
</span></span><span><span>llama_model_loader: - kv   9:          general.base_model.0.organization str              = HuggingFaceTB
</span></span><span><span>llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/HuggingFaceTB/...
</span></span><span><span>llama_model_loader: - kv  11:                               general.tags arr[str,4]       = [&#34;safetensors&#34;, &#34;onnx&#34;, &#34;transformers...
</span></span><span><span>llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [&#34;en&#34;]
</span></span><span><span>llama_model_loader: - kv  13:                          llama.block_count u32              = 24
</span></span><span><span>llama_model_loader: - kv  14:                       llama.context_length u32              = 8192
</span></span><span><span>llama_model_loader: - kv  15:                     llama.embedding_length u32              = 2048
</span></span><span><span>llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 8192
</span></span><span><span>llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32
</span></span><span><span>llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 32
</span></span><span><span>llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 130000.000000
</span></span><span><span>llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
</span></span><span><span>llama_model_loader: - kv  21:                          general.file_type u32              = 7
</span></span><span><span>llama_model_loader: - kv  22:                           llama.vocab_size u32              = 49152
</span></span><span><span>llama_model_loader: - kv  23:                 llama.rope.dimension_count u32              = 64
</span></span><span><span>llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2
</span></span><span><span>llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = smollm
</span></span><span><span>llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49152]   = [&#34;&lt;|endoftext|&gt;&#34;, &#34;&lt;|im_start|&gt;&#34;, &#34;&lt;|...
</span></span><span><span>llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
</span></span><span><span>llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48900]   = [&#34;Ġ t&#34;, &#34;Ġ a&#34;, &#34;i n&#34;, &#34;h e&#34;, &#34;Ġ Ġ...
</span></span><span><span>llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 1
</span></span><span><span>llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 2
</span></span><span><span>llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0
</span></span><span><span>llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 2
</span></span><span><span>llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
</span></span><span><span>llama_model_loader: - kv  34:            tokenizer.ggml.add_space_prefix bool             = false
</span></span><span><span>llama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = false
</span></span><span><span>llama_model_loader: - kv  36:               general.quantization_version u32              = 2
</span></span><span><span>llama_model_loader: - type  f32:   49 tensors
</span></span><span><span>llama_model_loader: - type q8_0:  169 tensors
</span></span><span><span>llm_load_vocab: special tokens cache size = 17
</span></span><span><span>llm_load_vocab: token to piece cache size = 0.3170 MB
</span></span><span><span>llm_load_print_meta: format           = GGUF V3 (latest)
</span></span><span><span>llm_load_print_meta: arch             = llama
</span></span><span><span>llm_load_print_meta: vocab type       = BPE
</span></span><span><span>llm_load_print_meta: n_vocab          = 49152
</span></span><span><span>llm_load_print_meta: n_merges         = 48900
</span></span><span><span>llm_load_print_meta: vocab_only       = 0
</span></span><span><span>llm_load_print_meta: n_ctx_train      = 8192
</span></span><span><span>llm_load_print_meta: n_embd           = 2048
</span></span><span><span>llm_load_print_meta: n_layer          = 24
</span></span><span><span>llm_load_print_meta: n_head           = 32
</span></span><span><span>llm_load_print_meta: n_head_kv        = 32
</span></span><span><span>llm_load_print_meta: n_rot            = 64
</span></span><span><span>llm_load_print_meta: n_swa            = 0
</span></span><span><span>llm_load_print_meta: n_embd_head_k    = 64
</span></span><span><span>llm_load_print_meta: n_embd_head_v    = 64
</span></span><span><span>llm_load_print_meta: n_gqa            = 1
</span></span><span><span>llm_load_print_meta: n_embd_k_gqa     = 2048
</span></span><span><span>llm_load_print_meta: n_embd_v_gqa     = 2048
</span></span><span><span>llm_load_print_meta: f_norm_eps       = 0.0e+00
</span></span><span><span>llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
</span></span><span><span>llm_load_print_meta: f_clamp_kqv      = 0.0e+00
</span></span><span><span>llm_load_print_meta: f_max_alibi_bias = 0.0e+00
</span></span><span><span>llm_load_print_meta: f_logit_scale    = 0.0e+00
</span></span><span><span>llm_load_print_meta: n_ff             = 8192
</span></span><span><span>llm_load_print_meta: n_expert         = 0
</span></span><span><span>llm_load_print_meta: n_expert_used    = 0
</span></span><span><span>llm_load_print_meta: causal attn      = 1
</span></span><span><span>llm_load_print_meta: pooling type     = 0
</span></span><span><span>llm_load_print_meta: rope type        = 0
</span></span><span><span>llm_load_print_meta: rope scaling     = linear
</span></span><span><span>llm_load_print_meta: freq_base_train  = 130000.0
</span></span><span><span>llm_load_print_meta: freq_scale_train = 1
</span></span><span><span>llm_load_print_meta: n_ctx_orig_yarn  = 8192
</span></span><span><span>llm_load_print_meta: rope_finetuned   = unknown
</span></span><span><span>llm_load_print_meta: ssm_d_conv       = 0
</span></span><span><span>llm_load_print_meta: ssm_d_inner      = 0
</span></span><span><span>llm_load_print_meta: ssm_d_state      = 0
</span></span><span><span>llm_load_print_meta: ssm_dt_rank      = 0
</span></span><span><span>llm_load_print_meta: ssm_dt_b_c_rms   = 0
</span></span><span><span>llm_load_print_meta: model type       = ?B
</span></span><span><span>llm_load_print_meta: model ftype      = Q8_0
</span></span><span><span>llm_load_print_meta: model params     = 1.71 B
</span></span><span><span>llm_load_print_meta: model size       = 1.69 GiB (8.50 BPW)
</span></span><span><span>llm_load_print_meta: general.name     = SmolLM2 1.7B Instruct
</span></span><span><span>llm_load_print_meta: BOS token        = 1 &#39;&lt;|im_start|&gt;&#39;
</span></span><span><span>llm_load_print_meta: EOS token        = 2 &#39;&lt;|im_end|&gt;&#39;
</span></span><span><span>llm_load_print_meta: EOT token        = 0 &#39;&lt;|endoftext|&gt;&#39;
</span></span><span><span>llm_load_print_meta: UNK token        = 0 &#39;&lt;|endoftext|&gt;&#39;
</span></span><span><span>llm_load_print_meta: PAD token        = 2 &#39;&lt;|im_end|&gt;&#39;
</span></span><span><span>llm_load_print_meta: LF token         = 143 &#39;Ä&#39;
</span></span><span><span>llm_load_print_meta: EOG token        = 0 &#39;&lt;|endoftext|&gt;&#39;
</span></span><span><span>llm_load_print_meta: EOG token        = 2 &#39;&lt;|im_end|&gt;&#39;
</span></span><span><span>llm_load_print_meta: max token length = 162
</span></span><span><span>llm_load_tensors:   CPU_Mapped model buffer size =  1734.38 MiB
</span></span><span><span>................................................................................................
</span></span><span><span>llama_new_context_with_model: n_seq_max     = 1
</span></span><span><span>llama_new_context_with_model: n_ctx         = 4096
</span></span><span><span>llama_new_context_with_model: n_ctx_per_seq = 4096
</span></span><span><span>llama_new_context_with_model: n_batch       = 2048
</span></span><span><span>llama_new_context_with_model: n_ubatch      = 512
</span></span><span><span>llama_new_context_with_model: flash_attn    = 0
</span></span><span><span>llama_new_context_with_model: freq_base     = 130000.0
</span></span><span><span>llama_new_context_with_model: freq_scale    = 1
</span></span><span><span>llama_new_context_with_model: n_ctx_per_seq (4096) &lt; n_ctx_train (8192) -- the full capacity of the model will not be utilized
</span></span><span><span>llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB
</span></span><span><span>llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB
</span></span><span><span>llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
</span></span><span><span>llama_new_context_with_model:        CPU compute buffer size =   280.01 MiB
</span></span><span><span>llama_new_context_with_model: graph nodes  = 774
</span></span><span><span>llama_new_context_with_model: graph splits = 1
</span></span><span><span>common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
</span></span><span><span>srv          init: initializing slots, n_slots = 1
</span></span><span><span>slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096
</span></span><span><span>main: model loaded
</span></span><span><span>main: chat template, built_in: 1, chat_example: &#39;&lt;|im_start|&gt;system
</span></span><span><span>You are a helpful assistant&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;user
</span></span><span><span>Hello&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;assistant
</span></span><span><span>Hi there&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;user
</span></span><span><span>How are you?&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;assistant
</span></span><span><span>&#39;
</span></span><span><span>main: server is listening on http://127.0.0.1:8080 - starting the main loop
</span></span><span><span>srv  update_slots: all slots are idle
</span></span></code></pre></div><p>And now we can access the web UI on <code>http://127.0.0.1:8080</code> or whatever host/port combo you’ve set.</p><p><img alt="llama.cpp webui" src="https://steelph0enix.github.io/img/llama-cpp/llama-cpp-webui.png"/></p><p>From this point, we can freely chat with the LLM using the web UI, or you can use the OpenAI-compatible API that <code>llama-server</code> provides.
I won’t dig into the API itself here, i’ve written <a href="https://github.com/SteelPh0enix/unreasonable-llama">a Python library</a> for it if you’re interested in using it (i’m trying to keep it up-to-date with <code>llama.cpp</code> master, but it might not be all the time).
I recommend looking into the <a href="https://github.com/ggerganov/llama.cpp/tree/master/examples/server"><code>llama-server</code> source code and README</a> for more details about endpoints.</p><p>Let’s see what we can do with web UI.
On the left, we have list of conversations.
Those are stored in browser’s localStorage (as the disclaimer on the bottom-left graciously explains), which means they are persistent even if you restart the browser.
Keep in mind that changing the host/port of the server will “clear” those.
Current conversation is passed to the LLM as context, and the context size is limited by server settings (we will learn how to tweak it in a second).
I recommend making new conversations often, and keeping their context focused on the subject for optimal performance.</p><p>On the top-right, we have (from left to right) “remove conversation”, “download conversation” (in JSON format), “configuration” and “Theme” buttons.
In the configuration window, we can tweak generation settings for our LLM.
<strong>Those are currently global, not per-conversation.</strong>
All of those settings are briefly described <a href="https://steelph0enix.github.io/posts/llama-cpp-guide/#llm-configuration-options-explained">below</a>.</p><p><img alt="llama.cpp webui config" src="https://steelph0enix.github.io/img/llama-cpp/llama-cpp-webui-config.png"/></p><h3 id="llamacpp-server-settings">llama.cpp server settings<a href="#llamacpp-server-settings" arialabel="Anchor">#</a></h3><p>Web UI provides only a small subset of configuration options we have available, and only those related to LLM samplers.
For the full set, we need to call <code>llama-server --help</code>.
With those options we can drastically improve (or worsen) the behavior of our model and performance of text generation, so it’s worth knowing them.
I won’t explain <em>all</em> of the options listed there, because it would be mostly redundant, but i’ll probably explain <em>most</em> of them in one way of another here.
I’ll try to explain all <em>interesting</em> options though.</p><p>One thing that’s also worth mentioning is that most of those parameters are read from the environment.
This is also the case for most other <code>llama.cpp</code> executables, and the parameter names (and environment variables) are the same for them.
Names of those variables are provided in <code>llama-server --help</code> output, i’ll add them to each described option here.</p><p>Let’s start with <code>common params</code> section:</p><ul><li><code>--threads</code>/<code>--threads-batch</code> (<code>LLAMA_ARG_THREADS</code>) - amount of CPU threads used by LLM.
Default value is -1, which tells <code>llama.cpp</code> to detect the amount of cores in the system.
This behavior is probably good enough for most of people, so unless you have <em>exotic</em> hardware setup and you know what you’re doing - leave it on default.
If you <em>do</em> have an exotic setup, you may also want to look at other NUMA and offloading-related flags.</li><li><code>--ctx-size</code> (<code>LLAMA_ARG_CTX_SIZE</code>) - size of the prompt context.
In other words, the amount of tokens that the LLM can remember at once.
Increasing the context size also increases the memory requirements for the LLM.
Every model has a context size limit, when this argument is set to <code>0</code>, <code>llama.cpp</code> tries to use it.</li><li><code>--predict</code> (<code>LLAMA_ARG_N_PREDICT</code>) - number of tokens to predict.
When LLM generates text, it stops either after generating end-of-message token (when it decides that the generated sentence is over), or after hitting this limit.
Default is <code>-1</code>, which makes the LLM generate text ad infinitum.
If we want to limit it to context size, we can set it to <code>-2</code>.</li><li><code>--batch-size</code>/<code>--ubatch-size</code> (<code>LLAMA_ARG_BATCH</code>/<code>LLAMA_ARG_UBATCH</code>) - amount of tokens fed to the LLM in single processing step.
Optimal value of those arguments depends on your hardware, model, and context size - i encourage experimentation, but defaults are probably good enough for start.</li><li><code>--flash-attn</code> (<code>LLAMA_ARG_FLASH_ATTN</code>) - <a href="https://www.hopsworks.ai/dictionary/flash-attention">Flash attention</a> is an optimization that’s supported by most recent models.
Read the linked article for details, in short - enabling it should improve the generation performance for some models.
<code>llama.cpp</code> will simply throw a warning when a model that doesn’t support flash attention is loaded, so i keep it on at all times without any issues.</li><li><code>--mlock</code> (<code>LLAMA_ARG_MLOCK</code>) - this option is called exactly like <a href="https://man7.org/linux/man-pages/man2/mlock.2.html">Linux function</a> that it uses underneath.
On Windows, it uses <a href="https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-virtuallock">VirtualLock</a>.
If you have enough virtual memory (RAM or VRAM) to load the whole model into, you can use this parameter to prevent OS from swapping it to the hard drive.
Enabling it can increase the performance of text generation, but may slow everything else down in return if you hit the virtual memory limit of your machine.</li><li><code>--no-mmap</code> (<code>LLAMA_ARG_NO_MMAP</code>) - by default, <code>llama.cpp</code> will map the model to memory (using <a href="https://man7.org/linux/man-pages/man2/mmap.2.html"><code>mmap</code></a> on Linux and <a href="https://learn.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-createfilemappinga"><code>CreateFileMappingA</code></a> on Windows).
Using this switch will disable this behavior.</li><li><code>--gpu-layers</code> (<code>LLAMA_ARG_N_GPU_LAYERS</code>) - if GPU offloading is available, this parameter will set the maximum amount of LLM layers to offload to GPU.
Number and size of layers is dependent on the used model.
Usually, if we want to load the whole model to GPU, we can set this parameter to some unreasonably large number like 999.
For partial offloading, you must experiment yourself.
<code>llama.cpp</code> must be built with GPU support, otherwise this option will have no effect.
If you have multiple GPUs, you may also want to look at <code>--split-mode</code> and <code>--main-gpu</code> arguments.</li><li><code>--model</code> (<code>LLAMA_ARG_MODEL</code>) - path to the GGUF model file.</li></ul><p>Most of the options from <code>sampling params</code> section are described in detail <a href="https://steelph0enix.github.io/posts/llama-cpp-guide/#list-of-llm-configuration-options-and-samplers-available-in-llamacpp">below</a>.
Server-specific arguments are:</p><ul><li><code>--no-context-shift</code> (<code>LLAMA_ARG_NO_CONTEXT_SHIFT</code>) - by default, when context is filled up, it will be shifted (“oldest” tokens are discarded in favour of freshly generated ones).
This parameter disables that behavior, and it will make the generation stop instead.</li><li><code>--cont-batching</code> (<code>LLAMA_ARG_CONT_BATCHING</code>) - continuous batching allows processing prompts in parallel with text generation.
This usually improves performance and is enabled by default.
You can disable it with <code>--no-cont-batching</code> (<code>LLAMA_ARG_NO_CONT_BATCHING</code>) parameter.</li><li><code>--alias</code> (<code>LLAMA_ARG_ALIAS</code>) - Alias for the model name, used by the REST API.
Set to model name by default.</li><li><code>--host</code> (<code>LLAMA_ARG_HOST</code>) and <code>--port</code> (<code>LLAMA_ARG_PORT</code>) - host and port for <code>llama.cpp</code> server.</li><li><code>--slots</code> (<code>LLAMA_ARG_ENDPOINT_SLOTS</code>) - enables <code>/slots</code> endpoint of <code>llama.cpp</code> server.</li><li><code>--props</code> (<code>LLAMA_ARG_ENDPOINT_PROPS</code>) - enables <code>/props</code> endpoint of <code>llama.cpp</code> server.</li></ul><p>Webserver is not the only thing <code>llama.cpp</code> provides.
There’s few other useful tools hidden in built binaries.</p><h3 id="llama-bench"><code>llama-bench</code><a href="#llama-bench" arialabel="Anchor">#</a></h3><p><code>llama-bench</code> allows us to benchmark the prompt processing and text generation speed of our <code>llama.cpp</code> build for a selected model.
To run an example benchmark, we can simply run the executable with path to selected model.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>llama-bench --model selected_model.gguf
</span></span></code></pre></div><p><code>llama-bench</code> will try to use optimal <code>llama.cpp</code> configuration for your hardware.
Default settings will try to use full GPU offloading (99 layers) and mmap.
I recommend enabling flash attention manually (with <code>--flash-attn</code> flag, unfortunately <code>llama-bench</code> does not read the environmental variables)
Tweaking prompt length (<code>--n-prompt</code>) and batch sizes (<code>--batch-size</code>/<code>--ubatch-size</code>) may affect the result of prompt processing benchmark.
Tweaking number of tokens to generate (<code>--n-gen</code>) may affect the result of text generation benchmark.
You can also set the number of repetitions with <code>--repetitions</code> argument.</p><p>Results for SmolLM2 1.7B Instruct quantized to Q8 w/ flash attention on my setup (CPU only, Ryzen 5900X, DDR4 RAM @3200MHz):</p><div><pre tabindex="0"><code data-lang="text"><span><span>&gt; llama-bench --flash-attn 1 --model ./SmolLM2.q8.gguf
</span></span><span><span>
</span></span><span><span>| model                          |       size |     params | backend    | threads | fa |          test |                  t/s |
</span></span><span><span>| ------------------------------ | ---------: | ---------: | ---------- | ------: | -: | ------------: | -------------------: |
</span></span><span><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |         pp512 |        162.54 ± 1.70 |
</span></span><span><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |         tg128 |         22.50 ± 0.05 |
</span></span><span><span>
</span></span><span><span>build: dc223440 (4215)
</span></span></code></pre></div><p>The table is mostly self-explanatory, except two last columns.
<code>test</code> contains the benchmark identifier, made from two parts.
First two letters define the bench type (<code>pp</code> for prompt processing, <code>tg</code> for text generation).
The number defines the prompt size (for prompt processing benchmark) or amount of generated tokens (for text generation benchmark).
<code>t/s</code> column is the result in tokens processed/generated per second.</p><p>There’s also a <em>mystery</em> <code>-pg</code> argument, which can be used to perform mixed prompt processing+text generation test.
For example:</p><div><pre tabindex="0"><code data-lang="text"><span><span>&gt; llama-bench --flash-attn 1 --model ./SmolLM2.q8.gguf -pg 1024,256
</span></span><span><span>
</span></span><span><span>| model                          |       size |     params | backend    | threads | fa |          test |                  t/s |
</span></span><span><span>| ------------------------------ | ---------: | ---------: | ---------- | ------: | -: | ------------: | -------------------: |
</span></span><span><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |         pp512 |        165.50 ± 1.95 |
</span></span><span><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |         tg128 |         22.44 ± 0.01 |
</span></span><span><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | CPU        |      12 |  1 |  pp1024+tg256 |         63.51 ± 4.24 |
</span></span><span><span>
</span></span><span><span>build: dc223440 (4215)
</span></span></code></pre></div><p>This is probably the most realistic benchmark, because as long as you have continuous batching enabled you’ll use the model like that.</p><h3 id="llama-cli"><code>llama-cli</code><a href="#llama-cli" arialabel="Anchor">#</a></h3><p>This is a simple CLI interface for the LLM.
It allows you to generate a completion for specified prompt, or chat with the LLM.</p><p>It shares most arguments with <code>llama-server</code>, except some specific ones:</p><ul><li><code>--prompt</code> - can also be used with <code>llama-server</code>, but here it’s bit more useful.
Sets the starting/system prompt for the LLM.
Prompt can also be loaded from file by specifying it’s path using <code>--file</code> or <code>--binary-file</code> argument.</li><li><code>--color</code> - enables colored output, it’s disabled by default.</li><li><code>--no-context-shift</code> (<code>LLAMA_ARG_NO_CONTEXT_SHIFT</code>) - does the same thing as in <code>llama-server</code>.</li><li><code>--reverse-prompt</code> - when LLM generates a reverse prompt, it stops generation and returns the control over conversation to the user, allowing him to respond.
Basically, this is list of stopping words/sentences.</li><li><code>--conversation</code> - enables conversation mode by enabling interactive mode and not printing special tokens (like those appearing in chat template)
This is probably how you want to use this program.</li><li><code>--interactive</code> - enables interactive mode, allowing you to chat with the LLM. In this mode, the generation starts right away and you should set the <code>--prompt</code> to get any reasonable output.
Alternatively, we can use <code>--interactive-first</code> to start chatting with control over chat right away.</li></ul><p>Here are specific usage examples:</p><h4 id="text-completion">text completion<a href="#text-completion" arialabel="Anchor">#</a></h4><div><pre tabindex="0"><code data-lang="text"><span><span>&gt; llama-cli --flash-attn --model ./SmolLM2.q8.gguf --prompt &#34;The highest mountain on earth&#34;
</span></span><span><span>
</span></span><span><span>build: 4215 (dc223440) with cc (GCC) 14.2.1 20240910 for x86_64-pc-linux-gnu
</span></span><span><span>main: llama backend init
</span></span><span><span>main: load the model and apply lora adapter, if any
</span></span><span><span>llama_model_loader: loaded meta data with 37 key-value pairs and 218 tensors from ./SmolLM2.q8.gguf (version GGUF V3 (latest))
</span></span><span><span>...
</span></span><span><span>llm_load_tensors:   CPU_Mapped model buffer size =  1734,38 MiB
</span></span><span><span>................................................................................................
</span></span><span><span>llama_new_context_with_model: n_seq_max     = 1
</span></span><span><span>llama_new_context_with_model: n_ctx         = 4096
</span></span><span><span>llama_new_context_with_model: n_ctx_per_seq = 4096
</span></span><span><span>llama_new_context_with_model: n_batch       = 2048
</span></span><span><span>llama_new_context_with_model: n_ubatch      = 512
</span></span><span><span>llama_new_context_with_model: flash_attn    = 1
</span></span><span><span>llama_new_context_with_model: freq_base     = 130000,0
</span></span><span><span>llama_new_context_with_model: freq_scale    = 1
</span></span><span><span>llama_new_context_with_model: n_ctx_per_seq (4096) &lt; n_ctx_train (8192) -- the full capacity of the model will not be utilized
</span></span><span><span>llama_kv_cache_init:        CPU KV buffer size =   768,00 MiB
</span></span><span><span>llama_new_context_with_model: KV self size  =  768,00 MiB, K (f16):  384,00 MiB, V (f16):  384,00 MiB
</span></span><span><span>llama_new_context_with_model:        CPU  output buffer size =     0,19 MiB
</span></span><span><span>llama_new_context_with_model:        CPU compute buffer size =   104,00 MiB
</span></span><span><span>llama_new_context_with_model: graph nodes  = 679
</span></span><span><span>llama_new_context_with_model: graph splits = 1
</span></span><span><span>common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
</span></span><span><span>main: llama threadpool init, n_threads = 12
</span></span><span><span>
</span></span><span><span>system_info: n_threads = 12 (n_threads_batch = 12) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 |
</span></span><span><span>
</span></span><span><span>sampler seed: 2734556630
</span></span><span><span>sampler params:
</span></span><span><span>        repeat_last_n = 64, repeat_penalty = 1,000, frequency_penalty = 0,000, presence_penalty = 0,000
</span></span><span><span>        dry_multiplier = 0,000, dry_base = 1,750, dry_allowed_length = 2, dry_penalty_last_n = -1
</span></span><span><span>        top_k = 40, top_p = 0,950, min_p = 0,050, xtc_probability = 0,000, xtc_threshold = 0,100, typical_p = 1,000, temp = 0,800
</span></span><span><span>        mirostat = 0, mirostat_lr = 0,100, mirostat_ent = 5,000
</span></span><span><span>sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist
</span></span><span><span>generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0
</span></span><span><span>
</span></span><span><span>The highest mountain on earth is Mount Everest, which stands at an astonishing 8,848.86 meters (29,031.7 feet) above sea level. Located in the Mahalangur Sharhungtrigangla Range in the Himalayas, it&#39;s a marvel of nature that draws adventurers and thrill-seekers from around the globe.
</span></span><span><span>
</span></span><span><span>Standing at the base camp, the mountain appears as a majestic giant, its rugged slopes and snow-capped peaks a testament to its formidable presence. The climb to the summit is a grueling challenge that requires immense physical and mental fortitude, as climbers must navigate steep inclines, unpredictable weather, and crevasses.
</span></span><span><span>
</span></span><span><span>The ascent begins at Base Camp, a bustling hub of activity, where climbers gather to share stories, exchange tips, and prepare for the climb ahead. From Base Camp, climbers make their way to the South Col, a precarious route that offers breathtaking views of the surrounding landscape. The final push to the summit involves a grueling ascent up the steep and treacherous Lhotse Face, followed by a scramble up the near-vertical wall of the Western Cwm.
</span></span><span><span>
</span></span><span><span>Upon reaching the summit, climbers are rewarded with an unforgettable sight: the majestic Himalayan range unfolding before them, with the sun casting a golden glow on the snow. The sense of accomplishment and awe is indescribable, and the experience is etched in the memories of those who have conquered this mighty mountain.
</span></span><span><span>
</span></span><span><span>The climb to Everest is not just about reaching the summit; it&#39;s an adventure that requires patience, perseverance, and a deep respect for the mountain. Climbers must be prepared to face extreme weather conditions, altitude sickness, and the ever-present risk of accidents or crevasses. Despite these challenges, the allure of Everest remains a powerful draw, inspiring countless individuals to push their limits and push beyond them. [end of text]
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>llama_perf_sampler_print:    sampling time =      12,58 ms /   385 runs   (    0,03 ms per token, 30604,13 tokens per second)
</span></span><span><span>llama_perf_context_print:        load time =     318,81 ms
</span></span><span><span>llama_perf_context_print: prompt eval time =      59,26 ms /     5 tokens (   11,85 ms per token,    84,38 tokens per second)
</span></span><span><span>llama_perf_context_print:        eval time =   17797,98 ms /   379 runs   (   46,96 ms per token,    21,29 tokens per second)
</span></span><span><span>llama_perf_context_print:       total time =   17891,23 ms /   384 tokens
</span></span></code></pre></div><h4 id="chat-mode">chat mode<a href="#chat-mode" arialabel="Anchor">#</a></h4><div><pre tabindex="0"><code data-lang="text"><span><span>&gt; llama-cli --flash-attn --model ./SmolLM2.q8.gguf --prompt &#34;You are a helpful assistant&#34; --conversation
</span></span><span><span>
</span></span><span><span>build: 4215 (dc223440) with cc (GCC) 14.2.1 20240910 for x86_64-pc-linux-gnu
</span></span><span><span>main: llama backend init
</span></span><span><span>main: load the model and apply lora adapter, if any
</span></span><span><span>llama_model_loader: loaded meta data with 37 key-value pairs and 218 tensors from ./SmolLM2.q8.gguf (version GGUF V3 (latest))
</span></span><span><span>...
</span></span><span><span>llm_load_tensors:   CPU_Mapped model buffer size =  1734,38 MiB
</span></span><span><span>................................................................................................
</span></span><span><span>llama_new_context_with_model: n_seq_max     = 1
</span></span><span><span>llama_new_context_with_model: n_ctx         = 4096
</span></span><span><span>llama_new_context_with_model: n_ctx_per_seq = 4096
</span></span><span><span>llama_new_context_with_model: n_batch       = 2048
</span></span><span><span>llama_new_context_with_model: n_ubatch      = 512
</span></span><span><span>llama_new_context_with_model: flash_attn    = 1
</span></span><span><span>llama_new_context_with_model: freq_base     = 130000,0
</span></span><span><span>llama_new_context_with_model: freq_scale    = 1
</span></span><span><span>llama_new_context_with_model: n_ctx_per_seq (4096) &lt; n_ctx_train (8192) -- the full capacity of the model will not be utilized
</span></span><span><span>llama_kv_cache_init:        CPU KV buffer size =   768,00 MiB
</span></span><span><span>llama_new_context_with_model: KV self size  =  768,00 MiB, K (f16):  384,00 MiB, V (f16):  384,00 MiB
</span></span><span><span>llama_new_context_with_model:        CPU  output buffer size =     0,19 MiB
</span></span><span><span>llama_new_context_with_model:        CPU compute buffer size =   104,00 MiB
</span></span><span><span>llama_new_context_with_model: graph nodes  = 679
</span></span><span><span>llama_new_context_with_model: graph splits = 1
</span></span><span><span>common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
</span></span><span><span>main: llama threadpool init, n_threads = 12
</span></span><span><span>main: chat template example:
</span></span><span><span>&lt;|im_start|&gt;system
</span></span><span><span>You are a helpful assistant&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;user
</span></span><span><span>Hello&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;assistant
</span></span><span><span>Hi there&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;user
</span></span><span><span>How are you?&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;assistant
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>system_info: n_threads = 12 (n_threads_batch = 12) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 |
</span></span><span><span>
</span></span><span><span>main: interactive mode on.
</span></span><span><span>sampler seed: 968968654
</span></span><span><span>sampler params:
</span></span><span><span>        repeat_last_n = 64, repeat_penalty = 1,000, frequency_penalty = 0,000, presence_penalty = 0,000
</span></span><span><span>        dry_multiplier = 0,000, dry_base = 1,750, dry_allowed_length = 2, dry_penalty_last_n = -1
</span></span><span><span>        top_k = 40, top_p = 0,950, min_p = 0,050, xtc_probability = 0,000, xtc_threshold = 0,100, typical_p = 1,000, temp = 0,800
</span></span><span><span>        mirostat = 0, mirostat_lr = 0,100, mirostat_ent = 5,000
</span></span><span><span>sampler chain: logits -&gt; logit-bias -&gt; penalties -&gt; dry -&gt; top-k -&gt; typical -&gt; top-p -&gt; min-p -&gt; xtc -&gt; temp-ext -&gt; dist
</span></span><span><span>generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0
</span></span><span><span>
</span></span><span><span>== Running in interactive mode. ==
</span></span><span><span> - Press Ctrl+C to interject at any time.
</span></span><span><span> - Press Return to return control to the AI.
</span></span><span><span> - To return control without starting a new line, end your input with &#39;/&#39;.
</span></span><span><span> - If you want to submit another line, end your input with &#39;\&#39;.
</span></span><span><span>
</span></span><span><span>system
</span></span><span><span>You are a helpful assistant
</span></span><span><span>
</span></span><span><span>&gt; hi
</span></span><span><span>Hello! How can I help you today?
</span></span><span><span>
</span></span><span><span>&gt;
</span></span><span><span>llama_perf_sampler_print:    sampling time =       0,27 ms /    22 runs   (    0,01 ms per token, 80291,97 tokens per second)
</span></span><span><span>llama_perf_context_print:        load time =     317,46 ms
</span></span><span><span>llama_perf_context_print: prompt eval time =    2043,02 ms /    22 tokens (   92,86 ms per token,    10,77 tokens per second)
</span></span><span><span>llama_perf_context_print:        eval time =     407,66 ms /     9 runs   (   45,30 ms per token,    22,08 tokens per second)
</span></span><span><span>llama_perf_context_print:       total time =    5302,60 ms /    31 tokens
</span></span><span><span>Interrupted by user
</span></span></code></pre></div><h2 id="building-the-llama-but-better">building the llama, but better<a href="#building-the-llama-but-better" arialabel="Anchor">#</a></h2><p>All right, now that we know how to use <code>llama.cpp</code> and tweak runtime parameters, let’s learn how to tweak build configuration.
We already set some generic settings in <a href="https://steelph0enix.github.io/posts/llama-cpp-guide/#building-the-llama">chapter about building the <code>llama.cpp</code></a> but we haven’t touched any backend-related ones yet.</p><p>Let’s start with clearing up the <code>llama.cpp</code> repository (and, optionally, making sure that we have the latest commit):</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cd llama.cpp
</span></span><span><span>git clean -xdf
</span></span><span><span>git pull
</span></span><span><span>git submodule update --recursive
</span></span></code></pre></div><p>Now, we need to generate the build files for a custom backend.
As of writing this, the list of backends supported by <code>llama.cpp</code> is following:</p><ul><li><code>Metal</code> - acceleration for Apple Silicon</li><li><code>Accelerate</code> - BLAS (Basic Linear Algebra Subprograms) acceleration for Mac PCs, enabled by default.</li><li><code>OpenBLAS</code> - BLAS acceleration for CPUs</li><li><code>BLIS</code> - relatively recently released high-performance BLAS framework</li><li><code>SYCL</code> - acceleration for Intel GPUs (Data Center Max series, Flex series, Arc series, Built-in GPUs and iGPUs)</li><li><code>Intel oneMKL</code> - acceleration for Intel CPUs</li><li><code>CUDA</code> - acceleration for Nvidia GPUs</li><li><code>MUSA</code> - acceleration for Moore Threads GPUs</li><li><code>hipBLAS</code> - BLAS acceleration for AMD GPUs</li><li><code>Vulkan</code> - generic acceleration for GPUs</li><li><code>CANN</code> - acceleration for Ascend NPU</li><li><code>Android</code> - yes, there’s also Android support.</li></ul><p>As we can see, there’s something for everyone.
My backend selection recommendation is following:</p><ul><li>Users without GPUs should try <code>Intel oneMKL</code> in case of Intel CPUs, or <code>BLIS</code>/<code>OpenBLAS</code>.</li><li>Users with Nvidia GPUs should use <code>CUDA</code> or <code>Vulkan</code></li><li>Users with AMD GPUs should use <code>Vulkan</code> or <code>ROCm</code> (order important here, ROCm was bugged last time i’ve used it)</li><li>Users with Intel GPUs should use <code>SYCL</code> or <code>Vulkan</code></li></ul><p>As we can see, Vulkan is the most generic option for GPU acceleration and i believe it’s the simplest to build for, so i’ll explain in detail how to do that.
The build process for every backend is very similar - install the necessary dependencies, generate the <code>llama.cpp</code> build files with proper flag to enable the specific backend, and build it.</p><p>Oh, and don’t worry about Python and it’s dependencies.
The performance of model conversion scripts is not limited by <code>pytorch</code>, so there’s no point in installing CUDA/ROCm versions.</p><p>Before generating the build file, we need to install <a href="https://www.lunarg.com/vulkan-sdk/">Vulkan SDK</a>.</p><p>On Windows, it’s easiest to do via MSYS.
That’s why i’ve recommended using it at the beginning.
I have tried installing it directly on Windows, but encountered issues that i haven’t seen when using MSYS - so, obviously, MSYS is a better option.
Run this command in MSYS (make sure to use UCRT runtime) to install the required dependencies:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>pacman -S git <span>\
</span></span></span><span><span><span></span>    mingw-w64-ucrt-x86_64-gcc <span>\
</span></span></span><span><span><span></span>    mingw-w64-ucrt-x86_64-cmake <span>\
</span></span></span><span><span><span></span>    mingw-w64-ucrt-x86_64-vulkan-devel <span>\
</span></span></span><span><span><span></span>    mingw-w64-ucrt-x86_64-shaderc
</span></span></code></pre></div><p>If you <em>really</em> don’t want to use MSYS, i recommend <a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#vulkan">following the docs</a></p><p>On Linux, i recommend installing Vulkan SDK using the package manager.
If it’s not in package manager of your distro, i assume you know what you’re doing and how to install it manually.</p><p>Afterwards, we can generate the build files (replace <code>/your/install/dir</code> with custom installation directory, if you want):</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cmake -S . -B build -G Ninja -DGGML_VULKAN<span>=</span>ON -DCMAKE_BUILD_TYPE<span>=</span>Release -DCMAKE_INSTALL_PREFIX<span>=</span>/your/install/dir -DLLAMA_BUILD_TESTS<span>=</span>OFF -DLLAMA_BUILD_EXAMPLES<span>=</span>ON -DLLAMA_BUILD_SERVER<span>=</span>ON
</span></span></code></pre></div><p>and build/install the binaries (replace <code>X</code> with amount of cores in your system):</p><div><pre tabindex="0"><code data-lang="sh"><span><span>cmake --build build --config Release -j X
</span></span><span><span>cmake --install build --config Release
</span></span></code></pre></div><p>Don’t mind the warnings you’ll see, i get them too.
Now our <code>llama.cpp</code> binaries should be able to use our GPU.
We can test it by running <code>llama-server</code> or <code>llama-cli</code> with <code>--list-devices</code> argument:</p><div><pre tabindex="0"><code data-lang="text"><span><span>&gt; llama-cli --list-devices
</span></span><span><span>
</span></span><span><span>ggml_vulkan: Found 1 Vulkan devices:
</span></span><span><span>ggml_vulkan: 0 = AMD Radeon RX 7900 XT (AMD open-source driver) | uma: 0 | fp16: 1 | warp size: 64
</span></span><span><span>Available devices:
</span></span><span><span>  Vulkan0: AMD Radeon RX 7900 XT (20464 MiB, 20464 MiB free)
</span></span></code></pre></div><p>Running that command previously would print an empty list:</p><div><pre tabindex="0"><code data-lang="text"><span><span>&gt; llama-cli --list-devices
</span></span><span><span>
</span></span><span><span>Available devices:
</span></span></code></pre></div><p>Remember the <code>llama-bench</code> results i’ve got previously on CPU build?
This is them now:</p><div><pre tabindex="0"><code data-lang="text"><span><span>&gt; llama-bench --flash-attn 1 --model ./SmolLM2.q8.gguf -pg 1024,256
</span></span><span><span>
</span></span><span><span>ggml_vulkan: Found 1 Vulkan devices:
</span></span><span><span>ggml_vulkan: 0 = AMD Radeon RX 7900 XT (AMD open-source driver) | uma: 0 | fp16: 1 | warp size: 64
</span></span><span><span>| model                          |       size |     params | backend    | ngl | fa |          test |                  t/s |
</span></span><span><span>| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------: | -------------------: |
</span></span><span><span>ggml_vulkan: Compiling shaders..............................Done!
</span></span><span><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | Vulkan     |  99 |  1 |         pp512 |        880.55 ± 5.30 |
</span></span><span><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | Vulkan     |  99 |  1 |         tg128 |         89.78 ± 1.66 |
</span></span><span><span>| llama ?B Q8_0                  |   1.69 GiB |     1.71 B | Vulkan     |  99 |  1 |  pp1024+tg256 |        115.25 ± 0.83 |
</span></span></code></pre></div><p>Now, that’s some <em>good shit</em> right here.
Prompt processing speed has increased from ~165 to ~880 tokens per second (5.3x faster).
Text generation - from ~22 to ~90 tokens per second (4x faster).
Mixed text processing went up from ~63 to ~115 tokens per second (1.8x faster).
All of this due to the fact that i’ve switched to <em>correct</em> backend.</p><h2 id="llm-configuration-options-explained">LLM configuration options explained<a href="#llm-configuration-options-explained" arialabel="Anchor">#</a></h2><p>This will be a relatively long and very informational part full of boring explanations.
But - it’s a good knowledge to have when playing with LLMs.</p><h3 id="how-does-llm-generate-text">how does LLM generate text?<a href="#how-does-llm-generate-text" arialabel="Anchor">#</a></h3><ol><li><p>Prompt</p><p>Everything starts with a prompt.
Prompt can be a simple raw string that we want the LLM to complete for us, or it can be an elaborate construction that allows the LLM to chat or use external tools.
Whatever we put in it, it’s usually in human-readable format with special “tags” (usually similar to XML tags) used for separating the parts of the prompt.</p><p>We’ve already seen an example of a prompt used for chat completion, provided by <code>llama-server</code>:</p><div><pre tabindex="0"><code data-lang="text"><span><span>&lt;|im_start|&gt;system
</span></span><span><span>You are a helpful assistant&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;user
</span></span><span><span>Hello&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;assistant
</span></span><span><span>Hi there&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;user
</span></span><span><span>How are you?&lt;|im_end|&gt;
</span></span><span><span>&lt;|im_start|&gt;assistant
</span></span></code></pre></div><p>(if you’re wondering <em>what are those funny &lt;| and |&gt; symbols</em> - those are ligatures from Fira Code font made out of <code>|</code>, <code>&gt;</code> and <code>&lt;</code> characters)</p></li><li><p>Tokenization</p><p>The LLM does not understand the human language like we do.
We use words and punctuation marks to form sentences - LLMs use tokens that can be understood as an equivalent to those.
First step in text generation is breaking the language barrier by performing prompt tokenization.
Tokenization is a process of translating input text (in human-readable format) into an array of tokens that can be processed by an LLM.
Tokens are simple numeric values, and with a vocabulary they can be easily mapped to their string representations (at least in case of BPE models, don’t know about others).
In fact, that vocabulary is available in SmolLM2 repository, in <code>tokenizer.json</code> file!
That file also contains some metadata for <em>special</em> tokens that have <em>special</em> meaning for the LLM.
Some of those tokens represent <em>meta</em> things, like start and end of a message.
Other can allow the LLM to chat with the user by providing tags for separating parts of conversation (system prompt, user messages, LLM responses).
I’ve also seen tool calling capabilities in LLM templates, which in theory should allow the LLM to use external tools, but i haven’t tested them yet (check out Qwen2.5 and CodeQwen2.5 for example models with those functions).</p><p>We can use <code>llama-server</code> API to tokenize some text and see how it looks after being translated. Hope you’ve got <code>curl</code>.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>curl -X POST -H <span>&#34;Content-Type: application/json&#34;</span> -d <span>&#39;{&#34;content&#34;: &#34;hello world! this is an example message!&#34;}&#39;</span> http://127.0.0.1:8080/tokenize
</span></span></code></pre></div><p>For SmolLM2, the response should be following:</p><div><pre tabindex="0"><code data-lang="json"><span><span>{<span>&#34;tokens&#34;</span>:[<span>28120</span>,<span>905</span>,<span>17</span>,<span>451</span>,<span>314</span>,<span>1183</span>,<span>3714</span>,<span>17</span>]}
</span></span></code></pre></div><p>Which we can very roughly translate to:</p><ul><li>28120 - hello</li><li>905 - world</li><li>17 - !</li><li>451 - this</li><li>314 - is</li><li>354 - an</li><li>1183 - example</li><li>3714 - message</li><li>17 - !</li></ul><p>We can pass this JSON back to <code>/detokenize</code> endpoint to get our original text back:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>curl -X POST -H <span>&#34;Content-Type: application/json&#34;</span> -d <span>&#39;{&#34;tokens&#34;: [28120,905,17,451,314,354,1183,3714,17]}&#39;</span> http://127.0.0.1:8080/detokenize
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="json"><span><span>{<span>&#34;content&#34;</span>:<span>&#34;hello world! this is an example message!&#34;</span>}
</span></span></code></pre></div></li><li><p>Dank Magick (feeding the beast)</p><p>I honestly don’t know what exactly happens in this step, but i’ll try my best to explain it in simple and very approximate terms.
The input is the tokenized prompt.
This prompt is fed to the LLM, and the digestion process takes a lot of processing time due to the insane amount of matrix operations that must be performed to satisfy the digital beast.
After the prompt is digested, the LLM starts talking to us.
LLM talks by generating pairs of tokens and probabilities of them appearing next in the completed text.
If we’d just use those as-is, the output would be complete gibberish and would drive people insane, as it’s usually the case with demons - digital or not.
Those tokens must be filtered out in order to form an output understandable to human beings (or whatever other beings you want to talk with), and that’s what token sampling is all about.</p></li><li><p>Token sampling</p><p>This is probably the most interesting step for us, because we can control it’s every single parameter.
As usual, i advise caution when working with raw output from demons - digital or not, it may result in unexpected stuff happening when handled incorrectly.
To generate a token, LLM outputs a batch of token-probability pairs that’s filtered out to a single one by a chain of samplers.
There’s plenty of different sampling algorithms that can be tweaked for different purposes, and list of those available in llama.cpp with their descriptions is presented below.</p></li><li><p>Detokenization</p><p>Generated tokens must be converted back to human-readable form, so a detokenization must take place.
This is the last step.
Hooray, we tamed the digital beast and forced it to talk.
I have previously feared the consequences this could bring upon the humanity, but here we are, 373 1457 260 970 1041 3935.</p></li></ol><h3 id="list-of-llm-configuration-options-and-samplers-available-in-llamacpp">list of LLM configuration options and samplers available in llama.cpp<a href="#list-of-llm-configuration-options-and-samplers-available-in-llamacpp" arialabel="Anchor">#</a></h3><ul><li><strong>System Message</strong> - Usually, conversations with LLMs start with a “system” message that tells the LLM how to behave.
This is probably the easiest-to-use tool that can drastically change the behavior of a model.
My recommendation is to put as much useful informations and precise behavior descriptions for your application as possible, to maximize the quality of LLM output.
You may think that giving the digital demon maximum amount of knowledge may lead to bad things happening, but our reality haven’t collapsed yet so i think we’re good for now.</li><li><strong>Temperature</strong> - per <code>llama.cpp</code> docs “Controls the randomness of the generated text by affecting the probability distribution of the output tokens. Higher = more random, lower = more focused”.
I don’t have anything to add here, it controls the “creativity” of an LLM.
High values result in more random and “creative” output, but overcooking the beast may result in hallucinations, and - in certain scenarios - screams that will destroy your sanity.
Keep it in 0.2-2.0 range for a start, and keep it positive and non-zero.</li><li><a href="https://rentry.org/dynamic_temperature"><strong>Dynamic temperature</strong></a> - Dynamic temperature sampling is an addition to temperature sampler.
The linked article describes it in detail, and it’s pretty short so i strongly recommend reading it - i can’t really do a better job explaining it.
There’s also the <a href="https://www.reddit.com/r/Oobabooga/comments/191klr8/some_information_about_dynamic_temperature_added/">reddit post</a> with more explanations from the algorithm’s author.
However, in case the article goes down - the short explanation of this algorithm is: it tweaks the temperature of generated tokens based on their entropy.
Entropy here can be understood as inverse of LLMs confidence in generated tokens.
Lower entropy means that the LLM is more confident in it’s predictions, and therefore the temperature of tokens with low entropy should also be low.
High entropy works the other way around.
Effectively, this sampling can encourage creativity while preventing hallucinations at higher temperatures.
I strongly recommend testing it out, as it’s usually disabled by default.
It may require some additional tweaks to other samplers when enabled, to produce optimal results.
The parameters of dynamic temperature sampler are:<ul><li><strong>Dynatemp range</strong> - the range of dynamic temperature to be added/subtracted</li><li><strong>Dynatemp exponent</strong> - changing the exponent changes the dynamic temperature in following way (figure shamelessly stolen from <a href="https://rentry.org/dynamic_temperature">previously linked reentry article</a>): <img alt="dynatemp exponent effects" src="https://steelph0enix.github.io/img/llama-cpp/dynatemp-exponent.png"/></li></ul></li><li><strong>Top-K</strong> - Top-K sampling is a fancy name for “keep only <code>K</code> most probable tokens” algorithm.
Higher values can result in more diverse text, because there’s more tokens to choose from when generating responses.</li><li><strong>Top-P</strong> - Top-P sampling, also called <em>nucleus sampling</em>, per <code>llama.cpp</code> docs “Limits the tokens to those that together have a cumulative probability of at least <code>p</code>”.
In human language, it means that the Top-P sampler takes a list of tokens and their probabilities as an input (note that the sum of their cumulative probabilities is by definition equal to 1), and returns tokens with highest probabilities from that list until the sum of their cumulative probabilities is greater or equal to <code>p</code>.
Or, in other words, <code>p</code> value changes the % of tokens returned by the Top-P sampler.
For example, when <code>p</code> is equal to 0.7, the sampler will return 70% of input tokens with highest probabilities.
There’s a <a href="https://rumn.medium.com/setting-top-k-top-p-and-temperature-in-llms-3da3a8f74832">pretty good article</a> about temperature, Top-K and Top-P sampling that i’ve found and can recommend if you wanna know more.</li><li><strong>Min-P</strong> - Min-P sampling, per <code>llama.cpp</code> docs “Limits tokens based on the minimum probability for a token to be considered, relative to the probability of the most likely token”.
There’s a <a href="https://arxiv.org/pdf/2407.01082">paper</a> explaining this algorithm (it contains loads of citations for other LLM-related stuff too, good read).
Figure 1 from this paper nicely shows what each of the sampling algorithms does to probability distribution of tokens:
<img alt="min-p-probabilities" src="https://steelph0enix.github.io/img/llama-cpp/min-p-probs.png"/></li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1ev8n2s/exclude_top_choices_xtc_a_sampler_that_boosts/"><strong>Exclude Top Choices (XTC)</strong></a> - This is a funky one, because it works a bit differently from most other samplers.
Quoting the author, <em>Instead of pruning the least likely tokens, under certain circumstances, it removes the most likely tokens from consideration</em>.
Detailed description can be found in <a href="https://github.com/oobabooga/text-generation-webui/pull/6335">the PR with implementation</a>.
I recommend reading it, because i really can’t come up with anything better in few sentences, it’s a really good explanation.
I can, however, steal this image from the linked PR to show you more-or-less what XTC does: <img alt="xtc" src="https://steelph0enix.github.io/img/llama-cpp/xtc.png"/>
The parameters for XTC sampler are:<ul><li><strong>XTC threshold</strong> - probability cutoff threshold for top tokens, in (0, 1) range.</li><li><strong>XTC probability</strong> - probability of XTC sampling being applied in [0, 1] range, where 0 = XTC disabled, 1 = XTC always enabled.</li></ul></li><li><a href="https://arxiv.org/pdf/2202.00666"><strong>Locally typical sampling (typical-P)</strong></a> - per <code>llama.cpp</code> docs “Sorts and limits tokens based on the difference between log-probability and entropy”.
I… honestly don’t know how exactly it works.
I tried reading the linked paper, but i lack the mental capacity to understand it enough to describe it back.
<a href="https://www.reddit.com/r/LocalLLaMA/comments/153bnly/what_does_typical_p_actually_do/">Some people on Reddit</a> also have the same issue, so i recommend going there and reading the comments.
I haven’t used that sampling much, so i can’t really say anything about it from experience either, so - moving on.</li><li><a href="https://github.com/oobabooga/text-generation-webui/pull/5677"><strong>DRY</strong></a> - This sampler is used to prevent unwanted token repetition.
Simplifying, it tries to detect repeating token sequences in generated text and reduces the probabilities of tokens that will create repetitions.
As usual, i recommend reading the linked PR for detailed explanation, and as usual i’ve stolen a figure from it that shows what DRY does: <img alt="dry" src="https://steelph0enix.github.io/img/llama-cpp/dry.png"/>
I’ll also quote a short explanation of this sampler:
<em>The penalty for a token is calculated as <code>multiplier * base ^ (n - allowed_length)</code>, where <code>n</code> is the length of the sequence before that token that matches the end of the input, and <code>multiplier</code>, <code>base</code>, and <code>allowed_length</code> are configurable parameters.</em>
<em>If the length of the matching sequence is less than <code>allowed_length</code>, no penalty is applied.</em>
The parameters for DRY sampler are:<ul><li><strong>DRY multiplier</strong> - see explanation above</li><li><strong>DRY base</strong> - see explanation above</li><li><strong>DRY allowed length</strong> - see explanation above. Quoting <code>llama.cpp</code> docs: <em>Tokens that extend repetition beyond this receive exponentially increasing penalty</em>.</li><li><strong>DRY penalty last N</strong> - how many tokens should be scanned for repetition. -1 = whole context, 0 = disabled.</li><li><strong>DRY sequence breakers</strong> - characters that are used as separators for parts of sentences considered for DRY. Defaults for <code>llama.cpp</code> are <code>(&#39;\n&#39;, &#39;:&#39;, &#39;&#34;&#39;, &#39;*&#39;)</code>.</li></ul></li><li><a href="https://openreview.net/pdf?id=W1G1JZEIy5_"><strong>Mirostat</strong></a> - is a funky sampling algorithm that <strong>overrides Top-K, Top-P and Typical-P samplers</strong>.
It’s an alternative sampler that produces text with controlled <em>perplexity</em> (entropy), which means that we can control how certain the model should be in it’s predictions.
This comes without side-effects of generating repeated text (as it happens in low perplexity scenarios) or incoherent output (as it happens in high perplexity scenarios).
The configuration parameters for Mirostat are:<ul><li><strong>Mirostat version</strong> - 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0.</li><li><strong>Mirostat learning rate (η, eta)</strong> - specifies how fast the model converges to desired perplexity.</li><li><strong>Mirostat target entropy (τ, tau)</strong> - the desired perplexity.
Depending on the model, it should not be too high, otherwise you may degrade it’s performance.</li></ul></li><li><strong>Max tokens</strong> - i think that’s pretty self-explanatory. -1 makes the LLM generate until it decides it’s end of the sentence (by returning end-of-sentence token), or the context is full.</li><li><strong>Repetition penalty</strong> - Repetition penalty algorithm (not to be mistaken with DRY) simply reduces the chance that tokens that are already in the generated text will be used again.
Usually the repetition penalty algorithm is restricted to <code>N</code> last tokens of the context.
In case of <code>llama.cpp</code> (i’ll simplify a bit), it works like that: first, it creates a frequency map occurrences for last <code>N</code> tokens.
Then, the current logit bias for each token is divided by <code>repeat_penalty</code> value.
By default it’s usually set to 1.0, so to enable repetition penalty it should be set to &gt;1.
Finally, frequency and presence penalties are applied based on the frequency map.
The penalty for each token is equal to <code>(token_count * frequency_penalty) + (presence_penalty if token_count &gt; 0)</code>.
The penalty is represented as logit bias, which can be in [-100, 100] range.
Negative values reduce the probability of token appearing in output, while positive increase it.
The configuration parameters for repetition penalty are:<ul><li><strong>Repeat last N</strong> - Amount of tokens from the end of the context to consider for repetition penalty.</li><li><strong>Repeat penalty</strong> - <code>repeat_penalty</code> argument described above, if equal to <code>1.0</code> then the repetition penalty is disabled.</li><li><strong>Presence penalty</strong> - <code>presence_penalty</code> argument from the equation above.</li><li><strong>Frequency penalty</strong> - <code>frequency_penalty</code> argument from the equation above.</li></ul></li></ul><p>Additional literature:</p><ul><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/17vonjo/your_settings_are_probably_hurting_your_model_why/">Your settings are (probably) hurting your model</a></li></ul><p>In <em>Other sampler settings</em> we can find sampling queue configuration.
As i’ve mentioned earlier, the samplers are applied in a chain.
Here, we can configure the order of their application, and select which are used.
The setting uses short names for samplers, the mapping is following:</p><ul><li><code>d</code> - DRY</li><li><code>k</code> - Top-K</li><li><code>y</code> - Typical-P</li><li><code>p</code> - Top-P</li><li><code>m</code> - Min-P</li><li><code>x</code> - Exclude Top Choices (XTC)</li><li><code>t</code> - Temperature</li></ul><p>Some samplers and settings i’ve listed above may be missing from web UI configuration (like Mirostat), but they all can be configured via environmental variables, CLI arguments for <code>llama.cpp</code> binaries, or llama.cpp server API.</p><h2 id="final-thoughts">final thoughts<a href="#final-thoughts" arialabel="Anchor">#</a></h2><p>That is a <strong>long</strong> post, damn.
I have started writing this post at the end of October.
It’s almost December now.
During that time, multiple new models have been released - including SmolLM2, fun fact - i have originally planned to use Llama 3.2 3B.
The speed at which the LLM community moves and releases new stuff is absolutely incredible, but thankfully <code>llama.cpp</code> is <em>relatively</em> stable now.
I hope the knowledge i’ve gathered in this post will be useful and inspiring to the readers, and will allow them to play with LLMs freely in their homes.
That’s it, i’m tired.
I’m releasing that shit into the wild.</p><p>Suggestions for next posts are welcome, for now i intend to make some scripts for automated benchmarks w/ <code>llama-bench</code> and gather some data.
I’ll try to keep this post up-to-date and <em>maybe</em> add some stuff if it’s requested.
Questions are welcome too, preferably in the comments section.
Have a nice one.</p><h3 id="bonus-where-to-find-models-and-some-recommendations">bonus: where to find models, and some recommendations<a href="#bonus-where-to-find-models-and-some-recommendations" arialabel="Anchor">#</a></h3><p>My favorite site for finding models and comparing them is <a href="https://llm.extractum.io">LLM Explorer</a>.
It’s basically a search engine for models.
The UI is not exactly <em>great</em> but it’s <em>good enough</em>, it has the most comprehensive list of LLMs i’ve seen, and lots of search options.</p><p>As for my recommendations, some relatively recent models i’ve tried that made a positive impression upon me are:</p><ul><li>Google Gemma 2 9B SimPO - a fine-tune of Google Gemma model. Gemma models are pretty interesting, and their responses are noticeably different from other models.</li><li>Meta Llama 3.1/3.2 - i recommend trying out Llama 3.1 8B Instruct, as it’s the default go-to model for most LLM applications. There’s also many finetunes and <em>abliterated</em> versions that don’t have any built-in restrictions available publicly.</li><li>Microsoft Phi 3.5 - a series of models from Microsoft. Most of them are small, but there’s also big MoE (Mixture of Experts) version available.</li><li>Qwen/CodeQwen 2.5 - series of models from Alibaba, currently one of the best open-source models available. At the moment of writing this, CodeQwen 14B is my daily driver model.</li></ul><h3 id="post-mortem">post-mortem<a href="#post-mortem" arialabel="Anchor">#</a></h3><p>I’ve shared this post on Reddit and after getting the feedback, i managed to fix multiple issues with this post. Thanks!
I’m currently at the process of <em>refactors</em>, so many small things may change, and many other small things may be added in following days.
I’ll update this section when i’m finished.</p></div></div></div>
  </body>
</html>
