<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://chrisloy.dev/post/2025/08/03/context-engineering">Original</a>
    <h1>Context engineering</h1>
    
    <div id="readability-page-1" class="page"><article><p>As our use of LLMs has changed from conversational chatbots and into integral decision-making components
of complex systems, our inference approach must also evolve. The practice of &#34;prompt engineering&#34;, in
which precise wording is submitted to the LLM to elicit desired responses, has serious limitations. And so this is
giving way to a more general practice of considering every token fed into the LLM in a way that is more dynamic,
targeted, and deliberate. This expanded, more structured practice is what we now call &#34;context engineering.&#34;</p>
<blockquote>
<p>Throughout, we&#39;ll use a toy example of understanding how an LLM might help us answer a subjective question such as
&#34;What is the best sci-fi film?&#34;</p>
</blockquote>
<hr/>
<h2>Context windows</h2>
<p>An <a href="https://chrisloy.dev/post/2025/03/23/will-ai-replace-software">LLM</a> is a machine learning model that understands language by modelling
it as a sequence of tokens and learning the meaning of those tokens from the patterns of their co-occurrence in large
datasets. The number of tokens that the model can comprehend is a fixed quantity for each model, often in the hundreds
of thousands, and is known as the <strong>context window</strong>:
<img src="https://chrisloy.dev/images/2025/llm-1.svg" alt="LLMs are models for understanding sequences of tokens" title="image_tooltip"/>
LLMs are trained through repeated exposure to coherent token sequences — normally large textual databases scraped from
the internet. Once trained, we use the LLM by running &#34;inference&#34; (i.e. prediction) of the next token based on all the
previous tokens in a sequence. This sequence of previous tokens is what we used to refer to as the <strong>prompt</strong>:
<img src="https://chrisloy.dev/images/2025/llm-2.svg" alt="At inference time they generate one new token" title="image_tooltip"/>
Inference continues the token sequence by adding high-probability tokens to the sequence one at a time.</p>
<blockquote>
<p>When prompted to complete the sentence &#34;the best sci-fi film ever made is...&#34;, the highest probability
tokens to be generated might be <code>probably</code>, <code>star</code>, and <code>wars</code>.</p>
</blockquote>
<p>Early uses of LLMs focused on this mode of &#34;completion&#34;, taking partially written texts and predicting each subsequent
token in order to complete the text based on the desired lines. While impressive at the time, this was limiting in
several ways, including that it was difficult to instruct the LLM exactly <em>how</em> you wished the text to be completed.</p>
<hr/>
<h2>Chat framing</h2>
<p>To address this limitation, model providers started training their models to expect sequences of tokens that framed
conversations, with special tokens inserted to indicate the hand-off between two speakers. By learning to replicate
this &#34;chat&#34; framing when generating a completion, models were suddenly far more usable in conversational settings, and
therefore easier to instruct:
<img src="https://chrisloy.dev/images/2025/llm-3.svg" alt="Most are tuned to operate in a chat mode" title="image_tooltip"/>
The context window started to be more greedily filled up by different types of messages — system messages (special
instructions telling the LLM what to do), and chat history from both the user and the response from the LLM itself.</p>
<blockquote>
<p>With a chat framing, we can instruct the LLM that it is &#34;a film critic&#34; before &#34;asking&#34; it what the best sci-fi film
is. Maybe we&#39;ll now get the response tokens <code>blade</code> and <code>runner</code>, as the AI plays the role of a speaker likely to
reflect critical rather than popular consensus.</p>
</blockquote>
<p>The crucial point to understand here is that the LLM architecture <strong>did not change</strong> — it was still just predicting the
next token one at a time. But it was now doing that with a worldview learned from a training dataset that framed
everything in terms of delimited back-and-forth conversations, and so would consistently respond in kind.</p>
<hr/>
<h2>Prompt engineering</h2>
<p>In this setting, getting the most out of LLMs involved finding the perfect sequence of prompt
tokens to elicit the best completions. This was the birth of so-called &#34;prompt engineering&#34;, though in practice
there was often far less &#34;engineering&#34; than trial-and-error guesswork. This could often feel closer to
uttering mystical incantations and hoping for magic to happen, rather than the deliberate construction and rigorous
application of <a href="https://chrisloy.dev/post/2025/01/24/modular-software-design">systems thinking</a> that epitomises true engineering.</p>
<blockquote>
<p>We might try imploring the AI to reflect critical consensus with a smarter system prompt, something like
<code>You are a knowledgeable and fair film critic who is aware of the history of cinema awards</code>. We might hope that this
will &#34;trick&#34; the LLM into generating more accurate answers, but this hope rests on linguistic probability
and offers no guarantees.</p>
</blockquote>
<hr/>
<h2>In-context learning</h2>
<p>As LLMs got smarter and more reliable, we were able to feed them more complex sequences of tokens, covering different
types of structured and unstructured data. This enabled LLMs to produce completions that displayed
&#34;knowledge&#34; of probable token sequences based on novel structures in the prompt, rather than just remembered patterns
from their training dataset. This mode of feeding examples to the LLM is known as <strong>in-context learning</strong> because the
LLM appears to &#34;learn&#34; how to produce output purely based on example sequences within its context window.</p>
<p>This approach led to an explosion of different token sequences that we might programmatically include within
the prompt:</p>
<ul>
<li><strong>Hard-coded examples</strong>, taken from our knowledge domain (documentation, past examples of good output from human
or generated sources, toy examples) to encourage predictable output.</li>
<li><strong>Non-text modalities</strong>, with tokens that represented images, audio, or video, that were either directly part of
the context window, or first transcribed to text and then tokenised.</li>
<li><strong>Tool and function calls</strong>, defining external functions that the LLM could tell the caller to invoke to
access data or computation from the outside world.</li>
<li><strong>Documents and summaries</strong>, returned via &#34;RAG&#34; from data sources, or uploaded by users, to feed knowledge into the
LLM that lay outside its training dataset.</li>
<li><strong>Memory and conversation history</strong>, condensing information from prior chats, that allowed continuity between a
single user and the &#34;chatbot&#34; over multiple conversations.</li>
</ul>
<blockquote>
<p>In our sci-fi film example, our prompt could include many things to help the LLM: historic box office receipts, lists
of the hundred greatest films from various publications, <em>Rotten Tomatoes</em> ratings, the full history of Oscar winners,
etc.</p>
</blockquote>
<p>Suddenly, our 100,000+ context window isn&#39;t looking so generous anymore, as we stuff it with tokens from all kinds of
places:
<img src="https://chrisloy.dev/images/2025/llm-4.svg" alt="But the full context can be controlled by the caller" title="image_tooltip"/>
This expansion of context not only depletes the available context window for output generation, it also increases the
overall footprint and complexity of what the LLM is paying attention to at any one time. This then increases the risk of
failure modes such as hallucination. As such, we must start approaching its construction with more nuance — considering
brevity, relevance, timeliness, safety, and other factors.</p>
<p>At this point, we aren&#39;t simply &#34;prompt engineering&#34; anymore. We are beginning to engineer the entire context in which
generation occurs.</p>
<hr/>
<h2>From oracle to analyst</h2>
<p>Language encodes knowledge, but it also encodes meaning, logic, structure, and thought. Training an LLM to encode
knowledge of what exists in the world, and to be capable of producing language that would describe it, therefore, also
produces a system capable of simulating thought. This is, in fact, the key utility of an LLM, and to take advantage
of it requires a mindset shift in how we approach inference.</p>
<p>To adopt context engineering as an approach to LLM usage is to reject using the LLM as a <strong>mystical oracle</strong> to
approach, pray to with muttered incantations, and await the arrival of wisdom. We instead think of briefing a <strong>skilled
analyst</strong>: bringing them all the relevant information to sift through, clearly and precisely defining the task at hand,
documenting the tools available to complete it, and avoiding reliance on outdated, imperfectly remembered training data.
<img src="https://chrisloy.dev/images/2025/llm-6.svg" alt="Context engineering reframes the LLM as an analyst, not an oracle" title="image_tooltip"/>
In practice, our integration of the LLM shifts from &#34;crafting the perfect prompt&#34;, towards instead the precise
construction of exactly the right set of
tokens needed to complete the task at hand. Managing context becomes an engineering problem, and the LLM is reframed
as a task solver whose output is natural language.</p>
<hr/>
<h2>Engineering context for agentic behaviour</h2>
<p>Let&#39;s consider a simple question you might wish an LLM to answer for you:</p>
<blockquote>
<p>What is the average weekly cinema box office revenue in the UK?</p>
</blockquote>
<p>In &#34;oracle&#34; mode, our LLM will happily quote a value learned from the data in its training dataset prior to its cutoff:</p>
<blockquote>
<p><em>As of 2019, the UK box office collects roughly £24 million in revenue per week on average.</em></p>
</blockquote>
<p>This answer from GPT 4.1 is accurate, but imprecise and outdated. Through context engineering, we can do a lot better.
Consider what additional context we might feed into the context window before generating the first token of the
response:</p>
<ul>
<li>The date, so we use updated stats (GPT 4.1 thinks <em>now</em> is June 2024)</li>
<li>Actual published statistics such as <a href="https://www.bbc.co.uk/news/articles/cx2j1jpnglvo">this BBC News article</a></li>
<li>Instructions on how to tell the caller to divide two numbers</li>
</ul>
<p>The above should be enough for the LLM to know how to: look for data for 2024; extract the total
figure of £979 million from the document; and call an external function to precisely divide that by 52 weeks.
Assuming the caller then runs that calculation and
invokes the LLM again, with all the above context, plus its own output, plus the result of the calculation, we will
then get our accurate answer:</p>
<blockquote>
<p><em>Across the full year of 2024, the UK box office collected £18.8 million in revenue per week on average.</em></p>
</blockquote>
<p>Even this trivial example involves multiple ways of engineering the context before generating the answer:</p>
<ul>
<li>Stating the current date and desired outcome;</li>
<li>Searching for and returning relevant documents;</li>
<li>Documenting available calculation operations;</li>
<li>Expanding context with intermediary results.</li>
</ul>
<p>Fortunately, we do not need to invent a new approach every single time.</p>
<hr/>
<h2>Is this just RAG?</h2>
<p>Retrieval-augmented generation (RAG) is a fashionable technique for injecting external knowledge into the
context window at inference time. Leaving aside implementation details of how to identify the correct documents to
include, we can clearly see that this is another specific form of context engineering:
<img src="https://chrisloy.dev/images/2025/llm-5.svg" alt="RAG is just one pattern of context engineering" title="image_tooltip"/>
This is a useful and obvious way to use pre-trained LLMs in contexts that need access to knowledge outside the training
dataset.</p>
<blockquote>
<p>For a correct answer, our application needs to be aware of <strong>up-to-date</strong> film reviews, ratings, and awards, to track
new films and critical opinion after the point the model was trained.
By including
relevant extracts in the context window, we enable our LLM to generate completions with today&#39;s data and avoid
hallucination.</p>
</blockquote>
<p>To do this, we can <strong>search for relevant documents</strong> and then <strong>include them in the context window</strong>. If this sounds
conceptually simple, that is because <em>it is</em> — though reliable
implementation is not trivial and requires robust engineering.</p>
<p>Complex systems can be brittle and opaque to build. We need a way to scale complexity without harming our ability to
maintain, debug, and reason about our code. Fortunately, we can apply the same thinking that traditional software design
used to solve this same problem.</p>
<p>We can think of RAG as simply the first of many <strong>design patterns</strong> for context engineering. And
just as with other software engineering design patterns, in future we will find that most complex systems will have to
employ variations and combinations of such patterns in order to be most effective.</p>
<hr/>
<h2>Composition over inheritance</h2>
<p>In software engineering, <strong>design patterns</strong> promote reusable software by providing proven, general solutions to common
design problems. They
encourage composition over inheritance, meaning systems are built from smaller, interchangeable components rather than
rigid class hierarchies. They make your codebase more flexible, testable, and easier to maintain or extend. They are a
crucial piece of the software design toolkit, that enable engineers to build large functioning codebases that can scale
over time.</p>
<p>Some examples of software engineering design patterns include:</p>
<ul>
<li><code>Factory</code>: standardises object creation to make isolated testing easier</li>
<li><code>Decorator</code>: extends behaviour without editing the original</li>
<li><code>Command</code>: passes work around as a value, similar to a lambda function</li>
<li><code>Facade</code>: hides internals with a simple interface to promote abstraction</li>
<li><code>Dependency injection</code>: wires modules externally using configuration</li>
</ul>
<p>These patterns were developed over a long time, though many were first codified in
<a href="https://en.wikipedia.org/wiki/Design_Patterns">a single book</a>. Context engineering is a nascent field, but already
we see some common patterns emerging that adapt LLMs well to certain tasks:</p>
<ul>
<li><code>RAG</code>: inject retrieved documents based on relevance to user intent</li>
<li><code>Tool calling</code>: list available tools and inject results into the context</li>
<li><code>Structured output</code>: fix a JSON/XML schema for the LLM completions</li>
<li><code>Chain of thought / ReAct</code>: emit reasoning tokens before answering</li>
<li><code>Context compression</code>: summarise long history into pertinent facts</li>
<li><code>Memory</code>: store and recall salient facts across sessions</li>
</ul>
<blockquote>
<p>In our examples above, we have already used some of these patterns:</p>
<ul>
<li>RAG for getting film reviews, critics&#39; lists, and box office data</li>
<li>Tool calling to calculate weekly revenues accurately</li>
</ul>
<p>Some of the other techniques, such as ReAct, could help our LLM to frame and verify its responses more carefully,
counterbalancing the weight of linguistic probability learnt from its training data.</p>
</blockquote>
<p>By seeing each as a <strong>context engineering design pattern</strong>, we are able to pick the right ones for the task at hand,
compose them into an &#34;agent&#34;, and avoid compromising our ability to test and reason about our code.</p>
<hr/>
<h2>Extending to multiple agents</h2>
<p>Production systems that rely on LLMs for decision-making and action will naturally evolve towards multiple agents with
different specialisations: safety guardrails; information retrieval; knowledge distillation; human interaction; etc.
Each of these is a component that interprets a task, then returning a sequence of tokens indicating
actions to take, the information retrieved, or both.
<img src="https://chrisloy.dev/images/2025/llm-7.svg" alt="Multi-agent systems feed context to each other" title="image_tooltip"/></p>
<blockquote>
<p>For our multi-agent film ranker, we might need several agents:</p>
<ul>
<li><strong>Chatbot Agent</strong>: to maintain a conversation with the user</li>
<li><strong>Safety Agent</strong>: to check that the user is not acting maliciously</li>
<li><strong>Preference Agent</strong>: recalls if the user wants to ignore some reviews</li>
<li><strong>Critic Agent</strong>: to synthesise sources and make a final decision</li>
</ul>
<p>Each of these is specialised for a given task, but this can be done purely through engineering the context they
consume, <strong>including outputs from other agents in the system</strong>.</p>
</blockquote>
<p>Outputs are then passed around the system and into the context windows of other agents. At every step, the crucial
aspect to consider is the <strong>patterns</strong> by which token sequences are generated, and how the output of one agent will
be used as <strong>context</strong> for another agent to complete its own task. The hand-off token sequence is effectively the
contract for agent interaction — apply as much rigour to it as you would any other API within your software
architecture.</p>
<hr/>
<h2>Summary</h2>
<p>Context engineering is the nascent but critical discipline that governs how we are able to effectively guide LLMs
into solving the tasks we feed into them. As a subfield of software engineering, it benefits from systems and design
thinking, and we can learn lessons from the application of <strong>design patterns</strong> for producing software that is
<a href="https://chrisloy.dev/post/2025/01/24/modular-software-design">modular</a>, robust, and comprehensible.</p>
<p>When working with LLMs, we must therefore:</p>
<ul>
<li>Treat the LLM as an <strong>analyst</strong>, not an oracle. Give it whatever it needs to solve the task.</li>
<li>Take responsibility for the <strong>entire context window</strong>, not just the system and user prompts.</li>
<li>Use composable, reusable <strong>design patterns</strong> that can be engineered and tested in isolation.</li>
<li>Frame the hand-off between agents as an <strong>API contract</strong> between their context windows.</li>
</ul>
<p>By doing these, we can control in-context learning with the same rigour as any other engineered software.</p></article></div>
  </body>
</html>
