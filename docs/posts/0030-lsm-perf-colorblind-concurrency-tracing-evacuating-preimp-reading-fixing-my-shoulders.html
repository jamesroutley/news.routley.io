<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://scattered-thoughts.net/log/0030/">Original</a>
    <h1>0030: lsm perf, colorblind concurrency, tracing, evacuating preimp, reading, fixing my shoulders</h1>
    
    <div id="readability-page-1" class="page"><article>
  <p>Things I&#39;ve been doing in November:</p>
<ul>
<li><a href="https://handmade-seattle.com/">handmade seattle</a>!</li>
<li>at tigerbeetle
<ul>
<li>hunting performance bugs in the lsm tree</li>
<li>adding tracing</li>
</ul>
</li>
<li>evacuating <a href="https://github.com/jamii/preimp">preimp</a></li>
<li>musing
<ul>
<li>colorblind concurrency</li>
</ul>
</li>
<li>reading
<ul>
<li><a href="https://www.databass.dev/">database internals</a></li>
<li><a href="https://www.goodreads.com/book/show/55742688-four-thousand-weeks">four thousand weeks</a></li>
<li><a href="https://www.goodreads.com/book/show/56587388-how-the-world-really-works">how the world really works</a></li>
</ul>
</li>
<li>fixing my shoulders</li>
</ul>
<p>Most of the handmade talks aren&#39;t up yet. I&#39;ll try to remember to link to them next month.</p>
<h2 id="lsm-perf">lsm perf</h2>
<p>The lsm tree is needed in tigerbeetle to reduce the time taken to recover from crashes and to reduce the cost (ram) of running large databases (vs just having a replicated log on disk and all other state in memory). It&#39;s not ready yet though - merging the lsm tree reduced the single node performance on my laptop to ~23k transfers/second (the goal is &gt;1m transfers/second). So that&#39;s my main focus at the moment. I&#39;ve identified a couple of issues so far but haven&#39;t fixed any of them yet.</p>
<p>The ssd on my laptop will happily sustain 2gb/s write. During end-to-end benchmarks TB maxes out at ~10mb/s read and 20-80mb/s write and does not appear to be cpu- or memory-bound. So my first suspicion is that we&#39;re scheduling io poorly.</p>
<p>Currently every level of every tree is compacted concurrently (actually odd and even levels are compacted in separate phases, but ignore that for now). Each compaction has a pre-allocated fixed-size buffer for reading from and writing to disk. But only insanely large databases ever reach enough lsm levels to make use of all these buffers. In an empty database we only have 1 level per tree, so we&#39;re only able to use 1/8th of the buffer space that we&#39;re paying for. Under simple benchmarks I&#39;m also seeing that most of the time we&#39;re just waiting for the Transfer.id tree to compact - 1 of our 20 trees. So most of the time we&#39;re only using 1/160th of our buffer space, leaving us with a maximum of one 64kb read in flight - far too little to maximise disk throughput. This will be solved by <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/269">doing only a few compactions at once</a> (replacing the 160 small buffers with a few large buffers) and by <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/270">better pipelining</a> within those few compactions.</p>
<p>We&#39;re probably also doing compactions too often. TB currently has very small in-memory tables (use less ram) and large on-disk tables (bigger tables =&gt; smaller manifest =&gt; less ram). The problem with this is that for  compactions from memory to level 0 of the lsm tree:</p>
<ul>
<li>The frequency depends on the size of the in-memory table. Smaller table =&gt; more frequent compactions.</li>
<li>THe cost depends on the size of the on-disk tables. Bigger tables =&gt; more expensive compactions.</li>
</ul>
<p>So we&#39;re currently in the not-so-sweet-spot of frequent expensive compactions. I tried to <a href="https://github.com/tigerbeetledb/tigerbeetle/pull/244">estimate</a> the cost of this and it looks like we could cut the compaction write bandwidth by something like 5-8x depending on which solution we go for. (The solution that reduces the compaction bandwidth the most also increases the size of the manifest, which increases checkpoint bandwidth, so it&#39;s not a clear win over the next-best option). </p>
<p>Finally, while we&#39;re not cpu-bound at the moment, we spend something like 30% cpu on hashing (to detect disk corruption) and 30% on merging tables during compaction. Reducing the number of compactions will help with both of those, but after finishing the <a href="https://github.com/tigerbeetledb/tigerbeetle/issues/274">current list</a> of optimizations I suspect we&#39;ll still end up cpu-bound before hitting 1m transers/second. If so we&#39;ll have to either figure out how to optimize one or both of those hotspots or farm them out to a thread pool.</p>
<h2 id="colorblind-concurrency">colorblind concurrency</h2>
<p>Zig&#39;s async/await is implemented as a calling convention rather than as a distinct type. This is intended to prevent the situation in eg rust where async types are contagious and the ecosystem ends up partitioned into sync and async versions of every library. In zig, most library code can just use stdlib io functions and will work correctly (in theory) in both sync and async contexts.</p>
<p>An unfortunate side effect of this is that suspend points are no longer syntactically distinguished. Any function call could potentially suspend, during which some other async function may come in and make arbitary changes to state. This makes it very difficult to reason about code. </p>
<p>For example, if you have a cache that is shared between many tasks then you it&#39;s unsafe to hold a pointer to a cached value across a suspend - something else might modify the cache while the current function is suspended. If you don&#39;t know where the suspend points are, then to be safe you have to never hold a pointer to a cached value across <em>any function call</em>.</p>
<p>An argument I&#39;ve seen repeated is that at least this is no worse than writing multi-threaded programs, where any other thread can change your state at any point. I think this is missing the point. Writing correct multi-threaded programs is incredibly hard! The whole appeal of cooperative models like async/await is that you can handle io concurrency without the cognitive overhead of data races and the cpu/cache overhead of scheduling mechanisms like locks and queues. </p>
<p>Perhaps data races are less of a problem if you are writing eg a web app backend, where the entire process is essentially stateless and all actual coordination is offloaded to a separate database server. But zig is a systems language - it seems likely that people will want to write stateful programs! For example, many databases avoid context switches by using a shared-nothing thread-per-core architecture, with a separate async executor per thread (eg <a href="https://www.scylladb.com/product/technology/shard-per-core-architecture/">ScyllaDB</a>, <a href="https://redpanda.com/blog/what-makes-redpanda-fast">Redpanda</a>, <a href="https://www.voltactivedata.com/wp-content/uploads/2017/03/hv-white-paper-voltdb-technical-overview.pdf">VoltDB</a>). It would be painful to replicate this architecture in zig at the moment.</p>
<p>In fact tigerbeetle started out by using zig&#39;s async/await but ran into too many bugs caused by invisible suspends and had to switch to using explicit callbacks instead. Callbacks have massive downsides of their own - much harder to read, useless stack traces, inability to stack-allocate across suspend points etc. But those downsides are still outweighed by the difficulty of writing correct async/await code without explicit yield points.</p>
<p>The design of async in zig is far from settled, so I&#39;ve been trying to figure out if there are minimal changes that would allow tigerbeetle to go back to using async/await. </p>
<p><code>nosuspend</code> blocks are close. They act as a runtime assertion that no function calls within the block will suspend. Perhaps we need a <code>sync</code> block that acts as a comptime assertion that no function calls within the block require an async calling convention. Getting variables out of a block might be syntatically annoying though, especially since zig doesn&#39;t have an equivalent yet to rust&#39;s statically checked initialization or syntactically-lightweight multiple return from blocks:</p>
<pre data-lang="rust"><code data-lang="rust"><span>// rust: uninitialized variable
</span><span>let</span><span> x</span><span>;
</span><span>let</span><span> y</span><span>;
</span><span>{
</span><span>    </span><span>// ...do sync stuff...
</span><span>    x </span><span>= </span><span>1</span><span>;
</span><span>    y </span><span>= </span><span>2</span><span>;
</span><span>}
</span><span>// error[E0384]: cannot assign twice to immutable variable `x`
</span><span>x </span><span>= </span><span>2</span><span>;
</span><span>
</span></code></pre>
<pre data-lang="zig"><code data-lang="zig"><span>// zig: uninitialized variable
</span><span>var </span><span>x</span><span>: </span><span>usize </span><span>= </span><span>undefined</span><span>;
</span><span>var </span><span>y</span><span>: </span><span>usize </span><span>= </span><span>undefined</span><span>;
</span><span>{
</span><span>    </span><span>// ...do sync stuff...
</span><span>    x </span><span>= </span><span>1</span><span>;
</span><span>    y </span><span>= </span><span>2</span><span>;
</span><span>}
</span><span>// no compile error
</span><span>x </span><span>= </span><span>2</span><span>;
</span><span>
</span></code></pre>
<pre data-lang="rust"><code data-lang="rust"><span>// rust: return from block
</span><span>let </span><span>(x</span><span>,</span><span>y) </span><span>= </span><span>{
</span><span>    </span><span>// ...do sync stuff...
</span><span>    (</span><span>1</span><span>, </span><span>2</span><span>)
</span><span>}</span><span>;
</span><span>
</span></code></pre>
<pre data-lang="zig"><code data-lang="zig"><span>// zig: return from block
</span><span>const</span><span> x_and_y </span><span>= </span><span>x_and_y</span><span>: {
</span><span>    </span><span>// ...do sync stuff...
</span><span>    </span><span>break</span><span> :</span><span>x_and_y</span><span> .{</span><span>1</span><span>, </span><span>2</span><span>}</span><span>;
</span><span>}</span><span>;
</span><span>const</span><span> x </span><span>=</span><span> x_and_y[</span><span>0</span><span>]</span><span>;
</span><span>const</span><span> y </span><span>=</span><span> x_and_y[</span><span>1</span><span>]</span><span>;
</span><span>
</span></code></pre>
<p>Another alternative might be a custom lint. At the moment there doesn&#39;t seem to be a way at comptime to detect if a function contains a suspend point eg </p>
<pre data-lang="zig"><code data-lang="zig"><span>fn </span><span>foo</span><span>() </span><span>void </span><span>{
</span><span>    </span><span>suspend </span><span>{}
</span><span>}
</span><span>
</span><span>pub fn </span><span>main</span><span>() </span><span>void </span><span>{
</span><span>    </span><span>// prints .Unspecified instead of .Async
</span><span>    std.debug.</span><span>print</span><span>(</span><span>&#34;{}&#34;</span><span>,</span><span> .{</span><span>@typeInfo</span><span>(</span><span>@TypeOf</span><span>(foo)).Fn.calling_convention})</span><span>;
</span><span>}
</span></code></pre>
<p>But if there was a way to detect at comptime whether a function is transitively async, then we could assert that every asynchronous function in our codebase has a name starting with <code>async_</code> and then it would be easier to spot yield points (especially if I tweak my syntax highlighting to make those names stand out).</p>
<p>Once zig has ide tools that can evaluate comptime code, this could even just be an ide feature rather than a lint, at which point the downsides of colorblind async would be mostly erased.</p>
<h2 id="tracing">tracing</h2>
<p>Sample-based flamecharts are not very useful for understanding performance issues in tigerbeetle because the callback-based async doesn&#39;t preserve stacktraces, and because the main thread is constantly switching between different tasks as io events complete.</p>
<p>Here&#39;s a sample-based flamechart from a single-node benchmark:</p>
<p><img src="https://macwright.com/log/0030/tracy-sample.png" alt=""/></p>
<p>I can see that it&#39;s spending some time in io_uring, in the storage engine and in merging. But I don&#39;t know <em>why</em>. Is that one operation or many? What&#39;s on the critical path?</p>
<p>Here&#39;s the same region with a few manually added traces (~20loc diff):</p>
<p><img src="https://macwright.com/log/0030/tracy-full.png" alt=""/></p>
<p>Immediately we can see what is going on - we&#39;re in the middle of a long compaction, the Transfer.id tree is still compacting, and all the other trees have finished compacting. The little sawtooths at the bottom of the pink trace show individual merges, so we can see that it&#39;s not merge-bound. Once I hook up the io system we&#39;ll also be able to see individual reads and writes between the pink merges, at which point it becomes obvious at a glance that the problem is we don&#39;t have enough concurrent io.</p>
<p>I thrashed a lot on the api design, eventually realizing that all the tracing systems I&#39;m familiar with are fundamentally designed for systems that are structured differently to tigerbeetle. </p>
<p>Dapper/zipkin/jaeger all have this model where spans have a hierarchy which can be inferred from rpc calls, and spans are grouped into a trace which matches 1:1 with some external rpc call (eg loading a homepage). In tigerbeetle most of the work isn&#39;t directly attributable to some external request. Instead there are a number of &#39;threads&#39; each of which is working on handling some subtask for an entire batch of requests, and then at pre-determined points in the schedule we wait for all the &#39;threads&#39; to complete their current work (so we can back-pressure the clients instead of letting background work like compaction fall behind).</p>
<p>Most tracing systems also take advantage of either threads or async/await to associate work with a specific context (a thread-local or task-local variable). Tigerbeetle does have a fork/join -like structure to it&#39;s concurrency, but it&#39;s not expressed in a way that the io scheduler is actually aware of so we can&#39;t take advantage of it to manage the span hierarchy.</p>
<p>I ended up resorting to a very manual api where the coder has to specify which logical &#39;thread&#39; the span belongs to, and is also responsible for ensuring the spans within each logical &#39;thread&#39; form a tree. </p>
<pre data-lang="zig"><code data-lang="zig"><span>tracer.</span><span>start</span><span>(
</span><span>    </span><span>// Pre-allocated storage for the span start
</span><span>    </span><span>&amp;</span><span>tree.tracer_slot</span><span>,
</span><span>    </span><span>// The &#39;thread&#39; name
</span><span>    .{ .tree </span><span>=</span><span> .{ .tree_name </span><span>=</span><span> tree_name } }</span><span>,
</span><span>    </span><span>// The event
</span><span>    .{ .tree_compaction_beat </span><span>=</span><span> .{ .tree_name </span><span>=</span><span> tree_name } }</span><span>,
</span><span>)</span><span>;
</span><span>
</span><span>// ...do the work...
</span><span>
</span><span>tracer.</span><span>end</span><span>(
</span><span>    </span><span>&amp;</span><span>tree.tracer_slot</span><span>,
</span><span>    </span><span>// Specify the &#39;thread&#39; name and event again to check that we did start/end in the right order.
</span><span>    .{ .tree </span><span>=</span><span> .{ .tree_name </span><span>=</span><span> tree_name } }</span><span>,
</span><span>    .{ .tree_compaction_beat </span><span>=</span><span> .{ .tree_name </span><span>=</span><span> tree_name } }</span><span>,
</span><span>)</span><span>;
</span></code></pre>
<p>Pretty clunky. If we were using async/await it could instead look like:</p>
<pre data-lang="zig"><code data-lang="zig"><span>// The span start is stored on the stack
</span><span>const</span><span> span </span><span>=</span><span> tracer.</span><span>start</span><span>(
</span><span>    </span><span>// No need to specify &#39;thread&#39; because we can set it once when spawning the task
</span><span>    .{ .tree_compaction_beat </span><span>=</span><span> .{ .tree_name </span><span>=</span><span> tree_name } }</span><span>,
</span><span>)</span><span>;
</span><span>// No way to get the ordering wrong if we use `defer`
</span><span>defer</span><span> span.</span><span>end</span><span>()</span><span>;
</span><span>
</span><span>// ...do the work...
</span></code></pre>
<p>The tracing backend is configured at compile-time. The default is <code>none</code> which simply compiles tracing out entirely. </p>
<p>I added a <a href="https://perfetto.dev/">perfetto</a> backend first because it&#39;s really easy to integrate with - just spit out a json file and upload it to <a href="https://ui.perfetto.dev/">ui.perfetto.dev</a>.</p>
<p>But the main backend is <a href="https://github.com/wolfpld/tracy">tracy</a>. It&#39;s more pain to adopt eg on mac/linux you have to build the ui from source yourself. But it can trace in real-time and also supports sampling, log capture, allocation tracking, perf counters etc all in one timeline. I keep stumbling over new features like the per-span callstacks being linked to the source code view which is annotated with cache miss frequency and linked to the asm view which looks up x86 op latency/throughput...</p>
<p><img src="https://macwright.com/log/0030/tracy-features.png" alt=""/></p>
<h2 id="evacuating-preimp">evacuating preimp</h2>
<p>Now that I don&#39;t have time to work on <a href="https://github.com/jamii/preimp">preimp</a> - my odd language/database/spreadsheet/thing - I need to port the programs I wrote in preimp to something more that won&#39;t bitrot the second I look away. </p>
<p>Most can just be manually-edited scripts, but the few I share with my wife need to be web apps. I couldn&#39;t quite bring myself to be entirely pragmatic and port them to javascript, so I wrote them all in zig. Data stored in sqlite. UI almost entirely static html, served by <a href="https://h2o.examp1e.net/">libh2o</a>. </p>
<p>(I tried to use <a href="https://github.com/uNetworking/uSockets">uSockets</a> first since it&#39;s used in <a href="https://github.com/oven-sh/bun">bun</a>. But many of the capi examples in the repo don&#39;t work at all - they crash, return wrong results etc.)</p>
<p>There is near zero documentation for libh2o outside of a few incomplete examples, but I was able to figure it out reasonably easily by just rummaging around in the source code.</p>
<p>I was struck by reading <a href="https://www.slideshare.net/kazuho/h2o-20141103pptx">these slides</a> (slideshare, sorry) on the design of h2o and noticing how many of the code examples would be easier in zig - slices instead of null-terminated pointers, loop unrolling (<code>inline for (0..8) |_| do_it()</code> vs repeated macros), comptime sprintf. </p>
<p>libh2o also has a http client but it looks very verbose to use, and it looks like we might get a native zig http client sometime in 2023, so I left all my external service integrations in clojure for now.</p>
<p>But I look forward to never having to maintain any clojure code again. I spent 15 minutes this week debugging this code:</p>
<pre data-lang="clj"><code data-lang="clj"><span>(if (= (get-in wise-transaction [</span><span>&#34;details&#34; &#34;type&#34;</span><span>] </span><span>&#34;CONVERSION&#34;</span><span>))
</span><span>  </span><span>; split conversions into two transactions
</span><span>  [{</span><span>&#34;id&#34; </span><span>(str </span><span>&#34;WISE &#34; </span><span>(get-in wise-transaction [</span><span>&#34;referenceNumber&#34;</span><span>]) </span><span>&#34; SOURCE&#34;</span><span>)
</span><span>    </span><span>&#34;date&#34; </span><span>(get-in wise-transaction [</span><span>&#34;date&#34;</span><span>])
</span><span>    </span><span>&#34;amount&#34; </span><span>(- (get-in wise-transaction [</span><span>&#34;details&#34; &#34;sourceAmount&#34; &#34;value&#34;</span><span>]))
</span><span>    </span><span>&#34;currency&#34; </span><span>(get-in wise-transaction [</span><span>&#34;details&#34; &#34;sourceAmount&#34; &#34;currency&#34;</span><span>])
</span><span>    </span><span>&#34;description&#34; </span><span>(get-in wise-transaction [</span><span>&#34;details&#34; &#34;description&#34;</span><span>])
</span><span>    </span><span>&#34;who&#34; </span><span>user}
</span><span>   {</span><span>&#34;id&#34; </span><span>(str </span><span>&#34;WISE &#34; </span><span>(get-in wise-transaction [</span><span>&#34;referenceNumber&#34;</span><span>]) </span><span>&#34; TARGET&#34;</span><span>)
</span><span>    </span><span>&#34;date&#34; </span><span>(get-in wise-transaction [</span><span>&#34;date&#34;</span><span>])
</span><span>    </span><span>&#34;amount&#34; </span><span>(get-in wise-transaction [</span><span>&#34;details&#34; &#34;targetAmount&#34; &#34;value&#34;</span><span>])
</span><span>    </span><span>&#34;currency&#34; </span><span>(get-in wise-transaction [</span><span>&#34;details&#34; &#34;targetAmount&#34; &#34;currency&#34;</span><span>])
</span><span>    </span><span>&#34;description&#34; </span><span>(get-in wise-transaction [</span><span>&#34;details&#34; &#34;description&#34;</span><span>])
</span><span>    </span><span>&#34;who&#34; </span><span>user}]
</span><span>  [{</span><span>&#34;id&#34; </span><span>(str </span><span>&#34;WISE &#34; </span><span>(get-in wise-transaction [</span><span>&#34;referenceNumber&#34;</span><span>]))
</span><span>    </span><span>&#34;date&#34; </span><span>(get-in wise-transaction [</span><span>&#34;date&#34;</span><span>])
</span><span>    </span><span>&#34;amount&#34; </span><span>(get-in wise-transaction [</span><span>&#34;amount&#34; &#34;value&#34;</span><span>])
</span><span>    </span><span>&#34;currency&#34; </span><span>(get-in wise-transaction [</span><span>&#34;amount&#34; &#34;currency&#34;</span><span>])
</span><span>    </span><span>&#34;description&#34; </span><span>(get-in wise-transaction [</span><span>&#34;details&#34; &#34;description&#34;</span><span>])
</span><span>    </span><span>&#34;who&#34; </span><span>user}])</span><span>)
</span></code></pre>
<p>One of those parens is in the wrong place, which results in the if condition always returning true, leading to a null pointer exception in <code>(- (get-in wise-transaction [&#34;details&#34; &#34;sourceAmount&#34; &#34;value&#34;]))</code>. This bug wouldn&#39;t have been possible in eg javascript, which is saying something.</p>
<p>Similarly for this bug:</p>
<pre data-lang="clj"><code data-lang="clj"><span>(client/post
</span><span>      fetch-url
</span><span>      (merge
</span><span>        fetch-auth
</span><span>        {</span><span>:headers </span><span>{</span><span>:content-type </span><span>&#34;application/json&#34;
</span><span>                   </span><span>:body </span><span>(json/write-str fetch-transactions)}}))
</span></code></pre>
<p>The <code>:body</code> should in the outer map. This silently sends the body as a header instead of complaining that no body was given, which manifests as a 40x error because it overflows nginx&#39; header buffer.</p>
<p>It&#39;s always really easy and pleasant to write code in clojure, but all the time I gain in writing the code I then lose in laboriously tracking down silly bugs that in a sane language, or even javascript, would have been caught and reported at the source.</p>
<p>Archiving all my back-burnered and abandoned projects felt surprisingly good. It&#39;s silly, but having officially declaring them not-in-progress frees up some background threads in my brain.</p>
<h2 id="reading">reading</h2>
<p><a href="https://www.goodreads.com/en/book/show/57850403-understanding-software-dynamics">Understanding software dynamics</a> is on the back-burner for now.</p>
<p>I&#39;m slowly working through <a href="https://www.databass.dev/">database internals</a>. It ignore data models and query engines (things I feel good about) and focuses entirely on storage engines and distributed transactions / consensus (things I feel very shaky on). It doesn&#39;t really explain any given topic in enough detail to fully understand, but it works really well as a thorough map of the space - what things you should understand and which papers you should read to understand them.</p>
<p>SplinterDB is a new embeddable database. I watched their <a href="https://www.youtube.com/watch?v=1gOlXfbiT_Y">talk</a> at cmu but haven&#39;t yet dug into the <a href="https://www.usenix.org/system/files/atc20-conway.pdf">paper</a>. The clear win is borrowing ideas from Beps trees to reduce write amplification. But ~2x less write amplification doesn&#39;t exaplain ~8x better throughput and it&#39;s not clear to me which of the other differences from existing lsm engines is responsible for the difference. Better concurrency might be the answer, but <a href="https://github.com/vmware/splinterdb/blob/main/docs/limitations.md">they don&#39;t support transactions yet</a>, which limits the possible use cases. They also <a href="https://github.com/vmware/splinterdb/issues/236">haven&#39;t implemented logging or checkpointing yet</a> which seems like a big caveat to omit from the benchmarks. Phil Eaton also just pointed me at <a href="https://smalldatum.blogspot.com/2022/12/benchmarketing-rocksdb-vs-splinterdb.html">Mark Callaghan&#39;s independent benchmarks</a> which note that splinterdb doesn&#39;t fsync.</p>
<p><a href="https://www.goodreads.com/book/show/55742688-four-thousand-weeks">Four thousand weeks</a> is repetitive but has a few interesting themes. It could have been a really good short book instead of a mediocre long book.</p>
<blockquote>
<p>Worry, at its core, is the repetitious experience of a mind attempting to generate a feeling of security about the future, failing, then trying again and again and again...</p>
</blockquote>
<blockquote>
<p>...develop a taste for having problems. Behind our urge to race through every obstacle or challenge, in an effort to get it &#34;dealt with,&#34; there&#39;s usually the unspoken fantasy that you might one day finally reach the state of having no problems whatsoever. ...we seem to believe, if only subconsciously, that we shouldn&#39;t have problems at all.</p>
</blockquote>
<p><a href="https://www.goodreads.com/book/show/56587388-how-the-world-really-works">How the world really works</a> looks at the full scope of industrial dependence on fossil fuels. In particular, the author is frustrated that environmental debate tends to focus on replacing fossil fuels in electricity production and personal transport, which are both fairly tractable, and ignores that eg we don&#39;t know how to produce enough food to stave off famine without using natural gas for synthesizing ammonia. (Kinda prophetic, given the recent spike in food prices.) It doesn&#39;t come across as anti- climate action, but rather as a plea to address the entire scope of the problem rather than focusing on the easy parts and declaring the entire problem as solvable.</p>
<h2 id="fixing-my-shoulders">fixing my shoulders</h2>
<p>Over 2022 I had increasingly bad impingement in both shoulders (climbing and coding both encourage a concave posture). Most of the things I was recommended (stretching, massage etc) provided temporary relief but didn&#39;t stop the slow constant detoriation.</p>
<p>What did work in the end was a mixture of <a href="https://www.youtube.com/watch?v=G-_WiyE9HDg">rotator cuff strengthening</a> (especially external rotation) and <a href="https://www.youtube.com/watch?v=lhZ88UpZfmQ">PAILS/RAILS stretching</a> (especially internal rotation). I went from unable to complete a single pullup to nearly full function in about 4 weeks. </p>
<p>Weirdly, I seem to be a little stronger now at pulling up and locking off despite having to avoid those movements most of the year. I&#39;ve read before that scapular and rotator cuff strength can be a limiter on pulling strength, because your nervous system won&#39;t let you apply full power if your shoulders are unstable. So maybe that&#39;s what&#39;s going on.</p>
<p>I&#39;m now trying to get ahead of the next chronic injury by doing regular antagonist training and RAILS/PAILS stretching (replacing static stretching, which doesn&#39;t seem to help) for all the troublesome joints, especially wrists, hips and knees. I&#39;m getting too old to only do rehab <em>after</em> injury :S</p>

</article></div>
  </body>
</html>
