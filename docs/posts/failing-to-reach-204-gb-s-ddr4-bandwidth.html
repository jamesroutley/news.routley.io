<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://unum.cloud/post/2022-01-29-ddr4/">Original</a>
    <h1>Failing to Reach 204 GB/S DDR4 Bandwidth</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p>A bit of history.
Not so long ago, we tried to use GPU acceleration from Python.
We <a href="https://unum.cloud/post/2022-01-26-cupy/">benchmarked NumPy vs CuPy</a> in the most common number-crunching tasks.
We took the highest-end desktop CPU and the highest-end desktop GPU and put them to the test.
The GPU, expectedly, won, but not just in Matrix Multiplications.</p>
<p>Sorting arrays, finding medians, and even simple accumulation was vastly faster.
So we implemented multiple algorithms for <a href="https://unum.cloud/post/2022-01-28-reduce/">parallel reductions in C++ and CUDA</a>, just to compare efficiency.
CUDA was obviously harder, than using <code>std::accumulate</code>, but there is a shortcut: <code>thrust::reduce</code>.</p>
<ul>
<li>Frm a usage perspective, they are almost equally simple.</li>
<li>From a performance perspective, the latter was 10x faster on 1 GB arrays.</li>
</ul>
<p>We only reached <strong>89 GB/s</strong> throughput on the CPU, while the <a href="https://www.amd.com/system/files/documents/tr-pro-thought-leadership.pdf">technical docs</a> suggest a number 129% higher:</p>
<blockquote>
<p>One ThreadripperTM PRO integrates up to eight chiplets, each with access to memory, I/O and each other via the established hyper-speed AMD InfinityTM Fabric interconnect. ThreadripperTM PRO bar-raising core counts would count for naught were they supported by insufficient memory, with respect to not just bandwidth, but capacity and latency as well. AMD ensured ThreadripperTM PRO memory subsystem would be up to the task, as the 3900WX processor family is backed up with the most on-chip cache and highest performing memory available in a single x86 CPU socket: <strong>eight 3200 MHz DDR4 memory channels with ECC, supporting up to 2 TB capacity, and delivering up to 204 GB/s of aggregate bandwidth</strong>, more than double that of Intel Xeon W-2200 family.</p>
</blockquote>
<p>I was sleepless, so I tried more things.</p>
<h2 id="initial-version">Initial Version</h2>
<p>Our initial version contained some <code>AVX2</code> code for 8-way accumulation into one <code>YMM</code> register plus 64x <code>std::threads</code>:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="cpp"><span>inline</span> <span>float</span> <span>accumulate</span><span>(</span><span>float</span> <span>const</span> <span>*</span> <span>it</span><span>,</span> <span>float</span> <span>const</span> <span>*</span> <span>const</span> <span>end</span><span>)</span> <span>noexcept</span> <span>{</span>
    <span>// SIMD-parallel summation stage
</span><span></span>    <span>auto</span> <span>sums</span> <span>=</span> <span>_mm256_set1_ps</span><span>(</span><span>0</span><span>);</span>
    <span>for</span> <span>(;</span> <span>it</span> <span>+</span> <span>8</span> <span>&lt;</span> <span>end</span><span>;</span> <span>it</span> <span>+=</span> <span>8</span><span>)</span>
        <span>sums</span> <span>=</span> <span>_mm256_add_ps</span><span>(</span><span>_mm256_loadu_ps</span><span>(</span><span>it</span><span>),</span> <span>sums</span><span>);</span>

    <span>// Horizontal reduction into scalar
</span><span></span>    <span>sums</span> <span>=</span> <span>_mm256_add_ps</span><span>(</span><span>sums</span><span>,</span> <span>_mm256_permute2f128_ps</span><span>(</span><span>sums</span><span>,</span> <span>sums</span><span>,</span> <span>1</span><span>));</span>
    <span>sums</span> <span>=</span> <span>_mm256_hadd_ps</span><span>(</span><span>sums</span><span>,</span> <span>sums</span><span>);</span>
    <span>sums</span> <span>=</span> <span>_mm256_hadd_ps</span><span>(</span><span>sums</span><span>,</span> <span>sums</span><span>);</span>
    <span>auto</span> <span>sum</span> <span>=</span> <span>_mm256_cvtss_f32</span><span>(</span><span>sums</span><span>);</span>

    <span>// Serial summation of the remainder
</span><span></span>    <span>for</span> <span>(;</span> <span>it</span> <span>!=</span> <span>end</span><span>;</span> <span>++</span><span>it</span><span>)</span>
        <span>sum</span> <span>+=</span> <span>*</span><span>it</span><span>;</span>
    <span>return</span> <span>sum</span><span>;</span>
<span>}</span>
</code></pre></td></tr></tbody></table>
</div>
</div><h2 id="more-attempts">More Attempts</h2>
<p>I started by enabling CPU frequency scaling and reducing the benchmark time to 10 seconds.
CPU went up from 2.7GHz to 4 GHz on all 64 cores.</p>
<h3 id="logic">Logic</h3>
<ol>
<li>Created an always-waiting thread-pool to avoid restarting threads.</li>
<li>Replaced <code>for</code> with <code>if</code> + <code>[[likely]]</code> + <code>goto</code> combination.</li>
<li>Precomputed the last pointer of <code>AVX</code>, instead of <code>while (x + 8 &lt; end)</code>.</li>
<li>Switched from <code>AVX</code> to lighter <code>SSE</code>.</li>
<li>Switched from <code>double</code> to <code>float</code>.</li>
<li>Switched from 8-way accumulation (1x <code>YMM</code> register) to 32-way (4x <code>YMM</code> registers), effectively 4x unrolling the loop.</li>
<li>Freed 4 threads.</li>
</ol>
<p>Every idea above - failed, except for one - the last one.
It’s the difference between <strong>89 GB/s</strong> and <strong>112 GB/s - 122 GB/s</strong> depending on the run.
Important to note that using even fewer cores (like 32 or 48) degraded the performance.</p>
<h3 id="memory">Memory</h3>
<p>Once done with logic, I tried a couple of tricks for the memory subsystem.</p>
<ol>
<li>Using <code>_mm_prefetch</code> before the next round of evaluation.</li>
<li>Switching from <code>_mm256_loadu_ps</code> to <code>_mm256_castsi256_ps(_mm256_lddqu_si256(ptr))</code></li>
<li>Switching from <code>_mm256_loadu_ps</code> to <code>_mm256_load_ps</code>.</li>
<li>Switching from <code>_mm256_loadu_ps</code> to <code>_mm256_stream_load_si256</code> .</li>
</ol>
<p>For variants #2, #3 and #4 I switched to <code>aligned_alloc</code>.
It was just easier, than <code>std::accumulate</code>-ing the inputs until first aligned address.
Nothing helped here, but in general I would use those instruction for something like:</p>
<ol>
<li>Use <code>_mm_prefetch</code> for a fast binary search.</li>
<li>Use <code>_mm256_lddqu_si256</code> in mid-length text search with frequent split loads.</li>
<li>Use <code>_mm256_load_ps</code> for advanced computing on big aligned buffers.</li>
<li>Use <code>_mm256_stream_load_si256</code> for lite scans, to avoid cache pollution.</li>
</ol>
<h3 id="bigger-datasets">Bigger Datasets</h3>
<p>We had 24 GB of VRAM on the GPU and used 1÷24th of it for the input.
I tried increasing the CPU task size to 1TB÷24 = 42 GB, no benefit.
Each of our RAM sticks has the capacity of 128 GB, so the 42x bigger dataset still fits onto one stick.
So I repeated the CPU experiments on a 512 GB dataset.
No bandwidth improvement, but a significant loss…</p>
<h2 id="performance-analysis">Performance Analysis</h2>
<p>This is a weekend activity between two hectic weeks, so I will not disassemble the binaries.
But we will look into the runtime metrics.
I have <a href="https://man7.org/linux/man-pages/man1/taskset.1.html">isolated</a> 120 threads for us and run <code>perf</code>:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span>1
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="sh">sudo perf stat taskset 0xEFFFEFFFEFFFEFFFEFFFEFFFEFFFEFFF ./release/reduce_bench
</code></pre></td></tr></tbody></table>
</div>
</div><p>Here are the results:</p>
<div><div>
<table><tbody><tr><td>
<pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span></code></pre></td>
<td>
<pre tabindex="0"><code data-lang="txt"> Performance counter stats for &#39;taskset 0xEFFFEFFFEFFFEFFFEFFFEFFFEFFFEFFF ./release/reduce_bench&#39;:

     10,815,127.99 msec task-clock                #   37.052 CPUs utilized          
         4,720,303      context-switches          #    0.436 K/sec                  
           173,096      cpu-migrations            #    0.016 K/sec                  
        20,779,144      page-faults               #    0.002 M/sec                  
44,201,417,143,908      cycles                    #    4.087 GHz                      (83.33%)
 2,914,615,728,426      stalled-cycles-frontend   #    6.59% frontend cycles idle     (83.33%)
38,224,888,776,045      stalled-cycles-backend    #   86.48% backend cycles idle      (83.34%)
 8,115,337,000,378      instructions              #    0.18  insn per cycle         
                                                  #    4.71  stalled cycles per insn  (83.33%)
 1,820,092,697,347      branches                  #  168.291 M/sec                    (83.33%)
     2,298,601,636      branch-misses             #    0.13% of all branches          (83.34%)

     291.892493579 seconds time elapsed

   10561.230765000 seconds user
     206.870335000 seconds sys
</code></pre></td></tr></tbody></table>
</div>
</div><p>Only 0.18 instructions per cycle, on average.
A good result for scalar code would be above 2.
Cache misses were only at 2%.
The biggest bottleneck is the backend.
<strong>86.48% backend cycles were idle</strong>, waiting for memory to be fetched.
The same can be seen on <code>htop</code> charts.
Those are not charts of a healthy <del>person</del> program.
Our threads are only half-busy.</p>
<p><img src="https://unum.cloud/assets/post/2022-01-29-ddr4/htop.png" alt="htop"/></p>
<p>The list of available hardware events, retrieved via <code>perfmon2</code>, was over 2&#39;000 lines.
Deeper introspection seemed a bit too troublesome.
Especially as AMD has far less documentation than Intel.
Maybe another time.</p>
<h2 id="impact-on-industry">Impact on Industry</h2>
<p>We presume that GPU cores are hard to organize, which is true.
But so is squeezing 100% performance from CPUs.
We got to <strong>60% of CPU ⇔ RAM</strong> memory bandwidth with all the tuning and tweaking.
While even the shortest and simplest GPU solutions got to <strong>79% of GPU ⇔ VRAM</strong> bandwidth.
The best GPU solution reached <strong>94% of theoretical throughput</strong>!
More visually:</p>
<table>
<thead>
<tr>
<th>Attempt</th>
<th>Bandwidth</th>
<th>Max Bandwidth</th>
<th>Saturation</th>
<th>Time to Code</th>
</tr>
</thead>
<tbody>
<tr>
<td>Parallel STL</td>
<td>87 GB/s</td>
<td>204 GB/s</td>
<td>42.6%</td>
<td>1m</td>
</tr>
<tr>
<td>Best CPU run</td>
<td><strong>122 GB/s</strong></td>
<td>204 GB/s</td>
<td>59.8%</td>
<td><strong>60m</strong></td>
</tr>
<tr>
<td>Thrust</td>
<td>743 GB/s</td>
<td>936 GB/s</td>
<td>79.4%</td>
<td>1m</td>
</tr>
<tr>
<td>Custom CUDA</td>
<td>817 GB/s</td>
<td>936 GB/s</td>
<td>87.3%</td>
<td>30m</td>
</tr>
<tr>
<td>CUB</td>
<td><strong>879 GB/s</strong></td>
<td>936 GB/s</td>
<td>93.9%</td>
<td><strong>5m</strong></td>
</tr>
</tbody>
</table>
<p>The “Time to Code” part is highly subjective and is a personal approximation.
Still, our best CPU solution is not just slower than GPU code, but also much further from it’s theoretical limit.
Counter-intuitive, right?</p>
<hr/>
<p>Everyone knows the PCI Gen4 bandwidth.
Everyone knows that rare linear complexity operations aren’t worth transferring to GPUs.
But this article isn’t about that.</p>
<p>Moreover, GPU code is becoming composable, just like the CPU code.
You don’t even need a custom kernel-fusing compiler, like <a href="https://www.tensorflow.org/xla">XLA</a> or <a href="https://tvm.apache.org">TVM</a>.
Old school <a href="https://www.modernescpp.com/index.php/expression-templates">expression templates</a> are more than enough.
Thrust gives you enough <a href="https://thrust.github.io/doc/group__fancyiterator.html">“fancy” iterators</a> for most cases, and you can easily add your own.
The higher the composability, the reasons to port even linear complexity code.</p>
<hr/>
<p>It’s not the first time GPU code ends up being more accessible than the CPU version.
Last year I was trying to surpass <a href="https://docs.nvidia.com/cuda/cublas/index.html">cuBLAS</a> and <a href="https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html">oneMKL</a> in <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3">GEMM</a>.
By now, you might guess how that story developed, but I will leave it for another time.</p>
<blockquote>
    <ul>
        
        <li><a href="https://twitter.com/intent/tweet?text=Failing+to+Reach+DDR4+Bandwidth+of+AMD+Zen2+in+Parallel+Reductions+%40unum_cloud+%40ashvardanian+%23cpp+%23amd+%23simd+https%3A%2F%2Funum.cloud%2Fpost%2F2022-01-29-ddr4%2F"><strong>Tweet</strong></a>
            <svg width="20" height="12" viewBox="0 0 22 18" fill="none" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" role="none">
                <path fill-rule="evenodd" clip-rule="evenodd" d="M7.04128 17.7861C4.53883 17.7861 2.21078 17.0602 0.25 15.8165C0.596807 15.857 0.949401 15.8773 1.30683 15.8773C3.38266 15.8773 5.29282 15.1764 6.80944 14.0003C4.87 13.9646 3.23461 12.6968 2.67056 10.9547C2.94041 11.0059 3.21803 11.0338 3.50342 11.0338C3.90767 11.0338 4.2993 10.9798 4.67133 10.8796C2.64431 10.4775 1.11689 8.70468 1.11689 6.5808C1.11689 6.56156 1.11689 6.54327 1.11792 6.52489C1.71505 6.85368 2.39787 7.05133 3.12448 7.07347C1.93514 6.28783 1.15299 4.94488 1.15299 3.42361C1.15299 2.62053 1.37213 1.86754 1.75297 1.21971C3.93781 3.87277 7.20298 5.61776 10.885 5.80097C10.8091 5.47987 10.7701 5.14535 10.7701 4.80118C10.7701 2.38039 12.7543 0.416626 15.2012 0.416626C16.4753 0.416626 17.6267 0.949734 18.4351 1.80197C19.4444 1.60535 20.392 1.23997 21.2484 0.737722C20.9172 1.76154 20.2148 2.62053 19.3002 3.1633C20.1963 3.0572 21.0506 2.82194 21.8444 2.47297C21.2512 3.35223 20.4993 4.12445 19.6342 4.7433C19.643 4.93129 19.6469 5.12031 19.6469 5.31018C19.6469 11.1042 15.1905 17.7861 7.04128 17.7861Z" fill="#1DA1F2"></path>
            </svg></li>
        <li><a href="https://bit.ly/3mG4q0f"><strong>Subscribe</strong></a> to receive similar articles 📨</li>
        <li><a href="mailto:info@unum.cloud"><strong>Reach Us</strong></a> to try the fastest data processing software
            🔥</li>
    </ul>
</blockquote></div></div>
  </body>
</html>
