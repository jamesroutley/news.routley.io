<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jpcamara.com/2024/07/15/ruby-methods-are.html">Original</a>
    <h1>Ruby methods are colorless</h1>
    
    <div id="readability-page-1" class="page"><div>

<article>
  <header>
    
    <p>
      <a href="https://jpcamara.com/2024/07/15/ruby-methods-are.html"><time>Jul 16, 2024</time></a>
      
      <span>JP Camara</span>
      
    </p>
    
    
  </header>
  <section><p><img src="https://cdn.uploads.micro.blog/98548/2024/aa82bee7-c210-489e-9b5c-b1fa798f2d8d.jpeg" alt=""/></p>
<blockquote>
<p>👋🏼 This is part of series on concurrency, parallelism and asynchronous programming in Ruby. It’s a deep dive, so it’s divided into 10 main parts:</p>
<ul>
<li><a href="https://jpcamara.com/2024/06/04/your-ruby-programs.html">Your Ruby programs are always multi-threaded: Part 1</a></li>
<li><a href="https://jpcamara.com/2024/06/23/your-ruby-programs.html">Your Ruby programs are always multi-threaded: Part 2</a>
<ul>
<li><a href="https://jpcamara.com/2024/06/27/consistent-requestlocal-state.html">Consistent, request-local state</a></li>
</ul>
</li>
<li>Ruby methods are colorless</li>
<li>Concurrent, colorless Ruby: Part 1, Threads</li>
<li>Concurrent, colorless Ruby: Part 2, Fiber and its MaNy friends</li>
<li>Parallel Ruby: Processes, Ractors and alternative runtimes</li>
<li>Streaming Ruby: Scaling concurrency</li>
<li>Abstracted, concurrent Ruby</li>
<li>Closing thoughts, kicking the tires and tangents</li>
<li>How I dive into CRuby concurrency</li>
</ul>
<p>You’re reading “Ruby methods are colorless”. I’ll update the links as each part is released, and include these links in each post.</p>
</blockquote>
<ul>
<li><a href="#some-context">First, some context</a>
<ul>
<li><a href="#function-colors">Function colors</a></li>
<li><a href="#paved-with-callbacks">The road to hell is paved with callbacks</a></li>
<li><a href="#invasive-async-await">Invasive async/await</a></li>
</ul>
</li>
<li><a href="#what-makes-ruby-colorless">What makes Ruby colorless?</a>
<ul>
<li><a href="#nested-concurrency">A nested concurrency model 🪆</a></li>
<li><a href="#threads-and-fibers">Colorless threads 🧵 and Fibers 🌾 </a></li>
<li><a href="#to-be-concurrent">What does it mean to be concurrent?</a></li>
<li><a href="#colorless-calls">Colorless calls</a></li>
<li><a href="#async-aside">A quick aside on Async</a></li>
<li><a href="#digging-deep">Digging deep</a></li>
<li><a href="#ruby-history-corner">PS: A historical sidenote - Ruby had its own callback phase</a></li>
</ul>
</li>
</ul>
<blockquote>
<p>📝 <strong>A quick note on Ruby runtimes</strong></p>
<p>I’m approaching this series from the perspective of CRuby. This is the original and most popular/common Ruby runtime. I’m also focusing primarily on Ruby 3.2/3.3+. This matters because it informs how things like Threads and Fibers behave.</p>
</blockquote>
<h2 id="some-context">First, some context</h2>
<p>Last year I <a href="https://twitter.com/ThePrimeagen/status/1689289653680033792">read a tweet</a> from <a href="https://twitter.com/ThePrimeagen">@ThePrimagen</a> about his experience with <a href="https://go.dev/">Go</a><sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. In it he mentioned some strengths and weaknesses (cheekily), and then he mentioned a phrase I’d never heard before: “colorless functions”.</p>
<img src="https://cdn.uploads.micro.blog/98548/2024/03867f06ca.jpeg" width="70%" height="70%" alt=""/>
<blockquote>
<p>real talk, colorless functions [are] amazing and I think that most people don’t realize how great it really is</p>
<p>– @ThePrimeagen</p>
</blockquote>
<h3 id="function-colors">Function colors</h3>
<p>Curious to understand what a “colorless function” is, a quick search returned this article as the first result: <a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">What color is your function?</a><sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p>
<p>In it, <a href="https://twitter.com/munificentbob">Bob Nystrom</a> goes through a fictional language (spoiler: it’s Javascript/node.js) that breaks functions down into one of two “colors”:</p>
<blockquote>
<ol>
<li>Every function has a color</li>
<li>The way you call a function depends on its color</li>
<li>You can only call a red function from within a red function</li>
<li>Red functions are more painful to call</li>
<li>Some core library functions are red</li>
</ol>
<p>It’s an allegory… red functions are asynchronous ones</p>
<p>– Nystrom</p>
</blockquote>
<p>Red and blue functions equate to asynchronous and synchronous functions. And having to indicate the color of your function infects the rest of your code and the API decisions you make.</p>
<h3 id="paved-with-callbacks">The road to hell is paved with callbacks</h3>
<p>Nystrom’s article was written in 2015 while Javascript/node was still in its <a href="http://callbackhell.com/">callback hell</a><sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> phase. Any operation that could block, like reading a file, making an HTTP call, or querying a database had to be done using a callback. Callbacks proliferated throughout your code - so asynchronous, red functions were plentiful and painful to interact with.</p>
<p>In JavaScript, everything happens in a single-ish threaded context called the “event loop”. You can’t block the loop - so you would set a callback to know when a blocking call completed:</p>
<div><pre tabindex="0"><code data-lang="js"><span>readFile</span>(<span>path</span>, (<span>content</span>, <span>err</span>) =&gt; {
  <span>if</span> (<span>err</span>) { <span>throw</span> <span>&#39;&#39;</span>; }
  <span>saveFile</span>(<span>content</span>, (<span>file</span>, <span>err</span>) =&gt; {
    <span>// and on and on...
</span><span></span>  });
});
</code></pre></div><p>After callbacks, he alludes to what would be the subsequent <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Using_promises">Promises</a> phase of Javascript/node.js with his section “<em>I promise the future is better</em>”. An ergonomic improvement to callbacks but still cumbersome, and does not solve most red/blue pain points:</p>
<div><pre tabindex="0"><code data-lang="js"><span>readFile</span>(<span>path</span>)
  .<span>then</span>((<span>content</span>) =&gt; {
    <span>saveFile</span>(<span>content</span>)
      .<span>then</span>((<span>file</span>) =&gt; {})
      .<span>catch</span>(<span>err</span>) =&gt; {})
  })
  .<span>catch</span>((<span>err</span>) =&gt; <span>console</span>.<span>error</span>(<span>err</span>));
</code></pre></div><p>Finally he describes <a href="https://javascript.info/async-await">the <code>async/await</code> phase</a> in “<em>I’m awaiting a solution</em>”, which leads us to modern Javascript:</p>
<div><pre tabindex="0"><code data-lang="js"><span>try</span> {
  <span>const</span> <span>content</span> <span>=</span> <span>await</span> <span>readFile</span>(<span>path</span>);
} <span>catch</span> (<span>err</span>) {
  <span>console</span>.<span>error</span>(<span>err</span>);
}
	
<span>try</span> {
  <span>const</span> <span>file</span> <span>=</span> <span>await</span> <span>saveFile</span>(<span>content</span>);
} <span>catch</span> (<span>err</span>) {
  <span>console</span>.<span>error</span>(<span>err</span>);
}
</code></pre></div><h3 id="invasive-async-await">Invasive async/await</h3>
<p>I’ll edit “<em>I’m awaiting a solution</em>” for modern JS, where he notes:</p>
<blockquote>
<ol>
<li>Synchronous functions return values, async ones return <code>Promise&lt;T&gt;</code> <em>(Promise containing type T)</em> wrappers around the value.</li>
<li>Sync functions are just called, async ones need an <code>await</code>.</li>
<li>If you call an async function you’ve got this wrapper object when you actually want the <code>T</code>. You can’t unwrap it unless you make <em>your</em> function async and await it.”</li>
</ol>
<p>– Nystrom</p>
</blockquote>
<p>#3 is the particularly infectious bit. Async code bubbles all the way to the top. If you want to use <code>await</code>, then you have to mark your function as <code>async</code>. Then if someone else calling your function wants to use <code>await</code>, they <em>also</em> have to mark themselves as <code>async</code>, on and on until the root of the call chain. If at any point you don’t then you have to use the <code>async</code> result (in JavaScript’s case a <code>Promise&lt;T&gt;</code>).</p>
<div><pre tabindex="0"><code data-lang="js"><span>async</span> <span>function</span> <span>readFile</span>()<span>:</span> Promise<span>&lt;</span><span>string</span><span>&gt;</span> {
  <span>return</span> <span>await</span> <span>read</span>();
}
	
<span>async</span> <span>function</span> <span>iCallReadFile</span>()<span>:</span> Promise<span>&lt;</span><span>string</span><span>&gt;</span> {
  <span>return</span> <span>await</span> <span>readFile</span>();
}
	
<span>async</span> <span>function</span> <span>iCallICallReadFile</span>()<span>:</span> Promise<span>&lt;</span><span>string</span><span>&gt;</span> {
  <span>return</span> <span>await</span> <span>iCallReadFile</span>();
}
	
<span>function</span> <span>iGiveUp</span>(<span>callback</span>) {
  <span>iCallICallReadFile</span>()
    .<span>then</span>(<span>callback</span>)
    .<span>catch</span>((<span>err</span>) =&gt; {})
}
</code></pre></div><p>Once your language is broken down into async and synchronous functions, you can’t escape it.</p>
<p>Even more onerous, if it isn’t built into your language core like JavaScript/node.js, adding it later means modifying your entire runtime, libraries and codebases to understand it. This was (still can be?) a painful transition for languages like Python and Rust.</p>
<div><pre tabindex="0"><code data-lang="rust"><span>// et tu, Rust?
</span><span></span><span>async</span> <span>fn</span> <span>main</span>() {
  <span>let</span> content <span>=</span> read_file().<span>await</span>;
}
</code></pre></div><h2 id="what-makes-ruby-colorless">What makes Ruby colorless?</h2>
<p>So what does a “blue” (synchronous) method look like in Ruby?</p>
<p>And what does that same “red” (asynchronous) call look like?</p>
<p>Wait. Is that right? Surely there must be more nuance?</p>
<p>HTTP?</p>
<div><pre tabindex="0"><code data-lang="rb">response <span>=</span> <span>Net</span><span>::</span><span>HTTP</span><span>.</span>get(
  <span>URI</span>(<span>&#34;https://example.com&#34;</span>)
)
</code></pre></div><p>Shell calls!</p>
<p>Process spawning??</p>
<div><pre tabindex="0"><code data-lang="rb">pid <span>=</span> fork
  <span># good stuff</span>
<span>end</span>
<span>Process</span><span>::</span><span>Status</span><span>.</span>wait pid
</code></pre></div><p>Mutex locks?!?</p>
<p>DNS resolution!??!</p>
<div><pre tabindex="0"><code data-lang="rb">ip <span>=</span> <span>Resolv</span><span>.</span>getaddress <span>&#34;ruby-lang.org&#34;</span>
</code></pre></div><p>Sleep!?!!!</p>
<p>😮‍💨😮‍💨😮‍💨</p>
<p>Well that’s nice! There is no difference between asynchronous and synchronous methods, so no color. We get asynchronous behavior for free. Blog post over!?</p>
<p>Well… not <em>exactly</em>. We still don’t know <em>how</em> we get that behavior.</p>
<p>In the section “<em>What language isn’t colored?</em>” Nystrom specifically mentions Ruby being one of a few colorless languages<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>:</p>
<blockquote>
<p>…languages that don’t have this problem: [Java,] Go, Lua, and Ruby.</p>
<p>Any guess what they have in common?</p>
<p><em>Threads</em>. Or, more precisely: multiple independent callstacks that can be switched between. It isn’t strictly necessary for them to be operating system threads. Goroutines in Go, coroutines in Lua, and fibers in Ruby are perfectly adequate.</p>
<p>– Nystrom</p>
</blockquote>
<p>So Ruby is colorless, and to be colorless you need independent call stacks that you can switch between. And apparently Threads and Fibers can give you that.</p>
<p>What does that mean and what does that enable? And since Nystrom’s article was written in 2015 - is there anything new to Ruby since then?</p>
<h3 id="nested-concurrency">A nested concurrency model 🪆</h3>
<p>In <a href="https://jpcamara.com/2024/06/23/your-ruby-programs.html">Your Ruby programs are always multi-threaded: Part 2</a>, we briefly discussed that Ruby has a layered concurrency model and that the best way to describe it is as a nesting doll - as you pull away the outer layers, you find new layers of concurrency inside.</p>
<img src="https://cdn.uploads.micro.blog/98548/2024/a44a601478.jpeg" width="70%" height="70%" alt=""/>
<blockquote>
<p>Here’s my key for mapping icons to concurrency</p>
<ul>
<li>💎 represents a Ruby process, which is essentially another instance of Ruby itself</li>
<li>🦖 represents a Ruby Ractor. Ractor stands for Ruby Actor, as in the Actor model. But I always think of the word “Raptor” instead, so I represent it with a Dinosaur 🤷🏻‍♂️</li>
<li>🧵 represents a Ruby Thread. It’s a thread icon, for obvious reasons</li>
<li>🌾 represents a Ruby Fiber. I think of wheat when I think of Fibers, hence 🌾</li>
</ul>
</blockquote>
<p>As a refresher, in your own Ruby code, you can easily inspect each layer directly:</p>
<div><pre tabindex="0"><code data-lang="rb">puts <span>&#34;Process </span><span>#{</span><span>Process</span><span>.</span>pid<span>}</span><span>&#34;</span>
puts <span>&#34;  Ractor </span><span>#{</span><span>Ractor</span><span>.</span>current<span>}</span><span>&#34;</span>
puts <span>&#34;    Thread </span><span>#{</span><span>Thread</span><span>.</span>current<span>}</span><span>&#34;</span>
puts <span>&#34;      Fiber </span><span>#{</span><span>Fiber</span><span>.</span>current<span>}</span><span>&#34;</span>
	
<span># Process 123</span>
<span>#   Ractor #&lt;Ractor:#1 running&gt;</span>
<span>#     Thread #&lt;Thread:0x0... run&gt;</span>
<span>#       Fiber #&lt;Fiber:0x0... (resumed)&gt;</span>
</code></pre></div><p>Your code is always operating within a specific instance of each layer. Mostly you can access those instances with a call to <code>current</code>.</p>
<p>Each layer has unique characteristics when it comes to running concurrent code. Some can operate in parallel, executing independent Ruby code at the exact same time. Others run your Ruby code concurrently, moving back and forth between them but never running simultaneously.</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/1f6316498b.jpeg" alt=""/></p>
<blockquote>
<p><em>source</em>: jpcamara.com</p>
</blockquote>
<p>The outermost layer is a <strong>Process 💎</strong>. Every program on every common operating system (Linux, Windows, macOS) runs inside a process - it’s the top-level unit of execution in the OS. You can have as many processes as your OS will allow - but the number of CPU cores determine how many processes can run in parallel. Multiple Ruby processes can run in parallel but they live in isolated memory spaces - they have no simple way of communicating with each other.</p>
<div><pre tabindex="0"><code data-lang="rb">pid <span>=</span> fork <span>do</span>
  <span>#...</span>
<span>end</span>
<span>Process</span><span>::</span><span>Status</span><span>.</span>wait pid
</code></pre></div><p>Internally, a Ruby process is made up of <strong>Ractors 🦖</strong>. Ractors don’t have a direct parallel in the OS - they’re a unique Ruby structure which wrap native OS threads<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>. Multiple Ractors can run Ruby code in parallel, and they have strict sharing rules to keep them memory safe - so you can only communicate between them by passing messages. Those messages can copy data, or they can move objects completely between Ractors and change ownership. Ractors <em>sound</em> pretty nice, but are still experimental. Regardless, every Ruby process starts out with a single Ractor that your Ruby code runs inside of.</p>
<div><pre tabindex="0"><code data-lang="rb">ractor <span>=</span> <span>Ractor</span><span>.</span>new <span>do</span>
 <span>#...</span>
<span>end</span>
ractor<span>.</span>take
</code></pre></div><p>Inside of Ractors, you have <strong>Threads 🧵</strong>. Threads are created 1:1 with threads on your operating system<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup>. These threads share the same memory space with each other, and every thread created within a process shares the same memory space as the process, and spends its lifetime there. Because threads share the same memory space they have to be carefully coordinated to safely manage state. Ruby threads cannot run CPU-bound Ruby code in parallel, but they can parallelize for blocking operations. Threads run <em>preemptively</em>, which means they can swap out of your code at any time, and move to other Ruby code. Every Ruby Ractor starts out with a single thread that your Ruby code runs inside of.</p>
<div><pre tabindex="0"><code data-lang="rb">thread <span>=</span> <span>Thread</span><span>.</span>new <span>do</span>
  <span>#...</span>
<span>end</span>
thread<span>.</span>join
</code></pre></div><p>Inside of threads, you have <strong>Fibers 🌾</strong>. Fibers are another structure with no direct OS equivalent (though they have parallels in other languages). Fibers are a tool intended for lightweight, cooperative concurrency. Like Ruby threads, Fibers share a memory space, but their coordination is more deterministic. Also like Ruby threads, Fibers cannot run CPU-bound Ruby code in parallel, but using a FiberScheduler can parallelize for blocking operations. Every Ruby thread starts out with a single fiber that your Ruby code runs inside of.</p>
<div><pre tabindex="0"><code data-lang="rb">fiber <span>=</span> <span>Fiber</span><span>.</span>new <span>do</span>
  <span>#...</span>
<span>end</span>
fiber<span>.</span>resume <span># transfer / Fiber.schedule</span>
</code></pre></div><p>When a Ruby program starts, all four layers start with it. The <strong>Process 💎</strong> starts, it creates a <strong>Ractor 🦖</strong>, which creates a <strong>Thread 🧵</strong>, which creates a <strong>Fiber 🌾</strong>. Ultimately, your Ruby code is in the context of all 4, but most specifically the fiber. Each layer is always active in a Ruby program whether you use them directly or not<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup>. We can utilize each in unique ways as well.</p>
<p>That’s a lot! In subsequent posts you’ll learn why each layer exists and the values each provides. Every layer will get some discussion - but as it relates to asynchronous, colorless programming, threads and fibers are the most relevant.</p>
<h3 id="threads-and-fibers">Colorless threads 🧵 and fibers 🌾</h3>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/group-83-1.jpg" alt=""/></p>
<blockquote>
<p><em>source</em>: jpcamara.com</p>
</blockquote>
<p><a href="https://rubyapi.org/3.3/o/thread">Threads</a> 🧵 are one of the oldest and most common units of concurrency in Ruby. They’re the OG for splitting up concurrent chunks of work. They’ve been in Ruby since Ruby 1.1<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup> and have enabled colorless calls for many years.</p>
<p><a href="https://rubyapi.org/3.3/o/fiber">Fibers</a> 🌾 aren’t too young themselves, but are about 10 years younger than their older sibling Threads. They were released in Ruby 1.9, but remained a bit of an esoteric option for a long time. They offered a form of concurrency, but for most use-cases it didn’t really help anyone - unlike Threads, they didn’t enable colorless programming at first.</p>
<p>But ever since Ruby 3, Fibers have been given superpowers in the form of the FiberScheduler. By using a FiberScheduler, Fibers can now provide seamless, colorless programming.</p>
<p>So both threads and fibers offer colorless, concurrent programming. We’ve got a definition for colorless, but what about concurrency?</p>
<h3 id="to-be-concurrent">What does it mean to be concurrent?</h3>
<p>The most popular source for a definition of concurrency is from the creator of Go, Rob Pike, in his talk <a href="https://youtu.be/oV9rvDllKEg?si=L3aitDpilMRk5Bfn">concurrency is not parallelism</a>. Another member of the Go team summarized it:</p>
<blockquote>
<p>When people hear the word concurrency they often think of parallelism, a related but quite distinct concept. In programming, <strong>concurrency is the composition of independently executing processes</strong>, while parallelism is the simultaneous execution of (possibly related) computations. <strong>Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.</strong></p>
<p><strong>source</strong>: <a href="https://go.dev/blog/waza-talk">https://go.dev/blog/waza-talk</a></p>
</blockquote>
<p>They describe it as a way of composing tasks. It allows you to spread out and coordinate work. How that work is ultimately executed is not relevant to whether the code is concurrent or not.</p>
<p>Look at the activity monitor for your OS right now. If you were to count the number of processes and threads, it would vastly exceed the number of available cores. And yet everything still chugs along smoothly. That’s concurrency in action.</p>
<img src="https://cdn.uploads.micro.blog/98548/2024/activity-monitor-final.png" width="60%" height="60%" alt=""/>
<blockquote>
<p>There are over 200 threads associated with just these 5 apps. I don’t have 200 CPU cores, so only a handful of them could ever be active in parallel. But all of them <em>could</em> operate <em>concurrently</em>.</p>
</blockquote>
<p>Having said all that - we do <em>want</em> our work to happen as parallel as possible in most cases. We want to maximize our resources and parallelize as much as we can.</p>
<p>In Rob Pike’s concurrency talk, he used the Go mascot, the gopher, to demonstrate how his task (incinerating obsolete language manuals 😅) was composed, and how it could be run in parallel, or with one gopher at a time, but it was still concurrent:</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/draggedimage.png" alt=""/></p>
<blockquote>
<p><strong>source</strong>: <a href="https://go.dev/talks/2012/waza.slide">https://go.dev/talks/2012/waza.slide</a></p>
</blockquote>
<p>Go’s gophers are kind of fun. Let’s use some concurrency mascots for Ruby to show our own example.</p>
<h3 id="process">Process!</h3>
<img src="https://cdn.uploads.micro.blog/98548/2024/process.png" width="35%" height="35%" alt=""/>
<h3 id="ractor">Ractor!</h3>
<img src="https://cdn.uploads.micro.blog/98548/2024/ractor.png" width="35%" height="35%" alt=""/>
<h3 id="thread">Thread!</h3>
<img src="https://cdn.uploads.micro.blog/98548/2024/thread.png" width="35%" height="35%" alt=""/>
<h3 id="fiber">Fiber!</h3>
<img src="https://cdn.uploads.micro.blog/98548/2024/fiber.png" width="35%" height="35%" alt=""/>
<h3 id="cpu">CPU!</h3>
<img src="https://cdn.uploads.micro.blog/98548/2024/cpu.png" width="35%" height="35%" alt=""/>
<blockquote>
<p><em>source</em>: jpcamara.com</p>
</blockquote>
<p>As a baseline, Processes and Ractors can run in parallel for as many cores as are available:</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/colorless-mascots.drawio-2.png" width="488" height="283" alt=""/><img src="https://cdn.uploads.micro.blog/98548/2024/colorless-mascots.drawio.png" width="488" height="273" alt=""/></p>
<p>But Threads and Fibers, while concurrent, effectively only parallelize against one core at a time<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup>:</p>
<img src="https://cdn.uploads.micro.blog/98548/2024/colorless-mascots.drawio-1.png" width="488" height="365" alt=""/>
<p>What happens when we start scaling Processes and Ractors past the number of available cores?</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/colorless-mascots.drawio-3.png" width="600" height="252" alt=""/><img src="https://cdn.uploads.micro.blog/98548/2024/colorless-mascots.drawio-4.png" width="600" height="245" alt=""/></p>
<p>Once we exceed the ability to parallelize, we are concurrent in the same way as Threads and Fibers! This model allows our units of concurrency adapt to the environment - when cores are available they can run in parallel, and when cores are not available they can swap between each other, transparently to the program itself. Every program eventually becomes concurrent as it scales, at least for CPU-bound processing.</p>
<p>But this leaves Threads and Fibers looking pretty limited. They allow you to breakup your work into independent, interleaving tasks, operating concurrently on Ruby code. But what good is that since they <em>never</em> operate in parallel… or do they?</p>
<h3 id="colorless-calls">Colorless calls</h3>
<p>How does this all relate to colorless methods?</p>
<p>Let’s see with an example. In our example, we’re just traversing a few different websites we care about, and we want to retrieve them as efficiently as possible.</p>
<p>First we try it with threads. For demonstration purposes, we use the <code>httpbin.com/delay</code> endpoint to simulate delays in responses.</p>
<div><pre tabindex="0"><code data-lang="rb">require <span>&#34;net/http&#34;</span>
	
<span>def</span> <span>log_then_get</span>(url, context)
  puts <span>&#34;Requesting </span><span>#{</span>url<span>}</span><span>...&#34;</span>
  get(url, context)
<span>end</span>
	
<span>def</span> <span>get</span>(uri, context)
  response <span>=</span> <span>Net</span><span>::</span><span>HTTP</span><span>.</span>get(uri)
  puts caller(<span>0</span>)<span>.</span>join(<span>&#34;</span><span>\n</span><span>&#34;</span>)
  response
<span>end</span>
	
<span>def</span> <span>get_http_thread</span>(url)
  <span>Thread</span><span>.</span>new <span>do</span>
    log_then_get(<span>URI</span>(url), <span>Thread</span><span>.</span>current)
  <span>end</span>
<span>end</span>
	
<span>def</span> <span>get_http_via_threads</span>
  threads <span>=</span> <span>[]</span>
  threads <span>&lt;&lt;</span> get_http_thread(
    <span>&#34;https://httpbin.org/delay/3?ex=1&#34;</span>
  )
  threads <span>&lt;&lt;</span> get_http_thread(
    <span>&#34;https://httpbin.org/delay/3?ex=2&#34;</span>
  )
  threads <span>&lt;&lt;</span> get_http_thread(
    <span>&#34;https://httpbin.org/delay/3?ex=3&#34;</span>
  )
  threads <span>&lt;&lt;</span> get_http_thread(
    <span>&#34;https://httpbin.org/delay/3?ex=4&#34;</span>
  )
  threads<span>.</span>map(<span>&amp;</span><span>:value</span>)
<span>end</span>
	
now <span>=</span> <span>Time</span><span>.</span>now
get_http_via_threads
puts <span>&#34;Thread runtime: </span><span>#{</span><span>Time</span><span>.</span>now <span>-</span> now<span>}</span><span>&#34;</span>
</code></pre></div><p>This code is doing a few things:</p>
<ol>
<li>It’s split into multiple methods to create a callstack (effectively, a backtrace). This callstack allows us to demonstrate that we maintain the position and state of each method call, even when we switch between threads.</li>
<li>It creates four threads and appends them to an array. After those threads are initialized they go into a ready state, available to be run by the thread scheduler.</li>
<li>It calls <code>value</code>on each thread. This blocks the program until each thread finishes and returns the last value returned from the thread. Once we block the main thread with <code>value</code>, the other threads have an immediate opportunity to start running. In this case we block until each thread finishes making an HTTP call.</li>
</ol>
<div><pre tabindex="0"><code data-lang="rb"><span># &gt; bundle exec ruby main.rb</span>
<span># </span>
<span># Requesting https://httpbin.org/delay/3?ex=1...</span>
<span># Requesting https://httpbin.org/delay/3?ex=2...</span>
<span># Requesting https://httpbin.org/delay/3?ex=3...</span>
<span># Requesting https://httpbin.org/delay/3?ex=4...</span>
<span># main.rb:12:in `get&#39;</span>
<span># main.rb:7:in `log_then_get&#39;</span>
<span># main.rb:18:in `block in get_http_thread&#39;</span>
<span># main.rb:12:in `get&#39;</span>
<span># main.rb:7:in `log_then_get&#39;</span>
<span># main.rb:18:in `block in get_http_thread&#39;</span>
<span># main.rb:12:in `get&#39;</span>
<span># main.rb:7:in `log_then_get&#39;</span>
<span># main.rb:18:in `block in get_http_thread&#39;</span>
<span># main.rb:12:in `get&#39;</span>
<span># main.rb:7:in `log_then_get&#39;</span>
<span># main.rb:18:in `block in get_http_thread&#39;</span>
<span># Thread runtime: 3.340238554</span>
</code></pre></div><p>Second we try it with fibers:</p>
<div><pre tabindex="0"><code data-lang="rb">require <span>&#34;net/http&#34;</span>
require <span>&#34;async&#34;</span>
	
<span># Same #log_then_get</span>
<span># Same #get</span>
	
<span>def</span> <span>get_http_fiber</span>(url, responses)
  <span>Fiber</span><span>.</span>schedule <span>do</span>
    responses <span>&lt;&lt;</span> log_then_get(<span>URI</span>(url), <span>Fiber</span><span>.</span>current)
  <span>end</span>
<span>end</span>
	
<span>def</span> <span>get_http_via_fibers</span>
  <span>Fiber</span><span>.</span>set_scheduler(<span>Async</span><span>::</span><span>Scheduler</span><span>.</span>new)
  responses <span>=</span> <span>[]</span>
  responses <span>&lt;&lt;</span> get_http_fiber(
    <span>&#34;https://httpbin.org/delay/3?ex=1&#34;</span>, responses
  )
  responses <span>&lt;&lt;</span> get_http_fiber(
    <span>&#34;https://httpbin.org/delay/3?ex=2&#34;</span>, responses
  )
  responses <span>&lt;&lt;</span> get_http_fiber(
    <span>&#34;https://httpbin.org/delay/3?ex=3&#34;</span>, responses
  )
  responses <span>&lt;&lt;</span> get_http_fiber(
    <span>&#34;https://httpbin.org/delay/3?ex=4&#34;</span>, responses
  )
  responses
<span>ensure</span>
  <span>Fiber</span><span>.</span>set_scheduler(<span>nil</span>)
<span>end</span>
	
now <span>=</span> <span>Time</span><span>.</span>now
get_http_via_fibers
puts <span>&#34;Fiber runtime: </span><span>#{</span><span>Time</span><span>.</span>now <span>-</span> now<span>}</span><span>&#34;</span>
</code></pre></div><p>Same as before, this code is doing a few things:</p>
<ol>
<li>We are split into multiple methods to create a callstack. Same as with threads, this callstack allows us to demonstrate that we maintain the position and state of each method call, even when we switch between fibers.</li>
<li>It schedules four Fibers. We append to an array inside the <code>Fiber.schedule</code> to get the result. Unlike threads, a scheduled fiber will start running its block immediately.</li>
<li>The fibers will coordinate between themselves and the current fiber. When we unset the FiberScheduler using <code>set_scheduler(nil)</code> in our <code>ensure</code>, the <code>Async::Scheduler</code> makes sure all fibers have finished running before returning.</li>
</ol>
<p>This results in the following, similar backtrace:</p>
<div><pre tabindex="0"><code data-lang="rb"><span># &gt; bundle exec ruby main.rb</span>
<span># </span>
<span># Requesting https://httpbin.org/delay/3?ex=1...</span>
<span># Requesting https://httpbin.org/delay/3?ex=2...</span>
<span># Requesting https://httpbin.org/delay/3?ex=3...</span>
<span># Requesting https://httpbin.org/delay/3?ex=4...</span>
<span># main.rb:12:in `get&#39;</span>
<span># main.rb:7:in `log_then_get&#39;</span>
<span># main.rb:24:in `block in get_http_fiber&#39;</span>
<span># .../async-2.6.3/lib/async/task.rb:160:in `block in run&#39;</span>
<span># .../async-2.6.3/lib/async/task.rb:330:in `block in schedule&#39;</span>
<span># main.rb:12:in `get&#39;</span>
<span># main.rb:7:in `log_then_get&#39;</span>
<span># main.rb:24:in `block in get_http_fiber&#39;</span>
<span># .../async-2.6.3/lib/async/task.rb:160:in `block in run&#39;</span>
<span># .../async-2.6.3/lib/async/task.rb:330:in `block in schedule&#39;</span>
<span># main.rb:12:in `get&#39;</span>
<span># main.rb:7:in `log_then_get&#39;</span>
<span># main.rb:24:in `block in get_http_fiber&#39;</span>
<span># .../async-2.6.3/lib/async/task.rb:160:in `block in run&#39;</span>
<span># .../async-2.6.3/lib/async/task.rb:330:in `block in schedule&#39;</span>
<span># main.rb:12:in `get&#39;</span>
<span># main.rb:7:in `log_then_get&#39;</span>
<span># main.rb:24:in `block in get_http_fiber&#39;</span>
<span># .../async-2.6.3/lib/async/task.rb:160:in `block in run&#39;</span>
<span># .../async-2.6.3/lib/async/task.rb:330:in `block in schedule&#39;</span>
<span># Fiber runtime: 3.291355669</span>
</code></pre></div><p>If we visualize what’s happening, we see how things are getting coordinated. It’s nearly identical between the two:</p>
<p>Threads:</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/colorless-threadsfibers-diagram.drawio.png" alt=""/></p>
<p>Fibers:</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/colorless-threadsfibers-diagram.drawio-1.png" alt=""/></p>
<p>👆🏼This diagram does look pretty sequential - that’s odd huh?</p>
<p>And here is what happens once we are waiting for a response:</p>
<p>Threads:</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/colorless-threadsfibers-diagram.drawio-3.png" alt=""/></p>
<p>Fibers:</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/colorless-threadsfibers-diagram.drawio-2.png" alt=""/></p>
<p>👆🏼Oh, that seems better!</p>
<p>Running these HTTP methods is the same if I run it with no threads/fibers, 4 threads/fibers, or 400 threads/fibers. The code never changes, the execution context is all we need to start operating asynchronously. <em>Something</em> is coordinating for us behind-the-scenes, and we’re able to handle our HTTP calls in parallel 🙌🏼.</p>
<p>This lines up with our timing. In each example we ran 4 HTTP calls, all guaranteed to block for around 3 seconds. Despite that, each example finished in roughly 3 seconds, rather than the 12 seconds it would take if they had run sequentially.</p>
<div><pre tabindex="0"><code data-lang="rb"><span># Fiber runtime: 3.291355669</span>
<span># Thread runtime: 3.340238554</span>
</code></pre></div><p>To paraphrase Bob Nystrom’s article one last time, we can apply his original points about Go almost perfectly to Ruby threads/fibers:</p>
<blockquote>
<p>As soon as you do any IO operation, it just parks that thread/fiber and resumes any other one that isn’t blocked on IO.</p>
<p>If you look at the IO operations in the standard library, they seem synchronous. In other words, they just do work and then return a result when they are done. But it’s not that they’re synchronous in the sense that it would mean in JavaScript. Other Ruby code can run while one of these operations is pending. It’s that Ruby has eliminated the distinction between synchronous and asynchronous code.</p>
</blockquote>
<p>As we saw in the initial examples of colorless method calls - it’s not purely IO that parks and resumes. It’s the majority of blocking operations. Here’s another example:</p>
<div><pre tabindex="0"><code data-lang="rb"><span># Threads</span>
threads <span>=</span> <span>[</span>
  <span>Thread</span><span>.</span>new { sleep <span>5</span> },
  <span>Thread</span><span>.</span>new { <span>`ruby -v`</span> },
  <span>Thread</span><span>.</span>new { 
    noop <span>=</span> fork {}
    <span>Process</span><span>.</span>wait noop
  }
<span>]</span>
	
results <span>=</span> threads<span>.</span>map(<span>&amp;</span><span>:value</span>)
	
<span># Fibers</span>
<span>Fiber</span><span>.</span>set_scheduler(<span>Async</span><span>::</span><span>Scheduler</span><span>.</span>new)
fibers <span>=</span> <span>[</span>
  <span>Async</span> { sleep <span>5</span> },
  <span>Async</span> { <span>`ruby -v`</span> },
  <span>Async</span> { 
    noop <span>=</span> fork {}
    <span>Process</span><span>.</span>wait noop
  }
<span>]</span>
	
results <span>=</span> fibers<span>.</span>map(<span>&amp;</span><span>:wait</span>)
</code></pre></div><p>We <a href="https://docs.ruby-lang.org/en/3.2/Kernel.html#method-i-fork"><code>fork</code></a> a ruby process, run a command line script to get the current ruby version using <code>ruby -v</code> and <a href="https://docs.ruby-lang.org/en/3.2/Kernel.html#method-i-sleep"><code>sleep</code></a> - all in parallel.</p>
<h3 id="async-aside">A quick aside on Async</h3>
<p>Manually setting the FiberScheduler and manually unsetting it is just to more clearly demonstrate the interface. But you would normally run the block format:</p>
<div><pre tabindex="0"><code data-lang="rb"><span>Async</span> <span>do</span>
  fibers <span>&lt;&lt;</span> get_http_fiber <span>#...</span>
<span>end</span>
</code></pre></div><p>You’d also use the <code>Async</code> helpers instead of <code>Fiber.schedule</code>, which have a more robust available API:</p>
<div><pre tabindex="0"><code data-lang="rb"><span>def</span> <span>get_http_fiber</span>(url)
  <span>Async</span> <span>do</span>
    log_then_get(<span>URI</span>(url), <span>Fiber</span><span>.</span>current)
  <span>end</span>
<span>end</span>
</code></pre></div><h3 id="digging-deep">Digging deep</h3>
<p>My toy examples aside, this is one of the things that makes threaded/fibered web servers like <a href="https://github.com/puma/puma">Puma</a>/<a href="https://github.com/socketry/falcon">Falcon</a> and threaded job servers like <a href="https://github.com/sidekiq/sidekiq">Sidekiq</a>/<a href="https://github.com/bensheldon/good_job">GoodJob</a> so powerful - parallelizing blocking operations. Every time one of your job or web server threads/fibers reads a file, queries a database, makes an HTTP call, or waits for a shared resource, Ruby parks the thread/fiber and another thread/fiber is able to take over. Depending on the type of workload you can have dozens/hundreds/thousands of threads/fibers running concurrently, and blocking in parallel. We’ll push some limits on that later in the series to see where that might break down, and how we can keep scaling past those limits!</p>
<p>Now that we understand what colorless Ruby programming means and how it works - we’re left with some questions.</p>
<ul>
<li>Why would we need or want both Threads and Fibers?</li>
<li>Why do some languages choose to introduce color to their languages? Are there upsides to color?</li>
<li>What is the deal with those “VM locks”?</li>
<li>What does the “OS Kernel” have to do with parallel IO?</li>
<li>What’s a Reactor?</li>
<li>Is there a best concurrency option?</li>
<li>Are there abstractions you should be using?</li>
<li>What’s the point of concurrency without parallelism?</li>
<li>We also keep talking about parallelizing blocking operations - why don’t threads and fibers already allow you to parallelize everything?</li>
</ul>
<p>To answer these questions (and more), we’re going to dig pretty far into the ruby runtime to better understand Ruby threads, fibers and other forms of concurrency in Ruby.</p>
<p>Let’s start with the OG, Threads, in “Concurrent, Colorless Ruby: Part 1, Threads”. More soon! 👋🏼</p>
<img src="https://cdn.uploads.micro.blog/98548/2024/thread.png" width="25%" height="25%" alt=""/>
<h3 id="ruby-history-corner">PS: A historical sidenote - Ruby had its own callback phase</h3>
<p>It was 2009. The Black Eyed Peas were on top of the charts with “Boom Boom Pow”. Microsoft had just launched Windows 7. Avatar was taking off in the box office. And of course most important of all, node.js was released (😉). During a brief ensuing insanity, everyone thought it would devour the world of web development.</p>
<p>Like any new option, it ultimately took its place <em>alongside</em> other tools, but it had a big influence in the popularity of the <a href="https://en.wikipedia.org/wiki/Reactor_pattern">reactor pattern</a>.</p>
<p>The reactor pattern is an event loop that registers events with the operating system and efficiently multiplexes them. It was a way of solving the <a href="https://en.m.wikipedia.org/wiki/C10k_problem">C10K problem</a> - a solution for serving 10 thousand+ clients from a single server.</p>
<p>To get a great insight into the Ruby concurrency landscape at that time, and why the node.js model seemed so appealing, see this post from Yehuda Katz in 2010 - <a href="https://yehudakatz.com/2010/08/14/threads-in-ruby-enough-already/">https://yehudakatz.com/2010/08/14/threads-in-ruby-enough-already/</a>. In summary:</p>
<ul>
<li>Early Rails was not thread-safe so you could only scale with Processes</li>
<li>Once Rails was “thread-safe”, it started with a giant mutex around the entire framework that meant only one thread could do anything at a time, including blocking operations</li>
<li>Even threaded servers at the time would often wrap things poorly and too broadly in mutexes</li>
<li>The most popular database for Ruby/Rails at the time, MySQL, had a Ruby driver which did not release the thread on blocking operations</li>
<li>Ruby 1.9 was the first Ruby to map to real operating system threads and loads of people still worked in Ruby 1.8</li>
<li>He doesn’t mention it specifically, but the gem landscape was also pretty scary in terms of thread safety because many people did not understand or worry about it prior to that point. Things are definitely better now, though I’d recommend reviewing my <a href="https://jpcamara.com/2024/06/04/your-ruby-programs.html">Your Ruby Programs are always multi-threaded</a> posts, and specifically <a href="https://jpcamara.com/2024/06/23/your-ruby-programs.html#tips-for-gems">tips for auditing gems</a></li>
</ul>
<p>It was a concurrency mess.</p>
<p>No wonder node.js seemed like a magical concurrency silver bullet. A single process handling hundreds to thousands of users vs dozens of heavyweight processes handling a paltry comparative amount.</p>
<p>In the Ruby world the reactor pattern was served by a tool called EventMachine. We’ll talk a bit deeper about EventMachine in the “Concurrent, Colorless Ruby” fiber post later, but it was solely callback based.</p>
<div><pre tabindex="0"><code data-lang="rb"><span>EventMachine</span><span>.</span>run <span>do</span>
  redis <span>=</span> <span>EM</span><span>::</span><span>Hiredis</span><span>.</span>connect
  http <span>=</span> <span>EM</span><span>::</span><span>HttpRequest</span><span>.</span>new(
    <span>&#34;http://google.com/&#34;</span>
  )<span>.</span>get <span>query</span>: { <span>&#34;keyname&#34;</span> <span>=&gt;</span> <span>&#34;value&#34;</span> }
	
  http<span>.</span>errback { <span>EventMachine</span><span>.</span>stop }
  http<span>.</span>callback {
    parse(http<span>.</span>response)<span>.</span>each <span>|</span>link<span>|</span> <span>do</span>
      req <span>=</span> <span>EM</span><span>::</span><span>HttpRequest</span><span>.</span>new(link)<span>.</span>get
      req<span>.</span>callback {
        body <span>=</span> req<span>.</span>response
        redis<span>.</span>set(link, body)<span>.</span>callback {
          <span># and on and on</span>
        }
      }
    <span>end</span>
  }
	
  <span>EventMachine</span><span>.</span>stop
<span>end</span>
</code></pre></div><p>You can see how messy coding like this becomes, just like the original callback model we presented in JavaScript.</p>
<p>EventMachine was a powerful scaling option, and people still maintain EventMachine-based code today, but it was at odds with the normal flow of Ruby code. Ruby could have moved to red and blue style calls like some other languages to clean that up, but instead superseded those Reactor efforts with the FiberScheduler.</p>

</section>

  
  
  
  

  
  
    
  

  
  
  
  <nav>
     
    <a href="https://jpcamara.com/2024/06/27/consistent-requestlocal-state.html">
      <span>
         Consistent, request-local state 
      </span>
	    <span>→</span>
    </a>
    
  </nav>
  

  
  

  
  
</article>



</div></div>
  </body>
</html>
