<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://jalammar.github.io/ai-image-generation-tools/">Original</a>
    <h1>Remaking old computer graphics with AI image generation</h1>
    
    <div id="readability-page-1" class="page"><div>
    <p>Can AI Image generation tools make re-imagined, higher-resolution versions of old video game graphics?</p>

<p>Over the last few days, I used AI image generation to reproduce one of my childhood nightmares. I wrestled with Stable Diffusion, Dall-E and Midjourney to see how these commercial AI generation tools can help retell an old visual story - the intro cinematic to an old video game (<a href="https://en.wikipedia.org/wiki/Nemesis_2_(MSX)">Nemesis 2 on the MSX</a>). This post describes the process and my experience in using these models/services to retell a story in higher fidelity graphics.</p>

<h2 id="meet-dr-venom">Meet Dr. Venom</h2>

<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-06.png"/>
  <br/>
</p>

<p>This fine-looking gentleman is the villain in a video game. Dr. Venom appears in the intro cinematic of Nemesis 2, a 1987 video game. This image, in particular, comes at a dramatic reveal in the cinematic.</p>

<p>Let’s update these graphics with visual generative AI tools and see how they compare and where each succeeds and fails.</p>

<h2 id="remaking-old-computer-graphics-with-ai-image-generation">Remaking Old Computer graphics with AI Image Generation</h2>

<p>Here’s a side-by-side look at the panels from the original cinematic (left column) and the final ones generated by the AI tools (right column):</p>

<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-storyboard-image-gen.png"/>
  <br/>
</p>

<p>This figure does not show the final Dr. Venom graphic because I want you to witness it as I had, in the proper context and alongside the appropriate music. You can watch that here:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/43bsSVnioI0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<!--more-->

<h3 id="panel-1">Panel 1</h3>
<p>Original image</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-01.png"/>
  <br/>
</p>

<p>The final image was generated by Stable Diffusion using Dream Studio.</p>

<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-01-gen-sd-2.png"/>
  <br/>
</p>

<p>The road to this image, however, goes through generating over 30 images and tweaking prompts. The first kind of prompt I’d use is something like:</p>

<blockquote>
  <p>fighter jets flying over a red planet in space with stars in the black sky</p>
</blockquote>

<p>This leads Dall-E to generate these candidates</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-01-gen-dalle-1.png"/></p></div>

<p>Pasting a similar prompt into Dream Studio generates these candidates:</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-01-gen-sd-3.png"/></p></div>

<p>This showcases a reality of the current batch of image generation models. It is not enough for your prompt to describe the subject of the image. Your image creation prompt/spell needs to mention the exact arcane keywords that guide the model toward a specific style.</p>

<h3 id="searching-for-prompts-on-lexica">Searching for prompts on Lexica</h3>

<p>The current solution is to either go through a prompt guide and learn the styles people found successful in the past, or search a gallery like <a href="https://lexica.art">Lexica</a> that contains millions of examples and their respective prompts. I go for the latter as learning arcane keywords that would work on specific versions of specific models is not a winning strategy for the long term.</p>

<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-01-gen-lexica-1.png"/>
  <br/>
</p>

<p>From here, I find an image that I like, and edit it with my subject keeping the style portion of the prompt, so finally it looks like:</p>

<blockquote>
  <p>fighter jets flying over a red planet in space flaming jets behind them, stars on a black sky, lava, ussr, soviet, as a realistic scifi spaceship!!!, floating in space, wide angle shot art, vintage retro scifi, realistic space, digital art, trending on artstation, symmetry!!! dramatic lighting.</p>
</blockquote>

<h2 id="midjourney">MidJourney</h2>
<p>The results of Midjourney have always stood out as especially beautiful. I tried it with the original prompt containing only the subject. The results were amazing.</p>

<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-01-gen-midjourney-3.png"/>
  <br/>
</p>

<p>While these look incredible, they don’t capture the essence of the original image as well as the Stable Diffusion one does. But this convinced me to try Midjourney first for the remainder of the story. I had about eight images to generate and only a limited time to get an okay result for each.</p>

<h2 id="panel-2">Panel 2</h2>
<p>Original Image:</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-02.png"/>
  <br/>
</p>

<p>Final Image:</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-02-gen-midjourney-1.png"/></p></div>

<h3 id="failed-attempts">Failed attempts</h3>
<p>While Midjourney could approximate the appearance of Dr. Venom, it was difficult to get the pose and restraint. My attempts at that looked like this:</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-02-gen-midjourney-2.png"/></p></div>

<p>That’s why I tweaked the image to show him behind bars instead.</p>

<h2 id="panel-3">Panel 3</h2>

<p>Original Image:</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-03.png"/>
  <br/>
</p>

<p>Final Image:</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-03-gen-midjourney-1.png"/></p></div>

<p>To instruct the model to generate a wide image, the <em>–ar 3:2</em> command specifies the desired aspect ratio.</p>

<h2 id="panel-4">Panel 4</h2>

<p>Original Image:</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-04.png"/>
  <br/>
</p>

<p>Final Image:</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-04-midjourney.png"/></p></div>

<p>Midjourney really captures the cool factor in a lot of fighter jet schematics. The text will not make sense, but that can work in your favor if you’re going for something alien.</p>

<p>In this workflow, it’ll be difficult to reproduce the same plane in future panels. Recent, more advanced methods like textual inversion or photobooth could aid in this, but at this time they are more difficult to use than text-to-image services.</p>

<h2 id="panel-5">Panel 5</h2>

<p>Original Image:</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-05.png"/>
  <br/>
</p>

<p>Final Image:</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-05-midjourney.png"/></p></div>

<p>This image shows a limitation in what is possible with the current batch of AI image tools:</p>

<p>1- Reproducing text correctly in images is still not yet widely available (although technically possible as demonstrated in <a href="https://imagen.research.google/">Google’s Imagen</a>)</p>

<p>2- Text-to-image is not the best paradigm if you need a specific placement or manipulation of elements</p>

<p>So to get this final image, I had to import the stars image into photoshop and add the text and lines there.</p>

<h2 id="panel-6">Panel 6</h2>

<p>Original Image:</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-06.png"/>
  <br/>
</p>

<p>I failed at reproducing the most iconic portion of this image, the three eyes. The models wouldn’t generate the look using any of the prompts I’ve tried.</p>

<p>I then proceeded to try in-painting in Dream Studio.</p>

<p><img src="https://jalammar.github.io/images/image-gen/in-painting-01.png"/>
  <br/>
</p>

<p>In-painting instructs the model to only generate an image for a portion of the image, in this case, it’s the portion I deleted with the brush inside of Dream Studio above.</p>

<p>I couldn’t get to a good result in time. Although looking at the gallery, the models are quite capable of generating horrific imagery involving eyes.</p>

<p><img src="https://jalammar.github.io/images/image-gen/eyes.jpg"/>
  <br/>
</p>

<h2 id="panel-7">Panel 7</h2>

<p>Original Image:</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-07.png"/>
  <br/>
</p>

<p>Candidate generations:</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-07-midjourney-2.jpg"/></p></div>

<h2 id="panel-8">Panel 8</h2>

<p>Original Image:</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-08.png"/>
  <br/>
</p>

<p>Candidate generations:</p>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-08-gen-midjourney-1.png"/></p></div>

<p>This image provided a good opportunity to try out DALL-E’s outpainting tool to expand the canvas and fill-in the surrounding space with content.</p>

<h2 id="expanding-the-canvas-with-dall-e-outpainting">Expanding the Canvas with DALL-E Outpainting</h2>

<p>Say we decided to go with this image for the ship’s captain</p>
<p><img src="https://jalammar.github.io/images/image-gen/nemesis-2-intro-08-gen-midjourney-2.png"/>
  <br/>
</p>

<p>We can upload it to DALL-E’s outpainting editor and over a number of generations continue to expand the imagery around the image (taking into consideration a part of the image so we keep some continuity).</p>

<p><img src="https://jalammar.github.io/images/image-gen/panel-9-outpainting.jpg"/>
  <br/>
</p>

<p>The outpainting workflow is different from the text2image in that the prompt has to be changed to describe the portion you’re describing at each portion of the image.</p>



<p>It’s been a few months since the vast majority of people started having broad access to AI image generation tools. The major milestone here is the open source release of Stable Diffusion (although some people had access to DALL-E before, and models like <a href="https://github.com/openai/glide-text2im">OpenAI GLIDE</a> were publicly available but slower and less capable). In this time, I’ve gotten to use three of these image generation services.</p>

<h3 id="dream-studio-by-stability-ai">Dream Studio by Stability AI</h3>

<div>
  <p><img src="/images/image-gen/
2464700474_Two_astronauts_exploring_the_dark__cavernous_interior_of_a_huge_derelict_spacecraft__digital_art__ne.png"/></p></div>

<p>This is what I have been using the most over the last few months.</p>

<h4 id="pros">Pros</h4>

<ul>
  <li>They made Stable Diffusion and serve a managed version of it – a major convenience and improvement in workflow.</li>
  <li>They have an API and so the models can be accessed programmatically. A key point for extending the capability and building more advanced systems that use an image generation component.</li>
  <li>Being the makers of Stable Diffusion, it is expected they will continue to be the first to offer the managed version of upcoming versions which are expected to keep getting better.</li>
  <li>The fact that Stable Diffusion is open source is another big point in their favor. The managed model can be used as a prototyping ground (or a production tool for certain use cases), yet you have the knowledge that if your use cases requires fine-tuning your own model you can revert to the open source versions.</li>
  <li>Currently the best user interface with the most options (without being overwhelming like some of the open source UIs). It has the key sliders you need to tweak and you can pick how many candidates to generate. They were quick to add user interface components for advanced features like in-painting.</li>
</ul>

<h4 id="cons">Cons</h4>

<ul>
  <li>Dream Studio still does not robustly keep a history of all the images the user generates.</li>
  <li>Older versions of Stable Diffusion (e.g. 1.4 and 1.5) remain easier to get better results with (aided by galleries like Lexica). The newer models are still being figured out by the community, it seems.</li>
</ul>

<h3 id="midjourney-1">Midjourney</h3>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/Two_astronauts_exploring_the_dark_cavernous_interio_8fae1463-94ab-45fd-be0f-2860d0873eef.png"/></p></div>

<h4 id="pros-1">Pros</h4>

<ul>
  <li>By far the best generation quality with the least amount of prompt tweaking</li>
  <li>The UI saves the archive of generation</li>
  <li>Community tab feed in the website is a great showcase of the artwork the community is pumping out. In a way, it is Midjourney’s own Lexica.</li>
</ul>

<h4 id="cons-1">Cons</h4>

<ul>
  <li>Can only be accessed via Discord, as far as I can tell. I don’t find that to be a compelling channel. As a trial user, you need to generate images in public “Newbie” channels (which didn’t work for me when I tried them a few months ago – understandable given the meteoric growth the platform has experienced). I revisited the service only recently and paid for a subscription that would allow me to directly generate images using a bot.</li>
  <li>No UI components to pick image size or other options. Options are offered as commands to add to the prompt. I found that to be less discoverable than Dream Studio’s UI which shows the main sliders and describes them.</li>
  <li>Can’t access it via API (as far as I can tell) or generate images in the browser.</li>
</ul>

<h3 id="dall-e">DALL-E</h3>

<div>
  <p><img src="https://jalammar.github.io/images/image-gen/DALL%C2%B7E%202023-01-01%2011.19.24%20-%20cavernous%20interior%20of%20a%20huge%20derelict%20spacecraft,%20digital%20art,%20neon%20blue%20glow,%20yellow%20crystal%20artifacts.png"/></p></div>

<h4 id="pros-2">Pros</h4>
<ul>
  <li>DALL-E was the first to dazzle the world with the capabilities of  this batch of image generation models.</li>
  <li>Inpainting and outpainting support</li>
  <li>Keeps the entire history of generated images</li>
  <li>Has an <a href="https://openai.com/blog/dall-e-api-now-available-in-public-beta/">API</a></li>
</ul>

<h4 id="cons-2">Cons</h4>
<ul>
  <li>Feels a little slower than Stable Diffusion, but good that it generates four candidate images</li>
  <li>Because it lags behind Midjourney in quality of images generated in response to simple prompts, and behind Stable Diffusion in community adoption and tooling (in my perception), I haven’t found a reason to spend a lot of time exploring DALL-E. Outpainting feels kinda magical, however. I think that’s where I may spend some more time exploring.</li>
</ul>

<p>That said, do not discount DALL-E just yet, however. OpenAI are quite the pioneers and I’d expect the next versions of the model to dramatically improve generation quality.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This is a good place to end this post although there are a bunch of other topics I had wanted to address. Let me know what you think on <a href="https://twitter.com/JayAlammar">@JayAlammar</a> or <a href="https://sigmoid.social/@JayAlammar">@JayAlammar@sigmoid.social</a>.</p>


  </div></div>
  </body>
</html>
