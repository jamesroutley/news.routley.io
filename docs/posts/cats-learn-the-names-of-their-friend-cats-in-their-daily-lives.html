<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.nature.com/articles/s41598-022-10261-5">Original</a>
    <h1>Cats learn the names of their friend cats in their daily lives</h1>
    
    <div id="readability-page-1" class="page"><div>
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div id="Abs1-section"><h2 id="Abs1">Abstract</h2><p>Humans communicate with each other through language, which enables us talk about things beyond time and space. Do non-human animals learn to associate human speech with specific objects in everyday life? We examined whether cats matched familiar cats’ names and faces (Exp.1) and human family members’ names and faces (Exp.2). Cats were presented with a photo of the familiar cat’s face on a laptop monitor after hearing the same cat’s name or another cat’s name called by the subject cat’s owner (Exp.1) or an experimenter (Exp.2). Half of the trials were in a congruent condition where the name and face matched, and half were in an incongruent (mismatch) condition. Results of Exp.1 showed that household cats paid attention to the monitor for longer in the incongruent condition, suggesting an expectancy violation effect; however, café cats did not. In Exp.2, cats living in larger human families were found to look at the monitor for increasingly longer durations in the incongruent condition. Furthermore, this tendency was stronger among cats that had lived with their human family for a longer time, although we could not rule out an effect of age. This study provides evidence that cats link a companion&#39;s name and corresponding face without explicit training.</p></div></section>
            

                
            
                <section data-title="Introduction"><div id="Sec1-section"><h2 id="Sec1">Introduction</h2><div id="Sec1-content"><p>Many human words have referential meanings: they evoke a visual mental image when heard or read<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Hurford, J. R. &amp; Hurford, J. R. In The Origins of Grammar: Language in the Light of Evolution II (Oxford University Press, 2012)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR1" id="ref-link-section-d289590345e543">1</a></sup>. For example, the word “apple” causes us to imagine a red or green fruit even if no such fruit is present. This language property, which expands the plasticity of communication, is also seen to some extent in non-human animals, mainly in the context of intraspecific vocal communication. Seyfarth, Cheney and Marler reported that vervet monkeys (now called <i>Chlorocebus pygerythrus</i>) responded differently to different types of alarm calls<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Seyfarth, R. M., Cheney, D. L. &amp; Marler, P. Monkey responses to three different alarm calls: evidence of predator classification and semantic communication. Science 210, 801–803 (1980)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR2" id="ref-link-section-d289590345e550">2</a></sup> (although some of the calls overlap acoustically<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Price, T. et al. Vervets revisited: A quantitative analysis of alarm call structure and context specificity. Sci. Rep. 5, 1–11 (2015)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR3" id="ref-link-section-d289590345e554">3</a></sup> and this view is currently debated<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Wheeler, B. C. &amp; Fischer, J. Functionally referential signals: A promising paradigm whose time has passed. Evol. Anthropol. 21, 195–205 (2012)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR4" id="ref-link-section-d289590345e558">4</a></sup>). More recently, west African green monkeys (<i>Chlorocebus sabaeus</i>) rapidly learned the novel referent of an alarm call that was given in response to a drone<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Wegdell, F., Hammerschmidt, K. &amp; Fischer, J. Conserved alarm calls but rapid auditory learning in monkey responses to novel flying objects. Nat. Ecol. Evol. 3, 1039–1042 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR5" id="ref-link-section-d289590345e566">5</a></sup>. Referential signaling is not limited to primates. Suzuki showed that tits (<i>Parus minor</i>) detected snake-like motion more rapidly when a snake-specific alarm call rather than a general alarm call was played back, suggesting that tits recall things to which at least one specific call refers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Suzuki, T. N. Alarm calls evoke a visual search image of a predator in birds. Proc. Natl. Acad. Sci. U. S. A. 115, 1541–1545 (2018)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR6" id="ref-link-section-d289590345e573">6</a></sup>. Such studies show that animals have specific calls with a referential meaning, increasing the likelihood of responses appropriate for survival.</p><p>In contrast to studies dealing with life-or-death-related issues and ecology, some studies have reported that companion animals understand human utterances in more neutral situations and use them in communication with us [e.g., dogs (<i>Canis lupus familiaris</i>)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Kaminski, J., Call, J. &amp; Fischer, J. Word learning in a domestic dog: Evidence for&#34; fast mapping&#34;. Science 304, 1682–1683 (2004)." href="#ref-CR7" id="ref-link-section-d289590345e583">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Pilley, J. W. &amp; Reid, A. K. Border collie comprehends object names as verbal referents. Behav. Processes 86, 184 (2011)." href="#ref-CR8" id="ref-link-section-d289590345e583_1">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Andics, A. et al. Neural mechanisms for lexical processing in dogs. Science (New York, NY) 353, 1030 (2016)." href="#ref-CR9" id="ref-link-section-d289590345e583_2">9</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Griebel, U. &amp; Oller, D. K. Vocabulary learning in a Yorkshire terrier: Slow mapping of spoken words. PLoS One 7, e30182 (2012)." href="#ref-CR10" id="ref-link-section-d289590345e583_3">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Dror, S., Miklósi, Á., Sommese, A., Temesi, A. &amp; Fugazza, C. Acquisition and long-term memory of object names in a sample of Gifted Word Learner dogs. R. Soc. Open Sci. 8, 210976 (2021)." href="#ref-CR11" id="ref-link-section-d289590345e583_4">11</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Fugazza, C. et al. Rapid learning of object names in dogs. Sci. Rep. 11, 1–11 (2021)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR12" id="ref-link-section-d289590345e586">12</a></sup>]. Dogs in particular have been studied in this context; for example, a few “expert” dogs trained in object-name fetching over several months remembered hundreds of object names and fetched the correct object upon verbal command<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Kaminski, J., Call, J. &amp; Fischer, J. Word learning in a domestic dog: Evidence for&#34; fast mapping&#34;. Science 304, 1682–1683 (2004)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR7" id="ref-link-section-d289590345e590">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Pilley, J. W. &amp; Reid, A. K. Border collie comprehends object names as verbal referents. Behav. Processes 86, 184 (2011)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR8" id="ref-link-section-d289590345e593">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Fugazza, C. et al. Rapid learning of object names in dogs. Sci. Rep. 11, 1–11 (2021)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR12" id="ref-link-section-d289590345e596">12</a></sup>. According to a recent report, “gifted” dogs learned object names after few exposures during social interactions, whereas the majority of dogs did not show such object-name association learning despite intensive training<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Fugazza, C. et al. Rapid learning of object names in dogs. Sci. Rep. 11, 1–11 (2021)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR12" id="ref-link-section-d289590345e600">12</a></sup>.</p><p>Similar to dogs, cats (<i>Felis catus</i>) are one of the most widespread companion animals in the world<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Driscoll, C. A., Macdonald, D. W. &amp; O’Brien, S. J. From wild animals to domestic pets, an evolutionary view of domestication. Proc. Natl. Acad. Sci. U. S. A. 106(Suppl 1), 9971–9978 (2009)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR13" id="ref-link-section-d289590345e610">13</a></sup> . Although the ancestral Libyan wildcat (<i>Felis lybica</i>) is a solitary species<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Driscoll, C. A. et al. The Near Eastern origin of cat domestication. Science 317, 519–523 (2007)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR14" id="ref-link-section-d289590345e617">14</a></sup>, many domestic cats live with humans and show evidence of social cognitive operations concerning humans. They can use human pointing cues<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Miklósi, Á., Pongrácz, P., Lakatos, G., Topál, J. &amp; Csányi, V. A comparative study of the use of visual communicative signals in interactions between dogs (Canis familiaris) and humans and cats (Felis catus) and humans. J. Comp. Psychol. 119, 179 (2005)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR15" id="ref-link-section-d289590345e621">15</a></sup> and gaze cues<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Pongrácz, P., Szapu, J. S. &amp; Faragó, T. Cats (Felis silvestris catus) read human gaze for referential information. Intelligence 74, 43–52 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR16" id="ref-link-section-d289590345e626">16</a></sup> to find food. They also discriminate between human facial expressions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Merola, I., Lazzaroni, M., Marshall-Pescini, S. &amp; Prato-Previde, E. Social referencing and cat-human communication. Anim. Cogn. 18, 639–648 (2015)." href="#ref-CR17" id="ref-link-section-d289590345e630">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Galvan, M. &amp; Vonk, J. Man’s other best friend: domestic cats (F. silvestris catus) and their discrimination of human emotion cues. Anim. Cogn. 19, 193–205 (2016)." href="#ref-CR18" id="ref-link-section-d289590345e630_1">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Quaranta, A., d’Ingeo, S., Amoruso, R. &amp; Siniscalchi, M. Emotion recognition in cats. Animals 10, E1107 (2020)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR19" id="ref-link-section-d289590345e633">19</a></sup> and attentional states<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ito, Y., Watanave, A., Tagagi, S., Arahori, M. &amp; Saito, A. Cats beg for food from the human who looks at and calls to them: Ability to understand humans’ attentional states. Psychologia 59, 112–120 (2016)." href="#ref-CR20" id="ref-link-section-d289590345e637">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Vitale, K. R. &amp; Udell, M. A. The quality of being sociable: The influence of human attentional state, population, and human familiarity on domestic cat sociability. Behav. Processes 158, 11–17 (2019)." href="#ref-CR21" id="ref-link-section-d289590345e637_1">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Koyasu, H. &amp; Nagasawa, M. Recognition of directed-gaze from humans in cats. Jpn. J. Anim. Psychol. 69(2), 3 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR22" id="ref-link-section-d289590345e640">22</a></sup>, and identify their owner’s voice<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Saito, A. &amp; Shinozuka, K. Vocal recognition of owners by domestic cats (Felis catus). Anim. Cogn. 16, 685–690 (2013)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR23" id="ref-link-section-d289590345e644">23</a></sup>. Furthermore, they cross-modally match their owner&#39;s voice and face<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Takagi, S. et al. Cats match voice and face: Cross-modal representation of humans in cats (Felis catus). Anim. Cogn. 22, 901–906 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR24" id="ref-link-section-d289590345e648">24</a></sup> when tested with their owner’s photo presented on a screen, and human emotional sounds and expressions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Quaranta, A., d’Ingeo, S., Amoruso, R. &amp; Siniscalchi, M. Emotion recognition in cats. Animals 10, E1107 (2020)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR19" id="ref-link-section-d289590345e652">19</a></sup>.</p><p>Cats have been shown to distinguish their own from another familiar cat’s name in a habituation–dishabituation procedure<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Saito, A., Shinozuka, K., Ito, Y. &amp; Hasegawa, T. Domestic cats (Felis catus) discriminate their names from other words. Sci. Rep. 9, 1–8 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR25" id="ref-link-section-d289590345e659">25</a></sup>, and they also distinguished those names from general nouns. Interestingly, cats living in multi-cat households habituated less to their companion cats’ names than to other nouns. Conceivably, therefore, cats might also recognize the name of another cat living in the same household.</p><p>Here we examined whether cats linked a human utterance and the corresponding object, using a relatively simple task that is applicable to many species: a visual-auditory expectancy violation task previously used to test cats’ ability to predict an object when hearing that objects’ name<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Takagi, S. et al. Cats match voice and face: Cross-modal representation of humans in cats (Felis catus). Anim. Cogn. 22, 901–906 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR24" id="ref-link-section-d289590345e667">24</a></sup>. As stimuli we used the names of other cats (“models”) cohabiting with the subjects in Exp.1, and human family members’ names in Exp.2. Cats were presented with the face of the other cat (Exp.1) or human (Exp.2) following presentation of the model’s name, called by the owner (Exp.1) or an experimenter (Exp.2). Half of the trials were “congruent,” i.e., the model’s face and name matched, whereas the other half were “incongruent” (the stimuli mismatched). Previous research showed that cats matched human photos and voices<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Takagi, S. et al. Cats match voice and face: Cross-modal representation of humans in cats (Felis catus). Anim. Cogn. 22, 901–906 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR24" id="ref-link-section-d289590345e671">24</a></sup>, which established the validity of presenting photos as stimuli. Our hypothesis was that cats learned face–name relationships by observing interactions involving their owner, and that more such observations would lead to stronger learning. We tested two groups of cats, differing in the number of other cats they lived with: cats belonging to cat cafés where many cats live together, and household cats. The latter probably have more opportunities to observe interactions between the owner and each of the other cohabitating cats, which might facilitate learning of the face–name relationship. Therefore, we analyzed data from household cats and cat café cats separately in Exp.1. In Exp.2, analysis concerned the number of cohabiting family members because more members would have more opportunities to hear other members’ names (e.g., people living as a couple probably say each other’s name less often than people living in a larger family). In Exp.2 we considered length of time living with the family as well as the number of family members.</p><p>We made two predictions. First, attention toward the stimulus face displayed on the monitor should be longer in incongruent trials due to expectancy violation. Second, the amount of violation is related to the amount of exposure to relevant interactions; specifically, household cats should show stronger violation effects than café cats in Exp.1, and cats living in households with more people should show more evidence of expectancy violation in Exp.2.</p></div></div></section><section data-title="Experiment 1"><div id="Sec2-section"><h2 id="Sec2">Experiment 1</h2><div id="Sec2-content"><h3 id="Sec3">Materials and methods</h3><h4 id="Sec4">Subjects</h4><p>We tested 48 cats (28 males and 19 females). Twenty-nine (17 males and 12 females, mean age 3.59 years, <i>SD</i> 2.71 years) lived in five “cat cafés” (mean number living together: 14.2, <i>SD</i> 10.01), where visitors can freely interact with the cats. The other 19 (11 males and 8 females, mean age 8.16 years, <i>SD</i> 5.16 years) were household cats (mean number living together: 6.37, <i>SD</i> 4.27). We tested household cats living with at least two other cats because the experiment required two cats as models. The model cats were quasi-randomly chosen from the cats living with the subject, on condition of a minimum period of 6 months cohabiting, and having different coat colors so that their faces might be more easily identified. We did not ask the owner to make any changes to water or feeding schedules.</p><h4 id="Sec5">Stimuli</h4><p>For each subject, visual stimuli consisted of two photos of two cats other than the subject who lived together, and auditory stimuli consisting of the voice of the owner calling the cats’ names. We asked the owner to call each cat’s name as s/he would usually do, and recorded the call using a handheld digital audio recorder (SONY ICD-UX560F, Japan) in WAV format. The sampling rate was 44,100 Hz and the sampling resolution was 16-bit. The call lasted about 1 s, depending on the length of cat’s name (mean duration 1.04 s, <i>SD</i> 0.02). All sound files were adjusted to the same volume with the help of version 2.3.0 of Audacity(R) recording and editing software<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Audacity team. Audacity® software is copyright © 1999–2021 Audacity Team. The name Audacity® is a registered trademark. (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR26" id="ref-link-section-d289590345e717">26</a></sup>. We took a digital, frontal face, neutral expression, color photo of each cat against a plain background (resolution range: x = 185 to 1039, y = 195 to 871) which was expanded or shrunk to fit the monitor size (12.3″ PixelSense™ built-in display).</p><h4 id="Sec6">Procedure</h4><p>We tested cats individually in a familiar room. The cat was softly restrained by Experimenter 1, 30 cm in front of the laptop computer (SurfacePro6, Microsoft) which controlled the auditory and visual stimuli. Each cat was tested in one session consisting of two phases. First, in the name phase the model cat’s name was played back from the laptop’s built-in speaker four times, each separated by a 2.5-s inter-stimulus interval. During this phase, the monitor remained black. Immediately after the name phase, the face phase began, in which a cat&#39;s face appeared on the monitor for 7 s. The face photos were ca. 16.5 × 16 cm on the monitor. Experimenter 1 gently restrained the cat, looking down at its head; she never looked at the monitor, and so was unaware of the test condition. When the cat was calm and oriented toward the monitor, Experimenter 1 started the name phase by pressing a key on the computer. She restrained the cat until the end of the name phase, and then released it. Some cats remained stationary, whereas others moved around and explored the photograph presented on the monitor. The trial ended after the 7-s face phase.</p><p>We conducted two congruent and two incongruent trials for each subject (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-022-10261-5#Fig1">1</a>), in pseudo-random order, with the restriction that the same vocalization was not repeated on consecutive trials. The inter-trial interval was at least 3 min. The subject’s behaviors were recorded on three cameras (two Gopros (HERO black 7) and SONY FDR-X3000): one beside the monitor for a lateral view, one in front of the cat to measure time looking at the monitor, and one recording the entire trial from behind.</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Figure 1"><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Figure 1</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-022-10261-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig1_HTML.png?as=webp"/><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="493"/></picture></a></div><p>Diagram illustrating each condition in Exp.1. Two model cats were chosen from cats living with subject. The model cat’s name called by owner was played through the speaker built into the laptop computer (Name phase). Immediately after playback, a cat’s face appeared on the monitor (Face phase). On half of the trials the name and face matched (congruent condition), on the other half they mismatched (incongruent condition).</p></div></figure></div><h4 id="Sec7">Analysis</h4><p>One cat completed only the first trial before escaping from the room and climbing out of reach. For the face phase we measured time attending to the monitor, defined as visual orientation toward or sniffing the monitor. Trials in which the subject paid no attention to the monitor in the face phase were excluded from the analyses. In total, 34 congruent trials and 33 incongruent trials for café cats, and 26 congruent trials and 27 incongruent trials for house cats were analyzed (69 trials excluded overall). A coder who was blind to the conditions counted the number of frames (30 frames/sec.) in which the cat attended to the monitor. To check inter-observer reliability, an assistant who was blind to the conditions coded a randomly chosen 20% of the videos. The correlation between the two coders was high and positive (Pearson’s <i>r</i> <span>\(=\)</span> 0.88, <i>n</i> <span>\(=\)</span> 24, <i>p</i> &lt; 0.001).</p><p>We used R version 3.5.1 for all statistical analyses<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="R Core Team. R: A Language and Environment for Statistical Computing. (R Foundation for Statistical Computing, 2018). 
                  https://www.R-project.org/
                  
                ." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR27" id="ref-link-section-d289590345e806">27</a></sup>. Time attending to the monitor was analyzed by a linear mixed model (LMM) using a lmer function in a lme4 package version 1.1.10<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Bates, D. et al. Package ‘lme4’. CRAN.R Foundation for Statistical Computing, Vienna, Austria (2012)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR28" id="ref-link-section-d289590345e810">28</a></sup>. We log-transformed attention time to get close to normal distribution. Congruency (congruent/ incongruent), environment (cat café/house), and the interaction were entered as fixed factors, and subject identity was a random factor. We ran F tests using an Anova function in a car package<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Fox, J. &amp; Weisberg, S. In An R Companion to Applied Regression (Sage Publications, 2018)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR29" id="ref-link-section-d289590345e814">29</a></sup> to test whether effects of each factor were significant. To test for differences between conditions, an emmeans function in an emmeans package<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Lenth, R. V. Lenth emmeans: Estimated Marginal Means, aka Least-Squares Means.R package version 1.5.3. (2020)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR30" id="ref-link-section-d289590345e818">30</a></sup> was used, testing differences of least squares means. Degrees of freedom were adjusted by the Kenward–Roger procedure.</p><p>In addition to attention to the monitor, we calculated the Violation Index (VI), which indicates how much longer cats attended in the incongruent condition than the congruent condition. VI was calculated by subtracting the mean congruent value from the mean incongruent value for each subject. Greater VI values indicate longer looking in incongruent conditions. Note that we used data only from subjects with at least one congruent—incongruent pair. Thus, if a subject had one congruent/incongruent data point, we used that value for analysis instead of calculating the mean. Data from 14 household cats and 16 café cats were analyzed. We ran a linear model (LM) using a lmer function in a lme4 package version 1.1.10<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Bates, D. et al. Package ‘lme4’. CRAN.R Foundation for Statistical Computing, Vienna, Austria (2012)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR28" id="ref-link-section-d289590345e825">28</a></sup>. Living environment (café/house) was entered as a fixed factor. To examine whether VI was greater than 0, we also conduct a one-sample t-test for each group.</p><h3 id="Sec8">Results and discussion</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-022-10261-5#Fig2">2</a> shows time attending to the monitor for each group. House cats attended for longer in the incongruent than the congruent condition, as predicted; however, café cats did not show this difference.</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Figure 2"><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Figure 2</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-022-10261-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig2_HTML.png?as=webp"/><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="473"/></picture></a></div><p>Time attending to the monitor during the face phase for each group in Exp.1. Red bar represents congruent condition; Blue bar represents incongruent condition. Left panel shows café cat data, right panel shows house cat data. The y-axis is log-transformed.</p></div></figure></div><p>LMM revealed a significant main effect of living environment (<span>\({\rm X}\)</span><sup>2</sup> (1) = 16.544, <i>p</i> &lt; 0.001), and a congruency x living environment interaction (<span>\({\rm X}\)</span><sup>2</sup> (1) = 6.743, <i>p</i> = 0.009). The differences of least squares means test confirmed a significant difference between congruent and incongruent conditions in house cat (<i>t</i> (86) = 2.027, <i>p</i> = 0.045), but not café cats (<i>t</i> (97.4) = 1.604, <i>p</i> = 0.110).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-022-10261-5#Fig3">3</a> shows the difference in VI between groups. House cats had a significantly greater VI than café cats (<i>F</i> (1,28) = 6.334, <i>p</i> = 0.017). A one-sample t-test revealed that house cats’ VI was greater than 0 (<i>t</i>(13) = 2.522, <i>p</i> = 0.025) whereas that of café cats was not (<i>t</i>(15) = 1.309, <i>p</i> = 0.210).</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Figure 3"><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Figure 3</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-022-10261-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig3_HTML.png?as=webp"/><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="483"/></picture></a></div><p>Violation Index for each group in Exp.1. Red boxplot (left) shows café cat data; blue boxplot (right) shows house cat data.</p></div></figure></div><p>These results indicate that only household cats anticipated a specific cat face upon hearing the cat’s name, suggesting that they matched the stimulus cat’s name and the specific individual. Cats probably learn such name-face relationships by observing third-party interactions; a role for direct receipt of rewards or punishments seems highly unlikely. The ability to learn others’ names would involve a form of social learning. New behaviors or other knowledge can also be acquired by observing other cats<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="John, E. R., Chesler, P., Bartlett, F. &amp; Victor, I. Observation learning in cats. Science 159, 1489–1491 (1968)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR31" id="ref-link-section-d289590345e963">31</a></sup>. Recent study has reported that cats learn new behaviors from humans<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Fugazza, C., Sommese, A., Pogány, Á. &amp; Miklósi, Á. Did we find a copycat? Do as I Do in a domestic cat (Felis catus). Anim. Cogn. 24, 121–131 (2021)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR32" id="ref-link-section-d289590345e967">32</a></sup>. However, we could not identify the mechanism of learning. It is still an open question how cats learn the other cats’ names and faces.</p><p>Environmental differences between house cats and café cats include how often they observe other cats being called and reacting to calls. Contrary to human infants who are able to disambiguate the referent of a new word among many potential ones<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Bion, R. A., Borovsky, A. &amp; Fernald, A. Fast mapping, slow learning: Disambiguation of novel word–object mappings in relation to vocabulary learning at 18, 24, and 30 months. Cognition 126, 39–53 (2013)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR33" id="ref-link-section-d289590345e975">33</a></sup>, cats might not do that at least in this study. Saito et al. showed that café cats did not distinguish their own name from the name of cohabiting cats whereas household cats did so, in a habituation–dishabituation procedure<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Saito, A., Shinozuka, K., Ito, Y. &amp; Hasegawa, T. Domestic cats (Felis catus) discriminate their names from other words. Sci. Rep. 9, 1–8 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR25" id="ref-link-section-d289590345e979">25</a></sup>. We extend this finding by showing that café cats also do not appear to learn the association between another cat’s name and its face.</p><p>We also asked whether the ability to recall another cat’s face upon hearing its name was limited to conspecifics. How about human family members? In Exp.2 we used household cats and re-ran the same experiment using a family member’s name and face.</p><p>A limitation of Exp.1 was that we could not analyze the effect of the duration of cohabiting with the model cat because this differed across cats, and in some cases the information was lacking (i.e., it was hard to track the exact length of time subject and model cats lived together, as the owner quarantined cats that didn&#39;t get along with others.). We predicted that the longer the cat and human had lived together, the stronger the association between name and face would be, due to more opportunities to learn it.</p></div></div></section><section data-title="Experiment 2"><div id="Sec9-section"><h2 id="Sec9">Experiment 2</h2><div id="Sec9-content"><p>The procedure in Exp.2 was almost the same as in Exp.1, but we used human instead of cat stimuli. In view of likely differential exposure to name–face relationships depending on the number of people living together (for example, someone living with a single other person calls that person’s names less often than someone living with multiple others), we took this factor, along with length of time living together, into account in the analysis.</p><h3 id="Sec10">Materials and methods</h3><h4 id="Sec11">Subjects</h4><p>We tested 26 household cats (15 males and 11 females, mean age 5.2 years, <i>SD</i> 3.27 years) living in houses with more than two people. Thirteen cats lived with two-person families, seven with three-person families, four with four-person families, and two with five-person families. Durations of living together ranged between 6 and 180 months (<i>mean</i> 49.79 months, <i>SD</i> 41.50). We did not ask the owner to change water or feeding schedules.</p><h4 id="Sec12">Stimuli</h4><p>The experimental stimuli were the same as Exp.1 except that we used human names and faces instead of cat names and faces, and unfamiliar voices instead of owners’ voices (mean duration 0.80 s, <i>SD</i> 0.30) (i.e., the person calling the name was never the person whose face was presented). As in Exp. 1, we used habitually used names instead of real names to ensure that the cats had the opportunity to learn on a daily basis (e.g., “mother”). All sound files were adjusted to the same volume with Audacity(R). One experimenter took the photos, face-forward and smiling, with a plain background (resolution range x = 304 to 4608, y = 340 to 3512) which were adjusted to the monitor size. If the model could not be present on the day of the experiment the owner sent a family photo by e-mail in advance. In households of more than two people, the models were decided randomly.</p><h4 id="Sec13">Procedure</h4><p>The procedure was the same as in Exp.1.</p><h4 id="Sec14">Analysis</h4><p>We conducted almost the same statistical analysis as in Exp. 1. One cat was tested only on the first trial because she escaped from the room. In total, 32 congruent and 27 incongruent trials were analyzed, after excluding 42 “no attention” trials. We measured duration of attending to the monitor as in Exp.1 and analyzed the data by a linear mixed model (LMM) using a lmer function in a lme4 package version 1.1.10<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Bates, D. et al. Package ‘lme4’. CRAN.R Foundation for Statistical Computing, Vienna, Austria (2012)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR28" id="ref-link-section-d289590345e1045">28</a></sup>. We log-transformed the attention data to better approximate a normal distribution. We log-transformed the duration of living together to reduce variance. Congruency (congruent/incongruent), number of family members (2–5), duration of living together and interactions were entered as fixed factors, with subject identity as random factor. To clarify the effect of duration of living together, we assigned cats to two groups: those living with their humans for above-median durations were the “Long” group, and those with below-median durations were the “Short” group.</p><p>In addition to attention, we analyzed VI. Because we used data from subjects with at least one congruent—incongruent pair, this concerned 16 subjects. We ran a linear model (LM) using a lmer function in a lme4 package version 1.1.10<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Bates, D. et al. Package ‘lme4’. CRAN.R Foundation for Statistical Computing, Vienna, Austria (2012)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR28" id="ref-link-section-d289590345e1052">28</a></sup> with the number of family members (2–5) and duration of living together entered as fixed factors.</p><h3 id="Sec15">Results and discussion</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-022-10261-5#Fig4">4</a> shows time spent attending to the monitor according to the number of family members. The more the number of family members increased, the longer cats attended to the monitor in the incongruent compared to the congruent condition. LMM revealed significant interactions of congruency × number of family members (<span>\({\rm X}^{2}\)</span>(1) = 3.885, <i>p</i> = 0.048) and congruency × number of family member × duration of living together (<span>\({\rm X}^{2}\)</span> (1) = 3.920, <i>p</i> = 0.047). There was no significant main effect of congruency (<span>\({\rm X}^{2}\)</span>(1) = 0.066, <i>p</i> = 0.797). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-022-10261-5#Fig5">5</a> shows attention to the monitor for each group divided by length of time living together, to illustrate the 3-way interaction. The Long group strengthened the tendency (the more family members, the greater attention in the incongruent condition), whereas the short group weakened the tendency, with fewer differences between congruent and incongruent conditions.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Figure 4"><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Figure 4</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-022-10261-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig4_HTML.png?as=webp"/><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="687"/></picture></a></div><p>Time attending to the monitor during the face phase in Exp.2. Red points represent congruent condition; blue points represents incongruent condition. Each line represents a regression line predicted by the LMM. Each ribbon represents the 95% confidence interval. The y-axis is log-transformed.</p></div></figure></div><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Figure 5"><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Figure 5</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-022-10261-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig5_HTML.png?as=webp"/><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="683"/></picture></a></div><p>Time attending to the monitor during the face phase grouped by time living together in Exp.2. We separated time living together into short and long groups by median for convenience because we found a significant interaction of time together, number of family members and congruency. Left panel represents short group; right panel represents long group. Red points represent congruent condition; blue points represent incongruent condition. Each line represents a regression line predicted by the LMM. Each ribbon represents the 95% confidence interval. The y-axis is log-transformed.</p></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41598-022-10261-5#Fig6">6</a> shows the relation between VI and the number of family members. With increasing family size, the VI scores were higher. LM revealed a significant main effect of number of family members (<i>F</i> (1,12) = 6.522, <i>p</i> = 0.025). However, there was no significant interaction between number of family members and duration of living together.</p><div data-test="figure" data-container-section="figure" id="figure-6" data-title="Figure 6"><figure><figcaption><b id="Fig6" data-test="figure-caption-text">Figure 6</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41598-022-10261-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig6_HTML.png?as=webp"/><img aria-describedby="Fig6" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-022-10261-5/MediaObjects/41598_2022_10261_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="420"/></picture></a></div><p>The relationship between Violation Index and number of family members. Grey area indicates the 95% confidence interval predicted by the LM.</p></div></figure></div><p>These results suggested that not all cats predict a familiar human’s face upon hearing that name; we found no main effect of congruency. However, the interaction among congruency, number of family members and time living together indicated that attending to the monitor was affected by time spent together and family environment: the bigger the family, the more cats attended in the incongruent condition and the less they attended in the congruent condition; this was especially so for the cats that had lived longest with their human family. Our interpretation is that cats living with more people have more opportunities to hear names being used than cats living with fewer people, and that living with a family for a longer time increases this experience. In other words, the frequency and number of exposure to the stimuli may make the name–face association more likely.</p><h3 id="Sec16">Ethical statement</h3><p>This study adhered to the ethical guidelines of Kyoto University and Azabu University, and was approved by the Animal Experiments Committee of the Graduate School of Letters, Kyoto University and the Ethics Committee of Azabu University, which follows “Guidelines for Proper Conduct of Animal Experiments” by the Science Council of Japan (2006). Informed consent was obtained from all owners.</p></div></div></section><section data-title="General discussion"><div id="Sec17-section"><h2 id="Sec17">General discussion</h2><div id="Sec17-content"><p>This study examined whether domestic cats learn that human utterances indicate specific objects in their daily life. In Exp.1, cats were presented with the face of another cat from the same household after hearing playback of either a matching or a mismatching name. Results revealed that house cats, but not café cats, attended to the monitor for longer in a name-face incongruent condition than the congruent condition. Upon hearing a cats’ name, the subjects expected the corresponding face. In Exp.2, we used human stimuli to examine whether cats also expect the face of human family members upon hearing their names. Results showed that, although not all cats attended for longer duration in the incongruent condition, the number of household members affected their responses: with more family members, cats attended for longer to the monitor in the incongruent condition. Furthermore, cats that had lived with their family for longer showed the longest durations of attention when the name–face relationship was incongruent. These results might suggest that cats might learn names from observing interactions between humans: a third-party perspective. However, it was not procedurally possible to identify the mechanism of learning in this study. Further study should clarify how cats learned the association. In summary, house cats matched at least their companion cats’ names and faces, and possibly their human family members’ names. This is the first evidence that domestic cats link human utterances and their social referents through every day experiences.</p><p>Acquisition of new signal-meaning pairs requires a high level of social cognition, such as knowing to whom others are talking and paying attention<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Thomas, J. &amp; Kirby, S. Self domestication and the evolution of language. Biol. Philos. 33, 1–30 (2018)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR34" id="ref-link-section-d289590345e1245">34</a></sup>. Many recent reports on social cognition in cats (see review<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Shreve, K. R. V. &amp; Udell, M. A. What’s inside your cat’s head? A review of cat (Felis silvestris catus) cognition research past, present and future. Anim. Cogn. 18, 1195–1206 (2015)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR35" id="ref-link-section-d289590345e1249">35</a></sup>), have shown their high sensitivity to human attentional states<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Ito, Y., Watanave, A., Tagagi, S., Arahori, M. &amp; Saito, A. Cats beg for food from the human who looks at and calls to them: Ability to understand humans’ attentional states. Psychologia 59, 112–120 (2016)." href="#ref-CR20" id="ref-link-section-d289590345e1253">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Vitale, K. R. &amp; Udell, M. A. The quality of being sociable: The influence of human attentional state, population, and human familiarity on domestic cat sociability. Behav. Processes 158, 11–17 (2019)." href="#ref-CR21" id="ref-link-section-d289590345e1253_1">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Koyasu, H. &amp; Nagasawa, M. Recognition of directed-gaze from humans in cats. Jpn. J. Anim. Psychol. 69(2), 3 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR22" id="ref-link-section-d289590345e1256">22</a></sup>. These results of the present study suggest that cats might understand who is talking to whom in everyday situations, which is consistent with those studies. However, it is still unclear how cats learned the name-face association. Further study should address this point.</p><p>In Exp.1, we found a difference between household cats and café cats. Previous studies have reported several behavioral differences between these two groups<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Takagi, S. et al. Cats match voice and face: Cross-modal representation of humans in cats (Felis catus). Anim. Cogn. 22, 901–906 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR24" id="ref-link-section-d289590345e1263">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Saito, A., Shinozuka, K., Ito, Y. &amp; Hasegawa, T. Domestic cats (Felis catus) discriminate their names from other words. Sci. Rep. 9, 1–8 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR25" id="ref-link-section-d289590345e1266">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Bucher, B., Arahori, M., Chijiwa, H., Takagi, S. &amp; Fujita, K. Domestic cats’ reactions to their owner and an unknown individual petting a potential rival. Pet. Behav. Sci. 9, 16–33 (2020)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR36" id="ref-link-section-d289590345e1269">36</a></sup>. In Saito et al. house cats but not café cats were shown to recognize their own name; café cats did not discriminate their own name from names of other cats living in the same environment. Whereas house cats probably learn by observing the reaction of the specific cat whose name was called, café cats are more likely to hear different names called by different guests, making such learning more difficult. Additionally, the number of cats living together might have an influence, as more cats probably means fewer opportunities to learn specific cat name-identity relationships. In our experiment, 75% of café cats tested lived in cafés holding over 30 cats (Supplementary Table <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41598-022-10261-5#MOESM3">S1</a>). In fact, recent research has shown that people with larger social networks show poorer voice recognition<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Lev-Ari, S. People with larger social networks show poorer voice recognition. Q. J. Exp. Psychol. 75, 17470218211030798 (2021)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR37" id="ref-link-section-d289590345e1276">37</a></sup>. To untangle any effects of number of cats living together and fewer opportunities to observe interactions of each cat, cats from cafés of different sizes could be tested.</p><p>In this study we did not take into account the nature of the social relationships between cats. Model cats were randomly chosen among subjects’ cohabitants, without regard to the quality or their relationship with the subject. It could be useful for further studies to examine this factor as well as the influences of experience, environment, and familiarity of model cats on cats’ learning of human utterances.</p><p>We used familiar voices as auditory stimuli in Exp.1 and unfamiliar voices in Exp.2. Cats responded more in incongruent condition in Exp.1 but less clearly so in Exp.2. Perhaps cats will generally show clearer expectancy violation effects when names are called by familiar voices. Some previous studies have shown cat-human communication effects specific to the owner, with little generalization of social cognitive abilities to a stranger<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Galvan, M. &amp; Vonk, J. Man’s other best friend: domestic cats (F. silvestris catus) and their discrimination of human emotion cues. Anim. Cogn. 19, 193–205 (2016)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR18" id="ref-link-section-d289590345e1287">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Pongrácz, P. &amp; Onofer, D. L. Cats show an unexpected pattern of response to human ostensive cues in a series of A-not-B error tests. Anim. Cogn. 23, 681–689 (2020)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR38" id="ref-link-section-d289590345e1290">38</a></sup>. Galvan and Vonk reported that cats differentiate between happy and aggressive expressions of their owner but not a stranger<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Galvan, M. &amp; Vonk, J. Man’s other best friend: domestic cats (F. silvestris catus) and their discrimination of human emotion cues. Anim. Cogn. 19, 193–205 (2016)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR18" id="ref-link-section-d289590345e1294">18</a></sup>. Although Saito et al. reported that cats recognized their own name even when called by a stranger<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Saito, A., Shinozuka, K., Ito, Y. &amp; Hasegawa, T. Domestic cats (Felis catus) discriminate their names from other words. Sci. Rep. 9, 1–8 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR25" id="ref-link-section-d289590345e1298">25</a></sup>, this was not the case for a family member’s name, possibly due to weaker association in the latter situation. To more closely examine whether cats understand common phonetic characteristics in human utterances beyond the owner’s voice, future studies should use strangers’ voices as stimuli.</p><p>We found that cats recognize at least one companion cat’s name and possibly a human family member’s name. It might be asked what motive cats have for remembering names. One possible explanation has to do with competition. For example, a cat might receive food when the owner calls her name but not when she calls another cat’s name. The fact that humans are probably not in competition with cats might explain the weaker association between human names and faces.</p><p>In Experiment 2 we found that cats&#39; looking behavior changed with the length of time living with a human family. However, this length was highly correlated with cat age (Pearson&#39;s r = 0.89). Because cognitive abilities develop with age, the relationship we observed may reflect an age effect. We were unable to isolate these factors in this study; this should be done in future research.</p><p>Previous studies of companion animals’ understanding of human speech have focused on “exceptional” subjects with intensive training and subsequent excellent performance in remembering names of many objects (in dogs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Kaminski, J., Call, J. &amp; Fischer, J. Word learning in a domestic dog: Evidence for&#34; fast mapping&#34;. Science 304, 1682–1683 (2004)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR7" id="ref-link-section-d289590345e1311">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Pilley, J. W. &amp; Reid, A. K. Border collie comprehends object names as verbal referents. Behav. Processes 86, 184 (2011)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR8" id="ref-link-section-d289590345e1314">8</a></sup>). By contrast, recent work revealed that “normal” dogs did not perform as impressively as “exceptional” dogs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Fugazza, C., Dror, S., Sommese, A., Temesi, A. &amp; Miklósi, Á. Word learning dogs (Canis familiaris) provide an animal model for studying exceptional performance. Sci. Rep. 11, 1–9 (2021)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR39" id="ref-link-section-d289590345e1318">39</a></sup>. However, those studies did not clarify whether subjects had a visual image of the referent after hearing a name. Our study demonstrated that cats expect a specific face upon hearing the specific name of a companion. We conducted no training, but exploited cats’ spontaneous learning of relationships between names and faces in their everyday experiences, similar to what human children do. Further study should test whether cats are sensitive to the symbolic nature of some human utterances.</p><p>We did not control or measure affective aspects of hearing the other cat&#39;s or human’s name. Further studies along these lines should consider using emotionally neutral items and their names, removing possible affect-related influences, to shed further light on cats’ ability to link names and objects of more neutral valence.</p><p>In conclusion, house cats linked at least two conspecific housemates’ human-given “names”. They might have a cross-modally integrated concept of another cat’s name and face, similar to humans. This study differs from well-known field studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Seyfarth, R. M., Cheney, D. L. &amp; Marler, P. Monkey responses to three different alarm calls: evidence of predator classification and semantic communication. Science 210, 801–803 (1980)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR2" id="ref-link-section-d289590345e1328">2</a></sup> in that the stimulus utterance was not related to any urgent, potential life or death situation. A remaining question is <i>how</i> cats learn names. Language learning is known to be affected by prosodic aspects. Infant-directed speech characterized by prosodic exaggeration and lexical and syntactic simplification facilitates word learning in infants<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Thiessen, E. D., Hill, E. A. &amp; Saffran, J. R. Infant-directed speech facilitates word segmentation. Infancy 7, 53–71 (2005)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR40" id="ref-link-section-d289590345e1335">40</a></sup>. An fMRI study revealed that dog brains dissociated lexical and emotional prosodic information in human spoken words, similar to humans<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Andics, A. et al. Neural mechanisms for lexical processing in dogs. Science (New York, NY) 353, 1030 (2016)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR9" id="ref-link-section-d289590345e1339">9</a></sup>, which might facilitate language learning. Prosodic factors might affect cats in the same way, which would be interesting for how they learn about the referential nature of human utterances. Another question concerns the evolution of this ability. Some researchers have proposed that (self-) domestication was important in human language evolution<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Thomas, J. &amp; Kirby, S. Self domestication and the evolution of language. Biol. Philos. 33, 1–30 (2018)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR34" id="ref-link-section-d289590345e1343">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Progovac, L. &amp; Benítez-Burraco, A. From physical aggression to verbal behavior: Language evolution and self-domestication feedback loop. Front. Psychol. 10, 2807 (2019)." href="https://www.nature.com/articles/s41598-022-10261-5#ref-CR41" id="ref-link-section-d289590345e1346">41</a></sup>. Future research could address this issue by working with African wildcats, or other domesticated animals such as dogs and horses.</p></div></div></section>
            

            <section data-title="Data availability"><div id="data-availability-section"><h2 id="data-availability">Data availability</h2><p>We attached data on e-letter which contains all data we used this study.</p></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div id="Bib1-section"><h2 id="Bib1">References</h2><div id="Bib1-content"><div data-container-section="references"><ol><li data-counter="1."><p id="ref-CR1">Hurford, J. R. &amp; Hurford, J. R. In <i>The Origins of Grammar: Language in the Light of Evolution II</i> (Oxford University Press, 2012).</p></li><li data-counter="2."><p id="ref-CR2">Seyfarth, R. M., Cheney, D. L. &amp; Marler, P. Monkey responses to three different alarm calls: evidence of predator classification and semantic communication. <i>Science</i> <b>210</b>, 801–803 (1980).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1980Sci...210..801S" aria-label="ADS reference 2">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DyaL3M%2Fls1ekuw%3D%3D" aria-label="CAS reference 2">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1126%2Fscience.7433999" aria-label="Article reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Monkey%20responses%20to%20three%20different%20alarm%20calls%3A%20evidence%20of%20predator%20classification%20and%20semantic%20communication&amp;journal=Science&amp;volume=210&amp;pages=801-803&amp;publication_year=1980&amp;author=Seyfarth%2CRM&amp;author=Cheney%2CDL&amp;author=Marler%2CP">
                    Google Scholar</a> 
                </p></li><li data-counter="3."><p id="ref-CR3">Price, T. <i>et al.</i> Vervets revisited: A quantitative analysis of alarm call structure and context specificity. <i>Sci. Rep.</i> <b>5</b>, 1–11 (2015).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Vervets%20revisited%3A%20A%20quantitative%20analysis%20of%20alarm%20call%20structure%20and%20context%20specificity&amp;journal=Sci.%20Rep.&amp;volume=5&amp;pages=1-11&amp;publication_year=2015&amp;author=Price%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="4."><p id="ref-CR4">Wheeler, B. C. &amp; Fischer, J. Functionally referential signals: A promising paradigm whose time has passed. <i>Evol. Anthropol.</i> <b>21</b>, 195–205 (2012).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1002%2Fevan.21319" aria-label="Article reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Functionally%20referential%20signals%3A%20A%20promising%20paradigm%20whose%20time%20has%20passed&amp;journal=Evol.%20Anthropol.&amp;volume=21&amp;pages=195-205&amp;publication_year=2012&amp;author=Wheeler%2CBC&amp;author=Fischer%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="5."><p id="ref-CR5">Wegdell, F., Hammerschmidt, K. &amp; Fischer, J. Conserved alarm calls but rapid auditory learning in monkey responses to novel flying objects. <i>Nat. Ecol. Evol.</i> <b>3</b>, 1039–1042 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs41559-019-0903-5" aria-label="Article reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 5" href="http://scholar.google.com/scholar_lookup?&amp;title=Conserved%20alarm%20calls%20but%20rapid%20auditory%20learning%20in%20monkey%20responses%20to%20novel%20flying%20objects&amp;journal=Nat.%20Ecol.%20Evol.&amp;volume=3&amp;pages=1039-1042&amp;publication_year=2019&amp;author=Wegdell%2CF&amp;author=Hammerschmidt%2CK&amp;author=Fischer%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="6."><p id="ref-CR6">Suzuki, T. N. Alarm calls evoke a visual search image of a predator in birds. <i>Proc. Natl. Acad. Sci. U. S. A.</i> <b>115</b>, 1541–1545 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXitVWjtbjJ" aria-label="CAS reference 6">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1073%2Fpnas.1718884115" aria-label="Article reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Alarm%20calls%20evoke%20a%20visual%20search%20image%20of%20a%20predator%20in%20birds&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20U.%20S.%20A.&amp;volume=115&amp;pages=1541-1545&amp;publication_year=2018&amp;author=Suzuki%2CTN">
                    Google Scholar</a> 
                </p></li><li data-counter="7."><p id="ref-CR7">Kaminski, J., Call, J. &amp; Fischer, J. Word learning in a domestic dog: Evidence for&#34; fast mapping&#34;. <i>Science</i> <b>304</b>, 1682–1683 (2004).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2004Sci...304.1682K" aria-label="ADS reference 7">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2cXks1Cgsro%3D" aria-label="CAS reference 7">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1126%2Fscience.1097859" aria-label="Article reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Word%20learning%20in%20a%20domestic%20dog%3A%20Evidence%20for%22%20fast%20mapping%22&amp;journal=Science&amp;volume=304&amp;pages=1682-1683&amp;publication_year=2004&amp;author=Kaminski%2CJ&amp;author=Call%2CJ&amp;author=Fischer%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="8."><p id="ref-CR8">Pilley, J. W. &amp; Reid, A. K. Border collie comprehends object names as verbal referents. <i>Behav. Processes</i> <b>86</b>, 184 (2011).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.beproc.2010.11.007" aria-label="Article reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Border%20collie%20comprehends%20object%20names%20as%20verbal%20referents&amp;journal=Behav.%20Processes&amp;volume=86&amp;publication_year=2011&amp;author=Pilley%2CJW&amp;author=Reid%2CAK">
                    Google Scholar</a> 
                </p></li><li data-counter="9."><p id="ref-CR9">Andics, A. <i>et al.</i> Neural mechanisms for lexical processing in dogs. <i>Science (New York, NY)</i> <b>353</b>, 1030 (2016).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2016Sci...353.1030A" aria-label="ADS reference 9">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XhsVemtb7O" aria-label="CAS reference 9">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1126%2Fscience.aaf3777" aria-label="Article reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Neural%20mechanisms%20for%20lexical%20processing%20in%20dogs&amp;journal=Science%20%28New%20York%2C%20NY%29&amp;volume=353&amp;publication_year=2016&amp;author=Andics%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="10."><p id="ref-CR10">Griebel, U. &amp; Oller, D. K. Vocabulary learning in a Yorkshire terrier: Slow mapping of spoken words. <i>PLoS One</i> <b>7</b>, e30182 (2012).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2012PLoSO...730182G" aria-label="ADS reference 10">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38Xjt1Krur8%3D" aria-label="CAS reference 10">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1371%2Fjournal.pone.0030182" aria-label="Article reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Vocabulary%20learning%20in%20a%20Yorkshire%20terrier%3A%20Slow%20mapping%20of%20spoken%20words&amp;journal=PLoS%C2%A0One&amp;volume=7&amp;publication_year=2012&amp;author=Griebel%2CU&amp;author=Oller%2CDK">
                    Google Scholar</a> 
                </p></li><li data-counter="11."><p id="ref-CR11">Dror, S., Miklósi, Á., Sommese, A., Temesi, A. &amp; Fugazza, C. Acquisition and long-term memory of object names in a sample of Gifted Word Learner dogs. <i>R. Soc. Open Sci.</i> <b>8</b>, 210976 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2021RSOS....810976D" aria-label="ADS reference 11">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1098%2Frsos.210976" aria-label="Article reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Acquisition%20and%20long-term%20memory%20of%20object%20names%20in%20a%20sample%20of%20Gifted%20Word%20Learner%20dogs&amp;journal=R.%20Soc.%20Open%20Sci.&amp;volume=8&amp;publication_year=2021&amp;author=Dror%2CS&amp;author=Mikl%C3%B3si%2C%C3%81&amp;author=Sommese%2CA&amp;author=Temesi%2CA&amp;author=Fugazza%2CC">
                    Google Scholar</a> 
                </p></li><li data-counter="12."><p id="ref-CR12">Fugazza, C. <i>et al.</i> Rapid learning of object names in dogs. <i>Sci. Rep.</i> <b>11</b>, 1–11 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs41598-020-79139-8" aria-label="Article reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=Rapid%20learning%20of%20object%20names%20in%20dogs&amp;journal=Sci.%20Rep.&amp;volume=11&amp;pages=1-11&amp;publication_year=2021&amp;author=Fugazza%2CC">
                    Google Scholar</a> 
                </p></li><li data-counter="13."><p id="ref-CR13">Driscoll, C. A., Macdonald, D. W. &amp; O’Brien, S. J. From wild animals to domestic pets, an evolutionary view of domestication. <i>Proc. Natl. Acad. Sci. U. S. A.</i> <b>106</b>(Suppl 1), 9971–9978 (2009).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2009PNAS..106.9971D" aria-label="ADS reference 13">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1MXotFKnsLo%3D" aria-label="CAS reference 13">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1073%2Fpnas.0901586106" aria-label="Article reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=From%20wild%20animals%20to%20domestic%20pets%2C%20an%20evolutionary%20view%20of%20domestication&amp;journal=Proc.%20Natl.%20Acad.%20Sci.%20U.%20S.%20A.&amp;volume=106&amp;issue=Suppl%201&amp;pages=9971-9978&amp;publication_year=2009&amp;author=Driscoll%2CCA&amp;author=Macdonald%2CDW&amp;author=O%27Brien%2CSJ">
                    Google Scholar</a> 
                </p></li><li data-counter="14."><p id="ref-CR14">Driscoll, C. A. <i>et al.</i> The Near Eastern origin of cat domestication. <i>Science</i> <b>317</b>, 519–523 (2007).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2007Sci...317..519D" aria-label="ADS reference 14">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXotFylurY%3D" aria-label="CAS reference 14">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1126%2Fscience.1139518" aria-label="Article reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Near%20Eastern%20origin%20of%20cat%20domestication&amp;journal=Science&amp;volume=317&amp;pages=519-523&amp;publication_year=2007&amp;author=Driscoll%2CCA">
                    Google Scholar</a> 
                </p></li><li data-counter="15."><p id="ref-CR15">Miklósi, Á., Pongrácz, P., Lakatos, G., Topál, J. &amp; Csányi, V. A comparative study of the use of visual communicative signals in interactions between dogs (<i>Canis familiaris</i>) and humans and cats (<i>Felis catus</i>) and humans. <i>J. Comp. Psychol.</i> <b>119</b>, 179 (2005).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1037%2F0735-7036.119.2.179" aria-label="Article reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparative%20study%20of%20the%20use%20of%20visual%20communicative%20signals%20in%20interactions%20between%20dogs%20%28Canis%20familiaris%29%20and%20humans%20and%20cats%20%28Felis%20catus%29%20and%20humans&amp;journal=J.%20Comp.%20Psychol.&amp;volume=119&amp;publication_year=2005&amp;author=Mikl%C3%B3si%2C%C3%81&amp;author=Pongr%C3%A1cz%2CP&amp;author=Lakatos%2CG&amp;author=Top%C3%A1l%2CJ&amp;author=Cs%C3%A1nyi%2CV">
                    Google Scholar</a> 
                </p></li><li data-counter="16."><p id="ref-CR16">Pongrácz, P., Szapu, J. S. &amp; Faragó, T. Cats (<i>Felis silvestris catus</i>) read human gaze for referential information. <i>Intelligence</i> <b>74</b>, 43–52 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.intell.2018.11.001" aria-label="Article reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Cats%20%28Felis%20silvestris%20catus%29%20read%20human%20gaze%20for%20referential%20information&amp;journal=Intelligence&amp;volume=74&amp;pages=43-52&amp;publication_year=2019&amp;author=Pongr%C3%A1cz%2CP&amp;author=Szapu%2CJS&amp;author=Farag%C3%B3%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="17."><p id="ref-CR17">Merola, I., Lazzaroni, M., Marshall-Pescini, S. &amp; Prato-Previde, E. Social referencing and cat-human communication. <i>Anim. Cogn.</i> <b>18</b>, 639–648 (2015).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BC2MvltFCrsQ%3D%3D" aria-label="CAS reference 17">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs10071-014-0832-2" aria-label="Article reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=Social%20referencing%20and%20cat-human%20communication&amp;journal=Anim.%20Cogn.&amp;volume=18&amp;pages=639-648&amp;publication_year=2015&amp;author=Merola%2CI&amp;author=Lazzaroni%2CM&amp;author=Marshall-Pescini%2CS&amp;author=Prato-Previde%2CE">
                    Google Scholar</a> 
                </p></li><li data-counter="18."><p id="ref-CR18">Galvan, M. &amp; Vonk, J. Man’s other best friend: domestic cats (<i>F. silvestris catus</i>) and their discrimination of human emotion cues. <i>Anim. Cogn.</i> <b>19</b>, 193–205 (2016).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs10071-015-0927-4" aria-label="Article reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Man%E2%80%99s%20other%20best%20friend%3A%20domestic%20cats%20%28F.%20silvestris%20catus%29%20and%20their%20discrimination%20of%20human%20emotion%20cues&amp;journal=Anim.%20Cogn.&amp;volume=19&amp;pages=193-205&amp;publication_year=2016&amp;author=Galvan%2CM&amp;author=Vonk%2CJ">
                    Google Scholar</a> 
                </p></li><li data-counter="19."><p id="ref-CR19">Quaranta, A., d’Ingeo, S., Amoruso, R. &amp; Siniscalchi, M. Emotion recognition in cats. <i>Animals</i> <b>10</b>, E1107 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.3390%2Fani10071107" aria-label="Article reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20recognition%20in%20cats&amp;journal=Animals&amp;volume=10&amp;publication_year=2020&amp;author=Quaranta%2CA&amp;author=d%27Ingeo%2CS&amp;author=Amoruso%2CR&amp;author=Siniscalchi%2CM">
                    Google Scholar</a> 
                </p></li><li data-counter="20."><p id="ref-CR20">Ito, Y., Watanave, A., Tagagi, S., Arahori, M. &amp; Saito, A. Cats beg for food from the human who looks at and calls to them: Ability to understand humans’ attentional states. <i>Psychologia</i> <b>59</b>, 112–120 (2016).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.2117%2Fpsysoc.2016.112" aria-label="Article reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Cats%20beg%20for%20food%20from%20the%20human%20who%20looks%20at%20and%20calls%20to%20them%3A%20Ability%20to%20understand%20humans%E2%80%99%20attentional%20states&amp;journal=Psychologia&amp;volume=59&amp;pages=112-120&amp;publication_year=2016&amp;author=Ito%2CY&amp;author=Watanave%2CA&amp;author=Tagagi%2CS&amp;author=Arahori%2CM&amp;author=Saito%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="21."><p id="ref-CR21">Vitale, K. R. &amp; Udell, M. A. The quality of being sociable: The influence of human attentional state, population, and human familiarity on domestic cat sociability. <i>Behav. Processes</i> <b>158</b>, 11–17 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.beproc.2018.10.026" aria-label="Article reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20quality%20of%20being%20sociable%3A%20The%20influence%20of%20human%20attentional%20state%2C%20population%2C%20and%20human%20familiarity%20on%20domestic%20cat%20sociability&amp;journal=Behav.%20Processes&amp;volume=158&amp;pages=11-17&amp;publication_year=2019&amp;author=Vitale%2CKR&amp;author=Udell%2CMA">
                    Google Scholar</a> 
                </p></li><li data-counter="22."><p id="ref-CR22">Koyasu, H. &amp; Nagasawa, M. Recognition of directed-gaze from humans in cats. <i>Jpn. J. Anim. Psychol.</i> <b>69</b>(2), 3 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognition%20of%20directed-gaze%20from%20humans%20in%20cats&amp;journal=Jpn.%20J.%20Anim.%20Psychol.&amp;volume=69&amp;issue=2&amp;publication_year=2019&amp;author=Koyasu%2CH&amp;author=Nagasawa%2CM">
                    Google Scholar</a> 
                </p></li><li data-counter="23."><p id="ref-CR23">Saito, A. &amp; Shinozuka, K. Vocal recognition of owners by domestic cats (<i>Felis catus</i>). <i>Anim. Cogn.</i> <b>16</b>, 685–690 (2013).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs10071-013-0620-4" aria-label="Article reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal%20recognition%20of%20owners%20by%20domestic%20cats%20%28Felis%20catus%29&amp;journal=Anim.%20Cogn.&amp;volume=16&amp;pages=685-690&amp;publication_year=2013&amp;author=Saito%2CA&amp;author=Shinozuka%2CK">
                    Google Scholar</a> 
                </p></li><li data-counter="24."><p id="ref-CR24">Takagi, S. <i>et al.</i> Cats match voice and face: Cross-modal representation of humans in cats (<i>Felis catus</i>). <i>Anim. Cogn.</i> <b>22</b>, 901–906 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs10071-019-01265-2" aria-label="Article reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Cats%20match%20voice%20and%20face%3A%20Cross-modal%20representation%20of%20humans%20in%20cats%20%28Felis%20catus%29&amp;journal=Anim.%20Cogn.&amp;volume=22&amp;pages=901-906&amp;publication_year=2019&amp;author=Takagi%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="25."><p id="ref-CR25">Saito, A., Shinozuka, K., Ito, Y. &amp; Hasegawa, T. Domestic cats (<i>Felis catus</i>) discriminate their names from other words. <i>Sci. Rep.</i> <b>9</b>, 1–8 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2019NatSR...9....1S" aria-label="ADS reference 25">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Domestic%20cats%20%28Felis%20catus%29%20discriminate%20their%20names%20from%20other%20words&amp;journal=Sci.%20Rep.&amp;volume=9&amp;pages=1-8&amp;publication_year=2019&amp;author=Saito%2CA&amp;author=Shinozuka%2CK&amp;author=Ito%2CY&amp;author=Hasegawa%2CT">
                    Google Scholar</a> 
                </p></li><li data-counter="26."><p id="ref-CR26">Audacity team. Audacity® software is copyright © 1999–2021 Audacity Team. The name Audacity® is a registered trademark. (2019).</p></li><li data-counter="27."><p id="ref-CR27">R Core Team. <i>R: A Language and Environment for Statistical Computing</i>. (R Foundation for Statistical Computing, 2018). <a href="https://www.R-project.org/">https://www.R-project.org/</a><u>.</u></p></li><li data-counter="28."><p id="ref-CR28">Bates, D. <i>et al</i>. Package ‘lme4’. <i>CRAN.R Foundation for Statistical Computing, Vienna, Austria</i> (2012).</p></li><li data-counter="29."><p id="ref-CR29">Fox, J. &amp; Weisberg, S. In <i>An R Companion to Applied Regression</i> (Sage Publications, 2018).</p></li><li data-counter="30."><p id="ref-CR30">Lenth, R. V. Lenth emmeans: Estimated Marginal Means, aka Least-Squares Means.R package version 1.5.3. (2020).</p></li><li data-counter="31."><p id="ref-CR31">John, E. R., Chesler, P., Bartlett, F. &amp; Victor, I. Observation learning in cats. <i>Science</i> <b>159</b>, 1489–1491 (1968).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=1968Sci...159.1489J" aria-label="ADS reference 31">ADS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://www.nature.com/articles/cas-redirect/1:STN:280:DyaF1M3lt1ylsw%3D%3D" aria-label="CAS reference 31">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1126%2Fscience.159.3822.1489" aria-label="Article reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=Observation%20learning%20in%20cats&amp;journal=Science&amp;volume=159&amp;pages=1489-1491&amp;publication_year=1968&amp;author=John%2CER&amp;author=Chesler%2CP&amp;author=Bartlett%2CF&amp;author=Victor%2CI">
                    Google Scholar</a> 
                </p></li><li data-counter="32."><p id="ref-CR32">Fugazza, C., Sommese, A., Pogány, Á. &amp; Miklósi, Á. Did we find a copycat? Do as I Do in a domestic cat (<i>Felis catus</i>). <i>Anim. Cogn.</i> <b>24</b>, 121–131 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs10071-020-01428-6" aria-label="Article reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=Did%20we%20find%20a%20copycat%3F%20Do%20as%20I%20Do%20in%20a%20domestic%20cat%20%28Felis%20catus%29&amp;journal=Anim.%20Cogn.&amp;volume=24&amp;pages=121-131&amp;publication_year=2021&amp;author=Fugazza%2CC&amp;author=Sommese%2CA&amp;author=Pog%C3%A1ny%2C%C3%81&amp;author=Mikl%C3%B3si%2C%C3%81">
                    Google Scholar</a> 
                </p></li><li data-counter="33."><p id="ref-CR33">Bion, R. A., Borovsky, A. &amp; Fernald, A. Fast mapping, slow learning: Disambiguation of novel word–object mappings in relation to vocabulary learning at 18, 24, and 30 months. <i>Cognition</i> <b>126</b>, 39–53 (2013).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1016%2Fj.cognition.2012.08.008" aria-label="Article reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%20mapping%2C%20slow%20learning%3A%20Disambiguation%20of%20novel%20word%E2%80%93object%20mappings%20in%20relation%20to%20vocabulary%20learning%20at%2018%2C%2024%2C%20and%2030%20months&amp;journal=Cognition&amp;volume=126&amp;pages=39-53&amp;publication_year=2013&amp;author=Bion%2CRA&amp;author=Borovsky%2CA&amp;author=Fernald%2CA">
                    Google Scholar</a> 
                </p></li><li data-counter="34."><p id="ref-CR34">Thomas, J. &amp; Kirby, S. Self domestication and the evolution of language. <i>Biol. Philos.</i> <b>33</b>, 1–30 (2018).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs10539-018-9612-8" aria-label="Article reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=Self%20domestication%20and%20the%20evolution%20of%20language&amp;journal=Biol.%20Philos.&amp;volume=33&amp;pages=1-30&amp;publication_year=2018&amp;author=Thomas%2CJ&amp;author=Kirby%2CS">
                    Google Scholar</a> 
                </p></li><li data-counter="35."><p id="ref-CR35">Shreve, K. R. V. &amp; Udell, M. A. What’s inside your cat’s head? A review of cat (<i>Felis silvestris catus</i>) cognition research past, present and future. <i>Anim. Cogn.</i> <b>18</b>, 1195–1206 (2015).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs10071-015-0897-6" aria-label="Article reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 35" href="http://scholar.google.com/scholar_lookup?&amp;title=What%E2%80%99s%20inside%20your%20cat%E2%80%99s%20head%3F%20A%20review%20of%20cat%20%28Felis%20silvestris%20catus%29%20cognition%20research%20past%2C%20present%20and%20future&amp;journal=Anim.%20Cogn.&amp;volume=18&amp;pages=1195-1206&amp;publication_year=2015&amp;author=Shreve%2CKRV&amp;author=Udell%2CMA">
                    Google Scholar</a> 
                </p></li><li data-counter="36."><p id="ref-CR36">Bucher, B., Arahori, M., Chijiwa, H., Takagi, S. &amp; Fujita, K. Domestic cats’ reactions to their owner and an unknown individual petting a potential rival. <i>Pet. Behav. Sci.</i> <b>9</b>, 16–33 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=Domestic%20cats%E2%80%99%20reactions%20to%20their%20owner%20and%20an%20unknown%20individual%20petting%20a%20potential%20rival&amp;journal=Pet.%20Behav.%20Sci.&amp;volume=9&amp;pages=16-33&amp;publication_year=2020&amp;author=Bucher%2CB&amp;author=Arahori%2CM&amp;author=Chijiwa%2CH&amp;author=Takagi%2CS&amp;author=Fujita%2CK">
                    Google Scholar</a> 
                </p></li><li data-counter="37."><p id="ref-CR37">Lev-Ari, S. People with larger social networks show poorer voice recognition. <i>Q. J. Exp. Psychol.</i> <b>75</b>, 17470218211030798 (2021).</p></li><li data-counter="38."><p id="ref-CR38">Pongrácz, P. &amp; Onofer, D. L. Cats show an unexpected pattern of response to human ostensive cues in a series of A-not-B error tests. <i>Anim. Cogn.</i> <b>23</b>, 681–689 (2020).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1007%2Fs10071-020-01373-4" aria-label="Article reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Cats%20show%20an%20unexpected%20pattern%20of%20response%20to%20human%20ostensive%20cues%20in%20a%20series%20of%20A-not-B%20error%20tests&amp;journal=Anim.%20Cogn.&amp;volume=23&amp;pages=681-689&amp;publication_year=2020&amp;author=Pongr%C3%A1cz%2CP&amp;author=Onofer%2CDL">
                    Google Scholar</a> 
                </p></li><li data-counter="39."><p id="ref-CR39">Fugazza, C., Dror, S., Sommese, A., Temesi, A. &amp; Miklósi, Á. Word learning dogs (<i>Canis familiaris</i>) provide an animal model for studying exceptional performance. <i>Sci. Rep.</i> <b>11</b>, 1–9 (2021).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1038%2Fs41598-020-79139-8" aria-label="Article reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 39" href="http://scholar.google.com/scholar_lookup?&amp;title=Word%20learning%20dogs%20%28Canis%20familiaris%29%20provide%20an%20animal%20model%20for%20studying%20exceptional%20performance&amp;journal=Sci.%20Rep.&amp;volume=11&amp;pages=1-9&amp;publication_year=2021&amp;author=Fugazza%2CC&amp;author=Dror%2CS&amp;author=Sommese%2CA&amp;author=Temesi%2CA&amp;author=Mikl%C3%B3si%2C%C3%81">
                    Google Scholar</a> 
                </p></li><li data-counter="40."><p id="ref-CR40">Thiessen, E. D., Hill, E. A. &amp; Saffran, J. R. Infant-directed speech facilitates word segmentation. <i>Infancy</i> <b>7</b>, 53–71 (2005).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.1207%2Fs15327078in0701_5" aria-label="Article reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=Infant-directed%20speech%20facilitates%20word%20segmentation&amp;journal=Infancy&amp;volume=7&amp;pages=53-71&amp;publication_year=2005&amp;author=Thiessen%2CED&amp;author=Hill%2CEA&amp;author=Saffran%2CJR">
                    Google Scholar</a> 
                </p></li><li data-counter="41."><p id="ref-CR41">Progovac, L. &amp; Benítez-Burraco, A. From physical aggression to verbal behavior: Language evolution and self-domestication feedback loop. <i>Front. Psychol.</i> <b>10</b>, 2807 (2019).</p><p><a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" href="https://doi.org/10.3389%2Ffpsyg.2019.02807" aria-label="Article reference 41">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 41" href="http://scholar.google.com/scholar_lookup?&amp;title=From%20physical%20aggression%20to%20verbal%20behavior%3A%20Language%20evolution%20and%20self-domestication%20feedback%20loop&amp;journal=Front.%20Psychol.&amp;volume=10&amp;publication_year=2019&amp;author=Progovac%2CL&amp;author=Ben%C3%ADtez-Burraco%2CA">
                    Google Scholar</a> 
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" href="https://citation-needed.springer.com/v2/references/10.1038/s41598-022-10261-5?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div id="Ack1-section"><h2 id="Ack1">Acknowledgements</h2><p>The authors acknowledge with thanks all owners and cats who volunteered in this study. The authors also wish to thank James R. Anderson for editing the article.</p></div></section><section data-title="Funding"><div id="Fun-section"><h2 id="Fun">Funding</h2><p>This study was financially supported by Grants-in-aid for Scientific Research (KAKENHI) No. 17J08974, No. 19J01485 to S. Takagi, No. JP16J1034 to M. Arahori, No. JP16J08691 to H. Chijiiwa, No. 25118003 to A. Saito and Nos. 25240020, 26119514, 16H01505, 15K12047, 25118002, and 16H06301 to K. Fujita from the Japan Society for the Promotion of Science (JSPS).</p></div></section><section aria-labelledby="author-information" data-title="Author information"><div id="author-information-section"><h2 id="author-information">Author information</h2><div id="author-information-content"><h3 id="affiliations">Affiliations</h3><ol><li id="Aff1"><p>Department of Psychology, Graduate School of Letters, Kyoto University, Yoshida-honmachi, Sakyo, Kyoto, 606-8501, Japan</p><p>Saho Takagi, Hitomi Chijiiwa, Kazuo Fujita &amp; Hika Kuroshima</p></li><li id="Aff2"><p>Department of Animal Science and Biotechnology, Azabu University, 1-17-71, Fuchinobe, Chuo-ku, Sagamihara, Kanagawa, 252-5201, Japan</p><p>Saho Takagi, Hikari Koyasu, Miho Nagasawa &amp; Takefumi Kikusui</p></li><li id="Aff3"><p>Japan Society for the Promotion of Science, 5-3-1, Chiyoda-ku, Tokyo, 102-0083, Japan</p><p>Saho Takagi</p></li><li id="Aff4"><p>Department of Psychology, Faculty of Human Sciences, Sophia University, 7-1, Kioicho, Chiyoda-ku, Tokyo, 102-8554, Japan</p><p>Atsuko Saito</p></li><li id="Aff5"><p>Research and Development Section, Anicom Speciality Medical Institute Inc., 2-6-3 Chojamachi 5F, Yokohamashi-Nakaku, Kanagawaken, 231-0033, Japan</p><p>Minori Arahori</p></li><li id="Aff6"><p>Wildlife Research Center, Kyoto University, 2-24 Tanaka-Sekiden-cho, Sakyo, Kyoto, 606-8203, Japan</p><p>Minori Arahori</p></li></ol><h3 id="contributions">Contributions</h3><p>S.T. participated in the design of the study, collected and analyzed the data, and drafted the manuscript as corresponding author. M.A, H.C. and A.S. collected data. A.S., K.F., M.N. T.K., and H.K. gave S.T. suggestions about experiment design, data analysis, discussions and helped draft the manuscript. All authors critically revised the report, commented on drafts of the manuscript, and approved the final report.</p><h3 id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:sa-takagi@azabu-u.ac.jp">Saho Takagi</a>.</p></div></div></section><section data-title="Ethics declarations"><div id="ethics-section"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
                <h3 id="FPar1">Competing interests</h3>
                <p>The authors declare no competing interests.</p>
              
            </div></div></section><section data-title="Additional information"><div id="additional-information-section"><h2 id="additional-information">Additional information</h2><div id="additional-information-content"><h3>Publisher&#39;s note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Supplementary Information"><div id="Sec18-section"><h2 id="Sec18">Supplementary Information</h2></div></section><section data-title="Rights and permissions"><div id="rightslink-section"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article&#39;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#39;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Cats%20learn%20the%20names%20of%20their%20friend%20cats%20in%20their%20daily%20lives&amp;author=Saho%20Takagi%20et%20al&amp;contentID=10.1038%2Fs41598-022-10261-5&amp;copyright=The%20Author%28s%29&amp;publication=2045-2322&amp;publicationDate=2022-04-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div id="article-info-section"><h2 id="article-info">About this article</h2><div id="article-info-content"><div><p><a data-crossmark="10.1038/s41598-022-10261-5" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41598-022-10261-5" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"/></a></p><div><h3 id="citeas">Cite this article</h3><p>Takagi, S., Saito, A., Arahori, M. <i>et al.</i> Cats learn the names of their friend cats in their daily lives.
                    <i>Sci Rep</i> <b>12, </b>6155 (2022). https://doi.org/10.1038/s41598-022-10261-5</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" href="https://citation-needed.springer.com/v2/references/10.1038/s41598-022-10261-5?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2021-10-01">01 October 2021</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2022-03-04">04 March 2022</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2022-04-13">13 April 2022</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1038/s41598-022-10261-5</span></p></li></ul></div></div></div></div></section>

            

            
                <section data-title="Comments"><div id="article-comments-section"><h2 id="article-comments">Comments</h2><p>By submitting a comment you agree to abide by our <a href="https://www.nature.com/info/tandc.html">Terms</a> and <a href="https://www.nature.com/info/community-guidelines.html">Community Guidelines</a>. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.</p></div></section>
                
            

            </div></div>
  </body>
</html>
