<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://giannirosato.com/blog/post/nvenc-v-qsv/">Original</a>
    <h1>Nvenc vs. QSV: Who Has the Best Hardware AV1 Encoder?</h1>
    
    <div id="readability-page-1" class="page"><div>
    <div>
        <p>Who Has the Best Hardware AV1 Encoder?</p>
        <p><a href="https://giannirosato.com/blog/">back</a></p><p>Apr 15, 2023</p>
        
        <p><a href="https://giannirosato.com/blog/tag/projects/">projects</a>
        
        <a href="https://giannirosato.com/blog/tag/av1/">av1</a>
        
    </p></div>
    <p><em>Please be aware that some images may not load on this page unless your browser supports JPEG-XL.</em></p>
<h2>Why?</h2>
<p>Nobody really knows which hardware accelerated AV1 encoder is better right now.</p>
<p>There are other resources available, but they don&#39;t use good metrics. Everyone&#39;s favorite bad, obsolete, easily tricked metric is VMAF, which was good for a time but simply isn&#39;t good anymore. You can check out <a href="https://nitter.kitsuna.net/jonsneyers/status/1573371624132419585">this Twitter thread</a> to see what I&#39;m talking about; these aren&#39;t edge cases either. You can produce a &#34;better quality&#34; video according to VMAF using Contrast Adaptive Sharpening filters which inherently do not improve video fidelity in any way.</p>
<p>Other metrics are PSNR, SSIM, MS-SSIM, SSIMULACRA, Butteraugli, &amp; SSIMULACRA2. PSNR doesn&#39;t accurately represent our visual system as humans, SSIM was good for its time but is now old and also provides inconsistent/misleading results, &amp; MS-SSIM does better but also doesn&#39;t correlate as well with our visual systems as newer metrics. The distance-based Butteraugli metric isn&#39;t bad, but SSIMULACRA2 is currently considered the gold standard for a visual quality metric.</p>
<p>You can read more about why this metric is great <a href="https://github.com/cloudinary/ssimulacra2">here</a>, but SSIMULACRA2 (ssimu2) is, in my opinion, the only way to really measure subjective visual quality right now. The scores it produces correlate best with my eyes, and many others agree.</p>
<p>Here&#39;s how SSIMULACRA2 assesses quality:</p>
<blockquote>
<p>Returns a score in range -inf..100, which correlates to subjective visual quality scores as follows:</p>
<ul>
<li>30 = low quality. This corresponds to the p10 worst output of mozjpeg -quality 30.</li>
<li>50 = medium quality. This corresponds to the average output of cjxl -q 40 or mozjpeg -quality 40, or the p10 output of cjxl -q 50 or mozjpeg -quality 60.</li>
<li>70 = high quality. This corresponds to the average output of cjxl -q 65 or mozjpeg -quality 70, p10 output of cjxl -q 75 or mozjpeg -quality 80.</li>
<li>90 = very high quality. Likely impossible to distinguish from the original when viewed at 1:1 from a normal viewing distance. This corresponds to the average output of mozjpeg -quality 95 or the p10 output of cjxl -q 95.</li>
</ul>
</blockquote>
<p>In my <a href="https://giannirosato.com/blog/post/image-comparison/">Image Codec Comparison</a> benchmark, I said &#34;Because of the irrelevance of negative SSIMULACRA2 scores, I stopped the vertical axis on the graph(s) at 0.&#34; I still maintain this logic was correct for the image benchmark, but with livestreams, it isn&#39;t unusual to see a low bitrate stream due to a poor network connection on either a streamer&#39;s end or your own. This test is also more about how the encoders perform at certain target bitrates, as mentioned above, and since the goal isn&#39;t necessarily always to target a certain quality like it is with images, I think some of the less relevant data (eg target 500kb/s) is still important to include because of how it frames the more useful data (eg the 2000-6000kb/s range popular with streamers).</p>
<p>Anyway, I&#39;ve seen testing from many using VMAF, and their conclusions are varied relative to which hardware encoder is the best. <a href="https://rigaya.github.io/vq_results/">These tests</a> are more thorough than mine, but aren&#39;t accompanied by much explanation or any VAAPI results for Linux. They also use VMAF, so everything must be taken with a grain of salt. I still think they&#39;re useful in judging the quality of many hardware encoders that I haven&#39;t tested here (like AMD&#39;s stuff) so I&#39;ll still recommend you go take a look if you&#39;re interested. So, what did we find in our own testing?</p>
<h2>Methodology (Gaming Corpus)</h2>
<p>In order to understand who uses hardware encoders and where, look no further than livestreaming. Livestreaming can be done via OBS, and you have a choice to decide between CBR, VBR, &amp; CQP encoding. Many choose to opt for CBR or VBR, since your connection is most likely the bottleneck &amp; you can target a certain bitrate. I&#39;d personally opt for VBR so that the encoder can intelligently allocate more or less bits when necessary while still targeting a specific bitrate on average. This also allows us to get better overall quality per bit, so the viewer can see a higher quality stream while using less data on average. It is more reliable for data usage &amp; bandwidth than CQP which targets quality, &amp; more efficient than CBR which will give every scene the same amount of bits no matter its content.</p>
<p>With help from a couple friends online (namely <a href="https://github.com/Bl4cKn1gh7">BlacKnight</a> &amp; <a href="https://github.com/Maikurosofuto">Maik</a>), I tested hardware &amp; software encoders from target 500kb/s to target 8500kb/s in increments of 500. This gave us 16 data points per encoder per clip to work with. I used two clips from DERF&#39;s Test Media - namely the CSGO clip &amp; the Minecraft clip under &#34;Gaming&#34; - and transcoded them to h264 with <code>crf 1</code> to provide the encoders with near mathematically lossless video that they could effectively decode. The loss introduced by reencoding at such high quality is miniscule &amp; should have a nearly undetectable effect on the benchmark results.</p>
<p>Here&#39;s a screenshot from the CSGO clip:</p>
<p><img alt="csgo" src="https://giannirosato.com/static/images/csgo.jxl"/></p>
<p>&amp; here&#39;s one from the Minecraft clip:</p>
<p><img alt="minecraft" src="https://giannirosato.com/static/images/minecraft.jxl"/></p>
<p>All hardware encoders were on their highest effort preset (-compression_level 7) for optimal quality. It didn&#39;t have a massive effect on speed in my testing.</p>
<p>The encoders tested for the CSGO clip were the following:</p>
<ul>
<li>h264 (UHD770 VAAPI)</li>
<li>h264 (Arc A770 VAAPI)</li>
<li>HEVC (UHD770 VAAPI)</li>
<li>HEVC (Arc A770 VAAPI)</li>
<li>HEVC (Arc A770 VAAPI, 10 bit)</li>
<li>VP9 (UHD770 VAAPI)</li>
<li>x264 (slower preset)</li>
<li>SVT-AV1 (preset 6, 10 bit)</li>
<li>SVT-AV1 (preset 8)</li>
<li>SVT-AV1 (preset 8, 10 bit)</li>
<li>Nvenc AV1 (RTX 4090)</li>
<li>Nvenc AV1 (RTX 4090, 10 bit)</li>
<li>QSV AV1 (Arc A770)</li>
<li>QSV AV1 (Arc A770, 10 bit)</li>
</ul>
<p>The encoders tested for the Minecraft clip were the following:</p>
<ul>
<li>x265 (medium preset, 10 bit)</li>
<li>SVT-AV1 (preset 4, 10 bit)</li>
<li>SVT-AV1 (preset 6, 10 bit)</li>
<li>SVT-AV1 (preset 8, 10 bit)</li>
<li>Nvenc AV1 (RTX 4090, 10 bit)</li>
<li>QSV AV1 (Arc A770, 10 bit)</li>
</ul>
<p>You may be wondering why there is such an emphasis on 10 bit encoding despite the 8 bit source. In my testing, even just with my eyes, encoding to 10 bit video with most lossy video codecs (HEVC &amp; AV1 in particular) yields better coding efficiency than encoding to 8 bit. I can&#39;t tell you exactly why, but I&#39;m sure others on the <a href="https://discord.gg/Ecu428C">AV1 Community Discord</a> can.</p>
<p>You may also be wondering about the lack of AMD numbers. I have a Radeon Rx 6600xt &amp; myself and others found it very difficult to properly test AMD&#39;s hardware encoders. You can look at the results I linked above that report VMAF numbers - the massive discrepancy in AMD&#39;s AV1 performance compared to other AV1 encoders should be enough to let you know how good AMD&#39;s results are.</p>
<h2>Results</h2>
<p>Finally, we come to the graphs. It is also worth noting that the default GOP size for the hardware encoders is around 300 frames; leaving it on &#34;auto&#34; in FFmpeg generally yielded <em>slightly</em> better results by a few fractions of a point in ssimu2, so that is what we did.</p>
<h3>CSGO</h3>
<p>Here is a graph for target bitrate (VBR) with the CSGO clip:</p>
<p><img alt="csgo_results" src="https://giannirosato.com/static/images/csgo_results.svg"/></p>
<p>And a log scale version that we can use to see discrepancies better:</p>
<p><img alt="csgo_results_log" src="https://giannirosato.com/static/images/csgo_results_log.svg"/></p>
<p>This is kind of a lot to unpack, as there are a considerable number of overlapping lines here. I&#39;ve attached a CSV file <a href="https://github.com/gianni-rosato/hwenc-ssimu2-plotter/blob/main/csgo_results.csv">here</a> so that you can visualize this data on your own in any way you please, but I think we can come to a couple interesting conclusions given this current visualization. For this clip:</p>
<ul>
<li>There are clear, marked differences in coding efficiency depending on the encoder even if the codec is the same. So, just because your video is h264 doesn&#39;t mean that is was encoded as efficiently as every other h264 video. You can see here that the Arc A770&#39;s h264 encoder yields better quality per bit than the h264 encoder that&#39;s part of the iGPU on my 13700k. The fixed function hardware Intel is deploying for these two implementations is distinct, and Arc&#39;s is clearly better. The software x264 encoder that runs directly on the CPU is certainly slower, but you can see how that pays off in spades.</li>
<li>SVT-AV1 is in a class by itself. Preset 8 dominates the hardware encoders across the board by a not insignificant margin, &amp; Preset 6 pulls ahead even further. While hardware encoders are very fast and a gem for streaming, software encoders still take home the efficiency crown.</li>
<li>NVENC AV1 is more efficient than QSV AV1 on Arc, even if the advantage is slight. Even when comparing 8 bit NVENC to 10 bit QSV, NVENC still manages to pull ahead ever so slightly.</li>
</ul>
<p>If you came here to rub salt in Arc users&#39; wounds, I wouldn&#39;t say the advantage is significant enough to warrant purchasing a 40-series card over an Arc card <em>solely</em> for encoding especially considering the price discrepancy. However, the advantage is still there, and shouldn&#39;t be ignored. My takeaway is if you have more money than you know what to do with, don&#39;t buy an Arc GPU for encoding if you have a 40-series card from Nvidia already. Spend it on a nice bowl of ramen instead (or a couple nice bowls of ramen, realistically).</p>
<h3>Minecraft</h3>
<p>Let&#39;s see what else we can find looking at the Minecraft results:</p>
<p><img alt="minecraft_results" src="https://giannirosato.com/static/images/minecraft_results.svg"/></p>
<p>Here&#39;s another log scale version:</p>
<p><img alt="minecraft_results_log" src="https://giannirosato.com/static/images/minecraft_results_log.svg"/></p>
<p>This one is more stripped down, &amp; everything is easier to see. Here are my takeaways:</p>
<ul>
<li>Now that we have x265 in the mix, we can see NVENC beat it across the board &amp; QSV came close. This is pretty impressive, as x265 medium isn&#39;t a fast setting by any stretch of the imagination.</li>
<li>SVT-AV1 preset 4&#39;s performance leaves us with some clues about our final conclusion, seeing as software AV1 encoding (even with SVT-AV1, which doesn&#39;t produce as good quality per bit as AOM encoder forks like aom-av1-lavish) continues to pull ahead significantly the more time &amp; effort we allow it to use. It may be possible to stream using SVT-AV1 preset 8 on higher end systems right now, but more likely you&#39;d be using preset 9 or 10, which is where I think SVT-AV1 is probably on par with hardware encoders. As CPUs become more powerful, this will become easier, &amp; it is clear that dedicated streaming hardware may continue to provide value for streamers who can afford it.</li>
</ul>
<h2>Conclusion</h2>
<h3>Who Won?</h3>
<p>Nvidia&#39;s hardware acceleration for AV1 video encoding is superior in these two clips. It is a tiny margin, and it is important to judge subjective visual quality for yourself when evaluating tiny discrepancies like this, but SSIMULACRA2 says that Nvidia has the better solution. I don&#39;t think that&#39;s the most important takeaway here, though, considering our peek at the potential software encoders have.</p>
<h3>Software vs hardware?</h3>
<p>Software encoders are run on the CPU without any fixed function hardware designed around any specific codec. Hardware accelerated video encoding takes place on specialized ASICs that are attached to most consumer GPUs, &amp; encoding with hardware acceleration means the encoder can also make use of a GPU&#39;s highly parallel compute to further accelerate certain video encoding functions. Developing these ASICs is expensive, and dedicating valuable silicon real estate on GPUs to such features must prove to be well worth a company&#39;s time and money. Nvidia &amp; Intel both seem to agree that AV1 has enough of a future to make it financially sane to put fast dedicated hardware in the hands of every consumer who has one of their latest GPUs, which is pretty cool.</p>
<p>The issue regarding efficiency is that this fixed function hardware is designed to be fast above all else, &amp; implementing less important codec features into this hardware can inflate costs without helping efficiency meaningfully. In software, while slower, encoders can pick and choose to enable/disable whatever features they like based on user specified parameters or internal defaults for each preset. This makes the use cases for both solutions very different.</p>
<h3>When should I use hardware encoders?</h3>
<p>As I mentioned earlier, livestreaming is where hardware accelerated video encoding is most popular. Nvidia has turned their NVENC hardware into a differentiating feature because of the popularity of livestreaming, &amp; as sites like Twitch &amp; YouTube roll out AV1 for the masses, livestreamers will appreciate having a powerful hardware implementation for the more efficient codec to stream with given their hardware readily supports it.</p>
<p>Hardware encoding can also be great for screen recording, capturing clips in games, and trancoding media on the fly for a media server or something similar. When speed is the priority over coding efficiency, hardware encoders are a fantastic choice.</p>
<p>Here are some places where you <em>probably shouldn&#39;t</em> use hardware encoders:</p>
<ul>
<li>Transcoding media you legally own for later consumption (unless you are burdened by a time limit or it is too painful to encode at less than real time)</li>
<li>Re-encoding home video that needs to take up less space somewhere or needs to be shared easily</li>
<li>Producing a final copy of a video project of some sort to be shared with others</li>
<li>Illegally releasing pirated media that has been transcoded to achieve a smaller filesize for viewer convenience (I cannot recommend this regardless)</li>
</ul>
<p>You should never transcode something that has already been transcoded unless you have to - it is best to search for the source. If you have a BluRay collection that you transcoded to h264 ten years ago, don&#39;t go re-encoding it to AV1 now - find the source &amp; do it over, else you risk preserving artifacts from the original encode. And do it with a software encoder. If you&#39;re exporting a video for a project or something similar, it may be worth encoding to a lossless or near lossless file &amp; reencoding as efficiently as possible later when you can afford the time so you can optimize for quality per bit.</p>
<p>SVT-AV1 is great for that sweet spot balance of speed &amp; efficiency for AV1 video. Rav1e, my personal favorite software AV1 encoder, isn&#39;t the best, but it is great for high fidelity &amp; should never segfault. <a href="https://github.com/Clybius/aom-av1-lavish">aom-av1-lavish</a> is the reigning champ for coding efficiency in a software encoder. Use at speed 4 or lower for excellent results; they will be results you&#39;ll have to wait very long for, though. Tools like <a href="https://github.com/master-of-zen/Av1an">av1an</a> can help.</p>
<p>You can try my SVT-AV1 GUI, <a href="https://github.com/gianni-rosato/aviator">Aviator</a>, if you&#39;d like to mess around with SVT-AV1 without using the command line. <a href="https://github.com/gianni-rosato/aviator/tree/rAV1ator">rAV1ator</a> it its cousin, powered by av1an &amp; rav1e instead of SVT-AV1 &amp; FFmpeg. Please give them a look if you&#39;re interested in getting started with AV1. Once you&#39;d like to dive down the rabbit hole, feel free to look into <a href="https://rentry.co/av1">this article</a> for a detailed guide written by a talented encoder. Thanks for reading!</p>
<p><a href="https://elk.zone/disobey.net/@gianni">Mastodon</a> | <a href="https://matrix.to/#/@computerbustr:matrix.org">Matrix</a> | <a href="https://radixproject.org/">The Radix Project</a></p>
</div></div>
  </body>
</html>
