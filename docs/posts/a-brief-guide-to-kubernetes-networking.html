<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ergomake.dev/blog/kubernetes-networking-guide/">Original</a>
    <h1>A brief guide to Kubernetes networking</h1>
    
    <div id="readability-page-1" class="page"><article><p>When I first encountered Kubernetes, its networking model seemed <a href="https://lab.cccb.org/en/arthur-c-clarke-any-sufficiently-advanced-technology-is-indistinguishable-from-magic/">sufficiently advanced to look like magic</a>. The problem is: I <em>hate</em> magic, and I hope you do too.</p>
<p>In this blog post, I hope to ruin your esoteric expectations about Kubernetes Networking.</p>
<p>This is the brief guide to Kubernetes networking I wish I had when I started writing <a href="https://ergomake.dev"><code>Ergomake</code></a>.</p>
<p>We&#39;ll start this post by explaining what an <a href="https://ergomake.dev"><code>Ergomake</code></a> environment is and what happens within our Kubernetes cluster when you run <code>ergomake up</code>.</p>
<p>Then, I&#39;ll go through each of those steps manually and explain everything that Kubernetes itself does when you create pods and services.</p>
<p>If you want to follow along, I&#39;d recommend creating your own three-node Minikube cluster and giving it quite a bit of memory, just in case.</p>

<h2>What is an Ergomake environment?</h2>
<p>At <a href="https://ergomake.dev">Ergomake</a>, we take your <code>docker-compose.yml</code> files, run them on &#34;the cloud&#34;, and make everything look like <code>localhost</code>.</p>
<p>Imagine you have the following <code>docker-compose.yml</code> file, for example.</p>

<p>When you run <code>ergomake up</code> within that file&#39;s folder, we&#39;ll run Kibana and Elasticsearch on <em>our</em> infrastructure and bind all &#34;exposed&#34; ports to <code>localhost</code>.</p>
<p>After that, you&#39;ll be able to access Kibana at <code>localhost:5601</code>, and you&#39;ll see that it talks to Elasticsearch using its service name, not an IP, as shown by the <code>ELASTICSEARCH_URL</code> environment variable above.</p>

<h2>What happens in Kubernetes when you run <code>ergomake up</code>?</h2>
<p>When you run <code>ergomake up</code>, our CLI reads your compose file and sends it to our back-end, which we call <code>kitchen</code>. Once the file&#39;s contents reach the <code>kitchen</code>, they&#39;re parsed and transformed into a <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"><code>Custom Resource Definition</code></a>: an <code>ErgomakeEnv</code>.</p>
<p>That <code>ErgomakeEnv</code> represents all services within your compose file, which images they use, and what ports they expose, among other things.</p>
<p>After generating an <code>ErgomakeEnv</code>, our back-end, <code>kitchen</code>, &#34;applies&#34; that <code>ErgomakeEnv</code> to the cluster.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/kitchen-step.png" alt=""/></p>
<p>Once an <code>ErgomakeEnv</code> is &#34;applied,&#34; it triggers a <a href="https://metalbear.co/blog/writing-a-kubernetes-operator/">Kubernetes Operator</a> of our own, which we call <code>dishwasher</code>.</p>
<p>The <code>dishwasher</code> is a piece of software that transforms an <code>ErgomakeEnv</code> into Kubernetes resources like pods and services and ensures that environments are always running smoothly.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/dishwasher-step.png" alt=""/></p>
<p>Now you know how Ergomake turns your <code>docker-compose.yml</code> file into Kubernetes resources.</p>
<h2>Replaying steps manually</h2>
<p>In this section, we&#39;ll manually apply each of those Kubernetes resources, essentially replaying Ergomake&#39;s steps. That way, you&#39;ll learn how these Kubernetes resources talk to each other at a high level.</p>
<p>Whenever a pod gets created, Kubernetes assigns an IP to that pod. Once your pod is up, it can talk to <em>any</em> other pods in your cluster unless you&#39;ve explicitly configured your cluster for that not to be possible.</p>
<p>If you create a pod for Kibana and another for Elasticsearch, for example, Kibana will be able to talk to Elasticsearch using the IP assigned to the <code>elasticsearch</code> pod.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/kibana-es-ip.png" alt=""/></p>
<p>Let&#39;s go ahead and try that ourselves. First, we&#39;ll create a pod for Kibana and another for Elasticsearch.</p>

<p>After deploying those with <code>kubectl apply -f ./example.yml</code>, get the pods IPs with <code>kubectl get pods -o wide</code>.</p>

<p>With Elasticsearch&#39;s IPs, get a shell within Kibana&#39;s container and try to <code>curl</code> Elasticsearch using its IP and default port, <code>9200</code>.</p>

<p>Although pods <em>can</em> talk to each other using IPs, there are two main problems with that approach.</p>
<p>The first problem is that you don&#39;t know which IP a pod will receive until you actually deploy it.</p>
<p>Imagine you wanted to configure that Kibana instance to connect to Elasticsearch. In that case, you&#39;d have to create the Elasticsearch pod first, get its IP, and only then deploy Kibana setting the <code>ELASTICSEARCH_URL</code> to that IP. In other words, you wouldn&#39;t be able to deploy both pods simultaneously because there&#39;d be no way to tell Kibana what&#39;s Elasticsearch&#39;s IP in advance.</p>
<p>The second problem with using IPs is that they may change.</p>
<p>For example, if you have to change environment variables for your <code>elasticsearch</code> pod, you&#39;ll have to deploy a new pod. When you do that, Kubernetes will again assign an IP to that pod. That new IP is not guaranteed to be the same as before (spoiler: it won&#39;t). The same thing will happen when <code>deployment</code> recreates replicas as it scales up or down and pods get reallocated to different nodes.</p>
<p><strong>To solve these problems, we can use a <code>service</code></strong>. By using a service, you can reference a particular set of services using a static name instead of an IP, which may change.</p>
<p>In our example, we will create a service called <code>elasticsearch</code>, just like Ergomake would do. Then, we will use that service name in Kibana&#39;s <code>ELASTICSEARCH_URL</code>.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/kibana-es-service.png" alt=""/></p>
<p>For that service to work, it must include a selector indicating the pods to which it will route traffic and the definitions of which logical ports map to which ports within the pod.</p>

<p>After applying that file, try getting a shell within Kibana again. From there, <code>curl</code> the <code>elasticsearch</code> pod&#39;s port <code>9200</code> using the service&#39;s name instead of the pod IP.</p>

<p>Now, you can also set <code>ELASTICSEARCH_URL</code> for the <code>kibana</code> pod to connect to Elasticsearch through the <code>elasticsearch</code> service.</p>

<p>Once you apply all these changes to your cluster, you&#39;ll see that Kibana successfully connects to Elasticsearch using its hostname.</p>

<p>Now that you know how these resources talk to each other at a high level, we&#39;ll dig deeper into Kubernetes to make it less magical.</p>
<h2>What happens when Kibana sends requests to <code>elasticsearch</code>?</h2>
<p>In this section, you&#39;ll learn how a pod can talk to another using a service name.</p>
<p>Regardless of where your application runs, it must resolve hostnames into IPs before sending requests. For example, when you send a request to <code>google.com</code>, you must resolve <code>google.com</code> into an IP and then send the request there.</p>
<p>In our previous example, the same thing happened when we sent a request to <code>elasticsearch</code> from within Kibana.</p>
<p>Before it could send the request, the sender had to &#34;translate&#34; <code>elasticsarch</code> into an IP. Only then was it able to send a request to it.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/dns-to-ip.png" alt=""/></p>
<p>You can see that <a href="https://www.techopedia.com/definition/29029/dns-lookup">DNS lookup</a> by sending a verbose request (<code>--vvvv</code>) with <code>curl</code> from the Kibana pod.</p>

<p>As shown above, the request to <code>elasticsearch</code> was sent to the IP <code>10.100.34.205</code>, which is the <code>elasticsearch</code> service&#39;s IP.</p>

<p>As you would expect, sending a request to the service&#39;s IP will yield the same result as sending a request to <code>elasticsearch</code>.</p>

<p>That&#39;s the IP to which Kibana sends requests whenever it needs to reach Elasticsearch.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/dns-to-ip-es-service.png" alt=""/></p>
<p>Now, two questions remain: who turns <code>elasticsearch</code> into the service&#39;s IP, and how do they know which IP it should be?</p>
<h3>Question 1: Who turns <code>elasticsearch</code> into an IP?</h3>
<p>In Linux systems, you can use the <code>/etc/resolve.conf</code> file to determine the server to which DNS lookup requests will be sent.</p>
<p>If we look at the contents of that file within our Kibana container, you&#39;ll see that it&#39;s sending DNS requests to <code>10.96.0.10</code>.</p>

<p>That IP refers to a service called <code>kube-dns</code>, which is in the <code>kube-system</code> namespace, so you usually don&#39;t see it.</p>

<p>That service, in turn, points to the <code>coredns</code> pod, which runs the actual DNS server: <a href="https://coredns.io/">CoreDNS</a>. That pod is also in the <code>kube-system</code> namespace, so you don&#39;t usually see it either.</p>

<p><strong>It&#39;s that <code>coredns</code> pod that resolves the <code>elasticsearch</code> name into the service&#39;s IP.</strong></p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/coredns-to-service.png" alt=""/></p>
<h3>Question 2: How does CoreDNS know what&#39;s the IP for <code>elasticsearch</code>?</h3>
<p>Among the pods in <code>kube-system</code> you don&#39;t usually see, there&#39;s <code>kube-controller-manager</code>.</p>

<p>The <code>kube-controller-manager</code> pod watches the cluster&#39;s desired state and takes action for that <em>desired</em> state to become the cluster&#39;s <em>actual</em> state.</p>
<p>When you create a <code>service</code>, for example, the <code>kube-controller-manager</code> will break it down into further resources called <code>Endpoint</code> and <code>EndpointSlices</code>.</p>

<p>CoreDNS uses these <code>Endpoints</code> and <code>EndpointSlices</code> to resolve DNS queries. Whenever it gets a query, it&#39;ll look at these resources and respond with the correct IP.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/coredns-watching-endpoints.png" alt=""/></p>
<p>If you look at its configuration, which is just a <code>ConfigMap</code> within <code>kube-system</code>, you&#39;ll see a reference to the <a href="https://github.com/coredns/coredns/blob/master/plugin/kubernetes/README.md">CoreDNS Kubernetes plugin</a>.</p>

<p>That plugin turns CoreDNS into a &#34;cluster-aware&#34; DNS server. Otherwise, it&#39;d be a DNS server like any other.</p>
<h2>How do services &#34;forward&#34; requests to pods?</h2>
<p>In this section, you&#39;ll learn how requests to a service get redirected to a particular pod.</p>
<p>By now, perspicacious readers may have noticed that the IP for the <code>elasticsearch</code> service does <em>not</em> match the IP for the Elasticsearch pod. That IP is also not bound to any pod or virtual machine.</p>

<p>In that case, <strong>how can requests to the service&#39;s IP reach that service&#39;s pods?</strong></p>
<p>The way a request gets to a service&#39;s pods is because its packets are <em>not</em> actually sent to the service&#39;s IP. Instead, <strong>the node rewrites packets addressed to the service&#39;s IP and addresses them to the pod&#39;s IP</strong>.</p>
<p>The way nodes rewrite packet addresses is by using a program called <a href="https://en.wikipedia.org/wiki/Iptables"><code>iptables</code></a>. That program allows administrators to configure rules determining how network packets get treated.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/outgoing-iptables-transform.png" alt=""/></p>
<p>Suppose you want to see some of these <code>iptables</code> rules. In that case, you can SSH into the Minikube node running these pods with <code>minikube ssh -n minikube-m02</code> and then list all the <code>iptables</code> rules with <code>iptables-save</code>. Alternatively, you can filter only the rules containing &#34;<code>elasticsearch</code>&#34; by using <code>iptables-save | grep elasticsearch</code>.</p>
<blockquote>
<p>You don&#39;t have to worry about understanding all the rules below. All you need to understand is that they (and a couple of others) get the packets addressed to the correct place: the pod.</p>
</blockquote>

<h3>Who creates these <code>iptables</code> rules?</h3>
<p>Remember our friend <code>kube-controller-manager</code>? When that fellow creates Endpoints and EndpointSlices, a pod called <code>kube-proxy</code> reads those resources to make the IP tables rules which redirect packets from a service to that service&#39;s pods.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/kube-proxy-iptables-transform.png" alt=""/></p>
<p>The <code>kube-proxy</code> pod runs in <em>every</em> node because it&#39;s spawned through a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>. That way, Kubernetes can ensure each node will have a <code>kube-proxy</code> to update that node&#39;s <code>iptables</code> rules.</p>
<blockquote>
<p>As a note, when a service targets multiple pods, such as pods for a <code>deployment</code> with numerous replicas, it will create <code>iptables</code> rules which load balance the traffic between them. Those <code>iptables</code> rules take care of randomly assigning traffic to pods.</p>
</blockquote>
<h2>Putting it all together</h2>
<p>Whenever you create a pod, it gets assigned an IP.</p>
<p>Any two pods in your cluster can talk to each other using their IP addresses.</p>
<p>The problem with using IP addresses for pods to talk to each other is that these IPs may change as pods get deleted and recreated.</p>
<p>For pods to consistently address each other correctly, you can use a Service.</p>
<p>When you create a <code>service</code> using <code>kubectl</code>, the Kubernetes <code>apiserver</code> will save its data, and another pod called <code>kubernetes-controller-manager</code> will wake up and break that service down into two resources: Endpoints and EndpointSlices.</p>
<p>CoreDNS will use those resources to know how to turn a service name into a service IP. Additionally, each node&#39;s <code>kube-proxy</code> pods will update the node&#39;s <code>iptables</code> rules. Those <code>iptables</code> rules cause requests to the service&#39;s IP to get addressed to the service&#39;s pods.</p>
<p>Finally, when a pod makes a request, it will do a DNS query to CoreDNS to get the service&#39;s IP. Then, when sending packets to that IP, the <code>iptables</code> rules created by <code>kube-proxy</code> will cause the packets to get addressed to an actual pod&#39;s IP.</p>
<p><img src="https://ergomake.dev/images/blog/kube-networking/kube-networking-final.png" alt=""/></p>
<h2>A few more notes</h2>
<p>I&#39;ve intentionally skipped a few details to avoid confusing the reader.</p>
<p>Among those details is <a href="https://ronaknathani.com/blog/2020/08/how-a-kubernetes-pod-gets-an-ip-address/">how a pod gets assigned an IP</a> and <a href="https://ronaknathani.com/blog/2020/07/kubernetes-nodeport-and-iptables-rules/">how <code>iptables</code> rules work</a>.</p>
<p>I also haven&#39;t touched on <a href="https://github.com/containernetworking/cni">CNI plugin</a> implementations, like <a href="https://www.tkng.io/cni/kindnet/">Kindnet</a>.</p>
<p>A tour through <a href="https://medium.com/techlog/diving-into-linux-networking-and-docker-bridge-veth-and-iptables-a05eb27b1e72">container networking itself</a> would also be helpful for most readers.</p>
<p>Finally, if you want to learn more about CoreDNS itself, <a href="https://www.youtube.com/watch?v=qRiLmLACYSY">this talk is a great start</a>.</p>
<h2>Wanna chat?</h2>
<p>We&#39;re a two-people startup, and we love talking to interesting people.</p>
<p>If you&#39;d like to chat, you can book a slot with me <a href="https://calendly.com/lucasfcosta/1-1-lucas-ergomake-blog">here</a>.</p>
<p>I&#39;d love to discuss Kubernetes, command-line interfaces, ephemeral environments, or what we&#39;re building at Ergomake.</p>
<p>Alternatively, you can send me a tweet or DM <a href="https://twitter.com/thewizardlucas">@thewizardlucas</a> or an email at lucas.costa@getergomake.com.</p>
</article></div>
  </body>
</html>
