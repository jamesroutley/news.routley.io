<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/mezbaul-h/june">Original</a>
    <h1>Show HN: Local voice assistant using Ollama, transformers and Coqui TTS toolkit</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<div dir="auto"><h2 tabindex="-1" dir="auto">Local Voice Chatbot: Ollama + HF Transformers + Coqui TTS Toolkit</h2><a id="user-content-local-voice-chatbot-ollama--hf-transformers--coqui-tts-toolkit" aria-label="Permalink: Local Voice Chatbot: Ollama + HF Transformers + Coqui TTS Toolkit" href="#local-voice-chatbot-ollama--hf-transformers--coqui-tts-toolkit"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>

<p dir="auto"><strong>june-va</strong> is a local voice chatbot that combines the power of Ollama (for language model capabilities), Hugging Face Transformers (for speech recognition), and the Coqui TTS Toolkit (for text-to-speech synthesis). It provides a flexible, privacy-focused solution for voice-assisted interactions on your local machine, ensuring that no data is sent to external servers.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mezbaul-h/june/blob/master/demo.gif"><img src="https://github.com/mezbaul-h/june/raw/master/demo.gif" alt="demo-text-only-interaction" data-animated-image=""/></a></p>

<ul dir="auto">
<li><strong>Text Input/Output:</strong> Provide text inputs to the assistant and receive text responses.</li>
<li><strong>Voice Input/Text Output:</strong> Use your microphone to give voice inputs, and receive text responses from the assistant.</li>
<li><strong>Text Input/Audio Output:</strong> Provide text inputs and receive both text and synthesised audio responses from the assistant.</li>
<li><strong>Voice Input/Audio Output (Default):</strong> Use your microphone for voice inputs, and receive responses in both text and synthesised audio form.</li>
</ul>


<ul dir="auto">
<li><a href="https://github.com/ollama/ollama">Ollama</a></li>
<li>Python <code>3.10+</code> (with <em>pip</em>)</li>
</ul>
<p dir="auto">You will also need the following native package installed on your machine:</p>
<div dir="auto" data-snippet-clipboard-copy-content="apt install portaudio19-dev  # requirement for PyAudio"><pre>apt install portaudio19-dev  <span><span>#</span> requirement for PyAudio</span></pre></div>

<p dir="auto">To install directly from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/mezbaul-h/june.git
cd june
pip install ."><pre>git clone https://github.com/mezbaul-h/june.git
<span>cd</span> june
pip install <span>.</span></pre></div>

<p dir="auto">Pull the language model (default is <code>llama3:8b-instruct-q4_0</code>) with Ollama first, if you haven&#39;t already:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ollama pull llama3:8b-instruct-q4_0"><pre>ollama pull llama3:8b-instruct-q4_0</pre></div>
<p dir="auto">Next, run the program (with default configuration):</p>

<p dir="auto">This will use <a href="https://ollama.com/library/llama3:8b-instruct-q4_0" rel="nofollow">llama3:8b-instruct-q4_0</a> for LLM capabilities, <a href="https://huggingface.co/openai/whisper-small.en" rel="nofollow">openai/whisper-small.en</a> for speech recognition, and <code>tts_models/en/ljspeech/glow-tts</code> for audio synthesis.</p>
<p dir="auto">You can also customize behaviour of the program with a json configuration file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="june-va --config path/to/config.json"><pre>june-va --config path/to/config.json</pre></div>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> The configuration file is optional. To learn more about the structure of the config file, see the <a href="#configuration">Configuration</a> section.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto"><g-emoji alias="warning">⚠️</g-emoji> Regarding Voice Input</h3><a id="user-content-️-regarding-voice-input" aria-label="Permalink: ⚠️ Regarding Voice Input" href="#️-regarding-voice-input"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">After seeing the <code>Listening for sound...</code> message, you can speak directly into the microphone. Unlike typical voice assistants, there&#39;s no wake command required. Simply start speaking, and the tool will automatically detect and process your voice input. Once you finish speaking, maintain silence for 3 seconds to allow the assistant to process your voice input.</p>

<p dir="auto">Many of the models (e.g., <code>tts_models/multilingual/multi-dataset/xtts_v2</code>) supported by Coqui&#39;s TTS Toolkit support voice cloning. You can use your own speaker profile with a small audio clip (approximately 1 minute for most models). Once you have the clip, you can instruct the assistant to use it with a custom configuration like the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;tts&#34;: {
    &#34;model&#34;: &#34;tts_models/multilingual/multi-dataset/xtts_v2&#34;,
    &#34;generation_args&#34;: {
      &#34;language&#34;: &#34;en&#34;,
      &#34;speaker_wav&#34;: &#34;/path/to/your/target/voice.wav&#34;
    }
  }
}"><pre>{
  <span>&#34;tts&#34;</span>: {
    <span>&#34;model&#34;</span>: <span><span>&#34;</span>tts_models/multilingual/multi-dataset/xtts_v2<span>&#34;</span></span>,
    <span>&#34;generation_args&#34;</span>: {
      <span>&#34;language&#34;</span>: <span><span>&#34;</span>en<span>&#34;</span></span>,
      <span>&#34;speaker_wav&#34;</span>: <span><span>&#34;</span>/path/to/your/target/voice.wav<span>&#34;</span></span>
    }
  }
}</pre></div>

<p dir="auto">The application can be customised using a configuration file. The config file must be a JSON file. The default configuration is as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &#34;llm&#34;: {
        &#34;disable_chat_history&#34;: false,
        &#34;model&#34;: &#34;llama3:8b-instruct-q4_0&#34;
    },
    &#34;stt&#34;: {
        &#34;device&#34;: &#34;torch device identifier (`cuda` if available; otherwise `cpu`&#34;,
        &#34;generation_args&#34;: {
            &#34;batch_size&#34;: 8
        },
        &#34;model&#34;: &#34;openai/whisper-small.en&#34;
    },
    &#34;tts&#34;: {
        &#34;device&#34;: &#34;torch device identifier (`cuda` if available; otherwise `cpu`&#34;,
        &#34;model&#34;: &#34;tts_models/en/ljspeech/glow-tts&#34;
    }
}"><pre>{
    <span>&#34;llm&#34;</span>: {
        <span>&#34;disable_chat_history&#34;</span>: <span>false</span>,
        <span>&#34;model&#34;</span>: <span><span>&#34;</span>llama3:8b-instruct-q4_0<span>&#34;</span></span>
    },
    <span>&#34;stt&#34;</span>: {
        <span>&#34;device&#34;</span>: <span><span>&#34;</span>torch device identifier (`cuda` if available; otherwise `cpu`<span>&#34;</span></span>,
        <span>&#34;generation_args&#34;</span>: {
            <span>&#34;batch_size&#34;</span>: <span>8</span>
        },
        <span>&#34;model&#34;</span>: <span><span>&#34;</span>openai/whisper-small.en<span>&#34;</span></span>
    },
    <span>&#34;tts&#34;</span>: {
        <span>&#34;device&#34;</span>: <span><span>&#34;</span>torch device identifier (`cuda` if available; otherwise `cpu`<span>&#34;</span></span>,
        <span>&#34;model&#34;</span>: <span><span>&#34;</span>tts_models/en/ljspeech/glow-tts<span>&#34;</span></span>
    }
}</pre></div>
<p dir="auto">When you use a configuration file, it overrides the default configuration but does not overwrite it. So you can partially modify the configuration if you desire. For instance, if you do not wish to use speech recognition and only want to provide prompts through text, you can disable that by using a config file with the following configuration:</p>

<p dir="auto">Similarly, you can disable the audio synthesiser, or both, to only use the virtual assistant in text mode.</p>
<p dir="auto">If you only want to modify the device on which you want to load a particular type of model, without changing the other default attributes of the model, you could use:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;tts&#34;: {
    &#34;device&#34;: &#34;cpu&#34;
  }
}"><pre>{
  <span>&#34;tts&#34;</span>: {
    <span>&#34;device&#34;</span>: <span><span>&#34;</span>cpu<span>&#34;</span></span>
  }
}</pre></div>

<div dir="auto"><h4 tabindex="-1" dir="auto"><code>llm</code> - Language Model Configuration</h4><a id="user-content-llm---language-model-configuration" aria-label="Permalink: llm - Language Model Configuration" href="#llm---language-model-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>llm.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>llm.disable_chat_history</code>: Boolean indicating whether to disable or enable chat history. Enabling chat history will make interactions more dynamic, as the model will have access to previous contexts, but it will consume more processing power. Disabling it will result in less interactive conversations but will use fewer processing resources.</li>
<li><code>llm.model</code>: Name of the text-generation model tag on Ollama. Ensure this is a valid model tag that exists on your machine.</li>
<li><code>llm.system_prompt</code>: Give a system prompt to the model. If the underlying model does not support a system prompt, an error will be raised.</li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto"><code>stt</code> - Speech-to-Text Model Configuration</h4><a id="user-content-stt---speech-to-text-model-configuration" aria-label="Permalink: stt - Speech-to-Text Model Configuration" href="#stt---speech-to-text-model-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>tts.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>stt.generation_args</code>: Object containing generation arguments accepted by Hugging Face&#39;s speech recognition pipeline.</li>
<li><code>stt.model</code>: Name of the speech recognition model on Hugging Face. Ensure this is a valid model ID that exists on Hugging Face.</li>
</ul>
<div dir="auto"><h4 tabindex="-1" dir="auto"><code>tts</code> - Text-to-Speech Model Configuration</h4><a id="user-content-tts---text-to-speech-model-configuration" aria-label="Permalink: tts - Text-to-Speech Model Configuration" href="#tts---text-to-speech-model-configuration"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><code>tts.device</code>: Torch device identifier (e.g., <code>cpu</code>, <code>cuda</code>, <code>mps</code>) on which the pipeline will be allocated.</li>
<li><code>tts.generation_args</code>: Object containing generation arguments accepted by Coqui&#39;s TTS API.</li>
<li><code>tts.model</code>: Name of the text-to-speech model supported by the Coqui&#39;s TTS Toolkit. Ensure this is a valid model ID.</li>
</ul>
</article></div></div>
  </body>
</html>
