<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://purplesyringa.moe/blog/i-sped-up-serde-json-strings-by-20-percent/">Original</a>
    <h1>I sped up serde_json strings by 20%</h1>
    
    <div id="readability-page-1" class="page"><section><div><h2>I sped up serde_json strings by 20%</h2><p><time>August 20, 2024</time><a href="https://www.reddit.com/r/rust/comments/1eyxspu/i_sped_up_serde_json_strings_by_20/"> Discuss on Reddit</a></p><p>I have recently done some performance work and realized that reading about my experience could be entertaining. Teaching to <em>think</em> is just as important as teaching to <em>code</em>, but this is seldom done; I think something I’ve done last month is a great opportunity to draw the curtain a bit.</p><p><code>serde</code> is <em>the</em> Rust framework for serialization and deserialization. Everyone uses it, and it’s the default among the ecosystem. <code>serde_json</code> is the official <code>serde</code> “mixin” for JSON, so when people need to parse stuff, that’s what they use instinctively. There are other libraries for JSON parsing, like <a href="https://lib.rs/crates/simd-json">simd-json</a>, but <code>serde_json</code> is overwhelmingly used: it has <a href="https://crates.io/crates/serde_json/reverse_dependencies">26916</a> dependents at the time of this post, compared to only <a href="https://crates.io/crates/simd-json/reverse_dependencies">66</a> for <code>simd-json</code>.</p><p>This makes <code>serde_json</code> a good target <s>(not in a Jia Tan way)</s> for optimization. Chances are, many of those 26916 users would profit from switching to <code>simd-json</code>, but as long as they aren’t doing that, smaller optimizations are better than nothing, and such improvements are reapt across the ecosystem.</p><p>I have recently been working on the <a href="https://thewitchofendor.com/2024/08/22/you-might-want-to-use-panics-for-error-handling/">#[iex]</a> library. I used <code>serde</code> and <code>serde_json</code> as benchmarks and noticed some questionable decisions in their performance-critical code while rewriting it to better suit <code>#[iex]</code>.</p><p><code>#[iex]</code> focuses on error handling, so the error path was the first thing I benchmarked. To my surprise, <code>serde_json</code>’s error path was more than 2x slower than the success path on the same data:</p><div><table><thead><tr><td rowspan="2">Speed (MB/s, higher is better)</td><th colspan="2"><code>canada</code></th><th colspan="2"><code>citm_catalog</code></th><th colspan="2"><code>twitter</code></th></tr><tr><th>DOM</th><th>struct</th><th>DOM</th><th>struct</th><th>DOM</th><th>struct</th></tr></thead><tbody><tr><td>Success path</td><td>283</td><td>416</td><td>429</td><td>864</td><td>275</td><td>541</td></tr><tr><td>Error path</td><td>122</td><td>168</td><td>135</td><td>195</td><td>142</td><td>226</td></tr><tr><td>Slowdown</td><td>-57%</td><td>-60%</td><td>-69%</td><td>-77%</td><td>-48%</td><td>-58%</td></tr></tbody></table></div><p>Why? Error propagation cannot be that slow. Profiling via <code>perf</code> reveals that the bottleneck is this innocent function:</p><pre><code><span>fn</span> <span>position_of_index</span>(&amp;<span>self</span>, i: <span>usize</span>) <span>-&gt;</span> Position {
    <span>let</span> <span>mut </span><span>position</span> = Position { line: <span>1</span>, column: <span>0</span> };
    <span>for</span> <span>ch</span> <span>in</span> &amp;<span>self</span>.slice[..i] {
        <span>match</span> *ch {
            <span>b&#39;\n&#39;</span> =&gt; {
                position.line += <span>1</span>;
                position.column = <span>0</span>;
            }
            _ =&gt; {
                position.column += <span>1</span>;
            }
        }
    }
    position
}
</code></pre><p>…which is called from <code>position()</code> to format the error, which is documented as:</p><pre><code>




</code></pre><p>…Well. I agree that between a faster success path and a faster error path, the former wins, but taking more time than just parsing just to format an error is taking it way too far.</p><p>Can we do anything about this? <code>position_of_index()</code> wants to convert an index in a string to a line/column pair. To do that, we can reduce the problem to two simpler ones:</p><ul><li>Count <code>\n</code>s in <code>self.slice[..i]</code>; that’s going to be the 0-based line number, and</li><li>Find the last <code>\n</code> in <code>self.slice[..i]</code> and subtract its position from <code>i</code>; that’s going to be the 1-based column number.</li></ul><p>Searching a string for a single-character needle is a long-solved problem. In C, we use <code>strchr</code> for that; in Rust, we use the <a href="https://crates.io/crates/memchr">memchr</a> crate. In fact, this crate also provides <a href="https://docs.rs/memchr/2.7.4/src/memchr/memchr.rs.html#327-333">an optimized way</a> to <em>count</em> occurences, which we need for the first subproblem.</p><p><code>memchr</code> uses SIMD in both cases, so it’s a lot faster than a naive loop. Indeed, replacing the implementation above with:</p><pre><code><span>fn</span> <span>position_of_index</span>(&amp;<span>self</span>, i: <span>usize</span>) <span>-&gt;</span> Position {
    <span>let</span> <span>start_of_line</span> = <span>match</span> memchr::<span>memrchr</span>(<span>b&#39;\n&#39;</span>, &amp;<span>self</span>.slice[..i]) {
        <span>Some</span>(position) =&gt; position + <span>1</span>,
        <span>None</span> =&gt; <span>0</span>,
    };
    Position {
        line: <span>1</span> + memchr::<span>memchr_iter</span>(<span>b&#39;\n&#39;</span>, &amp;<span>self</span>.slice[..start_of_line]).<span>count</span>(),
        column: i - start_of_line,
    }
}
</code></pre><p>…results in a great improvement:</p><div><table><thead><tr><td rowspan="2">Speed (MB/s, higher is better)</td><th colspan="2"><code>canada</code></th><th colspan="2"><code>citm_catalog</code></th><th colspan="2"><code>twitter</code></th></tr><tr><th>DOM</th><th>struct</th><th>DOM</th><th>struct</th><th>DOM</th><th>struct</th></tr></thead><tbody><tr><td>Success path</td><td>283</td><td>416</td><td>429</td><td>864</td><td>275</td><td>541</td></tr><tr><td>Error path (<code>memchr</code>)</td><td>216</td><td>376</td><td>238</td><td>736</td><td>210</td><td>492</td></tr><tr><td>Slowdown</td><td>-24%</td><td>-10%</td><td>-45%</td><td>-15%</td><td>-24%</td><td>-9%</td></tr></tbody></table></div><p>The error path is still slower than the success path, but the difference is a lot less prominent now.</p><p>I submitted <a href="https://github.com/serde-rs/json/pull/1160">a PR introducing this optimization</a> and wondered if it’s going to be merged. After all, <code>serde_json</code> has very few dependencies, and dtolnay seems to focus on build times, so would a PR adding a new dependency make it?</p><p>To my shock, the PR was quickly merged! Not a bad first contribution.</p><p>dtolnay advised me to look for other places where a similar optimization could be applied, so that’s what I did. (I can’t overestimate how helpful he was to me during this endeavor.)</p><p>The first place I found is this loop in string parsing:</p><pre><code><span>while</span> <span>self</span>.index &lt; <span>self</span>.slice.<span>len</span>() &amp;&amp; !ESCAPE[<span>self</span>.slice[<span>self</span>.index] <span>as</span> <span>usize</span>] {
    <span>self</span>.index += <span>1</span>;
}
</code></pre><p>What we want here is to find the first non-escape character. “Escape” characters are <code>\</code> (for obvious reasons) and <code>&#34;</code> (because it marks the end of the string), but also all ASCII codes up to and including <code>0x1F</code>, because the JSON specification <a href="https://www.crockford.com/mckeeman.html">forbids</a> control codes in strings (so e.g. <code>&#34;line 1\nline 2&#34;</code> is valid JSON, but replacing the <code>\n</code> with a literal newline invalidates it).</p><p><em>If</em> all I needed was to find the first <code>\</code> or <code>&#34;</code>, the <a href="https://docs.rs/memchr/latest/memchr/fn.memchr2.html">memchr2</a> function provided by <code>memchr</code> would suffice. But I need something more complicated, so how am I supposed to go about it?</p><h2>Looking for escape</h2><p>The first idea dtolnay and I had wasn’t a good one, and it touches a tangential topic, but I still think it’s important to discuss to learn how to not make the same mistake.</p><p>The idea was:</p><ul><li>Use <code>memchr2</code> to find the first <code>\</code> or <code>&#34;</code>, and after that</li><li>Go through the string character-by-character to ensure there are no control characters.</li></ul><p>The idea was that offloading the search for <code>\</code> and <code>&#34;</code> to a faster algorithm would improve the performance overall.</p><p>In reality, this turned out to be <em>slower</em> than the original code, because looping over the string <em>twice</em>, quickly and then slowly, is always bound to be worse than looping over the string <em>once</em>, just as slowly. Sure, a byte comparison (<code>ch &lt; 0x20</code>) is a little bit faster than a memory access (<code>ESCAPE[...]</code>), but that effect is quickly offset by using two passes (and thus increasing memory bandwidth) instead of one.</p><p>It turns out that dtolnay based his intuition on <a href="https://nrk.neocities.org/articles/cpu-vs-common-sense">a post</a> that studied various implementations of the standard C <code>strlcpy</code> function and found that a two-pass algorithm is faster than a single-pass algorithm. So what went wrong there?</p><p><code>strlcpy(char *dst, const char *src, size_t size)</code> copies a string from <code>src</code> to <code>dst</code>, truncating it to at most <code>size - 1</code> characters. The 1 byte is reserved for the always-added null terminator. The competing implementations were (adding the terminating NUL byte is irrelevant, so not listed here):</p><ul><li>Single-pass: perform <code>*dst++ = *src++</code> at most <code>size - 1</code> times, until <code>*src</code> is a NUL byte, and</li><li>Two-pass: compute <code>len = strlen(src)</code>, then call <code>memcpy(dst, src, min(len, size - 1))</code>.</li></ul><p>The two-pass algorithm was faster because <code>strlen</code> and <code>memcpy</code> were calls to SIMD-optimized glibc routines, but the loop of the single-pass algorithm was scalar. The author realized this and provided their own implementations of <code>strlen</code> and <code>memcpy</code>, pessimizing the two-pass <code>strlcpy</code>, so that the two algorithms were more competitive:</p><pre><code><span>size_t</span> <span>bespoke_strlcpy</span><span>(<span>char</span> *dst, <span>const</span> <span>char</span> *src, <span>size_t</span> size)</span> {
    <span>size_t</span> len = <span>0</span>;
    <span>for</span> (; src[len] != <span>&#39;\0&#39;</span>; ++len) {} 

    <span>if</span> (size &gt; <span>0</span>) {
        <span>size_t</span> to_copy = len &lt; size ? len : size - <span>1</span>;
        <span>for</span> (<span>size_t</span> i = <span>0</span>; i &lt; to_copy; ++i) 
            dst[i] = src[i];
        dst[to_copy] = <span>&#39;\0&#39;</span>;
    }
    <span>return</span> len;
}
</code></pre><p>GCC can easily detect such loops and replace them with glibc calls, so the author also explicitly disabled this with <code>-fno-builtin</code>. Even like this, the two-pass algorithm was still faster than a single-pass one.</p><p>However, one detail wasn’t explicit. <code>-fno-builtin</code> does not disable <em>all</em> <code>memcpy</code>-related optimizations: the <code>memcpy</code>-like loop can still be vectorized, and that’s what GCC did to <code>bespoke_strlcpy</code>. So the author was actually comparing scalar <code>strlen</code> (check for NUL, loop) + vectorized <code>memcpy</code> (check for size, loop) to scalar <code>strlcpy</code> (check for NUL, check for size, loop).</p><p>Disabling vectorization with <code>-fno-tree-vectorize</code> makes the two-pass algorithm <em>slower</em>, as it should be, because now we’re comparing two loops (check for NUL, loop; check for size, loop) to one loop (check for NUL, check for size, loop), and the latter is faster because it puts less pressure on the branch predictor and has fewer memory accesses.</p><hr/><p>The lesson here is that vectorization is king, but vectorizing the simpler half of the code while the leaving the complex one scalar is not going to provide any improvements. So if we want to optimize this, we have to use SIMD for both parts.</p><p>The original approach thus morphed into:</p><ul><li>Use <code>memchr2</code> to find the first <code>\</code> or <code>&#34;</code>, and after that</li><li>Use hand-written SIMD to ensure there are no control characters.</li></ul><p>We would need to reinvent the wheel, but this is quite neat if you think about it. In the success path, we <em>find positions</em> of <code>\</code> and <code>&#34;</code>, but we only <em>check the absence</em> of control codes. So we can avoid a conditional branch in the hot loop, by replacing this:</p><pre><code><span>for</span> simd_word <span>in</span> to_simd_words(data):
    <span>if</span> <span>any</span>(simd_word &lt; <span>0x20</span>):
        ...
</code></pre><p>…with this:</p><pre><code>mask = <span>False</span>
<span>for</span> simd_word <span>in</span> to_simd_words(data):
    mask |= simd_word &lt; <span>0x20</span>
<span>if</span> <span>any</span>(mask):
    ...
</code></pre><p>However, we quickly realized that this was a losing battle. We would slow down short strings by invoking a (runtime-selected!) function to search for <code>\</code> and <code>&#34;</code>, but we would pessimize longer strings by reading memory twice <em>too</em>. We needed something better.</p><p>We really needed to search for <code>\</code>, <code>&#34;</code>, <em>and</em> control codes in one pass.</p><p>But I tried hard to keep <code>serde_json</code> as simple as it was. I don’t usually care about code complexity in my projects, but something another person has to maintain should preferably be as uninvasive as possible. This made separately implementing SIMD for different platforms a no-go.</p><p>However, there is one technique people used before common processors supported SIMD natively. Instead of processing 128-bit words, we could process 64-bit words, using bitwise operations to simulate per-element behavior. This idea is called SIMD Within A Register, or SWAR for short. To give a simple example, converting 8 Latin letters stored in a 64-bit word to lowercase is as easy as computing <code>x | 0x2020202020202020</code>.</p><p>What I wanted to do is search for a control character in an 64-bit word, implicitly split into 8 bytes.</p><p>The way this works is that for <code>c: i8</code>, the condition we’re looking for is <code>c &gt;= 0 &amp;&amp; c &lt; 0x20</code>, which can be rewritten as <code>c &gt;= 0 &amp;&amp; c - 0x20 &lt; 0</code>. This just checks that the sign bit of <code>c</code> is <code>0</code> and the sign bit of <code>c - 0x20</code> is <code>1</code>, which is equivalent to <code>!c &amp; (c - 0x20) &amp; 0x80 != 0</code>.</p><p>So for 8 packed bytes, we compute <code>!c &amp; (c - 0x2020202020202020) &amp; 0x8080808080808080</code>. If it’s <code>0</code>, great, no control character. If it’s non-zero, we find the least significant non-zero byte in the mask, and that’s our first occurence of a control character.</p><p>There’s just one nuance. The <code>c - 0x20</code> in <code>c &gt;= 0 &amp;&amp; c - 0x20 &lt; 0</code> is a wrapping subtraction, but performing a 64-bit subtraction can propagate carry/borrow between bytes. This is, however, not a problem: the borrow can only be propagated from a byte if it’s less than <code>0x20</code>, and only to more significant bytes. We only wish to find the least significant control byte, so we don’t care if it corrupts more significant bytes.</p><p>This, of course, only works on little-endian machines. On big-endian machines, <code>c</code> has to be bytereversed.</p><p>What about matching <code>\</code> (and <code>&#34;</code>) though? The condition for <code>\</code> is as simple as <code>c ^ b&#39;\\&#39; &gt;= 0 &amp;&amp; c ^ b&#39;\\&#39; &lt; 1</code>; this is just the formula above with <code>0x20</code> replaced with <code>0x01</code>. <a href="https://github.com/serde-rs/json/pull/1161#discussion_r1713040513">The cherry on top</a> is that <code>b&#39;\\&#39;</code> doesn’t have the sign bit set, so <code>c ^ b&#39;\\&#39; &gt;= 0</code> is equivalent to <code>c &gt;= 0</code>.</p><p>All in all, the formula simplifies to:</p><pre><code>!c
&amp; (
    (c - <span>0x2020202020202020</span>)
    | ((c ^ (<span>b&#39;\\&#39;</span> * <span>0x0101010101010101</span>)) - <span>0x0101010101010101</span>)
    | ((c ^ (<span>b&#39;&#34;&#39;</span> * <span>0x0101010101010101</span>)) - <span>0x0101010101010101</span>)
)
&amp; <span>0x8080808080808080</span>
</code></pre><p>This is <code>9</code> bitwise operations (counting <code>a &amp; !b</code> as one instead of two). For comparison, this would require <code>7</code> SIMD operations on x86, so that’s quite close to what we’d get from SIMD, just with 2x or 4x smaller throughput, depending on whether AVX is available.</p><p>But for short strings, throughput doesn’t matter. Latency does. Maybe I made a mistake while trying out different variations, but this SWAR code was more efficient than “real” SIMD code on <a href="https://github.com/serde-rs/json-benchmark">json-benchmark</a>, probably because of this effect. Whatever the reason, this is the code we settled on eventually.</p><p>Whenever you optimize something by unrolling a loop or using SIMD, a good question to ponder is whether the array is long enough to profit from this. For example, using SIMD to find the length of a 0-16 byte string in a branchless way is neat, but can easily lose to the simplest <code>strlen</code> implementation if the strings are usually just 3 bytes long.</p><p>Something similar happened here. For strings of around 5 characters, the SWAR approach became slower than scalar code. We decided that regressing such very short strings is a worthwhile investment if we get faster code in other cases.</p><p>However, there is one very common short string – the empty string <code>&#34;&#34;</code>. Also, due to a technicality, a similar regression applied to strings with consecutive escapes, e.g. <code>\r\n</code> or <code>\uD801\uDC37</code>, which is a really common sight in Unicode-ridden data. We certainly don’t want to regress that. The fix is simple: just check if the very first character is an escape before entering the SWAR loop.</p><p>All in all, the improvements we got are this:</p><div><table><thead><tr><td rowspan="2">Speed (MB/s, higher is better)</td><th colspan="2"><code>canada</code></th><th colspan="2"><code>citm_catalog</code></th><th colspan="2"><code>twitter</code></th></tr><tr><th>DOM</th><th>struct</th><th>DOM</th><th>struct</th><th>DOM</th><th>struct</th></tr></thead><tbody><tr><td>Scalar</td><td>291</td><td>442</td><td>377</td><td>865</td><td>305</td><td>638</td></tr><tr><td>Vectorized</td><td>292</td><td>442</td><td>367</td><td>905</td><td>335</td><td>785</td></tr><tr><td>Speedup</td><td>0%</td><td>0%</td><td>-3%</td><td>+5%</td><td>+10%</td><td>+23%</td></tr></tbody></table></div><p><code>citm_catalog DOM</code> is quite flickery, so in the end there aren’t even regressions on <code>json-benchmark</code>. There is one other regression though: empty strings still take a bit longer to parse, but the slowdown is luckily within 2% on a very specific microbenchmark.</p><h2>When lexing becomes complicated</h2><p>What else about Unicode, by the way? <code>serde_json</code> can parse Unicode in both decoded and encoded formats, e.g. <code>&#34;🥺&#34;</code> and <code>&#34;\ud83e\udd7a&#34;</code>. While raw Unicode is trivial to parse, decoding <code>\u</code> escapes is a more complicated topic.</p><p>Can’t you just parse four hex digits and that’s it? Well, sort of. Number parsing is really hard, and it might surprise you how generic and complex some algorithms are.</p><p>Parsing a hex digit requires mapping disjoint intervals <code>&#39;0&#39;..=&#39;9&#39;</code>, <code>&#39;A&#39;..=&#39;F&#39;</code>, <code>&#39;a&#39;..=&#39;f&#39;</code> to <code>0..16</code>. You could use conditionals for that:</p><pre><code><span>match</span> c {
    <span>b&#39;0&#39;</span>..=<span>b&#39;9&#39;</span> =&gt; c - <span>b&#39;0&#39;</span>,
    <span>b&#39;A&#39;</span>..=<span>b&#39;F&#39;</span> =&gt; c - <span>b&#39;A&#39;</span> + <span>10</span>,
    <span>b&#39;a&#39;</span>..=<span>b&#39;f&#39;</span> =&gt; c - <span>b&#39;a&#39;</span> + <span>10</span>,
    _ =&gt; <span>return</span> <span>Err</span>(..),
}
</code></pre><p>…or branchless algorithms, which the Rust standard library does.</p><p>But nothing beats a LUT.</p><pre><code><span>static</span> HEX: [<span>u8</span>; <span>256</span>] = {
    <span>const</span> __: <span>u8</span> = <span>255</span>; 
    [
        
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        <span>00</span>, <span>01</span>, <span>02</span>, <span>03</span>, <span>04</span>, <span>05</span>, <span>06</span>, <span>07</span>, <span>08</span>, <span>09</span>, __, __, __, __, __, __, 
        __, <span>10</span>, <span>11</span>, <span>12</span>, <span>13</span>, <span>14</span>, <span>15</span>, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, <span>10</span>, <span>11</span>, <span>12</span>, <span>13</span>, <span>14</span>, <span>15</span>, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
        __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, __, 
    ]
};

<span>fn</span> <span>decode_hex_val</span>(val: <span>u8</span>) <span>-&gt;</span> <span>Option</span>&lt;<span>u16</span>&gt; {
    <span>let</span> <span>n</span> = HEX[val <span>as</span> <span>usize</span>] <span>as</span> <span>u16</span>;
    <span>if</span> n == <span>255</span> {
        <span>None</span>
    } <span>else</span> {
        <span>Some</span>(n)
    }
}
</code></pre><p>This is what <code>serde_json</code> used to utilize, and it actually worked pretty well (better than <code>std</code>, anyway: <code>std</code> has to be generic over radix, <code>serde_json</code> doesn’t have to). This function would then be used like this:</p><pre><code><span>let</span> <span>mut </span><span>n</span> = <span>0</span>;
<span>for</span> <span>_</span> <span>in</span> <span>0</span>..<span>4</span> {
    n = (n &lt;&lt; <span>4</span>) + <span>decode_hex_val</span>(<span>self</span>.slice[<span>self</span>.index])?;
    <span>self</span>.index += <span>1</span>;
}
</code></pre><p>That’s at least 3 <code>shl</code>s and 3 <code>add</code>s, with quite a few <code>mov</code>s, <code>cmp</code>s with 255, and conditional jumps inbetween. We can do better.</p><p>Let’s start by removing the <code>?</code> on each iteration. That’s quite simple. Instead of storing <code>HEX</code> as a <code>[u8; 256]</code> array, we can store it as a <code>[u32; 256]</code> array, mapping <code>__</code> to <code>u32::MAX</code>. No valid digit has the high 16 bits set, so we can easily figure out if some digit was invalid <em>after</em> the loop:</p><pre><code><span>let</span> <span>mut </span><span>n</span> = <span>0</span>;
<span>for</span> <span>_</span> <span>in</span> <span>0</span>..<span>4</span> {
    n = (n &lt;&lt; <span>4</span>) + HEX[<span>self</span>.slice[<span>self</span>.index] <span>as</span> <span>usize</span>];
    <span>self</span>.index += <span>1</span>;
}
ensure!(n &gt;= <span>65536</span>, <span>&#34;Invalid Unicode escape&#34;</span>);
<span>let</span> <span>n</span> = n <span>as</span> <span>u16</span>;
</code></pre><p>Saving memory (and thus cache!) by using <code>u16</code> instead of <code>u32</code> looks impossible, because a <code>u16::MAX = 0xFFFF</code> in the leading digit would quickly get shifted to <code>0xFxxx</code> in <code>n</code>, and at that point you can’t disambiguate between a valid codepoint and an invalid digit.</p><p>Or is it? Here’s a trick <a href="https://github.com/yuki0iq/">Yuki</a> invented. We can map <code>__</code> to <code>u16::MAX</code>, but also replace <code>n &lt;&lt; 4</code> with <code>n.rotate_left(4)</code> and addition with bitwise OR:</p><pre><code><span>let</span> <span>mut </span><span>n</span> = <span>0</span>;
<span>for</span> <span>_</span> <span>in</span> <span>0</span>..<span>4</span> {
    n = n.<span>rotate_left</span>(<span>4</span>) | HEX[<span>self</span>.slice[<span>self</span>.index] <span>as</span> <span>usize</span>];
    <span>self</span>.index += <span>1</span>;
}
</code></pre><p>If all hex digits are valid, nothing’s changed, <code>n</code> is still our codepoint. Rotation is exactly as efficient as shifts on x86, so no issues performance-wise either. But if some hex digit is invalid, it’s going to “infect” <code>n</code>, setting it to <code>0xFFFF</code>, and the next iterations <em>will keep yielding <code>0xFFFF</code></em>. Unicode defines <code>U+FFFF</code> as a codepoint that does not signify a character, meaning that it’s extremely unlikely to be used in realistic data, so we can just branch on <code>n == 0xFFFF</code> afterwards and re-check if we should emit an error or the JSON genuinely contained a <code>\uFFFF</code>. Isn’t that neat?</p><p>Just as I was writing this post I <a href="https://github.com/serde-rs/json/pull/1178">realized</a> that this is a classical case of overengineering. The <code>0xFFFF</code> only gets shifted out if we compute the codepoint in 16-bit arithmetic. But we aren’t in the stone age; we have 32-bit integers! Let’s store <code>HEX</code> as <code>[i8; 256]</code>, with <code>-1</code> stands for an invalid digit. Then</p><pre><code><span>let</span> <span>mut </span><span>n</span> = <span>0</span>;
<span>for</span> <span>_</span> <span>in</span> <span>0</span>..<span>4</span> {
    n = (n &lt;&lt; <span>4</span>) | HEX[<span>self</span>.slice[<span>self</span>.index] <span>as</span> <span>usize</span>] <span>as</span> <span>i32</span>;
    <span>self</span>.index += <span>1</span>;
}
</code></pre><p>…will produce a non-negative number on success and a negative number on failure. The seemingly operational <code>as i32</code> turns out to be a no-op because x86 fuses a memory load and a sign extension into one <code>movsx</code> instruction.</p><p>What I like about signed numbers is that most processors have a single instruction to branch on sign. Instead of <code>cmp r, imm; je label</code>, we can just do <code>js label</code> in most cases. This does not usually affect performance on modern CPUs, but hey, at least it looks prettier.</p><p>Shifts increase latency. Latency bad. Alisa want no latency.</p><p>Luckily, this is easy to fix by introducing two tables instead of one: <code>HEX0</code>, which is <code>HEX</code> cast to <code>[i16; 256]</code>, and <code>HEX1</code>, which is <code>HEX</code> cast to <code>[i16; 256]</code> but also left-shifted by <code>4</code>. This allows the loop to be unrolled very clearly and is the final hex-decoding implementation.</p><pre><code><span>fn</span> <span>decode_four_hex_digits</span>(a: <span>u8</span>, b: <span>u8</span>, c: <span>u8</span>, d: <span>u8</span>) <span>-&gt;</span> <span>Option</span>&lt;<span>u16</span>&gt; {
    <span>let</span> <span>a</span> = HEX1[a <span>as</span> <span>usize</span>] <span>as</span> <span>i32</span>;
    <span>let</span> <span>b</span> = HEX0[b <span>as</span> <span>usize</span>] <span>as</span> <span>i32</span>;
    <span>let</span> <span>c</span> = HEX1[c <span>as</span> <span>usize</span>] <span>as</span> <span>i32</span>;
    <span>let</span> <span>d</span> = HEX0[d <span>as</span> <span>usize</span>] <span>as</span> <span>i32</span>;

    <span>let</span> <span>codepoint</span> = ((a | b) &lt;&lt; <span>8</span>) | c | d;

    
    <span>if</span> codepoint &gt;= <span>0</span> {
        <span>Some</span>(codepoint <span>as</span> <span>u16</span>)
    } <span>else</span> {
        <span>None</span>
    }
}
</code></pre><p>Overall, this increased the performance of parsing JSON-encoded <em>War and Peace</em> in Russian from 284 MB/s to 344 MB/s, resulting in a 21% improvement.</p><h2>Transcoding</h2><p>After the last optimization, the bottleneck of Unicode string parsing shifted to UTF-8 encoding.</p><p>This is funny, because UTF-8 is supposed to be really simple. To give a quick reminder, UTF-8 encodes codepoints in one of the following ways:</p><ul><li>1 byte: <code>0xxxxxxx</code></li><li>2 bytes: <code>110xxxxx 10xxxxxx</code></li><li>3 bytes: <code>1110xxxx 10xxxxxx 10xxxxxx</code></li><li>4 bytes: <code>11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</code></li></ul><p>The <code>x</code>s signify the bits of a codepoint; the shortest representation long enough to fit all the bits is used. All codepoints fit in 21 bits.</p><p>Rust’s standard library implements UTF-8 encoding by providing, among other functions, <code>char::encode_utf8</code> to store the char encoding to a buffer. There is, however, one minor inconvenience. The signature of the method is</p><pre><code><span>fn</span> <span>encode_utf8</span>(<span>self</span>, dst: &amp;<span>mut</span> [<span>u8</span>]) <span>-&gt;</span> &amp;<span>mut</span> <span>str</span>;
</code></pre><p>…which means that it writes to a buffer that already stores <em>valid</em> <code>u8</code>s. You can’t just create an <em>uninitialized</em> buffer and put UTF-8 there; you need to initialize (e.g. zero-initialize) it in advance.</p><p>The assumption here is that the optimizer is smart enough to optimize out zeroization. This <em>would</em> be true in other cases, but UTF-8 is a <em>variable-length</em> encoding. If you zeroed more bytes than <code>encode_utf8</code> would place, zeroization would be wrong to optimize out. So you need to zeroize a variable amount of bytes. But the semantics become too complicated to capture accurately for LLVM here, so it just drops the ball.</p><hr/><p>So <code>serde_json</code> used another approach:</p><pre><code>scratch.<span>extend_from_slice</span>(c.<span>encode_utf8</span>(&amp;<span>mut</span> [<span>0u8</span>; <span>4</span>]).<span>as_bytes</span>());
</code></pre><p><code>[0u8; 4]</code> is a local variable, so zeroing more bytes than necessary shouldn’t be a problem because aliasing analysis should help with this. Which is kind of true in theory.</p><p>In practice, something horrendous happens instead. Remember how LLVM drops the ball on variable-length zeroization? Well, it drops the ball on a variable-length copy too. <code>Vec::extend_from_slice</code> needs to copy 1 to 4 bytes from the local buffer to heap, so LLVM invokes glibc’s <code>memcpy</code> to do that. Wonderful.</p><hr/><p>The best way to avoid calls to <code>memset</code> and <code>memcpy</code> turned out to be generating UTF-8 manually. This is trivial algorithm-wise, but requires unsafety, so I was initially somewhat scared of that, but I had to submit.</p><p>Together with a few other minor modifications, this further increased performance on <em>War and Peace</em> to 374 MB/s (+9%).</p><h2>Final results</h2><p>All in all, my work improved <code>serde_json</code> performance on various string-heavy JSON benchmarks by 10%, 23%, and 32%. A lot of JSON data contains many strings, so I believe that this will benefit the ecosystem in the long run.</p></div></section></div>
  </body>
</html>
