<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.definite.app/blog/duck-takes-flight">Original</a>
    <h1>Adding concurrent read/write to DuckDB with Arrow Flight</h1>
    
    <div id="readability-page-1" class="page"><div><p><span>January 25, 2025</span><span>10 minute read</span></p><p>Mike Ritchie</p></div><div>
<p>We&#39;ve been thinking a lot about latency, streaming and (near) real-time analytics lately. At <a href="https://definite.app">Definite</a>, we deal with a lot of data pipelines. In most cases (e.g. ingesting Stripe data), our customers are fine with batch processing (e.g. every hour). But as we&#39;ve grown, we&#39;ve seen more and more need for near real-time pipelines (e.g. ingesting events or CDC from Postgres).</p>
<p>But before I go any further, while writing this, I came across <a href="https://www.youtube.com/watch?v=21gOcdU1WeY">this video</a> and you really need to watch it. I&#39;ll see you back here in ~8 minutes.</p>
<p>Ok, quack to business.</p>
<p>DuckDB is amazing for &#34;single player&#34; analytics, but it has a couple serious limitations that prevent it from being used in more traditional data workflows.</p>
<ol>
<li>DuckDB doesn&#39;t support concurrent writers</li>
<li>You can&#39;t read (from a seperate process) while writing</li>
</ol>
<p>This is fine for batch processing, but it&#39;s a problem if you want to stream data continuously while running analytics.</p>
<p>There are several ways to tackle the problem, but one approach I really like leverages Arrow Flight. <a href="https://github.com/definite-app/duck-takes-flight">Duck Takes Flight</a> is about 200 lines of Python, and it neatly sidesteps DuckDB&#39;s concurrency limitations and shows how to use Flight paired with DuckDB.</p>
<h2>DuckDB + Flight</h2>
<p>Here&#39;s what the Flight server looks like:</p>
<pre><code>class DuckDBFlightServer(flight.FlightServerBase):
    def __init__(self, location=&#34;grpc://localhost:8815&#34;, db_path=&#34;duck_flight.db&#34;):
        super().__init__(location)
        self.db_path = db_path
        self.conn = duckdb.connect(db_path)

    def do_get(self, context, ticket):
        query = ticket.ticket.decode(&#34;utf-8&#34;)
        result_table = self.conn.execute(query).fetch_arrow_table()
        batches = result_table.to_batches(max_chunksize=1024)
        return flight.RecordBatchStream(pa.Table.from_batches(batches))

    def do_put(self, context, descriptor, reader, writer):
        table = reader.read_all()
        table_name = descriptor.path[0].decode(&#39;utf-8&#39;)
        batches = table.to_batches(max_chunksize=1024)
        aligned_table = pa.Table.from_batches(batches)
        self.conn.register(&#34;temp_table&#34;, aligned_table)
        self.conn.execute(f&#34;INSERT INTO {table_name} SELECT * FROM temp_table&#34;)
</code></pre>
<p>That&#39;s it. The server just sits there accepting data and queries.</p>
<h2>Feeding It Some Data</h2>
<p>The <code>load_data.py</code> script generates some random data and sends it to our Flight server:</p>
<pre><code>def generate_batch(batch_id):
    num_rows = 1_000
    data = {
        &#34;batch_id&#34;: [batch_id] * num_rows,
        &#34;timestamp&#34;: [datetime.now().isoformat()] * num_rows,
        &#34;value&#34;: [random.uniform(0, 100) for _ in range(num_rows)],
        &#34;category&#34;: [random.choice([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) for _ in range(num_rows)]
    }
    return pa.Table.from_pydict(data)
</code></pre>
<p>You can run multiple instances of this loader simultaneously. Try starting a few copies and watch them feed data in parallel. In contrast, vanilla DuckDB will throw an error as soon as you attempt to connect a second writer.</p>
<h2>Querying It</h2>
<p>And here&#39;s how you query it:</p>
<pre><code>def execute_query(client, query):
    try:
        ticket = flight.Ticket(query.encode(&#34;utf-8&#34;))
        reader = client.do_get(ticket)
        result = reader.read_all()
        return result
    except Exception as e:
        print(f&#34;Query error: {str(e)}&#34;)
        return None
</code></pre>
<p>The magic here is that these queries work while data is being loaded. You can have multiple loaders feeding data in and multiple clients running queries, all at the same time. That&#39;s not something DuckDB can do on its own.</p>
<h2>Running It</h2>
<p>If you want to try it:</p>
<pre><code>pip install duckdb pyarrow
python duckdb_flight_server.py
python load_data.py
python query_data.py
</code></pre>
<p>That&#39;s all it takes. No clusters or dependency hell (you only need <code>duckdb</code> and <code>pyarrow</code>). And unlike vanilla DuckDB, you can run as many writers and readers as you want.</p>
<h2>Conclusion</h2>
<p>If you&#39;re building something that needs to move data around quickly and query it on the fly - especially if you need concurrent access that DuckDB doesn&#39;t provide - this approach is worth exploring. It might be all you need. The full code is available on <a href="https://github.com/definite-app/duck-takes-flight">GitHub</a>.</p>
<p>And if you&#39;re looking for a complete analytics solution that handles everything from ETL to warehousing to BI, check out what we&#39;re building at <a href="https://definite.app">Definite</a>. We&#39;re taking this same philosophy - making powerful data tools simple and accessible - and applying it to the entire analytics stack.</p></div></div>
  </body>
</html>
