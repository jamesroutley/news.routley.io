<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://ai.facebook.com/research/no-language-left-behind/">Original</a>
    <h1>No Language Left Behind</h1>
    
    <div id="readability-page-1" class="page"><div><div data-ms="{&#34;creative&#34;:&#34;section&#34;,&#34;creative_detail&#34;:&#34;section&#34;,&#34;create_type&#34;:&#34;section&#34;,&#34;create_type_detail&#34;:&#34;section&#34;}"><div><div><div><h4>LASER (Language-agnostic sentence representations)</h4><h5>2018</h5><p>The first successful exploration of massively multilingual sentence representations shared publicly with the NLP community. The encoder creates embeddings to automatically pair up sentences sharing the same meaning in 50 languages.</p></div><div><h4>WMT-19</h4><h5>2019</h5><p>FB AI models outperformed all other models at WMT 2019, using large-scale sampled back-translation, noisy channel modeling and data cleaning techniques to help build a strong system.</p></div><div><h4>Flores V1</h4><h5>2019</h5><p>A benchmarking dataset for MT between English and low-resource languages introducing a fair and rigorous evaluation process, starting with 2 languages.</p></div><div><h4>WikiMatrix</h4><h5>2019</h5><p>The largest extraction of parallel sentences across multiple languages: Bitext extraction of 135 million Wikipedia sentences in 1,620 language pairs for building better translation models.</p></div><div><h4>M2M-100</h4><h5>2020</h5><p>The first, single multilingual machine translation model to directly translate between any pair of 100 languages without relying on English data. Trained on 2,200 language directions â€”10x more than previous multilingual models.</p></div><div><h4>CCMatrix</h4><h5>2020</h5><p>The largest dataset of high-quality, web-based bitexts for building better translation models that work with more languages, especially low-resource languages: 4.5 billion parallel sentences in 576 language pairs.</p></div><div><h4>LASER 2</h4><h5>2020</h5><p>Creates embeddings to automatically pair up sentences sharing the same meaning in 100 languages.</p></div><div><h4>WMT-21</h4><h5>2021</h5><p>For the first time, a single multilingual model outperformed the best specially trained bilingual models across 10 out of 14 language pairs to win WMT 2021, providing the best translations for both low- and high-resource languages.</p></div><div><h4>FLORES-101</h4><h5>2021</h5><p>FLORES-101 is the first-of-its-kind, many-to-many evaluation data set covering 101 languages, enabling researchers to rapidly test and improve upon multilingual translation models like M2M-100.</p></div><div><h4>NLLB-200</h4><h5>2022</h5><p>The NLLB model translates 200 languages.</p></div><div><h4>FLORES 200</h4><h5>2021</h5><p>Expansion of FLORES evaluation data set now covering 200 languages</p></div><div><h4>NLLB-Data-200</h4><h5>2022</h5><p>Constructed and released training data for 200 languages</p></div><div><h4>LASER 3</h4><h5>2022</h5><p>Creates embeddings to automatically pair up sentences sharing the same meaning in 200 languages.</p></div></div></div></div></div></div>
  </body>
</html>
