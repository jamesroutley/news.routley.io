<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://educatedguesswork.org/posts/challenges-web-decentralization/">Original</a>
    <h1>Challenges in building a decentralized web</h1>
    
    <div id="readability-page-1" class="page"><div>
          <main>
            
<div>
  <div>
    <article>
        
        

        

<p>There&#39;s been a lot of interest lately in what&#39;s often termed the
<a href="https://en.wikipedia.org/w/index.php?title=Decentralized_web&amp;oldid=1083941536">Decentralized Web</a> (dWeb),
though now it&#39;s quite common to hear the term <a href="https://en.wikipedia.org/w/index.php?title=Web3&amp;oldid=1083462159">Web3</a>
used as well. Mapping out the precise distinctions between these terms—assuming that&#39;s
possible—is outside the scope of this post (though it seems that Web3 somehow
involves blockchains), but the common thread here seems to be replacing the existing
rather centralized Web ecosystem with one that is, well, less centralized.
This post looks at the challenges of actually building a system like this.</p>
<p>The infrastructure of the Web is centralized in at least two major ways:</p>
<ol>
<li>
<p>There are relatively few major user-facing content distribution platforms
(Google, YouTube, Facebook, Twitter, TikTok, etc.) and they clearly have
outsized power over people&#39;s ability to get their message amplified.</p>
</li>
<li>
<p>Even if you&#39;re willing to forego posting on one of those content platforms,
the easiest way to build any large-scale system—and almost the only
economical way unless you are very well-funded—is to run it on
one of a relatively small number of infrastructure providers, such
as <a href="https://aws.amazon.com/">Amazon Web Services</a>, <a href="https://cloud.google.com/gcp/">Google Cloud Platform</a>,
<a href="https://www.cloudflare.com/">Cloudflare</a>, <a href="https://www.fastly.com/">Fastly</a>, etc.,
who already have highly scalable geographically distributed systems.</p>
</li>
</ol>
<p>In this context, decentralizing can mean anything from building
analogs to those specific content platforms that operate in a less centralized
fashion (e.g., <a href="https://joinmastodon.org/">Mastodon</a> or
<a href="https://diaspora.social/">Diaspora</a>) to rebuilding the entire
structure of the Web on a peer to peer platform like
<a href="https://ipfs.io/">IPFS</a> or <a href="https://beakerbrowser.com/">Beaker</a>.
Naturally, in the second case, you would also want to make it possible
to reproduce these content platforms—only better!—using
a mostly or fully peer-to-peer system; at least it shouldn&#39;t be
required to have a bunch of big servers somewhere to make it all work.
This second, more ambitious, project is the topic of this post.</p>
<h2 id="distributed-versus-decentralized">Distributed Versus Decentralized <a href="#distributed-versus-decentralized">#</a></h2>
<p>An important distinction to draw here is between systems which are <em>distributed</em>
(also often called <em>federated</em>) and those which are <em>decentralized</em>
(often called <em>peer-to-peer</em>). As an example, the Web is a distributed
system: it consists of lots of different sites operated by different
entities, but those sites run on servers and operating a site requires
running a server yourself or outsourcing that to someone else. Those servers have
to be prepared to handle the load for all your users, which means they
have to be somewhere with a lot of bandwidth, scale gracefully as more
users try to connect, etc.</p>
<p>By contrast,
<a href="https://en.wikipedia.org/w/index.php?title=BitTorrent&amp;oldid=1083471967">BitTorrent</a>
is a decentralized system: it uses the resources of BitTorrent users
themselves to serve data, which means that you don&#39;t need a giant
server to publish data into the BitTorrent network, even if a lot of
other people want to download it. This has some obvious operational
advantages even in a world where bandwidth is cheap, but especially if
you want to publish something which others would prefer wasn&#39;t
published, perhaps because of government censorship or more frequently
for copyright reasons. If you run a server, it&#39;s pretty hard to
conceal that a million people just connected to download <a href="https://en.wikipedia.org/w/index.php?title=John_Wick:_Chapter_3_%E2%80%93_Parabellum&amp;oldid=1081953064">John Wick:
Chapter 3 -
Parabellum</a>
(a pretty solid outing by Keanu, btw), and you should expect the
copyright police to come after you (see here, <a href="https://en.wikipedia.org/w/index.php?title=Kim_Dotcom&amp;oldid=1083570626">Kim Dotcom</a>)
but if you just publish your
copy into the BitTorrent network, it&#39;s a lot harder to figure out who
it was, especially if 50 other people did the same.</p>
<p>Note that it&#39;s possible to have mixed systems that are largely decentralized
but depend on centralized components. For instance, in a peer-to-peer system,
new peers often need to connect to some &#34;introduction server&#34; to help them
join the network; those servers need to be easy to find and one—though
not the only way—to
do that is to have them be operated centrally.</p>
<p>Historically, peer-to-peer systems have seen deployment in relatively
limited domains, mostly those associated with some kind of
deployment outside of the aforementioned censorship-resistance use case.
However, there has certainly been plenty of interest in broader use
cases, up to and including displacing large pieces of the Web.
This is a very difficult problem, in part because this kind of
system is inherently less efficient and flexible than a centralized or federated
system. This post looks at the challenges involved in building such a
system. This isn&#39;t to say it&#39;s not also challenging to
build something like Twitter or Facebook in a more federated fashion,
but the problems are of a different scale (and perhaps the subject of
a different post).</p>
<div>
<h4 id="peer-to-peer-versus-client%2Fserver">Peer-to-Peer versus Client/Server <a href="#peer-to-peer-versus-client%2Fserver">#</a></h4>
<p>The opposite of peer-to-peer is <em>client/server</em>, i.e., a system in
which the elements take on asymmetrical roles, with one element (often that belonging to the
user)
being the &#34;client&#34; and the other element (often some
kind of shared resource associated with an organization) being the &#34;server&#34;.
This is, for instance, how the Web works, with the client being the
browser. By contrast, peer-to-peer systems are thought of as
symmetrical.</p>
<p>In practice, however, the lines can be quite blurry. For instance,
common to have systems in which the same protocols are used to talk
between clients and servers and also between servers, with the second
mode more like a typical &#34;peer-to-peer&#34; configuration. For instance,
mail clients use SMTP to send e-mail but mail servers also use SMTP
to send e-mail to each other, with the sender taking on the &#34;client&#34;
role; obviously in this case, each &#34;server&#34; is both client and server,
depending on which direction the mail is flowing. Even in systems
which are nominally peer-to-peer, it&#39;s common to use protocols which
were designed for client/server applications (e.g., <a href="https://www.rfc-editor.org/rfc/rfc8846.html">TLS</a>),
in which case the nodes may take on client/server roles for those protocol
purposes even if the application above is symmetrical.</p>
</div>
<h2 id="basics-of-peer-to-peer-systems">Basics of Peer-to-Peer Systems <a href="#basics-of-peer-to-peer-systems">#</a></h2>
<p>We all (hopefully) know how a client/server publishing system like the
Web works (if not, review my <a href="https://0x539.lol/posts/web-security-model-intro1/">intro
post</a>, but how does a peer-to-peer
(hence-forth P2P) publishing system work?  Let&#39;s start by discussing
the simplest case, which is just publishing opaque binary resources
(documents, movies, whatever). This section tries to describe
just enough basics of such a system to have the rest of this post make sense.</p>
<p>In a client/server system, the resource to be published is stored
on the server, but in a P2P system, there are no servers, so the
resource is stored &#34;in the network&#34;. What this means operationally
is that it&#39;s stored on the computers of some subset of the users
who happen to be online at the moment. In order to make this work,
then, we need a set of rules (i.e., a protocol) that describes
which endpoints store a specific piece of content and how to find
them when you want to retrieve it. A common design here is what&#39;s
called a <a href="https://en.wikipedia.org/w/index.php?title=Distributed_hash_table&amp;oldid=1076477001">Distributed Hash Table</a>, which is basically an abstraction in which every resource
has a &#34;key&#34; (i.e., an address) which is used to reference it and a &#34;value&#34; which is
its actual content. The key determines which node(s) are responsible
for storing the value and is used by other nodes to store and/or
retrieve it.</p>
<p>As an intuition pump, consider the following toy DHT system. This is
an oversimplified version of <a href="https://en.wikipedia.org/w/index.php?title=Chord_(peer-to-peer)&amp;oldid=1082459600">Chord</a>,
one of the first DHTs, so let&#39;s call it &#34;Note&#34;. In Note, every
node in the system has a randomly generated identifier which is
just a number from $0$ to $2^{256}-1$ (sorry for the LaTeX notation,
newsletter folks). It&#39;s conventional to think of these being
organized in a circle, with the ids being assigned clockwise,
so that node $2^{256}-1$ is right next to (before) node $0$,
as shown in the following diagram:</p>
<p><img src="https://0x539.lol/img/note-dht.drawio.png" alt="note DHT ring"/></p>
<p>Each node in the network (the &#34;ring&#34;) maintains a set of
connections to some other set of nodes in the ring
(the arrows are colored according to the node maintaining
the connection). I won&#39;t
go into detail about the algorithms here, except to say that
having that work efficiently is a lot of the science of making a DHT.
In Note, we&#39;ll just assume that each node has a connection to the next
node (i.e., the one with the next highest identity) and to
some other nodes further along the ring, as shown in the
figure above.</p>
<p>In order to communicate with a node with id $i$,
a node sends a message to the node that it is connected
to with id $j$ that is closest to but not greater
than $i$ (i.e., that if you went around the circle
clockwise, there would be no node that you were
connected to that was in between them). Node $i$ does
the same. When you finally reach a node that is connected
directly to $j$, it delivers the message.
For instance, if node <strong>0</strong> wanted to send a message to node
<strong>c</strong> it would send it to <strong>b</strong> who would send it to <strong>c</strong>.
When <strong>c</strong> wants to reply, it sends it to node <strong>e</strong> which
is connected to node <strong>0</strong> and so sends it directly.
Note that this means that a request/response
pair takes an entire trip around the ring.</p>
<h3 id="storing-data">Storing Data <a href="#storing-data">#</a></h3>
<p>So far we just have a communications system, but it&#39;s (relatively)
easy to turn it into a storage system: we give each piece of
data an address in the same namespace as the node identifiers and
each node is responsible for storing any data with an address that
falls between it and the previous node. So, for instance, in the
diagram below, node <strong>c</strong> would be responsible for storing
the resource with address <strong>k</strong> and node <strong>e</strong> would be responsible
for storing the resource with address <strong>l</strong>.</p>
<p><img src="https://0x539.lol/img/note-dht-storage.drawio.png" alt="note DHT ring"/></p>
<p>If node <strong>a</strong> wants to store a value with address <strong>k</strong>
it would craft a message to <strong>c</strong> asking to store it. Similarly,
if node <strong>d</strong> wants to retrieve it, it would send a message to <strong>c</strong>.</p>
<p>Of course, there are several obvious problems here. First, what
happens if node <strong>c</strong> drops off the network? After all, it&#39;s somebody&#39;s
personal computer, so they might turn it off at any moment. The
natural answer to this is to <em>replicate</em> the data to some other
set of nodes so that there is a suitably low probability that
they will all go offline at once. The precise replication strategy
is also a complicated topic that varies depending on the DHT, and we don&#39;t need to go into it here.</p>
<p>Second, what if some value is both large and popular? In that case,
the node(s) storing it might suddenly have to transfer a lot of
data all at once. It&#39;s easy for this to totally saturate someone&#39;s
link, even if they have a fast Internet connection. The only real
fix is to distribute the load, which you can do in two ways.
First, you can shard the resource (e.g., break up your movie into
5 minute chunks) and then store each shard under a different address;
this has the impact that different nodes will be responsible for
sending each chunk and so their share of the bandwidth is
correspondingly reduced. You can also try to make more nodes
responsible for popular content, which also spreads out the
load.</p>
<p>Finally, if every message has to traverse several nodes in order
to be delivered, this increases the total load on the network
proportional to the path length (the number of nodes) as
well as decreasing performance due to latency. One way
to deal with that is to have the two communicating nodes establish
a direct connection for the bulk data transfer and just use the
DHT to get the in contact so they can do that. This significantly
reduces the overall load.</p>
<h3 id="naming-things">Naming Things <a href="#naming-things">#</a></h3>
<p>In the previous description, I&#39;ve handwaved how the addresses
for things are derived.</p>
<p>One common design is to compute the address
from the content of the object, for instance by hashing it. This is
what&#39;s called <em>Content Addressable Storage (CAS)</em> and is convenient in
a number of situations because it doesn&#39;t require any additional
content integrity in the DHT. If you know the hash of the object you
can retrieve it and then if the hash comes out wrong, you know there
has been a problem retrieving it.</p>
<p>Of course, given that you need the object in order to compute its
hash, this kind of design means that you need some service to map
objects whose names you know (e.g., &#34;John Wick&#34;) onto their
hashes, so now we either have a centralized service that does that or
we need to build a peer-to-peer version of that service and
we&#39;re back where we started.</p>
<p>Another common approach is to have names that are derived from
cryptographic keys. For instance, we might say that all of my
data is stored at the hash of my public key (again, maybe with
some suitable sharding system). When the data gets stored we would
require it to be signed and nodes would discard stored values whose
signatures didn&#39;t validate. This has a number of advantages, but one
critical one is that you can have the data at a given address <em>change</em>
because the address is tied to the cryptographic key not the content.
For instance, supposing that what&#39;s being stored is my Web site;
I might want to change that and not want to have to publish a new
address. With an address tied to keys this is possible.</p>
<p>Obviously, cryptographic keys don&#39;t make great identifiers either, because
they are hard to remember, but presumably
you would layer some kind of decentralized naming layer on top,
for instance one based on a <a href="https://0x539.lol/posts/dns-security-blockchain/">blockchain</a>.</p>
<h3 id="security">Security <a href="#security">#</a></h3>
<p>Any real system needs some way of ensuring the integrity of the
content. Unlike the Web, it&#39;s not enough to establish a TLS connection to the storing
node, because that&#39;s just someone&#39;s computer and it could lie (though you
still may want to for privacy reasons).
Instead, each object needs to be somehow integrity protected,
either by having its address be its hash or by being digitally signed.</p>
<p>Aside from the integrity of the content, there&#39;s still a lot to go wrong here. For instance,
what happens if the responsible node claims that a given object (or a
node you are trying to route to) doesn&#39;t exist? Or what if a set of
nodes try to saturate the network with traffic via a DDoS attack?
How do you deal with people trying to store or retrieve more than their
&#34;fair share&#34; (whatever that is) of data.
There are various approaches people have talked about to try to
address these issues, but our operational experience with DHTs is at a
smaller scale than our operational experience with the Web,
and in a setting that was much more tolerant of failure
(Disney doesn&#39;t lose a lot of money if people suddenly can&#39;t
download Frozen from BitTorrent)
and so it&#39;s not clear that they can be made to be really secure
at scale.</p>
<h2 id="a-decentralized-web-publishing-system">A Decentralized Web Publishing System <a href="#a-decentralized-web-publishing-system">#</a></h2>
<p>Now that we have a way to store data and find it again, we have the
start of how one might imagine building a decentralized version of
the Web. As we did when <a href="https://0x539.lol/posts/web-security-model-intro1/">looking at how the Web works</a> let&#39;s just
start with publishing static documents.</p>
<p>Recall the structure of URIs:</p>
<p><img src="https://0x539.lol/img/URL-structure.drawio.png" alt="URL Structure"/></p>
<p>What we need to do is to map this structure onto resources in
our P2P storage system. So we might end up with a URL like
the following:</p>
<p><img src="https://0x539.lol/img/URL-structure-note.drawio.png" alt="URL Structure for a P2P system"/></p>
<div>
<h4 id="the-origin">The Origin <a href="#the-origin">#</a></h4>
<p>A critical security requirement in this system is that
data associated with different authorities has different
origins (see <a href="https://0x539.lol/posts/web-security-model-origin/">here</a> for
background). If data published by multiple users has
<strike>different origins</strike> the same origin [2022-04-25 -- EKR], then they could attack each other
via the browser, which is an obvious problem.</p>
</div>
<p>The <code>note:</code> at the start tells us that we need to retrieve
the data using Note and not via HTTP. In the middle
section, instead of having a &#34;host&#34; field which tells us where
to retrieve the content in an ordinary HTTPS URI, we instead
have an &#34;authority&#34; field which just tells us the identity
of the user whose key will be used to sign the data for the
URL. As above, I&#39;m assuming we have some way of mapping
user friendly identities to keys; some systems don&#39;t have that,
which seems pretty user-hostile, but feel free to just think of
the authority as being a key hash if you prefer.</p>
<p>The resource itself is stored at an address given by <code>Hash(URL)</code>
(this is a small but simple change from my description above),
and as above, is signed by key associated with the authority.</p>
<p>This is all pretty straightforward if you assume the existence
of the P2P system in the first place. In order to publish
something, I do a store into the DHT at the address indicated
by the URL and sign it with my key. I can then hand the
URL to people who can retrieve the data from the DHT by
computing the address and then verifying the signed resource.
Note that because the address is computed from the URL and
not from the content, it can be updated in place just by
doing a new store.</p>
<p>Taking a step back, this really does sort of deliver on the value
proposition I described above: anyone can publish a site into
the network without having to have a room full of computers
or pay Amazon/Google/Fastly, etc. And so if you don&#39;t look
too closely, it seems like mission accomplished and it&#39;s easy
to understand the enthusiasm. Unfortunately this system also has some pretty serious drawbacks.</p>
<h3 id="performance">Performance <a href="#performance">#</a></h3>
<p>Performance—in this case the time it takes a page to load—is
a major consideration for Web browsers and servers.
What mostly matters for Web performance is the time it takes to
retrieve each resource. This is different from, say, videoconferencing
or gaming, where latency (the time it takes your packets to
get to the other side) or jitter (variation in latency) really matter.
In the Web it&#39;s mostly about download speed.</p>
<h4 id="connections">Connections <a href="#connections">#</a></h4>
<p>In order to understand the performance implications of a shift from
client/server to peer-to-peer it&#39;s necessary to understand a little
bit about how networking and data transfer works.  The Internet is a
<em>packet-switched</em> network, which means that it carries individually
addressed messages that are on the order of 1000 bytes. Because Web
resources are generally larger than 1K, clients and servers transfer
data by establishing a <em>connection</em>, which is a persistent association
on both sides that maps a set of packets into what looks like a stream
of data that each side can read and write to. The sender breaks the
file up into packets and sends them and the receiver is responsible
for reassembling them on receipt. Historically this was done by
<a href="https://en.wikipedia.org/w/index.php?title=Transmission_Control_Protocol&amp;oldid=1083491738">TCP</a>,
though are now seeing increased use of
<a href="https://en.wikipedia.org/w/index.php?title=QUIC&amp;oldid=1083353797">QUIC</a>,
which operates on similar principles, at least at the level we need to
talk about here).</p>
<p>The figure below shows the beginning of an HTTPS connection using TCP
and TLS 1.3 for security.</p>
<p><img src="https://0x539.lol/img/https-hs.png" alt="HTTPS Connection Ladder Diagram"/></p>
<div>
<h4 id="increasing-the-number-of-http-requests-on-a-connection">Increasing the number of HTTP Requests on a Connection <a href="#increasing-the-number-of-http-requests-on-a-connection">#</a></h4>
<p>When HTTP was originally designed, you could only have one
request on a single connection. This was horribly inefficient
for the reasons I&#39;ve described here, and—in
large part due to the work of <a href="http://jmogul.com/jeff.html">Jeff Mogul</a>—a
feature was added that allowed multiple requests to be issued
on the same connection. Unfortunately, those requests could
only be issued serially, which created a new bottleneck. In
response, browsers started creating multiple connections
in parallel to the same site, which let them make multiple
requests at once (as well as sometimes grab a larger fraction
of the available bandwidth, due to TCP dynamics). In 2015,
<a href="https://www.rfc-editor.org/rfc/rfc7540.html">HTTP/2</a>
added the ability to multiplex multiple requests on the same
TCP connection, with the responses being interleaved, but
still had the problem that a packet lost for response A
stalled every other response (a property called
<a href="https://en.wikipedia.org/w/index.php?title=Head-of-line_blocking&amp;oldid=1083849253">head-of-line blocking</a>),
which didn&#39;t happen between multiple connections.
Finally, <a href="https://www.rfc-editor.org/rfc/rfc7540.html">QUIC</a>,
published in 2021, added multiplexing without head-of-line blocking,
even over a single QUIC connection.</p>
</div>
<p>As you can see, the first two round trips are entirely consumed with
setting up the connection. After two round trips, the client can
finally ask for the resource and it&#39;s another round trip before it
finally gets any data.  Depending on the network details, each round
trip can be anywhere from a few milliseconds to 200 milliseconds, so
it can be up to 600ms before the browser sees the first byte of
data. This is a big deal and over the past few years the IETF has
expended considerable effort to shave round trips from connection
setup time for the Web (with <a href="https://www.rfc-editor.org/rfc/rfc8446.html">TLS
1.3</a> and
<a href="https://www.rfc-editor.org/rfc/rfc9000.html">QUIC</a>).</p>
<p>Once the connection has been established, you then need to deliver
the data, which doesn&#39;t happen all at once. As I mentioned before,
it gets broken up into a stream of packets which are sent to the
other side over time. This is where things get a little bit tricky
because neither the sender nor the receiver knows the capacity
of the network (i.e., how many bits/second it can carry) and if
the sender tries to send too fast, then the extra packets get
dropped. To avoid this, TCP (or QUIC) tries
to work out a safe sending rate by gradually sending faster
and faster until there are signs of congestion (e.g., packets
getting lost or delayed) and then backs off. Importantly,
this means that initially you won&#39;t be using the full capacity
of the network until the connection warms up (this is called
&#34;slow start&#34;), so the data transfer rate tends to get faster over
time until a steady state is reached.</p>
<p>The implication of all this is that new connections are expensive
and you want to send as much data over a single connection
as you can. In fact, much of the evolution of HTTP over the
past 30 years has been finding ways to use fewer and fewer
connections for a single Web page.</p>
<h4 id="peer-to-peer-performance">Peer-to-Peer Performance <a href="#peer-to-peer-performance">#</a></h4>
<p>This brings us to the question of performance in peer-to-peer
systems. As I mentioned above, if you want to move significant amounts
of data, you really want to have the client connect directly to the
node which is storing the data. This presents several problems.</p>
<p>First, we have the latency involved in just sending the first message
through the P2P network and back. This will generally be slower than a
direct message because it can&#39;t take a direct path.  Then, it&#39;s not
generally possible to simply initiate a connection directly to other people&#39;s
personal computers, as they are often behind network elements like
<a href="https://en.wikipedia.org/w/index.php?title=Network_address_translation&amp;oldid=1083794290">NATs</a>
and
<a href="https://en.wikipedia.org/w/index.php?title=Firewall_(computing)&amp;oldid=1083940793">Firewalls</a>.
So-called &#34;hole punching&#34; protocols like
<a href="https://en.wikipedia.org/w/index.php?title=Interactive_Connectivity_Establishment&amp;oldid=1041588442">ICE</a>
allow you to establish direct connections in many cases, but they
introduce additional latency (minimum one round trip, but often much
more). And once that&#39;s done you then still have to establish
an encrypted connection, so we&#39;re talking anywhere upward from 2 additional
round trips.
To make matters worse, there will be many cases
where the storing node is quite topologically far from you and
therefore has a long round trip time; big sites and CDNs deliberately
locate points of presence close to users, but this is a much harder
problem with P2P systems.
And of course, even once the connection has been established, we&#39;re still
in slow start.</p>
<p>This is all kind of a bad fit for Web sites, which tend to consist of
a lot of small files. For example, the Google home page, which is
generally designed to be lightweight, currently consists of 36 separate
resources, with the largest being 811 KB. If each of these resources
is stored separately in the DHT, then you&#39;re going to be running
the inefficient setup phase of the protocol a lot and will almost
never be in the efficient data transfer phase. This is by contrast
to HTTP and QUIC, which try to keep the connection to the server open so
that they can amortize out the startup phase.</p>
<p>It&#39;s obviously possible to bundle up some of the resources on a site
into a single object, but this has other problems. First, it&#39;s hard
on the browser cache because many of those objects will be reused
on subsequent loads. Second, it makes the connection to a single
node the rate limiting step in the download, which is bad if that
node—which, recall, is just someone else&#39;s computer—doesn&#39;t
have a good network connection or is temporarily overloaded.
The result is that we have a tension between what we want to
minimize individual fetch latency, which is to
send everything over a single connection, and what we want to
do in order to avoid bottlenecking on single elements, which is
to download from a lot of servers at once, like BitTorrent does.</p>
<p>All of this is less of an issue in contexts like movie downloading,
where the object is big and so overall throughput is more important
than latency. In that case, you can parallelize your connections
and keep the pipe full. However, this isn&#39;t the situation with
the Web, where people really notice page load time. As far as I know,
building a large P2P network with comparable load-time performance to
the Web is a mostly unsolved problem.</p>
<h3 id="security-and-privacy">Security and Privacy <a href="#security-and-privacy">#</a></h3>
<p>Even if we assume that the P2P network itself is secure in the
sense that attackers can&#39;t bring it down and the
data is signed, this system still has some concerning properties.</p>
<h4 id="privacy">Privacy <a href="#privacy">#</a></h4>
<p>In any system like the Web, the node that serves data to the
client learns which data a given client is interested in,
at least to the level of the client&#39;s IP address. This isn&#39;t
an ideal situation in the current Web, hence IP address
concealment techniques like Tor, VPNs, Private Relay, etc.,
but at least it&#39;s <em>somewhat</em> limited to identifiable entities
that you chose to interact with (though of course the ubiquitous
tracking in Web advertising makes the situation pretty bad).</p>
<p>The situation with P2P systems is even worse: downloading
a piece of content means contacting a more or less random
computer on the Internet and telling it what you want. As
I noted above, you could route all the traffic through the
P2P network but only by seriously compromising privacy, so
realistically you&#39;re going to be sharing your IP address
with the node. Worse yet, in most cases the data is
going to be sharded over multiple nodes, which means that
a lot of different random people are seeing your browsing
behavior. Finally, in many networks it&#39;s possible for nodes
to influence which data they are responsible for, in which
which case one might imagine entities who wished to do
surveillance trying to become responsible for particular
kinds of sensitive data and then recording who came to retrieve it;
indeed, it <a href="https://www.theregister.com/2013/08/20/ip_address_search_shows_prenda_copyright_trolls_seeded_smut_then_sued/">appears</a> this is already happening with BitTorrent.</p>
<h4 id="access-control%E2%80%94putting-the-public-in-publishing">Access control—putting the public in publishing <a href="#access-control%E2%80%94putting-the-public-in-publishing">#</a></h4>
<p>Much of the Web is available to everyone, but it&#39;s also quite
common to have situations in which you want to restrict access
to a piece of data. This can be the site&#39;s data, such as
the paywalls operated by sites like the New York Times, or
the user&#39;s data, such as with Facebook or Gmail. These
are implemented in the obvious way, by having an access
control list on the server which states which users can
access each piece of data and refusing to serve data to
unauthorized users. This won&#39;t work in a P2P system, however,
in that there&#39;s no server to do the enforcement: the data
is just stored on people&#39;s computers and even if the site
published access control rules, the site can&#39;t trust
the storing node to follow them. It might even be controlled
by the attacker.</p>
<p>The traditional answer to this problem is to use to encrypt
the content before it&#39;s stored in the DHT. Even if the data
in the DHT is public, that&#39;s just the ciphertext.
This actually works modestly well when the content
is the user&#39;s and they don&#39;t want to share it with anyone
because they can encrypt it to a key they know
and then just store it in the DHT. This could even be done
with existing APIs (e.g., <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Crypto_API">WebCrypto</a>), and the key is stored
on the user&#39;s computer. It works a lot less well if they
want to share it with other people—especially with
read/write applications like Google Docs—because you
need cryptographic enforcement mechanisms for all of
the access rules. There has been some real work on this
with cryptographic file systems like
<a href="https://hovav.net/ucsd/dist/xxfs.pdf">SiRiUS</a>
and <a href="https://tahoe-lafs.org/trac/tahoe-lafs">Tahoe-LAFS</a>,
but it&#39;s a complicated problem and I&#39;m not aware
of any really large scale deployments.</p>
<p>The paywall problem is actually somewhat harder.
For instance, the New York Times could encrypt all its content
and then give every subscriber a key which could be used to
decrypt it, but given the number of subscribers, and that only
one has to leak the key,
the chance
that that key will leak is essentially 100%.
Of course, people share NYT passwords too, but what makes
this problem harder is that the password then has to be
used on the NYT site and it&#39;s possible to detect misbehavior,
such as when 20 people use the same password. I&#39;m not
aware of any really good P2P-only solution here.</p>
<h2 id="non-static-content">Non-Static Content <a href="#non-static-content">#</a></h2>
<p>Access control is actually a special case of a more general problem:
many if not most Web sites do more than simple publishing of static
content and those sites depend on server side processing that is hard to
replicate in a decentralized system.</p>
<h3 id="non-secret-computation">Non-Secret Computation <a href="#non-secret-computation">#</a></h3>
<p>As a warm-up, let&#39;s take a comparatively easy problem, the shopping
site I described in <a href="https://0x539.lol/posts/web-security-model-intro2/">part II</a> of my
Web security model series. Effectively, this site has three
server-side functions that need to be replicated:</p>
<ul>
<li>Product search</li>
<li>Shopping cart maintenance</li>
<li>Purchasing</li>
</ul>
<p>The second and third of these are actually reasonably straightforward:
the shopping cart can be stored entirely on the client or, alternately,
stored self-encrypted by the client in the P2P system, as described in
the previous section. The purchasing piece can be handled by some
kind of cryptocurrency (though things are more complicated if you
want to take credit cards).
However, product search is more difficult.
The obvious solution would just be to publish the entire product
catalog in the network, have the client download it, and do search
locally. This obviously has some pretty undesirable performance consequences:
consider how much data is in Amazon&#39;s catalog and how often it changes.</p>
<p>Obviously, the way this works in the Web 2.0 world is that the
server just runs the computation and returns the result, and at
this point you usually hear someone propose some kind of distributed computation
system a la <a href="https://ethereum.org/en/smart-contracts/">Ethereum smart contracts</a>
(though you probably don&#39;t want the outcome recorded on the blockchain).
In this case, instead of publishing a static resource, the site
would publish a program to be executed that returned the results
(often these programs are written in <a href="https://webassembly.org/">WebAssembly</a>).</p>
<p>Aside from the obvious problem that this still requires the node
executing the program to have all the data, it&#39;s hard for the
end-user client to determine that the node has executed the
program correctly. Even in a simple case like searching for matching
records: if those records are signed then the node can&#39;t substitute
their own values, but they can potentially conceal matching ones.
There are, of course, cryptographic techniques that potentially
make it possible to prove that the computation was correct, but they
are far from trivial. So, this doesn&#39;t have a really great solution.</p>
<h3 id="secret-information">Secret Information <a href="#secret-information">#</a></h3>
<p>A shopping site is actually a relatively simple case because the
information is basically public—though in some cases the
site might not want their catalog to be public—but there
are a lot of cases where the site wants to compute with secret information.
There are two primary situations here:</p>
<ol>
<li>
<p>The site&#39;s secret information, for instance Twitter&#39;s recommendation
algorithm is not public.</p>
</li>
<li>
<p>The user&#39;s secret information, for instance which other users
they have &#34;swiped right&#34; on in a dating app, or even just
users&#39; profile details.</p>
</li>
</ol>
<p>In Web 2.0, the way this works is that the server knows the secret
information and uses it for the computation but doesn&#39;t reveal
it to the users. As with the search case, though, that doesn&#39;t
port easily to the P2P case because it&#39;s not safe to reveal the
information to random people&#39;s personal computers.</p>
<p>There are, of course, cryptographic mechanisms for computing specific
functions with encrypted data. For instance,
<a href="https://en.wikipedia.org/w/index.php?title=Private_set_intersection&amp;oldid=1081416156">Private Set Intersection</a>
techniques make it possible to determine whether Alice and Bob
both swiped right on each other and only tell them if they
both did, but they&#39;re complicated and more importantly task specific,
so you need a solution for each application, and sometimes that
means inventing new cryptography (to be clear, this is far from
all that is required to implement a secure P2P dating system!).</p>
<p>This is actually a general problem with cryptographic replacements
for computations performed on &#34;trusted&#34; servers. The positive
side of cryptographic approaches is that they can provide
strong security guarantees, but the negative side is that essentially
each new computation task requires some new cryptography,
which makes changes very slow and expensive. By contrast, if you&#39;re
doing computation on a server, then changing your computations
is just a matter of writing and loading it onto the server.
The obvious downside is that people have to trust the server,
but clearly a lot of people are willing to do that.</p>
<h3 id="hybrid-architectures">Hybrid Architectures <a href="#hybrid-architectures">#</a></h3>
<p>One idea that is sometimes floated for addressing this kind of
functional issue is to have a hybrid architecture.
For instance, one might imagine implementing the shopping site by
having the static content of the catalog served via the P2P network
but having a server which handled the searches and returned pointers
to the relevant sections of the catalog. You could even encrypt each
individual catalog chunk so that it was hard for a competitor to see
your entire catalog. You could even imagine building a dating site
with—handwaving alert!—some combination of P2P and server technology, with the logic for
determining which profiles you could see and which to match you with
implemented on the server, but the (encrypted) profiles distributed
P2P.</p>
<p>At this point, though, you have pretty substantial server component
that is in the critical path of your site and so you&#39;re mostly using the P2P
network as a kind of not-very-fast CDN (see, for instance,
<a href="https://www.youtube.com/watch?v=PnBIIdmKO9o">PeerCDN</a>). This gives
up most of the benefits of having your system decentralized in the
first place: you still have the problem of hosting your server
somewhere, which probably means some cloud service, and at that point
why not just use a CDN for your static content anyway? Similarly,
if you&#39;re worried about censorship, then you need to worry about
your server being censored, which makes your site unusable even
if the P2P piece still works.</p>
<h2 id="closing-thoughts">Closing Thoughts <a href="#closing-thoughts">#</a></h2>
<p>It&#39;s easy to see the appeal of a more decentralized Web: who wants
to have a bunch of faceless mega-corporations deciding what you can
or cannot say? And there certainly are plenty of jurisdictions that
censor people&#39;s access to the Web and to information more generally.
It&#39;s easy to look at the success of P2P content
distribution systems—albeit to a great extent for distributing
content for which other people hold the copyrights—and come
to the conclusion that it&#39;s a solution to the Web centralization
problem.</p>
<p>Unfortunately, for the reasons described above, I don&#39;t think that&#39;s
really the right conclusion. While the Web sort of superficially
resembles a content distribution system, it&#39;s actually something
quite different, with both a far broader variety of use cases
and much tighter security and performance requirements.
It&#39;s probably possible to rebuild some simpler systems on a P2P
substrate, but the Web as a whole is a different story, and even
systems that appear simple are often quite complex internally.
Of course,
the Web has had almost 30 years to grow into what it is,
and it&#39;s possible that there are technological improvements
that would let us build a decentralized system with similar properties,
but I don&#39;t think this is something we really understand
how to do today.</p>



    </article>

    
      
  </div>
  
</div>

<!-- Cloudflare Web Analytics --><!-- End Cloudflare Web Analytics -->


          </main>
        </div></div>
  </body>
</html>
