<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.cold-takes.com/does-x-cause-y-an-in-depth-evidence-review/">Original</a>
    <h1>Does X cause Y? An in-depth evidence review (2021)</h1>
    
    <div id="readability-page-1" class="page"><div>
            
<div>
    <main>
            <article>
    
    <div>
        <!--kg-card-begin: html--><p><img src="https://www.cold-takes.com/content/images/2021/07/does-x-cause-y-1.jpg"/></p>

<p>
There&#39;s an interesting theory out there that X causes Y. If this were true, it would be pretty important. So I did a deep-dive into the academic literature on whether X causes Y. Here&#39;s what I found.
</p>
<p>
(Embarrassingly, I can&#39;t actually remember what X and Y are. I think maybe X was enriched preschool, or just school itself, or eating fish while pregnant, or the Paleo diet, or lead exposure, or a clever &#34;nudge&#34; policy trying to get people to save more, or some self-help technique, or some micronutrient or public health intervention, or democracy, or free trade, or some approach to intellectual property law. And Y was ... lifetime earnings, or risk of ADHD diagnosis, or IQ in adulthood, or weight loss, or violent crime, or peaceful foreign policy, or GDP per capita, or innovation. Sorry about that! Hope you enjoy the post anyway! Fortunately, I think <strong>what I&#39;m about to write is correct for pretty much any (X,Y) from those sorts of lists.</strong>)
</p>
<p>
In brief:
</p>
<ul>

<li>There are hundreds of studies on whether X causes Y, but most of them are simple observational studies that are just essentially saying &#34;People/countries with more X also have more Y.&#34; For reasons discussed below, we can&#39;t really learn much from these studies.

</li><li>There are 1-5 more interesting studies on whether X causes Y. Each study looks really clever, informative and rigorous at first glance. However, the more closely you look at them, the more confusing the picture gets.

</li><li>We ultimately need to choose between (a) believing some overly complicated theory of the relationship between X and Y, which reconciles all of the wildly conflicting and often implausible things we&#39;re seeing in the studies; (b) more-or-less reverting to what we would&#39;ve guessed about the relationship between X and Y in the absence of any research.
</li>
</ul>
<h2 id="the-chaff">The chaff: lots of unhelpful studies that I&#39;m disregarding</h2>


<p>
First, the good news: there are hundreds of studies on whether X causes Y. The bad news? We need to throw most of them out. 
</p>
<p>
Many have comically small sample sizes (like studying 20 people) and/or comically short time horizons (like looking at weight loss over two weeks),<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup> or unhelpful outcome measures (like intelligence tests in children under 5).<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup> But by far the most common problem is that <strong>most of the studies on whether X causes Y are simple observational studies: they essentially just find that people/countries with more X also have more Y. </strong>
</p>
<p>
Why is this a problem? There could be a <em>confounder</em> - some third thing, Z, that is correlated with both X and Y. And there are specific reasons we should expect confounders to be common:
</p>
<ul>

<li>In general, people/countries that have more X also have more of lots of other helpful things - they&#39;re richer, they&#39;re more educated, etc. For example, if we&#39;re asking whether higher-quality schooling leads to higher earnings down the line, an issue is that people with higher-quality schooling also tend to come from better-off families with lots of other advantages.

</li><li>In fact, the <em>very fact that people in upper-class intellectual circles think X causes Y</em> means that richer, more educated people/countries tend to deliberately get more X, and also try to do a lot of other things to get more Y. For example, more educated families tend to eat more fish (complicating the attempt to see whether eating fish in pregnancy is good for the baby).<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup></li></ul>
<p>
Now, a lot of these studies try to &#34;control for&#34; the problem I just stated - they say things like &#34;We examined the effect of X and Y, while controlling for Z [e.g., how wealthy or educated the people/countries/whatever are].&#34; How do they do this? The short answer is, well, hm, jeez. Well you see, to simplify matters a bit, just try to imagine ... uh ... shit. Uh. The only high-level way I can put this is:
</p>
<ul>

<li>They use a technique called <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression analysis</a> that, as far as I can determine, cannot be explained in a simple, intuitive way (especially not in terms of how it &#34;controls for&#34; confounders).

</li><li>The &#34;controlling for&#34; thing relies on a lot of subtle assumptions and can break in all kinds of weird ways. <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152719">Here&#39;s </a>a technical explanation of some of the pitfalls; <a href="https://www.cold-takes.com/phil-birnbaums-regression-analysis/">here&#39;s</a> a set of deconstructions of regressions that break in weird ways.

</li><li>None of the observational studies about whether X causes Y discuss the pitfalls of &#34;controlling for&#34; things and whether they apply here.
</li>
</ul>
<p>
I don&#39;t think we can trust these papers, and to really pick them all apart (given how many there are) would take too much time. So let&#39;s focus on a smaller number of better studies.
</p>
<h2 id="the-wheat">The wheat: 1-5 more interesting studies</h2>


<p>
Digging through the sea of unhelpful studies, I found 1-5 of them that are actually really interesting! 
</p>
<p>
For example, one study examines some strange historical event you&#39;ve never heard of (perhaps a <a href="https://davidcard.berkeley.edu/papers/mariel-impact.pdf">surge in Cuban emigration triggered by Fidel Castro suddenly allowing it</a>, or <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3800113/">John Rockefeller&#39;s decision to fund a hookworm eradication campaign</a>, or a <a href="https://www.aeaweb.org/articles?id=10.1257/aer.103.6.2437">sudden collective pardon leading to release of a third of prison inmates in Italy</a>), where for abstruse and idiosyncratic reasons, X got distributed in what seems to be almost a random way. This study is really clever, and the authors were incredibly thorough in examining seemingly every way their results could have been wrong. They conclude that X causes Y!
</p>
<p>
But on closer inspection, I have a bunch of reservations. For example:
</p>
<ul>

<li>The paper doesn&#39;t make it easy to replicate its analysis, and when someone does manage to sort-of replicate it, they may <a href="https://blog.givewell.org/2017/12/07/questioning-evidence-hookworm-eradication-american-south/">get different results</a>. 

</li><li>There was other weird stuff going on (e.g., changes in census data collection methods<sup id="fnref5"><a href="#fn5" rel="footnote">5</a></sup>), during the strange historical event, so it&#39;s a little hard to generalize.

</li><li>In a response to the study, another academic advances a complex theory of how the study could actually have gotten a misleading result. This led to an intense back-and-forth between the original authors and the skeptic, stretched out over years because each response had to be published in a journal, and by the time I got to the end of it I didn&#39;t have any idea what to think anymore.<sup id="fnref6"><a href="#fn6" rel="footnote">6</a></sup></li></ul>
<p>
I found 0-4 other interesting studies. I can&#39;t remember all of the details, but they may have included:
</p>
<ul>

<li>A study comparing siblings, or maybe &#34;very similar countries,&#34; that got more or less of X.<sup id="fnref7"><a href="#fn7" rel="footnote">7</a></sup>

</li><li>A study using a complex mathematical technique claiming to cleanly isolate the effect of X and Y. I can&#39;t really follow what it&#39;s doing, and I’m guessing there are a lot of weird assumptions baked into this analysis.<sup id="fnref8"><a href="#fn8" rel="footnote">8</a></sup>

</li><li>A study with actual randomization: some people were randomly assigned to receive more X than others, and the researchers looked at who ended up with more Y. This sounds awesome! However, there are issues here too: 
<ul>
 
<li>It&#39;s kind of ambiguous whether the assignment to X was really &#34;random.&#34;<sup id="fnref9"><a href="#fn9" rel="footnote">9</a></sup>
 
</li><li>Extremely weird things happened during the study (for example, <a href="https://www.givewell.org/international/technical/programs/deworming/reanalysis#Our_initial_reservations">generational levels of flooding</a>), so it&#39;s not clear how well it generalizes to other settings.
 
</li><li>The result seems fragile (<a href="https://www.givewell.org/international/technical/programs/deworming#Other_studies_of_long-term_effects">simply adding more data weakens it a lot</a>) and/or just hard to believe (like <a href="https://nutritionj.biomedcentral.com/articles/10.1186/s12937-017-0287-9">schoolchildren doing noticeably better on a cognition test after a few weeks of being given fish instead of meat with their lunch, even though they mostly didn&#39;t eat the fish</a>). </li></ul></li></ul>
<p>
Compounding the problem, the 1-5 studies I found tell very different stories about the relationship between X and Y. How could this make sense? Is there a unified theory that can reconcile all the results?
</p>
<p>
Well, one possibility is that X causes Y sometimes, but only under very particular conditions, and the effect can be masked by some other thing going on. So - if you meet one of 7 criteria, you should do X to get more Y, but if you meet one of 9 other criteria, you should actually avoid X!
</p>
<h2 id="conclusion">Conclusion</h2>


<p>
I have to say, this all was simultaneously more fascinating and less <em>informative</em> than I expected it would be going in. I thought I would find some nice studies about the relationship between X and Y and be done. Instead, I&#39;ve learned a ton about weird historical events and about the ins and outs of different measures of X and Y, but I feel just super confused about whether X causes Y.
</p>
<p>
I guess my bottom line is that X does cause Y, because it intuitively seems like it would.
</p>
<p>
I&#39;m glad I did all this research, though. It&#39;s good to know that social science research can go haywire in all kinds of strange ways. And it&#39;s good to know that despite the confident proclamations of pro- and anti-X people, it&#39;s legitimately just super unclear whether X causes Y. 
</p>
<p>
I mean, how else could I have learned that?
</p>
<h2 id="appendix">Appendix: based on a true story</h2>


<p>
This piece was inspired by:
</p>
<ul>

<li>Most evidence reviews GiveWell has done, especially of <a href="https://www.givewell.org/international/technical/programs/deworming">deworming</a>

</li><li>Many evidence reviews by David Roodman, particularly <a href="https://www.cgdev.org/publication/macro-aid-effectiveness-research-guide-perplexed-working-paper-135">Macro Aid Effectiveness Research: a Guide for the Perplexed</a>; <a href="https://smile.amazon.com/dp/B0095V3J0S/">Due Diligence: an Impertinent Inquiry into Microfinance</a>; and <a href="https://www.openphilanthropy.org/blog/reasonable-doubt-new-look-whether-prison-growth-cuts-crime">Reasonable Doubt: A New Look at Whether Prison Growth Cuts Crime</a>. 

</li><li>Many evidence reviews by Slate Star Codex, collected <a href="https://slatestarcodex.com/tag/much-more-than-you-wanted-to-know/">here</a>.

</li><li>Informal evidence reviews I&#39;ve done for e.g. personal medical decisions.
</li></ul>
<p>
The basic patterns above apply to most of these, and the bottom line usually has the kind of frustrating ambiguity seen in this conclusion.</p>
    
<p>There are cases where things seem a bit less ambiguous and the bottom line seems clearer. Speaking broadly, I think the main things that contribute to this are:
</p>
<ul>

<li>Actual randomization. For years I&#39;ve nodded along when people say &#34;You shouldn&#39;t be dogmatic about randomization, there are many ways for a study to be informative,&#34; but each year I&#39;ve become a bit more dogmatic. Even the most sophisticated-, appealing-seeming alternatives to randomization in studies seem to have a way of falling apart. Randomized studies almost always have problems and drawbacks too. But I’d rather have a randomized study with drawbacks than a non-randomized study with drawbacks.

</li><li>Extreme thoroughness, such as Roodman&#39;s attempt to reconstruct the data and code for key studies in <a href="https://www.openphilanthropy.org/blog/reasonable-doubt-new-look-whether-prison-growth-cuts-crime">Reasonable Doubt</a>. This sometimes leads to outright dismissing a number of studies, leaving a smaller, more consistent set remaining.
</li>
</ul>

<!--kg-card-end: html--><!--kg-card-begin: html-->

<p><span><a href="https://api.addthis.com/oexchange/0.8/forward/twitter/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fdoes-x-cause-y-an-in-depth-evidence-review%2F&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20Does%20X%20cause%20Y%3F%20An%20in-depth%20evidence%20review&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/06/ct-twitter-square.png" alt="Twitter"/></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/facebook/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fdoes-x-cause-y-an-in-depth-evidence-review%2F&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20Does%20X%20cause%20Y%3F%20An%20in-depth%20evidence%20review&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/06/ct-facebook-square.png" alt="Facebook"/></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/reddit/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fdoes-x-cause-y-an-in-depth-evidence-review%2F&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20Does%20X%20cause%20Y%3F%20An%20in-depth%20evidence%20review&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/06/ct-reddit-square.png" alt="Reddit"/></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/menu/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fdoes-x-cause-y-an-in-depth-evidence-review%2F&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20Does%20X%20cause%20Y%3F%20An%20in-depth%20evidence%20review&amp;ct=1" target="_blank"><img width="32" src="https://www.cold-takes.com/content/images/2021/06/ct-addthis-square.png" alt="More"/></a></span>
        </p>
<center></center>
<!--kg-card-end: html--><!--kg-card-begin: html--><hr/>
<h2>Footnotes</h2>
<!--kg-card-end: html-->
    </div>
        
</article>                            </main>
</div>
        </div></div>
  </body>
</html>
