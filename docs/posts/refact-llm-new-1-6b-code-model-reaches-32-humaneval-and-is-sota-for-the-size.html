<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://refact.ai/blog/2023/introducing-refact-code-llm/">Original</a>
    <h1>Refact LLM: New 1.6B code model reaches 32% HumanEval and is SOTA for the size</h1>
    
    <div id="readability-page-1" class="page"><div>
            <div>
                
                <div>
                    <p>September 4, 2023</p>
                    <p>by Sergey Vakhreev, Oleg Klimov</p>
                </div>
                <p>Today we’re introducing Refact LLM: 1.6B code model with infill real-time code completion (including fill-in-the-middle(FIM) capability) and chat.
Refact LLM achieves the state-of-the-art performance among the code LLMs, coming closer to  HumanEval as Starcoder, being 10x smaller in size, and it beats other code models such as StableCode, CodeGen and ReplitCode on HumanEval metric.</p>
<p><strong>Summary</strong>:</p>
<ul><li>1.6b parameters</li><li>20 programming languages</li><li>4096 tokens context</li><li>code completion and chat capabilities</li><li>SoTA on HumanEval benchmark among similar code models</li><li>pre-trained on permissive licensed code and available for commercial use</li></ul>
<div><table><thead><tr><th>Model</th><th>Model Size</th><th>HumanEval pass@1</th></tr></thead><tbody><tr><td>DeciCoder-1b</td><td>1b</td><td>19.1%</td></tr><tr><td>Refact-1.6-fim</td><td>1.6b</td><td>32.0%</td></tr><tr><td>StableCode</td><td>3b</td><td>20.2%</td></tr><tr><td>ReplitCode v1</td><td>3b</td><td>21.9%</td></tr><tr><td>CodeGen2.5-multi</td><td>7b</td><td>28.4%</td></tr><tr><td>CodeLlama</td><td>7b</td><td>33.5%</td></tr><tr><td>StarCoder</td><td>15b</td><td>33.6%</td></tr></tbody></table></div>
<p>The base model was trained on our own set of code with permissive licenses only and open text datasets (the text to code ratio was 50:50). In total, we trained our base model on 1.2T tokens of code on our cluster.</p>
<p>The model was then fine-tuned with open code instruction-following datasets filtered for quality and a synthetic dataset based on <a href="https://huggingface.co/datasets/bigcode/the-stack-dedup">The Stack dedup v1.1</a> to improve FIM and boosting the base model performance.</p>
<p>You can read more about the architecture decisions that we made in the <a href="https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/">blog post</a>.</p>
<p>We aim for the model to be accessible to everyone, we’re releasing the model for commercial use under BigScience OpenRAIL-M license and making the weight available on <a href="https://huggingface.co/smallcloudai/Refact-1_6B-fim">HuggingFace</a>.</p>
<p>While the trend recently was for the model sizes to get bigger, we wanted to lower barriers to entry and make it a versatile tool for developers with varying hardware setups. With the smaller size, running the model is much faster and affordable than ever: the model can be served on most of all modern GPUs requiring just 3Gb RAM and works great for real-time code completion tasks.</p>
<p>Refact LLM can be easily integrated into existing developers workflows with <a href="https://github.com/smallcloudai/refact/">an open-source docker container</a> and <a href="https://marketplace.visualstudio.com/items?itemName=smallcloud.codify">VS Code</a> and <a href="https://plugins.jetbrains.com/plugin/20647-codify">JetBrains</a> plugins. With Refact’s intuitive user interface, developers can utilize the model easily for a variety of coding tasks. Finetune is available in the self-hosting (docker) and Enterprise versions, making suggestions more relevant for your private codebase.</p>
<p><img src="https://refact.ai/images/blog/introducing-refact-code-llm/palindrome.gif"/></p>
<p>Refact 1.6B LLM is the third model in the family of our code models, with <a href="https://huggingface.co/smallcloudai/codify_3b_multi">CodeContrast 3b</a> and <a href="https://huggingface.co/smallcloudai/codify_medium_multi">CodeContrast 0.3b</a> released previously. We aim to continue with our research and future updates to improve the LLM’s performance and capabilities. We would love to get community contributions and feedback to enhance the model further. For any questions and ideas, please visit our <a href="https://smallcloud.ai/discord">Discord</a>.</p>
            </div>
        </div></div>
  </body>
</html>
