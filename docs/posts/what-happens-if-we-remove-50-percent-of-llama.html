<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://neuralmagic.com/blog/24-sparse-llama-smaller-models-for-efficient-gpu-inference/">Original</a>
    <h1>What happens if we remove 50 percent of Llama?</h1>
    
    <div id="readability-page-1" class="page"><div>
		<div>
		<p>Nov 25, 2024</p>
					<div>
				<div>
					    <div>
                    <p><img src="https://neuralmagic.com/wp-content/uploads/2024/03/peoples.svg" alt="Icon"/></p><p>
                Author(s)            </p>
            </div>
				<div>
					    
    <div>

								<p><img src="https://neuralmagic.com/wp-content/uploads/2024/10/1517700462379-150x150.jpeg"/></p>
							<div>
                                <p>Alexandre Marques</p>
								<p>Manager of Machine Learning Research</p>

                                <div>
								<p><a href="https://www.linkedin.com/in/marquesan/">	<img src="https://neuralmagic.com/wp-content/uploads/2024/10/icon-linkedin.svg"/>
</a></p>																																																</div>
</div>
						
	</div>
    <div>

								<p><img src="https://neuralmagic.com/wp-content/uploads/2023/07/Kurz_Mark-001-150x150.png"/></p>
							<div>
                                <p>Mark Kurtz</p>
								<p>CTO, Neural Magic</p>

                                <div>
								<p><a href="https://www.linkedin.com/in/markkurtzjr/">	<img src="https://neuralmagic.com/wp-content/uploads/2024/10/icon-linkedin.svg"/>
</a></p>								<p><a href="https://twitter.com/markurtz_?lang=en">	<img src="https://neuralmagic.com/wp-content/uploads/2024/10/icon-twitter.svg"/>
</a></p>																																								</div>
</div>
						
	</div>
    <div>

								<p><img src="https://neuralmagic.com/wp-content/uploads/2024/10/1516835131626-150x150.jpeg"/></p>
							<div>
                                <p>Dan Alistarh</p>
								<p>Principal Research Scientist</p>

                                <div>
								<p><a href="https://www.linkedin.com/in/dan-alistarh-613ba739/">	<img src="https://neuralmagic.com/wp-content/uploads/2024/10/icon-linkedin.svg"/>
</a></p>								<p><a href="https://twitter.com/dalistarh">	<img src="https://neuralmagic.com/wp-content/uploads/2024/10/icon-twitter.svg"/>
</a></p>																																								</div>
</div>
						
	</div>
    <div>

								<p><img src="https://neuralmagic.com/wp-content/uploads/2024/11/1517680495436-150x150.jpeg"/></p>
							<div>
                                <p>Shubhra Pandit</p>
								<p>Senior Machine Learning Researcher</p>

                                <div>
								<p><a href="https://www.linkedin.com/in/shubhrapandit/">	<img src="https://neuralmagic.com/wp-content/uploads/2024/10/icon-linkedin.svg"/>
</a></p>																																																</div>
</div>
						
	</div>
				</div>
				</div>
			</div>
		
<h2><strong>A Sparse Summary</strong></h2>



<ul>
<li><strong>Sparse Foundation Model:</strong> The first sparse, highly accurate foundation model built on top of Meta’s Llama 3.1 8B with 98% recovery on Open LLM Leaderboard v1 and full recovery across fine-tuning tasks, including math, coding, and chat.</li>



<li><strong>Hardware-Accelerated Sparsity:</strong> Features a 2:4 sparsity pattern designed for NVIDIA Ampere GPUs and newer, delivering up to 30% higher throughput and 1.8x lower latency from sparsity alone with vLLM.</li>



<li><strong>Quantization Compatible:</strong> Fully integrates with advanced 4-bit quantization methods like GPTQ and efficient <a href="https://github.com/IST-DASLab/Sparse-Marlin">Sparse-Marlin</a> inference kernels, enabling faster inference anywhere from 1.4x to 4.9x depending on the hardware and scenario.</li>
</ul>



<h2>Introducing Sparse Llama 3.1 8B</h2>



<p>Large language models (LLMs) are approaching their limits in terms of traditional scaling, with billions of parameters added for relatively small accuracy gains and advanced quantization techniques squeezing out the last possible bits before accuracy plummets. These dense architectures remain large, costly, and resource-intensive, making it challenging and expensive to scale AI. <strong>Neural Magic is doubling down on this challenge with sparse LLMs—reducing the model size by removing unneeded connections while retaining accuracy.</strong> Sparse models, though underexplored in the LLM space due to the high compute demands of pretraining, offer an increasingly promising dimension in model compression and efficiency.</p>



<p><a href="https://huggingface.co/neuralmagic/Sparse-Llama-3.1-8B-2of4"><strong>Sparse-Llama-3.1-8B-2of4</strong></a> is our next step in this commitment—a 50% pruned version of Meta&#39;s open-source Llama 3.1 8B. Built with a GPU-friendly 2:4 sparsity structure, it removes two of every four parameters while preserving accuracy. Designed as a versatile foundation model for fine-tuning and instruction alignment, Sparse Llama is optimized for both speed and efficiency. Its quantization-friendly architecture enables faster, cheaper inference with roughly half the connections of its dense counterpart.</p>



<h3>Research Background</h3>



<p>Sparse Llama 3.1 originates from years of prior research, building on previous breakthroughs with <a href="https://neuralmagic.com/blog/sparsegpt-remove-100-billion-parameters-for-free/"><strong>SparseGPT</strong></a>, <a href="https://arxiv.org/abs/2310.06927"><strong>SquareHead Knowledge Distillation</strong></a>, and <a href="https://neuralmagic.com/blog/fast-llama-2-on-cpus-with-sparse-fine-tuning-and-deepsparse/"><strong>Sparse Llama 2</strong></a>. These contributions laid the groundwork for our state-of-the-art sparse training approach, tailored to the latest generation of LLMs. Leveraging SparseGPT developed in collaboration with ISTA, we efficiently removed redundant connections, while SquareHead’s layerwise knowledge distillation and Sparse Llama 2’s foundational training recipes provided the basis for sparsity optimization.</p>



<h3>Performance Snapshot</h3>



<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4-Sparse-Foundational-Llama-Supervised-Fine-tuning-and-Quantization-Results-Eval-tall-2.png&#34;,&#34;figureClassNames&#34;:&#34;wp-block-image size-large&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7319&#34;,&#34;imgStyles&#34;:&#34;object-fit:cover&#34;,&#34;targetWidth&#34;:1728,&#34;targetHeight&#34;:727,&#34;scaleAttr&#34;:&#34;cover&#34;,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="1024" height="431" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Supervised-Fine-tuning-and-Quantization-Results-Eval-tall-2-1024x431.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Supervised-Fine-tuning-and-Quantization-Results-Eval-tall-2-1024x431.png 1024w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Supervised-Fine-tuning-and-Quantization-Results-Eval-tall-2-300x126.png 300w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Supervised-Fine-tuning-and-Quantization-Results-Eval-tall-2-768x323.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Supervised-Fine-tuning-and-Quantization-Results-Eval-tall-2-1536x646.png 1536w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Supervised-Fine-tuning-and-Quantization-Results-Eval-tall-2-1568x660.png 1568w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Supervised-Fine-tuning-and-Quantization-Results-Eval-tall-2.png 1728w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><em>Figure 1: Results for dense, sparse, and sparse-quantized Llama 3.1 8B on fine-tuning and few-shot benchmarks.</em></figcaption></figure>



<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4.png&#34;,&#34;figureClassNames&#34;:&#34;wp-block-image size-large&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7327&#34;,&#34;imgStyles&#34;:&#34;object-fit:cover&#34;,&#34;targetWidth&#34;:1728,&#34;targetHeight&#34;:720,&#34;scaleAttr&#34;:&#34;cover&#34;,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="427" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-1024x427.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-1024x427.png 1024w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-300x125.png 300w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-768x320.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-1536x640.png 1536w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-1568x653.png 1568w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4.png 1728w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><em>Figure 2: Impact of 2:4 sparsity and quantization on inference performance with vLLM nightly (11/22/24).</em></figcaption></figure>



<p>Sparse Llama 3.1 demonstrates exceptional performance across few-shot benchmarks, fine-tuning tasks, and inference scenarios, showcasing the versatility and efficiency of 2:4 sparsity.</p>



<ul>
<li><strong>Few-Shot Benchmarks</strong>: It achieved <strong>98.4% accuracy recovery</strong> on the Open LLM Leaderboard V1 and <strong>97.3% recovery</strong> on the more challenging Mosaic Eval Gauntlet <strong>(Figure 1, right)</strong>, maintaining competitive performance with dense models. </li>



<li><strong>Fine-Tuning Results (Figure 1, left)</strong>: The most exciting results emerged during fine-tuning across <strong>math (GSM8K)</strong>, <strong>code (Evol-CodeAlpaca)</strong>, and <strong>conversational AI (Ultrachat-200K)</strong> tasks. Despite minimal regression in the few-shot benchmarks, it achieved <strong>full accuracy recovery</strong> and, in some cases, outperformed its dense counterparts. These results underscore Sparse Llama&#39;s robustness and adaptability across domains. As a proof of concept, these fine-tuned models are additionally publicly available.</li>



<li><strong>Sparse-Quantized Inference (Figure 2)</strong>: Using 4-bit post-training quantization combined with 2:4 sparsity delivered impressive inference speedups with vLLM nightly (11/22/24) and minimal effects on accuracy for most cases. The sparse-quantized models achieved <strong>5.0x speedup on A5000 GPUs</strong>, <strong>4.9x on A6000 GPUs</strong>, and <strong>3.7x on A100 GPUs</strong> in single-stream latency, with <strong>1.8x of the gains attributed to sparsity alone</strong>. Throughput scenarios showed a consistent <strong>1.4x improvement</strong>, even when quantization alone had minimal impact. </li>
</ul>



<p>For a detailed breakdown of metrics and benchmarks, visit the <a href="https://neuralmagic.com/blog/24-sparse-llama-smaller-models-for-efficient-gpu-inference/#performancedetails" title="">Full Performance Details section</a>.</p>



<h3>Get Started</h3>



<p>Explore the <a href="https://huggingface.co/neuralmagic/Sparse-Llama-3.1-8B-2of4">Sparse Llama base model</a> and our fine-tuned versions today on <a href="https://huggingface.co/neuralmagic">Neural Magic’s Hugging Face organization</a>.<strong> With open-sourced weights, evaluations, and benchmarks, we intend to empower the community to experiment and build on our foundation</strong>.</p>



<p>Are you looking to improve the performance of your AI deployments, reduce deployment costs, or enable better scaling? <a href="https://neuralmagic.com/book-a-demo/">Contact us</a>—we&#39;d love to help you work with your LLMs!</p>



<p>Stay tuned for more. The future of LLMs is open source, and with sparsity, we&#39;re excited to continue pushing the boundaries of what&#39;s possible for efficient, scalable, and performant AI.</p>



<h2 id="performancedetails">Full Performance Details</h2>



<h3>Base Model Evaluations</h3>



<p>We evaluated Sparse Llama on two widely used benchmarks to establish its baseline performance:</p>



<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4-Sparse-Foundational-Llama-Table-1-1.png&#34;,&#34;figureClassNames&#34;:&#34;wp-block-image size-large&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7339&#34;,&#34;imgStyles&#34;:null,&#34;targetWidth&#34;:1931,&#34;targetHeight&#34;:372,&#34;scaleAttr&#34;:false,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="197" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-1-1-1024x197.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-1-1-1024x197.png 1024w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-1-1-300x58.png 300w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-1-1-768x148.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-1-1-1536x296.png 1536w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-1-1-1568x302.png 1568w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-1-1.png 1931w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><em>Table 1: Dense vs. Sparse Llama 3.1 8B evaluation results for the Open LLM Leaderboard v1</em>.</figcaption></figure>



<p><a href="https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md"><strong>Mosaic Eval Gauntlet v0.3</strong></a>: A comprehensive benchmark covering reasoning, problem-solving, and reading comprehension tasks. Sparse Llama demonstrated robust performance, achieving <strong>97.3% accuracy recovery</strong>, even on more challenging datasets, as reported in <em>Table 2.</em></p>



<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4-Sparse-Foundational-Llama-Table-2-1.png&#34;,&#34;figureClassNames&#34;:&#34;wp-block-image size-large&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7340&#34;,&#34;imgStyles&#34;:null,&#34;targetWidth&#34;:1931,&#34;targetHeight&#34;:398,&#34;scaleAttr&#34;:false,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="211" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-2-1-1024x211.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-2-1-1024x211.png 1024w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-2-1-300x62.png 300w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-2-1-768x158.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-2-1-1536x317.png 1536w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-2-1-1568x323.png 1568w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-2-1.png 1931w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><em>Table 2: Dense vs. Sparse Llama 3.1 8B evaluation results for the Mosaic Eval Gauntlet v0.3</em>.</figcaption></figure>



<h3>Sparse Fine-Tuning Evaluations</h3>



<p>To evaluate Sparse Llama’s adaptability, we fine-tuned both sparse and dense versions across three domains using the same amount of hyperparameter tuning:</p>



<ul>
<li><strong>Mathematical Reasoning (GSM8K)</strong>: Strict-match accuracy in 0-shot setting via <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>.</li>



<li><strong>Coding (Evol-CodeAlpaca)</strong>: Pass@1 on HumanEval and HumanEval+ via <a href="https://github.com/evalplus/evalplus">EvalPlus</a>.</li>



<li><strong>Conversational AI (Ultrachat-200K)</strong>: Win rate on AlpacaEval, following the setup of <a href="https://arxiv.org/abs/2405.03594">Sparse Llama 2</a>.</li>
</ul>



<p>Sparse Llama consistently achieved <strong>full accuracy recovery</strong> during fine-tuning and even outperformed the dense baseline on some tasks, demonstrating its versatility. The results are detailed in <em>Table 3</em>.</p>



<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4-Sparse-Foundational-Llama-Table-3-1.png&#34;,&#34;figureClassNames&#34;:&#34;wp-block-image size-large&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7341&#34;,&#34;imgStyles&#34;:null,&#34;targetWidth&#34;:1931,&#34;targetHeight&#34;:398,&#34;scaleAttr&#34;:false,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="211" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-3-1-1024x211.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-3-1-1024x211.png 1024w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-3-1-300x62.png 300w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-3-1-768x158.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-3-1-1536x317.png 1536w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-3-1-1568x323.png 1568w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-3-1.png 1931w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><em>Table 3: Dense vs. Sparse Llama 3.1 8B evaluation results for fine-tuning across GSM8K, Evol-CodeAlpaca, and Ultrachat-200K</em>.</figcaption></figure>



<h3>Quantization and Sparsity</h3>



<p>To further optimize Sparse Llama, we applied 4-bit post-training quantization using the W4A16 scheme. This approach preserves the 2:4 sparsity pattern, where weights are quantized to 4-bit integers, and activations remain at 16-bit precision.</p>



<p>Sparse quantized versions maintained accuracy with minimal degradation compared to unquantized models while enabling significant compression and inference performance gains. See <em>Table 4</em> for detailed accuracy comparisons across tasks.</p>



<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4-Sparse-Foundational-Llama-Table-4-1.png&#34;,&#34;figureClassNames&#34;:&#34;wp-block-image size-large&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7342&#34;,&#34;imgStyles&#34;:null,&#34;targetWidth&#34;:1443,&#34;targetHeight&#34;:632,&#34;scaleAttr&#34;:false,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="448" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-4-1-1024x448.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-4-1-1024x448.png 1024w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-4-1-300x131.png 300w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-4-1-768x336.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-4-1.png 1443w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><em>Table 4: Dense, Sparse, and Sparse Quantized evaluation results for fine-tuning across GSM8K, Evol-CodeAlpaca, and Ultrachat-200K</em>.</figcaption></figure>



<h3>Inference</h3>



<p>Sparse Llama’s inference performance was benchmarked using <a href="https://github.com/vllm-project/vllm" title="">vLLM</a>, the high-performance inference engine, and compared to dense variants across several real-world use cases—ranging from code completion to large summarization—using the latest nightly build (11/22/2024), as detailed in <em>Table 5</em>.</p>


<div>
<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4-Sparse-Foundational-Llama-Table-5-1.png&#34;,&#34;figureClassNames&#34;:&#34;aligncenter size-full is-resized&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7344&#34;,&#34;imgStyles&#34;:&#34;width:500px&#34;,&#34;targetWidth&#34;:768,&#34;targetHeight&#34;:720,&#34;scaleAttr&#34;:false,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="768" height="720" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-5-1.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-5-1.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-5-1-300x281.png 300w" sizes="(max-width: 768px) 100vw, 768px"/><figcaption><em>Table 5: Use cases for benchmarking the performance impact of 2:4 sparsity and quantization.</em></figcaption></figure></div>


<h4>Single-Stream Deployments</h4>



<p>In single-stream scenarios, combining sparsity and quantization resulted in significant latency reductions ranging from <strong>3.7x to 5.0x faster inference</strong> than dense, 16-bit models, with <strong>1.8x speedups</strong> from sparsity alone. <em>Table 6</em> provides full results across the various use cases.</p>



<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4-Sparse-Foundational-Llama-Table-6-1.png&#34;,&#34;figureClassNames&#34;:&#34;wp-block-image size-large&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7345&#34;,&#34;imgStyles&#34;:null,&#34;targetWidth&#34;:1287,&#34;targetHeight&#34;:720,&#34;scaleAttr&#34;:false,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="573" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-6-1-1024x573.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-6-1-1024x573.png 1024w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-6-1-300x168.png 300w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-6-1-768x430.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-6-1.png 1287w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><em>Table 6: Effect of pruning and quantization on latency for synchronous deployment across multiple use cases.</em></figcaption></figure>



<h4>Asynchronous Deployments</h4>



<p>In multi-query scenarios, we highlight the gains focusing on maximum throughput for the various scenarios to compare Sparse Llama with its dense counterparts. Sparse Llama achieved <strong>1.4x to 2.1x speedup</strong> in maximum query rates compared to the dense, 16-bit model, while quantization alone offered only minimal improvements and, in some cases, performed worse. <em>Table 7</em> provides full results across the various use cases.</p>



<figure data-wp-context="{&#34;uploadedSrc&#34;:&#34;https:\/\/neuralmagic.com\/wp-content\/uploads\/2024\/11\/CHART-2_4-Sparse-Foundational-Llama-Table-7-1.png&#34;,&#34;figureClassNames&#34;:&#34;wp-block-image size-large&#34;,&#34;figureStyles&#34;:null,&#34;imgClassNames&#34;:&#34;wp-image-7346&#34;,&#34;imgStyles&#34;:null,&#34;targetWidth&#34;:1287,&#34;targetHeight&#34;:720,&#34;scaleAttr&#34;:false,&#34;ariaLabel&#34;:&#34;Enlarge image&#34;,&#34;alt&#34;:&#34;&#34;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="573" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-7-1-1024x573.png" alt="" srcset="https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-7-1-1024x573.png 1024w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-7-1-300x168.png 300w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-7-1-768x430.png 768w, https://neuralmagic.com/wp-content/uploads/2024/11/CHART-2_4-Sparse-Foundational-Llama-Table-7-1.png 1287w" sizes="(max-width: 1024px) 100vw, 1024px"/><figcaption><em>Table 7: Effect of pruning and quantization on maximum query rate for asynchronous deployment across multiple use cases.</em></figcaption></figure>



<h3><strong>Empowering AI Efficiency</strong></h3>



<p>Sparse Llama 3.1 represents a new step for scalable and efficient LLMs through SOTA sparsity and quantization techniques. From few-shot evaluations to fine-tuning performance and real-world inference benchmarks, Sparse Llama delivers substantial improvements in inference performance, making advanced AI more accessible.</p>



<p>We are excited to see how the community builds on this foundation. Whether you’re optimizing deployments, exploring model compression, or scaling AI to new heights, Sparse Llama offers a compelling path forward. Together, let’s shape the future of efficient AI.</p>
<div id="was-this-helpful" data-post-id="7302" data-thank-text="Thanks for your feedback!"><p>Was this article helpful?</p><p><span data-value="1">Yes</span><span data-value="0">No</span></p></div>		
		</div>
		
	</div></div>
  </body>
</html>
