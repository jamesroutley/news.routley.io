<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://minha.sh/posts/so,-you-want-to-chunk-really-fast">Original</a>
    <h1>So, you want to chunk really fast?</h1>
    
    <div id="readability-page-1" class="page"><article><p><img src="https://minha.sh/meme_chunk.png" width="auto" height="auto" alt=""/></p>
<p>we’ve been working on <a href="https://github.com/chonkie-inc/chonkie">chonkie<svg aria-hidden="true" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, a chunking library for RAG pipelines, and at some point we started benchmarking on wikipedia-scale datasets.</p>
<p>that’s when things started feeling… <em>slow</em>.</p>
<p>not unbearably slow, but slow enough that we started wondering: what’s the theoretical limit here? how fast can text chunking actually get if we throw out all the abstractions and go straight to the metal?</p>
<p>this post is about that rabbit hole, and how we ended up building memchunk.</p>
<h2 id="what-even-is-chunking">what even is chunking?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#what-even-is-chunking"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>if you’re building anything with LLMs and retrieval, you’ve probably dealt with this: you have a massive pile of text, and you need to split it into smaller pieces that fit into embedding models or context windows.</p>
<p>the naive approach is to split every N characters. but that’s dumb — you end up cutting sentences in half, and your retrieval quality tanks.</p>
<p>the smart approach is to split at semantic boundaries: periods, newlines, question marks. stuff that actually indicates “this thought is complete.”</p>
<pre><code>&#34;Hello world. How are you?&#34;
     → [&#34;Hello world.&#34;, &#34; How are you?&#34;]
</code></pre>
<h3 id="why-delimiters-are-enough">why delimiters are enough<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#why-delimiters-are-enough"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>there are fancy chunking strategies out there — sentence splitters, recursive chunkers, semantic chunkers that use embeddings. but for most use cases, the key thing is just not cutting sentences in half.</p>
<p>token-based and character-based chunkers don’t care where sentences end. they just split at N tokens or N bytes. that means you get chunks like:</p>
<pre><code>&#34;The quick brown fox jumps over the la&#34;
&#34;zy dog.&#34;
</code></pre>
<p>the embedding for that first chunk is incomplete. it’s a sentence fragment.</p>
<p>delimiter-based chunking avoids this. if you split on <code>.</code> and <code>?</code> and <code>\n</code>, your chunks end at natural boundaries. you don’t need NLP pipelines or embedding models to find good split points — just byte search.</p>
<p>simple concept. but doing it fast? that’s where things get interesting.</p>
<h2 id="enter-memchr">enter memchr<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#enter-memchr"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>the <a href="https://github.com/BurntSushi/memchr">memchr<svg aria-hidden="true" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> crate by Andrew Gallant is the foundation here. it’s a byte search library with multiple layers of optimization.</p>
<h3 id="the-fallback-swar">the fallback: SWAR<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-fallback-swar"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>even without SIMD instructions, memchr is fast. the generic implementation uses SWAR (SIMD Within A Register) — a clever trick that processes 8 bytes at a time using regular 64-bit operations.</p>
<p>the core idea: to find a byte in a chunk, XOR the chunk with the needle splatted across all 8 positions. any matching byte becomes zero. then use bit manipulation to detect if any byte is zero:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-light github-dark"><code data-language="rust" data-theme="github-light github-dark"><span data-line=""><span>fn</span><span> has_zero_byte</span><span>(x</span><span>:</span><span> usize</span><span>) </span><span>-&gt;</span><span> bool</span><span> {</span></span>
<span data-line=""><span>    const</span><span> LO</span><span>:</span><span> usize</span><span> =</span><span> 0x0101010101010101</span><span>;</span></span>
<span data-line=""><span>    const</span><span> HI</span><span>:</span><span> usize</span><span> =</span><span> 0x8080808080808080</span><span>;</span></span>
<span data-line=""><span>    (x</span><span>.</span><span>wrapping_sub</span><span>(</span><span>LO</span><span>) </span><span>&amp;</span><span> !</span><span>x </span><span>&amp;</span><span> HI</span><span>) </span><span>!=</span><span> 0</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>// needle = 0x2E (period), splatted = 0x2E2E2E2E2E2E2E2E</span></span>
<span data-line=""><span>// chunk XOR splatted -&gt; any matching byte becomes 0x00</span></span>
<span data-line=""><span>// has_zero_byte detects if any lane is zero</span></span></code></pre></figure>
<p>the <code>has_zero_byte</code> trick works because subtracting 1 from a zero byte causes a borrow that propagates to the high bit. no branches, no loops over individual bytes.</p>
<p><img src="https://minha.sh/swar_memchunk.png" width="auto" height="auto" alt=""/></p>
<h3 id="the-fast-path-avx2sse2">the fast path: AVX2/SSE2<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-fast-path-avx2sse2"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>on x86_64 with SIMD support, memchr upgrades to vector instructions. AVX2 processes 32 bytes at once:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-light github-dark"><code data-language="rust" data-theme="github-light github-dark"><span data-line=""><span>// broadcast needle to all 32 lanes</span></span>
<span data-line=""><span>let</span><span> needle_vec </span><span>=</span><span> _mm256_set1_epi8</span><span>(needle </span><span>as</span><span> i8</span><span>);</span></span>
<span data-line=""><span>// load 32 bytes from haystack</span></span>
<span data-line=""><span>let</span><span> chunk </span><span>=</span><span> _mm256_loadu_si256</span><span>(haystack);</span></span>
<span data-line=""><span>// compare all 32 bytes simultaneously</span></span>
<span data-line=""><span>let</span><span> matches </span><span>=</span><span> _mm256_cmpeq_epi8</span><span>(chunk, needle_vec);</span></span>
<span data-line=""><span>// extract match positions as a 32-bit mask</span></span>
<span data-line=""><span>let</span><span> mask </span><span>=</span><span> _mm256_movemask_epi8</span><span>(matches);</span></span></code></pre></figure>
<p>if <code>mask</code> is non-zero, at least one byte matched. <code>trailing_zeros()</code> gives the position.</p>
<p>memchr also has a hybrid strategy: it stores both SSE2 (16-byte) and AVX2 (32-byte) searchers, using SSE2 for small haystacks and AVX2 for larger ones.</p>
<h3 id="the-api">the API<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-api"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>memchr provides three variants:</p>
<ul>
<li><code>memchr(needle, haystack)</code> — find one byte</li>
<li><code>memchr2(n1, n2, haystack)</code> — find either of two bytes</li>
<li><code>memchr3(n1, n2, n3, haystack)</code> — find any of three bytes</li>
</ul>
<p>for two or three needles, it runs multiple comparisons and ORs the results together. but notice: it stops at three.</p>
<h2 id="the-three-character-limit">the three-character limit<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-three-character-limit"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>why only three? from memchr’s source code:</p>
<blockquote>
<p>only one, two and three bytes are supported because three bytes is about the point where one sees diminishing returns. beyond this point and it’s probably (but not necessarily) better to just use a simple <code>[bool; 256]</code> array or similar.</p>
</blockquote>
<p>each additional needle requires another comparison and OR operation. with 3 needles you’re doing 3 broadcasts, 3 comparisons, and 2 ORs per 32-byte chunk. a 4th needle adds another ~33% overhead, and the gains from SIMD start looking less impressive compared to simpler approaches.</p>
<p>the memchr authors explicitly chose not to implement <code>memchr4</code> or beyond — it’s not a limitation, it’s a design decision.</p>
<p>so what happens when you want to split on <code>\n</code>, <code>.</code>, <code>?</code>, <code>!</code>, and <code>;</code>? that’s 5 delimiters. memchr can’t help directly.</p>
<h2 id="the-fallback-lookup-tables">the fallback: lookup tables<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-fallback-lookup-tables"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>when you have more than 3 delimiters, we fall back to a classic technique: the 256-entry lookup table.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-light github-dark"><code data-language="rust" data-theme="github-light github-dark"><span data-line=""><span>fn</span><span> build_table</span><span>(delimiters</span><span>:</span><span> &amp;</span><span>[</span><span>u8</span><span>]) </span><span>-&gt;</span><span> [</span><span>bool</span><span>; </span><span>256</span><span>] {</span></span>
<span data-line=""><span>    let</span><span> mut</span><span> table </span><span>=</span><span> [</span><span>false</span><span>; </span><span>256</span><span>];</span></span>
<span data-line=""><span>    for</span><span> &amp;</span><span>byte </span><span>in</span><span> delimiters {</span></span>
<span data-line=""><span>        table[byte </span><span>as</span><span> usize</span><span>] </span><span>=</span><span> true</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>    table</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>fn</span><span> is_delimiter</span><span>(byte</span><span>:</span><span> u8</span><span>, table</span><span>:</span><span> &amp;</span><span>[</span><span>bool</span><span>; </span><span>256</span><span>]) </span><span>-&gt;</span><span> bool</span><span> {</span></span>
<span data-line=""><span>    table[byte </span><span>as</span><span> usize</span><span>]  </span><span>// single array lookup, no branching</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>this is O(1) per byte — just an array index. no loops, no comparisons, no branching. modern CPUs love this because it’s completely predictable.</p>
<p>is it as fast as SIMD? no. but it’s still really fast, and it handles arbitrary delimiter sets.</p>
<h2 id="why-search-backwards">why search backwards?<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#why-search-backwards"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>there’s one more trick: we search backwards from <code>chunk_size</code>, not forwards from 0.</p>
<p>if you search forward, you need to scan through the entire window to find where to split. you’d find a delimiter at byte 50, but you can’t stop there — there might be a better split point closer to your target size. so you keep searching, tracking the last delimiter you saw, until you finally cross the chunk boundary. that’s potentially thousands of matches and index updates.</p>
<p>backward search? one lookup. start at <code>chunk_size</code>, search backward, and the first delimiter you hit is your split point. done.</p>
<pre><code>target_size = 100
text: &#34;Hello. World. This is a test. Here&#39;s more text...&#34;

forward search:
  - find &#34;.&#34; at 6, save it, keep going
  - find &#34;.&#34; at 13, save it, keep going
  - find &#34;.&#34; at 30, save it, keep going
  - ... eventually cross boundary, use last saved position
  → multiple searches, lots of bookkeeping

backward search:
  - start at position 100, search backward
  - find &#34;.&#34; at 30, done
  → one search, no bookkeeping
</code></pre>
<p>fewer operations means less overhead. we use <code>memrchr</code> (reverse memchr) instead of <code>memchr</code> for exactly this reason.</p>
<h2 id="putting-it-together">putting it together<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#putting-it-together"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>memchunk automatically picks the right strategy:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="rust" data-theme="github-light github-dark"><code data-language="rust" data-theme="github-light github-dark"><span data-line=""><span>fn</span><span> find_last_delimiter</span><span>(window</span><span>:</span><span> &amp;</span><span>[</span><span>u8</span><span>], delimiters</span><span>:</span><span> &amp;</span><span>[</span><span>u8</span><span>], table</span><span>:</span><span> Option</span><span>&lt;</span><span>&amp;</span><span>[</span><span>bool</span><span>; 256]&gt;) </span><span>-&gt;</span><span> Option</span><span>&lt;</span><span>usize</span><span>&gt; {</span></span>
<span data-line=""><span>    if</span><span> let</span><span> Some</span><span>(t) </span><span>=</span><span> table {</span></span>
<span data-line=""><span>        // 4+ delimiters: use lookup table</span></span>
<span data-line=""><span>        window</span><span>.</span><span>iter</span><span>()</span><span>.</span><span>rposition</span><span>(</span><span>|&amp;</span><span>b</span><span>|</span><span> t[b </span><span>as</span><span> usize</span><span>])</span></span>
<span data-line=""><span>    } </span><span>else</span><span> {</span></span>
<span data-line=""><span>        // 1-3 delimiters: use SIMD-accelerated memchr</span></span>
<span data-line=""><span>        match</span><span> delimiters</span><span>.</span><span>len</span><span>() {</span></span>
<span data-line=""><span>            1</span><span> =&gt;</span><span> memchr</span><span>::</span><span>memrchr</span><span>(delimiters[</span><span>0</span><span>], window),</span></span>
<span data-line=""><span>            2</span><span> =&gt;</span><span> memchr</span><span>::</span><span>memrchr2</span><span>(delimiters[</span><span>0</span><span>], delimiters[</span><span>1</span><span>], window),</span></span>
<span data-line=""><span>            3</span><span> =&gt;</span><span> memchr</span><span>::</span><span>memrchr3</span><span>(delimiters[</span><span>0</span><span>], delimiters[</span><span>1</span><span>], delimiters[</span><span>2</span><span>], window),</span></span>
<span data-line=""><span>            _ </span><span>=&gt;</span><span> None</span><span>,</span></span>
<span data-line=""><span>        }</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>the table is only built once, lazily, on the first iteration. after that, it’s pure speed.</p>
<h2 id="benchmarks">benchmarks<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#benchmarks"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>we ran this against the other rust chunking libraries we could find:</p>








































<div><table><thead><tr><th>library</th><th>throughput</th><th>vs memchunk</th></tr></thead><tbody><tr><td>memchunk</td><td>164 GB/s</td><td>-</td></tr><tr><td>kiru</td><td>4.5 GB/s</td><td>36x slower</td></tr><tr><td>langchain</td><td>0.35 GB/s</td><td>469x slower</td></tr><tr><td>semchunk</td><td>0.013 GB/s</td><td>12,615x slower</td></tr><tr><td>llama-index</td><td>0.0035 GB/s</td><td>46,857x slower</td></tr><tr><td>text-splitter</td><td>0.0017 GB/s</td><td>96,471x slower</td></tr></tbody></table></div>
<p><img src="https://minha.sh/benchmark.png" width="auto" height="auto" alt=""/></p>
<p>the exact throughput varies with file size and delimiter count — larger files and fewer delimiters favor SIMD more. but even in pessimistic cases (small chunks, many delimiters), we’re still in the hundreds of GB/s range.</p>
<p>for reference, that’s chunking english wikipedia (~20GB) in roughly 120 milliseconds.</p>
<h2 id="python-and-wasm-bindings">python and wasm bindings<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#python-and-wasm-bindings"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>since chonkie is a python library, we needed python bindings. we also added wasm for browser/node use cases.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="python" data-theme="github-light github-dark"><code data-language="python" data-theme="github-light github-dark"><span data-line=""><span>from</span><span> memchunk </span><span>import</span><span> chunk</span></span>
<span data-line=""> </span>
<span data-line=""><span>text </span><span>=</span><span> &#34;Hello world. How are you?&#34;</span></span>
<span data-line=""><span>for</span><span> slice</span><span> in</span><span> chunk(text, </span><span>size</span><span>=</span><span>4096</span><span>, </span><span>delimiters</span><span>=</span><span>&#34;.?&#34;</span><span>):</span></span>
<span data-line=""><span>    print</span><span>(</span><span>bytes</span><span>(</span><span>slice</span><span>))</span></span></code></pre></figure>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="javascript" data-theme="github-light github-dark"><code data-language="javascript" data-theme="github-light github-dark"><span data-line=""><span>import</span><span> { chunk } </span><span>from</span><span> &#34;memchunk&#34;</span></span>
<span data-line=""> </span>
<span data-line=""><span>const</span><span> text</span><span> =</span><span> new</span><span> TextEncoder</span><span>().</span><span>encode</span><span>(</span><span>&#34;Hello world. How are you?&#34;</span><span>)</span></span>
<span data-line=""><span>for</span><span> (</span><span>const</span><span> slice</span><span> of</span><span> chunk</span><span>(text, { size: </span><span>4096</span><span>, delimiters: </span><span>&#34;.?&#34;</span><span> })) {</span></span>
<span data-line=""><span>  console.</span><span>log</span><span>(</span><span>new</span><span> TextDecoder</span><span>().</span><span>decode</span><span>(slice))</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>both return zero-copy views (<code>memoryview</code> in python, subarray in js), so you get most of the performance even across FFI boundaries.</p>
<h2 id="wrapping-up">wrapping up<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#wrapping-up"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>the techniques here aren’t novel — SIMD search and lookup tables are textbook stuff. the win comes from knowing when to use which:</p>
<ol>
<li>1-3 delimiters → memchr’s SIMD search</li>
<li>4+ delimiters → 256-entry lookup table</li>
<li>minimize allocations → return offsets, let the caller slice</li>
</ol>
<p>memchunk is on <a href="https://crates.io/crates/memchunk">crates.io<svg aria-hidden="true" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, <a href="https://pypi.org/project/memchunk/">pypi<svg aria-hidden="true" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>, and <a href="https://www.npmjs.com/package/memchunk">npm<svg aria-hidden="true" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> if you want to try it out.</p></article></div>
  </body>
</html>
