<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://peachyblog.bearblog.dev/graph-neural-network-potpourri/">Original</a>
    <h1>Graph Neural Network Potpourri</h1>
    
    <div id="readability-page-1" class="page"><div><p>Currently, I am trying to create an RL-based compiler optimizer for linear algebra kernels. Much of the details need to be ironed out, but the general idea is to create an IR, a set of rewrite rules over the IR, and an agent that learns, given an IR, how to apply rewrite rules effectively.</p>
<p>Since IRs are usually expressed as graphs or trees, a good model choice would be a <a href="https://en.wikipedia.org/wiki/Graph_neural_network">Graph Neural Network</a>. When it comes to Graph Neural Networks (GNNs), two popular choices are:</p>
<ul>
<li><a href="https://distill.pub/2021/understanding-gnns/">Graph Convolutional Network (GCN)</a>. Barring differences between <em>spectral</em> and <em>non-spectral</em> convolutions, these networks essentially perform, for each graph node, a convolution over the node&#39;s neighbors. We can think of CNNs as a special case of a GCN, where each pixel has an edge to all neighboring pixels.</li>
<li><a href="https://arxiv.org/pdf/1710.10903.pdf">Graph Attention Network (GAT)</a>. These networks use a form of <em>masked attention</em>, where they use graph&#39;s <em>adjacency matrix</em> to encode the graph structure into each attention module, ensuring that each node can only attend to its neighbors. A sequence-based masked attention can also be thought of as a special case of a GAT, where each token contains edges to all previous tokens.</li>
</ul>
<p>However, in both papers I found attempting similar work (<a href="https://openreview.net/attachment?id=SylyHkHYDB&amp;name=original_pdf">Knossos</a> and <a href="https://arxiv.org/pdf/1810.00337.pdf">NeuRewriter</a>), the choice of model was neither. Both opted for some kind of recurrent model--in <em>Knossos</em>, the authors use a GRU that combines neighbor embeddings for 10 timesteps. In <em>NeuRewriter</em>, the authors use an LSTM that, for each AST node, combines the embeddings of its children.</p>
<p>I don&#39;t know exactly why the authors chose this, but here is some (highly speculative) reasoning:</p>
<h2 id="why-not-gcn">Why not GCN?</h2>
<p>It turns out that the best-performing GCNs only use 2 layers. Any more, and we risk running into the <em>over-smoothing</em> problem, where node features become indistinguishable from each other (visualization coming sometime, hopefully). Knossos combines features from neighbors up to 10 steps away, far more than the 2 steps GCNs perform best at.</p>
<h2 id="why-not-gat">Why not GAT?</h2>
<p>A guess would be that IRs are too big. In Knossos, IRs can contain up to 10000 nodes. To support this, we would have to create a 1e4 * 1e4 matrix. Additionally, since IR shapes are highly variable, an IR with 100 nodes would still need to use the entire matrix.</p>
<h2 id="why-grus-lstms">Why GRUs/LSTMs?</h2>
<p>Again, I am speculating, but GRUs, through reset gates, and LSTMs, through forget gates, have the ability to drop old, irrelevant information. Additionally, through each cell&#39;s update gates, these nodes have the ability to determine how much a current node&#39;s information gets added to the hidden state. There also seems to be prior art (e.g. <a href="https://arxiv.org/pdf/1511.05493.pdf">here</a>).</p>
<p>So yea! That summarizes my weekend investigation in GNNs. Apologies for any inaccuracies and/or hand-waviness :)</p>
</div></div>
  </body>
</html>
