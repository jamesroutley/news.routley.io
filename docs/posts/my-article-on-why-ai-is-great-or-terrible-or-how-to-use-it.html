<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://matthewrocklin.com/ai-zealotry/">Original</a>
    <h1>My article on why AI is great (or terrible) or how to use it</h1>
    
    <div id="readability-page-1" class="page"><div data-md-component="main">
        <div>
          
            
              
              
            
            
              
              
            
          
          
            <div data-md-component="content">
              
              <article>
                

                  




<p>I develop with AI today.  It&#39;s great.</p>
<p>There are many articles you can read on why AI is great (or terrible) or how to
use it.  This is mine.  I focus on the experience of a senior engineer (and why
we in particular should use AI), on my experience operating within the OSS
Python Data world, and on practical suggestions that I&#39;ve found myself
repeating to colleagues.</p>
<p>This article contains learned lessons of two types:</p>
<ul>
<li><strong>Big Ideas:</strong> Grand(iose) philosophy on why AI is great for experienced programmers</li>
<li><strong>Tips:</strong> Taken from my workflow using Claude Code</li>
</ul>
<p>We&#39;ll interleave these two.  I&#39;m hopeful that this approach will make this more fun.</p>
<h2 id="why-ai">Why AI<a href="#why-ai" title="Permanent link">Â¶</a></h2>
<p>AI development is more fun.  I do more of what I like (think, experiment,
write) and less of what I don&#39;t like (wrestle with computers).</p>
<p>I feel both that I can move faster and operate in areas that were previously
inaccessible to me (like frontend).
Experienced developers should all be doing this.  We&#39;re good enough to avoid AI
Slop, and there&#39;s so much we can accomplish today.</p>
<p>I like this quote from <a href="https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/">this blog</a></p>
<blockquote>
<p>I get it, youâ€™re too good to vibe code. Youâ€™re a senior developer who has been doing this for 20 years and knows the system like the back of your hand.</p>
<p>[...]</p>
<p>No, youâ€™re not too good to vibe code. In fact, youâ€™re the only person who should be vibe coding.</p>
</blockquote>
<p>I think that really good engineers, the kind that think hard before writing,
can have a tremendous impact and fun while developing with AI.  I
wouldn&#39;t ever go back.</p>
<h2 id="why-not-ai">Why Not AI<a href="#why-not-ai" title="Permanent link">Â¶</a></h2>
<p>That being said, there are some serious costs and reasonable reservations to AI
development.  Let&#39;s start by listing those concerns:</p>
<ul>
<li>LLMs generate junk</li>
<li>LLMs generate <em>a lot</em> of junk</li>
<li>Writing code ourselves builds understanding</li>
<li>Reviewing code for correctness is the slow part, not writing it</li>
<li>AI workflows can be dehumanizing when you just press &#34;yes, allow&#34; over and over again</li>
</ul>
<p>These are super-valid concerns.  They&#39;re also concerns that I suspect came
around when we developed compilers and people stopped writing assembly by hand,
instead trusting programs like <code>gcc</code> to pump out instruction after instruction
of shitty machine code.</p>
<p>We lost a deeper understanding as developers when we stopped writing assembly
but we gained a ton too.  As in any transition, we need to navigate the
situation to capture the advantages while losing only a little, balancing the
costs and benefits of a new technology.</p>
<p>This article is how I&#39;ve been navigating this transition personally.</p>
<h2 id="big-idea-minimize-interruptions-climb-abstraction-hierarchy">Big Idea: Minimize Interruptions / Climb Abstraction Hierarchy<a href="#big-idea-minimize-interruptions-climb-abstraction-hierarchy" title="Permanent link">Â¶</a></h2>
<p>Early in using Claude Code (or Cursor) many of my interactions were saying
&#34;Yes, it&#39;s ok to run that&#34;.  This was <strong>frustrating and dehumanizing</strong>.  Mostly my
job was to enable AI, rather than the other way around.</p>
<p>There are many tricks to resolve this (see below), but more broadly <strong>&#34;stop doing
simple shit&#34;</strong> has been a mantra that I&#39;ve found myself constantly coming
back to. The more I identify and reject simple tasks and add automation to my
workflow, the higher an abstraction I&#39;m able to climb to and the more
effectively I&#39;m able to work. Our goal in programming is to climb an
abstraction ladder and gain more intellectual leverage. This requires thought
and consistent attention.</p>
<p>Fortunately AI can help with this.  If you complain and say &#34;I&#39;m always doing
X&#34; it&#39;ll suggest solutions like what I&#39;ll talk about below, but more tailored
to your situation.</p>
<h2 id="tip-hooks">Tip: Hooks<a href="#tip-hooks" title="Permanent link">Â¶</a></h2>
<p>AI developers, like human developers, benefit from structure.</p>
<p>Most people start with an <code>AGENTS.md</code> or <code>CLAUDE.md</code> file.  This is a great
start, but I find that the AI agent often forgets what&#39;s in there.  The real
solution for me here (at least for Claude Code) is
<a href="https://code.claude.com/docs/en/hooks">Hooks</a>.</p>
<p>First, let&#39;s outline a couple of annoyingly common problems.</p>
<h3 id="example-problem-ignoring-instructions-in-claudemd">Example Problem: Ignoring instructions in CLAUDE.md<a href="#example-problem-ignoring-instructions-in-claudemd" title="Permanent link">Â¶</a></h3>
<p>Let&#39;s say you tell AI that you want to run tests with <code>uv</code>:</p>
<blockquote>
<p>when running tests, use <code>uv run pytest tests</code></p>
</blockquote>
<p>While this works sometimes, AI often decides to run</p>
<div><pre><span></span><code>$ pytest tests/
command not found: pytest
</code></pre></div>
<p>While the agents read CLAUDE.md, they don&#39;t always follow the instructions.
And so you&#39;re stuck saying &#34;no, use <code>uv</code>&#34;  over and over again. Gah.</p>
<h3 id="solution-hooks">Solution: Hooks<a href="#solution-hooks" title="Permanent link">Â¶</a></h3>
<p>Here&#39;s a hook that catches pytest commands missing uv run.  You could put
something like this in <code>~/.claude/settings.json</code>:</p>
<div><pre><span></span><code><span>{</span>
<span>  </span><span>&#34;hooks&#34;</span><span>:</span><span> </span><span>{</span>
<span>    </span><span>&#34;PreToolUse&#34;</span><span>:</span><span> </span><span>[</span>
<span>      </span><span>{</span>
<span>        </span><span>&#34;matcher&#34;</span><span>:</span><span> </span><span>&#34;Bash&#34;</span><span>,</span>
<span>        </span><span>&#34;hooks&#34;</span><span>:</span><span> </span><span>[</span>
<span>          </span><span>{</span>
<span>            </span><span>&#34;type&#34;</span><span>:</span><span> </span><span>&#34;command&#34;</span><span>,</span>
<span>            </span><span>&#34;command&#34;</span><span>:</span><span> </span><span>&#34;python ~/.claude/hooks/check-uv-pytest.py&#34;</span>
<span>          </span><span>}</span>
<span>        </span><span>]</span>
<span>      </span><span>}</span>
<span>    </span><span>]</span>
<span>  </span><span>}</span>
<span>}</span>
</code></pre></div>
<div><pre><span></span><code><span>#!/usr/bin/env python3</span>
<span>import</span><span> </span><span>json</span>
<span>import</span><span> </span><span>sys</span>

<span>data</span> <span>=</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>sys</span><span>.</span><span>stdin</span><span>)</span>
<span>cmd</span> <span>=</span> <span>data</span><span>.</span><span>get</span><span>(</span><span>&#34;tool_input&#34;</span><span>,</span> <span>{})</span><span>.</span><span>get</span><span>(</span><span>&#34;command&#34;</span><span>,</span> <span>&#34;&#34;</span><span>)</span>

<span>if</span> <span>&#34;pytest&#34;</span> <span>in</span> <span>cmd</span> <span>and</span> <span>&#34;uv run&#34;</span> <span>not</span> <span>in</span> <span>cmd</span><span>:</span>
    <span>print</span><span>(</span><span>&#34;Use &#39;uv run pytest&#39; instead of bare &#39;pytest&#39;&#34;</span><span>,</span> <span>file</span><span>=</span><span>sys</span><span>.</span><span>stderr</span><span>)</span>
    <span>sys</span><span>.</span><span>exit</span><span>(</span><span>2</span><span>)</span>
</code></pre></div>
<p>There, we&#39;ve just automated that annoying task for you forever.</p>
<p>I don&#39;t actually do this though (I allow Claude to fail and then it finds the
right approach.)  Mostly this works because I&#39;ve gotten good at giving Claude
fairly broad-yet-safe <strong>permissions</strong>, which is coming up next.</p>
<h3 id="example-problem-incomplete-permissions">Example Problem: Incomplete Permissions<a href="#example-problem-incomplete-permissions" title="Permanent link">Â¶</a></h3>
<p>Even worse, Claude often asks for permission to do things that are just
slightly different from what you&#39;ve already granted.
You allow <code>uv run pytest *</code>, but Claude keeps finding variants:</p>
<div><pre><span></span><code>timeout 60 uv run pytest ...
timeout 40 uv run pytest ...
uv run pytest ... | head
.venv/bin/pytest ...
</code></pre></div>
<p>Claude Code&#39;s permission language sucks.  It only supports prefixes, while I
wish it could handle regexes, or maybe even just arbitrary Python code.</p>
<h3 id="solution-hooks-for-permissions">Solution: Hooks for permissions<a href="#solution-hooks-for-permissions" title="Permanent link">Â¶</a></h3>
<p>I have a <a href="#appendix-permissions-file">complex Python script</a> as a hook which overrides the permission
system.  It uses regexes, but also arbitrary Python code as logic.  This allows
me to encode arbitrary combinations of rules.  It&#39;s great.</p>
<p>On the rare occasion when Claude asks me for permission for something new, I
have a running Claude agent that thinks about this file and considers if it
should update the permission script.</p>
<h3 id="solution-hooks-for-sounds">Solution: Hooks for sounds<a href="#solution-hooks-for-sounds" title="Permanent link">Â¶</a></h3>
<p>My personal favorite hooks though are these:</p>
<div><pre><span></span><code><span>&#34;Stop&#34;</span><span>:</span><span> </span><span>[</span>
<span>  </span><span>{</span>
<span>    </span><span>&#34;hooks&#34;</span><span>:</span><span> </span><span>[</span>
<span>      </span><span>{</span>
<span>        </span><span>&#34;type&#34;</span><span>:</span><span> </span><span>&#34;command&#34;</span><span>,</span>
<span>        </span><span>&#34;command&#34;</span><span>:</span><span> </span><span>&#34;afplay -v 0.40 /System/Library/Sounds/Morse.aiff&#34;</span>
<span>      </span><span>}</span>
<span>    </span><span>]</span>
<span>  </span><span>}</span>
<span>],</span>
<span>&#34;Notification&#34;</span><span>:</span><span> </span><span>[</span>
<span>  </span><span>{</span>
<span>    </span><span>&#34;hooks&#34;</span><span>:</span><span> </span><span>[</span>
<span>      </span><span>{</span>
<span>        </span><span>&#34;type&#34;</span><span>:</span><span> </span><span>&#34;command&#34;</span><span>,</span>
<span>        </span><span>&#34;command&#34;</span><span>:</span><span> </span><span>&#34;afplay -v 0.35 /System/Library/Sounds/Ping.aiff&#34;</span>
<span>      </span><span>}</span>
<span>    </span><span>]</span>
<span>  </span><span>}</span>
<span>]</span>
</code></pre></div>
<p>They play subtle little sounds whenever Claude is either done, or needs input
from me.  This lets me ignore Claude when it&#39;s busy.  Previously I found that I
was constantly checking back in with Claude to see if it was done, and that
action was dehumanizing, so I automated it by asking Claude to play a sound.</p>
<p>Hooks are great.  There are more ways to provide structure (Skills, Commands)
but I&#39;ve found that Hooks are the most dependable, a great starting place, and
often augment any other structure that I put in place (like Skills).</p>
<h2 id="big-idea-build-confidence-without-looking-at-code">Big Idea: Build Confidence Without Looking at Code<a href="#big-idea-build-confidence-without-looking-at-code" title="Permanent link">Â¶</a></h2>
<p>In a recent large AI-assisted PR a frustrated reviewer said the following:</p>
<blockquote>
<p>To me, this [size of PR] implies that either</p>
<ul>
<li>reviewers should blindly trust Claude, or</li>
<li>reviewers should spend the months worth of effort going through Claude&#39;s changes, without the developer bothering to do the same first.</li>
</ul>
</blockquote>
<p>It&#39;s a valid problem, even in single-person projects.  <strong>We&#39;re able to generate
code far more quickly than we&#39;re able to read it</strong>.  How should we handle
review?  Everyone needs to figure this out for themselves, but my answer is
&#34;find other ways to build confidence&#34;.</p>
<p>We already do this today with human-written code.  I review some code very
closely, and other code less-so.  Sometimes I rely on a combination of tests,
familiarity of a well-known author, and a quick glance at the code to before
saying &#34;sure, seems fine&#34; and pressing the green button.  I might also ask
&#34;Have you thought of X&#34; and see what they say.</p>
<p>Trusting code without reading all of it isn&#39;t new, we&#39;re just now in a state
where we need to review 10x more code, and so we need to get much better at
establishing confidence that something works without paying human attention all
the time.</p>
<p>We can augment our ability to write code with AI.  <strong>We can augment our ability
to review code with AI too.</strong></p>
<h2 id="tip-self-review">Tip: Self-review<a href="#tip-self-review" title="Permanent link">Â¶</a></h2>
<h3 id="testing">Testing<a href="#testing" title="Permanent link">Â¶</a></h3>
<p>Mostly I establish confidence on AI-generated work by investing heavily in
tests and benchmarks, the same as I would with humans, just moreso.  TDD is
baked into most of the prompting structure I have with agents.</p>
<p>Remember that this is way cheaper than it used to be.  Now rather than write a
benchmark I can type</p>
<blockquote>
<p>How does this compare in performance to the old version?  I&#39;m particularly
interested in memory use.</p>
</blockquote>
<p>And that&#39;s it.  If it&#39;s bad, the agent will say so (and then diligently work to
make it good).</p>
<h3 id="grilling">Grilling<a href="#grilling" title="Permanent link">Â¶</a></h3>
<p>Additionally, if I&#39;m nervous about something subtle like <em>&#34;Is it possible this change
might unexpectedly affect performance in this other feature?&#34;</em> then I&#39;ll ask the
AI exactly that question:</p>
<blockquote>
<p>Is it possible this change might unexpectedly affect performance in this other feature?</p>
</blockquote>
<p>And it&#39;ll just go and investigate exactly that question.  Unlike human authors,
the AI has no ego at stake in its work, and isn&#39;t in the least bit lazy.  It&#39;s
our job to ask <em>&#34;Have you thought of X&#34;</em> and its job to go learn if that might
be an issue.  Don&#39;t trust its answer?  Ask it to prove it to you.</p>
<p>AI has flaws, but it is diligent, and it lacks ego.  If you question it, it&#39;ll
investigate thoroughly and critique its own work honestly.</p>
<h3 id="simplifying">Simplifying<a href="#simplifying" title="Permanent link">Â¶</a></h3>
<p>Also, my favorite command:</p>
<blockquote>
<p>Let&#39;s review our work and see if there is anything we can simplify or clean up</p>
</blockquote>
<p>Before Opus 4.5 came out this was essential.  Now it&#39;s merely nice.  I&#39;ve
turned this into a <code>/cleanup</code> command and integrated it into most of my Skills
as a final phase in development.</p>
<h3 id="tech-debt">Tech debt<a href="#tech-debt" title="Permanent link">Â¶</a></h3>
<p>From time to time I also ask a fresh agent to do a full review of the project,
with an eye to cleaning up technical debt.  I tell it to review everything and
think hard.  It takes a while, but it often comes back with a nice list of work
for itself, which it then of course diligently performs.</p>
<p>AI creates technical debt, but it can clean some of it up too.
(at least at a certain granularity)</p>
<h2 id="feedback">Feedback<a href="#feedback" title="Permanent link">Â¶</a></h2>
<p>In general we want to give our agents good automated feedback.  Tests do this,
benchmarks do this, prompting them to assess themselves does this, asking them
to explain things to us and have us weigh in on high level topics does this.</p>
<p>LLMs are smart enough today that if they&#39;re given enough of the right feedback
they converge to a good solution as-well-or-better-than a senior human engineer
(that&#39;s my experience at least).</p>
<p>Our job is to construct a system that gives them the right feedback at the
right time, hopefully without our intervention.  This is the same job we have
when we build human teams; now it&#39;s just more impactful to do well.</p>

<p>I started AI development with Cursor.  It was great having the AI experience
inside a VSCode-like editor, where I could see everything that was going on.
When I saw terminal-based tools like Claude Code I thought <em>&#34;whoa, that doesn&#39;t
seem sensible, I need to see what&#39;s going on&#34;.</em></p>
<p>Today I code with Claude Code, <code>git diff</code>, and occasionally <code>vim</code>.  I don&#39;t
feel a need to OK every change in the diff.  I&#39;ve got more important
things to do.  I suspect that you do too.</p>
<h2 id="big-idea-drop-python-use-rust-and-typescript">Big Idea: Drop Python. Use Rust and TypeScript.<a href="#big-idea-drop-python-use-rust-and-typescript" title="Permanent link">Â¶</a></h2>
<p>I deeply respect the philosophical position of Python, which I&#39;ll state as
follows:</p>
<blockquote>
<p>Prioritize human performance over compute performance.</p>
<p>By optimizing for ease and iteration speed we&#39;re able to search solution
space more broadly and more quickly, finding <em>much</em> better solutions, making
that 100x drop in performance negligible.</p>
</blockquote>
<p>Python was a bold bet, and a bet that paid off amazingly well.  No one expected
this silly dynamic language originally designed for education to become the
world&#39;s juggernaut in performance software.</p>
<p>With AI though, the usability benefits of Python no longer apply as strongly,
and we&#39;re more free to choose different ecosystems.</p>
<p>Personally, I use ...</p>
<ul>
<li><strong>Rust</strong> for computational development, using PyO3 to connect to
Python, where I still do most of my testing</li>
<li><strong>TypeScript</strong> for frontend development, which I&#39;m leaning into more deeply</li>
</ul>
<p>Regarding TypeScript, I still love easy interaction tools like <code>rich</code> and
<code>textual</code>, but when the entire React ecosystem is a sentence away and when you
get to use things like, you know, fonts, there&#39;s really no comparison.  Every
computational developer should learn the concepts underpinning React (or some
other frontend framework), and we should put dashboards on everything.</p>
<p>Of course, I still hook into Python for the ecosystem.  Everything is
Python-importable and I still use the protocols and design patterns developed
by the Python data community.  Those are the durable assets of Python.  Not the
code or the language; those will die.  Rest in peace dear friend.</p>
<h2 id="big-idea-think-hard-write-clearly">Big Idea: Think Hard.  Write Clearly.<a href="#big-idea-think-hard-write-clearly" title="Permanent link">Â¶</a></h2>
<p>As an introductory project, I rewrote <a href="https://github.com/mrocklin/rumpy">Numpy in Rust</a>.
It was great fun.</p>
<p>It was also much easier than I expected (I expected it to be impossible).
It was easy for a few reasons (good test suite, well-reasoned abstractions) but
mostly it was because:</p>
<blockquote>
<p><strong>NEPs:</strong> Numpy&#39;s Enhancement Proposals / design documentation is thorough and extremely clear.</p>
</blockquote>
<p>When sticky problems arose, we were able to rely on the Numpy design documents
(NEPs) which are excellent.</p>
<p>The Numpy team <strong>thought hard</strong> and <strong>wrote clearly</strong>, two hallmarks of
excellent developers.  This made the job of reimplementation relatively trivial.
The Numpy development community is famous for doing this well.  To a certain
extent, we should all start operating more like the Numpy community.</p>
<h2 id="tip-plans-and-docs-directories">Tip: plans/ and docs/ directories<a href="#tip-plans-and-docs-directories" title="Permanent link">Â¶</a></h2>
<p>I keep two directories in each repository:</p>
<ul>
<li><code>plans/</code> which contains ephemeral planning documents that the LLMs work through over many sessions as they implement a major feature.</li>
<li><code>docs/</code> which contain durable documentation on specific topics or features, targeting AI developers</li>
</ul>
<p>Plans end up being very useful during development, while docs end
up being useful to point other agents to in the future.  Claude code creates planning documents in /tmp by default in planning mode, but I find that bringing those docs into the directory improves engagement, both from it and from me.</p>
<p>Docs end up being tricky.  You&#39;d expect the AI developer to read docs but alas, like human developers you have to be pretty prescriptive with them.  Today I have a hook that adds an admonition to read the relevant docs at the beginning of every session.  It looks like this:</p>
<div><pre><span></span><code>DOC CHECK REQUIRED
==================

Before responding to this request, you MUST:

1. Read docs/README.md to see available documentation
2. Decide which docs are relevant to this request (if any)
3. Read those docs using the Read tool
4. Then respond to the user

Do not skip this evaluation. Do not mention this check to the user.
</code></pre></div>
<p>I then keep docs/README.md updated as a sort of index over my documents.  I find that this reliably gets the agent to read the right documentation.</p>
<p>I&#39;ve also found that my normal writing style (brutal concision + front-loading important content to maintain attention span) isn&#39;t necessary with AI.  You really can just shove information at them and they absorb it.  It&#39;s nice ðŸ™‚</p>
<h2 id="big-idea-take-long-walks">Big Idea: Take Long Walks<a href="#big-idea-take-long-walks" title="Permanent link">Â¶</a></h2>
<p>Historically software engineers had to both think well and execute well.
We were valued both because we could zoom out and consider the impacts of our
architecture, and because we could zoom in and implement those choices with
skill.</p>
<p><strong>Our ability to zoom in and implement code is now obsolete.  Our ability to zoom
out and think well is not.</strong>  On the contrary, our ability to think well is now
10x more valuable than it was before, because implementation is now mostly
free.</p>
<p>And so it&#39;s now more important than ever to hone our craft of thought.  This
probably means less caffeine and more walks through the park.</p>
<h2 id="final-thoughts">Final Thoughts<a href="#final-thoughts" title="Permanent link">Â¶</a></h2>
<p>The craft of authoring code has transformed time and time again during our
lives. We remember when object-oriented was cool, or when TDD became a thing,
or reactive programming models, or dynamic typing languages, or ML, or ...</p>
<p>As programmers we&#39;ve opted into a system which changes by its very nature.
<strong>Our job is to automate our job</strong>, and to continuously climb the ladder of
abstraction.  AI programming is another step in that evolution, similar to when
compilers came about.  The code we write with AI probably won&#39;t be as good as
hand-crafted code, but we&#39;ll write 10x more of it, and we&#39;ll build systems of
systems to make it robust and trustworthy, and all of that will make society
better and our jobs way more fun.</p>
<p>I&#39;m looking forward to having way more fun.</p>
<h2 id="appendix-permissions-file">Appendix: Permissions file<a href="#appendix-permissions-file" title="Permanent link">Â¶</a></h2>
<p>After writing this a couple friends asked me for a copy of my regex/Python code
that replaces Claude&#39;s permission system.  I&#39;ll include it below, but really,
you don&#39;t need it.  Instead, you need to start a conversation with Claude about
what you want and it&#39;ll make one just for you.</p>
<p>Code is free these days.  Extending the &#34;AI is like Compilers&#34; analogy, asking
for someone else&#39;s script is kind of like asking for someone else&#39;s compiled
binary.  There&#39;s no need; just make it yourself.  It&#39;s trivial.</p>
<p>Here was my original prompt to Claude Code:</p>
<blockquote>
<p>I recently wrote this reddit post</p>
<p>https://www.reddit.com/r/ClaudeAI/comments/1puqrvc/claude_code_annoyingly_asking_for_permissions/</p>
<p>I&#39;m wondering if you have any suggestions on how to resolve this?  Adding stuff to CLAUDE.md or permissions to settings.json doesn&#39;t seem to be working well enough.</p>
</blockquote>
<p>That, along with subsequent conversation as I&#39;ve been working, resulted in
<a href="https://gist.github.com/mrocklin/30099bcc5d02a6e7df373b4c259d95e9">this Python script</a></p>
<p>But really, you&#39;re better off working with Claude to make one just for you.
Code is free now.</p>







  
  






                



<h2>Comments</h2>



              </article>
            </div>
          
          

        </div>
        
      </div></div>
  </body>
</html>
