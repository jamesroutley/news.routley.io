<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.jeronimo.dev/the-two-versions-of-parquet/">Original</a>
    <h1>The two versions of Parquet</h1>
    
    <div id="readability-page-1" class="page"><div id="main" aria-label="Content">
  <article>
    

    <div>
      

      

      <div>
        <div>
          <p>A few days ago, the creators of DuckDB wrote the article: <a href="https://duckdb.org/2025/01/22/parquet-encodings.html" target="_blank" rel="noopener noreferrer">Query Engines: Gatekeepers of the Parquet File Format</a>, which explained how the engines that process Parquet files as SQL tables are blocking the evolution of the format. This is because those engines are not fully supporting the latest specification, and without this support, the rest of the ecosystem has no incentive to adopt it.</p>

<!--more-->

<p>In my experience, this issue is not limited to Query Engines but extends to the tools within the ecosystem. Soon after releasing the first version of <a href="https://github.com/jerolba/parquet-carpet" target="_blank" rel="noopener noreferrer">Carpet</a>, I discovered that there was a version 2 of the format and that the core Java Parquet library does not activate it by default. Since the specification had been finalized for some time, I decided that the best approach was to <a href="https://github.com/jerolba/parquet-carpet/commit/eecb813cdbddfde9a4b27c46104c5a8969ef4477" target="_blank" rel="noopener noreferrer">make Carpet use version 2 by default</a>.</p>

<p>A week later, I discovered at work the hard way that if you are not up to date with Pandas in Python, you cannot read files written with version 2. I had to <a href="https://github.com/jerolba/parquet-carpet/commit/983f87ec23b1d3c5c091daa4dc7dfb95c919f6ef" target="_blank" rel="noopener noreferrer">rollback the change</a> immediately.</p>

<h2 id="parquet-version-2">Parquet Version 2</h2>

<p>Upon researching the topic, you’ll find that even though the format specification is finalized, it is not fully implemented across the ecosystem. Ideally, the standard would be whatever the specification defines, but in reality, there is no agreement on the minimum set of features an implementation must support to be considered compatible with version 2.</p>

<p>In <a href="https://github.com/apache/parquet-format/pull/164" target="_blank" rel="noopener noreferrer">this Pull Request</a> from the project that describes the file format, <strong>there has been an ongoing discussion for four years about what constitutes the core</strong>, and there are no signs of a resolution anytime soon. Reading <a href="https://lists.apache.org/thread/th3ls02pd1yn74mtj12s05tbbx0x8bjj" target="_blank" rel="noopener noreferrer">this other thread</a> on the mailing list, I came to the conclusion that although they are part of the specification, two concepts are mixed that could evolve independently:</p>

<ul>
  <li>Given a series of values in a column, how to encode them efficiently. Being able to incorporate new encodings such as <code>RLE_DICTIONARY</code> or <code>DELTA_BYTE_ARRAY</code>, which further improve compression.</li>
  <li>Given an encoded column’s data, where to write it within the file along with its metadata such as headers, nulls, or statistics, which helps to maximize the available metadata while minimizing its size and the number of file reads. This is what they call Data Page V2.</li>
</ul>

<p>Many would likely prefer to prioritize improvements in encoding over page structure. Finding a file that uses an unknown encoding would make a column unreadable, but a change in how pages are structured would make the entire file unreadable.</p>

<p>What I came to understand is that new <a href="https://github.com/apache/parquet-format/blob/master/LogicalTypes.md" target="_blank" rel="noopener noreferrer">logical types</a> are not tied to a specific format version. On the one hand, there are the primitive types that are fixed, but on top of them, logical types are defined: a date is a representation of an <code>int64</code>, a Big Decimal or String is represented with a <code>BYTE_ARRAY</code>. Now the <code>VARIANT</code> type is being <a href="https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#variant" target="_blank" rel="noopener noreferrer">defined</a> and I have not seen it associated with either of the two versions.</p>

<p>Meanwhile, in the Machine Learning world, Parquet and ORC have become limiting, requiring specialized features such as handling files with thousands of columns. To address this, two new formats have emerged: <a href="https://github.com/facebookincubator/nimble" target="_blank" rel="noopener noreferrer">Nimble</a> from Facebook and <a href="https://blog.lancedb.com/lance-v2/" target="_blank" rel="noopener noreferrer">LV2</a> from LanceDB.</p>

<p>If you want to delve deeper into the topic, I recommend <a href="https://materializedview.io/p/nimble-and-lance-parquet-killers" target="_blank" rel="noopener noreferrer">this introductory article</a>. I consider these two formats to be niche solutions and <strong>Parquet will continue to reign in the world of data engineering</strong>.</p>

<h2 id="performance-of-version-2">Performance of Version 2</h2>

<p>The DuckDB article prompted me to investigate the performance implications of Parquet Version 2, which I hadn’t considered in [my previous post on compression algorithms]](/compression-algorithms-parquet/).</p>

<p>Configuring file writing with version 2 is straightforward, requiring only a property setting in the writer’s builder:</p>

<div><div><pre><code><span>CarpetWriter</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>writer</span> <span>=</span> <span>new</span> <span>CarpetWriter</span><span>.</span><span>Builder</span><span>&lt;&gt;(</span><span>outputFile</span><span>,</span> <span>clazz</span><span>)</span>
    <span>.</span><span>withWriterVersion</span><span>(</span><span>WriterVersion</span><span>.</span><span>PARQUET_2_0</span><span>)</span>
    <span>.</span><span>build</span><span>();</span>
</code></pre></div></div>

<h3 id="file-size">File Size</h3>

<p>Italian government dataset:</p>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Version 1</th>
      <th>Version 2</th>
      <th>Improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CSV</td>
      <td>1761 MB</td>
      <td>1761 MB</td>
      <td>-</td>
    </tr>
    <tr>
      <td>UNCOMPRESSED</td>
      <td>564 MB</td>
      <td>355 MB</td>
      <td>37 %</td>
    </tr>
    <tr>
      <td>SNAPPY</td>
      <td>220 MB</td>
      <td>198 MB</td>
      <td>10 %</td>
    </tr>
    <tr>
      <td>GZIP</td>
      <td>146 MB</td>
      <td>138 MB</td>
      <td>5 %</td>
    </tr>
    <tr>
      <td>ZSTD</td>
      <td>148  MB</td>
      <td>144 MB</td>
      <td>2 %</td>
    </tr>
    <tr>
      <td>LZ4_RAW</td>
      <td>209 MB</td>
      <td>192 MB</td>
      <td>8 %</td>
    </tr>
    <tr>
      <td>LZO</td>
      <td>215 MB</td>
      <td>195 MB</td>
      <td>9 %</td>
    </tr>
  </tbody>
</table>

<p>New York taxi dataset:</p>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Version 1</th>
      <th>Version 2</th>
      <th>Improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CSV</td>
      <td>2983 MB</td>
      <td>2983 MB</td>
      <td>-</td>
    </tr>
    <tr>
      <td>UNCOMPRESSED</td>
      <td>760 MB</td>
      <td>511 MB</td>
      <td>33 %</td>
    </tr>
    <tr>
      <td>SNAPPY</td>
      <td>542 MB</td>
      <td>480 MB</td>
      <td>11 %</td>
    </tr>
    <tr>
      <td>GZIP</td>
      <td>448 MB</td>
      <td>444 MB</td>
      <td>1 %</td>
    </tr>
    <tr>
      <td>ZSTD</td>
      <td>430 MB</td>
      <td>444 MB</td>
      <td>-3 %</td>
    </tr>
    <tr>
      <td>LZ4_RAW</td>
      <td>547 MB</td>
      <td>482 MB</td>
      <td>12 %</td>
    </tr>
    <tr>
      <td>LZO</td>
      <td>518 MB</td>
      <td>479 MB</td>
      <td>7 %</td>
    </tr>
  </tbody>
</table>

<p>The new encodings in Version 2 compact the data more effectively before compression, resulting in a larger relative improvement for the UNCOMPRESSED version. This leaves less room for subsequent compression algorithms to further reduce the file size (or even slightly worsening it like ZSTD).</p>

<h3 id="writing">Writing</h3>

<p>Italian government dataset (seconds):</p>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Version 1</th>
      <th>Version 2</th>
      <th>Improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>UNCOMPRESSED</td>
      <td>25.0</td>
      <td>23.6</td>
      <td>6 %</td>
    </tr>
    <tr>
      <td>SNAPPY</td>
      <td>25.2</td>
      <td>23.5</td>
      <td>7 %</td>
    </tr>
    <tr>
      <td>GZIP</td>
      <td>39.3</td>
      <td>35.8</td>
      <td>9 %</td>
    </tr>
    <tr>
      <td>ZSTD</td>
      <td>27.3</td>
      <td>25.7</td>
      <td>6 %</td>
    </tr>
    <tr>
      <td>LZ4_RAW</td>
      <td>24.9</td>
      <td>23.8</td>
      <td>4 %</td>
    </tr>
    <tr>
      <td>LZO</td>
      <td>26.0</td>
      <td>24.6</td>
      <td>5 %</td>
    </tr>
  </tbody>
</table>

<p>New York taxi dataset (seconds):</p>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Version 1</th>
      <th>Version 2</th>
      <th>Improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>UNCOMPRESSED</td>
      <td>57.9</td>
      <td>50.2</td>
      <td>13 %</td>
    </tr>
    <tr>
      <td>SNAPPY</td>
      <td>56.4</td>
      <td>50.7</td>
      <td>10 %</td>
    </tr>
    <tr>
      <td>GZIP</td>
      <td>91.1</td>
      <td>66.9</td>
      <td>27 %</td>
    </tr>
    <tr>
      <td>ZSTD</td>
      <td>64.1</td>
      <td>57.1</td>
      <td>11 %</td>
    </tr>
    <tr>
      <td>LZ4_RAW</td>
      <td>56.5</td>
      <td>50.5</td>
      <td>11 %</td>
    </tr>
    <tr>
      <td>LZO</td>
      <td>56.1</td>
      <td>51.1</td>
      <td>9 %</td>
    </tr>
  </tbody>
</table>

<p>The improvement in writing times is remarkable, but especially in the New York taxi dataset, with a majority of numeric values. Especially noteworthy is the improvement in GZIP format times.</p>

<h3 id="reading">Reading</h3>

<p>Italian government dataset (seconds):</p>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Version 1</th>
      <th>Version 2</th>
      <th>Improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>UNCOMPRESSED</td>
      <td>11.4</td>
      <td>11.3</td>
      <td>1 %</td>
    </tr>
    <tr>
      <td>SNAPPY</td>
      <td>12.5</td>
      <td>11.5</td>
      <td>8 %</td>
    </tr>
    <tr>
      <td>GZIP</td>
      <td>13.6</td>
      <td>12.8</td>
      <td>6 %</td>
    </tr>
    <tr>
      <td>ZSTD</td>
      <td>13.1</td>
      <td>12.2</td>
      <td>7 %</td>
    </tr>
    <tr>
      <td>LZ4_RAW</td>
      <td>12.8</td>
      <td>11.3</td>
      <td>12 %</td>
    </tr>
    <tr>
      <td>LZO</td>
      <td>13.1</td>
      <td>12.1</td>
      <td>7 %</td>
    </tr>
  </tbody>
</table>

<p>New York taxi dataset (seconds):</p>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Version 1</th>
      <th>Version 2</th>
      <th>Improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>UNCOMPRESSED</td>
      <td>37.4</td>
      <td>33.0</td>
      <td>12 %</td>
    </tr>
    <tr>
      <td>SNAPPY</td>
      <td>39.9</td>
      <td>34.0</td>
      <td>15 %</td>
    </tr>
    <tr>
      <td>GZIP</td>
      <td>40.9</td>
      <td>34.4</td>
      <td>16  %</td>
    </tr>
    <tr>
      <td>ZSTD</td>
      <td>41.5</td>
      <td>34.1</td>
      <td>18 %</td>
    </tr>
    <tr>
      <td>LZ4_RAW</td>
      <td>41.5</td>
      <td>33.6</td>
      <td>19 %</td>
    </tr>
    <tr>
      <td>LZO</td>
      <td>41.1</td>
      <td>33.7</td>
      <td>18 %</td>
    </tr>
  </tbody>
</table>

<p>In reading, we again see a notable improvement, but even better in the taxi dataset with many decimal types.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Although this post might seem like a critique of Parquet, that is not my intention.  I am simply documenting what I have learned and explaining the challenges maintainers of an open format face when evolving it. <strong>All the benefits and utilities that a format like Parquet has far outweigh these inconveniences</strong>.</p>

<p>The improvements that the latest version of Parquet brings help reduce file sizes and processing times, but the difference is not dramatic. Given the low adoption of Version 2 in the ecosystem, for now these improvements do not help to justify potential compatibility problems when you integrate with third parties. However, if you control all parts of the process, consider adopting the latest specification.</p>

<!--Most of what I have written is my interpretation, and I could be wrong. If you have better sources or a different opinion, feel free to share it in the comments.-->

        </div>

        

        
          

        

        

      </div>
    </div>
  </article>
</div></div>
  </body>
</html>
