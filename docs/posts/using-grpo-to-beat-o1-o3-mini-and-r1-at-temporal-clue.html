<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://openpipe.ai/blog/using-grpo-to-beat-o1-o3-mini-and-r1-on-temporal-clue">Original</a>
    <h1>Using GRPO to Beat o1, o3-mini and R1 at “Temporal Clue”</h1>
    
    <div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p><img alt="" data-framer-asset="data:framer/asset-reference,XbGE2sLW99jyy3eDz6naY1UFHE.png" data-framer-height="971" data-framer-width="1626" height="485" src="https://framerusercontent.com/images/XbGE2sLW99jyy3eDz6naY1UFHE.png" srcset="https://framerusercontent.com/images/XbGE2sLW99jyy3eDz6naY1UFHE.png?scale-down-to=512 512w,https://framerusercontent.com/images/XbGE2sLW99jyy3eDz6naY1UFHE.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/XbGE2sLW99jyy3eDz6naY1UFHE.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"/></p><h3>Background</h3><p>Since OpenAI launched its powerful <!--$--><a href="https://openai.com/index/learning-to-reason-with-llms/" rel="noopener">new o-series of reasoning models last year</a><!--/$-->, we&#39;ve seen rapid progress in Large Language Models (LLMs) trained with Reinforcement Learning (RL). Leading organizations like <!--$--><a href="https://deepmind.google/technologies/gemini/flash-thinking/" rel="noopener">Google DeepMind</a><!--/$-->, <!--$--><a href="https://qwenlm.github.io/blog/qwq-32b-preview/" rel="noopener">Alibaba</a><!--/$-->, <!--$--><a href="https://arxiv.org/abs/2501.12948" rel="noopener">DeepSeek</a><!--/$-->, and <!--$--><a href="https://www.anthropic.com/research/visible-extended-thinking" rel="noopener">Anthropic</a><!--/$--> quickly followed suit, and have trained their own advanced models to reason with long “chains-of-thought” (CoT), taught with reinforcement learning on verifiable problems. Many previously challenging benchmarks—in areas like mathematics and coding—now approach saturation.</p><p>Yet despite these impressive strides, logical deduction remains stubbornly difficult for even today&#39;s best models. Typically, LLMs struggle to consistently attend to all relevant details, maintain logically sound reasoning chains, or reliably link multiple deduction steps. Even state-of-the-art models generating outputs 10–100 times longer frequently introduce elementary mistakes that a human solver would easily catch.</p><p>Intrigued by this unsolved mystery, we donned our deerstalker caps and set out to investigate: Could smaller, open-weight models reach frontier-level deduction performance with the latest reinforcement learning techniques? We began with substantially weaker models and iteratively trained them on a novel deduction task. Over time, we observed clear improvements in their detective prowess, eventually matching or even exceeding some of the strongest proprietary models.</p><p>Now we&#39;re happy to share our findings, including <!--$--><a href="https://github.com/openpipe/rl-experiments" rel="noopener">our experiments</a><!--/$-->, <!--$--><a href="https://github.com/openpipe/deductive-reasoning" rel="noopener">training recipe</a><!--/$-->, <!--$--><a href="https://github.com/bradhilton/temporal-clue" rel="noopener">dataset</a><!--/$-->, and <!--$--><a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-32B" rel="noopener">model weights</a><!--/$-->, all freely available under the MIT license, along with key practical insights (right here). Grab your magnifying glass, detective; the game is afoot!</p><h3>Benchmarking</h3><p>To begin our experiments, we first had to identify a challenging reasoning task with clearly verifiable solutions and scalable complexity. As it happened (not coincidentally), one of the authors previously created a puzzle set named <!--$--><a href="https://github.com/bradhilton/temporal-clue" rel="noopener">Temporal Clue</a><!--/$--> that matched these needs perfectly. Beyond meeting the criteria of ground truth clarity, new puzzles can be created as needed—a neat bonus.</p><p>Temporal Clue is inspired by the popular board game, <!--$--><a href="https://en.wikipedia.org/wiki/Cluedo" rel="noopener">Clue (Cluedo)</a><!--/$-->, where players race to uncover who killed Mr. Boddy in his palatial estate. Temporal Clue turns the game into a solitary logic puzzle that extends beyond the standard dimensions—<strong>who</strong>, (with) <strong>what</strong>, and <strong>where</strong>—and incorporates two additional dimensions: <strong>when</strong> (time) and <strong>why </strong>(motive). Puzzles are randomly generated, and minimal yet sufficient clues are selected with  <!--$--><a href="https://developers.google.com/optimization" rel="noopener">OR-Tools&#39;</a><!--/$--> <!--$--><a href="https://developers.google.com/optimization/cp/cp_solver" rel="noopener">CP-SAT solver</a><!--/$-->.</p><blockquote><p><!--$--><a href="https://gist.github.com/bradhilton/911e208183a389dc00e52de73f2c66bb" target="_blank" rel="noopener">On a dark winter night, wealthy and enigmatic Mr. John Q. Boddy hosted a small, but lavish, dinner party for some of his closest associates. However, the night ended in tragedy when Mr. Boddy was found dead in one of the rooms of Tudor Mansion in the early hours of the morning. The following persons of interest have been identified as suspects…</a><!--/$--></p></blockquote><p>To establish the current state-of-the-art for this deduction task, we benchmarked leading reasoning models—including DeepSeek R1, OpenAI’s o1 and o3-mini, and Anthropic’s Claude Sonnet 3.7. Additionally, we have benchmarked the 14B and 32B Qwen models, which we later improve using reinforcement learning, and include a preview of our final results:</p><figure><table><tbody><tr><td><p>Organization</p></td><td><p>Model</p></td><td><p>Reasoning Effort</p></td><td><p>Avg. Accuracy</p></td><td><p>Avg. Cost</p></td></tr><tr><td><p>DeepSeek</p></td><td><p>R1</p></td><td><p>Default</p></td><td><p>51.6%</p></td><td><p>$0.029</p></td></tr><tr><td><p>OpenAI</p></td><td><p>o1</p></td><td><p>Default</p></td><td><p>54.9%</p></td><td><p>$0.901</p></td></tr><tr><td><p>OpenAI</p></td><td><p>o3-mini</p></td><td><p>Medium</p></td><td><p>55.9%</p></td><td><p>$0.068</p></td></tr><tr><td><p>OpenAI</p></td><td><p>o3-mini</p></td><td><p>High</p></td><td><p>56.7%</p></td><td><p>$0.170</p></td></tr><tr><td><p>Anthropic</p></td><td><p>Sonnet 3.7</p></td><td><p>None</p></td><td><p>51.7%</p></td><td><p>$0.017</p></td></tr><tr><td><p>Anthropic</p></td><td><p>Sonnet 3.7</p></td><td><p>16k Token Budget</p></td><td><p>61.7%</p></td><td><p>$0.222</p></td></tr><tr><td><p>Anthropic</p></td><td><p>Sonnet 3.7</p></td><td><p>64k Token Budget</p></td><td><p><strong>69.5%</strong></p></td><td><p>$0.392</p></td></tr><tr><td><p>Alibaba</p></td><td><p>Qwen 2.5 14B Instruct</p></td><td><p>None</p></td><td><p>28.1% → 59.4%</p></td><td><p><strong>$0.001</strong></p></td></tr><tr><td><p>Alibaba</p></td><td><p>Qwen 2.5 32B Instruct</p></td><td><p>None</p></td><td><p>37.3% → 67.1%</p></td><td><p>$0.002</p></td></tr></tbody></table></figure><p>From these benchmarks, we saw that Claude Sonnet 3.7 with a 64k token thinking budget performed best on our task, but that all the leading models showed room for improvement. DeepSeek R1, a popular open-weight model, performed nearly as well as OpenAI&#39;s o1 and o3-mini. However, the untuned Qwen 2.5 Instruct models’ performance is unimpressive in comparison. The big question is: Can we train these smaller, open-weight models to frontier-level performance? Elementary, our dear reader—we just need the right approach.</p><h3>Training</h3><p>To train a frontier-level deduction model, we turned to reinforcement learning—an approach that allows agents to learn from their own experience inside a controlled environment. Here, the LLMs were our agents, and the puzzles were our environment. We guided the LLMs’ learning by having them generate multiple responses for each puzzle, exploring the problem landscape. We reinforced deductions leading to correct solutions and penalized reasoning that took the models astray.</p><p>Among various RL methods, we selected the popular <!--$--><a href="https://arxiv.org/abs/2402.03300" rel="noopener">Group Relative Policy Optimization (GRPO)</a><!--/$--> algorithm developed by DeepSeek. GRPO simplifies the training process compared to more traditional methods like <!--$--><a href="https://en.wikipedia.org/wiki/Proximal_policy_optimization" rel="noopener">Proximal Policy Optimization (PPO)</a><!--/$-->, while still providing robust performance. To speed up our experiments, we omitted the  <!--$--><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener">Kullback–Leibler (KL) divergence</a><!--/$--> penalty, although our training recipe supports it for interested readers.</p><p>At a high level, our training loop followed these basic steps:</p><ul><li data-preset-tag="p"><p>Generate model responses to puzzle tasks</p></li><li data-preset-tag="p"><p>Grade responses and estimate advantages for each group of chat completions (that’s the “Group Relative” part in GRPO)</p></li><li data-preset-tag="p"><p>Fine-tune the model using clipped policy gradients guided by these advantage estimates</p></li><li data-preset-tag="p"><p>Repeat these steps with new puzzles and the latest version of the model until we reach peak performance</p></li></ul><p>For generating responses, we used the popular <!--$--><a href="https://blog.vllm.ai/2023/06/20/vllm.html" rel="noopener">vLLM</a><!--/$--> inference engine. We tuned our parameter choices to maximize throughput and minimize startup time. Prefix caching was particularly important because we sampled many responses for each task, and caching prompts helps avoid redundant computation.</p><p>We observed that overwhelming vLLM with too many requests forces preemption or swapping out of in-progress requests. To address this, we limited requests using a semaphore tuned to maintain high key-value (KV) cache utilization while minimizing swaps. More advanced scheduling mechanisms could yield even higher utilization while still supporting flexible generation lengths.</p><p>After sampling, we processed completions using the standard <!--$--><a href="https://huggingface.co/docs/transformers" rel="noopener">HuggingFace Transformers</a><!--/$--> <!--$--><a href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer" rel="noopener">AutoTokenizer</a><!--/$-->. Its chat template feature, which renders message objects as a prompt string, includes an assistant mask for determining which tokens the LLM generated. We found the models lacked the necessary <code>% generation %</code> tags in their default templates, so we modified them during the tokenization step. The resulting assistant mask was included in the dictionary of tensors used for tuning, identifying which positions required loss calculations.</p><p>After tokenizing responses and obtaining assistant masks, we packed the data for tuning. In addition to including multiple prompt/response pairs in each packed sequence, we identified shared prompt tokens and assigned each token a Parent ID alongside the standard Group ID. Particularly for tasks like Temporal Clue—averaging over 1,000 tokens per puzzle—generating numerous responses per task and efficiently packing tensors significantly reduced redundancy. Once packed with all necessary information, we could visualize our training dataset two-dimensionally, each row being a sequence of tokens potentially containing multiple prompts and completions:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,yTOg965WW2AFP43ZerKodVHRROE.png" data-framer-height="1990" data-framer-width="1469" height="995" src="https://framerusercontent.com/images/yTOg965WW2AFP43ZerKodVHRROE.png" srcset="https://framerusercontent.com/images/yTOg965WW2AFP43ZerKodVHRROE.png?scale-down-to=1024 755w,https://framerusercontent.com/images/yTOg965WW2AFP43ZerKodVHRROE.png 1469w" width="734" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"/></p><p>With tightly-packed data in hand, we could proceed to tuning. Our models were already pre-trained, instruction-tuned, fairly intelligent, and adept at following instructions. However, they could not yet reliably solve Temporal Clue puzzles. Still, they occasionally succeeded, and that was enough. By increasing the probability of good reasoning and decreasing the probability of “not good” reasoning, we incrementally steered the models toward Master Detective status. We achieved this using standard machine learning techniques, employing policy gradient methods to compute loss and shift the weights beneficially.</p><p>For training, we used the <!--$--><a href="https://pytorch.org/torchtune" rel="noopener">torchtune</a><!--/$--> library provided by the PyTorch team. Torchtune features efficient decoder-only transformer implementations for popular models including Llama, Gemma, Phi, and more. Although we primarily used the Qwen models for this project, we also ran experiments with 8B and 70B Llama models. Torchtune also provides memory-saving and performance-enhancing utilities, including:</p><ul><li data-preset-tag="p"><p>Activation Checkpointing</p></li><li data-preset-tag="p"><p>Activation Offloading</p></li><li data-preset-tag="p"><p>Quantization</p></li><li data-preset-tag="p"><p><!--$--><a href="https://arxiv.org/abs/2312.12148" rel="noopener">Parameter-Efficient Fine-Tuning (PEFT)</a><!--/$-->, e.g., <!--$--><a href="https://arxiv.org/abs/2106.09685" rel="noopener">Low Rank Adaptation (LoRA)</a><!--/$--></p></li></ul><p>See the <!--$--><a href="https://github.com/pytorch/torchtune?tab=readme-ov-file#optimization-flags" rel="noopener">README here</a><!--/$--> for the full list of supported optimizations.</p><p>Additionally, Torchtune supports multi-device (<!--$--><a href="https://pytorch.org/torchtune/main/tutorials/multinode.html" rel="noopener">and now multi-node</a><!--/$-->) training, making it ideal for larger models. It supports both Fully Sharded Data Parallel (FSDP) and Tensor Parallel (TP) training, which can be combined. They also provide <!--$--><a href="https://github.com/pytorch/torchtune/tree/main/recipes" rel="noopener">over a dozen recipes</a><!--/$-->, encouraging users to copy and customize them for their use cases. We created a modified version of their full fine-tune recipes supporting:</p><ul><li data-preset-tag="p"><p>Both multi-device and single-device training</p></li><li data-preset-tag="p"><p>Reference model loading and weight swapping for calculating KL divergences</p></li><li data-preset-tag="p"><p>Advanced causal mask calculations using group and parent IDs</p></li><li data-preset-tag="p"><p>GRPO loss integration and component logging</p></li></ul><p>The recipe can be seen <!--$--><a href="https://github.com/openpipe/deductive-reasoning/blob/main/lib/recipe.py" rel="noopener">here</a><!--/$-->. In the future, we would like to add tensor parallelism support and explore PEFT and quantization.</p><p>The RL training process involves selecting a myriad of hyperparameters. While training our models, we tested various configurations and largely settled upon the following:</p><ul><li data-preset-tag="p"><p>Models: Qwen 2.5 Instruct 14B &amp; 32B</p></li><li data-preset-tag="p"><p>Tasks per Iteration: 32</p></li><li data-preset-tag="p"><p>Samples per Task per Iteration: 50</p></li><li data-preset-tag="p"><p>Total Samples per Iteration: 32 * 50 = 1600</p></li><li data-preset-tag="p"><p>Learning Rate: 6e-6</p></li><li data-preset-tag="p"><p>Micro-Batch Size: 4 sequences for 14B model, 8 for 32B model</p></li><li data-preset-tag="p"><p>Batch Size: Variable, depending on the number of sequences</p></li></ul><p>The batch size is variable because response lengths can vary during training, sequence packing efficiency fluctuates each iteration, and responses with zero advantage are discarded. For one run, we tried dynamically adjusting learning rates inversely proportional to batch size, but this resulted in excessively high learning rates for small batch sizes, requiring a cap. The capped version didn’t meaningfully differ from using a constant learning rate, but tuning batch size and learning rate remains an interesting area for future experimentation.</p><p>We also ran brief experiments increasing tasks per iteration while reducing samples per task—and vice versa—keeping total samples per iteration roughly equal. Over a short training horizon, these variations showed no meaningful differences, suggesting the recipe is robust to different balances between the number of tasks and samples per task.</p><h3>Results</h3><p>After training our models for over 100 iterations, we reached frontier-level deduction.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,TnI1xBpe3Xn46Ql65su2Ad7p46I.png" data-framer-height="1078" data-framer-width="1626" height="539" src="https://framerusercontent.com/images/TnI1xBpe3Xn46Ql65su2Ad7p46I.png" srcset="https://framerusercontent.com/images/TnI1xBpe3Xn46Ql65su2Ad7p46I.png?scale-down-to=512 512w,https://framerusercontent.com/images/TnI1xBpe3Xn46Ql65su2Ad7p46I.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/TnI1xBpe3Xn46Ql65su2Ad7p46I.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"/></p><p>Our models quickly improved before accuracy gains started to taper off and eventually degrade, sometimes aggressively. At their best, the 14B model approached Claude Sonnet 3.7’s performance at 16k tokens and the 32B model nearly matched Sonnet&#39;s results with the larger 64k budget.</p><p>While training, performance gains followed a power law, forming a linear relationship on a log-log chart (before deteriorating).</p><p><img alt="" data-framer-asset="data:framer/asset-reference,En7QbGZgBPfIwsBtipfujcDH6gY.png" data-framer-height="1078" data-framer-width="1626" height="539" src="https://framerusercontent.com/images/En7QbGZgBPfIwsBtipfujcDH6gY.png" srcset="https://framerusercontent.com/images/En7QbGZgBPfIwsBtipfujcDH6gY.png?scale-down-to=512 512w,https://framerusercontent.com/images/En7QbGZgBPfIwsBtipfujcDH6gY.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/En7QbGZgBPfIwsBtipfujcDH6gY.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"/></p><p>We suspect the models may have converged too early on greedy strategies that worked out of the gate, but potentially limited their long-term prospects. A logical next step would explore approaches that encourage diverse responses, or that build capabilities incrementally (like with curriculum learning), or that assign larger rewards to particularly outstanding solutions incentivizing thorough exploration.</p><p>Additionally, we noted interesting patterns in output length during training. Initially, responses grew longer, then stabilized, before diverging near the end of training, with the 14B model’s responses getting longer and the 32B model’s response lengths collapsing, especially after reaching peak performance.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,jwjoe4Q3D2GPN5qKiA53kLu1fQ.png" data-framer-height="590" data-framer-width="1590" height="295" src="https://framerusercontent.com/images/jwjoe4Q3D2GPN5qKiA53kLu1fQ.png" srcset="https://framerusercontent.com/images/jwjoe4Q3D2GPN5qKiA53kLu1fQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/jwjoe4Q3D2GPN5qKiA53kLu1fQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/jwjoe4Q3D2GPN5qKiA53kLu1fQ.png 1590w" width="795" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"/></p><p>To qualitatively assess improvements in logical reasoning, we asked the strongest frontier model, Claude Sonnet 3.7, to identify and evaluate the soundness of deductions made by the Qwen 32B model—before and after training for 100+ iterations—on similar puzzles. <!--$--><a href="https://gist.github.com/bradhilton/c012ce9ed7b8e7c993b14b9761574a3c#file-base-md" rel="noopener">Sonnet identified 6 deductions from the base model, with all but one judged erroneous;</a><!--/$--> <!--$--><a href="https://gist.github.com/bradhilton/c012ce9ed7b8e7c993b14b9761574a3c#file-trained-md" rel="noopener">conversely, it identified 7 deductions from the trained model, with all but one judged logically sound.</a><!--/$--></p><p><img alt="" data-framer-asset="data:framer/asset-reference,r9dGPn71HUynBKCOZO5Sr03jInQ.png" data-framer-height="433" data-framer-width="1626" height="216" src="https://framerusercontent.com/images/r9dGPn71HUynBKCOZO5Sr03jInQ.png" srcset="https://framerusercontent.com/images/r9dGPn71HUynBKCOZO5Sr03jInQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/r9dGPn71HUynBKCOZO5Sr03jInQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/r9dGPn71HUynBKCOZO5Sr03jInQ.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"/></p><p>Finally, assuming <!--$--><a href="https://fireworks.ai/blog/why-gpus-on-demand" rel="noopener">sufficient throughput</a><!--/$--> with <!--$--><a href="https://fireworks.ai/pricing#ondemand" rel="noopener">on-demand deployments</a><!--/$-->, we estimated Qwen model costs from <!--$--><a href="https://fireworks.ai/" rel="noopener">Fireworks AI’s</a><!--/$--> <!--$--><a href="https://fireworks.ai/pricing#text" rel="noopener">serverless pricing tiers.</a><!--/$--> We plotted accuracy against the natural logarithm of the average inference cost per response, and observed a clear linear Pareto frontier among untuned models. By successfully training open-weight models to frontier-level accuracy, we dramatically improved the cost–accuracy trade-off.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,9Zwetqv69wY3YceuOG3KMeYyzKg.png" data-framer-height="1078" data-framer-width="1626" height="539" src="https://framerusercontent.com/images/9Zwetqv69wY3YceuOG3KMeYyzKg.png" srcset="https://framerusercontent.com/images/9Zwetqv69wY3YceuOG3KMeYyzKg.png?scale-down-to=512 512w,https://framerusercontent.com/images/9Zwetqv69wY3YceuOG3KMeYyzKg.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/9Zwetqv69wY3YceuOG3KMeYyzKg.png 1626w" width="813" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (max-width: 1199px) 100vw"/></p><p>Here, after sharing satisfied nods for a job well done, we hail a hansom cab and return to Baker Street—the perfect place to contemplate our findings.</p><h3>Conclusion</h3><p>In our investigation, we set out to explore whether smaller, open-weight language models could achieve frontier-level deductive reasoning through reinforcement learning. After training Qwen 14B and 32B models on challenging Temporal Clue puzzles—using carefully selected hyperparameters and the GRPO method—we achieved impressive performance gains. These improvements brought open-weight models to the cutting edge of reasoning performance, at significantly reduced costs. Our findings highlight the promising potential for reinforcement learning to efficiently train open models on complex deduction tasks.</p><p>As mentioned previously, the <!--$--><a href="https://github.com/bradhilton/temporal-clue" rel="noopener">dataset</a><!--/$-->, <!--$--><a href="https://github.com/openpipe/rl-experiments" rel="noopener">experiments</a><!--/$-->, <!--$--><a href="https://github.com/openpipe/deductive-reasoning" rel="noopener">training recipe</a><!--/$-->, and model weights (<!--$--><a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-14B" rel="noopener">14B</a><!--/$-->, <!--$--><a href="https://huggingface.co/OpenPipe/Deductive-Reasoning-Qwen-32B" rel="noopener">32B</a><!--/$-->) are freely available under the MIT license. We encourage you to try reproducing and improving on our results.</p><p>Additionally, we’ve held out one particularly exciting finding for the end. We discovered that meaningful performance improvements, as high as 10–15%, can be achieved with as <strong>few as 16 training examples</strong>. This means you don’t need a lot of data to get started; just some intuition about the problem you’d like to solve.</p><p>Are you interested in using reinforcement learning to train your own models, or would like some help getting started? Feel free to <!--$--><a href="https://openpipe.ai/contact" rel="noopener">reach out to us at OpenPipe</a><!--/$-->—we&#39;d love to chat!</p><p>Now, dear reader, please keep your deerstalker cap and magnifying glass handy; there&#39;s much more to explore. The game remains very much afoot.</p></div></div>
  </body>
</html>
