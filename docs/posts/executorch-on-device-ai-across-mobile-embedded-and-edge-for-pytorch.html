<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/pytorch/executorch">Original</a>
    <h1>Executorch: On-device AI across mobile, embedded and edge for PyTorch</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/pytorch/executorch/blob/main/docs/source/_static/img/et-logo.png"><img src="https://github.com/pytorch/executorch/raw/main/docs/source/_static/img/et-logo.png" alt="ExecuTorch logo mark" width="200"/></a></p>
  <p dir="auto"><strong>On-device AI inference powered by PyTorch</strong></p>
</div>
<p><a href="https://pypi.org/project/executorch/" rel="nofollow"><img src="https://camo.githubusercontent.com/0a655d9cfc2a1d6c3f54fba80bc87ebea4321ade5755afd0e8dada56fe35f159/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565" alt="PyPI - Version" data-canonical-src="https://img.shields.io/pypi/v/executorch?style=for-the-badge&amp;color=blue"/></a>
  <a href="https://github.com/pytorch/executorch/graphs/contributors"><img src="https://camo.githubusercontent.com/629c378cadd0fd62df5cf89107f8bc8b08b3f8d4fc5b939bde95d7acb9049cdb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f7079746f7263682f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565" alt="GitHub - Contributors" data-canonical-src="https://img.shields.io/github/contributors/pytorch/executorch?style=for-the-badge&amp;color=blue"/></a>
  <a href="https://github.com/pytorch/executorch/stargazers"><img src="https://camo.githubusercontent.com/461ec8564ce148802f8e6dc5bde3bc70997ad25ecc838c4c7b9813efd9533db4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7079746f7263682f6578656375746f7263683f7374796c653d666f722d7468652d626164676526636f6c6f723d626c7565" alt="GitHub - Stars" data-canonical-src="https://img.shields.io/github/stars/pytorch/executorch?style=for-the-badge&amp;color=blue"/></a>
  <a href="https://discord.gg/Dh43CKSAdc" rel="nofollow"><img src="https://camo.githubusercontent.com/5665a739b7459f532d6d1bdb198268464b4e52bbfa6f28b2f36bcd159467db62/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d4a6f696e25323055732d626c75653f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765" alt="Discord - Chat with Us" data-canonical-src="https://img.shields.io/badge/Discord-Join%20Us-blue?logo=discord&amp;logoColor=white&amp;style=for-the-badge"/></a>
  <a href="https://docs.pytorch.org/executorch/main/index.html" rel="nofollow"><img src="https://camo.githubusercontent.com/4ea00c7ce642fa2d5117120b3237ef4e7f310fdb8f96c6a7ed607d215348dcf9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63756d656e746174696f6e2d626c75653f6c6f676f3d676f6f676c65646f6373266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765" alt="Documentation" data-canonical-src="https://img.shields.io/badge/Documentation-blue?logo=googledocs&amp;logoColor=white&amp;style=for-the-badge"/></a>
</p>
<p dir="auto"><strong>ExecuTorch</strong> is PyTorch&#39;s unified solution for deploying AI models on-device‚Äîfrom smartphones to microcontrollers‚Äîbuilt for privacy, performance, and portability. It powers Meta&#39;s on-device AI across <strong>Instagram, WhatsApp, Quest 3, Ray-Ban Meta Smart Glasses</strong>, and <a href="https://docs.pytorch.org/executorch/main/success-stories.html" rel="nofollow">more</a>.</p>
<p dir="auto">Deploy <strong>LLMs, vision, speech, and multimodal models</strong> with the same PyTorch APIs you already know‚Äîaccelerating research to production with seamless model export, optimization, and deployment. No manual C++ rewrites. No format conversions. No vendor lock-in.</p>
<details>
  <summary><strong>üìò Table of Contents</strong></summary>
<ul dir="auto">
<li><a href="#why-executorch">Why ExecuTorch?</a></li>
<li><a href="#how-it-works">How It Works</a></li>
<li><a href="#quick-start">Quick Start</a>
<ul dir="auto">
<li><a href="#installation">Installation</a></li>
<li><a href="#export-and-deploy-in-3-steps">Export and Deploy in 3 Steps</a></li>
<li><a href="#run-on-device">Run on Device</a></li>
<li><a href="#llm-example-llama">LLM Example: Llama</a></li>
</ul>
</li>
<li><a href="#platform--hardware-support">Platform &amp; Hardware Support</a></li>
<li><a href="#production-deployments">Production Deployments</a></li>
<li><a href="#examples--models">Examples &amp; Models</a></li>
<li><a href="#key-features">Key Features</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#community--contributing">Community &amp; Contributing</a></li>
<li><a href="#license">License</a></li>
</ul>
</details>

<ul dir="auto">
<li><strong>üîí Native PyTorch Export</strong> ‚Äî Direct export from PyTorch. No .onnx, .tflite, or intermediate format conversions. Preserve model semantics.</li>
<li><strong>‚ö° Production-Proven</strong> ‚Äî Powers billions of users at <a href="https://engineering.fb.com/2025/07/28/android/executorch-on-device-ml-meta-family-of-apps/" rel="nofollow">Meta with real-time on-device inference</a>.</li>
<li><strong>üíæ Tiny Runtime</strong> ‚Äî 50KB base footprint. Runs on microcontrollers to high-end smartphones.</li>
<li><strong>üöÄ <a href="https://docs.pytorch.org/executorch/main/backends-overview.html" rel="nofollow">12+ Hardware Backends</a></strong> ‚Äî Open-source acceleration for Apple, Qualcomm, ARM, MediaTek, Vulkan, and more.</li>
<li><strong>üéØ One Export, Multiple Backends</strong> ‚Äî Switch hardware targets with a single line change. Deploy the same model everywhere.</li>
</ul>

<p dir="auto">ExecuTorch uses <strong>ahead-of-time (AOT) compilation</strong> to prepare PyTorch models for edge deployment:</p>
<ol dir="auto">
<li><strong>üß© Export</strong> ‚Äî Capture your PyTorch model graph with <code>torch.export()</code></li>
<li><strong>‚öôÔ∏è Compile</strong> ‚Äî Quantize, optimize, and partition to hardware backends ‚Üí <code>.pte</code></li>
<li><strong>üöÄ Execute</strong> ‚Äî Load <code>.pte</code> on-device via lightweight C++ runtime</li>
</ol>
<p dir="auto">Models use a standardized <a href="https://docs.pytorch.org/executorch/main/compiler-ir-advanced.html#intermediate-representation" rel="nofollow">Core ATen operator set</a>. <a href="https://docs.pytorch.org/executorch/main/compiler-delegate-and-partitioner.html" rel="nofollow">Partitioners</a> delegate subgraphs to specialized hardware (NPU/GPU) with CPU fallback.</p>
<p dir="auto">Learn more: <a href="https://docs.pytorch.org/executorch/main/intro-how-it-works.html" rel="nofollow">How ExecuTorch Works</a> ‚Ä¢ <a href="https://docs.pytorch.org/executorch/main/getting-started-architecture.html" rel="nofollow">Architecture Guide</a></p>



<p dir="auto">For platform-specific setup (Android, iOS, embedded systems), see the <a href="https://docs.pytorch.org/executorch/main/quick-start-section.html" rel="nofollow">Quick Start</a> documentation for additional info.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Export and Deploy in 3 Steps</h3><a id="user-content-export-and-deploy-in-3-steps" aria-label="Permalink: Export and Deploy in 3 Steps" href="#export-and-deploy-in-3-steps"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from executorch.exir import to_edge_transform_and_lower
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner

# 1. Export your PyTorch model
model = MyModel().eval()
example_inputs = (torch.randn(1, 3, 224, 224),)
exported_program = torch.export.export(model, example_inputs)

# 2. Optimize for target hardware (switch backends with one line)
program = to_edge_transform_and_lower(
    exported_program,
    partitioner=[XnnpackPartitioner()]  # CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm
).to_executorch()

# 3. Save for deployment
with open(&#34;model.pte&#34;, &#34;wb&#34;) as f:
    f.write(program.buffer)

# Test locally via ExecuTorch runtime&#39;s pybind API (optional)
from executorch.runtime import Runtime
runtime = Runtime.get()
method = runtime.load_program(&#34;model.pte&#34;).load_method(&#34;forward&#34;)
outputs = method.execute([torch.randn(1, 3, 224, 224)])"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>executorch</span>.<span>exir</span> <span>import</span> <span>to_edge_transform_and_lower</span>
<span>from</span> <span>executorch</span>.<span>backends</span>.<span>xnnpack</span>.<span>partition</span>.<span>xnnpack_partitioner</span> <span>import</span> <span>XnnpackPartitioner</span>

<span># 1. Export your PyTorch model</span>
<span>model</span> <span>=</span> <span>MyModel</span>().<span>eval</span>()
<span>example_inputs</span> <span>=</span> (<span>torch</span>.<span>randn</span>(<span>1</span>, <span>3</span>, <span>224</span>, <span>224</span>),)
<span>exported_program</span> <span>=</span> <span>torch</span>.<span>export</span>.<span>export</span>(<span>model</span>, <span>example_inputs</span>)

<span># 2. Optimize for target hardware (switch backends with one line)</span>
<span>program</span> <span>=</span> <span>to_edge_transform_and_lower</span>(
    <span>exported_program</span>,
    <span>partitioner</span><span>=</span>[<span>XnnpackPartitioner</span>()]  <span># CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm</span>
).<span>to_executorch</span>()

<span># 3. Save for deployment</span>
<span>with</span> <span>open</span>(<span>&#34;model.pte&#34;</span>, <span>&#34;wb&#34;</span>) <span>as</span> <span>f</span>:
    <span>f</span>.<span>write</span>(<span>program</span>.<span>buffer</span>)

<span># Test locally via ExecuTorch runtime&#39;s pybind API (optional)</span>
<span>from</span> <span>executorch</span>.<span>runtime</span> <span>import</span> <span>Runtime</span>
<span>runtime</span> <span>=</span> <span>Runtime</span>.<span>get</span>()
<span>method</span> <span>=</span> <span>runtime</span>.<span>load_program</span>(<span>&#34;model.pte&#34;</span>).<span>load_method</span>(<span>&#34;forward&#34;</span>)
<span>outputs</span> <span>=</span> <span>method</span>.<span>execute</span>([<span>torch</span>.<span>randn</span>(<span>1</span>, <span>3</span>, <span>224</span>, <span>224</span>)])</pre></div>

<p dir="auto"><strong><a href="https://docs.pytorch.org/executorch/main/using-executorch-cpp.html" rel="nofollow">C++</a></strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &lt;executorch/extension/module/module.h&gt;
#include &lt;executorch/extension/tensor/tensor.h&gt;

Module module(&#34;model.pte&#34;);
auto tensor = make_tensor_ptr({2, 2}, {1.0f, 2.0f, 3.0f, 4.0f});
auto outputs = module.forward(tensor);"><pre>#<span>include</span> <span><span>&lt;</span>executorch/extension/module/module.h<span>&gt;</span></span>
#<span>include</span> <span><span>&lt;</span>executorch/extension/tensor/tensor.h<span>&gt;</span></span>

Module <span>module</span>(<span><span>&#34;</span>model.pte<span>&#34;</span></span>);
<span>auto</span> tensor = make_tensor_ptr({<span>2</span>, <span>2</span>}, {<span>1</span>.<span>0f</span>, <span>2</span>.<span>0f</span>, <span>3</span>.<span>0f</span>, <span>4</span>.<span>0f</span>});
<span>auto</span> outputs = <span>module</span>.forward(tensor);</pre></div>
<p dir="auto"><strong><a href="https://docs.pytorch.org/executorch/main/ios-section.html" rel="nofollow">Swift (iOS)</a></strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="import ExecuTorch

let module = Module(filePath: &#34;model.pte&#34;)
let input = Tensor&lt;Float&gt;([1.0, 2.0, 3.0, 4.0], shape: [2, 2])
let outputs = try module.forward(input)"><pre><span>import</span> ExecuTorch

<span>let</span> <span>module</span> <span>=</span> <span>Module</span><span>(</span>filePath<span>:</span> <span>&#34;</span><span>model.pte</span><span>&#34;</span><span>)</span>
<span>let</span> <span>input</span> <span>=</span> <span>Tensor</span><span>&lt;</span><span>Float</span><span>&gt;</span><span>(</span><span>[</span><span>1.0</span><span>,</span> <span>2.0</span><span>,</span> <span>3.0</span><span>,</span> <span>4.0</span><span>]</span><span>,</span> shape<span>:</span> <span>[</span><span>2</span><span>,</span> <span>2</span><span>]</span><span>)</span>
<span>let</span> <span>outputs</span> <span>=</span> <span><span>try</span></span> module<span>.</span><span>forward</span><span>(</span>input<span>)</span></pre></div>
<p dir="auto"><strong><a href="https://docs.pytorch.org/executorch/main/android-section.html" rel="nofollow">Kotlin (Android)</a></strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="val module = Module.load(&#34;model.pte&#34;)
val inputTensor = Tensor.fromBlob(floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f), longArrayOf(2, 2))
val outputs = module.forward(EValue.from(inputTensor))"><pre><span>val</span> module <span>=</span> <span>Module</span>.load(<span><span>&#34;</span>model.pte<span>&#34;</span></span>)
<span>val</span> inputTensor <span>=</span> <span>Tensor</span>.fromBlob(floatArrayOf(<span>1.0f</span>, <span>2.0f</span>, <span>3.0f</span>, <span>4.0f</span>), longArrayOf(<span>2</span>, <span>2</span>))
<span>val</span> outputs <span>=</span> module.forward(<span>EValue</span>.from(inputTensor))</pre></div>

<p dir="auto">Export Llama models using the <a href="https://docs.pytorch.org/executorch/main/llm/export-llm.html" rel="nofollow"><code>export_llm</code></a> script or <a href="https://github.com/huggingface/optimum-executorch">Optimum-ExecuTorch</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Using export_llm
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte

# Using Optimum-ExecuTorch
optimum-cli export executorch \
  --model meta-llama/Llama-3.2-1B \
  --task text-generation \
  --recipe xnnpack \
  --output_dir llama_model"><pre><span><span>#</span> Using export_llm</span>
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte

<span><span>#</span> Using Optimum-ExecuTorch</span>
optimum-cli <span>export</span> executorch \
  --model meta-llama/Llama-3.2-1B \
  --task text-generation \
  --recipe xnnpack \
  --output_dir llama_model</pre></div>
<p dir="auto">Run on-device with the LLM runner API:</p>
<p dir="auto"><strong><a href="https://docs.pytorch.org/executorch/main/llm/run-with-c-plus-plus.html" rel="nofollow">C++</a></strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &lt;executorch/extension/llm/runner/text_llm_runner.h&gt;

auto runner = create_llama_runner(&#34;llama.pte&#34;, &#34;tiktoken.bin&#34;);
executorch::extension::llm::GenerationConfig config{
    .seq_len = 128, .temperature = 0.8f};
runner-&gt;generate(&#34;Hello, how are you?&#34;, config);"><pre>#<span>include</span> <span><span>&lt;</span>executorch/extension/llm/runner/text_llm_runner.h<span>&gt;</span></span>

<span>auto</span> runner = create_llama_runner(<span><span>&#34;</span>llama.pte<span>&#34;</span></span>, <span><span>&#34;</span>tiktoken.bin<span>&#34;</span></span>);
executorch::extension::llm::GenerationConfig config{
    .<span>seq_len</span> = <span>128</span>, .<span>temperature</span> = <span>0</span>.<span>8f</span>};
runner-&gt;<span>generate</span>(<span><span>&#34;</span>Hello, how are you?<span>&#34;</span></span>, config);</pre></div>
<p dir="auto"><strong><a href="https://docs.pytorch.org/executorch/main/llm/run-on-ios.html" rel="nofollow">Swift (iOS)</a></strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="import ExecuTorchLLM

let runner = TextRunner(modelPath: &#34;llama.pte&#34;, tokenizerPath: &#34;tiktoken.bin&#34;)
try runner.generate(&#34;Hello, how are you?&#34;, Config {
    $0.sequenceLength = 128
}) { token in
    print(token, terminator: &#34;&#34;)
}"><pre><span>import</span> ExecuTorchLLM

<span>let</span> <span>runner</span> <span>=</span> <span>TextRunner</span><span>(</span>modelPath<span>:</span> <span>&#34;</span><span>llama.pte</span><span>&#34;</span><span>,</span> tokenizerPath<span>:</span> <span>&#34;</span><span>tiktoken.bin</span><span>&#34;</span><span>)</span>
<span><span>try</span></span> runner<span>.</span><span>generate</span><span>(</span><span>&#34;</span><span>Hello, how are you?</span><span>&#34;</span><span>,</span> <span>Config</span> <span>{</span>
    $0<span>.</span>sequenceLength <span>=</span> <span>128</span>
<span>}</span><span>)</span> <span>{</span> token <span>in</span>
    <span>print</span><span>(</span>token<span>,</span> terminator<span>:</span> <span>&#34;</span><span>&#34;</span><span>)</span>
<span>}</span></pre></div>
<p dir="auto"><strong>Kotlin (Android)</strong> ‚Äî <a href="https://docs.pytorch.org/executorch/main/javadoc/org/pytorch/executorch/extension/llm/package-summary.html" rel="nofollow">API Docs</a> ‚Ä¢ <a href="https://github.com/meta-pytorch/executorch-examples/tree/main/llm/android/LlamaDemo">Demo App</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="val llmModule = LlmModule(&#34;llama.pte&#34;, &#34;tiktoken.bin&#34;, 0.8f)
llmModule.load()
llmModule.generate(&#34;Hello, how are you?&#34;, 128, object : LlmCallback {
    override fun onResult(result: String) { print(result) }
    override fun onStats(stats: String) { }
})"><pre><span>val</span> llmModule <span>=</span> <span>LlmModule</span>(<span><span>&#34;</span>llama.pte<span>&#34;</span></span>, <span><span>&#34;</span>tiktoken.bin<span>&#34;</span></span>, <span>0.8f</span>)
llmModule.load()
llmModule.generate(<span><span>&#34;</span>Hello, how are you?<span>&#34;</span></span>, <span>128</span>, <span>object</span> <span>:</span> <span>LlmCallback</span> {
    <span>override</span> <span>fun</span> <span>onResult</span>(<span>result</span><span>:</span> <span>String</span>) { <span>print</span>(result) }
    <span>override</span> <span>fun</span> <span>onStats</span>(<span>stats</span><span>:</span> <span>String</span>) { }
})</pre></div>
<p dir="auto">For multimodal models (vision, audio), use the <a href="https://github.com/pytorch/executorch/blob/main/extension/llm/runner">MultiModal runner API</a> which extends the LLM runner to handle image and audio inputs alongside text. See <a href="https://github.com/pytorch/executorch/blob/main/examples/models/llava/README.md">Llava</a> and <a href="https://github.com/pytorch/executorch/blob/main/examples/models/voxtral/README.md">Voxtral</a> examples.</p>
<p dir="auto">See <a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md">examples/models/llama</a> for complete workflow including quantization, mobile deployment, and advanced options.</p>
<p dir="auto"><strong>Next Steps:</strong></p>
<ul dir="auto">
<li>üìñ <a href="https://docs.pytorch.org/executorch/main/getting-started.html" rel="nofollow">Step-by-step tutorial</a> ‚Äî Complete walkthrough for your first model</li>
<li>‚ö° <a href="https://colab.research.google.com/drive/1qpxrXC3YdJQzly3mRg-4ayYiOjC6rue3?usp=sharing" rel="nofollow">Colab notebook</a> ‚Äî Try ExecuTorch instantly in your browser</li>
<li>ü§ñ <a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md">Deploy Llama models</a> ‚Äî LLM workflow with quantization and mobile demos</li>
</ul>
<div dir="auto"><h2 tabindex="-1" dir="auto">Platform &amp; Hardware Support</h2><a id="user-content-platform--hardware-support" aria-label="Permalink: Platform &amp; Hardware Support" href="#platform--hardware-support"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><strong>Platform</strong></th>
<th><strong>Supported Backends</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Android</td>
<td>XNNPACK, Vulkan, Qualcomm, MediaTek, Samsung Exynos</td>
</tr>
<tr>
<td>iOS</td>
<td>XNNPACK, MPS, CoreML (Neural Engine)</td>
</tr>
<tr>
<td>Linux / Windows</td>
<td>XNNPACK, OpenVINO, CUDA <em>(experimental)</em></td>
</tr>
<tr>
<td>macOS</td>
<td>XNNPACK, MPS, Metal <em>(experimental)</em></td>
</tr>
<tr>
<td>Embedded / MCU</td>
<td>XNNPACK, ARM Ethos-U, NXP, Cadence DSP</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">See <a href="https://docs.pytorch.org/executorch/main/backends-overview.html" rel="nofollow">Backend Documentation</a> for detailed hardware requirements and optimization guides. For desktop/laptop GPU inference with CUDA and Metal, see the <a href="https://github.com/pytorch/executorch/blob/main/desktop/README.md">Desktop Guide</a>. For Zephyr RTOS integration, see the <a href="https://github.com/pytorch/executorch/blob/main/zephyr/README.md">Zephyr Guide</a>.</p>

<p dir="auto">ExecuTorch powers on-device AI at scale across Meta&#39;s family of apps, VR/AR devices, and partner deployments. <a href="https://docs.pytorch.org/executorch/main/success-stories.html" rel="nofollow">View success stories ‚Üí</a></p>

<p dir="auto"><strong>LLMs:</strong> <a href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md">Llama 3.2/3.1/3</a>, <a href="https://github.com/pytorch/executorch/blob/main/examples/models/qwen3/README.md">Qwen 3</a>, <a href="https://github.com/pytorch/executorch/blob/main/examples/models/phi_4_mini/README.md">Phi-4-mini</a>, <a href="https://github.com/pytorch/executorch/blob/main/examples/models/lfm2/README.md">LiquidAI LFM2</a></p>
<p dir="auto"><strong>Multimodal:</strong> <a href="https://github.com/pytorch/executorch/blob/main/examples/models/llava/README.md">Llava</a> (vision-language), <a href="https://github.com/pytorch/executorch/blob/main/examples/models/voxtral/README.md">Voxtral</a> (audio-language), <a href="https://github.com/pytorch/executorch/blob/main/examples/models/gemma3">Gemma</a> (vision-language)</p>
<p dir="auto"><strong>Vision/Speech:</strong> <a href="https://github.com/meta-pytorch/executorch-examples/tree/main/mv2">MobileNetV2</a>, <a href="https://github.com/meta-pytorch/executorch-examples/tree/main/dl3">DeepLabV3</a>, <a href="https://github.com/pytorch/executorch/blob/main/examples/models/whisper/README.md">Whisper</a> </p>
<p dir="auto"><strong>Resources:</strong> <a href="https://github.com/pytorch/executorch/blob/main/examples"><code>examples/</code></a> directory ‚Ä¢ <a href="https://github.com/meta-pytorch/executorch-examples">executorch-examples</a> out-of-tree demos ‚Ä¢ <a href="https://github.com/huggingface/optimum-executorch">Optimum-ExecuTorch</a> for HuggingFace models ‚Ä¢ <a href="https://docs.unsloth.ai/new/deploy-llms-phone" rel="nofollow">Unsloth</a> for fine-tuned LLM deployment </p>

<p dir="auto">ExecuTorch provides advanced capabilities for production deployment:</p>
<ul dir="auto">
<li><strong>Quantization</strong> ‚Äî Built-in support via <a href="https://docs.pytorch.org/ao" rel="nofollow">torchao</a> for 8-bit, 4-bit, and dynamic quantization</li>
<li><strong>Memory Planning</strong> ‚Äî Optimize memory usage with ahead-of-time allocation strategies</li>
<li><strong>Developer Tools</strong> ‚Äî ETDump profiler, ETRecord inspector, and model debugger</li>
<li><strong>Selective Build</strong> ‚Äî Strip unused operators to minimize binary size</li>
<li><strong>Custom Operators</strong> ‚Äî Extend with domain-specific kernels</li>
<li><strong>Dynamic Shapes</strong> ‚Äî Support variable input sizes with bounded ranges</li>
</ul>
<p dir="auto">See <a href="https://docs.pytorch.org/executorch/main/advanced-topics-section.html" rel="nofollow">Advanced Topics</a> for quantization techniques, custom backends, and compiler passes.</p>

<ul dir="auto">
<li><a href="https://docs.pytorch.org/executorch/main/index.html" rel="nofollow"><strong>Documentation Home</strong></a> ‚Äî Complete guides and tutorials</li>
<li><a href="https://docs.pytorch.org/executorch/main/api-section.html" rel="nofollow"><strong>API Reference</strong></a> ‚Äî Python, C++, Java/Kotlin APIs</li>
<li><a href="https://docs.pytorch.org/executorch/main/backend-delegates-integration.html" rel="nofollow"><strong>Backend Integration</strong></a> ‚Äî Build custom hardware backends</li>
<li><a href="https://docs.pytorch.org/executorch/main/support-section.html" rel="nofollow"><strong>Troubleshooting</strong></a> ‚Äî Common issues and solutions</li>
</ul>

<p dir="auto">We welcome contributions from the community!</p>
<ul dir="auto">
<li>üí¨ <a href="https://github.com/pytorch/executorch/discussions"><strong>GitHub Discussions</strong></a> ‚Äî Ask questions and share ideas</li>
<li>üéÆ <a href="https://discord.gg/Dh43CKSAdc" rel="nofollow"><strong>Discord</strong></a> ‚Äî Chat with the team and community</li>
<li>üêõ <a href="https://github.com/pytorch/executorch/issues"><strong>Issues</strong></a> ‚Äî Report bugs or request features</li>
<li>ü§ù <a href="https://github.com/pytorch/executorch/blob/main/CONTRIBUTING.md"><strong>Contributing Guide</strong></a> ‚Äî Guidelines and codebase structure</li>
</ul>

<p dir="auto">ExecuTorch is BSD licensed, as found in the <a href="https://github.com/pytorch/executorch/blob/main/LICENSE">LICENSE</a> file.</p>

<hr/>

</article></div></div>
  </body>
</html>
