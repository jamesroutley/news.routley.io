<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/adamkarvonen/chess_llm_interpretability">Original</a>
    <h1>Chess-GPT&#39;s Internal World Model</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto">This evaluates LLMs trained on PGN format chess games and evaluates board understand, similar to the Othello World paper.</p>
<p dir="auto">This repo can train, evaluate, and visualize linear probes on LLMs that have been trained to play chess with PGN strings. For example, we can visualize where the model &#34;thinks&#34; the white pawns are. On the left, we have the actual white pawn location. In the middle, we clip the probe outputs to turn the heatmap into a more binary visualization. On the right, we have the full gradient of model beliefs, and we can see it&#39;s extremely confident that no white pawns are on either side&#39;s back rank.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://brian.abelson.live/adamkarvonen/chess_llm_interpretability/blob/main/images/pawn_probe.png"><img src="https://brian.abelson.live/adamkarvonen/chess_llm_interpretability/raw/main/images/pawn_probe.png" alt=""/></a></p>
<p dir="auto">I trained linear probes on the model&#39;s ability to estimate player ELO as it&#39;s predicting the next character. Here we can see a graph of ELO classification accuracy per layer of the LLM.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://brian.abelson.live/adamkarvonen/chess_llm_interpretability/blob/main/images/accuracy_per_layer_elo.png"><img src="https://brian.abelson.live/adamkarvonen/chess_llm_interpretability/raw/main/images/accuracy_per_layer_elo.png" alt=""/></a></p>
<p dir="auto">For more information, refer to this <a href="https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html" rel="nofollow">post</a>.</p>

<p dir="auto">Create a Python environment with Python 3.10.</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt
python model_setup.py"><pre><code>pip install -r requirements.txt
python model_setup.py
</code></pre></div>
<p dir="auto">Then click &#34;Run All&#34; on <code>lichess_data_filtering.ipynb</code>.
To visualise probe outputs, check out <code>probe_output_visualization.ipynb</code>.</p>
<p dir="auto">To train a linear probe or test a saved probe on the test set, set these two variables at the bottom of <code>train_test_chess.py</code>:
RUN_TEST_SET = True
USE_PIECE_BOARD_STATE = True</p>
<p dir="auto">Then run <code>python train_test_chess.py</code>.</p>

<p dir="auto">Much of my linear probing was developed using Neel Nanda&#39;s linear probing code as a reference. They can be found at:</p>
<p dir="auto"><a href="https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Othello_GPT.ipynb" rel="nofollow">https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Othello_GPT.ipynb</a>
<a href="https://colab.research.google.com/github/likenneth/othello_world/blob/master/Othello_GPT_Circuits.ipynb" rel="nofollow">https://colab.research.google.com/github/likenneth/othello_world/blob/master/Othello_GPT_Circuits.ipynb</a>
<a href="https://www.neelnanda.io/mechanistic-interpretability/othello" rel="nofollow">https://www.neelnanda.io/mechanistic-interpretability/othello</a>
<a href="https://github.com/likenneth/othello_world/tree/master/mechanistic_interpretability">https://github.com/likenneth/othello_world/tree/master/mechanistic_interpretability</a></p>
</article>
          </div></div>
  </body>
</html>
