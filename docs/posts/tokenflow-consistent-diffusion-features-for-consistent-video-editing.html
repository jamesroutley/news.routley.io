<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://diffusion-tokenflow.github.io/">Original</a>
    <h1>TokenFlow: Consistent diffusion features for consistent video editing</h1>
    
    <div id="readability-page-1" class="page">


  <section>
    <div>
      <div>
        <div>
          <div>
            
            

                  <p><span>Weizmann institute of science</span>
                    <span><small></small></span>
                  </p>

                  
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section>
   
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section>
  <div>
    <div>
      <div>
        <h2>Abstract</h2>
        <p>
            The generative AI revolution has been recently expanded
            to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual
            quality and user control over the generated content. In this
            work, we present a framework that harnesses the power of
            a text-to-image diffusion model for the task of text-driven
            video editing. Specifically, given a source video and a target
            text-prompt, our method generates a high-quality video that
            adheres to the target text, while preserving the spatial layout and dynamics of the input video. Our method is based
            on our key observation that consistency in the edited video
            can be obtained by enforcing consistency in the diffusion
            feature space. We achieve this by explicitly propagating
            diffusion features based on inter-frame correspondences,
            readily available in the model. Thus, our framework does
            not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-to-image editing method.
            We demonstrate state-of-the-art editing results on a variety
            of real-world videos.
          </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper poster -->
<section>
  <div>
    <div>
      <div>
        <div>
          <h2>Method</h2>
          <div>
        
      <table>
        <tbody>
          <tr>
            <td colspan="3">
              <p>
                We observe that the level of temporal consistency of a video is tightly related to the temporal consistency of its feature representation, as can be seen in the feature visualization below.
                The features of a natural video have a shared, temporally consistent representation. When editing the video per frame, this consistency breaks. Our method ensures the same level of feature consistency as in the original video features.
              </p>
            </td>
          </tr>
          <tr>
            <td>Original</td>
            <td>Per Frame Editing</td>
            <td>Ours</td>
          </tr>
          <tr>
            <td>
              <a href="https://diffusion-tokenflow.github.io/sm/assets/man_basket/input_fps30.mp4">
                <video preload="auto" width="224" src="sm/assets/man_basket/input_fps30.mp4" autoplay="" loop="" controls="" muted="">
              </video></a>
            </td>
            <td>
              <a href="https://diffusion-tokenflow.github.io/sm/assets/man_basket/Van-Gogh%20style%20portrait%20of%20a%20man%20spinning%20a%20basketball,%20oil%20painting,%20art%20by%20Van%20Gogh,%208k/pnp_per_frame_baseline_fps_30.mp4">
                <video preload="auto" width="224" src="sm/assets/man_basket/Van-Gogh style portrait of a man spinning a basketball, oil painting, art by Van Gogh, 8k/pnp_per_frame_baseline_fps_30.mp4" autoplay="" loop="" controls="" muted="">
              </video></a>
            </td>
            <td>
              <a href="https://diffusion-tokenflow.github.io/sm/assets/man_basket/Van-Gogh%20style%20portrait%20of%20a%20man%20spinning%20a%20basketball,%20oil%20painting,%20art%20by%20Van%20Gogh,%208k/result_fps30.mp4">
                <video preload="auto" width="224" src="sm/assets/man_basket/Van-Gogh style portrait of a man spinning a basketball, oil painting, art by Van Gogh, 8k/result_fps_30.mp4" autoplay="" loop="" controls="" muted="">
              </video></a>
            </td>
          </tr>
          <tr>
            <td>
              <a href="https://diffusion-tokenflow.github.io/videos/pca/tokens_origvideo_30.mp4">
                <video preload="auto" width="224" src="videos/pca/tokens_origvideo_30.mp4" autoplay="" loop="" controls="" muted="">
              </video></a>
            </td>
            <td>
              <a href="https://diffusion-tokenflow.github.io/videos/pca/tokens_pnpvideo_30.mp4">
                <video preload="auto" width="224" src="videos/pca/tokens_pnpvideo_30.mp4" autoplay="" loop="" controls="" muted="">
              </video></a>
            </td>
            <td>
              <a href="https://diffusion-tokenflow.github.io/videos/pca/tokens_flowvideo_30.mp4">
                <video preload="auto" width="224" src="videos/pca/tokens_flowvideo_30.mp4" autoplay="" loop="" controls="" muted="">
              </video></a>
            </td>
          </tr>
          <tr>
            
            <td colspan="3">
              <p>
                Our key finding is that a temporally-consistent edit can be achieved by enforcing consistency on the internal diffusion features across frames during the editing process.
                We achieve this by propagating a small set of edited features across frames, using the correspondences between the original video features.
                Given an input video I, we invert each frame, extract its tokens (i.e., output features from the self-attention modules), and extract inter-frame feature correspondences using a nearest-neighbor (NN) search. At each denoising step:
              </p>
              <ol>
                (I) We sample keyframes from the noisy video J_t and jointly edit them using an extended-attention block. The set of resulting edited tokens is T_base.
                </ol>
              To denoise J_t, we feed each frame to the network and replace the generated tokens with the tokens obtained from the propagation step (II).
            </td>
          </tr>
        </tbody>
      </table>

        <p><img src="https://diffusion-tokenflow.github.io/pics/pipeline.png" alt="" width="1000"/>
        </p>
      
      </div>
      </div>
  </div></div></div></section>
<!--End paper poster -->

<!-- Video grid -->
<section>
  <div>
    <div>
      <div>
        <div>
          <h2>TokenFlow Editing results</h2>
          <div>
            
              <p>
                Hover over the videos to see the original video and text prompts.
              </p>
            
  <!-- </td> -->
    
    
    
    </div></div></div></div></div></section>
<!-- End video preload="auto"grid -->

<!-- video preload="auto"carousel -->
<section>
  <div>
    <div>
      <div>
        <div>
          <h2>Comparisons</h2>
          <div>
            <div>
              <p>Input video</p>
              <p>             Ours</p>
              <p>             Text-to-video <a href="#ref-txt2vid">[1]</a></p>
              <p>Tune-a-video <a href="#ref-TAV">[2]</a></p>
              
              <p>        Per frame PnP<a href="#ref-pnp">[4]</a></p>
              <!-- Add remaining captions here  -->
            </div>
      
      </div>
    </div>
  </div>
</div></div></section>
<!-- End video preload="auto"carousel -->

<!--BibTex citation -->
  <section id="BibTeX">
    <div>
      <h2>BibTeX</h2>
      <pre><code>@article{tokenflow2023,
        title = {TokenFlow: Consistent Diffusion Features for Consistent Video Editing},
        author = {Geyer, Michal and Bar-Tal, Omer and Bagon, Shai and Dekel, Tali},
        journal={arXiv preprint arxiv:2307.10373},
        year={2023}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<section>
  <div>
    <div>
      <div>
        <div>
          <!-- <h2 class="title is-3"></h2> -->
          <div>
  <p>
    <a name="ref-txt2vid" id="ref-txt2vid"></a>
    [1] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023.
  </p>
  <p>
    <a name="ref-TAV" id="ref-TAV"></a>
    [2] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
    Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and
    Mike Zheng Shou. Tune-a-video: One-shot tuning of image
    diffusion models for text-to-video generation. arXiv preprint
    arXiv:2212.11565, 2022
  </p>
  <p>
    <a name="ref-gen1" id="ref-gen1"></a>
    [3] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
    Jonathan Granskog, and Anastasis Germanidis. Structure
    and content-guided video synthesis with diffusion models.
    arXiv preprint arXiv:2302.03011, 2023
  </p>
  <p>
    <a name="ref-pnp" id="ref-pnp"></a>
    [4] Narek Tumanyan, Michal Geyer, Shai Bagon, and
    Tali Dekel. Plug-and-play diffusion features for text-
    driven image-to-image translation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023
  </p>
  
  
  </div>
        </div>
      </div>
    </div>  
  

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    
  
  
</div></section></div>
  </body>
</html>
