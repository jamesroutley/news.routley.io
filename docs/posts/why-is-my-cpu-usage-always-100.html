<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.downtowndougbrown.com/2024/04/why-is-my-cpu-usage-always-100-upgrading-my-chumby-8-kernel-part-9/">Original</a>
    <h1>Why is my CPU usage always 100%?</h1>
    
    <div id="readability-page-1" class="page"><div>
				
<p>If you’re new to this series, I’ve been documenting the process I went through upgrading my old PXA166-based Chumby 8’s 2.6.28 Linux kernel to a modern 6.x version. Here are links to parts <a href="https://www.downtowndougbrown.com/2022/12/upgrading-my-old-chumby-8-linux-kernel-part-1-u-boot/" data-type="post" data-id="2063" target="_blank" rel="noreferrer noopener">1</a>, <a href="https://www.downtowndougbrown.com/2022/12/upgrading-my-chumby-8-kernel-part-2-initial-linux-boot/" data-type="post" data-id="2200" target="_blank" rel="noreferrer noopener">2</a>, <a href="https://www.downtowndougbrown.com/2023/01/upgrading-my-chumby-8-kernel-part-3-wi-fi/" data-type="post" data-id="2265" target="_blank" rel="noreferrer noopener">3</a>, <a href="https://www.downtowndougbrown.com/2023/03/upgrading-my-chumby-8-kernel-part-4-reboot-poweroff/" data-type="post" data-id="2751" target="_blank" rel="noreferrer noopener">4</a>, <a href="https://www.downtowndougbrown.com/2023/06/upgrading-my-chumby-8-kernel-part-5-graphics/" data-type="post" data-id="3156" target="_blank" rel="noreferrer noopener">5</a>, <a href="https://www.downtowndougbrown.com/2023/08/upgrading-my-chumby-8-kernel-part-6-pwm-backlight/" data-type="post" data-id="3360" target="_blank" rel="noreferrer noopener">6</a>, <a href="https://www.downtowndougbrown.com/2023/11/upgrading-my-chumby-8-kernel-part-7-touchscreen/" data-type="post" data-id="3564" target="_blank" rel="noreferrer noopener">7</a>, and <a href="https://www.downtowndougbrown.com/2024/04/upgrading-my-chumby-8-kernel-part-8-audio/" data-type="post" data-id="4911" target="_blank" rel="noreferrer noopener">8</a>. At this point in the project, all of the main hardware peripherals were working great. I noticed something odd when running top though. The CPU usage was always really high, and it wasn’t obvious why.</p>



<pre>Mem: 47888K used, 55968K free, 168K shrd, 3116K buff, 27480K cached</pre>



<p>That’s really weird! Why would top be using all of my CPU? It says 100% usr in the second line. Sometimes the usage showed up as 50% usr and 50% sys. Other times it would show up as 100% sys. And very rarely, it would show 100% idle. In that rare case, top would actually show up with 0% usage as I would expect. The 2.6.28 kernel did not have this problem, so it was something different about my newer kernel.</p>



<p>I started theorizing about what might be wrong here. My ideas were nothing more than wild guesses: maybe one of the drivers I got working earlier was totally monopolizing the CPU and making it look like other processes were using all the CPU. Or perhaps I was missing some kind of CPU idle support so the processor was stuck running at 100% at all times, never able to take a break. Maybe there was some power management support missing from the mainline kernel or something. I needed to figure out a way to narrow down the possibilities.</p>



<p>My first step in diagnosing this problem was to go back in time. Several years ago, I had played with getting Linux 3.13 working on my Chumby 8 before I gave up on the project due to not having enough time or experience. This ended up being useful in the present day because I could try booting the old 3.13 kernel to see if it had the same issue.</p>



<p>I quickly discovered that the 3.13 kernel had the same exact problem. This was definitely a useful data point. It meant nothing recent had broken it. Also, I didn’t have many drivers working in that old kernel, so it ruled out a lot of possible causes. I said goodbye to the old kernel and thanked it for its insight.</p>



<p>Next, I tried profiling my modern kernel by enabling CONFIG_PROFILING. Then I added <em>profile=2</em> to my kernel command line. Lastly, I made sure System.map from my Linux build directory was copied over to /boot on my Chumby. Now I was ready to do profiling.</p>



<pre>readprofile -r    # resets the counters</pre>



<p>The readprofile results were interesting. The entire output was far too long to put here, but here is the bottom of what it spit out.</p>



<pre>...</pre>



<p>As you can see based on the total, default_idle_call used up the vast majority of the time. This seemed normal to me. After this test I felt pretty confident that I shouldn’t be blaming any of the random peripheral drivers I had worked on. The CPU was definitely idle, but why didn’t top think so?</p>



<p>I traced my way through what happens in <em><a href="https://elixir.bootlin.com/linux/v6.8/source/kernel/sched/idle.c#L89" target="_blank" rel="noreferrer noopener">default_idle_call</a></em>. It seems like the main thing it ends up doing is calling <a href="https://elixir.bootlin.com/linux/v6.8/source/arch/arm/kernel/process.c#L75" target="_blank" rel="noreferrer noopener"><em>arch_cpu_idle</em></a>. On ARM, this ends up calling <em>arm_pm_idle</em> if it exists, and otherwise it will call <em>cpu_do_idle</em>. <em>arm_pm_idle</em> doesn’t seem to be used on this particular ARM machine, so I kept tracing through <em><a href="https://elixir.bootlin.com/linux/v6.8/source/arch/arm/include/asm/glue-proc.h#L252" data-type="link" data-id="https://elixir.bootlin.com/linux/v6.8/source/arch/arm/include/asm/glue-proc.h#L252" target="_blank" rel="noreferrer noopener">cpu_do_idle</a></em>. The final result is it ends up calling <em><a href="https://elixir.bootlin.com/linux/v6.8/source/arch/arm/mm/proc-mohawk.S#L79" target="_blank" rel="noreferrer noopener">cpu_mohawk_do_idle</a></em> on the PXA168, confirmed by disassembling the final vmlinux file with objdump.</p>



<p>This function doesn’t seem very complex (this is from the Linux 6.8 code):</p>


<div><pre title="">ENTRY(cpu_mohawk_do_idle)
	mov	r0, #0
	mcr	p15, 0, r0, c7, c10, 4		@ drain write buffer
	mcr	p15, 0, r0, c7, c0, 4		@ wait for interrupt
	ret	lr
</pre></div>


<p>Here’s the corresponding function in the 2.6.28 kernel source:</p>


<div><pre title="">ENTRY(cpu_mohawk_do_idle)
	mov	r0, #0
#ifdef CONFIG_ENABLE_COREIDLE
	mcr	p15, 0, r0, c7, c0, 4		@ Wait for interrupt
#endif
	mov	pc, lr
</pre></div>


<p>So they’re pretty much the same thing, except the newer kernel also drains the write buffer. I observed that CONFIG_ENABLE_COREIDLE was not defined on the 2.6.28 kernel though, so that was quite a big difference. I double checked the 2.6.28 kernel with objdump just to make sure:</p>


<div><pre title="">c0207000 &lt;cpu_mohawk_do_idle&gt;:
c0207000:       e3a00000        mov     r0, #0
c0207004:       e1a0f00e        mov     pc, lr
</pre></div>


<p>Yeah, the wait for interrupt was definitely not there in 2.6.28. This gave me some ideas to try and I was starting to feel some hope that I might have found something. I tinkered with making the modern kernel’s <em>cpu_mohawk_do_idle</em> function identical to the old version. This honestly didn’t make logical sense though. Wouldn’t <em>removing</em> a “wait for interrupt” instruction actually make things worse in terms of idling the CPU? I shut off my brain and tried it out anyway. Why not?</p>



<p>Even though I thought I was making great progress, this whole branch of research turned out to be a red herring. No matter what tinkering I did in <em>cpu_mohawk_do_idle</em>, top was taking up 100% CPU. I went from thinking I was on the cusp of solving the problem, right back to square one.</p>



<p>After walking away from the problem for a while, I realized I had been throwing around random guesses instead of using logic. I decided to attack this problem from another angle instead. One simple question led me in a new direction: how does top calculate the CPU usage it displays? I realized I had no freaking clue how top worked. It was all just magic to me. I looked at <a href="https://elixir.bootlin.com/busybox/1.35.0/source/procps/top.c" target="_blank" rel="noreferrer noopener">BusyBox’s source code</a> to try to understand how it works.</p>



<p>It turns out that all of top’s information comes from procfs, mounted at /proc. At the beginning of the source file you can see a comment that says “At startup this changes to /proc, all the reads are then relative to that.” This is confirmed <a href="https://elixir.bootlin.com/busybox/1.35.0/source/procps/top.c#L1149" target="_blank" rel="noreferrer noopener">on line 1150</a> where it changes the working directory.</p>



<p>What it does is read several files: /proc/stat, /proc/meminfo, and all of the /proc/&lt;pid&gt;/stat files — one for each running process. Iterating through all of the processes is handled by <a href="https://elixir.bootlin.com/busybox/1.35.0/source/libbb/procps.c#L282" target="_blank" rel="noreferrer noopener">procps_scan</a> in BusyBox’s implementation of top. A comment in the source helpfully informs us that <a href="https://man7.org/linux/man-pages/man5/proc.5.html" target="_blank" rel="noreferrer noopener">man 5 proc</a> gives more info about the content of these files. I focused on /proc/stat, which is responsible for providing the info in the second line of top’s output showing how the CPU load is distributed between user, system, idle, etc.</p>



<p>The man page says that the lines beginning with “cpu” in /proc/stat contain a bunch of numbers corresponding to the amount of time spent in a bunch of different states. Here is some example output from my desktop computer:</p>



<pre>cpu  33032 1025 8433 426488 1422 0 314 0 0 0</pre>



<p>The numbers, in order from left to right, represent time spent in: user mode, nice (user mode low priority), system, idle, iowait, irq, softirq, steal, guest, and guest_nice. They are in units of USER_HZ, which it claims is usually 1/100ths of a second but can vary. I can see that is correct on my x86_64 machine:</p>



<pre>$ getconf CLK_TCK</pre>



<p>I didn’t have getconf available on my buildroot-generated Chumby rootfs. I probably could have turned on a package to add it, but it’s easy enough to just throw together a quick C program to figure it out instead:</p>



<pre>#include &lt;stdio.h&gt;</pre>



<p>Surprise surprise, it’s the same as my desktop PC.</p>



<pre># /tmp/test</pre>



<p>Now that I knew my units were 1/100ths of a second, I tried a few experiments. It appears that top simply checks the values in /proc/stat periodically, and calculates the load by looking at the differences in the values between each iteration. So I theorized that if the CPU mostly has nothing going on, I should be able to look at the file content, sleep 10 seconds, look at the file content again, and expect the idle count to go up by about 1000. That’s because 1000 * (1/100ths of a second) = 10 seconds. I decided to try it out, starting on my desktop PC:</p>



<pre>$ cat /proc/stat | grep &#39;cpu&#39; ; sleep 10 ; cat /proc/stat | grep &#39;cpu&#39;</pre>



<p>…10 seconds passed by, and then this printed out:</p>



<pre>cpu  49844 370 14756 25016161 3472 0 169 0 0 0</pre>



<p>Aha, so there’s a total, along with a separate line for each CPU. Remember that the idle count is the 4th column. Let’s look at a before and after comparison of cpu13, for example:</p>



<pre>cpu13 2612 18 1158 1562700 400 0 0 0 0 0</pre>



<p>That looks about right! The idle count for that CPU increased by 996, which is about 1000. Here’s a comparison between the “total CPU” lines:</p>



<pre>cpu  49815 370 14743 25000206 3471 0 169 0 0 0</pre>



<p>The total idle count increased by 15955, which is about 1000 times 16. This is my AMD Ryzen 5700G with 8 cores and 16 threads. Each thread is treated as a CPU as far as Linux is concerned. Okay, the math checked out! I felt like I understood how top does its calculations.</p>



<p>Next, I performed a similar comparison in my modern Chumby kernel. There’s only one CPU, so it prints out cpu and cpu0 lines that are identical. I decided to filter it further to only show the total CPU:</p>



<pre># cat /proc/stat | grep &#39;cpu &#39; ; sleep 10 ; cat /proc/stat | grep &#39;cpu &#39;</pre>



<p>I had finally made some real progress in tracking down the issue. This output is just plain wrong. Even though 10 seconds elapsed between the two lines being printed, the idle count only went up by 2. This was indicating that the CPU had only spent 0.02 seconds idling during that period. That result made absolutely no sense, because the other columns only increased by a total of 1 + 9 + 1 = 11 ticks = 0.11 seconds.</p>



<p>At this point, I excitedly booted up the old 2.6.28 kernel which seemed to behave perfectly fine with top, and repeated the experiment:</p>



<pre># cat /proc/stat | grep &#39;cpu &#39; ; sleep 10 ; cat /proc/stat | grep &#39;cpu &#39;</pre>



<p>The idle tick count increased by exactly 1000 ticks, also known as 10 seconds — just as I would have expected! This gave me something to actually start looking at in my new kernel. It wasn’t incrementing the idle tick counter for some reason. Well, occasionally it did, but not very often. This perfectly explains why it would almost always show the CPU as being 100% busy. It’s because top just calculates the percentages of each column’s ticks added versus the total ticks added between each time it checks. With the difference in idle ticks being 0 or close to 0 every time, it makes sense that the CPU was usually showing up as 100% busy.</p>



<p>Now that I knew what was happening, the next question was: why? Where in the kernel would I find the code that increments the idle tick count? What could possibly be broken to only cause this to happen on the PXA16x?</p>



<p>I searched around a bit and happened to find <a href="http://dev.laptop.org/git/olpc-kernel/commit/arch/arm?h=arm-3.5&amp;id=5bd2520f8f51fc44911ec7a86b84f41a1f3e384c" target="_blank" rel="noreferrer noopener">this commit to the OLPC kernel</a>. It’s very straightforward: it disables CONFIG_NO_HZ in xo_4_defconfig. The commit message by Jon Nettleton mentioned a problem that seemed very similar to mine:</p>



<blockquote>
<p>There appears to be an accounting problem when CONFIG_NO_HZ is enabled in our kernel that leads to high system cpu percentage reporting. This in turn breaks fast user suspend.</p>
</blockquote>



<p>Interesting! The commit title starts with “arm: mmp3” which means it’s another Marvell CPU newer than mine, so I was intrigued by this. <a href="https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt" target="_blank" rel="noreferrer noopener">CONFIG_NO_HZ is explained in detail in the kernel documentation</a>. What it does is disables periodic timer interrupts used by the scheduler when the CPU is idle. It turns out that my kernel had CONFIG_NO_HZ_IDLE=y, which is effectively the same thing as what used to be CONFIG_NO_HZ=y. So I had a config similar to the config OLPC was using when they experienced this problem!</p>



<p>This left me with some new ideas to try. Luckily, the kernel documentation linked above explains old and new names for all of the relevant config options. In particular, it told me that the equivalent of disabling CONFIG_NO_HZ on a newer kernel would be to set CONFIG_HZ_PERIODIC=y instead. It also informed me that I could boot with an extra kernel command line option “nohz=off” to temporarily try it without recompiling. So I booted with nohz=off and reran my tests:</p>



<pre>Mem: 50240K used, 53616K free, 200K shrd, 3116K buff, 27276K cached</pre>



<p>I immediately knew the problem was gone, because top was no longer showing 100% CPU usage. That “99% idle” section above was music to my ears. And obviously, running the same test with /proc/stat worked correctly too:</p>



<pre># cat /proc/stat | grep &#39;cpu &#39; ; sleep 10 ; cat /proc/stat | grep &#39;cpu &#39;</pre>



<p>OLPC’s workaround fixed the problem for me too!</p>



<p>This was exciting, but I didn’t feel satisfied with it. It felt a bit like magic. I knew I needed to dig further to discover exactly what the problem actually was. CONFIG_NO_HZ_IDLE should have been working fine, so I was pretty sure there was a bug somewhere deeper. I wanted to be able to use my CPU in dyntick-idle mode!</p>



<p>I decided to trace backwards starting from /proc/stat. Where is the code in the kernel that provides /proc/stat, and where does it get its idle time number?</p>



<p>The content of /proc/stat is provided by the <em><a href="https://elixir.bootlin.com/linux/v6.8/source/fs/proc/stat.c#L82" target="_blank" rel="noreferrer noopener">show_stat</a></em> function in fs/proc/stat.c. At the bottom of the file you can see the call to <em><a href="https://elixir.bootlin.com/linux/v6.8/source/fs/proc/stat.c#L211" target="_blank" rel="noreferrer noopener">proc_create</a></em> which sets up “stat” as a file in /proc. Anyway, <em>show_stat</em> calls <em><a href="https://elixir.bootlin.com/linux/v6.8/source/fs/proc/stat.c#L25" target="_blank" rel="noreferrer noopener">get_idle_time</a></em>, which calls <a href="https://elixir.bootlin.com/linux/v6.8/source/kernel/time/tick-sched.c#L740" target="_blank" rel="noreferrer noopener"><em>get_cpu_idle_time_us</em></a>. This last function is definitely interesting because the comment above it informs us that “This time is measured via accounting rather than sampling.”</p>



<p>It looks at the time by calling <em><a href="https://elixir.bootlin.com/linux/v6.8/source/kernel/time/tick-sched.c#L694" target="_blank" rel="noreferrer noopener">get_cpu_sleep_time_us</a></em>, which itself is calling <em><a href="https://elixir.bootlin.com/linux/v6.8/source/kernel/time/timekeeping.c#L836" target="_blank" rel="noreferrer noopener">ktime_get</a></em> to find the current time to use in its accounting. I couldn’t imagine that any of the actual math being done here was wrong. Otherwise, wouldn’t there be huge uproar and lots of people with different architectures would all be seeing this? The fact that the same thing had happened on another Marvell ARCH_MMP processor was a major clue.</p>



<p>What does <em>ktime_get</em> return and how does it work? The idea behind it is simple. It returns a ktime_t which represents a monotonically increasing timer stored in nanoseconds. Specifically, <a href="https://docs.kernel.org/core-api/timekeeping.html" target="_blank" rel="noreferrer noopener">the documentation</a> says that the time returned by <em>ktime_get</em> starts at system boot but pauses during suspend. At some point it’s going to have to call into hardware-specific code, which is where I suspected the problem was.</p>



<p>If you follow the chain of calls from <em>ktime_get</em>, you end up going through some functions in kernel/time/timekeeping.c, eventually ending up at <em><a href="https://elixir.bootlin.com/linux/v6.8/source/kernel/time/timekeeping.c#L191" target="_blank" rel="noreferrer noopener">tk_clock_read</a></em> which calls the read function provided by a <a href="https://elixir.bootlin.com/linux/v6.8/source/include/linux/clocksource.h#L34" target="_blank" rel="noreferrer noopener">clocksource</a>. A clocksource is, as the kernel source code says, a “hardware abstraction for a free running counter.” Finally, I had found the path to some hardware-specific code.</p>



<p>I searched in the kernel’s arch/arm/mach-mmp source directory for anything related to clocksource. Bingo:</p>



<pre>time.c: *   Support for clocksource and clockevents</pre>



<p>Looking further at the clocksource that is registered in <a href="https://elixir.bootlin.com/linux/v6.0/source/arch/arm/mach-mmp/time.c" target="_blank" rel="noreferrer noopener">this file</a>, I saw that the relevant function for grabbing the current time would be <em>clksrc_read</em>:</p>


<div><pre title="">static struct clocksource cksrc = {
	.name		= &#34;clocksource&#34;,
	.rating		= 200,
	.read		= clksrc_read,
	.mask		= CLOCKSOURCE_MASK(32),
	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
};
</pre></div>


<p>That is simply a wrapper for <em>timer_read</em>, which is the actual code that reads from the hardware timer:</p>


<div><pre title="">/*
 * FIXME: the timer needs some delay to stablize the counter capture
 */
static inline uint32_t timer_read(void)
{
	int delay = 100;

	__raw_writel(1, mmp_timer_base + TMR_CVWR(1));

	while (delay--)
		cpu_relax();

	return __raw_readl(mmp_timer_base + TMR_CVWR(1));
}
</pre></div>


<p>This code is more complicated than what I expected to see. I was thinking it would just be a simple register read. Instead, it has to write a 1 to the register, and then delay for a while, and then read back the same register. There was also a very noticeable FIXME in the comment for the function, which definitely raised a red flag in my mind.</p>



<p>I decided to go back to the <a href="https://web.archive.org/web/20160428154454/http://www.marvell.com/application-processors/armada-100/assets/armada_16x_software_manual.pdf" target="_blank" rel="noreferrer noopener">Armada 16x software manual</a> to read up on the hardware timers. My first thought was perhaps I had the timer configured for the wrong clock rate, but I was able to confirm that the timer was indeed configured for 3.25 MHz just like code said it was. At the bottom of page 683 in the PDF, I found an interesting blurb about the timer count registers:</p>



<blockquote>
<p>The timer values are read under risk of metastability. Therefore, reading each one of the CR Register values in the timer clock time base is accomplished by either of the following: Double-read procedure and comparing the two read values to ensure that the value is valid using the CVWR Register, especially effective for fast clock timers.</p>
</blockquote>



<p>Some formatting or punctuation must have been lost during creation of the manual. I had to read the above paragraph about 10 times before I figured out what I think it was trying to say. Here’s my edited version:</p>



<blockquote>
<p>The timer values are read under risk of metastability. Therefore, reading each one of the CR Register values in the timer clock time base is accomplished by either of the following:</p>



<ol>
<li>Double-read procedure and comparing the two read values to ensure that the value is valid.</li>



<li>Using the CVWR Register, especially effective for fast clock timers.</li>
</ol>
</blockquote>



<p>Clearly the code in the Linux kernel was implementing the second solution. I checked out Marvell’s documentation for the CVWR register. Let’s just say the documentation about the register was quite sparse:</p>



<blockquote>
<p>This register prevents the risk of instability on counter value reading</p>



<p><strong>Write:</strong></p>
</blockquote>



<p>Yeah, that’s definitely what the kernel was trying to do. On a whim, I decided to try replacing the existing code with a simple read of the counter register instead, ignoring the metastability risk altogether:</p>


<div><pre title="">static inline uint32_t timer_read(void)
{
	return __raw_readl(mmp_timer_base + TMR_CR(1));
}
</pre></div>


<p>It worked! With this change, top showed good idle CPU time. I was confident I had found the source of the problem. I went back to the original broken code reading from the CVWR register and played around some more. If I increased the delay from 100 to 500 iterations, the original code started working. Tweaking it further, 200 didn’t work but 300 did.</p>



<p>I had finally found the root problem. My initial guesses about the issue were totally wrong. It was a simple timing problem during register reads. The existing code wasn’t waiting long enough after asking the timer to capture the latest value, so it was returning the value from the previous read attempt instead. The FIXME comment above the function was correct. Too bad the software manual didn’t give me any info about what the delay should actually be.</p>



<p>Around this point, I remembered that Chumby’s original 2.6.28 kernel worked correctly, so I decided to take a look at its version of this section of code. Not surprisingly, it had a different version of <em>timer_read</em>:</p>


<div><pre title="">/*
 * Note: the timer needs some delay to stablize the counter capture
 */
static inline uint32_t timer_read(void)
{
	volatile int delay = 4;
	unsigned long flags;
	uint32_t val = 0;

	local_irq_save(flags);
	__raw_writel(1, TIMERS_VIRT_BASE + TMR_CVWR(0));

	while (delay--) {
		val = __raw_readl(TIMERS_VIRT_BASE + TMR_CVWR(0));
	}

	val = __raw_readl(TIMERS_VIRT_BASE + TMR_CVWR(0));
	local_irq_restore(flags);

	return val;
}
</pre></div>


<p>First of all it was using timer 0 instead of timer 1, but the big difference was it implemented the delay by performing multiple reads from the register rather than an empty loop. It was also disabling interrupts during the capture.</p>



<p>I searched for more Marvell vendor kernels to see if they did the timer read any differently:</p>



<ul>
<li><a href="https://github.com/kumajaya/android_kernel_samsung_lt02/blob/24c62c9af6bf6d424aaa3f249099b0eef8ccbec4/arch/arm/mach-mmp/time.c#L60" target="_blank" rel="noreferrer noopener">https://github.com/kumajaya/android_kernel_samsung_lt02/blob/24c62c9af6bf6d424aaa3f249099b0eef8ccbec4/arch/arm/mach-mmp/time.c#L60</a></li>



<li><a href="https://github.com/embeddedTS/linux-2.6.34-ts471x/blob/c3de2e89f23d328e99253b2320efd8944e524b08/arch/arm/mach-mmp/time.c#L61" target="_blank" rel="noreferrer noopener">https://github.com/embeddedTS/linux-2.6.34-ts471x/blob/c3de2e89f23d328e99253b2320efd8944e524b08/arch/arm/mach-mmp/time.c#L61</a></li>
</ul>



<p>The kernel at the first link seemed to be implementing the double-read approach that Marvell had suggested, but only if the timer was configured for 32.768 KHz. Otherwise it did something similar to the Chumby 2.6.28 kernel, but without disabling interrupts. The second link seemed to just ignore the risk of metastability altogether when in 32.768 KHz mode, and otherwise also implemented a solution similar to the old Chumby kernel.</p>



<p>This bug has been in the kernel ever since the MMP architecture was added. <a href="https://lore.kernel.org/all/1235051161-8858-5-git-send-email-eric.y.miao@gmail.com/" target="_blank" rel="noreferrer noopener">The original patch submission adding PXA168 support in early 2009</a> contained the problematic code, but it was using timer 0 instead of timer 1. So it’s kind of a hybrid of the Chumby kernel’s code and the newer mainline code. At this point it didn’t have FIXME in the comment:</p>


<div><pre title="">/*
 * Note: the timer needs some delay to stablize the counter capture
 */
static inline uint32_t timer_read(void)
{
	int delay = 100;

	__raw_writel(1, TIMERS_VIRT_BASE + TMR_CVWR(0));

	while (delay--)
		cpu_relax();

	return __raw_readl(TIMERS_VIRT_BASE + TMR_CVWR(0));
}
</pre></div>


<p>However, the FIXME is in the initial commit in the kernel git history, so someone at the time must have known that something was still broken in the code. It appears that changing to timer 1 instead of 0 was a <a href="https://lore.kernel.org/all/20110607140456.GE11275@wantstofly.org/" target="_blank" rel="noreferrer noopener">change added later to ensure that clockevents had their own timer instead</a>, fixing a bug observed on the OLPC XO-1.75.</p>



<p>I was shocked that nobody had ever noticed this bug in the mainline kernel, but that’s what happened. I guess to be more accurate, the OLPC project did notice it at one point, but since they had a workaround it wasn’t a huge deal. I decided to go ahead and fix the underlying problem.</p>



<p>Because Marvell vendor kernels read from the register multiple times to implement the delay for the CVWR approach, that’s what I went for. I had to decide how many iterations to delay. Chumby’s kernel did a total of 5 reads of the CVWR register. The other two kernels did a total of 3 reads. I opted to use 4 as a middle ground, just in case Chumby had a real reason to have more iterations. I also decided against disabling interrupts during the delay. It seemed pointless — what’s the worst that could happen? Two timer reads happen nearly simultaneously and the first read ends up returning a slightly later time value than it would have otherwise returned? Or maybe the first read attempt is canceled and returns the old timer value instead? I suppose that could be problematic. Oh well, the existing code wasn’t disabling interrupts either.</p>



<p>I <a href="https://lore.kernel.org/all/20220905200804.438049-1-doug@schmorgal.com/" target="_blank" rel="noreferrer noopener">first submitted my fix for this problem in September 2022</a>, but I didn’t receive any responses. I ended up <a href="https://lore.kernel.org/all/20221204005117.53452-1-doug@schmorgal.com/" target="_blank" rel="noreferrer noopener">resubmitting it later that year and CCing the main SoC maintainers the second time</a>, and they took care of merging my fix. It was finally released with Linux 6.2 and was also backported to several 4.x, 5.x, and 6.x kernels. Ever since I implemented this fix, I haven’t noticed any problems with CPU time reporting on my Chumby.</p>



<p>That’s the story of how I figured out why the CPU usage was always showing up as 100% on my Chumby 8. It was quite the journey through the source code of BusyBox, /proc, and multiple layers of kernel code to find a little gremlin in the code that reads from the PXA168’s timer count register. It was very satisfying to be able to fix the problem! The time I spent was totally worth it too. I learned all about how procfs works and how top gets its info about CPU usage. I still feel like I know almost nothing about the internals of the Linux kernel, but solving a problem like this was a fantastic way to dip my toes into it.</p>



<p>In the next post in my Chumby kernel upgrade series, I’m going to go over how I got the real-time clock working, so that it would remember the date and time while it was turned off. <a href="https://www.downtowndougbrown.com/2024/06/upgrading-my-chumby-8-kernel-part-10-rtc/" data-type="post" data-id="5375" target="_blank" rel="noreferrer noopener">Click here to go there now</a>.</p>
			  
			</div></div>
  </body>
</html>
