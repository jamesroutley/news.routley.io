<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/">Original</a>
    <h1>Introduction to Diffusion Models for Machine Learning</h1>
    
    <div id="readability-page-1" class="page"><section>
      <p>Diffusion Models are generative models which have been gaining significant popularity in the past several years, and for good reason. A handful of seminal papers released in the 2020s <em>alone </em>have shown the world what Diffusion models are capable of, such as beating GANs<sup>[<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">6</a>]</sup> on image synthesis. Most recently, practitioners will have seen Diffusion Models used in <a href="https://www.assemblyai.com/blog/how-dall-e-2-actually-works/">DALL-E 2</a>, OpenAI&#39;s image generation model released last month.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-11.png" alt="" loading="lazy" width="782" height="778" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-11.png 600w, https://www.assemblyai.com/blog/content/images/2022/05/image-11.png 782w" sizes="(min-width: 720px) 720px"/><figcaption>Various images generated by DALL-E 2 (<a href="https://openai.com/dall-e-2/">source</a>).</figcaption></figure><p>Given the recent wave of success by Diffusion Models, many Machine Learning practitioners are surely interested in their inner workings. In this article, we will examine the <strong>theoretical foundations for Diffusion Models</strong>, and then demonstrate how to generate images with a <strong>Diffusion Model in PyTorch</strong>. Let&#39;s dive in!</p><h2 id="diffusion-modelsintroduction">Diffusion Models - Introduction</h2><p>Diffusion Models are <strong>generative </strong>models, meaning that they are used to generate data similar to the data on which they are trained.<em><strong> </strong></em>Fundamentally, Diffusion Models work by <strong>destroying training data</strong> through the successive addition of Gaussian noise, and then <strong>learning to recover</strong> the data by <em>reversing</em> this noising process. After training, we can use the Diffusion Model to generate data by simply <strong>passing randomly sampled noise through the learned denoising process</strong>.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-10.png" alt="" loading="lazy" width="915" height="311" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-10.png 600w, https://www.assemblyai.com/blog/content/images/2022/05/image-10.png 915w" sizes="(min-width: 720px) 720px"/><figcaption>Diffusion Models can be used to generate images from noise (adapted from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><p>More specifically, a Diffusion Model is a latent variable model which maps to the latent space using a fixed Markov chain. This chain gradually adds noise to the data in order to obtain the approximate posterior \( q(\textbf{x}_{1:T}|\textbf{x}_0) \), where \( \textbf{x}_1, ... , \textbf{x}_T \) are the latent variables with the same dimensionality as \( \textbf{x}_0 \). In the figure below, we see such a Markov chain manifested for image data.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image.png" alt="" loading="lazy" width="1584" height="257" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image.png 1000w, https://www.assemblyai.com/blog/content/images/2022/05/image.png 1584w" sizes="(min-width: 720px) 720px"/><figcaption>(Modified from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><p>Ultimately, the image is asymptotically transformed to pure Gaussian noise. The <strong>goal</strong> of training a diffusion model is to learn the <strong>reverse</strong> process - i.e. training \( p_\theta(x_{t-1}|x_t) \). By traversing backwards along this chain, we can generate new data.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-1.png" alt="" loading="lazy" width="1025" height="173" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-1.png 1000w, https://www.assemblyai.com/blog/content/images/2022/05/image-1.png 1025w" sizes="(min-width: 720px) 720px"/><figcaption>(Modified from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><h3 id="benefits-of-diffusion-models">Benefits of Diffusion Models</h3><p>As mentioned above, research into Diffusion Models has exploded in recent years. Inspired by non-equilibrium thermodynamics<sup>[<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">1</a>]</sup>, Diffusion Models currently produce <strong>State-of-the-Art image quality</strong>, examples of which can be seen below:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/diff_beat_gan.png" alt="" loading="lazy" width="893" height="892" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/diff_beat_gan.png 600w, https://www.assemblyai.com/blog/content/images/2022/05/diff_beat_gan.png 893w" sizes="(min-width: 720px) 720px"/><figcaption>(adapted from <a href="https://arxiv.org/pdf/2105.05233.pdf">source</a>)</figcaption></figure><p>Beyond cutting-edge image quality, Diffusion Models come with a host of other benefits, including <strong>not requiring adversarial training</strong>. The difficulties of adversarial training are well-documented; and, in cases where non-adversarial alternatives exist with comparable performance and training efficiency, it is usually best to utilize them. On the topic of training efficiency, Diffusion Models also have the added benefits of <strong>scalability and parallelizability.</strong></p><p>While Diffusion Models almost seem to be producing results out of thin air, there are a lot of careful and interesting mathematical choices and details that provide the foundation for these results, and best practices are still evolving in the literature. Let&#39;s take a look at the mathematical theory underpinning Diffusion Models in more detail now.</p><h2 id="diffusion-modelsa-deep-dive">Diffusion Models - A Deep Dive</h2><p>As mentioned above, a Diffusion Model consists of a <strong>forward process </strong>(or <strong>diffusion process</strong>), in which an data (generally an image) is progressively noised, and a <strong>reverse process </strong>(or <strong>reverse diffusion process</strong>), in which noise is transformed back into a sample from the target distribution.</p><p>The sampling chain transitions in the forward process can be set to conditional Gaussians when the noise level is sufficiently low. Combining this fact with the Markov assumption leads to a simple parameterization of the forward process:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/1-1.png" alt="" loading="lazy" width="2000" height="105" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/1-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/1-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/1-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/1-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>Where \( \beta_1, ..., \beta_T \) is a variance schedule (either learned or fixed) which, if well-behaved, <strong>ensures that</strong> \( x_T \) <strong>is nearly an isotropic Gaussian for sufficiently large T</strong>.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image.png" alt="" loading="lazy" width="1584" height="257" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image.png 1000w, https://www.assemblyai.com/blog/content/images/2022/05/image.png 1584w" sizes="(min-width: 720px) 720px"/><figcaption>Given the Markov assumption, the joint distribution of the latent variables is the product of the Gaussian conditional chain transitions (modified from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>).</figcaption></figure><p>As mentioned previously, the &#34;magic&#34; of diffusion models comes in the <strong>reverse process</strong>. During training, the model learns to reverse this diffusion process in order to generate new data. Starting with the pure Gaussian noise \( p(\textbf{x}_{T}) := \mathcal{N}(\textbf{x}_T, \textbf{0}, \textbf{I}) \), the model learns the joint distribution \( p_\theta(\textbf{x}_{0:T}) \) as </p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/2-1.png" alt="" loading="lazy" width="2000" height="125" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/2-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/2-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/2-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/2-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>where the time-dependent parameters of the Gaussian transitions are learned. Note in particular that the Markov formulation asserts that a given reverse diffusion transition distribution depends only on the previous timestep (or following timestep, depending on how you look at it):</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/3.png" alt="" loading="lazy" width="2000" height="77" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/3.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/3.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/3.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/3.png 2400w" sizes="(min-width: 720px) 720px"/></figure><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-1.png" alt="" loading="lazy" width="1025" height="173" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-1.png 1000w, https://www.assemblyai.com/blog/content/images/2022/05/image-1.png 1025w" sizes="(min-width: 720px) 720px"/><figcaption>(Modified from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><h3 id="training">Training</h3><p>A Diffusion Model is trained by <strong>finding the reverse Markov transitions that maximize the likelihood of the training data</strong>. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/4-1.png" alt="" loading="lazy" width="2000" height="168" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/4-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/4-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/4-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/4-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p>Notation Detail</p>
  </header>
	<p>Note that L<sub>vlb</sub> is technically an <i>upper</i> bound (the negative of the ELBO) which we are trying to minimize, but we refer to it as L<sub>vlb</sub> for consistency.</p>
</div><!--kg-card-end: html--><p>We seek to rewrite the \( L_{vlb} \) in terms of <strong>Kullback-Leibler (KL) Divergences</strong>. The KL Divergence is an asymmetric statistical distance measure of how much one probability distribution <em>P</em> differs from a reference distribution <em>Q</em>. We are interested in formulating \( L_{vlb} \)  in terms of KL divergences because the transition distributions in our Markov chain are Gaussians, and <strong>the KL divergence between Gaussians has a closed form</strong>.</p><!--kg-card-begin: html--><p>
    <h4>What is the KL Divergence?</h4>
</p><!--kg-card-end: html--><p>The mathematical form of the KL divergence for continuous distributions is</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-12.png" alt="" loading="lazy" width="944" height="167" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-12.png 600w, https://www.assemblyai.com/blog/content/images/2022/05/image-12.png 944w" sizes="(min-width: 720px) 720px"/><figcaption>The double bars indicate that the function is <em>not</em> symmetric with respect to its arguments.</figcaption></figure><p>Below you can see the KL divergence of a varying distribution <em>P</em> (blue) from a reference distribution <em>Q</em> (red). The green curve indicates the function within the integral in the definition for the KL divergence above, and the total area under the curve represents the value of the KL divergence of <em>P</em> from <em>Q</em> at any given moment, a value which is also displayed numerically.  </p><figure><div><video src="https://www.assemblyai.com/blog/content/media/2022/05/KL_Divergence.mp4" poster="https://img.spacergif.org/v1/2400x1168/0a/spacer.png" width="2400" height="1168" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video><div><div><p><span>0:00</span></p><p>/<span></span></p></div></div></div></figure><!--kg-card-begin: html--><p>
    <h4>Casting \( L_{vlb} \) in Terms of KL Divergences</h4>
</p><!--kg-card-end: html--><p>As mentioned previously, it is possible<sup>[<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">1</a>]</sup> to rewrite \( L_{vlb} \) almost completely in terms of KL divergences:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/5.png" alt="" loading="lazy" width="2000" height="103" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/5.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/5.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/5.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/5.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>where</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/6-1.png" alt="" loading="lazy" width="2000" height="423" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/6-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/6-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/6-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/6-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p>Derivation Details</p>
  </header>
	<div>
        <p>The variational lower bound is equal to</p>
        </div>
</div><!--kg-card-end: html--><p>Conditioning the forward process posterior on \( x_0 \) in \( L_{t-1} \) results in a tractable form that leads to <strong>all KL divergences being comparisons between Gaussians</strong>. This means that the divergences can be exactly calculated with closed-form expressions rather than with Monte Carlo estimates<sup>[<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">3</a>]</sup>. </p><h3 id="model-choices">Model Choices</h3><p>With the mathematical foundation for our objective function established, we now need to make several choices regarding how our Diffusion Model will be implemented. For the forward process, the only choice required is defining the variance schedule, the values of which are generally increasing during the forward process.</p><p>For the reverse process, we much choose the Gaussian distribution parameterization / model architecture(s). Note the <strong>high degree of flexibility</strong> that Diffusion Models afford - the <em>only</em> requirement on our architecture is that its input and output have the same dimensionality.</p><p>We will explore the details of these choices in more detail below.</p><!--kg-card-begin: html--><p>
    <h4>Forward Process and \( L_T \)</h4>
</p><!--kg-card-end: html--><p>As noted above, regarding the forward process, we must define the variance schedule. In particular, we set them to be <strong>time-dependent constants</strong>, ignoring the fact that they can be learned. For example<sup>[<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">3</a>]</sup>, a linear schedule from \(\beta_1=10^{-4}\) to \(\beta_T=0.2\) might be used, or perhaps a geometric series.</p><p>Regardless of the particular values chosen, the fact that the variance schedule is fixed results in \( L_{T} \) becoming a constant with respect to our set of learnable parameters, allowing us to ignore it as far as training is concerned. </p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/l_t_cross.png" alt="" loading="lazy" width="2000" height="157" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/l_t_cross.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/l_t_cross.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/l_t_cross.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/l_t_cross.png 2400w" sizes="(min-width: 720px) 720px"/></figure><!--kg-card-begin: html--><p>
    <h4>Reverse Process and \( L_{1:T-1} \)</h4>
</p><!--kg-card-end: html--><p>Now we discuss the choices required in defining the reverse process. Recall from above we defined the reverse Markov transitions as a Gaussian:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/7.png" alt="" loading="lazy" width="2000" height="103" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/7.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/7.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/7.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/7.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>We must now define the functional forms of \( \pmb{\mu}_\theta \) or \( \pmb{\Sigma}_\theta \). While there are more complicated ways to parameterize \( \pmb{\Sigma}_\theta \)<sup>[<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">5</a>]</sup>, we simply set</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/8.png" alt="" loading="lazy" width="2000" height="249" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/8.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/8.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/8.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/8.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>That is, we assume that the multivariate Gaussian is a product of independent gaussians with identical variance, a variance value which can change with time. We <strong>set these variances to be equivalent to our forward process variance schedule</strong>.</p><p>Given this new formulation of \( \pmb{\Sigma}_\theta \), we have</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/9.png" alt="" loading="lazy" width="2000" height="124" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/9.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/9.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/9.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/9.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p> which allows us to transform</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/6.2.png" alt="" loading="lazy" width="2000" height="125" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/6.2.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/6.2.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/6.2.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/6.2.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>to</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/10-1.png" alt="" loading="lazy" width="2000" height="126" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/10-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/10-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/10-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/10-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>where the first term in the difference is a linear combination of \(x_t\) and \(x_0\) that depends on the variance schedule \(\beta_t\). The exact form of this function is not relevant for our purposes, but it can be found in [<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">3</a>].</p><p>The significance of the above proportion is that <strong>the most straightforward parameterization of \( \mu_\theta \) simply predicts the diffusion posterior mean</strong>. Importantly, the authors of [<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">3</a>] actually found that training \(\mu_\theta\) to predict the <em>noise</em> component at any given timestep yields better results. In particular, let</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-16.png" alt="" loading="lazy" width="1211" height="216" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-16.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-16.png 1000w, https://www.assemblyai.com/blog/content/images/2022/05/image-16.png 1211w" sizes="(min-width: 720px) 720px"/></figure><p>where</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-17.png" alt="" loading="lazy" width="1348" height="166" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-17.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-17.png 1000w, https://www.assemblyai.com/blog/content/images/2022/05/image-17.png 1348w" sizes="(min-width: 720px) 720px"/></figure><p><strong>This leads to the following alternative loss function</strong>, which the authors of [<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">3</a>] found to lead to more stable training and better results:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-18.png" alt="" loading="lazy" width="1986" height="189" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-18.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-18.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/image-18.png 1600w, https://www.assemblyai.com/blog/content/images/2022/05/image-18.png 1986w" sizes="(min-width: 720px) 720px"/></figure><p>The authors of [<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">3</a>] also note connections of this formulation of Diffusion Models to score-matching generative models based on Langevin dynamics. Indeed, it appears that Diffusion Models and Score-Based models may be two sides of the same coin, akin to the independent and concurrent development of wave-based quantum mechanics and matrix-based quantum mechanics revealing two equivalent formulations of the same phenomena<sup>[<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">2</a>]</sup>.</p><!--kg-card-begin: html--><!--kg-card-end: html--><!--kg-card-begin: html--><p>
    <h4>Network Architecture</h4>
</p><!--kg-card-end: html--><p>While our simplified loss function seeks to train a model \( \pmb{\epsilon}_\theta \), we have still not yet defined the architecture of this model. Note that the <em>only</em> requirement for the model is that its input and output dimensionality are identical.</p><p>Given this restriction, it is perhaps unsurprising that image Diffusion Models are commonly implemented with U-Net-like architectures.</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-19.png" alt="" loading="lazy" width="1555" height="1036" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-19.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-19.png 1000w, https://www.assemblyai.com/blog/content/images/2022/05/image-19.png 1555w" sizes="(min-width: 720px) 720px"/><figcaption>Architecture of U-Net (<a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">source</a>)</figcaption></figure><!--kg-card-begin: html--><p>
    <h4>Reverse Process Decoder and \( L_0 \)</h4>
</p><!--kg-card-end: html--><p>The path along the reverse process consists of many transformations under continuous conditional Gaussian distributions. At the end of the reverse process, recall that we are trying to produce an <strong>image</strong>, which is composed of integer pixel values. Therefore, we must devise a way to obtain <strong>discrete (log) likelihoods</strong> for each possible pixel value across all pixels.</p><p>The way that this is done is by setting the last transition in the reverse diffusion chain to an <strong>independent discrete decoder</strong>. To determine the likelihood of a given image \(x_0\) given \(x_1\), we first impose independence between the data dimensions:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/12.png" alt="" loading="lazy" width="2000" height="147" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/12.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/12.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/12.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/12.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>where D is the dimensionality of the data and the superscript <em>i</em> indicates the extraction of one coordinate. The goal now is to determine how likely each integer value is for a given pixel <em>given</em> the distribution across possible values for the corresponding pixel in the slightly noised image at time \(t=1\):</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/13-1.png" alt="" loading="lazy" width="2000" height="130" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/13-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/13-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/13-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/13-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>where the pixel distributions for \(t=1\) are derived from the below multivariate Gaussian whose diagonal covariance matrix allows us to split the distribution into a product of univariate Gaussians, one for each dimension of the data:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/11-2.png" alt="" loading="lazy" width="2000" height="126" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/11-2.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/11-2.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/11-2.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/11-2.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>We assume that the images consist of integers in \({0, 1, ..., 255}\) (as standard RGB images do) which have been scaled linearly to \([-1, 1]\). We then break down the real line into small &#34;buckets&#34;, where, for a given scaled pixel value <em>x</em>, the bucket for that range is \([x-1/255, x+1/255]\). The probability of a pixel value <em>x,</em> given the univariate Gaussian distribution of the corresponding pixel in \(x_1\), is the <strong>area under that univariate Gaussian distribution within the bucket centered at <em>x</em></strong>. </p><p>Below you can see the area for each of these buckets with their probabilities for a mean-0 Gaussian which, in this context, corresponds to a distribution with an average pixel value of \(255/2\) (half brightness). The red curve represents the distribution of a specific pixel in the <em>t=1 </em>image, and the areas give the probability of the corresponding pixel value in the <em>t=0</em> image. </p><figure><div><video src="https://www.assemblyai.com/blog/content/media/2022/05/buckets_Trim.mp4" poster="https://img.spacergif.org/v1/2400x1168/0a/spacer.png" width="2400" height="1168" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video><div><div><p><span>0:00</span></p><p>/<span></span></p></div></div></div></figure><!--kg-card-begin: html--><div data-template="accordion">
  <header>
    <p>Technical Note</p>
  </header>
	<p>The first and final buckets extend out to -inf and +inf to preserve total probability.</p>
</div><!--kg-card-end: html--><p>Given a <em>t=0 </em>pixel value for each pixel, the value of \( p_\theta(x_0 | x_1) \) is simply their product. Succinctly, this process is succinctly encapsulated by the following equation:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/14-1.png" alt="" loading="lazy" width="2000" height="171" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/14-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/14-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/14-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/14-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>where</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/15.png" alt="" loading="lazy" width="2000" height="217" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/15.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/15.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/15.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/15.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>and</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/16.png" alt="" loading="lazy" width="2000" height="217" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/16.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/16.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/16.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/16.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>Given this equation for \( p_\theta(x_0 | x_1) \), we can calculate the final term of  \(L_{vlb}\) which is not formulated as a KL Divergence:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/6.1-1.png" alt="" loading="lazy" width="2000" height="109" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/6.1-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/6.1-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/6.1-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/6.1-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><h3 id="final-objective">Final Objective</h3><p>As mentioned in the last section, the authors of [<a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">3</a>] found that predicting the noise component of an image at a given time step produced the best results. Ultimately, they use the following objective:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/17-1.png" alt="" loading="lazy" width="2000" height="102" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/17-1.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/17-1.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/17-1.png 1600w, https://www.assemblyai.com/blog/content/images/size/w2400/2022/05/17-1.png 2400w" sizes="(min-width: 720px) 720px"/></figure><p>The training and sampling algorithms for our Diffusion Model therefore can be succinctly captured in the below figure:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-20.png" alt="" loading="lazy" width="2000" height="494" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-20.png 600w, https://www.assemblyai.com/blog/content/images/size/w1000/2022/05/image-20.png 1000w, https://www.assemblyai.com/blog/content/images/size/w1600/2022/05/image-20.png 1600w, https://www.assemblyai.com/blog/content/images/2022/05/image-20.png 2215w" sizes="(min-width: 720px) 720px"/><figcaption>(<a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>)</figcaption></figure><h3 id="diffusion-model-theory-summary">Diffusion Model Theory Summary</h3><p>In this section we took a detailed dive into the theory of Diffusion Models. It can be easy to get caught up in mathematical details, so we note the most important points within this section below in order to keep ourselves oriented from a birds-eye perspective:</p><ol><li>Our Diffusion Model is parameterized as a <strong>Markov chain</strong>, meaning that our latent variables \(x_1, ... , x_T\) depend only on the previous (or following) timestep.</li><li>The <strong>transition distributions </strong>in the Markov chain are <strong>Gaussian</strong>, where the forward process parameters require a variance schedule, and the reverse process parameters are learned.</li><li>The diffusion process ensures that \(x_T\) is <strong>asymptotically distributed as an isotropic Gaussian</strong> for sufficiently large T.</li><li>In our case, the <strong>variance schedule was fixed</strong>, but it can be learned as well. For fixed schedules, following a geometric progression may afford better results than a linear progression. In either case, the variances are generally increasing with time in the series (i.e. \(\beta_i &lt; \beta_j\) for \(i &lt; j\).</li><li>Diffusion Models are <strong>highly flexible</strong> and allow for <em>any </em>architecture whose input and output dimensionality are the same to be used. Many implementations use <strong>U-Net-like</strong> architectures.</li><li>The <strong>training objective </strong>is to<strong> </strong>maximize the likelihood<strong> </strong>of the training data. This is manifested as tuning the model parameters to <strong>minimize the variational upper bound of the negative log likelihood of the data</strong>.</li><li>Almost all terms in the objective function can be cast as <strong>KL Divergences</strong> as a result of our Markov assumption. These values <strong>become tenable to calculate</strong> given that we are using Gaussians, therefore omitting the need to perform Monte Carlo approximation.</li><li>Ultimately, using a <strong>simplified training objective</strong> to train a function which predicts the noise component of a given latent variable yields the best and most stable results.</li><li>A <strong>discrete decoder</strong> is used to obtain log likelihoods across pixel values as the last step in the reverse diffusion process.</li></ol><p>Whis high-level overview of Diffusion Models in our minds, let&#39;s move on to see how to use a Diffusion Models in PyTorch.</p><h2 id="diffusion-models-in-pytorch">Diffusion Models in PyTorch</h2><p>While Diffusion Models have not yet been democratized to the same degree as other older architectures/approaches in Machine Learning, there are still implementations available for use. The easiest way to use a Diffusion Model in PyTorch is to use the <code>denoising-diffusion-pytorch</code> package, which implements an image diffusion model like the one discussed in this article. To install the package, simply type the following command in the terminal:</p><pre><code>pip install denoising_diffusion_pytorch</code></pre><h3 id="minimal-example">Minimal Example</h3><p>To train a model and generate images, we first import the necessary packages:</p><pre><code>import torch
from denoising_diffusion_pytorch import Unet, GaussianDiffusion
</code></pre><p>Next, we define our network architecture, in this case a U-Net. The <code>dim</code> parameter specifies the number of feature maps before the first down-sampling, and the <code>dim_mults</code> parameter provides multiplicands for this value and successive down-samplings:</p><pre><code>model = Unet(
    dim = 64,
    dim_mults = (1, 2, 4, 8)
)</code></pre><p>Now that our network architecture is defined, we need to define the Diffusion Model itself. We pass in the U-Net model that we just defined along with several parameters - the size of images to generate, the number of timesteps in the diffusion process, and a choice between the L1 and L2 norms.</p><pre><code>diffusion = GaussianDiffusion(
    model,
    image_size = 128,
    timesteps = 1000,   # number of steps
    loss_type = &#39;l1&#39;    # L1 or L2
)</code></pre><p>Now that the Diffusion Model is defined, it&#39;s time to train. We generate random data to train on, and then train the Diffusion Model in the usual fashion:</p><pre><code>training_images = torch.randn(8, 3, 128, 128)
loss = diffusion(training_images)
loss.backward()
</code></pre><p>Once the model is trained, we can finally generate images by using the <code>sample()</code> method of the <code>diffusion</code> object. Here we generate 4 images, which are only noise given that our training data was random: </p><pre><code>sampled_images = diffusion.sample(batch_size = 4)</code></pre><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/image-21.png" alt="" loading="lazy" width="610" height="158" srcset="https://www.assemblyai.com/blog/content/images/size/w600/2022/05/image-21.png 600w, https://www.assemblyai.com/blog/content/images/2022/05/image-21.png 610w"/></figure><h3 id="training-on-custom-data">Training on Custom Data</h3><p>The <code>denoising-diffusion-pytorch</code> package also allow you to train a diffusion model on a specific dataset. Simply replace the <code>&#39;path/to/your/images&#39;</code> string with the dataset directory path in the <code>Trainer()</code> object below, and change <code>image_size</code> to the appropriate value. After that, simply run the code to train the model, and then sample as before. Note that PyTorch must be compiled with CUDA enabled in order to use the <code>Trainer</code> class:</p><pre><code>from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer

model = Unet(
    dim = 64,
    dim_mults = (1, 2, 4, 8)
).cuda()

diffusion = GaussianDiffusion(
    model,
    image_size = 128,
    timesteps = 1000,   # number of steps
    loss_type = &#39;l1&#39;    # L1 or L2
).cuda()

trainer = Trainer(
    diffusion,
    &#39;path/to/your/images&#39;,
    train_batch_size = 32,
    train_lr = 2e-5,
    train_num_steps = 700000,         # total training steps
    gradient_accumulate_every = 2,    # gradient accumulation steps
    ema_decay = 0.995,                # exponential moving average decay
    amp = True                        # turn on mixed precision
)

trainer.train()</code></pre><p>Below you can see progressive denoising from multivariate Gaussian noise to MNIST digits akin to reverse diffusion:</p><figure><img src="https://www.assemblyai.com/blog/content/images/2022/05/movie.gif" alt="" loading="lazy" width="122" height="122"/></figure><h2 id="final-words">Final Words</h2><p>Diffusion Models are a conceptually simple and elegant approach to the problem of generating data. Their State-of-the-Art results combined with non-adversarial training has propelled them to great heights, and further improvements can be expected in the coming years given their nascent status. In particular, Diffusion Models have been found to be essential to the performance of cutting-edge models like <a href="https://www.assemblyai.com/blog/how-dall-e-2-actually-works/">DALL-E 2</a>.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h2 id="references">References</h2><p>[1] <a href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a></p><p>[2] <a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a></p><p>[3] <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></p><p>[4] <a href="https://arxiv.org/abs/2006.09011">Improved Techniques for Training Score-Based Generative Models </a></p><p>[5] <a href="https://arxiv.org/abs/2102.09672">Improved Denoising Diffusion Probabilistic Models</a></p><p>[6] <a href="https://arxiv.org/abs/2105.05233">Diffusion Models Beat GANs on Image Synthesis</a></p><p>[7] <a href="https://arxiv.org/abs/2112.10741">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</a></p><p>[8] <a href="https://arxiv.org/abs/2204.06125">Hierarchical Text-Conditional Image Generation with CLIP Latents</a></p>
    </section></div>
  </body>
</html>
