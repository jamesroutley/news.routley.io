<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://news.ycombinator.com/item?id=42307393">Original</a>
    <h1>Launch HN: Vocera (YC F24) – Testing and Observability for Voice AI</h1>
    
    <div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN, we’re Shashij, Sidhant, and Tarush, founders of Vocera AI (<a href="https://www.vocera.ai">https://www.vocera.ai</a>) – a platform that automates the testing and monitoring of AI voice agents. We’re building the product we wish we had when we were developing voice agents. Here’s a demo video: <a href="https://www.youtube.com/watch?v=aRtAb_E60jY" rel="nofollow">https://www.youtube.com/watch?v=aRtAb_E60jY</a>. You can engage in a roleplay as well which we made just for fun: <a href="https://www.vocera.ai/talk">https://www.vocera.ai/talk</a>.</p><p>We were working on voice agents in healthcare, and kept running into the same problem: manually testing was incredibly time-consuming and error-prone. Testing voice AI in a comprehensive way was far more difficult than we had anticipated – not just the setup, but the ongoing monitoring of production calls. Despite our best efforts, some calls still failed once we went live.</p><p>The main challenges we faced were: (1) Demonstrating reliability to customers for production was really tough; (2) Manual testing was incomplete and didn&#39;t cover edge cases; (3) We couldn’t easily simulate all possible conversations, especially with diverse customer personas; (4) Monitoring every production call manually was a huge time sink.</p><p>We built Vocera to solve these problems. Vocera automatically simulates real personas, generates a wide range of testing scenarios from your prompts/call scripts and monitors all production calls. The result? You can be sure your voice agents are reliable, and you get real-time insights into how they’re performing.</p><p>Our platform tests how your AI responds to diverse personas, evaluates the conversation against different metrics and gives you directed feedback on the issues.</p><p>What’s different about us is that we don’t just automate the evaluation. We generate scenarios and metrics automatically, so developers do not have to spend time defining their scenarios or eval metrics. This saves them a ton of time. Obviously, we give them the option to define these manually as well. Also, we provide detailed analytics on the agent&#39;s performance across simulations so developers do not need to listen to all call recordings manually.</p><p>If you’re building voice agents and want to ensure they’re reliable and production-ready, or if you’re just interested in the challenges of Voice AI, we’d love to chat.</p><p>We’d love to get your feedback, thoughts, or experiences related to testing voice agents!</p></div></td></div></div>
  </body>
</html>
