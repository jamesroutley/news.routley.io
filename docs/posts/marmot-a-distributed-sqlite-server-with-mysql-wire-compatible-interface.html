<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/maxpert/marmot">Original</a>
    <h1>Marmot – A distributed SQLite server with MySQL wire compatible interface</h1>
    
    <div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://goreportcard.com/report/github.com/maxpert/marmot" rel="nofollow"><img src="https://camo.githubusercontent.com/a9cf99f212f56b414c865728a2659a62b2129d1edcf479e5a53d1366dc2def36/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6d6178706572742f6d61726d6f74" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/maxpert/marmot"/></a>
<a href="https://discord.gg/AWUwY66XsE" rel="nofollow"><img src="https://camo.githubusercontent.com/dfd80a5843107ada4de21a02a89c0bb029f240e404f38ee2b101e784685e0e8e/68747470733a2f2f62616467656e2e6e65742f62616467652f69636f6e2f646973636f72643f69636f6e3d646973636f7264266c6162656c3d4d61726d6f74" alt="Discord" data-canonical-src="https://badgen.net/badge/icon/discord?icon=discord&amp;label=Marmot"/></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d665d92a54ac639b00283b01653885ccfb206dab2d80fd32a1674608b7a6c03/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6178706572742f6d61726d6f74"><img src="https://camo.githubusercontent.com/7d665d92a54ac639b00283b01653885ccfb206dab2d80fd32a1674608b7a6c03/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6178706572742f6d61726d6f74" alt="GitHub" data-canonical-src="https://img.shields.io/github/license/maxpert/marmot"/></a></p>

<p dir="auto">Marmot v2 is a leaderless, distributed SQLite replication system built on a gossip-based protocol with distributed transactions and eventual consistency.</p>
<p dir="auto"><strong>Key Features:</strong></p>
<ul dir="auto">
<li><strong>Leaderless Architecture</strong>: No single point of failure - any node can accept writes</li>
<li><strong>MySQL Protocol Compatible</strong>: Connect with any MySQL client (DBeaver, MySQL Workbench, mysql CLI)</li>
<li><strong>Distributed Transactions</strong>: Percolator-style write intents with conflict detection</li>
<li><strong>Multi-Database Support</strong>: Create and manage multiple databases per cluster</li>
<li><strong>DDL Replication</strong>: Distributed schema changes with automatic idempotency and cluster-wide locking</li>
<li><strong>Production-Ready SQL Parser</strong>: Powered by rqlite/sql AST parser for MySQL→SQLite transpilation</li>
<li><strong>CDC-Based Replication</strong>: Row-level change data capture for consistent replication</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="# Start a single-node cluster
./marmot-v2

# Connect with MySQL client
mysql -h localhost -P 3306 -u root

# Or use DBeaver, MySQL Workbench, etc."><pre><span><span>#</span> Start a single-node cluster</span>
./marmot-v2

<span><span>#</span> Connect with MySQL client</span>
mysql -h localhost -P 3306 -u root

<span><span>#</span> Or use DBeaver, MySQL Workbench, etc.</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="# Test DDL and DML replication across a 2-node cluster
./scripts/test-ddl-replication.sh

# This script will:
# 1. Start a 2-node cluster
# 2. Create a table on node 1 and verify it replicates to node 2
# 3. Insert data on node 1 and verify it replicates to node 2
# 4. Update data on node 2 and verify it replicates to node 1
# 5. Delete data on node 1 and verify it replicates to node 2

# Manual cluster testing
./examples/start-seed.sh              # Start seed node (port 8081, mysql 3307)
./examples/join-cluster.sh 2 localhost:8081  # Join node 2 (port 8082, mysql 3308)
./examples/join-cluster.sh 3 localhost:8081  # Join node 3 (port 8083, mysql 3309)

# Connect to any node and run queries
mysql --protocol=TCP -h localhost -P 3307 -u root
mysql --protocol=TCP -h localhost -P 3308 -u root

# Cleanup
pkill -f marmot-v2"><pre><span><span>#</span> Test DDL and DML replication across a 2-node cluster</span>
./scripts/test-ddl-replication.sh

<span><span>#</span> This script will:</span>
<span><span>#</span> 1. Start a 2-node cluster</span>
<span><span>#</span> 2. Create a table on node 1 and verify it replicates to node 2</span>
<span><span>#</span> 3. Insert data on node 1 and verify it replicates to node 2</span>
<span><span>#</span> 4. Update data on node 2 and verify it replicates to node 1</span>
<span><span>#</span> 5. Delete data on node 1 and verify it replicates to node 2</span>

<span><span>#</span> Manual cluster testing</span>
./examples/start-seed.sh              <span><span>#</span> Start seed node (port 8081, mysql 3307)</span>
./examples/join-cluster.sh 2 localhost:8081  <span><span>#</span> Join node 2 (port 8082, mysql 3308)</span>
./examples/join-cluster.sh 3 localhost:8081  <span><span>#</span> Join node 3 (port 8083, mysql 3309)</span>

<span><span>#</span> Connect to any node and run queries</span>
mysql --protocol=TCP -h localhost -P 3307 -u root
mysql --protocol=TCP -h localhost -P 3308 -u root

<span><span>#</span> Cleanup</span>
pkill -f marmot-v2</pre></div>

<p dir="auto"><a href="https://starchart.cc/maxpert/marmot" rel="nofollow"><img src="https://camo.githubusercontent.com/ee99a4dc41e48fb1f38d51af628abb0ae1fd696beb5ccd37d3dfa28e7377565f/68747470733a2f2f7374617263686172742e63632f6d6178706572742f6d61726d6f742e7376673f76617269616e743d6164617074697665" alt="Stargazers over time" data-canonical-src="https://starchart.cc/maxpert/marmot.svg?variant=adaptive"/></a></p>

<p dir="auto">Marmot v2 uses a fundamentally different architecture from other SQLite replication solutions:</p>
<p dir="auto"><strong>vs. rqlite/dqlite/LiteFS:</strong></p>
<ul dir="auto">
<li>❌ They require a primary node for all writes</li>
<li>✅ Marmot allows writes on <strong>any node</strong></li>
<li>❌ They use leader election (Raft)</li>
<li>✅ Marmot uses <strong>gossip protocol</strong> (no leader)</li>
<li>❌ They require proxy layer or page-level interception</li>
<li>✅ Marmot uses <strong>MySQL protocol</strong> for direct database access</li>
</ul>
<p dir="auto"><strong>How It Works:</strong></p>
<ol dir="auto">
<li><strong>Write Coordination</strong>: 2PC (Two-Phase Commit) with configurable consistency (ONE, QUORUM, ALL)</li>
<li><strong>Conflict Resolution</strong>: Last-Write-Wins (LWW) with HLC timestamps</li>
<li><strong>Cluster Membership</strong>: SWIM-style gossip with failure detection</li>
<li><strong>Data Replication</strong>: Full database replication - all nodes receive all data</li>
<li><strong>DDL Replication</strong>: Cluster-wide schema changes with automatic idempotency</li>
</ol>

<p dir="auto">Marmot v2 supports <strong>distributed DDL (Data Definition Language) replication</strong> without requiring master election:</p>

<ol dir="auto">
<li>
<p dir="auto"><strong>Cluster-Wide Locking</strong>: Each DDL operation acquires a distributed lock per database (default: 30-second lease)</p>
<ul dir="auto">
<li>Prevents concurrent schema changes on the same database</li>
<li>Locks automatically expire if a node crashes</li>
<li>Different databases can have concurrent DDL operations</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Automatic Idempotency</strong>: DDL statements are automatically rewritten for safe replay</p>
<div dir="auto" data-snippet-clipboard-copy-content="CREATE TABLE users (id INT)
→ CREATE TABLE IF NOT EXISTS users (id INT)

DROP TABLE users
→ DROP TABLE IF EXISTS users"><pre><span>CREATE</span> <span>TABLE</span> <span>users</span> (id <span>INT</span>)
→ CREATE TABLE IF NOT EXISTS users (id <span>INT</span>)

<span>DROP</span> <span>TABLE</span> users
→ <span>DROP</span> <span>TABLE</span> <span>IF</span> EXISTS users</pre></div>
</li>
<li>
<p dir="auto"><strong>Schema Version Tracking</strong>: Each database maintains a schema version counter</p>
<ul dir="auto">
<li>Incremented on every DDL operation</li>
<li>Exchanged via gossip protocol for drift detection</li>
<li>Used by delta sync to validate transaction applicability</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Quorum-Based Replication</strong>: DDL replicates like DML through the same 2PC mechanism</p>
<ul dir="auto">
<li>No special master node needed</li>
<li>Works with existing consistency levels (QUORUM, ALL, etc.)</li>
</ul>
</li>
</ol>

<div dir="auto" data-snippet-clipboard-copy-content="[ddl]
# DDL lock lease duration (seconds)
lock_lease_seconds = 30

# Automatically rewrite DDL for idempotency
enable_idempotent = true"><pre>[<span>ddl</span>]
<span><span>#</span> DDL lock lease duration (seconds)</span>
<span>lock_lease_seconds</span> = <span>30</span>

<span><span>#</span> Automatically rewrite DDL for idempotency</span>
<span>enable_idempotent</span> = <span>true</span></pre></div>

<ul dir="auto">
<li>✅ <strong>Do</strong>: Execute DDL from a single connection/node at a time</li>
<li>✅ <strong>Do</strong>: Use qualified table names (<code>mydb.users</code> instead of <code>users</code>)</li>
<li><g-emoji alias="warning">⚠️</g-emoji> <strong>Caution</strong>: ALTER TABLE is less idempotent - avoid replaying failed ALTER operations</li>
<li>❌ <strong>Don&#39;t</strong>: Run concurrent DDL on the same database from multiple nodes</li>
</ul>

<p dir="auto">Marmot v2 uses <strong>Change Data Capture (CDC)</strong> for replication instead of SQL statement replay:</p>

<ol dir="auto">
<li><strong>Row-Level Capture</strong>: Instead of replicating SQL statements, Marmot captures the actual row data changes (INSERT/UPDATE/DELETE)</li>
<li><strong>Binary Data Format</strong>: Row data is serialized as CDC messages with column values, ensuring consistent replication regardless of SQL dialect</li>
<li><strong>Deterministic Application</strong>: Row data is applied directly to the target database, avoiding parsing ambiguities</li>
</ol>

<ul dir="auto">
<li><strong>Consistency</strong>: Same row data applied everywhere, no SQL parsing differences</li>
<li><strong>Performance</strong>: Binary format is more efficient than SQL text</li>
<li><strong>Reliability</strong>: No issues with SQL syntax variations between MySQL and SQLite</li>
</ul>

<p dir="auto">For UPDATE and DELETE operations, Marmot automatically extracts row keys:</p>
<ul dir="auto">
<li>Uses PRIMARY KEY columns when available</li>
<li>Falls back to ROWID for tables without explicit primary key</li>
<li>Handles composite primary keys correctly</li>
</ul>

<p dir="auto">Marmot can publish CDC events to external messaging systems, enabling real-time data pipelines, analytics, and event-driven architectures. Events follow the <strong><a href="https://debezium.io/" rel="nofollow">Debezium</a> specification</strong> for maximum compatibility with existing CDC tooling.</p>

<ul dir="auto">
<li><strong>Debezium-Compatible Format</strong>: Events conform to the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-events" rel="nofollow">Debezium event structure</a>, compatible with Kafka Connect, Flink, Spark, and other CDC consumers</li>
<li><strong>Multi-Sink Support</strong>: Publish to multiple destinations simultaneously (Kafka, NATS)</li>
<li><strong>Glob-Based Filtering</strong>: Filter which tables and databases to publish</li>
<li><strong>Automatic Retry</strong>: Exponential backoff with configurable limits</li>
<li><strong>Persistent Cursors</strong>: Survives restarts without losing position</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="[publisher]
enabled = true

[[publisher.sinks]]
name = &#34;kafka-main&#34;
type = &#34;kafka&#34;                    # &#34;kafka&#34; or &#34;nats&#34;
format = &#34;debezium&#34;               # Debezium-compatible JSON format
brokers = [&#34;localhost:9092&#34;]      # Kafka broker addresses
topic_prefix = &#34;marmot.cdc&#34;       # Topics: {prefix}.{database}.{table}
filter_tables = [&#34;*&#34;]             # Glob patterns (e.g., &#34;users&#34;, &#34;order_*&#34;)
filter_databases = [&#34;*&#34;]          # Glob patterns (e.g., &#34;prod_*&#34;)
batch_size = 100                  # Events per poll cycle
poll_interval_ms = 10             # Polling interval

# NATS sink example
[[publisher.sinks]]
name = &#34;nats-events&#34;
type = &#34;nats&#34;
format = &#34;debezium&#34;
nats_url = &#34;nats://localhost:4222&#34;
topic_prefix = &#34;marmot.cdc&#34;
filter_tables = [&#34;*&#34;]
filter_databases = [&#34;*&#34;]"><pre>[<span>publisher</span>]
<span>enabled</span> = <span>true</span>

[[<span>publisher</span>.<span>sinks</span>]]
<span>name</span> = <span><span>&#34;</span>kafka-main<span>&#34;</span></span>
<span>type</span> = <span><span>&#34;</span>kafka<span>&#34;</span></span>                    <span><span>#</span> &#34;kafka&#34; or &#34;nats&#34;</span>
<span>format</span> = <span><span>&#34;</span>debezium<span>&#34;</span></span>               <span><span>#</span> Debezium-compatible JSON format</span>
<span>brokers</span> = [<span><span>&#34;</span>localhost:9092<span>&#34;</span></span>]      <span><span>#</span> Kafka broker addresses</span>
<span>topic_prefix</span> = <span><span>&#34;</span>marmot.cdc<span>&#34;</span></span>       <span><span>#</span> Topics: {prefix}.{database}.{table}</span>
<span>filter_tables</span> = [<span><span>&#34;</span>*<span>&#34;</span></span>]             <span><span>#</span> Glob patterns (e.g., &#34;users&#34;, &#34;order_*&#34;)</span>
<span>filter_databases</span> = [<span><span>&#34;</span>*<span>&#34;</span></span>]          <span><span>#</span> Glob patterns (e.g., &#34;prod_*&#34;)</span>
<span>batch_size</span> = <span>100</span>                  <span><span>#</span> Events per poll cycle</span>
<span>poll_interval_ms</span> = <span>10</span>             <span><span>#</span> Polling interval</span>

<span><span>#</span> NATS sink example</span>
[[<span>publisher</span>.<span>sinks</span>]]
<span>name</span> = <span><span>&#34;</span>nats-events<span>&#34;</span></span>
<span>type</span> = <span><span>&#34;</span>nats<span>&#34;</span></span>
<span>format</span> = <span><span>&#34;</span>debezium<span>&#34;</span></span>
<span>nats_url</span> = <span><span>&#34;</span>nats://localhost:4222<span>&#34;</span></span>
<span>topic_prefix</span> = <span><span>&#34;</span>marmot.cdc<span>&#34;</span></span>
<span>filter_tables</span> = [<span><span>&#34;</span>*<span>&#34;</span></span>]
<span>filter_databases</span> = [<span><span>&#34;</span>*<span>&#34;</span></span>]</pre></div>

<p dir="auto">Events follow the <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-change-events-value" rel="nofollow">Debezium envelope structure</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &#34;schema&#34;: { ... },
  &#34;payload&#34;: {
    &#34;before&#34;: null,
    &#34;after&#34;: {&#34;id&#34;: 1, &#34;name&#34;: &#34;alice&#34;, &#34;email&#34;: &#34;alice@example.com&#34;},
    &#34;source&#34;: {
      &#34;version&#34;: &#34;2.0.0&#34;,
      &#34;connector&#34;: &#34;marmot&#34;,
      &#34;name&#34;: &#34;marmot&#34;,
      &#34;ts_ms&#34;: 1702500000000,
      &#34;db&#34;: &#34;myapp&#34;,
      &#34;table&#34;: &#34;users&#34;
    },
    &#34;op&#34;: &#34;c&#34;,
    &#34;ts_ms&#34;: 1702500000000
  }
}"><pre>{
  <span>&#34;schema&#34;</span>: { <span>... </span>},
  <span>&#34;payload&#34;</span>: {
    <span>&#34;before&#34;</span>: <span>null</span>,
    <span>&#34;after&#34;</span>: {<span>&#34;id&#34;</span>: <span>1</span>, <span>&#34;name&#34;</span>: <span><span>&#34;</span>alice<span>&#34;</span></span>, <span>&#34;email&#34;</span>: <span><span>&#34;</span>alice@example.com<span>&#34;</span></span>},
    <span>&#34;source&#34;</span>: {
      <span>&#34;version&#34;</span>: <span><span>&#34;</span>2.0.0<span>&#34;</span></span>,
      <span>&#34;connector&#34;</span>: <span><span>&#34;</span>marmot<span>&#34;</span></span>,
      <span>&#34;name&#34;</span>: <span><span>&#34;</span>marmot<span>&#34;</span></span>,
      <span>&#34;ts_ms&#34;</span>: <span>1702500000000</span>,
      <span>&#34;db&#34;</span>: <span><span>&#34;</span>myapp<span>&#34;</span></span>,
      <span>&#34;table&#34;</span>: <span><span>&#34;</span>users<span>&#34;</span></span>
    },
    <span>&#34;op&#34;</span>: <span><span>&#34;</span>c<span>&#34;</span></span>,
    <span>&#34;ts_ms&#34;</span>: <span>1702500000000</span>
  }
}</pre></div>
<p dir="auto"><strong>Operation Types</strong> (per <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-create-events" rel="nofollow">Debezium spec</a>):</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Operation</th>
<th><code>op</code></th>
<th><code>before</code></th>
<th><code>after</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>INSERT</td>
<td><code>c</code> (create)</td>
<td><code>null</code></td>
<td>row data</td>
</tr>
<tr>
<td>UPDATE</td>
<td><code>u</code> (update)</td>
<td>old row</td>
<td>new row</td>
</tr>
<tr>
<td>DELETE</td>
<td><code>d</code> (delete)</td>
<td>old row</td>
<td><code>null</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">Topics follow the pattern: <code>{topic_prefix}.{database}.{table}</code></p>
<p dir="auto">Examples:</p>
<ul dir="auto">
<li><code>marmot.cdc.myapp.users</code></li>
<li><code>marmot.cdc.myapp.orders</code></li>
<li><code>marmot.cdc.analytics.events</code></li>
</ul>

<ul dir="auto">
<li><strong>Real-Time Analytics</strong>: Stream changes to data warehouses (Snowflake, BigQuery, ClickHouse)</li>
<li><strong>Event-Driven Microservices</strong>: Trigger actions on data changes</li>
<li><strong>Cache Invalidation</strong>: Keep caches in sync with database changes</li>
<li><strong>Audit Logging</strong>: Capture all changes for compliance</li>
<li><strong>Search Indexing</strong>: Keep Elasticsearch/Algolia in sync</li>
</ul>
<p dir="auto">For more details, see the <a href="https://maxpert.github.io/marmot/integrations" rel="nofollow">Integrations documentation</a>.</p>
<div dir="auto"><h2 tabindex="-1" dir="auto">SQL Statement Compatibility</h2><a id="user-content-sql-statement-compatibility" aria-label="Permalink: SQL Statement Compatibility" href="#sql-statement-compatibility"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Marmot supports a wide range of MySQL/SQLite statements through its MySQL protocol server. The following table shows compatibility for different statement types:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Statement Type</th>
<th>Support</th>
<th>Replication</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DML - Data Manipulation</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>INSERT</code> / <code>REPLACE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Includes qualified table names (db.table)</td>
</tr>
<tr>
<td><code>UPDATE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Includes qualified table names</td>
</tr>
<tr>
<td><code>DELETE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Includes qualified table names</td>
</tr>
<tr>
<td><code>SELECT</code></td>
<td>✅ Full</td>
<td>N/A</td>
<td>Read operations</td>
</tr>
<tr>
<td><code>LOAD DATA</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Bulk data loading</td>
</tr>
<tr>
<td><strong>DDL - Data Definition</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>CREATE TABLE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>ALTER TABLE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>DROP TABLE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>TRUNCATE TABLE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td></td>
</tr>
<tr>
<td><code>RENAME TABLE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>CREATE/DROP INDEX</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>CREATE/DROP VIEW</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>CREATE/DROP TRIGGER</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><strong>Database Management</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>CREATE DATABASE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>DROP DATABASE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>ALTER DATABASE</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Replicated with cluster-wide locking</td>
</tr>
<tr>
<td><code>SHOW DATABASES</code></td>
<td>✅ Full</td>
<td>N/A</td>
<td>Metadata query</td>
</tr>
<tr>
<td><code>SHOW TABLES</code></td>
<td>✅ Full</td>
<td>N/A</td>
<td>Metadata query</td>
</tr>
<tr>
<td><code>USE database</code></td>
<td>✅ Full</td>
<td>N/A</td>
<td>Session state</td>
</tr>
<tr>
<td><strong>Transaction Control</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>BEGIN</code> / <code>START TRANSACTION</code></td>
<td>✅ Full</td>
<td>N/A</td>
<td>Transaction boundary</td>
</tr>
<tr>
<td><code>COMMIT</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Commits distributed transaction</td>
</tr>
<tr>
<td><code>ROLLBACK</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Aborts distributed transaction</td>
</tr>
<tr>
<td><code>SAVEPOINT</code></td>
<td>✅ Full</td>
<td>✅ Yes</td>
<td>Nested transaction support</td>
</tr>
<tr>
<td><strong>Locking</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>LOCK TABLES</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>Requires distributed locking coordination</td>
</tr>
<tr>
<td><code>UNLOCK TABLES</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>Requires distributed locking coordination</td>
</tr>
<tr>
<td><strong>Session Configuration</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>SET</code> statements</td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>Session-local, not replicated</td>
</tr>
<tr>
<td><strong>XA Transactions</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>XA START/END/PREPARE</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>Marmot uses its own 2PC protocol</td>
</tr>
<tr>
<td><code>XA COMMIT/ROLLBACK</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>Not compatible with Marmot&#39;s model</td>
</tr>
<tr>
<td><strong>DCL - Data Control</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>GRANT</code> / <code>REVOKE</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>User management not replicated</td>
</tr>
<tr>
<td><code>CREATE/DROP USER</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>User management not replicated</td>
</tr>
<tr>
<td><code>ALTER USER</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>User management not replicated</td>
</tr>
<tr>
<td><strong>Administrative</strong></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>OPTIMIZE TABLE</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>Node-local administrative command</td>
</tr>
<tr>
<td><code>REPAIR TABLE</code></td>
<td>✅ Parsed</td>
<td>❌ No</td>
<td>Node-local administrative command</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<ul dir="auto">
<li>✅ <strong>Full</strong>: Fully supported and working</li>
<li>✅ <strong>Parsed</strong>: Statement is parsed and recognized</li>
<li><g-emoji alias="warning">⚠️</g-emoji> <strong>Limited</strong>: Works but has limitations in distributed context</li>
<li>❌ <strong>No</strong>: Not supported or not replicated</li>
<li><strong>N/A</strong>: Not applicable (read-only or session-local)</li>
</ul>

<ol dir="auto">
<li>
<p dir="auto"><strong>Schema Changes (DDL)</strong>: DDL statements are fully replicated with cluster-wide locking and automatic idempotency. See the DDL Replication section for details.</p>
</li>
<li>
<p dir="auto"><strong>XA Transactions</strong>: Marmot has its own distributed transaction protocol based on 2PC. MySQL XA transactions are not compatible with Marmot&#39;s replication model.</p>
</li>
<li>
<p dir="auto"><strong>User Management (DCL)</strong>: User and privilege management statements are local to each node. For production deployments, consider handling authentication at the application or proxy level.</p>
</li>
<li>
<p dir="auto"><strong>Table Locking</strong>: <code>LOCK TABLES</code> statements are recognized but not enforced across the cluster. Use application-level coordination for distributed locking needs.</p>
</li>
<li>
<p dir="auto"><strong>Qualified Names</strong>: Marmot fully supports qualified table names (e.g., <code>db.table</code>) in DML and DDL operations.</p>
</li>
</ol>
<div dir="auto"><h2 tabindex="-1" dir="auto">MySQL Protocol &amp; Metadata Queries</h2><a id="user-content-mysql-protocol--metadata-queries" aria-label="Permalink: MySQL Protocol &amp; Metadata Queries" href="#mysql-protocol--metadata-queries"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Marmot includes a MySQL-compatible protocol server, allowing you to connect using any MySQL client (DBeaver, MySQL Workbench, mysql CLI, etc.). The server supports:</p>

<p dir="auto">Marmot provides full support for MySQL metadata queries, enabling GUI tools like DBeaver to browse databases, tables, and columns:</p>
<ul dir="auto">
<li><strong>SHOW Commands</strong>: <code>SHOW DATABASES</code>, <code>SHOW TABLES</code>, <code>SHOW COLUMNS FROM table</code>, <code>SHOW CREATE TABLE</code>, <code>SHOW INDEXES</code></li>
<li><strong>INFORMATION_SCHEMA</strong>: Queries against <code>INFORMATION_SCHEMA.TABLES</code>, <code>INFORMATION_SCHEMA.COLUMNS</code>, <code>INFORMATION_SCHEMA.SCHEMATA</code>, and <code>INFORMATION_SCHEMA.STATISTICS</code></li>
<li><strong>Type Conversion</strong>: Automatic SQLite-to-MySQL type mapping for compatibility</li>
</ul>
<p dir="auto">These metadata queries are powered by the <strong>rqlite/sql AST parser</strong>, providing production-grade MySQL query compatibility.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Connecting with MySQL Clients</h3><a id="user-content-connecting-with-mysql-clients" aria-label="Permalink: Connecting with MySQL Clients" href="#connecting-with-mysql-clients"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Using mysql CLI
mysql -h localhost -P 3306 -u root

# Connection string for applications
mysql://root@localhost:3306/marmot"><pre><span><span>#</span> Using mysql CLI</span>
mysql -h localhost -P 3306 -u root

<span><span>#</span> Connection string for applications</span>
mysql://root@localhost:3306/marmot</pre></div>

<p dir="auto">Marmot handles various failure and recovery scenarios automatically:</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">Network Partition (Split-Brain)</h3><a id="user-content-network-partition-split-brain" aria-label="Permalink: Network Partition (Split-Brain)" href="#network-partition-split-brain"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Scenario</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Minority partition</strong></td>
<td>Writes <strong>fail</strong> - cannot achieve quorum</td>
</tr>
<tr>
<td><strong>Majority partition</strong></td>
<td>Writes <strong>succeed</strong> - quorum achieved</td>
</tr>
<tr>
<td><strong>Partition heals</strong></td>
<td>Delta sync + LWW merges divergent data</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>How it works:</strong></p>
<ol dir="auto">
<li>During partition, only the majority side can commit writes (quorum enforcement)</li>
<li>When partition heals, nodes exchange transaction logs via <code>StreamChanges</code> RPC</li>
<li>Conflicts resolved using Last-Writer-Wins (LWW) with HLC timestamps</li>
<li>Higher node ID breaks ties for simultaneous writes</li>
</ol>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Scenario</th>
<th>Recovery Method</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Brief outage</strong></td>
<td>Delta sync - replay missed transactions</td>
</tr>
<tr>
<td><strong>Extended outage</strong></td>
<td>Snapshot transfer + delta sync</td>
</tr>
<tr>
<td><strong>New node joining</strong></td>
<td>Full snapshot from existing node</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Anti-Entropy Background Process:</strong></p>
<p dir="auto">Marmot v2 includes an automatic anti-entropy system that continuously monitors and repairs replication lag across the cluster:</p>
<ol dir="auto">
<li><strong>Lag Detection</strong>: Every 60 seconds (configurable), each node queries peers for their replication state</li>
<li><strong>Smart Recovery Decision</strong>:
<ul dir="auto">
<li><strong>Delta Sync</strong> if lag &lt; 10,000 transactions AND &lt; 1 hour: Streams missed transactions incrementally</li>
<li><strong>Snapshot Transfer</strong> if lag exceeds thresholds: Full database file transfer for efficiency</li>
</ul>
</li>
<li><strong>Gap Detection</strong>: Detects when transaction logs have been GC&#39;d and automatically falls back to snapshot</li>
<li><strong>Multi-Database Support</strong>: Tracks and syncs each database independently</li>
<li><strong>GC Coordination</strong>: Garbage collection respects peer replication state - logs aren&#39;t deleted until all peers have applied them</li>
</ol>
<p dir="auto"><strong>Delta Sync Process:</strong></p>
<ol dir="auto">
<li>Lagging node queries <code>last_applied_txn_id</code> for each peer/database</li>
<li>Requests transactions since that ID via <code>StreamChanges</code> RPC</li>
<li><strong>Gap Detection</strong>: Checks if first received txn_id has a large gap from requested ID
<ul dir="auto">
<li>If gap &gt; delta_sync_threshold_txns, indicates missing (GC&#39;d) transactions</li>
<li>Automatically falls back to snapshot transfer to prevent data loss</li>
</ul>
</li>
<li>Applies changes using LWW conflict resolution</li>
<li>Updates replication state tracking (per-database)</li>
<li>Progress logged every 100 transactions</li>
</ol>
<p dir="auto"><strong>GC Coordination with Anti-Entropy:</strong></p>
<ul dir="auto">
<li>Transaction logs are retained with a two-tier policy:
<ul dir="auto">
<li><strong>Min retention</strong> (2 hours): Must be &gt;= delta sync threshold, respects peer lag</li>
<li><strong>Max retention</strong> (24 hours): Force delete after this time to prevent unbounded growth</li>
</ul>
</li>
<li>Config validation enforces: <code>gc_min &gt;= delta_threshold</code> and <code>gc_max &gt;= 2x delta_threshold</code></li>
<li>Each database tracks replication progress per peer</li>
<li>GC queries minimum applied txn_id across all peers before cleanup</li>
<li><strong>Gap detection</strong> prevents data loss if GC runs while nodes are offline</li>
</ul>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Write Consistency</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ONE</code></td>
<td>Returns after 1 node ACK (fast, less durable)</td>
</tr>
<tr>
<td><code>QUORUM</code></td>
<td>Returns after majority ACK (default, balanced)</td>
</tr>
<tr>
<td><code>ALL</code></td>
<td>Returns after all nodes ACK (slow, most durable)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Conflict Resolution:</strong></p>
<ul dir="auto">
<li>All conflicts resolved via LWW using HLC timestamps</li>
<li>No data loss - later write always wins deterministically</li>
<li>Tie-breaker: higher node ID wins for equal timestamps</li>
</ul>

<ul dir="auto">
<li><strong>Selective Table Watching</strong>: All tables in a database are replicated. Selective table replication is not supported.</li>
<li><strong>WAL Mode Required</strong>: SQLite must use WAL mode for reliable multi-process changes.</li>
<li><strong>Eventually Consistent</strong>: Rows may sync out of order. <code>SERIALIZABLE</code> transaction assumptions may not hold across nodes.</li>
<li><strong>Concurrent DDL</strong>: Avoid running concurrent DDL operations on the same database from multiple nodes (protected by cluster-wide lock with 30s lease).</li>
</ul>

<div dir="auto"><h3 tabindex="-1" dir="auto">AUTO_INCREMENT and Integer Types</h3><a id="user-content-auto_increment-and-integer-types" aria-label="Permalink: AUTO_INCREMENT and Integer Types" href="#auto_increment-and-integer-types"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<blockquote>
<p dir="auto"><strong>IMPORTANT: Marmot automatically converts <code>INT AUTO_INCREMENT</code> to <code>BIGINT</code></strong></p>
<p dir="auto">This is a <strong>breaking change</strong> from standard MySQL/SQLite behavior. Marmot does not respect 32-bit <code>INT</code> for auto-increment columns - they are automatically promoted to <code>BIGINT</code> to support distributed ID generation.</p>
</blockquote>
<p dir="auto"><strong>Why?</strong></p>
<p dir="auto">In a distributed, leaderless system, each node must generate unique IDs independently without coordination. Marmot uses HLC-based (Hybrid Logical Clock) 64-bit IDs to ensure:</p>
<ol dir="auto">
<li><strong>Global Uniqueness</strong>: IDs are unique across all nodes without central coordination</li>
<li><strong>Monotonicity</strong>: IDs increase over time (within each node)</li>
<li><strong>No Collisions</strong>: Unlike auto-increment sequences, HLC IDs cannot collide between nodes</li>
</ol>
<p dir="auto"><strong>How It Works:</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>DDL Transformation</strong>: When you create a table with <code>AUTO_INCREMENT</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="CREATE TABLE users (id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100))
-- Becomes internally:
CREATE TABLE users (id BIGINT PRIMARY KEY, name TEXT)"><pre><span>CREATE</span> <span>TABLE</span> <span>users</span> (id <span>INT</span> AUTO_INCREMENT <span>PRIMARY KEY</span>, name <span>VARCHAR</span>(<span>100</span>))
<span><span>--</span> Becomes internally:</span>
<span>CREATE</span> <span>TABLE</span> <span>users</span> (id <span>BIGINT</span> <span>PRIMARY KEY</span>, name <span>TEXT</span>)</pre></div>
</li>
<li>
<p dir="auto"><strong>DML ID Injection</strong>: When inserting with <code>0</code> or <code>NULL</code> for an auto-increment column:</p>
<div dir="auto" data-snippet-clipboard-copy-content="INSERT INTO users (id, name) VALUES (0, &#39;alice&#39;)
-- Becomes internally:
INSERT INTO users (id, name) VALUES (7318624812345678901, &#39;alice&#39;)"><pre><span>INSERT INTO</span> users (id, name) <span>VALUES</span> (<span>0</span>, <span><span>&#39;</span>alice<span>&#39;</span></span>)
<span><span>--</span> Becomes internally:</span>
<span>INSERT INTO</span> users (id, name) <span>VALUES</span> (<span>7318624812345678901</span>, <span><span>&#39;</span>alice<span>&#39;</span></span>)</pre></div>
</li>
<li>
<p dir="auto"><strong>Explicit IDs Preserved</strong>: If you provide an explicit non-zero ID, it is used as-is:</p>
<div dir="auto" data-snippet-clipboard-copy-content="INSERT INTO users (id, name) VALUES (12345, &#39;bob&#39;)
-- Remains:
INSERT INTO users (id, name) VALUES (12345, &#39;bob&#39;)"><pre><span>INSERT INTO</span> users (id, name) <span>VALUES</span> (<span>12345</span>, <span><span>&#39;</span>bob<span>&#39;</span></span>)
<span><span>--</span> Remains:</span>
<span>INSERT INTO</span> users (id, name) <span>VALUES</span> (<span>12345</span>, <span><span>&#39;</span>bob<span>&#39;</span></span>)</pre></div>
</li>
</ol>
<p dir="auto"><strong>Important Considerations:</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Aspect</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ID Range</strong></td>
<td>64-bit (up to 9.2 quintillion) instead of 32-bit (4.2 billion)</td>
</tr>
<tr>
<td><strong>ID Format</strong></td>
<td>HLC-based, not sequential integers</td>
</tr>
<tr>
<td><strong>SQLite ROWID</strong></td>
<td>Not used - Marmot manages IDs explicitly</td>
</tr>
<tr>
<td><strong>Client Libraries</strong></td>
<td>Ensure your client handles <code>BIGINT</code> correctly (some JSON serializers may lose precision)</td>
</tr>
<tr>
<td><strong>Existing Data</strong></td>
<td>Migrate existing <code>INT</code> columns to <code>BIGINT</code> before enabling Marmot</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Schema-Based Detection:</strong></p>
<p dir="auto">Marmot automatically detects auto-increment columns by querying SQLite schema directly. A column is considered auto-increment if:</p>
<ul dir="auto">
<li>It is a single-column <code>INTEGER PRIMARY KEY</code> (SQLite rowid alias), or</li>
<li>It is a single-column <code>BIGINT PRIMARY KEY</code> (Marmot&#39;s transformed columns)</li>
</ul>
<p dir="auto">This means:</p>
<ul dir="auto">
<li><strong>No registration required</strong> - columns are detected from schema at runtime</li>
<li><strong>Works across restarts</strong> - no need to re-execute DDL statements</li>
<li><strong>Works with existing databases</strong> - tables created directly on SQLite work too</li>
</ul>

<p dir="auto">Marmot v2 uses a TOML configuration file (default: <code>config.toml</code>). All settings have sensible defaults.</p>

<div dir="auto" data-snippet-clipboard-copy-content="node_id = 0  # 0 = auto-generate
data_dir = &#34;./marmot-data&#34;"><pre><span>node_id</span> = <span>0</span>  <span><span>#</span> 0 = auto-generate</span>
<span>data_dir</span> = <span><span>&#34;</span>./marmot-data<span>&#34;</span></span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="[transaction]
heartbeat_timeout_seconds = 10  # Transaction timeout without heartbeat
conflict_window_seconds = 10    # Conflict resolution window
lock_wait_timeout_seconds = 50  # Lock wait timeout (MySQL: innodb_lock_wait_timeout)"><pre>[<span>transaction</span>]
<span>heartbeat_timeout_seconds</span> = <span>10</span>  <span><span>#</span> Transaction timeout without heartbeat</span>
<span>conflict_window_seconds</span> = <span>10</span>    <span><span>#</span> Conflict resolution window</span>
<span>lock_wait_timeout_seconds</span> = <span>50</span>  <span><span>#</span> Lock wait timeout (MySQL: innodb_lock_wait_timeout)</span></pre></div>
<p dir="auto"><strong>Note</strong>: Transaction log garbage collection is managed by the replication configuration to coordinate with anti-entropy. See <code>replication.gc_min_retention_hours</code> and <code>replication.gc_max_retention_hours</code>.</p>

<div dir="auto" data-snippet-clipboard-copy-content="[connection_pool]
pool_size = 4              # Number of SQLite connections
max_idle_time_seconds = 10 # Max idle time before closing
max_lifetime_seconds = 300 # Max connection lifetime (0 = unlimited)"><pre>[<span>connection_pool</span>]
<span>pool_size</span> = <span>4</span>              <span><span>#</span> Number of SQLite connections</span>
<span>max_idle_time_seconds</span> = <span>10</span> <span><span>#</span> Max idle time before closing</span>
<span>max_lifetime_seconds</span> = <span>300</span> <span><span>#</span> Max connection lifetime (0 = unlimited)</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="[grpc_client]
keepalive_time_seconds = 10    # Keepalive ping interval
keepalive_timeout_seconds = 3  # Keepalive ping timeout
max_retries = 3                # Max retry attempts
retry_backoff_ms = 100         # Retry backoff duration"><pre>[<span>grpc_client</span>]
<span>keepalive_time_seconds</span> = <span>10</span>    <span><span>#</span> Keepalive ping interval</span>
<span>keepalive_timeout_seconds</span> = <span>3</span>  <span><span>#</span> Keepalive ping timeout</span>
<span>max_retries</span> = <span>3</span>                <span><span>#</span> Max retry attempts</span>
<span>retry_backoff_ms</span> = <span>100</span>         <span><span>#</span> Retry backoff duration</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="[coordinator]
prepare_timeout_ms = 2000 # Prepare phase timeout
commit_timeout_ms = 2000  # Commit phase timeout
abort_timeout_ms = 2000   # Abort phase timeout"><pre>[<span>coordinator</span>]
<span>prepare_timeout_ms</span> = <span>2000</span> <span><span>#</span> Prepare phase timeout</span>
<span>commit_timeout_ms</span> = <span>2000</span>  <span><span>#</span> Commit phase timeout</span>
<span>abort_timeout_ms</span> = <span>2000</span>   <span><span>#</span> Abort phase timeout</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="[cluster]
grpc_bind_address = &#34;0.0.0.0&#34;
grpc_port = 8080
seed_nodes = []                # List of seed node addresses
cluster_secret = &#34;&#34;            # PSK for cluster authentication (see Security section)
gossip_interval_ms = 1000      # Gossip interval
gossip_fanout = 3              # Number of peers to gossip to
suspect_timeout_ms = 5000      # Suspect timeout
dead_timeout_ms = 10000        # Dead timeout"><pre>[<span>cluster</span>]
<span>grpc_bind_address</span> = <span><span>&#34;</span>0.0.0.0<span>&#34;</span></span>
<span>grpc_port</span> = <span>8080</span>
<span>seed_nodes</span> = []                <span><span>#</span> List of seed node addresses</span>
<span>cluster_secret</span> = <span><span>&#34;</span><span>&#34;</span></span>            <span><span>#</span> PSK for cluster authentication (see Security section)</span>
<span>gossip_interval_ms</span> = <span>1000</span>      <span><span>#</span> Gossip interval</span>
<span>gossip_fanout</span> = <span>3</span>              <span><span>#</span> Number of peers to gossip to</span>
<span>suspect_timeout_ms</span> = <span>5000</span>      <span><span>#</span> Suspect timeout</span>
<span>dead_timeout_ms</span> = <span>10000</span>        <span><span>#</span> Dead timeout</span></pre></div>

<p dir="auto">Marmot supports Pre-Shared Key (PSK) authentication for cluster communication. <strong>This is strongly recommended for production deployments.</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="[cluster]
# All nodes in the cluster must use the same secret
cluster_secret = &#34;your-secret-key-here&#34;"><pre>[<span>cluster</span>]
<span><span>#</span> All nodes in the cluster must use the same secret</span>
<span>cluster_secret</span> = <span><span>&#34;</span>your-secret-key-here<span>&#34;</span></span></pre></div>
<p dir="auto"><strong>Environment Variable (Recommended):</strong></p>
<p dir="auto">For production, use the environment variable to avoid storing secrets in config files:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export MARMOT_CLUSTER_SECRET=&#34;your-secret-key-here&#34;
./marmot"><pre><span>export</span> MARMOT_CLUSTER_SECRET=<span><span>&#34;</span>your-secret-key-here<span>&#34;</span></span>
./marmot</pre></div>
<p dir="auto">The environment variable takes precedence over the config file.</p>
<p dir="auto"><strong>Generating a Secret:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Generate a secure random secret
openssl rand -base64 32"><pre><span><span>#</span> Generate a secure random secret</span>
openssl rand -base64 32</pre></div>
<p dir="auto"><strong>Behavior:</strong></p>
<ul dir="auto">
<li>If <code>cluster_secret</code> is empty and <code>MARMOT_CLUSTER_SECRET</code> is not set, authentication is disabled</li>
<li>A warning is logged at startup when authentication is disabled</li>
<li>All gRPC endpoints (gossip, replication, snapshots) are protected when authentication is enabled</li>
<li>Nodes with mismatched secrets will fail to communicate (connection rejected with &#34;invalid cluster secret&#34;)</li>
</ul>
<div dir="auto"><h3 tabindex="-1" dir="auto">Cluster Membership Management</h3><a id="user-content-cluster-membership-management" aria-label="Permalink: Cluster Membership Management" href="#cluster-membership-management"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto">Marmot provides admin HTTP endpoints for managing cluster membership (requires <code>cluster_secret</code> to be configured):</p>
<p dir="auto"><strong>Node Lifecycle:</strong></p>
<ul dir="auto">
<li>New/restarted nodes <strong>auto-join</strong> via gossip - no manual intervention needed</li>
<li>Nodes marked REMOVED via admin API <strong>cannot auto-rejoin</strong> - must be explicitly allowed</li>
<li>This prevents decommissioned nodes from accidentally rejoining the cluster</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="# View cluster members and quorum info
curl -H &#34;X-Marmot-Secret: your-secret&#34; http://localhost:8080/admin/cluster/members

# Remove a node from the cluster (excludes from quorum, blocks auto-rejoin)
curl -X POST -H &#34;X-Marmot-Secret: your-secret&#34; http://localhost:8080/admin/cluster/remove/2

# Allow a removed node to rejoin (node must then restart to join)
curl -X POST -H &#34;X-Marmot-Secret: your-secret&#34; http://localhost:8080/admin/cluster/allow/2"><pre><span><span>#</span> View cluster members and quorum info</span>
curl -H <span><span>&#34;</span>X-Marmot-Secret: your-secret<span>&#34;</span></span> http://localhost:8080/admin/cluster/members

<span><span>#</span> Remove a node from the cluster (excludes from quorum, blocks auto-rejoin)</span>
curl -X POST -H <span><span>&#34;</span>X-Marmot-Secret: your-secret<span>&#34;</span></span> http://localhost:8080/admin/cluster/remove/2

<span><span>#</span> Allow a removed node to rejoin (node must then restart to join)</span>
curl -X POST -H <span><span>&#34;</span>X-Marmot-Secret: your-secret<span>&#34;</span></span> http://localhost:8080/admin/cluster/allow/2</pre></div>
<p dir="auto">See the <a href="https://maxpert.github.io/marmot/operations" rel="nofollow">Operations documentation</a> for detailed usage and examples.</p>

<div dir="auto" data-snippet-clipboard-copy-content="[replication]
default_write_consistency = &#34;QUORUM&#34;      # Write consistency level: ONE, QUORUM, ALL
default_read_consistency = &#34;LOCAL_ONE&#34;    # Read consistency level
write_timeout_ms = 5000                   # Write operation timeout
read_timeout_ms = 2000                    # Read operation timeout

# Anti-Entropy: Background healing for eventual consistency
# - Detects and repairs divergence between replicas
# - Uses delta sync for small lags, snapshot for large lags
# - Includes gap detection to prevent incomplete data after GC
enable_anti_entropy = true                 # Enable automatic catch-up for lagging nodes
anti_entropy_interval_seconds = 60         # How often to check for lag (default: 60s)
delta_sync_threshold_transactions = 10000  # Delta sync if lag &lt; 10K txns
delta_sync_threshold_seconds = 3600        # Snapshot if lag &gt; 1 hour

# Garbage Collection: Reclaim disk space by deleting old transaction records
# - gc_min must be &gt;= delta_sync_threshold (validated at startup)
# - gc_max should be &gt;= 2x delta_sync_threshold (recommended)
# - Set gc_max = 0 for unlimited retention
gc_min_retention_hours = 2   # Keep at least 2 hours (&gt;= 1 hour delta threshold)
gc_max_retention_hours = 24  # Force delete after 24 hours"><pre>[<span>replication</span>]
<span>default_write_consistency</span> = <span><span>&#34;</span>QUORUM<span>&#34;</span></span>      <span><span>#</span> Write consistency level: ONE, QUORUM, ALL</span>
<span>default_read_consistency</span> = <span><span>&#34;</span>LOCAL_ONE<span>&#34;</span></span>    <span><span>#</span> Read consistency level</span>
<span>write_timeout_ms</span> = <span>5000</span>                   <span><span>#</span> Write operation timeout</span>
<span>read_timeout_ms</span> = <span>2000</span>                    <span><span>#</span> Read operation timeout</span>

<span><span>#</span> Anti-Entropy: Background healing for eventual consistency</span>
<span><span>#</span> - Detects and repairs divergence between replicas</span>
<span><span>#</span> - Uses delta sync for small lags, snapshot for large lags</span>
<span><span>#</span> - Includes gap detection to prevent incomplete data after GC</span>
<span>enable_anti_entropy</span> = <span>true</span>                 <span><span>#</span> Enable automatic catch-up for lagging nodes</span>
<span>anti_entropy_interval_seconds</span> = <span>60</span>         <span><span>#</span> How often to check for lag (default: 60s)</span>
<span>delta_sync_threshold_transactions</span> = <span>10000</span>  <span><span>#</span> Delta sync if lag &lt; 10K txns</span>
<span>delta_sync_threshold_seconds</span> = <span>3600</span>        <span><span>#</span> Snapshot if lag &gt; 1 hour</span>

<span><span>#</span> Garbage Collection: Reclaim disk space by deleting old transaction records</span>
<span><span>#</span> - gc_min must be &gt;= delta_sync_threshold (validated at startup)</span>
<span><span>#</span> - gc_max should be &gt;= 2x delta_sync_threshold (recommended)</span>
<span><span>#</span> - Set gc_max = 0 for unlimited retention</span>
<span>gc_min_retention_hours</span> = <span>2</span>   <span><span>#</span> Keep at least 2 hours (&gt;= 1 hour delta threshold)</span>
<span>gc_max_retention_hours</span> = <span>24</span>  <span><span>#</span> Force delete after 24 hours</span></pre></div>
<p dir="auto"><strong>Anti-Entropy Tuning:</strong></p>
<ul dir="auto">
<li><strong>Small clusters (2-3 nodes)</strong>: Use default settings (60s interval)</li>
<li><strong>Large clusters (5+ nodes)</strong>: Consider increasing interval to 120-180s to reduce network overhead</li>
<li><strong>High write throughput</strong>: Increase <code>delta_sync_threshold_transactions</code> to 50000+</li>
<li><strong>Long-running clusters</strong>: Keep <code>gc_max_retention_hours</code> at 24+ to handle extended outages</li>
</ul>
<p dir="auto"><strong>GC Configuration Rules (Validated at Startup):</strong></p>
<ul dir="auto">
<li><code>gc_min_retention_hours</code> must be &gt;= <code>delta_sync_threshold_seconds</code> (in hours)</li>
<li><code>gc_max_retention_hours</code> should be &gt;= 2x <code>delta_sync_threshold_seconds</code></li>
<li>Violating these rules will cause startup failure with helpful error messages</li>
</ul>

<div dir="auto" data-snippet-clipboard-copy-content="[query_pipeline]
transpiler_cache_size = 10000  # LRU cache for MySQL→SQLite transpilation
validator_pool_size = 8        # SQLite connection pool for validation"><pre>[<span>query_pipeline</span>]
<span>transpiler_cache_size</span> = <span>10000</span>  <span><span>#</span> LRU cache for MySQL→SQLite transpilation</span>
<span>validator_pool_size</span> = <span>8</span>        <span><span>#</span> SQLite connection pool for validation</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="[mysql]
enabled = true
bind_address = &#34;0.0.0.0&#34;
port = 3306
max_connections = 1000"><pre>[<span>mysql</span>]
<span>enabled</span> = <span>true</span>
<span>bind_address</span> = <span><span>&#34;</span>0.0.0.0<span>&#34;</span></span>
<span>port</span> = <span>3306</span>
<span>max_connections</span> = <span>1000</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="[publisher]
enabled = false  # Enable CDC publishing to external systems

[[publisher.sinks]]
name = &#34;kafka-main&#34;              # Unique sink name
type = &#34;kafka&#34;                   # &#34;kafka&#34; or &#34;nats&#34;
format = &#34;debezium&#34;              # Debezium-compatible JSON (only option)
brokers = [&#34;localhost:9092&#34;]     # Kafka broker addresses
topic_prefix = &#34;marmot.cdc&#34;      # Topic pattern: {prefix}.{db}.{table}
filter_tables = [&#34;*&#34;]            # Glob patterns for table filtering
filter_databases = [&#34;*&#34;]         # Glob patterns for database filtering
batch_size = 100                 # Events to read per poll cycle
poll_interval_ms = 10            # Polling interval (default: 10ms)
retry_initial_ms = 100           # Initial retry delay on failure
retry_max_ms = 30000             # Max retry delay (30 seconds)
retry_multiplier = 2.0           # Exponential backoff multiplier"><pre>[<span>publisher</span>]
<span>enabled</span> = <span>false</span>  <span><span>#</span> Enable CDC publishing to external systems</span>

[[<span>publisher</span>.<span>sinks</span>]]
<span>name</span> = <span><span>&#34;</span>kafka-main<span>&#34;</span></span>              <span><span>#</span> Unique sink name</span>
<span>type</span> = <span><span>&#34;</span>kafka<span>&#34;</span></span>                   <span><span>#</span> &#34;kafka&#34; or &#34;nats&#34;</span>
<span>format</span> = <span><span>&#34;</span>debezium<span>&#34;</span></span>              <span><span>#</span> Debezium-compatible JSON (only option)</span>
<span>brokers</span> = [<span><span>&#34;</span>localhost:9092<span>&#34;</span></span>]     <span><span>#</span> Kafka broker addresses</span>
<span>topic_prefix</span> = <span><span>&#34;</span>marmot.cdc<span>&#34;</span></span>      <span><span>#</span> Topic pattern: {prefix}.{db}.{table}</span>
<span>filter_tables</span> = [<span><span>&#34;</span>*<span>&#34;</span></span>]            <span><span>#</span> Glob patterns for table filtering</span>
<span>filter_databases</span> = [<span><span>&#34;</span>*<span>&#34;</span></span>]         <span><span>#</span> Glob patterns for database filtering</span>
<span>batch_size</span> = <span>100</span>                 <span><span>#</span> Events to read per poll cycle</span>
<span>poll_interval_ms</span> = <span>10</span>            <span><span>#</span> Polling interval (default: 10ms)</span>
<span>retry_initial_ms</span> = <span>100</span>           <span><span>#</span> Initial retry delay on failure</span>
<span>retry_max_ms</span> = <span>30000</span>             <span><span>#</span> Max retry delay (30 seconds)</span>
<span>retry_multiplier</span> = <span>2.0</span>           <span><span>#</span> Exponential backoff multiplier</span></pre></div>
<p dir="auto">See the <a href="https://maxpert.github.io/marmot/integrations" rel="nofollow">Integrations documentation</a> for details on event format, Kafka/NATS configuration, and use cases.</p>

<div dir="auto" data-snippet-clipboard-copy-content="[logging]
verbose = false          # Enable verbose logging
format = &#34;console&#34;       # Log format: console or json"><pre>[<span>logging</span>]
<span>verbose</span> = <span>false</span>          <span><span>#</span> Enable verbose logging</span>
<span>format</span> = <span><span>&#34;</span>console<span>&#34;</span></span>       <span><span>#</span> Log format: console or json</span></pre></div>

<div dir="auto" data-snippet-clipboard-copy-content="[prometheus]
enabled = true  # Metrics served on gRPC port at /metrics endpoint"><pre>[<span>prometheus</span>]
<span>enabled</span> = <span>true</span>  <span><span>#</span> Metrics served on gRPC port at /metrics endpoint</span></pre></div>
<p dir="auto"><strong>Accessing Metrics:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Metrics are multiplexed with gRPC on the same port
curl http://localhost:8080/metrics

# Prometheus scrape config
scrape_configs:
  - job_name: &#39;marmot&#39;
    static_configs:
      - targets: [&#39;node1:8080&#39;, &#39;node2:8080&#39;, &#39;node3:8080&#39;]"><pre><span><span>#</span> Metrics are multiplexed with gRPC on the same port</span>
curl http://localhost:8080/metrics

<span><span>#</span> Prometheus scrape config</span>
scrape_configs:
  - job_name: <span><span>&#39;</span>marmot<span>&#39;</span></span>
    static_configs:
      - targets: [<span><span>&#39;</span>node1:8080<span>&#39;</span></span>, <span><span>&#39;</span>node2:8080<span>&#39;</span></span>, <span><span>&#39;</span>node3:8080<span>&#39;</span></span>]</pre></div>
<p dir="auto">See <code>config.toml</code> for complete configuration reference with detailed comments.</p>

<p dir="auto">Performance benchmarks on a local development machine (Apple M-series, 3-node cluster, single machine):</p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nodes</td>
<td>3 (ports 3307, 3308, 3309)</td>
</tr>
<tr>
<td>Threads</td>
<td>16</td>
</tr>
<tr>
<td>Batch Size</td>
<td>10 ops/transaction</td>
</tr>
<tr>
<td>Consistency</td>
<td>QUORUM</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Throughput</td>
<td><strong>4,175 ops/sec</strong></td>
</tr>
<tr>
<td>TX Throughput</td>
<td><strong>417 tx/sec</strong></td>
</tr>
<tr>
<td>Records Loaded</td>
<td>200,000</td>
</tr>
<tr>
<td>Errors</td>
<td>0</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Throughput</td>
<td><strong>3,370 ops/sec</strong></td>
</tr>
<tr>
<td>TX Throughput</td>
<td><strong>337 tx/sec</strong></td>
</tr>
<tr>
<td>Duration</td>
<td>120 seconds</td>
</tr>
<tr>
<td>Total Operations</td>
<td>404,930</td>
</tr>
<tr>
<td>Errors</td>
<td>0</td>
</tr>
<tr>
<td>Retries</td>
<td>37 (0.09%)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Operation Distribution:</strong></p>
<ul dir="auto">
<li>READ: 20%</li>
<li>UPDATE: 30%</li>
<li>INSERT: 35%</li>
<li>DELETE: 5%</li>
<li>UPSERT: 10%</li>
</ul>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Percentile</th>
<th>Latency</th>
</tr>
</thead>
<tbody>
<tr>
<td>P50</td>
<td>4.3ms</td>
</tr>
<tr>
<td>P90</td>
<td>14.0ms</td>
</tr>
<tr>
<td>P95</td>
<td>36.8ms</td>
</tr>
<tr>
<td>P99</td>
<td>85.1ms</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>

<p dir="auto">All 3 nodes maintained identical row counts (346,684 rows) throughout the test, confirming consistent replication.</p>
<blockquote>
<p dir="auto"><strong>Note</strong>: These benchmarks are from a local development machine with all nodes on the same host. Production deployments across multiple machines will have different characteristics based on network latency.</p>
</blockquote>

<ul dir="auto">
<li>For FAQs visit <a href="https://maxpert.github.io/marmot/intro#faq" rel="nofollow">this page</a></li>
<li>For community visit our <a href="https://discord.gg/AWUwY66XsE" rel="nofollow">discord</a> or discussions on GitHub</li>
</ul>
</article></div></div>
  </body>
</html>
