<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://genna.win/blog/convolution-simd/">Original</a>
    <h1>Performance optimization, and how to do it wrong</h1>
    
    <div id="readability-page-1" class="page"><div>
            <p>I recently tried to optimize convolutions using
<a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">SIMD</a>
instructions, but what I thought would be a simple task ended up taking me days,
with issue after issue popping up one after another. Some of them make sense in
hindsight, but others were utterly baffling. While the specific examples are for
direct convolution, these considerations apply to pretty much any code with a
hot loop.</p>

<h2 id="background">Background</h2>
<p>I work on <a href="https://github.com/Tracel-AI/burn">burn</a> and recently wanted to
optimize direct convolution on the <code>burn-ndarray</code> CPU backend.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>iter_range_par!</span><span><span>(</span><span>0</span><span>,</span> batch_size <span>*</span> oc_blocks</span><span><span>)</span></span><span>.</span><span>for_each</span><span><span>(</span><span><span><span>|</span></span></span><span><span><span>k</span><span>|</span></span> </span><span><span><span>{</span>
</span></span></span></span><span><span><span><span>  <span>for</span> oh <span>in</span> <span>0</span><span>..</span>out_height <span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>    <span>for</span> ow_block <span>in</span> <span>0</span><span>..</span>ow_blocks <span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>      <span>seq!</span><span><span>(</span>N <span>in</span> <span>0</span><span>..</span><span>8</span> <span><span>{</span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>        <span>let</span> <span>mut</span> acc<span>~</span>N <span>=</span> bias<span>;</span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>      </span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span><span><span><span><span><span><span>
</span></span></span></span></span></span><span><span><span><span><span><span>      <span>for</span> ic <span>in</span> <span>0</span><span>..</span>in_channels <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>        <span>for</span> kh <span>in</span> <span>0</span><span>..</span>k_height <span><span>{</span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>          <span>if</span> <span>!</span>in_bounds_h <span><span>{</span>
</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span>            <span>continue</span><span>;</span>
</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span>          </span><span><span>}</span></span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>          <span>for</span> kw <span>in</span> <span>0</span><span>..</span>k_width <span><span>{</span>
</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span>            <span>let</span> f0 <span>=</span> <span>vload</span><span><span>(</span><span>&amp;</span>weights<span><span>[</span><span><span>[</span>ic<span>,</span> kh<span>,</span> kw<span>,</span> oc<span>]</span></span><span>]</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span>            <span>seq!</span><span><span>(</span>N <span>in</span> <span>0</span><span>..</span><span>8</span> <span><span>{</span>
</span></span></span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span><span><span>              <span>if</span> in_bounds_w <span><span>{</span>
</span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span><span><span><span>                <span>let</span> i<span>~</span>N <span>=</span> <span>splat</span><span><span>(</span>x<span><span>[</span><span><span>[</span>ic<span>,</span> ih<span>,</span> iw <span>+</span> N<span>]</span></span><span>]</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span><span><span><span>                acc<span>~</span>N <span>=</span> <span>E<span>::</span></span>vmuladd<span><span>(</span>simd<span>,</span> i<span>~</span>N<span>,</span> f0<span>,</span> acc<span>~</span>N</span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span><span><span><span>              </span><span><span>}</span></span>
</span></span></span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span><span><span>            </span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span>          </span><span><span>}</span></span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>      </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>
</span></span></span></span></span></span><span><span><span><span><span><span>      <span>seq!</span><span><span>(</span>N <span>in</span> <span>0</span><span>..</span><span>8</span> <span><span>{</span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>        <span>if</span> ow <span>+</span> N <span>&gt;=</span> out_width <span><span>{</span>
</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span>          <span>continue</span><span>;</span>
</span></span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>        <span>vstore</span><span><span>(</span><span>&amp;</span><span>mut</span> out<span><span>[</span><span><span>[</span>ow <span>+</span> N<span>,</span> oc<span>]</span></span><span>]</span></span><span>,</span> acc<span>~</span>N</span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span></span><span><span><span><span><span><span><span><span>      </span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span><span><span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span></span></span><span><span><span><span><span>  </span><span><span>}</span></span>
</span></span></span></span><span><span><span><span></span><span><span>}</span></span></span></span><span><span>)</span></span><span>;</span>
</span></code></pre>
<p>In this implementation I use several techniques. In addition to SIMD loads and
fmadds I use the optimized loop order and register blocking (using the
<a href="https://crates.io/crates/seq-macro"><code>seq</code></a> macro) techniques from
<a href="https://arxiv.org/abs/1808.05567">this paper</a>. I finished the implementation,
executed a benchmark, and... it&#39;s slower. More than two times slower than a
naive unvectorized implementation in fact (~670ms vs ~300ms).</p>
<h2 id="starting-to-investigate">Starting to investigate</h2>
<p>To do this I tried to use various profilers,
<a href="https://github.com/flamegraph-rs/flamegraph">cargo-flamegraph</a>,
<a href="https://github.com/mstange/samply">samply</a> and, after a lot of desperation,
<a href="https://www.amd.com/en/developer/uprof.html">AMD μProf</a>. After a few days of
trying to get useful information out of these profilers (and getting μProf to
work at all), I realized it wasn&#39;t getting me anywhere. The flamegraph and
hotspots just didn&#39;t seem to make any sense at all.</p>
<p>So what&#39;s the next step?</p>
<h2 id="reducing-the-problem">Reducing the problem</h2>
<p>Ok, none of my attempts to profile led to any success. So let&#39;s try to reduce
the code to only what&#39;s actually needed in the benchmark. The benchmark uses
unpadded, unstrided, undilated and ungrouped convolutions, so I stripped all
padding checks and all stride/dilation calculations - it was faster, but still
slow.</p>
<p>There was one branch left to eliminate: The check for border pixels in the
register-blocking loop.</p>
<p>Just to check I shortened the loop to only consider pixels up to the last
multiple of 8. This yields incorrect results, but should help with debugging
performance.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>let</span> ow_blocks <span>=</span> out_width <span>/</span> ow_b </span><span></span><span>
</span><span><span>for</span> ow_block <span>in</span> <span>0</span><span>..</span>ow_blocks <span><span>{</span>
</span></span><span><span>  <span>seq!</span><span><span>(</span>N <span>in</span> <span>0</span><span>..</span><span>8</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>    <span>let</span> <span>mut</span> acc<span>~</span>N <span>=</span> bias<span>;</span>
</span></span></span></span><span><span><span><span>  </span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>for</span> ic <span>in</span> <span>0</span><span>..</span>in_channels <span><span>{</span>
</span></span></span><span><span><span>    <span>for</span> kh <span>in</span> <span>0</span><span>..</span>k_height <span><span>{</span>
</span></span></span></span><span><span><span><span>      <span>for</span> kw <span>in</span> <span>0</span><span>..</span>k_width <span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>let</span> f0 <span>=</span> <span>vload</span><span><span>(</span><span>&amp;</span>weights<span><span>[</span><span><span>[</span>ic<span>,</span> kh<span>,</span> kw<span>,</span> oc<span>]</span></span><span>]</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span><span><span><span><span><span>        <span>seq!</span><span><span>(</span>N <span>in</span> <span>0</span><span>..</span><span>8</span> <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>          <span>let</span> i<span>~</span>N <span>=</span> <span>splat</span><span><span>(</span>x<span><span>[</span><span><span>[</span>ic<span>,</span> ih<span>,</span> iw <span>+</span> N<span>]</span></span><span>]</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>          acc<span>~</span>N <span>=</span> <span>E<span>::</span></span>vmuladd<span><span>(</span>simd<span>,</span> i<span>~</span>N<span>,</span> f0<span>,</span> acc<span>~</span>N</span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>        </span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span><span><span><span><span><span>      </span><span><span>}</span></span>
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span><span><span><span>  </span><span><span>}</span></span>
</span></span><span><span>
</span></span><span><span>  <span>seq!</span><span><span>(</span>N <span>in</span> <span>0</span><span>..</span><span>8</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>    <span>vstore</span><span><span>(</span><span>&amp;</span><span>mut</span> out<span><span>[</span><span><span>[</span>ow <span>+</span> N<span>,</span> oc<span>]</span></span><span>]</span></span><span>,</span> acc<span>~</span>N</span><span><span>)</span></span><span>;</span>
</span></span></span></span><span><span><span><span>  </span><span><span>}</span></span></span><span><span>)</span></span><span>;</span>
</span></span><span><span></span><span><span>}</span></span>
</span></code></pre>
<p>Executing the benchmark, the code is now <em>significantly faster on a single
thread than the old code with multiple threads!</em></p>
<pre data-lang="txt"><code data-lang="txt"><span>Benchmarking - conv2d-input_16x512x512_weight_16x3x3_stride_1
</span><span>―――――――― Result ―――――――――
</span><span>  Timing      full
</span><span>  Samples     40
</span><span>  Mean        205.12ms
</span><span>  Variance    69.420µs
</span><span>  Median      203.24ms
</span><span>  Min         201.12ms
</span><span>  Max         207.23ms
</span><span>―――――――――――――――――――――――――
</span></code></pre>
<p>The problem seems to have been a mixture between spilling registers (as someone
previously focused on GPU, I was shocked to find modern CPUs only have 16 of
them), and too many branches. This is why the profilers didn&#39;t lead me anywhere
useful. Branching in modern CPUs is just too complicated to be meaningfully
represented by a profiler hotspot. This is probably the biggest takeaway from
this article: branches are much worse than you think, because the CPU can&#39;t
predict more than one branch per cycle. A single <code>if</code> statement inside a loop is enough to
stop any further instructions from being decoded in that cycle. Since optimal performance requires
2 FMA instructions per cycle (they take 1 cycle with a 5 cycle latency, and Zen 4 has 2 FMA
units), having a branch on every instruction massively hamstrings the performance. This may be
different on Zen 5, but remember, we still have other branches in addition to the <code>ow</code> bounds check
that need predicting. So it&#39;s worse than 50% performance in practice.</p>
<p>Alright, now we have a good place to start. Let&#39;s start adding things back again
and see where the performance starts getting bad.</p>
<p>First, we need to deal with the remaining pixels after the register-blocking
code. To do this we&#39;re going to use a technique, that we&#39;ll be using several
more times coming up:</p>
<h2 id="why-have-one-loop-when-you-can-have-two">Why have one loop, when you can have two?</h2>
<p>I mentioned before that I shortened the loop to only deal with clean multiples
of the register-blocking factor. So the way to deal with these remaining pixels
is to just... - <em>add another loop.</em></p>
<p>We add a second unblocked loop that starts at the end of the first loop, and
runs until the edge of the feature map. Since it&#39;s not unrolled, we don&#39;t need
to add any bounds checks.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>for</span> ow_block <span>in</span> <span>0</span><span>..</span>ow_blocks <span><span>{</span>
</span></span><span><span>  </span></span><span><span></span><span><span>}</span></span>
</span><span><span>for</span> ow <span>in</span> ow_blocks <span>*</span> <span>8</span><span>..</span>out_width <span><span>{</span>
</span></span><span><span>  <span>let</span> <span>mut</span> acc <span>=</span> bias<span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>for</span> ic <span>in</span> <span>0</span><span>..</span>in_channels <span><span>{</span>
</span></span></span><span><span><span>    <span>for</span> kh <span>in</span> <span>0</span><span>..</span>k_height <span><span>{</span>
</span></span></span></span><span><span><span><span>      <span>for</span> kw <span>in</span> <span>0</span><span>..</span>k_width <span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>        <span>let</span> f0 <span>=</span> <span>vload</span><span><span>(</span><span>&amp;</span>weights<span><span>[</span><span><span>[</span>ic<span>,</span> kh<span>,</span> kw<span>,</span> oc<span>]</span></span><span>]</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span><span><span><span><span><span>        <span>let</span> i0 <span>=</span> <span>splat</span><span><span>(</span>x<span><span>[</span><span><span>[</span>ic<span>,</span> ih<span>,</span> iw <span>+</span> N<span>]</span></span><span>]</span></span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span><span><span><span><span><span>        acc <span>=</span> <span>E<span>::</span></span>vmuladd<span><span>(</span>simd<span>,</span> i0<span>,</span> f0<span>,</span> acc</span><span><span>)</span></span><span>;</span>
</span></span></span></span></span><span><span><span><span><span>      </span><span><span>}</span></span>
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span><span><span><span>  </span><span><span>}</span></span>
</span></span><span><span>
</span></span><span><span>  <span>vstore</span><span><span>(</span><span>&amp;</span><span>mut</span> out<span><span>[</span><span><span>[</span>ow <span>+</span> N<span>,</span> oc<span>]</span></span><span>]</span></span><span>,</span> acc<span>~</span>N</span><span><span>)</span></span><span>;</span>
</span></span><span><span></span><span><span>}</span></span>
</span></code></pre>
<p>Running the benchmarks, it&#39;s still fast - <em>yay!</em> It&#39;s much more efficient to run
two loops than to check if we&#39;re in bounds on every iteration.</p>
<h2 id="adding-back-the-other-variables">Adding back the other variables</h2>
<p>To add back padding, stride and dilation, without tanking the performance again,
I decided to use
<a href="https://rustc-dev-guide.rust-lang.org/backend/monomorph.html">compile-time monomorphization</a>
to eliminate the common zero-padding and/or unit stride/dilation cases. So I use
a technique I saw used in the original convolution implementation, added by
<a href="https://github.com/DrChat">Justin Moore</a>, to enable auto-vectorization for unit
stride convolutions. By adding an <code>if</code>-statement that checks if stride and
dilation are all <code>1</code>, we allow the compiler to
<a href="https://en.wikipedia.org/wiki/Constant_folding#Constant_propagation">constant propagate</a>
this value into that branch. The inner loop is extracted into a separate,
inlined function. This trick allows unstrided convolution to be auto-vectorized in the original,
non-SIMD implementation.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>for</span> ow_block <span>in</span> <span>0</span><span>..</span>ow_blocks <span><span>{</span>
</span></span><span><span>  <span>let</span> ow <span>=</span> ow_block <span>*</span> ow_b <span>+</span> ow_start<span>;</span>
</span></span><span><span>
</span></span><span><span>  </span></span><span><span>  <span><span>#</span><span>[</span><span>allow</span><span><span><span>(</span></span></span><span><span>clippy::if_same_then_else</span></span><span><span><span>)</span></span></span><span>]</span></span>
</span></span><span><span>  <span>if</span> <span><span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span></span><span><span>)</span></span> <span>==</span> <span><span>(</span>stride_h<span>,</span> stride_w<span>,</span> dilate_h<span>,</span> dilate_w</span><span><span>)</span></span> <span><span>{</span>
</span></span></span><span><span><span>    <span>conv2d_inner</span><span><span>(</span>
</span></span></span></span><span><span><span><span>      simd<span>,</span> <span>&amp;</span>x<span>,</span> <span>&amp;</span>weights<span>,</span> <span>&amp;</span><span>mut</span> out<span>,</span> bias<span>,</span> oh<span>,</span> ow<span>,</span> oc<span>,</span> ic_off<span>,</span> stride_h<span>,</span> stride_w<span>,</span>
</span></span></span></span><span><span><span><span>      dilate_h<span>,</span> dilate_w<span>,</span> k_height<span>,</span> k_width<span>,</span> pad_h<span>,</span> pad_w<span>,</span>
</span></span></span></span><span><span><span><span>    </span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>  </span><span><span>}</span></span> <span>else</span> <span><span>{</span>
</span></span></span><span><span><span>    <span>conv2d_inner</span><span><span>(</span>
</span></span></span></span><span><span><span><span>      simd<span>,</span> <span>&amp;</span>x<span>,</span> <span>&amp;</span>weights<span>,</span> <span>&amp;</span><span>mut</span> out<span>,</span> bias<span>,</span> oh<span>,</span> ow<span>,</span> oc<span>,</span> ic_off<span>,</span> stride_h<span>,</span> stride_w<span>,</span>
</span></span></span></span><span><span><span><span>      dilate_h<span>,</span> dilate_w<span>,</span> k_height<span>,</span> k_width<span>,</span> pad_h<span>,</span> pad_w<span>,</span>
</span></span></span></span><span><span><span><span>    </span><span><span>)</span></span><span>;</span>
</span></span></span><span><span><span>  </span><span><span>}</span></span>
</span></span><span><span></span><span><span>}</span></span>
</span></code></pre>
<p>The padding support is added back via a const generic <code>bool</code>, that sets the
padding to <code>0</code>. This allows the compiler to, once again, constant propagate it.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span><span><span>fn</span> </span><span>run_conv2d</span></span><span><span>&lt;</span>, <span>const</span> PAD<span>:</span> <span>bool</span><span>&gt;</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> </span><span><span><span>{</span>
</span></span></span><span><span><span>  </span></span></span><span><span><span>  <span>if</span> <span>!</span><span>PAD</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>    pad_h <span>=</span> <span>0</span><span>;</span>
</span></span></span></span><span><span><span><span>    pad_w <span>=</span> <span>0</span><span>;</span>
</span></span></span></span><span><span><span><span>  </span><span><span>}</span></span>
</span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p><strong>Nice and easy.</strong></p>
<p>Let&#39;s run the benchmark again!</p>
<pre data-lang="txt"><code data-lang="txt"><span>Benchmarking - conv2d-input_16x512x512_weight_16x3x3_stride_1
</span><span>―――――――― Result ―――――――――
</span><span>  Timing      full
</span><span>  Samples     40
</span><span>  Mean        8.136s (+3868%)
</span><span>  Variance    75.115µs
</span><span>  Median      8.042s (+3861%)
</span><span>  Min         8.020s (+3890%)
</span><span>  Max         8.341s (+3929%)
</span><span>―――――――――――――――――――――――――
</span></code></pre>
<p>Oh. Oh dear. What happened?</p>
<h2 id="when-the-compiler-gets-it-wrong">When the compiler gets it wrong</h2>
<p>To explain what just happened I need to add another small background detail I
didn&#39;t mention before. To use modern SIMD features, the code uses runtime
feature selection with <a href="https://github.com/sarah-quinones/pulp">pulp</a>. The way
this works is that <code>pulp</code> annotates a function with something like
<code>#[target_feature(enable = &#34;avx2&#34;)]</code>, based on the available features. This
tells the compiler it&#39;s allowed to use avx2 features, even if the target
wouldn&#39;t normally include avx2. However, only inlined functions will have the
features enabled and non-inlined function calls will not (<em>this is
foreshadowing</em>).</p>
<p>This is where <code>samply</code> starts becoming actually useful. Running it allows me to
see the assembly for each function and find the hotspots. <em>And this time they
are actually meaningful!</em> <code>samply</code> tells me, I&#39;m spending all my time in the
line that calls the FMA and in the FMA itself. So I take a look at the assembly
and -<em>oh no!</em></p>
<pre data-lang="asm"><code data-lang="asm"><span><span>movaps</span><span> </span><span>xmm6</span><span>,</span><span> </span><span>x</span><span>m</span><span>m</span><span>w</span><span>o</span><span>r</span><span>d</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0x2e0</span><span>]</span>
</span><span><span>movaps</span><span> </span><span>xmm7</span><span>,</span><span> </span><span>x</span><span>m</span><span>m</span><span>w</span><span>o</span><span>r</span><span>d</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0x2f0</span><span>]</span>
</span><span><span>movaps</span><span> </span><span>x</span><span>m</span><span>m</span><span>w</span><span>o</span><span>r</span><span>d</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0x170</span><span>]</span><span>,</span><span> </span><span>xmm15</span>
</span><span><span>movaps</span><span> </span><span>x</span><span>m</span><span>m</span><span>w</span><span>o</span><span>r</span><span>d</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0x160</span><span>]</span><span>,</span><span> </span><span>xmm14</span>
</span><span><span>movaps</span><span> </span><span>xmm0</span><span>,</span><span> </span><span>x</span><span>m</span><span>m</span><span>w</span><span>o</span><span>r</span><span>d</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0x180</span><span>]</span>
</span><span><span>movaps</span><span> </span><span>x</span><span>m</span><span>m</span><span>w</span><span>o</span><span>r</span><span>d</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0xb0</span><span>]</span><span>,</span><span> </span><span>xmm0</span>
</span><span><span>movaps</span><span> </span><span>xmm0</span><span>,</span><span> </span><span>x</span><span>m</span><span>m</span><span>w</span><span>o</span><span>r</span><span>d</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0x190</span><span>]</span>
</span><span><span>movaps</span><span> </span><span>x</span><span>m</span><span>m</span><span>w</span><span>o</span><span>r</span><span>d</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0xa0</span><span>]</span><span>,</span><span> </span><span>xmm0</span>
</span><span><span>mov</span><span> </span><span>rcx</span><span>,</span><span> </span><span>rbx</span>
</span><span><span>lea</span><span> </span><span>rbx</span><span>,</span><span> </span><span>qword</span><span> </span><span>[</span><span>rsp</span><span> </span><span>+</span><span> </span><span>0x320</span><span>]</span>
</span><span><span>mov</span><span> </span><span>rdx</span><span>,</span><span> </span><span>rbx</span>
</span><span><span>mov</span><span> </span><span>r8</span><span>,</span><span> </span><span>rdi</span>
</span><span><span>mov</span><span> </span><span>r9</span><span>,</span><span> </span><span>r14</span>
</span><span><span>call</span><span> </span><span>0x4edb0</span>
</span></code></pre>
<ul>
<li>Why is it using SSE registers instead of AVX registers?</li>
<li>Why is it putting them onto the stack?</li>
<li>It&#39;s using <code>call</code> to execute the
<a href="https://doc.rust-lang.org/beta/core/arch/x86_64/fn._mm256_fmadd_ps.html">_mm256_fmadd_ps</a>
intrinsic for some reason?</li>
</ul>
<p>Turns out: These things are linked. Coming up is what I think is a pretty
accurate guess of what happened here.</p>
<p>See, the compiler has a size limit for inlined functions. <code>#[inline(always)]</code>
tells the compiler to ignore the size limit, and <em>almost</em> all of my functions
were marked as <code>#[inline(always)]</code>. However, the <em>outermost</em> function <strong>was
not</strong>.</p>
<p>These are the steps I think happened next:</p>
<ul>
<li>Adding these inlined branches caused the size of the function to exceed
Rust&#39;s inlining limit</li>
<li>Rust outlined (is that a word?) the top-level function from <code>pulp</code>s wrapper
function, the one that is marked with <code>#[target_feature]</code></li>
<li>The compiler now treats my function as a regular function, causing it to
fall back to the default feature set (<code>x86-64-v1</code>)</li>
<li>This means 256-bit registers are no longer available in my function.</li>
<li>Since <code>_mm256_fmadd_ps</code> is an AVX2 instruction that requires 256-bit
registers, the compiler must now call it dynamically and transfer the data
via the stack. This is slow. <strong>Very</strong> slow.</li>
</ul>
<p>I&#39;m somewhat unsure about that last step, maybe someone with more knowledge of
compiler internals can enlighten me on the actual reason the intrinsic is no
longer inlined.</p>
<p>Fortunately, the solution was much simpler than this chain of events: Annotate
the top-level function with <code>#[inline(always)]</code>.</p>
<pre data-lang="asm"><code data-lang="asm"><span><span>vbroadcastss</span><span> </span><span>ymm9</span><span>,</span><span> </span><span>dword</span><span> </span><span>[</span><span>r12</span><span> </span><span>+</span><span> </span><span>r11</span><span> </span><span>*</span><span> </span><span>1</span><span>]</span>
</span><span><span>lea</span><span> </span><span>rcx</span><span>,</span><span> </span><span>qword</span><span> </span><span>[</span><span>r11</span><span> </span><span>+</span><span> </span><span>r12</span><span> </span><span>*</span><span> </span><span>1</span><span>]</span>
</span><span><span>v</span><span>f</span><span>m</span><span>a</span><span>d</span><span>d</span><span>2</span><span>3</span><span>1</span><span>p</span><span>s</span><span> </span><span>ymm6</span><span>,</span><span> </span><span>ymm8</span><span>,</span><span> </span><span>ymm10</span>
</span></code></pre>
<p>Much better. And the benchmark?</p>
<pre data-lang="txt"><code data-lang="txt"><span>Benchmarking - conv2d-input_16x512x512_weight_16x3x3_stride_1
</span><span>―――――――― Result ―――――――――
</span><span>  Timing      full
</span><span>  Samples     40
</span><span>  Mean        230.12ms
</span><span>  Variance    69.420µs
</span><span>  Median      232.24ms
</span><span>  Min         224.12ms
</span><span>  Max         236.23ms
</span><span>―――――――――――――――――――――――――
</span></code></pre>
<p>Nice!</p>
<h2 id="finishing-up-the-optimizations">Finishing up the optimizations</h2>
<p>Performance was good for unpadded convolutions, but would still have been
lackluster for padded ones, since we need to check if we are in padding <em>in
every single loop iteration</em>. To solve this, we can use the same technique we
used for the <code>out_width</code> earlier: All pixels that are more than <code>padding</code> away
from the edges are guaranteed to always be in bounds, so we can run one loop
from <code>padding_h</code> to <code>out_height - padding_h</code> and <code>padding_w</code> to
<code>width - padding_w</code> without bounds checks, then a second loop for the border
pixels that <em>does</em> do bounds checks. This is much faster than checking every
pixel, since most pixels are always in bounds.</p>
<pre data-lang="rust"><code data-lang="rust"><span><span>if</span> <span><span>(</span>pad_h<span>,</span> pad_w</span><span><span>)</span></span> <span>!=</span> <span><span>(</span><span>0</span><span>,</span> <span>0</span></span><span><span>)</span></span> <span><span>{</span>
</span></span><span><span>  <span>let</span> v_borders <span>=</span> <span><span>(</span><span>0</span><span>..</span>pad_h</span><span><span>)</span></span>
</span></span><span><span>    <span>.</span><span>chain</span><span><span>(</span>out_height<span>.</span><span>saturating_sub</span><span><span>(</span>pad_h</span><span><span>)</span></span><span>..</span>out_height</span><span><span>)</span></span>
</span></span><span><span>    <span>.</span><span>cartesian_product</span><span><span>(</span><span>0</span><span>..</span>out_width</span><span><span>)</span></span><span>;</span>
</span></span><span><span>  <span>let</span> h_borders <span>=</span> <span><span>(</span><span>0</span><span>..</span>out_height</span><span><span>)</span></span>
</span></span><span><span>    <span>.</span><span>cartesian_product</span><span><span>(</span><span><span>(</span><span>0</span><span>..</span>pad_w</span><span><span>)</span></span><span>.</span><span>chain</span><span><span>(</span>out_width<span>.</span><span>saturating_sub</span><span><span>(</span>pad_w</span><span><span>)</span></span><span>..</span>out_width</span><span><span>)</span></span></span><span><span>)</span></span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>for</span> <span><span>(</span>oh<span>,</span> ow</span><span><span>)</span></span> <span>in</span> v_borders<span>.</span><span>chain</span><span><span>(</span>h_borders</span><span><span>)</span></span> <span><span>{</span>
</span></span></span><span><span><span>    </span></span></span><span><span><span>  </span><span><span>}</span></span>
</span></span><span><span></span><span><span>}</span></span>
</span></code></pre>
<h2 id="final-thoughts">Final thoughts</h2>
<p>Modern CPUs are weird, and performance is not always obvious. Inlining is
fragile, and adding a single line of code can completely change the way your
program is compiled without you even noticing. Profilers aren&#39;t always helpful,
especially when your problem is more complex than something like using a slow
function or allocating memory too often. My tip is to just try to figure out
just when things start getting bad and maybe learn some basic assembly, so you
can spot things like way too many stack loads/stores (which indicates register
spilling).</p>
<p>I hope this helps someone deal with their performance issues a little bit more
quickly than I did.</p>
<p>Also big shout out once again to <a href="https://github.com/mstange/samply">samply</a>.
Even when the performance data didn&#39;t mean much, being able to easily view
assembly for any given function was very useful.</p>
<p>The final version of the code for this implementation can be found
<a href="https://github.com/tracel-ai/burn/blob/90ca4f3a9b61cf2f3c5a38cf6566684b7fb3b287/crates/burn-ndarray/src/ops/simd/conv.rs">here</a>.
And just for fun, here is the final benchmark after all optimizations, using
multi-threading:</p>
<pre data-lang="txt"><code data-lang="txt"><span>Benchmarking - conv2d-input_16x512x512_weight_16x3x3_stride_1
</span><span>―――――――― Result ―――――――――
</span><span>  Timing      full
</span><span>  Samples     40
</span><span>  Mean        42.731ms
</span><span>  Variance    5.115µs
</span><span>  Median      42.906ms
</span><span>  Min         38.162ms
</span><span>  Max         47.554ms
</span><span>―――――――――――――――――――――――――
</span></code></pre>
<hr/>


        </div></div>
  </body>
</html>
