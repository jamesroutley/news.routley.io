<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://github.com/PKU-YuanGroup/Video-LLaVA">Original</a>
    <h1>Video-LLaVA</h1>
    
    <div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/57c23a144556d97c6f405a55eea646f4c048941f9f473f30df17d6656cdbc9f2/68747470733a2f2f7a312e617831782e636f6d2f323032332f31312f30372f70696c347371482e706e67"><img src="https://camo.githubusercontent.com/57c23a144556d97c6f405a55eea646f4c048941f9f473f30df17d6656cdbc9f2/68747470733a2f2f7a312e617831782e636f6d2f323032332f31312f30372f70696c347371482e706e67" width="150" data-canonical-src="https://z1.ax1x.com/2023/11/07/pil4sqH.png"/></a>
</p><h2 tabindex="-1" dir="auto"><a id="user-content--video-llava-learning-united-visual-representation-by-alignment-before-projection" aria-hidden="true" tabindex="-1" href="#-video-llava-learning-united-visual-representation-by-alignment-before-projection"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a> <a href="https://arxiv.org/abs/2311.10122" rel="nofollow">Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</a></h2>
<h5 tabindex="-1" dir="auto"><a id="user-content--if-you-like-our-project-please-give-us-a-star--on-github-for-latest-update--" aria-hidden="true" tabindex="-1" href="#-if-you-like-our-project-please-give-us-a-star--on-github-for-latest-update--"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a> If you like our project, please give us a star ‚≠ê on GitHub for latest update.  </h5>
<h5 tabindex="-1" dir="auto"><a id="" aria-hidden="true" tabindex="-1" href="#"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>
<p dir="auto"><a href="https://huggingface.co/spaces/LanguageBind/Video-LLaVA" rel="nofollow"><img src="https://camo.githubusercontent.com/8653431c9418242f831f7058eea5a0f56eeec9b73e89a4d0395d99bad43942c4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462541342539372d4f70656e253230496e2532305370616365732d626c75652e737667" alt="hf_space" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97-Open%20In%20Spaces-blue.svg"/></a>
<a href="https://replicate.com/nateraw/video-llava" rel="nofollow"><img src="https://camo.githubusercontent.com/4ddae07f8ed47af939fa1ca9a97f30cde0409ae4a64ffcaec85c5a15371b190d/68747470733a2f2f7265706c69636174652e636f6d2f6e6174657261772f766964656f2d6c6c6176612f6261646765" alt="Replicate demo and cloud API" data-canonical-src="https://replicate.com/nateraw/video-llava/badge"/></a>
<a href="https://twitter.com/arankomatsuzaki/status/1726421417963516144" rel="nofollow"><img src="https://camo.githubusercontent.com/8edbd3eba2763e9afc2e689207c5b415035e86e3bbc24f2e186910fa1e03cbfd/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f547769747465722532302d626c61636b" alt="zhihu" data-canonical-src="https://img.shields.io/badge/Twitter%20-black"/></a>
<a href="https://mp.weixin.qq.com/s/EFqLv_Euf5VU024zOtzkkg" rel="nofollow"><img src="https://camo.githubusercontent.com/3c4cb2555fdf114c203363e05466132fdeece586ae560163a18e4453e0dbfe20/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545392538372538462545352541442539302545342542442538442532302d626c61636b" alt="zhihu" data-canonical-src="https://img.shields.io/badge/%E9%87%8F%E5%AD%90%E4%BD%8D%20-black"/></a>
<a href="https://arxiv.org/abs/2311.10122" rel="nofollow"><img src="https://camo.githubusercontent.com/0b59096d2603353846afd391074266433a8f44575fbef0eb6cf09dd136adf507/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41727869762d323331312e31303132322d6233316231622e7376673f6c6f676f3d6172586976" alt="arXiv" data-canonical-src="https://img.shields.io/badge/Arxiv-2311.10122-b31b1b.svg?logo=arXiv"/></a>
<a href="https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/842143dc4bc8bf0a65bc5c1e6a4e53972478545a6427e6e446602cd2ba36cff7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d79656c6c6f77" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-yellow"/></a>
<a href="https://hits.seeyoufarm.com" rel="nofollow"><img src="https://camo.githubusercontent.com/4c15205f7ae5a1afc2f0467fb7d1aa58464a9cb69d6e911a0a1ed615fa92467f/68747470733a2f2f686974732e736565796f756661726d2e636f6d2f6170692f636f756e742f696e63722f62616467652e7376673f75726c3d68747470732533412532462532466769746875622e636f6d253246504b552d5975616e47726f7570253246566964656f2d4c4c61564126636f756e745f62673d253233373943383344267469746c655f62673d2532333535353535352669636f6e3d2669636f6e5f636f6c6f723d253233453745374537267469746c653d56697369746f7226656467655f666c61743d66616c7365" alt="Hits" data-canonical-src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FPKU-YuanGroup%2FVideo-LLaVA&amp;count_bg=%2379C83D&amp;title_bg=%23555555&amp;icon=&amp;icon_color=%23E7E7E7&amp;title=Visitor&amp;edge_flat=false"/></a>
<a href="https://github.com/PKU-YuanGroup/Video-LLaVA/issues?q=is%3Aopen+is%3Aissue"><img src="https://camo.githubusercontent.com/eadc91d34c7b92e4f5fc2d27b261b8c0d97b187d12cb75753ab22e8fb123895e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f504b552d5975616e47726f75702f566964656f2d4c4c6156413f636f6c6f723d637269746963616c266c6162656c3d497373756573" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/PKU-YuanGroup/Video-LLaVA?color=critical&amp;label=Issues"/></a>
<a href="https://github.com/PKU-YuanGroup/Video-LLaVA/issues?q=is%3Aissue+is%3Aclosed"><img src="https://camo.githubusercontent.com/d73e45affdc375da7f4d31f9967ecd7cdce52f773e0f5bf66d0d81e409e9878f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d636c6f7365642f504b552d5975616e47726f75702f566964656f2d4c4c6156413f636f6c6f723d73756363657373266c6162656c3d497373756573" alt="GitHub closed issues" data-canonical-src="https://img.shields.io/github/issues-closed/PKU-YuanGroup/Video-LLaVA?color=success&amp;label=Issues"/></a></p>
</h5>
<p dir="auto"><a href="https://paperswithcode.com/sota/zeroshot-video-question-answer-on-msrvtt-qa?p=video-llava-learning-united-visual-1" rel="nofollow"><img src="https://camo.githubusercontent.com/7f3d08e5a1e7b2c3a20e4daf36c71313bcc45ac9de84965bea8d7b1f9eda2c18/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f766964656f2d6c6c6176612d6c6561726e696e672d756e697465642d76697375616c2d312f7a65726f73686f742d766964656f2d7175657374696f6e2d616e737765722d6f6e2d6d73727674742d7161" alt="PWC" data-canonical-src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/video-llava-learning-united-visual-1/zeroshot-video-question-answer-on-msrvtt-qa"/></a> </p>
<details open=""><summary>üí° I also have other video-language projects that may interest you ‚ú®. </summary><blockquote>
<p dir="auto"><a href="https://arxiv.org/abs/2310.01852" rel="nofollow"><strong>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment</strong></a></p>
</blockquote>
<blockquote>
<p dir="auto"><a href="https://arxiv.org/abs/2311.08046" rel="nofollow"><strong>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</strong></a></p>
</blockquote>
</details>
<h2 tabindex="-1" dir="auto"><a id="user-content--news" aria-hidden="true" tabindex="-1" href="#-news"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üì∞ News</h2>
<ul dir="auto">
<li><strong>[2023.11.20]</strong>  ü§ó<a href="https://huggingface.co/spaces/LanguageBind/Video-LLaVA" rel="nofollow">Demo</a> and code are available now! Welcome to <strong>watch</strong> üëÄ this repository for the latest updates.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content--highlights" aria-hidden="true" tabindex="-1" href="#-highlights"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üòÆ Highlights</h2>
<p dir="auto">Video-LLaVA exhibits remarkable interactive capabilities between images and videos, despite the absence of image-video pairs in the dataset.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content--simple-baseline-learning-united-visual-representation-by-alignment-before-projection" aria-hidden="true" tabindex="-1" href="#-simple-baseline-learning-united-visual-representation-by-alignment-before-projection"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üí° Simple baseline, learning united visual representation by alignment before projection</h3>
<ul dir="auto">
<li>With <strong>the binding of unified visual representations to the language feature space</strong>, we enable an LLM to perform visual reasoning capabilities on both images and videos simultaneously.</li>
</ul>
<h3 tabindex="-1" dir="auto"><a id="user-content--high-performance-complementary-learning-with-video-and-image" aria-hidden="true" tabindex="-1" href="#-high-performance-complementary-learning-with-video-and-image"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üî• High performance, complementary learning with video and image</h3>
<ul dir="auto">
<li>Extensive experiments demonstrate <strong>the complementarity of modalities</strong>, showcasing significant superiority when compared to models specifically designed for either images or videos.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/assets/main.jpg"><img src="https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/assets/main.jpg"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content--demo" aria-hidden="true" tabindex="-1" href="#-demo"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ü§ó Demo</h2>
<ul dir="auto">
<li><strong>Gradio Web UI</strong></li>
</ul>
<p dir="auto">Highly recommend trying out our web demo by the following command, which incorporates all features currently supported by Video-LLaVA. We also provide <a href="https://huggingface.co/spaces/LanguageBind/Video-LLaVA" rel="nofollow">online demo</a> in Huggingface Spaces.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m  llava.serve.gradio_web_server"><pre>python -m  llava.serve.gradio_web_server</pre></div>
<details open="">
  <summary>
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true">
    <path d="M16 3.75v8.5a.75.75 0 0 1-1.136.643L11 10.575v.675A1.75 1.75 0 0 1 9.25 13h-7.5A1.75 1.75 0 0 1 0 11.25v-6.5C0 3.784.784 3 1.75 3h7.5c.966 0 1.75.784 1.75 1.75v.675l3.864-2.318A.75.75 0 0 1 16 3.75Zm-6.5 1a.25.25 0 0 0-.25-.25h-7.5a.25.25 0 0 0-.25.25v6.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-6.5ZM11 8.825l3.5 2.1v-5.85l-3.5 2.1Z"></path>
</svg>
    <span aria-label="Video description demo.mp4">demo.mp4</span>
    <span></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/62638829/284110937-71ab15ac-105e-4b18-b0b5-e1b35d70607b.mp4" data-canonical-src="https://user-images.githubusercontent.com/62638829/284110937-71ab15ac-105e-4b18-b0b5-e1b35d70607b.mp4" controls="controls" muted="muted">

  </video>
</details>

<ul dir="auto">
<li><strong>CLI Inference</strong></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="python -m llava.serve.cli --model-path &#34;LanguageBind/Video-LLaVA-7B&#34; --video-file &#34;path/to/your/video.mp4&#34; --load-4bit"><pre>python -m llava.serve.cli --model-path <span><span>&#34;</span>LanguageBind/Video-LLaVA-7B<span>&#34;</span></span> --video-file <span><span>&#34;</span>path/to/your/video.mp4<span>&#34;</span></span> --load-4bit</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/assets/videocli.gif"><img src="https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/assets/videocli.gif" width="500" data-animated-image=""/></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m llava.serve.cli --model-path &#34;LanguageBind/Video-LLaVA-7B&#34; --image-file &#34;path/to/your/image.jpg&#34; --load-4bit"><pre>python -m llava.serve.cli --model-path <span><span>&#34;</span>LanguageBind/Video-LLaVA-7B<span>&#34;</span></span> --image-file <span><span>&#34;</span>path/to/your/image.jpg<span>&#34;</span></span> --load-4bit</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/assets/imagecli.gif"><img src="https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/assets/imagecli.gif" width="500" data-animated-image=""/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content--main-results" aria-hidden="true" tabindex="-1" href="#-main-results"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üöÄ Main Results</h2>
<h3 tabindex="-1" dir="auto"><a id="user-content-image-understanding" aria-hidden="true" tabindex="-1" href="#image-understanding"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Image understanding</h3>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/assets/res_img.jpg"><img src="https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/assets/res_img.jpg" width="80%"/></a>
</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-video-understanding" aria-hidden="true" tabindex="-1" href="#video-understanding"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Video understanding</h3>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/assets/res_vi.jpg"><img src="https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/assets/res_vi.jpg" width="80%"/></a>
</p>
<h2 tabindex="-1" dir="auto"><a id="user-content-Ô∏è-requirements-and-installation" aria-hidden="true" tabindex="-1" href="#Ô∏è-requirements-and-installation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üõ†Ô∏è Requirements and Installation</h2>
<ul dir="auto">
<li>Python &gt;= 3.10</li>
<li>Pytorch == 2.0.1</li>
<li>CUDA Version &gt;= 11.7</li>
<li>Install required packages:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/PKU-YuanGroup/Video-LLaVA
cd Video-LLaVA
conda create -n videollava python=3.10 -y
conda activate videollava
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
pip install -e &#34;.[train]&#34;
pip install flash-attn --no-build-isolation
pip install decord opencv-python git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d"><pre>git clone https://github.com/PKU-YuanGroup/Video-LLaVA
<span>cd</span> Video-LLaVA
conda create -n videollava python=3.10 -y
conda activate videollava
pip install --upgrade pip  <span><span>#</span> enable PEP 660 support</span>
pip install -e <span>.</span>
pip install -e <span><span>&#34;</span>.[train]<span>&#34;</span></span>
pip install flash-attn --no-build-isolation
pip install decord opencv-python git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content--api" aria-hidden="true" tabindex="-1" href="#-api"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ü§ñ API</h2>
<p dir="auto"><strong>We open source all codes.</strong> If you want to load the model (e.g. <code>LanguageBind/Video-LLaVA-7B</code>) on local, you can use the following code snippets.</p>
<h3 tabindex="-1" dir="auto"><a id="user-content-inference-for-image" aria-hidden="true" tabindex="-1" href="#inference-for-image"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference for image</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from llava.constants import X_TOKEN_INDEX, DEFAULT_X_TOKEN
from llava.conversation import conv_templates, SeparatorStyle
from llava.model.builder import load_pretrained_model
from llava.utils import disable_torch_init
from llava.mm_utils import tokenizer_X_token, get_model_name_from_path, KeywordsStoppingCriteria

def main():
    disable_torch_init()
    image = &#39;llava/serve/examples/extreme_ironing.jpg&#39;
    inp = &#39;What is unusual about this image?&#39;
    model_path = &#39;LanguageBind/Video-LLaVA-7B&#39;
    device = &#39;cuda&#39;
    load_4bit, load_8bit = True, False
    model_name = get_model_name_from_path(model_path)
    tokenizer, model, processor, context_len = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device)
    image_processor = processor[&#39;image&#39;]
    conv_mode = &#34;llava_v1&#34;
    conv = conv_templates[conv_mode].copy()
    roles = conv.roles

    image_tensor = image_processor.preprocess(image, return_tensors=&#39;pt&#39;)[&#39;pixel_values&#39;]
    if type(image_tensor) is list:
        tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]
    else:
        tensor = image_tensor.to(model.device, dtype=torch.float16)
    key = [&#39;image&#39;]

    print(f&#34;{roles[1]}: {inp}&#34;)
    inp = DEFAULT_X_TOKEN[&#39;IMAGE&#39;] + &#39;\n&#39; + inp
    conv.append_message(conv.roles[0], inp)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()
    input_ids = tokenizer_X_token(prompt, tokenizer, X_TOKEN_INDEX[&#39;IMAGE&#39;], return_tensors=&#39;pt&#39;).unsqueeze(0).cuda()
    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
    keywords = [stop_str]
    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)

    with torch.inference_mode():
        output_ids = model.generate(
            input_ids,
            images=[tensor, key],
            do_sample=True,
            temperature=0.2,
            max_new_tokens=1024,
            use_cache=True,
            stopping_criteria=[stopping_criteria])

    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()
    print(outputs)

if __name__ == &#39;__main__&#39;:
    main()"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>llava</span>.<span>constants</span> <span>import</span> <span>X_TOKEN_INDEX</span>, <span>DEFAULT_X_TOKEN</span>
<span>from</span> <span>llava</span>.<span>conversation</span> <span>import</span> <span>conv_templates</span>, <span>SeparatorStyle</span>
<span>from</span> <span>llava</span>.<span>model</span>.<span>builder</span> <span>import</span> <span>load_pretrained_model</span>
<span>from</span> <span>llava</span>.<span>utils</span> <span>import</span> <span>disable_torch_init</span>
<span>from</span> <span>llava</span>.<span>mm_utils</span> <span>import</span> <span>tokenizer_X_token</span>, <span>get_model_name_from_path</span>, <span>KeywordsStoppingCriteria</span>

<span>def</span> <span>main</span>():
    <span>disable_torch_init</span>()
    <span>image</span> <span>=</span> <span>&#39;llava/serve/examples/extreme_ironing.jpg&#39;</span>
    <span>inp</span> <span>=</span> <span>&#39;What is unusual about this image?&#39;</span>
    <span>model_path</span> <span>=</span> <span>&#39;LanguageBind/Video-LLaVA-7B&#39;</span>
    <span>device</span> <span>=</span> <span>&#39;cuda&#39;</span>
    <span>load_4bit</span>, <span>load_8bit</span> <span>=</span> <span>True</span>, <span>False</span>
    <span>model_name</span> <span>=</span> <span>get_model_name_from_path</span>(<span>model_path</span>)
    <span>tokenizer</span>, <span>model</span>, <span>processor</span>, <span>context_len</span> <span>=</span> <span>load_pretrained_model</span>(<span>model_path</span>, <span>None</span>, <span>model_name</span>, <span>load_8bit</span>, <span>load_4bit</span>, <span>device</span><span>=</span><span>device</span>)
    <span>image_processor</span> <span>=</span> <span>processor</span>[<span>&#39;image&#39;</span>]
    <span>conv_mode</span> <span>=</span> <span>&#34;llava_v1&#34;</span>
    <span>conv</span> <span>=</span> <span>conv_templates</span>[<span>conv_mode</span>].<span>copy</span>()
    <span>roles</span> <span>=</span> <span>conv</span>.<span>roles</span>

    <span>image_tensor</span> <span>=</span> <span>image_processor</span>.<span>preprocess</span>(<span>image</span>, <span>return_tensors</span><span>=</span><span>&#39;pt&#39;</span>)[<span>&#39;pixel_values&#39;</span>]
    <span>if</span> <span>type</span>(<span>image_tensor</span>) <span>is</span> <span>list</span>:
        <span>tensor</span> <span>=</span> [<span>image</span>.<span>to</span>(<span>model</span>.<span>device</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>) <span>for</span> <span>image</span> <span>in</span> <span>image_tensor</span>]
    <span>else</span>:
        <span>tensor</span> <span>=</span> <span>image_tensor</span>.<span>to</span>(<span>model</span>.<span>device</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>)
    <span>key</span> <span>=</span> [<span>&#39;image&#39;</span>]

    <span>print</span>(<span>f&#34;<span><span>{</span><span>roles</span>[<span>1</span>]<span>}</span></span>: <span><span>{</span><span>inp</span><span>}</span></span>&#34;</span>)
    <span>inp</span> <span>=</span> <span>DEFAULT_X_TOKEN</span>[<span>&#39;IMAGE&#39;</span>] <span>+</span> <span>&#39;<span>\n</span>&#39;</span> <span>+</span> <span>inp</span>
    <span>conv</span>.<span>append_message</span>(<span>conv</span>.<span>roles</span>[<span>0</span>], <span>inp</span>)
    <span>conv</span>.<span>append_message</span>(<span>conv</span>.<span>roles</span>[<span>1</span>], <span>None</span>)
    <span>prompt</span> <span>=</span> <span>conv</span>.<span>get_prompt</span>()
    <span>input_ids</span> <span>=</span> <span>tokenizer_X_token</span>(<span>prompt</span>, <span>tokenizer</span>, <span>X_TOKEN_INDEX</span>[<span>&#39;IMAGE&#39;</span>], <span>return_tensors</span><span>=</span><span>&#39;pt&#39;</span>).<span>unsqueeze</span>(<span>0</span>).<span>cuda</span>()
    <span>stop_str</span> <span>=</span> <span>conv</span>.<span>sep</span> <span>if</span> <span>conv</span>.<span>sep_style</span> <span>!=</span> <span>SeparatorStyle</span>.<span>TWO</span> <span>else</span> <span>conv</span>.<span>sep2</span>
    <span>keywords</span> <span>=</span> [<span>stop_str</span>]
    <span>stopping_criteria</span> <span>=</span> <span>KeywordsStoppingCriteria</span>(<span>keywords</span>, <span>tokenizer</span>, <span>input_ids</span>)

    <span>with</span> <span>torch</span>.<span>inference_mode</span>():
        <span>output_ids</span> <span>=</span> <span>model</span>.<span>generate</span>(
            <span>input_ids</span>,
            <span>images</span><span>=</span>[<span>tensor</span>, <span>key</span>],
            <span>do_sample</span><span>=</span><span>True</span>,
            <span>temperature</span><span>=</span><span>0.2</span>,
            <span>max_new_tokens</span><span>=</span><span>1024</span>,
            <span>use_cache</span><span>=</span><span>True</span>,
            <span>stopping_criteria</span><span>=</span>[<span>stopping_criteria</span>])

    <span>outputs</span> <span>=</span> <span>tokenizer</span>.<span>decode</span>(<span>output_ids</span>[<span>0</span>, <span>input_ids</span>.<span>shape</span>[<span>1</span>]:]).<span>strip</span>()
    <span>print</span>(<span>outputs</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#39;__main__&#39;</span>:
    <span>main</span>()</pre></div>
<h3 tabindex="-1" dir="auto"><a id="user-content-inference-for-video" aria-hidden="true" tabindex="-1" href="#inference-for-video"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Inference for video</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from llava.constants import X_TOKEN_INDEX, DEFAULT_X_TOKEN
from llava.conversation import conv_templates, SeparatorStyle
from llava.model.builder import load_pretrained_model
from llava.utils import disable_torch_init
from llava.mm_utils import tokenizer_X_token, get_model_name_from_path, KeywordsStoppingCriteria

def main():
    disable_torch_init()
    video = &#39;llava/serve/examples/sample_demo_1.mp4&#39;
    inp = &#39;Why is this video funny?&#39;
    model_path = &#39;LanguageBind/Video-LLaVA-7B&#39;
    device = &#39;cuda&#39;
    load_4bit, load_8bit = True, False
    model_name = get_model_name_from_path(model_path)
    tokenizer, model, processor, context_len = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device)
    video_processor = processor[&#39;video&#39;]
    conv_mode = &#34;llava_v1&#34;
    conv = conv_templates[conv_mode].copy()
    roles = conv.roles

    video_tensor = video_processor(video, return_tensors=&#39;pt&#39;)[&#39;pixel_values&#39;]
    if type(video_tensor) is list:
        tensor = [video.to(model.device, dtype=torch.float16) for video in video_tensor]
    else:
        tensor = video_tensor.to(model.device, dtype=torch.float16)
    key = [&#39;video&#39;]

    print(f&#34;{roles[1]}: {inp}&#34;)
    inp = DEFAULT_X_TOKEN[&#39;VIDEO&#39;] + &#39;\n&#39; + inp
    conv.append_message(conv.roles[0], inp)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()
    input_ids = tokenizer_X_token(prompt, tokenizer, X_TOKEN_INDEX[&#39;VIDEO&#39;], return_tensors=&#39;pt&#39;).unsqueeze(0).cuda()
    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
    keywords = [stop_str]
    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)

    with torch.inference_mode():
        output_ids = model.generate(
            input_ids,
            images=[tensor, key],
            do_sample=True,
            temperature=0.1,
            max_new_tokens=1024,
            use_cache=True,
            stopping_criteria=[stopping_criteria])

    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()
    print(outputs)

if __name__ == &#39;__main__&#39;:
    main()"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>llava</span>.<span>constants</span> <span>import</span> <span>X_TOKEN_INDEX</span>, <span>DEFAULT_X_TOKEN</span>
<span>from</span> <span>llava</span>.<span>conversation</span> <span>import</span> <span>conv_templates</span>, <span>SeparatorStyle</span>
<span>from</span> <span>llava</span>.<span>model</span>.<span>builder</span> <span>import</span> <span>load_pretrained_model</span>
<span>from</span> <span>llava</span>.<span>utils</span> <span>import</span> <span>disable_torch_init</span>
<span>from</span> <span>llava</span>.<span>mm_utils</span> <span>import</span> <span>tokenizer_X_token</span>, <span>get_model_name_from_path</span>, <span>KeywordsStoppingCriteria</span>

<span>def</span> <span>main</span>():
    <span>disable_torch_init</span>()
    <span>video</span> <span>=</span> <span>&#39;llava/serve/examples/sample_demo_1.mp4&#39;</span>
    <span>inp</span> <span>=</span> <span>&#39;Why is this video funny?&#39;</span>
    <span>model_path</span> <span>=</span> <span>&#39;LanguageBind/Video-LLaVA-7B&#39;</span>
    <span>device</span> <span>=</span> <span>&#39;cuda&#39;</span>
    <span>load_4bit</span>, <span>load_8bit</span> <span>=</span> <span>True</span>, <span>False</span>
    <span>model_name</span> <span>=</span> <span>get_model_name_from_path</span>(<span>model_path</span>)
    <span>tokenizer</span>, <span>model</span>, <span>processor</span>, <span>context_len</span> <span>=</span> <span>load_pretrained_model</span>(<span>model_path</span>, <span>None</span>, <span>model_name</span>, <span>load_8bit</span>, <span>load_4bit</span>, <span>device</span><span>=</span><span>device</span>)
    <span>video_processor</span> <span>=</span> <span>processor</span>[<span>&#39;video&#39;</span>]
    <span>conv_mode</span> <span>=</span> <span>&#34;llava_v1&#34;</span>
    <span>conv</span> <span>=</span> <span>conv_templates</span>[<span>conv_mode</span>].<span>copy</span>()
    <span>roles</span> <span>=</span> <span>conv</span>.<span>roles</span>

    <span>video_tensor</span> <span>=</span> <span>video_processor</span>(<span>video</span>, <span>return_tensors</span><span>=</span><span>&#39;pt&#39;</span>)[<span>&#39;pixel_values&#39;</span>]
    <span>if</span> <span>type</span>(<span>video_tensor</span>) <span>is</span> <span>list</span>:
        <span>tensor</span> <span>=</span> [<span>video</span>.<span>to</span>(<span>model</span>.<span>device</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>) <span>for</span> <span>video</span> <span>in</span> <span>video_tensor</span>]
    <span>else</span>:
        <span>tensor</span> <span>=</span> <span>video_tensor</span>.<span>to</span>(<span>model</span>.<span>device</span>, <span>dtype</span><span>=</span><span>torch</span>.<span>float16</span>)
    <span>key</span> <span>=</span> [<span>&#39;video&#39;</span>]

    <span>print</span>(<span>f&#34;<span><span>{</span><span>roles</span>[<span>1</span>]<span>}</span></span>: <span><span>{</span><span>inp</span><span>}</span></span>&#34;</span>)
    <span>inp</span> <span>=</span> <span>DEFAULT_X_TOKEN</span>[<span>&#39;VIDEO&#39;</span>] <span>+</span> <span>&#39;<span>\n</span>&#39;</span> <span>+</span> <span>inp</span>
    <span>conv</span>.<span>append_message</span>(<span>conv</span>.<span>roles</span>[<span>0</span>], <span>inp</span>)
    <span>conv</span>.<span>append_message</span>(<span>conv</span>.<span>roles</span>[<span>1</span>], <span>None</span>)
    <span>prompt</span> <span>=</span> <span>conv</span>.<span>get_prompt</span>()
    <span>input_ids</span> <span>=</span> <span>tokenizer_X_token</span>(<span>prompt</span>, <span>tokenizer</span>, <span>X_TOKEN_INDEX</span>[<span>&#39;VIDEO&#39;</span>], <span>return_tensors</span><span>=</span><span>&#39;pt&#39;</span>).<span>unsqueeze</span>(<span>0</span>).<span>cuda</span>()
    <span>stop_str</span> <span>=</span> <span>conv</span>.<span>sep</span> <span>if</span> <span>conv</span>.<span>sep_style</span> <span>!=</span> <span>SeparatorStyle</span>.<span>TWO</span> <span>else</span> <span>conv</span>.<span>sep2</span>
    <span>keywords</span> <span>=</span> [<span>stop_str</span>]
    <span>stopping_criteria</span> <span>=</span> <span>KeywordsStoppingCriteria</span>(<span>keywords</span>, <span>tokenizer</span>, <span>input_ids</span>)

    <span>with</span> <span>torch</span>.<span>inference_mode</span>():
        <span>output_ids</span> <span>=</span> <span>model</span>.<span>generate</span>(
            <span>input_ids</span>,
            <span>images</span><span>=</span>[<span>tensor</span>, <span>key</span>],
            <span>do_sample</span><span>=</span><span>True</span>,
            <span>temperature</span><span>=</span><span>0.1</span>,
            <span>max_new_tokens</span><span>=</span><span>1024</span>,
            <span>use_cache</span><span>=</span><span>True</span>,
            <span>stopping_criteria</span><span>=</span>[<span>stopping_criteria</span>])

    <span>outputs</span> <span>=</span> <span>tokenizer</span>.<span>decode</span>(<span>output_ids</span>[<span>0</span>, <span>input_ids</span>.<span>shape</span>[<span>1</span>]:]).<span>strip</span>()
    <span>print</span>(<span>outputs</span>)

<span>if</span> <span>__name__</span> <span>==</span> <span>&#39;__main__&#39;</span>:
    <span>main</span>()</pre></div>
<h2 tabindex="-1" dir="auto"><a id="user-content-Ô∏è-training--validating" aria-hidden="true" tabindex="-1" href="#Ô∏è-training--validating"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üóùÔ∏è Training &amp; Validating</h2>
<p dir="auto">The training &amp; validating instruction is in <a href="https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/TRAIN_AND_VALIDATE.md">TRAIN_AND_VALIDATE.md</a>.</p>
<h2 tabindex="-1" dir="auto"><a id="user-content--acknowledgement" aria-hidden="true" tabindex="-1" href="#-acknowledgement"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üëç Acknowledgement</h2>
<ul dir="auto">
<li><a href="https://github.com/haotian-liu/LLaVA">LLaVA</a> The codebase we built upon and it is an efficient large language and vision assistant.</li>
<li><a href="https://github.com/mbzuai-oryx/Video-ChatGPT">Video-ChatGPT</a> Great job contributing the evaluation code and dataset.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content--related-projects" aria-hidden="true" tabindex="-1" href="#-related-projects"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>ü§ù Related Projects</h2>
<ul dir="auto">
<li><a href="https://github.com/PKU-YuanGroup/LanguageBind">LanguageBind</a> An open source five modalities language-based retrieval framework.</li>
<li><a href="https://github.com/PKU-YuanGroup/Chat-UniVi">Chat-UniVi</a> This framework empowers the model to efficiently utilize a limited number of visual tokens.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content--license" aria-hidden="true" tabindex="-1" href="#-license"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>üîí License</h2>
<ul dir="auto">
<li>The majority of this project is released under the Apache 2.0 license as found in the <a href="https://github.com/PKU-YuanGroup/Video-LLaVA/blob/main/LICENSE">LICENSE</a> file.</li>
<li>The service is a research preview intended for non-commercial use only, subject to the model <a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md">License</a> of LLaMA, <a href="https://openai.com/policies/terms-of-use" rel="nofollow">Terms of Use</a> of the data generated by OpenAI, and <a href="https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb" rel="nofollow">Privacy Practices</a> of ShareGPT. Please contact us if you find any potential violation.</li>
</ul>
<h2 tabindex="-1" dir="auto"><a id="user-content-Ô∏è-citation" aria-hidden="true" tabindex="-1" href="#Ô∏è-citation"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>‚úèÔ∏è Citation</h2>
<p dir="auto">If you find our paper and code useful in your research, please consider giving a star ‚≠ê and citation üìù.</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{lin2023videollava,
      title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection}, 
      author={Bin Lin and Bin Zhu and Yang Ye and Munan Ning and Peng Jin and Li Yuan},
      year={2023},
      eprint={2311.10122},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre><span>@misc</span>{<span>lin2023videollava</span>,
      <span>title</span>=<span><span>{</span>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Bin Lin and Bin Zhu and Yang Ye and Munan Ning and Peng Jin and Li Yuan<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2023<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2311.10122<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{zhu2023languagebind,
      title={LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment}, 
      author={Bin Zhu and Bin Lin and Munan Ning and Yang Yan and Jiaxi Cui and HongFa Wang and Yatian Pang and Wenhao Jiang and Junwu Zhang and Zongwei Li and Wancai Zhang and Zhifeng Li and Wei Liu and Li Yuan},
      year={2023},
      eprint={2310.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre><span>@misc</span>{<span>zhu2023languagebind</span>,
      <span>title</span>=<span><span>{</span>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Bin Zhu and Bin Lin and Munan Ning and Yang Yan and Jiaxi Cui and HongFa Wang and Yatian Pang and Wenhao Jiang and Junwu Zhang and Zongwei Li and Wancai Zhang and Zhifeng Li and Wei Liu and Li Yuan<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2023<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2310.01852<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>
}</pre></div>

<h2 tabindex="-1" dir="auto"><a id="user-content--star-history" aria-hidden="true" tabindex="-1" href="#-star-history"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>‚ú® Star History</h2>
<p dir="auto"><a href="https://star-history.com/#PKU-YuanGroup/Video-LLaVA&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/cee5d3edcc5f350603ba8c1bc226ed080096cce460c6f3c732b257203e38c84b/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d504b552d5975616e47726f75702f566964656f2d4c4c61564126747970653d44617465" alt="Star History" data-canonical-src="https://api.star-history.com/svg?repos=PKU-YuanGroup/Video-LLaVA&amp;type=Date"/></a></p>
<h2 tabindex="-1" dir="auto"><a id="user-content-contributors" aria-hidden="true" tabindex="-1" href="#contributors"><svg viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a>Contributors</h2>
<a href="https://github.com/PKU-YuanGroup/Video-LLaVA/graphs/contributors">
  <img src="https://camo.githubusercontent.com/0cf0b4b6869ffc094fa249ea6fddf8e657c15361295f2c1cbf2cb48003a9f075/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d504b552d5975616e47726f75702f566964656f2d4c4c615641" data-canonical-src="https://contrib.rocks/image?repo=PKU-YuanGroup/Video-LLaVA"/>
</a>
</article>
          </div></div>
  </body>
</html>
