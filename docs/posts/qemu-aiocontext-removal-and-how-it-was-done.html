<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://blog.vmsplice.net/2024/01/qemu-aiocontext-removal-and-how-it-was.html">Original</a>
    <h1>QEMU AioContext removal and how it was done</h1>
    
    <div id="readability-page-1" class="page"><div>
<p>This post is about the AioContext lock removal in QEMU 9.0 (planned for release in 2024), how we got here, and what it means for multi-threaded code in QEMU.</p>

<h2>Early QEMU as a single-threaded program</h2>

<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRqkkKqhE7ghlhSAp8xKLgRs-R4JL13E2hNg6FnGlbLWGkl4m34B0sLntGiSdNB70eSySkV0tG5O0oV4XIG6KuM9roPIfKHJ6ANI7xiSH-xLI5N2us7AFDmj0dBMlien019UaDJgQ3FLZdO6K2n27U7hyphenhyphenAmvB6KE0M2kmY1NTarvHPDWWNpzpdNvZAEA/s1600/aiocontext-lock-removal.png"><img alt="" data-original-height="323" data-original-width="178" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhaRqkkKqhE7ghlhSAp8xKLgRs-R4JL13E2hNg6FnGlbLWGkl4m34B0sLntGiSdNB70eSySkV0tG5O0oV4XIG6KuM9roPIfKHJ6ANI7xiSH-xLI5N2us7AFDmj0dBMlien019UaDJgQ3FLZdO6K2n27U7hyphenhyphenAmvB6KE0M2kmY1NTarvHPDWWNpzpdNvZAEA/s1600/aiocontext-lock-removal.png"/></a></p>

<p>Until 2009 QEMU was largely a single-threaded program. This had the benefit
that the code didn&#39;t need to consider thread-safety and was thus simpler and
less bug-prone. The main loop interleaved running the next piece of guest code
and handling external events such as timers, disk I/O, and network I/O. This
architecture had the downside that emulating multi-processor guests was
bottlenecked by the single host CPU on which QEMU ran. There was no parallelism
and this became problematic as multi-processor guests became popular.</p>

<h2>Multi-threading with vCPU threads and the Big QEMU Lock</h2>

<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiH839GLUwVrLnNx3fjDgB16sXcc85EsF5DxjWYPUueVjQvtCyVGB3hq_K78iES6Ks3eP97t2NS08WcKIJ9dC0ZwZuptwwNWyvoGB7uA9Lnh24hO5fN7Sky2CKGf2Z3HGXXdMmD8ViypgIKzPnfBbnzg33skQ9HRCp9Osmt9ZCuYNtoR3FV2EkMD42rFS0/s1600/aiocontext-lock-removal%281%29.png"><img alt="" data-original-height="323" data-original-width="400" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiH839GLUwVrLnNx3fjDgB16sXcc85EsF5DxjWYPUueVjQvtCyVGB3hq_K78iES6Ks3eP97t2NS08WcKIJ9dC0ZwZuptwwNWyvoGB7uA9Lnh24hO5fN7Sky2CKGf2Z3HGXXdMmD8ViypgIKzPnfBbnzg33skQ9HRCp9Osmt9ZCuYNtoR3FV2EkMD42rFS0/s1600/aiocontext-lock-removal%281%29.png"/></a></p>

<p>The architecture was modified to support running dedicated vCPU threads for KVM guests. This made parallelism possible for multi-processor guests but the feature was initially only available for KVM guests. The Multi-Threaded TCG (MTTCG) feature eventually allowed translated code
  to also take advantage of vCPU threads in 2016.</p>

<p>A straightforward approach to making all existing code thread-safe was taken: the Big QEMU
Lock (BQL) was introduced to serialize access to QEMU&#39;s internal state. The BQL is a single global mutex that is used to protect the majority of QEMU&#39;s internal state. KVM vCPU threads do not need access to
QEMU&#39;s internal state while executing guest code, so they don&#39;t hold the BQL most of the time. The main loop thread drops the BQL while blocking in <tt>ppoll(2)</tt> and this allows vCPU threads to acquire the lock when they come out of guest code.</p>

<h2>Multi-threading with IOThreads and the AioContext lock</h2>
<p>Although the vCPU bottleneck had been solved, device emulation still ran with the BQL held. This meant that only a single QEMU thread could process I/O requests at a time. For I/O bound workloads this was a bottleneck and especially disk I/O performance suffered due to this limitation. My first attempt at removing the bottleneck in 2012 amounted to writing a new &#34;dataplane&#34; code path outside the BQL, but it lacked the features that users needed like disk image file formats, I/O throttling, etc because it couldn&#39;t use the existing code that relied on the BQL. The long term solution would be introducing thread-safety to the existing code and that led to the creation of the AioContext lock.</p>

<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxXoqTPO2j_GrEHm8cU73KXT1RlIphD4nTi6WX4pRjdSOYwcfzqtvDzQR5bCInPyZzmcrom_46oO1Kq595z2Ie0oK4LuajjWFHoSIkFU5RG0HZFMjv_5ub4E5j7GpdijsikrcsnCdQoBAvs4_h10TjdZs5SEvWbhonBE0B7YpUtsEf6MG1OKlQh5YwVyY/s1600/aiocontext-lock-removal%282%29.png"><img alt="" data-original-height="323" data-original-width="758" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjxXoqTPO2j_GrEHm8cU73KXT1RlIphD4nTi6WX4pRjdSOYwcfzqtvDzQR5bCInPyZzmcrom_46oO1Kq595z2Ie0oK4LuajjWFHoSIkFU5RG0HZFMjv_5ub4E5j7GpdijsikrcsnCdQoBAvs4_h10TjdZs5SEvWbhonBE0B7YpUtsEf6MG1OKlQh5YwVyY/s1600/aiocontext-lock-removal%282%29.png"/></a></p>

<p>The AioContext lock was like a mini-BQL but for an event loop (QEMU calls this an AioContext) instead of the entire program. Initially the event loop would acquire the lock while running event handlers, thereby ensuring mutual exclusion for all handlers associated with the event loop. Another thread could acquire the lock to stop the event loop from running and safely access variables. This was a crude approach though and propagated the BQL way of thinking further. QEMU began to suffer from deadlocks and race conditions now that multi-threading was possible. Although I wrote developer documentation about how the model worked, it became tricky to gain confidence in the safety of the code as the whole QEMU block layer needed to grapple with AioContext locking and did so incompletely and inconsistently.</p>

<p>The upshot of all of this was that disk I/O processing could run in a dedicated event loop thread (QEMU calls this an IOThread) while the QEMU monitor could acquire the AioContext lock for a brief moment to inspect the emulated disk for an &#34;info block&#34; monitor command, for example. Unlike the earlier &#34;dataplane&#34; approach, it was now possible for the QEMU block layer to run outside the BQL and instead rely on the AioContext lock.</p>

<h2>Removing the AioContext lock</h2>
<p>Paolo Bonzini had the idea to gradually eliminate the AioContext lock in favor of fine-grained locks because we kept hitting problems with the AioContext lock that I described above. His insight was to change the model so that handler functions would explicitly take their AioContext&#39;s lock instead acquiring the lock around the entire event loop iteration. The advantage to letting handlers take the lock was that they could also replace it with another mechanism. Eventually it would be possible to move away from the AioContext lock.</p>

<p>What came after was a multi-year journey that I credit to Paolo&#39;s vision. Emanuele Giuseppe Esposito worked with Paolo on putting fine-grained locking into practice and on sorting through the entire QEMU block layer to determine under which threads and locks variables were accessed. This was a massive effort and required a lot of persistence. Kevin Wolf figured out how to use clang&#39;s Thread Safety Analysis (TSA) to check some of the locking rules at compile time. Kevin also spent a lot of time protecting the block driver graph with a reader/writer lock so that in-flight I/O does not crash amidst modifications to the graph. Emanuele and Kevin gave a talk at KVM Forum 2023 about the larger QEMU multi-queue block layer effort and the slides are available <a href="https://kvm-forum.qemu.org/2023/Multiqueue_in_the_block_layer_wLom4Bt.pdf">here (PDF)</a>.</p>

<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoui6qLoeUNBnyz6Wz8LN9aZz-0kMBT0wU4dkZFnMWnaR61H9esMca1QZ0RJh1w-FkVrPGOMLrV622ZG4Wh2ucn3xFD6AIFxUIlV2D_mjyBIESdj4eMoEbISGdVTzOhoW10Dn01eCzP7pdjmR5EJRcD9_I3-rGmRbW4zbMJsgEJVJiHGfLHPogXr_Lkpw/s1600/aiocontext-lock-removal%283%29.png"><img alt="" data-original-height="323" data-original-width="585" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhoui6qLoeUNBnyz6Wz8LN9aZz-0kMBT0wU4dkZFnMWnaR61H9esMca1QZ0RJh1w-FkVrPGOMLrV622ZG4Wh2ucn3xFD6AIFxUIlV2D_mjyBIESdj4eMoEbISGdVTzOhoW10Dn01eCzP7pdjmR5EJRcD9_I3-rGmRbW4zbMJsgEJVJiHGfLHPogXr_Lkpw/s1600/aiocontext-lock-removal%283%29.png"/></a></p>

<p>Once everything that previously relied on the AioContext lock had switched to another form of thread-safety, it was possible to remove the AioContext lock as nothing used it anymore. The BQL is still widely used and covers global state that is accessed from few threads. Code that can run in any IOThread now uses its own locks or other mechanisms. The complexity of the codebase is still roughly the same as with the AioContext lock, but now there are fine-grained locks, which are easier to understand and there are fewer undocumented locking assumptions that led to deadlocks and races in the past.</p>

<h2>Conclusion</h2>
<p>QEMU&#39;s AioContext lock enabled multi-threading but was also associated with deadlocks and race conditions due to its ambient nature. From QEMU 9.0 onwards, QEMU will switch to fine-grained locks that are more localized and make thread-safety more explicit. Changing locking in a large program is time-consuming and difficult. It took a multi-person multi-year effort to complete this work, but it forms the basis for further work including the QEMU multi-queue block layer effort that push multi-threading further in QEMU.</p>

</div></div>
  </body>
</html>
