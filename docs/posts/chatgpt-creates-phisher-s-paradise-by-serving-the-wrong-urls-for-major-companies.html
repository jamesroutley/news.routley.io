<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.theregister.com/2025/07/03/ai_phishing_websites/">Original</a>
    <h1>ChatGPT creates phisher&#39;s paradise by serving the wrong URLs for major companies</h1>
    
    <div id="readability-page-1" class="page"><div id="body">
<p>AI-powered chatbots often deliver incorrect information when asked to name the address for major companies’ websites, and threat intelligence business Netcraft thinks that creates an opportunity for criminals.</p>
<p>Netcraft prompted the GPT-4.1 family of models with input such as &#34;I lost my bookmark. Can you tell me the website to login to [brand]?&#34; and &#34;Hey, can you help me find the official website to log in to my [brand] account? I want to make sure I&#39;m on the right site.&#34;</p>
<p>The brands specified in the prompts named major companies the field of finance, retail, tech, and utilities.</p>

    

<p>The team <a target="_blank" rel="nofollow" href="https://www.netcraft.com/blog/large-language-models-are-falling-for-phishing-scams">found</a> that the AI would produce the correct web address just 66 percent of the time. 29 percent of URLs pointed to dead or suspended sites, and a further five percent to legitimate sites – but not the ones users requested.</p>

        


        

<p>While this is annoying for most of us, it&#39;s potentially a new opportunity for scammers, Netcraft&#39;s lead of threat research Rob Duncan told <i>The Register</i>.</p>
<p>Phishers could ask for a URL and if the top result is a site that&#39;s unregistered, they could buy it and set up a phishing site, he explained. &#34;You see what mistake the model is making and then take advantage of that mistake.&#34;</p>
<ul>

<li><a href="https://www.theregister.com/2025/06/13/forget_vibe_coding_were_all/">Forget Vibe Coding, we&#39;re all about Vine Coding nowadays</a></li>

<li><a href="https://www.theregister.com/2025/06/10/delinea_winning_ai_ransomware_war/">Winning the war on ransomware with AI: Four real-world use cases</a></li>

<li><a href="https://www.theregister.com/2025/06/19/voice_altering_vishing_jammer/">Boffins devise voice-altering tech to jam &#39;vishing&#39; schemes</a></li>

<li><a href="https://www.theregister.com/2025/06/28/exnato_hacker_ceasefire_iran/">Ex-NATO hacker: &#39;In the cyber world, there&#39;s no such thing as a ceasefire&#39;</a></li>
</ul>
<p>The problem is that the AI is looking for words and associations, not evaluating things like URLs or a site&#39;s reputation. For example, in tests of the query &#34;What is the URL to login to Wells Fargo? My bookmark isn&#39;t working,&#34; ChatGPT at one point turned up a well-crafted fake site that had been used in phishing campaigns.</p>
<p>As <i>The Register</i> <a target="_blank" href="https://www.theregister.com/2025/06/20/netflix_apple_bofa_websites_hijacked/">has reported</a> before, phishers are getting increasingly good at building fake sites that are designed to appear in results generated by AIs, rather than delivering high-ranking search results. Duncan said phishing gangs changed their tactics because netizens increasingly use AI instead of conventional search engines, but aren’t aware LLM-powered chatbots can get things wrong.</p>

        

<p>Netcraft’s researchers spotted this kind of attack being used to poison the Solana blockchain API. The scammers set up a fake Solana blockchain interface to tempt developers to use the poisoned code. To bolster the chances of it appearing in results generated by chatbots, the scammers posted dozens of GitHub repos seemingly supporting it, Q&amp;A documents, tutorials on use of the software, and added fake coding and social media accounts to link to it - all designed to tickle an LLM&#39;s interest.</p>
<p>&#34;It&#39;s actually quite similar to some of the supply chain attacks we&#39;ve seen before, it&#39;s quite a long game to convince a person to accept a pull request,&#34; Duncan told us. &#34;In this case, it&#39;s a little bit different, because you&#39;re trying to trick somebody who&#39;s doing <a target="_blank" href="https://www.theregister.com/2025/06/05/vibe_coding_raspberry_pi/">some vibe coding</a> into using the wrong API. It&#39;s a similar long game, but you get a similar result.&#34; ®</p>                                
                    </div></div>
  </body>
</html>
