<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.theguardian.com/global-development/2025/oct/20/ai-generated-poverty-porn-fake-images-being-used-by-aid-agencies">Original</a>
    <h1>AI-generated &#39;poverty porn&#39; fake images being used by aid agencies</h1>
    
    <div id="readability-page-1" class="page"><div id="maincontent"><div><p>AI-generated images of extreme poverty, children and sexual violence survivors are flooding stock photo sites and increasingly being used by leading health NGOs, according to global health professionals who have voiced concern over a new era of “poverty porn”.</p><p>“All over the place, people are using it,” said Noah Arnold, who works at Fairpicture, a Swiss-based organisation focused on promoting ethical imagery in global development. “Some are actively using AI imagery, and others, we know that they’re experimenting at least.”</p><p><a href="https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(23)00329-7/fulltext" data-link-name="in body link">Arsenii Alenichev, a researcher</a> at the Institute of Tropical Medicine in Antwerp studying the production of global health images, said: “The images replicate the visual grammar of poverty – children with empty plates, cracked earth, stereotypical visuals.”</p><p>Alenichev has collected more than 100 AI-generated images of extreme poverty used by individuals or NGOs as part of social media campaigns against hunger or sexual violence. Images he shared with the Guardian show exaggerated, stereotype-perpetuating scenes: children huddled together in muddy water; an African girl in a wedding dress with a tear staining her cheek. In a <a href="https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(25)00313-4/fulltext?rss=yes" data-link-name="in body link">comment piece published on Thursday</a> in the Lancet Global Health, he argues these images amount to “poverty porn 2.0”.</p><p>While it is hard to quantify the prevalence of the AI-generated images, Alenichev and others say their use is on the rise, driven by concerns over consent and cost. Arnold said that US funding cuts to NGO budgets had made matters worse.</p><p>“It is quite clear that various organisations are starting to consider synthetic images instead of real photography, because it’s cheap and you don’t need to bother with consent and everything,” said Alenichev.</p><p>AI-generated images of extreme poverty now appear in their dozens on popular stock photo sites, including <a href="https://stock.adobe.com/uk/search/images?filters%5Bcontent_type%3Aphoto%5D=1&amp;filters%5Bcontent_type%3Aillustration%5D=1&amp;filters%5Bcontent_type%3Azip_vector%5D=1&amp;filters%5Bcontent_type%3Avideo%5D=0&amp;filters%5Bcontent_type%3Atemplate%5D=0&amp;filters%5Bcontent_type%3A3d%5D=0&amp;filters%5Bcontent_type%3Aaudio%5D=0&amp;filters%5Binclude_stock_enterprise%5D=0&amp;filters%5Bis_editorial%5D=0&amp;filters%5Bfree_collection%5D=0&amp;filters%5Bcontent_type%3Aimage%5D=1&amp;filters%5Bgentech%5D=only&amp;k=poverty&amp;order=relevance&amp;price%5B%24%5D=1&amp;search_type=filter-select&amp;get_facets=1" data-link-name="in body link">Adobe Stock Photos</a> and <a href="https://www.freepik.com/search?ai=only&amp;format=search&amp;last_filter=ai&amp;last_value=only&amp;query=poverty" data-link-name="in body link">Freepik</a>, in response to queries such as “poverty”. Many bear captions such as “Photorealistic kid in refugee camp”; “Asian children swim in a river full of waste”; and “Caucasian white volunteer provides medical consultation to young black children in African village”. Adobe sells licences to the last two photos in that list for about £60.</p><p>“They are so racialised. They should never even let those be published because it’s like the worst stereotypes about Africa, or India, or you name it,” said Alenichev.</p><p>Joaquín Abela, CEO of Freepik, said the responsibility for using such extreme images lay with media consumers, and not with platforms such as his. The AI stock photos, he said, are generated by the platform’s global community of users, who can receive a licensing fee when Freepik’s customers choose to buy their images.</p><p>Freepik had attempted to curb biases it had found in other parts of its photo library, he said, by “injecting diversity” and trying to ensure gender balance into the photos of lawyers and CEOs hosted on the site.</p><p>But, he said, there was only so much his platform could do. “It’s like trying to dry the ocean. We make an effort, but in reality, if customers worldwide want images a certain way, there is absolutely nothing that anyone can do.”</p><figure id="22d6a6a3-f724-4379-a7bf-c72cf2240898" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/c4f9a6f4fa8474a2c2ac158c0745bdceab2c7ae0/5_0_2227_1211/master/2227.jpg?width=620&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"/><source srcset="https://i.guim.co.uk/img/media/c4f9a6f4fa8474a2c2ac158c0745bdceab2c7ae0/5_0_2227_1211/master/2227.jpg?width=620&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 660px)"/><source srcset="https://i.guim.co.uk/img/media/c4f9a6f4fa8474a2c2ac158c0745bdceab2c7ae0/5_0_2227_1211/master/2227.jpg?width=605&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"/><source srcset="https://i.guim.co.uk/img/media/c4f9a6f4fa8474a2c2ac158c0745bdceab2c7ae0/5_0_2227_1211/master/2227.jpg?width=605&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 480px)"/><source srcset="https://i.guim.co.uk/img/media/c4f9a6f4fa8474a2c2ac158c0745bdceab2c7ae0/5_0_2227_1211/master/2227.jpg?width=445&amp;dpr=2&amp;s=none&amp;crop=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"/><source srcset="https://i.guim.co.uk/img/media/c4f9a6f4fa8474a2c2ac158c0745bdceab2c7ae0/5_0_2227_1211/master/2227.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" media="(min-width: 320px)"/><img alt="A series of photographs showing black and brown-skinned people living in poverty in what appear to be refugee camps" src="https://i.guim.co.uk/img/media/c4f9a6f4fa8474a2c2ac158c0745bdceab2c7ae0/5_0_2227_1211/master/2227.jpg?width=445&amp;dpr=1&amp;s=none&amp;crop=none" width="445" height="241.98248765154915" loading="lazy"/></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>A screen grab showing AI-generated images of ‘poverty’ on a stock photo site. Pictures such as these have raised concerns over biased imagery and stereotypes.</span> Illustration: Freepik</figcaption></figure><p>In the past, leading charities have used AI-generated images as part of their communications strategies on global health. In 2023, the Dutch arm of UK charity Plan International released a <a href="https://www.youtube.com/watch?v=wD_JjBWddq8" data-link-name="in body link">video campaign against child marriage</a> containing AI-generated images of a girl with a black eye, an older man and a pregnant teenager.</p><p>Last year, the <a href="https://www.youtube.com/watch?v=S38jbjC47oY" data-link-name="in body link">UN posted a video</a> on YouTube with AI-generated “re-enactments” of sexual violence in conflict, which included AI-generated testimony from a Burundian woman describing being raped by three men and left to die in 1993 during the country’s civil war. The video was removed after the Guardian contacted the UN for comment.</p><p>A UN Peacekeeping spokesperson said: “The video in question, which was produced over a year ago using a fast-evolving tool, has been taken down, as we believed it shows improper use of AI, and may pose risks regarding information integrity, blending real footage and near-real artificially generated content. </p><p>Arnold said the rising use of these AI images comes after years of debate in the sector around ethical imagery and dignified storytelling about poverty and violence. “Supposedly, it’s easier to take ready-made AI visuals that come without consent, because it’s not real people.”</p><figure id="0ba426e9-6789-453f-8fd9-09f2df65280d" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&#34;richLinkIndex&#34;:16,&#34;element&#34;:{&#34;_type&#34;:&#34;model.dotcomrendering.pageElements.RichLinkBlockElement&#34;,&#34;prefix&#34;:&#34;Related: &#34;,&#34;text&#34;:&#34;How AI monitoring is cutting stillbirths and neonatal deaths in a clinic in Malawi&#34;,&#34;elementId&#34;:&#34;0ba426e9-6789-453f-8fd9-09f2df65280d&#34;,&#34;role&#34;:&#34;richLink&#34;,&#34;url&#34;:&#34;https://www.theguardian.com/global-development/2024/dec/06/how-ai-monitoring-is-cutting-stillbirths-and-neonatal-deaths-in-a-clinic-in-malawi&#34;},&#34;ajaxUrl&#34;:&#34;https://api.nextgen.guardianapps.co.uk&#34;,&#34;format&#34;:{&#34;design&#34;:0,&#34;display&#34;:0,&#34;theme&#34;:0}}"><div data-print-layout="hide" data-link-name="rich-link-16 | 16" data-component="rich-link" data-name="placeholder"></div></gu-island></figure><p>Kate Kardol, an NGO communications consultant, said the images frightened her, and recalled earlier debates about the use of “poverty porn” in the sector.</p><p>“It saddens me that the fight for more ethical representation of people experiencing poverty now extends to the unreal,” she said.</p><p>Generative AI tools have long been found to replicate – and at times exaggerate – <a href="https://www.theguardian.com/technology/2024/mar/16/ai-racism-chatgpt-gemini-bias" data-link-name="in body link">broader societal biases</a>. The proliferation of biased images in global health communications may make the problem worse, said Alenichev, because the images could filter out into the wider internet and be used to train the next generation of AI models, a process which has been <a href="https://www.nytimes.com/interactive/2024/08/26/upshot/ai-synthetic-data.html" data-link-name="in body link">shown to amplify prejudice</a>.</p><p>A spokesperson for Plan International said the NGO had, as of this year: “adopted guidance advising against using AI to depict individual children”, and said the 2023 campaign had used AI-generated imagery to safeguard “the privacy and dignity of real girls”.</p><p>Adobe declined to comment.</p></div></div></div>
  </body>
</html>
