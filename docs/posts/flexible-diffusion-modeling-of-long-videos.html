<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://plai.cs.ubc.ca/2022/05/20/flexible-diffusion-modeling-of-long-videos/">Original</a>
    <h1>Flexible diffusion modeling of long videos</h1>
    
    <div id="readability-page-1" class="page"><div>
					
<p>PLAI group members <a href="https://www.cs.ubc.ca/~wsgh/" target="_blank" rel="noopener">Will Harvey</a>, <a href="https://www.cs.ubc.ca/~saeidnp/" target="_blank" rel="noopener">Saeid Naderiparizi</a>, <a href="https://vmasrani.github.io/" target="_blank" rel="noopener">Vaden Masrani</a>, <a href="https://whilo.github.io/" target="_blank" rel="noopener">Christian Weilbach</a> (under the supervision of <a href="http://www.cs.ubc.ca/~fwood/">Dr. Frank Wood</a>) have just released a paper on an astounding new deep generative model for video.  Think <a href="http://www.openai.com/" target="_blank" rel="noopener">OpenAI</a>‘s <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener">GPT-3</a> but, instead of generating text given a prompt, their “Flexible Diffusion Model” (FDM) completes videos given a few frames of context.  What is more, FDM (described in a recent arXiv paper entitled <a href="http://arxiv.org/abs/2205.11495" target="_blank" rel="noopener">Flexible Diffusion Modeling of Long Videos</a>) generates <em>photorealistic, coherent</em> long videos like this (128x128x45000).</p>



<div>




<p>
<figure><video autoplay="" controls="" loop="" src="https://www.cs.ubc.ca/~wsgh/fdm/long-videos/long-carla_80_hierarchy-2.mp4"></video><figcaption>Be patient.  The generated video includes realistically “stopping” at traffic lights.</figcaption></figure>
</p>




</div>



<p>Dr. Wood says “This is simply the most impressive AI result I have personally seen in my career.  Long range coherence is a challenge even for modern language models with massive parameter counts.  Will, Saeid, Vaden, and Christian have taken a huge step forward by being able to stably generate coherent, photo-realistic 1hour+ long videos; 70x’s longer than their longest training video, and more than 2000x’s longer than the maximum of 20 frames they ever look at at once during training.  There is something very special in the training procedure they have developed and the architecture they employ.  Never have we been closer to being able to formulate AI agents that plan visually in domains with life-like complexity.”</p>



<p>The team experimented with training their model on videos gathered in multiple environments, from the <a href="https://carla.org/" target="_blank" rel="noreferrer noopener">CARLA self-driving car simulator</a> (provisioned via <a href="https://www.inverted.ai/" target="_blank" rel="noreferrer noopener">Inverted AI</a>‘s Simulate cloud platform), a <a href="https://minerl.io/" target="_blank" rel="noreferrer noopener">Minecraft reinforcement learning environment</a>, and the <a href="https://github.com/deepmind/gqn-datasets" target="_blank" rel="noreferrer noopener">Mazes</a> environment from the <a href="https://www.deepmind.com/" target="_blank" rel="noreferrer noopener">DeepMind</a> <a href="https://github.com/deepmind/lab" target="_blank" rel="noreferrer noopener">Lab</a> suite.  The video used to train FDM was collected from the first-person point of view as agents moved around in these environments.  In CARLA a car drove around a single small town (Town 01), stopping at traffic lights.  Otherwise it just cruised randomly in different weather conditions and at different times of day.  In MineRL, video was collected from agents that moved more or less in straight lines through different MineCraft worlds to a goal block 64 blocks away.  And in the Mazes environment agents moved from random starting positions to a random goal positions in procedurally generated maze worlds with brightly coloured and textured walls and floor. </p>



<p>Let’s see what FDM learned in each environment.  </p>



<p>Here’s CARLA Town01:</p>



<figure><video autoplay="" controls="" loop="" src="https://www.cs.ubc.ca/~wsgh/fdm/video_arrays/carla_adaptive-hierarchy-2.mp4"></video><figcaption>Top row: test videos (128x128x1000).  Bottom row: generated videos (128x128x1000).  Red outline: “observed frames.”  Test videos consist solely of observed frames.  Bottom row videos are generated <em>condition</em>ally given the frames highlighted in red.   Pauses at stoplights occur in the training data and are realistically reflected in the generated video resulting in long “pauses.”</figcaption></figure>



<p>Here’s MineRL:</p>



<figure><video autoplay="" controls="" loop="" src="https://www.cs.ubc.ca/~wsgh/fdm/video_arrays/minerl_optimal-autoreg-nice.mp4"></video><figcaption>Top row: Test videos (64x64x500).  Bottom three rows: three different generated video continuations (64x64x500) generated conditionally given the red-highlighted frames at the start of each video.</figcaption></figure>



<p>Here’s Mazes</p>



<figure><video autoplay="" controls="" loop="" src="https://www.cs.ubc.ca/~wsgh/fdm/video_arrays/mazes_cwvae_hierarchy-2_5_3.mp4"></video><figcaption>Top row: Test video (64x64x300).  Bottom three rows: three different generated video continuations (64x64x300) generated conditionally given the red-highlighted frames at the start of each video.</figcaption></figure>



<p>Whole books could be written about what we see, good and bad, in these example videos.  In CARLA the video model sometimes jumps from one location to another distant location that looks similar.  Traffic lights aren’t captured well.  But the yellow lines and object constancy evidenced through building appearance around corners are encouraging.  As are weather, shadows, and so forth.  FDM arguably just has to memorize this small town, but the CARLA training dataset was about 11Gb of video data and FDM only has 78M parameters, so, some kind of generalization is happening.  The MineRL environment makes this much more clear.  Every training video in MineRL is a from a <em>different world</em>!  This means that the visual futures FDM images reflect the MineCraft engine’s world generative model parameters as well — for instance how often hills are followed by plateaus with forest vs. valleys with villages and water.  As FDM imagines MineCraft futures it is visually hallucinating entirely new worlds, obeying MineCraft blockiness and biome transition rules.  The MineRL agent action space also includes block breaking.  Look at the middle column which exhibits, in the video generative model, block breaking and mining!  In Mazes we see nearly pixel perfect environment generation, but, when the agent clearly “returns” to the same place in the maze (to us), the world it generates there can be different.  This means that semantic drift shows up visually here and indicates the necessity of some kind of complimentary memory system.  The video is still coherent though, and the generative model, amazingly, does not diverge into blurriness or craziness, it just continues creating an ever changing maze-like world.  </p>



<p>Here is one last thing this model can do.  Using the CARLA Town 01 training data we trained a CNN to decode x,y map position from a single front view frame.  This is possible because Town 01 is small and the view from each point in its map is reasonably distinct.  We then generated a CARLA Town 01 video from FDM and fed the <em>generated</em> frames into this “place regressor.”  The results from this are amazing.</p>



<figure><video autoplay="" controls="" loop="" src="https://www.cs.ubc.ca/~wsgh/fdm/long-videos/carla_80_hierarchy-2_no_interp.mp4 "></video><figcaption>Left: generated video.  Right: inferred location in CARLA Town 01 using a regressor from single image (1 frame) to location.  We now use such “paths taken” by a video generative model as a new way of evaluating semantic coherence of video generative models.  Note that here again FDM “stops” at true stop-light locations in CARLA Town 01, so, be patient at times.  Note also that this particular manner of generating from FDM has the highest empirical scores on many measures but notably jumps around the map more than alternatives.</figcaption></figure>



<p>We are working on improving goal-conditioned video generation for vision-based planning, integrating actions and rewards explicitly into FDM, and are conducting studies on FDM’s capabilities in more complex environments, looking particularly forward to seeing what happens when we add other agents into the CARLA environment and what happens when we include both more CARLA towns but also dash-cam video from the real world.  </p>
									</div></div>
  </body>
</html>
