<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://positive.security/blog/video-depixelation">Original</a>
    <h1>Recovering redacted information from pixelated videos</h1>
    
    <div id="readability-page-1" class="page"><div><p>Information that has been redacted is often the most interesting. It&#39;s therefore no wonder that some people might have a motivation to try to reverse such a redaction for various reasons.</p><p>In this blog post, I&#39;ll discuss image/video blurring methods and their weaknesses and present a simple yet effective method to get a high-resolution image from a pixelated video in order to recover redacted information (with no guessing involved).</p><p>One of the <a href="https://www.schneier.com/blog/archives/2005/05/pdf_radacting_f.html">best known</a> attacks on digital redaction methods works only on specific file formats, e.g. PDFs and Office documents. As text and objects/images in those files are stored as different objects in &#34;separate layers&#34;, redacted text can sometimes still simply be copied, and colored boxes that were intended to hide information can often be removed to reveal the redacted content. This technique however does not work for simple image formats that only store one layer of pixel information (except for <a href="https://twitter.com/wdormann/status/1215746766659837953">surprising edge cases when using transparency</a>).</p><p>A popular alternative to redacting information with a colored box is blurring. Instead of removing all information for a particular region, here the information density is only reduced.</p><p>Blurring is usually achieved using one of the following two methods:</p><ul role="list"><li><strong>Mosaic/Pixelization/Box Linear Filter:</strong> Multiple pixels are merged to a bigger one with the color being the average of the original pixel values. This makes everything look pixelated.</li><li><strong>Gaussian blur </strong>(or similar): Using a <a href="https://en.wikipedia.org/wiki/Gaussian_filter#Digital_implementation">specific kernel</a>, the new color of every pixel is influenced by its surrounding pixels (via a weighted average). This makes everything look blurry.</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd3ddbcfb0c52f7bb87f7_logo_with_border_blurred_50_50.png" loading="lazy" alt=""/></p></a><figcaption>Top Left: Mosaic Filter - Bottom Right: Gaussian Blur Filter</figcaption></figure><p>One of my favorite unblurring stories is about a Bitcoin Cash private key that was <a href="https://youtu.be/hN0_1pU4U5w?t=1060">shown on French Television</a>. An entrepreneur wanted to give away some crypto during an interview, but since the station was not authorized to give out money prizes during that TV transmission, the private key information was blurred. By combining tiny pieces of information from different camera shots and knowledge about the encoding scheme of a QR code, two researchers could recover the private key and claim the prize (after <a href="https://www.freecodecamp.org/news/lets-enhance-how-we-found-rogerkver-s-1000-wallet-obfuscated-private-key-8514e74a5433/">16 hours of manual reverse engineering</a>).</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd3eba0ce483f2b0a73f7_qr_tv_segment.png" loading="lazy" alt=""/></p></a></figure><p>While this attack was very specific to QR codes and combined different manual techniques, more generic methods have also been published.</p><h2>Image Unblurring</h2><p>All of the following attacks share the same core idea: Pixelate potential input data and compare the results.</p><p>Similar to a dictionary attack, only probable inputs are compared, greatly increasing the brute force efficiency. The techniques also exploit the missing <a href="https://en.wikipedia.org/wiki/Avalanche_effect">Avalanche effect</a> of the Mosaic &#34;hash function&#34;: When a detail in the input image changes, this will only affect a small area of the output image, thus making it possible to divide the problem into easier to solve sub-problems and only search for matches of smaller regions.</p><ul role="list"><li><a href="http://dheera.net/projects/blur">Recovering blurred credit card numbers by bruteforcing possible combinations</a> (2007)</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd3f6febedb36cf722ed8_unblur_1.png" loading="lazy" alt=""/></p></a></figure><ul role="list"><li><a href="https://github.com/beurtschipper/Depix">Recovering text by brute forcing possible inputs</a> (2020). As input, Depix takes a <a href="https://en.wikipedia.org/wiki/De_Bruijn_sequence">De Bruijn sequence</a> with expected characters with the same font settings as the blurred image</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd401cbb92177c35fca84_unblur_2.png" loading="lazy" alt=""/></p></a></figure><ul role="list"><li><a href="https://hovav.net/ucsd/dist/redaction.pdf">Recovering text using Hidden Markov Models</a> (2016). These results seem to be less dependent on the font and the paper also attacks Gaussian blur by mosaicing it first (but no code available)</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd40bbcfb0c70e2bb885b_unblur_3.png" loading="lazy" alt=""/></p></a></figure><ul role="list"><li><a href="http://pulse.cs.duke.edu/">Recovering high-resolution face images from low-res images</a> (2020). <a href="https://thispersondoesnotexist.com/">The PULSE algorithm generates faces using a GAN</a> and tweaks them until the mosaiced version matches the input image</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61dde2839bf28f1f3c84e61f_face_unblur_2.png" loading="lazy" alt=""/></p></a></figure><ul role="list"><li>Since a blurred image can also be interpreted as just a low-resolution image, any kind of AI-based image &#34;superresolution&#34; algorithm could be suitable (e.g. <a href="https://github.com/idealo/image-super-resolution">ISR</a> or <a href="https://github.com/thunil/TecoGAN">TecoGAN</a> (GAN with temporal coherence))</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd4343c03727de36fb1ca_sr_2.gif" loading="lazy" alt=""/></p></a><figcaption>TecoGAN (row 1: Reference, row 2: TecoGAN output, row 3: TecoGAN input)</figcaption></figure><ul role="list"><li>A GAN-based superresolution and text recognition algorithm was presented in 2016 as TextSR (also no code available)</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd43d691b2c84e58f3ecf_sr_3.png" loading="lazy" alt=""/></p></a></figure><p>Those results are definitely impressive, but due to the involved creativity of the AI, they can not be fully trusted. For a hacker, that can uncover a redacted QR code or credit card number this way (and check its validity via its checksum), this does not matter, but I hope PULSE-upscaled faces won&#39;t find their way into courts soon.</p><p>Due to the information loss inherent to image downscaling, this &#34;creativity&#34; is however essential to achieving the impressive-looking results of AI-based approaches.</p><h2>Video unblurring</h2><p>In comparison to a single image, a video provides much more possibilities for mistakes, some of which eradicate the need for unblurring completely:</p><ul role="list"><li>Bad object tracking (in the following shot of a <a href="https://en.wikipedia.org/wiki/Funk_(service)">Funk</a> documentary on hacking, the URL that was supposed to be blurred, was not properly motion-tracked and shown for a few frames)</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd44a8d10e293f88e2d72_vid_unblur_1.png" loading="lazy" alt=""/></p></a></figure><ul role="list"><li>Missing blur for a full video section (e.g. in the following shot of another Funk documentary on cyber bullying, the <a href="https://haveibeenpwned.com/">HaveIBeenPwned</a> results were blurred, but the reporter&#39;s private email address was not)</li></ul><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd457691b2ce21a8f3f07_vid_unblur_2.png" loading="lazy" alt=""/></p></a><figcaption>Blurred HIBP results</figcaption></figure><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd64301387a63638da513_vid_unblur_3_censored.png" loading="lazy" alt=""/></p></a><figcaption>Unblurred reporter&#39;s email address which could also be used to retrieve the previously blurred result page (and cyber bully her?) (the black box was added by me)</figcaption></figure><ul role="list"><li>Missing blur in the first/last frame after/before a cut</li><li>Multiple camera shots, each leaking different information that can be combined (see the BTC QR code story)</li></ul><p>In some of the observed cases, the mishaps were fixed after being noticed (e.g. <a href="https://www.youtube.com/watch?v=fiCtEg7atxs&amp;t=540s">both</a> <a href="https://www.youtube.com/watch?v=rkw4D90dM_I&amp;t=1069s">examples</a> shown above). While this is usually desirable to reduce the spread of this information, it also makes it possible for a malicious actor to detect them easily and in an automated fashion by immediately downloading new uploads from documentary/news channels and diffing them with a download from a later date.</p><p>In case all frames are correctly blurred, a different approach is required.</p><p>Since a pixelated video can also be interpreted as low-resolution, using video upscaling techniques could be a viable approach. The current state-of-the art algorithms use some form of machine learning such as a Convolutional Neural Networks (<a href="https://arxiv.org/pdf/1611.05250.pdf">VESPCN</a>) or Generative Adversarial Networks (<a href="https://github.com/thunil/TecoGAN">TecoGAN</a>).</p><p><strong>Side note: </strong>The potentially most extensive research on the problem of programmatically unblurring mosaic&#39;ed regions from videos was done by Japanese Adult Video enthusiasts. <a href="https://www.javplayerfree.com/">Javplayer</a> automatically detects blurred regions and performs upscaling via TecoGAN, and another person spent months improving their custom GAN that was trained with leaked videos (search for &#34;De-Mosaic JAV with AI, Deep Learning and Adversarial Networks&#34;).</p><p>After seeing many videos where information has been blurred using the Mosaic method, I wanted to implement an idea that came to my mind: Since the camera is often moving relative to the blurred object, the boundaries for the bigger squares are also often moving relative to the real world. Therefore, by observing the color difference of the bigger squares in between frames, and correlating it with the movement, it should be possible to extract additional information and create an output image with a higher resolution.</p><p>The following animations illustrate this idea. A black bar in different positions is &#34;redacted&#34; using a mosaic pattern. In the first animation, a slight right shift of the mosaic in relation to the redacted content moves the gray block to the left. This suggests that the dark pixels causing the overall gray were located at the left border of the original mosaic pattern.</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd473eac0e99d4b4f4739_animation_2.gif" loading="lazy" alt=""/></p></a></figure><p>Compare this to the next animation where the black bar is located slightly further to the right. Here, the movement did not change the mosaic coloring:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd47dd0feb8f1d0cba21a_animation_3.gif" loading="lazy" alt=""/></p></a></figure><p>In contrast to an AI-based approach, this would not be inventing information, but extracting spatial data (higher resolution) from existing temporal data (multiple frames). The differences to classic video upscaling are, that the larger part of the video already has a high resolution (allowing for more precise object tracking) and that a single super-high-resolution frame is an acceptable output (compared to a higher-resolution video).</p><p>To test the hypothesis, I created a simply example &#34;video&#34; by slightly moving the following input image between each frame while keeping a mosaic blur at the same location.</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd4dd0a3667ba1667d029_AB_1.png" loading="lazy" alt=""/></p></a></figure><p>The result was this image sequence:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd48bd396cdfdbdfd1bba_animation_4.gif" loading="lazy" alt=""/></p></a></figure><p>By reversing the image movements and putting all images on top of each other (either using GIMP or programmatically), I was able to get the following result:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd4f08d10e221ac8e2ecd_AB_2.png" loading="lazy" alt=""/></p></a><figcaption>Overlapped images with the movement reversed</figcaption></figure><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd4f9d396cd408efd1ef8_AB_3.png" loading="lazy" alt=""/></p></a><figcaption>Left: One frame of the input video - Right: GIMP/Python output of reversing the motion and overlaying images</figcaption></figure><p>As an alternative, I also tried upscaling the mosaic&#39;ed video via TecoGAN, which does not help (much) with improving readability:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61dde0110011758f37408282_AB_4_tecogan_bigger.gif" loading="lazy" alt=""/></p></a><figcaption>TecoGAN-upscaled video</figcaption></figure><p>While the output image was not as detailed as the input, the approach seemed promising, so I created a test video with my phone camera (Moto G9 Plus) and a document I had laying around (coincidentally a request to pay the TV tax that <a href="https://en.wikipedia.org/wiki/Funk_(service)">paid</a> for the above documentaries). I then retracted the IBAN using the mosaic filter in <a href="https://shotcut.org/">Shotcut</a>:</p><p><figure width="50%">
<video preload="none" poster="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61dddb4fc4e7c9800b208e5d_Recording_blurred_250f-poster-00001.jpg" controls="" data-wf-ignore="true" data-object-fit="cover">
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61dddb4fc4e7c9800b208e5d_Recording_blurred_250f-transcode.mp4" data-wf-ignore="true"/>
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61dddb4fc4e7c9800b208e5d_Recording_blurred_250f-transcode.webm" data-wf-ignore="true"/></video>
<figcaption>Letter with the IBAN pixelated using Shotcut&#39;s Mosaic filter</figcaption>
</figure></p><p>As I did not create the image sequence computationally this time, I did initially not know anything about the movement between frames. Therefore I imported the video clip into Blender, added tracking markers in high-contrast areas and let Blender track them throughout the video.</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd52159c1185b275e399e_gez_1.png" loading="lazy" alt=""/></p></a></figure><p>Using one marker close to the blurred area for the position, and two other markers further away to calculate the scale and rotation, I created a motion-tracked and stabilized video:</p><p><figure width="50%">
<video preload="none" poster="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddea759236b4674d6e79f8_gez_iban_stabilized-poster-00001.jpg" controls="" data-wf-ignore="true" data-object-fit="cover">
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddea759236b4674d6e79f8_gez_iban_stabilized-transcode.mp4" data-wf-ignore="true"/>
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddea759236b4674d6e79f8_gez_iban_stabilized-transcode.webm" data-wf-ignore="true"/></video>
<figcaption>Stabilized footage</figcaption>
</figure></p><p>Overlapping all frames via a small Python script resulted in the following image:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd53aff9ad307fac4ffe0_gez_2.png" loading="lazy" alt=""/></p></a></figure><p>From there, the redacted text is quite readable and seems to be &#34;DE3000<strong>1</strong>00000001272&#34; (correct) or maybe &#34;DE3000<strong>7</strong>00000001272&#34;.</p><p>As a next step, this improved output image could then of course also be fed into one of the previously mentioned image unblurring/text recovery techniques.</p><h2>Comparison with other techniques</h2><p>I was quite happy with those results and curious how they would compare, so I tested three other techniques that were easily accessible (meaning that the code/tool was released and is usable without a lengthy setup and debugging process):</p><ul role="list"><li><a href="https://www.topazlabs.com/video-enhance-ai">Topaz Video Enhance AI</a>, a popular (commercial) AI-based video upscaling tool that claims to also use temporal information from multiple frames</li><li><a href="http://www.infognition.com/VideoEnhancer/">VideoEnhancer</a>, another video superresolution tool (apparently not in development anymore) that also mentions &#34;subpixel accurate motion compensation&#34;</li><li><a href="https://github.com/rafaelmaeuer/MultiFrameSuperResolution">MFSR</a>, a MATLAB-based open source implementation of various algorithms to generate one high-res image out of a lower-res video</li></ul><p>Attempting to use Video Enhance AI, even on maximum upscaling and quality settings, did not yield any improvement on the blurred text&#39;s readability:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddf582ed8397436f6d44aa_gez_3_Topaz_Comparison_cropped.png" loading="lazy" alt=""/></p></a></figure><p>Downscaling the whole mosaiced video with a lossless video codec to match the resolution of the mosaic (<a href="http://www.infognition.com/tutorials/how_to_remove_mosaic.html">for better results</a>) and then iteratively applying VideoEnhance&#39;s SR algorithm with the best settings produced the following illegible result:</p><p><figure width="50%">
<video preload="none" poster="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61de1ef563ddc4f6c76f3a3a_gez_small-sr-poster-00001.jpg" controls="" data-wf-ignore="true" data-object-fit="cover">
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61de1ef563ddc4f6c76f3a3a_gez_small-sr-transcode.mp4" data-wf-ignore="true"/>
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61de1ef563ddc4f6c76f3a3a_gez_small-sr-transcode.webm" data-wf-ignore="true"/></video>
<figcaption>Video upscaled by VideoEnhance</figcaption>
</figure></p><p>I then tried MFSR and played with different Res-Factors and SR algorithms, but could also not achieve a readable result:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddf59ca1ec47613b710e79_gez_4_msfr_fast_out_8r_cropped.png" loading="lazy" alt=""/></p></a></figure><h2>Real-world example</h2><p>The presented technique relies on alignment changes between a real-world object and the mosaic pattern. While in the previous examples, the mosaic pattern was at a static location with only the camera moving, this is not a requirement, and the technique should also work with a moving mosaic grid (as long as it does not <em>perfectly</em> line up with the hidden real-world object throughout the whole sequence).</p><p>The following snippet is from yet <a href="https://youtu.be/4KJK-cVruQs?t=100">another Funk documentary</a>, where they go undercover to a gun market:</p><p><figure width="50%">
<video preload="none" poster="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddd931c69da34c5d7424f4_plate_vid-poster-00001.jpg" controls="" data-wf-ignore="true" data-object-fit="cover">
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddd931c69da34c5d7424f4_plate_vid-transcode.mp4" data-wf-ignore="true"/>
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddd931c69da34c5d7424f4_plate_vid-transcode.webm" data-wf-ignore="true"/></video>
<figcaption>Pixelated number plate in Funk documentary</figcaption>
</figure></p><p>Applying the same motion tracking technique from above (with markers near the number plate to keep its location static) results in the following tracked video:</p><p><figure width="50%">
<video preload="none" poster="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddea95653e69ec7189f6ae_plate_stabilized-poster-00001.jpg" controls="" data-wf-ignore="true" data-object-fit="cover">
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddea95653e69ec7189f6ae_plate_stabilized-transcode.mp4" data-wf-ignore="true"/>
<source src="https://uploads-ssl.webflow.com/5f6498c074436c50c016e745/61ddea95653e69ec7189f6ae_plate_stabilized-transcode.webm" data-wf-ignore="true"/></video>
<figcaption>Motion-tracked number plate</figcaption>
</figure></p><p>When overlaying the frames, I ignored 5 frames with a high amount of motion blur due to camera shake. The result can be seen here:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd6e4c62f7f79a867ab70_plate_1_censored.png" loading="lazy" alt=""/></p></a></figure><p>As we&#39;re only interested in the number plate, I then used Gimp to change the contrast, emphasizing the fine differences in gray that we reconstructed earlier:</p><figure><a href="https://positive.security/#zoom" target="_blank"><p><img src="https://uploads-ssl.webflow.com/5f6498c074436c349716e747/61ddd6f235605c0c47bc39eb_plate_2_censored.png" loading="lazy" alt=""/></p></a></figure><p>While again not completely conclusive, I think there is a high chance that the number plate part shown above says  &#34;J 1354&#34; or &#34;J 1314&#34;. This time however, without the original footage, I can&#39;t verify whether that&#39;s correct.</p><p><strong>Notes:</strong></p><ul role="list"><li>Law enforcement could obviously also use partial number plate information to e.g. track down a specific vehicle of a known brand and color</li><li>The outcome could probably further be improved by performing 3D motion tracking, or weighing frames based on the amount of new information they provide</li></ul><p>This specific (or a closely related) computer vision problem is called &#34;Multi-Frame Super Resolution&#34; and already received quite some attention.</p><p>One of the earliest related work I found was from 1996, titled &#34;<a href="https://ieeexplore.ieee.org/document/503915">Extraction of high-resolution frames from video sequences</a>&#34;.</p><p>In 2006, a <a href="http://people.duke.edu/~sf59/TIP_Demos_Final_Color.pdf">paper was published</a>, presenting a way to tackle the <a href="https://en.wikipedia.org/wiki/Demosaicing#Video_super-resolution/demosaicing">related problems of &#34;Color Demosaicing&#34; (for digital sensors) and Superresolution</a> together.</p><p>In 2018, <a href="https://ai.googleblog.com/2018/10/see-better-and-further-with-super-res.html">Google implemented such a technique for their flagship smartphones</a> with impressive results, and released a paper at SIGGRAPH 2019:</p><blockquote>In this paper, we supplant the use of traditional demosaicing in single-frame and burst photography pipelines with a multi-frame super-resolution algorithm that creates a complete RGB image directly from a burst of CFA raw images. We harness natural hand tremor, typical in handheld photography, to acquire a burst of raw frames with small offsets</blockquote><p>Two extensive and well curated lists with more research in this area can be found here:</p><ul role="list"><li><a href="https://github.com/subeeshvasu/Awesome-Deblurring">https://github.com/subeeshvasu/Awesome-Deblurring</a></li><li><a href="https://github.com/Pea-Shooter/awesome-video-enhancement">https://github.com/Pea-Shooter/awesome-video-enhancement</a></li></ul><p>So while the idea and technique presented in this blog post are not new, I did not find any research that focuses on recovering information from intentionally redacted videos, which has several key distinctions from other use cases:</p><ul role="list"><li>Some parts of the video are high-resolution, allowing for high-precision motion tracking</li><li>The frames are from a long time period, resulting in more camera and/or object movement compared to a burst of images</li><li>As the focus is only on retrieving the redacted information, it&#39;s not necessary to create an overall good looking picture. Motion tracking e.g. can be optimized to only keep the blurred area as still as possible</li><li>The desired increase in resolution is very high and computation time almost doesn&#39;t matter</li></ul><p>Please let me know if you are aware of such research.</p><p>In this blog post, I discussed various image and video (un-)blurring methods and presented a simple yet effective technique to potentially uncover redacted information from pixelated videos.</p><p>Content creators and journalists should be aware of the additional risks when redacting information in videos and use a sufficiently high mosaic size/blur radius, or better yet, use an opaque, single-colored box.</p><p>Furthermore, the search for information leaks must happen before publishing (preferably by another person with a fresh view) and should not be crowd-sourced to the first viewers.</p><p>‍</p></div></div>
  </body>
</html>
