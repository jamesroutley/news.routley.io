<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1">Original</a>
    <h1>How does GPT obtain its ability? Tracing emergent abilities of language models</h1>
    
    <div id="readability-page-1" class="page">
		<p><strong><a href="https://franxyao.github.io">Yao Fu</a></strong>, <a href="https://yaofu.notion.site/cdn-cgi/l/email-protection#afd6cec081c9daefcacb81cecc81dac4"><span data-cfemail="730a121c5d15063316175d12105d0618">[email protected]</span></a></p>
<p>University of Edinburgh ****</p>
<p>with <strong><a href="https://haopeng-nlp.github.io">Hao Peng</a></strong> and <strong><a href="https://allenai.org/team/tushark">Tushar Shot</a></strong></p>
<p>work done at Allen Institute for AI</p>
<p>Thank <a href="https://jxhe.github.io"><strong>Junxian He</strong></a> @SJTU, <a href="https://lupantech.github.io"><strong>Lu Pan</strong></a> @UCLA, <a href="https://www.cs.dartmouth.edu/~rbliu/">**Ruibo Liu</a>** @Dartmouth for insightful initial discussions and suggestions.</p>
<p>Thank <a href="http://prithvirajva.com"><strong>Raj Ammanabrolu</strong></a> @AI2, <strong><a href="https://peterjliu.com">Peter Liu</a></strong> @Google Brain for discussions and suggestions after release, which greatly improved the comprehensiveness.</p>
<p>Started writing on Thu Dec 08, 2022, Released on Dec 11, 2022</p>
<p>Other versions: [pdf] [Arxiv] (don’t click, not done yet)</p>
<p>Discuss on <a href="https://twitter.com/Francis_YAO_/status/1602213927102066688?s=20&amp;t=9wkRcr0wva_RCaKpsRjFfw">twitter</a> with the author</p>
<p>TL; DR</p>
<p><a href="https://embed.notionlytics.com/wt/ZXlKd1lXZGxTV1FpT2lKaU9XRTFOMkZqTUdaalpqYzBaak13WVRGaFlqbGxNMlV6Tm1aaE1XUmpNU0lzSW5kdmNtdHpjR0ZqWlZSeVlXTnJaWEpKWkNJNklrTnlVbFp3WkVOMWEyRnJNblU1U0hWVVdXUjNJbjA9"></a><a href="https://embed.notionlytics.com/wt/ZXlKd1lXZGxTV1FpT2lKaU9XRTFOMkZqTUdaalpqYzBaak13WVRGaFlqbGxNMlV6Tm1aaE1XUmpNU0lzSW5kdmNtdHpjR0ZqWlZSeVlXTnJaWEpKWkNJNklrTnlVbFp3WkVOMWEyRnJNblU1U0hWVVdXUjNJbjA9">https://embed.notionlytics.com/wt/ZXlKd1lXZGxTV1FpT2lKaU9XRTFOMkZqTUdaalpqYzBaak13WVRGaFlqbGxNMlV6Tm1aaE1XUmpNU0lzSW5kdmNtdHpjR0ZqWlZSeVlXTnJaWEpKWkNJNklrTnlVbFp3WkVOMWEyRnJNblU1U0hWVVdXUjNJbjA9</a></p>
<p>Recently, the field has been greatly impressed and inspired by OpenAI’s ChatGPT. It is undoubtedly clever, capable, and very fun to talk to. Its multi-faceted abilities are significantly beyond many NLP researchers’ and practitioners’ expectations based on the impression of (not-that-strong) Original GPT-3. The natural question is how ChatGPT gets there, and where these fantastic abilities come from. In this post, we try to dissect the emergent abilities and trace them to their sources, hoping to give a comprehensive roadmap about how the GPT-3.5 model family, along with related large language models, evolved to their current forms.</p>
<p>We hope this post can promote the transparency of large language models and serve as the roadmap for the community’s ongoing efforts of reproducing GPT-3.5.</p>
<p>To readers:</p>
<ul>
<li>leave a message if you feel any part of this article is not supported by strong enough evidence. You can directly comment on the corresponding part/ email me/ comment on my twitter to request clarification.</li>
<li>Please do contact me if you want to translate this article into other languages.</li>
<li>如果有中国的同学想把这篇文章翻译成中文，请加我的微信。</li>
</ul>
<p><strong>Table of Content</strong></p>

	

</div>
  </body>
</html>
