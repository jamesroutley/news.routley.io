<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.google/blog/scaling-up-linear-programming-with-pdlp/">Original</a>
    <h1>Scaling up linear programming with PDLP</h1>
    
    <div id="readability-page-1" class="page"><div data-gt-publish-date="20240920">
                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    <p data-block-key="vflng">Classic <a href="https://en.wikipedia.org/wiki/Linear_programming" target="_blank" rel="noopener noreferrer">linear programming</a> (LP) problems are one of the most foundational problems in computer science and operations research. With extensive applications across numerous sectors of the global economy, such as manufacturing, networking, and other fields, LP has been the cornerstone of mathematical programming and has significantly influenced the development of today’s sophisticated modeling and algorithmic frameworks for data-driven decision making. If there&#39;s something to optimize, there&#39;s a good chance LP is involved.</p><p data-block-key="c5c0o">Since the late 1940s, LP solving has evolved significantly, with the <a href="https://en.wikipedia.org/wiki/Simplex_algorithm" target="_blank" rel="noopener noreferrer">simplex method</a> by Dantzig and various <a href="https://en.wikipedia.org/wiki/Interior-point_method" target="_blank" rel="noopener noreferrer">interior-point methods</a> being the most prevalent techniques. Today&#39;s advanced commercial LP solvers utilize these methods but face challenges in scaling to very large instances due to computational demands. In response to this limitation, <a href="https://en.wikipedia.org/wiki/Category:First_order_methods" target="_blank" rel="noopener noreferrer">first-order methods</a> (FOMs) have gained traction for large-scale LP problems.</p><p data-block-key="3ip58">With the above in mind, we introduce our solver PDLP (<a href="https://link.springer.com/article/10.1007/s10851-010-0251-1" target="_blank" rel="noopener noreferrer">Primal-dual hybrid gradient</a> enhanced for LP), a new FOM–based LP solver that significantly scales up our LP solving capabilities. Utilizing <a href="https://en.wikipedia.org/wiki/Matrix_multiplication#Definitions" target="_blank" rel="noopener noreferrer">matrix-vector multiplication</a> rather than <a href="https://en.wikipedia.org/wiki/Matrix_decomposition" target="_blank" rel="noopener noreferrer">matrix factorization</a>, PDLP requires less memory and is more compatible with modern computational technologies like GPUs and distributed systems, offering a scalable alternative that mitigates the memory and computational inefficiencies of traditional LP methods. PDLP is open-sourced in Google’s <a href="https://github.com/google/or-tools" target="_blank" rel="noopener noreferrer">OR-Tools</a>. This project has been in development since 2018 [<a href="https://proceedings.neurips.cc/paper/2021/file/a8fbbd3b11424ce032ba813493d95ad7-Paper.pdf" target="_blank" rel="noopener noreferrer">1</a>, <a href="https://arxiv.org/abs/2105.12715" target="_blank" rel="noopener noreferrer">2</a>, <a href="https://epubs.siam.org/doi/full/10.1137/22M1510467" target="_blank" rel="noopener noreferrer">3</a>], and we are proud to announce that it was co-awarded the prestigious <a href="https://www.mathopt.org/?nav=boh" target="_blank" rel="noopener noreferrer">Beale — Orchard-Hays Prize</a> at the <a href="https://ismp2024.gerad.ca/" target="_blank" rel="noopener noreferrer">International Symposium of Mathematical Programming</a> in July 2024. This accolade is one of the highest honors in the field of computational optimization, awarded every three years by the <a href="https://www.mathopt.org/" target="_blank" rel="noopener noreferrer">Mathematical Optimization Society</a>.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    <h2 data-block-key="vflng">LP and first-order methods for LP</h2><p data-block-key="covcv">Scaling the methods used in today’s state of the art LP solvers presents significant challenges. The primary computational limitations for both methods relate to matrix factorization required for solving linear equations, introducing two key challenges as problem sizes grow:</p><ol><li data-block-key="ekud4"><i>Memory overflows:</i> LP solvers that use the simplex method (such as Google&#39;s <a href="https://en.wikipedia.org/wiki/GLOP" target="_blank" rel="noopener noreferrer">GLOP</a>) employ <a href="https://en.wikipedia.org/wiki/LU_decomposition" target="_blank" rel="noopener noreferrer">LU factorization</a>, and solvers that use the interior point method use <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition" target="_blank" rel="noopener noreferrer">Cholesky factorization</a>. For both these methods the resulting factorization uses considerably more memory than the LP instance itself.</li><li data-block-key="3dv18"><i>Hardware-related challenges:</i> Both methods face difficulties leveraging modern computing architectures, such as GPUs or distributed systems, because the sparse matrix factorization step usually requires highly sequential operations.</li></ol><p data-block-key="3ac05">Given the above limitations associated with traditional LP methods, FOMs have emerged as a promising alternative for tackling large-scale LP problems. Unlike methods that rely on matrix factorization, FOMs utilize gradient information to iteratively update their solutions, with the primary computational requirement being matrix-vector multiplication. This distinction means that FOMs require only the storage of the LP instance itself, without needing additional memory to store factorized forms. Additionally, advances in FOMs for machine learning and deep learning have enhanced their scalability, <a href="https://arxiv.org/abs/1606.04838" target="_blank" rel="noopener noreferrer">making them highly efficient</a> on modern computing platforms such as GPUs and distributed computing. This scalability and reduced memory dependency make FOMs particularly suitable for large and complex LP tasks where traditional methods may falter.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    <h2 data-block-key="vflng">Restarted primal-dual hybrid gradient for LP</h2><p data-block-key="a2f41"><a href="https://optimization-online.org/2010/06/2646/" target="_blank" rel="noopener noreferrer">Primal-dual hybrid gradient</a> (PDHG) is widely recognized for its application in image processing. When applied to LP, PDHG&#39;s primary computational demand involves matrix-vector multiplication, eliminating the need for matrix factorizations. This makes PDHG particularly efficient for large-scale computational tasks, but it is not reliable in solving LP. For example, in a benchmark of 383 instances, PDHG can only solve <a href="https://proceedings.neurips.cc/paper/2021/file/a8fbbd3b11424ce032ba813493d95ad7-Paper.pdf" target="_blank" rel="noopener noreferrer">113 instances to moderate accuracy</a>.</p><p data-block-key="4m87g">To enhance PDHG’s reliability in solving LP problems, we have developed a modified approach called <a href="https://arxiv.org/abs/2105.12715" target="_blank" rel="noopener noreferrer">restarted PDHG</a>. This version uses a two-loop structure where PDHG is run until a restarting condition is triggered, after which the average of the PDHG iterations is computed. The algorithm then restarts from this average point. This approach is visualized below where the trajectory of the standard PDHG is depicted with a blue line, the average iteration with a red line, and the restarted PDHG with a green line. Notably, the restarted PDHG shows a quicker convergence to the optimal solution, marked by a star on the plot.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    <p data-block-key="qp0k3">The intuition behind this faster convergence is that by restarting from the computed average at the end of each spiral phase, the restarted PDHG effectively shortens the path to convergence. This strategy leverages the cyclical nature of the PDHG spirals to expedite the solution process.</p><p data-block-key="e8fdh">We show in <a href="https://arxiv.org/abs/2105.12715" target="_blank" rel="noopener noreferrer">our research</a> that this restarting technique can significantly speed up the convergence behaviors of PDHG for LP both in theory and in practice. This establishes restarted PDHG as a highly efficient and theoretically sound method for tackling LP challenges, reinforcing its utility and effectiveness in computational optimization.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    <h2 data-block-key="qp0k3">PDLP</h2><p data-block-key="6ptml">We designed <a href="https://developers.google.com/optimization/lp/pdlp_math" target="_blank" rel="noopener noreferrer">PDLP</a> as a software package that can solve linear programming problems efficiently. The core algorithm of PDLP is based on the restarted PDHG, which we have enhanced significantly through five improvements:</p><ul><li data-block-key="336v2"><i>Presolving</i>: This process simplifies the LP problem before solving. It involves detecting inconsistent bounds, detecting duplicate rows, tightening bounds, etc. These steps reduce complexity and improve the efficiency of the solver.</li><li data-block-key="60lg4"><i>Preconditioning</i>: A preconditioner in PDLP rescales variables and constraints within the LP instance. This adjustment helps speed up the algorithm by optimizing the numerical condition of the problem, thereby enhancing convergence rates.</li><li data-block-key="fo387"><i>Infeasibility detection</i>: In real-world scenarios, LP problems may often be infeasible or unbounded. Our approach utilizes the iterates of PDHG, which encodes information about the problem&#39;s feasibility and boundedness, allowing for detection without extra computational effort. The theory of this method is detailed in <a href="https://epubs.siam.org/doi/abs/10.1137/22M1510467" target="_blank" rel="noopener noreferrer">our SIAM Journal paper</a>.</li><li data-block-key="6ubbl"><i>Adaptive restarts</i>: This technique involves strategically deciding when to optimally restart the PDHG algorithm to enhance its efficiency, particularly speeding up the convergence to a high-accuracy solution.</li><li data-block-key="6u9l9"><i>Adaptive step-size</i>: We introduced an adaptive method for selecting the step-size in the PDHG, which significantly reduces the need for manual tuning. This approach adjusts the step-size dynamically based on the problem&#39;s characteristics and the algorithm&#39;s performance, promoting faster convergence.</li></ul><p data-block-key="8lct3">PDLP is open-sourced as part of Google’s <a href="https://developers.google.com/optimization" target="_blank" rel="noopener noreferrer">OR-Tools</a>, an open-source software suite for optimization. The solver is easy to use and it has interfaces in <a href="https://en.wikipedia.org/wiki/Python_(programming_language)" target="_blank" rel="noopener noreferrer">Python</a>, <a href="https://en.wikipedia.org/wiki/C%2B%2B" target="_blank" rel="noopener noreferrer">C++</a>, <a href="https://en.wikipedia.org/wiki/Java_(programming_language)" target="_blank" rel="noopener noreferrer">Java</a> and <a href="https://en.wikipedia.org/wiki/C_Sharp_(programming_language)" target="_blank" rel="noopener noreferrer">C#</a>. More details and examples on how to use PDLP can be found in the <a href="https://developers.google.com/optimization/lp/lp_example" target="_blank" rel="noopener noreferrer">OR-Tools documentation</a>.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    <h2 data-block-key="qp0k3">Applications</h2><p data-block-key="56kp2">Scaling up and speeding up LP enables new applications — here, we briefly mention three:</p><ol><li data-block-key="6f59p"><i>Data center network traffic engineering (</i><a href="https://cloud.google.com/blog/topics/systems/the-evolution-of-googles-jupiter-data-center-network" target="_blank" rel="noopener noreferrer"><i>blog post</i></a><i>,</i> <a href="https://research.google/pubs/jupiter-evolving-transforming-googles-datacenter-network-via-optical-circuit-switches-and-software-defined-networking/"><i>paper</i></a><i>):</i> Google&#39;s data centers rely on dynamically optimized traffic engineering for high-performance efficiency. The challenge of optimizing network traffic routing is periodically addressed as a large-scale LP problem. Previously, solving this large problem fast enough was not possible, leading to the development of partition heuristics. These heuristics decomposed the problem into many smaller-scale LPs that could be solved concurrently, albeit at the cost of optimality. With the introduction of PDLP, we can now efficiently optimize traffic routing across an entire data center network, effectively saving a significant amount of machine resources across the network. This solution has been deployed in Google&#39;s production environment since May 2023.</li><li data-block-key="a199v"><i>Container shipping optimization (</i><a href="https://research.google/blog/heuristics-on-the-high-seas-mathematical-optimization-for-cargo-ships/"><i>blog post</i></a><i>):</i> The world&#39;s shipping supply chain relies on optimizing the order in which vessels visit ports and the placement of containers on those vessels. Due to the extreme scale of real-world instances, a direct solution often is intractable. Consequently, various heuristic approaches have been proposed to enhance efficiency and practicality in solving this complex optimization problem. The problem can be formulated as a type of optimization problem called a massive integer two-layer <a href="https://en.wikipedia.org/wiki/Multi-commodity_flow_problem" target="_blank" rel="noopener noreferrer">multi-commodity flow problem</a>. PDLP enables solving the linear relaxation of this formulation, quantifying the quality of the heuristics.</li><li data-block-key="bii5j"><i>Traveling salesman problem:</i> The <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem" target="_blank" rel="noopener noreferrer">traveling salesman problem</a> (TSP) poses a classic question: given a list of cities and their distances, what&#39;s the shortest route that visits every city once and returns to the starting point? This problem is notoriously challenging, holding significant importance in theoretical computer science and operations research. PDLP has <a href="https://research.google/blog/google-research-2022-beyond-algorithmic-advances/">demonstrated its power</a> by solving real-world TSP lower bound LP instances of immense scale, encompassing up to 12 billion non-zero entries in the constraint matrix. This capability far surpasses the capacity of even the most advanced commercial solvers available today, showcasing PDLP&#39;s potential for tackling large-scale LP challenges.</li></ol>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    <h2 data-block-key="qp0k3">Broader impacts</h2><p data-block-key="947n">Since its initial release, PDLP has attracted significant interest, leading to further enhancements. Here are some notable developments: <a href="https://github.com/jinwen-yang/cuPDLP.jl" target="_blank" rel="noopener noreferrer">cuPDLP.jl</a> is an open-sourced GPU implementation of PDLP, written in <a href="https://julialang.org/" target="_blank" rel="noopener noreferrer">Julia</a>. The commercial solver company, <a href="https://www.copt.de/" target="_blank" rel="noopener noreferrer">Cardinal Optimizer</a>, has incorporated PDLP into their software in <a href="https://github.com/COPT-Public/COPT-Release" target="_blank" rel="noopener noreferrer">Version 7.1</a> in January 2024. The open-source solver, <a href="https://highs.dev/" target="_blank" rel="noopener noreferrer">HiGHS</a>, has incorporated a version of PDLP in their software in <a href="https://conan.io/center/recipes/highs" target="_blank" rel="noopener noreferrer">V1.7.0</a> in March 2024. In addition, the academic community has continued to explore and expand upon the theoretical foundations of PDLP. Recent studies have focused on areas such as <a href="https://arxiv.org/pdf/2206.12061" target="_blank" rel="noopener noreferrer">new analysis on PDHG</a>, <a href="https://arxiv.org/pdf/2312.14774" target="_blank" rel="noopener noreferrer">condition number theory</a>, <a href="https://arxiv.org/pdf/2307.03664v2" target="_blank" rel="noopener noreferrer">trajectory-based analysis</a>, extensions to <a href="https://arxiv.org/pdf/2311.07710" target="_blank" rel="noopener noreferrer">quadratic programming</a> and <a href="https://arxiv.org/pdf/2402.00311" target="_blank" rel="noopener noreferrer">semi-definite programming</a>, etc. These efforts not only deepen the understanding of PDLP&#39;s underlying mechanics but also explore its potential applications to more complex problems. These developments reflect PDLP&#39;s significant impact on the field of optimization, bridging the gap between theoretical research and practical application. As PDLP continues to evolve, its influence is expected to grow, pushing the boundaries of what can be achieved in computational optimization.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    <h2 data-block-key="qp0k3">Acknowledgments</h2><p data-block-key="e8dq7"><i>We are grateful to our co-authors Mateo Diaz, Oliver Hinder, Miles Lubin, and Warren Schudy for their exceptional support and contributions. We would also like to thank our managers, Vahab Mirrokni, Jon Orwant and Aaron Archer, and our collaborators in the Data Center Networking team, the Algorithm team and the Operations Research team.</i></p>
</div>

    </div>
</section>

                    
                </div></div>
  </body>
</html>
