<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.mattkeeter.com/blog/2023-01-25-branch/">Original</a>
    <h1>Do not taunt happy fun branch predictor (2023)</h1>
    
    <div id="readability-page-1" class="page"><div id="content">
<!-- End header -->





<h2>Do Not Taunt Happy Fun Branch Predictor</h2>
<p>I&#39;ve been writing a lot of AArch64 assembly, for <em>reasons</em>.</p>
<p>I recently came up with a &#34;clever&#34; idea to eliminate one jump from an inner
loop, and was surprised to find that it slowed things down.  Allow me to explain
my terrible error, so that you don&#39;t fall victim in the future.</p>
<p>A toy model of the relevant code looks something like this:</p>
<pre><code>float run(const float* data, size_t n) {
    float g = 0.0;
    while (n) {
        n--;
        const float f = *data++;
        foo(f, &amp;g);
    }
    return g;
}

static void foo(float f, float* g) {
    // do some stuff, modifying g
}
</code></pre>
<p>(eliding headers and the forward declaration of <code>foo</code> for space)</p>
<p>A simple translation into AArch64 assembly gives something like this:</p>
<pre><code>// x0: const float* data
// x1: size_t n
// Returns a single float in s0

// Prelude: store frame and link registers
stp   x29, x30, [sp, #-16]!

// Initialize g = 0.0
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    // Do some work, reading from s1 and accumulating into s0
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Here, <code>foo</code> is kinda like a <a href="https://github.com/rust-lang/rfcs/blob/master/text/1201-naked-fns.md">naked
function</a>:
it uses the same stack frame and registers as the parent function, reads from
<code>s1</code>, and writes to <code>s0</code>.</p>
<p>The call to <code>foo</code> uses the the <code>bl</code> instruction, which is &#34;branch and link&#34;:
it jumps to the given label, and stores the <strong>next</strong> instruction address in the
link register (<code>lr</code> or <code>x30</code>).</p>
<p>When <code>foo</code> is done, the <code>ret</code> instruction jumps to the address in the link
register, which is the instruction following the original <code>bl</code>.</p>
<p>Looking at this code, I was struck by the fact that it does two branches,
one after the other.  Surely, it would be more efficient to only branch once.</p>
<p>I had the clever idea to do so <strong>without changing <code>foo</code></strong>:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

bl loop // Set up x30 to point to the loop entrance
loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

foo:
    // Do some work, accumulating into `s0`
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a little subtle:</p>
<ul>
<li>The first call to <code>bl loop</code> stores the beginning of the <code>loop</code> block in <code>x30</code></li>
<li>After checking for loop termination, we fall through into the <code>foo</code> function
(without a branch!)</li>
<li><code>foo</code> still ends with <code>ret</code>, which returns to the <code>loop</code> block (because
that&#39;s what&#39;s in <code>x30</code>).</li>
</ul>
<p>Within the body of the loop, we never change <code>x30</code>, so the repeated <code>ret</code>
instructions always return to the same place.</p>
<p>I set up a benchmark using a very simple <code>foo</code>:</p>
<pre><code>foo:
    fadd s0, s0, s1
    ret
</code></pre>
<p>With this <code>foo</code>, the function as a whole sums the incoming array of <code>float</code>
values.</p>
<p>Benchmarking with <a href="https://docs.rs/criterion/latest/criterion/"><code>criterion</code></a>
(on an M1 Max CPU),
with a 1024-element array:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Original </td><td>969 ns
</td></tr><tr><td>&#34;Optimized&#34;</td><td>3.85 µs
</td></tr></tbody></table>
<p>The &#34;optimized&#34; code with one jump per loop is about <strong>4x slower</strong>
than the original version with two jumps per loop!</p>
<p>I found this surprising, so I asked a few colleagues about it.</p>
<p>Between <a href="https://hachyderm.io/@cliffle">Cliff</a> and
<a href="https://discuss.systems/@cross">Dan</a>,
the consensus was that mismatched <code>bl</code> / <code>ret</code>
pairs were confusing the
<a href="https://en.wikipedia.org/wiki/Branch_predictor">branch predictor</a>.</p>
<p>The <a href="https://developer.arm.com/documentation/102374/0101/Function-calls">ARM documentation</a> agrees:</p>
<blockquote>
<p>Why do we need a special function return instruction? Functionally, BR LR
would do the same job as RET. Using RET tells the processor that this is a
function return. Most modern processors, and all Cortex-A processors, support
branch prediction. Knowing that this is a function return allows processors to
more accurately predict the branch.</p>
<p>Branch predictors guess the direction the program flow will take across
branches. The guess is used to decide what to load into a pipeline with
instructions waiting to be processed. If the branch predictor guesses
correctly, the pipeline has the correct instructions and the processor does
not have to wait for instructions to be loaded from memory.</p>
</blockquote>
<p>More specifically, the branch predictor probably keeps an internal stack of
function return addresses, which is pushed to whenever a <code>bl</code> is executed. When
the branch predictor sees a <code>ret</code> coming down the pipeline, it assumes that
you&#39;re returning to the address associated with the most recent <code>bl</code> (and begins
prefetching / speculative execution / whatever), then pops that top address from
its internal stack.</p>
<p>This works if you&#39;ve got matched <code>bl</code> / <code>ret</code> pairs, but the prediction will
fail if the same address is used by multiple <code>ret</code> instructions; you&#39;ll end up
with (<em>vague handwaving</em>) useless prefetching, incorrect speculative execution,
and pipeline stalls / flushes</p>
<p>Dan made the great suggestion of replacing <code>ret</code> with <code>br x30</code> to test this
theory.  Sure enough, this fixes the performance regression:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr></tbody></table>
<p>In fact, it&#39;s slightly faster, probably because it&#39;s only doing one branch
per loop instead of two!</p>
<p>To further test the &#34;branch predictor&#34; theory, I opened up Instruments and
examined performance counters for the first two programs. Picking out the worst
offenders, the results seem conclusive:</p>
<table>
<tbody><tr><th>Counter</th><th>Matched <code>bl</code> / <code>ret</code></th><th>One <code>bl</code>, many <code>ret</code>
</th></tr><tr><td><code>BRANCH_RET_INDIR_MISPRED_NONSPECIFIC</code></td><td>92</td><td>928,644,975
</td></tr><tr><td><code>FETCH_RESTART</code></td><td>61,121</td><td>987,765,276
</td></tr><tr><td><code>MAP_DISPATCH_BUBBLE</code></td><td>1,155,632</td><td>7,350,085,139
</td></tr><tr><td><code>MAP_REWIND</code></td><td>6,412,734</td><td>2,789,499,545
</td></tr></tbody></table>
<p>These measurements are captured while summing an array of 1B elements.  We see
that with mismatched <code>bl</code> / <code>ret</code> pairs, the return branch predictor fails about
93% of the time!</p>
<p>Apple doesn&#39;t fully document these counters, but I&#39;m guessing that the other
counters are downstream effects of bad branch prediction:</p>
<ul>
<li><code>FETCH_RESTART</code> is presumably bad prefetching</li>
<li><code>MAP_DISPATCH_BUBBLE</code> probably refers to <a href="https://en.wikipedia.org/wiki/Pipeline_stall">pipeline stalls</a></li>
<li><code>MAP_REWIND</code> might be bad speculative execution that needs to be rewound</li>
</ul>
<p>In conclusion,
<a href="https://www.youtube.com/watch?v=GmqeZl8OI2M">do not taunt happy fun branch predictor</a>
with asymmetric usage of <code>bl</code> and <code>ret</code> instructions.</p>
<hr/>
<h2>Appendix: Going Fast</h2>
<p>Take a second look at this program:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    fadd s0, s0, s1
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Upon seeing this program, it&#39;s a common reaction to ask &#34;why is <code>foo</code> a
subroutine at all?&#34;</p>
<p>The answer is &#34;because this is a didactic example, not code that&#39;s trying
to go as fast as possible&#34;.</p>
<p>Still, it&#39;s a fair question.  You wanna go fast?  Let&#39;s go fast.</p>
<p>If we know the contents of <code>foo</code> when building this
function (and it&#39;s shorter than the maximum jump distance), we can remove the
<code>bl</code> and <code>ret</code> entirely:</p>
<pre><code>loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    // foo is completely inlined here
    fadd s0, s0, s1

    b loop

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a roughly 6% speedup: from 969 ns to 911 ns.</p>
<p>We can get faster still by trusting the compiler:</p>
<pre><code>pub fn sum_slice(f: &amp;[f32]) -&gt; f32 {
    f.iter().sum()
}
</code></pre>
<p>This brings us down to 833 ns, a significant improvement!</p>
<p><a href="https://godbolt.org/z/crxKnnMee">Looking at the assembly</a>,
it&#39;s doing some loop unrolling.
However, even when compiled with <code>-C target-cpu=native</code>, it&#39;s not generating
<a href="https://developer.arm.com/Architectures/Neon">NEON SIMD instructions</a>.
Can we beat it?</p>
<p><strong>We sure can!</strong></p>
<pre><code>stp   x29, x30, [sp, #-16]!

fmov s0, #0.0
dup v1.4s, v0.s[0]
dup v2.4s, v0.s[0]

loop:  // 1x per loop
    ands xzr, x1, #3
    b.eq simd

    sub x1, x1, #1
    ldr s3, [x0], #4

    fadd s0, s0, s3
    b loop

simd:  // 4x SIMD per loop
    ands xzr, x1, #7
    b.eq simd2

    sub x1, x1, #4
    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]

    fadd v1.4s, v1.4s, v3.4s

    b simd

simd2:  // 2 x 4x SIMD per loop
    cmp x1, #0
    b.eq exit

    sub x1, x1, #8

    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]
    fadd v1.4s, v1.4s, v3.4s

    ldp d5, d6, [x0], #16
    mov v5.d[1], v6.d[0]
    fadd v2.4s, v2.4s, v5.4s

    b simd2

exit: // function exit
    fadd v2.4s, v2.4s, v1.4s
    mov s1, v2.s[0]
    fadd s0, s0, s1
    mov s1, v2.s[1]
    fadd s0, s0, s1
    mov s1, v2.s[2]
    fadd s0, s0, s1
    mov s1, v2.s[3]
    fadd s0, s0, s1

    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This code includes three different loops:</p>
<ul>
<li>The first loop (<code>loop</code>) sums individual values
into <code>s0</code> until we have a multiple of four values remaining</li>
<li>The second loop (<code>simd</code>) uses SIMD instructions to sum 4 values at a time
into the vector register <code>v1</code>, until we have a multiple of 8 values remaining</li>
<li>The last loop (<code>simd2</code>) is the same as <code>simd</code>, but is unrolled 2x so it
handles 8 values per loop iteration, summing into <code>v1</code> and <code>v2</code></li>
</ul>
<p>At the function exit, we accumulate the values in the vector registers <code>v1</code>/<code>v2</code>
into <code>s0</code>, which is returned.</p>
<p>The type punning here is particularly cute:</p>
<pre><code>ldp d3, d4, [x0], #16
mov v3.d[1], v4.d[0]
fadd v1.4s, v1.4s, v3.4s
</code></pre>
<p>Remember, <code>x0</code> holds a <code>float*</code>.  We pretend that it&#39;s a <code>double*</code> to load 128
bits (i.e. 4x <code>float</code> values) into <code>d3</code> and <code>d4</code>.  Then, we move the &#34;double&#34; in <code>d4</code>
to occupy the top 64 bits of the <code>v3</code> vector register (of which <code>d3</code> is the
<em>lower</em> 64 bits).</p>
<p>Of course, each &#34;double&#34; is two floats, but that doesn&#39;t matter when shuffling
them around.  When summing with <code>fadd</code>, we tell the processor to treat them as
four floats (the <code>.4s</code> suffix), and everything works out fine.</p>
<p>How fast are we now?</p>
<p>This runs in 94 ns, or about <strong>8.8x faster</strong> than our previous best.</p>
<p>Here&#39;s a summary of performance:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr><tr><td>Plain loop with <code>b</code></td><td>911 ns
</td></tr><tr><td>Rewrite it in Rust</td><td>833 ns
</td></tr><tr><td>SIMD + manual loop unrolling</td><td>94 ns
</td></tr></tbody></table>
<p>Could we get even faster?  I&#39;m sure it&#39;s possible; I make no claims to being
the <a href="https://www.agner.org/optimize/">Agner Fog</a> of AArch64 assembly.</p>
<p>Still, this is a reasonable point to wrap up: we&#39;ve demystified the initial
performance regression, and had some fun hand-writing assembly to go very
fast indeed.</p>
<p>The SIMD code does come with one asterisk, though: because floating-point
addition is not associative, and it performs the summation in a different
order, it <strong>may not get the same result</strong> as straight-line code.  In retrospect,
this is likely why the compiler doesn&#39;t generate SIMD instructions to compute
the sum!</p>
<p>Does this matter for your use case?  Only you can know!</p>
<hr/>
<p>All of the code from this post is
<a href="https://github.com/mkeeter/arm64-test">published to GitHub</a>.</p>
<p>You can reproduce benchmarks by running <code>cargo bench</code> on an ARM64 machine.</p>

<!-- Begin footer -->
</div></div>
  </body>
</html>
