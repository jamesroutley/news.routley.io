<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://martinfowler.com/articles/patterns-of-distributed-systems/">Original</a>
    <h1>Patterns of Distributed Systems</h1>
    
    <div id="readability-page-1" class="page"><div>
<section id="WhatThisIsAbout">
<h2> What this is about </h2>

<p>
        For the last several months, I have been conducting workshops on distributed systems at Thoughtworks. 
        One of the key challenges faced while conducting the workshops was how to map the
        theory of distributed systems to open source code bases such as Kafka or Cassandra, while 
        keeping the discussions generic enough to cover a broad range of solutions.
        The concept of patterns provided a nice way out. 
      </p>

<p>  
        Pattern structure, by its very nature, 
        allows us to focus on a specific problem, making it very clear why a particular solution is needed. 
        Then the solution description enables us to give a code structure, which is concrete enough to show 
        the actual solution but generic enough to cover a broad range of variations.
        This patterns technique also allows us to link various patterns together to build a complete system. 
        This gives a nice vocabulary to discuss distributed system implementations.      
      </p>

<p>
        What follows is a first set of patterns observed in mainstream open source distributed systems. 
        I hope that this set of patterns will be useful to all developers.
      </p>

<section id="DistributedSystems-AnImplementationPerspective">
<h3> Distributed systems - An implementation perspective </h3>

<p>
        Today&#39;s enterprise architecture is full of platforms and frameworks which are distributed by nature. 
        If we see the sample list of frameworks and platforms used in typical enterprise architecture today, 
        it will look something like following:
      </p>

<table>
<thead>
<tr><th>Type of platform/framework</th><th>Example</th></tr>
</thead>

<tbody>
<tr><td>Databases</td><td>Cassandra, HBase, Riak</td></tr>

<tr><td>Message Brokers</td><td>Kafka, Pulsar</td></tr>

<tr><td>Infrastructure</td><td>Kubernetes, Mesos, Zookeeper, etcd, Consul</td></tr>

<tr><td>In Memory Data/Compute Grids</td><td>Hazelcast, Pivotal Gemfire</td></tr>

<tr><td>Stateful Microservices</td><td>Akka Actors, Axon</td></tr>

<tr><td>File Systems</td><td>HDFS, Ceph</td></tr>
</tbody>
</table>

<p>
        All these are &#39;distributed&#39; by nature. What does it mean for a system to be distributed? 
        There are two aspects:
      </p>

<ul>
<li> They run on multiple servers. The number of servers in a cluster can 
               vary from as few as three servers to a few thousand servers.  </li>

<li> They manage data. So these are inherently &#39;stateful&#39; systems. </li>
</ul>

<p>
        There are several ways in which things can go wrong when multiple servers are involved in storing data. 
        All the above mentioned systems need to solve those problems. 
        The implementation of these systems have some recurring solutions to these problems. 
        Understanding these solutions in their general form helps in understanding 
        the implementation of the broad spectrum of these systems and 
        can also serve as a good guidance when new systems need to be built. Enter patterns.
      </p>

<section id="Patterns">
<h4> Patterns </h4>

<p><a href="https://rgoswami.me/articles/writingPatterns.html">Patterns</a>, a concept introduced by Christopher Alexander, 
      is widely accepted in the software community to document design constructs which are 
      used to build software systems. Patterns provide a structured way of 
      looking at a problem space along with the solutions which are seen multiple times and proven. 
      An interesting way to use patterns is the ability to link several patterns together, 
      in a form of pattern sequence or pattern language, which gives some guidance of implementing a ‘whole’ or a complete system. 
      Looking at distributed systems as a series of patterns is a useful way to gain insights into their implementation.
      </p>
</section>
</section>
</section>

<section id="ProblemsAndTheirRecurringSolutions.">
<h2>Problems and Their Recurring Solutions.</h2>



<p>
        Several things can go wrong when data is stored on multiple servers.
      </p>

<section id="ProcessCrashes">
<h3> Process crashes </h3>

<p>
        Processes can crash at any time maybe due to hardware faults or software faults. 
        There are numerous ways in which a process can crash.
        </p>

<ul>
<li>It can be taken down for routine maintenance by system administrators.</li>

<li>It can be killed doing some file IO because the disk is full and the exception is not properly handled.</li>

<li>In cloud environments, it can be even trickier, as some unrelated events can bring the servers down. </li>
</ul>

<p>
        The bottom line is that if the processes are responsible for storing data, they must be designed to give a durability guarantee for the data stored on the servers. 
        Even if a process crashes abruptly, it should preserve all the data for which it has notified the user that it&#39;s stored successfully. 
        Depending on the access patterns, different storage engines have different storage structures, 
        ranging from a simple hash map to a sophisticated graph storage. 
        Because flushing data to the disk is one of the most time consuming operations, 
        not every insert or update to the storage can be flushed to disk. 
        So most databases have in-memory storage structures which are only periodically flushed to disk. 
        This poses a risk of losing all the data if the process abruptly crashes.
        </p>

<p>
        A technique called <a href="https://rgoswami.me/posts/managing-scanned-books/wal.html">Write-Ahead Log</a> is used to tackle this situation. 
        Servers store each state change as a command in an append-only file on a hard disk. 
        Appending a file is generally a very fast operation, so it can be done without impacting performance.
        A single log, which is appended sequentially, is used to store each update. 
        At the server startup, the log can be replayed to build in memory state again.
        </p>

<p>
        This gives a durability guarantee. The data will not get lost even if the server abruptly crashes 
        and then restarts. 
        But clients will not be able to get or store any data till the server is back up. 
        So we lack availability in the case of server failure.
        </p>

<p>
         One of the obvious solutions is to store the data on multiple servers. 
         So we can replicate the write ahead log on multiple servers.
        </p>

<p>
         When multiple servers are involved, there are a lot more failure scenarios which need to be considered.
        </p>
</section>

<section id="NetworkDelays">
<h3>Network delays</h3>

<p>
          In the TCP/IP protocol stack, there is no upper bound on delays caused in transmitting messages across a network. 
          It can vary based on the load on the network. For example, a 1 Gbps network link can get flooded with a big data 
          job that&#39;s triggered, filling the network buffers, 
          which can cause arbitrary delay for some messages to reach the servers.
        </p>

<p>
        In a typical data center, servers are packed together in racks, and there are multiple racks connected by 
        a top-of-the-rack switch. There might be a tree of switches connecting one part of the data center to the other.
        It is possible in some cases, that a set of servers can communicate with each other, but are disconnected from another set of servers. This situation is called a network partition.
        One of the fundamental issues with servers communicating over a network then is how to know a particular server has failed. 
        </p>

<p>
        There are two problems to be tackled here. 
        </p>

<ul>
<li> A particular server can not wait indefinitely to know if another server has crashed. </li>

<li> There should not be two sets of servers, each considering another set to have failed, 
          and therefore continuing to serve different sets of clients. This is called the split brain. </li>
</ul>

<p>
         To tackle the first problem, every server sends a <a href="https://rgoswami.me/posts/managing-scanned-books/heartbeat.html">HeartBeat</a> message to other servers at a regular interval. 
         If a heartbeat is missed, the server sending the heartbeat is considered crashed.
         The heartbeat interval is small enough to make sure that it does not take a lot of time to detect server failure. 
         As we will see below, in the worst case scenario, the server might be up and running, 
         but the cluster as a group can move ahead considering the server to be failing. This makes sure that services provided to clients are not interrupted.
        </p>

<p>
        The second problem is the split brain. With the split brain, if two sets of servers accept updates independently, 
        different clients can get and set different data, and once the split brain is resolved, it&#39;s impossible to resolve conflicts automatically.        
        </p>

<p>
        To take care of the split brain issue, we must ensure that the two sets of servers, 
        which are disconnected from each other, should not be able to make progress independently. 
        To ensure this, every action the server takes, is considered successful only if the majority of the servers can confirm the action. 
        If servers can not get a majority, they will not be able to provide the required services, and some group of the clients might not be receiving the service, but servers in the cluster will always be in a consistent state. 
        The number of servers making the majority is called a <a href="https://rgoswami.me/posts/managing-scanned-books/quorum.html">Quorum</a>.
        How to decide on the quorum? That is decided based on the number of failures the cluster can tolerate. 
        So if we have a cluster of five nodes, we need a quorum of three.  
        In general, if we want to tolerate <code>f</code> failures we need a cluster size of 2f + 1.
        </p>

<p>
        Quorum makes sure that we have enough copies of data to survive some server failures. But it is not enough to give strong consistency guarantees to clients. Lets say a client initiates a write operation on the quorum, but the write operation succeeds only on one server. The other servers in the quorum still have old values. When a client reads the values from the quorum, it might get the latest value, if the server having the latest value is available. But it can very well get an old value if, just when the client starts reading the value, the server with the latest value is not available. To avoid such situations, someone needs to track if the quorum agrees on a particular operation and only send values to clients which are guaranteed to be available on all the servers.
        <a href="https://rgoswami.me/posts/managing-scanned-books/leader-follower.html">Leader and Followers</a> is used in this situation. One of the servers is elected a leader and the other servers act as followers. The leader controls and coordinates the replication on the followers. 
        The leader now needs to decide, which changes should be made visible to the clients. 
        A <a href="https://rgoswami.me/posts/managing-scanned-books/high-watermark.html">High-Water Mark</a> is used to track the entry in the write ahead log 
        that is known to have successfully replicated to a quorum of followers. 
        All the entries upto the high-water mark are made visible to the clients.
        The leader also propagates the high-water mark to the followers. So in case the leader fails and one of the followers becomes the new leader, there are no inconsistencies in what a client sees.
        </p>
</section>

<section id="ProcessPauses">
<h3>Process Pauses</h3>

<p>
         Even with quorums and leader and followers, there is a tricky problem that needs to be solved. Leader processes can pause arbitrarily. There are a lot of reasons a process can pause. For languages which support garbage collection, there can be a long garbage collection pause.
         A leader with a long garbage collection pause, 
         can be disconnected from the followers, and will continue sending messages to followers after the pause is over. 
         In the meanwhile, because followers did not receive a heartbeat from the leader, they might have elected a new leader 
         and accepted updates from the clients. If the requests from the old leader are processed as is, 
         they might overwrite some of the updates. So we need a mechanism to detect requests from out-of-date leaders. 
         Here <a href="https://rgoswami.me/posts/managing-scanned-books/generation.html">Generation Clock</a> is used to mark and detect requests from older leaders. 
         The generation is a number which is monotonically increasing. 
        </p>
</section>

<section id="UnsynchronizedClocksAndOrderingEvents">
<h3> Unsynchronized Clocks and Ordering Events </h3>

<p>
        The problem of detecting older leader messages from newer ones is the problem of maintaining ordering of messages. It might appear that we can use system timestamps to order a set of messages, but we can not.
        The main reason we can not use system clocks is that system clocks across servers are not guaranteed to be synchronized.
        A time-of-the-day clock in a computer is managed by a quartz crystal and measures time based on the oscillations of the crystal.
        </p>

<p>
        This mechanism is error prone, as the crystals can oscillate faster or slower and so different servers can have very different times.
        The clocks across a set of servers are synchronized by a service called NTP.
        This service periodically checks a set of global time servers, and adjusts the computer clock accordingly. 
        </p>

<p>
        Because this happens with communication over a network, and network delays can vary as discussed in the above sections, the clock synchronization might be delayed because of a network issue. This can cause server clocks to drift away from each other, and after the NTP sync happens, even move back in time.
        Because of these issues with computer clocks, time of day is generally not used for ordering events.
        Instead a simple technique called <a href="https://rgoswami.me/posts/managing-scanned-books/lamport-clock.html">Lamport Clock</a> is used. 
        <a href="https://rgoswami.me/posts/managing-scanned-books/generation.html">Generation Clock</a> is an example of that.
        Lamport Clocks are just simple numbers, which are incremented only when some event happens in the system.
        In a database, the events are about writing and reading the values, so the lamport clock 
        is incremented only when a value is written. The Lamport Clock numbers are 
        also passed in the messages sent to other processes.
        The receiving process can then select the larger of the two numbers, 
        the one it receives in the message and the one it maintains.
        This way Lamport Clocks also track happens-before relationship between events across processes 
        which communicate with each other.
        An example of this is the servers taking part in a transaction.   
        While the <a href="https://rgoswami.me/posts/managing-scanned-books/lamport-clock.html">Lamport Clock</a> allows ordering of events, it does not have 
        any relation to the time of the day clock. To bridge this gap, 
        a variation called <a href="https://rgoswami.me/posts/managing-scanned-books/hybrid-clock.html">Hybrid Clock</a>
        is used. The Hybrid Clock uses system time along with a separate number 
        to make sure the value increases monotonically, 
        and can be used the same way as Lamport Clock.
        </p>

<p>
         The Lamport Clock allows determining the order of events across a set of communicating servers. 
         But it does not allow detecting concurrent updates to the same value happening across a set of replicas.
         <a href="https://rgoswami.me/posts/managing-scanned-books/version-vector.html">Version Vector</a> is used to detect conflict across a set of replicas.
        </p>

<p>
         The Lamport Clock or Version Vector needs to be associated with the stored values, to detect which 
         values are stored after the other or if there are conflicts. 
         So the servers store the values as <a href="https://rgoswami.me/posts/managing-scanned-books/versioned-value.html">Versioned Value</a>.
        </p>
</section>
</section>

<section id="PuttingItAllTogether-PatternSequences">
<h2>Putting it all together - Pattern Sequences </h2>

<p> We can see how understanding these patterns, helps us build a complete
      system, from the ground up. We will take consensus implementation as an
      example. 
      </p>

<section id="FaultTolerantConsensus">
<h3>Fault Tolerant Consensus</h3>

<p>
          Distributed Consensus is a special case of distributed system
          implementation, which provides the strongest consistency guarantee. 
          Common examples seen in popular enterprise systems include, <a href="https://zookeeper.apache.org/">Zookeeper</a>, <a href="https://etcd.io/">etcd</a> and <a href="https://www.consul.io/">Consul</a>. They implement consensus algorithms such as
          <a href="https://zookeeper.apache.org/doc/r3.4.13/zookeeperInternals.html#sc_atomicBroadcast">zab</a> and <a href="https://raft.github.io/">Raft</a> to provide
          replication and strong consistency. There are other popular algorithms to
          implement consensus; <a href="https://www.youtube.com/watch?v=JEpsBg0AO6o&amp;t=1920s">multi-paxos</a> which 
          is used in Google&#39;s <a href="https://research.google/pubs/pub27897/">Chubby</a> locking service, 
          view stamp replication and <a href="https://www.cs.cornell.edu/ken/History.pdf">virtual-synchrony</a>.
          In very simple terms, Consensus refers to a set of servers which agree on
          stored data, the order in which the data is stored and when to make that
          data visible to the clients. </p>

<p>
            Assuming a crash fault model, where it is assumed that
            cluster nodes stop working and crash when any fault happens, 
            the basic technique to implement consensus for a single value, is 
            implemented as <a href="https://rgoswami.me/posts/managing-scanned-books/paxos.html">Paxos</a>.            
            Paxos describes a few simple rules to use two phase execution, 
            <a href="https://rgoswami.me/posts/managing-scanned-books/quorum.html">Quorum</a> and <a href="https://rgoswami.me/posts/managing-scanned-books/generation.html">Generation Clock</a> 
            to achieve consensus across a set of cluster nodes even when there are
            process crashes, network delays and unsynchronized clocks. 
          </p>




<p>
            When data is replicated across cluster nodes, achieving consensus on 
            a single value is not enough. All the replicas need to reach agreement 
            on all the data. This requires executing <a href="https://rgoswami.me/posts/managing-scanned-books/paxos.html">Paxos</a>
            multiple times while maintaining strict order. 
            <a href="https://rgoswami.me/posts/managing-scanned-books/replicated-log.html">Replicated Log</a> describes how basic
            <a href="https://rgoswami.me/posts/managing-scanned-books/paxos.html">Paxos</a> can be extended to achieve this.
            
</p><p>
              This technique is also known as <a href="https://en.wikipedia.org/wiki/State_machine_replication">state machine replication</a>
              to achieve fault tolerance. In state machine replication, the storage services, 
              like a key value store, are replicated on all the servers, 
              and the user inputs are executed in the same order on each server. 
              The key implementation technique used to achieve this is to
              replicate <a href="https://rgoswami.me/posts/managing-scanned-books/wal.html">Write-Ahead Log</a> on all the servers to 
              have a <a href="https://rgoswami.me/posts/managing-scanned-books/replicated-log.html">Replicated Log</a>.
            </p>

</section>

<section id="PatternSequenceForImplementingReplicatedLog">
<h3>Pattern Sequence for implementing replicated log</h3>

<p>
          We can put the patterns together to implement Replicated Wal as follows. 
        </p>

<p>
          To provide durability guarantees, you can use the <a href="https://rgoswami.me/posts/managing-scanned-books/wal.html">Write-Ahead Log</a> pattern. 
          The Write Ahead Log is divided into multiple segments using <a href="https://rgoswami.me/posts/managing-scanned-books/log-segmentation.html">Segmented Log</a>. 
          This helps with log cleaning, which is handled by <a href="https://rgoswami.me/posts/managing-scanned-books/low-watermark.html">Low-Water Mark</a>. 
          Fault tolerance is provided by replicating the write-ahead log on multiple servers. 
          The replication among the servers is managed using the <a href="https://rgoswami.me/posts/managing-scanned-books/leader-follower.html">Leader and Followers</a> pattern
          and <a href="https://rgoswami.me/posts/managing-scanned-books/quorum.html">Quorum</a> is used to update the <a href="https://rgoswami.me/posts/managing-scanned-books/high-watermark.html">High-Water Mark</a> 
          to decide which values are visible to clients. 
          All the requests are processed in strict order, by using <a href="https://rgoswami.me/posts/managing-scanned-books/singular-update-queue.html">Singular Update Queue</a>. 
          The order is maintained while sending the requests from leaders to followers using 
          <a href="https://rgoswami.me/posts/managing-scanned-books/single-socket-channel.html">Single Socket Channel</a>. To optimize for throughput and latency over
          a single socket channel, 
          <a href="https://rgoswami.me/posts/managing-scanned-books/request-pipeline.html">Request Pipeline</a> can be used.
          Followers know about availability of the leader via the <a href="https://rgoswami.me/posts/managing-scanned-books/heartbeat.html">HeartBeat</a> 
          received from the leader. 
          If the leader is temporarily disconnected from the cluster because of network partition, 
          it is detected by using <a href="https://rgoswami.me/posts/managing-scanned-books/generation.html">Generation Clock</a>.
          If all the requests are served only by the leader, it might get overloaded. 
          When the clients are read only and tolerate reading stale values, 
          they can be served by the follower servers. <a href="https://rgoswami.me/posts/managing-scanned-books/follower-reads.html">Follower Reads</a> allows 
          handling read requests from follower servers.
        </p>


</section>

<section id="AtomicCommit">
<h3>Atomic Commit</h3>

<p>
          Consensus algorithms are useful when multiple cluster nodes all store 
          the same data. Often, data size is too big to store and process on a single node.
          So data is partitioned across a set of nodes using various partitioning
          schemes such as <a href="https://rgoswami.me/posts/managing-scanned-books/fixed-partitions.html">Fixed Partitions</a> or 
          <a href="https://rgoswami.me/posts/managing-scanned-books/key-range-partitions.html">Key-Range Partitions</a>. 
          To achieve fault tolerance, each partition is also replicated across 
          a few cluster nodes using <a href="https://rgoswami.me/posts/managing-scanned-books/replicated-log.html">Replicated Log</a>.
        </p>

<p>
          Sometimes data across a set of partitions needs to be stored as 
          one atomic operation. If processes storing a partition crash or 
          if there are network delays or process pauses, 
          it might happen that data is copied on a few partitions and 
          failed on a few. To maintain atomicity,
          the data needs to be stored and made accessible on all the partitions 
          or none of them. 
          <a href="https://rgoswami.me/posts/managing-scanned-books/two-phase-commit.html">Two Phase Commit</a> is used to guarantee
          atomicity across a set of partitions. 
          To guarantee atomicity,
          two-phase-commit often needs to lock the data items involved. This can
          severely impact throughput, particularly when there are long running
          read-only operations holding locks. To allow better throughput without
          using conflicting locks, two-phase-commit implementations often use
          <a href="https://rgoswami.me/posts/managing-scanned-books/versioned-value.html">Versioned Value</a> based storage.
        </p>



</section>

<section id="KubernetesOrKafkaControlPlane">
<h3> Kubernetes or Kafka Control Plane</h3>

<p>
        Products like <a href="https://kubernetes.io/">Kubernetes</a> or 
        <a href="https://kafka.apache.org/">Kafka&#39;s</a> architecture are built around a 
        strongly consistent metadata store. 
        We can understand it as a pattern sequence.
        <a href="https://rgoswami.me/posts/managing-scanned-books/consistent-core.html">Consistent Core</a> is used as a strongly consistent, 
        fault tolerant metadata store. 
        <a href="https://rgoswami.me/posts/managing-scanned-books/time-bound-lease.html">Lease</a> is used to implement group membership and 
        failure detection of cluster nodes.
        Cluster nodes use <a href="https://rgoswami.me/posts/managing-scanned-books/state-watch.html">State Watch</a> to get notified when any cluster 
        node fails or updates its metadata
        The <a href="https://rgoswami.me/posts/managing-scanned-books/consistent-core.html">Consistent Core</a> implementation uses 
        <a href="https://rgoswami.me/posts/managing-scanned-books/idempotent-receiver.html">Idempotent Receiver</a> to ignore duplicate requests sent
         by cluster nodes in case of retries on network failure.
         The Consistent Core is built with a &#39;Replicated Wal&#39;, which is described 
         as a pattern sequence in the above section.
        
</p>

</section>

<section id="LogicalTimestampUsage">
<h3>Logical Timestamp usage </h3>

<p>
          Usage of various types of <a href="https://en.wikipedia.org/wiki/Logical_clock">logical timestamps</a> 
          can also be seen as a pattern sequence.
          Various products use either a <a href="https://rgoswami.me/posts/managing-scanned-books/gossip-dissemination.html">Gossip Dissemination</a> 
          or a <a href="https://rgoswami.me/posts/managing-scanned-books/consistent-core.html">Consistent Core</a> for group membership and failure detection of cluster nodes. 
          The data storage uses <a href="https://rgoswami.me/posts/managing-scanned-books/versioned-value.html">Versioned Value</a> to be able to determine which values are most recent.
          If a single server is responsible for updating the values or <a href="https://rgoswami.me/posts/managing-scanned-books/leader-follower.html">Leader and Followers</a> is used, 
          then a <a href="https://rgoswami.me/posts/managing-scanned-books/lamport-clock.html">Lamport Clock</a> can be used as a version, in the <a href="https://rgoswami.me/posts/managing-scanned-books/versioned-value.html">Versioned Value</a>. 
          When the timestamp values need to be derived from the time of the day, 
          a <a href="https://rgoswami.me/posts/managing-scanned-books/hybrid-clock.html">Hybrid Clock</a> is used instead of a simple Lamport Clock.
          If multiple servers are allowed to handle client requests to update the same value, 
          a <a href="https://rgoswami.me/posts/managing-scanned-books/version-vector.html">Version Vector</a> is used to be able to detect concurrent writes on different 
          cluster nodes.
        </p>



<p> This way, understanding problems and their recurring solutions in their general form, helps in understanding building blocks of a complete system</p>
</section>
</section>

<section id="NextSteps">
<h2>Next Steps</h2>

<p> 
         Distributed systems is a vast topic. 
         The set of patterns covered here is a small part, covering different categories to showcase how a patterns approach can help understand and design distributed systems. 
         I will keep adding to this set to broadly include the following categories of problems solved in any distributed system
        </p>

<ul>
<li>Group Membership and Failure Detection</li>

<li>Partitioning </li>

<li>Replication and Consistency </li>

<li>Storage </li>

<li>Processing </li>
</ul>
</section>

<nav>
<p>This page is part of:</p>

<p>Patterns of Distributed Systems</p>

<p>by <b>Unmesh Joshi</b></p>
<img src="https://rgoswami.me/posts/managing-scanned-books/card.png"/>
<p><a href="https://rgoswami.me/posts/managing-scanned-books/">Main Narrative Article</a></p>

<p>Patterns</p>

<ul>
<li><a href="https://rgoswami.me/posts/managing-scanned-books/clock-bound.html">Clock-Bound Wait</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/consistent-core.html">Consistent Core</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/emergent-leader.html">Emergent Leader</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/fixed-partitions.html">Fixed Partitions</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/follower-reads.html">Follower Reads</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/generation.html">Generation Clock</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/gossip-dissemination.html">Gossip Dissemination</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/heartbeat.html">HeartBeat</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/high-watermark.html">High-Water Mark</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/hybrid-clock.html">Hybrid Clock</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/idempotent-receiver.html">Idempotent Receiver</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/key-range-partitions.html">Key-Range Partitions</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/lamport-clock.html">Lamport Clock</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/leader-follower.html">Leader and Followers</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/time-bound-lease.html">Lease</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/low-watermark.html">Low-Water Mark</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/paxos.html">Paxos</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/quorum.html">Quorum</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/replicated-log.html">Replicated Log</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/request-batch.html">Request Batch</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/request-pipeline.html">Request Pipeline</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/request-waiting-list.html">Request Waiting List</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/log-segmentation.html">Segmented Log</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/single-socket-channel.html">Single Socket Channel</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/singular-update-queue.html">Singular Update Queue</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/state-watch.html">State Watch</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/two-phase-commit.html">Two Phase Commit</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/version-vector.html">Version Vector</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/versioned-value.html">Versioned Value</a></li>

<li><a href="https://rgoswami.me/posts/managing-scanned-books/wal.html">Write-Ahead Log</a></li>
</ul>
</nav>

<hr/>
</div></div>
  </body>
</html>
