<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://pile.eleuther.ai/">Original</a>
    <h1>The Pile is a 825 GiB diverse, open-source language modelling data set</h1>
    
    <div id="readability-page-1" class="page"><section>
      <!-- Left Column -->
      <div>
        <div>
          <div>
            <h2>What is the Pile?</h2>
            <p>
              The Pile is a <span>825 GiB</span> diverse, open
              source language modelling data set that consists of 22 smaller,
              high-quality datasets combined together.
            </p>
            
          </div>
          <div>
            <h2>Download</h2>
            <p>The Pile is hosted by <a href="https://the-eye.eu/">the Eye</a>.</p>
            
            <p>
              Have a model that uses or evaluates on the Pile?
              <a href="mailto:contact@eleuther.ai">Let us know</a>!
            </p>
          </div>
          <div>
            <h2>Why is the Pile a good training set?</h2>
            <p>
              Recent work has shown that especially for large models, diversity
              in data sources improves general cross-domain knowledge of the
              model, as well as downstream generalization capability. In our
              evaluations, not only do models trained on the Pile show moderate
              improvements in traditional language modeling benchmarks, they
              also show significant improvements on Pile BPB.
            </p>
          </div>
          <div>
            <h2>Why is the Pile a good benchmark?</h2>
            <p>
              To score well on Pile BPB (bits per byte), a model must be able to
              understand many disparate domains including books, github
              repositories, webpages, chat logs, and medical, physics, math,
              computer science, and philosophy papers. Pile BPB is a measure of
              world knowledge and reasoning ability in these domains, making it
              a robust benchmark of general, cross-domain text modeling ability
              for large language models.
            </p>
          </div>
          <div>
            <h2>Citing</h2>
            <p>
                If you use the Pile or any of the components, please cite us!
            </p>
            <pre>@article{pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
                </pre>
            
          </div>
        </div>
        <!-- Right Column -->
        <div>
          
          <div>
            <h2>Leaderboard</h2>
            <p>
              * indicates potential test-set overlap. Zero-shot indicates that
              not all of the components of the Pile were present in the training
              data.
            </p>
            <table>
              <thead>
                <tr>
                  <th>Rank</th>
                  <th>Model</th>
                  <th>Test BPB</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p>1.</p>
                    <p><span>Jan 1.2021</span></p>
                  </td>
                  <td>
                    <p>GPT-3 (Zero-Shot)*</p>
                    <p>OpenAI</p>
                  </td>
                  <td><p>0.7177</p></td>
                </tr>
                <tr>
                  <td>
                    <p>2.</p>
                    <p><span>Jan 1.2021</span></p>
                  </td>
                  <td>
                    <p>GPT-2 (Zero-Shot)*</p>
                    <p>OpenAI</p>
                  </td>
                  <td><p>1.2253</p></td>
                </tr>
              </tbody>
            </table>
            
          </div>
        </div>
      </div>
    </section></div>
  </body>
</html>
