<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://huggingface.co/openlm-research/open_llama_13b">Original</a>
    <h1>OpenLLaMA 13B Released</h1>
    
    <div id="readability-page-1" class="page"><div>
	<!-- HTML_TAG_START -->
<p>In this repo, we present a permissively licensed open source reproduction of Meta AI&#39;s <a rel="noopener nofollow" href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">LLaMA</a> large language model. We are releasing 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the <a rel="noopener nofollow" href="https://github.com/openlm-research/open_llama">project homepage of OpenLLaMA</a> for more details.</p>
<h2>
	<a rel="noopener nofollow" href="#weights-release-license-and-usage" id="weights-release-license-and-usage">
		
	</a>
	<span>
		Weights Release, License and Usage
	</span>
</h2>
<p>We release the weights in two formats: an EasyLM format to be use with our <a rel="noopener nofollow" href="https://github.com/young-geng/EasyLM">EasyLM framework</a>, and a PyTorch format to be used with the <a rel="noopener nofollow" href="https://huggingface.co/docs/transformers/index">Hugging Face transformers</a> library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.</p>
<h3>
	<a rel="noopener nofollow" href="#loading-the-weights-with-hugging-face-transformers" id="loading-the-weights-with-hugging-face-transformers">
		
	</a>
	<span>
		Loading the Weights with Hugging Face Transformers
	</span>
</h3>
<p>Preview checkpoints can be directly loaded from Hugging Face Hub. <strong>Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations.</strong> This can be achieved by directly using the <code>LlamaTokenizer</code> class, or passing in the <code>use_fast=False</code> option for the <code>AutoTokenizer</code> class. See the following example for usage.</p>
<pre><code><span>import</span> torch
<span>from</span> transformers <span>import</span> LlamaTokenizer, LlamaForCausalLM



model_path = <span>&#39;openlm-research/open_llama_13b&#39;</span>

tokenizer = LlamaTokenizer.from_pretrained(model_path)
model = LlamaForCausalLM.from_pretrained(
    model_path, torch_dtype=torch.float16, device_map=<span>&#39;auto&#39;</span>,
)

prompt = <span>&#39;Q: What is the largest animal?\nA:&#39;</span>
input_ids = tokenizer(prompt, return_tensors=<span>&#34;pt&#34;</span>).input_ids

generation_output = model.generate(
    input_ids=input_ids, max_new_tokens=<span>32</span>
)
print(tokenizer.decode(generation_output[<span>0</span>]))
</code></pre>
<p>For more advanced usage, please follow the <a rel="noopener nofollow" href="https://huggingface.co/docs/transformers/main/model_doc/llama">transformers LLaMA documentation</a>.</p>
<h3>
	<a rel="noopener nofollow" href="#evaluating-with-lm-eval-harness" id="evaluating-with-lm-eval-harness">
		
	</a>
	<span>
		Evaluating with LM-Eval-Harness
	</span>
</h3>
<p>The model can be evaluated with <a rel="noopener nofollow" href="https://github.com/EleutherAI/lm-evaluation-harness">lm-eval-harness</a>. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in <code>use_fast=False</code> to <a rel="noopener nofollow" href="https://github.com/EleutherAI/lm-evaluation-harness/blob/4b701e228768052cfae9043dca13e82052ca5eea/lm_eval/models/huggingface.py#LL313C9-L316C10">this part of lm-eval-harness</a>, as shown in the example below:</p>
<pre><code>tokenizer = self.AUTO_TOKENIZER_CLASS.from_pretrained(
    pretrained <span>if</span> tokenizer <span>is</span> <span>None</span> <span>else</span> tokenizer,
    revision=revision + (<span>&#34;/&#34;</span> + subfolder <span>if</span> subfolder <span>is</span> <span>not</span> <span>None</span> <span>else</span> <span>&#34;&#34;</span>),
    use_fast=<span>False</span>
)
</code></pre>
<h3>
	<a rel="noopener nofollow" href="#loading-the-weights-with-easylm" id="loading-the-weights-with-easylm">
		
	</a>
	<span>
		Loading the Weights with EasyLM
	</span>
</h3>
<p>For using the weights in our EasyLM framework, please refer to the <a rel="noopener nofollow" href="https://github.com/young-geng/EasyLM/blob/main/docs/llama.md">LLaMA documentation of EasyLM</a>. Note that unlike the original LLaMA model, our OpenLLaMA tokenizer and weights are trained completely from scratch so it is no longer needed to obtain the original LLaMA tokenizer and weights. Note that we use BOS (beginning of sentence) token (id=1) during training, so it is best to prepend this token for best performance during few-shot evaluation.</p>
<h2>
	<a rel="noopener nofollow" href="#dataset-and-training" id="dataset-and-training">
		
	</a>
	<span>
		Dataset and Training
	</span>
</h2>
<p>We train our models on the <a rel="noopener nofollow" href="https://www.together.xyz/blog/redpajama">RedPajama</a> dataset released by <a rel="noopener nofollow" href="https://www.together.xyz/">Together</a>, which is a reproduction of the LLaMA training dataset containing over 1.2 trillion tokens. We follow the exactly same preprocessing steps and training hyperparameters as the original LLaMA paper, including model architecture, context length, training steps, learning rate schedule, and optimizer.  The only difference between our setting and the original one is the dataset used: OpenLLaMA employs the RedPajama dataset rather than the one utilized by the original LLaMA.</p>
<p>We train the models on cloud TPU-v4s using <a rel="noopener nofollow" href="https://github.com/young-geng/EasyLM">EasyLM</a>, a JAX based training pipeline we developed for training and fine-tuning large language models. We employ a combination of normal data parallelism and <a rel="noopener nofollow" href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">fully sharded data parallelism (also know as ZeRO stage 3)</a> to balance the training throughput and memory usage. Overall we reach a throughput of over 2200 tokens / second / TPU-v4 chip for our 7B model.</p>
<h2>
	<a rel="noopener nofollow" href="#evaluation" id="evaluation">
		
	</a>
	<span>
		Evaluation
	</span>
</h2>
<p>We evaluated OpenLLaMA on a wide range of tasks using <a rel="noopener nofollow" href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>.  The LLaMA results are generated by running the original LLaMA model on the same evaluation metrics. We note that our results for the LLaMA model differ slightly from the original LLaMA paper, which we believe is a result of different evaluation protocols. Similar differences have been reported in <a rel="noopener nofollow" href="https://github.com/EleutherAI/lm-evaluation-harness/issues/443">this issue of lm-evaluation-harness</a>. Additionally, we present the results of GPT-J, a 6B parameter model trained on the <a rel="noopener nofollow" href="https://pile.eleuther.ai/">Pile</a> dataset by <a rel="noopener nofollow" href="https://www.eleuther.ai/">EleutherAI</a>.</p>
<p>The original LLaMA model was trained for 1 trillion tokens and GPT-J was trained for 500 billion tokens.  We present the results in the table below. OpenLLaMA exhibits comparable performance to the original LLaMA and GPT-J across a majority of tasks, and outperforms them in some tasks.</p>
<div>
	<table>
		<thead><tr>
<th><strong>Task/Metric</strong></th>
<th>GPT-J 6B</th>
<th>LLaMA 7B</th>
<th>LLaMA 13B</th>
<th>OpenLLaMA 7B</th>
<th>OpenLLaMA 3B</th>
<th>OpenLLaMA 13B</th>
</tr>

		</thead><tbody><tr>
<td>anli_r1/acc</td>
<td>0.32</td>
<td>0.35</td>
<td>0.35</td>
<td>0.33</td>
<td>0.33</td>
<td>0.33</td>
</tr>
<tr>
<td>anli_r2/acc</td>
<td>0.34</td>
<td>0.34</td>
<td>0.36</td>
<td>0.36</td>
<td>0.32</td>
<td>0.33</td>
</tr>
<tr>
<td>anli_r3/acc</td>
<td>0.35</td>
<td>0.37</td>
<td>0.39</td>
<td>0.38</td>
<td>0.35</td>
<td>0.40</td>
</tr>
<tr>
<td>arc_challenge/acc</td>
<td>0.34</td>
<td>0.39</td>
<td>0.44</td>
<td>0.37</td>
<td>0.34</td>
<td>0.41</td>
</tr>
<tr>
<td>arc_challenge/acc_norm</td>
<td>0.37</td>
<td>0.41</td>
<td>0.44</td>
<td>0.38</td>
<td>0.37</td>
<td>0.44</td>
</tr>
<tr>
<td>arc_easy/acc</td>
<td>0.67</td>
<td>0.68</td>
<td>0.75</td>
<td>0.72</td>
<td>0.69</td>
<td>0.75</td>
</tr>
<tr>
<td>arc_easy/acc_norm</td>
<td>0.62</td>
<td>0.52</td>
<td>0.59</td>
<td>0.68</td>
<td>0.65</td>
<td>0.70</td>
</tr>
<tr>
<td>boolq/acc</td>
<td>0.66</td>
<td>0.75</td>
<td>0.71</td>
<td>0.71</td>
<td>0.68</td>
<td>0.75</td>
</tr>
<tr>
<td>hellaswag/acc</td>
<td>0.50</td>
<td>0.56</td>
<td>0.59</td>
<td>0.53</td>
<td>0.49</td>
<td>0.56</td>
</tr>
<tr>
<td>hellaswag/acc_norm</td>
<td>0.66</td>
<td>0.73</td>
<td>0.76</td>
<td>0.72</td>
<td>0.67</td>
<td>0.76</td>
</tr>
<tr>
<td>openbookqa/acc</td>
<td>0.29</td>
<td>0.29</td>
<td>0.31</td>
<td>0.30</td>
<td>0.27</td>
<td>0.31</td>
</tr>
<tr>
<td>openbookqa/acc_norm</td>
<td>0.38</td>
<td>0.41</td>
<td>0.42</td>
<td>0.40</td>
<td>0.40</td>
<td>0.43</td>
</tr>
<tr>
<td>piqa/acc</td>
<td>0.75</td>
<td>0.78</td>
<td>0.79</td>
<td>0.76</td>
<td>0.75</td>
<td>0.77</td>
</tr>
<tr>
<td>piqa/acc_norm</td>
<td>0.76</td>
<td>0.78</td>
<td>0.79</td>
<td>0.77</td>
<td>0.76</td>
<td>0.79</td>
</tr>
<tr>
<td>record/em</td>
<td>0.88</td>
<td>0.91</td>
<td>0.92</td>
<td>0.89</td>
<td>0.88</td>
<td>0.91</td>
</tr>
<tr>
<td>record/f1</td>
<td>0.89</td>
<td>0.91</td>
<td>0.92</td>
<td>0.90</td>
<td>0.89</td>
<td>0.91</td>
</tr>
<tr>
<td>rte/acc</td>
<td>0.54</td>
<td>0.56</td>
<td>0.69</td>
<td>0.60</td>
<td>0.58</td>
<td>0.64</td>
</tr>
<tr>
<td>truthfulqa_mc/mc1</td>
<td>0.20</td>
<td>0.21</td>
<td>0.25</td>
<td>0.23</td>
<td>0.22</td>
<td>0.25</td>
</tr>
<tr>
<td>truthfulqa_mc/mc2</td>
<td>0.36</td>
<td>0.34</td>
<td>0.40</td>
<td>0.35</td>
<td>0.35</td>
<td>0.38</td>
</tr>
<tr>
<td>wic/acc</td>
<td>0.50</td>
<td>0.50</td>
<td>0.50</td>
<td>0.51</td>
<td>0.48</td>
<td>0.47</td>
</tr>
<tr>
<td>winogrande/acc</td>
<td>0.64</td>
<td>0.68</td>
<td>0.70</td>
<td>0.67</td>
<td>0.62</td>
<td>0.70</td>
</tr>
<tr>
<td>Average</td>
<td>0.52</td>
<td>0.55</td>
<td>0.57</td>
<td>0.55</td>
<td>0.53</td>
<td>0.57</td>
</tr>
</tbody>
	</table>
</div>
<p>We removed the task CB and WSC from our benchmark, as our model performs suspiciously well on these two tasks. We hypothesize that there could be a benchmark data contamination in the training set.</p>
<h2>
	<a rel="noopener nofollow" href="#contact" id="contact">
		
	</a>
	<span>
		Contact
	</span>
</h2>
<p>We would love to get feedback from the community. If you have any questions, please open an issue or contact us.</p>
<p>OpenLLaMA is developed by:
<a rel="noopener nofollow" href="https://young-geng.xyz/">Xinyang Geng</a>* and <a rel="noopener nofollow" href="https://www.haoliu.site/">Hao Liu</a>* from Berkeley AI Research.
*Equal Contribution</p>
<h2>
	<a rel="noopener nofollow" href="#acknowledgment" id="acknowledgment">
		
	</a>
	<span>
		Acknowledgment
	</span>
</h2>
<p>We thank the <a rel="noopener nofollow" href="https://sites.research.google/trc/about/">Google TPU Research Cloud</a> program for providing part of the computation resources. We’d like to specially thank Jonathan Caton from TPU Research Cloud for helping us organizing compute resources, Rafi Witten from the Google Cloud team and James Bradbury from the Google JAX team for helping us optimizing our training throughput. We’d also want to thank Charlie Snell, Gautier Izacard, Eric Wallace, Lianmin Zheng and our user community for the discussions and feedback.</p>
<p>The OpenLLaMA 13B model is trained in collaboration with <a rel="noopener nofollow" href="https://stability.ai/">Stability AI</a>, and we thank Stability AI for providing the computation resources. We’d like to especially thank David Ha and Shivanshu Purohit for the coordinating the logistics and providing engineering support.</p>
<h2>
	<a rel="noopener nofollow" href="#reference" id="reference">
		
	</a>
	<span>
		Reference
	</span>
</h2>
<p>If you found OpenLLaMA useful in your research or applications, please cite using the following BibTeX:</p>
<pre><code>@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}
</code></pre>
<pre><code>@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}
</code></pre>
<pre><code>@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\&#39;e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
</code></pre>
<!-- HTML_TAG_END --></div></div>
  </body>
</html>
