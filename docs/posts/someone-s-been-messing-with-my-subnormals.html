<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="http://moyix.blogspot.com/2022/09/someones-been-messing-with-my-subnormals.html">Original</a>
    <h1>Someone’s Been Messing with My Subnormals</h1>
    
    <div id="readability-page-1" class="page"><div id="post-body-6546057836548108320">
<p><i><b>TL;DR: After noticing an annoying warning, I went on an absurd yak shave, and discovered that because of a tiny handful of Python packages built with an appealing-sounding but dangerous compiler option, more than 2,500 Python packages—some with more than a million downloads per month—could end up causing any program that uses them to compute incorrect numerical results.</b></i></p><h3>Once Upon a Time in My Terminal</h3><p>Recently, whenever I tried to import certain Python packages (notably, some models from Huggingface Transformers), I would see this weird and unsightly warning:</p><p><span>Python 3.8.10 (default, Jun 22 2022, 20:18:18)<span> </span></span></p><p><span>Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information</span></p><p><span>IPython 8.4.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.</span></p><p><span><span>In [</span><span><b>1</b></span><span>]: </span><span><b>from</b></span><span> </span><span><b>transformers</b></span><span> </span><span><b>import</b></span><span> CodeGenForCausalLM</span></span></p><p><span>/home/moyix/.virtualenvs/sfcodegen/lib/python3.8/site-packages/numpy/core/getlimits.py:498: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float32&#39;&gt; type is zero.</span></p><p><span><span>  </span>setattr(self, word, getattr(machar, word).flat[0])</span></p><p><span>/home/moyix/.virtualenvs/sfcodegen/lib/python3.8/site-packages/numpy/core/getlimits.py:88: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float32&#39;&gt; type is zero.</span></p><p><span><span>  </span>return self._float_to_str(self.smallest_subnormal)</span></p><p><span>/home/moyix/.virtualenvs/sfcodegen/lib/python3.8/site-packages/numpy/core/getlimits.py:498: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float64&#39;&gt; type is zero.</span></p><p><span><span>  </span>setattr(self, word, getattr(machar, word).flat[0])</span></p><p><span>/home/moyix/.virtualenvs/sfcodegen/lib/python3.8/site-packages/numpy/core/getlimits.py:88: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float64&#39;&gt; type is zero.</span></p><p><span><span>  </span>return self._float_to_str(self.smallest_subnormal)</span></p><p><i><b>Someone was messing with my floating point subnormals!</b></i> Aside from the error messages being really annoying, it also made me a little worried. Floating point math is notoriously tricky, and if something is changing the behavior of the floating point unit (FPU) on the CPU it can cause all sorts of weird problems. For example, <a href="https://simonbyrne.github.io/notes/fastmath/#flushing_subnormals_to_zero">some numerical algorithms depend on the standard FPU behavior and will fail to converge</a> if the FPU is set to treat subnormal/denormal numbers as zero (on x86, by setting the FTZ/DAZ flags in the MXCSR register).</p><p>Some Googling <a href="https://github.com/Qiskit/qiskit-aer/issues/1461">led me to this issue</a>, which pointed toward some shared library compiled with the gcc/clang option <span>-ffast-math</span> that was being loaded as the culprit. It turns out (somewhat insanely) that when <span>-ffast-math</span> is enabled, the compiler will link in a constructor that sets the FTZ/DAZ flags whenever the library is loaded — even on shared libraries, which means that any application that loads that library will have its floating point behavior changed <i>for the whole process</i>. And <span>-Ofast</span>, which sounds appealingly like a &#34;make my program go fast&#34; flag, automatically enables <span>-ffast-math</span>, so some projects may unwittingly turn it on without realizing the implications.</p><p>But what shared libraries were even being loaded? We can find out by taking the PID of our Python interpreter and then looking at /proc/[PID]/maps; this will show all the mapped memory regions in the process and show the names for the ones that are file-backed (which shared libraries are). We can then filter that by grepping for <span>r-xp</span> (readable + executable + copy-on-write) to only list code sections, and then look for shared objects:</p><p><span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ cat /proc/2902749/maps | grep &#39;r-xp&#39; | grep -F &#39;.so&#39; | wc -l</span></span></p><p><span>158</span></p><p>Oof, 158 shared objects? In <i>my</i> Python process? It&#39;s more common than you think.</p><p>So now I wanted to narrow down which library (or libraries) was actually setting FTZ/DAZ. After going down a somewhat painful blind alley where I tried to modify QEMU&#39;s user-mode emulation so that all updates to MXCSR would get logged along with the name of the library containing the current program counter (NB: it turns out to be really annoying to map the program counter back to a library name), I hit on a simpler strategy.</p><p>Python lets you load arbitrary shared objects using ctypes.CDLL. So if I could just load each of those shared libraries one at a time and then trigger the numpy code that prints the warning, I could identify which library was messing with my floating point behavior. By reading through the getlimits.py file mentioned in the warning, I figured out that the check could be triggered by printing out <span>numpy.finfo(numpy.float32)</span>, and ended up with this script:</p><p><span><span><b>import</b></span><span> </span><span><b>sys</b></span></span></p><p><span><span><b>from</b></span><span> </span><span><b>ctypes</b></span><span> </span><span><b>import</b></span><span> CDLL</span></span></p><p><span><span>CDLL(sys</span><span>.</span><span>argv[</span><span>1</span><span>])</span></span></p><p><span><span><b>import</b></span><span> </span><span><b>numpy</b></span><span> </span><span><b>as</b></span><span> </span><span><b>np</b></span></span></p><p><span><span>print</span><span>(np</span><span>.</span><span>finfo(np</span><span>.</span><span>float32))</span></span></p><p>We can check that it works by building an empty C file as a shared library with and without <span><span face="ui-monospace, SFMono-Regular, &#34;SF Mono&#34;, Menlo, Consolas, &#34;Liberation Mono&#34;, monospace">-Ofast</span></span>. Without <span>-Ofast</span>:</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ pygmentize foo.c</span></span></p><p><span><span>void</span><span> </span><span>empty</span><span>()</span><span> </span><span>{</span><span> </span><span>}</span></span></p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ gcc -fpic -shared foo.c -o foo.so</span></span></p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ python fptest.py ./foo.so</span></span></p><p><span>Machine parameters for float32</span></p><p><span>---------------------------------------------------------------</span></p><p><span>precision = <span>  </span>6 <span>  </span>resolution = 1.0000000e-06</span></p><p><span>machep =<span>    </span>-23 <span>  </span>eps =<span>        </span>1.1920929e-07</span></p><p><span>negep = <span>    </span>-24 <span>  </span>epsneg = <span>    </span>5.9604645e-08</span></p><p><span>minexp = <span>  </span>-126 <span>  </span>tiny = <span>      </span>1.1754944e-38</span></p><p><span>maxexp =<span>    </span>128 <span>  </span>max =<span>        </span>3.4028235e+38</span></p><p><span>nexp =<span>        </span>8 <span>  </span>min =<span>        </span>-max</span></p><p><span>smallest_normal = 1.1754944e-38 <span>  </span>smallest_subnormal = 1.4012985e-45</span></p><p><span>---------------------------------------------------------------</span></p><p>But with <span>-Ofast</span> enabled:</p><p><span>/home/moyix/.virtualenvs/sfcodegen/lib/python3.8/site-packages/numpy/core/getlimits.py:498: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float32&#39;&gt; type is zero.</span></p><p><span><span>  </span>setattr(self, word, getattr(machar, word).flat[0])</span></p><p><span>/home/moyix/.virtualenvs/sfcodegen/lib/python3.8/site-packages/numpy/core/getlimits.py:88: UserWarning: The value of the smallest subnormal for &lt;class &#39;numpy.float32&#39;&gt; type is zero.</span></p><p><span><span>  </span>return self._float_to_str(self.smallest_subnormal)</span></p><p><span>Machine parameters for float32</span></p><p><span>---------------------------------------------------------------</span></p><p><span>precision = <span>  </span>6 <span>  </span>resolution = 1.0000000e-06</span></p><p><span>machep =<span>    </span>-23 <span>  </span>eps =<span>        </span>1.1920929e-07</span></p><p><span>negep = <span>    </span>-24 <span>  </span>epsneg = <span>    </span>5.9604645e-08</span></p><p><span>minexp = <span>  </span>-126 <span>  </span>tiny = <span>      </span>1.1754944e-38</span></p><p><span>maxexp =<span>    </span>128 <span>  </span>max =<span>        </span>3.4028235e+38</span></p><p><span>nexp =<span>        </span>8 <span>  </span>min =<span>        </span>-max</span></p><p><span>smallest_normal = 1.1754944e-38 <span>  </span>smallest_subnormal = 0.0000000e+00</span></p><p><span>---------------------------------------------------------------</span></p><p>Great! So now we have a detector for shared libraries that set FTZ/DAZ. It&#39;s kind of annoying that we have to load the library to check, but we&#39;ll fix that later. Running it in a loop over all the libraries that we found were loaded in the Python process earlier, I found that the culprit was <b>gevent</b>, of all things (why is an event-based networking library messing with floating point behavior??).</p><p>Of course, now that I knew it was <b>gevent</b>, some more Googling located the relevant bug report. It seems that it was a <a href="https://github.com/gevent/gevent/pull/1820">known bug with an attempted fix</a>, but the fix didn&#39;t quite work (it turns out that when you use <span>-Ofast</span>, <span>-fno-fast-math</span> does not, in fact, disable fast math. lol. lmao.) and so the most recent version of <b>gevent</b> on PyPI still messes with floating point behavior for no good reason.</p><h3>What else is out there?</h3><p>With the immediate mystery solved, I wanted to figure out how many other projects might have (intentionally or inadvertently) enabled <span>-ffast-math</span> in their shared libraries uploaded to PyPI. So I decided to take the top 25% of projects on PyPI by number of downloads and scan their binary wheels (Python jargon for precompiled binaries) to see if they, too, messed with floating point behavior.</p><p>Let&#39;s start by finding the top 25% of projects. We can get a list of the number of downloads for each project on PyPI <a href="https://packaging.python.org/en/latest/guides/analyzing-pypi-package-downloads/">by querying a handy BigQuery table that PyPI publishes</a>. Skipping over all the warnings about how inaccurate the download numbers are, I was able to write this little bit of BigQuery SQL that gives the total downloads for each project over the past 30 days:</p><div><p><span>#standardSQL</span></p><p><span>SELECT</span> file.<span>project</span>, <span>COUNT</span><span>(*)</span> <span>AS</span> num_downloads</p><p><span>FROM</span> <span>`bigquery-public-data.pypi.file_downloads`</span></p><p><span>WHERE</span></p><p><span>-- Only query the last 30 days of history</span></p><p><span>DATE</span><span>(</span><span>timestamp</span><span>)</span></p><p><span>BETWEEN</span> <span>DATE_SUB</span><span>(</span><span>CURRENT_DATE</span><span>()</span>, <span>INTERVAL</span> <span>30</span> DAY<span>)</span></p><p><span>AND</span> <span>CURRENT_DATE</span><span>()</span></p><p><span>GROUP</span> <span>BY</span></p><p>  file.<span>project</span></p></div><p>And then save that as a CSV file for further analysis. Once we have the CSV, we can drop it into pandas (or your favorite data analysis toolbox) and extract out a list of the names of the top 25% of projects by download count pretty easily:</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ ipython</span></span></p><p><span>Python 3.8.10 (default, Jun 22 2022, 20:18:18)<span> </span></span></p><p><span>Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information</span></p><p><span>IPython 8.4.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.</span></p><p><span><span>In [</span><span><b>1</b></span><span>]: </span><span><b>import</b></span><span> </span><span><b>pandas</b></span><span> </span><span><b>as</b></span><span> </span><span><b>pd</b></span></span></p><p><span><span>In [</span><span><b>2</b></span><span>]: </span><span><b>import</b></span><span> </span><span><b>numpy</b></span><span> </span><span><b>as</b></span><span> </span><span><b>np</b></span></span></p><p><span><span>In [</span><span><b>3</b></span><span>]: </span><span>df = pd.read_csv(</span><span>&#39;pypi_downloads_20220901_30d.csv&#39;</span><span>)</span></span></p><p><span><span>In [</span><span><b>4</b></span><span>]: </span><span>top_25p = df[df[</span><span>&#39;num_downloads&#39;</span><span>] &gt; df[</span><span>&#39;num_downloads&#39;</span><span>].quantile(</span><span>0.75</span><span>)]</span></span></p><p><span><span>In [</span><span><b>5</b></span><span>]: </span><span>top_25p.head()</span></span></p><p><span><span>Out[</span><span><b>5</b></span><span>]:<span> </span></span></span></p><p><span><span>        </span>project<span>  </span>num_downloads</span></p><p><span>0<span>          </span>lima <span>          </span>2384</span></p><p><span>1<span>    </span>kiwisolver <span>      </span>25721252</span></p><p><span>2 <span>  </span>quill-delta <span>          </span>6128</span></p><p><span>3 <span>      </span>aiorpcx<span>          </span>12998</span></p><p><span>4<span>  </span>flake8-flask <span>          </span>6843</span></p><p><span><span>In [</span><span><b>6</b></span><span>]: </span><span>len</span><span>(top_25p)</span></span></p><p><span><span>Out[</span><span><b>6</b></span><span>]: </span><span>102864</span></span></p><p><span><span>In [</span><span><b>7</b></span><span>]: </span><span>np.savetxt(</span><span>&#39;top_25p_projects.txt&#39;</span><span>, top_25p[</span><span>&#39;project&#39;</span><span>].values, fmt=</span><span>&#39;</span><span><b>%s</b></span><span>&#39;</span><span>)</span></span></p><p>Okay, around 100K projects, that&#39;s manageable. Now it was time to try to download the wheels for those projects, extract them, and check for any floating point funny business. But wait, how are we going to do that check? I don&#39;t really want to load a bunch of .so files downloaded off the internet, given that any of them could execute arbitrary code. [My worries here about executing arbitrary code will seem richly ironic later in the post.]</p><h3>Interlude: A Static Checker for <span>crtfastmath</span></h3><p>How can we check if a library will mess with FTZ/DAZ without actually running any of its code? Let&#39;s go back to how the code that sets these bits actually gets executed in the first place. Shared libraries on Linux get loaded by the dynamic loader, which loops over the contents of the .init_array section looking for constructors that should be called when the library is loaded and then calling each one. We can print out the contents of this section using objdump:</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ objdump -s -j .init_array foo.so</span></span></p><p><span>foo.so: <span>    </span>file format elf64-x86-64</span></p><p><span>Contents of section .init_array:</span></p><p><span><span> </span>3e78 10110000 00000000 40100000 00000000<span>  </span>........@.......</span></p><p>Each entry in this array is a pointer stored in little-endian form; on a 64-bit system a pointer is 8 bytes, so in this example we have two constructors, and we can print their addresses with this awful incantation (my specialty!):</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ objdump -s -j .init_array foo.so | sed -e &#39;1,/Contents/ d&#39; | cut -c 7-40 | xxd -r -p | od -An -t x8 -w8</span></span></p><p><span><span> </span>0000000000001110</span></p><p><span><span> </span>0000000000001040</span></p><p>Now we can disassemble them with objdump. The first one is not very exciting:</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ objdump -d --start-address=0x0000000000001110 foo.so | head -20</span></span></p><p><span>foo.so: <span>    </span>file format elf64-x86-64</span></p><p><span>Disassembly of section .text:</span></p><p><span>0000000000001110 &lt;frame_dummy&gt;:</span></p><p><span><span>    </span>1110: <span>      </span>f3 0f 1e fa <span>            </span>endbr64<span> </span></span></p><p><span><span>    </span>1114: <span>      </span>e9 77 ff ff ff<span>          </span>jmpq <span>  </span>1090 &lt;register_tm_clones&gt;</span></p><p><span><span>    </span>1119: <span>      </span>0f 1f 80 00 00 00 00<span>    </span>nopl <span>  </span>0x0(%rax)</span></p><p>But the second one is what we want to look for (note: in stripped binaries, you won&#39;t see the nice function name, so we&#39;ll have to detect it by looking at the actual instructions):</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>~</b></span><span>$ objdump -d --start-address=0x0000000000001040 foo.so | head -20</span></span></p><p><span>foo.so: <span>    </span>file format elf64-x86-64</span></p><p><span>Disassembly of section .text:</span></p><p><span>0000000000001040 &lt;set_fast_math&gt;:</span></p><p><span><span>    </span>1040: <span>      </span>f3 0f 1e fa <span>            </span>endbr64<span> </span></span></p><p><span><span>    </span>1044: <span>      </span>0f ae 5c 24 fc<span>          </span>stmxcsr -0x4(%rsp)</span></p><p><span><span>    </span>1049: <span>      </span>81 4c 24 fc 40 80 00<span>    </span>orl<span>    </span>$0x8040,-0x4(%rsp)</span></p><p><span><span>    </span>1050: <span>      </span>00<span> </span></span></p><p><span><span>    </span>1051: <span>      </span>0f ae 54 24 fc<span>          </span>ldmxcsr -0x4(%rsp)</span></p><p><span><span>    </span>1056: <span>      </span>c3<span>                      </span>retq<span>   </span></span></p><p><span><span>    </span>1057: <span>      </span>66 0f 1f 84 00 00 00<span>    </span>nopw <span>  </span>0x0(%rax,%rax,1)</span></p><p><span><span>    </span>105e: <span>      </span>00 00<span> </span></span></p><p>This snippet uses stmxcsr to get the value of the MXCSR register, ORs it with 0x8040 (MTZ | DAZ), and then uses ldmxcsr to save it back into the MXCSR register. Simple enough!</p><p>I wrote a <a href="https://gist.github.com/moyix/6ebe542affd555218bdb82b40ec49291">hacky Python script</a> that uses basically these same objdump commands to disassemble each constructor up until the first return (retq) instruction, and report if any of them contain all three of stmxcsr, ldmxcsr, and the constant 0x8040. This will miss any code pattern that doesn&#39;t exactly match the code found in gcc&#39;s <span>crtfastmath</span> library, but it&#39;s good enough and shouldn&#39;t have false positives.</p><p>Unfortunately, it also turns out to be quite slow, since it invokes objdump once to get the .init_array entries, and then N times to disassemble (once for each constructor). This really adds up when you want to scan thousands of files, especially because (as I now am doomed to know forever) shared libraries built in C++ can have thousands of constructors (the highest I saw was in <span>scine-serenity-wrapper</span>&#39;s <span>serenity.module.so</span>, which has a whopping 11,841 different constructors (!!!)).</p><p>Instead I switched to searching for the <i>exact</i> byte sequence that encodes the stmxcsr, orl, ldmxcsr, and retq instructions. This is even more brittle than the disassembly-based technique, but since none of the instructions involved use any memory addresses (which would change depending on exactly where <span>crtfastmath</span> was linked into the binary), the sequence is pretty consistent over the range of compilers I could check (gcc-{5,7,8,9,11} and clang-{10,11,12,14}). The only difference I saw is that newer compilers start the function with <span>endbr64</span> (an instruction that tells Intel&#39;s Control-flow Enforcement Technology, an exploit mitigation technique, that this is a valid jump target).</p><p>The <a href="https://gist.github.com/moyix/2154125d0cb9947ec0525fb49449fab7">final script can be found here</a>. It uses <a href="https://github.com/eliben/pyelftools">pyelftools</a> to parse the binary and extract the list of constructors, then tests for a string match at each of the constructors listed. This is <i>much</i> faster than multiple calls to objdump and can check thousands of binaries per second, which is the kind of scale we need.</p><h3>Obtaining <strike>100GB</strike> 11.6TB of Shared Libraries from PyPI</h3><p>In the first draft of this post, I was going to wimp out and just do the top 25%, and only one binary wheel for the most recent version of each project. But my ambition and curiosity (and a strong desire to procrastinate on preparing for my first lecture on Tuesday) got the better of me, so I decided to go ahead and get <i>all</i> of the x86-64 binary wheels for every version of all packages on PyPI.</p><p>We start by getting a list of all Python package names using the <a href="https://pypi.org/simple/">PyPI Simple Index</a>, which is basically just a giant HTML file with one link per package.</p><p>Next we want to get the metadata for each package, including the list of releases and all the URLs to the actual binary packages. Each package has a JSON description at https://pypi.org/pypi/NAME/json; for example, here&#39;s the <a href="https://pypi.org/pypi/gevent/json">metadata for gevent</a>, the package that kicked off this quixotic quest. We can download all of them using wget; I&#39;ll also request gzip compression so that I can save PyPI some bandwidth:</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>/fastdata/pypi_wheels</b></span><span>$ cat all_packages.txt | sed &#39;s|^|https://pypi.org/pypi/|;s|$|/json|&#39; &gt; pypi_all_urls.txt</span></span></p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>/fastdata/pypi_wheels</b></span><span>$ wget --no-verbose --header=&#34;Accept-Encoding: gzip&#34; -O pypi_all.json.gz -i pypi_all_urls.txt 2&gt; wget_errors.txt</span></span></p><p>When you use -O with multiple files like this, wget will concatenate all of them together into one big file, but luckily it turns out that the concatenation of multiple gzip files is itself a valid gzip file, so we end up with all package metadata (well, almost all – about 10,723 packages returned 404, but that&#39;s not too bad compared to the 386,544 packages we <i>did</i> manage to get info for).</p><p>After about an hour and an embarrassing amount of searching StackOverflow, I came up with a terrifying <span>jq</span> one-liner to extract out all the URLs of the x86-64 Linux wheels from the 1.5GB of compressed JSON data:</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>/fastdata/pypi_wheels</b></span><span>$ </span><span>zcat</span><span>  </span><span>pypi_all.json.gz | jq -cr &#39;.releases[] | .[] | [select(.packagetype | contains(&#34;bdist_wheel&#34;))] | .[] | select(.url | test(&#34;manylinux.*x86_64&#34;)) | [.size, .url] | join(&#34; &#34;)&#39; &gt; pypi_all_wheels.txt</span></span></p><p>This gives us 269,752 packages to download, along with their sizes. Adding up all the sizes, we get a grand total of 4 TB, which I managed to download in about 12 hours using <a href="https://aria2.github.io/">aria2</a> (while I love <span>wget</span>, for a job this big I wanted something that could save/resume sessions and make many connections at once).</p><p>Now we have a small snag. While I do have quite a lot (10 TB) of NVME storage, it&#39;s still not enough to unpack all the shared objects at once for scanning. Instead I put together a <a href="https://gist.github.com/moyix/71896182bc5937fb8a1a882d765bc8ac">small script</a> to unpack just one package at a time, scan it, and then remove the extracted data. Finally, I am <a href="https://git.savannah.gnu.org/cgit/parallel.git/tree/doc/citation-notice-faq.txt">legally obligated</a> to inform you that I used GNU Parallel to scan all the wheels:</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>/fastdata/pypi_wheels</b></span><span>$ find wheels -type f -print0 | parallel --progress -0 --xargs python extract_and_scan.py {#} {}</span></span></p><p>[This didn&#39;t go quite as smoothly as I&#39;m making out — when you&#39;re scanning ~2.4 million shared libraries across ~270K wheels, you find all sorts of edge cases in your code; I discovered <a href="https://twitter.com/moyix/status/1565525352180580352">.so files that were in fact text files</a>, ARM and 32-bit x86 libraries bundled into an x86-64 package, and some partially mangled zip files.]</p><p>And so now we have everything we need to count how many x86-64 shared libraries there are in all of PyPI that were linked with <span>-ffast-math</span>:</p><p><span><span>(sfcodegen) </span><span><b>moyix@isabella</b></span><span>:</span><span><b>/fastdata/pypi_wheels</b></span><span>$ grep -F &#39;contains ffast-math constructor&#39; ffast_results_all.txt | wc -l</span></span></p><p><span>5830</span></p><p>A mere 5,830 out of the 2.4 million we scanned!</p><h3>On the Origin of Bad Floating Point Math</h3><p>To find out a little more about the packages that they&#39;re associated with, we can extract the dist-info metadata from each wheel. Unfortunately <a href="https://pypi.org/project/wheel-inspect/">the first library I found to do this</a> was extremely strict, and barfed on the many, many wheels in my dataset with slightly malformed metadata. I went on a <i>yet another </i>giant yak shaving expedition and <a href="https://gist.github.com/moyix/1bf820837930ec56d214952b0cce2d32">wrote my own robust parser</a>.</p><p>Along the way I learned a lot of fun facts about Python&#39;s packaging metadata. Did you know that the format of the METADATA file is actually based on email? And that because email is notoriously difficult to specify, the standard says that the format is &#34;[...] what the standard library email.parser module can parse using the compat32 policy&#34;? Or that the various files that can appear in the dist-info directory are an exciting menagerie of CSV, JSON, and Windows INI formats? So much knowledge that I now wish I could unlearn!</p><p>Anyway, let&#39;s get some stats. I dumped all shared library info and package metadata into a big pandas DataFrame to play around with for this.</p><p>First, how many distinct packages have at least one binary that was built with <span>-ffast-math</span>?</p><p><span><span>In [</span><span><b>28</b></span><span>]: </span><span>df[</span><span>&#39;Package&#39;</span><span>].unique()</span></span></p><p><span><span>Out[</span><span><b>28</b></span><span>]:<span> </span></span></span></p><p><span>array([&#39;archive-pdf-tools&#39;, &#39;bgfx-python&#39;,</span></p><p><span><span>       </span>&#39;bicleaner-ai-glove&#39;, &#39;BTrees&#39;, &#39;cadbiom&#39;,</span></p><p><span><span>       </span>&#39;ctranslate2&#39;, &#39;dyNET&#39;, &#39;dyNET38&#39;, &#39;gevent&#39;,</span></p><p><span><span>       </span>&#39;glove-python-binary&#39;, &#39;higra&#39;, &#39;hybridq&#39;, &#39;ikomia&#39;,</span></p><p><span><span>       </span>&#39;ioh&#39;, &#39;jij-cimod&#39;, &#39;lavavu&#39;, &#39;lavavu-osmesa&#39;,</span></p><p><span><span>       </span>&#39;MulticoreTSNE&#39;, &#39;neural-compressor&#39;, &#39;nwhy&#39;,</span></p><p><span><span>       </span>&#39;openjij&#39;, &#39;openturns&#39;, &#39;perfmetrics&#39;, &#39;pHashPy&#39;,</span></p><p><span><span>       </span>&#39;pyace-lite&#39;, &#39;pyapr&#39;, &#39;pycompadre&#39;,</span></p><p><span><span>       </span>&#39;pycompadre-serial&#39;, &#39;PyKEP&#39;, &#39;pykep&#39;,</span></p><p><span><span>       </span>&#39;pylimer-tools&#39;, &#39;pyqubo&#39;, &#39;pyscf&#39;, &#39;PyTAT&#39;,</span></p><p><span><span>       </span>&#39;python-prtree&#39;, &#39;qiskit-aer&#39;, &#39;qiskit-aer-gpu&#39;,</span></p><p><span><span>       </span>&#39;RelStorage&#39;, &#39;sail-ml&#39;, &#39;segmentation&#39;, &#39;sente&#39;,</span></p><p><span><span>       </span>&#39;sinr&#39;, &#39;snapml&#39;, &#39;superman&#39;, &#39;symengine&#39;,</span></p><p><span><span>       </span>&#39;systran-align&#39;, &#39;texture-tool&#39;, &#39;tsne-mp&#39;, &#39;xcsf&#39;],</span></p><p><span><span>      </span>dtype=object)</span></p><p><span><span>In [</span><span><b>29</b></span><span>]: </span><span>len</span><span>(df[</span><span>&#39;Package&#39;</span><span>].unique())</span></span></p><p><span><span>Out[</span><span><b>29</b></span><span>]: </span><span>49</span></span></p><p>Not nearly as bad as I thought! There are only 49 packages that, at some point in their history, released a binary wheel that had a shared library linked with <span>-ffast-math</span>. We can find out more about them by having pandas make an HTML table with the package summary for us:</p><p><span><span>In [</span><span><b>54</b></span><span>]: </span><span>counts = df.groupby([</span><span>&#39;Package&#39;</span><span>,</span><span>&#39;Summary&#39;</span><span>]).size()</span></span></p><p><span><span>In [</span><span><b>55</b></span><span>]: </span><span>counts = counts.reset_index().rename(columns={</span><span>0</span><span>:</span><span>&#39;Count&#39;</span><span>})</span></span></p><p><span><span>In [</span><span><b>56</b></span><span>]: </span><span>counts = counts.sort_values(by=</span><span>&#39;Count&#39;</span><span>,ascending=</span><span><b>False</b></span><span>)</span></span></p><p><span><span>In [</span><span><b>57</b></span><span>]: </span><span>counts.to_html(</span><span>&#39;count.html&#39;</span><span>,justify=</span><span>&#39;center&#39;</span><span>,index=</span><span><b>False</b></span><span>)</span></span></p><p>And we get a (pretty bare-bones) table <a href="https://moyix.net/~moyix/ffast_summary.html">that you can look at here</a>. Here are the first few rows:</p><div><center><table><thead><tr><th>Package</th><th>Summary</th><th>Count</th></tr></thead><tbody><tr><td>BTrees</td><td>Scalable persistent object containers</td><td>1166</td></tr><tr><td>gevent</td><td>Coroutine-based network library</td><td>1054</td></tr><tr><td>qiskit-aer</td><td>Qiskit Aer - High performance simulators for Qiskit</td><td>589</td></tr><tr><td>qiskit-aer-gpu</td><td>Qiskit Aer - High performance simulators for Qiskit</td><td>448</td></tr><tr><td>ctranslate2</td><td>Optimized inference engine for OpenNMT models</td><td>335</td></tr><tr><td>snapml</td><td>Snap Machine Learning</td><td>258</td></tr><tr><td>neural-compressor</td><td>Repository of Intel® Neural Compressor</td><td>234</td></tr><tr><td>RelStorage</td><td>A backend for ZODB that stores pickles in a relational database.</td><td>222</td></tr><tr><td>ikomia</td><td>Ikomia Python API for Computer Vision workflow and plugin integration in Ikomia Studio</td><td>165</td></tr></tbody></table></center></div><p>Unsurprisingly, a lot of these are various kinds of scientific software. I have never met a scientist who can resist the lure of fast-but-dangerous math when doing numerical simulations. But others, I think, are much more likely to simply be mistakes:</p><ul><li><b><a href="https://pypi.org/project/BTrees/">BTrees</a></b> - Scalable persistent object containers. I don&#39;t think a generic data structure library should be changing the floating point behavior. And indeed if we look at their GitHub repo, there&#39;s an <a href="https://github.com/zopefoundation/BTrees/pull/179">open pull request to disable the use of -Ofast</a>.</li><li><b><a href="https://pypi.org/project/gevent/">gevent</a></b> - Coroutine-based network library. I covered this one in the intro; it definitely way out of line for a networking library to be messing with the FPU; we found <a href="https://github.com/gevent/gevent/pull/1864">the pull request that fixes it</a> (also still un-merged, sadly) earlier.</li><li><b><a href="https://pypi.org/project/RelStorage/">RelStorage</a></b> - A backend for ZODB that stores pickles in a relational database. I can&#39;t imagine how storing pickles in a database would need floating point math, so this seems like a mistake. This time I don&#39;t see any issues or PRs asking about <span>-Ofast</span>, but we can see in <a href="https://github.com/zodb/relstorage/blob/de786b3e0d434748e1f56e9b02089662c84ac76b/scripts/releases/make-manylinux#L33-L34">the script used to generate the manylinux builds</a> that it is indeed enabled.</li><li><b><a href="https://pypi.org/project/perfmetrics/">perfmetrics</a></b> - Send performance metrics about Python code to Statsd. This has nothing to do with floating point math. Once again no PR or issue, but I notice that this, RelStorage, and BTrees are all Zope Foundation projects. The BTrees issue mentioned the Zope Foundation meta project as the source of the build configurations, and look what we find there (in <a href="https://github.com/zopefoundation/meta/blob/master/config/c-code/tests.yml.j2#L3-L6">config/c-code/tests.yml.j2</a>):</li></ul><blockquote><p><span># Initially copied from</span></p><p><span># https://github.com/actions/starter-workflows/blob/main/ci/python-package.yml</span></p><p><span># And later based on the version jamadden updated at</span></p><p><span># gevent/gevent, and then at zodb/relstorage and zodb/perfmetrics</span></p></blockquote><p>So in fact all the packages we&#39;ve seen so far can trace their use of <span>-Ofast</span> to <b>gevent</b>!</p><h3>How Bad is This? It <i>Depends</i></h3><p>Well who cares about all this, you might ask; I can just avoid using using those libraries until the problem is fixed, right? Well, maybe. But I had no idea I was using <b>gevent</b> until numpy started yelling at me, and I still don&#39;t know the exact chain of dependencies that caused it to get installed in the first place. So can we get a count of how many packages might (directly or indirectly) depend on a package that has an <span>-ffast-math</span> library?</p><p>This is not as easy as it seems. Python packaging metadata is in a pretty sorry state, and you can&#39;t easily get a simple list of the dependencies of each package in machine-readable form. It turns out that the recommended way to get the dependencies of a particular package is to... just install it with pip and see what happens. And since we&#39;re looking for <i>reverse</i> dependencies, it&#39;s even worse -- we would have to install every package on PyPI and see if any of them pulled in one of the libraries we discovered!</p><p>I actually started down this path and set about running <span>pip install --dry-run --ignore-installed --report</span> on all 397,267 packages. This turned out to be a <i><b>terrible</b></i> idea. Unbeknownst to me, <span><b>even with --dry-run pip will execute arbitrary code found in the package&#39;s setup.py</b></span>. In fact, <b><span>merely asking pip to <i>download</i> a package can execute arbitrary code</span></b> (see pip issues <a href="https://github.com/pypa/pip/issues/7325">7325</a> and <a href="https://github.com/pypa/pip/issues/1884">1884</a> for more details)! So when I tried to dry-run install almost 400K Python packages, <a href="https://twitter.com/moyix/status/1566561433898426368">hilarity ensued</a>. I spent a long time <a href="https://twitter.com/moyix/status/1566578412663209984">cleaning up the mess</a>, and discovered some <a href="https://twitter.com/moyix/status/1566609622680608770">pretty poor setup.py practices</a> along the way. But hey, at least I got <a href="https://twitter.com/moyix/status/1566612152558944257">two free pictures of anime catgirls</a>, deposited directly into my home directory. Convenient!</p><p>Once I had managed to clean up the mess (or hopefully, anyway<span face="Roboto, arial, sans-serif"><span>—</span></span>I never did find out what package tried to execute sudo), I decided I needed a different approach. <a href="https://twitter.com/sirdarckcat">Eduardo Vela</a> helpfully pointed me to <a href="https://deps.dev/">Google&#39;s Open Source Insights project, deps.dev</a>, which catalogues the dependencies and dependents (reverse dependencies) for PyPI, npm, Go, Maven, and Cargo. They don&#39;t have an official API, but a bit of poking around in Chrome Devtools&#39; Network tab turned up a simple JSON endpoint that I could query. And I figured since I was only querying 422 package+version combinations, I could probably get away with it.</p><p>That, finally, let me produce this table, <a href="https://moyix.net/~moyix/ffast_reverse_dep_counts.html">which you can see in its full form here</a> (I&#39;ve omitted all packages with less than 4 dependents below since this post is already pretty long):</p><div><center><table><tbody></tbody><thead><tr><th>Package</th><th>Version</th><th>Count</th></tr></thead><tbody><tr><td>gevent</td><td>21.12.0</td><td>1592</td></tr><tr><td>BTrees</td><td>4.10.0</td><td>618</td></tr><tr><td>gevent</td><td>20.9.0</td><td>51</td></tr><tr><td>gevent</td><td>21.1.2</td><td>36</td></tr><tr><td>gevent</td><td>20.6.2</td><td>24</td></tr><tr><td>gevent</td><td>21.8.0</td><td>13</td></tr><tr><td>perfmetrics</td><td>3.2.0</td><td>12</td></tr><tr><td>ctranslate2</td><td>2.17.0</td><td>9</td></tr><tr><td>gevent</td><td>20.12.1</td><td>9</td></tr><tr><td>qiskit-aer</td><td>0.8.2</td><td>9</td></tr><tr><td>qiskit-aer</td><td>0.10.3</td><td>9</td></tr><tr><td>dyNET</td><td>2.1.2</td><td>8</td></tr><tr><td>qiskit-aer</td><td>0.9.1</td><td>7</td></tr><tr><td>dyNET38</td><td>2.1</td><td>6</td></tr><tr><td>gevent</td><td>20.6.1</td><td>6</td></tr><tr><td>qiskit-aer</td><td>0.7.5</td><td>6</td></tr><tr><td>qiskit-aer</td><td>0.1.1</td><td>5</td></tr><tr><td>gevent</td><td>20.6.0</td><td>4</td></tr><tr><td>glove-python-binary</td><td>0.2.0</td><td>4</td></tr><tr><td>pyqubo</td><td>1.0.13</td><td>4</td></tr><tr><td>qiskit-aer</td><td>0.6.1</td><td>4</td></tr><tr><td>qiskit-aer</td><td>0.7.3</td><td>4</td></tr><tr><td>qiskit-aer</td><td>0.9.0</td><td>4</td></tr><tr></tr></tbody></table></center><br/></div><p>A total of <b>2,514</b> packages eventually depend on a package that uses <span>-ffast-math</span>. (This number is still approximate; deps.dev doesn&#39;t tell you the actual names and versions of all of the reverse dependencies (just a random sample), so the counts here may include some duplicates. It may also <i>underestimate</i> the extent of the problem, since there are some fairly popular packages like PyTorch that aren&#39;t on PyPI).</p><p>Also, even with only the random sample of dependents provided by <a href="http://deps.dev">deps.dev</a>, we can use the download count CSV we generated at the beginning of this post to get a sense of how popular the affected projects are. Assuming we have a list of the names of the reverse dependencies in a file named &#34;rdeps.txt&#34;, we can just do:</p><div><pre>(sfcodegen) <span><b>moyix@isabella</b></span>:<span><b>/fastdata/pypi_wheels</b></span>$ ipython
Python 3.8.10 (default, Jun 22 2022, 20:18:18) 
Type &#39;copyright&#39;, &#39;credits&#39; or &#39;license&#39; for more information
IPython 8.4.0 -- An enhanced Interactive Python. Type &#39;?&#39; for help.

<span>In [</span><span><b>1</b></span><span>]: </span><span><b>import</b></span> <span><b>pandas</b></span> <span><b>as</b></span> <span><b>pd</b></span>

<span>In [</span><span><b>2</b></span><span>]: </span>df = pd.read_csv(<span>&#39;pypi_downloads_20220901_30d.csv&#39;</span>)

<span>In [</span><span><b>3</b></span><span>]: </span>rdeps = <span>set</span>(<span>open</span>(<span>&#39;rdeps.txt&#39;</span>).read().splitlines())

<span>In [</span><span><b>4</b></span><span>]: </span>sorted_df = df.sort_values(by=<span>&#39;num_downloads&#39;</span>, ascending=<span><b>False</b></span>, ignore_index=<span><b>True</b></span>)

<span>In [</span><span><b>5</b></span><span>]: </span>sorted_df[<span>&#39;rank&#39;</span>] = sorted_df.index+<span>1</span>

<span>In [</span><span><b>6</b></span><span>]: </span>rdep_ranks = sorted_df[sorted_df[<span>&#39;project&#39;</span>].isin(rdeps)]

<span>In [</span><span><b>7</b></span><span>]: </span>top20 = rdep_ranks.nsmallest(<span>20</span>, <span>&#39;rank&#39;</span>)

<span>In [</span><span><b>8</b></span><span>]: </span><span><b>from</b></span> <span><b>tabulate</b></span> <span><b>import</b></span> tabulate

<span>In [</span><span><b>9</b></span><span>]: </span><span>print</span>(tabulate(top20, headers=<span>&#39;keys&#39;</span>, tablefmt=<span>&#39;psql&#39;</span>, showindex=<span><b>False</b></span>))
+--------------------------+-----------------+--------+
| project                  |   num_downloads |   rank |
|--------------------------+-----------------+--------|
| geventhttpclient         |         1116639 |   1296 |
| locust                   |         1045789 |   1345 |
| flask-socketio           |          914846 |   1424 |
| dagster                  |          497982 |   1974 |
| grequests                |          365292 |   2328 |
| dedupe                   |          358737 |   2352 |
| websocket                |          346515 |   2397 |
| gevent-websocket         |          322249 |   2466 |
| dagster-graphql          |          310382 |   2525 |
| locust-plugins           |          273452 |   2669 |
| interpret-community      |          268129 |   2692 |
| zope-index               |          240577 |   2836 |
| dedupe-variable-datetime |          223080 |   2932 |
| parallel-ssh             |          220882 |   2947 |
| azureml-interpret        |          211233 |   3009 |
| locustio                 |          102322 |   4203 |
| allennlp                 |           92268 |   4498 |
| interpret                |           91404 |   4521 |
| rasa-sdk                 |           87875 |   4609 |
| pykafka                  |           78679 |   4837 |
+--------------------------+-----------------+--------+</pre></div><p>So there are some very popular packages that will pull in one of the 49 packages that was built with <span>-ffast-math</span>! With a little more work we can include the name of the fast-math-enabled package as well:</p><div><pre>+--------------------------+----------+-----------------+--------+
| project                  | source   |   num_downloads |   rank |
|--------------------------+----------+-----------------+--------|
| geventhttpclient         | gevent   |         1116639 |   1296 |
| locust                   | gevent   |         1045789 |   1345 |
| flask-socketio           | gevent   |          914846 |   1424 |
| dagster                  | gevent   |          497982 |   1974 |
| grequests                | gevent   |          365292 |   2328 |
| dedupe                   | btrees   |          358737 |   2352 |
| websocket                | gevent   |          346515 |   2397 |
| gevent-websocket         | gevent   |          322249 |   2466 |
| dagster-graphql          | gevent   |          310382 |   2525 |
| locust-plugins           | gevent   |          273452 |   2669 |
| interpret-community      | gevent   |          268129 |   2692 |
| zope-index               | btrees   |          240577 |   2836 |
| dedupe-variable-datetime | btrees   |          223080 |   2932 |
| parallel-ssh             | gevent   |          220882 |   2947 |
| azureml-interpret        | gevent   |          211233 |   3009 |
| locustio                 | gevent   |          102322 |   4203 |
| allennlp                 | gevent   |           92268 |   4498 |
| interpret                | gevent   |           91404 |   4521 |
| rasa-sdk                 | gevent   |           87875 |   4609 |
| pykafka                  | gevent   |           78679 |   4837 |
+--------------------------+----------+-----------------+--------+</pre></div><h3>Conclusion</h3><h4>So after all this work, what did we learn?</h4><ul><li>Turning on <span>-Ofast</span> will end up turning on <span>-ffast-math</span>, and that can cause all sorts of problems for any program unlucky enough to load them.</li><li>Even if you explicitly ask for no fast math, you will still get fast math as long as <span>-Ofast</span> is enabled.</li><li>It is surprisingly feasible (though perhaps not wise) for a single individual with a good internet connection to download 4 TB of Python packages and scan 11 TB of shared libraries in a single day.</li><li>It is <i>definitely</i> not wise to try to run pip download or pip install --dry-run on every package listed in PyPI, at least not without some good sandboxing, because it will execute tons of random code from setup.py files and leave you with a giant mess to clean up.</li><li>Because of <a href="https://en.wikipedia.org/wiki/Software_supply_chain">highly connected nature of the modern software supply chain</a>, even though a mere 49 packages were actually built with <span>-ffast-math</span>, thousands of other packages, with a total of at least <b>9.7 <i>million</i></b> downloads over the past 30 days, are affected.</li></ul><h4>What can we actually do about it?</h4><div><p>Well, for now you can just try to be careful about what libraries you use, perhaps with the help of the tables I generated in this post. If you have a numerical function in Python that you really don&#39;t want to be affected by this, I wrote a <a href="https://gist.github.com/moyix/5bac4b2e383a466b7d015b8c04db13b5">somewhat alarming script, ensure_fp.py</a>, that provides a decorator named <span>@ensure_clean_fpu_state</span>. The decorator resets the value of MXCSR to its power-on state for the duration of the function. (I say &#34;alarming&#34; because it runs assembly code from Python by mapping an executable memory region and then copying in the raw code bytes; hopefully it goes without saying that it needs some work before it&#39;s production-ready.)</p></div><div><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/a/AVvXsEid854WZEzp-Rtw3KuEZVBoPtT7eSci150VHH0wWvQ7QureR9pKovwQQPq1_e89uIeE1CA9g4SmxDvE6YQnWzEzVQMRuJe-bXI1frjcaxzNKpWKPfSYq3NKfLNkITBfVttrLil4FY4Um5wacFHUFVIXKj9u8MXwVZK9nrGO6p7Xu7M2lorX_v3swPOyPQ"><img alt="" data-original-height="613" data-original-width="1549" height="254" src="https://blogger.googleusercontent.com/img/a/AVvXsEid854WZEzp-Rtw3KuEZVBoPtT7eSci150VHH0wWvQ7QureR9pKovwQQPq1_e89uIeE1CA9g4SmxDvE6YQnWzEzVQMRuJe-bXI1frjcaxzNKpWKPfSYq3NKfLNkITBfVttrLil4FY4Um5wacFHUFVIXKj9u8MXwVZK9nrGO6p7Xu7M2lorX_v3swPOyPQ=w640-h254" width="640"/></a></td></tr><tr><td>Some absolutely 31337 ASCII-art I put together for <span>ensure_fp.py</span>. Thanks to <a href="https://twitter.com/bjg/status/1566518384245854208">Ben Gras</a>, <a href="https://twitter.com/peter_a_goodman/status/1566506623895486464">Peter Goodman</a>, and <a href="https://twitter.com/DRMacIver/status/1566506166183776256">David R. MacIver</a> on Twitter for helping me rephrase the caption so that it was fully justified to the width of that box in the top-left.</td></tr></tbody></table></div><p>Longer-term, gcc and clang should provide more sane defaults. Ideally, <span>-ffast-math</span> should simply not enable FTZ/DAZ; that functionality (if anyone wants it) could be split out into an option and <i>definitely</i> not enabled by <span>-ffast-math</span>. A less radical compromise would be to at least avoid linking in <span>crtfastmath</span> when building a shared library. I&#39;m not all that optimistic about this, though, given that <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=55522">the relevant gcc bug report will turn 10 years old at the end of this November and is still marked NEW</a>. Still, maybe if more people complain about it (ideally with a pull request or patch attached) it will get fixed.</p><p>Oh, and as for <b>gevent</b>? I decided to just replace the code that messes with MXCSR with no-ops until they get around to making a release that doesn&#39;t mess with my FPU.</p>
</div></div>
  </body>
</html>
