<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://commoncog.com/blog/the-restaurant-business-seen-through-the-lens-of-incentives/">Original</a>
    <h1>Lousy Incentives In The Restaurant Business</h1>
    
    <div id="readability-page-1" class="page"><div role="main">
                <div>
                    




<article>
  <p>I spent the last hour looking and failing to find the paper that made this
specific claim:</p>

<p>Language models can be trained mostly on synthetic, hierarchical data and it
works fine. A little bit of natural language data used for fine tuning allows
it to translate language it hasn’t seen before just fine.</p>

<p><a href="https://arxiv.org/pdf/2004.13645.pdf">Closest I could find is a paper about synthetic language data</a></p>

<blockquote>
  <p>Using deep mind optimal compute estimates, you need ~50,000x more flops and
~1300x more tokens than PaLM in order to train a 10T param alien mind.
Assuming similar util rates to PaLM, and given you discover like ~50x price
improvements, I expect it would cost abt $10B to train rn</p>
</blockquote>

<p>– <a href="https://twitter.com/bayeslord/status/1513574073984434177">bayeslord</a></p>

<p>I think <a href="https://github.com/digama0/mm0">MetaMath 0</a> could be used to generate
a lot of those tokens. Any theorem prover could work, but mm0 is really fast,
which is of special value here.</p>

<p>By the argument about language models learning hierarchy, this would provide
training data for them, since formal language is a subset of natural language<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<p>In one sense, theorem provers are “realer than real”: whatever hierarchy they
learn is true, basically because it’s math. Bob Osserman comes to mind.</p>

<blockquote>
  <p>There has been a tendency in recent years to take the notion of proof down
from its pedestal. Critics point out that standards of rigor change from
century to century. New gray areas appear all the time. Is a proof by computer
an acceptable proof? Is a proof that is spread over many journals and
thousands of pages, that is too long for any one person to master, a proof?
And of course, venerable Euclid is full of flaws, some filled in by Hilbert,
others possibly still lurking.</p>

  <p>Clearly it is worth examining closely and critically the most basic notion of
mathematics, that of proof. On the other hand, it is important to bear in mind
that all distinctions and niceties about what precisely constitutes a proof
are mere quibbles compared to the enormous gap between any generally accepted
version of a proof and the notion of a convincing argument. Compare Euclid,
with all his flaws to the most eminent of the ancient exponents of the
convincing argument – Aristotle. Much of Aristotle’s reasoning was
brilliant, and he certainly convinced most thoughtful people for over a
thousand years. In some cases his analyses were exactly right, but in others,
such as heavy objects falling faster than light ones, they turned out to be
totally wrong. In contrast, there is not to my knowledge a single theorem
stated in Euclid’s Elements that in the course of two thousand years turned
out to be false.</p>
</blockquote>

<p>– Bob Osserman</p>

<p>Exploring the space of mathematical language is hard but offers the potential
for infinite data. There’s lots of bootstrapping issues, but at some point
soonish we’re gonna run out of novel human text to feed the model.</p>

<p>Simulators also offer this, though from what I’ve heard they tend to hit a gap
where the model doesn’t get much better, so we go out into the real world
looking for more data to capture all the weird high frequency information.</p>

<p>In an even more speculative vein, I often wonder about embedding language inside
a simulator. We do it already. Our speech exists on 2 levels: as some abstract
language and as raw audio. I can imagine a very fancy version of <a href="https://arxiv.org/abs/2204.00598">Socratic
Models</a> that communicate even more like humans
do.</p>

<p>At some point for increased capabilities, I think models are going to have to
make up and solve their own problems, much like bright children do.</p>



</article>

  <!-- mathjax -->
  <!-- XXX Need to add  mathjax: true to each post to get it to work-->
  

  




  <p>If you have anything you want to say to me (compliments, criticism, requests for future posts, etc.), please fill out my <a href="https://docs.google.com/forms/d/e/1FAIpQLScY0Iyy94BJqaROoFv1S-2wYf3TraWXi5UFpOVT9k41VPD24g/viewform">feedback form</a></p>






  
  
  





  <h3>Related Posts</h3>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  


                </div>
            </div></div>
  </body>
</html>
