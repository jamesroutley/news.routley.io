<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.appliedexploration.com/p/dark-side-of-graph-neural-nets">Original</a>
    <h1>The Dark Side of Graph Neural Networks</h1>
    
    <div id="readability-page-1" class="page"><div><div><article><div class=""><div><div dir="auto"><p><span>In part 1, </span><a href="https://www.appliedexploration.com/p/graph-neural-networks-future-of-ai?s=w" rel="">How I Learned to Stop Worrying and Love Graphs</a><span>, we talked about how graphs are the most universal representation of almost any data and benefit from several advantageous properties.</span></p><div><figure><a target="_blank" rel="nofollow" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png" width="1456" height="819" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:819,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:1134609,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:&#34;image/png&#34;,&#34;href&#34;:null,&#34;belowTheFold&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F140c0ba0-0586-4bf6-988f-2a993b6df3d7_1920x1080.png 1456w" sizes="100vw"/></picture></a></figure></div><p>Does that mean we can disregard Computer Vision (CV) and Natural Language Processing (NLP), convert all images or text to graphs and have one “Master Algorithm” learn on all kinds of data?</p><p><span>Not so fast. CV/NLP is a collection of domain-specific techniques that make it easier for ML algorithms to learn on highly structured, use-case-specific data. Let’s see where the dominant Graph Neural Network (GNN) architectures, </span><a href="https://towardsdatascience.com/introduction-to-message-passing-neural-networks-e670dc103a87" rel="">Message Passing Neural Networks</a><span> currently fall short.</span></p><h2><strong>Graphs are powerful, but GNNs don’t always excel</strong></h2><p><strong>GNNs are hard to parallelise</strong></p><p><span>There are sequential steps during message passing</span><a id="footnote-anchor-1" href="#footnote-1" rel="">1</a><span>, so they’ll never be as fast as Convolutional Neural Networks or Transformers, which removed the need for recurrence and therefore can batch process lots of data up concurrently. This can not be overstated - Deep Learning is enjoying its heyday because it has won the </span><a href="https://arxiv.org/abs/2009.06489" rel="">Hardware Lottery</a><span>, and old architectures that can’t be made embarrassingly parallel easily are</span><a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" rel=""> sinking into oblivion faster than I can finish this sentence</a><span>.</span></p><div><figure><a target="_blank" rel="nofollow" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png" width="1456" height="549" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:549,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F75c32fae-e57c-4f8e-984d-c2baf6d5f144_1600x603.png 1456w" sizes="100vw"/></picture></a><figcaption>Computational Graph of a selected node during Massage Passing. Sequential steps make it harder to parallelize</figcaption></figure></div><p><strong>GNNs introduce computational complexity</strong></p><p><span>The compute needed to do message passing blows up quickly with the size of the graph: we end up with </span><code>kⁿ</code><span> neighbours to aggregate for each node (where k is the average degree and n is the number of neighbours to hop on). In practice, this is mitigated by sampling a fixed number of neighbours on each level - but that both limits the information that can be aggregated and brings stochasticity into the mix, none of you may actually want!</span></p><p>Most of this computation is often not useful: summing or taking an average in the aggregation step, as well as the required pooling step, mean a lot of the specific information gets smoothed out.</p><div><figure><a target="_blank" rel="nofollow" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png" width="1456" height="631" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:631,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1deac4e5-2e5e-4b60-8a3b-33e4b533798f_1600x693.png 1456w" sizes="100vw" loading="lazy"/></picture></a><figcaption>The size of the aggregated neighbourhood scales by kⁿ with each neighbourhood hop k and average degree n.</figcaption></figure></div><p><strong>All while pretraining doesn’t work yet</strong></p><p>It looks like the otherwise widespread practice of pre-training has yet to be successfully applied to GNNs (we can only guess why this is the case).</p><p><span>Nevertheless, until we see large-scale pre-training emerge in the respective field of application, GNNs will start their learning journey with random weights - a massive disadvantage compared to, let’s say, word embeddings in NLP, models of unthinkable amounts of data. There’s even an </span><a href="https://arxiv.org/abs/2105.03322" rel="">argument that we confused the success of the Transformers architecture with the successful practice of pre-training.</a></p><div data-attrs="{&#34;url&#34;:&#34;https://www.appliedexploration.com/p/dark-side-of-graph-neural-nets?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;}"><p>Know someone who is interested in Machine Learning or Graphs? Share it here:</p><p data-attrs="{&#34;url&#34;:&#34;https://www.appliedexploration.com/p/dark-side-of-graph-neural-nets?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&#34;,&#34;text&#34;:&#34;Share&#34;}"><a href="https://www.appliedexploration.com/p/dark-side-of-graph-neural-nets?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><p><strong>Discrete features are blurred together</strong></p><p>During message passing the target node’s representation is derived from the aggregated features of the neighbourhood, thereby “blurring the boundary” between discrete nodes into regional representations. This may not matter when you want to treat similar customers the same way but could be an issue with fundamentally discrete entities, like types of atoms. That could rather result in a hilarious or fatal mistake.</p><div><figure><a target="_blank" rel="nofollow" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png 1456w" sizes="100vw"/><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png" width="1456" height="440" data-attrs="{&#34;src&#34;:&#34;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png&#34;,&#34;fullscreen&#34;:null,&#34;imageSize&#34;:null,&#34;height&#34;:440,&#34;width&#34;:1456,&#34;resizeWidth&#34;:null,&#34;bytes&#34;:null,&#34;alt&#34;:null,&#34;title&#34;:null,&#34;type&#34;:null,&#34;href&#34;:null,&#34;belowTheFold&#34;:true}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F85b7ea58-95d7-46e1-b2c4-2970b1f643c0_1600x483.png 1456w" sizes="100vw" loading="lazy"/></picture></a><figcaption>Features for each node are calculated from the aggregated features of connected nodes during message passing.</figcaption></figure></div><p><span>This is demonstrated in </span><a href="https://arxiv.org/abs/1810.00826" rel="">“How Powerful are Graph Neural Networks?”</a><span> - there are certain, simple structures that a GNN can’t distinguish from each other.</span></p><p><strong>Limits on the model size</strong></p><p><span>The smoothing issue also creates a hard limit on how many GNN layers you can have at your disposal. The more n-hop neighbours you’re the model is aggregating information from, the more smooth each node’s final embedding will be - if you don’t pay attention, and aggregate all the information from a huge graph, all of your node embeddings will look very, very similar. This is called </span><a href="https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472" rel="">oversmoothing</a><span>, and in practice limits you to a practical maximum of 3-4 GNN layers.</span></p><p><strong>All the above often result in poor performance</strong></p><p><span>In many use cases, GNNs don’t necessarily outperform</span><a href="https://arxiv.org/abs/2203.15789" rel=""> </a><span>simpler methods.</span></p><p><span>In other cases, the simplest form of a Graph Convolutional Network outperforms more complex ones, such as </span><a href="https://arxiv.org/abs/2011.07929" rel="">learning molecule features</a><span> with what the authors call single-layer GCN – Quantum Deep Fields. Does the additional complexity introduced by the graph format superfluous?</span></p><p>We may need a new architecture to resolve these problems.</p><h2><strong>What could come next?</strong></h2><p>So despite all of this convincing, are we here to just declare defeat? </p><p><span>Although there are some shortcomings with the current approaches, Graph Neural Networks are used in production by large companies, especially where the core of their data is a graph, like social networks. As a standout example, Jure Leskovec lead Pinterest in developing its </span><a href="https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48#:~:text=We%20developed%20PinSage%2C%20a%20random,shopping%20with%20Pinterest%20at%20scale." rel="">PinSAGE</a><span> architecture. </span></p><p>Some current unresolved are:</p><ul><li><p>Could we find enough commonalities between graphs to create foundational models, which could be used for transfer learning?</p></li><li><p>Is there a way to shortcut message passing, thereby making it easily parallelizable?</p></li><li><p>Is it possible to preserve local features of nodes while keeping the invariant nature of message passing?</p></li><li><p>Can universal data structures, like graphs ever replace their specialized (and optimized) counterparts? Or does their universality mean they’ll never outperform a custom-tailored approach?</p></li></ul><p>In a separate post, we discuss the engineering realities of ML on Graphs, and our experience of blood and sweat with a Kaggle competition.</p></div></div></div></article></div></div></div>
  </body>
</html>
