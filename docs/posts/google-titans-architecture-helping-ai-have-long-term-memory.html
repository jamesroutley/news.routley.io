<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/">Original</a>
    <h1>Google Titans architecture, helping AI have long-term memory</h1>
    
    <div id="readability-page-1" class="page"><div data-gt-publish-date="20251204">
                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="il1w2">The <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning)" target="_blank" rel="noopener noreferrer">Transformer architecture</a> revolutionized <a href="https://medium.com/machine-learning-basics/sequence-modelling-b2cdf244c233" target="_blank" rel="noopener noreferrer">sequence modeling</a> with its introduction of <a href="https://en.wikipedia.org/wiki/Attention_%28machine_learning%29" target="_blank" rel="noopener noreferrer">attention</a>, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.</p><p data-block-key="36kb5">The research community explored various approaches for solutions, such as efficient linear <a href="https://www.d2l.ai/chapter_recurrent-modern/index.html" target="_blank" rel="noopener noreferrer">recurrent neural networks</a> (RNNs) and <a href="https://huggingface.co/blog/lbourdois/get-on-the-ssm-train" target="_blank" rel="noopener noreferrer">state space models</a> (SSMs) like <a href="https://arxiv.org/pdf/2405.21060" target="_blank" rel="noopener noreferrer">Mamba-2</a>. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.</p><p data-block-key="40m00">In two new papers, <a href="https://arxiv.org/abs/2501.00663" target="_blank" rel="noopener noreferrer"><i>Titans</i></a> and <a href="https://arxiv.org/pdf/2504.13173" target="_blank" rel="noopener noreferrer"><i>MIRAS</i></a>, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.</p><p data-block-key="eic3n">The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="Titans: Learning new context on the fly">
    


    <p>
        
            
                <h2>Titans: Learning new context on the fly</h2>
            
        
        
    </p>



    <p data-block-key="il1w2">An effective learning system requires distinct yet interconnected memory modules, mirroring the <a href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/">human brain&#39;s separation of short-term and long-term memory</a>.</p><p data-block-key="dpe7v">While attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural <a href="https://arxiv.org/abs/2306.07174#:~:text=LongMem%20can:%20*%20Memorize%20long%20past%20context,Yan%20*%20Jianfeng%20Gao%20*%20Furu%20Wei" target="_blank" rel="noopener noreferrer">long-term memory module</a>, that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" target="_blank" rel="noopener noreferrer">multi-layer perceptron</a>). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn&#39;t simply taking notes; it&#39;s understanding and synthesizing the entire story.</p><p data-block-key="e95op">Crucially, Titans doesn’t just passively store data. It actively learns <i>how</i> to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="il1w2">In the context of Titans, the &#34;surprise metric&#34; is the model detecting a large difference between what it currently remembers and what the new input is telling it.</p><ul><li data-block-key="a9lns"><i>Low surprise</i>: If the new word is &#34;cat&#34; and the model&#39;s memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word &#34;cat&#34; in its permanent long-term state.</li><li data-block-key="2t2sa"><i>High surprise</i>: If the model&#39;s memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.</li></ul><p data-block-key="djj22">The model uses this internal error signal (the gradient) as a mathematical equivalent of saying, &#34;This is unexpected and important!&#34; This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.</p><p data-block-key="dm2am">Titans refines this mechanism by incorporating two critical elements:</p><ol><li data-block-key="bb101"><i>Momentum</i>: The model considers both &#34;momentary surprise&#34; (the current input) and &#34;past surprise&#34; (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.</li><li data-block-key="b269a"><i>Forgetting (weight decay)</i>: To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.</li></ol>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="MIRAS: A unified view of sequence modeling">
    


    <p>
        
            
                <h2>MIRAS: A unified view of sequence modeling</h2>
            
        
        
    </p>



    <p data-block-key="il1w2">Every major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex <a href="https://www.geeksforgeeks.org/computer-organization-architecture/associative-memory/" target="_blank" rel="noopener noreferrer">associative memory</a> module.</p><p data-block-key="d91su">Accordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten<b>.</b></p><p data-block-key="7u78e">MIRAS defines a sequence model through four key design choices:</p><ul><li data-block-key="abcem"><i>Memory architecture</i>: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).</li><li data-block-key="5s0u1"><i>Attentional bias</i>: The internal learning objective the model optimizes that determines what it prioritizes.</li><li data-block-key="4qd03"><i>Retention gate</i>: The memory regularizer. MIRAS reinterprets &#34;forgetting mechanisms&#34; as specific forms of <a href="https://dev.to/nareshnishad/day-27-regularization-techniques-for-large-language-models-llms-4af3" target="_blank" rel="noopener noreferrer">regularization</a> that balance new learning against retaining past knowledge.</li><li data-block-key="9savd"><i>Memory algorithm</i>: The optimization algorithm used to update the memory.</li></ul>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="Transcending the mean squared error paradigm">
    


    <p>
        
            
                <h3>Transcending the mean squared error paradigm</h3>
            
        
        
    </p>



    <p data-block-key="il1w2">Virtually all successful existing sequence models rely on <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="noopener noreferrer">mean squared error</a> (MSE) or <a href="https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de" target="_blank" rel="noopener noreferrer">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</p><p data-block-key="1cust">MIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with <a href="https://en.wikipedia.org/wiki/Non-Euclidean_geometry" target="_blank" rel="noopener noreferrer">non-Euclidean objectives</a> and regularization.</p><p data-block-key="40qp3">Using MIRAS, we created three specific attention-free models:</p><ul><li data-block-key="fpeib"><i>YAAD</i>: We designed this MIRAS variant to be less sensitive to major errors or &#34;outliers&#34; (like a single typo in a large document). It uses a gentler math penalty (<a href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank" rel="noopener noreferrer">Huber loss</a>) for mistakes, so it doesn&#39;t overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.</li><li data-block-key="28vrl"><i>MONETA</i>: This model explores the use of more complex and strict mathematical penalties (called <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)" target="_blank" rel="noopener noreferrer">generalized norms</a>). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.</li><li data-block-key="d4e49"><i>MEMORA</i>: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="noopener noreferrer">mean squared error</a> (MSE) or <a href="https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de" target="_blank" rel="noopener noreferrer">dot-product similarity</a> for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.</li></ul>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="Experiments and results">
    


    <p>
        
            
                <h2>Experiments and results</h2>
            
        
        
    </p>



    <p data-block-key="il1w2">We rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including <a href="https://arxiv.org/abs/2003.04974" target="_blank" rel="noopener noreferrer">Transformer++</a>, <a href="https://arxiv.org/pdf/2405.21060" target="_blank" rel="noopener noreferrer">Mamba-2</a>, and <a href="https://arxiv.org/pdf/2412.06464" target="_blank" rel="noopener noreferrer">Gated DeltaNet</a>. We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.</p><p data-block-key="a9v6c">Across both standard language modeling datasets (<a href="https://c4model.com/" target="_blank" rel="noopener noreferrer">C4</a>, <a href="https://huggingface.co/datasets/Salesforce/wikitext" target="_blank" rel="noopener noreferrer">WikiTex</a>t) and <a href="https://medium.com/@hetzer2807/zero-shot-reasoning-unleashed-the-magic-of-large-language-models-4e877dfe470e" target="_blank" rel="noopener noreferrer">zero-shot reasoning tasks</a> (<a href="https://arxiv.org/abs/1905.07830" target="_blank" rel="noopener noreferrer">HellaSwag</a>, PIQA), our models consistently demonstrated higher accuracy and <a href="https://en.wikipedia.org/wiki/Perplexity" target="_blank" rel="noopener noreferrer">perplexity</a> (a measure of how surprised an LLM is when looking at a piece of text).</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="The power of deep memory">
    


    <p>
        
            
                <h3>The power of deep memory</h3>
            
        
        
    </p>



    <p data-block-key="il1w2">Ablation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="Language modeling and efficiency">
    


    <p>
        
            
                <h3>Language modeling and efficiency</h3>
            
        
        
    </p>



    <p data-block-key="il1w2">In language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="Extreme long-context recall">
    


    <p>
        
            
                <h3>Extreme long-context recall</h3>
            
        
        
    </p>



    <p data-block-key="il1w2">The most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the <a href="https://github.com/booydar/babilong" target="_blank" rel="noopener noreferrer">BABILong benchmark</a>, a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.</p>
</div>

    </div>
</section>

                    
                    
    


<section>
    
</section>

                    
                    
    


<section>
    <div>
        
  <div data-gt-id="rich_text" data-gt-component-name="Conclusion">
    


    <p>
        
            
                <h2>Conclusion</h2>
            
        
        
    </p>



    <p data-block-key="il1w2">The introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.</p>
</div>

    </div>
</section>

                    
                </div></div>
  </body>
</html>
