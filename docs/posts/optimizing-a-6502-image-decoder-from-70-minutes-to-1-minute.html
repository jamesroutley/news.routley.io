<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>James Routley | Feed</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../styles.css"
      media="screen"
    />
  </head>
  <body>
    <a href="/index.html">Back</a>
    <a href="https://www.colino.net/wordpress/en/archives/2025/09/28/optimizing-a-6502-image-decoder-from-70-minutes-to-1-minute/">Original</a>
    <h1>Optimizing a 6502 image decoder, from 70 minutes to 1 minute</h1>
    
    <div id="readability-page-1" class="page"><div>
					<p><span><i></i>2025/09/28</span>
						<span> - </span>
						<span><i></i>About 8 minutes read</span>
					</p>
					
				</div><div>
					<div>	
						
<p>When I set out to write a program that would allow me to do basic digital photography on the Apple II, I decided I would do it with the Quicktake cameras. It seemed the obvious choice as they were Apple cameras, and their interface to the computer is via serial port.</p>



<p>The scope creeped a bit after managing to decode Quicktake 100 photos. I wanted it to be able to decode Quicktake 150 and Quicktake 200 pictures too. This threw me into image processing much more than I initially wanted to. This article explains the process of how I got the Quicktake 150 decoder to a reasonabl-ish speed on a 6502 at 1MHz.</p>



<p>The Quicktake 150 format is proprietary and undocumented. Free software decoders exist though, in the dcraw project. This was my source for the initial Apple II decoder. Sadly, it is written in C, is extremely non-documented, and is extremely hard to read and understand (to me). The compression is based on Huffman coding, with variable-length codes (which means bit-shifting), and the image construction involves a lot of 16-bits math. None of this is good on a 6502.</p>



<p>But first I had to rework the original algorithm to work with bands of 20 pixels, for memory reasons. Once I had a functional decoder, it ran perfectly, but it took… seventy minutes to decode a single picture.</p>



<figure><a href="https://esoteric.codes/wordpress/wp-content/uploads/QT150_24.png"><img fetchpriority="high" decoding="async" width="640" height="480" src="https://esoteric.codes/wordpress/wp-content/uploads/QT150_24.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/QT150_24.png 640w, https://www.colino.net/wordpress/wp-content/uploads/QT150_24-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/QT150_24-150x113.png 150w" sizes="(max-width: 640px) 100vw, 640px"/></a><figcaption>A Quicktake 150 picture, fully decoded by dcraw</figcaption></figure>



<figure><a href="https://esoteric.codes/wordpress/wp-content/uploads/TEST150.DGHR_.png"><img decoding="async" width="560" height="384" src="https://esoteric.codes/wordpress/wp-content/uploads/TEST150.DGHR_.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_.png 560w, https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_-400x274.png 400w, https://www.colino.net/wordpress/wp-content/uploads/TEST150.DGHR_-150x103.png 150w" sizes="(max-width: 560px) 100vw, 560px"/></a><figcaption>The same picture, dithered for display on a monochrome Apple II</figcaption></figure>



<p>Of course, I didn’t get there that fast. My first implementation was released two years ago, in November 2023. Getting where I’m now took, I think, five or six deep dives with each time, one or two weeks worth of late evenings and full week-ends dedicated to progressing, wading through hundreds or thousands of debug printf()s, gdb’ing, variables and offsets comparisons, etc.</p>



<p>Follow me through the algorithmic iterations that allowed me to get that decoding time to under one minute. My implementation is now full assembly, but the commits I will link to here are to the general decoding algorithm, that is easier to read in C. </p>



<p>I have noticed that hand-optimizing assembler yields good results, but usually optimizing the algorithm itself leads to much more impressive speed gains. Doing <strong>too many things</strong> faster is not as good as doing <strong>the minimum</strong> faster. And that Quicktake 150 decoder sure did useless things, especially in my case where I don’t care about color and end up with a 256×192 image!</p>



<p>I have made a specific repository to track these algorithmic changes. It started <a href="https://github.com/colinleroy/qtkn_decoder/commit/ceef883b4528a01658f446f9fde233d8846b98e0#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/ceef883b4528a01658f446f9fde233d8846b98e0#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3">here</a> (already a little bit deobfuscated <a href="https://github.com/ncruces/dcraw/blob/8fe4d3825595816ea7d6880d9253f9edd143cacb/dcraw.c#L2198">compared to dcraw</a>), at 301 millions x86_64 instructions.</p>



<p><strong>Dropping color</strong></p>



<p>I didn’t have to decode color at all, as I was going to drop it, anyway. <a href="https://github.com/colinleroy/qtkn_decoder/commit/615ecd7406a65dd43da07443cb5c3f93a8d5fa95#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3R217">I added a flag to only decode the green pixels out of the Bayer matrix, and drop the rest</a>. 264M instructions.</p>



<p><strong>Understanding the buffers</strong></p>



<p>I then set out to understand the use of the various temporary buffers: the more buffers, the more intermediary steps, the more copy and looping. I wanted to drop as much of them as possible. <a href="https://github.com/colinleroy/qtkn_decoder/commit/fd6cd2c44d1cc961f8827ec6b5958521543e6d8d#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L244">The first step towards it was unrolling some little imbricated loops that worked on y [1,2], x [col+1,col].</a> 238M instructions.</p>



<p>I figured I still had extra processing I didn’t need, <a href="https://github.com/colinleroy/qtkn_decoder/commit/73935481468c2a1d20a4640352caed4e47308b54#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3">removed it, dropped a buffer</a> (and dropped the #ifdef COLOR conditional to make things clearer). 193M instructions.</p>



<figure><a href="https://esoteric.codes/wordpress/wp-content/uploads/image-green.png"><img decoding="async" width="640" height="480" src="https://esoteric.codes/wordpress/wp-content/uploads/image-green.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-green.png 640w, https://www.colino.net/wordpress/wp-content/uploads/image-green-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-green-150x113.png 150w" sizes="(max-width: 640px) 100vw, 640px"/></a><figcaption>Our image looks like this now</figcaption></figure>



<p>At that point, my implementation still outputted green pixels only in a Bayer matrix to a 640×480 buffer, and then interpolated them. It was useless, so <a href="https://github.com/colinleroy/qtkn_decoder/commit/75a45d8fdf6943cc26a56c3f53c21f57e2877bbf#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L263" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/75a45d8fdf6943cc26a56c3f53c21f57e2877bbf#diff-3564ad20c962b9eec56839b1adac221e589cda13b85547ac3d409396e9b6a28c">I dropped that entirely</a>. 29M instructions.</p>



<figure><a href="https://esoteric.codes/wordpress/wp-content/uploads/out-grayscale-large.png"><img loading="lazy" decoding="async" width="640" height="480" src="https://esoteric.codes/wordpress/wp-content/uploads/out-grayscale-large.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large.png 640w, https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/out-grayscale-large-150x113.png 150w" sizes="auto, (max-width: 640px) 100vw, 640px"/></a><figcaption>Without interpolating, we can clearly see we only have half the pixels.</figcaption></figure>



<p>I still had half the pixels black in the destination buffer, so I dropped them earlier rather than later, by <a href="https://github.com/colinleroy/qtkn_decoder/commit/fdad1e8efb45e7597659b6c7d08e0a7714094d59#diff-538608780127f20219d177da9582dffbd84063d74807cd8980d62ee09f7689a3L130" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/fdad1e8efb45e7597659b6c7d08e0a7714094d59">outputting a 320×240 images with only the relevant pixels</a>. 25M instructions.</p>



<figure><a href="https://esoteric.codes/wordpress/wp-content/uploads/image-119.png"><img loading="lazy" decoding="async" width="640" height="480" src="https://esoteric.codes/wordpress/wp-content/uploads/image-119.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-119.png 640w, https://www.colino.net/wordpress/wp-content/uploads/image-119-400x300.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-119-150x113.png 150w" sizes="auto, (max-width: 640px) 100vw, 640px"/></a><figcaption>The 8-bits grayscale buffer at that point</figcaption></figure>



<p>At this point I was able to figure out that out of the three buf_m[3], only <a href="https://github.com/colinleroy/qtkn_decoder/commit/b3ac5d70edf0e836aa4b784fc06093d2606f7747">buf_m[1] was used to construct the picture</a>, that <a href="https://github.com/colinleroy/qtkn_decoder/commit/8b6a1c65011645c810f1a9fbb7d9eb56fe3e702e">buf_m[2] was only used to feed back into buf_m[0] at the start of a row</a>, that I could <a href="https://github.com/colinleroy/qtkn_decoder/commit/b44aac77d966c292f0762165cdf5b0818a8a0f91">construct the image from the buf_m[1] values on the fly instead of doing an extra loop on it</a>, and that <a href="https://github.com/colinleroy/qtkn_decoder/commit/4142412bda1388b6fadb5d343de07605c1e93ae8">I could entirely drop it too</a>. This allowed me to <a href="https://github.com/colinleroy/qtkn_decoder/commit/5161e9e33e5c3f064b0a3c0e5d29e6e7ece4280f" data-type="link" data-id="https://github.com/colinleroy/qtkn_decoder/commit/5161e9e33e5c3f064b0a3c0e5d29e6e7ece4280f">rename the last remaining buffer for more clarity</a>. 22M instructions. </p>



<p><strong>Optimizing divisions</strong></p>



<p>That was about it for the buffers. The rework of the code, at that point, made clear that every final pixel value was computed by dividing the 16-bits values computed from the image data by a given factor, and that this factor changes at most once every two rows. The result of that division was then clamped to [0-255]. This allowed me to <a href="https://github.com/colinleroy/qtkn_decoder/commit/865ca287f15bfccbca909b464b5939d8c4368f5a">precompute a division table every two rows, storing the final result, pre-clamped, in a simple array.</a> This also came with a bit of non-visible precision loss. On an x86_64, still 22M instructions, but on 6502, this was a huge gain, transforming 153600 divisions into less than 2000.</p>



<p>I verified the precision loss was acceptable using a small ad-hoc tool displaying my output buffers and comparing the reference decoding to the approximated one. Pixel values differ by at most 1.</p>



<figure><a href="https://esoteric.codes/wordpress/wp-content/uploads/image-122.png"><img loading="lazy" decoding="async" width="1541" height="386" src="https://esoteric.codes/wordpress/wp-content/uploads/image-122.png" alt="" srcset="https://www.colino.net/wordpress/wp-content/uploads/image-122.png 1541w, https://www.colino.net/wordpress/wp-content/uploads/image-122-400x100.png 400w, https://www.colino.net/wordpress/wp-content/uploads/image-122-150x38.png 150w, https://www.colino.net/wordpress/wp-content/uploads/image-122-768x192.png 768w, https://www.colino.net/wordpress/wp-content/uploads/image-122-1536x385.png 1536w, https://www.colino.net/wordpress/wp-content/uploads/image-122-1280x321.png 1280w" sizes="auto, (max-width: 1541px) 100vw, 1541px"/></a><figcaption>Left: the normal output; middle: the comparison; right: the output with approximated divisions</figcaption></figure>



<p><strong>Output index</strong></p>



<p>So far we set the output buffer using the usual <strong>buffer[y*WIDTH+x]</strong> access method, which is really slow on a processor with no multiplication support. <a href="https://github.com/colinleroy/qtkn_decoder/commit/7112b2b96e3d249f4a65532035dff9a8708bc914">I changed that to a much simpler line-by-line indexing</a>. (Even on x86_64, the change is notable: 20M instructions).</p>



<p><strong>Huffman decoding</strong></p>



<p>The algorithm initialized full tables so that it was possible to get a Huffman code by just looking at the bitbuffer: for code <strong>10001</strong>, for example, all codes from <strong>10001000</strong> to <strong>10001111</strong> were matched to the correct value, then the bitbuffer shifted &lt;&lt;5. This seems good at first, but not on 6502, as this requires a 16-bits bitbuffer to make sure we always have a full byte to look at. <a href="https://github.com/colinleroy/qtkn_decoder/commit/b0179e034a4396b8b6d09d489608bf0080e7b8d6">I reworked that to get bits one at a time</a>. This made the x86_64 implementation slower, but the 6502 one 20 seconds faster, spending 9 seconds shifting bits instead of 29. It also allowed me to pack the tables more tight, freeing up some memory for the cache.</p>



<p><strong>Assembly</strong></p>



<p><a href="https://github.com/colinleroy/qtkn_decoder/blob/b0179e034a4396b8b6d09d489608bf0080e7b8d6/qtkn-decoder.c">This algorithm</a> still performs very poorly when compiled by cc65, but is far easier to manually translate into optimized 6502 assembly. There are also a lot of ad-hoc optimisations, for example:</p>



<ul>
<li>The division factor for final pixel values for a pair of rows is 48 more than 50% of the time, on any image I tested. So the 6502 implementation has two divisions lookup tables, one for 48 that is never recomputed, one for another factor, recomputed if needed at the start of a pair of rows.</li>



<li>The row initialization multiplies all 320 next_line values by a factor, which is 255 about 66% of the time. In this case, instead of multiplying a = a*255, the assembly version does (a*256)-a, which is (a&lt;&lt;8)-a, which is much faster.</li>



<li>There is a whole lot of &lt;&lt;4 going on in the main loop, which is lookup-table based in the assembly implementation. &lt;&lt;4 is larger than 8 bits, so there are two tables needed, but it still is worth the memory usage.</li>



<li>Half the Huffman codes read are discarded (they are used for blue and red pixels), so “discarder” functions are used in that case, only shifting the bitbuffer without fetching the value.</li>



<li>Buffers accesses (to next_line and output buffer) are patched in self-modifying code rather than using zero-page pointers, which require to keep track and patch about 54 labels on each page cross. This is ugly as hell, but this requires about 50k cycles per image, but spares 9M cycles overall.</li>
</ul>



<p><strong>The final code</strong></p>



<p>I have pointed to commits to my “test” repository so far, but if you’re interested in the actual 6502 implementation, you can find it in my repository: <a href="https://github.com/colinleroy/a2tools/blob/master/src/quicktake/qtkn_platform.s">the decoder</a>, and <a href="https://github.com/colinleroy/a2tools/blob/master/src/quicktake/qtk_bithuff.s">the bitbuffer</a>.</p>



<p><strong>Questions remain</strong></p>



<p>There still are things I don’t understand in dcraw’s decoder, that my simplifications didn’t uncover. The main thing I wonder is, how did Dave Coffin, dcraw’s author, implement this decoder first? It seems so full of “magic” numbers and arithmetic operations that I have no idea how one would look at pictures at the bit level, and figure out <em>anything</em> about the format. Did he reverse-engineer Apple’s binary? Did he have documentation? Is it in fact a common kind of encoding I have no idea about?</p>



<p>I would love to see documentation of this format, maybe I would understand more and be able to progress further.</p>



<p><strong>Bonus: first and current implementation video</strong></p>



<figure><video controls="" src="/wordpress/wp-content/uploads/qt150-decoding-web.mp4"></video><figcaption>The current implementation</figcaption></figure>



<figure><video controls="" src="/wordpress/wp-content/uploads/qt150-original.mp4"></video><figcaption>The first implementation (are you patient?)</figcaption></figure>
																
					</div><!--/.entry-->
				</div></div>
  </body>
</html>
